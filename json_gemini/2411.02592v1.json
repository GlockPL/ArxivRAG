{"title": "DECOUPLED DATA AUGMENTATION FOR IMPROVING IMAGE CLASSIFICATION", "authors": ["Ruoxin Chen", "Zhe Wang", "Keyue Zhang", "Shuang Wu", "Jiamu Sun", "Shouli Wang", "Taiping Yao", "Shouhong Ding"], "abstract": "Recent advancements in image mixing and generative data augmentation have shown promise in enhancing image classification. However, these techniques face the challenge of balancing semantic fidelity with diversity. Specifically, image mixing involves interpolating two images to create a new one, but this pixel-level interpolation can compromise fidelity. Generative augmentation uses text-to-image generative models to synthesize or modify images, often limiting diversity to avoid generating out-of-distribution data that potentially affects accuracy. We propose that this fidelity-diversity dilemma partially stems from the whole-image paradigm of existing methods. Since an image comprises the class-dependent part (CDP) and the class-independent part (CIP), where each part has fundamentally different impacts on the image's fidelity, treating different parts uniformly can therefore be misleading. To address this fidelity-diversity dilemma, we introduce Decoupled Data Augmentation (De-DA), which resolves the dilemma by separating images into CDPs and CIPs and handling them adaptively. To maintain fidelity, we use generative models to modify real CDPs under controlled conditions, preserving semantic consistency. To enhance diversity, we replace the image's CIP with inter-class variants, creating diverse CDP-CIP combinations. Additionally, we implement an online randomized combination strategy during training to generate numerous", "sections": [{"title": "1 Introduction", "content": "Data augmentation is extensively employed to enhance neural network performance. Traditional data augmentation, such as random shifting, cropping, and rotation, are widely used due to their simplicity and effectiveness, becoming standard practice in nearly all training algorithms. Recently, two innovative types of data augmentation have shown potential for improving image classification:\n\u2022 Image-Mixing Data Augmentation. Generate augmented images by integrating two or more randomly picked natural images at the pixel or feature level, creating virtual data between classes. The online combination paradigm allows for the efficient production of many images with extensive pixel-level variations at a low cost, yet the images often look unrealistic and face fidelity problems, as noted by (Kang & Kim, 2023;\nIslam et al., 2024).\n\u2022 Generative Data Augmentation. This method leverages generative models to create images using prompts generated manually or via textual inversion to align with class labels. However, as noted by (Islam et al., 2024), this method is not yet mature for data-rich learning scenarios. Crafting prompts that ensure model-generated images match the actual data distribution is difficult, requiring expert knowledge to describe class objects and challenges in capturing the dataset's style. Additionally, textual inversion often leads to limited image diversity due to information loss, reducing the diversity of the generated images, as mentioned by (Wang et al., 2024). Both forms of prompt guidance encounter issues of misalignment or limited variation, resulting in limited performance improvements.\nReaders can refer to Figure 1 for examples of various data augmentation methods. It is evident that a trade-off exists between semantic fidelity and diversity in these methods. Naturally, the question arises: 'How can semantic fidelity be preserved while simultaneously enhancing diversity?'\nThe prevailing practice of treating images as indivisible units in existing data augmentation methods presents a fundamental obstacle to achieving both fidelity and diversity. This whole-image paradigm, while enriching diversity, often results in excessive and detrimental variations to class-dependent objects, severely compromising fidelity. In contrast, viewing images from a disentangled perspective could alleviate this challenge by applying distinct strategies to class-dependent parts and class-independent parts: a conservative strategy on CDPs to maintain fidelity and an aggressive strategy on CIPs to enhance diversity.\nBased on this insight, we propose a novel data augmentation framework, Decoupled Data Augmentation (De-DA), which addresses the fidelity-diversity dilemma through a decoupling strategy. Specifically, we first separate images into class-dependent parts (CDPs) and class-independent parts (CIPs) using SAM (Kirillov et al., 2023), and then tailor our adaptive strategies for respective parts according to their distinct characteristics. To preserve semantic fidelity, we use class identifiers derived from intra-class CDPs as conditions to edit real CDPs with controlled strength, elaborately varying them while preserving their semantic consistency. To encourage diversity, we replace the original CIP of the images with a random CIP sampled from an inter-class image. Furthermore, we adopt an online randomized combination strategy, pairing one CDP (real or synthetic) with one CIP (cross-class real CIPs) at random positions and transformations to provide the model with various combinations during the training stage, further enhancing diversity. In summary, both conservatively translated CDPs and real CIPs align with the actual data, ensuring that the generated images maintain fidelity, while the semantic edits on CDPs and diverse CDP-CIP combinations significantly enrich variety.\nCompared to previous image-mixing methods, De-DA fuses CDPs and CIPs at the semantic level rather than the pixel level, thereby enhancing fidelity. De-DA also distinguishes itself from other generative methods via applying textual inversion (Gal et al., 2022) and SDEdit (Meng et al., 2021) to isolated CDPs instead of the entire image, thus avoiding the negative effects of noisy information in the image. Furthermore, De-DA's decouple-and-combine paradigm enables the production of more images at a lower cost than prior generative methods. Our contributions include:\n\u2022 De-DA shows a solution to the fidelity-diversity dilemma in previous data augmentation methods by decoupling images into class-dependent parts and class-independent parts and managing these parts adaptively.\n\u2022 To our knowledge, we are the first to apply textual inversion and SDEdit to isolated CDPs instead of entire images in the field of data augmentation, which minimizes the negative impact from the noisy information in the images. Additionally, we propose truncated-timestep textual inversion to reduce the computational burden, enhancing practicability."}, {"title": "2 Related Work", "content": "Image-mixing and generative data augmentation methods are two approaches akin to De-DA. Table 1 offers an overview of prominent image-mixing and generative data augmentation methods, with Figure 2 depicting their mechanisms.\nImage-Mixing Data Augmentation. Image mixing is a non-generative data augmentation technique used during training to provide classifiers with numerous mixed images, which helps smooth decision boundaries and enhance image classification (Zhang et al., 2018). Early methods, such as Mixup (Zhang et al., 2018) and CutMix (Yun et al., 2019), create new images by linearly combining two images at the pixel or patch level. However, this mixing can compromise the semantic integrity of class-specific objects. To address this, advanced approaches like SaliencyMix (Uddin et al., 2020), SnapMix (Huang et al., 2021), PuzzleMix (Kim et al., 2020a), CoMixup (Kim et al., 2020b), and GuidedMixup (Kang & Kim, 2023) use saliency maps to ensure important regions are preserved. Despite this guidance, class-specific objects may still be distorted, producing virtual images that deviate significantly from the actual data distribution, resulting in limited semantic fidelity. In contrast, De-DA addresses this issue by combining CDPs and CIPs at the semantic level, rather than at the pixel level, to preserve fidelity.\nGenerative Data Augmentation. Generative data augmentation leverages advanced text-to-image models to create new images. Initial research (He et al., 2023) demonstrates that text-to-image diffusion can generate synthetic data that effectively enhances classification performance in data-scarce scenarios, particularly when conditioned on"}, {"title": "3 Decoupled Data Augmentation", "content": "De-DA is a framework designed to address the fidelity-diversity trade-off through a decoupling strategy. As illustrated in Figure 3, it initially separates class-dependent parts (CDPs) and class-independent parts (CIPs) using SAM, which forms the foundation of De-DA. De-DA employs class identifiers derived from intra-class CDPs to conditionally edit real CDPs, then pairs a real or synthetic CDP with randomly selected CIPs to create new images.\nDecoupling Images into CDPs and CIPs. The initial phase of De-DA involves separating training samples into class-dependent parts and class-independent parts, as the basement of our De-DA. Practically, we utilize Lang-SAM (Kirillov et al., 2023), an off-the-shelf, prompt-based segmentation tool, to obtain segmentation masks for class-dependent parts using domain or class name prompts (e.g., \"bird\" for CUB-200-2011). If multiple CDP masks are generated in one image, they are aggregated into a single mask to ensure complete coverage of the CDP. Masked\nregions are labeled as CDPs, while remaining image portions are classified as CIPs. We apply alpha pyramid image blending to fill the missing areas in the segmented CIPs.\nConservative Generation of CDP. Following prior research (Trabucco et al., 2024; Zhou et al., 2023), we use textual inversion to derive identifiers for each class and employ SDEdit to transform natural images conditioned on these prompts. Unlike previous methods, our approach applies textual inversion and SDEdit solely to the class-dependent parts (CDPs) rather than the entire image. This strategy addresses two critical issues: (1) Learning class prompts from CDPs ensures that the derived concept accurately corresponds to class-specific objects. (2) Applying SDEdit to CDPs prevents interference from class-independent parts, enhancing SDEdit's performance. This contrast is shown in Figure 4. However, applying textual inversion and SDEdit to CDPs is challenging, as traditional methods are only designed for RGB images. To accommodate transparent CDPs, we employ LayerDiffuse (Zhang & Agrawala, 2024), which equips diffusion models with a dedicated transparency encoder and decoder capable of encoding the alpha channel into latents and decoding latents into RGBA images. Specifically, our transparency image-to-image pipeline operates as follows: we first add noise $ \\epsilon \\sim N(0, 1) $ to real CDPs (indicated by $ x_f $) at timestep $ [T_s] $, where $ s \\in [0, 1] $ indicates the generation strength ($ s = 0 $ refers no editing and $ s = 1.0 $ indicates generation from scratch), followed by denoising:\n$ x_{[ST]} = x_f + \\sqrt{1 - [ST]} \\epsilon $  (1)\nDenoise $ x_{[ST]} $ using LayerDiffuse reverse diffusion conditioned on the learned identifier $ V_{CDP} $ (We will discuss later), starting from the timestep $ [T_{[ST]}] $ to 0, yielding the final edited CDP $ x_0 $.\n$ x_{t-1} = \\epsilon_\\theta (x_t, t, V_{CDP}), t = [ST],..., 1 $  (2)\nHere, $ V_{CDP} $ is the class identifier derived from each class's real CDPs using textual inversion. To alleviate the computational cost of textual inversion, we apply truncated-timestep textual inversion tailored for SDEdit. In this method, the prompts are trained only on the timestep from 0 to $ [ST_s] $ instead of all timesteps, promoting quicker convergence. Formally, truncated-timestep textual inversion learns $ V_{CDP} $ by\n$ V_{CDP} = arg \\min_C E_{t \\in [0, [ST_s]]} [|\\epsilon - \\epsilon_\\theta (x_t, C, t)||] $  (3)\nInter-class Random Sampling of Class-Independent Parts. Our approach to handling Class-Independent Parts (CIPs) derives from observations of real datasets. Specifically, real datasets exhibit significant intra-class uniformity but restricted cross-class diversity. For example, in the CUB-200-2011 dataset (Wah et al., 2011), 90% of Common Yellowthroat images feature branches in the background. Albatross images often show water surfaces, while Jaeger"}, {"title": "4 Experiments", "content": "In this section, we comprehensively analyze De-DA by answering the following questions:\nQ1: Can De-DA outperform other methods in conventional classification tasks?\nQ2: Can De-DA still surpass other methods in various settings such as data-scarce scenarios?\nQ3: How do the modules in our approach and the hyperparameters affect our method's performance?\nTo answer Q1, in Section 4.1, we compare De-DA to peer methods across different domain-specific datasets. In Section 4.2, we address Q2 by examining its performance in data-scarce scenarios, multi-label classification, and a replaced-background dataset, demonstrating the performance gains of De-DA in various contexts. For Q3, we conduct extensive ablation studies on each module and hyperparameter to assess their impacts and explain our chosen settings in Section 4.3."}, {"title": "4.1 Comparison on Conventional Classification", "content": "Experimental Setting. We tested data augmentation methods on three classical domain-specific datasets: CUB-200-2011 (Wah et al., 2011), Aircraft (Maji et al., 2013), and Stanford Cars (Krause et al., 2013), following the experimental settings of DiffuseMix (Wang et al., 2024) and Diff-Mix (Wang et al., 2024). Experiments are conducted using three smaller models-ResNet-18 (He et al., 2016), ResNet-50 (He et al., 2016), and DenseNet121 (Huang et al., 2017)\u2014as well as a large pretrained model, ViT-B/16 (Dosovitskiy et al., 2021). To ensure fairness, we adhere to prior work for our training implementations. For the small models, we followed the GuidedMix implementation (Kang & Kim, 2023), training from scratch with cross-entropy loss. For ViT-B/16, we follow Diff-Mix, fine-tuning the ViT model with label smoothing loss. Unless otherwise specified, in De-DA, we set the expansion multiplier for each real CDP to 3. The generation strength for textual inversion and SDEdit is fixed at s = 0.4. During training with De-DA, nautral images are replaced with augmented data with a probability $ P_{aug} = 0.5 $. For CDP-CIP combinations, the probability of using mixed CDP $ P_{mix} $ is 0.5, and the synthetic CDP is used with a probability $ P_{syn} = 0.25 $.\nPeer Methods. We compare De-DA with ten peer methods, including six image-mixing and four generative approaches. The image-mixing methods include: (1) Mixup (Zhang et al., 2018), which linearly combines pairs of images and their labels; (2) CutMix (Yun et al., 2019), which replaces a portion of one image with a patch from another; (3) SaliencyMix (Uddin et al., 2020); (4) Co-Mixup (Kim et al., 2020b); and (5) Guided-AP and (6) Guided-SR (Kang & Kim, 2023), which use saliency maps to guide the mixing process, alleviating the issue of corrupted class-specific objects. The generative methods include: (1) Real-Guidance (He et al., 2023), which augments the dataset using label-name guidance at a fixed low strength s = 0.1; (2) DiffuseMix (Islam et al., 2024), which creates hybrid images from different conditional prompts using fractal blending; (3) DA-Fusion (Trabucco et al., 2024), which"}, {"title": "4.2 Comparisons on Various Tasks", "content": "To evaluate performance in data-scarce scenarios, we create a version of the CUB-200-2011 dataset by randomly selecting 10 images per class, following the settings of DiffuseMix (Islam et al., 2024). To assess how data augmentation aids in learning background-robust features, we test accuracy on the Waterbird dataset, which combines bird foregrounds from CUB-200-2011 with backgrounds from the Places dataset (Zhou et al., 2017). Here, (Waterbird, Water) indicates (the type of bird, the type of background). We further compare De-DA to other methods on the multi-label classification dataset Pascal (Everingham et al., 2010) to validate its performance in improving multi-label classification. Additionally, we demonstrate that De-DA is compatible with other data augmentation techniques, such as RandAugment(Cubuk et al., 2020).\nComparison in Data-Scarce Scenarios. The results on the data-scarce CUB-200-2011 dataset are shown in Table 4. We observe that De-DA significantly outperforms all other methods, achieving an accuracy of 54.52% on ResNet-18, which is 8.77% higher than the second-best method. This remarkable improvement is due to the substantially larger number of augmented images generated by De-DA's online combination strategy, which compensates for the lack of data. Furthermore, compared to image-mixing methods that also produce a large number of mixed images, De-DA shows significant improvement, demonstrating that the images generated by De-DA are much more effective due to their high diversity and fidelity.\nComparison on Background Robustness. Table 5 presents the experimental results on the Waterbird dataset, which evaluates the model's robustness against background replacement, rather than relying on the background. De-DA clearly outperforms other methods in all categories, achieving an average improvement of 5.98%, which is 3.70% higher than the second-best method, Diff-Mix. This validates our earlier statement that De-DA helps the model learn CIP-independent features, enabling the model to focus on the class-specific object for classification.\nComparison on Multi-Label Classification. Figure 5a compares different methods on a multi-label classification. De-DA achieves 23.02% on ResNet-18 and 22.05% on ResNet-50, surpassing all other methods by a non-trivial margin, demonstrating that De-DA can effectively improve multi-label classification through mixed-CDPs augmented samples.\nComparing Diversity of Generative Data Augmentation. Figure 5b quantitatively compares De-DA to other generative methods using Peak Signal-to-Noise Ratio (PSNR). A lower PSNR value indicates higher diversity. The results show that De-DA achieves greater diversity than the other methods. Figure 6 presents the examples of different genera-tive data augmentation methods, validating our aforementioned statement. Specifically, we observe that (1) DiffuseMix diversifies images from a stylistic perspective rather than a semantic level. While it shows robustness to adversarial noise, it is less effective in improving test accuracy. (2) Real-Guidance slightly modifies images using SDEdit at a low strength. Although it maintains semantic consistency, it struggles with background invariance. (3) Da-Fusion has the same issue as Real-Guidance. (4) Diff-Mix uses identifiers from other classes to transform the input image, aiming to vary the background while preserving the semantic fidelity of the foreground. However, it often significantly alters the foreground greatly without effectively diversifying the background."}, {"title": "4.3 Ablation Studies", "content": "Experimental Setting. We first evaluate the impact of each hyperparameter of De-DA on CUB-200-2011 with ResNet-18. Then, to evaluate the contribution of each component of our approach, an ablation study is conducted by incrementally adding each component. We focused on components including synthetic CDP, CIP replacement, the randomized combination and the CDP mixing technique. The experimental baseline model is ResNet-18 and the dataset is Aircraft.\nAblation on Hyperparameters. The impact of the De-DA's hyperparameters is shown in Figure 8. The experimental results lead to the following conclusions: (1) The performance of De-DA improves as $ P_{aug} $ increases from 0.00 to 0.50, peaking at 0.5, indicating that a balanced approach of using both generated and original data is optimal. (2) The highest performance for three datasets is observed at $ P_{syn} = 0.25 $. Using either no synthetic CDP or only synthetic\nCDPs results in a performance decline. (3) We observe that CDP mixing leads to significant improvement on CUB-200-2011 at $ P_{mix} = 0.25 $, demonstrating that CDP mixing effectively boosts classification. (4) A generation strength of $ s = 0.4 $ consistently yields improvement across the three datasets. Peak performance occurs at different strengths: Aircraft and Standford Car peak at $ s = 0.4 $, while CUB-200-2011 peaks at $ s = 0.2 $, possibly because the inter-class images in the CUB-200-2011 are more similar than in the other two datasets. However, too high a strength can result in performance decline, e.g. $ s = 0.8 $ achieves a lower accuracy than $ s = 0.0 $ on CUB-200-2011. These ablation studies explain our hyperparameter choices. The results indicate that De-DA is relatively robust to hyperparameter settings. The impact of the expansion multiplier is discussed in the appendix."}, {"title": "Ablation on Each Module of De-DA", "content": "Figure 9 shows the module ablation results. (a) represents vanilla training without data augmentation. (b) involves replacing the CDP of the original sample with a same-size synthetic CDP at the same position, with a probability $ P_{syn} = 0.25 $. This improves upon the baseline, validating the effectiveness of semantically edited CDPs in image classification. (c) replaces the CDP with a synthetic one using a random combination strategy that varies the position and size of the CDPs, further enhancing (b) by 1.34 %, indicating that random combination strategy do compile with CDP editing. (d) employs only the CIP replacement strategy without CDP editing or random combination, effectively boosting accuracy, which highlights the importance of CIP diversity. (e) utilizes an inter-class CIP strategy with random combination, further improving performance, indicating that CIP replacement and random combination are two mutually reinforcing mechanisms. (f) incorporates the strategies of CDP editing, inter-class CIP replacement, and random combination, achieving an accuracy of 80.15%, which surpasses the accuracies of all other peer methods. (g) is the complete version of De-DA, incorporating the CDP-mixing strategy. This integration further enhances the performance."}, {"title": "5 Conclusions and Future Works", "content": "In this work, we propose an innovative approach to address the fidelity-diversity dilemma through decoupled data augmentation (De-DA). We decouple images into class-dependent and class-independent parts, with CDP maintaining semantic fidelity and CIP enhancing diversity. The decouple-and-combine strategy of De-DA enables the production of faithful and diverse images at scale, with lower computational costs compared to generative methods. Experiments validate that De-DA effectively improves conventional classification, data-scarce classification, and multi-label classification. De-DA also helps models learn background-independent features. Future work could explore several directions: (1) decoupling images in a more fine-grained manner to improve performance in fine-grained retrieval tasks; (2) developing adaptive strategies for designing generation strength based on dataset characteristics; (3) exploring new CDP-CIP combination approach to further boost diversity."}, {"title": "A Appendix", "content": "All of our experiments are conducted on a system equipped with 96 CPU cores (Platinum 8255C @ 2.50GHz) and 8 GPU Tesla V100 cards. For doupling the images into CDPs and CIPs, we employ LangSAM 3 with prompt guided. For inpainting the missing part of For the training implementations of ResNet-18, ResNet-50, and DenseNet-121, we adhere to the official training script from GuidedMix (Kang & Kim, 2023) 4. For the training of Vit-B/16, we followed the official implementation provided by Diff-Mix (Wang et al., 2024) 5. Additionally, we introduce the hyperparameters related to decoupling, truncated-timestep textual inversion and SDEdit. Specific values for these hyperparameters are provided in Table 6 and Table 7."}, {"title": "A.2 Ablation Study", "content": "The impact of the expansion multiplier is presented in Table 8, which shows the accuarcy at different expansion multipliers \u00d71, x3, x6, \u00d710. De-DA achieves optimal accuracy at a multiplier of \u00d73 for both Aircraft and Standford Car datasets, while Cub-200-2011 peaks at \u00d76. The difference likely stems from the distinct data distributions inherent to each dataset. Notably, increasing the expansion multiplier does not necessarily improve performance, a phenomenon also observed in (Trabucco et al., 2024; Wang et al., 2023). This suggests that excessive data augmentation may bias the model towards the generated data, hindering its ability to generalize effectively to real data."}]}