{"title": "Don't flatten, tokenize! Unlocking the key to SoftMoE's efficacy in deep RL", "authors": ["Ghada Sokar", "Johan Obando-Ceron", "Aaron Courville", "Hugo Larochelle", "Pablo Samuel Castro"], "abstract": "The use of deep neural networks in reinforcement learning (RL) often suffers from performance degradation as model size increases. While soft mixtures of experts (SoftMoEs) have recently shown promise in mitigating this issue for online RL, the reasons behind their effectiveness remain largely unknown. In this work we provide an in-depth analysis identifying the key factors driving this performance gain. We discover the surprising result that tokenizing the encoder output, rather than the use of multiple experts, is what is behind the efficacy of SoftMoEs. Indeed, we demonstrate that even with an appropriately scaled single expert, we are able to maintain the performance gains, largely thanks to tokenization.", "sections": [{"title": "1. Introduction", "content": "Deep networks have been central to many of the successes of reinforcement learning (RL) since their initial use in DQN (Mnih et al., 2015a), which has served as the blueprint (in particular with regards to network architecture) for most of the followup works. However, recent works have highlighted the need to better understand the network architectures used, and question whether alternate choices can be more effective (Graesser et al., 2022; Obando Ceron et al., 2024; Sokar et al., 2022, 2023). Indeed, Obando Ceron* et al. (2024) and Willi* et al. (2024) demonstrated that the use of Mixtures-of-Experts (MoEs) (Shazeer et al., 2017), and in particular soft mixtures-of-experts (SoftMoEs) (Puigcerver et al., 2024), yields models that are more parameter efficient. Obando Ceron* et al. (2024) hypothesized that the underlying cause of the effectiveness of SoftMoEs was in the induced structured sparsity in the network architecture. While some of their analyses did seem to support this, the fact that performance gains were observed even with a single expert seems to run counter to this hypothesis.\nIn this work we seek to better understand the true underlying causes for the effectiveness of SoftMoEs in RL settings. Specifically, we perform a careful and thorough investigation into the many parts that make up SoftMoEs aiming to isolate the impact of each. Our investigation led to the surprising finding that the core strength of SoftMoEs lies in the tokenization of the outputs of the convolutional encoder.\nThe main implications of this finding are two-fold. On the one hand, it suggests that we are still under-utilizing experts when incorporating MoEs in deep RL, given that we are able to obtain comparable performance with a single expert (see Figure 1). On the other hand, this finding has broader implications for deep RL agents trained on pixel-based environments, suggesting that the common practice of flattening the multi-dimensional outputs of the convolutional encoders is ineffective.\nOur main contributions can be summarized as follows:\n\u2022 We conduct a series of analyses to identify the key factors that allow soft Mixture of Experts (SoftMoEs) to effectively scale RL agents.\n\u2022 We demonstrate that tokenization plays a crucial role in the performance improvements.\n\u2022 We demonstrate that experts within the SoftMoEs are not specialized to handle specific subsets of tokens and exhibit redundancy. We explore the use of recent techniques for improving network utilization and plasticity to mitigate this redundancy.\nThe paper is organized as follows. We introduce the necessary background for RL in Section 2 and MoEs in Section 3; we present our experimental setup in Section 4.1, and present our findings from Section 4.2 to Section 6; finally, we discuss related works in Section 7 and provide a concluding discussion in Section 8."}, {"title": "2. Online Deep Reinforcement Learning", "content": "In online reinforcement learning, an agent interacts with an environment and adjusts its behaviour based on the reinforcement (in the form of rewards or costs) it receives from the environment. At timestep t, the agent inhabits a state xt \u2208 X (where X is the set of all possible states) and selects an action at \u2208 A (where A is the set of all possible actions) according to its behaviour policy \u03c0 : X \u2192 P(A); the environment's dynamics yield a new state xt+1 \u2208 X and a numerical reward rt \u2208 R. The goal of an RL agent is to find a policy \u03c0* that maximizes V\u03c0(x) := \u2211\u221et=0 [\u03b3trt|x0 = x, at ~ \u03c0(xt)], where \u03b3\u2208 [0, 1) is a discount factor. In value-based RL, an agent maintains an estimate of Q\u03c0 : X \u00d7 A \u2192 R, defined as Q\u03c0 (x, a) = \u2211\u221et=0 [\u03b3trt|x0 = x, a0 = a, at ~ \u03c0(x\u012b)]; from any state x \u2208 X, a policy a can then be improved via \u03c0(x) = arg maxa\u2208 A Q\u03c0 (x, a) (Sutton and Barto, 1998).\nIn its most basic form, value-based deep reinforcement learning (DRL) extends this approach by using neural networks, parameterized by \u03b8, to estimate Q. While the network architectures used may vary across environments, the one originally introduced by Mnih et al. (2015a) for DQN has served as the basis for most. Given that its inputs consist of frames of pixels from Atari games (Bellemare et al., 2013), this network consisted of a series of convolutional layers followed by a series of fully-connected layers; for the remainder of this work we will refer to the convolutional layers as the encoder. The top panel of Figure 2 displays a simplified version of this network, where the encoder produces a 3-dimensional tensor with dimensions (h, w, d); this tensor is flattened to a vector of length h * w * d,"}, {"title": "3. Mixtures of Experts", "content": "While there have been a number of architectural variations proposed, in this work we focus on Mixtures of Experts as used by Obando Ceron* et al. (2024). Mixtures of experts (MoEs) were originally proposed as a network architecture for supervised learning tasks using conditional computation, which enabled training very large networks (Fedus et al., 2022; Shazeer et al., 2017). This was achieved by including MoE modules which route a set of input tokens to a set of n sub-networks, or experts; the output of these experts are then combined and fed through the rest of the outer network. The routing/gating mechanism and the expert sub-networks are all trainable. In top-k routing, we consider two methods for pairing tokens and experts. In token choice routing, each token gets an assignation to k experts, each of which has p slots; this may result in certain tokens being dropped entirely by the MoE module and, more critically, certain experts may receive no tokens (Gale et al., 2023). To overcome having load-imbalanced experts, Zhou et al. (2022) propose expert choice routing, where each expert chooses p tokens for its p slots, which results in better expert load-balancing. See the bottom row of Figure 2 for an illustration of this. In most of our analyses, we focus on expert choice routing as it demonstrates higher performance than token choice routing (see Appendix B.1 for full empirical comparison between the two across varying number of experts).\nPuigcerver et al. (2024) introduced SoftMoEs, which replace the hard token-expert pairing with a \"soft\" assignation. Specifically, rather than having each expert slot assigned to a specific token, each slot is assigned a (learned) weighted combination of all the tokens. Thus, each expert receives p weighted averages of all the input tokens. See the middle row of Figure 2 for an illustration of this."}, {"title": "4. Understanding the impact of the SoftMoE components", "content": "While Obando Ceron* et al. (2024) demonstrated the appeal of SoftMoEs for online RL agents, it was not clear whether all of the components of SoftMoEs were important for the performance gains, or whether some were more critical. Further, it was observed that SoftMoEs failed to provide similar gains in actor-critic algorithms such as PPO (Schulman et al., 2017) and SAC (Haarnoja et al., 2018). This section aims to isolate the impact of each of these components."}, {"title": "4.1. Experimental setup", "content": "The genesis of our work is to provide a deeper understanding of SoftMoEs in online RL. For this reason, we will mostly focus on SoftMoEs applied to the Rainbow agent (Hessel et al., 2018) with 4 experts"}, {"title": "4.2. Analysis of SoftMoE Components", "content": "At a high level, SoftMoE modules for online RL agents consist of (i) the use of a learnable tensor \u03a6 used to obtain the dispatch (\u03a6\u0189) and combine (\u03a6\u2282) weights; (ii) processing p input slots per expert; (iii) the architectural dimensions (network depth and width); (iv) the use of n experts; and (v) the tokenization of the encoder output. The middle row of Figure 2 provides an illustration of these. In this section we discuss the first four with empirical evaluation demonstrated in Figures 1 and 4, and devote a separate section for tokenization, which turns out to be crucial for performance.\nProcessing combined tokens To assess the impact of SoftMoE's weighted combination of tokens via the learnable weights \u03a6, we compare it with expert choice routing, where experts select sets of (non-combined) tokens. While combined tokens yield improved performance, expert choice routing itself demonstrates substantial gains over the baseline and effectively scales the network (see group [i] in Figure 4). Thus, combined tokens alone do not explain SoftMoE's efficacy.\nExpert specialization Each expert in a MoE architecture has a limited set of slots: with token/expert selection each slot corresponds to one token, while in SoftMoE each slot corresponds to a particular weighted combination of all tokens. This constraint could encourage each expert to specialize in certain types of tokens in a way that is beneficial for performance and/or scaling. To investigate,"}, {"title": "5. Don't flatten, tokenize!", "content": "A crucial structural difference remains in how SoftMoE and the baseline model process the encoder's output: while the baseline model flattens the output of the final convolutional layer, SoftMoE divides it into distinct tokens (see Section 3). We conduct a series of experiments to properly assess the impact of this choice, going from the standard baseline architecture to a single-expert SoftMoE.\nTokenized baseline Our first experiment in this series simply adds tokenization to the baseline architecture. Specifically, given the 3-dimensional encoder output with shape [h, w, d], we replace the standard flattening (to a vector of dimension h * w * d) with a reshape to a matrix of shape [h *w, d]; then, prior to the final linear layer, we either sum over the first dimension or take their average. Since this operation is not immediately applicable to Rainbow (due to Noisy networks), we explored this experiment with Rainbow-lite, a version of Rainbow in Dopamine which only includes C51 (Bellemare et al., 2017), multi-step returns, and prioritized experience replay. Figure 5 demonstrates that this simple change can yield statistically significant improvements for Rainbow-lite, providing strong evidence that tokenization plays a major role in the efficacy of SoftMoE.\nSingle expert processing all tokens sparsely To assess the influence of tokenization within the MoE framework, while ablating all other four components studied in Section 4.2, we explore using a single expert to process all input tokens. Each token is sparsely processed in one of the single expert slots. This corresponds to expert choice-1. In line with the \u201cexpert width\" investigation from the last section, we study expert choice-1 with the scaled expert dimensionality to match the scaled baseline one. As Figure 6 shows, even with a single expert the performance is comparable to that of SoftMoE with multiple experts. This suggests that tokenization is one of the major factors in the performance gains achieved by SoftMoE.\nCombined tokenization is a key driver We further investigate the impact of combined tokenization in the single expert setting. To this end, we study SoftMoE-1, with scaled expert dimensionality as before. As Figure 6 shows, using combined tokens with a single expert brings the performance even closer to the multiple expert setting. This further confirms that (combined) tokenization is the key driver of SoftMoE's efficacy."}, {"title": "5.1. Additional analyses", "content": "The main takeaway from the previous section is that SoftMoE with a single (scaled) expert is as performant as the variant with multiple unscaled experts. To better understand this finding, in this section we perform an extra set of experiments with this variant of SoftMoE.\nAre all types of tokenization equally effective? We compare the different tokenization types presented in Section 3, in addition to two new types of tokenization, which serve as useful references: PerPatch tokenization splits the 3-dimensional [h, w, d] tensor into p patches, over which average pooling is performed resulting in a new tensor of dimensions [h/p,w/p, d]; Shuffled is similar to PerConv, except it first applies a fixed permutation on the encoder output. See Figure 3 for an illustration of these. The comparison of the different tokenization types is summarized in Figure 7. Consistent with the findings of Obando Ceron* et al. (2024), PerConv performs best, and in particular outperforms PerFeat. It is interesting to observe that PerPatch is close in performance to PerConv, while Shuffled is the worst performing. The degradation from PerPatch is likely due to the average pooling and reduced dimensionality; the degradation from shuffling is clearly indicating that it is important to maintain the spatial structure of the encoding output. Combining these results with"}, {"title": "6. Calling all experts!", "content": "Our findings in Sections 4 and 5 reveal that the main benefit of using SoftMoE architectures is in the tokenization and their weighted combination, rather than in the use of multiple experts. The fact that we see no evidence of expert specialization suggests that most experts are redundant (see Figure 4). Rather than viewing this as a negative result, it's an opportunity to develop new techniques to improve expert utilization and maximize the use of MoE architectures in RL.\nAs a first step in this direction, we adopt existing techniques designed to improve network utilization: (1) periodic reset of parameters (Nikishin et al., 2022) and (2) using Shrink and Perturb (S&P) (Ash and Adams, 2020), as used by D'Oro et al. (2022) and Schwarzer* et al. (2023). We perform these experiments on Rainbow and DER with SoftMoE-4. We explore applying resets and S&P to all experts and to a subset of experts, where the experts to reset are those that have the highest fraction of dormant neurons (Sokar et al., 2023). For Rainbow, we reset every 20M environment steps, whereas for DER we reset every 200k environment steps.\nInterestingly, the effectiveness of these techniques varied across different reinforcement learning agents. While weight reset and S&P did not improve expert utilization in the Rainbow agent, they led to enhanced performance in the sample-efficient DER agent (Figure 11). Within the DER agent, resetting a subset of experts proved more beneficial than resetting all, while applying S&P to all experts yielded the most significant performance gains. These findings highlight the need for further research into methods that can optimize expert utilization and improve performance in reinforcement learning with SoftMoE."}, {"title": "7. Related Works", "content": "Mixture of Experts Mixture of Experts (MoEs) have demonstrated remarkable success in scaling language and computer vision models to trillions of parameters (Abbas and Andreopoulos, 2020; Fedus et al., 2022; Lepikhin et al., 2020; Pavlitskaya et al., 2020; Wang et al., 2020; Yang et al.,"}, {"title": "8. Conclusions", "content": "We began our investigation by evaluating the components that make up a SoftMoE architecture, so as to better understand the strong performance gains reported by Obando Ceron* et al. (2024) when applied to value-based deep RL agents. One of the principal, and rather striking, findings is that tokenizing the encoder outputs is one of the primary drivers of the performance improvements. Since tokenization is mostly necessary for pixel-based environments (where encoders typically consist of convolutional layers), it can explain the absence of performance gains observed by Obando Ceron* et al. (2024) and Willi* et al. (2024) when using MoE architectures in actor-critic algorithms, since these were evaluated in non-pixel-based environments.\nMore generally, our findings suggests that the common practice of flattening the outputs of deep RL encoders is sub-optimal, as it is likely not maintaining the spatial structure output by the convolutional encoders. Indeed, the results with PerPatch and Shuffled tokenization in Figure 7 demonstrate that maintaining this spatial structure is important for performance. This is further confirmed by our"}, {"title": "A. Experimental Details", "content": "Tasks We evaluate Rainbow on 20 games from the Arcade Learning Environment (Bellemare et al., 2013). We use the same set of games evaluated by Obando Ceron* et al. (2024) for direct comparison. This includes: Asterix, SpaceInvaders, Breakout, Pong, Qbert, DemonAttack, Seaquest, WizardOf Wor, RoadRunner, BeamRider, Frostbite, CrazyClimber, Assault, Krull, Boxing, Jamesbond, Kangaroo, UpNDown, Gopher, and Hero. In addition, we run some of our main results on all suite of 60 games.\nOn Atari100k benchmark, we evaluate on the full 26 games. These are: Alien, Amidar, Assault, Asterix, BankHeist, BattleZone, Boxing, Breakout, ChopperCommand, CrazyClimber, DemonAttack, Freeway, Frostbite, Gopher, Hero, Jamesbond, Kangaroo, Krull, KungFuMaster, MsPacman, Pong, PrivateEye, Qbert, RoadRunner, Seaquest, UpNDown.\nHyper-parameters We use the default hyper-parameter values for DQN (Mnih et al., 2015a), Rainbow (Hessel et al., 2018), and DER (Van Hasselt et al., 2019) agents. We share the details of these values in Table 1."}, {"title": "B. Additional Experiments", "content": ""}, {"title": "B.1. Comparison between token choice routing and expert choice routing", "content": "We focus in our analysis on expert choice routing, where experts select tokens, as opposed to token choice routing, where tokens select experts. We compare these two routing strategies across different numbers of experts using a Rainbow agent with a ResNet architecture.\nFigure 12 demonstrates that expert choice routing consistently outperforms token choice routing across all numbers of experts. Moreover, this performance gap widens as the number of experts increases, highlighting the advantages of expert choice routing in addressing the limitations of token choice routing."}, {"title": "B.2. Don't flatten tokenize", "content": "In this section, we provide further analysis on the impact of tokenization and their weight combination on models with larger number of experts. To this end, we investigate SoftMoE-8 and compare it against the scaled SoftMoE-1 and ExpertChoice-1 (non-combined tokenization). Figure 13 shows that single scaled expert can reach a performance close to 8 experts. In addition, consistent with our results, combined tokenization adds benefits over the sparse one. Figure 14 shows a comparison between the learning curves of single scaled expert and multiple ones."}, {"title": "B.3. More agents", "content": "Same as our finding for DQN using SoftMoE-4, we observe that single expert has a closer performance to the scaled baseline than SoftMoE-8, as shown in Figure 15. Further investigation is needed to fully understand the behavior of SoftMoE on DQN."}, {"title": "B.4. Expert Utilization", "content": "In this section we provide more analysis to understand the utilization of experts in multi expert setting and the effect of existing techniques on improving plasticity.\nSome experts are redundant In Section 4.2, we showed that expert are not specialized in learning subset of tokens. We expand this analysis by assessing the redundancy of experts. To this end, we prune two experts of SoftMoE-4 during training. We studied two pruning schemes: pruning the two experts once and gradually prune throughout training. As shown in Figure 16, pruning these experts does not change the performance of the agent.\nReset experts and router weights In investigating current techniques to improve expert utilization, we explore reset either experts weights only or both experts and router weights. As illustrated in Figure 17, both methods do not help in improving utilization, with resetting the router weights leads to worse performance."}]}