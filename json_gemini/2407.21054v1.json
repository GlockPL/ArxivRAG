{"title": "Sentiment Reasoning for Healthcare", "authors": ["Khai Le-Duc", "Khai-Nguyen Nguyen", "Bach Phan Tat", "Duy Le", "Jerry Ngo", "Long Vo-Dang", "Anh Totti Nguyen", "Truong-Son Hy"], "abstract": "Transparency in AI decision-making is crucial in healthcare due to the severe consequences of errors, and this is important for building trust among AI and users in sentiment analysis task. Incorporating reasoning capabilities helps Large Language Models (LLMs) understand human emotions within broader contexts, handle nuanced and ambiguous language, and infer underlying sentiments that may not be explicitly stated. In this work, we introduce a new task Sentiment Reasoning - for both speech and text modalities, along with our proposed multimodal multitask framework and dataset. Our study showed that rationale-augmented training enhances model performance in sentiment classification across both human transcript and ASR settings. Also, we found that the generated rationales typically exhibit different vocabularies compared to human-generated rationales, but maintain similar semantics. All code, data (English-translated and Vietnamese) and models are published online.", "sections": [{"title": "Introduction", "content": "The global market for sentiment analysis is projected to expand from an estimated value of US$4 billion in 2023 to US$10.1 billion by 2030, exhibiting a compound annual growth rate (CAGR) of 14.2% over the forecast period from 2023 to 2030 (Inc, 2024). In recent years, speech sentiment analysis has emerged as a significant interdisciplinary field at the intersection of natural language processing (NLP), machine learning, and automatic speech recognition (ASR). This field focuses on the automated detection and interpretation of human emotions and attitudes conveyed through speech, overcoming the limitations of text-based sentiment analysis (Murugaiyan and Uyyala, 2023) by analyzing the attitude expressed through human voice.\nSentiment analysis plays a pivotal role across diverse domains, with healthcare being particularly significant. In healthcare customer service, it facilitates real-time evaluation of customer satisfaction, enhancing empathetic and responsive interactions (Xia et al., 2009; Na et al., 2012). Moreover, sentiment analysis aids in monitoring the emotional well-being of patients (Cambria et al., 2012a), including those with mental health issues such as suicide (Pestian et al., 2012). However, these studies only work on text-based sentiment analysis rather than speech-based sentiment analysis.\nDespite its potential, speech sentiment analysis presents several technical challenges. First, speech signals are inherently noisy (Rajnoha and Poll\u00e1k, 2011) and exhibit significant variability due to differences in accents (Turan et al., 2020), speaking styles (Shafran and Rose, 2003), and recording conditions (Yamagishi et al., 2010), complicating the extraction of reliable acoustic features (Chen et al., 2017). Second, emotions are subjective (Wearne et al., 2019), complex (Golan et al., 2006), and multidimensional, making accurate categorization difficult even for humans (Kuusikko et al., 2009), thereby necessitating the role of explainable artificial intelligence (AI). Third, given the critical nature of healthcare decisions, where errors can have severe consequences, transparency in AI decision-making is essential to build trust among machines, healthcare professionals, and patients (Antoniadi et al., 2021).\nTo tackle these challenges, we introduce a novel multimodal framework for a novel task: Sentiment Reasoning. Reasoning in AI is crucial for sentiment analysis because it enables deeper understanding beyond surface-level sentiment polarity. By incorporating reasoning capabilities, AI can contextualize emotions within broader discourse, account for nuanced expressions, handle ambiguous language, and infer underlying intent or sentiment that may not be explicitly stated. This enhances the accuracy and reliability of sentiment analysis by considering complex human communication factors that simplistic methods may overlook. Our contributions are as follows:\n1.  We introduce a new task: Sentiment Reasoning for both speech and text modalities, along with our dataset MultiMed-SA\n2.  We propose our novel multimodal speech-text sentiment reasoning framework\n3.  We empirically evaluate the baselines on our dataset using state-of-the-art backbone models\n4.  We provide in-depth analysis of rationale / Chain-of-Thought (CoT)-augmented training\nAll code, data (English-translated and Vietnamese) and models are published online\u00b9."}, {"title": "Data", "content": "The dataset employed for constructing the sentiment reasoning dataset was VietMed (Le-Duc, 2024), recognized as the largest, most generalizable, and publicly accessible medical ASR dataset in the world. We then annotated sentiment labels and their corresponding rationales. This dataset comprises real-world doctor-patient conversations covering a broad spectrum of all available ICD-10 codes (all medical topics), wherein doctors diagnose conditions, explain medical issues, and provide treatment advice.\nDetails of data annotation pipeline, annotation guidelines, data quality control, and data statistics are shown Section B in the Appendix."}, {"title": "Data Statistics", "content": "Table 1 shows the distribution of sentiment labels in the dataset. In total, the dataset comprises 7878 samples with sentiment labels with rationales. This distribution reflects the dataset's slight emphasis on neutral content, typical in medical conversations involving detailed explanations and advice."}, {"title": "Speech/Text-based Sentiment Reasoning Framework", "content": "Let $x := x_1,x_2,...,x_T$ be an audio signal of length T. Let C be the set of all possible sentiment classes, we should build an speech sentiment analysis model f that estimates the probability $p(c|x_f)$ for each $c \\in C$. Therefore, the decision rule to predict a single class is:\n$x \u2192 c = \\text{arg max}_{c \\in C} f(c|x_1)$"}, {"title": "ASR Model", "content": "An ASR model aims to convert audio signal into text by mapping an audio signal $x_f$ to the most likely word sequence $w^*$. The relation $w^*$ between the acoustic and word sequence is:\n$w^* = \\text{arg max}_{w} p(x_f|w)$"}, {"title": "", "content": "By utilizing Bayes' Theorem, the probability p(x) can be ignored during maximization since it merely serves as a normalization factor and does not affect the outcome:\n$p(w|x_f) = \\frac{p(x_f|w)p(w)}{p(x_f)}$\nTherefore:\n$w^* = \\text{arg max}_{w} p(x_f|w) \u00b7 p(w)$"}, {"title": "Language Model", "content": "Sentiment classification: Let the transcribed audio signal (ASR transcript) $w_1$ serve as the input for the sentiment classification model g, which maps $w_1$ to a class label $c$:\n$w_1 \u2192 c = \\text{arg max}_{c \\in C} g(c/w_1)$ \ng is trained to minimize a loss function L(g($w_1$), c). The optimal parameters \u03b8 of the model are found by solving the optimization problem $\\text{mine} L(g(w; \u03b8), c)$. Once trained, the model can predict the class of the transcribed audio signal by evaluating $c$ = g($w_1$).\nSentiment reasoning (rationale generation):\nLet the transcribed audio signal (ASR transcript) $w_N$ serve as the input for the sentiment reasoning model h, which maps $w_1$ to a rationale sequence $r_M$ of M length:\n$w_N \u2192 r^* = \\text{arg max}_{r^*} h(r^*|w_N)$\nhis trained to minimize a loss function L(h($w_N$), r). The optimal parameters @ of the model are found by solving the optimization problem $\\text{ming} L(g(w; \u03b8),r)$. Once trained, the model can generate rationale of the transcribed audio signal by evaluating $r = h(w)$.\nMulti-task learning (joint sentiment classification and reasoning):\nLet the transcribed audio signal (ASR transcript) $w_N$ serve as the input for the sentiment classification and reasoning model j, which maps w to both a class label \u0109 and a rationale sequence $r_M$:\nL(j(w), $c$, r) = \u03b1 \u00b7 L(g($w_1$), c) + (1 \u2212 \u03b1) \u00b7 L(h($w_1$), r)"}, {"title": "Experimental Setups", "content": "We employed hybrid ASR setup using wav2vec 2.0 encoder (Le-Duc, 2024) to transcribe speech to text. First, we generated alignments obtained by using Gaussian-Mixture-Model/Hidden-Markov-Model (GMM/HMM) as labels for wav2vec 2.0 (Baevski et al., 2020) neural network training. The labels used in the acoustic modeling are context-dependent phonemes, triphones in this case. In GMM/HMM process, we used a CART (Classification And Regression Tree) (Breiman, 2017) to tie the states s, resulting 4501 CART labels:\n$p(x) = \\sum_{[s]} \\prod_{t=1}^{T} p(x_t, s_t| s_{t-1}, w_1)$\n$\\ = \\sum_{[sT]} \\prod_{t=1}^{T} p(s_t| s_{t-1}, w_1) \u00b7 p(x_t|s_t, s_{t-1})$"}, {"title": "Language Model", "content": "Encoders\nWe use phoBERT (110M params) (Nguyen and Nguyen, 2020), a version of RoBERTa (Liu et al., 2019) pre-trained on 20GB Vietnamese text, and ViHealthBERT (110M params) (Minh et al., 2022), phoBERT trained on 32GB of Vietnamese text in the healthcare domain. For ViHealthBERT, we report the syllable version which achieved better performance than the word version.\nGenerative Models\nWe reformulated sentiment classification into a text-to-text problem. where given the input transcript $w_1$, the generative model g and the predicted sentiment class c, we have g(w) = c with c \u2208 C = {\"0\", \"1\", \"2\"} where \"0\",\"1\",\"2\" corresponds to the labels NEGATIVE, NEUTRAL and POSITIVE.\nEncoder-Decoder: BARTpho (139M params) (Tran et al., 2022a) is the Vietnamese variant of BART (Lewis et al., 2019) trained on 20GB of Vietnamese text from Wikipedia and news corpus. ViT5 (223M params) (Phan et al., 2022) is the Vietnamese version of T5 (Raffel et al., 2020)"}, {"title": "Training with Rationale", "content": "Previous works (Wadhwa et al., 2024; Chen et al., 2024; Hsieh et al., 2023; Ho et al., 2022) have shown that rationale-augmented targets consistently improve the performance of Language Models (LMs). Furthermore, having the option to output the model's rationale during inference is highly desirable in many scenarios. As such, we also incorporate human rationale into our training pipeline. Our rationale-augmented training methods are based on, to our knowledge, the current state-of-the-art CoT-distillation approaches for each architecture.\n(i) Multitask Training (Hsieh et al., 2023): We train our encoder-decoders using distilling step-by-step. Distilling step-by-step is a multitask training approach that prepends particular prefixes to the input, guiding the model to output either the answer or generate a rationale. Hsieh et al. found that it consistently improves encoder-decoders performance compared with single-task training which treats rationale and label predictions as a single task.\n(ii) Post-thinking (Chen et al., 2024): For decoder-based models, we augment the training targets by append the human rationale to the label (<LABEL> <RATIONALE>) in a single prompt. Previous works have shown that post-thinking achieve impressive performance (Chen et al., 2024; Wadhwa et al., 2024) and compared to pre-thinking where the model first generates its chain-of-thought then provide the label (<RATIONALE> <LABEL>), post-thinking is more stable and token-efficient (Chen et al., 2024; Wadhwa et al., 2024) as the model suffers less from hallucination, consistently yields better performance and is more resource efficient as users can already retrieve the target label from the first generated token."}, {"title": "Rationale Format", "content": "We further study the effects of the format of the rationale on the performance of the generative models. In particular, given the human rationale and human label, we further prompt GPT-3.5-turbo\u2074 to enhance the rationale into two different format:\n\u2022 Elaborated rationale: An elaborated version of the human rationale that is 1-2 sentence(s) long, grounded on the provided human rationale and the sentiment label.\n\u2022 CoT rationale: A step-by-step, elaborated version of the human rationale, which includes the following steps: (1) identifies the medical entity, (2) extracts the progress of the corresponding medical entity in the transcript, and (3) provides the elaborated rationale on the sentiment grounded on the provided human rationale, the sentiment label, and information from steps (1) and (2). This approach is inspired by aspect-based sentiment instruction-tuning approaches (Varia et al., 2022)."}, {"title": "Evaluation Metrics", "content": "For sentiment classification task, we employ both accuracy and F1 score, the harmonic mean of precision and recall. It is particularly useful for evaluating models, as it considers both false positives and false negatives, providing a more comprehensive performance metric than accuracy alone.\nFor sentiment reasoning, we employ ROUGE (Recall-Oriented Understudy for Gisting Evaluation) score (Lin, 2004), which is a set of metrics used to evaluate the quality of generated text by comparing the overlap of n-grams (Ney et al., 1994), word sequences, and word pairs between the generated texts and reference texts. Also, we employ BERTScore (Zhang et al.), which is an evaluation metric that leverages pre-trained BERT embeddings (Devlin et al., 2019) to compute cosine similarity scores between generated texts and reference texts, capturing contextual and semantic nuances more effectively than traditional count-based metrics like ROUGE. BERTscore correlates well with human judgment on sentence-level as proved by Zhang et al.."}, {"title": "Results and Analysis", "content": "We evaluate and analyze our models performance on Table 2. Based on the obtained results, we make the following observations:\nEncoders are efficient yet effective sentiment classification baselines: Encoder models yields the best performance compared to their encoder-decoder and decoder counterparts, with high accuracy scores (> 0.665) and stable F1 scores (macro F1 of both models > 0.665). Furthermore, compared to other models, encoders are more parameter efficient which makes them highly economical for training and deployment. We further observe that domain-specific encoders yield notably better performance, with ViHealthBERT outperforming phoBERT in accuracy (+0.8%) and macro F1 (+0.9%).\nRationale-augmented training improve model performance: Consistent with previous findings, performing CoT-augmented training on both encoder-decoders and decoders improve our models performance compared to the baseline. Aside from ViT5, other models seem to have benefited from rationale-augmented training, especially the decoder-based LLMs. We observe an average accuracy gain of approximately +0.8% and average macro-F1 gain of approximately +1.3% for these models compared to their label-only counterparts. We further conducted a Student's t-test (Student, 1908) and found that the gains are statistically significant for \u03b1 = 0.1. This pattern holds for the results in Table 4. We observe a decline in all of our models performance on ASR data which is anticipated due to its WER of 29.6 %. Nonetheless,"}, {"title": "", "content": "The format of post-thinking rationale doesn't affect the generative models performance: We study the effects of the format of post-thinking rationale on the performance of generative models on Table 4 and observe that it is unclear whether there is a performance gain from more elaborated rationales. This result agrees with previous findings (Wadhwa et al., 2024). Particularly, we observe that some models trained on the elaborated rationales have better performance (ViT5 and vmlu-llm). However, for the other models, there is a notable decline in all metrics.\nModels are likely to misclassify POSITIVE and NEGATIVE transcripts as NEUTRAL: We study the confusion matrix of our best model on human transcript, Vistral7B finetuned with human rationale, on Figure 1. We observe a notable misclassification tendency between NEUTRAL and the other two classes (23.43% and 27.08% with NEGATIVE and POSITIVE respectively). On the other hand, we found that models can easily distinguish NEGATIVE transcripts from POSITIVE ones. This reflects the ambiguity of sentiment analysis data, as in certain cases there is not enough distinctive features that differentiate neutral from other sentiments. Furthermore, given the slightly imbalanced nature of our dataset with fewer POSITIVE examples, its average F1 score is the lowest among the three labels across all models.\nAnalysis of Generated Rationale: Compared to human rationale, we observe from Table 2 and Table 3 that the models trained with rationale have high BERTscore (around 0.8) with low ROUGE score, indicating that while the vocabulary used in the rationale is different, the overall semantic of the generated rationale remains similar to that of humans."}, {"title": "Conclusion", "content": "In this work, we introduce a new task - Sentiment Reasoning - for both speech and text modalities, along with the framework and dataset. We thoroughly evaluate the use of rationale / CoT during training to improve our models' performance and interpretability. We found that rationale-augmented training improves model performance in sentiment classification in both text and ASR settings. We also found that the generated rationales generally have different vocabulary to that of human but with similar semantics. This could potentially stimulate the development of more interpretable AI in healthcare."}, {"title": "Limitations", "content": "This study utilized the hybrid ASR system, which is generally recognized as superior in performance compared to the attention-based encoder-decoder or end-to-end ASR systems (L\u00fcscher et al., 2019; Prabhavalkar et al., 2023; Raissi et al., 2023). However, the hybrid ASR requires multiple steps, beginning with acoustic feature extraction and progressing through GMM-HMM modeling before transitioning to DNN-HMM modeling, which complicates reproducibility for non-experts.\nAdditionally, we employed a cascaded speech sentiment reasoning approach. This approach uses"}, {"title": "Related Works", "content": "Multimodal Speech Sentiment Analysis\nIt is widely known that there have been two research directions in the field of speech sentiment analysis, as also confirmed by Chen et al. (2020).\n\u2022 Single modality model (unimodal): In speech sentiment analysis, single modality models focus on utilizing a single type of data to predict sentiment. These models may rely exclusively on acoustic features, such as pitch, tone, and rhythm, to infer emotional states from spoken language (Li et al., 2019, 2018; Wu et al., 2019; Xie et al., 2019). Alternatively, they might use raw waveforms (Tzirakis et al., 2018; Zheng et al., 2022; Villatoro-Tello et al., 2021) or the textual content of transcripts to predict sentiment (Lakomkin et al., 2019). The strength of single modality models lies in their simplicity and specialization, allowing them to hone in on specific attributes of the data source they are designed for. However, this specialization can also be a limitation, as these models might miss out on the richer, more nuanced information that can be gleaned from combining multiple data types. Despite this, single modality models remain a fundamental approach in the field, providing valuable insights and serving as a benchmark for more complex multimodal systems.\n\u2022 Multimodality models: In speech sentiment analysis, multimodality models leverage the combined strengths of both acoustic and textual data to provide more accurate and nuanced sentiment predictions. While traditional models might rely solely on either the acoustic features- such as tone, pitch, and rhythm-or the text derived from speech transcripts, multimodal models integrate these two data streams. This integration allows for a more holistic understanding of sentiment, as it captures the emotional cues present in the speaker's voice along with the contextual and semantic content of the spoken words. By maximizing the mutual information between these modalities, multimodal models can better discern subtleties in speech that single modality models might miss, leading to accuracy improvements (Kim and Shin, 2019; Cho et al., 2018; Gu et al., 2018; Eskimez et al., 2018; Zhang et al., 2019).\nOur dataset is ideal for both single modal and multimodal research, as it includes both acoustic and text features.\nASR-based Speech Sentiment Analysis\nSpeech sentiment analysis on ASR transcripts is a field that aims to interpret and classify sentiments conveyed in spoken language. As technology advances, ASR systems have become increasingly proficient at transcribing spoken words into text with high accuracy (Schneider et al., 2019; Baevski et al., 2020, 2019; Wang et al., 2021b; Chen et al., 2022; Wang et al., 2021a), providing a rich source of data for sentiment analysis. Sentiment analysis algorithms then analyze the transcribed text from speech signal, utilizing language models as decoders to detect positive, negative, or neutral sentiments (Lu et al., 2020; Shon et al., 2021; Wu et al., 2022; Tashev and Emmanouilidou, 2019; Kaushik et al., 2017).\nIn the era of deep learning, as surveyed by Al-Qablan et al. (2023), many researchers have been applying deep learning methods to the sentiment analysis process on transcript, leading to the development of various models like Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), Long Short-Term Memory (LSTM), and Bidirectional LSTM (BLSTM) (Araque et al., 2017; Devipriya et al., 2020; Yadav and Vishwakarma, 2020). CNNs, primarily used for image processing, have been adapted for text by treating sentences as sequences of words and applying convolutional filters to capture local features. This approach helps in identifying crucial patterns within the text that are indicative of sentiment (Kumar and Malarvizhi, 2020; Wang et al., 2020). On the other hand, RNNs are designed to handle sequential data by maintaining a hidden state that captures the history of previous inputs, making them suitable for understanding the context and temporal dependencies in sentences. However, traditional RNNS face challenges with long-term dependencies due to issues like vanishing gradients, which is where LSTMs come in. LSTMs, an advanced form of RNNs, address these issues by incorporating gates that regulate the flow of information, allowing them to maintain and update long-term dependencies effectively. Furthermore, BLSTMs enhance this by processing the input sequence in both forward and backward directions, thus capturing dependencies from both past and future contexts simultaneously. This bidirectional approach is especially useful for"}, {"title": "Details about Data", "content": "Data Annotation\nThe data annotation process is as followed. First, all the subtitles are separated into different chunks. These segments are subsequently input into a GPT-4 model, which conducts a weakly supervised 3-label classification task to categorize each segment as NEGATIVE, NEUTRAL, or POSITIVE. In addition to the sentiment label, GPT-4 also provides a brief synthetic rationales for the classification, such as 'Negative medical condition' or 'Objective description'. The labels and rationales generated by GPT-4 are subsequently reviewed and independently corrected by a team of 3 developers.\nAnnotation Guidelines\nTo ensure consistency, our TESOL-certificated professional linguist has developed an initial guideline inspired by (Chen et al., 2020) and revised it frequently if necessary as followed:\nThe NEGATIVE label is for chunks that discuss negative, serious diseases, disorders, symptoms, risks, negative emotions, or counter-positive statements (e.g. \"This would NOT bring a good outcome\"). It also applies to incomplete chunks where the amount of negativity is greater than the amount of positivity.\nThe NEUTRAL label is for incomplete chunks where the ratio of negativity is equal to the ratio of positivity, as well as chunks that describe processes, ask questions, provide advice, or are too short.\nThe POSITIVE label is for chunks that discuss positive outcomes, recovery processes, positive emotions, or counter-negative statements (e.g. \"This will reduce discrimination\"). It also applies to incomplete chunks where the ratio of positivity is greater than the ratio of negativity.\nIt is important to note that all chunks are considered independent, even though they may be incomplete and related to preceding or following chunks. Given that this data is derived from spoken language, the chunks contain a significant amount of filler words, which are disregarded in the labeling process. The majority of the NEUTRAL labels are attributed to chunks that involve sharing advice or descriptions. Additionally, the presence of modal verbs (e.g., should, would, need) often indicates advice sharing, thereby classifying the chunk as NEUTRAL regardless of its content.\nData Quality Control\nDuring the independent annotation process conducted by three annotators, we observed a low inter-annotator agreement, a common occurrence in real-world datasets as noted by Chen et al. (2020). To address this issue, we implemented an alternative label merging approach. We convened a discussion meeting involving the three annotators and two reviewers (one professional linguist and one with a biomedical background). Each annotator was required to justify their chosen sentiment label and its corresponding rationale. A label and its rationale were selected based on the consensus of all three annotators and two reviewers, rather than a majority vote, as employed in other studies (Aziz and Dimililer, 2020; Saleena et al., 2018).\nData Statistics Discussion\nThe Neutral category is the most predominant, accounting for a significant portion of the dataset. With 3802 instances for both train and test set, Neutral sentiments make up approximately half of the dataset. This prevalence of Neutral sentiment is expected, as also seen by a real-world conversational dataset (Chen et al., 2020), given the nature of medical consultations, which often involve objective descriptions, explanations, and advice. The Negative category is the second most common, with around 2395 instances. Negative sentiments include discussions about serious diseases, negative emotions, and adverse medical outcomes. The substantial presence of Negative sentiments reflects the medical context, where discussions about illnesses and symptoms are common. The Positive category, while the least common, still represents a significant portion of the dataset with 1681 instances. Positive sentiments typically involve discussions about recovery processes, positive outcomes, and favorable emotions."}, {"title": "Details about Experimental Setups", "content": "Training Setup\nOur encoders and encoder-decoders were trained on a cluster of 2 NVIDIA A40s with 46 GBs of memory. All models were trained on 30 epochs with with a learning rate of 2e-5 and batch size of 64. We evaluated every epoch with early stopping with patience = 3.\nFor the decoder-based LLMs, due to their massive number of parameters, we use LoRA (Hu et al., 2021) for finetuning with hyperparameters"}, {"title": "Results on English subset", "content": "We use Google Translate to translate our data into English. To ensure that our translated data is usable, we randomly sampled 50 transcripts and check their quality. We further train English models on this English subset of our dataset to ensure full usability.\nThe result of our experiments is in Table 5. More information on the models used can be found in the same table. Overall, we found that rationale-augmented training also help boost the model's performance. This finding is consistent with what when observed in our experiments in Section 5."}, {"title": "Sample Rationales", "content": "We report our best model's misclassified transcripts with the highest label confidence (defined as the softmax of the logits of the model prediction) in Table 6. By analyzing at the model's rationale, we hypothesize that the model is confounded by the appearance of certain keywords that elicit either extremely positive ( h\u1eefu \u00edch (helpful)) or negative, disease-related words and sentiment which pushes the model away from the NEUTRAL label."}]}