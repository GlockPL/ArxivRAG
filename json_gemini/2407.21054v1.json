[{"title": "Training with Rationale", "authors": ["Khai Le-Duc", "Khai-Nguyen Nguyen", "Bach Phan Tat", "Duy Le", "Jerry Ngo", "Long Vo-Dang", "Anh Totti Nguyen", "Truong-Son Hy"], "abstract": "Transparency in AI decision-making is crucial in healthcare due to the severe consequences of errors, and this is important for building trust among AI and users in sentiment analysis task. Incorporating reasoning capabilities helps Large Language Models (LLMs) understand human emotions within broader contexts, handle nuanced and ambiguous language, and infer underlying sentiments that may not be explicitly stated. In this work, we introduce a new task Sentiment Reasoning - for both speech and text modalities, along with our proposed multimodal multitask framework and dataset. Our study showed that rationale-augmented training enhances model performance in sentiment classification across both human transcript and ASR settings. Also, we found that the generated rationales typically exhibit different vocabularies compared to human-generated rationales, but maintain similar semantics. All code, data (English-translated and Vietnamese) and models are published online.", "sections": [{"title": "Introduction", "content": "The global market for sentiment analysis is projected to expand from an estimated value of US$4 billion in 2023 to US$10.1 billion by 2030, exhibiting a compound annual growth rate (CAGR) of 14.2% over the forecast period from 2023 to 2030 (Inc, 2024). In recent years, speech sentiment analysis has emerged as a significant interdisciplinary field at the intersection of natural language processing (NLP), machine learning, and automatic speech recognition (ASR). This field focuses on the automated detection and interpretation of human emotions and attitudes conveyed through speech, overcoming the limitations of text-based sentiment analysis (Murugaiyan and Uyyala, 2023) by analyzing the attitude expressed through human voice.\nSentiment analysis plays a pivotal role across diverse domains, with healthcare being particularly significant. In healthcare customer service, it facilitates real-time evaluation of customer satisfaction, enhancing empathetic and responsive interactions (Xia et al., 2009; Na et al., 2012). Moreover, sentiment analysis aids in monitoring the emotional well-being of patients (Cambria et al., 2012a), including those with mental health issues such as suicide (Pestian et al., 2012). However, these studies only work on text-based sentiment analysis rather than speech-based sentiment analysis.\nDespite its potential, speech sentiment analysis presents several technical challenges. First, speech signals are inherently noisy (Rajnoha and Poll\u00e1k, 2011) and exhibit significant variability due to differences in accents (Turan et al., 2020), speaking styles (Shafran and Rose, 2003), and recording conditions (Yamagishi et al., 2010), complicating the extraction of reliable acoustic features (Chen et al., 2017). Second, emotions are subjective (Wearne et al., 2019), complex (Golan et al., 2006), and multidimensional, making accurate categorization difficult even for humans (Kuusikko et al., 2009), thereby necessitating the role of explainable artificial intelligence (AI). Third, given the critical nature of healthcare decisions, where errors can have severe consequences, transparency in AI decision-making is essential to build trust among machines, healthcare professionals, and patients (Antoniadi et al., 2021).\nTo tackle these challenges, we introduce a novel multimodal framework for a novel task: Sentiment Reasoning. Reasoning in AI is crucial for sentiment"}, {"title": "Data", "content": "The dataset employed for constructing the sentiment reasoning dataset was VietMed (Le-Duc, 2024), recognized as the largest, most generalizable, and publicly accessible medical ASR dataset in the world. We then annotated sentiment labels and their corresponding rationales. This dataset comprises real-world doctor-patient conversations covering a broad spectrum of all available ICD-10 codes (all medical topics), wherein doctors diagnose conditions, explain medical issues, and provide treatment advice.\nDetails of data annotation pipeline, annotation guidelines, data quality control, and data statistics are shown Section B in the Appendix."}, {"title": "Data Statistics", "content": "Table 1 shows the distribution of sentiment labels in the dataset. In total, the dataset comprises 7878 samples with sentiment labels with rationales. This distribution reflects the dataset's slight emphasis on neutral content, typical in medical conversations involving detailed explanations and advice."}, {"title": "Speech/Text-based Sentiment Reasoning Framework", "content": "Let \\(x = x_1,x_2,...,x_T\\) be an audio signal of length T. Let C be the set of all possible sentiment classes, we should build an speech sentiment analysis model \\(f\\) that estimates the probability \\(p(c|x^f)\\) for each \\(c \\in C\\). Therefore, the decision rule to predict a single class is:\n\\[x \\rightarrow c = \\underset{c \\in C}{\\text{arg max }} f(c|x^f) \\]\n3.1 ASR Model\nAn ASR model aims to convert audio signal into text by mapping an audio signal \\(x^f\\) to the most likely word sequence \\(w^*\\). The relation \\(w^*\\) between the acoustic and word sequence is:\n\\[w^* = \\underset{w}{\\text{arg max }} p(x^f|w) \\]\nBy utilizing Bayes' Theorem, the probability \\(p(x^f)\\) can be ignored during maximization since it merely serves as a normalization factor and does not affect the outcome:\n\\[p(w|x^f) = \\frac{p(x^f|w) p(w)}{p(x^f)} = \\underset{w}{\\propto} p(x^f|w)p(w)\\]\nTherefore:\n\\[w^* = \\underset{w}{\\text{arg max }} \\underbrace{p(x^f|w)}_{\\text{acoustic model}} \\cdot \\underbrace{p(w)}_{\\text{language model}}\\]\n3.2 Language Model\nSentiment classification: Let the transcribed audio signal (ASR transcript) \\(w_1\\) serve as the input for the sentiment classification model \\(g\\), which maps \\(w_1\\) to a class label \\(\\hat{c}\\):\n\\[w_1 \\rightarrow \\hat{c} = \\underset{c \\in C}{\\text{arg max }} g(c|w_1) \\]\n\\(g\\) is trained to minimize a loss function \\(L(g(w_1), c)\\). The optimal parameters \\(\\theta\\) of the"}, {"title": "Experimental Setups", "content": "We employed hybrid ASR setup using wav2vec 2.0 encoder (Le-Duc, 2024) to transcribe speech to text. First, we generated alignments obtained by using Gaussian-Mixture-Model/Hidden-Markov-Model (GMM/HMM) as labels for wav2vec 2.0 (Baevski et al., 2020) neural network training. The labels used in the acoustic modeling are context-dependent phonemes, triphones in this case. In GMM/HMM process, we used a CART (Classification And Regression Tree) (Breiman, 2017) to tie the states s, resulting 4501 CART labels:\n\\[p(x^f|w_i) = \\sum_{[s_1^T]} \\prod_{t=1}^T p(x_t, s_t | s_{t-1}, w_i)\\]\n\\[ = \\sum_{[s_1^T]} \\prod_{t=1}^T \\underbrace{p(s_t | s_{t-1}, w_i)}_{\\text{transition prob.}} \\cdot \\underbrace{p(x_t | s_t, s_{t-1})}_{\\text{emission prob.}} \\]\nAfter inputting CART labels for hybrid wav2vec 2.0 training, we employed frame-wise cross-entropy (fCE) loss (Good, 1952) to train the acoustic model.\nTo transcribe speech given the acoustic observations, the acoustic model and n-gram language model (Ney et al., 1994) should be combined based on the Bayes decision rule using Viterbi algorithm (Forney, 1973) which recursively computes the maximum path to a find best-path in the alignment graph of all possible predicted words to the acoustic observations:\n\\[w^* = \\underset{w_i}{\\text{arg max }} (\\prod_{n=1}^T p(w_n | w_{i}) \\\\cdot \\underset{[s_1^T]}{\\text{max }}\\prod_{t=1}^T p(x_t, s_t | s_{t-1}, w_i) )\\]\nFinally, acoustic model and n-gram language model pruning (beam search) is used to only focus on the most promising predicted words at each time step t (Ortmanns et al., 1997).\nThe final ASR model has 118M trainable parameters and Word-Error-Rate (WER) of 29.6% on VietMed test set."}, {"title": "Language Model", "content": "We use phoBERT (110M params) (Nguyen and Nguyen, 2020), a version of RoBERTa (Liu et al., 2019) pre-trained on 20GB Vietnamese text, and ViHealthBERT (110M params) (Minh et al., 2022), phoBERT trained on 32GB of Vietnamese text in the healthcare domain. For ViHealthBERT, we report the syllable version which achieved better performance than the word version."}, {"title": "Generative Models", "content": "We reformulated sentiment classification into a text-to-text problem. where given the input transcript \\(w_1\\), the generative model \\(g\\) and the predicted sentiment class \\(c\\), we have \\(g(w) = c\\) with \\(c \\in C = {\\text{\\"}, "text{\\", 1, "}, \\text{\\", 2, "}}\\) where \\\"0\\\",\\\"1\\\",\\\"2\\\" corresponds to the labels NEGATIVE, NEUTRAL and POSITIVE.\nEncoder-Decoder: BARTpho (139M params) (Tran et al., 2022a) is the Vietnamese variant of BART (Lewis et al., 2019) trained on 20GB of Vietnamese text from Wikipedia and news corpus. ViT5 (223M params) (Phan et al., 2022) is the Vietnamese version of T5 (Raffel et al., 2020)"], "content": "Previous works (Wadhwa et al., 2024; Chen et al., 2024; Hsieh et al., 2023; Ho et al., 2022) have shown that rationale-augmented targets consistently improve the performance of Language Models (LMs). Furthermore, having the option to output the model's rationale during inference is highly desirable in many scenarios. As such, we also incorporate human rationale into our training pipeline. Our rationale-augmented training methods are based on, to our knowledge, the current state-of-the-art CoT-distillation approaches for each architecture.\n(i) Multitask Training (Hsieh et al., 2023): We train our encoder-decoders using distilling step-by-step. Distilling step-by-step is a multitask training approach that prepends particular prefixes to the input, guiding the model to output either the answer or generate a rationale. Hsieh et al. found that it consistently improves encoder-decoders performance compared with single-task training which treats rationale and label predictions as a single task.\n(ii) Post-thinking (Chen et al., 2024): For decoder-based models, we augment the training targets by append the human rationale to the label (<LABEL> <RATIONALE>) in a single prompt. Previous works have shown that post-thinking achieve impressive performance (Chen et al., 2024; Wadhwa et al., 2024) and compared to pre-thinking where the model first generates its chain-of-thought then provide the label (<RATIONALE> <LABEL>), post-thinking is more stable and token-efficient (Chen et al., 2024; Wadhwa et al., 2024) as the model suffers less from hallucination, consistently yields better performance and is more resource efficient as users can already retrieve the target label from the first generated token."}, {"title": "Rationale Format", "content": "We further study the effects of the format of the rationale on the performance of the generative models. In particular, given the human rationale and human label, we further prompt GPT-3.5-turbo\u2074 to enhance the rationale into two different format:\n\u2022 Elaborated rationale: An elaborated version of the human rationale that is 1-2 sentence(s) long, grounded on the provided human rationale and the sentiment label.\n\u2022 CoT rationale: A step-by-step, elaborated version of the human rationale, which includes the following steps: (1) identifies the medical entity, (2) extracts the progress of the corresponding medical entity in the transcript, and (3) provides the elaborated rationale on the sentiment grounded on the provided human rationale, the sentiment label, and information from steps (1) and (2). This approach is inspired by aspect-based sentiment instruction-tuning approaches (Varia et al., 2022)."}, {"title": "Evaluation Metrics", "content": "For sentiment classification task, we employ both accuracy and F1 score, the harmonic mean of precision and recall. It is particularly useful for evaluating models, as it considers both false positives and false negatives, providing a more comprehensive performance metric than accuracy alone.\nFor sentiment reasoning, we employ ROUGE (Recall-Oriented Understudy for Gisting Evaluation) score (Lin, 2004), which is a set of metrics used to evaluate the quality of generated text by comparing the overlap of n-grams (Ney et al., 1994), word sequences, and word pairs between the generated texts and reference texts. Also, we employ BERTScore (Zhang et al.), which is an evaluation metric that leverages pre-trained BERT embeddings (Devlin et al., 2019) to compute cosine similarity scores between generated texts and reference texts, capturing contextual and semantic nuances more effectively than traditional count-based metrics like ROUGE. BERTscore correlates well with human judgment on sentence-level as proved by Zhang et al.."}, {"title": "Results and Analysis", "content": "We evaluate and analyze our models performance on Table 2. Based on the obtained results, we make the following observations:"}, {"title": "Conclusion", "content": "In this work, we introduce a new task - Sentiment Reasoning - for both speech and text modalities, along with the framework and dataset. We thoroughly evaluate the use of rationale / CoT during training to improve our models' performance and interpretability. We found that rationale-augmented training improves model performance in sentiment classification in both text and ASR settings. We also found that the generated rationales generally have different vocabulary to that of human but with similar semantics. This could potentially stimulate the development of more interpretable AI in healthcare."}, {"title": "Limitations", "content": "This study utilized the hybrid ASR system, which is generally recognized as superior in performance compared to the attention-based encoder-decoder or end-to-end ASR systems (L\u00fcscher et al., 2019; Prabhavalkar et al., 2023; Raissi et al., 2023). However, the hybrid ASR requires multiple steps, beginning with acoustic feature extraction and progressing through GMM-HMM modeling before transitioning to DNN-HMM modeling, which complicates reproducibility for non-experts.\nAdditionally, we employed a cascaded speech sentiment reasoning approach. This approach uses"}, {"title": "Related Works", "content": "It is widely known that there have been two research directions in the field of speech sentiment analysis, as also confirmed by Chen et al. (2020).\n\u2022 Single modality model (unimodal): In speech sentiment analysis, single modality models focus on utilizing a single type of data to predict sentiment. These models may rely exclusively on acoustic features, such as pitch, tone, and rhythm, to infer emotional states from spoken language (Li et al., 2019, 2018; Wu et al., 2019; Xie et al., 2019). Alternatively, they might use raw waveforms (Tzirakis et al., 2018; Zheng et al., 2022; Villatoro-Tello et al., 2021) or the textual content of transcripts to predict sentiment (Lakomkin et al., 2019). The strength of single modality models lies in their simplicity and specialization, allowing them to hone in on specific attributes of the data source they are designed for. However, this specialization can also be a limitation, as these models might miss out on the richer, more nuanced information that can be gleaned from combining multiple data types. Despite this, single modality models remain a fundamental approach in the field, providing valuable insights and serving as a benchmark for more complex multimodal systems.\n\u2022 Multimodality models: In speech sentiment analysis, multimodality models leverage the combined strengths of both acoustic and textual data to provide more accurate and nuanced sentiment predictions. While traditional models might rely solely on either the acoustic features-such as tone, pitch, and rhythm-or the text derived from speech transcripts, multimodal models integrate these two data streams. This integration allows for a more holistic understanding of sentiment, as it captures the emotional cues present in the speaker's voice along with the contextual and semantic content of the spoken words. By maximizing the mutual information between these modalities, multimodal models can better discern subtleties in speech that single modality models might miss, leading to accuracy improvements (Kim and Shin, 2019; Cho et al., 2018; Gu et al., 2018; Eskimez et al., 2018; Zhang et al., 2019).\nOur dataset is ideal for both single modal and multimodal research, as it includes both acoustic and text features."}, {"title": "ASR-based Speech Sentiment Analysis", "content": "Speech sentiment analysis on ASR transcripts is a field that aims to interpret and classify sentiments conveyed in spoken language. As technology advances, ASR systems have become increasingly proficient at transcribing spoken words into text with high accuracy (Schneider et al., 2019; Baevski et al., 2020, 2019; Wang et al., 2021b; Chen et al., 2022; Wang et al., 2021a), providing a rich source of data for sentiment analysis. Sentiment analysis algorithms then analyze the transcribed text from speech signal, utilizing language models as decoders to detect positive, negative, or neutral sentiments (Lu et al., 2020; Shon et al., 2021; Wu et al., 2022; Tashev and Emmanouilidou, 2019; Kaushik et al., 2017).\nIn the era of deep learning, as surveyed by Al-Qablan et al. (2023), many researchers have been applying deep learning methods to the sentiment analysis process on transcript, leading to the development of various models like Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), Long Short-Term Memory (LSTM), and Bidirectional LSTM (BLSTM) (Araque et al., 2017; Devipriya et al., 2020; Yadav and Vishwakarma, 2020). CNNs, primarily used for image processing, have been adapted for text by treating sentences as sequences of words and applying convolutional filters to capture local features. This approach helps in identifying crucial patterns within the text that are indicative of sentiment (Kumar and Malarvizhi, 2020; Wang et al., 2020). On the other hand, RNNs are designed to handle sequential data by maintaining a hidden state that captures the history of previous inputs, making them suitable for understanding the context and temporal dependencies in sentences. However, traditional RNNs face challenges with long-term dependencies due to issues like vanishing gradients, which is where LSTMs come in. LSTMs, an advanced form of RNNs, address these issues by incorporating gates that regulate the flow of information, allowing them to maintain and update long-term dependencies effectively. Furthermore, BLSTMs enhance this by processing the input sequence in both forward and backward directions, thus capturing dependencies from both past and future contexts simultaneously. This bidirectional approach is especially useful for"}, {"title": "Speech Sentiment Analysis in Healthcare", "content": "Sentiment analysis in healthcare is an emerging field that leverages NLP and machine learning techniques to analyze and interpret the emotional tone conveyed in biomedical textual data. This technology is particularly useful for understanding patient feedback, monitoring public health trends, and improving patient-provider communication. By analyzing large volumes of data from sources such as social media, online reviews, electronic health records (EHRs), and patient surveys, sentiment analysis can provide valuable insights into patient experiences, satisfaction levels, and overall public sentiment towards healthcare services and policies. For instance, analyzing patient reviews on healthcare platforms can help identify common concerns and areas needing improvement, allowing healthcare providers to address issues proactively and enhance the quality of care. Additionally, sentiment analysis can play a critical role in mental health monitoring by detecting signs of distress or dissatisfaction in patient communications, enabling timely intervention and support. As this technology continues to evolve, it holds the promise of transforming healthcare by fostering a more patient-centric approach, enhancing service delivery, and ultimately improving patient outcomes (Denecke and Deng, 2015). However, the sentiments expressed in clinical narratives have not been extensively analyzed or exploited, based on the total number of previous works we have identified to the best of our knowledge:\n\u2022 Sentiment analysis from the medical web: Most sentiment analysis research in the medical domain focuses on web data, such as medical blogs and forums, to mine patient opinions or assess quality (Ali et al., 2013; Xia et al., 2009; Na et al., 2012; Sokolova et al., 2013; Biyani et al., 2013; Ofek et al., 2013; Smith and Lee, 2012; Sharif et al., 2014; Melzi et al., 2014).\n\u2022 Sentiment analysis from biomedical literature: In addition to the analysis of medical social media data, biomedical literature has been examined concerning the outcomes of medical treatments. Within this framework, sentiment denotes the results or efficacy of a treatment or intervention (Niu et al., 2005; Sarker et al., 2011).\n\u2022 Sentiment analysis from medical text (except biomedical literature): Several researchers have focused on leveraging supplementary sources of medical texts to implement sentiment analysis and emotion detection methodologies, suicide notes or patient questionnaire for example (Pestian et al., 2012; Cambria et al., 2012a; Liu and Singh, 2004; Cambria et al., 2012b).\nTo the best of our knowledge, no literature among those cited has addressed speech sentiment analysis specifically within the domain of healthcare."}, {"title": "Details about Data", "content": "The data annotation process is as followed. First, all the subtitles are separated into different chunks. These segments are subsequently input into a GPT-4 model, which conducts a weakly supervised 3-label classification task to categorize each segment as NEGATIVE, NEUTRAL, or POSITIVE. In addition to the sentiment label, GPT-4 also provides a brief synthetic rationales for the classification, such as 'Negative medical condition' or 'Objective description'. The labels and rationales generated by GPT-4 are subsequently reviewed and independently corrected by a team of 3 developers."}, {"title": "Annotation Guidelines", "content": "To ensure consistency, our TESOL-certificated professional linguist has developed an initial guideline inspired by (Chen et al., 2020) and revised it frequently if necessary as followed:\nThe NEGATIVE label is for chunks that discuss negative, serious diseases, disorders, symptoms, risks, negative emotions, or counter-positive statements (e.g. \"This would NOT bring a good outcome\"). It also applies to incomplete chunks where the amount of negativity is greater than the amount of positivity.\nThe NEUTRAL label is for incomplete chunks where the ratio of negativity is equal to the ratio of positivity, as well as chunks that describe processes, ask questions, provide advice, or are too short.\nThe POSITIVE label is for chunks that discuss positive outcomes, recovery processes, positive emotions, or counter-negative statements (e.g. \"This will reduce discrimination\"). It also applies to incomplete chunks where the ratio of positivity is greater than the ratio of negativity.\nIt is important to note that all chunks are considered independent, even though they may be incomplete and related to preceding or following chunks. Given that this data is derived from spoken language, the chunks contain a significant amount of filler words, which are disregarded in the labeling process. The majority of the NEUTRAL labels are attributed to chunks that involve sharing advice or descriptions. Additionally, the presence of modal verbs (e.g., should, would, need) often indicates advice sharing, thereby classifying the chunk as NEUTRAL regardless of its content."}, {"title": "Data Quality Control", "content": "During the independent annotation process conducted by three annotators, we observed a low inter-annotator agreement, a common occurrence in real-world datasets as noted by Chen et al. (2020). To address this issue, we implemented an alternative label merging approach. We convened a discussion meeting involving the three annotators and two reviewers (one professional linguist and one with a biomedical background). Each annotator was required to justify their chosen sentiment label and its corresponding rationale. A label and its rationale were selected based on the consensus of all three annotators and two reviewers, rather than a majority vote, as employed in other studies (Aziz and Dimililer, 2020; Saleena et al., 2018)."}, {"title": "Data Statistics Discussion", "content": "The Neutral category is the most predominant, accounting for a significant portion of the dataset. With 3802 instances for both train and test set, Neutral sentiments make up approximately half of the dataset. This prevalence of Neutral sentiment is expected, as also seen by a real-world conversational dataset (Chen et al., 2020), given the nature of medical consultations, which often involve objective descriptions, explanations, and advice. The Negative category is the second most common, with around 2395 instances. Negative sentiments include discussions about serious diseases, negative emotions, and adverse medical outcomes. The substantial presence of Negative sentiments reflects the medical context, where discussions about illnesses and symptoms are common. The Positive category, while the least common, still represents a significant portion of the dataset with 1681 instances. Positive sentiments typically involve discussions about recovery processes, positive outcomes, and favorable emotions."}, {"title": "Details about Experimental Setups", "content": "Our encoders and encoder-decoders were trained on a cluster of 2 NVIDIA A40s with 46 GBs of memory. All models were trained on 30 epochs with with a learning rate of 2e-5 and batch size of 64. We evaluated every epoch with early stopping with patience = 3.\nFor the decoder-based LLMs, due to their massive number of parameters, we use LoRA (Hu et al., 2021) for finetuning with hyperparameters"}, {"title": "Training Setup", "content": "Our encoders and encoder-decoders were trained on a cluster of 2 NVIDIA A40s with 46 GBs of memory. All models were trained on 30 epochs with with a learning rate of 2e-5 and batch size of 64. We evaluated every epoch with early stopping with patience = 3.\nFor the decoder-based LLMs, due to their massive number of parameters, we use LoRA (Hu et al., 2021) for finetuning with hyperparameters"}, {"title": "Student's T-Test", "content": "A Student's t-test, is a statistical method used to compare the means of one or two populations through hypothesis testing. It can assess whether a single group mean differs from a known value (one-sample t-test), compare the means of two independent groups (independent two-sample t-test), or determine if there is a significant difference between paired measurements (paired or dependent samples t-test)."}, {"title": "Results on English subset", "content": "We use Google Translate to translate our data into English. To ensure that our translated data is usable, we randomly sampled 50 transcripts and check their quality. We further train English models on this English subset of our dataset to ensure full usability.\nThe result of our experiments is in Table 5. More information on the models used can be found in the same table. Overall, we found that rationale-augmented training also help boost the model's performance. This finding is consistent with what when observed in our experiments in Section 5."}, {"title": "Sample Rationales", "content": "We report our best model's misclassified transcripts with the highest label confidence (defined as the softmax of the logits of the model prediction) in Table 6. By analyzing at the model's rationale, we hypothesize that the model is confounded by the appearance of certain keywords that elicit either extremely positive ( h\u1eefu \u00edch (helpful)) or negative, disease-related words and sentiment which pushes the model away from the NEUTRAL label."}]