{"title": "Towards Human-Level Understanding of Complex Process Engineering Schematics: A Pedagogical, Introspective Multi-Agent Framework for Open-Domain Question Answering", "authors": ["Sagar Srinivas Sakhinana", "Geethan Sannidhi", "Venkataramana Runkana"], "abstract": "In the chemical and process industries, Process Flow Diagrams (PFDs) and Piping and Instrumentation Diagrams (P&IDs) are critical for design, construction, and maintenance. Recent advancements in Generative AI, such as Large Multimodal Models (LMMs) like GPT-4 (Omni), have shown promise in understanding and interpreting process diagrams for Visual Question Answering (VQA). However, proprietary models pose data privacy risks, and their computational complexity prevents knowledge editing for domain-specific customization on consumer hardware. To overcome these challenges, we propose a secure, on-premises enterprise solution using a hierarchical, multi-agent Retrieval-Augmented Generation (RAG) framework for open-domain question answering (ODQA) tasks, offering enhanced data privacy, explainability, and cost-effectiveness. Our novel multi-agent framework employs introspective and specialized sub-agents using open-source, small-scale multimodal models with the ReAct (Reason+Act) prompting technique for PFD and P&ID analysis, integrating multiple information sources to provide accurate and contextually relevant answers. Our approach, supported by iterative self-correction, aims to deliver superior performance in ODQA tasks. We conducted rigorous experimental studies, and the empirical results validated the proposed approach's effectiveness.", "sections": [{"title": "1 Introduction", "content": "PFDs and P&IDs play crucial roles in the chemical and process industries, finding applications in various sectors such as oil and gas, pharmaceuticals, the semiconductor industry, and more. PFDs illustrate major equipment interconnections and material/energy flow in a chemical and process plant, while P&IDs detail piping, instrumentation, and control systems. Both PFDs and P&IDs are essential documents for the design, construction, operation, and maintenance of chemical and process plants. Recent advancements in Generative AI, such as Large Multimodal Models (LMMs) with advanced vision-language processing capabilities, including OpenAI GPT-4 (Omni) [4] and Google Gemini [5], have the ability to understand and interpret PFDs and P&IDs for visual question-answering (VQA). However, using proprietary vision-language models raises data privacy concerns, as the risk of sharing intellectual property could compromise enterprise"}, {"title": "2 Proposed Method", "content": "ODQA addresses a wider range of questions than traditional QA, often relying on vast, unstructured databases or large document collections (e.g., web search, Wikipedia). In this work, we utilize two distinct documents: (a) PFDS (DPFDs) and (b) P&IDs (DP&IDs). We begin by performing document parsing to extract text and images embedded within complex, unstructured PDFs. Each document is then split into smaller segments using the sliding window chunking technique to preserve context and improve retrieval. A fixed-size window (in words) is moved by a predefined stride to create overlapping chunks. After chunking, each text segment is converted into a vector representation using embedding techniques. These vector representations are indexed in a vector database, allowing for efficient similarity search and retrieval of relevant text chunks based on semantic similarity to user queries. We use GPT-4(omni) to generate alternative text descriptions for the extracted images, which serve as metadata providing content, context, and details of the images. By indexing this metadata, we enable the retrieval of images based on their content, thereby enhancing the effectiveness of multi-modal search queries. Our approach employs a multi-agent framework where the introspective (meta) agent comprises a main agent (AM) and a critique agent (AC). The main agent orchestrates specialized sub-agents (APFDs, AP&IDs), each responsible for handling tasks related to PFDs and P&IDs analysis. The introspective agent utilizes a reflective agentic pattern, consisting of the main agent generating the initial response to user queries and the critique agent performing reflection and correction to evaluate and improve the response. The main agent leverages language models (LMs) like Google Gemma to understand and interpret complex user queries, utilizing the ReACT technique [7] to"}, {"title": "2.1 Instance-wise/Pair-wise validity", "content": "To evaluate the validity and relevance of each summary sk in supporting its corresponding answer candidate ak, we utilize a two-step validation process. First, we check if sk is degenerate, i.e., if it fails to provide meaningful support for ak due to insufficient information from the retrieved passages. Second, for non-degenerate summaries, we evaluate how well sk specifically supports ak compared"}, {"title": "2.2 Iterative Self-Correction through Reflection", "content": "We now enable the main agent to self-correct the sub-agent's outputs based on feedback obtained during verification by a critique agent. The sub-agent's output undergoes progressive refinement through repeated cycles of verification and correction until a predetermined termination criterion is satisfied, such as attaining a target accuracy threshold or executing a fixed number of iterations. The iterative process involves two key steps: reflection and correction. The 'reflection' step refers to the verification process, where the main agent relays the generated answer to a critique agent. The critique agent utilizes a high-quality benchmark, like GPT-4 Turbo, to evaluate the generated answer's quality based on question-answer relevance, factual correctness, and comparison to the ground truth using NLP metrics like BLEU, ROUGE, and METEOR. The purpose is to evaluate and provide feedback on the sub-agent's output to determine whether it meets the desired standards of accuracy and truthfulness. The 'correction' step occurs after the reflection step. Based on the feedback generated during verification, the main agent delegates the task to the sub-agent to correct and improve its output. In summary, the reflection (verification of the sub-agent's output by the critique agent to generate feedback) and correction cycle (using the feedback from the verification step to revise and improve the previous output) can be repeated iteratively, enabling progressive self-improvement of the sub-agent's generated answer. This process allows the sub-agent to ground its reasoning in factual knowledge, reducing hallucination and leading to more accurate outputs. Additionally, the interleaved reasoning traces provide transparency, resulting in a more interpretable and trustworthy VQA framework. As mentioned earlier, the multi-agent framework consists of (a) an introspective agent, which includes (i) a main agent and (ii) a critique agent. The main agent plays a crucial role by delegating tasks to the relevant sub-agents and coordinating the iterative self-correction through reflection with the critique agent. Given the question (q) and the retrieved relevant passages (C++) from parsed documents, memory databases, web articles, and Wikipedia, the sub-agent generates an initial output (\u00e3i) as follows:\n$\\tilde{a_i} = \\hat{a_{k^*}} = M(p_{can}(q, C^{++}))_{k^*}$\n$k^* = arg max [v(M(p_{sum}(q, C^{++},\\hat{a_k}))) + r(M(p_{sum}(q, C^{++},\\hat{a_k})), S_K)]$\nWhere ak \u2208 a = M(pcan(q, C++)) are the generated answer candidates, v(.) represents the instance-wise validity score. r(, SK) is the ranking score of a summary sk, comparing its relevance and informativeness against other generated summaries in the set Sk = {M(psum(q, C++,\u0101k))}K_1. ak* denotes the specific answer candidate from a identified as the most plausible or optimal. The index k* is determined by the arg max operation, selecting the index k that maximizes the combined score of validity and ranking. Equations (1) and (2) detail the steps of generating answer candidates, performing conditional summarization, evaluating the validity of each summary, and ranking the summaries to select"}, {"title": "2.3 Vision-Language Instruction/Preference Tuning", "content": "We bridge the gap between the general knowledge of pre-trained student models and new task requirements by updating the student model's domain knowledge through vision-language instruction tuning on a task-specific dataset (input-output pairs + instructions), all while mitigating the catastrophic forgetting of pre-trained knowledge. Instruction and preference tuning are necessary to adapt and align student models to particular domain-specific tasks and human preferences, respectively. This typically involves a multi-stage approach: (a) instruction-tuning using task-specific data to adapt the student model to the target task by minimizing the cross-entropy loss, followed by (b) direct preference optimization (DPO) for preference alignment using human preference data to increase the likelihood of generating preferred responses over rejected responses for the target task, thereby minimizing the binary cross-entropy loss. Traditionally, these methods require extensive and often expensive expert-annotated data. In this study, we use teacher-student transfer learning to avoid expensive manual data labeling. Teacher models, trained on vast labeled datasets, transfer their task-specific knowledge to student models through knowledge distillation. This enables student models to achieve high task-specific performance comparable to proprietary teacher models without relying on extensive human annotation. We use parameter-efficient fine-tuning (PEFT) with quantization to adapt pre-trained student models to various tasks by updating a small subset of additional parameters. This reduces memory usage and computational overhead, enabling efficient training and scaling on consumer hardware. We utilize OpenAI GPT-4(Omni) and Google Gemini Pro as teacher models to generate a customized instruction-following dataset of image-question-answer (IQA) triplets and human preference data (image-question-chosen-rejected quadruples). The machine-generated data is tailored to customize student models for image captioning and VQA tasks on PFD and P&ID analysis. Furthermore, we leverage the Google Cloud Vision API for text detection and OCR tasks, generating IQA triplet data for bounding boxes and detected text recognition tasks. We employ PaliGemma-8K-instruct, a single-turn vision-language model, as the student model. The instruction-tuned student model excels in single-turn analysis but struggles with"}, {"title": "3 Experiments", "content": "We evaluated our multi-agent framework on open-domain and close-domain QA tasks for analyzing complex PFDs and P&IDs through image captioning, VQA, and OCR tasks. We built a dataset from academic sources, industry examples, and public repositories, comprising 75 PFDs and 50 P&IDs. We generated image captions and detailed text descriptions of the PFDs and P&IDs using GPT-4(Omni). These were compiled into a PDF document, resulting in two distinct categories: DPFDs containing the PFDs with their captions and descriptions, and DP&IDs containing the P&IDs with their captions and descriptions. PFD and P&ID documents were parsed using a sliding window technique to improve information retrieval. Text chunks were embedded and indexed, while images were processed using GPT-4 to generate text descriptions, which were then indexed for multi-modal search. OpenAI GPT-4(Omni) and Google Gemini Pro were utilized as teacher models to generate high-quality instruction-tuning and preference-tuning data, including image-question-answer triplets and image-question-chosen-rejected pairs, tailored for PFD and P&ID analysis. We generated diverse QA pairs to address domain-specific challenges and ensure a high-quality machine-generated dataset, including 625 image captioning QA triplets, 16,000 VQA pairs, and 10,500 text detection and OCR annotations (including image-augmented data). These datasets were split into 70% training, 15% validation, and 15% test sets. They are essential for building and evaluating a robust multi-agent framework capable of handling real-world PFD and P&ID analysis tasks. We compared the proposed framework's performance against baseline models, including proprietary models, on image captioning, VQA, text detection, and OCR tasks. The baselines include GPT-4 Turbo-preview, Claude-3 Opus, and Google Gemini 1.0 Pro for a rigorous comparison with state-of-the-art LMMs. For image captioning and VQA (including logical, common sense, and multi-step reasoning), we used BLEU, ROUGE, and METEOR to evaluate caption accuracy versus ground truth. For multiple-choice VQA tasks, we used precision, recall, F1, and exact match to measure answer correctness against ground truth. Evaluating text detection and OCR involved metrics for localization accuracy (Bounding Box Precision, Recall, F1-Score, Intersection over Union (IoU)) and recognition quality (Character Error Rate (CER), Word Error Rate (WER)). The main agent uses Google Gemma-7b-it. The sub-agents use PaliGemma-3b-mix, and the critique agent uses GPT-4 Turbo. We use the open-source BGE embedding method as a search engine to retrieve relevant passages from external sources for knowledge-augmented text generation. We fine-tune an adapter for task-specific customization to improve BGE embedding retrieval performance for PFD and P&ID analysis. The adapter is trained to rank relevant documents higher than irrelevant passages for a given query by learning the semantic relationships between similar questions and their corresponding answers,"}, {"title": "4 Conclusion", "content": "The proposed multi-agent framework significantly advances human-level understanding of complex engineering diagrams, ensuring enhanced data privacy, explainability, and cost-effectiveness while achieving superior performance in PFD and P&ID analysis. Experimental results confirm the framework's effectiveness, highlighting its transformative potential in the chemical and process industries."}]}