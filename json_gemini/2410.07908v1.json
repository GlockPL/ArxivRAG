{"title": "ONCOPILOT: A Promptable CT Foundation Model For Solid Tumor Evaluation", "authors": ["L. Machado", "H. Philippe", "E. Ferreres", "J. Khlaut", "J. Dupuis", "K. Le Floch", "D. Habip Gatenyo", "P. Roux", "J. Gr\u00e9gory", "M. Ronot", "C. Dancette", "D. Tordjman", "P. Manceron", "P. H\u00e9rent"], "abstract": "Carcinogenesis is a proteiform phenomenon that can lead to metastatic spread, with tumors emerging in various locations and displaying complex, diverse shapes. As a crucial focus at the intersection of research and clinical practice, it demands precise and flexible assessment. However, current biomarkers, such as RECIST 1.1's long and short axis measurements, fall short of capturing this complexity, offering an only approximate estimate of tumor burden and an overly simplistic representation of a far more intricate process.\nAdditionally, existing supervised AI models face challenges in adequately addressing the variability in tumor presentations, which limits their clinical utility. These limitations arise from the scarcity of annotations and the models' focus on narrowly defined tasks.\nTo address these challenges, we developed ONCOPILOT, an interactive radiological foundation model trained on approximately 7,500 CT scans covering the whole body, from both normal anatomy and a wide range of oncological cases. ONCOPILOT performs 3D tumor segmentation using visual prompts like point-click and bounding boxes, outperforming state-of-the-art models (e.g., nnUnet-based) and achieving radiologist-level accuracy in RECIST 1.1 measurements. The key advantage of this foundation model is its ability to surpass state-of-the-art performance while keeping the radiologist in the loop, a capability that previous models could not achieve. When radiologists interactively refine the segmentations, accuracy improves even further. ONCOPILOT also accelerates measurement processes and reduces inter-reader variability. Moreover, it facilitates volumetric analysis, unlocking new biomarkers for deeper insights.\nThis AI assistant is expected to enhance the precision of RECIST 1.1 measurements, unlock the potential of volumetric biomarkers, and improve patient stratification and clinical care, while seamlessly integrating into the radiological workflow.", "sections": [{"title": "1 Introduction", "content": "The wide variability in tumor appearance and location makes precise monitoring of oncological disease a critical challenge for both clinical care and research. Effective evaluation of oncological disease is essential for accurately assessing tumor aggressiveness, predicting prognosis, and guiding treatment decisions.\nThe Response Evaluation Criteria in Solid Tumors (RECIST v1.1) has long been regarded as the gold standard for radiologically assessing solid tumors over time [10], allowing for patient stratification based on disease response or progression. However, this method has significant limitations: the low information yield from linear long axis measurement in comparison to total tumor burden [18, 6], the arbitrary and non-reproducible selection of target lesions, which can result in the misclassification of disease status [16] and significant inaccuracies in measuring the long axis, with inter-reader variability exceeding 20% [28], further contributing to classification errors.\nTraditionally, the long and short axes of the tumor are used as widely accepted proxies for estimating tumor size on CT scans. However, in the era of quantitative imaging, these linear measurements are increasingly considered inadequate as the field shifts toward more informative quantitative markers, such as volumetry [20] and shape assessments, including tumor eccentricity and irregularity [25].\nVolumetric analysis, more sensitive to change than diameter due to its proportionality to the cube of the radius, is proving advantageous in detecting tumor burden changes, especially for tumors with irregular shapes, where linear measurements fail to capture their complexity [12]. Recently, novel radiomics biomarkers derived from volumetric analysis have shown significant promise in oncological evaluation, notably in colon and lung cancers [8, 7].\nDespite its promise, volumetric measurement is time-consuming [29] and impractical to perform manually. While efforts have been made to automate the volumetric delineation of oncological lesions, from early models relying on manual feature extraction to approaches using deep learning with convolutional neural networks [22], these models remain limited. Most are organ-specific, and the solutions currently employed in clinical practice are effective primarily in straightforward cases, such as lung nodules, but struggle with the diverse appearances of metastatic lesions. Furthermore, these methods often lack interactivity and adaptability to varying inputs, which restricts their integration into the radiological workflow.\nThe emergence of foundation models, a paradigm shift in deep learning, could alleviate these issues. Powered by transformer architecture and self-attention mechanisms [23], foundation models, when trained on extensive datasets, can significantly outperform traditional deep-learning systems [5]. Their key strengths lie in transfer learning and zero-shot classification, enabling them to handle tasks not encountered during initial training capabilities that traditional deep-learning models lack.\nRecently, these models have been positioned as the future of medical imaging, offering potential solutions to critical challenges such as poor generalization and the need for large quantities of labeled training data [3]. Notably, they can generate reliable segmentation masks using text or visual prompts (e.g., actions taken on the images by the user), such as bounding boxes or point-click inputs on regions of interest [15]. The ability to dynamically refine segmentation masks and generate varying outcomes from different visual prompts is a crucial step toward explainable AI, addressing the opaque nature of traditional models, and making it usable for radiologists.\nIn response to these advancements, we developed ONCOPILOT, an interactive foundation model trained using publicly available CT scans of normal anatomy and more than 7,500 tumors from diverse organs. Our model aims to deliver precise and reproducible RECIST measurements and to facilitate the volumetric analysis of oncologic lesions within an interactive viewer. We evaluated ONCOPILOT against a panel of radiologists and investigated the potential for integrating this AI assistant into the radiologist's workflow."}, {"title": "2 Materials and Methods", "content": ""}, {"title": "2.1 Foundation Model", "content": "ONCOPILOT is a foundation model adapted from SAM [15], specifically aiming to segment biomedical images. Such an approach has been concomitantly adopted in the literature in the form of MedSAM [17], SegVol [9], and SAM-Med3D [24]. This model is trained to perform image segmentation tasks. It processes 2D images and prompts, such as a bounding box, a point, or a mask. The model aims to generate a 3D prediction of the volume of a specific anatomical structure based on the input image and visual prompt. From the initial slice 2D segmentation masks are propagated sequentially across the z-axis until they encounter the boundaries of the object. Alternatively, the propagation can be halted based on predefined stopping criteria once specific conditions are met. The result of this propagation is a 3D segmentation mask.\nThe model is initialized with the released weights of the SAM model [15] and trained in a supervised manner with an initial general pre-training on normal anatomy and oncological lesions followed by a specialization on oncological lesions with a focused fine-tuning on tumors only. The initial training took 40 hours on 32 V100 Nvidia GPUs (totaling 1280 GPU hours) with a learning rate of 10-5."}, {"title": "2.2 Evaluation criteria of ONCOPILOT's performances", "content": "To assess the effectiveness of our model, three parameters are taken into account:\nSegmentation performance: Measured using the DICE score, a standard metric for evaluating segmentation quality. The performance of ONCOPILOT is compared to that of a state-of-the-art model, see section Baseline.\nLong axis measurement performance: Evaluated using the absolute error and inter-operator variability as metrics. For more details, see section RECIST measurement and ONCOPILOT evaluation against radiologists.\nRadiologist's time efficiency and precision: The average time a radiologist takes to complete one measurement and the inter-reader variability serve as metrics for evaluating the integration of the model into a radiologist's workflow. For further information, see section ONCOPILOT integration into radiologist's workflow."}, {"title": "2.3 Baseline", "content": "To compare the performance of our foundation model to state-of-the-art segmentation models we used the model provided by the ULS23 oncological lesion segmentation challenge as a baseline [11]. We used the result of their full model (nnUnet-ResEnc+SS) on the 10% held-out test set originating from their fully-labeled dataset. This model is based on the nnUnet architecture [14] and has been trained and fine-tuned on a dataset of 38,693 lesions including fully-labeled and partially-labeled tumor masks."}, {"title": "2.4 Datasets", "content": "ONCOPILOT was pre-trained on publicly available datasets containing medical images and segmentation masks for general anatomy and oncological lesions:\n\u2022 1204 CT scans from TotalSegmentator v1 [26], with 104 labeled anatomical structures (27 organs, 59 bones, 10 muscles, 8 vessels).\n\u2022 743 diverse tumors from the DeepLesion dataset [27], curated and segmented for the ULS23 challenge [11], referred to as ULS23 DeepLesion.\n\u2022 697 bone oncological lesions and 120 pancreatic tumors from the Radboudume hospital, available through the ULS23 dataset [11].\n\u2022 470 volumes from the multimodal MSD challenge [1], using only the Lung, Colon, Pancreas datasets.\n\u2022 700 lung nodules from the LNDb dataset [19].\n\u2022 300 kidney tumors from the KITS23 dataset [13].\n\u2022 832 liver tumors from the LiTS dataset [4], also part of the MSD challenge.\n\u2022 932 mediastinal and abdominal lymph nodes from the NIH-LN dataset [2, 21].\n\u2022 2236 lung oncological lesions from the LIDC-IDRI dataset [2].\nA random 90% training set was selected, with a 10% held-out test dataset, by analogy with the ULS23 challenge. A randomly selected set of 67 tumors > 10 mm in size (\u2265 15 mm for lymph nodes) from the ULS23 DeepLesion training set was kept out for validation against radiologists."}, {"title": "2.5 Segmentation Process", "content": "The model had access to the entire volume and to the visual prompt. The image thresholding was fixed at -500 ; +1000 UH, an unrestricted window akin to bone window-ing, which empirically led to the best overall results (data not shown). The model outputs then an initial segmentation mask in 2D. The segmentation masks for the remaining 2D axial slices are then calculated autoregressively, using the mask from the adjacent slice as the prompt for the next slice. This process propagates the segmentation masks from the middle slice, resulting in a 3D segmentation mask.\nThe ONCOPILOT model is evaluated in three experimental settings using visual prompts that simulate real-life usage:\n\u2022 Bounding box: the model is prompted with a 2D bounding box outlining the lesion from the middle slice of the ground-truth mask, expanded with an offset of 15 pixels.\n\u2022 Point-click: the model is prompted by a single point, which is determined as the barycenter of the ground-truth mask or the closest point that falls within the segmentation mask.\n\u2022 Point-edit: to simulate interactions with radiologists, the 3D segmentation mask proposed in point-click mode is refined using up to 4 edit point-clicks chosen as the barycenter of the prediction error that can be negative (i.e., reduce an over-segmented mask) or positive (i.e., expand an under-segmented mask)."}, {"title": "2.6 Morphology Analysis", "content": "Sphericity index is calculated as the ratio of the surface area of a sphere to the surface area of the ground truth segmentation mask, given equal volumes, with a perfect sphere having a sphericity index of 1 while irregular structures being closer to 0. The following formula was used:\n$S = \\frac{\\pi^{1/3} \\cdot (6V)^{2/3}}{A}$\nwhere S is the sphericity, V is the volume of the object, and A is the surface area of the object."}, {"title": "2.7 RECIST Measurement", "content": "RECIST measurements from ONCOPILOT were inferred from the segmentation masks in bounding box, point and point-edit modes. The primary measurement evaluated was the long axis of the oncological lesion, with the following amendments for simplicity and consistency: it was applied even to lymph nodes and restricted to the axial plane.\nTo further understand ONCOPILOT's performance in real oncological evaluations, we compared it against a panel of radiologists for RECIST v1.1 measurements. Using a validation set of 67 tumors from diverse organs kept out from the ULS23 DeepLesion dataset, we compared the long axis in the axial plane inferred from ONCOPILOT predicted segmentation masks to manual annotations made by the radiologist panel composed of three radiologists with a minimum of 18 months of experience. These 67 tumors are selected for their inclusivity as potential target lesions according to the RECIST v1.1 guidelines (i.e., solid lesions with a long axis > 10 mm, lymph nodes with a short axis \u2265 15 mm) and filtered for their segmentation quality. The measures proposed by ONCOPILOT and the radiologists were compared to those inferred from the ground-truth segmentation masks to extract the measurement error.\nRadiologists used our in-house viewer for manual and ONCOPILOT-assisted measurement of the lesion's long axis. They could zoom at will, modify the image's window-ing, and navigate the volume freely, without the help of multi-planar reconstruction. The barycenter of the lesion was superimposed on the initial volume to indicate the lesion of interest without biasing the radiologist by showing the ground-truth mask. For ONCOPILOT-assisted measurements, the radiologists used the measures inferred by the model after automatic segmentation of the target lesion by ONCOPILOT using bounding box visual prompts only for simplicity.\nInter-operator variability was defined as the deviation of each radiologist's measurement from the overall average of all measurements for a given lesion, whether those measurements are aided by the segmentation model or done manually. Essentially, it represents the mean error of each radiologist compared to the collective average. Measurement duration was recorded for each assessment. It is defined as the time from the CT's initial display to getting the object's final measurement."}, {"title": "2.8 ONCOPILOT integration into radiologist's workflow", "content": "Finally, to determine whether ONCOPILOT could serve as an AI companion, we evaluated its integration and improvement of the oncological evaluation workflow within our in-house environment. We measured inter-reader variability as the mean deviation from the average between radiologists performing RECIST measurements manually and radiologists performing these measures semi-automatically using the segmentation model by drawing a bounding box around the lesion. We also timed the duration required to obtain measurements in each scenario."}, {"title": "3 Results", "content": ""}, {"title": "3.1 Foundation Model", "content": "Our foundation model, ONCOPILOT, was pre-trained on a diverse dataset comprising normal anatomy and oncological lesions, totaling 2,374 CT scans including 104 anatomical structures (e.g., organs, bones) and 4 oncological lesions regardless of histology and malignity (i.e., lung, liver, pancreas and colon tumors) from the MSD dataset (Figure 1A), without distinction regarding their histological type or malignancy.\nTo become specialized for oncology the model was subsequently fine-tuned on a comprehensive dataset of 6,229 tumors from various organs (e.g., pancreas, bone, liver, kidney, lung, lymph nodes). ONCOPILOT is designed to interactively segment oncological lesions in 3D, utilizing visual prompts such as a bounding box (referred to as bbox) around the lesion of interest or a point-click (referred to as point) inside it (Figure 1B). To simulate the dynamic refinement of the predicted segmentation masks by radiologists we developed an editing mechanism (referred to as point-edit, see Material & Methods)."}, {"title": "3.2 Segmentation Performance", "content": "ONCOPILOT surpassed the baseline model in all evaluation metrics-point, point-edit, and bbox-across all lesion types, with the exception of lung tumors, where only the point-edit model demonstrated superior performance (Figure 2A, with examples of successful segmentations in Figure 2B).\nThe test dataset was imbalanced, with over 40% of the lesions being lung tumors, which biased the overall DICE score in favor of the baseline model (further adressed in the Discussion section). ONCOPILOT achieved mean DICE scores of 0.70 for point mode, 0.70 for bbox mode, and 0.78 for point-edit mode, compared to 0.70 for the baseline. The distribution of lesion sizes by organ and examples of failed segmentations are provided in Supplementary Figures S1A and S1B. Additionally, it is worth mentioning that nnUNet models are often restricted to specific tasks, tends to underperform on complex datasets, and require longer training and inference times while ONCOPILOT offers an all-in-one solution."}, {"title": "3.3 Morphology Analysis", "content": "The segmentation masks outputted by the model in point mode were influenced by the lesion morphology and size. Indeed, ONCOPILOT exhibited lower DICE scores for lesions with irregular, non-spherical shapes, with a mean DICE of 0.66 for tumors with a sphericity index below 0.6, compared to 0.71 for more spherical tumors in point mode (p < 0.001, Figure 3A, Supplementary Figure S2A).\nSimilarly, smaller lesions yielded lower DICE scores, with a mean of 0.67 for lesions with a long axis < 15mm versus 0.73 for larger lesions (p < 0.001, Figure 3B, Supplementary Figure S2B). This trend persisted when using volume as a metric: lesions under 1 mL had a mean DICE of 0.67, compared to 0.74 for larger lesions (p < 0.001, Figure 3C, Supplementary Figure S2C). Crucially, interactive editing mitigated these biases, eliminating significant differences in DICE scores between lesions of varying sphericity, long axis, or volume in point-edit mode. This approach also reduced disparities in DICE between lesion types (Figure 3D). Additionally, when using RECIST measurements for the long axis instead of DICE scores, interactive editing significantly reduced measurement errors, with the median error decreasing from 14.1% in point mode to 9.6% in point-edit mode (p < 0.001). This level of accuracy is consistent with the reported inter-reader variability among radiologists for single-lesion measurements ([28], Figure 3E)."}, {"title": "3.4 ONCOPILOT Evaluation Against Radiologists", "content": "ONCOPILOT demonstrated radiologist-level performance in point, point-edit, and bbox modes (Figure 4A, 4B). There was no statistically significant difference between the different ONCOPILOT models when evaluated against radiologists, with a median absolute error in long axis measurement of 1.3 mm for radiologists (8.6% of the median lesion size) versus 1.1 mm for ONCOPILOT in point-edit mode (7.4%), 1.6 mm in point mode (10.8%), and 1.5 mm in bbox mode (10.4%).\nRadiologists demonstrated a faster measurement speed using ONCOPILOT, with an average time of 17.2 seconds per measurement compared to 20.6 seconds with manual annotations (p < 0.05). Notably, this improvement in speed was achieved without focusing on speed optimization, as it operated on a non-optimized web-based platform (show-cased in Supplementary Figure S3A to S3E). Most of the measurement time was spent locating the lesion within the exam, suggesting that ONCOPILOT could be further accelerated with targeted improvements."}, {"title": "3.5 ONCOPILOT Integration into Radiologist's Workflow", "content": "ONCOPILOT enhanced the reproducibility and efficiency of radiologist measurements, with an inter-reader deviation of 1.7 mm when assisted by ONCOPILOT versus 2.4 mm manually (Figure 4C, 4D, p < 0.05). Additionally, ra-"}, {"title": "4 Discussion", "content": "In summary, ONCOPILOT demonstrated state-of-the-art performance in tumor segmentation across a diverse set of oncological lesions, achieving radiologist-level accuracy in RECIST 1.1 measurements. The model's flexibility, enabled by interactive visual prompts and refinement ca-"}]}