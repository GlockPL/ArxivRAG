{"title": "Disambiguate First Parse Later: Generating Interpretations for Ambiguity Resolution in Semantic Parsing", "authors": ["Irina Saparina", "Mirella Lapata"], "abstract": "Handling ambiguity and underspecification is an important challenge in natural language interfaces, particularly for tasks like text-to-SQL semantic parsing. We propose a modular approach that resolves ambiguity using natural language interpretations before mapping these to logical forms (e.g., SQL queries). Although LLMs excel at parsing unambiguous utterances, they show strong biases for ambiguous ones, typically predicting only preferred interpretations. We constructively exploit this bias to generate an initial set of preferred disambiguations and then apply a specialized infilling model to identify and generate missing interpretations. To train the infilling model, we introduce an annotation method that uses SQL execution to validate different meanings. Our approach improves interpretation coverage and generalizes across datasets with different annotation styles, database structures, and ambiguity types.", "sections": [{"title": "Introduction", "content": "Natural language utterances are often ambiguous, vague, or underspecified, giving rise to multiple valid interpretations. Figure 1 shows an ambiguous request (\"return the rating of each hotel\") in the context of natural language interfaces. In the example, \"rating\" could refer to the number of stars hotels receive as an indication of their quality (e.g., 4 stars) or guest reviews on booking websites (e.g., 8.5 out of 10), or both. Ignoring such ambiguity can lead to incomplete or incorrect results, undermining user trust and limiting the practical usefulness of any conversational system.\nWhile state-of-the-art large language models (LLMs) demonstrate remarkable performance on tasks like question answering, text-to-SQL parsing, and natural language inference, recent studies (Liu et al., 2023; Floratou et al., 2024) have shown they are not adept at handling ambiguity. They display systematic biases in their choice of interpretation (Kamath et al., 2024; Stengel-Eskin et al., 2024; Saparina and Lapata, 2024a), typically default to a single interpretation when multiple ones exist.\nWhat are the response strategies LLMs should adopt to address ambiguity? An approach might be to respond with a clarification question, which directly engages users and ensures accurate resolution of the ambiguity but introduces additional interaction turns. Alternatively, presenting multiple possible interpretations (an \u201coverton response\") would allow users to select the most relevant answer themselves. This approach minimizes interruptions, caters to users with different levels of expertise, and provides interpretability by making the system's reasoning explicit. For tasks like semantic parsing (see Figure 1), a hypothetical response should not only include interpretations in their final form (e.g., SQL queries) but also their different readings in natural language. From a modeling perspective, interpretations serve as explanations (of the system's output) and as intermediate representations, providing a way to decompose complex semantic parsing problems into simpler steps.\nIn this work, we focus on text-to-SQL parsing of ambiguous questions, and propose a two-stage approach that first disambiguates by generating all possible meanings in natural language, and then parses each unambiguous interpretation. Separating the disambiguation and parsing tasks allows us to use existing semantic parsers that perform generally well on unambiguous inputs. We obtain interpretations, by prompting an LLM to generate all possible meanings for an utterance. These initial interpretations are often incomplete due to inherent biases stemming from statistical patterns found in LLM training data, lack of intrinsic knowledge, and different alignment preferences. Rather than attempting to correct these biases, we introduce an infilling model that reviews the ambiguous question and initial interpretations, and then generates any missing ones.\nThe infilling model is trained on pairs of default interpretations and missing readings. Problematically, existing datasets provide (multiple) SQL parses for ambiguous questions without explicitly verbalising the interpretations they correspond to. We create synthetic reference interpretations for AmbiQT (Bhaskar et al., 2023), a recently proposed benchmark for parsing ambiguous questions into SQL. We exploit the fact that generated interpretations can be converted to SQL queries which we execute to verify whether they are correct and to establish which interpretations are missing, i.e., whether there exist gold SQL parses for which no interpretation was found. We evaluate our approach on Ambrosia (Saparina and Lapata, 2024a), a dataset different from AmbiQT in terms of database content and the types of ambiguity it represents. Our contributions are summarized as follows:\n\u2022 We propose a modular approach that uses natural language to spell out ambiguity before mapping individual interpretations to logical forms (e.g., SQL queries).\n\u2022 We use LLMs to generate an initial set of preferred disambiguations and then apply a specialized infilling model to identify and generate missing interpretations.\n\u2022 Experiments show that our \u201cdisambiguate first parse later\u201d strategy improves the coverage of interpretations for ambiguous questions and generalizes across annotation styles, database structures, and ambiguity types."}, {"title": "Disambiguate First Parse Later", "content": "2.1 Problem Formulation\nSemantic parsing is the task of mapping a natural language utterance u to formal expression e in grammar G, where e captures the intended meaning of u. The expression e can be then executed in an environment \u0190 to produce a denotation [e]\u0190. In the unambiguous case, there is a single valid expression e that corresponds to the user's intent. However, when u is ambiguous, there are multiple valid expressions {e1,..., en} where [ei]\u0190 \u2260 [ej]\u0190 for some i, j, with each ei representing a valid interpretation of the user's intent.\nIn our text-to-SQL task, grammar G defines valid SQL queries for a given database schema S. The database schema and table descriptions provide context C that can help disambiguate some queries. We focus on questions that remain ambiguous even when C is known, i.e., there exist multiple valid SQL queries {e1, ..., en} that respect S and produce different result sets [ei]\u0190 when executed on the database. In the example in Figure 1, the question u = \"return the rating of each hotel\" remains ambiguous for a given schema S (see top table), as \"rating\" could refer to star ratings and guest reviews even when database content is known.\n2.2 Natural Language Interpretations for Explicit Disambiguation\nWe propose to resolve ambiguity prior to generating SQL expressions with natural language interpretations. More formally, for ambiguous utterance u, we first produce a set of unambiguous natural language interpretations {\u00fb1, ..., \u00fbn}, where each \u00fbi captures one meaning of u. We then map each \u00fbi to formal expression ei. For example, given u = \"return the rating of each hotel\", our goal is to produce \u00fb\u2081 = \u201cHow many stars were assigned to each hotel?\u201d, \u00fb2 = \u201cHow did the customers review each hotel?\u201d and \u00fb3 = \"Show me the guest scores and star rating of each hotel\" and mapped them to corresponding SQL queries e1, e2 and e3 (see Figure 1). This approach has several advantages:\n\u2022 Unambiguous interpretations \u00fbi are easier to map to formal expressions, as they can be handled by standard semantic parsers (e.g., existing text-to-SQL models);\n\u2022 Natural language interpretations provide transparency, making the system's internal working explicit to users;"}, {"title": "Default Interpretation Generation", "content": "LLMs exhibit strong biases in interpretation generation, consistently favouring certain types while missing others. We take advantage of these biases by using LLMs to generate an initial set of interpretations for ambiguous utterances.\nThe first component of our approach is a prompt-based interpretation generator that produces candidate interpretations given utterance u and database context C. We prompt an LLM to identify and list all semantically different interpretations of u. In practice, LLMs generate only a subset of possible interpretations, typically the most straightforward ones. We refer to them as preferred or default interpretations. The prompt is designed to handle both ambiguous and unambiguous cases, allowing the model to return a single interpretation when appropriate. As we rely on the LLM's ability to disambiguate to its default interpretations, no additional training is required.\nThe output of this module is a set of natural language interpretations {\u00fb1,..., \u00fbk}, which are mapped to formal expressions {e1, ..., ek} using a standard semantic parser. For text-to-SQL tasks, executing these queries allows us to identify paraphrases (interpretations that lead to identical execution results) and filter redundancy."}, {"title": "Interpretation Infilling", "content": "While initial interpretations from LLMs capture a common understanding of queries, they often miss valid alternative meanings. We propose to address this issue by introducing a specialised model that identifies and generates missing interpretations.\nGiven utterance u, database context C, and default interpretations {\u00fb1, ..., \u00fbk}, our model determines whether additional interpretations exist and generates these accordingly. The output is a set of interpretations {\u00fb\u03b9, ..., \u00fbm} which complement the initial set. These interpretations are then also mapped to formal expressions {el, ..., em} (SQL queries in our case) using a standard semantic parser. We train the infilling model in a supervised manner. For now, let us assume we have access to reference interpretations and corresponding gold SQL queries. By comparing the set of default interpretations with reference interpretations, we identify which meanings are missing from the initial set. Reference interpretations absent from the default set serve as target outputs for the infilling model.\nDetermining whether two sentences have the same meaning, given some (database) context, is an extremely challenging task. However, instead of comparing natural language expressions directly, we compare their corresponding SQL queries. In particular, we know which gold SQL queries are associated with each reference interpretation. Similarly, we can obtain SQL queries predicted by a text-to-SQL parser for the initial set of default interpretations. By executing these queries and comparing their results, we determine which interpretations match and are thus covered by the initial set. Reference interpretations absent from this set serve as training targets for the infilling model which also explicitly indicates when all interpretations are covered by the initial set, and there is nothing to add (the target output in this case is the sentence: \"All interpretations are covered.\") Figure 3 illustrates the annotation process just described."}, {"title": "Discussion", "content": "The strength of our proposal lies in its modular design, consisting of three separate components: default interpretation generation, infilling, and text-to-SQL parsing. Only infilling requires training, since we resort to existing pre-trained models for generating interpretations and parsing these into SQL. This plug-and-play functionality allows us to experiment with different LLMs and text-to-SQL parsers, without retraining. Moreover, any improvements in the infilling module should in theory translate into improved coverage.\nA potential limitation is that infilling requires reference interpretations and SQL queries, which may be difficult to come by. In the next section, we demonstrate how reference interpretations can be synthetically generated. In addition, the text-to-SQL parser may occasionally introduce errors, leading to duplicate interpretations in the final set, coming from the initial generator and infilling model. However, this redundancy can improve robustness by providing multiple formulations of the same meaning. Additionally, duplicates can be filtered in post-processing by comparing execution results. It is also important to note that the infilling does not filter any interpretations from the initial set; it can only add new ones or leave the set unchanged. This means that, in some cases, incorrect interpretations may persist, potentially reducing precision. However, we believe this is not a major concern in practice, as users can simply ignore irrelevant interpretations.\nWhen questions are unambiguous, the initial set contains a single interpretation (or paraphrases with the same meaning, which can be filtered as duplicates), and the infilling model simply outputs \"All interpretations are covered.\u201d This means that we do not need to explicitly determine whether a question is ambiguous or not, our approach naturally handles both cases."}, {"title": "Experimental Setup", "content": "3.1 Evaluation Datasets\nWe evaluate our approach on two recent benchmarks aiming to assess the capabilities of text-to-SQL parsers when faced with ambiguous questions: AmbiQT (Bhaskar et al., 2023) and Ambrosia (Saparina and Lapata, 2024a). Examples can be found in Appendix A.\nAmbiQT builds upon the widely used Spider dataset (Yu et al., 2018). Specifically, ambiguity is injected by generating synonyms for column and table names (via ChatGPT and heuristically), by having tables with overlapping column names (which leads to join ambiguities), and by introducing columns which are aggregates of certain values in addition to aggregating values with the group-by clause of the SELECT statement. AmbiQT inherits from Spider diverse SQL queries and real-world databases, but also introduces redundancy as the databases can contain duplicates (e.g., identical columns \"singer\u201d and \u201cperformer\"). It features two types of ambiguity, namely lexical ambiguity (based on ambiguous column and table names) and structural ambiguity in SQL queries (due to join and pre-computed aggregates). While the AmbiQT test set contains 3K examples, we filter out those that yield empty execution results, leaving a final set of 1.8K non-trivial examples.\nAmbrosia showcases three different types of ambiguity, namely scope ambiguity, attachment ambiguity, and vagueness. It contains human-written questions, SQL queries either manually written or based on templates, and synthetically generated databases. In addition to ambiguous questions and corresponding SQL queries, Ambrosia has explicit interpretations in natural language. The official test set has 1K ambiguous questions and 2K interpretations. We use the ambiguous subset in all experiments unless otherwise stated. Notably, AmbiQT and Ambrosia handle vagueness differently: AmbiQT consistently provides two valid interpretations, whereas Ambrosia may provide up to three interpretations for vague questions."}, {"title": "Evaluation Metrics", "content": "Our primary metric is Full Interpretation Coverage, which measures the proportion of examples where all valid SQL interpretations are present in the output. This metric aligns with the \u201cAll Found\" metric from Ambrosia and \"BothInTopK\" from AmbiQT. We also report Single Interpretation Coverage, i.e., the proportion of examples where at least one valid interpretation is found (\u201cEitherInTopK\" from AmbiQT). We compare SQL queries based on their execution results."}, {"title": "Training Data", "content": "To train the infilling model, we need three components: (1) generated default interpretations, (2) reference SQL queries, and (3) reference interpretations to identify missing interpretations from the default set. It is straightforward to elicit default interpretations from an LLM and SQL queries are available in many text-to-SQL datasets. To collect reference interpretations, we propose a novel approach that extends the data generation process in AmbiQT (Bhaskar et al., 2023).\nAmbiQT relies on ChatGPT to generate two synonyms for a selected column or table in an SQL query which naturally renders the corresponding question vague. The original SQL mentions are replaced with their synonyms, resulting in two gold SQL queries. Building upon this approach, we use these synonyms and prompt an LLM (with three in-context examples) to replace the original mentions in the questions with their synonyms. Our experiments use the instruction tuned Llama 3.1 8B (Dubey et al., 2024). The full prompt is provided in Appendix B. We verify the quality of the synthetically generated interpretations by attempting to generate correct SQL queries using a specialized code generation LLM (instruction-tuned Qwen2.5-Coder 32B; Hui et al. 2024). We only accept examples where both interpretations succeed within five attempts.\nThis approach is particularly effective for AmbiQT and other datasets based on Spider (Yu et al., 2018) as it contains many questions which directly mention table and column names (Deng et al., 2021; Suhr et al., 2020; Gan et al., 2021), making synonym substitution natural and fail-safe. We generate interpretations for approximately 5K examples from a subset of the Spider training data. Although our experiments focus primarily on AmbiQT, the proposed interpretation generation method can be applied to other domains and datasets (together with AmbiQT's approach of generating ambiguous examples using column and table synonyms)."}, {"title": "Implementation Details", "content": "The first component of our method is to generate default interpretations for an ambiguous question. We design a zero-shot prompt that uses the provided database and question to generate interpretations (see Appendix C). Here and throughout, we represent the database as an SQL dump. For our experiments, we use instruction-tuned Llama-3.1 8B, as it produced the most coherent interpretations among similarly sized models.\nThe second component is the infilling model which takes the database, question, and default interpretations as input, and outputs missing interpretations. As our infilling model, we train a LoRA adapter (Hu et al., 2022) on top of the instruction-tuned Llama-3.1 8B (see Appendix D).\nThe final component is a text-to-SQL model for which we select the instruction-tuned Qwen2.5-Coder-32B, a specialized model for code generation. We also use this model to match and identify missing interpretations (Section 2.4). A zero-shot prompt for unambiguous text-to-SQL parsing is provided in Appendix E. Thanks to our modular structure, any component of our system can be easily replaced with a more powerful or more efficient model if needed."}, {"title": "Experimental Results", "content": "It is better to disambiguate first and parse later both in in-domain and out-of-domain settings. Table 1 presents our main experimental results on AmbiQT and Ambrosia. We compare our approach with both prompt-based and fine-tuning baselines. Note that for methods requiring fine-tuning, AmbiQT represents in-domain evaluation, whereas evaluation on Ambrosia is out of domain. All methods use the same model, instruction-tuned Llama-3.1 8B, and those that require training are fine-tuned using a LoRA adapter on the same AmbiQT subset, augmented with interpretations.\nTable 1 is split into two sections. The first one presents end-to-end approaches, which attempt to directly predict multiple SQL queries for ambiguous questions. We report results for zero-shot prompting, few-shot prompting, and end-to-end fine-tuning. For prompting, we follow Saparina and Lapata (2024a) and explicitly instruct the model to generate multiple SQL queries if the question is ambiguous. For few-shot prompting, we sample 3 random examples from the corresponding dataset. The second section in Table 1 lists methods which use natural language interpretations to disambiguate first and then rely on text-to-SQL parsing of unambiguous questions. We report results for generating all possible interpretations through LLM prompting (which corresponds to our method without infilling), applying self-correction to this approach (details in Appendix F), a fine-tuning method which is trained to generate reference interpretations instead of SQL queries, and our proposal which uses infilling to augment the set of default interpretations. We use an instruction-tuned Qwen2.5-Coder 32B for text-to-SQL generation.\nAs can be seen in Table 1, prompting (0-shot, 3-shot) performs poorly on ambiguous questions, which is consistent with the findings of Saparina and Lapata (2024a). Fine-tuning achieves excellent coverage on in-domain evaluation (AmbiQT), but fails to generalize to Ambrosia, which suggests that the model overfits specific patterns in AmbiQT. Interpretation generation (via prompting) shows promising results on both datasets compared to end-to-end methods. Self-correction filters out some valid interpretations. Fine-tuning on interpretations behaves very similarly to fine-tuning on SQL queries, but provides higher single-interpretation coverage, which suggests it is more accurate in predicting at least one correct interpretation.\nInterpretation generation with infilling further improves single and full interpretation coverage on both datasets. While it does not achieve the highest full coverage on AmbiQA, it substantially improves over end-to-end prompting reaching 53%, while delivering the best single interpretation coverage of 92%. It effectively generalises to new domains and ambiguity types, showing the best results on Ambrosia among all methods. However, the full coverage on Ambrosia is still relatively low (at 19%). Ambrosia is a challenging benchmark on its own but it is also possible that some interpretations are missed due to annotation differences between the two datasets.\nInfilling boosts performance across interpretation generation models. We next analyze key components of our approach through ablation studies. Table 2 focuses on the first stage of our method, namely disambiguation. We compare default interpretations from three different instruction-tuned models of similar size: Qwen-2.5 7B (Yang et al., 2024), Llama-3.1 8B, and Gemma-2 9B (Riviere et al., 2024). We observe that Llama-3.1 8B provides the best default interpretations and our infilling model improves upon all interpretations, irrespective of the generation model.\nText-to-SQL parsing is hard even with gold interpretations! To provide an upper bound for our approach, we use gold interpretations from Ambrosia and evaluate how well the text-to-SQL parser performs when all interpretations are correct. The results are, as expected, significantly higher, indicating room for improvement in interpretation generation. However, since the full coverage reaches only 49%, a substantial number of errors may come from the unambiguous text-to-SQL parsing alone.\nTable 3 compares our text-to-SQL model, the instruction-tuned Qwen-2.5 Coder 32B, with its smaller 7B variant. Results decrease substantially when the weaker text-to-SQL model is used. As our approach is modular, we anticipate our results would improve with a better text-to-SQL parser.\nDisambiguation also improves coverage for unambiguous questions. While our approach does not specifically target unambiguous examples, in Table 4 we examine how different methods perform when the input question is unambiguous, i.e., when it has one interpretation and one SQL query. In this case, single and full coverage are the same and we simply show whether the gold SQL query was found. Note that we do not penalize additional queries in the answer.\nAs Table 4 shows, methods which disambiguate first, show better results by a wide margin (compared to end-to-end systems), with our proposed method achieving the best score of 77.9%. Explicit disambiguation serves as an intermediate representation of the question, thereby clarifying its meaning. Our results further demonstrate that generating a single interpretation is less challenging than handling multiple valid interpretations simultaneously (compare Table 1).\nThe infilling module is robust to different interpretation types. We now evaluate our approach when the infilling model is trained on Ambrosia. Specifically, we re-split Ambrosia to use 80% for training (1K ambiguous and 2K unambiguous examples), 10% for validation, and 10% for testing (131 ambiguous examples). All databases in the test set are unseen during training, similar to the original split. Note that in this setting the infilling model is trained on human-written interpretations which Ambrosia provides. As a result, Ambrosia becomes the in-domain test set, whereas AmbiQT represents out-of-domain evaluation.\nTable 5 shows results for an end-to-end fine-tuned text-to-SQL model, and three variants of the disambiguate first parse later framework: a model fine-tuned on gold (Ambrosia) interpretations, a prompt-based model that generates default interpretations without infilling, and the full model with infilling. The latter achieves the best single and full interpretation coverage on the out-of-domain AmbiQT test set and the highest single interpretation coverage on Ambrosia. However, the full interpretation coverage on Ambrosia is much lower than the end-to-end fine-tuned baseline. Manual examination of the predicted interpretations revealed that some correct interpretations are parsed incorrectly during the text-to-SQL stage, which relies on zero-shot prompting and may not capture dataset-specific conventions. This finding is supported by the results of the model fine-tuned on interpretations, which also performs significantly worse than end-to-end fine-tuning. Finally, we found that training on both datasets leads to results similar to the in-domain setting, see Appendix G for details."}, {"title": "Related Work", "content": "Ambiguity Resolution in NLP Numerous studies have focused on ambiguity in natural language tasks using strategies like generating multiple answers (Min et al., 2020), asking clarification questions (Lee et al., 2023; Zhang et al., 2024), and estimating uncertainty (Cole et al., 2023). Similar to our work, Sun et al. (2023) use iterative prompting to refine and generate alternative interpretations in the context of question answering, while Kim et al. (2024) first detect ambiguous questions and then resolve them through clarification requests.\nAmbiguity in Semantic Parsing Ambiguity has been studied across semantic parsing tasks, from code generation (Li et al., 2023; Mu et al., 2024) to A-calculus translation (Rasmussen and Schuler, 2020), and logical form prediction (Stengel-Eskin et al., 2024). In the domain of text-to-SQL parsing, recent work has emphasized the fact that benchmarks often overlook ambiguity by providing single interpretations (Floratou et al., 2024; Pourreza and Rafiei, 2023). Existing approaches focus on detecting column ambiguity through counterfactual examples (Wang et al., 2023), special-purpose decoding (Bhaskar et al., 2023), and resolving ambiguity through clarification questions (Dong et al., 2024). Our work uses explicit disambiguation before parsing and thus extends to different types of ambiguities, question styles, and database formats.\nIntermediate Representations in Text-to-SQL Intermediate representations are commonly used to bridge the gap between natural language and database queries. Several approaches decompose complex questions into a sequence of simpler operations expressed in natural language (Wolfson et al., 2022; Saparina and Osokin, 2021), modular execution plans (Eyal et al., 2023), or simplify the parsing task by augmenting questions with SQL keywords that mention necessary computation steps (Liu and Tan, 2024; Caferoglu and Ulusoy, 2024). Building on this work, we also use intermediate representations to make implicit information explicit but focus on resolving ambiguity.\nLearning to Correct LLM Outputs More recently, various approaches have been proposed to correct systematic biases in LLM outputs. For example, Ji et al. (2024) propose a model-agnostic module that learns correctional residuals between preferred and dispreferred outputs. Welleck et al. (2023) use a corrector model to iteratively review imperfect generations from a base model. Similarly, critique generators can be developed using reinforcement learning (Wadhwa et al., 2024a) or through fine-grained feedback (Wadhwa et al., 2024b). Such correction approaches are most effective when guided by external tools (Kamoi et al., 2024). We follow this paradigm using a specialized infilling model to correct systematic LLM biases towards certain interpretations and validate our output through SQL execution."}, {"title": "Conclusion", "content": "In this work, we present a novel approach for handling ambiguity in text-to-SQL semantic parsing. We first disambiguate questions by explicitly verbalising their interpretations and then leverage LLM capabilities to predict all valid SQL queries. A generator (LLM) provides an initial set of default interpretations, which are then augmented with a specialized infilling model. We propose a method for training this model based on automatic annotations obtained by comparing SQL query execution results rather than natural language interpretations.\nOur experimental results on AmbiQT and Ambrosia demonstrate the effectiveness of our approach. Our method achieves the highest single interpretation coverage on both datasets and maintains consistent full interpretation coverage in both in-domain and out-of-domain evaluation. However, generating all valid interpretations remains challenging. Future work could explore ways to improve ambiguity handling through better processing of database structure or using query execution as a signal for both training and test-time search."}, {"title": "Limitations", "content": "Our approach relies on reference interpretations and SQL queries for training the infilling model. While we show how to generate synthetic interpretations, scaling to new domains might be challenging. The sequential pipeline can propagate errors, with mistakes in disambiguation affecting text-to-SQL parsing and parser errors leading to incorrect SQL predictions. Duplicates in generated interpretations can help capture valid meanings through different wording but at the cost of longer output. The generator can produce incorrect interpretations that are currently not filtered. The infilling model can also miss valid interpretations that are under-represented in the training data.\nAdditionally, generating and parsing multiple interpretations requires more computation than single-stage approaches. Although our approach handles both ambiguous and unambiguous questions, it occasionally provides multiple interpretations for unambiguous requests. Future work should focus on improving precision through better filtering and interpretation validation."}, {"title": "AmbiQT and Ambrosia Examples", "content": "We provide samples from the AmbiQT and Ambrosia evaluation sets. AmbiQT is publicly available under the MIT license and Ambrosia is under the CC BY 4.0 license.\nAmbiQT contains four types of ambiguity: column and table ambiguities, join ambiguity, and ambiguity due to precomputed aggregates. Ambrosia covers three types of ambiguities: scope, attachment, and vagueness. Note that column and table ambiguities from AmbiQT correspond to vagueness in Ambrosia, although they differ in the number of gold queries provided (2 in AmbiQT versus 3 in Ambrosia)."}, {"title": "Prompt for Annotating AmbiQT with Interpretations", "content": "We use the following prompt with three in-context examples to generate natural language interpretations for ambiguous questions in AmbiQT:\nYour task is to rewrite the question using a given word or phrase.\nExamples:\nQuestion: Show titles of songs and names of singers.\nPlease rewrite using \"stage name\":\nGive me titles of songs and stage names of singers.\nQuestion: Show the name of the conductor that has conducted the most number of orchestras.\nPlease rewrite using \"director\":\nList the name of the director who has conducted the most number of orchestras.\nQuestion: Return the id of the document with the fewest paragraphs.\nPlease rewrite using \"passages\":\nWhat is the id of the document with the fewest passages?\nPlease provide rewritten question for the following instance. Do not add any explanation or description, output only the rewritten question.\nQuestion:\nPlease rewrite using"}, {"title": "Prompt for Default Interpretation Generation", "content": "The following prompt is used to generate default interpretations:\nYou are tasked with analyzing questions and providing their possible interpretations.\nThe questions are related to database queries and may be ambiguous or unambiguous.\nYour task:\nList every distinct way the question could be understood\nBe thorough and consider all possible meanings\nExplore different ways the question could be interpreted\nDon't limit yourself to obvious interpretations\nImportant:\nList each interpretation on a separate line\nDo not include explanations or reasoning\nFocus on semantically different interpretations\nBe specific and precise in wording\nGiven the following database context:\nProvide interpretations for this question:"}, {"title": "Interpretation Infilling Details", "content": "We use the following instructions for the model:\nThe task is to review the provided context, question, and existing interpretations, and determine if any additional interpretations are missing. If there are missing interpretations, list them on separate lines without explanations. If all interpretations have already been covered, simply state: \"All possible interpretations are covered.\"\nGiven the following context:\nQuestion:\nExisting interpretations:\nProvide any missing interpretations or confirm that all possible interpretations are covered.\nWe fine-tune the instruction-tuned Llama 3.1 8B model using LoRA (Hu et al., 2022) with rank 16 (\u03b1 = 16) and NEFTune (Jain et al., 2024) (noise \u03b1 = 5). The model is trained for 15 epochs using a cosine learning rate schedule with an initial learning rate of 5e-5, weight decay of 0.01, and a warmup ratio of 0.01. Training is performed on a single NVIDIA A100 GPU with a batch size of 8 and gradient clipping at 0.3. The total time for training and evaluation of one run is under 10 hours.\nWe sample 10% of the training data as development set and select the best-performing model from a single run for final evaluation.\nWe apply the same fine-tuning procedure to all comparison methods, i.e., end-to-end text-to-SQL fine-tuning and fine-tuning to predict interpretations. We observe that these methods tend to overfit with more epochs and thus reduce the number of training epochs to 5."}, {"title": "Text-to-SQL Parsing", "content": "To generate SQL queries for unambiguous questions (or interpretations), we use the following prompt across all text-to-SQL tasks, including AmbiQT annotation validation, evaluation of baseline models with interpretations, and our approach:\nThe task is to write SQL queries based on the provided questions in English. Questions can take the form of an instruction or command. Do not include any explanations, and do not select extra columns beyond those requested in the question.\nGiven the following SQLite database schema:"}, {"title": "Self-Correction Prompt", "content": "We compare our approach against a self-correction method where the LLM is prompted to review the generated interpretations (Appendix C), choose the valid ones, and add any missing interpretations. The prompt is shown below:\nThe task is to review the provided context, question, and candidate interpretations, and based on this information provide the interpretations that accurately reflect the meaning (or one of the possible meanings) of the question. If any of the candidate interpretations are correct, provide them as a list of interpretations. If there are missing interpretations, provide them as well. Avoid providing interpretations that are incorrect or duplicates. Do not provide any explanations.\nGiven the following context:\nQuestion:\nCandidate interpretations:\nProvide the interpretations that accurately reflect the meaning (or one of the possible meanings) of the question."}, {"title": "Training on AmbiQT and Ambrosia*", "content": "Table 6 provides results when the models are trained on the maximum available data: the AmbiQT train set and re-split Ambrosia* training set. As both test sets are in-domain, we would not expect to see any advantages from our method in terms of full coverage. Nevertheless, it still provides the best single-interpretation coverage. Overall, the results in Table 6 are very similar to those obtained previously in the in-domain setting."}]}