{"title": "DRIFT2MATRIX: KERNEL-INDUCED SELF REPRESENTATION FOR CONCEPT DRIFT ADAPTATION IN CO-EVOLVING TIME SERIES", "authors": ["Kunpeng Xu", "Lifei Chen", "Shengrui Wang"], "abstract": "In the realm of time series analysis, tackling the phenomenon of concept drift poses a significant challenge. Concept drift \u2013 characterized by the evolving statistical properties of time series data, affects the reliability and accuracy of conventional analysis models. This is particularly evident in co-evolving scenarios where interactions among variables are crucial. This paper presents Drift2Matrix, a novel framework that leverages kernel-induced self-representation for adaptive responses to concept drift in time series. Drift2Matrix employs a kernel-based learning mechanism to generate a representation matrix, encapsulating the inherent dynamics of co-evolving time series. This matrix serves as a key tool for identification and adaptation to concept drift by observing its temporal variations. Furthermore, Drift2Matrix effectively identifies prevailing patterns and offers insights into emerging trends through pattern evolution analysis. Our empirical evaluation of Drift2Matrix across various datasets demonstrates its effectiveness in handling the complexities of concept drift. This approach introduces a novel perspective in the theoretical domain of co-evolving time series analysis, enhancing adaptability and accuracy in the face of dynamic data environments.", "sections": [{"title": "1 INTRODUCTION", "content": "Co-evolving time series data analysis plays a crucial role in diverse sectors including finance, healthcare, and meteorology. Within these areas, multiple time series evolve simultaneously and interact with one another, forming complex, dynamic systems. The evolving statistical properties of such data present significant analytical challenges. A particularly pervasive issue is concept drift Lu et al. (2018b); Yu et al. (2024), which refers to shifts in the underlying data distribution over time, thereby undermining the effectiveness of static models. Miyaguchi & Kajino (2019); You et al. (2021).\nTraditional time series approaches commonly rely on the assumptions of stationarity and linear relationships. Methods such as ARIMA and VAR Box (2013), for instance, perform well in circumstances with stable and predictable dynamics. However, their effectiveness decreases when dealing with non-stationary data, particularly in the presence of concept drift. Conversely, machine learning methodologies Li et al. (2022); Wen et al. (2020), such as diverse neural network architectures Ho et al. (2022); Li et al. (2023); Yang et al. (2024), offer more flexibility but often require large amounts of data and face difficulties in terms of interpretability and adaptability, especially in dynamic contexts.\nThe evolving study has steered the field towards more adaptive and dynamic models. Methods like change point detection Deldari et al. (2021); Liu et al. (2023) and online learning algorithms Huang et al. (2022); Zhang et al. (2024) are designed to detect shifts in patterns. Nonetheless, these methods are typically restricted to detecting structural breaks or focusing on univariate series, rather than tracking and predicting subtle, ongoing changes in concepts, which limits their applicability in real-world co-evolving time series. In the complex environments, where multiple time series evolve and interact simultaneously, capturing the nonlinear relationships among variables is criticalMarcotte et al. (2023); Bayram et al. (2022). Despite recent advancements Matsubara & Sakurai (2019); Li et al. (2022); Wen et al. (2024), most multivariate models define the concept as a collective behavior"}, {"title": "2 THE LANDSCAPE OF CONCEPT DRIFT", "content": "Concept Drift. Concept drift in time series refers to the scenario where the statistical properties of the target variable, or the joint distribution of the input-output pairs, change over time. These drifts primarily exhibit in two manners Ren et al. (2018); Kim et al. (2021): the first is characterized by subtle, ongoing changes, reflecting the evolving dynamics of the time series, while the second arises from sudden shifts caused by structural breaks in the relationships among time series. Both gradual and abrupt changes can significantly disrupt model performance if not detected and adapted to in a timely manner, as they challenge the stability and accuracy of predictive models.\nChallenges in Co-evolving Scenarios. Recent advances in time series analysis have led to progress in addressing concept drift. Dish-TS Fan et al. (2023) offers a general approach for alleviating distribution shift in time series forecasting by normalizing model inputs and outputs to better handle distribution changes. Similarly, Cogra's application of the Sequential Mean Tracker (SMT) adjusts to changes in data distribution, improving forecast accuracy Miyaguchi & Kajino (2019). Despite these strides, these methodologies exhibit limitations when applied to co-evolving time series, where interdependencies between series introduce additional complexity. In such scenarios, a shift in one variable can propagate through the network of interrelations, affecting the entire system. DDG-DA Li et al. (2022) for data distribution generation has been adapted to better suit co-evolving scenarios, addressing the unique challenges presented by the interplay of multiple data streams under concept drift conditions. However, this method defines the concept as a collective behavior represented by co-evolving time series rather than capturing the dynamics of individual series and their interactions. Notably, even the most recent deep learning methods that mention concept drift, such as OneNet Wen et al. (2024) and FSNet Pham et al. (2022), primarily aim to mitigate the impact of concept drift on forecasting rather than addressing the challenges of adaptive concept identification and dynamic concept drift. They achieve this by incorporating an ensemble of models with diverse data biases or by refining network parameters for better adaptability. Due to the space limit, more related works about concept-drift, representation learning on times series and motivation are left in the Appendix A."}, {"title": "3 PRELIMINARIES", "content": "Problem Definition. Consider a co-evolving time series dataset $S = {S_1, S_2, ..., S_N} \\in R^{T\\times N}$, with N being the number of variables and T represents the total number of time steps. Our goal is to (1) to automatically identify a set of latent concepts $C = {C_1, C_2,..., C_k}$, where k represents the total number of distinct concepts; (2) to track the evolution and drift of these concepts across time; and (3) to predict future concepts.\nConcept. Throughout this paper, a concept is defined as the profile pattern of a cluster of similar subseries, observed within a specific segement/window. Here, the term \u201cprofile pattern\" refers to a subseries, the vector representation of which aligns with the centroid of similar subseries. We use a tunable hyp erparameter p, to differentiate profile patterns and modulate whether concept drift"}, {"title": "4 DRIFT2MATRIX", "content": "This section introduces the fundamental concepts and design philosophy of Drift2Matrix. Our objective is to identify significant concept trends and encapsulate them into a succinct yet powerful and adaptive representative model."}, {"title": "4.1 KERNEL-INDUCED REPRESENTATION LEARNING", "content": "To model concepts, we propose kernel-induced representation learning to cluster subseries retrieved using a sliding window technique. We begin with a simple case, where we treat the entire series as a single window. Given a collection of time series $S = (S_1, ..., S_N) \\in R^{T\\times N}$ as described in Eq. 1, its linear self-representation Z would make the inner product SZ come close to S. Nevertheless, the objective function in Eq. 1 may not efficiently handle nonlinear relationships inherent in time series. A solution involves employing \u201ckernel tricks\u201d to project the time series into a high-dimensional RKHS. Building upon this kernel mapping, we present a new kernel representation learning strategy, with the ensuing self-representation objective:\n$\\min \\frac{1}{2}||\\Phi(S) - \\Phi(S)Z||^2 = \\min \\frac{1}{2}Tr(K - \\alpha KZ + Z^TKZ)$, s.t. $Z = Z^T > 0$, $diag(Z) = 0$\nHere, the mapping function $\\Phi(\\cdot)$ needs not be explicitly identified and is typically replaced by a kernel K subject to $K = \\Phi(S)^T\\Phi(S)$. It's noteworthy that the parameter $\\alpha$ is key to preserving the local manifold structure of time series during this projection, further explained in Sec. 5.2.\nIdeally, we aspire to achieve the matrix Z having k block diagonals under some proper permutations if time series S contains k concepts. To this end, we add a regularization term to Z and define the kernel objective function as:\n$\\min \\frac{1}{2}Tr(K - \\alpha KZ + Z^TKZ) + \\frac{\\gamma}{2}||Z||_k$\ns.t. $Z = Z^T \\ge 0$, $diag(Z) = 0$"}, {"title": "4.2 ADAPTATION TO CONCEPT DRIFT", "content": "For b sliding windows ${W_1,...,W_b}$, Drift2Matrix constructs individual kernel representations for each window. Let's consider k distinct concepts identified across these windows, denoted as $C_{c\\in[1,k]} = {C_1,\u2026\u2026, C_k}$. It is important to know that the concepts identified from the subseries $S_p$ within the p-th (p \u2208 [1, b]) window may differ from those in other windows. This reveals the variety of concepts in time series and the demand for a dynamic representation.\nFor two consecutive windows $W_p$ and $W_{p+1}$, the effective probability of a suddenly switching in concept from $C_r$ to $C_m$ can be calculated for series $S_i$ as follows:\n$P(C_r \\rightarrow C_m|W_p \\rightarrow W_{p+1}, S_i) = \\frac{\\eta^{r,m}_{p,p+1} \\cdot A^{r,m}_{p,p+1}}{\\sum_{l=1}^{k} \\sum_{j=1}^{k} \\eta(C_l \\rightarrow C_j|T_r (S_i|W_p))}$ (4)\nwhere $s_1, s_2 \\in {1,\u2026\u2026,k}$ and\n$\\eta^{r,m}_{p,p+1}= \\frac{min{\\eta(C_r, W_l), \\eta(C_m, W_{l+1})}}{max{\\eta(C_r, W_l), \\eta(C_m, W_{l+1})}}$ (5)\nHere, the trajectory $T_r (S_i|W_p)$ represents the sequence of concepts exhibited by series $S_i$ over time. The term $\\eta (C_r \\rightarrow C_m|T_r (S_i|W_p))$ counts the occurrences of the sequence $C_r$, $C_m$ within this trajectory. $\\eta(C_r, W_i)$ (resp. $\\eta(C_m, W_{i+1})$) denotes the number of series exhibiting concept $C_r$ (resp. $C_m$) at window $W_i$ (resp. $W_{i+1}$).\nNotably, the component $m_{p,p+1}^{r,m}$ gauges the immediate risk of observing concept $C_m$ after the prior concept $C_r$. Meanwhile, $AD_{p,p+1}^{r,m}$ quantifies the likelihood of transitions between concepts within the entire dataset S. Consequently, Eq. 4 integrates both the immediate risk for a single series and the collective concept of series in S.\nIn the event that the exhibited concept of $S_i$ in $W_p$ is $C_r$ and the most probable concept switch goes to one of the concepts $C_m$, we can estimate the series value based on the previous realized value observed and the concept predicted. The predicted values of $S_i$ under the window $W_{p+1}$ can be calculated as:\n$Pre\\text{-}S_i = \\sum_{l=1}^{p} (\\mathcal{R}_m S_i, W_l) \\cdot T^{p-l+1} \\cdot \\{S_i|W_l\\}$ (6)"}, {"title": "4.3 INTEGRATION INTO DEEP LEARNING BACKBONES", "content": "One of the key strengths of Drift2Matrix is its flexibility, which allows it to be easily integrated into most modern deep learning backbones. Here, we take Autoencoder-Drift2Matrix (Auto-D2M) as an example, which comprises an Encoder, a Kernel Representation Layer, and a Decoder.\nEncoder: The encoder maps input S into a latent representation space. Specifically, the encoder performs a nonlinear transformation $H_e = Encoder_{\\Theta_e} (S)$, where $H_e$ represents the latent representations.\nKernel Representation Layer: Implemented as a fully connected layer without bias and non-linear activations, this layer captures intrinsic relationships among the latent representations and ensures that each latent representation can be expressed as a combination of others $\\Phi(H_e) = \\Phi(H_{\\Theta_e})\\Theta_s$, where $\\Theta_s \\in R^{n\\times n}$ is the self-representation coefficient matrix. Each column $\\theta_{s,i}$ of $\\Theta_s$ represents the weights used to reconstruct the i-th latent representation from all latent representations. To promote sparsity in $\\Theta_s$ and highlight the most significant relationships, we introduce an $l_1$ norm regularization: $L_{kernel}(\\Theta_s) = ||\\Theta_s||_1$.\nDecoder: The decoder reconstructs the input from the refined latent representations $\\widehat{S}_{ed} = Decoder_{\\Theta_d} (H_e)$, where $\\widehat{S}_{ed}$ represents the reconstructed time series segments.\nLoss Function: Training involves minimizing a loss function that combines reconstruction loss, self-representation regularization, and a temporal smoothness constraint:\n$L(\\Theta) = \\frac{1}{d}||S - \\widehat{S}_{ed}||^2 + \\lambda_1 ||\\Theta_s||_1 + \\lambda_2 ||\\Phi(H_{\\Theta_e}) - \\Phi(H_{\\Theta_e})\\Theta_s||^2$,\nwhere $\\Theta = {\\Theta_e, \\Theta_s, \\Theta_d}$ includes all learnable parameters, with $\\lambda_1, \\lambda_2$, and $\\lambda_3$ balancing the different loss components. Specifically, $\\lambda_1$ promotes sparsity in the self-representation $\\Theta_s$, and $\\lambda_2$ preserves the self-representation property."}, {"title": "5 THEORETICAL ANALYSIS", "content": ""}, {"title": "5.1 BEHAVIOR OF THE REPRESENTATION MATRIX", "content": "The core of Drift2Matrix is the kernel representation matrix Z, which encapsulates the relationships and concepts within time series. Without loss of generality, let $S = [S^{(1)}, S^{(2)}, \u2026\u2026\u2026, S^{(k)}]$ be ordered according to their concept. Ideally, we wish to obtain a representation Z such that each point is represented as a combination of points belonging to the same concept, i.e., $S^{(i)} = S^{(i)}Z^{(i)}$. In this case, Z in Eq. 1 has the k-block diagonal structure (up to permutations), i.e.,\n$Z = \\begin{bmatrix}\nZ^{(1)} & 0 & \\cdots & 0 \\\\\n0 & Z^{(2)} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & &  Z^{(k)}\n\\end{bmatrix}$ (7)\nThis representation reveals the underlying structure of S, with each block $Z^{(i)}$ in the diagonal representing a specific concept. k represents the number of blocks, which is directly associated with the number of distinct concept. Though we assume that $S = [S^{(1)}, S^{(2)}, ..., S^{(k)}]$ is ordered according to the true membership for the simplicity of discussion, the input matrix in Eq. 2 can be $S = SP$, where P can be any permutation matrix which reorders the columns of S."}, {"title": "5.2 KERNEL-INDUCED REPRESENTATION", "content": "The kernel-induced representation in Drift2Matrix is a key step that reveals nonlinear relationships among co-evolving time series in a high-dimensional space. This approach transforms complex, intertwined patterns in the original time series, which may not be discernible in low-dimensional spaces, into linearly separable entities in the transformed space. Essentially, it allows for a deeper and more nuanced understanding of the dynamics hidden within complex time series structures. Through kernel transformation, previously obscured correlations and patterns become discernible, enabling more precise and insightful analysis of co-evolving time series data.\nAdditionally, our kernel-induced representation not only projects the time series into a high-dimensional space but also preserves the local manifold structure of the time series in the original space. This preservation ensures that the intrinsic geometric and topological characteristics of the data are not lost during transformation. Our goal is to ensure that, once the time series are mapped into a higher-dimensional space, the integrity of the identified concepts remains consistent with the structure of the original space, without altering the distribution or shape of these concepts."}, {"title": "6 EXPERIMENTS", "content": "This section presents our experiments to evaluate the effectiveness of Drift2Matrix. The experiments were designed to answer the following questions:\n(Q1) Effectiveness: How well does Drift2Matrix identify and track concept drift?\n(Q2) Accuracy: How accurately does Drift2Matrix forecast future concept and series value?\n(Q3) Scalability: How does Drift2Matrix perform in online forecasting scenarios?"}, {"title": "6.1 DATA AND EXPERIMENTAL SETUP", "content": "The data utilized in our experiments consists of a synthetic dataset (SyD) constructed to allow the controllability of the structures/numbers of concepts and the availability of ground truth, as well as several real-life datasets: GoogleTrend Music Player dataset (MSP), Customer electricity load (ELD) data, Chlorine concentration data (CCD), Earthquake data (EQD), Electrooculography signal (EOG), Rock dataset (RDS), two financial datasets (Stock1 & Stock2), four ETT (ETTh1, ETTh2, ETTm1, ETTm2), Traffic and Weather datasets. In our kernel representation learning process, we chose the Gaussian kernel function. Detailed information about these datasets and the setup of the kernel function can be found in Appendix F. The source code is public to the research community\u00b3.\nIn the learning process, a fixed window slides over all the series and generates subseries under different windows. Then, we learn a kernel representation for the subseries in each window. Given our focus on identifying concepts and concept drift, varying window sizes actually demonstrate Drift2Matrix's ability to extend across multi-scale time series. Specifically, smaller window sizes represent a low-scale perspective, uncovering short-term subtle concept variations (fluctuations), while larger window sizes (high-scale) reflect overall concept trends (moderation). In this paper, we employ MDL (Minimum Description Length) techniques Rissanen (1998) on the segment-score obtained through our kernel-induced representation to determine a window size 4 that establishes highly similar or repetitive concepts across different windows (see Appendix D). Although adaptively determining a domain-agnostic window size forms a part of our work, to ensure fairness in comparison, all models, including Drift2Matrix, were evaluated using the same window size settings in our experiments."}, {"title": "6.2 Q1: EFFECTIVENESS", "content": "Drift2Matrix's forecasting effectiveness is evaluated through its ability to identify important concepts. Due to space limitations, here we only describe our results for the SyD and Stock1 datasets, the outputs with the other datasets are shown in Appendix H.2. Our method for automatically estimating"}, {"title": "6.3 Q2: ACCURACY", "content": "For real datasets, we lack the ground truth for validating the obtained concepts. Instead, we validate the value and gain of the discovered concepts for time series forecasting as they are employed in the forecasting formula Eq. 6. In this section, we evaluate the forecasting performance of the proposed model against seventeen different models, utilizing the Root Mean Square Error (RMSE) as an evaluative metric. Due to space limitations, we only present results for seven comparison models here; the complete experimental results can be found in the Appendix H.3. These seven models include four forecasting models (ARIMA Box (2013), KNNR Chen & Paschalidis (2019), INFORMER Zhou et al. (2021), and a ensemble model N-BEATS Oreshkin et al. (2019)), and three are concept-drift models (Cogra Miyaguchi & Kajino (2019), OneNet Wen et al. (2024) and OrBitMap Matsubara & Sakurai (2019)). For the existing methods, we use the codes released by the authors, and the details of the parameter settings can be found in Appendix G."}, {"title": "6.4 Q3: SCALABILITY", "content": "To further illustrate the predictive scal-ability of Drift2Matrix, we employed it for one of the most challenging tasks in time series analysis \u2013 i.e., online forecasting, leveraging the discovered con-cepts. For this task, our objective is to forecast upcoming unknown future events, at any given moment, while dis-carding redundant information. This ap-proach is inherently aligned with online learning paradigms, where the model continually learns and adapts to new data points, making it highly pertinent in the dynamic landscape of financial markets. We conducted tests on the Stock2 dataset."}, {"title": "7 CONCLUSION", "content": "In this work, we devised a principled method for identifying and modeling intricate, non-linear interactions within an ecosystem of multiple time series. The method enables us to predict both concept drift and future values of the series within this ecosystem. One noteworthy feature of the proposed method is its ability to identify and handle multiple time series dominated by concepts. This is accomplished by devising a kernel-induced representation learning, from which the time-varying kernel self-representation matrices and the block-diagonal property are utilized to determine concept drift. The proposed method adeptly reveals diverse concepts in the series under investigation without requiring prior knowledge. This work opens up avenues for further research into time series analysis, particularly regarding concept drift mechanisms in multi-series ecosystems.\nDespite its strengths, Drift2Matrix has a limitation when applied to time series with few variables. For example, converting a dataset with five variables into a 5x5 matrix makes block diagonal regularization less effective. Conversely, larger datasets, like those with 500 variables, benefit significantly from our method, enabling the identification of nonlinear relationships and concept drift. This characteristic is somewhat counterintuitive compared to most existing time series models that often focus on single or low-dimensional (few variables) time series forecasting, such as sensor data streams. Despite this limitation, we believe it underscores Drift2Matrix's unique appeal. It addresses a gap in handling concept drift in time series with a large number of variables, offering excellent interpretability and reduced computational complexity."}, {"title": "A EXTENDED RELATED WORK AND MOTIVATION", "content": "Concept drift models. Co-evolving time series analysis, by its nature, entails the simultaneous observation and interpretation of interdependent data streams Cavalcante et al. (2016); Xu et al. (2024f;c;b). This complexity is further heightened when concept drift is introduced into the model Webb et al. (2016). In such scenarios, a shift in one variable can propagate through the network of interrelations, affecting the entire co-evolving system. Matsubara et al. Matsubara & Sakurai (2016) put forth the RegimeCast model, which learns potential patterns within a designated time interval in a co-evolving environment and predicts the subsequent pattern most likely to emerge. While the approach can forecast following patterns, it is not designed to account for any interdependencies between them. In their subsequent work Matsubara & Sakurai (2019), the authors introduced the deterministic OrbitMap model to capture the temporal transitions across displayed concepts. Notably, this approach relies on pre-labeled concepts (known beforehand). DDG-DA Li et al. (2022) for data distribution generation has been adapted to better suit co-evolving scenarios, addressing the unique challenges presented by the interplay of multiple data streams under concept drift conditions. However, this method defines the concept as a collective behavior represented by co-evolving time series, rather than capturing the dynamics of individual series and their interactions. While acknowledging that deep learning has made significant advances in time series field, we must also note that most of these progress aims at improving accuracy. For example, OneNet Wen et al. (2024) addresses the concept drift problem by integrating an ensemble of models that share different data biases and learning to dynamically combine forecasts from these models for enhanced prediction. It maintains two forecasting models focusing on temporal correlation and cross-variable dependency, trained independently and dynamically adjusted during testing; FSNet Pham et al. (2022), on the other hand, is designed to quickly adapt to new or recurring patterns in non-stationary environments by enhancing a neural network backbone with two key components: an adapter for recent changes and an associative memory for recurrent patterns. Dish-TS Fan et al. (2023) offers a general approach for alleviating distribution shift in time series forecasting by normalizing model inputs and outputs to better handle distribution changes. Similarly, Cogra's application of the Sequential Mean Tracker (SMT) adjusts to changes in data distribution, improving forecast accuracy Miyaguchi & Kajino (2019).\nRepresentation Learning on TS. Representation learning on time series (TS) has gained significant attention due to its potential in uncovering underlying patterns and features essential for various downstream tasks. T-Rep Fraikin et al. (2023) leverages time-embeddings for time series representation. This method focuses on capturing temporal dependencies and variations through time-specific embeddings. TimesURL Liu & Chen (2024) employs self-supervised contrastive learning to create"}, {"title": "B ESTIMATING THE NUMBER OF CONCEPTS", "content": "In time series analysis, accurately identifying the number of concepts, such as periods of varying volatility in financial market, stages of a disease in medical monitoring, or climatic patterns in meteorology, is crucial. These concepts offer insights for data-driven decision-making, understanding underlying dynamics, and predicting future behaviors. While estimating the number of concepts is generally challenging, our kernel-induced representation learning approach offers a promising solution. Leveraging the block-diagonal structure of the self-representation matrix produced by our method, we can effectively estimate the number of concepts. According to the Laplacian matrix property Von Luxburg (2007), a strictly block-diagonal matrix Z allows us to determine the number of concepts k by first calculating the Laplacian matrix of Z ($L_Z$) and then counting the number of zero eigenvalues of $L_Z$. Although the dataset is not always clean or noise-free (as is often the case in practice), we propose an eigengap thresholding approach to estimating the number of concepts. This approach estimates the number of concepts k as:\n$k = \\arg \\min_i \\{i|g(\\sigma_i) \\le \\tau\\}_{i=1}^N$\nWhere 0 <\u03c4 < 1 is a parameter and g(\u00b7) is an exponential eigengap operator defined as:\n$g(\\sigma_i) = e^{\\lambda_{i+1}} - e^{\\lambda_{i}}$\nHere, $1\u2026\u03bb_N$ are the eigenvalues of $L_Z$ in increasing order. The eigengap, or the difference between the ith and (i+1)th eigenvalues, plays a crucial role. According to matrix perturbation theory Stewart (1990), a larger eigengap indicates a more stable subspace composed of the selected k eigenvectors. Thus, the number of concepts can be determined by identifying the first extreme value of the eigengap"}, {"title": "C PROOFS AND OPTIMIZATION", "content": ""}, {"title": "C.1 PERMUTATION INVARIANCE OF REPRESENTATION MATRIX IN SEQMATRIX", "content": "Theorem C.1 In Drift2Matrix, the representation matrix obtained for a permuted input data is equivalent to the permutation-transformed original representation matrix. Specifically, let Z be feasible to $\\Phi(S) = \\Phi(S)Z$, then $\\Tilde{Z} = P^TZP$ is feasible to $\\Phi(\\Tilde{S}) = P \\Phi(\\Tilde{S})\\Tilde{Z}$.\nProof C.2 Given a permutation matrix P, consider the self-representation matrix \u017e for the permuted data matrix SP. The objective for SP becomes:\n$\\min_{\\Tilde{Z}} \\frac{1}{2}||\\Phi(SP) - \\Phi(SP)\\Tilde{Z}||^2 + \\Omega(\\Tilde{Z})$, s.t. $\\Tilde{Z} = \\Tilde{Z}^T \\ge 0$, $diag(\\Tilde{Z}) = 0$\nBy the properties of kernel functions and permutation matrices, we have $\\Phi(SP) = \\Phi(SP)$. Substituting this into the objective function for Z, we have:\n$\\min_{\\Tilde{Z}} \\frac{1}{2}||\\Phi(SP) - \\Phi(S)P\\Tilde{Z}||^2 + \\Omega(\\Tilde{Z})$ s.t. $\\Tilde{Z} = \\Tilde{Z}^T \\ge 0$, $diag(\\Tilde{Z}) = 0$\nSince P is a permutation matrix, $PP^T = I$, the identity matrix. We apply the transformation $P\\Tilde{Z}P^T$ to the objective function:\n$\\min_{\\Tilde{Z}} \\frac{1}{2}||\\Phi(S) - \\Phi(S)P\\Tilde{Z}P^T||^2 + \\Omega(\\Tilde{Z})$ s.t. $\\Tilde{Z} = \\Tilde{Z} \\ge 0$, $diag(\\Tilde{Z}) = 0$"}, {"title": "C.2 MANIFOLD STRUCTURE PRESERVATION IN DRIFT2MATRIX", "content": "Theorem C.3 Drift2Matrix reveals nonlinear relationships among time series in a high-dimensional space while simultaneously preserving the local manifold structure of series.\nProof C.4 Optimization problem Eq. 3 can be converted to the form of a matrix trace:\n$\\min \\frac{1}{2}Tr(K - 2\\alpha KZ + Z^TKZ) + \\frac{\\gamma}{2}||Z||_k$\nIn the above, the negative term \u2013Tr(KZ) can be transformed into\n$\\min -Tr(KZ) = \\min \\sum_{i=1}^{N} \\sum_{j=1}^{N} -\\Phi(S_i)^T \\Phi(S_j) Z_{ij} =  \\min \\sum_{i=1}^{N} \\sum_{j=1}^{N} -K(S_i, S_j) Z_{ij}$\nwhere $K(S_i, S_j)$ indicates the similarity between $S_i$ and $S_j$ in kernel space. It can be seen from Eq. 14 that a large similarity (small distance) $K(S_i, S_j)$ tends to cause a large $Z_{ij}$, and vice versa. This is in fact an kernel extension of preserving local manifold structure in linear space, i.e., $\\min_z \\sum_{i=1}^{N} \\sum_{j=1}^{N} ||X_i \u2013 X_j||^2 Z_{ij}$. Suppose a small weight is given to this negative term, which means that the self-representation of the data will take into account the contribution of all other data. Conversely, it will only consider the contribution of other data that are nearest neighbours to the data, thus further enhancing the sparsity of the self-representation Z while maintaining the local manifold structure."}, {"title": "C.3 OPTIMIZATION OF NONCONVEX PROBLEM", "content": "The optimization problem of Eq. 3 can be solved by the Augmented Lagrange method with Alternating Direction Minimization strategy Lin et al. (2011). Normally, we require Z in Eq. 3 to be nonnegative and symmetric, which are necessary for defining the block diagonal regularizer. However, the restrictions on Z will limit its representation capability. Thus, we introducing an intermediate-term V and transform Eq. 3 to:\n$\\min_{\\substack{Z,V}} \\frac{1}{2}Tr(K - \\alpha KV) - \\frac{1}{2}Tr(KV) + \\frac{\\beta}{2}||V - Z||^2 + \\frac{\\gamma}{2}||Z||_k \\\\\n\\min_{\\substack{Z,V}} \\frac{\\alpha}{2} ||\\Phi(S) - \\Phi(S)V||^2 + \\frac{\\beta}{2}||V - Z||^2 + \\frac{\\gamma}{2}||Z||_k$\ns.t. $Z = Z^T \\ge 0$, diag(Z) = 0,1TZ = 1T\nThe above two models Eq. 3 and Eq. 15 are equivalent when \u03b2 > 0 is sufficiently large. As will be seen in optimization, another benefit of the relaxation term $||Z \u2013 V||^2$ is that it makes the objective function separable. More importantly, the subproblems for updating Z and V are strongly convex, making the final solutions unique and stable.\nConsider that $||Z||_k \\sum_{i=N-k+1}^{N} \\lambda_i(L_Z)$ is a nonconvex term. Drawing from the eigenvalue summation property presented in Dattorro (2010), we reformulate it as $\\sum_{i=N-k+1}^{N} \\lambda_i(L_Z) = \\min_W < L_Z, W >$, where $0 < W < I$, Tr(W) = k, see Appendix A for detail. So Eq. 15 is equivalent to\n$\\min_{\\substack{Z,V,W}} \\frac{\\alpha}{2} ||\\Phi(S) - \\Phi(S)V||^2 + \\frac{\\beta}{2}||V - Z||^2 \\\\\n+ \\gamma < Diag(Z_1) - Z, W >$\ns.t. $Z = Z^T \\ge 0$, diag(Z) = 0,0 < W < I, Tr(W) = k"}, {"title": "D DOMAIN AGNOSTIC WINDOW SIZE SELECTION", "content": "To segment time series", "as": "n$WS(w) = \\frac{1}{w} max \\{C_p, 1 < p \\le b\\}$ (24)\nHere, w is the window size, b is the number of non-overlapping segments, and $C_p$ is the number of concepts that can be discovered within window p using Eq. 8 and Eq. 9. This implies that max{$C_p$, 1 < p < b} corresponds to the maximum number of concepts observed in S. This score measures the concept-consistency \u2013 i.e., how the whole time series varies with various segmentation size. A small segmentation size w ("}]}