{"title": "Enhancing TinyML Security: Study of Adversarial Attack Transferability", "authors": ["Parin Shah", "Yuvaraj Govindarajulu", "Pavan Kulkarni", "Manojkumar Parmar"], "abstract": "The recent strides in artificial intelligence (AI) and machine learning (ML) have propelled the rise of TinyML, a paradigm enabling AI computations at the edge without dependence on cloud connections. While TinyML offers real-time data analysis and swift responses critical for diverse applications, its devices' intrinsic resource limitations expose them to security risks. This research delves into the adversarial vulnerabilities of AI models on resource-constrained embedded hardware, with a focus on Model Extraction and Evasion Attacks. Our findings reveal that adversarial attacks from powerful host machines could be transferred to smaller, less secure devices like ESP32 and Raspberry Pi. This illustrates that adversarial attacks could be extended to tiny devices, underscoring vulnerabilities, and emphasizing the necessity for reinforced security measures in TinyML deployments. This exploration enhances the comprehension of security challenges in TinyML and offers insights for safeguarding sensitive data and ensuring device dependability in AI-powered edge computing settings.", "sections": [{"title": "I. INTRODUCTION", "content": "Artificial intelligence (AI) simulates human intelligence in machines, enabling them to reason and acquire information similarly to people. This field of computer science devel- ops machines capable of tasks requiring human intelligence, like understanding language, recognizing images, and making quick decisions.\nTinyML, short for Tiny Machine Learning, is a subfield of AI specializing in deploying machine learning models on small, resource-constrained devices like microcontrollers and sensors. These devices face limitations in computing power, mem- ory, and energy, making traditional machine learning models challenging to run. TinyML overcomes these constraints by designing small, efficient models that operate on low-power devices.\nEmerging technology, TinyML [1] has the potential to revo- lutionize our interactions with the world. The development of small, low-power devices capable of advanced machine learning tasks like image and speech recognition opens doors in various industries, from healthcare to industrial automation. Applications include monitoring vital signs in remote loca- tions, detecting and responding to equipment malfunctions, and providing real-time feedback for personal fitness. How- ever, like any technology, TinyML poses security risks.\nDeploying TinyML systems in sensitive environments like healthcare and industrial control raises significant security con- cerns. These systems often handle sensitive data and control critical infrastructure. Additionally, their resource constraints and limited computational power make them vulnerable to attacks.\nThe rapid progress of AI has introduced remarkable break- throughs but it also raises concerns about adversarial attacks [2] [3]. These attacks manipulate AI systems through deceptive input, compromising decision-making. The paper [4] delves into the advancements in adversarial AI, shedding light on the vulnerability of AI models especially deep networks, to intentional input perturbations designed to exploit weaknesses in Al systems. These refer to malicious actions that exploit vulnerabilities in Al systems. Such attacks can manifest in various forms and can target different aspects of an AI system, including data, models, or infrastructure. To systematically identify potential security attacks and defense mechanisms for a given use case, threat model, which defines the capabilities and goals of the attacker under realistic assumptions, is required. Adversarial attacks are categorized into Black Box Attacks, Grey Box Attacks, and White Box Attacks, based on the extent of the attacker's knowledge regarding the targeted AI system's inner workings and components.\n1) Black Box Attack: Black Box AI attacks [4] refer to malicious actions or activities that exploit vulnerabili- ties in AI systems without requiring knowledge of the internal workings of the system. These types of attacks are particularly challenging to defend against because they do not rely on a known vulnerability or weakness in the system.\n2) Grey Box Attack: Grey Box attacks [5] involve ex- ploiting vulnerabilities in AI systems where the attacker has partial knowledge of the internal workings of the system. These types of attack can involve combining different pieces of information that the attacker may have access to, such as knowledge of the model architecture or training data. These attacks are different from black- box attacks, where the attacker has no information about the system.\n3) White Box Attack: In a white-box attack [6], the adver- sary has complete knowledge about the target model, including learned weights, parameters, and sometimes labeled training data. The common strategy involves modeling the distribution from the known weights to"}, {"title": "II. RELATED WORK", "content": "The authors [7] provides in overview of the current state of adversarial machine learning research. They review the progress made on adversarial attacks and defenses over the past decade and discuss the limitations of current defense techniques. They also present an overview of the most recent and promising research directions in this field, such as the development of more robust models, the use of adversarial training, and the use of more advanced methods for detecting and mitigating adversarial attacks.\nTinyML refers to the field of machine learning applied to small, low-power devices such as sensors, wearables and Inter- net of Things (IoT) devices [8]. Current progress in TinyML includes advances in techniques for compression, quantization, and energy-efficient computing to enable machine learning models to run on small devices. Research challenges include improving model accuracy, reducing power consumption, and addressing the limitations of current hardware. The future roadmap for TinyML includes continued research in these areas, as well as the development of new technologies and architectures to further enable the deployment of machine learning on small devices.\nThere is a growing body of research on benchmarking TinyML systems, with studies focusing on various aspects such as evaluation methods, datasets, and metrics. Benchmark- ing [9] these systems can be challenging due to the diversity of devices and use cases, as well as the lack of standardized evaluation methods and datasets. However, benchmarking is important for comparing the performance of different TinyML systems and for guiding the development of new ones. Some potential directions for benchmarking TinyML systems include developing standardized evaluation methods, creating bench- mark datasets that reflect real-world scenarios, and creating metrics that take into account not only accuracy but also factors such as power consumption and memory usage.\nIn paper [10], the authors explore the realm of Edge Al systems, examining challenges and presenting techniques to improve performance, energy efficiency, reliability, and security. They introduce a cross-layer framework integrating cutting-edge methods to enhance energy efficiency and robust- ness in Edge AI. Additionally, they discuss advancements and challenges in neuromorphic computing, particularly focusing on Spiking Neural Networks (SNNs). The work aims to provide researchers and practitioners with insights into the evolving landscape of Edge Al systems and the strategies driving their progress.\nThe aim of this research is to examine the vulnerabilities of Al models against adversarial attacks on resource-constrained"}, {"title": "III. METHODOLOGY", "content": "Architectural Details of Embedded Hardware Used in the Experiments\n1) Raspberry Pi: The Raspberry Pi 3B+ [11], as shown in Fig. 1 (a) stands out as a compelling platform for exploring and implementing AI applications. Its cost-effectiveness makes it an accessible gateway for individuals and organizations to experiment with AI, fostering innovation and development. The 64 bit quad-core processor and diverse I/O ports provide the necessary horsepower for tackling essential AI tasks, while compatibility with renowned frameworks like TensorFlow and PyTorch empowers users to leverage pre-trained models and development tools. This synergy between affordability, capability, and ease of use positions the Raspberry Pi 3B+ as a valuable asset for AI education, research, and hobbyist endeavors. As AI continues to mature and permeate our daily lives, the Raspberry Pi 3B+ is poised to remain a relevant and popular platform for those seeking to engage with this transformative technology.\n2) ESP32 Wroom Dev KitC: The ESP32 [12], illustrated in Fig. 1 (b), with its dual-core processor and rich wireless connectivity, carves a niche in the realm of edge AI. This versatile microcontroller boasts impressive processing power (240 MHz) to handle complex AI tasks efficiently. Built-in Wi-Fi, Bluetooth, and BLE unlock seamless communication within IoT networks, making it ideal for AI applications like smart home systems and sensor data analysis. Notably, its low power consumption makes it the perfect choice for battery-powered devices, enabling portable and sustainable AI solutions.\nEmpowering its AI capabilities, the ESP32 readily han- dles large datasets and is compatible with frameworks like TensorFlow Lite [13], allowing on-device AI computations without relying on the cloud. This combination of processing power, wireless connectivity, and efficient power management makes the ESP32 an adaptable tool for diverse AI projects, ranging from IoT applications and home automation to sensor networks. As edge AI continues to evolve, the ESP32 is poised to remain a prominent player, offering a robust and versatile platform for bringing intelligence to the very edge of computing."}, {"title": "B. Adversarial Attacks Employed for Subsequent Experiments", "content": "1) Model Extraction Attack: Model extraction attack [14] is a method of stealing the knowledge of the pre-trained model (targeted model) and transferring it to a surrogate model. It is achieved by carefully curating queries to exploit the targeted model in any one of the settings specified earlier. Since we artificially generate queries for the targeted model, the targeted model predicts the output corresponding to the curated input. Thus, we achieve a surrogate data set that can be used to train your custom model, which on completion of training mimics the functionality of the targeted model. Model extraction attack [15] starts out with the attacker querying the targeted model with a large number of inputs in order to get as much information about the model's functionality as possible. The attacker can get insights into the model's decision-making process, such as decision boundaries, feature importance, and model architecture, by analyzing the model's outputs.\nOnce the attacker accumulates enough knowledge about the targeted model, they can utilise it to train a substitute model that mimics the original model's behaviour. This substitute model can then be used to generate effective adversarial instances for attacking the original model. The model ex- traction attack has the advantage of not requiring access to the model's architecture or parameters, making it a successful attack even when the model's specifics are not publicly known. The effectiveness of this attack, however, is dependent on the quality and quantity of data collected by the attacker, as well as the capacity to train a substitute model that accurately matches the behaviour of the targeted model. Consider a trained model (f) or victim model (V) deployed on the device, an adversary can pass an attack vector as input (x) to obtain a prediction (y) on input feature vectors through the non- secure communication channel (NSComm) . The adversary will eventually be able to reconstruct a Learned Labeled Data Set (LLDS). The attacker can then train a replication model (fe) or surrogate model (S) on LLDS to approximate (f), without prior knowledge about its parameters.\n2) Evasion Attack: The Evasion attack [16] is a type of adversarial attack that seeks to manipulate a AI models by altering the input data so that the model miss-classifies it. In other words, the attacker creates a changed input, known as an adversarial example, that is identical to the original input but can lead to an inaccurate prediction by the model. The goal of evasion attack is to determine the smallest change to the original input that will result in a miss-classification. The attacker can accomplish this by maximizing the discrepancy between the predicted output of the original input and the intended output using optimization techniques. This difference is often evaluated with a loss function that measures the cost of miss-classification.\n3) Fast Gradient Sign Method \u2013 FGSM: Fast Gradient Sign Method [17] (FGSM) is a type of White Box Attack that aims to generate adversarial examples by computing the gradient of the loss function with respect to the input data and using it to generate perturbations to the input. This method performs by applying a small perturbation to each feature of the input data in the direction of the gradient of the loss function with respect to that feature. The size of the perturbation is controlled by a small value known as the attack's strength,"}, {"title": "C. Hardware Attacks and Security", "content": "Robust hardware security requires safeguarding embedded firmware, data, and overall system functionality. This becomes especially crucial when protecting sensitive information like cryptographic keys or personal data. Unhindered access to firmware poses a grave threat, allowing attackers to delve into the program and potentially uncover vulnerabilities, by- pass licensing, and override software restrictions. This breach could lead to replicating custom algorithms or deploying cloned hardware. Even in open-source environments, ensuring code authenticity is paramount to prevent malicious firmware insertion. Furthermore, denial-of-service (DoS) attacks [18], [19] pose a significant threat to crucial systems like environ- mental monitoring (e.g., gas, fire) and security apparatuses like intrusion detection alarms or surveillance cameras. While implementing stringent security measures can introduce com- plexity, it's essential to maintain the resilience and reliability of these systems, ensuring their uninterrupted and dependable operation. IoT, or smart devices, have increased the demand for security. Hackers find connected devices very attractive be- cause they can access them remotely. Protocol vulnerabilities offer an angle of attack through connectivity. The ever-evolving landscape of hardware security demands constant vigilance against a diverse range of attacks. Each category of attack presents unique challenges, necessitating the development of specialized defense mechanisms to effectively counter them\n\u2022 Software Attacks: Software attacks exploit vulnerabilities in the code, such as bugs or protocol weaknesses, and can be executed remotely without physical access to the device. These attacks, often leveraging untrusted pieces of code, pose a significant threat, with interception or usurpation of communication channels being common tactics. Software attacks, being widespread and relatively inexpensive, represent the majority of cases, emphasizing the need for robust defenses against code-based vulnera- bilities.\n\u2022 Hardware Attacks: Hardware attacks [20] require phys- ical access to the device, adding an additional layer of complexity to the threat landscape. The most apparent hardware attack involves exploiting the debug port, partic- ularly if it lacks adequate protection. However, hardware attacks, in general, are sophisticated endeavors that can incur substantial costs. These attacks require specialized materials and electronics engineering skills. They are further categorized into noninvasive attacks, conducted at the board or chip level without causing device destruction, and invasive attacks, which occur at the device-silicon level and often involve package destruction.\nThe security measures rely on hardware mechanisms ac- tivated based on careful configuration using option bytes or dynamic orchestration by hardware components. These safeguards are essential in various critical areas.\n\u2022 Memory Protection: It involves implementing measures to prevent unauthorized access, manipulation, or exploita- tion of crucial program instructions and data segments. This is achieved by employing hardware mechanisms such as Memory Management Units (MMUs) and access control lists, ensuring that only authorized entities can interact with specific areas of the system's memory.\n\u2022 Software Isolation: It focuses on strengthening the in- ternal architecture of the system to prevent potential breaches and maintain the integrity of individual pro- cesses.\n\u2022 Interface Protection: It aims to mitigate the risks asso- ciated with external attacks that exploit vulnerable entry points to gain unauthorized access to the device. This"}, {"title": "IV. EXPERIMENTAL SETUP AND ATTACK PREPARATION", "content": "In this section, we present a comprehensive overview of the experimental results, focusing on the evaluation of AI models' performance on the host machine\u2014an x64-based processor system with 32 GB RAM and a 6 GB NVIDIA RTx A3000 GPU. The experiments involved executing adversarial attacks on two publicly available datasets: MNIST [24], an image classification dataset, and Gesture Recognition, which serves as an illustration of a time series dataset. The MNIST dataset comprises handwritten digits ranging from 0 to 9, while the Gesture Recognition [25] dataset consists of three distinct classes: Ring, Wing, and Slope. Our objective was to assess the susceptibility of the AI models to malicious attacks by conducting two types of attacks: Model Extraction attack and Model Evasion attack. The outcomes of these attacks are elaborated upon in subsequent sections, providing a detailed account of the results."}, {"title": "A. Results for MNIST Dataset", "content": "In this section, we present the experimental results derived from our analysis of the MNIST dataset, focusing on the outcomes of model extraction and evasion attacks. Simul- taneously, we assess the real-world performance of our Al model by deploying them on the the resource-constrained tiny intelligence RPi. Our experiment encompasses an in-depth examination of model accuracy, susceptibility to adversarial attacks, and their implications on RPi. This section offers comprehensive understanding of adversarial attack delivering an in-depth discussion of our observations and findings.\n1) Model Extraction Attack: In this experiment, a Con- volutional Neural Network (CNN) trained on the MNIST dataset, referred to as our victim model (V), was utilized, achieving a 99.3% accuracy. For the model extraction attack, attack queries were generated. These attack queries are passed through V to generate labels, which are then utilized to train the surrogate model (S) model, mimicking the behavior of V. Impressively, despite having only 14K parameters, S achieved a notable 90% accuracy.\nTable II provides a summary of the outcomes from the model extraction attack, presenting a comparison between the accuracy and model size of the V and the S.\n2) Evasion Attack: After executing the model extraction attack, we proceeded to conduct an evasion attack on V using the FGSM. In this attack, we crafted attack vectors by introducing imperceptible perturbations to the input dataset. These subtle alterations, invisible to the human eye, have the potential to induce misclassification by the model. The magnitude of these perturbations, referred to as attack strength, directly impacts the extent to which the model's robustness is compromised. generated by following formula:\n$advx = x + e * sign(\\nabla xJ(0,x, y)$\nwhere\n1) x = Original Input Data\n2) y = Labels corresponding to Input Data (x)\n3) 0 = Original Model\n4) \u20ac = Scalar value to control the magnitude of perturbation\n5) J(0),x,y) = Loss Function of the Model\n6) $\\Delta x J(0),x,y)$ = Gradient of the loss function with respect to input (x)\n7) $\\nabla xJ(0),x,y)$ = Sign of the Gradient, which is either -1 or 1)\nNotably, with (\u20ac) = 0.31, we flipped 67.9% of the original class. This"}, {"title": "B. Results of Gesture Recognition Dataset", "content": "In this section, we will detail the outcomes of our experi- ments conducted on the Gesture Recognition Dataset, encom- passing three distinct classes: Ring, Wing, and Slope. As a component of the experiment, we executed two adversarial attacks, namely model extraction and evasion. Subsequent to the successful completion of these experiments on the host system, we proceeded to deploy the model on ESP32, aiming to assess the vulnerabilities of the AI model on the resource-constrained platform. This holds particular signifi- cance, considering the prevalent use of AI models on devices with limited resources, necessitating their resilience against potential adversarial attacks. This section provides a thorough comprehension of adversarial attacks, presenting an in-depth discussion of our observations and findings.\n1) Model Extraction: A Convolutional Neural Network (CNN)-based architecture was selected as the V, attaining an accuracy of 92.37%. For the execution of the model extraction attack, we generated a set of attack queries directed to the V to acquire its predictions. By leveraging these queries and the responses from V, we successfully trained a new S capable of replicating V's behavior. Remarkably, despite possessing a mere 38K parameters, the S attained an accuracy of 69.23 %. Table III offers a comprehensive overview of the model extraction attack's results, presenting a comparative analysis of both V and S in terms of their accuracy and size.\n2) Evasion Attack: In this section, we delve into the execution of an evasion attack on the V model following the model extraction phase, utilizing the FGSM. Throughout the evasion attack, we crafted attack vectors by introducing imperceptible perturbations to the input dataset-undetectable to the human eye yet capable of inducing misclassifications by the model. The magnitude of these perturbations, referred to as Attack Strength, governs the extent to which the model's robustness is compromised. To measure the effectiveness of our evasion attack, we evaluated variations in the model's classification accuracy on the perturbed dataset. With a rise in Attack Strength, there is a corresponding augmentation in the percentage of misclassified"}, {"title": "V. EXPERIMENTAL RESULTS ON RPI & ESP32", "content": "Subsequent to our experiments on the host device and the ensuing outcomes, our experiment extended to evaluate the performance of the models across diverse hardware platforms. Specifically, deploying the models on resource-constrained devices such as the RPi and ESP32. The primary objective was to scrutinize the models' vulnerability to adversarial attacks concerning the hardware platform of deployment. Our thorough analysis revealed a consistent accuracy across vari- ous hardware platforms, with no noticeable deviation. Upon meticulous examination of the experimental outcomes, we concluded that the susceptibility of AI models to adversarial attacks is not limited to any specific hardware platform."}, {"title": "A. Implementing MNIST Dataset on RPi", "content": "In this section, our primary objective is to conduct an ex- haustive analysis of the susceptibility exhibited by AI models deployed on the RPi, coupled with a meticulous summarization of the observed performance metrics. To achieve this, we sys- tematically executed methodical experiments involving both model extraction and evasion attacks, employing the MNIST Dataset as our experimental foundation.\n1) Model Extraction: After the successful execution of the model extraction attack on the host device, our aim was to deploy the obtained model on RPi platforms. The objective was to specifically evaluate the model's susceptibility on the resource constrained platform. Surprisingly, our observations revealed that the performance of the extracted model did not experience significant degradation in comparison to its performance on the host device. The model maintained a high accuracy of 90 %, aligning with the accuracy achieved on the host device. This result highlights the importance of evaluating the secu- rity measures and robustness of models, even when deployed on resource-constrained hardware devices like the RPi.\n2) Evasion Attack: In this section, our focus shifts to eval- uating the model's robustness through evasion attacks targeted at the V. The primary objective is to assess vulnerabilities on the RPi platform. The outcomes and insights derived from these experiments are systematically presented and detailed"}, {"title": "B. Implementing Gesture Recognition on ESP32", "content": "In this section, our emphasis is on an in-depth analysis into the susceptibility of AI models deployed on the ESP32 platform, coupled with a detailed summary of the quantitative performance metrics observed. Our experimental methodology involved the systematic execution of model extraction and evasion attack experiments, with an emphasis on a Gesture Recognition Dataset. The objective was to assess the ESP32's computational robustness in precisely recognizing and clas- sifying gestures, particularly when subjected to adversarial perturbations.\n1) Model Extraction Attack: As we observed that on the host platform, we successfully replicated the functionalities of the original model with an accuracy of 69.23%. Subsequently, we compiled the model into the TensorFlow Lite (TFLite) format and further transformed it into Bytes format to facil- itate its deployment onto the ESP32 device. This conversion ensured compatibility and optimization for the execution of our AI model on the ESP32. Upon conducting inference on the ESP32 using the converted Bytes model, a noteworthy observation was made: the model's accuracy remained con- sistent, maintaining its previous accuracy level of 69.23%. This signifies that the deployment procedure, encompassing the conversion of the model into a compatible format for the ESP32, did not introduce adverse effects on its performance. However, it is important to acknowledge that the model also remains susceptible to adversarial attacks.\n2) Evasion Attack: In this section, we'll evaluate the model's robustness in the evasion attack on the ESP32 plat- form following the conclusion of the model extraction phase. Our primary objective was to assess the robustness and sus- ceptibility of the V to evasion attacks when deployed on the ESP32. The outcomes and significant findings derived from these experiments have been summarized and presented"}, {"title": "VI. POTENTIAL DEFENSE", "content": "From the above experiments, we saw that how attacks crafted on a powerful host machine can be transferred seam- lessly to a non-secure Tiny devices. Al defense mechanisms are critical components in safeguarding AI models deployed across various systems and devices. These defenses encom- pass a range of techniques and strategies designed to pro- tect AI models from adversarial attacks, data breaches, and unauthorized access. Simple hardware defenses involve basic measures such as encryption, access controls, and secure boot mechanisms. While these methods provide a foundational level of security, they may not be sufficient for protecting AI models deployed on resource-constrained devices. Complex hardware defense methods, such as secure communication pro- tocols and hardware-based intrusion detection systems, offer more advanced protection against security threats. However, implementing these methods on low-power devices presents significant challenges. Low-power devices prioritize energy efficiency and may not have the necessary hardware compo- nents or processing capabilities to support complex security protocols. Resource-constrained devices, such as IoT sensors, edge computing devices, and embedded systems, often have limited processing power and memory. As a result, they may lack the computational resources needed to implement robust security measures at the hardware level."}, {"title": "VII. CONCLUSION", "content": "Our experimentation reveals that the vulnerability of AI models to adversarial attacks remains consistent across var- ious deployment scenarios, whether on cloud-based servers or edge devices. Factors like hardware type or model size have minimal impact on this risk. Safeguarding AI models on edge devices is paramount as edge computing gains promi- nence. Prioritizing defense against adversarial threats ensures the reliability, security, and trustworthiness of Al-powered systems, regardless of industry or organization. As part of our future work, we intend to conduct more extensive experi- ments on a broader range of low-power, resource-constrained devices. This comprehensive approach will provide us with the opportunity to rigorously evaluate the effectiveness of diverse defense mechanisms against adversarial attacks."}]}