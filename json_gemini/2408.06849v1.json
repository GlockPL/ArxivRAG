{"title": "Causal Agent based on Large Language Model", "authors": ["Kairong Han", "Kun Kuang", "Ziyu Zhao", "Junjian Ye", "Fei Wu"], "abstract": "Large language models (LLMs) have achieved significant success across various domains. However, the inherent complexity of causal problems and causal theory poses challenges in accurately describing them in natural language, making it difficult for LLMs to comprehend and use them effectively. Causal methods are not easily conveyed through natural language, which hinders LLMs' ability to apply them accurately. Additionally, causal datasets are typically tabular, while LLMs excel in handling natural language data, creating a structural mismatch that impedes effective reasoning with tabular data. This lack of causal reasoning capability limits the development of LLMs. To address these challenges, we have equipped the LLM with causal tools within an agent framework, named the Causal Agent, enabling it to tackle causal problems. The causal agent comprises tools, memory, and reasoning modules. In the tools module, the causal agent applies causal methods to align tabular data with natural language. In the reasoning module, the causal agent employs the ReAct framework to perform reasoning through multiple iterations with the tools. In the memory module, the causal agent maintains a dictionary instance where the keys are unique names and the values are causal graphs. To verify the causal ability of the causal agent, we established a benchmark consisting of four levels of causal problems: variable level, edge level, causal graph level, and causal effect level. We generated a test dataset of 1.3K using ChatGPT-3.5 for these four levels of issues and tested the causal agent on the datasets. Our methodology demonstrates remarkable efficacy on the four-level causal problems, with accuracy rates all above 80%. For further insights and implementation details, our code is accessible via the GitHub repository https://github.com/Kairong-Han/Causal_Agent.", "sections": [{"title": "1. Introduction", "content": "In recent years, generative artificial intelligence technology has gained significant success, achieving remarkable behavior in the natural language processing field [1], image, audio synthesis, etc [84]. This advancement lays the foundation for propelling research in general artificial intelligence [85], both in terms of framework development and practical implementation. However, due to the complexity of causal problems, the causal reasoning capabilities of the LLM remain insufficient. Causal theory is difficult to describe in natural language that the LLM can understand accurately. Researchers have evaluated the pure causal reasoning abilities of the LLM and found that their pure causal reasoning is close to random [16]. Additionally, researchers believe that the current LLM are merely \"causal parrots\" that mimic without truly possessing causal understanding [19]. This inherent limitation severely hampers the performance of large models in tasks requiring causal reasoning. Moreover, causal datasets are typically tabular data, while large models excel in handling natural language data. When we need to draw causal conclusions based on the analysis of tabular data, LLMs that are not specifically designed cannot directly utilize tabular data and perform reasoning. This structural heterogeneity hinders LLM from effectively reasoning with tabular data. These two limitations restrict the ability of LLMs to solve causal problems effectively."}, {"title": "2. Related Work", "content": "2.1. Causality\nCausality, as a tool for data analysis, aims to accurately identify and quantify the actual effects of specific factors (causes) on outcome variables (effects) in a complex system environment [30]. It is everywhere in our daily lives. Such as statistics [3-5], economics [5], computer science [6,7], epidemiology [8,9] and psychology [10]. Different from correla-tion, causality explores in depth the changing pattern of how the result variable responds when the cause variable changes. Therefore, the \"Ladder of Causality\" theory proposed by Pearl divides causality into three progressive levels [11]: association, intervention, and counterfactual. The association focuses on discovering the correlation between variables through observation of data. However, this can only reveal the accompanying phenom-ena between events, and cannot indicate the causal flow between events; Intervention emphasizes when we actively change the state of an event, whether and how other related events will change accordingly; Counterfactual imagines how the current observed results would have changed if there had not been an event that had occurred. The core purpose of studying causality is to reveal the true causal chain between things and to abandon those confusing pseudo-causal relationships. Cause field problems can be briefly divided into two broad directions: causal discovery and causal inference. Causal discovery is based on directed acyclic graphs and Bayesian models, focusing on obtaining causal relationships from observation data, and methods can be divided into constraint-based methods [65-67], such as IC, PC, FCI, and function-based methods [68-70] such as LiNGAM and ANM, and hybrid methods [58] to combine the advantages of the above two methods. Common frameworks for causal inference are structural causal model[13] and potential outcome framework [12]. The potential outcome framework is also known as the Neyman-Rubin\n2.2. LLM-based agent\nAutonomous agents have long been considered a promising approach to achieving ar-tificial general intelligence (AGI), which accomplishes tasks through autonomous planning and action [73]. In previous studies, simple and heuristic policy functions were designed for agents to learn in isolated and constrained environments [74,75]. In recent years, LLM has achieved great success in the field of natural language. Human-like intelligence has shown great potential [2,35,36] and there has been a large amount of research using LLM as the decision-making and reasoning center of agents [37-39], achieving great success in natural sciences [40,41], engineering sciences [29,42,43], and human simulation [27,44]. The LLM agent is composed of four parts, namely profile module, action module, plan module, and action module. The identity module assigns an imaginary role to the agent, such as a teacher or poet. According to different text sources. The planning module helps the agent use thinking chains to break down tasks and use different search methods to obtain solutions in the problem space, such as CoT [32], ToT [33], AoT [34], Reflexion [35], etc. The memory module is subdivided into two categories: short-term memory and long-term memory. Its specific implementation forms are diverse, depending on the data structure and technical means used. The action module is the key for the intelligent agent to take specific actions in the physical or virtual environment. The agent implements actions by using tools to change the environmental state and task process and also triggers changes in its state.\n2.3. Combining LLM and causality\nSince the advent of LLM, some researchers have evaluated and analyzed the causality ability of LLM. Jin et al. [16] introduced a new task CORR2CAUSE, which can infer causal relationships from correlations, to evaluate the causal inference ability of large models. This task first constructs a causal graph based on the original data and then converts it into natural language by the D-separation principle. From the experimental results, it is generally believed that the LLM with a higher version or better reasoning ability does not show positive correlation results in the causal inference task, and the performance of the LLM in the causal inference task is akin to random. Jin et al. [18] further investigated whether large language models can reason about causality and proposed a new NLP task: causal inference in natural language. Inspired by the \"causal inference engine\" and hypotheses proposed by Judea Pearl. They built a large dataset, CLADDER, with 10K samples: a collection (association, intervention, and counterfactual) based on causal graphs and queries. In addition, they introduce and evaluate a customized chain of thought prompting strategy CausalCOT. Gao et al. [17] presents a comprehensive evaluation of ChatGPT's causal reasoning capabilities. They found that ChatGPT is not a good causal reasoner, but is good at causal explanation and that ChatGPT has a serious problem of causal illusion, which is further exacerbated by In-Context Learning (ICL) and chain of thought techniques. Ze\u010devi\u0107 et al. [19] argues that large language models cannot be causal and define a new subgroup of structure causal models, called meta-SCMs. Their empirical analysis provides favorable evidence that current LLMs are even weak \"causal parrots\". Long et al. [46] further investigated how imperfect expert knowledge can be used to improve the output of causal discovery algorithms. A greedy algorithm is also proposed to iteratively reject graphs from MEC while controlling the probability of excluding true graphs. They found a reduction in performance when using large models as experts. Nonetheless, their results still suggest a clear potential for LLM to help discover causal relationships. K\u0131c\u0131man et al. [47] found LLM can achieve competitive performance in determining pairwise causality, with an accuracy of up to 97%, but their performance"}, {"title": "3. Materials and Methods", "content": "3.1. Modeling causal problems from the perspective of LLM\nDespite the development of LLM, like ChatGPT, demonstrating strong natural lan-guage understanding and question-answering capabilities, there is still a significant gap between the problem paradigms that data-driven causal focuses on tabular data but LLM focuses on the field of natural language processing. Furthermore, LLMs struggle to truly understand and handle the intricate causal relationships inherent in complex data. The in-ability of LLMs to adapt to causal tasks remains a significant limitation in their development and widespread reliable use.\nTherefore, it is meaningful to re-establish a causal problem framework from the perspective of the LLM. This has a significant impact on evaluating the causal ability of LLMs and enhancing their causal ability. To model causal problems within the field of LLM, We formulate our settings as follows:\nLet $T \\in \\mathbb{R}^{n\\times c}$be a tabular data with $n$ rows and $c$ columns, in which each row $t_i$ is a piece of data, and each column $c_i$ represents a variable. So\n$T = \\{t_i\\}_{i=0}^{N}$\nWe formalize the causal problem space as a $Q$ and $q_i \\in Q$ is one question in the form of natural language. We combine the tabular data and the problem description by Cartesian product to create the dataset $D$ and each item $d_i \\in \\mathbb{R}^{n\\times c} \\times Q$.So\n$D = \\{d_i\\} = \\{(T_i,q_i) \\in \\mathbb{R}^{n\\times c} \\times Q\\}$\nThe user inputs a pair of $(T_i, q_i)$ samples from $D$, and then the causal agent analyses the tabular data $T_i$ and the causal problem $Q$ to generate a reasonable answer $A$. The format of answer $A$ is not limited to the form of natural language. A can also be a causal graph or other heterogeneous non-textual data to explain the question clearly.\n3.1.1. Variable level\nAt the variable level, we focus on determining the correlation between different variables, which is the first level of the causal ladder. To obtain correlation from tabular data, we transform the problem of correlation testing into independence testing. That is given variables $V_i$ and $V_j$, determining whether they are independent or conditional Independence under variables $\\{V_k\\}_{k=1}^{N}$. If two variables are correlated, they are statistically dependent, and vice versa. Through such modeling methods, we aim to test the causal agent with the ability to analyze correlations. Specifically, we divide the problem of correlation into two subclasses: direct independence testing and conditional independence testing. The difference between them lies in whether condition variables are given when judging independence. In particular, direct independence testing can be regarded as the number of condition variables is zero. To more finely measure the model's capabilities, we further divide conditional independence testing into independence testing under a single condition and independence testing under multiple numbers of conditions, in which the difference is whether the number of conditions is one or beyond one.\n3.1.2. Causal graph level\nAt the causal graph level, the focus is more macroscopic, examining whether the causal agent possesses the capability to generate a causal graph. The causal graph is a directed acyclic graph (DAG), and in DAG the direction of edges represents causal relationships. In this article, we choose the PC algorithm [57] as the method for generating causal graphs, which generates Markov equivalence classes of causal graphs without considering the presence and influence of unobserved variables. Modeling the capabilities of intelligence at the level of the causal graph involves two categories: generating a causal graph that includes all variables in tabular data, and the other generating a partial causal graph that includes only a subset of variables in tabular data. The capability to generate causal graphs and to reason on these graphs can effectively guide the agent to understand causal relationships and discern true causal connections amidst the fog of spurious correlations.\n3.1.3. Edge level\nAt the edge level modeling, we still consider the relationships between variables. Instead of the associations from a statistical correlation, we focus on the deeper causal relationships between variables from a causal viewpoint. Unlike quantitative estimation of causal effects, edge level modeling provides qualitative analysis results that need to reflect the true relationships of the edges in the causal graph reconstructed from tabular data. We consider the following three types of relationships: direct causal relationship, collider relationship, and confound relationship. As discussed in Section 3.1.2, We used the PC algorithm to generate Markov equivalence classes for causal graphs, therefore we formalize three types of relationships as follows:\nDenote $G$ as a Markov equivalence class generated by the PC algorithm from tabular data, containing edges set $\\{< V_i, V_j >\\}$. And edge $< V_i, V_j > \\in \\{\\rightarrow, - \\}$.\n3.1.4. Causal effect level\nThe causal effect level attempts to quantify how the outcome for an individual or system would differ if it experienced a certain intervention. Ideally, this requires controlling confounders to ensure an accurate assessment of the intervention's effect. Thus, randomized controlled trial (RCT) is the golden standard for estimating causal effects. However, in practical production scenarios, ethical constraints or high experimental costs often make it difficult to obtain results from RCT. Additionally, the sample distribution in RCT may not represent the overall population distribution due to limited sample sizes. To address the limitations of sample size and distribution bias and to balance covariates and confounding factors when estimating causal effects, researchers have proposed numerous methods based on observational data, such as IPSW [78], etc.\nWe expect the causal agent not only to utilize causal explanations for qualitative analysis but also to employ classical causal inference for quantitative interpretation. To simplify the problem, at the level of causal effects, we only consider the quantitative calculation of the ATE, denoted as $E(Y(T = t_1) \u2013 Y(T = t_0))$, from tabular data. Modeling at the granularity of causal effects can equip the causal agent with a more fine-grained causal perception capability.\n3.2. Causal Agent Framework Based on LLM\nBased on the causal modeling methods mentioned in Section 3.1, we have specifically implemented causal agents with causal reasoning capabilities for different modeling gran-ularities. Our causal agent framework consists of three modules: tools module, memory module, and plan module. In terms of tools, to align the tabular data with natural language, we invoke causal analysis tools that can accept tabular data as input. For the output of tools, we use prompts to interpret and explain the results, enabling the causal agent to understand the output. In the planning aspect, inspired by the ReAct framework [63], through multiple rounds of reflection, we continuously invoke causal analysis tools and reflect on their output, considering whether we can derive the answer to the original question based on the agent's understanding of the causal question. If the answer to the question cannot be derived, we continue to iterate and reflect until we reach the final answer or limited iteration times. Besides, to better understand tools' usage, we use in-context learning and one-shot examples to empower the causal agent. A manual handwritten example is designed to use all tools to guide the causal agent in invoking and understanding the tool. In terms of memory, we store the output of the causal analysis tools in a dictionary in memory as short-term memory, ensuring that the agent can continuously access the causal graph before the final answer is obtained.\n3.2.1. Tools\nThe causal agent invokes causal analysis tools to analyze tabular data, thereby compen-sating for the LLM's shortcomings in handling tabular data. This approach aligns tabular data with causal conclusions and enhances the LLM's causal capabilities through tool invocation. Specifically, our causal analysis tools select the library causal-learn for causal discovery and EconML for causal inference. Starting from the perspective of modeling causal problems for the LLM, we have designed specific tool functions at the variable level, edge level, causal graph level, and causal effect level. To make the tool functions easily invoked by the LLM, we have re-encapsulated the interfaces, changing the tool inputs to JSON string format, and using manual rules and handwritten prompt templates to help the\n3.2.2. Plan process\nInspired by the ReAct framework, the causal agent adopts an iterative multi-turn dialogue approach, using prompt templates to facilitate interaction and understanding between the causal agent and the tools. During the process of invoking causal analysis tools, the agent must continuously think and experiment to determine the next action. This process is crucial for the causal agent, as solving complex causal problems may require the use of causal tools more than one, as well as the observation and integration of different outputs from these tools. Therefore, the causal agent needs to repeatedly invoke these tools, observe their output, and engage in deep thinking to arrive at the solution.\nSpecifically, in each round of the dialogue, the user provides a problem description q and the table data $T$ to be analyzed as input $(q, T)$. Guided by the prompt, from the first round of each round $i$, the causal agent will generate a $Thought$ $O$ and use a tool as $action$ $a_i$. The tool will accept $a_i$ and generate observation $Obs$. Based on this observation, the causal agent attempts to answer the original question description. If it is still unable to answer, it concatenates this round's observation as input to continue the next round. The input for the next round is $(q, T, O_1, a_1, Oobs, O_2, a_2, Oobs,..., Oobs)$, until the causal agent comes up with the final answer or reaches the maximum times of iterations.\n3.2.3. Memory\nCurrently, mainstream memory mechanisms in LLM-based agents are primarily imple-mented in two forms: textual form and parametric form Zhang et al. [83]. Although most current memory mechanisms tend to use the textual form, parametric memory, as an emerg-ing area of exploration, has unique application potential. Each form has its advantages and disadvantages, suitable for different application scenarios. The memory operations of an agent include three key stages: memory writing, memory management, and memory reading. These three operations interact in the agent's interaction with the environment, collectively enhancing the agent's interactive capabilities.\nIn this paper, the causal agent considers only short-term memory. Specifically, during the ReAct reasoning and interaction process of the causal agent, it needs to maintain the currently generated causal graph and use this graph in subsequent causal relationship judgments. Therefore, in the implementation of the causal agent's memory, the memory is not a textual form of data but the data corresponding to the causal graph Python class instance. The causal agent maintains a dictionary as memory, adding an entry and establishing a name index during memory writing, and using the index to read the corresponding causal graph information during multi-turn dialogues.\nFor example, some questions require invoking multiple tools. When analyzing the relationships of edges, a causal graph needs to be generated first, followed by reasoning"}, {"title": "4. Results", "content": "To test our causal intelligence agent, we start from the perspective of causal question modeling in Section 3.1 and have designed a series of question templates for variable-level, edge-level, causal graph-level, and causal effect-level respectively, details in the appendix. To obtain the data required for causal questions, we have generated tabular data using the nonlinear additive gaussian noise method [89]. In addition, for questions at the variable level, edge level, causal graph level, and causal effect level, we have constructed a dataset of size 1.3K for testing by using ChatGPT-3.5, as shown in Figure 4. Through testing, our causal agent has achieved high accuracy over four-level questions. All three sub-problems at the variable level achieved an accuracy of over 92%, all three sub-problems at the edge level achieved an accuracy of over 89%, all three sub-problems at the causal graph level achieved an accuracy of over 81%, and all two sub-problems at the causal effect level achieved an accuracy of over 93%;"}, {"title": "5. Discussion", "content": "In this work, we harnessed LLM to construct a causal agent by invoking causal tools, modeling causal problems across four levels in the causal domain, and endowing the large model with causal capabilities at four levels, followed by an assessment of the agent's abilities. The experimental results of the agent in solving causal problems showed that it performed particularly well at the variable level. In tasks of independence testing, accuracy rates exceeded 92% and even reached 100% in the multi-conditional independence test. This endowed the agent with the ability to leverage correlation analysis driven by tabular data.\nAt the edge level, the agent achieved accuracy rates of over 89% in judging direct causal relationships, confounding factors, and colliders, indicating its high capability in handling causal relationships. At the causal graph level, the accuracy rates for generating complete and partial causal graphs were 81.8% and 91.6%, respectively, demonstrating the agent's potential in constructing causal relationship networks using data-driven approaches and causal tools. The agent can correctly invoke tools and generate causal graphs, which is significant for the popularization of the causal community and the automation of causal tools. Even users who are not familiar with the concept of causality can utilize the agent to produce an end-to-end process from data to causal graphs. At the causal effect level, the agent can produce the correct average causal effect, achieving an accuracy rate of 93% on our small-scale dataset.\nMoreover, the use of causal tools ensures interpretability and reliability, which is of great significance for the future practical application of the causal agent.\nAnalyzing the agent's errors, we can find that there is a bias in the agent's under-standing and application of causal tools, leading to a decrease in accuracy in some cases. However, this issue will be gradually resolved as the capabilities and generalization per-formance of large models improve. From this perspective, causal scientists can focus on improving the interaction efficiency and accuracy of the agent and causal tools. As the capabilities of LLM are enhanced in the future, the agent's causal inference capabilities will also increase accordingly. Additionally, the agent's performance varies across different domains (such as marketing and medical), indicating that domain-specific knowledge and further domain adaptation may help improve the agent's performance. Addressing the issue of poor robustness of the agent in different domains will greatly affect the practical application of the causal agent. Moreover, the current causal agent cannot select models and perceive data. The agent in this work only directly invokes simple causal models, such as the PC algorithm and LinearDML algorithm, but the applicability of these algorithms is limited and heavily relies on our functional assumptions about the data and the assumption of no confounding. How to endow the agent with the ability to perceive data and to have prior understanding and knowledge of tool invocation is of great significance for the agent's promotion and practical application."}]}