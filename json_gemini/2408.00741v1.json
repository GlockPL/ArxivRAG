{"title": "DynamoLLM: Designing LLM Inference Clusters for Performance and Energy Efficiency", "authors": ["Jovan Stojkovic", "Chaojie Zhang\u2020", "\u00cd\u00f1igo Goiri\u2020", "Josep Torrellas", "Esha Choukse"], "abstract": "The rapid evolution and widespread adoption of generative large language models (LLMs) have made them a pivotal workload in various applications. Today, LLM inference clusters receive a large number of queries with strict Service Level Objectives (SLOs). To achieve the desired performance, these models execute on power-hungry GPUs causing the in-ference clusters to consume large amount of energy and, conse-quently, result in excessive carbon emissions. Fortunately, we find that there is a great opportunity to exploit the heterogeneity in inference compute properties and fluctuations in inference work-loads, to significantly improve energy-efficiency. However, such a diverse and dynamic environment creates a large search-space where different system configurations (e.g., number of instances, model parallelism, and GPU frequency) translate into different energy-performance trade-offs. To address these challenges, we propose DynamoLLM, the first energy-management framework for LLM inference environments. DynamoLLM automatically and dynamically reconfigures the inference cluster to optimize for energy and cost of LLM serving under the service's performance SLOs. We show that at a service-level, DynamoLLM conserves 53% energy and 38% operational carbon emissions, and reduces 61% cost to the customer, while meeting the latency SLOs.", "sections": [{"title": "I. INTRODUCTION", "content": "The exponential growth in the adoption of generative large language models (LLMs) has positioned them at the core of numerous technological advancements and applications. Today, we see use-cases of LLMs in various domains, such as healthcare [52], developer productivity [13], data analyt-ics [68], education [5] and other. As the popularity of LLMs increases among users, the inference clusters receive millions of queries per day [27] resulting in large infrastructures with sophisticated software and expensive hardware systems.\nTo meet these ever increasing computing demands, re-searchers proposed various software [9], [26], [35], [73], [81] and hardware [4], [50], [78] techniques. Such techniques improve the performance efficiency of LLM inference clusters. However, one aspect that has been largely overlooked is the energy consumption of these environments [58], [60]. The substantial energy requirements of serving LLMs running on power-hungry GPUs have emerged as a significant concern. As these models become integral to various services, minimizing their energy consumption and, consequently, carbon emissions while maintaining high performance is paramount.\nTo address this gap, this paper starts by characterizing the energy-efficiency properties of LLM inference workloads. Our characterization underscores that such environments present a distinct set of challenges, divergent from existing energy management schemes tailored for traditional datacenters appli-"}, {"title": "II. BACKGROUND", "content": "Computational phases of LLMs Generative LLMs [34], [38], [56], [67], [80] are auto-regressive: they process the whole input in parallel, and serially generate the output tokens. This property leads to two computationally distinct phases [49], [50]. First is the prefill phase, where the input tokens are computed in parallel. This is a compute-intensive phase and scales with the number of input tokens. Second is the decode phase, where each output token is generated serially, based on all the tokens seen so far. This is a memory-intensive phase, and scales with the number of output tokens.\nPerformance metrics for LLMs To evaluate the performance, we use: time to first token (TTFT), time between tokens (TBT), and throughput [50], [63]. TTFT is the latency of generating the first output token; while TBT is the latency to generate each new output token. To quantify the energy efficiency, we measure the energy consumption in Watt-hours (Wh) while meeting certain latency SLOs. The SLOs vary depending on their use cases for different tasks. For latency-sensitive tasks, both TTFT and TBT are important metrics with strict SLOs. We define SLOs for TTFT and TBT based on maximum achievable performance, described in Table IV.\nLLM parallelism A single model can be divided across GPUs to improve performance and allow larger memory footprints. LLM inference typically uses pipeline and tensor parallelism. Pipeline parallelism (PP) partitions the LLM layers among GPUs, while keeping all the operators/tensors of a layer on the GPU. GPUs then communicate only in between two consecutive stages. Tensor parallelism (TP) allocates a slice of each layer to each GPU. This requires aggregation across all the GPU for each layer, in turn needing high bandwidth"}, {"title": "III. OPPORTUNITIES FOR ENERGY EFFICIENCY", "content": "To understand the energy-efficiency properties of LLM inference environments, we characterize open-source mod-els [33], [38], [39], [66] on an NVIDIA DGX H100 server [44] using vLLM [26] inference engine. We analyze the energy properties of LLMs by varying the request lengths, request load, model, and service SLO. Additionally, we analyze how the profiled variables change over time in a real-production environment using the invocation traces of two LLM services from Azure: Coding and Conversation. The traces include a subset of invocations received by the profiled services during one week, and contain the timestamp of the invocation, along with the number of input and output tokens. These traces are a super-set of our open-source traces for the same services [50].\nRequest lengths The prefill and decode phases in an LLM inference exhibit distinct execution behaviors (Section II), suggesting that requests of different input and output lengths possess different compute and energy characteristics. We cate-gorize the requests based on the number of input/output tokens into 9 buckets: SS (short input, short output), SM (short input, medium output), SL (short input, long output), MS, MM, ML, LS, LM, and LL. \nWe use these categories to characterize the energy consump-tion of different request types running the Llama2-70B [33] model with a medium system load of 2000 tokens per second (TPS) under various GPU frequencies and model parallelisms.\nSince shorter requests are not computationally intensive, they meet their SLOs with any tensor parallelism, and generally at lower"}, {"title": "B. Dynamic LLM Inference Workloads", "content": "Changing request-length distribution We measure the distri-bution of request types for Coding and Conversation services.\nAs observed earlier, different request types require different energy-optimal configurations. Thus, the system needs to split its resources into per request-type pools, configure pools individually, and dynamically adapt the pools' configurations based on the current request distribution. However, if the system classifies the requests into too few classes, it will not be able to fine-tune the system for best energy. On the other hand, too many classes may lead to fragmentation and negatively impact energy efficiency. Thus, the system has to find the right number of resource pools. In DynamoLLM, we will use historical data to set the number of pools such that requests with distinct SLO requirements (TTFT or TBT bound) and compute properties (compute or memory bound) have separate pools. Moreover, as the load of a given request type reduces, DynamoLLM will avoid fragmentation by merging the pool with the next available pool that serves longer requests.\nRequest load fluctuations LLM inference workloads, as user-facing applications, exhibit a typical diurnal pattern with peaks during working hours and valleys at night and weekends.\nLLM service SLO and model diversity Finally, different services may time-share the same LLM model instance [14]. They may have different SLOs, requiring the configuration to be adapted based on the current service-user. On the other hand, the same service may concurrently use multiple different models [11]. This requires different execution plans for the optimal energy consumption of the individual queries. Thus, it is not trivial for service providers to operate in an energy-optimal setting while meeting the performance SLOs."}, {"title": "C. Reconfiguration Overheads", "content": "To capture the fast changes in LLM inference workloads, we need to quickly transition between configurations. However, there are overheads to change (1) number of inference server instances, (2) model parallelism, and (3) GPU frequency.\nChanging instance number To adjust to fluctuating load, it is cost-beneficial to dynamically adjust the number of LLM instances to serve the requests (i.e., scale in and out). However, the overheads of adding a new inference server are too large to be tolerable on the critical path of inference loads.\nChanging model parallelism To modify the model paral-lelism of an LLM inference server, we need to perform two operations. First, we need to re-shard the model weights and transfer them to the memory of the right GPUs. Second, the inference engine needs to synchronize the involved GPUs.\nChanging GPU frequency Setting the GPU frequency (e.g., via nvidia-smi [46]) incurs non-negligible overheads. It involves invoking the OS, communicating with the GPU driver via system calls, and performing hardware interactions via firmware. On average, setting the GPU frequency takes"}, {"title": "IV. DYNAMOLLM: AN ENERGY MANAGEMENT FRAMEWORK FOR LLM INFERENCE CLUSTERS", "content": "We use the insights to design DynamoLLM, the first energy management framework for LLM inference environments. DynamoLLM seamlessly integrates with existing inference plat-forms, enabling LLM workloads to operate energy-efficiently and cost-effectively while meeting their performance SLOs. DynamoLLM has four key principles. First, it is energy optimized and SLO-aware, leveraging model profiles to au-tomatically select the most energy-efficient configuration for specific LLMs and inference workloads within their SLO requirements. Second, DynamoLLM fine-tunes configurations for heterogeneous LLM workloads by dividing cluster re-sources into instance pools tailored to specific request types. Third, DynamoLLM accommodates fluctuating LLM infer-ence loads by dynamically reconfiguring the chosen organiza-tion. Finally, to ensure frequent and smooth reconfiguration, DynamoLLM minimizes reconfiguration overheads.\nArchitecture Figure 4 shows the DynamoLLM architecture. The system is organized hierarchically at the cluster, pool, and instance levels. At each level, the controllers tune their assigned configuration knob, and communicate their decisions with the controllers from the upper and lower levels. The controllers use energy-performance models generated in the profiling phase to determine the number of instances, model parallelization, and GPU frequency for an energy-optimized operation given the current system state. (1) Cluster Manager receives inference requests, predicts their type, and forwards them to the appropriate instance pool. Additionally, it peri-"}, {"title": "A. Configuring Instances for Energy-Efficiency", "content": "Generating LLM profiles When deploying their service to DynamoLLM, users specify the LLM used by the service and the expected performance SLOs. Then, the system character-izes the model and generates its energy-performance profile. DynamoLLM profiles the model by running loads of different request lengths at different model parallelisms (TP2, TP4 and TP8) and GPU frequencies (800\u20131980MHz, with a step of 200MHz). The system profiles a few load levels, up to the maximum throughput, and then extrapolates the behavior for the loads in between the measured ones. The profiling result is a function that takes the load, request length, model parallelism and GPU frequency as inputs and outputs the expected energy consumption and TTFT/TBT latencies.\nAs many services may use the same model, DynamoLLM can reuse the profiles across services, minimizing the profiling overheads. Such profiles are stored in a global DynamoLLM repository, and then cached in a cluster-local storage when a given service is deployed in the cluster.\nSelecting the energy-optimized configuration Given the current load and available resources, DynamoLLM uses the generated profiles to minimize energy consumption while staying within performance constraints. The system formulates this task as an optimization problem for the mixed integer linear programming (MILP) solver. The solver needs to output how many instances of each tensor parallelism ($N_{TP2}$, $N_{TP4}$ and $N_{TP8}$) are needed, at which frequency they should run ($f_{TP2}$, $f_{TP4}$ and $f_{TP8}$), and which load should be assigned to each instance ($L_{TP2}$, $L_{TP4}$ and $L_{TP8}$). We assume that all instances of a given parallelism run at the same frequency and receive fair-share amount of work.\nThe optimization target of the solver is to minimize the total energy consumption (E), while the constraints are: 1) the total number of GPUs used by all instance types does not exceed the assigned number of GPUs (N); 2) the load assigned to individual instances sums up to the total expected load (L); and 3) the expected performance of all instances with the assigned load is within the requirements (SLO). Functions $Energy_{TP_i,f_i}(L_{TP_i})$ and $Performance_{TP_i,f_i}(L_{TP_i})$ output the expected energy and performance, respectively, when run-ning the load $L_{TP_i}$ with $TP_i$ parallelism at $f_i$ GPU frequency. Then, the optimization task can be formulated as:\nmin $\\sum_i (N_{TP_i} \\times Energy_{TP_i,f_i}(L_{TP_i}))$\ns.t. $\\sum i \\times N_{TP_i} \\le N$\n$\\sum(N_{TP_i} \\times L_{TP_i}) \\ge L$\n$Performance_{TP_i,f_i}(L_{TP_i}) < SLO$ $\\forall i \\in \\{2,4,8\\}$        (1)\nThis approach guarantees the energy optimal configuration. However, it introduces non-negligible overheads (i.e., ~100s of ms) due to the large search-space for the solver. Hence, it cannot be used to select the correct system configuration at fine-grain intervals (e.g., every few seconds). Next we show how to break the task into a hierarchy of subtasks and use an approximation heuristic to reduce the computation complexity."}, {"title": "B. Hierarchical Control for Dynamic Load", "content": "DynamoLLM simplifies computations by assigning spe-cific optimization tasks to individual controllers. Instead of searching for a globally optimal configuration, controllers set locally optimal values for individual knobs under the constraints imposed by the upper-level controllers and under the assumption that the lower-level controllers operate at the highest performance configuration. This allows the controllers to operate at varying time scales\u2013from minutes for node ad-justments down to seconds for frequency tuning. The different scales are needed as each operational change involves distinct overheads and energy-efficiency impacts."}, {"title": "C. Reduced Overheads for Smooth Reconfiguration", "content": "To enable frequent reconfiguration, DynamoLLM proposes a set of techniques to minimize the overheads of (1) scaling-in/out the number of LLM inference servers, (2) sharding-up/down the parallelism of a given instance, and (3) scaling-up/down the GPU frequency of a given instance.\nScaling in/out inference servers DynamoLLM reduces the overheads of creating a new server instance by implementing several strategies. First, it keeps the model weights cached locally within the cluster (shown in Figure 4) avoiding the need to fetch them from a global repository. Second, it starts VMs from a snapshot with the entire state of the inference engine already initialized, reducing the boot-up time. This snapshot includes pre-loaded libraries, GPU drivers and inference en-gine configurations. Third, it proactively creates new VMs in the background, outside of the critical path of active workload handling. Specifically, DynamoLLM predicts the peak load for the next scheduling epoch and starts the extra VMs before the epoch starts. By having these VMs ready to go, DynamoLLM can seamlessly offload a fraction of the load to new instances without any noticeable latency impact.\nSharding up/down an instance To reduce the re-sharding overheads, DynamoLLM optimizes the distribution of weights across GPUs. We propose two techniques to minimize the data transfers and latency of individual transfers. First, the system develops a graph matching algorithm that maximizes the amount of weights that remain stationary in their current GPUs. The algorithm takes current weight distribution and desired tensor parallelism as inputs, and outputs the source and destination GPUs and fraction of weights to be transferred between each source-destination pair. Specifically, the algo-rithm constructs a bipartite graph where nodes represent GPUs in the current and next configurations. Edges between nodes represent potential transfers, weighted by the amount of data to be transferred. Then, it applies a maximum weight matching algorithm to find the optimal transfer plan that minimizes the total weight of the edges (i.e., minimizes the amount of data transferred). Second, to reduce the transfer latency, the system uses inter-GPU direct transfers via NVLink, allowing them to send fractions of their weights in parallel to other GPUs without any host intervention."}, {"title": "D. Predictive Scheduling for Request Heterogeneity", "content": "To map the heterogeneity of requests to the heterogeneous instance pools, the cluster controller in DynamoLLM uses an output-length predictor to anticipate the request type and steer requests to the correct instance pool. The predictor acts as a proxy model that takes input prompt and classifies the output as short, medium or long. Based on the predicted output length and known input length, the cluster manager forwards the request to the pool manager being in charge for a given request type. If the instance pool is currently overloaded, the cluster manager forwards the request to the next available pool for a larger request type. Once the request arrives to the correct pool, the pool manager needs to pick an instance from the pool. Specifically, the manager uses the generated models from the profiling step to predict energy and response times of each instance after potentially adding a new request to that instance. Then, it chooses the instance that minimizes total energy while staying within per-instance throughput determined by the SLO.\nHandling mis-predictions If the system over-estimates a request length, the request gets routed to a higher-performance pool. Hence, it runs with sub-optimal energy, but its latency remains unaffected. Conversely, if a request length is under-estimated, the request is placed to a lower-performance pool, potentially missing its SLOs. Similarly, load mis-predictions can result in insufficient resources for a given pool during request bursts. In both cases, due to some mis-predictions, the system needs to react to the created emergency event.\nWhen an instance manager detects that its queue is building up, indicating that the rate of request processing is lower than the rate of request arrival, it triggers an emergency event. First, the instance manager tries to re-order the requests in its queue and prioritizes those requests that are about to miss their deadline. Second, if some requests will miss their deadlines even after the reordering, the instance manager ramps up the frequency of its GPUs to the maximum value. Third, if the"}, {"title": "V. EVALUATION", "content": "We run our experiments on servers with 8 H100 GPUs [44]. We show the results for Llama2-70B [67], but other models (i.e., Mixtral [38], Falcon [66], BLOOM [59]) follow the same trends. We set the load using production-level traces: 1 hour open-source traces [50] and 1-day and 1-week traces for Coding and Conversation from our fleet. We compare Dy-namoLLM to five systems. SinglePool (a state-of-the-practice baseline) schedules all the requests to the common pool of instances running with TP8 at the highest GPU frequency. MultiPool separates LLM instances in multiple per-request-type pools. ScaleInst, ScaleShard, and ScaleFreq additionally"}, {"title": "B. Cluster-Level Experiments", "content": "We first evaluate the system on a cluster of GPU servers using the 1h open source production traces for the Conver-sation service [50]. We provision the baselines with 12 H100 servers to handle the peak load, while DynamoLLM scales the number of servers according to the current load.\nLatency Figure 7 shows the TTFT/TBT latencies for each sys-tem. By separating request types into different resource pools, MultiPool removes the head-of-line blocking effect and re-duces the latencies over SinglePool. Similarly, ScaleShard and ScaleFreq, and DynamoLLM reduce the tail latency. However, these systems slightly increase the P50 latency by operating in lower-performance modes when there is available SLO slack. On the other hand, ScaleInst increases the tail latency due to the large overheads of creating a new inference server on the critical path of users' load. Overall, DynamoLLM reduces"}, {"title": "C. Sensitivity Studies", "content": "Sensitivity to predictor accuracy We analyze how the ac-curacy of the prediction models affects the overall system efficiency. We introduce bounded errors for the output length misclassification and measure the energy consumption with medium load.\nSensitivity to load We evaluate DynamoLLM with different system loads. We generate Low, Medium, and High loads with a Poisson distribution for request inter-arrival times.\nSensitivity to number of pools Figure 13 shows the energy consumption and performance (TTFT) of DynamoLLM with different number of request pools. Recall that our chosen design has 9 pools. By adding too many pools (12 or 16), the system gets fragmented, and the idle energy of GPUs results in the overall energy increase. Reducing the number of pools (2 or 4) prevents the system from fine tuning the frequency and the model parallelism for specific request types.\nD. Long Cluster-Level Experiments\nWe run longer experiments by running the 1-day traces for the Conversation service. The trace covers all invocations for"}, {"title": "E. Large-Scale Simulations", "content": "To generalize our insights to large-scale, we develop a discrete-time simulator that simulates the energy consumption of different systems using production traces. DynamoLLM significantly reduces the energy con-sumption for both types of services. DynamoLLM operates in higher energy-efficient modes for the Conversation service due to its typically shorter input lengths (ML dominant request type). On the other hand, the Coding service has deep valleys during the night and weekends. Thus, DynamoLLM exploits the periods of low load to save energy. DynamoLLM reduces the energy consumption over the baseline by 47% and 56% for the Conversation and Coding services, respectively."}, {"title": "F. Cost and Carbon Emission", "content": "User cost DynamoLLM reduces the operational cost for users by minimizing the number of GPUs and optimizing their energy efficiency. The number of GPU servers for the week-long experiments reduces from 40 to 24.6 on average (38.5% cost reduction). Using the current GPU VM pricing [8], this saves $1362.7/hour. By reducing the energy consumption, DynamoLLM reduces the associated energy costs by up to\n56%. As energy costs [28] are currently substantially lower than GPU costs, this translates to only $4.4/hour savings.\nCarbon emissions The energy consumption translates into the amount of operational CO2 emissions. We use the traces of carbon intensity [2] for a week-long period from multiple grids and map the carbon intensity to the energy consumption over time for the SinglePool baseline and DynamoLLM. DynamoLLM reduces the associated energy costs by up toSinglePool and DynamoLLM consume 5t and 3.1t/week of CO2. These substantial savings (38%) make a step towards sustainable LLM environments."}, {"title": "VI. RELATED WORK", "content": "Cluster resource and power management A rich body of work seeks to improve resource efficiency under the SLO constraints through resource management for a wide range of latency sensitive workloads, such as microservices [76] and DL workloads, through effective resource sharing [6], [43], [51], dynamic allocation [71], and hardware reconfigura-tion [24]. Others focus on approaches that enable safe power management and oversubscription [16], [29], [49] leveraging workload characteristics [25], [75] and system state [62].\nEnergy-efficient workloads Prior works focused on energy-efficiency for CPU workloads [15], [31], [43], [61], and researchers started exploring unique energy properties of GPU workloads [58], [65], [74]. Recent schemes build on top and manage energy and power consumption for DNN inference and training [69], [70], [72] through frequency scaling [20], [23], [40], [41], [64], [77], [82], autoscaling [22], and re-source partitioning and mapping [18], [64]. We show that improving energy efficiency for LLM inference necessitates a comprehensive view of all available knobs. DynamoLLM is holistic framework that dynamically reconfigures all the knobs considering the diversity and dynamism of requests and loads."}, {"title": "VII. CONCLUSION", "content": "We present DynamoLLM, the first energy-management framework for LLM inference clusters. DynamoLLM exploits heterogeneity in inference compute properties and fluctuations in inference workloads to save energy. The system automat-ically and dynamically configures the energy-optimal organi-zation of the cluster (number of instances, model parallelism and GPU frequency) while performing under performance guarantees. DynamoLLM reduces energy, carbon emissions and cost to the customer by 53%, 38% and 61%, respectively."}]}