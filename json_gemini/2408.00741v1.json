{"title": "DynamoLLM: Designing LLM Inference Clusters for Performance and Energy Efficiency", "authors": ["Jovan Stojkovic", "Chaojie Zhang\u2020", "\u00cd\u00f1igo Goiri\u2020", "Josep Torrellas", "Esha Choukse"], "abstract": "The rapid evolution and widespread adoption of generative large language models (LLMs) have made them a pivotal workload in various applications. Today, LLM inference clusters receive a large number of queries with strict Service Level Objectives (SLOs). To achieve the desired performance, these models execute on power-hungry GPUs causing the inference clusters to consume large amount of energy and, consequently, result in excessive carbon emissions. Fortunately, we find that there is a great opportunity to exploit the heterogeneity in inference compute properties and fluctuations in inference workloads, to significantly improve energy-efficiency. However, such a diverse and dynamic environment creates a large search-space where different system configurations (e.g., number of instances, model parallelism, and GPU frequency) translate into different energy-performance trade-offs. To address these challenges, we propose DynamoLLM, the first energy-management framework for LLM inference environments. DynamoLLM automatically and dynamically reconfigures the inference cluster to optimize for energy and cost of LLM serving under the service's performance SLOs. We show that at a service-level, DynamoLLM conserves 53% energy and 38% operational carbon emissions, and reduces 61% cost to the customer, while meeting the latency SLOs.", "sections": [{"title": "I. INTRODUCTION", "content": "The exponential growth in the adoption of generative large language models (LLMs) has positioned them at the core of numerous technological advancements and applications. Today, we see use-cases of LLMs in various domains, such as healthcare [52], developer productivity [13], data analytics [68], education [5] and other. As the popularity of LLMs increases among users, the inference clusters receive millions of queries per day [27] resulting in large infrastructures with sophisticated software and expensive hardware systems.\nTo meet these ever increasing computing demands, researchers proposed various software [9], [26], [35], [73], [81] and hardware [4], [50], [78] techniques. Such techniques improve the performance efficiency of LLM inference clusters. However, one aspect that has been largely overlooked is the energy consumption of these environments [58], [60]. The substantial energy requirements of serving LLMs running on power-hungry GPUs have emerged as a significant concern. As these models become integral to various services, minimizing their energy consumption and, consequently, carbon emissions while maintaining high performance is paramount.\nTo address this gap, this paper starts by characterizing the energy-efficiency properties of LLM inference workloads. Our characterization underscores that such environments present a distinct set of challenges, divergent from existing energy management schemes tailored for traditional datacenters applications [7], [17], [21], [31], [61], [80]. Specifically, we observe that heterogeneity in LLM inference compute properties and fluctuations in LLM inference workloads create a dynamic environment with large variations. Such variations arise from: (1) requests with varying input/output token lengths, (2) distinct compute properties of different LLMs, and (3) different SLOs required by the services using an LLM.\nRequests with a large number of input tokens are compute intensive, thus, sensitive to GPU frequency. Conversely, requests with a few input tokens and many output tokens have low compute, but high memory requirements. Reducing their GPU frequency would save the energy without significantly impacting the performance. Moreover, the number of model parameters also affects the LLM's sensitivity to the number of GPUs and GPU frequency. Finally, depending on the service currently using the LLM, the SLO requirements can be strict requiring high-performance configurations, or loose allowing for lower-performance but more energy-efficient configurations. Importantly, these characteristics rapidly change due to load fluctuations and dynamic distributions of requests. Such dynamic changes cause a system configuration that is energy-efficient at a given point, to quickly become sub-optimal. This requires a dynamic approach to resource management.\nTo pave the way towards energy-efficient and sustainable LLM inference clusters, this paper introduces DynamoLLM, the first energy-management framework for LLM inference environments. DynamoLLM exploits the unique properties of LLM inference workloads to reduce their energy consumption while meeting the performance SLOs. The system uses energy-performance profiles of models and their workloads to automatically and dynamically select the energy-efficient configuration. It leverages multiple knobs, including scaling in/out the number of server instances, model parallelism across GPUs, and GPU frequency scaling.\nTo handle workload heterogeneity, DynamoLLM maintains differently configured pools of LLM instances that are optimal for different types of incoming requests. For instance, compared to a request with many input and output tokens, a request that processes and outputs fewer tokens runs more efficiently on a model parallelized across fewer GPUs running at a lower frequency. As request distribution varies over time, DynamoLLM dynamically sizes the pools. These pools can be merged into fewer pools or divided into multiple pools over time, providing a balance between right-sizing and fragmentation of resources. To efficiently manage the"}, {"title": "II. BACKGROUND", "content": "Computational phases of LLMs Generative LLMs [34], [38], [56], [67], [80] are auto-regressive: they process the whole input in parallel, and serially generate the output tokens. This property leads to two computationally distinct phases [49], [50]. First is the prefill phase, where the input tokens are computed in parallel. This is a compute-intensive phase and scales with the number of input tokens. Second is the decode phase, where each output token is generated serially, based on all the tokens seen so far. This is a memory-intensive phase, and scales with the number of output tokens.\nPerformance metrics for LLMs To evaluate the performance, we use: time to first token (TTFT), time between tokens (TBT), and throughput [50], [63]. TTFT is the latency of generating the first output token; while TBT is the latency to generate each new output token. To quantify the energy efficiency, we measure the energy consumption in Watt-hours (Wh) while meeting certain latency SLOs. The SLOs vary depending on their use cases for different tasks. For latency-sensitive tasks, both TTFT and TBT are important metrics with strict SLOs. We define SLOs for TTFT and TBT based on maximum achievable performance, described in Table IV.\nLLM parallelism A single model can be divided across GPUs to improve performance and allow larger memory footprints. LLM inference typically uses pipeline and tensor parallelism. Pipeline parallelism (PP) partitions the LLM layers among GPUs, while keeping all the operators/tensors of a layer on the GPU. GPUs then communicate only in between two consecutive stages. Tensor parallelism (TP) allocates a slice of each layer to each GPU. This requires aggregation across all the GPU for each layer, in turn needing high bandwidth communication. TP performs better for GPUs within the same server, connected with high bandwidth interconnects (e.g., NVLink [45]), while PP is preferred across servers. Since most open source models [34], [38], [67] fit on 8 GPUs in a single server, we consider only TP in the rest of the paper; the ideas can easily extend to PP. We denote tensor parallelism across 2, 4 and 8 GPUs as TP2, TP4 and TP8, respectively.\nPower and energy in datacenters A rich body of work explored power/energy efficiency in traditional datacenters [7], [29], [31], [62]. However, the rapid growth of LLMs has posed new challenges that have not yet been extensively studied. LLM inference workloads comprise a swiftly increasing percentage of datacenter load [49]. This, coupled with the power-dense hardware like DGX A100s and H100s being deployed to serve these workloads makes them power, energy, and carbon-intensive [12], [49], [58]. To effectively address this challenge, it is important to have a comprehensive framework for managing energy in these systems."}, {"title": "III. OPPORTUNITIES FOR ENERGY EFFICIENCY", "content": "To understand the energy-efficiency properties of LLM inference environments, we characterize open-source models [33], [38], [39], [66] on an NVIDIA DGX H100 server [44] using vLLM [26] inference engine. We analyze the energy properties of LLMs by varying the request lengths, request load, model, and service SLO. Additionally, we analyze how the profiled variables change over time in a real-production environment using the invocation traces of two LLM services from Azure: Coding and Conversation. The traces include a subset of invocations received by the profiled services during one week, and contain the timestamp of the invocation, along with the number of input and output tokens. These traces are a super-set of our open-source traces for the same services [50].\nA. Heterogeneous Energy-Performance Profiles\nRequest lengths The prefill and decode phases in an LLM inference exhibit distinct execution behaviors (Section II), suggesting that requests of different input and output lengths possess different compute and energy characteristics. We categorize the requests based on the number of input/output tokens into 9 buckets: SS (short input, short output), SM (short input, medium output), SL (short input, long output), MS, MM, ML, LS, LM, and LL. Table IV shows the thresholds and corresponding TTFT/TBT SLOs. We set the thresholds for request lengths using the 33rd, 66th and 100th percentiles of the input/output lengths from a trace for a Conversation service from Azure. We set the SLOs to 5\u00d7 the latency of a single request running isolated on a system [30].\nWe use these categories to characterize the energy consumption of different request types running the Llama2-70B [33] model with a medium system load of 2000 tokens per second (TPS) under various GPU frequencies and model parallelisms. Table I shows our results in the form of a heat map. Since shorter requests are not computationally intensive, they meet their SLOs with any tensor parallelism, and generally at lower"}, {"title": "B. Dynamic LLM Inference Workloads", "content": "Changing request-length distribution We measure the distribution of request types for Coding and Conversation services. Figure 1 shows the distribution of requests for each workload over a week. The distribution differs across services. Conversation has typically longer outputs and shorter inputs, while Coding shows the opposite trend. However, both services have a significant fraction of each request type, and importantly, the popularity of request types changes over time.\nAs observed earlier, different request types require different energy-optimal configurations. Thus, the system needs to split its resources into per request-type pools, configure pools individually, and dynamically adapt the pools' configurations based on the current request distribution. However, if the system classifies the requests into too few classes, it will not be able to fine-tune the system for best energy. On the other hand, too many classes may lead to fragmentation and negatively impact energy efficiency. Thus, the system has to find the right number of resource pools. In DynamoLLM, we will use historical data to set the number of pools such that requests with distinct SLO requirements (TTFT or TBT bound) and compute properties (compute or memory bound) have separate pools. Moreover, as the load of a given request type reduces, DynamoLLM will avoid fragmentation by merging the pool with the next available pool that serves longer requests.\nRequest load fluctuations LLM inference workloads, as user-facing applications, exhibit a typical diurnal pattern with peaks during working hours and valleys at night and weekends. Figure 2 shows the load in tokens per second of the two workloads over a week. The load is normalized to the peak of the individual workloads. The Coding trace shows a clear diurnal pattern, with peaks every day, lower load at night, and much lower load during weekends. Conversation shows a less extreme, but still significant, diurnal pattern.\nThe peak load of Conversation is 1.7\u00d7 and 3.3\u00d7 higher than its average and valley loads, respectively. The peak load of Coding is 2.8\u00d7 and 34.6\u00d7 higher than its average and valley loads, respectively. This large slack indicates that the LLM inference servers can frequently operate in a less performant but energy-optimized configuration without violating the SLO. Once the load starts building up, the server needs to switch to a more performant mode of operation.\nLLM service SLO and model diversity Finally, different services may time-share the same LLM model instance [14]. They may have different SLOs, requiring the configuration to be adapted based on the current service-user. On the other hand, the same service may concurrently use multiple different models [11]. This requires different execution plans for the optimal energy consumption of the individual queries. Thus, it is not trivial for service providers to operate in an energy-optimal setting while meeting the performance SLOs."}, {"title": "C. Reconfiguration Overheads", "content": "To capture the fast changes in LLM inference workloads, we need to quickly transition between configurations. However, there are overheads to change (1) number of inference server instances, (2) model parallelism, and (3) GPU frequency.\nChanging instance number To adjust to fluctuating load, it is cost-beneficial to dynamically adjust the number of LLM instances to serve the requests (i.e., scale in and out). However, the overheads of adding a new inference server are too large to be tolerable on the critical path of inference loads. Table V shows the breakdown of the overheads to: (1) instantiate a new GPU VM in the cloud (such as H100 VM [36]), (2) initialize the distributed multi-GPU environment (e.g., Ray, MPI), (3) download the model weights, (4) setup the inference engine, and (5) install the weights and key-value cache on the GPUs. In total, these overheads can take even a few minutes. Hence, the conventional LLM inference environments typically provision the static number of instances to handle their peak load resulting in heavy underutilization. In DynamoLLM, we will propose techniques to efficiently scale the number of instances (with the current load) while minimizing most of the scale-out overheads.\nChanging model parallelism To modify the model parallelism of an LLM inference server, we need to perform two operations. First, we need to re-shard the model weights and transfer them to the memory of the right GPUs. Second, the inference engine needs to synchronize the involved GPUs. Current systems stop the engine, unload the weights from GPUs, load the weights from the host to the new set of GPUs, and re-start the engine from the scratch. This adds intolerable overheads (around 1-2 minutes) if performed on the critical path. In DynamoLLM, we will show how to minimize the re-sharding overheads by smartly mapping the logical to physical GPUs, exploiting inter-GPU direct NVLink connections and moving the weights between GPUs in the background.\nChanging GPU frequency Setting the GPU frequency (e.g., via nvidia-smi [46]) incurs non-negligible overheads. It involves invoking the OS, communicating with the GPU driver via system calls, and performing hardware interactions via firmware. On average, setting the GPU frequency takes"}, {"title": "IV. DYNAMOLLM: AN ENERGY MANAGEMENT FRAMEWORK FOR LLM INFERENCE CLUSTERS", "content": "We use the insights to design DynamoLLM, the first energy management framework for LLM inference environments. DynamoLLM seamlessly integrates with existing inference platforms, enabling LLM workloads to operate energy-efficiently and cost-effectively while meeting their performance SLOs. DynamoLLM has four key principles. First, it is energy-optimized and SLO-aware, leveraging model profiles to automatically select the most energy-efficient configuration for specific LLMs and inference workloads within their SLO requirements. Second, DynamoLLM fine-tunes configurations for heterogeneous LLM workloads by dividing cluster resources into instance pools tailored to specific request types. Third, DynamoLLM accommodates fluctuating LLM inference loads by dynamically reconfiguring the chosen organization. Finally, to ensure frequent and smooth reconfiguration, DynamoLLM minimizes reconfiguration overheads.\nArchitecture Figure 4 shows the DynamoLLM architecture. The system is organized hierarchically at the cluster, pool, and instance levels. At each level, the controllers tune their assigned configuration knob, and communicate their decisions with the controllers from the upper and lower levels. The controllers use energy-performance models generated in the profiling phase to determine the number of instances, model parallelization, and GPU frequency for an energy-optimized operation given the current system state. (1) Cluster Manager receives inference requests, predicts their type, and forwards them to the appropriate instance pool. Additionally, it peri-"}, {"title": "A. Configuring Instances for Energy-Efficiency", "content": "Generating LLM profiles When deploying their service to DynamoLLM, users specify the LLM used by the service and the expected performance SLOs. Then, the system characterizes the model and generates its energy-performance profile. DynamoLLM profiles the model by running loads of different request lengths at different model parallelisms (TP2, TP4 and TP8) and GPU frequencies (800\u20131980MHz, with a step of 200MHz). The system profiles a few load levels, up to the maximum throughput, and then extrapolates the behavior for the loads in between the measured ones. The profiling result is a function that takes the load, request length, model parallelism and GPU frequency as inputs and outputs the expected energy consumption and TTFT/TBT latencies.\nAs many services may use the same model, DynamoLLM can reuse the profiles across services, minimizing the profiling overheads. Such profiles are stored in a global DynamoLLM repository, and then cached in a cluster-local storage when a given service is deployed in the cluster.\nSelecting the energy-optimized configuration Given the current load and available resources, DynamoLLM uses the generated profiles to minimize energy consumption while staying within performance constraints. The system formulates this task as an optimization problem for the mixed integer linear programming (MILP) solver. The solver needs to output how many instances of each tensor parallelism ($N_{TP2}$, $N_{TP4}$ and $N_{TP8}$) are needed, at which frequency they should run ($f_{TP2}$, $f_{TP4}$ and $f_{TP8}$), and which load should be assigned to each instance ($L_{TP2}$, $L_{TP4}$ and $L_{TP8}$). We assume that all instances of a given parallelism run at the same frequency and receive fair-share amount of work.\nThe optimization target of the solver is to minimize the total energy consumption ($E$), while the constraints are: 1) the total number of GPUs used by all instance types does not exceed the assigned number of GPUs ($N$); 2) the load assigned to individual instances sums up to the total expected load ($L$); and 3) the expected performance of all instances with the assigned load is within the requirements ($SLO$). Functions $Energy_{TP_i,f_i}(L_{TP_i})$ and $Performance_{TP_i,f_i}(L_{TP_i})$ output the expected energy and performance, respectively, when running the load $L_{TP_i}$ with $TP_i$ parallelism at $f_i$ GPU frequency. Then, the optimization task can be formulated as:\n$min \\sum_i (N_{TP_i} \\times Energy_{TP_i,f_i}(L_{TP_i})) \\\\ s.t. \\sum_i i \\times N_{TP_i} < N \\\\ \\sum_i (N_{TP_i} \\times L_{TP_i}) > L \\\\ Performance_{TP_i,f_i}(L_{TP_i}) < SLO \\quad \\quad \\quad \\quad \\quad \\quad  \\forall i \\in \\{2,4,8\\}$"}, {"title": "B. Hierarchical Control for Dynamic Load", "content": "DynamoLLM simplifies computations by assigning specific optimization tasks to individual controllers. Instead of searching for a globally optimal configuration, controllers set locally optimal values for individual knobs under the constraints imposed by the upper-level controllers and under the assumption that the lower-level controllers operate at the highest performance configuration. This allows the controllers to operate at varying time scales\u2013from minutes for node adjustments down to seconds for frequency tuning. The different scales are needed as each operational change involves distinct overheads and energy-efficiency impacts."}, {"title": "C. Reduced Overheads for Smooth Reconfiguration", "content": "To enable frequent reconfiguration, DynamoLLM proposes a set of techniques to minimize the overheads of (1) scaling-in/out the number of LLM inference servers, (2) sharding-up/down the parallelism of a given instance, and (3) scaling-up/down the GPU frequency of a given instance.\nScaling in/out inference servers DynamoLLM reduces the overheads of creating a new server instance by implementing several strategies. First, it keeps the model weights cached locally within the cluster (shown in Figure 4) avoiding the need to fetch them from a global repository. Second, it starts VMs from a snapshot with the entire state of the inference engine already initialized, reducing the boot-up time. This snapshot includes pre-loaded libraries, GPU drivers and inference engine configurations. Third, it proactively creates new VMs in the background, outside of the critical path of active workload handling. Specifically, DynamoLLM predicts the peak load for the next scheduling epoch and starts the extra VMs before the epoch starts. By having these VMs ready to go, DynamoLLM can seamlessly offload a fraction of the load to new instances without any noticeable latency impact.\nSharding up/down an instance To reduce the re-sharding overheads, DynamoLLM optimizes the distribution of weights across GPUs. We propose two techniques to minimize the data transfers and latency of individual transfers. First, the system develops a graph matching algorithm that maximizes the amount of weights that remain stationary in their current GPUs. The algorithm takes current weight distribution and desired tensor parallelism as inputs, and outputs the source and destination GPUs and fraction of weights to be transferred between each source-destination pair. Specifically, the algorithm constructs a bipartite graph where nodes represent GPUs in the current and next configurations. Edges between nodes represent potential transfers, weighted by the amount of data to be transferred. Then, it applies a maximum weight matching algorithm to find the optimal transfer plan that minimizes the total weight of the edges (i.e., minimizes the amount of data transferred). Second, to reduce the transfer latency, the system uses inter-GPU direct transfers via NVLink, allowing them to send fractions of their weights in parallel to other GPUs without any host intervention."}, {"title": "V. EVALUATION", "content": "A. Evaluation Setup\nWe run our experiments on servers with 8 H100 GPUs [44]. We show the results for Llama2-70B [67], but other models (i.e., Mixtral [38], Falcon [66], BLOOM [59]) follow the same trends. We set the load using production-level traces: 1 hour open-source traces [50] and 1-day and 1-week traces for Coding and Conversation from our fleet. We compare DynamoLLM to five systems. SinglePool (a state-of-the-practice baseline) schedules all the requests to the common pool of instances running with TP8 at the highest GPU frequency. MultiPool separates LLM instances in multiple per-request-type pools. ScaleInst, ScaleShard, and ScaleFreq additionally"}, {"title": "B. Cluster-Level Experiments", "content": "We first evaluate the system on a cluster of GPU servers using the 1h open source production traces for the Conversation service [50]. We provision the baselines with 12 H100 servers to handle the peak load, while DynamoLLM scales the number of servers according to the current load.\nEnergy Figure 6 shows the energy consumption of the cluster for the experiment. MultiPool increases the energy consumption by 20% over SinglePool, because it allocates a larger number of resources while always operating at the highest-performance configuration. Meanwhile, ScaleInst, ScaleShard, ScaleFreq and DynamoLLM reduce the energy consumption by 4.1%, 7%, 19% and 35%, respectively. ScaleInst/Shard/Freq reduce the energy by configuring one knob but leave substantial space for further savings. Finally, DynamoLLM synchronously scales multiple knobs to achieve the lowest energy consumption. We further breakdown the total energy per request type. Figure 6 shows that longer requests (e.g., LL) and highly-popular requests (e.g., ML) consume disproportionally more energy than the other types.\nLatency Figure 7 shows the TTFT/TBT latencies for each system. By separating request types into different resource pools, MultiPool removes the head-of-line blocking effect and reduces the latencies over SinglePool. Similarly, ScaleShard and ScaleFreq, and DynamoLLM reduce the tail latency. However, these systems slightly increase the P50 latency by operating in lower-performance modes when there is available SLO slack. On the other hand, ScaleInst increases the tail latency due to the large overheads of creating a new inference server on the critical path of users' load. Overall, DynamoLLM reduces"}, {"title": "C. Sensitivity Studies", "content": "Sensitivity to predictor accuracy We analyze how the accuracy of the prediction models affects the overall system efficiency. We introduce bounded errors for the output length misclassification and measure the energy consumption with medium load. Figure 11 shows that the impact of the predictor accuracy is modest for both energy and performance. Compared to an environment with no error, an environment with an 40% error increases the energy consumption by 13% and TTFT by 7.3%. The reason for robustness to prediction errors is that DynamoLLM can promptly detect mis-predictions and re-configures the knobs accordingly.\nSensitivity to load We evaluate DynamoLLM with different system loads. We generate Low, Medium, and High loads with a Poisson distribution for request inter-arrival times. Figure 12 shows the energy consumption of the five evaluated systems with different load levels. With Low, Medium, and High load, DynamoLLM reduces the energy of SinglePool baseline by 51%, 40%, and 23.4%, respectively. As the load increases, the energy savings of DynamoLLM reduce, because the system more frequently needs to operate at higher frequencies with higher levels of model parallelism.\nSensitivity to number of pools Figure 13 shows the energy consumption and performance (TTFT) of DynamoLLM with different number of request pools. Recall that our chosen design has 9 pools. By adding too many pools (12 or 16), the system gets fragmented, and the idle energy of GPUs results in the overall energy increase. Reducing the number of pools (2 or 4) prevents the system from fine tuning the frequency and the model parallelism for specific request types. The performance improves by adding moderately more pools because it helps remove head of the line blocking and introduces more resources for execution."}, {"title": "D. Long Cluster-Level Experiments", "content": "We run longer experiments by running the 1-day traces for the Conversation service. The trace covers all invocations for"}, {"title": "E. Large-Scale Simulations", "content": "To generalize our insights to large-scale, we develop a discrete-time simulator that simulates the energy consumption of different systems using production traces. Figure 14 shows the normalized energy consumption for the five evaluated systems using 1-week traces for Conversation and Coding services. DynamoLLM significantly reduces the energy consumption for both types of services. DynamoLLM operates in higher energy-efficient modes for the Conversation service due to its typically shorter input lengths (ML dominant request type). On the other hand, the Coding service has deep valleys during the night and weekends. Thus, DynamoLLM exploits the periods of low load to save energy. DynamoLLM reduces the energy consumption over the baseline by 47% and 56% for the Conversation and Coding services, respectively."}, {"title": "F. Cost and Carbon Emission", "content": "User cost DynamoLLM reduces the operational cost for users by minimizing the number of GPUs and optimizing their energy efficiency. The number of GPU servers for the week-long experiments reduces from 40 to 24.6 on average (38.5% cost reduction). Using the current GPU VM pricing [8], this saves $1362.7/hour. By reducing the energy consumption, DynamoLLM reduces the associated energy costs by up to"}, {"title": "VI. RELATED WORK", "content": "Cluster resource and power management A rich body of work seeks to improve resource efficiency under the SLO constraints through resource management for a wide range of latency sensitive workloads, such as microservices [76] and DL workloads, through effective resource sharing [6], [43], [51], dynamic allocation [71], and hardware reconfiguration [24]. Others focus on approaches that enable safe power management and oversubscription [16], [29], [49] leveraging workload characteristics [25], [75] and system state [62].\nEnergy-efficient workloads Prior works focused on energy efficiency for CPU workloads [15], [31], [43], [61], and researchers started exploring unique energy properties of GPU workloads [58], [65], [74]. Recent schemes build on top and manage energy and power consumption for DNN inference and training [69], [70], [72] through frequency scaling [20], [23], [40], [41], [64], [77], [82], autoscaling [22], and resource partitioning and mapping [18], [64]. We show that improving energy efficiency for LLM inference necessitates a comprehensive view of all available knobs. DynamoLLM is holistic framework that dynamically reconfigures all the knobs considering the diversity and dynamism of requests and loads."}, {"title": "VII. CONCLUSION", "content": "We present DynamoLLM, the first energy-management framework for LLM inference clusters. DynamoLLM exploits heterogeneity in inference compute properties and fluctuations in inference workloads to save energy. The system automatically and dynamically configures the energy-optimal organization of the cluster (number of instances, model parallelism and GPU frequency) while performing under performance guarantees. DynamoLLM reduces energy, carbon emissions and cost to the customer by 53%, 38% and 61%, respectively."}]}