{"title": "Motion Control in Multi-Rotor Aerial Robots Using Deep Reinforcement Learning", "authors": ["Gaurav Shetty", "Mahya Ramezani", "Hamed Habibi", "Holger Voos", "Jose Luis Sanchez-Lopez"], "abstract": "This paper investigates the application of Deep Reinforcement Learning (DRL) to address motion control challenges in drones for additive manufacturing (AM). Drone-based additive manufacturing promises flexible and autonomous material deposition in large-scale or hazardous environments. However, achieving robust real-time control of a multi-rotor aerial robot under varying payloads and potential disturbances remains challenging. Traditional controllers like PID often require frequent parameter re-tuning, limiting their applicability in dynamic scenarios. We propose a DRL framework that learns adaptable control policies for multi-rotor drones performing waypoint navigation in AM tasks. We compare Deep Deterministic Policy Gradient (DDPG) and Twin Delayed Deep Deterministic Policy Gradient (TD3) within a curriculum learning scheme designed to handle increasing complexity. Our experiments show TD3 consistently balances training stability, accuracy and success, particularly when mass variability is introduced. These findings provide a scalable path toward robust, autonomous drone control in additive manufacturing.", "sections": [{"title": "I. INTRODUCTION", "content": "Unmanned Aerial Vehicles (UAVs) constitute a technology within autonomous systems, with applications in precision agriculture, infrastructure inspection, and logistics [1]. Recently, the integration of UAVs with Additive Manufacturing (AM) capabilities has been recognized as a promising development [2]. UAV-based AM combines the aerial mobility of drones with 3D printing technologies, enabling material deposition in complex or otherwise inaccessible environments [3]. This integration introduces new possibilities for construction and repair, particularly for large-scale structures that are difficult to access using traditional methods.\nHowever, the deployment of UAVs for AM tasks presents control challenges [4]. As drones deposit materials, their mass distribution changes continuously, altering both the center of gravity and inertia. In addition, external disturbances further complicate control, making it difficult to maintain stability and accuracy in dynamic three-dimensional environments [5]. These factors necessitate the development of robust and adaptive control systems.\nTraditional control systems [6], [7], have been widely utilized in UAV navigation. These methods rely on fixed parameters and predefined models, performing effectively in predictable environments [8], [9]. However, within the context of UAV-based AM, the continuous changes in mass distribution and the presence of external forces require frequent recalibration of these controllers. This necessity limits their suitability for real-time adaptive applications, where dynamic and unpredictable conditions are prevalent [10].\nReinforcement Learning (RL) offers an alternative approach to UAV control by enabling systems to learn control policies through interaction with their environment, without depending on explicit system models. This model-free method allows RL algorithms to adapt dynamically to both internal variations and external disturbances [10], [11]. However, standard RL methods encounter challenges in high-dimensional environments like AM, where state and action spaces are extensive and the environment is highly unpredictable [12].\nDeep Reinforcement Learning (DRL) builds on traditional reinforcement learning by using neural networks to approximate value functions and policies. This approach enables effective learning in high-dimensional state and action spaces. DRL proves effective in addressing the complex challenges associated with UAV-based AM [13].\nA key consideration in applying DRL to UAV-based AM is the choice between deterministic and stochastic policy methods. Deterministic approaches, such as Deep Deterministic Policy Gradient (DDPG) [14], produce specific actions for a given state. This ensures high precision and consistency [15], which are essential for accurate material deposition in AM. Stochastic methods sample actions from probability distributions, introducing variability that can reduce precision. Deterministic methods are better suited for UAV-based AM as they provide predictable and repeatable control. They also reduce computational overhead and work well in stable, low-noise environments [16]. However, DDPG faces challenges such as instability and overestimation bias [11]. Twin Delayed Deep Deterministic Policy Gradient (TD3) [17] addresses these issues. It uses twin critics to reduce overestimation and delayed policy updates to improve stability. These improvements make TD3 an effective choice for UAV-based AM tasks that require precise and reliable control under varying conditions [18].\nDespite advancements, applying DRL to UAV-based AM faces several practical issues. These include improving policy stability during training, adapting to dynamic task complexity, and ensuring reliable performance under changing operational conditions [19]. Curriculum learning helps address these issues by using a gradual training strategy. It starts with simple navigation tasks and incrementally introduces"}, {"title": "II. PROBLEM FORMULATION AND PRELIMINARIES", "content": "This study focuses on developing a precise and stable UAV control system for AM in a structured environment. The UAV operates in a three-dimensional space governed by nonlinear dynamics, incorporating thrust forces, gravitational effects, and external disturbances. The mathematical formulation of the UAV's motion dynamics follows standard multirotor modeling, as detailed in [23].\nThe control objective is to enable accurate waypoint navigation while adapting to system variations. To simplify the problem, waypoints are predefined in a fixed sequence, eliminating the need for path planning and allowing a focus on control adaptation. Material deposition is modeled by introducing a reaction force in the negative z-direction to simulate the UAV counteracting deposition forces. The force exerted on the UAV due to material deposition is given by\n$F = \\dot{m}v_{exit}$,\nwhere $\\dot{m}$ is the mass flow rate of the extruded material and $v_{exit}$ is the velocity of material leaving the nozzle. Based on the material properties reported in [24], with a density of"}, {"title": "III. PROPOSED APPROACH", "content": "The proposed approach develops a DRL-based control system for UAV navigation in AM applications. The methodology is organized into two main phases:\nTwo off-policy DRL algorithms, DDPG and TD3, are evaluated for their performance in continuous action spaces. DDPG, while effective for high-dimensional tasks, is prone to overestimation bias, which TD3 mitigates using twin critics, target policy smoothing, and delayed updates. Both algorithms are tested in a simplified 3D simulation, where the UAV navigates between fixed waypoints. Metrics such as accuracy, precision, and success ratio are used for performance evaluation.\nNext, curriculum learning is used to gradually increase task complexity. The agent starts with simple waypoint navigation in static environments and progresses to dynamic waypoints, variable payloads, and external disturbances. This staged approach stabilizes training, accelerates convergence, and ensures the UAV learns robust and adaptive control strategies for AM applications."}, {"title": "A. System Architecture", "content": "The environment is modeled as an MDP, characterized by state and action spaces. An MDP is defined by a tuple (S, A, P, R, \u03b3), where S denotes the state space, A represents the action space, $P : S \\times A \\times S \\rightarrow [0,1]$ specifies the transition probability function $P(s'|s, a)$, indicating the likelihood of transitioning from the current state s to a new state s' upon executing action a; \u03b3 is a discount factor within the range (0, 1), and $R : S \\times A$ defines the reward function. The objective is to maximize cumulative rewards:\n$\\underset{\\pi}{max} E [\\sum_{t=0}^T \\gamma^t R_t]$,\nwhere \u03b3 is the discount factor, and $R_t$ represents the reward at time t, designed to ensure stability and precise trajectory tracking.\nTo enhance training efficiency and improve policy robustness, curriculum learning is implemented to gradually introduce complexity. The agent begins with basic navigation tasks and progressively encounters more challenging scenarios, including dynamic waypoints, variable mass distributions, and external disturbances. The task complexity at time step t, represented as C(t), which evolves as C(t) \u2208 {$C_1, C_2,..., C_n$}. The curriculum-based objective is then defined as:\n$\\underset{\\pi}{max} E [\\sum_{t=0}^T R(s_t, a_t, C(t))]$\nwhere C(t) adjusts task complexity throughout training, ensuring the UAV progressively learns to handle increasingly difficult conditions while maintaining stability.\nThe state representation for the UAV in the proposed control framework is defined by a state vector $s_t$, which encapsulates key physical and positional parameters critical for navigation and control. This vector is expressed as:\n$s_t = [a_{x,t}, a_{y,t}, a_{z,t}, v_{x,t}, v_{y,t}, v_{z,t}, \\Delta x_t, \\Delta y_t, \\Delta z_t, \\theta_{\\tau}, \\phi_{\\tau}, \\psi_{\\tau}, z_t]^T$,\nwhere $a_{x,t}, a_{y,t}, a_{z,t}$ are the drone's acceleration along the x-, y-, and z-axes, respectively. $ \\Delta x_t, \\Delta y_t, \\Delta z_t$ are the positional differences between the drone's current position and the target position along the x-, y-, and z-axes. $v_{x,t}, v_{y,t}, v_{z,t}$ are the velocity components of the drone in the x-, y-, and z-axes. $ \\theta_t, \\phi_t$, and $ \\psi_t$ are the roll, pitch, and yaw angles, representing the drone's orientation around the x-, y-, and z-axes. $z_t$ is the current height of the drone.\nTo ensure consistent scaling and enhance the stability of the RL agent's policy updates, each observation variable in $s_t$ is normalized using Z-score normalization:\n$s_t = \\frac{s_t - \\mu}{\\sigma}$,\nwhere $ \\mu$ is the mean of each state variable and $ \\sigma$ is the standard deviation of each state variable."}, {"title": "B. Algorithm Evaluation", "content": "Two state-of-the-art off-policy DRL algorithms DDPG and TD3 are evaluated for their effectiveness in handling UAV navigation. Each algorithm is tested in simplified environments to determine baseline performance in waypoint navigation. For more information on the DDPG and TD3 algorithms, see [27], [28]."}, {"title": "C. Curriculum Learning for Complexity Management", "content": "To improve the robustness and adaptability of DRL agents, curriculum learning is employed to progressively increase task complexity. The task difficulty at time step t, represented as C'(t), transitions through predefined levels $C_1, C_2,..., C_n$, allowing the agent to build foundational skills before addressing more advanced challenges.\nStages of curriculum learning include:\n\u2022 Basic Navigation ($C_1$): The agent trains with static waypoints in a controlled environment, optimizing a reward function:\n$R(s, a) = - ||P_{target} - P_{current}||^2$,\nwhere $P_{target}$ and $P_{current}$ denote the target and current positions, respectively.\n\u2022 Dynamic Waypoints ($C_2$): Randomized start and target positions simulate adaptive path planning, with an added reward for waypoint completion.\n\u2022 Mass Variability ($C_3$): Variable drone mass emulates material deposition, testing the agent's adaptation to center-of-gravity shifts.\n\u2022 External Disturbances ($C_4$): Wind and noise test resilience, with penalties for deviations due to disturbances."}, {"title": "IV. EXPERIMENTS", "content": "This paper employs MATLAB\u2019s Simulink platform, along with the RL Toolbox and UAV Toolbox, to develop a simulation environment for a drone engaged in AM. The Simulink setup includes essential blocks such as the RL agent, observation, reward, and termination blocks, designed to enable real-time adaptive learning for stable control. The architecture, shown in Fig 2, outlines the interconnected components that drive the simulation and learning processes.\nThe Simulink environment functions in a continuous loop. At each step, the RL agent processes the latest observations to generate control actions. These actions are scaled, with roll and pitch constrained between $-\\pi$ and $\\pi$, and thrust limited between 0 and 10. The scaled actions are then processed"}, {"title": "V. RESULTS", "content": "We conducted a comparative analysis of two RL algorithms, TD3 and DDPG, by training each agent on a target-reaching task. The objective was to navigate a drone to a specified target position. Evaluation metrics included average cumulative reward, standard deviation of rewards, average positional error measured in meters, precision (standard deviation of positional error), and success ratio across 100 test trials."}, {"title": "A. Performance on Target-Reaching Task", "content": "The performance of TD3 and DDPG was analyzed using their training progress, as shown in Fig. 3, where Agent 1 represents TD3 and Agent 2 represents DDPG. The TD3 agent outperformed DDPG, achieving a higher average cumulative reward, lower variance, and greater consistency in"}, {"title": "B. Waypoint Navigation Task", "content": "To evaluate performance in more complex scenarios, the TD3 agent was tested on a waypoint navigation task involving multiple sequential targets. Without curriculum learning, TD3 achieved limited success, with lower cumulative rewards and success ratios. However, introducing curriculum learning, which progressively increases task complexity, significantly improved the agent's performance."}, {"title": "C. Adaptation to Dynamic Mass Changes", "content": "To address challenges arising from dynamic mass changes, such as those encountered during AM, the TD3 agent was adapted to account for variations in drone mass. Initially, the curriculum-trained agent achieved an average cumulative reward of 450 and a success ratio of only 26% under such conditions.\nTo enhance adaptability, the observation space was expanded to include the drone's accelerations in the x, y and z directions. This modification allowed the agent to predict thrust commands corresponding to changes in mass, leveraging the relationship between thrust, mass and acceleration.\nRetraining the agent with the expanded observation space led to significant improvements. The average cumulative reward increased to 760 and the success ratio rose to 94% with a reduced average positional error of 0.09857m and a better precision of 0.0626."}, {"title": "D. Summary of Agent Performance", "content": "Table IV summarizes the training and testing results for all agents, highlighting their performance across various tasks. The TD3 agent demonstrated clear superiority in baseline tasks and in complex scenarios involving curriculum learning"}, {"title": "VI. CONCLUSION", "content": "This study evaluated the performance of two RL algorithms, DDPG and TD3, for UAV control in additive manufacturing tasks. Results showed that TD3 outperformed DDPG in key metrics, including cumulative reward, average positional error, precision, and success ratio, confirming its suitability for precise and consistent target-reaching tasks.\nThe integration of curriculum learning further enhanced TD3's performance, enabling the agent to handle complex waypoint navigation scenarios. Curriculum-trained TD3 achieved higher rewards, greater success ratios, and improved accuracy compared to its non-curriculum counterpart. Additionally, adapting the observation space to include acceleration data allowed TD3 to handle dynamic mass variations effectively, achieving a cumulative reward of 760 and a success ratio of 94%."}, {"title": ""}]}