{"title": "Understanding and Supporting Formal Email Exchange by Answering Al-Generated Questions", "authors": ["Yusuke Miura", "Chi-Lan Yang", "Masaki Kuribayashi", "Keigo Matsumoto", "Hideaki Kuzuoka", "Shigeo Morishima"], "abstract": "Replying to formal emails is time-consuming and cognitively demanding, as it requires crafting polite phrasing and providing an adequate response to the sender's demands. Although systems with Large Language Models (LLMs) were designed to simplify the email replying process, users still need to provide detailed prompts to obtain the expected output. Therefore, we proposed and evaluated an LLM-powered question-and-answer (QA)-based approach for users to reply to emails by answering a set of simple and short questions generated from the incoming email. We developed a prototype system, ResQ, and conducted controlled and field experiments with 12 and 8 participants. Our results demonstrated that the QA-based approach improves the efficiency of replying to emails and reduces workload while maintaining email quality, compared to a conventional prompt-based approach that requires users to craft appropriate prompts to obtain email drafts. We discuss how the QA-based approach influences the email reply process and interpersonal relationship dynamics, as well as the opportunities and challenges associated with using a QA-based approach in AI-mediated communication.", "sections": [{"title": "1 Introduction", "content": "Email is a common tool for users to share information [59, 73] and manage tasks [7]. It has been found that users spend an average of 28% of their workweek reading and replying to emails [38]. However, for many, checking and responding to emails is time-consuming and cognitively demanding [50]. Responding to emails, especially in cultures that value courteous email exchanges, requires users to understand the sender's requests and compose polite messages that reflect the sender's intentions. This involves considering various elements such as tone, style, diction, and structure [18]. Putting effort into constructing courteous emails and responding promptly is important because the lack of these elements can result"}, {"title": "2 Related Work", "content": "Here, we first describe existing AIMC systems designed to support email writing and highlight their limitations. Second, we review QA-based approaches in goal-oriented tasks and their potential to assist with composing email replies. Finally, we examine the psychological and relational impacts of AIMC tools, focusing on the dynamics of recipient-sender relationships and the sender's self-perception."}, {"title": "2.1 AI-Mediated Communication Systems for Supporting Writing Emails", "content": "Various machine learning-based approaches have been employed to enhance the productivity of writers. Earlier writing assistants had modest AI intervention, primarily providing short or single-word suggestions [25, 27, 36, 62] and basic grammar correction [34]. With the advancement of LLMs [16], which allows users to obtain the long and natural-from text output by manipulating prompts, AIMC tools could enhance user input efficiency by suggesting more useful long-form text [20, 29]. Several AIMC tools allow users to select preferred tone, style, length [6, 30, 32, 34], as well as specific content options [6, 34], such as \u201cdecline politely\" or \"ask a follow-up question\", without manual crafting of prompt. However, as these tools only offer simple suggestions, in situations where complex and polite replies are necessary (e.g., exchanging workplace emails with colleagues), users still need to carefully craft their prompts, which can impose a high workload. Crafting effective prompts for replying to emails requires prompt engineering skills and can be a challenging task [75]. When the initial output does not meet expectations, users may need to create prompts multiple times [30], resulting in negative user satisfaction and task engagement. To address this, we propose replacing open-ended prompt creation with an LLM-powered QA-based approach, where the system leads the user through a structured question-and-answer process, effectively performing prompt engineering behind the scenes. This method aims to reduce users' cognitive load and reliance on prompt expertise while maintaining the quality and personalization of the email reply."}, {"title": "2.2 QA-based Approaches in Goal-Oriented Tasks", "content": "Answering questions is one of the effective approaches for users to clarify their needs [33, 43]. For example, Kim et al. [43] designed a workbook using questions that guide users to organize their"}, {"title": "2.3 The Psychological and Relational Impact of AIMC Tools", "content": "Previous research has extensively discussed the impact of AIMC tools on the psychological aspects of both recipients and senders, as well as on their relationships. These effects can be broadly categorized into two areas: (1) The impact on the recipient and sender relationships (2) The impact on the sender's self-perception."}, {"title": "2.3.1 Impact on Recipient and Sender Relationships", "content": "Emails can influence recipient-sender relationships through their content, speed, and context.\nRegarding the content of the email, Robertson et al. [65] identified three key elements of email content that, when missing, negatively impact the sender's perception. The first element, structural features, refers to whether the email includes structural components such as greetings, signatures, and closings. The second element, personal authenticity, measures the extent to which the suggested email aligns with the user's personal tone. The third element, semantic and tone coherence, concerns whether the proposed email reflects the user's intent and the broader context of the communication. Failure to meet these criteria can lead to confusion on the recipient's part and might reveal or raise suspicions about the use of AI, ultimately damaging the sender's impression [37, 39, 49]. Therefore, we see the QA-based approach has the potential to address these issues by preserving the three key components of email while reflecting the user's intent."}, {"title": "2.3.2 Impact on Sender's Self-Perception", "content": "Previous research indicates that significant AI intervention, with minimal operator input, tends to reduce people's sense of agency [29, 52] and control [13, 22, 45]. The agency is often characterized by action initiation [54] and determination [5], both by the user. Sankaran et al.[66] identified factors critical to maintaining people's agency, such as considering user preferences and allowing decision-making. For instance, tasks like creating prompts and revising AI output have been found to foster a sense of accomplishment in users [45]. However, in a QA-based approach, it is still unclear whether users have an increased or decreased sense of agency and control. Thus, this study examines how the QA-based approach affects users' sense of agency and control and how they perceive the trade-off between these feelings and the system's benefits to better understand the approach's advantages and risks."}, {"title": "3 Research Questions and Hypotheses", "content": "This paper aims to explore the effectiveness and potential risks of a QA-based response-writing support method by addressing the following three research questions:\nRQ1: How does a QA-based response-writing support approach affect users' email-replying process?\nRQ2: How does a QA-based response-writing support approach affect the quality of the email response?\nRQ3: How does a QA-based response-writing support approach affect the perceived relationship between email sender and recipient?\nTo answer the three research questions, we formed three sets of hypotheses. The first set of hypotheses investigates the impact of a QA-based system on users' email-replying process. AI-powered text generation reduces user input, saves time, and enhances efficiency [6]. It also helps users quickly grasp email content with"}, {"title": "4 Proposed LLM-Powered QA-Based Approach: ResQ", "content": "This section describes the proposed approach, ResQ, for supporting email response tasks. Fig. 2 illustrates the overview of how a reply message is created using ResQ. Fig. 3 shows the actual interface of ResQ. The following sections describe the specific functions involved in each step of this process.\nA: Generate Questions. When a user first activates ResQ, the system uses an LLM (in this study, GPT-40 [61]) to generate multiple-choice questions (Fig. 3-C). The LLM extracts all parts of the email that require a reply, generates corresponding questions and presents possible response options. Additionally, if a user clicks on any generated question, the relevant part of the email is highlighted (Fig. 3-A). Following the approach described in [12], we designed a structured prompt that guides the LLM to determine how many questions are necessary and sufficient to cover all requirements in the incoming email without omission. Instead of pre-specifying a fixed number of questions, the prompt instructs the model to produce an \"appropriate\" number of questions, where \u201cappropriate\u201d is defined as the minimal set of questions needed to address all points raised by the sender while avoiding redundant or irrelevant inquiries. To ensure that the questions were generated systematically rather than randomly, we provided explicit criteria within the prompt. These criteria included referencing the sender's intent, quoting relevant portions of the original email verbatim, and offering multiple-choice options where applicable. We also provided concrete examples within the prompt to illustrate the desired format and style of the generated questions and corresponding answer choices. By doing so, we ensured that the LLM's output was both well-grounded and easy for the recipient to answer. The detailed prompt used to guide the LLM in this process is shown in the appendix.\nB: Answer Questions. Next, users view the incoming email (Fig. 3-B) alongside the generated questions (Fig. 3-C) and options and proceed to answer them. In anticipation of situations where none of the provided options are useful, we enabled users to add their own options (Fig. 3-D). Additionally, to help the LLM better understand the context of the email, we introduced a box where users can specify the relationship between the sender and the recipient (Fig. 3-E, top). Furthermore, following previous research [30], we provided users with controls to adjust the tone, style, and length of the reply to match their preferences better, thereby giving them more flexibility in customizing the AI-generated response (Fig. 3-E, middle). A free-text field was also included to allow users to make other specific AI requests (Fig. 3-E, bottom). After completing these steps, users can click the \"Generate Reply\" button (Fig. 3-F).\nC: Generate Reply Draft. When the user clicks the \"Generate Reply\" button, ResQ detects the action and uses the LLM to generate a reply draft. The prompt used for this function is shown in the appendix.\nD: Review Reply Draft. Once the draft reply is generated, users can review the draft in detail (Fig. 3-G). Moreover, if users find that extensive revisions are needed or if they want to explore alternative phrasing, they have the option to request the AI to regenerate a"}, {"title": "5 Method of Study 1", "content": "To test our hypotheses and answer three research questions, we first conducted a controlled experiment, focusing on gaining a quantitative understanding."}, {"title": "5.1 Experiment Design", "content": "Study 1 was designed to quantitatively assess how ResQ influences the writing process (RQ1), the quality of email replies (RQ2), and the perceived relationships with others (RQ3) compared to scenarios without Al intervention and when using traditional AIMC tools. The experiment targeted Japanese participants and was conducted entirely in Japanese. This experimental context was designed as communication in formal settings, such as office-related communication, research collaboration in academic institutions, and interactions with external organizations. It focused on time-consuming emails that were characterized as lengthy, containing multiple requests, or requiring detailed and polite responses. Simple and straightforward emails, such as those that can be answered with a single word or phrase (e.g., \"Understood\"), were excluded from the scope.\nParticipants were assigned the role of message recipients and required to craft replies based on the scenarios and supplementary information provided. The messages used in the experiment were collected from 10 volunteers who provided real emails they had received in formal communication contexts. These volunteers included office workers, graduate students, and teaching staff, all of whom were Japanese and engaged in email communication regularly (at least once per month). To ensure anonymity, identifying details were removed during the preparation process. Based on the design principles of ResQ, we excluded extremely short emails and emails that could be replied to with a single word from the selection process. Each scenario included details about the sender, the recipient, and the context in which the message was received. Multiple scenarios were included in the experiment to minimize the influence of any single scenario and increase the variety. Additionally, supplementary information, such as the recipient's schedule and potential questions, was provided to prevent excessive variability in the responses among participants.\nIn total, we created twenty types of emails, with two assigned to the practice session and eighteen to the test session. The length of the emails used in the test session averaged 404 Japanese characters, with the shortest being 135 characters and the longest being 925 characters. The scenarios covered a wide range of formal communication situations, including responding to a request for data submission in the workplace, answering a survey from a professor, asking questions based on guidance from a language school's customer support team, and addressing a request for schedule adjustments as a part-time worker. Additionally, these emails varied in structure, ranging from structured formats with bullet points to more non-structured, free-text formats. The specific emails and examples of ResQ-generated questions and options used in the experiment can be referred to in the supplementary materials."}, {"title": "5.2 Experimental Conditions", "content": "We employed a within-subject design with three conditions: QA-based, Prompt-based, and No-AI. This design was chosen to control for individual differences among participants, such as varying levels of language proficiency or familiarity with Al systems, ensuring a fair comparison across conditions. To illustrate each condition, consider the scenario of a participant who, as an employee of a company, is asked by their superiors to assume the role of a fixed asset committee member.\nIn the QA-based condition, participants created replies using the QA-based AI. The system detected when participants navigated to the next email screen, inferred that they were initiating a reply, and then generated relevant questions. For example, the system might ask, \"Would you be willing to take on the role of the fixed asset manager?\" \"Is there any issue with handling the annual inventory"}, {"title": "5.3 Participants", "content": "Twelve participants (six males and six females, aged 20-57) were recruited via a local Japanese participant recruiting platform (see Tab. 1). The average age of the participants was 29.6 (SD = 11.0). The sample size n = 12 was determined based on an a priori power analysis (effect size f = 0.4, significance level p = 0.05, power = 0.8, correlation among repeated measures = 0.5) as well as the previous study [56]. The participants were paid approximately $21 USD for participation, and the experiment lasted around two hours. This study was approved by the institute's ethical review board."}, {"title": "5.4 Procedure", "content": "The participants first read the study instructions and the right to participate and then consented to participate in the experiment. Next, they were given an explanation of the purpose of the experiment and the use of the AI systems (Prompt-based and QA-based systems). Participants were then randomly assigned to reply to six emails per condition using a Latin square design, which counterbalanced the order of conditions and mitigated potential order effects 2. In each condition, participants first engaged in a practice session where they read and replied to two emails to familiarize themselves with the system. Then, they read and replied to six emails, which were presented in a randomly assigned order to further reduce any sequence-related biases. After replying to six emails for each condition, participants were asked to complete a questionnaire regarding their experience with the task. To ensure participants could manage their workload during the study, they were allowed to take a short break after completing tasks in each condition. After completing all conditions, they were asked to fill out a comparative questionnaire evaluating the three conditions. In addition, follow-up interviews were conducted to gather deeper insights into their experiences and preferences. This study was conducted remotely for all participants and lasted approximately two and a half hours in total."}, {"title": "5.5 Evaluation Session", "content": "After completing the main experiment, we conducted an additional evaluation session to assess the quality of the email responses created by participants and the impressions of participants as email senders. This session involved a group of eighteen Japanese evaluators (ten males and eight females, aged 20-57) recruited via a local participant recruiting platform 3. The average age of the evaluators was 40.6 (SD = 8.3). The evaluators had a minimum of five years and an average of 18.7 years of experience in email-based communication. Furthermore, with the exception of one individual, the evaluators engaged in email-based communication at least once a month. Each evaluator assessed email replies written by twelve different participants for a specific scenario. The evaluators were paid"}, {"title": "5.6 Measurements", "content": "We used multiple measurements to test our hypotheses. From participants' behavior during the email reply task, we calculated two measures: efficiency and prompt character count. From their post-experiment questionnaire responses, we evaluated cognitive load, difficulty in understanding email content, satisfaction with completing the task, difficulty in initiating the action for replying to emails, sense of agency, sense of control, and psychological distance between participants and their counterparts. Additionally, from evaluators' questionnaire responses during the evaluation session, we assessed the perceived quality of the email and the impression of participants as email senders."}, {"title": "5.6.1 Efficiency of Replying to Emails (H1-a)", "content": "We calculated the efficiency of replying to emails using task completion time and total character count. The efficiency of replying to emails is defined as the amount of text contributing to the final output that can be typed per second, where a higher score indicates better task efficiency. For task completion time, we recorded the time participants took to reply to an email, starting from when the email appeared on the screen to when the participant pressed the send button. For total character count, we considered the text in the reply box when the participant pressed the Reply button as the final response and counted its characters."}, {"title": "5.6.2 Prompt Character Counts (H1-a)", "content": "We also calculated the average number of characters typed by participants to have the AI generate email drafts as the prompt character counts in each condition. Under the Prompt-based condition, we measured the number of characters participants typed in the free-text field for the AI. Under the QA-based condition, the prompt character counts included this number plus any additional characters typed by the participants when they added their own options."}, {"title": "5.6.3 Cognitive Load for Replying to Emails (H1-b)", "content": "We used the NASA-TLX [35] questionnaire to measure cognitive load across six"}, {"title": "5.6.4 Difficulty in Understanding Email Content (H1-b)", "content": "Additionally, to assess cognitive load specifically related to understanding received emails, we used a 7-point Likert scale. Participants rated their agreement with the statement, \"I found it difficult to understand the sender's intentions or requests in the email,\" where 1 indicates strongly disagree, 4 indicates neutral, and 7 indicates strongly agree."}, {"title": "5.6.5 Satisfaction with Completing Task (H1-c)", "content": "We evaluated participants' satisfaction with completing their task using a 7-point Likert scale, where 1 indicates strongly disagree, 4 indicates neutral, and 7 indicates strongly agree. Specifically, the satisfaction of completing their task was evaluated based on their satisfaction with efficiency and their satisfaction with the quality of the email they created. We asked the following questions: (1) I felt that I was able to create a high-quality response. (2) I felt that I was able to complete the response efficiently. We averaged the scores from two items and treated them as an index of the satisfaction with completing their task."}, {"title": "5.6.6 Difficulty in Initiating the Action for Replying to Emails (H1-d)", "content": "We tested H1-d using a survey with a 7-point Likert scale (1 = strongly disagree, 4 = neutral, and 7 = strongly agree) to evaluate perceived barriers to task initiation. Specifically, we asked the following question: I felt a high barrier to initiating email response tasks."}, {"title": "5.6.7 Sense of Agency and Control (H1-e)", "content": "We evaluated participants' perceived sense of agency and control using a 7-point Likert scale, where 1 indicates strongly disagree, 4 indicates neutral, and 7 indicates strongly agree. Specifically, drawing on previous research [22, 29], the sense of agency was evaluated by assessing whether participants felt they were the ones who wrote the responses, while the sense of control was evaluated by whether they felt they had control over the content of the responses."}, {"title": "5.6.8 Perceived Quality of the Email by Evaluators (H2)", "content": "The quality of each email reply was evaluated by evaluators using a 7-point Likert scale, where 1 indicates strongly disagree, 4 indicates neutral, and 7 indicates strongly agree. It was evaluated on three aspects: politeness (whether it was politely written), readability (whether it had an easy-to-understand structure), and meeting demands (whether it appropriately addressed the recipient's demands). We averaged the scores from three items and treated it as an index of the perceived quality of the email."}, {"title": "5.6.9 Perceived Impression of Participants by Evaluators (H3-a)", "content": "Following a previous study [63], we asked the evaluators to read the email and assess their impressions of the senders (participants) based on two aspects: whether the participants were perceived as likable and whether they were perceived as kind, using a 7-point Likert scale, where 1 indicates strongly disagree, 4 indicates neutral, and 7 indicates strongly agree. We averaged the scores from these two items to create an index of the impression of the email sender."}, {"title": "5.6.10 Psychological Distance between Participants and Their Counterpart (H3-b)", "content": "We evaluated the perceived psychological distance using the Inclusion of Other in the Self (IOS) scale [4]. Participants choose a pair of circles from seven with different degrees of overlap (see Fig. 4). 1 = no overlap; 2 = little overlap; 3 = some overlap; 4 = equal overlap; 5 = strong overlap; 6 = very strong overlap; 7 = most overlap. The number chosen is the participants' score. The higher the score was, the closer participants felt they were with the email sender."}, {"title": "6 Results of Study 1", "content": "Here, we first present the quantitative results of Study 1 for each research question. Subsequently, we include comments provided by the participants."}, {"title": "6.1 Participants' Email-Replying Process (RQ1)", "content": "First, we compared the efficiency of replying to emails across three conditions. After checking the data normality assumption with the Shapiro-Wilk test, the result of one-way repeated measures ANOVA showed that there was a significant difference in participants' efficiency of replying to emails across three conditions (F[2,22] = 14.8, p < 0.001, \u03b7\u03b2 = 0.57). Post-hoc analysis with Holm correction revealed that participants' efficiency of replying to emails in the QA-based condition was significantly higher compared to both the No-AI (t(11), p = 0.002, d = 1.38) and the Prompt-based (t(11), p = 0.046, d = 0.65) conditions. Thus, H1-a was supported. The QA-based approach enhanced participants' email replying efficiency."}, {"title": "6.1.2 Prompt Character Counts (H1-a)", "content": "In order to understand how participants wrote prompts differently, we calculated the prompt character counts. After the Shapiro-Wilk test, the paired t-test revealed that participants in the QA-based condition typed significantly fewer characters in their prompts than those in the Prompt-based condition (t(11), p = 0.010, d = 0.90)."}, {"title": "6.1.3 Cognitive Load for Replying to Emails (H1-b)", "content": "The results of the Raw-TLX are shown in Fig. 5. According to the one-way repeated measures ANOVA with Greenhouse-Geisser correction, there was a significant difference in participants' cognitive load for replying to emails among the three conditions (F[1.1, 12.1] = 12.6, p = 0.003, \u03b7 = 0.53). Post-hoc analysis with Holm correction revealed that participants' cognitive load for replying to emails in the QA-based condition was significantly lower compared to both the No-AI (t(11), p = 0.008, d = 1.12) and Prompt-based (t(11), p = 0.018, d = 0.81) conditions. Therefore, H1-b was supported. The QA-based approach reduced participants' cognitive workload while replying to the emails."}, {"title": "6.1.4 Difficulty in Understanding Email Content (H1-b)", "content": "Additionally, H1-b was also supported by the questionnaire survey results (Fig. 6 H1-b). The Friedman test revealed a significant difference among the three conditions in terms of understanding the sender's intent and requests (x\u00b2(2) = 10.6, p = 0.005, W = 0.44). Post-hoc analysis using the Durbin-Conover test with Holm correction showed that participants in the QA-based condition found it significantly easier to understand the sender's intent and requests"}, {"title": "6.1.5 Satisfaction with Completing Task (H1-c)", "content": "The results of the satisfaction with completing participants' tasks are shown in Fig. 6, H1-c. The two items measuring satisfaction showed high internal consistency, with a Cronbach's Alpha of 0.889. After checking the data normality assumption with the Shapiro-Wilk test, the result of one-way repeated measures ANOVA showed that there was a significant difference in participants' satisfaction with completing tasks across three conditions (F[2, 22], p < 0.001, \u03b7\u00b2 = 0.79). Post-hoc analysis with Holm correction revealed that participants' satisfaction with completing tasks in the QA-based condition was significantly higher compared to both the No-AI (t(11), p < 0.001, d = 2.39) and the Prompt-based (t(11), p = 0.029, d = 0.72) conditions. Therefore, H1-c was supported. The QA-based approach"}, {"title": "6.1.6 Difficulty in Initiating the Action for Replying to Emails (H1-d)", "content": "The questionnaire survey results about participants' difficulty in initiating the action for replying to emails are shown in Fig. 6, in H1-d. According to the Friedman test, a significant difference in participants' difficulty in initiating the action for replying to emails was observed among the three conditions (x\u00b2(2) = 19.8, p < 0.001, W = 0.83). Post-hoc analysis using the Durbin-Conover test with Holm correction revealed that participants in the QA-based condition perceived significantly higher barriers to initiating email response tasks than those in the No-AI (p < 0.001, r = 0.85) and Prompt-based (p < 0.001, r = 0.68) conditions. Therefore, H1-d was supported. The QA-based approach reduced participants' difficulty in initiating the action to reply to emails."}, {"title": "6.1.7 Sense of Agency and Control (H1-e)", "content": "The questionnaire survey results about a sense of agency and control are shown in Fig. 6, H1-e. The Friedman test revealed a significant difference among the three conditions for both the sense of agency (x\u00b2(2) = 22.8, p < 0.001, W = 0.95) and the sense of control (x\u00b2(2) = 21.3, p < 0.001, W = 0.89). Post-hoc analysis using the Durbin-Conover test with Holm correction showed that participants in the QA-based condition found that it significantly reduced their sense of agency compared to both the No-AI (p < 0.001, r = 0.88) and the Prompt-based (p < 0.001, r = 0.77) conditions. Additionally, post-hoc analysis using the Durbin-Conover test with Holm correction showed that participants in the QA-based condition experienced a significantly reduction in their sense of control compared to both the No-AI (p < 0.001, r = 0.88) and the Prompt-based (p = 0.006, r = 0.56) conditions. Thus, H1-e was supported. The QA-based approach reduced participants' sense of agency and sense of control while replying to the emails."}, {"title": "6.2 Quality of the Email Responses (RQ2)", "content": "In Fig. 6, H2 shows the results regarding the quality of the emails. The Cronbach's Alpha of three items measuring the perceived quality of the email is 0.846. After checking the data normality assumption with the Shapiro-Wilk test, the result of one-way repeated measures ANOVA showed that there was a significant difference in the perceived quality of the email across three conditions (F[2, 22] = 9.1, p = 0.001, \u03b7\u00b2 = 0.45). Post-hoc analysis with Holm correction revealed that the perceived quality of the emails participants wrote in the QA-based condition was significantly higher compared to the No-AI (t(11), p = 0.005, d = 1.21) condition. Thus, H2 was partially supported. The QA-based approach improved the quality of the email responses compared to the No-AI condition.\nTab. 2 shows the detailed results regarding the perceived quality of the emails across three evaluation dimensions (politeness, readability, and meeting demands). These results further supported the"}, {"title": "6.3 Relationship between Participants and Their Counterpart (RQ3)", "content": "The results of the perceived impression of the participants rated by another group of evaluators are shown in Fig. 6 H3-a. The two items assessing participants' impression as email senders showed high internal consistency, with a Cronbach's Alpha of 0.946. After checking the data normality assumption with the Shapiro-Wilk test, the result of one-way repeated measures ANOVA showed that there was a significant difference in impression of the participants as an email sender across three conditions (F[2, 22] = 5.9, p = 0.009, \u03b7 = 0.35). Post-hoc analysis with Holm correction revealed that participants' impression in the QA-based condition was not significantly higher compared to both the No-AI (t(11), p = 0.058, d = 0.79) and the Prompt-based (t(11), p = 0.939, d = 0.02) conditions. Thus, H3-a was not supported. The QA-based approach didn't improve the impression of participants as email senders."}, {"title": "6.3.2 Psychological Distance between Participants and Their Counterpart (H3-b)", "content": "The IOS result is shown in Fig. 6. The Friedman test showed a significant difference in psychological distance among the three conditions (x\u00b2(2) = 7.47, p = 0.024, W = 0.31). Post-hoc analysis using the Durbin-Conover test with Holm correction revealed that IOS in the No-AI condition was significantly higher than in the QA-based condition (p = 0.021, r = 0.51). This result partially supports H3-b; however, because there was no significant difference between the Prompt-based and QA-based conditions (p = 0.053, r = 0.30), H3-b was partially supported."}, {"title": "6.4 Qualitative Feedback", "content": "This section synthesizes qualitative feedback to provide further insights into participants' experiences across three conditions. The interview comments were translated from Japanese into English."}, {"title": "6.4.1 Participants' Email-Replying Process (RQ1)", "content": "Feedback from participants confirmed that the QA-based condition functioned as expected, contributing to improvements in efficiency, a reduction in workload, and a lowering of barriers to task initiation compared to the other conditions.\n\"In the QA-based condition, Al summarized key points through questions and highlighted relevant sections of the email body, which facilitated my understanding of the email and reduced my overall burden\" [P10].\n\"In the QA-based condition, I could easily obtain the desired output even without the technical skills to create prompts\" [P6]."}, {"title": "6.4.2 Quality of the Email Responses (RQ2)", "content": "All participants stated that using Al improved their writing structure, politeness, and choice of words, ultimately enabling them to produce better overall responses. Furthermore, participants remarked, \"Under the prompt-based condition, I might have overlooked the recipient's requests, but under the QA-based condition, I was able to craft responses with confidence\" [P2]. Additionally, one participant emphasized that \"Under the QA-based condition, the Al even provided polite responses to matters where a reply was optional, such as acknowledging something with phrases like 'I Understood regarding XX, etc.\" [P9], highlighting how the QA-based condition scaffolded user to construct a polite email in a formal setting."}, {"title": "6.4.3 Relationship between Participants and Their Counterpart (RQ3)", "content": "Participants shared differing views on how Al's involvement affected their psychological distance from their counterparts. P2, P9, and P11 reported that the psychological distance they felt from the other person was directly related to the amount of effort they put in. Furthermore, P6 noted that \u201cespecially under QA-based condition, I barely thought about the counterpart because I only selected options to create responses.\" In contrast, P8 reported that \"compared to composing replies myself, using AI allowed me to create messages that left a better impression on my counterpart, which made the relationship feel closer.\" These results suggested that, on the one hand, Al's mediation can potentially increase the psychological distance between senders and receivers. On the other hand, it can also diminish the perceived distance from the sender due to effective impression management. Thus, we conducted a field study to further clarify the impact of AI on interpersonal relationships."}, {}]}