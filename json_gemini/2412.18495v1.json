{"title": "How \"Real\" is Your Real-Time Simultaneous Speech-to-Text Translation System?", "authors": ["Sara Papi", "Peter Pol\u00e1k", "Dominik Mach\u00e1\u010dek", "Ond\u0159ej Bojar"], "abstract": "Simultaneous speech-to-text translation (SimulST) translates source-language speech into target-language text concurrently with the speaker's speech, ensuring low latency for better user comprehension. Despite its intended application to unbounded speech, most research has focused on human pre-segmented speech, simplifying the task and overlooking significant challenges. This narrow focus, coupled with widespread terminological inconsistencies, is limiting the applicability of research outcomes to real-world applications, ultimately hindering progress in the field. Our extensive literature review of 110 papers not only reveals these critical issues in current research but also serves as the foundation for our key contributions. We 1) define the steps and core components of a SimulST system, proposing a standardized terminology and taxonomy; 2) conduct a thorough analysis of community trends, and 3) offer concrete recommendations and future directions to bridge the gaps in existing literature, from evaluation frameworks to system architectures, for advancing the field towards more realistic and effective SimulST solutions.", "sections": [{"title": "Introduction", "content": "The term of \"simultaneous\" was first coined in the field of language interpretation, which is the practice of conveying a speaker's message orally in another language to listeners who would not otherwise understand it.\u00b9 Unlike consecutive interpreting (Paulik and Waibel, 2010; Lv and Liang, 2019), where interpretation occurs after the speaker has finished talking, simultaneous interpreting2 happens concurrently with the speech.3\nApplying this concept to computer science, specifically in automatic translation, simultaneous speech-to-text translation (SimulST) is defined as the process that \u201ctranslates source-language speech into target-language text concurrently\u201d (Ren et al., 2020), meaning that the translation process occurs in parallel with the incremental acquisition of the input speech. Within this context, the real-time aspect, i.e. the \u201cimmediate processing and response to inputs, often within milliseconds to seconds\u201d (Laplante, 1992) and, in general with low latency, is crucial for ensuring the synchronicity between input and output, enhancing user comprehension of the translated content (Bangalore et al., 2012).\nIn F\u00fcgen et al. (2007), the SimulST task has been formalized for the first time and described as the process that takes as input an \"audio stream\", a continuous and unsegmented flow of speech information, and produces the automatic textual translation. Despite this broad definition, the field has since predominantly focused on a much narrower task: translating speech that has been pre-segmented into short utterances of few seconds by humans before translation (Kolss et al., 2008; Cho et al., 2015; Ma et al., 2020b; Zhang et al., 2024, among others), following sentence boundaries. While this approach simplifies the translation process by sidestepping challenges related to audio segmentation (Pol\u00e1k, 2023) and selecting audio-textual context to retain from the past (Papi et al., 2024b), it offers an incomplete and overly simplistic view of the broader challenges inherent"}, {"title": "Background", "content": "Offline speech translation (ST) is the task of translating speech from the source language into text in the target language. Differently from simultaneous ST, which processes input incrementally, offline ST deals with complete and typically well-formed speech segments, representing one or more sentences. This task was the first addressed by the community (Waibel, 2004), and its model architectures have evolved significantly over time. Initially, offline ST was tackled using cascade architectures (Stentiford and Steer, 1988; Waibel et al., 1991), consisting of an automatic speech recognition model (ASR) that transcribes the speech content, followed by a machine translation (MT) model that translates the transcript into the target language. Lately, direct architectures \u2013 first developed as statistical approaches (Casacuberta et al., 2001; Matusov et al., 2006) and, later, as neural-based models (B\u00e9rard et al., 2016; Weiss et al., 2017) - emerged with the promise of overcoming cascade architectures' inherent limitations (Sperber and Paulik, 2020), such as error propagation4 by bypassing intermediate ASR outputs. Although direct architectures initially faced a performance gap compared to cascade models (Niehues et al., 2018a, 2019), their effectiveness has been steadily improving (Bentivogli et al., 2021), with an increasing number of works adopting this paradigm, as highlighted in the survey by Latif et al. (2023).\nMost contemporary neural systems for speech processing, both cascade and direct models, are primarily designed to handle short utterances due to inherent memory and modeling limitations (Dai et al., 2019; Chiu et al., 2019). To address this, the common approach has been to segment speech into smaller chunks before feeding it into the model. Ever since the early SimulST systems (e.g., Woszczyna et al., 1998; F\u00fcgen et al., 2006b, 2007), audio segmentation has been a natural part of the pipeline in practical settings. In cascaded systems, a typical method for segmentation involves introducing punctuation into the ASR-generated text (Lu and Ng, 2010; Rangarajan Sridhar et al., 2013; Cho et al., 2015, 2017; Iranzo-S\u00e1nchez et al., 2020) and segmenting based on the punctuation obtained for the subsequent steps of the SimulST process. Direct models,"}, {"title": "Long-Form Speech", "content": "Long-form speech refers to long audio segments, such as entire lectures, podcasts, or interviews, where the speech is continuous and unsegmented. In the related field of ASR, handling such inputs typically involves segmenting the audio into smaller segments, commonly using VAD tools to detect pauses or speech boundaries (Atal and Rabiner, 1976; Ferrer et al., 2003; Novitasari et al., 2022). More recent work has introduced approaches where segmentation decisions are embedded directly within the ASR model itself (Yoshimura et al., 2020; Huang et al., 2022). Additionally, some methods employ fixed segmentation with heuristics to stitch segments together, ensuring the continuity of the recognized speech (Chiu et al., 2019; Radford et al., 2023) or explore architectures capable of performing ASR without segmentation, processing the speech in its entirety (Narayanan et al., 2019; Chiu et al., 2019; Lu et al., 2021; Zhang et al., 2023b).\nIn cascaded ST, the challenge extends to MT systems, which have to handle the long texts generated by ASR models. While segmenting long text is usually guided by punctuation and supported by using past sentences as context (Tiede-mann and Scherrer, 2017; Agrawal et al., 2018; Kim et al., 2019; Donato et al., 2021; Fernandes et al., 2021), it becomes challenging when ASR output lacks punctuation. This issue is typically addressed by inserting punctuation (Lu and Ng, 2010; Rangarajan Sridhar et al., 2013; Cho et al., 2017). Recent methods have aimed to completely bypass segmentation, allowing translation models to process continuous text streams and improving translation coherence (Schneider and Waibel, 2020; Iranzo-S\u00e1nchez et al., 2024). In direct ST, research on long-form speech has primarily focused on addressing segmentation challenges. Some studies have integrated previous context to improve translation coherence and quality by mitigating audio segmentation errors (Gaido et al., 2020; Zhang et al., 2021; Ahmad et al., 2024). Recent advances in SimulST suggest the potential to completely eliminate external segmentation, significantly reducing latency and improving translation quality (Pol\u00e1k and Bojar, 2023; Papi et al., 2024b)."}, {"title": "What is Simultaneous Speech-to-Text Translation?", "content": "In this section, we present the first contribution of our work. We begin with the definition of steps characterizing the SimulST process (\u00a73.1), and then provide a unified terminology and taxonomy of the current models developed in the field (\u00a73.2)."}, {"title": "Process Decomposition", "content": "We describe the SimulST as a 6-step process, deriving it from a high-level conceptualization of the task from which system implementations may depart in many ways. We start with audio acquisition and conclude with the translation presentation to the user. Throughout the paper, we assume the processing of clean non-overlapping speech in one language, delivered by a single speaker. We leave aspects such as robustness to background noise (Chen et al., 2022; Hwang et al., 2024), speaker diarization (Park et al., 2022), overlapping speech (Wang et al., 2022a), code-switching (Weller et al., 2022; Huber et al., 2022), and any other issues connected to sound to future work on the topic.\nThe entire process is illustrated in Figure 1 and described as follows:\n1. Audio Acquisition: The speaker speaks to a microphone that is constantly recording, i.e.,"}, {"title": "Terminology and Models' Components", "content": "Considering the process described in \u00a73.1, we define the terminology related to the SimulST task in Table 1. This terminology offers a precise and unified framework for understanding and analyzing SimulST models and will be consistently adopted throughout this paper.\nBuilding on this terminology and considering the common distinctions in the context of speech translation (\u00a72), we classify 110 papers proposing SimulST solutions based on their fundamental components, namely: input (either bounded or unbounded speech), architecture (either direct or cascade), and output strategy (either incremental or re-translation). The papers are collected through Semantic Scholar using relevant keywords, whose details and specific categorization are presented in Appendix A. The resulting taxonomy is visualized in Figure 2.\nBounded vs. Unbounded Input Speech. The input of a SimulST system can be either bounded or unbounded speech, depending on whether the audio has been pre-segmented into sentences in advance (i.e., offline) or not. Bounded speech refers to short audio segments, usually of a few seconds, representing one or more sentences, while unbounded speech refers to long audio segments or streams with an unknown duration (\u00a72.3). When the input is unbounded and the system processes audio streams directly without any segmentation step (without Step 2 in Section 3.1), we categorize it as a segmentation-free system (Iranzo-S\u00e1nchez et al., 2024). In this case, selecting the speech and text history to retain from the past \u2013 stored in the Speech and Text Buffers"}, {"title": "Is it \"Real\" Simultaneous Translation?", "content": "In the following, we analyze and discuss the results obtained by categorizing the papers using the taxonomy depicted in Figure 2 and whose differences are discussed in \u00a73.2.\nThe Terminological Chaos. Although \u201csimultaneous\" is the most widely adopted term by the research community to refer to the concurrent speech-to-text translation task, mentioned in 100 out of 110 papers, it is not the only term used in the literature. Other commonly used synonyms include \"streaming\u201d, \u201conline\u201d, and \u201creal-time\u201d. While \"streaming\" is tied to ASR research, where it indicates a model capable of processing incremental speech inputs with the lowest latency possible (Zhang et al., 2020; Moritz et al., 2020), \"online\" serves to describe the SimulST task as a counterpart to offline speech translation (Ansari et al., 2020; Anastasopoulos et al., 2021, 2022; Agarwal et al., 2023). Instead, \u201creal-time\u201d is frequently misused to indicate a process that guarantees low latency, which is a goal rather than an accurate description of the concurrent translation task itself. We visualize this terminological chaos in Figure 3, which shows that over 65% of the papers mix and match these terms. Specifically, 39 papers use at least one of \u201cstreaming\u201d, \u201conline\u201d, or \u201creal-time\u201d terms (mostly opting for the former two) interchangeably with \u201csimultaneous\" within the same document, 30 papers employ two of the synonyms (preferring \u201cstreaming\" and \"online\" over other combinations), and 3 papers even use all four terms. Moreover, some papers exclusively use \u201creal-time\u201d (1 paper) or \"streaming\" (6 papers) to denote the simultaneous transla-"}, {"title": "Recommendations and Future Directions", "content": "In this section, we outline best practices derived from the analysis in \u00a74 and the recent advances in the field (!), and we highlight key areas where future research is needed to develop more robust, accurate, and efficient SimulST systems capable of meeting real-world demands ().\n\u25b2 Use (at least) Automatic Pre-Segmentation. As discussed in \u00a74, the SimulST community has predominantly relied on using gold segmentation for training and evaluating their systems. Since this represents unrealistic conditions for real-world SimulST applications, we encourage future research in the bounded speech scenario to"}, {"title": "Conclusions", "content": "In this paper, we examined the state of simultaneous speech translation research under several aspects, identifying significant gaps in the existing literature. Our analysis of 110 papers revealed a predominant focus in SimulST on human-segmented speech, which oversimplifies the task and neglects the complexities of real-world applications. We also uncovered substantial terminological inconsistencies, revealing real terminological chaos. To address these issues, we formalized the SimulST task as a 6-step process and introduced a unified terminology to standardize research outcomes. We identified the core components of SimulST systems (input, architecture, and output strategy), discussed current research trends, and provided key recommendations, including transitioning from human to automatic segmentation and adopting consistent terminology. We also emphasized the need for improvement in current evaluation frameworks, highlighting the importance of creating an easy-to-use tool that can handle unbounded speech, incorporating contextual information during translation, and investigating more user-centric assessments to ensure that improvements measured by automatic metrics align with those in the user experience."}]}