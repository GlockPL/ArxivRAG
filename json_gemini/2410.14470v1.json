{"title": "How Do Training Methods Influence the Utilization of Vision Models?", "authors": ["Paul Gavrikov", "Shashank Agnihotri", "Margret Keuper", "Janis Keuper"], "abstract": "Not all learnable parameters (e.g., weights) contribute equally to a neural network's decision function. In fact, entire layers' parameters can sometimes be reset to random values with little to no impact on the model's decisions. We revisit earlier studies that examined how architecture and task complexity influence this phenomenon and ask: is this phenomenon also affected by how we train the model?\nWe conducted experimental evaluations on a diverse set of ImageNet-1k classification models to explore this, keeping the architecture and training data constant but varying the training pipeline. Our findings reveal that the training method strongly influences which layers become critical to the decision function for a given task. For example, improved training regimes and self-supervised training increase the importance of early layers while significantly under-utilizing deeper layers. In contrast, methods such as adversarial training display an opposite trend. Our preliminary results extend previous findings, offering a more nuanced understanding of the inner mechanics of neural networks.", "sections": [{"title": "1 Introduction", "content": "A famous neurology myth often misattributed to Albert Einstein states that humans only use 10%\nof the neural connections in their brains (Radford, 1999). While modern research assumes that\nhumans use all neural connections (Boyd, 2008; Herculano-Houzel, 2009) \u2013 the same cannot be said\nabout artificial neural networks. Quite the contrary, it is well known that trained neural networks\ndo not utilize their entire capacity. This becomes evident through the lens of parameter pruning\n(LeCun et al., 1989b; Hassibi et al., 1993), where (large numbers) of neurons can be removed after\ntraining without affecting performance, or, alternatively, the distillation of large into equivalent\nsmaller networks (Hinton et al., 2015; Hoffmann et al., 2021). Alas, the learned decision function\nonly occupies a fraction of the neural network and the remaining neurons seem to be wasted.\nZhang et al. (2022) showed that this seems to affect layers disproportionally. The learnable parame-\nters\u00b9 of some layers are critical to the decision function and replacing them with any other values than\nthe learned ones (significantly) affects accuracy. In contrast, the performance is barely affected when\nthe parameters in auxiliary layers are randomized. For instance, entire residual blocks of ResNets\n(He et al., 2015) trained on ImageNet (Russakovsky et al., 2015) can be randomized without hurting\naccuracy. Affected layers seem to be dictated by training data size or more generally the complexity\nof the training function in addition to the architecture. Chatterji et al. (2020) have even extended these"}, {"title": "2 Methodology", "content": "Our study aims to assess the contribution of individual layers to a neural network's decision function.\nTo gauge a layer's importance, we replace its parameters with random values (randomization). If\nthe network's decisions remain largely unchanged after randomization, it suggests that the learned\nparameters contribute little beyond noise.\nThis methodology largely follows Zhang et al. (2022), who reset the parameters of individual layers to\nvalues drawn from the original initialization\u00b2. However, while Zhang et al. (2022) measured criticality\nof a layer by the change in accuracy due to the randomization, we measure the angle between the\nprobability vectors resulting from the randomization. Specifically, we apply a Softmax function to\nthe network logits to obtain (pseudo-)probabilities, measure the cosine distance between those before\nand after randomization, and aggregate the measurements into a single scalar by averaging over all\nsamples. The effect of each layer randomization on this measured distance is what we define as the\ncriticality of a layer.\nThis methodological change can be evaluated in an unsupervised manner and more importantly, is\nalso sensitive to changes in the probability distribution including variations in errors (we refer the\nreader to (Geirhos et al., 2020) for a discussion on why this is important). As such, it provides a\nmore holistic measurement of consistency in the decision before and after randomization well beyond\ncorrect predictions.\nWe call a layer auxiliary if the decision is insignificantly affected by the reset (\u2248 0% criticality) and\ncritical (\u2248 100% criticality) if the distance between decisions changes significantly. Realistically, the\ncriticality for most layers does not lie on the extremes of this spectrum, but anywhere in between. Due\nto a significant variance (standard error of up to 45% on a few layers in specific models; see Figure 5)\nin criticality on some layers, we repeat experiments with different random seeds and report the mean\nover three trials. For computational reasons, we evaluate layers on a subset of 10,000 random images\nfrom the ImageNet ILSVRC-2012 challenge validation set (Russakovsky et al., 2015)."}, {"title": "3 Results", "content": "Due to the wide use and availability of pre-trained models, currently, all our results are obtained on\nResNet-50 (He et al., 2015). Recall that this architecture consists of a stem (denoted by [0.*]), 4\nstages (denoted by [1-4.*]), a pooling layer, and a fully-connected classification head (denoted by\n[Head]). Each stage consists of several residual bottle-neck blocks which include learnable 1 \u00d7 1\nconvolutions (conv1, conv3), 3 \u00d7 3 convolutions (conv2), as well as batch-normalization layers (Ioffe\n& Szegedy, 2015). The first residual block in each stage is special, as it downsamples by a strided\nconvolution, thus, adding a learnable layer on the skip connection (downsample).\nGeneral Observations. The results in Figure 1 (analogously see Appendix A for more views) clearly\nshow that the training method influences what layers become critical \u2013 despite that all models were\ntrained on the same training set (some with more extreme forms of data augmentation utilizing a\nnegligible amount of extra data).\nIn contrast to previous findings (Zhang et al., 2022; Chatterji et al., 2020), we observe that no layer\nis always auxiliary across training methods. For instance, we observe an average criticality of just\n36% for a spatial convolution layer ([3.5] conv2). Yet, if we randomize the same layer in a PixMix\n(Hendrycks et al., 2022) model, we observe a strong criticality of 95%. On the opposite, we do find\nlayers that are always critical. As expected, these include the initial stem convolution ([0.0] conv)\nand the classification head (fc). Beyond, we find that most first convolution layers in each stage\n([*.0] conv1) are critical \u2013 yet the number of outliers increases with depth. Similarly, we find that the\ndownsampling convolution ([*.0] downsample) in each stage is often critical. In stage 1 this layer\nis critical for all models but again the criticality of deeper downsampling convolutions depends on\nthe training strategy used for the model. Lastly, akin to Gavrikov & Keuper (2023), we find that\npointwise convolution layers tend to be more critical than spatial convolution layers (except for the\nstem). For all other layers, criticality depends on the training method. In the following paragraphs,\nwe analyze specific categories of training methods.\nAdversarial Training (AT). This training technique intends to increase the robustness of neural\nnetworks by training on adversarially perturbed training samples (Madry et al., 2018). To avoid\nperturbations that cause a shift in semantic meaning, perturbations are often constrained by an attack\nperturbation budget e for some $l_p$ norm. We consider AT using a PGD attack (Madry et al., 2018)\nwhich optimizes the perturbations over several iterations (here: three). Please note, that reported e\nvalues for $l_\\infty$ norms are short for \u20ac/255 (but not for the l2 norm).\nWe find that AT increases the criticality proportional to the attack budget e during training results. To\nmake this more tangible, we average the criticality over all layers and show the results in Figure 2.\nWe do not observe differences between training that utilizes $l_2$ or $l_\\infty$ norms for attacks. Our findings\nin Figure 1 suggest that neural networks utilize more of their capacity under increasing training\nattack strength. This augments previous findings that showed similar insights through accuracy\nimprovements with larger networks (Madry et al., 2018) or richer representations in convolutions\nfilters (Gavrikov & Keuper, 2022a,b). AT primarily increases the criticality in the layers of the first\nand second stages and slightly in the third stage. The criticality of layers in the fourth stage is barely\naffected but rather decreases, compared to the baseline (please refer to Figure 4).\nAugmentations. Compared to AT, the influence of different augmentation strategies seems weaker.\nWe do find that augmentations tend to increase the average criticality, i.e., they do occupy more of the\nnetwork capacity \u2013 but most changes are rather small. Affected layers seem to fluctuate by method,\nbut we find that all augmentation methods consistently increase the criticality of some of the deepest\nlayers ([4.0] downsample, [4.1] conv2, [4.2] conv2/3). Similar to the organization of the human brain\n(Hubel & Wiesel, 1979), deeper layers in neural networks are associated with activations of more\ncomplex features. For images, these tend to correspond to shapes as opposed to texture information\nthat is captured by early layers (LeCun et al., 1989a; Yosinski et al., 2014). Indeed, prior work has\nobserved that our tested augmentations increase in their shape responses (Gavrikov & Keuper, 2024).\nThus, a reasonable hypothesis is that the increased criticality in deeper layers correlates with stronger\nshape representations.\nStrong outliers to our observations are the PixMix models (Hendrycks et al., 2022). These models have\nthe highest average criticality in our model zoo without a single auxiliary layer. The augmentation\ntechnique has been shown to improve multiple safety dimensions beyond test accuracy and combined"}, {"title": "4 Conclusion, Limitations, and Future Work", "content": "Our ongoing study extends previous findings about the complexity of learned decision functions of\nimage classification models. Instead of analyzing individual models as often done in mechanistic\ninterpretability works (e.g., (Olah et al., 2020; Goh et al., 2021)) we focus on the common impact\nof training methods on layer criticality. We have shown that some forms of training leave distinct\npatterns in the decision function."}]}