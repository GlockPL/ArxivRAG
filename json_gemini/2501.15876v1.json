{"title": "Optimizing Sentence Embedding with Pseudo-Labeling and Model Ensembles: A Hierarchical Framework for Enhanced NLP Tasks", "authors": ["Ziwei Liu", "Qi Zhang", "Lifu Gao"], "abstract": "Sentence embedding tasks are important in natural language processing (NLP), but improving their performance while keeping them reliable is still hard. This paper presents a framework that combines pseudo-label generation and model ensemble techniques to improve sentence embeddings. We use external data from Simple Wiki, Wikipedia, and BookCorpus to make sure the training data is consistent. The framework includes a hierarchical model with an encoding layer, refinement layer, and ensemble prediction layer, using ALBERT-xxlarge, ROBERTa-large, and DeBERTa-large models. Cross-attention layers combine external context, and data augmentation techniques like synonym replacement and back-translation increase data variety. Experimental results show large improvements in accuracy and F1-score compared to basic models, and studies confirm that cross-attention and data augmentation make a difference. This work presents an effective way to improve sentence embedding tasks and lays the groundwork for future NLP research.", "sections": [{"title": "I. INTRODUCTION", "content": "Sentence embedding is an important task in natural language processing (NLP) for applications like text classification, semantic similarity, and information retrieval. Word-level embeddings like Word2Vec and GloVe helped understand semantic relationships but do not work well for sentence-level meanings. Recent models like BERT and its variations improved this by capturing meaning at the sentence level, leading to better performance in many NLP tasks. Still, there are challenges such as overfitting with limited domain-specific data, difficulty in adapting to different datasets, and issues with combining external knowledge sources.\nTo solve these problems, we propose a new framework that combines pseudo-label generation with model ensemble techniques to improve sentence embeddings. Our approach adds external data from SimpleWiki, Wikipedia, and BookCorpus to make the training data richer and help generalize better. Using external knowledge helps overcome the limitations of small or domain-specific datasets and improves the model's ability to understand complex sentence structures.\nThe framework uses a hierarchical model with three parts: an encoding layer, a refinement layer, and an ensemble prediction layer. The encoding layer uses transformer models like ALBERT-xxlarge, RoBERTa-large, and DeBERTa-large to create high-dimensional embeddings. These models were chosen because they can capture different linguistic features. The refinement layer adds convolutional layers and attention mechanisms to capture n-gram dependencies and local context, improving sentence representation. The final prediction is made by combining the outputs of different models using ridge regression, ensuring strong performance.\nWe also apply data augmentation techniques like synonym replacement, back-translation, and contextual rewriting to increase the diversity of the training data and reduce overfitting. Cross-attention layers are added to help the model use external context, improving its ability to understand long-range dependencies.\nThis framework improves the accuracy, robustness, and generalization of sentence embeddings, offering a strong basis for future research in NLP, especially for cross-lingual tasks and real-time applications."}, {"title": "II. RELATED WORK", "content": "Recent progress in NLP has been driven by transformer models and new techniques. Lu et al. [1] combined multiple models like LightGBM, DeepFM, and DIN to improve prediction accuracy, inspiring our hybrid approach for improving NLP tasks. Reimers et al. [2] introduced Sentence-BERT, which enhances sentence-level tasks with high-quality embeddings, influencing our strategy for sentence representation. Liu et al. [3] improved BERT through RoBERTa, making pretraining better for NLP tasks, which influences our approach to embedding optimization.\nXLNet, introduced by Yang et al. [4], advanced language understanding by capturing complex dependencies, guiding our work on contextualized embeddings. Khosla et al. [5] developed supervised contrastive learning to improve embedding differentiation, which we use in our model for sentence-level tasks. Wei and Zou [6] introduced EDA, a data augmentation technique that improves generalization in text classification, which we use for data diversity. Mathew and Bindu [7] reviewed NLP methods using pre-trained models, which form the basis for our use of pre-trained models in different tasks.\nRaffel et al. [8] proposed the T5 model, which reformulates NLP tasks as text-to-text problems, providing a unified framework that we use to improve task adaptability. Bouraoui et al. [9] reviewed deep learning in NLP, offering insights that influence our model development.\nFinally, Siyue et al. [10] introduced a dual-agent approach for improving reasoning in large language models, which we use to enhance contextual coherence in our embeddings."}, {"title": "III. METHODOLOGY", "content": "In this section, we proposes a novel hierarchical approach to sentence embedding tasks by leveraging hybrid model architectures and a robust pseudo-labeling pipeline. The model architecture integrates transformer-based encoders with convolutional and attention-based refinement layers, followed by an ensemble strategy utilizing ridge regression for final predictions. The proposed method is trained on both gold-standard datasets and pseudo-labeled external data, carefully curated using cosine similarity and error-based filtering. Through extensive experiments, we demonstrate the superiority of this hierarchical and hybrid methodology, achieving significant performance improvements over traditional transformer-based baselines."}, {"title": "A. Model Network", "content": "The proposed model architecture employs a hierarchical design to leverage the strengths of different neural network components. The architecture can be divided into three key stages: encoding, refinement, and ensemble prediction."}, {"title": "B. Encoding Layer", "content": "At the encoding layer, transformer-based models such as ALBERT-xxlarge, ROBERTa-large, and DeBERTa-large are used as the backbone. These models generate high-dimensional embeddings for each sentence. The output of a transformer encoder for a given sentence x is represented as:\n$$H^{(l+1)} = LayerNorm(H^{(l)} + FFN(MultiHead(H^{(l)}))),$$ (1)"}, {"title": "C. Refinement Layer", "content": "The refinement layer integrates convolutional layers and attention-based aggregation to enhance local feature detection and contextual understanding. A convolutional refinement is applied to capture n-gram-level dependencies:\n$$F_{conv} = \\sigma(W_{conv} * H + b_{conv}),$$ (3)\nwhere * denotes the convolution operation, $W_{conv}$ and $b_{conv}$ are the weights and bias of the convolutional layer, and $\\sigma$ is the activation function.\nTo integrate the refined features, an attention mechanism is used:\n$$A = Softmax(QK^T / \\sqrt{d}),$$ (4)\n$$H_{attn} = AV,$$ (5)\nwhere Q, K, and V are the query, key, and value matrices derived from $F_{conv}$, and d is the dimensionality scaling factor."}, {"title": "D. Ensemble Prediction Layer", "content": "The final predictions are generated by an ensemble of models using ridge regression. The outputs from each model are combined to produce the final prediction:\n$$y = \\sum_{i=1}^{n} w_i f_i(x),$$ (6)\nwhere $f_i(x)$ represents the prediction of model i, and $w_i$ are weights optimized by solving the ridge regression objective:\n$$min_{w} ||y - Fw ||^2 + \\lambda||w||^2,$$ (7)\nwhere F is the matrix of predictions, y is the ground truth, w is the weight vector, and $\\lambda$ is the regularization parameter."}, {"title": "E. Loss Function", "content": "The loss function used during training is the mean squared error (MSE) for regression tasks, defined as:\n$$L = \\frac{1}{N} \\sum_{i=1}^{N}(y_i - \\hat{y}_i)^2,$$ (8)\nwhere N is the number of samples, $\\hat{y}_i$ is the predicted value, and $y_i$ is the ground truth."}, {"title": "F. Data Preprocessing", "content": "The data preprocessing pipeline is designed to maximize the utility of external datasets and align them with the distribution of the training data. The pipeline includes several innovative steps: data collection, snippet generation, embedding retrieval, and error-based filtering."}, {"title": "1) Data Collection", "content": "We began by curating a large corpus of external datasets, including SimpleWiki, Wikipedia, and BookCorpus, as these sources offer diverse linguistic patterns and semantic structures relevant to the training data. To ensure relevance, only documents with a domain overlap or thematic similarity to the target task were included."}, {"title": "2) Snippet Generation", "content": "To align the length of external text with the training data, we segmented documents into snippets of similar size. The snippet length L was set to approximate the average length of sentences in the training dataset:\n$$L = \\frac{\\sum_{i=1}^{N} |x_i|}{N},$$ (10)\nwhere $|x_i|$ is the word count of input $x_i$. Text snippets shorter than L were padded, and those exceeding L were truncated."}, {"title": "3) Embedding-Based Retrieval", "content": "For each training example $x_i$, we utilized a pre-trained Sentence-BERT model to compute sentence embeddings. Let $e_i$ denote the embedding of $x_i$. For each snippet $s_j \\in S$, its embedding $e_j$ was computed. The cosine similarity between embeddings was used to retrieve the top-k relevant snippets:\n$$CosSim(e_i, e_j) = \\frac{e_i \\cdot e_j}{||e_i|| ||e_j||},$$ (11)\nThe top-k snippets {$s_{j1}, s_{j2},..., s_{jk}$} with the highest similarity scores were selected for pseudo-labeling. Here, k was set to 5 based on empirical analysis."}, {"title": "4) Pseudo-Labeling and Error-Based Filtering", "content": "Using a fine-tuned RoBERTa-base model, we assigned pseudo-labels $\\hat{y}_j$ to the selected snippets. However, to maintain label quality, we employed standard error filtering. Let $\\sigma_i$ denote the standard error of the gold label $y_i$ in the training data. A snippet $s_j$ with a pseudo-label $\\hat{y}_j$ was retained only if:\n$$|\\hat{y}_j - y_i| \\leq \\sigma_i.$$, (12)\nThis filtering step ensures that the pseudo-labeled data distribution remains aligned with the gold-label distribution, minimizing noise."}, {"title": "5) Augmentation Strategies", "content": "To further enhance the training data, we applied data augmentation techniques, including synonym replacement, back-translation, and contextual paraphrasing, to the retained snippets. This step increases variability and robustness in the training dataset. The augmented data was denoted as $D_{aug}$."}, {"title": "6) Final Data Preparation", "content": "The final dataset for training, $D_{final}$, was constructed by combining the gold training data, pseudo-labeled external data, and augmented data:\n$$D_{final} = D_{train} \\cup D_{pseudo} \\cup D_{aug}.$$ (13)"}, {"title": "7) Innovative Contribution", "content": "The combination of embedding-based retrieval and error-based filtering introduces a novel mechanism to curate high-quality pseudo-labeled data. By carefully aligning external data with the original training distribution, we ensure that the model benefits from additional training samples without being adversely affected by noise or label discrepancies."}, {"title": "IV. EVALUATION METRICS", "content": "To assess the performance of our models, we employed four key evaluation metrics tailored to the task:"}, {"title": "A. Accuracy", "content": "The proportion of correctly predicted instances over the total instances:\n$$Accuracy = \\frac{TP + TN}{TP + TN + FP + FN},$$ (14)\nwhere TP, TN, FP, and FN represent true positives, true negatives, false positives, and false negatives, respectively."}, {"title": "B. F1-Score", "content": "The harmonic mean of precision and recall, providing a balanced view of the model's performance:\n$$F1\\text{-}Score = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}},$$ (15)"}, {"title": "C. LogLoss", "content": "Measures the distance between predicted probabilities and the true labels:\n$$LogLoss = - \\frac{1}{N} \\sum_{i=1}^{N} (y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)),$$ (16)\nwhere $y_i$ is the true label and $\\hat{y}_i$ is the predicted probability for instance i."}, {"title": "D. AUROC", "content": "Evaluates the ability of the model to distinguish between classes:\n$$AUROC = \\int_{0}^{1} TPR(FPR) d(FPR),$$ (17)\nwhere TPR and FPR are the true positive rate and false positive rate, respectively."}, {"title": "V. EXPERIMENT RESULTS", "content": "The proposed hierarchical model ensemble was compared against multiple baselines, including individual transformer-based models and simple ensembles."}, {"title": "VI. CONCLUSION", "content": "This study demonstrates the effectiveness of pseudo-labeling and model ensembles in enhancing sentence embedding tasks. By integrating external data and employing rigorous training and evaluation techniques, the proposed approach achieves superior performance, providing a robust framework for future advancements in natural language processing."}]}