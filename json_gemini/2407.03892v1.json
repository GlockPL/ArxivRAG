{"title": "On the Effectiveness of Acoustic BPE in Decoder-Only TTS", "authors": ["Bohan Li", "Feiyu Shen", "Yiwei Guo", "Shuai Wang", "Xie Chen", "Kai Yu"], "abstract": "Discretizing speech into tokens and generating them by a decoder-only model have been a promising direction for text-to-speech (TTS) and spoken language modeling (SLM). To shorten the sequence length of speech tokens, acoustic byte-pair encoding (BPE) has emerged in SLM that treats speech tokens from self-supervised semantic representations as characters to further compress the token sequence. But the gain in TTS has not been fully investigated, and the proper choice of acoustic BPE remains unclear. In this work, we conduct a comprehensive study on various settings of acoustic BPE to explore its effectiveness in decoder-only TTS models with semantic speech tokens. Experiments on LibriTTS verify that acoustic BPE uniformly increases the intelligibility and diversity of synthesized speech, while showing different features across BPE settings. Hence, acoustic BPE is a favorable tool for decoder-only TTS.", "sections": [{"title": "1. Introduction", "content": "The language modeling paradigm has been revolutionizing text-to-speech (TTS) by its strong generation ability since the birth of large decoder-only language models (LMs). There have been LM-based TTS models that exhibits versatile, expressive and even emergent abilities [1, 2, 3, 4, 5, 6].\nUnlike text, speech is intrinsically a continuous signal with a much lower information density. To adapt speech into LMs and generate it in an autoregressive manner, researchers have adopted different methods to map speech into discrete tokens [2, 4]. By the purpose of discretization, discrete speech tokens can be divided into acoustic tokens [7, 8, 9] that aim to reconstruct the signal perfectly, and semantic tokens [10, 11, 12, 13], from self-supervised models that provide a compact abstraction of speech semantics. After discretization, decoder-only LMs like VALL-E [3] can be seamlessly applied to TTS, where the discrete speech tokens are treated as the targets given a text input.\nHowever, the low rate of information behind speech still leads to excessively lengthy discrete speech sequences compared to text transcriptions [14]. For example, more than 500 HuBERT [11] tokens may be required for a single sentence of around 30 words just to convey an idea or two. It is even worse for the acoustic tokens, since most of neural speech codec models require a short frame shift and the residual vector quantization [8] technique to reconstruct the signal decently. Regardless of the type of tokens, this feature of speech poses a great challenge for long-context modeling of LM-based TTS systems.\nTo address this issue, one possible way is to further compress the discrete speech sequence. A promising approach is the acoustic byte-pair encoding (BPE) technique, which is proposed in [15]. It is a similar method to the traditional BPE algorithm [16] in natural language processing. It treats the discrete indexes of speech as literal characters and iteratively compresses consecutive tokens based on the frequency in the training corpus. Such compression will coherently reduce sequence length with the increase of vocabulary size. For speech discrete tokens, usually a group of multiple tokens occur together to represent a specific phoneme or syllable, and organizing them to be a unique modeling unit would provide a higher abstraction of semantics and morphological information. Therefore, it is intuitively reasonable to apply acoustic BPE on discrete speech tokens in both reducing sequence length and improving representability. In previous researches, such acoustic BPE has been adopted encode pseudo-target label in HuBERT pretraining [15] and automatic speech recognition [17].\nNevertheless, the effectiveness of applying acoustic BPE in the TTS task still remains unclear to the literature. Although BASE-TTS [6] and VoxtLM [18] mentions the use of acoustic BPE in generation, the design space of acoustic BPE is still not fully investigated, and the gain of such technique in TTS needs to be further explored. Despite of the possible improvements, acoustic BPE could bring more difficulties in choosing the correct unit for generation, since the vocabulary of the LM in TTS is greatly enlarged. Too much abstraction of acoustic units could also make language modeling harder. Moreover, acoustic BPE might have different behaviors and performances on different types of discrete speech tokens.\nTherefore, in this work, we conduct a comprehensive study on the effectiveness of introducing acoustic BPE to TTS in the decoder-only LM paradigm. We implement various settings of acoustic BPE on the semantic tokens extracted by speech self-supervised models, and then train a VALL-E autoregressive decoder-only transformer as the acoustic model to generate acoustic BPEs from text inputs. Afterwards, the original semantic tokens are unfolded and fed to a unit-based vocoder [19, 20] for waveform synthesis. We consider HuBERT [11] and WavLM [13] as the source of semantic tokens, adjust the number of clusters in extracting the semantic tokens from 2048 to 8192, and increase the vocabulary size of acoustic BPE up to 20k in order to observe the effects made by different acoustic BPE settings. We perform most of the experiments on LibriTTS [21] and evaluate the effectiveness of such settings via speech intelligibility, sample diversity and speech quality in objective and subjective measurements.\nOur findings suggest that models employing acoustic BPE method are capable of generating audio with high intelligibility and quality, while also achieving much faster training and in-"}, {"title": "2. Decoder-Only TTS with Acoustic BPE", "content": "2.1. Acoustic BPE\nBPE is a widely-used algorithm for data compression and text encoding. In natural language processing, it is initialized with a vocabulary that contains all the characters with their emergence frequency in the text corpus, and iteratively merges the most frequent character pairs until a specified number of merges or a target vocabulary size is reached [22]. For most languages like English, there exists blanks as the obvious boundaries between words in the text. However, the textual sequences of audio discrete tokens lack obvious boundaries, akin to linguistic features found in Chinese. Following [14], we first obtain speech discrete tokens from k-means clusters of features derived from speech self-supervised learning (SSL) models (HuBERT, WavLM). Then we establish a bijective mapping for conversion between the speech discrete tokens and the Chinese-character Unicodes, simply using an integer offset. Chinese characters mapping to speech discrete tokens are used to train the BPE model. With these operations, the acoustic BPE encoding is composed of conversion to Chinese characters and the encoding of pretrained BPE model. The acoustic BPE decoding is exactly the inverse process. This research utilizes the acoustic BPE method as a part of the tokenizer, which plays a role in preprocessing inputs for the decoder-only language model mentioned in the next subsection(2.2). Similar to natural language processing, we can consider the upcoming modeling process as a spoken language modeling process.\nBased on the method description above, it is evident that the method involves multiple dimensions of configuration, such as the SSL model, k-means cluster number, BPE vocabulary size, and so on. Configuring combinations across different dimensions may lead to entirely new characteristics, which can have significant implications in spoken language modeling. A comprehensive and systematic exploration of these configurations' impact on modeling is crucial for better constructing spoken language models.\n2.2. Decoder-Only TTS Language Modeling\nThe language model can estimate the probability of the input sequence, which is usually expressed as:\n$$p(x|\\theta) = \\prod_{i=1}^{t}P(x_i|x_{<i}, \\theta)$$\nwhere $x = {x_1,..., x_t}$ is the input sequence with length t and the language model is parameterized by $\\theta$.\nHowever, the spoken language model in TTS pays the other attention on text conditions. Based on the decoder-only Transformer architecture, the training and inference process of the TTS model follows the autoregressive (AR) stage in VALL-E.\n2.2.1. Decoder-only TTS model\nDuring training process, we put the phoneme sequence x and tokenized audio sequence s into the decoder-only language model (with parameters denoted as $\\theta$), estimating the probability autoregressively, formulated as:\n$$p(s|x; \\theta) = \\prod_{i=1}^{t}P(s_i|s_{:i-1}, x; \\theta)$$\nThe model optimizes its parameters by performing the task of predicting the next acoustic feature based on the phonemized text and historical speech features. This training process can be viewed as maximizing the sequence probability of the tokenized audio sequence under the condition of the text, which can be described as:\n$$\\theta = \\arg \\max_{\\theta} p(s|x; \\theta) = \\arg \\max_{\\theta} \\prod_{i=1}^{t}P(s_i|s_{:i-1}, x; \\theta)$$\nwhere $\\theta$ is our optimization target.\n2.2.2. Inference with prompts\nDuring inference, the model generates speech with similar speakers and prosody as the given audio prompt. Concatenating tokenized audio prompt $s_{prompt}$, phonemized source text x, and the phonemized text prompt $x_{prompt}$ corresponding to the audio prompt, we build the input of the spoken language model in format of {$x_{prompt}$, x, $s_{prompt}$}. The model then autoregressively generates the remaining sequence after this sequence. The decoding process consists of acoustic BPE decoding (if used) and the vocoding process of a discrete-unit-based vocoder [20] for speech waveform synthesis. Additionally, the Mel-spectrogram extracted from the speech prompt is also input into the vocoder to achieve better speaker control."}, {"title": "3. Experiments and Results", "content": "In this section, we will introduce the exploration involving the configuration of various dimensions in the acoustic BPE method and analyze the results regarding Decoder-only TTS performance with these configurations.\n3.1. Experimental Setup\n3.1.1. Datasets\nThe experiments were conducted on the LibriSpeech [23] and LibriTTS [21] datasets. LibriTTS, designed for text-to-speech tasks, consists of approximately 585 hours of English speech from multiple speakers. Its train-960 subset was used for model training. The LibriSpeech dataset served primarily as the test set for speech synthesis. In this experiment, sentences shorter than 4 seconds or longer than 10 seconds were filtered out from the test-clean subset of LibriSpeech. The remaining 1145 sentences, totaling approximately 2.02 hours of speech, were used as test cases. All speech audio data was downsampled to 16kHz before use.\n3.1.2. Settings of acoustic BPE\nThe encoding process of acoustic BPE consists of two stages: speech discretization and acoustic BPE model training. In the speech discretization stage, we utilized the HuBERT-large\u00b9 model pretrained with masked prediction on the 60k hours of LibriLight [24] dataset and the WavLM-large\u00b2 model pretrained on extra dataset consisting of 10k hours of Gigaspeech [25] and 24k hours English data subset of VoxPopuli [26], as the self-supervised speech representation model. Continuous audio features were extracted from the final layer's output activations of-"}, {"title": "3.2. Acoustic BPE Enhances TTS Model Performance", "content": "After our preliminary experiments, it was found that in general situation, incorporating the acoustic BPE method indeed leads to significant improvements in the TTS model's ability to synthesize speech. We primarily focus on intelligibility and quality of synthesized speech to reflect this ability. The results of selected outperforming settings are presented in Table 1.\n3.2.1. Improvement of speech integibility and quality\nWe conducted integibility experiments on the entire test set, which consists of 1145 sentences. The WER of the synthesized audios ASR results are calculated. As shown in Table 1, the intelligibility performance of audio generated by TTS models using acoustic BPE method has a significant advantage over those without using it. Under the pre-configuration (tokenizer composed of HuBERT-large and a 2048-centroid kmeans model), the reduction is up to 3.9% WER. The results of the MOS metric indicate that models using the acoustic BPE method can generate competitive audio quality. In the worst-case scenario, it is slightly lower by 0.02 compared to not using it, and it is accompanied by lower confidence. However, in the best-case scenario, it can be higher by 0.07, accompanied by better confidence.\n3.2.2. Acceleration of inference and training process\nThe calculation of RTF in this research is equivalently defined as the ratio of the docoder-only LM inference time to the length of its input. As the presented result, it significantly decreases as the expansion of its vocabulary increase. This is due to the merging operations of BPE, which shorten the sequence length of the input language model. The resulting acceleration effect of the model far exceeds the time spent on the increased com-"}, {"title": "3.3. Enrichment of sample diversity", "content": "We are interested in investigating whether the acoustic BPE method can influence the generation of more diverse intonations or speech rates when reading the same sentence. We opted to utilize NDB and JS divergence as metrics for assessing the diversity of samples in this TTS model. Experimental results from various configurations are presented in Table 3.\n3.3.1. The NDB and JS divergence metrics\nThe NDB metric is proposed in [30]. In our experiment, we extracted prosodic features from selected low-WER synthesized utterances in the test set. Additionally, with speaker information already present in prompts, the influence of semantic and speaker information on sample diversity can be almost negligible. Samples, considered as the prosodic features of frames, were divided into a training and evaluation set. We first set a fixed number k of cluster bins by training a k-means model with samples in the training set, maintaining k centroids. Let p and q denote the statistical distributions of samples in the training and evaluation set on these bins, and let m and n denote the sample numbers of the training and evaluation set, respectively. The calculation of NDB metric was conducted with a two-proportion z-test on p, q and m, n, counting the number of the bins whose final p-values are less than a manually set value of significant level, and dividing it by k.\nThe definition of Jensen-Shannon (JS) divergence is\n$$JS = \\frac{1}{2} [KLD(p || \\frac{p+q}{2}) + KLD(q || \\frac{p+q}{2})]$$\nwhere KLD(.||.) is the Kullback-Leibler (KL) divergence function. To reduce the impact of uncertain selection of two sample sets, we calculated two metrics using a public tools for 10 times and used their average values as the final results. Greater values of NDB and JS indicate a larger difference between the statistical distributions of the samples, suggesting better performance in terms of diversity.\n3.3.2. Increment of TTS sample diversity with acoustic BPE\nAccording to the presented results, TTS models employing the acoustic BPE method showed significant advantages in synthesized audio diversity compared to those without it. Referring to Figure 1, this phenomenon is essentially prevalent. We will discuss extreme boundary cases in detail in next section 3.4."}, {"title": "3.4. Discussion on boundary cases and limitations", "content": "Although the benefits of the acoustic BPE in improving model performance are substantial, every method has its applicable range and limitations. This is particularly observed in our study, where there are configurable parameters that can be flexibly adjusted. Exploring the boundary conditions of this method under our research conditions can provide intuitive insights for constructing acoustic BPE methods in other decoder-only LM-based TTS systems with complex configurations. According to the results in Table 2, both too small and too large number of k-means centroids can lead to a plateau or even a decline in model performance. In practical experiments, when there is a significant gap between the number of k-means centroids and vocabulary size, instability may arise, e.g. leading to the model repeatedly outputting the same token. Moreover, WavLM also exhibits instability in the TTS architecture used in this experiment. At this time, the use of acoustic BPE method may exacerbate this instability, resulting in worse TTS performance."}, {"title": "4. Conclusion", "content": "In conclusion, the application of the acoustic BPE method in TTS tasks brings significant performance benefits, including improvements in the intelligibility, quality, and diversity of generated audio, as well as notable enhancements in training and inference speed. This undoubtedly demonstrates the potential of the method in discrete speech language modeling and speech-text language modeling. However, the presence of the acoustic BPE vocabulary increases the number of discrete tokens in speech. This imposes certain limitations on its configuration selection. In future work, we will conduct experiments with this method under scaled up datasets and models. Additionally, we will explore other effective methods for tokenizing audio."}]}