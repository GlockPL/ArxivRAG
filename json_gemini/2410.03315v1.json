{"title": "Influence-Oriented Personalized Federated Learning", "authors": ["Yue Tan", "Guodong Long", "Jing Jiang", "Chengqi Zhang"], "abstract": "Traditional federated learning (FL) methods often rely on fixed weighting for parameter aggregation, neglecting the mutual influence by others. Hence, their effectiveness in heterogeneous data contexts is limited. To address this problem, we propose an influence-oriented federated learning framework, namely FedC2I, which quantitatively measures Client-level and Class-level Influence to realize adaptive parameter aggregation for each client. Our core idea is to explicitly model the inter-client influence within an FL system via the well-crafted influence vector and influence matrix. The influence vector quantifies client-level influence, enables clients to selectively acquire knowledge from others, and guides the aggregation of feature representation layers. Meanwhile, the influence matrix captures class-level influence in a more fine-grained manner to achieve personalized classifier aggregation. We evaluate the performance of FedC2I against existing federated learning methods under non-IID settings and the results demonstrate the superiority of our method.", "sections": [{"title": "Introduction", "content": "Federated learning (FL) is a promising machine learning paradigm where multiple clients with diverse behaviors and preferences can collaboratively train models without sharing their private data (McMahan et al. 2017). A core challenge in FL is the statistical heterogeneity, also known as the non-IID problem, i.e., clients may own data samples from different domains with different feature attributes. The data distribution shift over multiple clients can prevent the globally learned model from achieving high performance on local data distribution. The vanilla FL method, namely FedAvg, significantly suffers from performance degradation due to the widely existing non-IID data in real-world scenarios. Existing works aim to alleviate this by adding regularization terms (Wang et al. 2020; Zhang et al. 2021b), local model decoupling (Luo et al. 2022; Chen and Chao 2022; Collins et al. 2021), etc. However, most of these works still follow the traditional model aggregation rule where each client is assigned a fixed weight (mostly proportional to its dataset size) corresponding to its local models without considering the underlying mutual effect between any two clients.\nThere is also a line of works that uses loss or gradient similarity-based aggregation schemes to reweight each client (Nishio and Yonetani 2019; Cho, Wang, and Joshi 2020; Ribero and Vikalo 2020; Balakrishnan et al. 2021). These works aim to learn a robust and general model by selecting a set of the most representative and informative clients rather than constructing a personalized reweighting scheme for each client to improve their local performance. As a result, in these methods, the less informative clients tend to stay unexplored during the federated training process, degrading the local performance at these clients (Balakrishnan et al. 2021). Moreover, in these works, the model aggregation procedure is still conducted at the server side, hindering a customized aggregation for each client.\nAs humans, we often enhance our intelligence by learning from others in collaborative environments, and we are especially influenced by those who share similar preferences or backgrounds. Inspired by this, we propose an influence-oriented Personalized Federated Learning (PFL) framework which allows each client to improve its local model by leveraging the influence of other models. Specifically, influence is measured by the consistency between a client's local model and those of its neighbors. We introduce two types of influences: class-specific influence, which captures similarities in background, and distribution-specific influence, which reflects shared preferences. By incorporating these influences, our framework updates personalized models, allowing clients to benefit from neighbors with similar preferences and backgrounds.\nAs shown in Fig. 1(a), there are five clients owning digit images from five different domains. Some of them explicitly share informative prior knowledge, e.g., the background color, font style, and/or hand-writing habit, suggesting the underlying correlation among clients with heterogeneous data. Unfortunately, most existing model aggregation schemes fail to leverage this natural correlation to extract useful underlying knowledge from potential clients. Apart from client-level influence, clients may have similar or different understandings toward a specific class. Taking digit classification as an example, in Fig. 1(b), most clients share the same understanding toward the digit \u201c0\u201d and digit \u201c8\u201d. In contrast, as for digit \"7\", the client in the right column is prone to capture the detail of digit \"7\" with a slash through it, while others hold opposite opinions. Traditional model aggregation neglects this disagreement, which may lead to misunderstandings about class-specific perception.\nMotivated by the above insight and observation, we propose an influence-oriented federated learning framework where both client-level influence and class-level influence are explicitly quantified and utilized to guide the parameter aggregation. With an influence-aware personalized model aggregation scheme, the local performance of each client can be further boosted. Specifically, to measure the influence of a specific client on another one during parameter sharing, we follow the leave-one-out principle and observe how the local performance changes if a client does not contribute to the aggregated model. We introduce the influence vector with each element specifying the influence brought by a client in the FL system. By utilizing the influence vector to aggregate model parameters of the feature representation layers, underlying client-wise correlative knowledge can be shared more efficiently. To further identify the class-level disagreement and address it during the aggregation procedure, we introduce an influence matrix to measure class-level influence, which provides a more fine-grained metric to investigate how a specific class in a client makes its contribution to the current client. The influence matrix is capable of capturing class-level influence and leading to a better parameter aggregation of classifiers. By measuring the client-level and class-level influence, clients are capable of aggregating the feature representation layers and the classifier with a unique weight allocation among all participating clients. Therefore, the knowledge conveyed by the model parameters can be acquired by clients in an efficient and personalized way.\nIn summary, the main contributions of the paper are summarized as follows:\n\u2022 We take the first step toward exploring the client-level and class-level influence within an FL system, which considerably improves the original parameter aggregation mechanism.\n\u2022 We propose an influence-oriented federated learning framework, namely, FedC2I, which provides a concrete and feasible solution to quantify the influence and enable clients to personalize the model aggregation procedure.\n\u2022 We carry out extensive experiments on benchmark datasets to show the superiority of FedC2I compared with baselines and verify the effect of key components in FedC2I."}, {"title": "Related Work", "content": "Federated Learning\nFederated learning (FL) is a new promising field of machine learning (Yang et al. 2019; Zhang et al. 2021a; Li et al. 2020a). It enables clients to collaboratively train a global model and/or multiple local models in a distributed manner without sharing their private data (Mothukuri et al. 2021). One core challenge in FL is the statistical heterogeneity issue across clients, also known as non-IID problem (Zhao et al. 2018; Li et al. 2019), where local datasets of clients may have heterogeneous distributions in label and/or feature spaces (Kairouz et al. 2021), degrading the local performance and resulting in unstable and slow convergence (Karimireddy et al. 2020).\nVarious works are proposed to deal with the heterogeneous problems in FL. There are recent studies proposing to use clustering-based techniques to improve the vanilla parameter aggregation schemes (Sattler, M\u00fcller, and Samek 2020; Ghosh et al. 2020; Long et al. 2022). In these methods, local models are usually clustered into multiple groups according to different clustering strategies. (Jiang et al. 2019) and (Fallah, Mokhtari, and Ozdaglar 2020) apply meta-learning in FL to obtain a better-initialized model which can be adapted to various clients by several local training steps. Other studies solve the heterogeneous problems via model decoupling (Li et al. 2020c; Shen, Zhou, and Yu 2022; Arivazhagan et al. 2019), representation learning (Oh, Kim, and Yun 2022; Liang et al. 2020), knowledge distillation (Li and Wang 2020; Jeong et al. 2018; Shen, Zhou, and Yu 2022), etc. Most of the above methods still follow the traditional parameter aggregation pattern where each client is assigned a fixed weighting coefficient, mostly in the proportion of their data sizes. In this case, how a client is influenced by others and how it contributes to others is not explicitly measured or discussed.\nThere is also a branch of works that investigates client reweighting in FL. Most of them introduce centralized client reweighting schemes where the server employs different aggregation weights to clients according to their local performance (Tang et al. 2022; Cho, Wang, and Joshi 2020; Nishio and Yonetani 2019), model update (Wan et al. 2022; Ribero and Vikalo 2020), and consensus on a public dataset (Zhang et al. 2021b; Feng et al. 2021). Some of these works aim to increase the rate of convergence when there are numerous clients in an FL system. The others focus on learning a robust global model by reweighting clients at the central server.\nInfluence-Oriented Deep Learning\nThere is a branch of works aiming at identifying the influence of training samples in the context of deep learning. In (Koh and Liang 2017), the authors aim to use the results of leave-one-out retraining to identify the contribution of training points for a given prediction. It focuses on approximating the contribution via influence function in a simple but efficient way. In (Jia et al. 2019) and (Ghorbani and Zou 2019), the influence of training samples is identified based on the accuracy of the model. (Jia et al. 2019) utilizes the Shapley value originated in cooperative game theory to evaluate the profit brought by each data. (Ghorbani and Zou 2019) further develops Monte Carlo and gradient-based methods to efficiently approximate the Shapley values in some real-world settings. (Yeh et al. 2018) presents a deeper understanding by decomposing the neural network into multiple activations of training samples and associating it with the influence of training points. (Pruthi et al. 2020) proposes a general and simple method to compute the influence of a training example on the prediction and applies it to various machine learning models.\nTo facilitate FL system by influence-oriented techniques, (Wang, Dang, and Zhou 2019) and (Xue et al. 2021) propose to measure the individual contribution during the collaborative training procedure. (Wang, Dang, and Zhou 2019) calculate the instance-level influence by removing it during the training process and use the result to calculate the contribution of a client. (Xue et al. 2021) quantifies the influence over the model parameters and designs an estimator to improve robustness and efficiency. Nevertheless, both of them use influence measurement for client selection from the perspective of global optimization, while our work focuses on parameter aggregation in a personalized manner."}, {"title": "Problem Formulation", "content": "In this section, we provide the basic formulation of the federated learning problem. We consider an FL system with M clients and a central server, where the m-th (m\u2208 [1,..., M]) client owns a private dataset Dm. In this work, we consider the following optimization objective for the FL framework,\n$\\min_{\\{W_1,W_2,...,W_M\\}} \\frac{1}{M} \\sum_{m=1}^{M}L_m (W_m; D_m),$ (1)\nwhere wm and Lm are the model parameters and loss function of the m-th client, respectively. Am is the weight of client m when performing parameter aggregation. Here Am is often proportional to the size of local dataset, i.e., Am = $\\frac{|D_m|}{N}$ where N is the total number of instances over all clients.\nUsually, for traditional classification tasks, the local model of the m-th client, parameterized by wm, can be decoupled into two parts: (1) the feature representation layers parameterized by em, and (2) the classifier parameterized by 4m. Suppose there are C classes, Om can be further written as\n$\\varphi_m = [\\varphi_m^1, \\varphi_m^2, ..., \\varphi_m^C],$ (2)\nwhere $\\varphi_m^c$ is a weight vector related to the c-th class in the linear classifier.\nTo learn the optimal model parameters for each client and surpass the pure local training performance, a variety of FL approaches propose different parameter aggregation and/or local training strategies. Vanilla FL computes the weighted average of all model parameters as\nw\u2190$\\sum_{m=1}^{M} \\frac{|D_m|}{N} W_m.$\n(3)\nFor classification tasks, each local model can be decoupled into two parts, i.e., feature representation and classifier. The model parameters can be partially shared for different purposes. For example, in FedRep (Collins et al. 2021), only the feature representation is shared across clients. In FedRoD (Chen and Chao 2022), in addition to the feature representation, one branch of the classifier is shared, and the other branch of that is locally trained."}, {"title": "FedC2I: An Influence-Oriented Federated Learning Framework", "content": "An Overview of FedC2I\nTo capture and exploit the mutual effect among different clients in a personalized FL system, the core idea of FedC2I is to quantify the client-level and class-level influence with well-crafted measurements and then execute local parameter aggregation with the guidance of these measurements. To measure the influence reasonably, we leverage the leave-one-out principle to estimate the client-wise and class-wise contributions effectively.\nA brief pipeline of FedC2I is demonstrated in Fig. 2. To represent how other clients influence the m-th client, we introduce an influence vector computed from the loss values from multiple leave-one-out models. Meanwhile, to identify the class-level disagreement, we construct an influence matrix that quantifies the class-level correlations from a more fine-grained perspective. At the beginning of local training, the client-level and class-level influence measurements serve as the weights for parameter aggregation of the feature representation layers and classifier, respectively. In this case, clients are capable of learning a more powerful local model by assimilating cross-domain knowledge in a personalized way. The following subsections respectively introduce these crucial designs in detail.\nClient-Level Influence Measure\nIn an FL system, sharing the knowledge among clients with similar data patterns can mutually boost the local performance, while sharing between clients with totally different data patterns tends to be less helpful to each other. Most existing FL methods neither explicitly model this property nor leverage this property to during parameter aggregation. If we can quantitatively measure how a local model contributes to another and consider the contribution when aggregating parameters, the local performance can be further improved potentially. To achieve this, a direct solution is to analyze the local data located at the client side or transmit sensitive data-related information. However, it will induce huge privacy concerns and make the FL system unreliable (Lyu et al. 2022). Therefore, it remains an open problem to figure out how a client contributes to others via the model parameters. Inspired by recent studies that measure the influence of training samples on prediction results (Koh and Liang 2017), we propose to measure the client-level influence by the leave-one-out principle before the model aggregation procedure.\nTo understand how client m is influenced by client i (where m, i \u2208 [M] and m \u2260 i), we start by removing the contribution of client i when performing model aggregation. Without the model parameter of client i, the parameter aggregation is only conducted over M-1 clients rather than M clients. In this way, the variation on local performance, i.e., the value of local training loss, can suggest whether client i has a positive impact on client m or not and can further quantify how much impact is brought by client i.\nFormally, at client m, we use $\\{0^{-i}, \\varphi_m\\}$ to denote the aggregated model parameter after removing the contribution of client i. Concretely, $\\{0^{-i}, \\varphi_m\\}$ is obtained by\n$\\frac{1}{M-1} \\sum_{m\\in[M],m\\neq i} \\theta_m.$\n(4)\nThen, we compute the loss of $\\{0^{-i}, \\varphi_m\\}$ on a random batch sampled from Dm, where the loss value is denoted as Im. Here, lm is a metric that is not only easy to compute but also efficient in suggesting the ability of a model on a target dataset. In concrete, a large lm indicates that the i-th model plays a more important role in current local task. On the contrary, when Im is small, the i-th model has less effect on improving local performance. Besides, compared with computing the loss on the whole local dataset, a batch randomly sampled at each round has the ability to describe local data attributes but has no need for massive computing resources.\nBy collecting the losses of multiple leave-one-out models, we construct a loss vector consisting of loss value lm where i \u2208 [M], meaning that the performance of all participants on the current local dataset is confirmed. To further model the influence explicitly, we transfer the loss vector to a quantitative measurement of client-level influence. Specifically, the definition of client-level influence is given as follows.\nDefinition 0.1. For the m-th client, we define Am = [\u03bb\u03b7, \u03bb\u03b7,..., \u03bb] \u2208 RM as the influence vector. The i-th element in Am is computed as\n$\\lambda_m^i = \\frac{[l_m^i]^{\\gamma}}{\\sum_{i=1}^{M}[l_m^i]^{\\gamma}}$ (5)\nwhere y is a hyper-parameter that tunes the sensitivity of clients to the influence. Setting y = 0 means that all clients share the same model aggregation weight rather than considering different levels of contribution from different clients. A larger y means that clients contributing more to the local performance can further strengthen their influence, thus potentially dominating the model parameters at the beginning of each communication round.\nDuring the parameter aggregation phase, the client-level influence vector can be further used to build a personalized feature representation model for client m \u2208 [M] as\n$0_m = \\sum_{i=1}^{M} \\lambda_m^i \\theta_i$. (6)\nNote that the influence of client m on itself is also considered. By scaling up those clients with greater influence on others, a stronger collaboration between clients is achieved.\nClass-Level Influence Measure\nBy considering client-level influence, the feature representation layers are aggregated toward more optimal local performance. However, class-level diversity, as an inherent characteristic of heterogeneous data, is still out of consideration. To further measure the inter-class influence among participating clients, we apply the leave-one-out strategy to see how the performance of the classifier differs without the contribution of a class-specific weight vector. In this way, the influence from the more fine-grained level is successfully measured."}, {"title": "", "content": "Here, we denote the classifier at client mas m = [Ym,1,Im,2,\u2026\u2026,4m,c], where each 4m,c in Om represents the weight vector corresponding to class c (Luo et al. 2021). To see how the local performance differs without the contribution from the i-th client to the c-th class, following the leave-one-out principle, we first remove the class-specific weight vector with respect to the c-th class and generate the weight vector by\n$\\bar{\\varphi}_{m,c} = \\frac{1}{M-1} \\sum_{m\\in[M],m\\neq i} \\varphi_{m,c}.$(7)\nThen, we replace the c-th weight vector in 4m, i.e., Om,c, by $\\bar{\\varphi}_{m,c}$ and compute the loss l\u012bmi,\u2212c on a random batch sampled from Dm. Similar to the definition and computation of client-level influence vector, we construct an influence matrix to indicate the class-level influence on each client, which can be defined as below.\nDefinition 0.2. For the m-th client, we define Am\u2208 RMXC as its influence matrix:\n$A_m = \\begin{bmatrix}\n\u039b1,1 & \u039b1,2 & ... & A1,C\\\\\n\u039b2,1 & 12,2 & ... & A2,C\\\\\n... & ... & ... & ...\\\\\nAM,1 & M,2 & ... & AM,C\n\\end{bmatrix}$ (8)\nwhere M and C represent the number of clients and the number of classes, respectively. The element at the i-th row and the c-th column, denoted as Ac, is computed by\n$\\Lambda_{m}^{i,c} = \\frac{[l_{m}^{i,-c}]^{\\gamma}}{\\sum [l_{m}^{i,-c}]^{\\gamma}}$(9)\nAnd represents the influence brought by the i-th client regarding to the c-th class. For the sensitivity hyper-parameter y, we use the same value as in Eq. (5) for simplicity.\nIntuitively, similar semantic knowledge toward the same class can help clients to boost their performance when sharing their class-specific weight vector, which corresponds to a larger An value. In contrast, a smaller Aim indicates the diverse understanding toward a class, which may deteriorate the performance of local model (Luo et al. 2021). To leverage the quantified influence during the aggregation of classifier, we use the class-level influence matrix to build the personalized local classifier\u0113m = [Pm,1,Pm,2,\u2026\u2026, \u00d3m,c], where the c-th element is computed by\n$\u00d3_{m,c} = \\sum_{i=1}^{M} A_{m}^{i,c} \\varphi_{m,c}$.(10)\nRemark. By measuring client-level and class-level influence at the beginning of local training, a client is able to aggregate the feature representation layers and classifier with a unique weight allocation among participating clients. Therefore, clients can selectively acquire knowledge from others in a personalized but efficient way.\nClient Update\nIn each communication round, as shown in Fig. 2(b), we propose a two-stage local update scheme which includes (1) influence-oriented aggregation stage and (2) local training stage. We summarize the steps of FedC2I in Algorithm 1.\nIn the influence-oriented aggregation stage, we first compute the client-level influence vector and class-level influence matrix for each client, which provides the weights for efficient parameter aggregation. Specifically, on a random batch sampled from the local dataset, the influences can be computed by Eq. (5) and Eq. (7), respectively. Then, we aggregate the feature representation layers with the influence vector following Eq. (6). Meanwhile, as formulated in Eq. (10), we use the influence matrix to guide the aggregation of classifier from a more fine-grained perspective. Under the influence-oriented aggregation scheme, a parameter aggregation solution is customized for each client, and the local model parameter of feature representation layers and the classifier are initialized as om and Om, respectively.\nIn the local training stage, similar to most existing FL methods, gradient-based backward propagation computation is conducted on the whole local training data to update the local learnable model parameters. At the end of the local training stage, the local model parameterized by Wm = {0m,4m} is uploaded to the central server for the next round of training.\nDiscussion. Influence-based measurement during the local update is beneficial in the following two aspects: (1) Instead of weighing clients with a unified criterion, the influence-oriented scheme applied here allows clients to have their unique aggregation schemes due to the heterogeneous data distributions. (2) Since the model aggregation process is moved from the server side to the client side, the server is free from performing model aggregation in each round, making it potential for the FL system to evolve from a centralized to a decentralized manner."}, {"title": "Experimental Setup", "content": "Datasets. We evaluate FedC2I on two benchmark heterogeneous FL settings: Digit-5 (Zhou et al. 2020) and Office-10 (Gong et al. 2012). The former contains ten digit classes from five domains, namely MNIST, SVHN, USPS, SynthDigits, and MNIST-M. The latter contains ten overlapping categories from four domains including Amazon, Caltech, DSLR, and WebCam. To simulate the data heterogeneous scenario in FL, each dataset owned by a client is from a different domain. Details about the non-IID settings can be found in Appendix A.\nBaselines. We compare FedC2I with six baselines including (1) Local where clients train their models locally; (2) FedAvg (McMahan et al. 2017), the vanilla FL method; (3) FedProx (Li et al. 2020b) that tackles non-IID problems in FL; (4) FedRep (Collins et al. 2021), (5) FedRoD (Chen and Chao 2022), and (6) FedProto (Tan et al. 2022), three state-of-the-art personalized FL methods.\nImplementation Details. We use the LeNet (LeCun et al. 1998) as the local model where there are two convolutional layers with kernel size of 5. The local epoch number and batch size are 2 and 32, respectively. We use an Adam (Kingma and Ba 2014) optimizer with weight decay of 0 and learning rate of 0.001. The number of communication rounds is 20 and 40 for Digit-5 and Office-10, respectively. We report the results with the average over 3 runs of different random seeds. For the hyper-parameter y, we use the best value y = 5 for both Digit-5 and Office-10 founded by grid search from {0.5, 1, 2, 5, 10}. All the methods are implemented by PyTorch and all experiments are conducted on one NVIDIA Quadro RTX 6000 GPU. More implementation details about the datasets, model architecture, hyperparameters, and baselines can be found in Appendix A."}, {"title": "Experimental Results", "content": "Performance Comparison. We compare our proposed FedC2I with the baseline methods to verify that effectiveness of FedC2I in feature shift non-IID scenarios. Table 1 and Table 2 report the results on Digit-5 and Office-10 datasets, respectively. The results are in mean (std) format over three independent runs with different random seeds.\nFrom the results, we can see that FedC2I outperforms the best baseline method in at least half of the clients/domains and achieves a certain improvement by 0.78% - 2.62% on the average test accuracy. Moreover, it is worthwhile to note that there is a significant performance boost in MNIST-M. The potential reason is that some similar domain properties shared by MNIST-M and other clients, i.e., MNIST, are recognized and leveraged by FedC2I during the parameter aggregation procedure in the format of influence measurement.\nAblation Studies. To verify the effect of the proposed influence vector and influence matrix that are used to measure client-level and class-level influence, we conduct an ablation study by comparing FedC2I with its variants. Specifically, we consider three variants including (1) only client-level influence is measured (FedC2I - \u5165); (2) only class-level influence is measured while the feature representation @ is locally trained (FedC2I - A/I); (3) only class-level influence is measured while the feature representation @ is globally shared following vanilla parameter aggregation scheme (FedC2I - A/g). As shown in Table 3, client-level influence measurement directly contributes to the final performance improvement, while class-level influence makes its contribution based on the client-level influence.\nEffects of Varying y. The hyper-parameter y plays an important role in tuning the sensitivity of clients to influences. A larger y makes the system more influence-sensitive and strengthens the influence of those important clients. We tune y from the candidate set {0.5, 1, 2, 5, 10} and select the one with the best performance on the validation dataset. In Table 4, we provide the results of FedC2I with varying values of y under Digit-5 and Office-10 datasets. As we can see, the best value of y is 5 for both datasets. There can be reasonable techniques developed for adaptively choosing appropriate y, which can be further discovered in future works.\nVisualization of Client-Level Influence. To better understand how the influence measurement is carried out and how the client-level influence varies in different communication rounds, we visualize the values of influence vector in Fig. 3."}, {"title": "Conclusion", "content": "In this paper, we propose a novel federated learning framework, namely FedC2I, that measures both client-level and class-level influence to achieve a more efficient model aggregation and benefit more participating clients. We construct the influence vector to specify the influence brought by other clients and leverage the client-level influence to aggregate the feature representation layers. We also construct the influence matrix to aggregate class-level knowledge in a more fine-grained manner, which enables each client to learn a class-personalized local classifier. Experimental results illustrate the superiority of FedC2I compared with baselines and verify the effect of the key components in FedC2I."}, {"title": "Appendix", "content": "Experimental Details\nWe provide more experimental details here due to the page limit.\nData Splitting Details We provide the data splitting details for the experiments.\nFor data splitting, we use a portion of data as training samples (~10%) and a larger set of samples as test set. We first take out a 20% subset of the training set for validation and return the validation set back to the training set and retrain the model after selecting the optimal hyper-parameters.\nSome raw data samples\nDetailed Experimental Setup We select the value of important hyper-parameters through small grid search on the validation dataset, and keep the rest insensitive hyper-parameters to be fixed values. Concretely, the grid search is carried out on the following search space:\n\u2022 The hyper-parameter gamma that tunes the sensitivity of clients to the influence: {0.5, 1, 2, 5, 10}\n\u2022 The learning rate: {5e-3, 1e-3, 5e-4, 1e-4}\n\u2022 The weight decay: {7e-4,5e-4,3e-4,1e-4,0}\nDetails of the Baseline Methods We compare FedC2I with six baselines. The details of these baselines are provided as follows.\n\u2022 Local: Each client trains their local model based on the local data without communication with others.\n\u2022 FedAvg: Clients send all the learnable parameters to the server and receive the aggregated parameters from the server for their next-round training.\n\u2022 FedProx: Based on FedAvg, a regularization term with importance weight \u03bc is added to the original loss function. In our experiments, \u00b5 is set to 0.01.\n\u2022 FedRep: Compared with FedAvg, FedRep decouples the model into two parts, i.e., globally shared representation layers and personalized client-specific heads.\n\u2022 FedRoD: There are two loss values and two predictors in the FedRoD framework, which decouples the duty of local model for generic FL and personalized FL, respectively.\n\u2022 FedProto: Instead of transmitting model parameters between clients and the server, class-specific prototypes serve as the information carrier to improve the tolerance to heterogeneity in FL.\nModel Architecture We use the LeNet (LeCun et al. 1998) as the local model. The concrete model architecture is shown in Table 7."}]}