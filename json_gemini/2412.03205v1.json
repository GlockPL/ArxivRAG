{"title": "U-MATH: A UNIVERSITY-LEVEL BENCHMARK FOR EVALUATING MATHEMATICAL SKILLS IN LLMS", "authors": ["Konstantin Chernyshev", "Vitaliy Polshkov", "Ekaterina Artemova", "Sergei Tilga", "Alex Myasnikov", "Vlad Stepanov", "Alexei Miasnikov"], "abstract": "The current evaluation of mathematical skills in LLMs is limited, as existing benchmarks are either relatively small, primarily focus on elementary and high-school problems, or lack diversity in topics. Additionally, the inclusion of visual elements in tasks remains largely under-explored.\nTo address these gaps, we introduce U-MATH, a novel benchmark of 1,100 unpublished open-ended university-level problems sourced from teaching materials. It is balanced across six core subjects, with 20% of multimodal problems. Given the open-ended nature of U-MATH problems, we employ an LLM to judge the correctness of generated solutions. To this end, we release \\(\\mu\\)-MATH, a dataset to evaluate the LLMs' capabilities in judging solutions.\nThe evaluation of general domain, math-specific, and multimodal LLMs highlights the challenges presented by U-MATH. Our findings reveal that LLMs achieve a maximum accuracy of only 63% on text-based tasks, with even lower 45% on visual problems. The solution assessment proves challenging for LLMs, with the best LLM judge having an F1-score of 80% on \\(\\mu\\)-\u039c\u0391\u03a4\u0397\nWe open-source U-MATH, \\(\\mu\\)-\u039cMATH, and evaluation code on GitHub.\u00b9", "sections": [{"title": "1 INTRODUCTION", "content": "Mathematical reasoning is a fundamental domain for assessing the true capabilities of Large Language Models (LLMs) to reason (Ahn et al., 2024). While existing benchmarks like GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021) provide valuable insights, they primarily focus on school-level mathematics. This leaves a significant gap in understanding how LLMs perform on more advanced, university-level problems. Moreover, these benchmarks are becoming saturated, as GPT-4, using advanced prompting techniques, has achieved over 92% success rate on GSM8K and 80% on MATH (Achiam et al., 2023).\nRecent works, such as CHAMP (Mao et al., 2024) and MathOdyssey (Fang et al., 2024), aim to introduce more challenging problems but are limited in size (<400 samples) and lack comprehensive topic coverage. The most challenging problems stem from school-level competitions or olympiads, missing the crucial middle ground of university-level coursework that reflects academic demands.\nFurthermore, there is a growing interest in assessing multi-modal LLMs' abilities to perform mathe-matical reasoning involving visual elements (Ahn et al., 2024). Large datasets like MathVista (Lu et al., 2023), We-Math (Qiao et al., 2024), or MathVerse (Zhang et al., 2024) provide an extensive set of (mostly) visual tasks but may lack university-level problems and often rely on multiple-choice validation, leading to easier problems and faster saturation of benchmarks."}, {"title": "2 BACKGROUND", "content": "Enhancing and evaluating the mathematical reasoning capabilities of LLMs is essential in Al research (Ahn et al., 2024). Studies show that finetuning with mathematical and code-related data enhances models' general skills (Prakash et al., 2024). Mathematical tasks require logical thinking and multi-step problem-solving, thus improving overall reasoning abilities in LLMs (Chen et al., 2024).\nThis leads to the problem of evaluating LLM's math abilities. Despite the significant progress, many existing benchmarks are limited in scope, focusing primarily on school-level mathematics or limited in size and topic coverage."}, {"title": "Textual Mathematical Benchmarks.", "content": "Early efforts to assess LLMs' mathematical abilities have emerged in datasets like MathQA (Amini et al., 2019) and the mathematics subset of MMLU (Hendrycks et al., 2020). These early benchmarks emphasized the importance of operation-based reasoning in solving mathematical word problems, typically in a multiple-choice format. Nowadays, even smaller models (e.g., 7B parameters) have achieved high scores on these tasks (Li et al., 2024b), suggesting that these benchmarks are becoming saturated. In response, more comprehensive datasets have emerged, such as GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021), or MGSM (Shi et al., 2022) (multilingual version of 250 GSM8K samples). These popular benchmarks are crucial for evaluating LLMs' mathematical reasoning skills. However, they primarily focus on school-level problems, which may not fully assess the depth of mathematical reasoning.\nRecent efforts attempt to address more advanced mathematical concepts. MathOdyssey (Fang et al., 2024) with competition problems, OCWCourses (Lewkowycz et al., 2022) from actual MIT courses, and ProofNet (Azerbayev et al., 2023) focusing on proofs aim to evaluate undergraduate-level or olympiad-level knowledge. However, these datasets are constrained by their small sizes (e.g., 387, 272, and 371 samples), limiting their statistical robustness and topic coverage. For example, MathOdyssey is limited to 101 samples in university-level topics (Calculus, Algebra, and Diff. Equations and Statistics). Other specialized datasets like MiniF2F (Zheng et al., 2021) provide valuable parallel corpora in formal languages, while CHAMP (Mao et al., 2024) offers helpful context and hints, but both are similarly limited in scale with 244 and 270 samples. Additionally, both heavily rely on already published resources: CHAMP sources material from a book, while MiniF2F re-uses international olympiads and MATH dataset problems. An attempt to provide a more robust evaluation,"}, {"title": "Visual Mathematical Benchmarks.", "content": "As multimodal LLMs gain prominence, there is a growing need for visual mathematical benchmarks (Zhang et al., 2024; Qiao et al., 2024). Early efforts in this domain focus primarily on geometric problems, as seen in datasets like GeoQA (Chen et al., 2022b), UniGeo (Chen et al., 2022a), and Geometry3K (Lu et al., 2021). These datasets have a narrow focus that does not encompass the breadth of mathematical visual reasoning required at advanced levels.\nMore recent benchmarks attempt to broaden the scope of visual mathematical evaluation. One of the first comprehensive attempts is the mathematical subset of MMMU (Yue et al., 2023), which offers 505 college-level multiple-choice questions, all with images. However, its multiple-choice format limits the complexity of problems that can be posed. MathVista (Lu et al., 2023) collects 28 existing datasets and introduces 3 new datasets with a total of 5k samples (1k testmini samples). However, as shown by Qiao et al. (2024), it faces challenges with data quality due to its compilation from older datasets.\nThe latest benchmarks, such as MATH-V (Vision) (Wang et al., 2024a) and We-Math (Qiao et al., 2024), extend this approach to collect 3k and 1.7k visual samples, respectively. However, both datasets rely on multiple-choice questions in the test set, leading to faster saturation. MathVerse (Zhang et al., 2024) further extends this approach, relying on visual elements and providing some simple text problems with 1.2k brand-new samples. Among these, only the We-Math dataset includes university-level mathematical problems.\nOur U-MATH dataset improves on existing benchmarks with 225 of 1,100 university-level problems that require visual elements (graph, table, diagram) to be solved. This balanced ratio ensures models are challenged to handle both traditional and visual problem-solving without over-relying on visuals, mirroring real-world scenarios."}, {"title": "Large Language Models for Mathematics.", "content": "The application of LLMs to mathematical problem-solving shows promising results, particularly with models like GPT-3.5 and GPT-4 demonstrating strong reasoning abilities for complex tasks such as those in the MATH dataset (Achiam et al., 2023). While open-source models initially lagged in performance on advanced mathematical tasks, the Llama-3.1 (Dubey et al., 2024) is approaching parity with proprietary models. The most popular benchmarks, MATH and GSM8K, are nearing saturation, with Llama 3.1 405B achieving scores of 73.8% and 96.8%, respectively. Similarly, a Qwen2.5-Math-72B model (Yang et al., 2024b; Team, 2024) reach 85.9% on MATH while Qwen2-Math-72B (Yang et al., 2024a) reaches 96.7% on GSM8k.\nTo enhance LLMs' mathematical capabilities, researchers develop various prompt-based methods (Liu et al., 2021). These include techniques for encouraging chain-of-thought generation (Wei et al., 2022), selecting final results from multiple sampled outputs (Wang et al., 2022), and using external tools such as calculators, WolframAlpha or Python interpreters (Gao et al., 2023) to reduce arithmetic errors. Additionally, instruction tuning during pre-training has been identified as a key factor in improving performance (Wang et al., 2017). While these approaches show promise, their effectiveness on university-level problems still needs to be explored due to the lack of suitable large-scale benchmarks."}, {"title": "Mathematical solution verification.", "content": "Evaluating mathematical solutions is uniquely challenging due to the open-ended nature of answers and the inherent ambiguity in mathematical expressions. Consequently, many benchmarks opt for multiple-choice formats due to their grading simplicity. However, this approach often simplifies tasks, providing hints that models can exploit (Li et al., 2024c; Pezeshkpour and Hruschka, 2023).\nWhile free-form evaluation using LLM judges is widespread (Zheng et al., 2023), it is known to introduce potential errors (Zheng et al., 2023), since evaluating mathematical solutions is a complex task in its own right (Zeng et al., 2023; Xia et al., 2024). These evaluation errors are largely overlooked and unaccounted for, limiting the reliability of inferences drawn from such evaluations."}, {"title": "3 U-MATH", "content": "We present U-MATH (stands for University Math) a benchmark designed to challenge LLMs with problems requiring deep understanding and advanced reasoning. The problems span 6 core topics and range in difficulty and number of questions. A subset of 20% of problems includes images to test the models' ability to interpret and reason with graphical information. Reference solutions and answers accompany all problems.\nAccuracy is the primary performance metric for U-MATH, its text-only problems (U-MATHT) and problems that include a visual component (U-MATH). The main performance measure for \\(\\mu\\)-\u039c\u0391\u03a4\u0397 is macro-F1.\nWe use an LLM as a judge (Zheng et al., 2023) to measure the accuracy of the free-form answers against the golden solutions. A problem is considered solved only if all required questions are answered and all requested items (e.g., all saddle points) are correctly identified."}, {"title": "3.1 DATASET COLLECTION", "content": "To create a benchmark that authentically reflects university-level mathematics, we collaborate with Gradarius, a platform providing learning content and software for top US universities specialized in mathematics. The problems are sourced from ongoing courses across various institutions currently run on the Gradarius platform. Problems and solutions are crafted by subject matter experts and represent real-world academic standards. These samples are unpublished and have not been exposed to any external sources. Thus, the dataset could not be leaked to current LLMs.\nWe employ a multi-stage filtering process to select challenging problems from tens of thousands of available samples. First, we filter out problems with short solutions (< 100 characters) and problems in multiple-choice format. As LLMs are not designed to perform arithmetic calculations and are prone to errors (Hendrycks et al., 2021; Lewkowycz et al., 2022), we focus on testing mathematical reasoning rather than calculations. We filter out problems marked as allowing calculator usage. As for the visual problems selection, we chose to keep problems with a single image for convenience.\nNext, we employ several small LLMs (LLaMA-3.1-8B (Dubey et al., 2024), Qwen2-7B (Yang et al., 2024a), Mistral-7B (Jiang et al., 2023), Mathstral-7B, NuminaMath-7B (Beeching et al., 2024)) to solve the problems. We select 150 most challenging problems for each subject based on the average problem solution rate. For this step, we use the same pipeline as described in Section4. This way, we ensure that none of the individual models influence problem selection largely and that there is no overfitting to a specific LLM. As the last step, we hold extra validation high risk problems (with low solve rate) using our in-house math experts and Gradarius content team.\nAfter collection, we enlist a team of experts from the Stevens Institute of Technology, who actively teach various Calculus courses. The experts verify that each problem is suitable either for assessing the subject knowledge expected of college or university students or for testing prerequisite knowledge."}, {"title": "3.2 DATASET STATISTICS", "content": "The U-MATH benchmark comprises 1,100 carefully curated and validated mathematical problems. These problems are distributed across 6 core subjects with about 20% of the tasks incorporating visual elements, such as graphs, tables, and geometric figures, mirroring the multi-modal nature of real-world mathematical problems: Precalculus (Review), Algebra, Differential Calculus (+Differential Equations), Integral Calculus, Multivariable Calculus, and Sequences & Series."}, {"title": "3.3 META-EVALUATION FRAMEWORK (\\(\\mu\\)-\u039c\u0391\u03a4\u0397)", "content": "The evaluation of mathematical problems is not straightforward. Even simple expressions such as x. 0.5 may have valid forms like, x \u00f7 2, x/2, or unsimplified variants like 9x/18. In practice, evaluating free-form solutions requires testing expression equivalence in much less trivial cases, especially with more advanced problems (refer to Section A.3 in Appendix for an example).\nTo systematically study the ability of LLMs to evaluate free-form mathematical solutions on advanced, university-level problems, we introduce the \\(\\mu\\)-MATH (Meta U-MATH) benchmark. It consists of a curated subset of U-MATH samples, supplied with LLM-generated solutions both correct and not. The solutions are labeled using a combination of manual inspection and automated verification via Gradarius-API, which allows to test formal equivalence of mathematical expressions.\nWe selected 271 U-MATH problems (around 25%) based on their assessment difficulty to create a challenging meta-evaluation set. This subset does not aim to reflect the overall U-MATH distribution but rather to provide a robust test for LLM judges. We focused on text-only problems, excluding those needing images, due to the limited size of the labeled U-MATH subset. Four solutions have been generated for each of the selected problems - using Qwen2.5-72B, Llama3.1-8B, GPT-40 and Gemini-1.5-Pro models 1084 samples in total.\nA tested model is provided with a problem statement, a reference answer, and a solution to evaluate. We treat this as a binary classification task, using the macro-averaged F1-score as the primary metric to minimize the effect of class imbalance. Additionally, we report Positive Predictive Value (PPV or Precision) and True Positive Rate (TPR or Recall) for the positive class as well as Negative Predictive Value (NPV) and True Negative Rate (TNR) for the negative class, offering a finer-grained performance evaluation. We also report all of the scores computed both across the entire set of samples and only across those with solutions produced by a specific model, separately for each of the author models."}, {"title": "4 EXPERIMENTS AND RESULTS", "content": "We select some top-performing recent LLMs to evaluate."}, {"title": "4.1 EXPERIMENTAL SETUP", "content": "All LLMs are tested using the same prompts and settings for fair comparison. The LLMs are restricted to a single generation of 4096 tokens with the temperature set to 0. We employ chain-of-thought (CoT) prompting (Wei et al., 2022) to encourage models to 'think' before providing an answer. mages are included directly in the prompts for multimodal LLMs. To text-only LLMs the problem description is provided as-is without visual elements.\nWe report accuracy based on GPT-40-2024-08-06 as-a-judge for our final results, despite it not being the best performing judge - due to the model still residing among the top-ranked judges, being the more conservative one in terms of false positive rate, as well as widely available, leading to easier reproducibility. Details on the judgment setup and comparisons of various judges are all discussed in Section 4.3."}, {"title": "4.2 U-MATH RESULTS", "content": "Figure 2 compare popular text-only and multimodal models in U-MATH as well as U-MATHText and U-MATHvisual. Table 4 summarizes the performance of all evaluated LLMs on the U-MATH benchmark. Reference to Appendix E for model performance vs model size comparison.\nAmong text-only models, the math-specific model Qwen2.5-Math-72B achieves the highest overall accuracy at 50.2%, showcasing strong mathematical reasoning capabilities. In the multi-modal model group, Gemini-1.5-pro-002 leads with an overall accuracy of 60.1%, highlighting the advantages of integrating visual processing. In contrast, best open-weights model Qwen2-VL-72B lacks mathematical abilities in visual and textual tasks with 31.2% on a U-MATH benchmark. Building on these results, several key trends emerge:\n\u2022 Model Size vs. Specialization: Larger models expectedly outperform smaller ones. However, the small specialized model Qwen2.5-Math-7B surpasses or performs on par with 10 times larger models like Qwen2.5-72B or LLaMA-3.1-70B and almost reaching leading Gemeni-1.5-Pro level. On the other hand, Pixtral-12B performs consistently worse than minor Qwen2-VL-7B, indicating a lack of university-level data in training.\n\u2022 Textual vs. Visual Problem-Solving: Across multimodal models, text-only problems' accuracy vastly exceeds visual problems, highlighting areas for further improvement. The text-only models"}, {"title": "4.3 META-EVALUATION (\\(\\mu\\)-MATH) RESULTS", "content": "For meta-evaluation we use the same setup as described in Section 4.1. Additionally, we experiment with two distinct prompting schemes - a standard Automatic Chain-of-Thought (AutoCoT) prompt involving a simple task description together with an instruction to think step-by-step, and a manual Chain-of-Thought prompt (which we refer to as simply CoT) with explicit instructions on which steps to follow when approaching the task. We find the latter prompting scheme to perform best, so we use manual CoT for the main results. The judge's output is also further processed by an extractor model (Qwen2.5 72B is fixed for consistency), prompted to produce a single label either 'Yes', 'No' or 'Inconclusive'. We include 'Inconclusive' for cases when judge refuses to evaluate or generation fails; such judgments are treated as incorrect. Reference Appendix C.2 for full contents of all the prompts.\nWe find that using manual CoT instructions instead of the standard AutoCoT improves or main-tains judgment performance, save for Llama models, as shown in Table 5. Llama's performance drop is largely due to increased inconclusive judgment rates (see Appendix G). At the same time, Gemini models benefit the most from this transition, gaining over 10% in F1-score and becoming the top-ranked models, surpassing Qwen and GPT models that outperform Gemini in the AutoCoT setting. This shows that prompting effects are substantial yet inhomogeneous across models. Please refer to Appendix F for a visual comparison.\nIn terms of the resulting performance, we see that correctly identifying a positive label is harder on average compared to negative labels, with the best TPR being almost 10% lower than the best TNR, and that the best attainable F1 score is only 80.7%. This constitutes a considerable deficiency in the context of judgment, because judges' error rates directly limit the precision of capability evaluations, potentially even biasing them in case the errors are systematic in nature as opposed to pure noise.\nOur results, for instance, reveal a consistent bias towards some models better performance on Llama solutions and worse performance on Qwen solutions most pronounced with smaller-sized judges and AutoCoT prompting. This bias is generally reduced for both small and large judges when transitioning to CoT prompting, which is also illustrated with Figure 3. At the same time, no noticeable 'self-judgment' effects are found.\nIt is also evident that being a better solver does not necessarily lead to being a better judge. In fact, our results suggest a trade-off existing between these skill; refer to Appendix H for visualizations and a more detailed discussion."}, {"title": "5 CONCLUSION", "content": "We introduce U-MATH, a novel multimodal benchmark for evaluating the university-level mathe-matical reasoning of LLMs. U-MATH includes 1,100 unpublished free-form problems from real teaching materials, covering 6 core mathematical subjects, with 20% involving image-based reason-ing. Additionally, we provide \\(\\mu\\)-MATH, a meta-evaluation dataset, to assesses LLMs' ability to evaluate free-form mathematical solutions.\nOur experiments highlight significant challenges for LLMs in advanced reasoning and visual problem-solving. The highest accuracy achieved was 63.4% on text-based tasks and 45.0% on visual problems (Gemini-1.5-pro-002). Solution assessment remains difficult, with Gemini hiy top \\(\\mu\\)-MATH F1-score of 80%, showing room for improvement and underscoring the limitations of widely used models like GPT-40 in evaluation tasks.\nLimitations. While U-MATH offers diverse university-level problems, it does not cover the full range of advanced topics and may introduce biases by favoring certain problem types. Also, selection process may introduce biases, potentially favoring certain problem types or difficulty levels (e.g., more accessible topics like Precalculus and Algebra). The inclusion of 20% visual problems, yet reflect real distribution, limits the evaluation of visual reasoning. Furthermore, reliance on LLMs for valuation introduces potential, as models struggle with complex reasoning and instructions, evidenced by our findings with the \\(\\mu\\)-MATH. The \\(\\mu\\)-MATH dataset encompass of 25% of U-MATH problems narrows the evaluation scope, but provide 4 diverse model families as solution generators.\nFuture Work. Future research can focus on enhancing LLM performance by integrating existing tool-augmented models and exploring their effectiveness on U-MATH and \\(\\mu\\)-MATH tasks. For instance, incorporating external tools, such as formal solvers, could improve complex textual and multimodal reasoning capabilities. Additionally, our findings indicate that widely used models like GPT-40 are not a silver bullet for solution evaluation; thus, developing specialized (finetuned) models or techniques for more accurate and unbiased assessment is a promising direction. Expanding \\(\\mu\\)-MATH with formal verification methods could further enhance the evaluation processes. Additionally, conducting deeper prompt sensitivity analyses would provide valuable insights for the field.\nBy open-sourcing U-MATH, \\(\\mu\\)-MATH, and the evaluation code, we aim to facilitate further research in advancing the mathematical reasoning capabilities of LLMs and encourage the development of models better equipped to tackle complex, real-world mathematical problems."}, {"title": "A.1 U-MATH PROBLEMS", "content": ""}, {"title": "A.2 U-MATH PROBLEM AND SOLUTION", "content": "The function s(t) = 2 \u2022 t\u00b3 \u2013 3 \u2022 t\u00b2 12t+8 represents the position of a particle traveling along a horizontal line.\n1. Find the velocity and acceleration functions.\n2. Determine the time intervals when the object is slowing down or speeding up.\nGolden answer:\n1. The velocity function v(t) = 6 \u00b7 t2 6 \u00b7 t - 12 and acceleration function a(t) = 12 t - 6\n2. The time intervals when the object speeds up (0, \u00bd), (2, \u221e) and slows down (\u00bd, 2)\nReference solution:\nThe velocity is the derivative of the position function:\nv(t) = s'(t) = 6t2 6t-12\nThe acceleration is the derivative of the velocity function:\na(t) = v'(t) = 12t - 6\nTo determine when the object is speeding up or slowing down, we compare the signs of v(t) and a(t).\nStep 1: Find the Zeros of v(t) and a(t)\nFirst, solve for v(t) = 0:\n6t2 6t 12 = 0\n\u21d2 t2 - t - 2 = 0 \u21d2 (t - 2)(t + 1) = 0\nThus, t = 2 and t = -1.\nNext, solve for a(t) = 0:\n12t - 6 = 0\n\u21d2 t=\\frac{1}{2}\nStep 2: Analyze the Signs of v(t) and a(t)\nWe analyze the signs of v(t) and a(t) on the intervals determined by t = -1, t = \u00bd, and t = 2.\nThe object is speeding up on (0, \\frac{1}{2}) and (2,\u221e) and slowing down on (\\frac{1}{2}, 2)."}, {"title": "\u0391.3 \u03bc-MATH META-EVALUATION", "content": ""}, {"title": "B SUB-TOPICS DISTRIBUTION", "content": "The U-MATH dataset cover variety of topics across 6 core subjects. Below is the count of unique topics per subject:\n\u2022 Differential Calculus: 51 unique topics\n\u2022 Sequences and Series: 28 unique topics\n\u2022 Integral Calculus: 35 unique topics\n\u2022 Precalculus Review: 19 unique topics\n\u2022 Algebra: 74 unique topics\n\u2022 Multivariable Calculus: 53 unique topics"}, {"title": "C PROMPTS", "content": ""}, {"title": "C.1 PREDICTION PROMPT", "content": ""}, {"title": "C.2 JUDGMENT PROMPTS", "content": ""}, {"title": "D SOLUTION PREDICTIONS LENGTH DISTRIBUTION", "content": ""}, {"title": "E MODEL ACCURACY VS SIZE", "content": ""}, {"title": "F \u03bc-MATH PROMPTING SCHEMES COMPARISON", "content": ""}, {"title": "G\u03bc-MATH INCONCLUSIVE JUDGMENT RATES", "content": ""}, {"title": "H COMPARISON OF PROBLEM SOLVING AND JUDGMENT PERFORMANCE", "content": "In this section, we provide a detailed comparison of models' performances on U-MATH and \\(\\mu\\)-MATH. The overall distribution of scores visualized in Figure 14 not only shows that improved problem-solving performance does not immediately lead to better judgment performance, as discussed in Section 4.3, but also suggests a possible trade-off present between these capabilities.\nThis possibility is further illustrated when considering specific models. For instance, Qwen2.5-Math demonstrates strong problem solving compared to most of the other models, but does so at the expense of weaker instruction following eye-gaze inspections reveal this model to be struggling with instruction comprehension and adherence to formatting rules leading to lower relative judgment performance. In contrast, Claude does not rank low as a judge despite its weak performance on U-MATH. Meanwhile, Gemini, known to excel in both mathematical problem solving and instruction following, comes out as the top-ranked judge."}, {"title": "\u0399\\(\\mu\\)-MATH BEHAVIOR OF JUDGES", "content": "In Figure 15 we visualize the difference in 'performance profiles' of the judges which we've dis-cussed in Section 4.3 - proprietary models behaving more conservatively and Qwen family models exhibiting the opposite tendencies.\nThis is in line with eye-gazing inspections suggesting that:\n\u2022 Qwens tend to 'follow the solution' and are also good at going into involved derivation chains necessary to arrive at a true positive verdict in more complex scenarios, albeit at the cost of increased hallucination risk.\n\u2022 Proprietary models are more \u2018anchored on the label' and less heavy on long hallucination-prone transformation chains, which comes at the expense of missing more complex true positives.\nNotably, Claude Sonnet 3.5 and Qwen2.5-Math 72B are the 'opposite extremes' of the observed patterns - having respectively the highest overall TNR and highest overall TPR with an approxi-mately equal F1-score. To illustrate the patterns, Appendix J provides an example comparing the Claude's and Qwen's judgments on a single \\(\\mu\\)-\u039c\u0391\u03a4\u0397 sample. Notice how Claude is restrictive and superficial in its comparison, whereas Qwen \u2018loses the structure' along the way, designating only the first two steps prescribed with the CoT prompt (see prompt contents in Appendix C.2) and omitting points three and four.\nThese effects are further amplified with reduction in model size: proprietary models mainly lose in TPR when moving from larger models to smaller ones, whereas Qwens, once again on the contrary, lose more in TNR. A possible interpretation is that model size helps to mitigate the natural tendencies potentially induced by the training data, perhaps due to better generalization across general and domains-specific skills, or due to increased reliability.\nAlso, among the higher-ranking judges, the large Qwen2.5 and Gemini models turn out to be the two most balanced representatives of their respective source classes. We speculate that these two models are trained with data explicitly balancing math-specific skills and general capabilities: Gemini being heavily optimized for both math and instruction following, and Qwen likely trained on a blend containing synthetic data produced by various specialist models, including mathematical synthetic data from Qwen2-Math.\nThe behavioral differences may also be observed with predicted label agreement rates between judges, see Figure 16 for the comparison. Interestingly, no pair of models has agreement above around 80% - even for same-family models like Qwen2.5 and Qwen2.5-Math - despite the pairwise \\(\\mu\\)-\u039c\u0391\u03a4\u0397 performance deltas being small compared to 20% disagreement.\nAll of this shows that judge comparison is substantive beyond the one-dimensional choice of the better model and suggests judge ensembling to be a potentially fruitful approach to evaluation."}, {"title": "J\\(\\mu\\)-MATH JUDGMENT EXAMPLES", "content": ""}]}