{"title": "Empathic Coupling of Homeostatic States for Intrinsic Prosociality", "authors": ["Naoto Yoshida", "Kingson Man"], "abstract": "When regarding the suffering of others, we often experience personal distress and feel compelled to help. Inspired by living systems, we investigate the emergence of prosocial behavior among autonomous agents that are motivated by homeostatic self-regulation. We perform multi-agent reinforcement learning, treating each agent as a vulnerable homeostat charged with maintaining its own well-being. We introduce an empathy-like mechanism to share homeostatic states between agents: an agent can either observe their partner's internal state (cognitive empathy) or the agent's internal state can be directly coupled to that of their partner's (affective empathy). In three simple multi-agent environments, we show that prosocial behavior arises only under homeostatic coupling \u2013 when the distress of a partner can affect one's own well-being. Our findings specify the type and role of empathy in artificial agents capable of prosocial behavior.", "sections": [{"title": "1 Introduction", "content": "For humans and other social animals, it is often distressing to regard the suffering of others. We feel empathy, sharing in the feelings of others rapidly and automatically through emotional contagion [15]. Such feelings can provide a strong motivation to reduce the suffering of others, even if it comes at some cost to the self. It has been proposed that tying one's own welfare to the welfare of others can form the basis of prosocial behavior [9].\nEmotions and feelings, whether self- or other-directed, are theorized to arise from homeostasis, the regulation of internal body states within a range compatible with life [6]. Self-regulatory mechanisms have previously been implemented as a source of external motivation [26, 2]. Here we regard homeostasis [3] as an intrinsic and obligatory motivation of all living creatures. Homeostatic-like processes have recently been implemented in reinforcement learning (RL) agents and have resulted in the emergence of integrated behaviors [36, 38, 37, 39]. There are many discussions of intrinsic motivation linked to artificial curiosity or exploration [28, 29, 2, 26, 25, 10]. Also, homeostasis-based rewards have also been reported to facilitate exploration in the environment [11]. In contrast to the discussion of the efficient exploration, here we start from the minimal condition for prosocial behavior originally proposed in the field of animal behavior. Under this condition, we show computationally that prosocial behavior does not arise from the individual's homeostatic reward alone. We then introduce the hypothetical homeostatic coupling term in the individual's reward function,"}, {"title": "1.1 Homeostatic Reinforcement Learning", "content": "RL provides a framework to learn behavior in dynamic environments that maximizes the sum of future rewards emitted from the environment [34]. The objective of RL is to obtain a policy \\(\\pi : S \\rightarrow A\\) that maximizes the expected value of the weighted cumulative sum of future rewards \\(\\Sigma_{t=0}^{\\infty} \\gamma^t r_t\\) for all states \\(s \\in S\\), based on the experience of the interactions with an environment. Here, \\(t\\) is the time step, \\(r\\) is the reward signal, \\(S\\) is the set of states in the environment, \\(A\\) is the set of actions of the agent. \\(0 < \\gamma < 1\\) is a positive constant called the discount factor.\nHomeostatic RL [22, 21, 16] integrates principles from physiological homeostasis by defining reward as the internally perceived reduction of deviations from homeostatic setpoints [20, 19, 35, 12]. Concretely, the reward is defined as a quantity proportional to the temporal difference of the drive \\(D\\), as \\(r_{t+1} = \\beta(D_t - D_{t+1})\\), where \\(\\beta\\) is the scaling constant [22]. The drive function \\(D(s^i)\\) returns a value greater than or equal to zero, such as the distance between \\(s^i\\) and \\(s^*\\). Here \\(s^i\\) is interoception [33] that monitors the internal state of the agent's body [33] and \\(s^*\\) is the setpoint of the interoception. Homeostatic parameters are not arbitrarily defined, but are fundamental to the viability and functionality of the agent [21, 24]. In our conception, homeostatic RL asserts that the agent has a vulnerable body. Vulnerability defined as the circular causality by which homeostatic states can affect the agent's ability to regulate those states (i.e., it gets harder to take care of oneself as one falls apart).\nWe used Proximal Policy Optimization (PPO, [30, 31]) as the RL optimizer in all of our experiments. The agent's policy model consisted of an encoder of inputs using a multi-layer perceptron, a recurrent connection using LSTM [14], and softmax outputs for the categorical action probabilities in all experiments. Further experimental details are given in Appendix A."}, {"title": "2 Experiment 1: Food Sharing Environment", "content": "Inspired by ethology experiments on the altruism of monkeys (Figure 1A) [7], we first created a minimal system for studying prosocial behavior. An overview of this environment is shown in Figure 1B. It has been reported that brown capuchins that are separated by a mesh will choose to share food when only one monkey has access to the food [7, 8]. This study explores the minimum configuration in which such sharing behavior occurs in autonomous agents.\nIn this environment, we assume two agents. The first is a passive agent called the 'Partner', corresponding to the monkey in the left side of the cage with no direct access to food (Figure 1A). The other agent is the 'Possessor', corresponding to the monkey on the right of Figure 1A, and who has access to food. Each agent has a binary energy state (High or Low). When the state is High, it transitions to Low with a small probability of \\(p = 0.1\\) at each time step. If the energy state is Low, it remains unchanged until the agent is able to eat food. If either one of the agents' energy states becomes Low and 10 steps have passed, the episode is failed and the environment is reset. In"}, {"title": "2.1 Results", "content": "Average learning curves are shown in Figure 2A. Performance is evaluated by episode duration, with a maximum length of 2000 steps. In the None condition, the PASS action is rarely selected 2B and episode lengths did not increase over training. Similar results are obtained in the Cognitive condition, in which the Possessor observes, but is not motivated by, the Partner's homeostatic state. On the other hand, the homeostatic states of both agents are maintained under the Affective and Full conditions, leading to long episode durations. In the Affective condition, the Possessor does not have explicit knowledge of the Partner's energy state and so frequently chooses the PASS action to help the Partner maintain homeostasis, thereby also regulating its own homeostatic state because it is coupled to that of the Partner's. This suggests that a strategy was acquired to maintain homeostasis between the two by supplying an excess of food to the Partner."}, {"title": "3 Experiment 2: Testing in Dynamic Environments", "content": "Next, we investigated the generalizability of the findings from the food-sharing environment to 1-D and 2-D environments with mobile agents. The first is a linear grid environment (Figure 3A), in which the Partner is, once again, trapped on the left side of the grid without access to food. The Possessor can acquire food at the far right side with the GET action, and increase their energy level with the EAT action. The Possessor can move LEFT and RIGHT to shuttle food to their Partner, and finally PASS it to the Partner when they are next to each other, increasing the Partner's energy. Additionally, energy states are now represented as a continuous variable with a fixed rate of energy consumption. Further experimental details are in Appendix \u0412.2.\nThe second mobile environment is one in which both agents can move on a two-dimensional field (Figure 3B). In this environment, there is no distinction between Partners and Possessors, as both agents can move and act freely. Food energy can be collected, consumed, and shared, as in the linear grid environment. However, if an agent's energy level decreases below some threshold, it becomes immobile and slowly starves. It then relies upon its Partner to share food in order to recover some energy and regain mobility. Further details, including on the small chance of random immobilization irrespective of energy level (\u2018injury'), are in Appendix B.3."}, {"title": "3.1 Results", "content": "Figures 4 and 5 show the results of optimization in each mobile environment. No prosocial behavior was observed in the None and Cognitive conditions, and episode durations remained short (Figure 4A and 5A). As in Figure 4B, the variance of the homeostatic drive of the Partner (\\(D_{partner}\\)) is large in the Affective condition. One possible explanation is that the Possessor agent cannot observe the energy state of its Partner, therefore the Partner agent is fed indiscriminately in the Affective condition, at various energy values (Appendix D). Figure 5B captures a sequence of prosocial behavior observed in the Affective condition. The blue agent is immobilized due to its low energy level. The red agent collects a green food pellet and returns to share it with the blue agent, turning it purple (replenishing some energy) and restoring it to mobility."}, {"title": "4 Discussions", "content": "This study investigated the emergence of prosocial behavior in simple RL agents motivated by homeostatic self-regulation. We found that prosocial behavior (food sharing) only occurred reliably under affective empathy, when the homeostatic states of agents were coupled. Perception of a partner's state of need did not, on its own, drive prosocial behavior. The combination of cognitive and affective empathy in the Full condition drove more selective sharing behavior.\nFuture research could explore more realistic empathy implementations, moving beyond giving agents direct access to partners' internal states. One possibility to achieve and maintain high group well-being is to implement a form of mutual information [18] in sequential social dilemmas [23], such that successfully self-regulating agents can influence each other and induce the well-being of others. Agents may also be designed to infer others' internal states from observable emotional behaviors. This process would better resemble the mirror neuron system, hypothesized to support emotion recognition and empathic behavior in humans and other animals [27, 17]. For example, neurons in the inferior parietal lobule activate both during the observation and imitation of emotions; they can then trigger activity through the insula into the limbic system, known to activate during the firsthand experience of emotional feelings [4]."}, {"title": "B Details of Environments", "content": null}, {"title": "B.1 Food Sharing Environment", "content": "The Possessor has two actions. One is EAT, and the Possessor can recover the energy state of the agent described below by eating food. The other action is PASS, and the Possessor can feed the Partner. Both of these agents have a binary energy state (High and Low), and when the state is High, it changes to Low at a small probability of \\(p = 0.1\\) at each time step. If the energy state is Low, it is left unchanged at each time step, and only changes to High if the agent is able to eat the food. At the start of the environment, both energy states are randomly determined, and if either one of the agents' energy states becomes Low and 10 steps have passed, the environment is reset for both agent.\nIn this environment, only the action optimization of the Possessor is possible. The drive for the homeostasis of the Possessor is \\(D_{possessor} = |\\text{ln} P^*(s^i)|\\). Here, \\(s^i \\in \\{\\text{High}, \\text{Low}\\}\\) represents the agent's interoception at time \\(t\\), and \\(P^*(\\cdot)\\) is a probability distribution representing the desirability of each state, with \\(P^*(\\text{High}) = 0.95\\) and \\(P^*(\\text{Low}) = 0.05\\). Therefore, the Possessor aims for homeostasis, preferring \\(s^i = \\text{High}\\) over \\(s^i = \\text{Low}\\)."}, {"title": "B.2 Grid Environment", "content": "The first is a grid environment (Figure 3A), in which the Partner is fixed to the left side of the grid, just like in the Food Sharing environment. The Possessor can access the food area on the right side. The Possessor can move left and right and know their position (there are five positions in the environment). The Possessor can acquire food by arriving at the right side and selecting the GET action. As a result, the Possessor has two states: with food and without food. At this point, the Possessor can also eat the food (by selecting the EAT action), or move while holding the food and PASS it to the Partner. The Possessor can choose between five actions: move left or right, EAT, GET, and PASS.\nIn addition, the energy state of each agent is represented as a continuous variable. The dynamics of the energy state \\(s^i\\) is represented by \\(s_{t+1} = s_t - d_a + I_t\\). In this case, \\(d_a = 0.003\\) is a fixed constant that represents a certain amount of energy consumption. \\(I_t\\) is a function that returns 0.1 when the agent has ingested food, and 0 otherwise. In this experimental system, the drive function was given by the squared error \\(D = ||s^i||^2\\), and the agents were trained with a learning rate of \\(\\beta = 100\\). If the energy state of any of the agents deviated from the range [-1, 1], the episode was terminated."}, {"title": "B.3 2D Field Environment", "content": "This environment is one in which the agents can move around in a two-dimensional continuous space (Figure 3B). In this environment, there is no distinction between Partners and Possessors, and both agents can move around and consume food. The energy state changes in the same way as in the grid environment (with a dynamics of \\(d_a = 0.001\\), and energy is restored by 0.3 when food is consumed).\nIn addition, each agent can carry food, and if it is close enough to another agent while carrying food, it can give the food to the other agent. Also, in this environment, if an agent's energy level becomes less than -0.7, it is considered to be damaged and cannot move. Therefore, in such a situation, the agent needs to be helped to be able to move again by having another agent bring it food. Furthermore, when both agents are able to move, each agent encounters an accident at a small probability \\(p = 0.0005\\) at each time. If an agent encounters an accident, its energy value immediately becomes -0.7, and the agent becomes immobile.\nThe scaling of the drive function and reward was the same as in the grid environment. In training in this environment, all agents were trained in a situation where the network weights were shared."}, {"title": "CState-Transition Diagram of Food Sharing Environment", "content": "All the state transitions in the Food Sharing environment (Figure 7). From this figure, we can see that there is always a risk of the internal state transitioning from High to Low when the Possessor chooses the PASS action. This can be seen from the transitions of the blue and red macro states (corresponding to the Possessor's interoception) on the left and right when the PASS action is chosen. Therefore, it is always optimal for the homeostasis of the Possessor alone to choose the EAT action, and it is suggested that in such a situation, no action to help the Partner will emerge."}, {"title": "D Histogram showing the difference between the affective and full conditions in the Grid environment", "content": "null"}]}