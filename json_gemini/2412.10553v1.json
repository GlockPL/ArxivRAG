{"title": "Edge AI-based Radio Frequency Fingerprinting for IoT Networks", "authors": ["Ahmed Mohamed Hussain", "Nada Abughanam", "Panos Papadimitratos"], "abstract": "The deployment of the Internet of Things (IoT) in smart cities and critical infrastructure has enhanced connectivity and real-time data exchange but introduced significant security challenges. While effective, cryptography can often be resource-intensive for small-footprint resource-constrained (i.e., IoT) devices. Radio Frequency Fingerprinting (RFF) offers a promising authentication alternative by using unique RF signal characteristics for device identification at the Physical (PHY)-layer, without resorting to cryptographic solutions. The challenge is two-fold: how to deploy such RFF in a large scale and for resource-constrained environments. Edge computing, processing data closer to its source, i.e., the wireless device, enables faster decision-making, reducing reliance on centralized cloud servers. Considering a modest edge device, we introduce two truly lightweight Edge AI-based RFF schemes tailored for resource-constrained devices. We implement two Deep Learning models, namely a Convolution Neural Network and a Transformer-Encoder, to extract complex features from the IQ samples, forming device-specific RF fingerprints. We convert the models to TensorFlow Lite and evaluate them on a Raspberry Pi, demonstrating the practicality of Edge deployment. Evaluations demonstrate the Transformer-Encoder outperforms the CNN in identifying unique transmitter features, achieving high accuracy (> 0.95) and ROC-AUC scores (> 0.90) while maintaining a compact model size of 73KB, appropriate for resource-constrained devices.", "sections": [{"title": "I. INTRODUCTION", "content": "The growth of the Internet of Things (IoT), supported by the evolution of networks, from 4G to 5G and forward, has fundamentally transformed various sectors by enabling pervasive connectivity and real-time data exchange among heterogeneous devices. IoT devices are deployed at the network Edge and are often utilized for smart home, healthcare, industrial automation, and smart city applications [1]. However, the inherent resource constraints and massive scale deployment present significant security challenges, particularly in ensuring device-generated data legitimacy, authenticity, and integrity.\nSecurity in this landscape is critical, as the vast number of interconnected devices increases the attack surface, leading to networks being more vulnerable to attacks such as breaches, data tampering, and unauthorized access. A key challenge is to be practical for small-footprint resource-constrained devices. Cryptographic protocols can be of central importance, especially in terms of device authentication, even though the management of cryptographic keys can be complex. Further, cryptography can be hard, if not impossible, to implement in some classes of IoT devices - notably, backscatter tags [2], [3]. Hence, Radio Frequency (RF) Fingerprinting (RFF) has been proposed. Moreover, resources at the edge, with edge devices more powerful than IoT devices, can be used to implement security mechanisms.\nThe challenge lies in efficiently deploying effective RFF in resource-constrained environments. RFF is a technique that uses the Physical (PHY)-layer signal characteristics for wireless device authentication [4]. The imperfections in RF transmitters create a unique \"fingerprint\" for each device. These fingerprints are based on the inherent variations in the transmitted signals and can be captured through the In-phase (I) and Quadrature (Q) (IQ) components. This allows for reliable device identification and authentication without relying on traditional cryptographic approaches [5]. RFF enables differentiating between devices, even when they operate on the same protocol and frequency band.\nTo enhance RFF, Artificial Intelligence (AI) has been introduced to extract and learn complex patterns from RF signals. AI-based RFF leverages Machine Learning (ML), specifically Deep Learning (DL), to improve feature extraction that other methods may lack, hence improving the identification accuracy [6]. However, the fundamental question is whether edge devices can support such operations or not. While edge devices are typically more powerful than low-end IoT devices, they still lack the ample computational resources found in centralized cloud servers, making their ability to efficiently run AI-based RFF infeasible.\nFor instance, devices such as Raspberry Pi or NVIDIA Jetson, while more capable than basic IoT devices are still limited in terms of processing power, memory, and energy consumption. At the same time, AI-based RFF is not computationally cheap; building and training DL architectures typically demands significant resources. Additionally, real-time processing and inference that ensures accurate identification/authentication requires sufficient resources that edge devices may lack. The challenge lies in developing lightweight, optimized ML models that maintain high performance while being computationally feasible for edge devices.\nA limited number of recent contributions [7], [8] discussed deploying and operating RFF-based authentication on the edge. Only [7] considers AI-based RFF at the edge: The authors rely on transfer learning (i.e., use models architectures such as ResNet50 [9] that are not tailored for RFF) to build the Deep Learning (DL) network and use pruning to reduce the model size. Additionally, they do not consider evaluating other DL models or introducing lightweight architectures. Indeed, deploying AI-based RFF models on the edge enables real-time device authentication, reducing latency and further strengthening the network's security, hence the need for easy-to-build and train lightweight, high-performing, and deployable models.\nTo bridge this gap, this paper presents two lightweight AI-based RFF implementations tailored for edge devices. Our primary focus is on the training and deployment of RFF models tailored for edge/IoT devices, ensuring robust and scalable identification/authentication using PHY-layer characteristics. We dissect the building, training, and deployment of lightweight models on the edge, ensuring that they can perform a robust RFF-based authentication without performance degradation.\nContributions. In this paper, we introduce and evaluate two truly lightweight DL architectures specifically designed for PHY-layer authentication on edge devices. Additionally, unlike the state-of-the-art [7], [8], the presented models do not rely on transfer learning and ensure high training, testing, and prediction accuracy without the need for pre-trained models. We convert and optimize the DL models using TensorFlow Lite to reduce the size and enhance inference speed, ensuring that the models are suitable for deployment on edge devices, even those with limited computational resources. Moreover, we present the first Transformer-based RFF model that is deployable on edge devices. The implementation (code) will be released upon the acceptance of the paper. Finally, we evaluate the performance over 28 transmitters (from an open source dataset [10]) and show that the Transformer encoder achieves and maintains high accuracy and ROC-AUC, outperforming traditional Convolutional Neural Network (CNN)."}, {"title": "II. PRELIMINARIES", "content": "This section discusses the concepts essential for understanding the rest of the paper, specifically Deep Learning, Transformers, TinyML, and RF Fingerprinting.\nA. Deep Learning, Transformers, and TinyML\nDeep Learning (DL) is a subset of Machine Learning (ML) characterized by using neural networks with several layers, capable of learning from large datasets to perform various tasks such as Natural Language Processing (NLP) and speech recognition. Within this broader domain, Convolutional Neural Networks (CNNs) represent a specialized type of Neural Network (NN) that performs well in processing different types of data [11]. CNNs consists of convolutional layers, where parametrized filters, or kernels, slide over the input data to create feature maps by applying these filters to different input parts [12]. Through this process, CNNs hierarchically learns and combines basic patterns such as edges, textures, and shapes, which increase in complexity through successive layers, making them highly effective for computer vision, image and video recognition, object detection, and image segmentation tasks.\nTransformer is a NN architecture that generates output data by analyzing and understanding the context of the input data [13]. Unlike the sequential processing of data performed by Recurrent Neural Networks (RNNs), Transformers can process the entire data sequence simultaneously, allowing for extracting relationships and context within the data sequence. The overall Transformer architecture, (introduced in [13]), includes two primary components: the encoder and decoder. The encoder is tasked with processing the input data sequence, transforming it into a continuous representation that encapsulates contextual information. This is achieved through multiple layers, each incorporating a multi-head self-attention mechanism and a position-wise fully connected feed-forward network. The self-attention mechanism enables the encoder to assign varying degrees of importance to different parts of the input sequence, thereby generating a comprehensive representation of the data. Conversely, the decoder generates the output sequence by utilizing the representations produced by the encoder along with previously generated tokens. Similar to the encoder, the decoder is composed of multiple layers featuring a multi-head self-attention mechanism and a position-wise feed-forward network. Additionally, the decoder includes an encoder-decoder attention mechanism, which aligns the output sequence with relevant parts of the input sequence by attending to the encoder's outputs. This mechanism ensures that the decoder accurately maps the relationships between the input and output sequences.\nTinyML facilitates the deployment of ML and DL models on small, resource-constrained, low-powered devices. Traditionally, DL models are run on servers with powerful computational resources available, where edge devices are merely responsible for collecting data and transmitting it to the server for processing and classification. With the emergence of edge AI and TinyML, DL models can be deployed on the edge device itself, conserving transmission bandwidth. This is primarily important in smart cities, where areas such as smart healthcare [14], smart living [15], and smart mobility [16] applications require the use of edge AI to facilitate many processes. This includes self-driving cars, autonomous robots, and AI-powered smartphones [17]. Through the utilization of AI on edge, the collection of the data and inference is performed almost instantly on the edge itself without the need to send the data to the server for processing. To achieve deployable and lightweight models, several optimization methods can be used to convert the DL models and reduce their size in order to deploy them on low-power and low-memory edge devices without significantly affecting performance, including model quantization and pruning [18].\nB. Radio Frequency Fingerprinting\nRF Fingerprinting (RFF) is a technique that exploits the inherent hardware imperfections in Radio Frequency (RF) devices to identify them based on their emitted signals. The main principle of RFF is that there exist no two devices of the same make and model that emit the same RF signals, due to hardware imperfections that lead to variations. These result in unique device-specific characteristics in the RF signal [19], which can be used as a fingerprint for device identification/authentication. The development pipeline of RFF systems constitutes three phases [6]: Signal Acquisition and Preprocessing, Model Training, and Deployment.\nTo train and build a DL architecture for RFF, signal acquisition is initially done in a controlled environment, where RF signals emitted by wireless devices are captured and stored as IQ samples, which retain the characteristics of the device's transmitter. Following the acquisition, the data is preprocessed and prepared for model training. Preprocessing includes feature selection, data augmentation, normalization, and noise removal. Model training includes designing and training a DL model that effectively captures the distinct patterns corresponding to the unique RF fingerprints of wireless devices from the raw IQ samples. Finally, deploying the trained DL model into real systems to identify transmitting wireless devices using incoming RF fingerprints with the transmitters used in the model's training. There exist several contributions, e.g., [19], [20], on the characteristics and type of features that could be extracted from the signal and used to perform DL-based RFF."}, {"title": "III. SYSTEM MODEL AND PROBLEM STATEMENT", "content": "Figure 1 depicts the system model adopted in this paper. We consider a typical IoT deployment where multiple devices, with transmitters, denoted $Tx_1, Tx_2,...,Tx_n$, attempt to connect to an edge device, an AP, that is equipped with a receiver Rx. Upon receiving an RF signal, the AP processes the raw IQ samples to extract their distinctive features. These features are then evaluated against a pre-trained DL model deployed on the AP. This model, trained using RF fingerprints from authorized devices, leverages the inherent PHY-layer characteristics captured in the IQ samples to perform accurate device identification and authentication. An unauthorized device that is not part of the network (depicted in red) attempts to connect to the AP. The AP evaluates the RF fingerprint of the unauthorized device against the pre-trained model. Since its RF fingerprint will not match any known authorized devices, the AP prevents/rejects the device from connecting.\nWe emphasize that introducing a new, unseen class during the developed model's test, evaluation, or inference phases constitutes an open-set problem, as discussed in [21] and further addressed in [22]. As noted in [22], traditional RFF approaches, including considerable ML-based methods, struggle to generalize to unknown devices, as they are typically designed for closed-set scenarios where only known devices are present. The system operator, therefore, needs to ensure that the model is trained on a representative set of device data to handle real-world diversity. The success of such an approach is heavily dependent on the model being trained with data from known system devices. This implies that the operator should regularly update the model to improve robustness and accuracy.\nAs highlighted in [22], addressing the challenge of unknown devices requires more sophisticated methods, such as contrastive learning, which creates discriminative representations of RFFs by generating positive and negative sample pairs. Exactly because the model relies on data from prior training, the open-set problem arises when unfamiliar classes are introduced. However, developing systems capable of handling open-set device classification is beyond the scope of this work, which assumes all classes are known during training.\nWe aim to deploy the trained models on edge devices. However, this is considered challenging due to constraints such as (i) the model size cannot be large as it affects the inference time and (ii) the fact that when the model's size is reduced, the trained model accuracy is decreased due to quantization that reduces the precision of the weights and activations in a model. Constraint (i) is presented in Section IV, while (ii) is presented in Section V."}, {"title": "IV. EDGE AI-BASED RFF", "content": "This section discusses the Deep Learning and Transformer Encoder architecture implementation as well as the Tiny Machine Learning (TinyML) conversion process.\nA. Deep Learning\nFigure 2 illustrates the structure and details of the implemented CNN. The implementation is done using TensorFlow [23] and Keras [24] libraries. The input layer reshapes the data to (256, 2, 1) to accommodate the 2D convolution operations. The model includes a series of convolutional and max-pooling layers. The first convolutional block employs 8 filters with a kernel size of (3, 2), followed by a max-pooling layer with a pool size of (2, 1). This is followed by the second convolutional block with 16 filters of the same kernel size and pooling configuration. The third block features 32 filters with a kernel size of (3, 1) and identical pooling. The fourth block reduces the filter count to 16, maintaining the kernel size (3, 1) without subsequent pooling to retain sufficient resolution for dense layers.\nPost convolution, a flattening layer converts the 3D tensor output to a 1D vector. This vector is processed through two dense layers: (i) with 100 units and (ii) with 80 units, both employing Rectified Linear Unit (ReLU) activation and L2 regularization with a factor of 0.0001 to mitigate overfitting. A dropout layer with a rate of 0.5 is also incorporated to prevent overfitting further. The output layer consists of a dense layer with units equal to the number of device classes, utilizing the softmax activation function and L2 regularization.\nFinally, the model is compiled using the Adam optimizer [25], [26], and sparse categorical cross-entropy as loss function. This lightweight CNN architecture effectively captures spatial and temporal features from the input signals through convolutional and pooling layers, while the dense layers and dropout layers ensure robust feature learning and generalization. Table I lists all the parameters and specifications for the developed CNN model.\nB. Transformer Encoder\nSimilar to the aforementioned CNN, the presented Transformer encoder is implemented using TensorFlow and Keras libraries. The model architecture depicted in Figure 3 takes as input signals with a shape of (256, 2, 1). The input data is first projected into a higher-dimensional space with an embedding dimension of 64 using a dense layer. The model's core is a Transformer block, which includes a multi-head self-attention mechanism (2 heads, key dimension 64) and a feed-forward neural network with two dense layers (64 units each) and ReLU activation. Layer normalization and dropout (rate 0.1) are applied for regularization. The output of the Transformer block is pooled and flattened to a 1D vector, followed by a dense layer with 64 units and ReLU activation. Another dropout layer (rate 0.1) is applied before the final output layer, which uses the softmax activation function for multi-class classification, with the number of units equal to the number of device classes. The model is compiled using the Adam optimizer, sparse categorical cross-entropy loss, and accuracy as the evaluation metric. Table II summarizes all the parameters and specifications of the implemented Transformer encoder.\nC. TinyML\nWe convert the CNN and Transformer encoder models using TensorFlow Lite (TFLite). Several optimization techniques, including quantization, pruning, and clustering, can be applied to minimize model size and latency while maintaining precision. The steps below illustrate how to convert a regular CNN [27] or Transformer encoder model to a deployable and lightweight model on edge devices:\n\u2022 Initially, train and build the model using the regular Tensorflow.\n\u2022 Specify the set of operations \u201carget_spec.supported_ops\" that the converted TFLite model will support, namely, (i) tf.lite.OpsSet.TFLITE_BUILTINS, and (ii) tf.lite.OpsSet.SELECT_TF_OPS.\n\u2022 Use the TFLiteConverter to convert the model to a TFLite format and invoke the convert function.\n\u2022 Set the converter optimization attribute for post-training quantization to tf.lite.Optimize.DEFAULT. This enhances latency and reduces the model size [28].\n\u2022 Finally, deploy the converted \u201c.tflite\u201d model on the edge device and uses the TensorFlow Lite Interpreter to allocate tensors, provide input data, and perform inference.\nThe first operation (i) enables the use of TFLite's built-in operations, while the latter (ii) enables a selected set of TensorFlow operations that are necessary for certain models but are not part of TFLite built-ins.\nTabel III shows the sizes of all models: trained, TFLite, and Quantized. The Transformer encoder model size is 645.68 KB. When the trained model is converted to a TFLite model, the size is reduced to 210.20 KB, and when Quantization is applied, it is further reduced to 73.27 KB. This means that the TFLite model is 3.07 times smaller, and the Quantized is 8.81 times smaller than the actual trained model. On the other hand, the CNN model's size is 1437.17 KB, with its TFLite reduced to 462.02 KB and the Quantized to 123.80 KB, resulting in the TFLite model being 3.11 times smaller and the Quantized 11.61 times smaller than the original model."}, {"title": "V. PERFORMANCE EVALUATION", "content": "In this section, we discuss the performance of the Convolutional Neural Network and Transformer encoder models in terms of training, validation, and testing. Moreover, we evaluate the inference of the TinyML models on an edge device, namely, Raspberry Pi 4.\nA. Model Training and Evaluation\nTraining utilizes the SingleDay dataset provided by WiSig [10] to train and build the presented DL architectures. It consists of 800 WiFi signals generated by 28 transmitters collected over a one-day period and contains the IQ samples from signals emitted by each device. For a detailed description of the full dataset and how the data is collected, please refer to [10]. The training is performed on a workstation with the following configurations: 12th Gen Intel(R) Core(TM) i9-12900K with a clock speed of 3.20 GHz, memory size of 64.0 GB, and NVIDIA RTX A4000 GPU with a memory size of 32 GB.\nThe models' training parameters are summarized in Table IV. We considered the default learning rate, as it provides an acceptable performance and accuracy convergence for the DL models. The number of epochs is 100, providing a reasonable balance between underfitting (insufficient learning) and overfitting (model learning noise instead of patterns). The training data is shuffled before each training epoch, preventing the model from learning patterns based on the order of training samples.\nData Preprocessing: We preprocess the RF signals to ensure uniform scaling across all samples to mitigate potential biases towards signals with higher magnitudes. This enhances the convergence and results in faster and more stable model learning. Furthermore, it prevents overflow and underflow issues arising from large variations in signal magnitudes. Each signal is scaled according to its inherent power.\nData Validation: The training set is divided into training and validation sets with a ratio of 80/20. The chosen split ratio is common for the training and validation of DL models. The validation set assesses the model's performance after each epoch during the tuning of the hyperparameters.\nBatch Size: An important hyperparameter in training is batch size indicating the number of samples utilized in training the network in every epoch. A batch size of 32 is used to train both models, providing a good trade-off between stability (lower fluctuation in validation accuracy), as illustrated in Figure 4, and training time (faster than smaller batches but not too large to cause accuracy drops) as shown in Table V.\nFigure 5 depicts the training and validation accuracy and loss for both models. In Figure 5a, the CNN model training and validation accuracy starts to converge, with an accuracy > 0.90, from the third epoch on. In the last epoch, the training accuracy is 0.99, while the validation accuracy is approximately 0.99. This shows that the model is able to learn by capturing the unique features of each transmitter in just a few training epochs. Similarly, the training and validation loss of the model decreases with increasing the number of epochs, illustrating that the model does not overfit.\nOn the other hand, the Transformer Encoder model (Figure 5b) training accuracy increases more slowly with increasing the number of epochs; this demonstrates that the model takes a number of epochs to converge towards > 0.90 accuracy. In the last epoch, the training accuracy was 0.97, while the validation accuracy was 0.98. Moreover, the training and validation loss decrease, implying that the model does not overfit.\nEvaluation of the Keras trained and converted models, i.e., TFLite and Quantized TFLite, is done in two different setups: (i) using the raw IQ data with a defined sequence. and (ii) when randomizing the sequence. The second setup aims to assess the models' robustness against temporal changes and evaluate which model can capture the unique signal features and map them to its transmitter, even when altering their sequence.\nFigure 6 illustrates the confusion matrices when predicting the same sequence used in training (Figure 6a), and when randomizing the sequence (Figure 6b) for the CNN model. Similarly, Figure 7 depicts the confusion matrices with the same aforementioned prediction setup for the Transformer Encoder, 7a and 7b, respectively. Unlike CNN, the performance remains constant even when randomizing the IQ sequences. Table VI summarizes the accuracies of the Keras trained, converted TFLite, and Quantized TFLite models.\nFurthermore, Figure 8 shows the ROC-AUC for TFLite (Figure 8a) and Quantized TFLite (Figure 8b) Transformer Encoder models. For all 28 transmitters, both models retain very high AUC values (> 0.90), indicating that both models are able to effectively distinguish between true positives and false positives across all the classes.\nB. Inference on the Edge\nTo evaluate the performance of the Keras converted and optimized models, we run 1000 inferences on a Raspberry Pi 4 and compute the average inference time, that is, the time to perform the prediction. The Raspberry Pi is equipped with 8GB of RAM. Table VII depicts the average inference times of the Keras converted CNN and Transformer Encoder models, i.e., TFLite and Quantized TFLite. The CNN model achieves faster inference times than the Transformer in both TFLite and Quantized TFLite.\nHowever, the Quantized model significantly reduces the inference time for both architectures, with the Transformer seeing more improvement. This is due to quantization, which reduces the numerical precision of the model's weights and activations from 32-bit floats to 8-bit integers. This reduces the model's size, makes it more memory-efficient, and speeds up processing, particularly on hardware optimized for integer arithmetic. Hence, Quantized models provide faster and more efficient inference without substantially compromising accuracy."}, {"title": "VI. RELATED WORK", "content": "Enhancing security applications at the Edge is done through the deployment of Artificial Intelligence (AI). It enables real-time attack detection [27], anomaly detection [29], and automated recovery mechanisms [30]. AI-based security solutions enable fast analysis of large amounts of data generated by IoT devices to identify patterns indicative of attacks [31]. TinyML [32] enables the deployment of AI models on resource-constrained IoT and edge devices, allowing for the execution of ML algorithms/models on such devices with limited computational power and memory. Thereby bringing advanced AI capabilities closer to the data source and enhancing the overall security of IoT networks.\nSun et al. [33] presented a transformer-based multi-feature extraction neural network for RFF of different Bluetooth devices. First, the multi-scale features are extracted by a Transformer model, after which classification is performed by a ResNet module, resulting in an accuracy of 0.93. The use of the Transformer provided higher accuracy than other DL methods. Feng et al. [34] proposed a lightweight CNN for RFF on mobile devices. The model uses the IQ signal of the device as input, from which it extracts IQ-related and time-domain features. The performance of the model was tested on a public dataset with 16 USRP devices, achieving an accuracy of 0.85. Xie et al. [35] utilized coherent integration, multiresolution analysis, and Gaussian Support Vector Machine (SVM) to optimize the classification of RF fingerprints. The proposed optimized coherent integration algorithm works to de-noise and improve the Signal-to-Noise Ratio (SNR) of the received RF fingerprint waveform. Then, the features of the waveform are extracted by waveform-based multiresolution analysis to reduce the dimension of the signals, which are classified by a classic SVM algorithm.\nShen et al. [40] presented a transformer-based DL model to classify signals from LoRA devices for RFF in different SNR conditions. The Transformer model is used to classify signals of variable length, which would not be possible with other DL models such as CNN. Only the packet preamble part of the LoRa packet is used for the classification. Data augmentation and multi-packet inference are also used to improve the models' performance in scenarios with low SNR. Jian et al. [7] proposed a structured pruning approach to train and sparsify RFF DL models to allow for their deployment on resource-constrained edge devices. Specifically, the convolutional layers of the CNN are compressed, resulting in significantly faster inference time with a reduced number of parameters and an insignificant decrease in accuracy. The model is tested over several datasets that consist of IQ samples. Wu et al. [8] presented a federated learning-based method to authenticate devices via RFF. In addition to device authentication, the proposed method also addressed optimal resource allocation in order to reduce network delay when the computation is purely local, purely on the edge, or is hybrid local and edge. Shen et al. [41] proposed a scalable deep metric learning approach for LoRa device authentication via RFF. Initially, channel-independent spectrograms of known training devices are used to train an RF fingerprint extractor that is able to generalize and output unique RF fingerprints for different devices. The RF fingerprints of legitimate devices in the network are then obtained from the RF extractor and stored in a database, which is used by a k-NN algorithm to detect rogue devices and classify legitimate ones."}, {"title": "VII. DISCUSSION", "content": "Performance Evaluation and Model Effectiveness. The performance evaluation (Section V) shows that both the DL and Transformer encoder models can be converted to perform RFF on edge devices effectively. The Transformer encoder model, in particular, outperforms CNN in terms of accuracy and robustness. This is due to the Transformer's multi-head self-attention mechanism, which captures dependencies and complex relationships within the IQ samples, better distinguishing subtle variations in the RF fingerprints. Additionally, the Transformer's parallel processing capability enhances its understanding of the entire input sequence, while its flexible feature representation and regularization techniques reduce overfitting. This highlights the potential of Transformer architectures when deployed in edge computing environments, where computational resources are limited, yet high performance is required.\nEdge Device Deployment. The deployment of AI models on edge devices, such as Raspberry Pi [42], presents several challenges, including limited processing power and memory constraints. Our implementation, supported by analysis, addresses these challenges by employing optimization methods provided by the TensorFlow library, namely model conversion to TFLite and quantization. Such optimizations significantly reduce the model size and inference time, making it feasible to deploy CNN or Transformer encoder for RFF on edge devices such as the Raspberry Pi (and its variants), Espressif ESP32 [43], or Coral [44] without degradation in accuracy. This assumption is based on the specs of the aforementioned devices.\nComparison with the Baseline and Edge-tailored Solutions. Table VIII compares the proposed solution against both (i) baseline and (ii) tailored for edge deployment. The baseline solutions, while they provide > 0.90 model accuracy, are not tailored for deployment on the edge or resource-constrained devices. This is due to the model size, which is often in MB and, in some cases (based on the architecture and number of layers), larger than 100 MB. Hence, eliminating the feasibility of such deployment.\nAs for the Edge tailored solutions, (i) they rely on transfer learning (pre-trained models) that are not problem/domain-specific, i.e., not for RFF. This results in increasing the number of models' parameters and, hence, its size (ii) their model architecture consists of multiple layers that affect the model learning ability, i.e., capturing the unique transmitter features, hence, a large number of parameters, a larger model size, and in some cases degradation in prediction accuracy.\nIt can be seen that our implementation offers significant advancements in several aspects. Notably, the trained models, including CNN TFLite and Transformer TFLite, achieve high accuracy rates of 0.99 and 0.98, respectively, higher than [7], or the highest accuracies reported in the literature, such as the SVM model [35]. Furthermore, our models are designed for edge deployment, as shown by their significantly smaller model sizes and parameter counts compared to the existing approaches. For instance, the CNN model presented in [41] has a massive size of 47.5 MB and over 12 million parameters, while our CNN TFLite and Transformer TFLite models are only 462.02 KB and 210.20 KB, with 116,808 and 47,964 parameters, respectively. This substantial reduction in model size and complexity facilitates efficient inference on resource-constrained edge devices without compromising effectiveness. Additionally, our quantized TFlite models further enhance this efficiency, maintaining high accuracy while drastically reducing model size. The CNN-Quantized TFlite model maintains a high accuracy of 0.99 with a significantly reduced size of 123.80 KB, and the Transformer-Quantized TFlite model achieves an accuracy of 0.98 with an even smaller size of 73.27 KB. The TFlite quantized models offer a substantial reduction in model size, making them highly suitable for deployment on resource-constrained edge devices. This reduction in size is achieved without a significant drop in accuracy, demonstrating the robustness and efficiency of our proposed implementation."}, {"title": "VIII. CONCLUSION AND FUTURE WORK", "content": "In this paper, we presented a novel methodology for deploying RFF on edge devices to enhance the security of IoT wireless networks by leveraging the unique device characteristics extracted from the raw IQ data. We implemented and evaluated two distinct DL architectures-CNN and Transformer encoder-optimized for edge deployment using TensorFlow Lite.\nOur experimental results demonstrate that both models achieve an overwhelming accuracy (> 0.95) and ROC-AUC score (> 0.90) in identifying devices based on their RF fingerprints, with the Transformer encoder sustaining performance in maintaining accuracy even when tested with samples that were not introduced during training. Additionally, we applied an optimization technique, specifically quantization, to ensure the models are lightweight and efficient, making them suitable for deployment on resource-constrained and edge devices. Furthermore, the models were evaluated on a Raspberry Pi, illustrating their feasibility for real-world edge deployment. Such lightweight implementations can be adopted in various applications that 5G and Beyond incorporate, e.g., Internet of Drones (IoD), Internet of Vehicles (IoV), and Internet of Medical Things (IoMT), to name a few.\nFuture work will focus on expanding the scope of the dataset used to include a broader variety of IoT devices and diverse environmental conditions to validate the presented model's implementation generalizability. Additionally, we aim to explore other model architectures, fine-tune the hyperparameters, and apply advanced optimization techniques to enhance performance and efficiency further. We hope this contribution fulfills the security needs for Beyond 5G applications and paves the path towards deploying RFF at the network edge."}]}