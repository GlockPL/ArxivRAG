{"title": "XSUB: Explanation-Driven Adversarial Attack\nagainst Blackbox Classifiers via Feature Substitution", "authors": ["Kiana Vu", "Phung Lai", "Truc Nguyen"], "abstract": "Despite its significant benefits in enhancing the\ntransparency and trustworthiness of artificial intelligence (AI)\nsystems, explainable AI (XAI) has yet to reach its full potential\nin real-world applications. One key challenge is that XAI can\nunintentionally provide adversaries with insights into black-\nbox models, inevitably increasing their vulnerability to various\nattacks. In this paper, we develop a novel explanation-driven\nadversarial attack against black-box classifiers based on fea-\nture substitution, called XSUB. The key idea of XSUB is to\nstrategically replace important features (identified via XAI) in\nthe original sample with corresponding important features from\na \"golden sample\" of a different label, thereby increasing the\nlikelihood of the model misclassifying the perturbed sample.\nThe degree of feature substitution is adjustable, allowing us\nto control how much of the original sample's information is\nreplaced. This flexibility effectively balances a trade-off between\nthe attack's effectiveness and its stealthiness. XSUB is also highly\ncost-effective in that the number of required queries to the\nprediction model and the explanation model in conducting the\nattack is in O(1). In addition, XSUB can be easily extended to\nlaunch backdoor attacks in case the attacker has access to the\nmodel's training data. Our evaluation demonstrates that XSUB\nis not only effective and stealthy but also cost-effective, enabling\nits application across a wide range of AI models.", "sections": [{"title": "I. INTRODUCTION", "content": "As Artificial Intelligence (AI)/Machine Learning (ML) has\nincreasingly become an auspicious technology in tackling var-\nious problems in big data [1]\u2013[5], its trustworthiness has been\nplaced under scrutiny. Previous studies have shown that ML\nclassification models are particularly vulnerable to adversarial\nattacks in which, given a sample that is correctly classified\nby a trained model, an adversary can add small - often\nimperceptible - perturbations to the sample so as to arbitrarily\nalter the model's output [6]\u2013[12]. These perturbed samples are\ncommonly referred to as \u201cadversarial examples\". In fact, it is\nalmost always possible to construct adversarial examples given\nany trained models [11], necessitating rigorous research efforts\nto proactively identify potential attack vectors before deploy-\nment. These adversarial attacks are often categorized as white-\nbox or black-box. White-box attacks assume that the adversary\nhas complete knowledge of the target model, while black-box\nattacks only have query access to the model. Although various\nblack-box attacks have been proposed, most of them either\nrely on the transferability of white-box adversarial examples\nto black-box models [8], [10] or require many queries to the\ntarget models [9], [13]\u2013[15]. This number of queries directly\nis an important metric to measure the cost and the stealthiness\nof the attacks, as an AI system may charge a fee based on the\nnumber of queries and may also raise suspicion if it receives\ntoo many queries.\nAnother line of research in trustworthy AI is the field\nof explainable AI (XAI) which aims to address the lack of\ntransparency in the decision-making process of ML models.\nThrough featuring various model-agnostic explainers [16]\u2013\n[23], XAI has emerged as a promising pathway to adding inter-\npretable explanations on top of the existing black-box models,\nhelping to create more effective and human-understandable AI\nsystems. Particularly, given an input sample and a model, a\nfeature-based explainer would indicate the importance of each\nfeature to the model's decision. In fact, several systems have\nadopted the practice of releasing an explanation together with\na model's output to promote trust and transparency.\nHowever, previous research has shown that XAI could be a\npotential double-edged sword: these explanations inadvertently\nreveal additional information about black-box models to adver-\nsaries. These additional information could then be exploited by\nattackers, thereby making the models more vulnerable [24]-\n[29]. This presents an inherent trade-off between improving\ntransparency and keeping models secure.\nLeveraging this trade-off of XAI, we propose a new\nexplanation-driven adversarial attack, XSUB, against black-\nbox classifiers. With only access to the target model's out-\nputs and their corresponding explanations, we demonstrate\nthat an adversary can effectively craft adversarial examples\nwith minimal perturbations and high success rates. Note that\nour attack strategy does not rely on any transferable white-\nbox adversarial examples. Additionally, XSUB maintains a\nconstant query complexity, i.e., given a data sample, the\nnumber of queries to the target model needed to find an\nadversarial perturbation is in O(1). This gives our attack a\ncritical advantage over other black-box adversarial attacks in\nterms of practicality, efficiency, and stealthiness.\nThe main concept behind XSUB is to target the most impor-"}, {"title": "II. RELATED WORK", "content": "Despite their benefits for ML model transparency and\ntrustworthiness, explanations pose security risks by enabling\nadversaries to uncover vulnerabilities in black-box models.\nRecent works have highlighted such risks via explanation-\ndriven attacks [13], [25]-[31]. In [29], SHAP [17], [32]\ncan be used to extract important features that a malware\nclassifier focuses on by aggregating explanations from multiple\nsamples. These features are then used to craft backdoor\ntriggers, which are blended into background data to change\nthe prediction of malware samples embedded with the same\ntrigger during inference. However, XRand [24] has mitigated\nsuch attacks by using local differential privacy to protect\nthese features, ensuring their indistinguishability to attackers\nwhile minimizing explanation loss to preserve the utility of\nthe explanations. Despite the progress in this area, both the\nattacks and defenses have primarily focused on structured\ndata domains like malware detection, without considering the\ncorrelation among features often present in image domains.\nThese correlations may introduce additional vulnerabilities\nthat adversaries could potentially exploit, such as revealing the\nimportance of one feature through another highly correlated\nfeature, thereby posing further risks.\nSeveral studies have investigated explanation-driven attacks\nin domains with highly correlated features, such as image data\nand graphs [13], [30], [31], [33], [34]. In [13], the authors\nintroduce EG-Booster that utilizes feature-based explanations\nfrom image classifiers to guide the crafting of adversarial\nexamples. They selectively add perturbations that are likely\nto cause model evasion while avoiding non-consequential\nperturbations that are unlikely to affect the model's decision.\nThis approach leverages existing attacks to generate baseline\nadversarial samples and then queries the classifier multiple\ntimes to identify which consequential and non-consequential\nperturbations should be applied to enhance the baseline attack.\nWhile this approach is highly effective in white-box attacks,\nit performs poorly against black-box models, achieving only\na 28.87% evasion rate. In addition, this approach requires\nmultiple queries to the ML classifier to check the labels of\nmodified samples, which limits its practicality in scenarios\nwhere querying incurs usage fees.\nA recent line of work focuses on defending against adversar-\nial and backdoor attacks by detecting and removing poisoned\nsamples before training or testing models [35]-[38]. In [37],\nthe authors propose the Beatrix defense mechanism, which\nidentifies potentially poisoned data samples by searching for\nanomalous patterns. Specifically, the defense examines the\nmedian absolute deviations among the inner products of fea-\nture maps between a known clean sample and a potentially\npoisoned one. If the deviations exceed a predefined threshold,\nthe sample is flagged as a potential poisoned sample."}, {"title": "III. PRELIMINARIES", "content": "In this section, we first revisit model explanations and then\ndefine the threat model for our work, essentially outlining the\ncapabilities and goals of the adversary."}, {"title": "A. Model Explanations", "content": "The goal of model explanations is to enhance the trans-\nparency and reasoning of machine learning (ML) models by\ncapturing how each feature influences the model's decisions\nand which class such decisions favor. Given a sample x =\n{$x_j$}$_{j=1}^d$ where xj represents the $j^{th}$ feature of the sample\nand d is the number of features, let f be a model function\nin which f(x) is the probability that x belongs to a certain\nclass y. An explanation model g(x) is typically simpler than\nf(x) and easier for users to understand. For instances, linear\nmodels and decision trees are commonly used as explanation\nmodels [16], [17], [39].\nShapley Additive Explanations (SHAP) [17], [32]. A SHAP\nexplanation is based on Shapley values [40], [41], which use"}, {"title": "B. Threat Model", "content": "In this work, we focus on an adversarial attack (i.e.,\ninference-time) setting and further extend our attack to a\nbackdoor attack (i.e., training-time) setting, based on the\nattacker's access to data samples. First, for an adversarial\nsetting, the attacker only has access to and can poison the\ntesting samples. Their goal is to alter the label of these\nsamples during inference. In this scenario, the attacker can\nmanipulate the labels of poisoned samples at inference time\nbut cannot influence how the model is trained. Second, for\na backdoor setting, the attacker can inject poisoned samples\ninto the training data, altering the training process to create\na backdoored classifier that differs from a clean classifier. In\nthis case, the attacker aims to make the model misclassify\nsamples embedded with a trigger while ensuring that the\nmodel's responses to clean inputs remain consistent with those\nof the clean classifier. This is a practical setting for ML-as-a-\nService (MLaaS), where models are trained on crowd-sourced"}, {"title": "IV. XSUB: EXPLANATION-DRIVEN ADVERSARIAL\nATTACK WITH FEATURE SUBSTITUTION", "content": "In this section, we introduce XSUB, a novel explanation-\ndriven adversarial attack tailored for black-box classifiers. The\ngoal of XSUB is to utilize model explanations to guide the\nperturbation of data samples, while minimizing the differences\nbetween the perturbed data samples and their original. This\nadversarial attack operates entirely in a black-box setting,\nwhere the attack has no access to the ML model itself. In\naddition, it can be easily extended to a backdoor attack if the\nadversary gains access to the model's training data, which is\na practical settings such as ML-as-a-Service (MLaaS) [44],\n[45]. A shining feature of XSUB is its efficiency, requiring\nonly a constant query complexity to the ML model. This\nfeature makes it highly cost-effective in settings where access\nto ML models incurs usage fees such as Amazon SageMaker\n(AWS), Google Cloud AI Platform, Microsoft Azure Machine\nLearning, IBM Watson Machine Learning, etc.\nGiven a sample x \u2208 Rd with label y from a test set Dtest,\na black-box classifier f, and an explanation model g, our goal\nis to construct a poisoned sample x' such that its label y'\ndiffers from y with minimal changes compared with x. \u03a4\u03bf\nachieve the goal, our idea is to identify important features\nbased on model explanations for a given data sample, and then\nsubstituting them with those from another data sample with a\ndifferent label. This raises the following question: Which data\nsample should be chosen for substitution, and how should the\nsubstitution be performed to optimize the trade-off between\neffectiveness and stealthiness?\nTo answer these questions, we introduce a concept of a\ngolden sample and a novel explanation-driven substitution\nmechanism, as described below."}, {"title": "A. Golden Sample Selection", "content": "Given a set of samples S from a specific class ys \u2260 y,\nthe explanation model g provides an explanation vector e\u00bf =\n[eil, ei2,..., eid] for the sample xi \u2208 S. A golden sample IG\nof the class ys is defined as the sample in S with the highest\nexplanation value e for that class, as follows:\n$I_G = arg \\max_{x_i \\in S} e_{x_i max}$ (2)\nwhere $e_{x_i max} = arg \\max_{e_{ij}, j \\in [1, d]} e_{ij}$"}, {"title": "B. Explanation-Driven Substitution", "content": "After selecting the golden sample IG from class ys \u2260 Yx,\nwe construct a poisoned sample x' from the original sample\nx by substituting all K important features contributing to the\nprediction of yx with the corresponding K important features\nof the golden sample. These important features are identified\nby the explanation vector, where higher values indicate greater\nimportance to the model decision. The K important features\nbeing substituted are referred to as the golden positions. The\nsubstitution is performed in the same order of importance\nwithin these golden positions. For instance, the top-1 important\nfeature of x is substituted by the top-1 important feature of\nthe golden sample, and so on, up to the top-K important\nfeature. It is important to note that different samples may have\ndifferent golden positions, depending on the locations of their\nmost important features. The explanation-driven substitution\nis formulated as follows:\n$x' = x - \\alpha \\delta_{\\alpha K} + \\beta \\delta_{IG K}$ (3)\nwhere a and \u1e9e are positive amplification hyper-parameters.\nTechnically, \u03b4\u03b1\u039a and SICK represent masks of the same size as\nx, with all elements set to zero, except for those corresponding\nto the golden positions of x and IG, respectively. The non-\nzero values match the values of the features in their respective\ngolden positions. For example, in our experiments with image\nclassifiers, setting \u03b1 = \u03b2 = 1, we substitute pixel values at\ngolden positions in the original image with pixel values from\nthe golden image."}, {"title": "C. Explanation-Driven Adversarial Attack", "content": "As shown in Algorithm 1 and Figure 3, to perform an\nadversarial attack, given a data sample (x, yx) XSUB first\nqueries the classifier f and the explanation model g to obtain\nthe prediction \u0177 = f(x) and explanation g(x) (Line 4). Next,\na different class ys \u2260 yx is randomly selected, and several test\nsamples belonging to that class are randomly chosen to form\nthe set S. We then query to obtain the explanations g(xi) for\nall xi \u2208 S. A golden sample IG is identified based on these"}, {"title": "D. Extension to Explanation-Driven Backdoor Attack", "content": "The adversarial attack in XSUB can be easily adapted to\na backdoor attack setting by assuming that the adversary\ngains access to the training data of the model. This setting\nis practical and feasible in many real-world ML services,\nsuch as MLaaS, where models are frequently updated using\ncrowd-sourced data. By exploiting this access, adversaries can\ncraft and submit poisoned data for model training, potentially\naltering the model decision boundary to respond differently\nwhen a trigger is present. In this context, we consider the\nsubstitution itself as the trigger.\nIn XSUB, to carry out a backdoor attack, we first poison\np percentage of the ML model's training data to create a\npoisoned training set Dp (Line 9). We then submit Dp to the\nserver frequently for model training (Line 10). By doing that,\nwe modify the decision boundary to adopt the poisoned data,\ncausing it to respond differently when the trigger (i.e., the\nsubstitution) appears."}, {"title": "E. Summary of XSUB Novelty and Benefits", "content": "The novelty and benefits of XSUB stem from its unique\ndesign, which leverages golden sample selection and sub-\nstitution to manipulate model outcomes. Here are the key\nadvantages: 1) The use of a golden sample with the highest\nexplanation values for substitution increases the probability of\nchanging labels, thereby enhancing the effectiveness of the\nattack. In addition, this approach improves the stealthiness\nof the attack by minimizing the number of features K that\nneed to be replaced. 2) The substitution process itself is both\nsimple and flexible. By varying values of \u03b1, \u03b2, and K, we\ncan control how much information in the original sample will\nbe replaced, effectively balancing the trade-off between the\nattack's effectiveness and its stealthiness. 3) Another standout\nfeature of XSUB is its efficiency. For each sample, it only\nrequires a constant query complexity to the prediction model\nand the explanation model, making it highly cost-effective in\nsettings where querying models incurs usage fees. 4) XSUB\ncan easily be adapted into a backdoor attack by providing\naccess to the model's training data, which is a practical\nscenario in many MLaaS environments. As a result, XSUB\neffectively balances the trade-off between attack effectiveness\nand stealthiness while remaining simple and cost-effective.\nThese features enable its application across a wide range of\nML models and scenarios with minimal modifications."}, {"title": "V. EXPERIMENTS", "content": "In this section, we conduct extensive experiments to shed\nlight on 1) The effectiveness and stealthiness of XSUB in\nattacking image classifiers, 2) The impact of hyper-parameters\n\u03b1, \u03b2, and K on the attack's effectiveness, and 3) The robust-\nness of our attack against defenses."}, {"title": "A. Baselines and ML Explainer", "content": "To evaluate our attack XSUB and compare it with 1) EG-\nBooster [13], which is one of the state-of-the adversarial attack\nagainst image classifiers, 2) Clean model, which refers to the\noriginal model without any attacks or defenses, and 3) the\ndefense method Beatrix [37] to assess how well our attack\nperforms against a defense.\nIn our experiments, we focus on image datasets and neural\nnetwork models. Therefore, we choose SHAP [17], [32],\nwhich has been shown to be effective in explaining deep neural\nnetworks. Specifically, we employ the SHAP DeepExplainer\ntool, which is tailored by SHAP authors for deep learning\nmodels in image classification tasks. In addition, SHAP has\nno access to the target model, which makes XSUB well-suited\nfor the threat models discussed."}, {"title": "B. Datasets and Model Configurations", "content": "We evaluated our attack on benchmark image classifier\ndatasets, including CIFAR-10 [42] and Imagenette [43]. The\nCIFAR-10 dataset has 50,000 training samples, each having\n32 x 32 x 3 pixels. The Imagenette dataset has 9,469 images\nin its training set, with each image being 128 \u00d7 128 \u00d7 3 pixels.\nThere are 10 classes in each dataset. To pre-process the data,\nwe scaled all pixel values to the range [0, 1]. Each image was\nthen normalized using Z-score normalization. For the CIFAR-\n10 dataset, we used means of {0.4914, 0.4822,0.4465} and\nstandard deviations of {0.2023,0.1994,0.2010} for the red,\ngreen, and blue color channels, respectively. For the Ima-\ngenette dataset, the mean values are {0.485, 0.456, 0.406}, and\nthe standard deviation values are {0.229,0.224, 0.225}.\nTo evaluate the attacks and avoid counting the Clean\nmodel's misclassification of clean data as the attack's success,\nwe only consider samples that were correctly classified by the\nClean model. Therefore, the testing sets used for the CIFAR-10\nand Imagenette datasets are 9,806 and 3, 925, respectively. In"}, {"title": "C. Evaluation metrics", "content": "We evaluate our attack with image classification tasks using\n1) Qualitative evaluation, by visualizing images before and\nafter being attacked or under different attack scenarios, and\n2) Quantitative metrics, including model accuracy and attack\nsuccess rate, as follows:\n$Accuracy = \\frac{\\sum_{i=1}^{N_{test}} \\mathbb{I} (f(x_i) = y_i)}{N_{test}}$ (4)\n$Attack\\ SR = \\frac{\\sum_{i=1}^{N_{test}} \\mathbb{I} (f(x'_i) \\neq y_i)}{N_{test}}$ (5)\nwhere Ntest is the total number of testing data samples, and\n$\\mathbb{I}(\\cdot)$ is the indicator function in which $\\mathbb{I}(x) = 1$ if x is True\nand $\\mathbb{I}(x) = 0$ if x is False. Here, yi is the ground-truth label of\nxi and $x'_i$ is the poisoned sample of xi. Intuitively, the higher\nAttack SR indicates a more effective attack. In addition, in a\nbackdoor attack setting, a smaller gap between the Accuracy\nof the Clean model and the attacked model signifies a more\neffective attack.\nTo evaluate our attack and compare with other base-\nlines, we tested a wide rage of hyper-parameters, including\n\u03b1\u2208 {1,5,10,100,200}, \u03b2 \u2208 {1,5,10, 100, 200}, and K\u2208\n{1, 5, 30, 60, 90, 120}."}, {"title": "D. Evaluation Results and Discussions", "content": "In the CIFAR-10 and Imagenette datasets, the Accuracy\nof the Clean model is 82% and 88%, respectively. These\nvalues are considered the upper bounds for model utility, i.e.,\nAccuracy, on each dataset.\nAdversarial Attack. Figure 4 illustrates the Attack SR of\nXSUB as a function of \u03b1, \u03b2 with K = 1 with the CIFAR-10\nand Imagenette datasets. In the CIFAR-10 dataset (Figure 4a),\nas a and increase, the Attack SR significantly increases,\nespecially when a is small (i.e., a \u2208 [1, 5]). For instance, with\n\u03b1 \u2208 [1,5], when \u1e9e increases from 1 to 100, the Attack SR\nincreases by 46.92% - 67.14%. The gap is smaller when a is\nlarger. When both a and \u03b2 are high, i.e., a = \u03b2 = 100, the\nAttack SR is high, ranging from 74.06% - 79.62%. When a\nor \u1e9e are larger than 5, XSUB outperforms EG-Booster, which\nhas an Attack SR of only 28.87% [13]. Note that XSUB not\nonly achieves a higher Attack SR but also operates as a black-\nbox model, eliminating the need for multiple requests to the\nprediction model as required by EG-Booster. The Imagenette\ndataset (Figure 4b) follows the similar trend as in the CIFAR-\n10 dataset in which Attack SR generally increases when a and\n\u1e9e increase. However, the results for this dataset exhibit more\nfluctuation. This may be due to the Imagenette's significant\nhigher resolution compared with the CIFAR-10 dataset, with\n128 x 128 pixels versus 32 \u00d7 32 pixels. As a result, perturbing\na single pixel in the larger image may introduce greater\nvariability, leading to more fluctuating outcomes.\nIntuitively, given the fixed value of K, as a and \u03b2 increase,\nthe important features of the ground-truth label in the original\nsample, indicated by the model explanations, are replaced by\nimportant features of other labels. This substitution causes the\nmodel's focus to deviate from the correct class, diminishing\nits ability to recognize the ground-truth label accurately. With\nhigh values of a and \u1e9e, this replacement effect is exacer-\nbated. The model begins to emphasize irrelevant or mislead-\ning features, which significantly increases the probability of\nmisclassification. Meanwhile, the Accuracy exhibits a slight\ndecrease across all values of a and \u03b2, compared with that\nof the Clean model. The model becomes more susceptible to"}, {"title": "Impacts of \u03b1, \u03b2, and K on Attack Effectiveness and\nStealthiness.", "content": "Figures 4 and 5 illustrate that \u03b1, \u03b2, and K have\nsubstantial impacts on Attack SR, with K showing particularly\nstrong effects. When \u03b1, \u03b2, and K increase, the Attack SR\nrises notably. For instance, in the CIFAR-10 dataset, with\nK = 1, increasing a from 1 to 100 results in a substantial\nuplift in Attack SR, from 10.30% to 79.62%. For values of\n\u03b1\u2208 [1,10], increasing \u1e9e also greatly increases Attack SR\nwith 67.14% uplift. However, when a is sufficiently high,\ni.e., a = 100, the impact of \u1e9e is subtle, with Attack SR\nfluctuating between 74.57% \u2013 79.62% as \u03b2\u2208 [1,100]. When\nK > 1, even with a small value of a and \u03b2, i.e., a = 1 and\n\u03b2\u2208 {1,2,3}, Attack SR significantly improves compared with\nthat of K = 1. For instances, at K = 90, \u03b1 = 1, and \u1e9e = 3,\nAttack SR can reach 96.01%. This is because increasing K\nresults in more important features of the ground-truth label\nbeing replaced by features from other labels, which increases\nthe likelihood of misclassification. However, we also observe\nthat when K is sufficiently high, i.e., 120, Attack SR tends\nto decrease. This reduction is because, with high values of\nK, the replacement for high order features, i.e., features 91th\nto 120th, which are less important, becomes more random.\nSuch random replacement can lead to confusion rather than\nmisclassification, which results in a slight drop in Attack SR."}, {"title": "Extension to Backdoor Attack.", "content": "To further extend our attack\nto a backdoor setting where the attacker can have (limited) ac-\ncess the training data of the ML model, we poison the training\nsamples in a manner akin to the adversarial attack setting, as\noutlined in Algorithm 1. These poisoned samples are then used\nto train the ML model, similar to ML-as-a-Service scenarios\nthat utilize crowd-sourced data. This approach allows attackers\nto manipulate the training data, thereby compromising the\nintegrity of the model.\nFor this experiment, we randomly select 10% of the training\ndata, e.g., 5,000 images in the CIFAR-10 dataset or 1,000\nimages in the Imagenette dataset and apply XSUB to poison\nthem. For each image chosen for poisoning, we select a ran-\ndom label different from its ground-truth label as a target for\nperturbation. Using the setting that achieved the highest Attack\nSR in the adversarial attack setting, i.e., K = 1, \u03b1 = 100, and\n\u03b2 = 1, we achieve an Attack SR of 75.54% in the CIFAR-10\ndataset and 75.31% in the Imagenette dataset, reflecting slight\ndecreases of 4.08% and 5.01% from the adversarial settings.\nThe subsequent backdoored model exhibits an Accuracy of\n74.05% in the CIFAR-10 dataset and 82.04% in the Imagenette\ndataset. Basically, we perturb the samples from a class to\nanother random untargeted class, which can influence the\ndecision boundary of all classes (not only a certain class as\nin targeted backdoor attacks), causing a moderate drop in\naccuracy. In addition, during backdoor training, the decision\nboundary shifts to accommodate both clean and poisoned data,\nresulting in a slight decrease in Attack SR compared with the\nadversarial setting, which does not affect the ML model."}, {"title": "XSUB against Defenses.", "content": "To further evaluate the robustness\nof our attack, we examine the effectiveness of XSUB against\nthe Beatrix defense [37]. We adopt the detection threshold\nspecified in the paper, set at 99%. This threshold indicates that\n99% of the data samples in the benign training dataset have\nmedian absolute deviations smaller than the chosen detection\nthreshold. The detection rate is calculated as the percentage\nof poisoned samples that are correctly detected as poisoned"}, {"title": "VI. CONCLUSION", "content": "This paper introduced XSUB, a novel explanation-driven\nadversarial attack against black-box classifiers by leveraging\nfeature substitution. The key concept behind XSUB involves\nperturbing the most important features that are identified by an\nexplainer. Specifically, an attacker would strategically substi-\ntute important features in the original sample with correspond-\ning features from the golden sample, which is a sample from\na different class that contains the most influential feature for\nthat class. By doing that, our attack significantly increases the\nprobability of the model incorrectly classifying the perturbed\nsamples. This method allows for precise control over the\ntrade-off between attack effectiveness and stealthiness, making\nXSUB both a potent and adaptable tool for various attack\nscenarios. Additionally, XSUB maintains a constant query\ncomplexity. Its cost-effectiveness and ease of adaptation to\nbackdoor attacks further highlight its potential impact. Our\nexperiments show that XSUB outperforms existing attacks and\nis robust against defense mechanisms. Our research reinforces\nand highlights a security trade-off of XAI in that it promotes\ntransparency while simultaneously revealing more information\nto adversaries, making black-box models more vulnerable\nto attacks. This calls for future research in addressing this\nsecurity trade-off."}]}