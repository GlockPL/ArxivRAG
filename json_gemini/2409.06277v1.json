{"title": "Ferret: Federated Full-Parameter Tuning at Scale for Large Language Models", "authors": ["Yao Shu", "Wenyang Hu", "See-Kiong Ng", "Bryan Kian Hsiang Low", "Fei Richard Yu"], "abstract": "Large Language Models (LLMs) have become indispensable in numerous real- world applications. Unfortunately, fine-tuning these models at scale, especially in federated settings where data privacy and communication efficiency are critical, presents significant challenges. Existing methods often resort to parameter-efficient fine-tuning (PEFT) to mitigate communication overhead, but this typically comes at the cost of model accuracy. To address these limitations, we propose federated full-parameter tuning at scale for LLMs (Ferret), the first first-order method with shared randomness to enable scalable full-parameter tuning of LLMs across decentralized data sources while maintaining competitive model accuracy. Ferret accomplishes this through three aspects: (1) it employs widely applied first-order methods for efficient local updates; (2) it projects these updates into a low-dimensional space to considerably reduce communication overhead; and (3) it reconstructs local updates from this low-dimensional space with shared randomness to facilitate effective full-parameter global aggregation, ensuring fast convergence and competitive final performance. Our rigorous theoretical analyses and insights along with extensive experiments, show that Ferret significantly enhances the scalability of existing federated full-parameter tuning approaches by achieving high computational efficiency, reduced communication overhead, and fast convergence, all while maintaining competitive model accuracy. Our implementation is available at https://github.com/allen4747/Ferret.", "sections": [{"title": "1 Introduction", "content": "Recently, Large Language Models (LLMs) have become indispensable tools across a wide range of real-world applications, from natural language processing tasks like translation [1] and summarization [2] to more complex tasks such as code generation [3] and decision-making systems [4]. The immense scale and versatility of LLMs make them highly valuable in practice, but they also introduce significant challenges, particularly when they are fine-tuned in federated settings. Federated Learning (FL) offers a decentralized approach to fine-tuning LLMs while retaining data on local clients to ensure privacy. However, while this approach effectively addresses privacy concerns, it also results in prohibitive communication overhead when the model parameters of LLMs scale to billions.\nOne of the straightforward strategies to mitigate the prohibitive communication costs in the federated tuning of LLMs is parameter-efficient fine-tuning (PEFT). PEFT methods [5, 6] focus on fine-tuning only a subset of model parameters, which is able to significantly reduce the communication overhead between clients and a central server [7\u201310]. Despite the effectiveness in reducing bandwidth usage,"}, {"title": "2 Problem Setup", "content": "In this paper, we consider the federated full-parameter tuning of an LLM using decentralized data  {Di}i=1N on N local clients while preserving data privacy, i.e., without sharing raw data. Specifically, we aim to minimize a global objective L(w) defined as the average loss across {Di}i=1N over the model parameters w \u2208 Rd of an LLM. That is,\nminwL(w) = 1/N \u2211i\u2208[N] L(i)(w) where L(i)(w) \u225c Ex(i)\u2208Di [l(w; x(i))] . (1)\nFollowing the practice in federated learning (FL), (1) can be solved through multiple rounds of local training and global aggregation. In each communication round, each client i independently updates its local model parameters by minimizing its local objective L(i)(w) based on its local data Di. After local training, the clients transmit their updated local model parameters to a central server, where they are aggregated to form an updated global model. This updated global model is then redistributed to all clients, and the process is repeated over rounds.\nThe main challenge in LLM federated full-parameter tuning is to ensure the computational efficiency and the convergence speed of the global model while reducing the communication overheads, particularly given that the parameter size d of LLMs often reaches billions. While existing first-order FL [15\u201317] can ensure compelling computational efficiency and convergence speed by applying first-order updates, they typically incur O(d) communication overheads due to the need to transmit the entire set of model parameters between clients and the central server. This type of methods hence is impractical for LLM federated full-parameter tuning due to the enormous size of LLMs. In contrast, although zeroth-order FL [12] can reduce these communication costs by transmitting only several scalar gradients from their finite difference-based gradient estimation with shared randomness, they"}, {"title": "3 The Ferret Algorithm", "content": "To answer it, we introduce Ferret, federated full-parameter tuning at scale for LLMs, in Algo. 1. We present an overview of our Ferret algorithm in Sec.3.1, followed by a detailed explanation of the key techniques in Ferret in Sec. 3.2."}, {"title": "3.1 Overview of Ferret", "content": "To achieve scalable LLM federated full-parameter tuning, our Ferret algorithm combines the strengths of both first-order FL, which offers efficient computation and fast convergence, and zeroth-order FL, which reduces communication overhead. Specifically, Ferret (a) follows first-order FL to apply first-order optimization methods for local updates on clients, ensuring both computational efficiency and fast convergence, and (b) draws inspiration from zeroth-order FL by projecting updates into a low-dimensional space using random bases that can be regenerated using shared randomness among clients for the reconstruction of these updates, thereby reducing communication overhead.\nOur Ferret algorithm operates by repeating the following three sequential steps over many communication rounds, denoted by r \u2208 [R], where R is the total number of rounds. For simplicity, we omit the subscript r from the seeds, random bases, and projected coordinates in our notation.\nStep 1: Global Aggregation (Line 3-5 in Algo. 1). At the beginning of the first round (r = 1), each client initializes its local model parameters using the pre-trained model parameters wo, i.e., w1 \u2190 wo. For subsequent rounds (r > 1), each client j \u2208 [N] receives the random seeds s(i) and the corresponding K projected coordinates {\u03b3k(i)}k=1K of every client i \u2208 [N] from the previous round. These random seeds (i.e., shared randomness) are then used to generate d-dimensional random bases {vk(i)}k=1K for each client i. These random bases, along with the corresponding projected coordinates {\u03b3k(i)}k=1K, are applied to reconstruct local updates as \u0394i\u22121(i) in every client i. The global model is then updated by aggregating these local contributions as follows:\nwr\u22121 = wr\u22122 \u2212 1/N \u2211i\u2208[N] \u0394i\u22121(i), where \u0394i\u22121(i) = \u2211k\u2208[K] \u03b3k(i) vk(i). (2)"}, {"title": "3.2 Update Projection and Reconstruction", "content": "As mentioned before, we aim to project the local updates into K-dimensional coordinates (K \u226a d) to substantially reduce the communication overhead in LLM full-parameter tuning. To accomplish this, let \u2206 \u2208 Rd denote any local update, and let V = [v1 v2 \u2026\u2026\u2026 vK] \u2208 Rd\u00d7K represent the K random bases generated by any random seed s, we solve the following convex minimization problem to determine the K-dimensional projected coordinates \u03b3 = [\u03b31 \u03b32 \u2026\u2026\u2026 \u03b3K]T:\n\u03b3 = arg miny ||V\u03b3 \u2212 \u0394||. (4)\nAs V is singular with K \u226a d, the close-form of \u03b3 and its corresponding reconstruction \u0394 will be\n\u03b3 = (VTV)\u22121VT\u0394, \u0394 = V(VTV)\u22121VT\u0394. (5)\nChoice of Random Bases V. Particularly, if V is a rectangular matrix with ones on its main diagonal, meaning that each vk is a standard basis vector, (5) simplifies to \u03b3 = VT\u0394, which then corresponds to a block-wise dimension selection for local update projection and reconstruction. However, this approach significantly reduces the number of parameters updated per round as K \u226a d, potentially hindering the overall tuning performance. We thus propose to sample each element in vk (k \u2208 [K]) independently from a normal distribution with bounded 2-norm, i.e., ||vk|| \u2264 1, aiming to realize and stabilize full-parameter tuning of LLMs for competitive overall performance. To achieve this, we can sample from a truncated normal distribution: v \u223c N(0, 1) with v \u2208 [\u22121/\u221ad, 1/\u221ad] instead. The efficacy of this bounded norm will be demonstrated in Sec. 4.1.\nReconstruction w/o Inversion. Unfortunately, (5) incurs a computational complexity of O(K2d + K3) and storage complexity of O(Kd) owing to the inversion of VTV in (5), which is prohibitively costly, especially when K is large and d reaches billions. Since VTV is a scaled empirical covariance for the aforementioned distribution of an identity covariance matrix [19], we propose to approximate VTV with IK (i.e., K \u00d7 K-dimensional identity matrix) and (5) as\n\u03b3 \u2248 (\u03c1K)\u22121VT\u0394. \u0394. (6)\nHere, \u03c1 \u225c 1/(2\u03a6(1/\u221ad)\u22121), where \u03c6(1/\u221ad) and \u03a6(1/\u221ad) is the probability density function (PDF) and cumulative distribution function (CDF) of the standard normal distribution evaluated at 1/\u221ad, respectively. This approximation leads to improved computational complexity of O(Kd) and storage"}, {"title": "4 Theoretical Analyses and Insights", "content": "We then provide theoretical analyses to substantiate the effectiveness of Ferret: (a) reconstruction analysis in Sec. 4.1; (b) convergence analysis in Sec. 4.2; and (c) scalability and beyond in Sec. 4.3."}, {"title": "4.1 Reconstruction Analysis", "content": "Theorem 1 (Unbiased Reconstruction). Given the reconstruction in (7), we have\nE[\u0394] = \u0394.\nTo begin with, we demonstrate in Thm. 1 that our reconstruction in (7) is unbiased, with the proof provided in Appx. B.1. Of note, Thm. 1 shows that (a) the scalar 1/(\u03c1K) is crucial for (7) to achieve an unbiased reconstruction of the ground-truth update \u0394, and (b) our (7) avoids the bias commonly found in zeroth-order FL methods [14], including FedZO [20] and FedKSeed [12]. As a result, (7) is expected to provide a more accurate update reconstruction, which we will elaborate more below.\nTheorem 2 (Reconstruction Error). Given the reconstruction in (7), we have\nE[||\u0394 \u2212 \u0394||] < \u00d5 {2\u221a2ln(2d) / \u03c1K} ||\u0394||.\nWe then demonstrate the efficacy of our reconstruction in (7) by theoretically bounding the difference between the reconstructed update \u0394 and the ground truth \u0394 in Thm. 2. The proof is in Appx. B.2. Of note, 1/\u03c1 typically has an asymptotic rate of O(d), which we will verify empirically in Appx. C.4. Thm. 2 offers three critical insights of our Ferret: (a) Our reconstruction in (7) incurs a reconstruction error at a rate of \u00d5(d/K) for T local update iterations when \u221ad > K, which generally aligns with the results in [21]. This indicates that the reconstruction error of our (7) can be linearly reduced by increasing K. (b) Ferret avoids additional constant error items [14] that are caused by the biased estimation in these zeroth-order FL methods, implying that our (7) can be more accurate. We will justify this further in our Thm. 3 below. (c) Thanks to the independence from the iterations (i.e., T) of local updates in Thm. 2, Ferret prevents the error accumulation over the local update iterations T, which is a common issue in zeroth-order FL methods [20, 12]."}, {"title": "Theorem 3 (Connection with Zeroth-Order Method)", "content": "Define gk = (l(w+evk; x(i))\u2212l(w; x(i))) /e where each element v in vk is sampled from v \u223c N(0, 1) with v \u2208 [\u22121/\u221ad, 1/\u221ad], g\u2261 [g1 \u00b7\u00b7\u00b7 gK]T, and V \u225c [v1 v2 \u00b7\u00b7\u00b7 vK] \u2208 Rd\u00d7K, assume l(\u00b7; \u00b7) is \u03b2-smooth w.r.t its first argument, the zeroth-order reconstruction Vg/K used in [20, 12] then incurs:\n||1/K Vg \u2212 1/K VVT\u2207l(w; x(i))|| \u2264 \u03b2\u03f5/2.\nWe then show in Thm. 3 the connection between our update projection (6) and zeroth-order method used in [20, 12]. The proof is provided in Appx. B.3. Thm. 3 delivers three essential insights: (a) When \u03f5 \u2192 0, the reconstruction Vg/K in zeroth-order method is equivalent to 1/K VVT\u2207l(w; x(i)) and shares a similar form of (7) when \u0394 is replaced by \u2207l(w; x(i)), implying that zeroth-order method in fact aims to approximate our reconstruction (7). (b) In practice, \u03f5 > 0. So, zeroth-order method leads to a biased reconstruction with an additional error term of \u03b2\u03f5/2 compared to our (7), and this error will accumulate over T local iterations, implying that our (7) can indeed be more accurate as we have demonstrated above. (c) In addition, zeroth-order method is typically coupled with a single gradient (i.e., \u2207l(w; x(i))), whereas our (7) can be applied to any vector, making it more general. Overall, these results further verify the advantages of our (7) over the zeroth-order method used in [20, 12], which we will also support empirically in Appx. C.4."}, {"title": "Proposition 1 (Block-Wise Reconstruction Speedup)", "content": "For block-wise reconstruction (8) of size L,\n\u2211l\u2208[L] Kl dl < (\u2211l\u2208[L] dl)(\u2211l\u2208[L] Kl) = dK.\nWe next highlight the computational advantage of our block-wise reconstruction (8) in Prop. 1. The proof is in Appx. B.4. Prop. 1 indicates that by dividing the reconstruction of d-dimensional updates into smaller blocks {dl}l=1L, we get a reduction in overall computational complexity that is strictly less than that of the full dimension d in (7). E.g., when dl = ... = dl and K1 = ... = KL, we have \u2211l\u2208[L] Kl dl = Kd/L, showing that our block-wise reconstruction (8) reduces the computational complexity of (7) by a factor of 1/L. This implies that increasing the number of blocks L can further enhance the computational efficiency of our block-wise reconstruction (8)."}, {"title": "Proposition 2 (Block-Wise Reconstruction Error)", "content": "For block-wise reconstruction (8) of size L, when \u221adl > K1 for any l \u2208 [L],\nE[||\u0394 \u2212 \u0394||] < \u00d5 (\u2211l\u2208[L] \u221a(dl) / \u03c1l Kl) ,\nwhich is minimized by choosing K1 \u00d7 \u221a||\u0394l|| /\u03c1l .\nWe conclude by analyzing the error induced by our block-wise reconstruction (8) and the corre- sponding optimal random bases allocation in Prop. 2. The proof is provided in Appx. B.5. Prop. 2 demonstrates that reconstruction error can be minimized by adaptively allocating the number of random bases according to the gradient norm of each block. This is intuitively reasonable because a larger gradient norm typically indicates a need for more immediate model updates in practice. Hence, this insight not only provides a theoretical foundation for optimizing Ferret but also offers practical guidance. That is, by aligning the number of random bases with gradient norms, practitioners can enhance reconstruction accuracy and overall model performance. This adaptive approach ensures effi- cient use of computational resources, making Ferret versatile and effective across different datasets and federated learning scenarios."}, {"title": "4.2 Convergence Analysis", "content": "In this subsection, we present the convergence of Ferret in our Thm. 4 below when using stochastic gradient descent (SGD) for the local updates in (3). To simplify the analysis, we primarily focus on deriving theoretical results for a homogeneous setting, where L(i)(w) = L(w) in (1). Results in the heterogeneous setting can be derived by following the same proof idea.\nTheorem 4 (Convergence). Define D \u2261 L(wo) \u2212 minw L(w). Assume that L(w) is \u03b2-smooth and non-convex, and E[||\u2207L(i)(w) \u2212 l(w; x)||2] < \u03c32 for any x, w, when choosing \u03b7 \u2264 20/(\u03b2T\u221aR) in Algo. 1,"}, {"title": "Appendix A Related Work", "content": "Federated PEFT for LLMs. The field of federated learning (FL) has gained significant traction in its application to the fine-tuning of large language models (LLMs). Traditional FL approaches in this domain [8, 9, 28, 29] have predominantly focused on parameter-efficient fine-tuning (PEFT) techniques [5, 6, 30\u201332], which reduce the number of trainable parameters in LLMs to mitigate the extensive communication overheads in FL scenarios. Unfortunately, while PEFT methods such as those proposed in [9, 28] have shown promise, they often fall short in achieving the accuracy levels possible with full-parameter tuning [11], particularly in non-IID (non-independent and identically distributed) data settings commonly encountered in FL. In contrast, this paper focuses on federated full-parameter tuning of LLMs, aiming to achieve significantly reduced communication overhead while maintaining competitive model accuracy.\nFederated Learning with Shared Randomness. Several approaches leveraging shared randomness have been proposed to enhance communication efficiency in FL. Methods including [12, 33\u201338] demonstrate that by transmitting only a limited set of random seeds and scalar gradients, communica- tion overhead can be drastically reduced. However, these methods rely on zeroth-order optimization (ZOO) for their local updates on each client. This reliance often results in poor scalability, as these methods require substantial computational costs per round to achieve the same local update progress and a larger number of communication rounds to converge compared with their first-order counterparts, such as FedAvg [15] and FedProx [16]. This limitation therefore becomes a bottleneck in large-scale federated environments. In contrast, our paper introduces the use of shared randomness within first-order FL, aiming to improve both computational and communication-round efficiency of zeroth-order FL. To the best of our knowledge, this is the first time that shared randomness has been introduced in first-order FL to reduce communication overhead."}, {"title": "Appendix B Proofs", "content": "B.1 Proof of Thm. 1\nSuppose v is randomly and independently sampled from a truncated normal distribution, i.e., v \u223c N(0, 1) with v \u2208 [\u22121/\u221ad, 1/\u221ad], we have\nE[v] = 0, (9)\nand also\nE[v2] = (E[v])2 + VAR(v)\n= VAR(v)\n= 1 \u2212 (\u03c6(1/\u221ad)(v(1/\u221ad) + (\u22121/\u221ad))) / (\u03a6(1/\u221ad) \u2212 \u03a6(\u22121/\u221ad)) \u2212 (\u03c6(1/\u221ad) \u2212 \u03c6(\u22121/\u221ad)) / (\u03a6(1/\u221ad) \u2212 \u03a6(\u22121/\u221ad)))2 (10)\n= 1 \u2212 (2 \u03c6(1/\u221ad)/(\u221ad))/(2 \u03a6(1/\u221ad) \u2212 1).\nwhere \u03c6(\u00b7) and \u03a6(\u00b7) is the probability density function (PDF) and cumulative distribution function (CDF) of the standard normal distribution evaluated at 1/\u221ad, respectively.\nAccording to Sec. 3.2, each element v in V is randomly and independently sampled from the truncated normal distribution above. We therefore have the following to conclude our proof:\nE[\u0394] = E[1/(\u03c1K) VVT\u0394]\n= 1/(\u03c1K) \u2211k=1K E[vk vk T] \u0394 (11)\n= \u0394."}, {"title": "4.3 Scalability and Beyond", "content": "With the theoretical results above, we summarize the scalability of Ferret and compare it to existing methods like zeroth-order FL (e.g., FedZO and FedKSeed) and first-order FL (e.g., FedAvg) in Tab. 1.\nComputation Per Round. Of note, Ferret enjoys a computational complexity of O(T1T) for any client i \u2208 [N] per round, where T1 is the per-iteration complexity of the first-order update (including forward and backward passes) in (3), and T is the number of local iterations. This is comparable to the well-established FedAvg. In contrast, both FedZO and FedKSeed incur a complexity of O(T0K), with T0 being the per-iteration complexity of the zeroth-order update (i.e., forward pass) and K representing the number of forward passes. As first-order updates use more accurate gradients, T will be smaller than K (i.e., T \u226a K) to attain the same local update progress. Although T1 can be at most twice T0, our Ferret is still more computationally efficient than FedZO and FedKSeed (see Sec. 5).\nCommunication Per Round. As only one seed s(i) and K projected coordinates {\u03b3k(i)}k=1K from a client i \u2208 [N] need to be transmitted per round in Algo. 1 with K \u226a d, Ferret incurs a communication overhead of O(K), which is similar to that of FedKSeed. This is significantly more efficient than FedAvg and FedZO, which have a communication complexity of O(d) due to their need to transmit the entire model (or gradients). This significantly reduced communication cost therefore makes Ferret especially suitable for federated full-parameter tuning of LLMs with billions of parameters.\nRounds to Converge. As revealed in Sec.4.2, our Ferret benefits from unbiased update reconstruction in (7) (validated in Thm. 1), enabling fast convergence with a small number of communication rounds to achieve \u03f5 convergence error (see Thm. 4). This is significantly more efficient than zeroth-order FL methods like FedZO and FedKSeed, which require many more communication rounds to converge due to poor gradient estimation [20]. FedAvg, applying the ground truth local update for its global"}, {"title": "6 Conclusion", "content": "In conclusion, our Ferret algorithm offers a highly desirable solution for the scalable, full-parameter tuning of LLMs in federated environments. By achieving high computational efficiency, fast conver- gence, and reduced communication overhead, Ferret overcomes the limitations of existing methods, striking an improved balance among these critical factors. Moreover, our rigorous theoretical analyses and extensive experiments validate Ferret as a robust and reliable approach for deploying LLMs in large-scale federated settings."}, {"title": "3.2 Update Projection and Reconstruction", "content": "As mentioned before, we aim to project the local updates into K-dimensional coordinates (K \u226a d) to substantially reduce the communication overhead in LLM full-parameter tuning. To accomplish this, let \u2206 \u2208 Rd denote any local update, and let V = [v1 v2 \u2026\u2026\u2026 vK] \u2208 Rd\u00d7K represent the K random bases generated by any random seed s, we solve the following convex minimization problem to determine the K-dimensional projected coordinates \u03b3 = [\u03b31 \u03b32 \u2026\u2026\u2026 \u03b3K]T:\n\u03b3 = arg miny ||Vy \u2212 \u0394||. (4)\nAs V is singular with K \u226a d, the close-form of \u03b3 and its corresponding reconstruction \u0394 will be\n\u03b3 = (VTV)\u22121VT\u0394, \u0394 = V(VTV)\u22121VT\u0394. (5)\nChoice of Random Bases V. Particularly, if V is a rectangular matrix with ones on its main diagonal, meaning that each vk is a standard basis vector, (5) simplifies to \u03b3 = VT\u0394, which then corresponds to a block-wise dimension selection for local update projection and reconstruction. However, this approach significantly reduces the number of parameters updated per round as K \u226a d, potentially hindering the overall tuning performance. We thus propose to sample each element in vk (k \u2208 [K]) independently from a normal distribution with bounded 2-norm, i.e., ||vk|| \u2264 1, aiming to realize and stabilize full-parameter tuning of LLMs for competitive overall performance. To achieve this, we can sample from a truncated normal distribution: v \u223c N(0, 1) with v \u2208 [\u22121/\u221ad, 1/\u221ad] instead. The efficacy of this bounded norm will be demonstrated in Sec. 4.1.\nReconstruction w/o Inversion. Unfortunately, (5) incurs a computational complexity of O(K2d + K3) and storage complexity of O(Kd) owing to the inversion of VTV in (5), which is prohibitively costly, especially when K is large and d reaches billions. Since VTV is a scaled empirical covariance for the aforementioned distribution of an identity covariance matrix [19], we propose to approximate VTV with IK (i.e., K \u00d7 K-dimensional identity matrix) and (5) as\n\u03b3 \u2248 (\u03c1K)\u22121VT\u0394. \u0394. (6)\nHere, \u03c1 \u225c 1/(2\u03a6(1/\u221ad)\u22121), where \u03c6(1/\u221ad) and \u03a6(1/\u221ad) is the probability density function (PDF) and cumulative distribution function (CDF) of the standard normal distribution evaluated at 1/\u221ad, respectively. This approximation leads to improved computational complexity of O(Kd) and storage"}]}