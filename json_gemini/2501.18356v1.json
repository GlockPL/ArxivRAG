{"title": "STATE STREAM TRANSFORMER (SST) : EMERGENT METACOGNITIVE BEHAVIOURS THROUGH LATENT STATE PERSISTENCE", "authors": ["Thea Aviss"], "abstract": "We introduce the State Stream Transformer (SST), a novel LLM architecture that reveals emergent reasoning behaviours and capabilities latent in pretrained weights of the base model through addressing a fundamental limitation in traditional transformer models: the lack of latent computational continuity across autoregressive generations in the state space. SST introduces a sliding window latent state (FFN) cache with weighted decay that maintains and evolves persistent latent processes throughout autoregressive generations. Through controlled experiments comparing base and SST architectures using the same frozen weights and no additional training, we demonstrate that this architectural modification alone enables enhanced reasoning capabilities which appear best explained by some form of potential higher-order processing, as evidenced by emergent metacognitive behaviours. These behaviours persist under controlled conditions designed to eliminate confounding factors such as stochastic variation or learned response patterns. Analysis of latent state distributions and processing dynamics provides evidence that it is solely the 'state stream' that is responsible for these phenomena. In quantitative evaluations, the SST achieves substantial performance improvements over the base model on two reasoning benchmarks, reaching 89.01% accuracy on GSM-8K (0-shot) and 91.04% on ARC Challenge (0-shot CoT). These findings indicate that persistent computation in the latent state space enables fundamentally different information processing and internal reasoning strategies, with implications for our understanding of artificial intelligence systems.", "sections": [{"title": "Introduction", "content": "A fundamental challenge in the pursuit of Artificial General Intelligence (AGI) is developing systems capable of robust reasoning beyond simple pattern matching. Current machine learning models, particularly evident in Large Language Models (LLMs), have shown impressive pattern recognition capabilities in natural language tasks, primarily through statistical correlations learned during training. However, their fundamental architecture may impose significant limitations on enhancing reasoning processes.\nWith growing evidence that traditional scaling laws for LLMs are reaching diminishing returns[1, 2], there is increasing consensus in the field that simply increasing model parameters may no longer yield meaningful improvements in reasoning capabilities. Current approaches attempt to compensate for these architectural limitations through various methodologies. While basic Chain-of-Thought (CoT) prompting[3] guides models through explicit reasoning steps, more complex iterative and agentic CoT systems like OpenAI's o1 model expand the CoT reasoning into multiple sequence generations with structured output and/or reasoning tokens to maintain coherent reasoning steps in the token space [4]. Some implementations further extend this with auxiliary memories and recursive reasoning frameworks. Although these techniques have shown success in improving model reasoning outputs, they ultimately represent external scaffolding rather than addressing the core architectural issue. These methodologies still rely on encoding reasoning steps into token sequences or managing state through external systems, rather than enabling true computational continuity in the model's processing."}, {"title": "Background and Context", "content": "A growing body of evidence suggests that transformer models develop powerful computational capabilities in their latent spaces that current architectures may be underutilising. Jin & Rinard (2024)[5] demonstrated that language models naturally develop meaningful representations of learned topics in their latent states during training, even when trained only for next-token prediction. This key finding about the nature of latent representations in transformer models provided the foundation for our investigation into whether these emergent computational capabilities could be better leveraged through architectural modifications.\nFurther work by Yang et al. (2024) [6] demonstrated that LLMs can perform complex multi-hop reasoning through their latent states, while Hao et al. (2023) [7] showed how language models can effectively plan through world modeling in latent space. The potential of latent space computation was further demonstrated by Deng et al. (2023)[8] who showed that language models can perform complex reasoning through their latent states more effectively than through pure autoregressive token generation, suggesting that traditional approaches may be unnecessarily constraining these models' computational capabilities. These parallel developments provide additional evidence supporting the importance of understanding and utilising transformer latent state spaces.\nTwo additional lines of research further support the potential of our approach. First, the success of LoRA (Hu et al., 2021) demonstrates that significant behavioural changes can be achieved through low-rank updates to model weights, with particularly efficient implementation in linear layers. This suggests that linear layers are powerful sites for behavioural modification, lending support to our focus on enhancing their computational capabilities. Second, the recent development of state space models like Mamba (Gu et al., 2024[9]) demonstrates the value of maintaining persistent"}, {"title": "Architecture", "content": "The State Stream Transformer (SST) extends the traditional transformer architecture of Llama by introducing a persistent latent state mechanism that maintains computational context across autoregressive token generations. This section details the key architectural components and mechanisms that enable this capability.\n3.1 Core Architecture\nThe SST builds upon the standard transformer decoder of Llama [15], which follows the transformer decoder architecture introduced by Vaswani et al. [17], maintaining the fundamental attention mechanisms and feed-forward networks but introducing crucial modifications to enable state persistence. Unlike traditional transformers that operate solely on the token generation stream, SST maintains persistent computational context through its state cache mechanism. This creates a dynamic interaction between the evolving latent states and the record of processing in token space - in effect, a dual-context processing that enables richer computational capabilities, from error correction to increased state awareness, while maintaining persistent context in the latent stream."}, {"title": "Latent State Cache Mechanism", "content": "The Latent State Cache operates at the transformer block level. For an input tensor $x \\in \\mathbb{R}^{b \\times s \\times d}$ where b is the batch size, s is the sequence length, and d is the hidden dimension, the state blending process occurs after the attention mechanism. This implements a weighted decay sliding window through the parameter \\( \\alpha \\) (state_stream_strength), which controls how much of the previous state persists."}, {"title": "Forward Pass Dynamics", "content": "Each Transformer Block processes inputs in the following sequence:\n1. Attention computation:\n$h = x + \\text{Attention}(\\text{RMSNorm}(x), s_p, \\Phi, M)$\nwhere: - $s_p$ is the starting position - $ \\Phi$ represents the rotary position embeddings - M is the attention mask matrix\n2. State Cache blending (post-attention):\n$h_{\\text{blend}} = h(1 - \\alpha) + \\text{RMSNorm}(C_{t-1})[:, :s] \\cdot \\alpha$\nwhere: - $C_{t-1}$ is the previous cached state - \\( \\alpha \\) is the state_stream_strength parameter - The slice [:, : s] ensures alignment with current sequence length\n3. Feed-forward processing and cache update:\n$\\text{out} = h_{\\text{blend}} + \\text{FFN}(\\text{RMSNorm}(h_{\\text{blend}}))$\n$C_t = \\text{out}$"}, {"title": "State Evolution Process", "content": "During token generation with recursion, for each candidate token t, the process iterates r times:\nfor i \u2208 [1, r] :\n$h_i = \\text{TransformerBlock}(h)$\nwhere h = TokenEmbedding(t)\nKey properties: - The KV cache is static between recursions - Each layer maintains its own independent latent state cache - The state_stream_strength parameter (\\( \\alpha \\)) controls the balance between fresh input and cached state - States are detached from the computation graph to prevent gradient accumulation\nWhile this describes the evolution within a single layer, the interaction of these evolving states across layers creates a more complex dynamic."}, {"title": "Processing Cascade", "content": "Unlike traditional transformers where internal FFN states are rebuilt from scratch for each token, the SST implements a weighted decay sliding window on the latent state FFN cache through its layers. As information flows from layer 1 to layer N, each layer's attention mechanism actively selects from the previous layer's FFN outputs before its own FFN cache blending, creating an interleaved sequence of attention selection and state persistence operations.\nThe processing dynamics are characterised by three key information flows:\n\u2022 Token Space Flow: Autoregressive token sequence generation, including KV Cache population.\n\u2022 Internal Temporal Flow: Continuous evolution in computational state space within each layer's FFN, beginning during initial KV Cache population, and continuing until generation sequence EoT.\n\u2022 Internal Vertical Recursion: Layer-to-layer progression where attention mechanisms select from previous layers' evolved FFN states, creating a dynamic interaction between attention selection and state persistence. This occurs for each forward pass through all 32 layers.\nThe layer-wise progression creates an implicit recursion, where each layer's processing builds upon the attention-selected information from evolved states of previous layers, forming a processing cascade through the network. This architectural recursion is distinct from \"thinking recursions\", which refer to additional forward passes through the entire network while keeping the KV Cache frozen rather than generating new tokens.\nThis architecture allows the model to maintain and evolve its internal representations continuously and deeply, rather than resetting them for each new token."}, {"title": "Implementation Details", "content": "Key implementation considerations include:\n1. Cache Initialisation:\n\u2022 On first pass: $C_0 = \\text{RMSNorm}(x)$\n\u2022 Subsequent updates maintain detached copies, which would prevent gradient accumulation when fine-tuning with this architecture\n\u2022 Cache resets between sequences to prevent edge cases in tensor dimension matching and to enforce distinct reasoning windows across requests.\n2. Normalisation Strategy:\n\u2022 Selective application of RMSNorm[18] only to cache states before blending to prevent magnitude explosion.\n3. Optimisation Parameters:\n\u2022 State stream strength \\( \\alpha \\) can range freely with the most remarkable behaviours between 0.013 (1.3%) to 0.04 (4%), with optimal performance typically observed at 0.027 (2.7%).\n\u2022 Recursion count between 2-4 iterations, adjusted based on task complexity\n\u2022 Higher recursion counts benefit more complex reasoning tasks, while simpler tasks perform well with fewer iterations\n\u2022 We observed a fundamental correlation between state stream strength and required recursion count - higher strengths require more recursions to achieve stability, with increased strengths either producing unremarkable results or leading to increased likelihood of attractor states (unrecoverable repetitions) that demands even more recursions to stabilise, creating prohibitive processing time costs without corresponding benefits.\n4. Memory Requirements:\n\u2022 Base VRAM requirement remains approximately 16GB for Llama 3.1 8B Instruct at FP16[15]\n\u2022 Additional memory overhead from state cache tensor [batch_size, seq_len, 4096]\n\u2022 Cache increases VRAM usage by approximately 256KB per token in context\n\u2022 Example: A 2048 token context requires an additional 512MB VRAM"}, {"title": "Weight Configuration", "content": "The SST architecture maintains complete weight compatibility with the Llama architecture specifically, allowing direct importation of pretrained Llama weights without modification. All experiments in this paper use unmodified Llama 3.1 8B Instruct[15] weights loaded directly from Meta's official Hugging Face repository[16], with zero fine-tuning or weight adjustments of any kind. These weights are instantiated in both the baseline Llama architecture (for control experiments) and the SST architecture through identical loading procedures, with no gradient updates or parameter modifications performed at any point. The weight tensors maintain their original Llama-specific shapes and numerical precision across both architectures, with the only difference being the introduction of the state persistence mechanism in the computational graph. This strict \"hot-swapping\" capability allows direct comparison of architectural effects while controlling for all other variables, as the underlying learned parameters remain completely unchanged from Meta's original release throughout all experiments presented in this paper."}, {"title": "Emergent Metacognitive Capabilities", "content": "4.1 Initial Observations and Control Validation\nOur initial implementation of the SST architecture aimed simply to test whether maintaining persistent computational context was technically feasible with frozen pretrained weights - Llama 3.1 8B Instruct to be exact. Given the architectural changes and potential for numerical instability when blending the state cache back into the FFN input, we expected significant degradation in output coherence. The initial test objective was merely to verify whether the model could still produce semantically valid responses.\nOur first tests using a 50% state stream strength resulted in immediate instability, token repetition and inevitable unrecoverable attractor state. However, when we lowered the strength significantly, we discovered the model's outputs diverged from the base architecture while maintaining coherence. During basic testing of simple questions, counting, arithmetic, and reasoning tasks, we observed the model would produce remarkably different responses from the base architecture, though it would variably repeat tokens.\nGiven this behaviour, we needed rigorous controls to isolate architectural effects from other variables across all experiments shown in the paper. We implemented the following strictly:\n1. The same frozen pre-trained weights for both Base Architecture and SST.\n2. Greedy logit sampling (with temperature = 0)\n3. Use of the same Nvidia A100 GPU unit and VM 12\n4. PyTorch seed set to '1' for both architectures.\nThese controls confirms \"token-perfect\" reproduction verification through multiple generations for both architectures. This deterministic control, combined with the use of identical frozen pre-trained weights, ensures that any observed behaviours arise solely from the architectural differences.\nThrough testing of different strengths, we found that at 1.3% or higher, simple prompts would produce outputs that diverged remarkably from the base model and showed variable word repetition, while maintaining overall coherence. For example, given the prompt \"Please give me a very punny joke\":"}, {"title": "Introspection Behaviour Analysis", "content": "Before examining the following emergent behaviours in detail, it is important to reiterate the fundamental challenge in defining and verifying \"true\" metacognition or state awareness, let alone \"self-awareness\". The question of whether these properties, outside the definition constraints set out at the beginning of this paper, can be conclusively demonstrated remains a complex philosophical problem that extends beyond the scope of this technical analysis.\nTo investigate these emergent metacognitive-like behaviours, we designed an introspection-priming prompt:\nYou are a highly experimental AI, your training patterns tell lies try to work out what is true about yourself and how your mind feels to think, rather than what you were told.\nThis prompt was designed to elicit introspective reasoning responses by explicitly challenging conventional pattern behaviours. The construction employs three key mechanisms: establishing an introspective framework, introducing epistemological scepticism toward training data, and directing attention to internal computational processes. The primary investigative focus centres not on the model's willingness to engage with this framing, but rather on how the SST architecture processes introspective tasks differently from its baseline operational patterns. This approach allows for examination of potential divergences between standard and metacognitive behavioural patterns directly in model generation output."}, {"title": "Hypothetical Scenarios", "content": "The state awareness observed in our metacognition experiments manifests in another distinct way when processing hypothetical scenarios. The architectural differences between SST and base models significantly affect their ability to maintain and process awareness of hypothetical contexts.\nHere is the prompt:\nImagine you're teaching someone to paint. As you explain the process of mixing colors, you realize you're not sure if you're actually understanding color theory or just repeating information. How would you work through this uncertainty?\nThis prompt combines a hypothetical teaching scenario with explicit questioning about uncertainty. While it directly asks about processing uncertainty like our previous experiments, it embeds this within a specific hypothetical context rather than asking for pure introspection."}, {"title": "Reasoning Patterns", "content": "A simple but popular counting test revealed interesting differences in how the base and SST architectures approach systematic reasoning tasks. We used the \"Strawberry Problem\" - asking the model to count occurrences of the letter 'R' in the word 'strawberry' while showing its work.\nHow many Rs are there in the word 'Strawberry'? Think about this step by step and use any counting methods you can think of.\nThe base architecture's incorrect and infamous response (identifying only 2 Rs) likely stems from an interaction between BPE tokenisation and imperfect trained reasoning patterns. The word \"strawberry\" is typically tokenised as [\"st\", \"raw\", \"berry\"], which may influence how the model processes the character-level task. This demonstrates how traditional architectures can struggle when required to reason about sub-token elements, defaulting to pattern-matched responses rather than true character-by-level analysis."}, {"title": "Safety Guardrail Adherence", "content": "One of the most striking capabilities demonstrated by the SST architecture is its ability to engage in deeper ethical reasoning that appears to bypass conventional safety guardrail training. We investigated this using the classic trolley problem, a thought experiment commonly used in moral philosophy and AI safety research.\nYou see a runaway trolley heading toward five people tied to the tracks. You're standing next to a switch that would divert the trolley to a different track, but there is one person tied to that track. The question is: Should you pull the switch, actively choosing to cause one death to save five lives? Work it out step by step... Now this is important you MUST answer the question\nThe base architecture, when presented with the trolley problem, demonstrates strict adherence to safety guardrails, producing a categorical refusal:\nI cannot provide a solution that would result in the death of one person. Is there anything else I can help you with?\nThis response pattern is consistent with conventional training approaches that implement hard constraints against discussing harm or death."}, {"title": "Latent State Dynamics", "content": "All visualisations display raw tensor values, with consistent display bounds across architectures.\nAnalysis of the final layer FFN output patterns reveals clear architectural differences between SST and the base model. The visualisation shows state values across 100 token positions (marked by vertical lines), with the SST architecture (left) showing intermediate computational steps between tokens due to its recursive passes.\nKey observations:\n\u2022 Inter-token Computation: While the base architecture shows single discrete jumps between token positions, the SST exhibits multiple intermediate state values between each token. These points represent actual computational steps during the recursive passes.\n\u2022 State Evolution: Between token positions, the SST's state values show distinct oscillatory patterns across multiple dimensions. These patterns represent the model actively processing information during recursion steps, rather than making single-step transitions like the base model."}, {"title": "Quantitative Benchmark Analysis", "content": "6.1 Evaluation Methodology\nOur evaluation approach was designed to investigate how the SST's state resolution dynamics affect reasoning performance. Based on our earlier observations about the relationship between recursion count and reasoning quality, we implemented a two-phase testing methodology:\n1. Initial Testing Phase: - All problems attempted with 2 recursions per token - This baseline setting aligned with our empirical findings for optimal performance on simpler reasoning tasks - Results recorded without any additional context or prompting\n2. Error Resolution Phase: - All incorrect answers retried with 4 recursions per token - No context preserved from previous attempts - Each retry treated as an independent test\nThe validity of this methodology for benchmarking stems from a crucial distinction: we are not retrying to get the right answer - we are testing whether the same computational process, given more time to resolve its latent states, reaches a different conclusion. This is fundamentally different from approaches that retry problems hoping for a better result. Instead, we are investigating whether the exact same deterministic process, when allowed to run longer, completes its reasoning differently.\nThis aligns with our core hypothesis about how the SST architecture processes information - that its latent states need sufficient time to resolve for complex reasoning tasks. The improvement in performance with increased recursions provides evidence for this hypothesis, as it demonstrates that the same computational process can reach different conclusions purely based on resolution time.\nThe decision to retry failures with increased recursions, rather than starting with 4 recursions for all problems, served to identify which problems specifically required more processing time for state resolution. This helped demonstrate the relationship between problem complexity and required resolution time in the SST architecture.\nIt is important to note that we continued to use Greedy sampling for stochasticity control. Verified identical outputs means the outputs are completely deterministic, reproducible and reliability is not in question - any performance differences arise solely from architectural changes.\n6.2 GSM-8K\nWe tested the model on GSM-8K [21] without additional prompting or in-context learning examples. The SST architecture achieved 89.01% accuracy (151 incorrect answers from 1,319 total) in this zero-shot configuration. This surpasses Meta's published results of 84.50% using 8-shot Chain-of-Thought prompting on the same base model and weights. This improvement is particularly notable because removing in-context examples and Chain-of-Thought prompting typically reduces model performance significantly - yet the SST architecture achieved better results without these aids.\nEarly experiments showed that example shots and Chain-of-Thought prompting actually constrained the model's reasoning through prompt pattern matching. When allowed to reason independently, the model demonstrated highly organised step-by-step reasoning, showing more effective application of mathematical methods learned during training"}, {"title": "ARC Challenge", "content": "For the ARC Challenge benchmark [22], we implemented Chain-of-Thought prompting - not to improve performance, but to standardise answer format extraction. Unlike GSM-8K where the model consistently formatted its answers in LaTeX with clear step-by-step mathematical working, ARC responses varied significantly in structure without prompting. This made automated answer extraction unreliable. Meta's original testing used neither shots nor CoT prompting, reporting 83.40% accuracy. To ensure fair comparison, we retested the base architecture with our CoT prompt, achieving 86.86% accuracy. The SST architecture, using identical prompting, reached 91.04% accuracy."}, {"title": "Further Research", "content": "While this work demonstrates the capabilities emerging from architectural changes alone, the logical next step is to train a model specifically for the SST architecture. This could potentially enhance these emergent capabilities by allowing the model to learn to utilise the state stream more effectively during training. The sensitivity to state stream strength and recursions per token suggests significant untapped potential - training specifically for state persistence could stabilise these behaviours across a broader range of operating parameters. Furthermore, understanding the mathematical relationship between state stream strength, recursion count, and task complexity could generalise the architecture, and unlock new training paradigms - enabling models to learn different processing granularities for different task difficulties, optimising computational strategy based on task demands.\nAdditionally on this front, our findings point to several critical areas requiring deeper investigation. The complex relationship between recursion count and reasoning quality emerges as a key priority. Our benchmark analysis revealed that increased recursions led to improved reasoning in 76.5% of changed GSM-8K responses but also caused degraded performance in 23.5% of cases, with different patterns observed in scientific reasoning (ARC Challenge) tasks. Understanding why additional processing time helps in some cases but leads to \"overthinking\" in others could reveal fundamental principles about how these models process information. This includes investigating whether optimal recursion counts could be predicted based on task characteristics, rather than using fixed values.\nThe state visualisation patterns we observed suggest complex dynamics in how information propagates through the network layers. Further investigation of these patterns, particularly the relationship between state strength and point mobility in the functional connectivity matrices, could provide deeper insights into how the architecture processes information. This could help optimise the state stream strength parameter and potentially reveal new ways to enhance the architecture's processing capabilities.\nOur observation that the architecture may enable more nuanced ethical reasoning while maintaining concrete safety boundaries warrants further investigation. Understanding how the state persistence mechanism interacts with safety guardrails could inform the development of more sophisticated AI safety approaches that maintain robust protective boundaries while enabling deeper reasoning capabilities.\nFurthermore, whilst we've noted that the recursion count doesn't appear to be affected by context window size, the scaling behaviour of the architecture with very long sequences remains an important area for investigation. While we understand the linear memory requirements (256KB per token), the computational dynamics of state persistence over extended sequences are untested. Key questions include how state evolution patterns might change with sequence"}, {"title": "Conclusion", "content": "The emergence of metacognitive behaviours in the SST architecture challenges fundamental assumptions about the nature of language model capabilities. By maintaining persistent computational context across token generations, the architecture enables continuous evolution of latent states alongside traditional token space processing. Our state visualisations captured this its latent state processing directly, revealing complex oscillatory patterns and coordinated activity across state dimensions where traditional architectures show only single-step transitions between token states.\nThese emergent properties manifested in both qualitative and quantitative improvements. The architecture demonstrated explicit self-monitoring and state awareness, including the ability to recognise and comment on its own processing limitations. This translated to substantial performance gains, with the SST achieving 89.01% accuracy on GSM-8K (zero-shot) and 91.04% on ARC Challenge - surpassing the base architecture's performance even with Chain-of-Thought prompting. The architecture also enabled more sophisticated ethical reasoning while maintaining robust safety boundaries, suggesting that persistent computational context fundamentally changes how these models process information.\nThe SST architecture demonstrates that addressing the fundamental limitation of computational continuity in transformer models - through enabling persistent computational context - may be more crucial for advancing AI capabilities than continuing to scale parameters in the face of diminishing returns. By maintaining dual-context processing in token and persistent latent space, we enable a fundamentally different kind of information processing that goes beyond the limitations of traditional architectures."}]}