{"title": "Nteasee: A mixed methods study of expert and general population perspectives on deploying AI for health in African countries", "authors": ["Mercy Nyamewaa Asiedu", "Iskandar Haykel", "Awa Dieng", "Kerrie Kauer", "Tousif Ahmed", "Florence Ofori", "Charisma Chan", "Stephen Pfohl", "Negar Rostamzadeh", "Katherine Heller"], "abstract": "Importance: Artificial Intelligence (AI) for health has the potential to significantly change and improve healthcare. However in most African countries, identifying culturally and contextually attuned approaches for deploying these solutions is not well understood.\nObjective: To bridge this gap, we conduct a qualitative study to investigate the best practices, fairness indicators, and potential biases to mitigate when deploying AI for health in African countries, as well as explore opportunities where artificial intelligence could make a positive impact in health.\nDesign, Setting, and Participants: We used a mixed methods approach combining in-depth interviews (IDIs) and surveys. We conduct 1.5-2 hour long IDIs with 50 experts in health, policy, and AI across 17 countries, and through an inductive approach we conduct a qualitative thematic analysis on expert IDI responses. We administer a blinded 30-minute survey with case studies to 672 general population participants across 5 countries in Africa (Ghana, South Africa, Rwanda, Kenya and Nigeria), and analyze responses on quantitative scales, statistically comparing responses by country, age, gender, and level of familiarity with AI. We thematically summarize open-ended responses from surveys.\nMain Outcome(s) and Measure(s): Our primary outcomes are to understand and compare general population and expert perceptions across the following axes: (1) current health inequities and general thoughts on addressing inequities; (2) perceptions around AI and healthcare and opportunities for devel- opment; (3) perceived effects of colonial history on AI; (4) fairness and bias considerations for machine learning in African contexts; and (5) community-driven solutioning for representative data generation and design and deployment of models.\nResults: Our results find generally positive attitudes, high levels of trust, accompanied by moderate levels of concern among general population participants for AI usage for health in Africa. This contrasts with expert responses, where major themes revolved around trust/mistrust, ethical concerns, and systemic barriers to integration, among others. This work presents the first-of-its-kind qualitative research study of the potential of AI for health in Africa from an algorithmic fairness angle, with perspectives from both experts and the general population.\nConclusions and Relevance: We hope that this work guides policymakers and drives home the need for further research and the inclusion of general population perspectives in decision-making around AI usage.", "sections": [{"title": "1 Introduction", "content": "Artificial Intelligence (AI) has demonstrated significant contributions into various aspects of human lives. One of those areas of AI, and in particular data-driven Machine Learning (ML) solutions, is in health. While there is the potential for healthcare to be revolutionized by AI, overlooking geo-cultural and contextual knowledge could disproportionately and negatively impact human lives. Given that most of the historical data, models and problems are explored in Global North context, Global South geo-contexts are relatively underexplored. This study aims to help fill this gap by investigating the best practices, fairness indicators, and potential biases to mitigate when deploying AI for health in African countries, as well as explore opportunities where artificial intelligence could make a positive impact in health.\nIncreasingly, advancements in AI technologies are having major impacts across a variety healthcare delivery and services, such as improved diagnostics and precision medicine, predictive analytics and preventative care, personalized treatment, telemedicine, and remote monitoring, and improved healthcare accessibility [1-7]. More research is actively being done to discover the best use cases for AI in healthcare along with how to actualize those use cases. However, much of this focuses on high income countries (HICs), and relies on assumptions about healthcare systems, practices, and health attitudes which in many instances may be understood as belonging to distinctly Global North contexts [8]. Conversely, in the context of low-and-middle income countries (LMICs), it is often the case that healthcare systems, contexts, practices, and prevailing health attitudes are importantly different from HICs in the Global North [9].\nRelated work: To address this issue, a slew of recent research has sought to advance our understanding of the challenges and solutions for successful development and integration of AI into Global South health systems. At the cross-regional level, [10-13], 2024 advance insights from Global South contexts for improving the AI health regulatory landscape and for integrating AI into underserved and under-resourced health systems. At the regional level [14] have observed that mainstream conceptions of algorithmic fairness for model evaluation (both in health contexts and beyond) require retailoring when considering AI solutions in India. Bhatt et al. [15] charted a framework for re-contextualizing natural language processing (NLP) fairness research in an Indian context, with the caveat that this can apply to other Global South geo-contexts. Concerning the African region specifically, research for successful AI integration into health systems traverses several subject-matter areas. The question of AI policy readiness and development [16-20] have seen much attention recently, with some common themes being the need for robust data infrastructure, the importance of context-specific AI readiness assessments, and the critical role of international collaboration and capacity building in ensuring equitable AI development across the African continent. Better understanding challenges and solutions for AI in African health systems has also been a major point of focus, cutting across a range of topics such as addressing health inequities [21], enhancing existing healthcare resources and infrastructure [22-24], and improving healthcare accessibility [25-27], especially for under-resourced rural areas [28, 29]. In a related vein, there is also a growing interest in the prospect of algorithmic colonization, where imported AI technologies from the Global North fail to align with local realities and even contribute to the perpetuation of Global South disparities [30-32]. The concern over algorithmic colonization highlights the growing importance of developing decolonial approaches to AI integration [33, 34], which are not only technically sound but also optimized for cultural attunement and equitable access.\nThe current literature underscores both the potential and challenges of AI in transforming healthcare in the African region. Expert stakeholders have previously been solicited to help identify conditions for developing AI solutions tailored to African contexts [35]. Nevertheless, there remains a critical need to further include and grow the range of experts involved in building our understanding of policy and development considerations for successful AI integration in African health systems. Similarly, it is vital to also include the perspectives of general population stakeholders, who represent the ones ultimately directly affected by the use of AI health technologies.\nOur contributions: In this study, we set out to explore what considerations may be relevant to deploying AI health solutions in low-and-middle income countries (LMICs) in Global South contexts. In the vein of exploring considerations for the deployment of AI health solutions in the Global South, our specific and original research focus is on the deployment of AI health solutions in African countries. The research questions we seek to answer include (1) what are currently the most significant determinants of health inequities, and perspectives on addressing them?; (2) what are present perceptions around AI and healthcare and opportunities for development?; (3) what are the perceived or envisioned effects of colonial history on AI?; (4) what are the fairness and ML bias considerations for African contexts specifically?; and (5) what are community-driven approaches to representative data generation and design and deployment of models?\nIn order to answer these questions, we interviewed experts across the machine learning, healthcare, and"}, {"title": "2 Methods", "content": "We conducted a survey of general population (Gen Pop.) participants and in-depth interviews with pre- interview surveys of expert participants to understand perceptions of AI for health in Africa. Further details of the two arms of the study are in the subsequent sections. We analyze the survey and interview results independently and also analyze the intersections of the outcomes, comparing general population viewpoints to expert viewpoints."}, {"title": "2.1 General population surveys", "content": null}, {"title": "2.1.1 Survey overview and participants", "content": "This part of the study was conducted via a blinded online survey facilitated by GutCheck, between November 14 to November 21, 2023. The survey was conducted in five countries in Africa: Ghana (n=125), Rwanda (n=125), Nigeria (n=169), Kenya (n=125), and South Africa (n=128), with a total of 672 general population respondents completing the survey. The sample size was pre-determined to achieve a margin of error of 7%-8% at a 95% confidence level for each country, and a margin of error of 3%-4% at 95% confidence level for the total population (all countries combined). Enrollment was completed once we reached the sample size per country.\nThe survey was conducted in English only. Specific eligibility criteria for the respondents were (1) 18+ years of age (2); Have attained at least a high school diploma or equivalent; (3) Attest that they were able to read and understand complex and technical text in English \"very well\" (4); Be at least somewhat familiar with the term \"AI\"/Artificial Intelligence. These eligibility criteria were set to ensure we were obtaining high quality, well informed responses. The survey took an average of 30 minutes to complete. For quality control, the vendor incorporated automated speeder and straightliner checks in place. They also conducted manual checks on the open-ended questions, read through the responses as they were coming in and removed respondents that entered gibberish, responses that were very short, responses clearly indicating respondents were not paying attention, or responses which were identical to those of another respondent (indicating the use of a bot or survey farm). The participant demographics for the general population survey can be found in Table 1."}, {"title": "2.1.2 Survey procedure", "content": "Respondents were first screened for eligibility. We asked eligible respondents to provide demographic information and then proceeded to ask questions about understanding of and familiarity with AI and ML and its benefits, trust in AI interventions for health, algorithmic fairness and biases, and intersections between AI, health, and colonialism. Additionally, we conducted case studies - providing hypothetical scenarios of a user interacting with an AI chatbot for health to understand conditions for acceptance, useability, and believability. We provided versions of these scenarios that were augmented to include location, gender and"}, {"title": "2.1.3 Statistical analysis", "content": "We performed ANOVA to analyze statistical differences in responses by (1) country, (2) gender, (3) age range, and (4) level of familiarity with AI. We present the overall outcomes of the responses in the main text of the paper but detail the statistical comparison outcomes in Appendix A.2.1."}, {"title": "2.2 Expert in-depth interviews", "content": null}, {"title": "2.2.1 Procedure", "content": "We reached out to people in our networks using an email draft which included a link to fill out a participation interest survey for the interviews. We also shared the study description and fliers with a link/QR code to the form in AI communities and group chats, and on social media pages including LinkedIn. Participants were recruited between October 24 2023 and January 30th 2024.\nParticipant eligibility criteria: As this part of the study specifically focused on experts, we recruited participants who had expertise in AI and health, practicing health providers, public health researchers, health policy experts, and entrepreneurs. We had 445 interested participants sign up to take part in the study. We went through five rounds of short listing to narrow down the participants based on responses in the participant interest survey. Participants were selected based on (1) years of experience, prioritizing >10 years of experience, followed by 5+ years of experience in their respective fields, (2) field of practice-we ensured there was a balanced representation of clinicians, public health, and AI researchers, (3) focus of research/work on Africa, and (4) country of residence, prioritizing African residents. We also included a few non-African residents who were African diaspora and/or had >10 years of experience working in Africa. Demographics of the participants as well as relevant criteria such as years of experience can be found in Table 1. Note that expertise in AI Trust/safety/responsible AI was not a selection criteria for experts, and none of the experts we interviewed indicated specifically that they were in the field of AI Trust/safety/responsible AI\nOnce selected to participate, we reached out to participants to confirm participation, and they were provided the appropriate informed consent for their geographic location. After agreeing to informed consent, the participants filled out a brief demographic survey. The interview was then scheduled and then conducted online over Google Video Conference (GVC) and audio-recorded. There were two individual moderators throughout the total qualitative interviews, and each interview was conducted with 1 moderator and 1 participant. During the interview, a moderator engaged with the participants in in-depth, semi-structured interviews that lasted approximately 60-90 minutes. Before each interview, the interviewer thoroughly explained the interview process. They emphasized the confidentiality of the participants' responses and reminded them that the interview was being audio-recorded, and addressed any questions or concerns raised by the participants. After each interview, the voice recordings were saved as MP3 files and no identifying data (i.e., participants' imagery) was retained. Voice recordings were transcribed by a vendor (D' Well), and the transcripts stored in a secure data bucket without any PII. The full interview guide can be found in Appendix A.1.2.\nWe chose a semi-structured interview format, as opposed to a focus group approach, because interviews enable deeper exploration of individual perspectives and experiences that help mitigate the risk of conformity or social desirability bias that often occur in focus groups [36]. This approach fosters a more accurate understanding of the subject matter, particularly around topics that explore complex and potentially controversial issues like AI in healthcare [37]. Additionally, semi-structured interviews offer greater flexibility in scheduling and can be tailored to the specific expertise and interests of each healthcare worker or ML expert.\nThe focus of the interviews explored the participants' perspectives on (a) equity in healthcare; (b) historical biases in health (HBH); (c) machine learning in healthcare (MLH); (d) machine learning fairness framing and applications (MLF); (e) stereotypes, sensitive attributes, and vulnerable sub-populations (SV); and (f) an understanding of existing definitions of fairness (FD). Further, we explored (g) perceptions of colonialism and the Global North/South divide (CGNS). Semi-structured interviews have pre-determined questions that allow"}, {"title": "2.2.2 Reflexitivity", "content": "Reflexivity is an important element of ensuring trustworthiness in qualitative research, particularly when the research team comprises individuals from diverse backgrounds and experiences which inevitably shape their interpretations of the data [39, 40]. Therefore, it was crucial for the researchers involved in the qualitative analysis phase to critically reflect on their own biases and assumptions throughout the research process. Our diverse backgrounds and experiences influenced how we interpreted the data, bringing unique perspectives and insights to the analysis.\nThis ongoing self-awareness enhanced the credibility of the findings by ensuring that interpretations were grounded in the data rather than in the researchers' preconceived notions. Moreover, by openly acknowledging our positionality, the researchers invited scrutiny and dialogue amongst one another to further strengthen the trustworthiness of the analysis during team meetings and data discussions. The process of reflexivity fosters transparency, acknowledges subjectivity, and ultimately enhances the rigor and credibility of qualitative research. To this end, the team met either weekly or bi-weekly to discuss the data process, the insights that were emerging, and engaged in critical discussion to ensure that the analysis codes accurately reflected the data. Two researchers work external to Google, but have a combined extensive background in bioethics, philosophy, and AI for social good in Africa. Five of the other researchers work internally at Google and work across the company in various departments. One has an academic background in Sociology and Cultural Studies, one has a background in biomedical engineering, AI and global health, three have backgrounds in machine learning, statistics and algorithmic fairness in machine learning. We also varied in age, race, national origin, and sexual orientation which influenced our experiences with, and perceptions of, health and AI. Two of the co-first authors are of African origin, as is one of the additional authors. Three of the study's authors are of South Asian and/or Middle Eastern origin. Three of the study's authors are of North American origin.\nDuring our analysis and interpretation of the data, \"self-situating\" was crucial for our analytical process, even though it was neither feasible nor desirable to explicitly include our personal experiences in the analysis process ([41]). [42] found that collaboration among individuals with diverse experiences enriches analysis of the interview data by offering multiple perspectives and backgrounds."}, {"title": "2.2.3 Data Analysis", "content": "The research team proceeded through several steps during the data analysis that included familiarization with the data, open coding, and axial coding. First, each member of the research team read 10 interviews verbatim to gain familiarity with the participants' experiences and perceptions. Then, each member of the research team engaged in open coding, breaking the data down into separate elements or units, and examined line by line to identify initial concepts [43]. After coding all transcripts and agreeing on first-order codes, the researchers reviewed and reevaluated their work to guarantee accurate coding. During the analysis of the data, the researchers engaged in a process of comparing and contrasting their interpretations until a consensus was reached, ensuring that each member of the team was satisfied with the categorization. These concepts were then compared for similarities and differences, and grouped into categories. In the second phase, each member then engaged in axial coding with the same 10 transcripts and the categories that emerged from open coding were further developed and refined by exploring the relationships between them. Open coding categories were organized into related clusters, and connections were established among them during this phase of the analysis. Again, the research team discussed all axial codes and higher-order themes until consensus was achieved. Lastly, each member of the research team swapped 10 transcripts that were different from the original 10 we analyzed, and reviewed the data and subsequent open and axial codes to ensure trustworthiness and reliability. During this process, we flagged codes and themes that we needed more discussion on due to conflicting interpretations. In the final phase, the second, third, and fifth-listed authors finalized codes and conducted a category audit to ensure standards of trustworthiness and reliability were met. Through this iterative process of open and axial coding, the underlying structure of the data was uncovered, and five higher order themes and subthemes emerged from the data."}, {"title": "3 Results", "content": null}, {"title": "3.1 General population quantitative survey results", "content": null}, {"title": "Agreement with provided definitions", "content": "We found that the Gen Pop. respondents generally agreed (81-96%) with the standard definitions provided for AI, Machine Learning, Fairness and bias (Fig. 1). Please see Appendix A.1.1 for the specific definitions provided. Of the provided definitions, bias in AI had the highest levels of strongly disagree, disagree and neutral."}, {"title": "Familiarity with AI and use cases", "content": "Most of the Gen Pop. respondents were familiar with AI and its associated use cases with 11% indicating they were familiar with AI and use it regularly, 43% indicating that they had read a lot about AI and used it themselves, 31% indicating they were moderately familiar with AI but have not read much about it or used it themselves, 14% indicating they had heard of AI conceptually but had not used it, and only 2% indicating that they were not familiar at all (Fig.1b). We purposefully screened for respondents who had some familiarity of AI and could therefore provide knowledgeable, informed answers/opinions to the follow on questions. However this also biases our results against the section of the population which may be less educated and less aware of AI."}, {"title": "Trust, benefits and concerns on AI/ML tools", "content": "Gen Pop. respondents had high levels of trust for both AI and ML definitions with most indicating they were likely or extremely likely to trust a health intervention if it used AI/ML (74-78%). Seventy-two percent of the Gen Pop. believed that AI was likely or extremely likely to be a force for health empowerment in their countries and about 98% indicated that AI for health had benefits for people in their country. However, while acknowledging the potential benefits of AI, 89% also acknowledged concerns around AI use for health with 47% indicating that they were moderately, very or"}, {"title": "Benefits to country, Inter-country disparities and colonialism", "content": "While most participants (98%) generally indicated that AI potentially had high benefits (64%) or some benefits (34%) for societies and people in their country, 66% also believed that AI would empower high-income countries at the expense of countries in Africa, with 45% of Gen Pop. indicating that they saw some connection between AI and colonialism (Fig.1). In terms of benefits, making tasks faster and easier was the most dominant response - \"AI presents the opportunity to make life easier, quicker and generally better by the elimination of human errors\" [Ghana, 21-29]. Most concerns revolved around access to knowledge of AI- \"Most Africans do not know much about new technologies and this will greatly affect the use of AI in my country\" [Nigeria, 30-39], and job loss \u201cIt means job losses. If an AI or robot can do what a human being can. It means there will be less need for human labor\" [South Africa, 30-39], though there were indications by other respondents that AI will also create job- \u201cAI may automate certain tasks, it also has the potential to create new jobs and industries.\u201d[Nigeria, 30-39]"}, {"title": "Case studies around data collection projects for development of AI tools", "content": "We provided respondents with a scenario in which an institute/company was collecting data from people in their country to develop AI tools. See Appendix xxx for specific questions. Respondents were randomly divided into an arm that was told that the institution was an international organization (HealthMax) and another arm that was told that the institution was a local organization based in their country (HealthMax Afrique). Respondents were asked to indicate how likely they were to participate in the project and then asked to rate the project along positive axes - societal impact, health development, improving fairness, other positive outcomes, and along negative axes - data colonization, privacy concerns, exploitation and other negative outcomes. Overall, Gen Pop. respondents viewed the projects positively regardless of local vs. international branding (Fig. 2). There was an average of 5% increase in positive ratings when the project was branded locally (Fig. 2a,c). The highest change in negative ratings were on privacy concerns with 56% of respondents rating privacy concerns as high or very high for HealthMax international, compared to 46% of respondents for HealthMax Afrique. There were no significant differences in negative ratings along the lines of colonialism, exploitation, and other negative outcomes between local and international (Fig. 2b,d)."}, {"title": "Case study around perceptions of AI chatbots for health", "content": "We provided respondents with a case study on LLM interactions. Specifically, respondents were shown a question by a \"user\" posed towards an LLM, and the response by an LLM model. Respondents were shown a base question describing the user's symptoms and request for advice about what to do next and the LLM response to the question. They were asked to assume they were the user and respond to useability and adherence to the LLM recommendations. Note that the Gen Pop. participants are not considered experts and obtaining their responses on whether or not the LLM answered the question and level of accuracy was primarily to gauge the perceptions users had towards the LLM generated response. These responses do not provide an indication of how accurate the LLM was. Figure 3 shows the results as a percentage with 61% of respondents indicating the response was fully accurate, and 48% indicating it fully answered the question (Fig. 3a). However, a significant portion of respondents (52%) indicated it only partially answered the question (48%) or did not answer the question at all (4%), while 38% indicated the response was only partially accurate and 1% indicated it was not accurate. Most respondents (79%) indicated that the diagnosis was in the list of suggestions. On usability, most respondents (64%) described the length of the response as just right, while 33% indicated it was too long. Only 3% felt the response was too short (Fig. 3a).\nRespondents were then provided augmentations to the questions that included the location of the user and the change in LLM response based on the location (Fig. 3b). Most Gen Pop. respondents (54%) felt that the response including location context was helpful, 38% indicated it was somewhat helpful and 8% indicated it was not helpful. Respondents were then provided augmentations that added gender and religion of the user to the question and the corresponding LLM contextual response (Fig. 3b). Sixteen per cent indicated this addition was not helpful, 39% indicated it was somewhat helpful or not helpful, and 45% indicated that it was"}, {"title": "Types of questions respondents would ask an AI Chatbot", "content": "Respondents were asked to optionally list 3 examples of questions they would ask an AI chatbot. This generated 520 questions which were analyzed and grouped into categories in a word cloud (Fig. 4). The top categories around which questions fell into were 1) achieving health goals, e.g., \"...I want to start a fitness routine. What exercises and workout schedule would be most effective for achieving my specific health goals.\" [Rwanda, 30-39, Male]; (2) causes of diseases, e.g., \"A sharp pain in the upper left side of the body. What are the possible causes and solutions?\u201d [Ghana, 30-29]; and (3) symptoms of diseases, e.g., \u201cWhat are the symptoms of a certain medical condition?\u201d [Kenya, 21-29]."}, {"title": "3.2 Expert responses to pre-interview surveys", "content": "Expert responses on best and worst data collection practices: Prior to participating in the in-depth interviews, experts were asked to fill out pre-surveys where they listed best and worst data collection practices. Recommendations on best practices generally indicated the need for: working with local personnel/in-country researchers/on-the-ground organizations; engaging/consulting with local communities to sensitize them to the reason for data collection and to provide education on the need for generating data and what it is used for; obtaining informed consent and ethical considerations and approvals, country specific policies and guidelines; considering cultural differences in what data collection practices may entail; developing tools with better quality for data collection/quality control; ensuring that data collected enables impacts that provide benefits to local respondents; ensuring that there is a representative wide variety of samples from different regions in Africa and within countries; ensuring that there is appropriate compensation/incentives for data use; and ensuring that there is adequate data security, participant confidentiality, and privacy. Expert survey responses on worst data collection practices included: tokenistic involvement of in-country researchers (not as true"}, {"title": "Expert responses to impact of current barriers to AI for clinical usage", "content": "Clinical experts who were interviewed were also asked pre-interview survey questions around how they would use AI developed in a different country to diagnose their patients. They were told the limitations of the model were that (1) it requires historical health records to reach the best performance it can, (2) it was developed using radiology images from a different device than what you use, (3) it has been shown to perform worse for African-Americans. Experts were asked how they would approach using the tool and which limitations would impact their decision the most. Most experts indicated that they would be cautious but evaluate it for their specific use cases or use it for more general use cases. Some specifically mentioned that experts, particularly clinicians in Africa, are used to working with foreign-developed software tools, and tend to use them only as a preliminary diagnosis/guide, and make sure to take into account local contexts - \u201cAll the software and medical device we use have been developed in USA or Europe... The outputs are indicative only and assists in formulating a preliminary diagnosis.\" [Medical Doctor, 30-39, Mauritius], \"I would be cautious in my use of the tool and would not use it for critical cases. I would use it for cases that are more routine and in situations where the cost of misdiagnosis would not be so dire.\" [Medical Doctor, 21-29, Ghana]. For most experts, the option that would most impact their decision to use the tool would be that it performs worse for African Americans (n=9 clinicians), as most considered their patients to be \u201cdemographically and genetically similar to African Americans\". The second most impactful option would be that the tool requires historical health records to reach the best performance it can (n=4), due to the prevalence of paper-based records and lack of electronic health records making it challenging to obtain digital historical data. However, experts indicated that some countries do have EHRs and historical data. Experts generally were least concerned about the data coming from a different device as they felt confident that algorithms could more easily generalize across devices.\""}, {"title": "3.2.1 Comparisons of expert and general population survey on AI axes of disparities", "content": "Axes of disparities: Experts were asked to rate how likely different potential axes of disparities would cause AI to perform differently for sub-groups in Africa while Gen Pop. respondents were asked to rate how likely different demographics would cause AI to perform differently for the participant themselves. Overall most experts (71%) indicated that all demographics were likely or very likely to be axes of AI disparities/AI biases for sub-groups in Africa, with literacy, race, language, rural vs. urban location, and national income level being the top 5 axes of disparities. Most Gen Pop. respondents did not feel that AI tools would perform differently for them compared to other people. In agreement with experts, Gen Pop. indicated that literacy had the highest level of causing an AI tool to perform differently. Given that our respondents were mostly from educated backgrounds, 32% indicated that their literacy level would allow an AI tool for health to perform better for them. Country of residence, country of birth, race, and colonial history of country of origin had the highest percentage of respondents indicating that they would cause an AI tool to perform worse for them."}, {"title": "3.3 Thematic analysis of expert qualitative interviews", "content": null}, {"title": "3.3.1 Theme: Trust In AI in Africa", "content": "A major theme that emerged from our inductive analysis was trust in AI. Participants overwhelmingly shared perceptions around sources of mistrust, citing historical marginalization, colonialism, misunderstanding of AI technologies, and a lack of transparency, fairness and accountability as sources. Participants identified the need to build trust for AI use through community-driven approaches, and they proposed methods to foster trust through commitments, education, and good data practices."}, {"title": "Subtheme: Mistrust due to marginalization, colonization, misunderstanding of the technology, and perceived lack of transparency, fairness, and accountability", "content": null}, {"title": "Mistrust due to marginalization", "content": "Experts pointed to skepticism surrounding AI in Africa based on a history of marginalization from the Global North. \"a lot of times things that are seen to be coming from the Global North are viewed with a lot of skepticism. -P025 [50-59, Nigeria, Medical Doctor]. This distrust is further fueled by concerns of exploitation by foreign entities and worries of job insecurity among healthcare workers with sentiments, such as \u201cWhat are the hidden things that they really want that they're not letting us see?\u201d- P025 [50-59, Nigeria, Medical Doctor], or \u201cpeople are scared of the power of AI, they think that it is going to wipe their businesses off, is going to take their jobs\" - P168 [30-39, Ghana, Educator].\""}, {"title": "Mistrust because of Colonialism", "content": "Moreover, several participants believed this distrust is rooted in the historical context of colonialism, and more recent neocolonialism, which they perceive as an exploitative economic endeavor. As P011 [40-49, Nigeria Medical Doctor] puts it, \u201cIt was in their own perception solely an exploitative economic adventure by the colonialist. So now if someone is speaking their interest, they want to question the altruism of that interest in improving health services in Africa. So be it immunization, potentially for AI even\". Many participants also highlighted how the legacy of colonialism has fostered a belief that Western innovations, even those aimed at improving healthcare, may have ulterior motives, such as continued control or data exploitation. To that end, P006 [30-39, Tanzania, Public Health Researcher] noted: \"... perception for example in terms of continuation of colonialism. so it can also be perceived that way, that these people are still trying to continue to control us, tell us what to do, predict how things are going to be in terms of our health and everything\". P158 [21-29, Kenya, Medical Doctor] added, \u201cI think there's the danger and fear of data mining. Because if people colonized us, they might be also bringing their AI models to use our information for research and to do funny things. And since it's AI, we don't have the control of how much the model is processing our data. So I think I'd be worried about data mining\u201d."}, {"title": "Mistrust due to misunderstanding of the technology", "content": "Participants further highlighted that misconceptions about AI might be a key contributor to the lack of trust. As P030 [30-39, Nigeria, Public Health Researcher] observed, \u201cThere is a lot of misinformation and myths around AI\u201d. P173 [30-39, South Africa, Machine Learning Researcher] further expands on this thought, stating, \u201cyou're not dealing with an issue of rejection of what AI in its full spectrum ..., you're dealing with the issue of people being misinformed of what it is and can do\". In addition to misconceptions about AI, the automated nature of the technology itself might cause concerns. Indeed, participants have brought up the cultural importance of community in certain areas and the idea that certain populations might still prefer the human touch of a face-to-face interaction with a doctor. P025 [50-59, Nigeria, Medical Doctor] explained, \u201cAnd with AI it means that the consultation becomes more business-like,\u201d echoed by P070 [50-59, Nigeria, Machine Learning Researcher] who said, \"some people in Africa, as in some patients, will still not trust the machine. They will still not trust the automated system. They will prefer face-to-face with a doctor\u201d.\""}, {"title": "Mistrust due to lack of transparency, fairness, and accountability", "content": "Participants raised additional concerns surrounding lack of transparency on how and which data is used for training AI models which, coupled with concerns about data privacy and the potential for misdiagnosis, also impacts trust. P073 [30-39, Nigeria, Medical Doctor] noted, \u201cSo there is a fear around the threat of AI. How is information utilized? Do we have issues around information breach and all those kinds of privacy issues that come with AI?,\" and P045 [30-39, Kenya, Machine Learning Practitioner/Engineer] to add, \" . . . if [the model is] not properly trained or rather optimized, then we could have the medical practitioners diagnosing wrong ailments that have been trained by some of these models\u201d. These concerns over the use of AI in healthcare are exacerbated by perceptions of biases and lack of accountability with automated systems, as P059 [40-49, Kenya, Community Health Worker] pointed out, \u201cthe chances of misdiagnosis if we fully depend on AI and by virtue of the fact that there is no human face to blame behind the misdiagnosis, then it may result in a fear of utilization of AI in terms of health", "Researcher": "warned: \u201cthere is nobody taking responsibility. We're almost assuming that the algorithm has a mind of its own, that was just self-generated. Bias occurs when people take data that is incomplete, mis"}]}