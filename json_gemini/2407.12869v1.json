{"title": "Bilingual Adaptation of Monolingual Foundation Models", "authors": ["Gurpreet Gosal", "Yishi Xu", "Gokul Ramakrishnan", "Rituraj Joshi", "Avraham Sheinin", "Zhiming (Charles) Chen", "Biswajit Mishra", "Natalia Vassilieva", "Joel Hestness", "Neha Sengupta", "Sunil Kumar Sahu", "Bokang Jia", "Satheesh Katipomu", "Onkar Pandit", "Samta Kamboj", "Rahul Pal", "Parvez Mullah", "Soundar Doraiswamy", "Mohamed El Karim Chami"], "abstract": "We present an efficient method for adapting a monolingual Large Language Model (LLM) to another language, addressing challenges of catastrophic forgetting and tokenizer limitations. We focus this study on adapting Llama 2 to Arabic. Our two-stage approach begins with expanding the vocabulary and training only the embeddings matrix, followed by full model continual pre-training on a bilingual corpus. By continually pre-training on a mix of Arabic and English corpora, the model retains its proficiency in English while acquiring capabilities in Arabic. Our approach results in significant improvements in Arabic and slight enhancements in English, demonstrating cost-effective cross-lingual transfer. We also perform extensive ablations on embedding initialization techniques, data mix ratios, and learning rates and release a detailed training recipe.", "sections": [{"title": "1. Introduction", "content": "There has been a rapid advancement in open source English-dominant foundation language models like Llama 2 (Touvron et al., 2023), Mistral 7B (Jiang et al., 2023), and Llama 3, primarily trained on extensive English corpora with minimal inclusion of non-English languages. To create models proficient in low-resource languages, two approaches can be taken: training a bilingual or multilingual model from scratch or adapting an existing strong English-dominant model to the target language. While bilingual and monolingual models trained from scratch, like Jais (Sengupta et al., 2023b) and Bloom (et. al., 2023), have shown promise in non-English capabilities, they are expensive to train and have inferior capabilities in English. Adapting strong English-dominant models to new languages also pose challenges, such as catastrophic forgetting of English capabilities, inefficiencies of English-dominant tokenizers, and the need for hyperparameter adjustments. (Fujii et al., 2024; Luo et al., 2024; French, 1999; Huang et al., 2024). In this paper we focus on addressing the challenges of the model adaptation approach.\nIt has been shown that capabilities of Large Language Models (LLMs) such as knowledge, reasoning and truthfulness are transferable across languages (Yang et al., 2024; Sengupta et al., 2023b). This gives us the basis and motivation to explore efficient methods for cross-lingual transfer from English to Arabic through continual pre-training of a monolingual English-dominant LLM without degradation of English capabilities. Several recent works demonstrate cross-lingual transfer of foundation models (de Vries & Nissim, 2021; Marchisio et al., 2023; Csaki et al., 2023; Zhao et al., 2024; Huang et al., 2024; Da Dalt et al., 2024), yet they lack comprehensive analysis of hyperparameter tuning, tokenizer and data mix selections, and the impact of different model sizes.\nWe study the following aspects of cross-lingual adaptation.\nVocabulary extension We establish that adapting an existing model to a new language requires expanding the vocabulary, along with employing the methods below to maintain the model's original capabilities while acquiring new linguistic skills. We determine the optimal extension ratio of the original vocabulary through experimentation.\nEmbedding alignment We find that it ensuring alignment between the embeddings of the original and newly added vocabulary tokens is vital. We explore three techniques for initializing newly added token embeddings. We follow with embedding-only pre-training, which further aligns the embedding scale and orientation for original and new tokens.\nContinual pre-training Following the embedding-only pre-training, we unfreeze the transformer backbone to continually pre-train the full model. We conduct experiments at the 7B model scale to assess various English-Arabic mix ratios and learning rates. We leverage the insights obtained from these experiments to perform cross-lingual adaptation to Arabic with Llama 2 13B and Llama 2 70B models.\nCareful experimental study of winning strategies for vocabulary extension, embedding alignment, Arabic and English"}, {"title": "2. Pre-training Datasets", "content": "We use the AraV5 Arabic dataset, which includes documents from various sources such as web pages, Wikipedia, Arabic news outlets, books and social media. It also includes high-quality English corpora, Books and Wikipedia, translated to Arabic. Curated by (Sengupta et al., 2023b), it was used for pre-training the Jais series of Arabic foundation models. Prior domain adaptation studies have emphasized the importance of using \"replay\" data, which aligns with the pre-training data domain, to preserve the foundational model's knowledge and reasoning capabilities, as proposed in works by (Gupta et al., 2023), (Chen et al., 2023), and (Azerbayev et al., 2024). We use the Pile corpus, comprising data from 22 diverse sources including ArXiv, Wikipedia, PubmedCentral, CommonCrawl, OpenWebText, and Github (Gao et al., 2020). For Llama 2 7B adaptation, we utilize 20B tokens from AraV5, whereas for Llama 2-13B and 70B, we utilize the entire AraV5 corpus, totaling 140B tokens."}, {"title": "3. Methodology", "content": "The first step in adapting a monolingual foundation model for multilingual use is to construct a balanced vocabulary that includes all target languages. Recent state-of-the-art models such as Llama 2(Touvron et al., 2023) use byte pair encoding (BPE) (Sennrich et al., 2016) tokenizers, primarily trained on English data. These tokenizers often split non-English words into characters or bytes, creating a significant imbalance among languages. Fertility, which measures the average number of subwords produced by a single word upon tokenization, can be used to quantify this imbalance.\nThis imbalance introduces inefficiency in pre-training, fine-tuning and inference. Table 1 shows that the Llama 2 tokenizer needs as many as 4 times the number of tokens to represent the same Arabic text as Jais' Arabic-English bilingual tokenizer (MLV2) (Sengupta et al., 2023b). Balanced multilingual tokenizer offers three main advantages (Petrov et al., 2023): 1) lower training and inference cost; 2) reduced latency during inference; 3) longer context windows.\nWe experiment with two methods, vocabulary replacement and vocabulary extension, to create balanced tokenizers for English and Arabic. Vocabulary replacement implies maintaining the base vocabulary and replacing its least frequent tokens with the most frequent Arabic tokens. Vocabulary extension adds the most frequent Arabic tokens, increasing the vocabulary size. In both methods, we ensure that the newly introduced tokens are not present in the original vocabulary. For both methods, we determine the optinmal number of new tokens to create a balanced multilingual vocabulary. Using Arabic tokens from the MLV2 vocabulary, we create two candidate tokenizers and perform intrinsic and extrinsic evaluations following (Ali et al., 2024).\nFor intrinsic evaluation, we use fertility score to measure the efficiency of the tokenization process. We define fertility as f = S/W, where S is the total number of tokens in the tokenized text and W is the number of words in the raw text. Subsets of validation sets of Pile and AraV5 are used to calculate the English and Arabic fertility, respectively. Table 1 shows the intrinsic evaluations of two tokenizers, i) Llama 2-replace30, and ii) Llama 2-extend100. Llama 2-replace30 replaces 30% of the base Llama 2 tokens while Llama 2-extend100 extends the Llama 2 vocabulary by 100%. Llama 2-extend100 reduces the fertility of Llama 2's tokenizer by 72.17% while maintaining the fertility in English. It also reaches a fertility in Arabic comparable to MLV2.\nWe perform extrinsic evaluation by continually training Llama 2 7B on a mixture of AraV5 and Pile, and monitoring the AraV5 validation loss. For a fair comparison of the tokenizers, we fix the raw text bytes at 67 GB for Pile and 345 GB for AraV5. Using Llama 2-extend100 as the candidate tokenizer and Llama 2 as the baseline, we tokenize the raw corpora and continually pre-train Llama 2 7B. Although the base Llama 2 tokenizer achieves a lower AraV5 validation loss compared to Llama 2-extend100 (see Table 1), it is trained on significantly more Arabic tokens due to its ~ 3.5 times higher fertility in Arabic. In an iso-token comparison, where the number of AraV5 tokens is fixed, Llama 2-extend100 outperforms base Llama 2 tokenizer by \u2248 2%. Considering both the intrinsic and extrinsic evaluations, we select Llama 2-extend100. We correct all losses to align with the Llama 2 tokenizer (see A and B)."}, {"title": "3.2. Embedding initialization", "content": "For Llama-extend100, we add 32000 new Arabic tokens to the Llama 2 vocabulary, expanding the embedding and unembedding layers as [32000, d] \u2192 [64000, d] where d is the hidden size of the transformer decoder block. Our studies reveal that a choice of embeddings initialization for newly added tokens is critical. Simple methods such as Xavier (Glorot & Bengio, 2010) or Kaiming (He et al., 2015) initialization, or using the mean of the embedding vectors of Llama 2 tokens, do not yield satisfactory results. Therefore, we explore alternative methods, described below, which"}, {"title": "Similarity-Based Token Embedding Initialization", "content": "This method is inspired by the approach proposed in (Minixhofer et al., 2022). For each new token, we identify the top k similar tokens in the base vocabulary, using an external embedding. We use OpenAI's text-embedding-3-large embeddings (Kusupati et al., 2024) for their superior quality and multilingual performance. Using cosine similarity, we find top k similar tokens a new token and initialize the new token embeddings by taking the weighted average of base embeddings of these similar tokens. After experimenting with different values for the k, we achieve the best results with k = 5."}, {"title": "Embedding Space Transformation", "content": "In this initialization method, we leverage the pre-trained embedding vectors of Jais-30B (Sengupta et al., 2023a). We use 21377 embedding vectors corresponding to tokens present in the intersection of the Llama 2 and Jais vocabularies to transform the Jais embeddings of the added tokens to the Llama 2 embedding space. Let $E_{jais}$ and $E_{Llama2}$ to denote the embedding matrices of the overlapping tokens of Jais and Llama 2, $E_{jais} \u2208 R^{21377\u00d77168}$ and $E_{Llama2} \u2208 R^{21377\u00d74096}$. We find a linear transformation to project $E_{jais}$ to $E_{llama2}$ 's space by solving for W and b using the least squares method, $WE_{Jais} + b = E_{Llama2}$.\nWe find W and b such that the Euclidean 12 norm $||WE_{Jais} + b - E_{Llama2}||^{2}_{2}$ is minimized. The parameters W and b are then used to project added tokens into the Llama 2 embedding space. This method performs better than similarity-based initialization (see C)."}, {"title": "3.3. Embedding-only pre-training", "content": "Even with Embedding Space Transformation initaliza-tion, the scale and the orientation of English and resulting Arabic embeddings are not aligned. Following (de Vries & Nissim, 2021), we do embedding-only pre-training using 15 billion tokens of AraV5 and Pile, mixed in 9:1 ratio. During this stage, gradient updates are applied to the embedding and the unembedding layers only, while keeping the other layers frozen. In our experiments, this method resulted in up to 2% improvement in upstream loss for Arabic."}, {"title": "3.4. Hyper-parameter tuning", "content": "Hyperparameter sweep is important to determine best hyper-parameters such as learning rate (lr), warm-up schedule and batch size (bs). We use a linear warmup for 1% of the total steps followed by cosine decay (Loshchilov & Hutter, 2017) to 1/10th of the peak learning rate. We compare batch sizes of 4M tokens and 6M tokens but don't see a significant difference in upstream losses. We pick 4M tokens as the final batch size. Following (Gupta et al., 2023), we setup a learning rate sweep taking three different learning rates in different ranges. Let $lr_{peak}$ be the peak Llama 2 learning rate which is 3e-4, we \u201cre-warm\u201d the learning rate to i) $lr_{peak}$, ii) $lr_{peak}$/2, and iii) $lr_{peak}$/4. These experiments use 14 billion AraV5 tokens mixed with Pile in 1 : 1, 3: 1 and 9: 1 ratios. Across all ratios we find that $lr_{peak}$ performs the best as shown in table 2."}, {"title": "3.5. Data mixture", "content": "Domain adaptation involves continual pre-training a foundation model on new data not seen during the pre-training. When this new domain data is out-of-distribution, it can cause significant forgetting of prior capabilities. Adding a small proportion of general domain data, or replay data, can mitigate the forgetting. We conduct exhaustive experiments to find a minimum proportion of Pile data that should be mixed with AraV5 to mitigate forgetting. Table 2 shows results from the experiments with different data mixes. We found that mixing 1 part English with 9 parts Arabic (1:9 En:Ar) is sufficient to mitigate forgetting. We also don't see any forgetting in downstream evaluation as discussed in section 4. Interestingly, increasing the amount of English data while keeping Arabic tokens constant improves Arabic performance, indicating cross-lingual capability transfer."}, {"title": "4. Results", "content": "Using the methodology described in section 3 we adapt Llama 2 7B, 13B and 70B models to Arabic. We use linearly warm up of the learning rate to $lr_{peak}$ for the first 1% of the tokens followed by cosine decay to 1/10th of"}, {"title": "5. Conclusion", "content": "We present an efficient recipe to significantly enhance capabilities of an English-dominant foundational LLM in another language. Our approach includes extending the vocabulary, applying a novel method for embedding initialization and alignment, and continually pre-training the foundation LLM on a bilingual data mix. We perform hyperparameter optimization for batch size, learning rate schedule, and data mix ratio to ensure successful adaptation without experiencing \"catastrophic forgetting\". We successfully use this approach to enhance Arabic capability of Llama 2 base models, resulting in a state-of-the-art 70B Arabic base language model. Furthermore, we apply this approach for other languages such as Turkic and Hindi and other foundation LLMs, with results for these adaptations to be presented in the future."}, {"title": "A. Tokenizer ablations", "content": "We experimented with one more tokenizer variant, Llama3-replace5 in addition to Llama 2-extend100 and Llama3-replace30. Here we replace only 5% of the Llama 2 tokenizaer vocabulary with that of MLV2's most frequent Arabic tokens. Experiment design:\n\u2022 Continually pre-train a monolingual LLM on native + target language (Arabic) mix tokenized with the new vocabulary.\n\u2022 Use the same hyperparameters across the ablations.\n\u2022 Fix the raw text corpus for each language \u2013 this will ensure fairness as the total information/bytes are fixed.\n\u2022 Select the size of raw text corpus for each language such that when tokenized, by a monolingual tokenizer in the respective language, the total tokens are in the same range."}, {"title": "B. Cross Entropy Loss Correction", "content": "When comparing cross-entropy loss between models trained with data tokenized by different tokenizers, we need to apply correction or normalization to the losses. This normalization is required because cross-entropy's units of measurement are nats/tokens, and therefore, the definition of a token becomes very important. Depending on the size of the vocabulary and type of tokenizer, the information represented per token varies. The loss correction factor to compare cross-entropy loss between two models is then the ratio of the number of tokens in the validation sets for each model."}, {"title": "C. Embedding initialization", "content": "Here we discuss the ablations that we performed with different embeddings initialization methods as discussed in the main body. We also ablated an additional initialization method which we refer to as Subword Mean. Following are the initialization methods under consideration:\n\u2022 Mean: Initialize all the new tokens' embeddings with the mean of source language token embeddings.\n\u2022 Subword Mean: For a newly added Arabic token, tokenize it using base Llama tokenizer and use the mean of the token embeddings ot the sequence of tokens."}, {"title": "D. Block expansion adapter approach for multilingual models", "content": "Following the work outlined in (Wu et al., 2024) we leverage the block expansion approach for multilingual models, making it highly effective for language adaptation. By adding and fine-tuning additional Transformer blocks initialized to identity mappings, the model can integrate new domain-specific knowledge without forgetting previous information. Although, the techniques described in the original paper focus on code and math, we were able to successfully adapt the approach for our experiments with English and Arabic. We initialized our base model with Llama-2 7B and expanded the number of blocks from 32 to 40 using an interleaved approach. In our experiments for language adaptation, we found that an optimal data mix of 1:9(En:Ar) yielded the best results (in downstream 0 shot tasks in both English and Arabic) relative to adapting the newly added layers only on domain specific data. In both experiments we trained on a total of 67B tokens in Arabic in order to maintain the same token count for the appropriate comparison. Our results show that the block-expansion approach is a strong candidate for language adaptation with a faster time to train and lower training costs. In the future, this work could expand to other types of models(like MoE models) and modalities and would be interesting to analyse the impact on overall accuracy in downstream tasks For language adaptation with block expansion [sectionD], we experiment with different number of adapter layers. We find that the optimal adapter layer is 25% of the existing layer. Similarly, 960 is the optimal batch size. Table 8 summarizes our results using the above approach for language adaptation at the LLama 2 7B scale"}, {"title": "E. Fine-tuning", "content": "Upstream loss is typically assumed to indicate downstream performance [(Isik et al., 2024), (Gadre et al., 2024)]. In order to verify performance on downstream tasks in the adapted domain, we fine-tune both pre-trained and adapted pre-trained models. Instruction fine-tuning allows us to assess both performance and generation quality, which may not always match upstream performance (Tay et al., 2022).\nThe data used is an extended fine-tuning dataset following from (Sengupta et al., 2023b). We add additional data in English and Arabic, focusing on bilingual examples and quality for language and style adaptation. In total, our instruction fine-tuning dataset contains approximately 10 million English and 4 million Arabic examples in both single-turn and multi-turn settings\nWe fine-tuned for 3 epochs with a standard linear learning rate decay. Instead of padding, examples are packed together up to the sequence length and separated with EOS tokens, increasing training efficiency by up to 8 times. As in [Jais], we calculate loss on answer tokens only.\nWe observe that downstream performance of the fine-tuned model trained on top of the pre-trained model is lower than that of the fine-tuned model trained after domain adaptation. It suggests that, similar to the findings in (Isik et al., 2024), downstream task performance after fine-tuning is highly dependent on the alignment between pre-training data and downstream tasks which is improved through adaptation."}, {"title": "F. Hardware setup", "content": "The training runs were conducted on two Condor Galaxy supercomputers, each equipped with 64 Cerebras CS-2 Wafer-Scale Engines (WSE-2). Each CS-2 features 40 GB of SRAM and achieves a peak throughput of 7.5 PetaFLOP/s in half precision, providing a total of 960 PetaFLOP/s in half precision across both supercomputers. Utilizing the weight streaming mode of the Cerebras software stack, the Condor Galaxy supercomputers can flexibly schedule multiple jobs based on hardware resource requirements and priority. The number of CS-2s allocated to a job can be dynamically adjusted during training, with performance scaling linearly up to 64 CS-2s per job. This scalability is facilitated by the Cerebras software stack's use of pure data parallelism to distribute the workload across multiple CS-2s. Jobs are managed by a priority queue system, ensuring efficient allocation of computational resources."}, {"title": "G. Downstream Tasks", "content": ""}]}