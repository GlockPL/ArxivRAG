{"title": "EDformer: Embedded Decomposition Transformer for Interpretable Multivariate Time Series Predictions", "authors": ["Sanjay Chakraborty", "Ibrahim Delibasoglu", "Fredrik Heintz"], "abstract": "Time series forecasting is a crucial challenge with significant applications in areas such as weather prediction, stock market analysis, and scientific simulations. This paper introduces an embedded decomposed transformer, 'EDformer', for multivariate time series forecasting tasks. Without altering the fundamental elements, we reuse the Transformer architecture and consider the capable functions of its constituent parts in this work. Edformer first decomposes the input multivariate signal into seasonal and trend components. Next, the prominent mul- tivariate seasonal component is reconstructed across the reverse dimensions, followed by applying the attention mechanism and feed-forward network in the encoder stage. In particular, the feed-forward network is used for each variable frame to learn nonlinear representations, while the attention mechanism uses the time points of individual seasonal series embedded within variate frames to capture multivariate correlations. Therefore, the trend signal is added with projection and performs the final forecasting. The EDformer model obtains state-of-the-art pre- dicting results in terms of accuracy and efficiency on complex real-world time series datasets. This paper also addresses model explainability techniques to provide insights into how the model makes its predictions and why specific features or time steps are important, enhancing the interpretability and trustworthiness of the forecasting results.", "sections": [{"title": "I. INTRODUCTION", "content": "Time-series forecasting plays an important role in a variety of industries, such as sensor network monitoring [1], smart grid management [2], economics and finance [3], and disease propagation analysis [4]. By replacing the Long Short-Term Memory (LSTM) and Recurrent Neural Network (RNN) based structure, which has been the main approach for processing time-series data, with several self-attention modules, Trans- former shows improved performance, especially in the time series analysis field [5]. Transformer is emerging in time series forecasting, driven by the tremendous success in the natural language processing field. Transformer has the ability to extract multi-level representations from sequences and il- lustrate pairwise connections [3]. In time series data analysis, Transformers that combine frequency-domain filtering with temporal and channel-wise transformations offer a powerful approach to capturing complex dependencies and patterns. By leveraging frequency-domain filtering, these models can efficiently separate and analyze the signal's various frequency components, isolating key seasonal or trend-related elements [5]. Additionally, temporal and channel-wise transformations allow the model to process both the sequential nature of time series data and the relationships across multiple channels, mak- ing it particularly adept at handling multivariate time series and discovering intricate dependencies between variables [6]. This dual capability enables the model to address both local and global patterns, improving performance in tasks such as forecasting, anomaly detection, and classification. In addition, the transformer-based model suffers from slow training and inference speed due to the bottleneck incurred by a deep encoder and step-by-step decoder inference.\nIn this paper, Our main contributions are as follows:\n\u2022\n\u2022\n\u2022\n\u2022\nWe analyse the Transformer architecture and discover that its native components' efficacy for multivariate time series has not received enough attention.\nThe EDformer model decomposes the input multivariate time series into separate seasonal and trend components. Each component is then independently embedded into frames using reverse operation, allowing the self-attention module to capture multivariate correlations while the feed-forward network encodes the representations of each series.\nEDformer is a lightweight and computationally efficient architecture, that significantly reduces processing time compared to existing state-of-the-art methods while main- taining or even improving forecasting accuracy.\nEDformer experimentally reaches cutting-edge perfor- mance on real-world benchmarks. A promising direction for future developments in Transformer-based forecasting models is shown by our thorough examination of its embedded modules and architectural decisions.\nWe do comprehensive tests on different time-series forecasting benchmarks to support our motivation and hypothesis. When compared to various forecasting techniques, the performance of our proposed model is at the cutting edge. Extensive ablation investigations and analysis trials support the viability of suggested designs and their consequent benefit over earlier methods.\nThis paper is organized as follows. Section II explains a set of typical times series forecasting works from the literature which sets the notion and motivation of this paper. Section III briefly"}, {"title": "II. LITERATURE REVIEW", "content": "In long-term and short-term time series forecasting, trans- formers have gained considerable attention due to their ability to model complex temporal dependencies [7, 8, 9, 10]. In- former [11], one of the first widely recognized transformers for time-series forecasting, employs a generative-style decoder and ProbSparse self-attention to address the challenge of quadratic time complexity. Subsequently, various transformer- based models have been proposed, including Autoformer [12], Pyraformer [13], iTransformer [14], Reformer [15], and FED- Former [16]. Pyraformer emphasizes multiresolution attention for efficient signal processing. Autoformer [12] leverages auto-correlation and decomposition techniques to improve fore- casting, while FEDFormer [16] integrates frequency-domain analysis to enhance the representation of time series. PatchTST [9] focuses on the use of patches to improve the model's capacity to capture both local and global dependencies in the data. Crossformer [17] introduces the Dimension-Segment- Wise (DSW) embedding method, which encodes input time- series data into a 2D vector array while preserving temporal and dimensional structure. It also employs a Two-Stage Atten- tion (TSA) layer to capture cross-time and cross-dimension dependencies. By combining DSW embedding and TSA, Crossformer builds a Hierarchical Encoder-Decoder (HED) framework to operate across multiple scales for more effective predictions. The Multi-resolution Time Series Transformer (MTST) [18] adopts a multi-branch architecture to model temporal patterns across different resolutions [19]. MTST distinguishes itself by employing relative positional encoding, which is better suited for capturing periodic components at various scales, compared to the fixed or absolute positional encodings used in many other transformer models. Time series decomposition, a standard method in time series analysis [20], breaks down a time series into distinct components, each reflecting a specific category of patterns that exhibit greater predictability. This technique is particularly effective for examining historical changes over time. In forecasting tasks, decomposition is commonly applied as a pre-processing step for historical data before predicting future trends [21]. Notable examples include Prophet [22], which uses trend- seasonality decomposition, N-BEATS [23], which employs basis expansion, and DeepGLO [24], which utilizes matrix decomposition. However, such pre-processing approaches are often constrained by the basic decomposition effects of histori- cal series, failing to capture the hierarchical interactions among underlying patterns in long-term forecasts. In this paper, a basic AvgPooling(.) decomposition technique has been used to separate the seasonal and trend patterns from the original input signal."}, {"title": "III. PROBLEM STATEMENT", "content": "This paper addresses the challenge of long-term fore- casting for multivariate time series, using historical data. We define a multivariate time series at time $t$ as $x_t = [x_{t,1}, x_{t,2},..., x_{t,N}]^T \\in \\mathbb{R}^{T \\times N}$, where $x_{t,n}$ represents the value of the n-th variable at time t, for n = 1, 2, ..., N. The aim is to develop a model for forecasting the future values of the series over the next T time steps, based on the most recent L time steps. The parameters L and H are referred to as the look-back window and the prediction horizon, respectively. Specifically, for a given initial time $t_0$, the model takes as input the sequence $x_{t_{0-1}}$, corresponding to the past L time steps, and outputs the predicted sequence $x_{t_0+h}$, representing the forecasted values for the next H time steps. The predicted value of $x_t$ at time t is denoted by $\\hat{x}_t$. In brief, the goal of multivariate time series forecasting is to predict future values $X_{t+h} \\in \\mathbb{R}^{T \\times N}$ given past observations $X_{t-1}$:\n$X_{t+h} = f(X_{t-1})$\n(1)\nThe forecasting performance of the model is assessed by computing the mean squared error (MSE) and the mean absolute error (MAE) between the prediction and the ground truth on the test set:\n$MSE = \\frac{1}{N} \\sum_{i=1}^N ||X_{t+h}^{(i)} - \\hat{X}_{t+h}^{(i)}||^2$\n(2)\n$MAE = \\frac{1}{N} \\sum_{i=1}^N |X_{t+h}^{(i)} - \\hat{X}_{t+h}^{(i)}|$\n(3)"}, {"title": "IV. METHODOLOGY", "content": "In this work, we have discussed the proposed methodology with a deep decomposition architecture, which includes a series decomposition block, reverse operation, embedding op- eration, and corresponding encoder. Our proposed EDformer architecture uses transformer's encoder-only architecture. The general overview of the proposed EDformer architecture is represented in Figure 1. In time-token-based transformers, the embedding of points from the same time step, which fundamentally represent distinct physical meanings captured by inconsistent measurements, results in the loss of multivari- ate correlations within a single token. Such tokens, formed from isolated time steps, face challenges in extracting useful information due to their overly local receptive fields and the misalignment of events across simultaneous time points. Furthermore, while the order of sequences plays a critical role in influencing time series variations, the use of permutation- invariant attention mechanisms on the temporal dimension is unsuitable, as it disregards the sequential nature of the data [10]."}, {"title": "A. Decomposition Block", "content": "Decomposition has been a widely used method in time series analysis for many years [6]. The decomposition block separates a time series into its seasonality and trend-cycle com- ponents, enhancing the model's ability to accurately capture these aspects. In time series analysis, decomposition involves breaking down a time series into three systematic components: trend-cycle, seasonal variation, and random fluctuations. The trend component reflects the long-term direction of the series, which may be increasing, decreasing, or stable over time. The seasonal component captures recurring patterns within the series, while the random (or \"irregular\") component accounts for noise that cannot be explained by the trend or seasonal factors. Decomposition can be performed in two main ways: additive and multiplicative. By breaking down a time series into these components, we gain a deeper understanding of the underlying patterns, allowing for more effective modelling. As can be seen, the encoder and decoder use a decomposition block to aggregate the trend-cyclical part and extract the seasonal part from the series progressively. Example, let us take an input series $X \\in \\mathbb{R}^{L \\times D}$ with length L, and the decomposition layer returns $X_T$ and $X_S$ defined as,\n$X_T = AvgPooling(Padding(X))$\n$X_S = X \u2013 X_T$\n(4)\n$X_S$ and $X_T$ denote the seasonal and the extracted trend- cyclical parts, respectively. We use the AvgPool(.) for the mov- ing average with the padding procedure to maintain the series length. Applying a reverse time series transformer (EDformer) on decomposed signals, specifically on the seasonal and trend components separately, enhances model interpretability and precision. By isolating these components, the model can better capture distinct temporal patterns and dependencies within each, leading to improved accuracy in detecting seasonality and trends. This approach also reduces noise interference, allowing the transformer to focus on key features in each decomposed signal. Consequently, the model achieves a more nuanced representation of the underlying data dynamics than when applied to the whole signal at once."}, {"title": "B. Reversible Model Inputs and Embedding", "content": "Instead of taking multiple temporal tokens, our approach takes one whole signal (Seasonal and Trends components) as a single frame. The concept of processing the entire signal within a single frame is inspired by the iTransformer model [14]. Across multivariate series, our proposed encoder-only EDformer architecture encourages adaptive correlation and representation learning. Every time series is first tokenized into a frame capture the distinct characteristics of every variable from both the components (seasonal+trend). After modeling mutual interactions with self-attention, feed-forward networks process each series (seasonal+trends) separately to provide its representation. This framework is optimized for capturing intricate temporal dependencies in time series. The procedure of forecasting future series of each distinct frame $Y_{t :, n}$ based on the lookback series $X_{S :,n}$ and $X_{T :, n}$ in EDformer is straightforwardly expressed as follows in light of the aforementioned considerations,\n$h = Embedding(Reverse(X_{S :,n}))$\n$+Embedding(Reverse(X_{T :,n}))$,\n$H^{(l+1)} = IntBlock(H^l)$, l = 0, ....., L \u2212 1,\n$Y_{t :, n} = Projection(h)$,\n(5)\nwhere the superscript indicates the layer index and H = $h_1, ..., h_n \\in \\mathbb{R}^{N \\times D}$ contains N embedded tokens of size D. Multi-layer perceptrons (MLPs) are used to implement both embedding and projection. The shared feed-forward network in each IntBlock() processes the acquired frames individually"}, {"title": "C. Model Encoding", "content": "In the encoder block, we arrange a stack of L blocks made up of the feed-forward network, self-attention, and layer normalization modules.\n1) Multivariate Self-attention: The proposed model views the entire series of a single variable as an independent process, whereas the attention mechanism is typically used to facilitate the modelling of temporal connections in forecasting. The self- attention mechanism allows the model to weigh the importance of different tokens in a sequence relative to each other, facil- itating the capture of long-range dependencies and contextual relationships. It adopts linear projections to obtain queries(Q), Keys(K) and values(V),\n$Q = SW_Q, K = SW_k,V = SW_v$\n(6)\nWhere, $W_Q, W_k, W_u$ are learned weight matrices. Several keys, queries, and values are fed into multi-scaled dot-product attention blocks by multi-head attention, which then concate- nates the attention to produce the desired result. The attention scores can be computed as,\n$scores = \\frac{Q K^T}{\\sqrt{d_k}}$\n(7)\nwhere n represents an input sequence of length and $d_k$ repre- sents the projected dimension. The computational complexity of computing the attention score is O($n^2d$). Therefore, we execute Softmax normalization (O($n^2d$)) and weighted sum approaches.\n$Attention(Q, K, V) = Softmax(scores)$\n(8)\n$Output = Attention(Q, K, V) * V$\n(9)\nTransformer uses multivariate self-attention (MVA) with mul- tiple sets of learned projections (H different time series) instead of a single attention function.\n$MVA(Q, K, V) = Concat(h_1, ...,h_H)W^O$\n(10)\nWhere, $h_i = Attention(Q W_i^Q, K W_i^K,V W_i^V)$.\n2) Layer Normalization: The \"Layer Normalization (Lay- erNorm)\" block comes next [25]. Initially, layer normalisation was suggested as a way to improve deep networks' training stability and convergence. The module in most Transformer- based forecasters gradually fuses the variables with one an- other by normalising the multivariate representation of the same timestamp. Our inverted version applies normalisation to the individual variate's series representation as Equation 11, which has been researched and shown to be useful in solving non-stationary situations. On the other hand, an oversmooth time series will result from the normalisation of various tokens of time steps in the prior design.\n$LayerNorm(H) = \\frac{h_n - Mean(h_n)}{\\sqrt{Var(h_n)}}$, n = 1, ......, N\n(11)\n3) Feed-forward network: The feed-forward network (FFN) is used by the transformer as the fundamental building block for token representation encoding, and it is applied consistently to every token. As previously indicated, several variations of the same timestamp that make up the token in the vanilla Transformer may be malpositioned and too localised to pro- vide sufficient information for predictions. FFN is used on each variate frame's series representation in the reversed form. They are focused on storing the observed decomposed time series and decoding the representations for subsequent series utilising dense non-linear connections by stacking inverted blocks [26]. As a more effective predictive representation learner than self-attention applied on time points, a logical explanation has been provided in which the neurones of MLP are trained to depict the intrinsic properties of any time series, such as the amplitude, periodicity, and even frequency spectrums. This is sent to a \"feed-forward (FFN)\" block, the output of which has a \"LayerNorm\" block. In the encoder block, the entire multi-head attention and feed-forward blocks are repeated n number of times.\n$FFN(H') = ReLU(H'W^1 + b^1)W^2 + b^2$\n(12)\nHere, H' is an output of the previous layer, $W^1, W^2, b^2$ are trainable parameters. In a deeper module, a residual connection module followed by a layer normalization module is inserted around each module.\n$H' = LayerNorm(MVSelf Atten(X) + X)$\n(13)\n$H = LayerNorm(FFN(H') + H')$ \n(14)\nWhere MVSelfAtten(.) defines the multivariate self-attention module and LayerNorm(.) denotes the layer normalization task."}, {"title": "D. EDformer Components", "content": "The entire architecture of the proposed EDformer model is discussed below,\n1. Decomposition:\nThe decomposition layer decomposes the input time series data into trend and seasonality components. The movingavg() layer is used to capture the trend, and the residual (difference between the input and trend) represents the seasonality.\n2. Model:\nThe model sets up the structure, including:\n2.1 Embedding Layer: Converts the time series data into a form suitable for the transformer encoder.\n2.2 Transformer Encoder: Includes multiple layers of self- attention, allowing the model to capture dependencies within the time series.\n3. Forecasting Process:\nThis method performs the forecasting process by:\nNormalizing seasonality to improve stability.\nEncoding the normalized seasonality with embeddings and transformer layers.\nProjecting and de-normalizing the output to make it compatible with the prediction.\nAdding the trend component back for the final forecast.\n4. Forward Process: Handles forward propagation, using the forecast method for forecasting tasks."}, {"title": "V. RESULT ANALYSIS", "content": "The datasets used in this study, detailed in Table I, encom- pass both long-term and short-term time series forecasting scenarios. The 'Electricity Transformer Temperature (ETT)' dataset contains 7 factors of electricity transformer measure- ments spanning with four subsets: ETTh1 and ETTh2 recorded hourly, and ETTm1 and ETTm2 recorded every 15 minutes. The Exchange dataset comprises daily exchange rates from 8 countries between 1990 and 2016. Additional datasets in- clude 'Weather' (21 meteorological factors collected every 10 minutes), 'Electricity Consumption Load (ECL)' with hourly data from 321 clients, and Traffic data consisting of hourly road occupancy rates from 862 sensors in the San Francisco Bay area. The Solar-Energy dataset records 10-minute samples of solar power production from 137 PV plants. PEMS traffic data is widely associated with the California 'Performance Measurement System (PeMS)', a comprehensive database used for freeway performance monitoring in California. For short- term forecasting, we utilize four subsets of the PEMS traffic network data (PEMS03, PEMS04, PEMS07, and PEMS08), which contain traffic flow information recorded at 5-minute intervals [27]. These diverse datasets, with varying tempo- ral granularities and dimensionalities, enable comprehensive evaluation of forecasting models across different domains and time scales. The M4 experimental dataset consists of 100,000 time series covering diverse domains, as detailed in Table II."}, {"title": "B. Experimental Analysis", "content": "In this section, we perform in-depth experiments to assess how well our proposed EDformer model forecasts in conjunc- tion with state-of-the-art time-series forecasting architectures. An ablation work is also applied to measure the effect of the proposed modules. All the experiments are implemented in PyTorch, CUDA version 12.2 and conducted on a single NVIDIA-GeForce RTX 3090 with 24GB GPU. We have repli- cated all of the compared baseline models and implemented them using the benchmark Time-Series Library (TSLib) [28] repository, which is based on the configurations provided by the official code or actual article for each model. We train the proposed model on all datasets using a batch size of 32, a learning rate of 0.0001 except PEMS, and the ADAM optimizer with L2 loss. The model operates with K = 4 scales. The comprehensive forecasting results are listed in Tables III, IV, and V. The inclusion of an embedded reverse de- composition process further optimizes the model's perfor- mance. A lower MSE/MAE reflects more accurate predictions, and our proposed lightweight EDformer consistently achieves the best performance across most datasets for multivariate long-term forecasting analysis. It outperforms state-of-the-art models such as Autoformer [12], Informer [11], Reformer [15], Pyraformer, Nonstationary-transformer (NS-Trans [29]), ATFNet [30], and MICN [31], particularly excelling in high- dimensional time series forecasting. Figure 4 and 5 show sample long-term predictions for some popular architectures: EDformer, Autoformer, Informer, MICN, Nonstationary trans- former and Pyraformer on ETTm1 and traffic datasets. In addition to the results presented in Table V, further comparison of forecasting methods, including models listed in the table, is provided in Figure 3. Additionally, a comparison of these widely used models based on computational cost, speed, and average execution time is provided in Tables IX and X. For short-term forecasting performance, the proposed model has competitive performance with Autoformer, Informer, MICN, Non-stationary transformer (NS-Trans) and Pyraformer as shown in Table VII. The additional experiment using the M4 dataset is presented in Table VIII. Table VIII shows that our proposed EDformer model performs well and provides state-of-the-art accuracy compared to other models. Table IX presents a comparison of multivariate short-term forecasting models in terms of execution time (in seconds) across four PEMS datasets. Figure 6 shows sample short-term predic- tions for some popular architectures: EDformer, Autoformer, Informer, and Pyraformer on the PEMS03 dataset. From the perspective of the PEMS dataset, our EDformer model competes strongly with state-of-the-art models in terms of accuracy. Additionally, EDformer stands out for its lightweight design, achieving comparable results in significantly less time compared to other approaches. However, an extensive analysis"}, {"title": "VI. ABLATION STUDIES AND PERFORMANCE COMPARISONS", "content": "The ablation study presented in Table XI shows the average performance of different configurations on seven datasets: Etth1, Ettm1, Weather, Electricity, and Traffic, over four prediction lengths (96, 192, 336, 720). We aim to show the effects of incorporating reverse-embedding operations and decomposition mechanisms into the forecasting model. The table compares the mean squared error (MSE) and mean abso- lute error (MAE) for three key configurations: using without decomposition, with decomposition, and reverse embedding in combination with decomposition. The results demonstrate that when average decomposition is employed (second and third rows), the model consistently outperforms the model without decomposition (first row) across all datasets. This highlights the significance of allowing the model to learn how to decompose the input time series rather than using a predefined approach. Moreover, the addition of reverse embedding further improves the results (third row), suggesting that reverse embedding, which reverses the whole seasonal signal embedding, provides crucial temporal information that enhances forecasting accuracy. Overall, the combination of average decomposition and reverse embedding leads to the most significant performance gains, making these features"}, {"title": "VII. EXPLAINABILITY ANALYSIS", "content": "In multivariate time series forecasting, model explainability is essential because it enables stakeholders to comprehend, believe, and analyse forecasts produced by intricate models. Understanding how each feature affects the prediction outcome is essential for diagnosing model behaviour, enhancing model robustness, and learning about variable dependencies because multivariate time series data is high-dimensional and involves multiple interdependent variables that affect the forecast [32]. Each prediction can be attributed to a different characteris- tic using some well-known feature explainability techniques like feature ablation, feature occlusion, integrated gradients, gradient-SHAP, and windowed feature importance in time. These techniques are briefly described below.\n\u2022 Feature Ablation (FA) is an explainability technique that systematically removes one feature at a time from the in- put data to assess its impact on the model's performance. By measuring the change in prediction accuracy or out- put, this method identifies the importance of individual features. It helps highlight which features contribute most significantly to the model's decisions, making it useful for understanding the behavior of complex models [33].\n\u2022 Feature Occlusion (FO) involves masking or replacing parts of the input data (e.g., setting a feature to zero) to observe the change in the model's output. By selectively occluding features or segments of the input, this method can identify which components are most influential in the prediction process. It is commonly used in time series and image data analysis to understand spatial or temporal feature importance [33].\n\u2022 Integrated Gradients (IG) is an attribution method that explains model predictions by accumulating gradients along a path from a baseline input (e.g., all zeros) to the actual input. It calculates the contribution of each feature by integrating the gradients of the model's output with respect to the input features. This technique ensures that the attributions are consistent and satisfy properties like completeness, making it a reliable approach for model interpretability [34].\n\u2022 Shapley Additive explanations (SHAP) uses coopera- tive game theory to allocate feature priority, quantifying each variable's contribution across all conceivable feature subsets [35]. In the context of a Transformer model for multivariate time series forecasting, SHAP can provide insights into [36]:\nWhich time steps (lags) are most influential?\nWhich variables (features) contribute most to the prediction?\nHow do different values impact the forecast?\nGradient SHAP (GS) is a hybrid explainability tech- nique that combines SHAP (SHapley Additive exPla- nations) values with gradient-based methods. It uses stochastic sampling and integrates gradients over a range of input samples to approximate the Shapley values for each feature. This method provides robust feature attributions and is particularly effective for complex, non- linear models, offering insights into how each feature influences the predictions [34].\n\u2022 Windowed Feature Importance in Time (WinIT) is a feature removal based explainability approach. WinIT explicitly incorporates temporal dependencies by consid- ering the relationship between consecutive observations of the same feature when calculating its importance score. Additionally, it accounts for the dynamic nature of feature importance over time by summarizing its significance across a window of previous time steps, thereby capturing temporal variations in the feature's influence [28, 37].\nTwo metrics [28] have been used here to measure the impor- tance of these above explainability methods.\nA. Comprehensiveness: It evaluates how important a subset of features is for the model's prediction. It measures the impact of removing these key features on the model's output. The idea is that if the identified important features are truly contributing to the prediction, then omitting them should lead to a significant drop in the model's confidence or prediction score. A higher comprehensiveness score indicates better feature attribution, as it confirms that the model relies on these features to make its decisions. It helps in validating the quality of the explainability technique by checking if the most influential features indeed hold crucial information for the model. Table XII presents a comparison of 'Comprehensiveness' scores (av-"}, {"title": "B. Sufficiency", "content": "erage MAE) for some state-of-the-art explainability methods on the Electricity dataset. Table XIII presents a comparison of 'Comprehensiveness' scores (average MSE) for some state- of-the-art explainability methods on the Electricity dataset. It is evident that our proposed EDformer model registers the highest number of wins in terms of 'Comprehensiveness' compared to other methods.\nB. Sufficiency: It assesses whether the identified features alone are sufficient to maintain the prediction outcome. It does this by measuring the performance of the model when only the key attributed features are retained, while the rest are removed or masked. If the model's predictions remain consistent, it indicates that these features are adequate for making accurate predictions. For the sufficiency metric, a higher value is considered better. The sufficiency score helps determine if the feature attribution is complete, ensuring that the identified features not only contribute significantly but also capture enough information to make reliable predictions. Table XIV presents a comparison of 'Sufficieny' scores (average MAE) for some state-of-the-art explainability methods on the Electricity dataset. Table XV presents a comparison of 'Sufficieny' scores (average MSE) for some state-of-the-art ex- plainability methods on the Electricity dataset. It is evident that our proposed EDformer model registers the highest number of wins in terms of 'Sufficiency' compared to other methods."}, {"title": "A. Discussion on Explainability and Feature Importance", "content": "Table XII to Table XIV evaluate the comprehensiveness and sufficiency of explainability methods across six fore- casting models (Autoformer, Informer, NS-Trans, Reformer, MICN, and EDformer) for the Electricity (ECL) dataset with a prediction length of 24. Comprehensiveness measures the reduction in predictive accuracy when the most important features identified by an explainability method are removed, while sufficiency assesses the model's performance when only these key features are retained. The highest scores (marked in red) indicate the explainability method's ability to capture the most impactful features effectively. The analysis reveals that EDformer consistently achieves the highest scores in both met- rics, particularly excelling in Sufficiency. This suggests that EDformer benefits the most from the features identified by the explainability methods, reinforcing its robustness in utilizing relevant information. Conversely, models like Informer and Reformer have fewer \"wins,\" indicating potential room for improvement in aligning their performance with explainability insights. The use of this explainability analysis ensures that models are not treated as \"black boxes.\" It helps identify the significance of input features, improves transparency, and fosters trust in AI predictions. This is particularly crucial for datasets like 'Electricity', where decisions can impact critical energy management systems. By understanding feature importance, stakeholders can optimize model inputs, enhance interpretability, and ensure alignment with real-world domain knowledge.\nThe feature importance analysis in multivariate time se- ries forecasting models provides valuable insights into the contribution of different input variables toward predicting the target outcomes. Using methods such as Gradient SHAP (GS), and feature ablation (FA), the significance of electricity (ECL) dataset features is evaluated for the EDformer model. The visual representations of Fig. 8 highlight variations in feature importance across methods and the EDformer model. Figure 8 highlights that the electricity consumption/load for future time steps (OT) is the most influential feature in the overall prediction of the EDformer model, as it directly represents the target class. Additionally, other features such as seasonality, temperature, time of day, and humidity also exhibit notable impacts on the prediction results. The saliency analysis depicted in Fig.9, illustrates the predictive relevance of individual features in the multivariate time-series models applied to the electricity (ECL) dataset. The saliency maps show variation in feature importance across models, indi- cating diverse sensitivity patterns. This analysis underscores the potential of interpretability methods in guiding model selection and enhancing understanding of process dynamics in multivariate time-series forecasting tasks."}, {"title": "VIII. CONCLUSIONS", "content": "In this paper, EDformer, an architecture for multivariate time series forecasting that integrates decomposition into seasonal and trend components with a reverse embedding process, followed by an encoding-based forecaster, has been introduced. EDformer consistently achieves state-of-the-art performance across a wide range of benchmarks, showcasing its generality and robustness for long-term and short-term forecasting tasks. Moreover, EDformer has demonstrated state- of-the-art runtime efficiency, reduced cost time, and significant speed-up compared to other models. This paper delves into advanced model explainability techniques\u2014such as feature ablation, feature occlusion, integrated gradients, gradient- SHAP, and windowed feature importance over time-within the realm of time series forecasting. It aims to identify which black-box algorithms derive the greatest benefits from these explainability methods, thereby highlighting their robustness in effectively utilizing critical features and improving model interpretability. Detailed visualizations and ablations are in- cluded to provide insights into our architecture. In the future, this model will be tested for some real-life time series datasets."}]}