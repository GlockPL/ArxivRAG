{"title": "SELF-AUGMENTED GAUSSIAN SPLATTING WITH STRUCTURE-AWARE MASKS FOR SPARSE-VIEW 3D RECONSTRUCTION", "authors": ["Lingbei Meng", "Bi'an Du", "Wei Hu"], "abstract": "Sparse-view 3D reconstruction stands as a formidable challenge in computer vision, aiming to build complete three-dimensional models from a limited array of viewing perspectives. This task confronts several difficulties: 1) the limited number of input images that lack consistent information; 2) dependence on the quality of input images; and 3) the substantial size of model parameters. To address these challenges, we propose a self-augmented coarse-to-fine Gaussian splatting paradigm, enhanced with a structure-aware mask, for sparse-view 3D reconstruction. In particular, our method initially employs a coarse Gaussian model to obtain a basic 3D representation from sparse-view inputs. Subsequently, we develop a fine Gaussian network to enhance consistent and detailed representation of the output with both 3D geometry augmentation and perceptual view augmentation. During training, we design a structure-aware masking strategy to further improve the model's robustness against sparse inputs and noise. Experimental results on the MipNeRF360 and OmniObject3D datasets demonstrate that the proposed method achieves state-of-the-art performances for sparse input views in both perceptual quality and efficiency.", "sections": [{"title": "INTRODUCTION", "content": "Reconstructing and rendering three-dimensional objects from two-dimensional images is a fundamental problem in the realm of computer vision. This intricate process involves translating sparse visual data into detailed and comprehensive 3D models, enabling applications such as augmented reality, robotics and 3D game/movie asset creation. Despite the potential, this task requires not only understanding complex geometries, but also typically capturing dozens of multi-view images, which is cumbersome and costly. Hence, it is desirable to efficiently reconstruct high-quality 3D objects from highly sparse captured images.\nPrevious approaches primarily focus on reducing reliance on dense captures. The goal is that the auxiliary consistency loss could avoid a degenerate solution to its image reconstruction objective Jain et al. (2021); Niemeyer et al. (2022); Shi et al. (2024); Guangcong et al. (2023); Yang et al. (2023); Zhou & Tulsiani (2023); Zhu et al. (2023); Song et al. (2023a) However, there are still several significant hurdles when the views become extremely sparse, e.g, only 4 images in a 360\u00b0 range. Primarily, the limited number of available input images severely constrains the achievable level of detail, often resulting in reconstructions that lack fidelity to the original objects. This scarcity of data undermines the consistency of the information that can be leveraged, complicating the generation of high-quality surface textures. Moreover, the performance of existing models is heavily contingent upon the quality and number of input images, resulting in a pronounced vulnerability to variations in object complexity and environmental noise. Such dependencies underscore a critical robustness gap in contemporary approaches. Additionally, the substantial parameter size inherent to these models not only burdens the training phase but also impairs the efficiency of inference, further restricting their practicality and scalability."}, {"title": "RELATED WORKS", "content": "Recent advancements in differentiable point-based rendering have provided transformative ap- proaches for utilizing point cloud information in the reconstruction of complex scenes. DSS Yi- fan et al. (2019) and SynSin Wiles et al. (2020) have pioneered the enhancement of point clouds with additional features, converting these enriched point representations into images through neural networks. These methods have utilized convolutional neural networks (CNNs) to synthesize views directly from point clouds, addressing challenges associated with traditional rendering techniques that often result in incomplete or noisy reconstructions. On the forefront of this innovation, Point- NeRF Xu et al. (2023a) has expedited the process of neural radiance fields(NeRF) Mildenhall et al. (2020) by rendering discrete neural points, reducing the computational burden typically associated with continuous volumetric fields. While significantly speeding up the rendering process, Point- NeRF still necessitates a few seconds for image rendering, highlighting a trade-off between speed and visual fidelity. Complementing these methods, Point-Radiance Zhang et al. (2023) leverages spherical harmonics for dynamic point color representation. This method employs a novel splatting strategy that not only preserves high visual quality but also facilitates real-time rendering. The use of spherical harmonics enables a more accurate and flexible color interpolation at rendering time, making it particularly effective for scenes with complex lighting conditions.\nBuilding on these developments, 3D Gaussian Splatting(3DGS) Kerbl et al. (2023) has integrated 3D Gaussian models, pushing the boundaries further by achieving high-quality real-world scene reconstructions at impressive speeds. Despite these leaps forward, a recurring challenge remains in the dependency of point-based methods on the initial precision and density of point configurations, which poses a substantial obstacle particularly in sparse-view 360\u00b0 reconstruction scenarios.Our approach innovatively marries object structure priors with point-based rendering, offering a robust"}, {"title": "NEURAL RENDERING FOR SPARSE VIEW RECONSTRUCTION", "content": "In the realm of neural rendering for sparse view reconstruction, while NeRF's original formulation falters in under-sampled scenarios, subsequent advancements have sought to address these limi- tations. Techniques Deng et al. (2022); Roessle et al. (2022); Somraj et al. (2023); Somraj & Soundararajan (2023) deploying structure-from-motion (SfM) derived Sch\u00f6nberger & Frahm (2016) visibility or depth have shown promise, but typically focus on closely aligned views Xu et al. (2022) and often necessitate costly ground-truth depth maps for real-world images. Alternative methods Song et al. (2023a); Guangcong et al. (2023) resort to monocular depth estimation models Ranftl et al. (2021; 2020) or sensors; however, these often yield reconstructions that are too coarse for de- tailed renderings. Utilizing a vision-language model Radford et al. (2021) for unseen view rendering has been explored, although it tends to provide high-level semantic consistency which may not ef- fectively guide low-level reconstruction. Another approach Shi et al. (2024) integrates a deep image prior with factorized NeRF, capturing overall appearance but potentially sacrificing finer details in input views. Various priors grounded in information theory Kim et al. (2022), continuity Niemeyer et al. (2022), symmetry Seo et al. (2023), and frequency regularization Song et al. (2023b); Yang et al. (2023) have shown effectiveness in specific contexts, yet their applicability beyond these sce- narios remains limited. Additionally, there exist alternative methods Hong et al. (2024); Jang & Agapito (2024); Jiang et al. (2023); Wang et al. (2023); Xu et al. (2023b); Zou et al. (2023) em- ploying Visual Transformers (ViT) Dosovitskiy et al. (2021) to mitigate the complexities involved in constructing NeRFs.\nThe GaussianObject model Yang et al. (2024), a precursor in this field, has leveraged structure- prior-aided Gaussian initialization to enhance reconstruction from sparse inputs. This innovation has significantly reduced the necessity for extensive view inputs\u2014from over twenty views required by methods like FSGS Zhu et al. (2023) to a mere four-indicating a substantial stride forward in the efficiency of neural rendering techniques.\nNevertheless, despite such progress, many current models, including GaussianObject, struggle with sparse 360\u00b0 views. They frequently rely on SfM points, which can be a bottleneck in scenarios with insufficient data. Our work builds upon this foundation and extends it by integrating a robust frame- work for sparse setups without the heavy dependence on precise point cloud data, streamlining the reconstruction process and widening the applicability of neural rendering in real-world scenarios."}, {"title": "METHOD", "content": "In this section, we elaborate on the proposed self-augmented coarse-to-fine Gaussian splatting paradigm. We start with a overview of the key ideas, and then present our 3D geometry augmenta- tion with coarse Gaussian points and perceptual view augmentation with coarse rendering images. Furthermore, we describe how we integrate the structure-aware masks in coarse-to-fine Gaussian process and detail the training objective."}, {"title": "OVERVIEW", "content": "We start with a set of sparse reference images $X_{ref} = {x_i}_{i=1}^N$, each capturing a single object from within a complete 360\u00b0 range, complete with their respective camera parameters $H_{ref} = {\\pi_i}_{i=1}^N$ and object-specific masks $M_{ref} = {m_i}_{i=1}^N$. Our aim is to create a coarse 3D Gaussian model $G_c$ and a fine 3D Gaussian model $G_f$ that are capable of rendering the object photo-realistic from any desired viewpoint. To facilitate this, we adopt the 3DGS model, chosen for its straightforward ap- proach to integrating structural priors and its efficiency in rendering. The coarse 3D Gaussian model takes sparse-view images as input and output the coarse point cloud and coarse rendering images in novel views. This stage is followed by a 3D geometry augmentation module and a perceptual view augmentation module, which serve as initialized input of the fine Gaussian model. During training, we integrate the point-based masks and patch-based masks in the coarse Gaussian model and fine Gaussian model, respectively. Our overall framework is shown in Fig. 1."}, {"title": "3D GEOMETRY AUGMENTATION WITH COARSE GAUSSIAN POINTS", "content": "Sparse views, particularly when limited to as few as four images, provide very limited 3D informa- tion for reconstruction. Under these circumstances, the SFM points essential for initializing 3DGS are often lost, and the valid geometry is missing, reducing the accuracy and efficiency of subsequent densification of 3D Gaussians.\nWe propose the 3D geometry augmentation module with coarse Gaussian points to to provide more accurate initial geometric information for fine Gaussian. Specifically, we obtain the coarse point cloud $P_c = G(x_i, \\pi_i, m_i)_{i=1}^P$. Each 3D Gaussian point is composed of the center location $\\mu$, rotation quaternion $q$, scaling vector $s$, opacity $o$, and spherical harmonic (SH) coefficient $sh$, and the coarse point cloud is parameterized as a set of Gaussian points $P_c = {p_i : \\mu_i, q_i, s_i, o_i, sh_i}_{i=1}^P$.\nThe coarse point cloud $P_c$ provides more accurate geometry and color information of the object than the SFM point cloud in the initial stage of our coarse-to-fine 3D Gaussian paradigm. When transitioning to the fine Gaussian stage, we convert the spherical harmonic coefficient into color information and only retain and utilize the spatial coordinates $\\mu_i$ and color information $c_i$ of the points, deliberately excluding other properties such as spherical harmonics and transparency. This selective transfer focuses only on enhancing the geometric form of the reconstruction, thus avoiding the complication of unnecessary data that could reduce the structural accuracy of the model."}, {"title": "PERCEPTUAL VIEW AUGMENTATION WITH COARSE RENDERING IMAGES", "content": "Another reason why 3D reconstruction is difficult under sparse views is that reference images pro- vide too little perspective information. For unobserved perspectives, 3D Gaussians do not have any reference information. Given this observation, we propose perceptual view augmentation with coarse rendering images. These images provide additional contextual and textural information, func- tioning similarly to pseudo-labels that enrich the model's understanding of the scene from the sparse view inputs.\nSpecifically, from the coarse Gaussian model, we obtain the rendered images $X_{coarse} = {x_i}^{N'}_{i=1}$ in N' novel views, and we denote the augmented reference images as $X_{fine} = X_{ref} \\cup X_{coarse}$ and the corresponding camera parameters as $\\If_{ine} = \\Pi_{ref} \\cup \\Pi_{coarse}$. In addition, note that since the coarse rendered images do not carry additional background information, the corresponding mask is no longer needed to extract the object. Then we train the fine Gaussian model to obtain photo- realistic rendering from any viewpoint $x = G{(\\pi | {x_i, \\pi_i}}_{i=1}^{N+N'}}$. By avoiding the introduction"}, {"title": "INTEGRATION OF STRUCTURE-AWARE MASKS IN COARSE-TO-FINE GAUSSIAN PROCESS", "content": "We employ sophisticated masking strategies in both the coarse and fine Gaussian models to enhance the accuracy and detail of 3D models from sparse-view data. These strategies are segmented into two distinct components: point-based masks for coarse Gaussians and patch-based masks for fine Gaussians, each contributing uniquely to the coarse-to-fine Gaussian process."}, {"title": "POINT-BASED MASKS FOR COARSE GAUSSIANS", "content": "As mentioned above, we apply point-based random masks to 3D Gaussians to improve the robust- ness of the coarse 3D Gaussian model to sparse inputs as well as noise.\nSpecifically, we set a ratio $r_c$ of the mask and update the iteration gap $t_c$ of the mask. For every $t_c$ iterations, we randomly select all current 3D gaussian points with ratio $r_c$. The selected $r_c * P$ 3D gaussian points will be discarded and will not participate in the subsequent densification process.\nIn the coarse Gaussian model, point-based masks are utilized to improve structural accuracy and integrity. These masks target individual points, or Gaussian centers, derived from sparse input data and are dynamically generated by analyzing rendering errors against the actual images. This allows the identification of regions where the model's representation diverges significantly from the real scene. The application of these masks directs focus and resources towards areas with higher re- construction errors, guiding the adjustment of Gaussian parameters such as means and covariances. This adaptive process ensures that the model refines areas that require attention while conserving resources on well-represented regions, thereby laying a robust foundation for subsequent detailed enhancements."}, {"title": "PATCH-BASED MASKS FOR FINE GAUSSIANS", "content": "With sparse-view images as input, the information of certain parts of the object is usually completely missing, such as a certain side of the kitchen. This is reflected in the SFM point cloud as no points or very sparse points in the corresponding parts. This is difficult to completely solve during the densification process.\nTo improve the robustness of the fine 3D Gaussian model to partially-missing input and noise, we apply patch-based random masks to 3D Gaussians in the fine Gaussian model. Specifically, we extract C patch centers from the fine Gaussian point cloud through the farthest point sampling"}, {"title": "EXPERIMENT", "content": ""}, {"title": "IMPLEMENTATION DETAIL", "content": "Our reconstruction framework extends the 3D Gaussian Splatting (3DGS) Kerbl et al. (2023) tech- nique, incorporating a novel coarse-to-fine approach tailored for sparse-view 3D reconstruction. The coarse phase involves multiple training iterations, with densification and opacity resets applied periodically, inspired by the original 3DGS methodology. Depth estimation for this phase utilizes ZoeDepth Bhat et al. (2023) for precise monocular depth predictions. Integration of a point-based mask strategy ensures focused refinement by selectively masking points to address areas critical for geometric fidelity."}, {"title": "EVALUATION", "content": "In our evaluation, we compare the performance of our AugGaussian model against a range of state- of-the-art (SOTA) reconstruction models, as shown in Tab. 1, including the latest GaussianObject Yang et al. (2024), which represents the current benchmark in 3D reconstruction with its use of visual hull techniques, floater elimination, and a pretrained Gaussian repair model to enhance results. Our comparisons also include baseline methods such as the vanilla 3DGS Kerbl et al. (2023) with random initialization, DVGO Sun et al. (2022), and few-view specialized models like RegNeRF Niemeyer et al. (2022), DietNeRF Jain et al. (2021), SparseNeRF Guangcong et al. (2023), and ZeroRF Shi et al. (2024), which implement various regularization strategies. FSGS Zhu et al. (2023), another model built upon Gaussian Splatting with Structure from Motion (SfM)-point initialization, was also evaluated, provided with extra SfM points to accommodate the highly sparse 360\u00b0 setting. Each model was benchmarked using its publicly released implementation, ensuring consistency in evaluation.\nPerformance was assessed across two datasets, showcasing that our AugGaussian not only com- petes but in many instances surpasses the GaussianObject on key metrics, without the reliance on additional optimization strategies. This achievement underscores the effectiveness and efficiency of our straightforward, lightweight model architecture. Particularly noteworthy is our model's ability to outperform in perceptual quality metrics such as PSNR, a critical indicator of the visual qual- ity of reconstructed images. Our methodology, informed by a structure-aware training approach, effectively addresses the challenges posed by sparse data and scales efficiently with increased data availability. Through rigorous testing across diverse datasets, our model consistently surpasses state- of-the-art (SOTA) benchmarks in both PSNR and SSIM metrics. It notably exceeds the current best models by more than 10% in optimal scenarios, affirming its capability to reconstruct high-fidelity images from limited inputs. In terms of the LPIPS metric, which assesses perceptual image quality, our model outperforms SOTA in over 90% of cases with 4 and 6 views, and in 70% of cases with 9 views. Even where it does not outperform, the difference is kept within a 15% margin. Qualitatively, our results reveal a smoother reconstruction of structures not present in the input views, avoiding issues such as fragmentation, missing details, and interlacing."}, {"title": "ABLATION STUDIES", "content": "In our ablation study, we first investigated the impact of different augmentation strategies on the reconstruction quality through four distinct experimental setups: no augmentation application, dis- abling perceptual view or mask augmentation, and a combined approach integrating all augmen- tation types. The results, quantitatively illustrated in Tab. 4 and qualitatively depicted in Fig. 3, clearly demonstrate the significance of each augmentation strategy in achieving optimal reconstruc- tion outcomes. Specifically, the incorporation of each augmentation strategy significantly improves the model's performance compared to the baseline scenario with no augmentation, while the com- bined use of all augmentation strategies yields the most pronounced enhancements in reconstruction fidelity. From Fig. 3, we can see that the results not using all augmentation strategies fail to ef- fectively reconstruct the quarter-exposed gear structure inside the vehicle body, as well as the front structure of the vehicle that is obscured by the connecting structure between the bucket and the body. However, using all augmentation strategies provides some modeling capabilities for these obscured structures. This study demonstrates the critical role of augmentation strategies in refining both the structural integrity and surface detail of 3D models, affirming their necessity for attaining high-quality reconstruction results.\nIn the second part of our ablation study, we examined the effect of iteration numbers on the perfor- mance of our model, conducting experiments on three objects within the MipNeRF360 dataset with both 4 and 9 input views, as shown in Fig. 4. We structured the coarse Gaussian phase with groups every 500 iterations and the fine Gaussian phase with groups every 1000 iterations. Experiments conducted on 17 objects from the Omni3D dataset revealed that there is no universal optimal iter- ation count across different objects. For the coarse Gaussian results, the optimal iterations ranged from 1000 to 5000, with no clear correlation between iteration count and object features. In the case of fine Gaussian refinement, the best results were generally achieved within 6000 iterations; exceeding this count tended to degrade the results. Additionally, more complex structures typically required a higher number of fine Gaussian iterations to achieve optimal refinement. This observation highlights that the structural complexity of an object significantly influences the required iterations for each phase, necessitating a tailored approach to achieve the best reconstruction outcomes. No- tably, an optimal performance of the coarse Gaussian phase did not necessarily guarantee improved outcomes in the fine Gaussian refinement phase.\nThe third part of our ablation study delves into the efficiency of our AugGaussian model, the results of which are encapsulated in Fig. 5. The data underscores a significant efficiency advantage of our approach over the current state-of-the-art (SOTA) model, GaussianObject. Owing to our frame- work's simplicity, devoid of complex feature extraction structures and the absence of pre-trained models, we achieved a marked reduction in training time-approximately 6.3% of that required by the SOTA model. This model's lightweight nature translates to higher training efficiency and re- duced computational demands. We measured the total runtime, training time, and rendering time in seconds across several tests on the 4-view kitchen dataset and calculated averages. Consistently, our model's training duration remained within 10% of the SOTA model's training time. While the rendering times of AugGaussian are largely on par with GaussianObject, the overall runtime of our model is about 7.1% of the SOTA, indicating an efficiency improvement by over an order of magni- tude. Furthermore, the streamlined nature of our model translates to lower demands on memory and GPU resources. This efficiency is not only crucial for expediting the research and development cycle but also offers considerable advantages for deployment on edge devices and platforms with limited computational power, opening the door to more accessible and sustainable real-world applications of 3D reconstruction technology."}, {"title": "CONCLUSION", "content": "This paper introduced a novel sparse-view 3D reconstruction approach leveraging self-view aug- mented Gaussian Splatting with structure-aware masks, addressing the challenge of generating high-fidelity 3D models from limited 2D inputs. Through our innovative coarse-to-fine Gaussian modeling and the strategic application of point-based and patch-based masks, we achieved signif- icant improvements in geometric precision and surface detail. The method demonstrated superior performance against state-of-the-art techniques across several benchmark datasets, including Mip- NeRF360 and OmniObject3D, underscoring its efficiency and the reduced computational footprint.\nFuture efforts will focus on optimizing the coarse-to-fine process, harnessing deep learning for more sophisticated mask generation, and extending the approach to dynamic scenes. Exploring synergies with other advanced computer vision techniques may further enhance the capability to reconstruct complex scenes with unprecedented detail and accuracy. Our work not only sets a new standard for sparse-view 3D reconstruction but also opens new avenues for research in the field, promising to significantly impact applications requiring detailed 3D models from minimal input data."}]}