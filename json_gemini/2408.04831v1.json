{"title": "SELF-AUGMENTED GAUSSIAN SPLATTING WITH STRUCTURE-AWARE MASKS FOR SPARSE-VIEW 3D RECONSTRUCTION", "authors": ["Lingbei Meng", "Bi'an Du", "Wei Hu"], "abstract": "Sparse-view 3D reconstruction stands as a formidable challenge in computer vision, aiming to build complete three-dimensional models from a limited array of viewing perspectives. This task confronts several difficulties: 1) the limited number of input images that lack consistent information; 2) dependence on the quality of input images; and 3) the substantial size of model parameters. To address these challenges, we propose a self-augmented coarse-to-fine Gaussian splatting paradigm, enhanced with a structure-aware mask, for sparse-view 3D reconstruction. In particular, our method initially employs a coarse Gaussian model to obtain a basic 3D representation from sparse-view inputs. Subsequently, we develop a fine Gaussian network to enhance consistent and detailed representation of the output with both 3D geometry augmentation and perceptual view augmentation. During training, we design a structure-aware masking strategy to further improve the model's robustness against sparse inputs and noise. Experimental results on the MipNeRF360 and OmniObject3D datasets demonstrate that the proposed method achieves state-of-the-art performances for sparse input views in both perceptual quality and efficiency.", "sections": [{"title": "INTRODUCTION", "content": "Reconstructing and rendering three-dimensional objects from two-dimensional images is a funda-mental problem in the realm of computer vision. This intricate process involves translating sparsevisual data into detailed and comprehensive 3D models, enabling applications such as augmentedreality, robotics and 3D game/movie asset creation. Despite the potential, this task requires not onlyunderstanding complex geometries, but also typically capturing dozens of multi-view images, whichis cumbersome and costly. Hence, it is desirable to efficiently reconstruct high-quality 3D objectsfrom highly sparse captured images.\nPrevious approaches primarily focus on reducing reliance on dense captures. The goal is that theauxiliary consistency loss could avoid a degenerate solution to its image reconstruction objective. However, there are still severalsignificant hurdles when the views become extremely sparse, e.g, only 4 images in a 360\u00b0 range.Primarily, the limited number of available input images severely constrains the achievable level ofdetail, often resulting in reconstructions that lack fidelity to the original objects. This scarcity of dataundermines the consistency of the information that can be leveraged, complicating the generation ofhigh-quality surface textures. Moreover, the performance of existing models is heavily contingentupon the quality and number of input images, resulting in a pronounced vulnerability to variations inobject complexity and environmental noise. Such dependencies underscore a critical robustness gapin contemporary approaches. Additionally, the substantial parameter size inherent to these modelsnot only burdens the training phase but also impairs the efficiency of inference, further restrictingtheir practicality and scalability."}, {"title": "RELATED WORKS", "content": ""}, {"title": "DIFFERENTIABLE POINT-BASED RENDERING", "content": "Recent advancements in differentiable point-based rendering have provided transformative ap-proaches for utilizing point cloud information in the reconstruction of complex scenes. DSS and SynSin have pioneered the enhancement of point cloudswith additional features, converting these enriched point representations into images through neuralnetworks. These methods have utilized convolutional neural networks (CNNs) to synthesize viewsdirectly from point clouds, addressing challenges associated with traditional rendering techniquesthat often result in incomplete or noisy reconstructions. On the forefront of this innovation, Point-NeRF has expedited the process of neural radiance fields(NeRF) by rendering discrete neural points, reducing the computational burden typically associatedwith continuous volumetric fields. While significantly speeding up the rendering process, Point-NeRF still necessitates a few seconds for image rendering, highlighting a trade-off between speedand visual fidelity. Complementing these methods, Point-Radiance leverages spherical harmonics for dynamic point color representation. This method employs a novel splattingstrategy that not only preserves high visual quality but also facilitates real-time rendering. The useof spherical harmonics enables a more accurate and flexible color interpolation at rendering time,making it particularly effective for scenes with complex lighting conditions.\nBuilding on these developments, 3D Gaussian Splatting(3DGS) has integrated3D Gaussian models, pushing the boundaries further by achieving high-quality real-world scenereconstructions at impressive speeds. Despite these leaps forward, a recurring challenge remains inthe dependency of point-based methods on the initial precision and density of point configurations,which poses a substantial obstacle particularly in sparse-view 360\u00b0 reconstruction scenarios.Ourapproach innovatively marries object structure priors with point-based rendering, offering a robust"}, {"title": "NEURAL RENDERING FOR SPARSE VIEW RECONSTRUCTION", "content": "In the realm of neural rendering for sparse view reconstruction, while NeRF's original formulationfalters in under-sampled scenarios, subsequent advancements have sought to address these limi-tations. Techniques deploying structure-from-motion (SfM) derived visibility or depth have shown promise, but typically focus on closely aligned views and often necessitate costly ground-truth depth maps for real-world images. Alternative methodsresort to monocular depth estimation models or sensors; however, these often yield reconstructions that are too coarse for de-tailed renderings. Utilizing a vision-language model for unseen view renderinghas been explored, although it tends to provide high-level semantic consistency which may not ef-fectively guide low-level reconstruction. Another approach integrates a deep imageprior with factorized NeRF, capturing overall appearance but potentially sacrificing finer details ininput views. Various priors grounded in information theory, continuity, symmetry, and frequency regularization have shown effectiveness in specific contexts, yet their applicability beyond these sce-narios remains limited. Additionally, there exist alternative methodsemploying Visual Transformers (ViT) to mitigate the complexities involvedin constructing NeRFs.\nThe GaussianObject model, a precursor in this field, has leveraged structure-prior-aided Gaussian initialization to enhance reconstruction from sparse inputs. This innovationsignificantly reduced the necessity for extensive view inputs\u2014from over twenty views requiredby methods like FSGS to a mere four-indicating a substantial stride forward inthe efficiency of neural rendering techniques.\nNevertheless, despite such progress, many current models, including GaussianObject, struggle withsparse 360\u00b0 views. They frequently rely on SfM points, which can be a bottleneck in scenarios withinsufficient data. Our work builds upon this foundation and extends it by integrating a robust frame-work for sparse setups without the heavy dependence on precise point cloud data, streamlining thereconstruction process and widening the applicability of neural rendering in real-world scenarios."}, {"title": "METHOD", "content": "In this section, we elaborate on the proposed self-augmented coarse-to-fine Gaussian splattingparadigm. We start with a overview of the key ideas, and then present our 3D geometry augmenta-tion with coarse Gaussian points and perceptual view augmentation with coarse rendering images.Furthermore, we describe how we integrate the structure-aware masks in coarse-to-fine Gaussianprocess and detail the training objective."}, {"title": "OVERVIEW", "content": "We start with a set of sparse reference images \\(X_{ref} = {x_i}_{i=1}^N\\), each capturing a single object fromwithin a complete 360\u00b0 range, complete with their respective camera parameters \\(H_{ref} = {\\pi_i}_{i=1}^N\\)and object-specific masks \\(M_{ref} = {m_i}_{i=1}^N\\). Our aim is to create a coarse 3D Gaussian model \\(G_c\\)and a fine 3D Gaussian model \\(G_f\\) that are capable of rendering the object photo-realistic from anydesired viewpoint. To facilitate this, we adopt the 3DGS model, chosen for its straightforward ap-proach to integrating structural priors and its efficiency in rendering. The coarse 3D Gaussian modeltakes sparse-view images as input and output the coarse point cloud and coarse rendering images innovel views. This stage is followed by a 3D geometry augmentation module and a perceptual viewaugmentation module, which serve as initialized input of the fine Gaussian model. During training,we integrate the point-based masks and patch-based masks in the coarse Gaussian model and fineGaussian model, respectively."}, {"title": "3D GEOMETRY AUGMENTATION WITH COARSE GAUSSIAN POINTS", "content": "Sparse views, particularly when limited to as few as four images, provide very limited 3D informa-tion for reconstruction. Under these circumstances, the SFM points essential for initializing 3DGSare often lost, and the valid geometry is missing, reducing the accuracy and efficiency of subsequentdensification of 3D Gaussians.\nWe propose the 3D geometry augmentation module with coarse Gaussian points to to provide moreaccurate initial geometric information for fine Gaussian. Specifically, we obtain the coarse pointcloud \\(P_c = G_c(x_i, \\pi_i, m_i)_{i=1}^P\\). Each 3D Gaussian point is composed of the center location \\(\\mu\\), rotationquaternion \\(q\\), scaling vector \\(s\\), opacity \\(o\\), and spherical harmonic (SH) coefficient \\(sh\\), and the coarsepoint cloud is parameterized as a set of Gaussian points \\(P_c = {p_i : \\mu_i, q_i, s_i, o_i, sh_i}_{i=1}^P\\).\nThe coarse point cloud \\(P_c\\) provides more accurate geometry and color information of the objectthan the SFM point cloud in the initial stage of our coarse-to-fine 3D Gaussian paradigm. Whentransitioning to the fine Gaussian stage, we convert the spherical harmonic coefficient into colorinformation and only retain and utilize the spatial coordinates \\(\\mu_i\\) and color information \\(c_i\\) of thepoints, deliberately excluding other properties such as spherical harmonics and transparency. Thisselective transfer focuses only on enhancing the geometric form of the reconstruction, thus avoidingthe complication of unnecessary data that could reduce the structural accuracy of the model."}, {"title": "PERCEPTUAL VIEW AUGMENTATION WITH COARSE RENDERING IMAGES", "content": "Another reason why 3D reconstruction is difficult under sparse views is that reference images pro-vide too little perspective information. For unobserved perspectives, 3D Gaussians do not haveany reference information. Given this observation, we propose perceptual view augmentation withcoarse rendering images. These images provide additional contextual and textural information, func-tioning similarly to pseudo-labels that enrich the model's understanding of the scene from the sparseview inputs.\nSpecifically, from the coarse Gaussian model, we obtain the rendered images \\(X_{coarse} = {\\hat{x}_i}_{i=1}^{N'}\\)in \\(N'\\) novel views, and we denote the augmented reference images as \\(X_{fine} = X_{ref} \\cup X_{coarse}\\) andthe corresponding camera parameters as \\(I_{fine} = \\Pi_{ref} \\cup \\Pi_{coarse}\\). In addition, note that since thecoarse rendered images do not carry additional background information, the corresponding maskis no longer needed to extract the object. Then we train the fine Gaussian model to obtain photo-realistic rendering from any viewpoint \\(x = G_f{(\\pi | {x_i, \\pi_i}_{i=1}^{N+N'})\\). By avoiding the introduction"}, {"title": "INTEGRATION OF STRUCTURE-AWARE MASKS IN COARSE-TO-FINE GAUSSIAN PROCESS", "content": "We employ sophisticated masking strategies in both the coarse and fine Gaussian models to enhancethe accuracy and detail of 3D models from sparse-view data. These strategies are segmented intotwo distinct components: point-based masks for coarse Gaussians and patch-based masks for fineGaussians, each contributing uniquely to the coarse-to-fine Gaussian process."}, {"title": "POINT-BASED MASKS FOR COARSE GAUSSIANS", "content": "As mentioned above, we apply point-based random masks to 3D Gaussians to improve the robust-ness of the coarse 3D Gaussian model to sparse inputs as well as noise.\nSpecifically, we set a ratio \\(r_c\\) of the mask and update the iteration gap \\(t_c\\) of the mask. For every \\(t_c\\)iterations, we randomly select all current 3D gaussian points with ratio \\(r_c\\). The selected \\(r_c * P\\) 3Dgaussian points will be discarded and will not participate in the subsequent densification process.\nIn the coarse Gaussian model, point-based masks are utilized to improve structural accuracy andintegrity. These masks target individual points, or Gaussian centers, derived from sparse input dataand are dynamically generated by analyzing rendering errors against the actual images. This allowsthe identification of regions where the model's representation diverges significantly from the realscene. The application of these masks directs focus and resources towards areas with higher re-construction errors, guiding the adjustment of Gaussian parameters such as means and covariances.This adaptive process ensures that the model refines areas that require attention while conservingresources on well-represented regions, thereby laying a robust foundation for subsequent detailedenhancements."}, {"title": "PATCH-BASED MASKS FOR FINE GAUSSIANS", "content": "With sparse-view images as input, the information of certain parts of the object is usually completelymissing, such as a certain side of the kitchen. This is reflected in the SFM point cloud as no pointsor very sparse points in the corresponding parts. This is difficult to completely solve during thedensification process.\nTo improve the robustness of the fine 3D Gaussian model to partially-missing input and noise, weapply patch-based random masks to 3D Gaussians in the fine Gaussian model. Specifically, weextract \\(C\\) patch centers from the fine Gaussian point cloud through the farthest point sampling"}, {"title": "INTEGRATION DETAIL", "content": "Our reconstruction framework extends the 3D Gaussian Splatting (3DGS) technique, incorporating a novel coarse-to-fine approach tailored for sparse-view 3D reconstruction.The coarse phase involves multiple training iterations, with densification and opacity resets appliedperiodically, inspired by the original 3DGS methodology. Depth estimation for this phase utilizesfor precise monocular depth predictions. Integration of a point-basedmask strategy ensures focused refinement by selectively masking points to address areas critical forgeometric fidelity."}, {"title": "EVALUATION", "content": "In our evaluation, we compare the performance of our AugGaussian model against a range of state-of-the-art (SOTA) reconstruction models, as shown in Tab. 1, including the latest GaussianObjectwhich represents the current benchmark in 3D reconstruction with its use of visualhull techniques, floater elimination, and a pretrained Gaussian repair model to enhance results. Ourcomparisons also include baseline methods such as the vanilla 3DGS with randominitialization, DVGO and few-view specialized models like RegNeRF, DietNeRF, SparseNeRF, and ZeroRF, which implement various regularization strategies. FSGS, anothermodel built upon Gaussian Splatting with Structure from Motion (SfM)-point initialization, wasalso evaluated, provided with extra SfM points to accommodate the highly sparse 360\u00b0 setting.Each model was benchmarked using its publicly released implementation, ensuring consistency inevaluation.\nPerformance was assessed across two datasets, showcasing that our AugGaussian not only com-petes but in many instances surpasses the GaussianObject on key metrics, without the reliance onadditional optimization strategies. This achievement underscores the effectiveness and efficiency ofour straightforward, lightweight model architecture. Particularly noteworthy is our model's abilityto outperform in perceptual quality metrics such as PSNR, a critical indicator of the visual qual-ity of reconstructed images. Our methodology, informed by a structure-aware training approach,effectively addresses the challenges posed by sparse data and scales efficiently with increased dataavailability. Through rigorous testing across diverse datasets, our model consistently surpasses state-of-the-art (SOTA) benchmarks in both PSNR and SSIM metrics. It notably exceeds the current bestmodels by more than 10% in optimal scenarios, affirming its capability to reconstruct high-fidelityimages from limited inputs. In terms of the LPIPS metric, which assesses perceptual image quality,our model outperforms SOTA in over 90% of cases with 4 and 6 views, and in 70% of cases with 9views. Even where it does not outperform, the difference is kept within a 15% margin. Qualitatively,our results reveal a smoother reconstruction of structures not present in the input views, avoidingissues such as fragmentation, missing details, and interlacing."}, {"title": "ABLATION STUDIES", "content": "In our ablation study", "setups": "no augmentation application, dis-abling perceptual view or mask augmentation, and a combined approach integrating all augmen-tation types. The results, quantitatively illustrated in Tab. 4 and qualitatively depicted in Fig. 3,clearly demonstrate the significance of each augmentation strategy in achieving optimal reconstruc-tion outcomes. Specifically, the incorporation of each augmentation strategy significantly improvesthe model's performance compared to the baseline scenario with no augmentation, while the com-bined use of all augmentation strategies yields the most pronounced enhancements in reconstructionfidelity. From Fig. 3, we can see that the results not using all augmentation strategies fail to ef-fectively reconstruct the quarter-exposed gear structure inside the vehicle body, as well as the frontstructure of the vehicle that is obscured by the connecting structure between the bucket and thebody. However, using all augmentation strategies provides some modeling capabilities for theseobscured structures. This study demonstrates the critical role of augmentation strategies in refiningboth the structural integrity and surface detail of 3D models, affirming their necessity for attaininghigh-quality reconstruction results.\nIn the second part of our ablation study, we examined the effect of iteration numbers on the perfor-mance of our model, conducting experiments on three objects within the MipNeRF360 dataset withboth 4 and 9 input views, as shown in Fig. 4. We structured the coarse Gaussian phase with groups"}]}