{"title": "MambaU-Lite: A Lightweight Model based on Mamba and Integrated Channel-Spatial Attention for Skin Lesion Segmentation", "authors": ["Thi-Nhu-Quynh Nguyen", "Quang-Huy Ho", "Duy-Thai Nguyen", "Hoang-Minh-Quang Le", "Van-Truong Pham", "Thi-Thao Tran"], "abstract": "Early detection of skin abnormalities plays a crucial role in diagnosing and treating skin cancer. Segmentation of affected skin regions using AI-powered devices is relatively common and supports the diagnostic process. However, achieving high performance remains a significant challenge due to the need for high-resolution images and the often unclear boundaries of individual lesions. At the same time, medical devices require segmentation models to have a small memory footprint and low computational cost. Based on these requirements, we introduce a novel lightweight model called MambaU-Lite, which combines the strengths of Mamba and CNN architectures, featuring just over 400K parameters and a computational cost of more than 1G flops. To enhance both global context and local feature extraction, we propose the P-Mamba block, a novel component that incorporates VSS blocks alongside multiple pooling layers, enabling the model to effectively learn multi-scale features and enhance segmentation performance. We evaluate the model's performance on two skin datasets, ISIC2018 and PH2, yielding promising results. Our source code is publicly available at: https://github.com/nqnguyen812/MambaU-Lite.", "sections": [{"title": "1 Introduction", "content": "The segmentation of skin lesions plays an important role in computer-aided diagnostic systems for skin cancer. However, before automated technology made its step into this medical area, the manual method of segmentation was thought to be tedious and inaccurate, which is unreliable and costly overall. Fortunately, with the advances of deep learning, especially the U-Net [1] and variants [2], [3], [4], various attempts to implement those into the segmentation tasks have been done with the aim of eliminating human error as well as increasing speed."}, {"title": "2 Related Work", "content": "Visual State Space Model [12]. Inspired by the Mamba [10], which successfully applied the State Space Model (SSM) from Control Theory to Natural Language Processing (NLP), Vision Mamba [12] was introduced as a novel approach"}, {"title": "3 The Proposed Model", "content": "In this section, the architecture of the proposed MambaU-Lite model is fully detailed and demonstrated in Fig.1. The model contains three fundamental sub-structures: Encoders, Bottleneck, and Decoders, together forming a U-shape combination similar to that of the classical U-Net [1]. Additionally, the model goes through four processing stages with four CBAM [16] blocks in the Skip-connection assisting the Decoders with rich spatial information from the Encoders.\nInitially, the input image is passed through an InitConv layer to adjust the number of channels to 16, resulting in a feature map of size $16 \\times H \\times W$. The image then undergoes a sequence of Encoder layers. Specifically, in the proposed architecture, we use the first two P-Mamba Encoder blocks (PE Blocks). After these two blocks together with max-pooling layers to reduce the spatial dimensions after each Encoder, the feature map sizes are $32 \\times \\frac{H}{2} \\times \\frac{W}{2}$ and $64 \\times \\frac{H}{4} \\times \\frac{W}{4}$, respectively. For the next two Encoder layers, the input is split into two parts, ef-fectively reducing the number of channels by half, and is then processed through the PE Block and the Axial Encoder Block (\u0391\u0395 Block). The sizes of the feature maps after passing through both the PE and AE Blocks and max-pooling layers are identical, with dimensions $64 \\times \\frac{H}{8} \\times \\frac{W}{8}$ and $128 \\times \\frac{H}{16} \\times \\frac{W}{16}$, respectively. The outputs of the final two blocks are concatenated, resulting in a feature map of size $256 \\times \\frac{H}{16} \\times \\frac{W}{16}$ and fed to a bottleneck and then combined with skip connections, is passed through the Decoder layers. After passing through all the Decoders and upsampling layers, the output from each decoder block is interpolated back to the original input size. These outputs are subsequently concatenated and pro-cessed through a FinalConv layer to produce the predicted mask for the input image."}, {"title": "3.1 The proposed PMamba Block", "content": "The proposed P-Mamba block, illustrated in Fig.2, is structured to improve the model's ability to learn diverse features. This is accomplished by processing the input feature maps through two distinct branches."}, {"title": "3.2 The Encoder Block", "content": "As described in Sec.3, the encoder is composed of two main blocks: the AE Block and the PE Block, as shown in Fig.2d and Fig.2e. For the AE Block, the outputs"}, {"title": "3.3 The Decoder Block", "content": "The overview of the Decoder block is presented in Fig.2f. Initially, the output from the previous Decoder layer is upsampled to match the size of the corre-sponding skip connection. It is then passed through the Attention Gate (AG) block, as shown in Fig.2c. The output of the AG block is concatenated with the upsampled feature maps from the previous Decoder layer. This concatenated output is then processed through a sequence of layers: a Pointwise convolu-tion layer for dimensionality reduction, followed by Batch Normalization, ReLU activation, another Pointwise convolution, and finally an Axial Depthwise con-volution with a 7x7 kernel. The combination of these layers enables the model to effectively extract meaningful features while minimizing the parameter count and computational overhead."}, {"title": "3.4 The Skip Connection and Bottleneck Block", "content": "The skip connection and bottleneck components play crucial roles in the model, helping prevent information loss during processing. In the proposed model, we use the CBAM block introduced by Woo et al. [16] for the skip connections, while the bottleneck employs an Integrated Channel-Spatial Attention (ICSA) block.\nThe ICSA block consists of two consecutive Priority Channel Attention (PCA) blocks followed by a Priority Spatial Attention (PSA) block, as proposed by Le et al. [18], which demonstrated outstanding performance in fish classification tasks. Specifically, the PCA block utilizes depthwise convolution to enhance feature extraction on each channel individually, while the PSA block applies pointwise convolution, improving feature maps across spatial regions. The use of the ICSA block in the bottleneck effective capture of high-level features effectively before passing them to the Decoder."}, {"title": "4 Experiment", "content": null}, {"title": "4.1 Dataset", "content": "To assess the efficacy of the proposed method, we implement experiments on two skin lesion datasets: ISIC 2018 and PH2. The ISIC 2018 dataset comprises 2,594"}, {"title": "4.2 Training and Evaluation Metric", "content": "We conducted experiments using the PyTorch framework, applying the Adam optimization strategy. The model underwent 300 epochs of training, with an initial learning rate of 1 \u00d7 10-3, and the learning rate reduced by half if the Dice score did not improve after 10 consecutive epochs. For training, we used a composite loss function comprising Dice loss and Tversky loss. We set the hyperparameters for the Tversky loss as $\\gamma_1 = 0.7$ and $\\gamma_2 = 0.3$. The loss function formula is as follows:\n$L_{Dice}(g,p) = 1 - \\frac{2 \\sum_{i=1}^{n} g_i p_i}{\\sum_{i=1}^{n} (g_i + p_i)}$\n$L_{Tversky}(g,p) = 1 - \\frac{2 \\sum_{i=1}^{n} g_i p_i}{\\sum_{i=1}^{n} (g_i p_i) + \\gamma_1 \\sum_{i=1}^{n} (g_i (1 - p_i)) + \\gamma_2 \\sum_{i=1}^{n} ((1 - g_i) p_i)}$\n$L(g,p) = 0.5L_{Dice}(g,p) + 0.5L_{Tversky}(g,p)$\nwhere $g_i \\in \\{0,1\\}$ denotes the ground truth label, $p_i \\in (0,1)$ refers to the pre-dicted mask value for each pixel $i \\in \\{1, 2, ..., N\\}$, and N indicates the overall pixel count in the output segmentation mask.\nTo evaluate the model's performance, we utilize the two main metrics used in semantic segmentation: the Dice Similarity Coefficient (DSC) and Intersection over Union (IoU). These metrics help determine the similarity overlap between the predicted mask and the ground truth label, clearly demonstration of the effectiveness of the models."}, {"title": "4.3 Results and Comparison", "content": "To assess the proposed model's effectiveness, we compare it with previously proposed methods, encompassing U-Net [1], Attention U-Net [3], UNeXt [2], DCSAU-Net [19], and U-Lite [13]. These models were trained under conditions identical as the proposed model, and all implementations were sourced from the authors' open-source code repositories. The comparison results be-tween MambaU-Lite and other models are conducted on the ISIC 2018 and PH2 datasets. Some visual segmentation results are illustrated in Fig.3. As shown in this figure, the proposed MambaU-Lite model produces outputs more closely aligned with the original ground truth masks than other models, further affirm-ing the accuracy and reliability of the segmentation model."}, {"title": "5 Conclusion", "content": "In this study, we introduced the lightweight MambaU-Lite model for the skin lesion segmentation, designed to minimize the number of parameters, computa-tion cost, and memory usage. We proposed the P-Mamba block, integrated into an innovative architecture that combines the strengths of Mamba and CNNs to effectively capture both high-level and fine-grained features. While our model has shown promising results on skin lesion datasets, future work will aim to further optimize and generalize the model for a range of medical imaging tasks, enhancing its adaptability and making it well-suited for deployment in medical devices."}]}