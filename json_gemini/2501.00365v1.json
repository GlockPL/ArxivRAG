{"title": "Low-Rank Adaptation for Foundation Models: A Comprehensive Review", "authors": ["Menglin Yang", "Jialin Chen", "Yifei Zhang", "Jiahong Liu", "Jiasheng Zhang", "Qiyao Ma", "Harshit Verma", "Qianru Zhang", "Min Zhou", "Irwin King", "Rex Ying"], "abstract": "The rapid advancement of foundation models-large-scale neural networks trained on diverse, extensive datasets-has revolutionized artificial intelligence, enabling unprecedented advancements across domains such as natural language processing, computer vision, and scientific discovery. However, the substantial parameter count of these models, often reaching billions or trillions, poses significant challenges in adapting them to specific downstream tasks. Low-Rank Adaptation (LoRA) has emerged as a highly promising approach for mitigating these challenges, offering a parameter-efficient mechanism to fine-tune foundation models with minimal computational overhead. This survey provides the first comprehensive review of LoRA techniques beyond large Language Models to general foundation models, including recent techniques foundations, emerging frontiers and applications of low-rank adaptation across multiple domains. Finally, this survey discusses key challenges and future research directions in theoretical understanding, scalability, and robustness. This survey serves as a valuable resource for researchers and practitioners working with efficient foundation model adaptation.", "sections": [{"title": "1 INTRODUCTION", "content": "Foundation models represent a paradigm shift in artificial intelligence, wherein large-scale neural architectures, pre-trained on extensive and broad datasets, establish generalizable representational frameworks that can be adapted to a wide range of downstream applications [1], [2]. These models span multiple domains, including natural language processing (e.g., GPT-3.5 [3], LLaMA [4]), computer vision (e.g., Swin Transformer [5], MAE [6], SAM [7]), speech processing (e.g., Wav2vec2 [8], Whisper [9]), multi-modal learning (e.g., Stable Diffusion [10], DALL\u00b7E 2 [11]), and scientific applications (e.g., AlphaFold [12], ChemBERTa [13], ESM-2 [14]).\nFoundation models are characterized by their unprecedented scale, with parameter counts reaching billions or even trillions, and exhibit emergent properties - capabilities that arise spontaneously without explicit training [1]. These architectures have become fundamental building blocks of modern AI systems, enabling breakthrough performance across diverse domains [1], [2] While these models exhibit broad capabilities, task-specific optimization through fine-tuning remains essential for enhancing generalization [15], promoting algorithmic fairness [16], enabling customization [17], and aligning with ethical and societal standards [18], [19]. However, their scale introduces significant computational challenges, particularly in the computational resources required for both training and fine-tuning [20].\nAlthough traditional fine-tuning methods involving full parameters updates have demonstrated effectiveness across various tasks [21], [22], their computational demands often render them impractical for foundation models [23], [24].\nParameter-efficient fine-tuning (PEFT) methodologies have emerged as a solution to these computational challenges [17], [24], [25], [26], [27], [28]. These approaches enable model adaptation by minimizing the number of trainable parameters, substantially reducing computational requirements without compromising task performance. Among these approaches, Low-Rank Adaptation (LoRA) [17] and its variants have gained widespread attention due to their simplicity, empirical effectiveness, and"}, {"title": "2 BASICS", "content": "LORA [17] constitutes a substantial advancement in parameter-efficient fine-tuning (PEFT). Although originally developed for LLMs, subsequent research has demonstrated its effectiveness across a diverse of foundation models.\nThe mathematical formulation of LORA centers on constraining the update matrix AW to be low-rank during fine-tuning, as shown in Fig. 2, which is implemented through matrix decomposition:\n$\\Delta W = BA$,\nwhere $B \\in R^{d \\times r}$, $A \\in R^{r \\times k}$, and the rank $r < min(d, k)$. By restricting AW to be low-rank, LoRA minimizes the number of parameters that need to be learned during the fine-tuning process, resulting in significant computational and storage efficiency.\nParameter Initialization Strategies. LoRA employs specific initialization strategies to ensure stable and efficient training. Matrix A is typically initialized with values drawn from a random Gaussian distribution, while matrix B is initialized with zeros, which ensures that at the start of training, $\\Delta W = BA$ is effectively a zero matrix."}, {"title": "Fine-tuning Process.", "content": "In LoRA, the fine-tuning process follows these key principles:\n\u2022\tThe original pretrained weights $W_0$ are kept frozen and do not receive gradient updates during training.\n\u2022\tThe low-rank matrices A and B contain the only trainable parameters, capturing task-specific adjustments.\n\u2022\tBoth $W_0$ and $\\Delta W$ are applied to the input vector x separately and their outputs are combined.\n\u2022\tThe output $\\Delta Wx$ is scaled by $\\alpha/r$.\n\u2022\tThe resulting output vectors are summed element-wise:\n$f(x) = W_0x + \\Delta Wx = W_0x + \\frac{\\alpha}{r}BAx$,\nwhere $\\alpha/r$ is a scaling factor controlling the magnitude of the low-rank update. When optimizing using Adam [33], tuning the scaling factor \u03b1 becomes roughly analogous to adjusting the learning rate [17], provided that the initialization is scaled appropriately. In practice, the value of \u03b1 can be set based on the rank r, eliminating the need for extensive hyperparameter tuning.\nAdvantages of LoRA over full fine-tuning. LoRA offers several key advantages over full fine-tuning when applied to large foundation models:\n(1) Parameter Efficiency. LoRA introduces a minimal set of trainable parameters through low-rank decomposition, typically reducing the number of task-specific parameters by several orders of magnitude. This approach is particularly advantageous in resource-constrained environments and multi-task scenarios where multiple adaptations of a base model are required.\n(2) Enhanced Training Efficiency. Unlike conventional full fine-tuning, which updates all model parameters, LoRA optimizes only the low-rank adaptation matrices. This approach substantially reduces computational costs and memory requirements, especially for models with billions of parameters. The reduced parameter space typically leads to faster convergence during training.\n(3) None-latency Inference. LoRA does not introduce additional inference latency since the update matrix $\\Delta W$ can be explicitly incorporated into the original frozen weights W. This integration ensures that the adapted model maintains efficiency during deployment and inference."}, {"title": "3 FOUNDATIONS", "content": "In this section, we examine the fundamental technical aspects of LORA across four critical dimensions: parameter efficiency enhancement, rank adaptation strategies, training process refinements, and theoretical foundations. These components constitute the technical foundation of LoRA's effectiveness."}, {"title": "3.1 Parameter Efficiency Enhancement", "content": "Despite the parameter efficiency gains achieved through LORA with its project-down A and project-up B matrices, the method still requires a significant number of trainable parameters. For instance, applying LoRA to the LLaMA-2-70B model [4] necessitates updating over 16 million parameters [34], surpassing the total parameter count of some BERT architectures [35]. Current research addresses this challenge through four primary approaches: parameter decomposition, pruning, freezing and sharing, and quantization. Fig. 4 illustrates examples of these techniques."}, {"title": "3.1.1 Parameter Decomposition", "content": "Parameter decomposition methods achieve parameter efficiency by decomposing matrices in more compact forms while maintaining task performance. Beyond reducing trainable parameters, these methods also enable more granular control during fine-tuning. Current methodologies can be categorized into two principal approaches: update matrix decomposition [34], [36], [37], [38], and pre-trained weight decomposition [39].\n(1) Update Matrix Decomposition. In update matrix decomposition approaches, two primary strategies have emerged: singular value decomposition (SVD) based method and tensor train (TT)-based decomposition.\n(i) SVD-based Methods. AdaLoRA [36] parameterizes the updates weights AW in the form of SVD [40]:\n$W = W_0 + \\Delta W = W_0 + PAQ,$\nwhere $P \\in R^{d \\times r}$ and $Q \\in R^{r \\times k}$ represent the left and right singular vectors of \u2206W, respectively, and the diagonal matrix $A \\in R^{r \\times r}$ contains the singular values. AdaLoRA dynamically adjusts the rank of AW based on importance scoring, enabling adaptive parameter efficiency during fine-tuning. Building upon this, BiLoRA [37] extends this framework with bi-level optimization, separating singular vector and value training across different data subsets to mitigate overfitting.\n(ii) TT-based Decomposition. LoRETTA [34] takes a different approach by employing TT decomposition [41], which represents a matrix into a series of low-rank, small, three-dimensional tensors, commonly referred to as cores. Given a matrix $W \\in R^{d \\times k}$, it is first reshaped into a tensor $\\mathbb{W} \\in R^{k_1 \\times \\dots \\times k_d}$, where $\\Pi_{i=1}^d k_i = d \\times k$. The TT representation of $\\mathbb{W}$ can be formulated as:\n$TT(\\mathbb{W}) \\leftarrow \\Pi C_i,$\nwhere $C_i \\in R^{r_{i-1} \\times k_i r_i}$ represents a core tensor, and $[r_0,\\dots,r_d]$ denotes TT rank with $r_0=r_d = 1$. This decomposition reduces the parameter count from $d \\times k$ to $\\Sigma_{i=1}^d r_{i-1}k_ir_i$\nLoRETTA introduced two variants: LoRETTAadp and LoRETTArep. LoRETTAadp employs tensorized adapters, inserting these lightweight modules after each attention and feed-forward sub-layer in the transformer blocks. LORETTArep, on the other hand, reparameterizes the weight matrix with tensor factors, offering an even more compact PEFT approach. It updates the weights using two unbiased tensorized layers, further reducing the number of trainable parameters while maintaining comparable performance.\nTT-LORA [38] applies this concept directly to the low-rank matrices in the original LoRA formulation. Note that TT-LORA operates as a parallel adapter, directly modifying the update matrices in the original LoRA formulation. In contrast, LoRETTAadp functions as a series of adapters inserted into the pre-trained model architecture.\n(2) Pre-trained Weight Decomposition. DoRA [39] decomposes the pre-trained weight $W_0$ into magnitude and directional components by normalization method:\n$W_0 = m \\frac{V}{\\|V\\|_c} = \\|W_0\\|_c \\frac{W_0}{\\| W_0\\|_c}$,\nwhere $m \\in R^{1 \\times k}$ is initialized as the magnitude vector $\\|W_0\\|_c$, $V \\in R^{d \\times k}$ is initialized as $W_0$ and kept frozen, and $\\| \\cdot \\|$ denotes the vector-wise norm of a matrix across each column. During fine-tuning, the weight is adapted as:\nW' = m \\frac{W_0 + BA}{\\|W_0 + BA\\|_c},\nwhere m becomes trainable and $BA$ represents the LoRA update to the directional component. This decomposition enables independent optimization of magnitude and direction during fine-tuning, leading to learning patterns that more closely resemble full fine-tuning.\nBoth approaches offer unique advantages in terms of parameter efficiency and fine-tuning flexibility. The update matrix decomposition methods focus on decomposing the incremental updates applied during fine-tuning, while pre-trained weight decomposition directly modifies the structure of the original model weights, where Table 1 provides a detailed comparison of these methods."}, {"title": "3.1.2 Parameter Pruning", "content": "Parameter pruning techniques focus on assessing the importance of different parameters within the LoRA matrices and removing those deemed less important. These methods can be categorized based on their pruning approaches: importance-based pruning, regularization-based pruning, and output-based pruning.\n(1) Importance-based Pruning. These methods evaluate parameter importance using multiple metrics. SparseAdapter [42] applies traditional network pruning techniques to LoRA parameters, assessing importance through parameter magnitude, gradient information, and sensitivity analysis. RoseLoRA [50] extends this concept by implementing sensitivity-based scoring for row/column pruning, enabling selective knowledge updates while preserving low-rank adaptation benefits.\n(2) Regularization-based Pruning. Regularization-based pruning techniques induce sparsity through optimization constraints. SoRA [43] utilizes a gating mechanism between the down-projection and up-projection matrices of LoRA, utilizing proximal gradient descent with L1 regularization. This approach enables automatic sparsification during training, with zero-valued elements eliminated post-training.\n(3) Output-based Pruning. Output-based methods evaluate LoRA parameters based on their layer-wise impact. LoRA-drop [44] evaluates the importance of LoRA modules by analyzing the distribution of $\\|\\Delta W_i x_i\\|_2$ across different layers. The method retains individual LoRA modules for the most important layers while sharing a single LoRA across other layers deemed less critical. The importance score computation utilizes $\\Sigma_{x \\in D_s}. \\| \\Delta W_i x_i \\|^2$, where Ds represents the sampled dataset."}, {"title": "3.1.3 Parameter Freezing and Sharing", "content": "Parameter freezing and sharing techniques reduce trainable parameters through matrix-wise Freezing and cross-layer parameter sharing.\n(1) Matrix-wise Freezing. Research has revealed asymmetric roles of matrices A and B in adaptation. LoRA-FA [45] demonstrates that freezing a randomly initialized matrix A while only updating B can maintain model performance. Asymmetric LoRA [46] provides theoretical foundations for this approach, showing that A primarily acts as a feature extractor while B serves as a task-specific projector. This leads to an enhanced design using a frozen random orthogonal matrix for A, further reducing parameters while preserving performance.\n(2) Cross-layer Parameter Sharing. Several methods explore parameter sharing across network layers. VeRA [47] shares frozen matrices A and B across layers, training only scaling vectors for adaptation. NOLA [48] extends this concept by representing A and B as trainable linear combinations of shared frozen basis matrices. Tied-LoRA [49] implements layer-wise parameter tying while keeping the shared matrices trainable, offering a flexible framework that includes VeRA as a special case when the shared matrices are frozen.\nCombined with parameter pruning techniques (Section 3.1.2), these methods enable a significant reduction in parameter count while maintaining adaptation effectiveness. Table 2 provides a comprehensive comparison of these approaches."}, {"title": "3.1.4 Parameter Quantization", "content": "Quantization [51], [52], [53] optimizes neural network complexity through lower-precision numerical representations, substantially reducing storage and computational requirements. For a comprehensive quantization background, readers may refer to [54]. In LoRA contexts, quantization approaches are characterized by two primary dimensions: quantization timing and quantization techniques.\n(1) Quantization Timing. Quantization timing refers to when quantization occurs before, during, or after fine-tuning.\nPre-finetuning quantization. Pre-finetuning quantization is that the pretrained weights are quantized prior to any LoRA-based adaptation. For example, QLoRA [55] employs a 4-bit NormalFloat (NF4) quantization method. Similarly, LoftQ [56] improves upon this by addressing discrepancies introduced by quantizing high-precision weights. LoftQ jointly quantizes the pretrained model while optimizing low-rank initializations using an iterative algorithm.\nDuring-finetuning quantization. During-finetuning quantization applies quantization both before and throughout the fine-tuning process. Methods like QA-LoRA [57] leverage group-wise quantization to adjust the precision dynamically during training, ensuring a more balanced interaction between low-rank updates and quantized weights.\nPost-finetuning quantization. Post-finetuning quantization, such as in LQER [58], occurs after the fine-tuning is completed, focusing on quantization primarily for inference. LQER utilizes a low-rank SVD-based decomposition to minimize quantization errors, ensuring that the quantized weights closely match the original high-precision weights."}, {"title": "3.2 Ranking Adaptation", "content": "Rank is a crucial parameter in LoRA, directly impacting the model adaptability and number of trainable parameters. The original LoRA method employs a fixed low rank across all layers, which may not be optimal for different downstream tasks and model architectures. To address limitations, recent works have proposed various approaches to optimize rank allocation in LoRA, which can be broadly categorized into two main aspects: rank refinement and rank augmentation. Figure 5 presents an illustration of these two methods."}, {"title": "3.2.1 Rank Refinement", "content": "Rank refinement methods aim to adaptively select the rank of LoRA modules during the fine-tuning. The key insight is that different layers may require varying degrees of adaptation and thus benefit from different ranks. Rank refinement approaches can be grouped into three main types: adaptive allocation, heuristic strategies, and multi-rank training.\n(1) Adaptive Allocation. Adaptive allocation methods dynamically adjust the ranks of LoRA modules during training based on importance metrics derived from the data or model parameters. AdaLoRA [36] introduces an adaptive mechanism for rank allocation by parameterizing LoRA updates using SVD. It dynamically prunes singular values based on their magnitudes, allowing each layer to have a"}, {"title": "3.2.2 Rank Augmentation", "content": "Rank augmentation methods aim to achieve high-rank model updates through sequences of low-rank modifications, bridging the performance gap between LoRA and full-parameter fine-tuning. These methods can be categorized into two types: matrix merging-based methods and matrix resampling-based methods.\n(1) Matrix merging-based methods. Matrix merging-based methods increase the rank by merging low-rank update matrices. The key idea is that the sum of multiple low-rank matrices can approximate a higher-rank matrix, thereby enhancing the ability to capture complex patterns without incurring substantial computational overhead.\nReLORA [62] introduces an iterative training framework where low-rank LoRA modules are trained and periodically merged into the pre-trained model weights. By resetting the optimizer and initializing new LoRA modules after each merge, ReLoRA effectively increases the overall rank while maintaining memory efficiency.\nCOLA [63] proposes a similar iterative optimization strategy inspired by the Frank-Wolfe algorithm [64]. It iteratively trains LoRA modules and merges them into the model, incrementally building a higher-rank adaptation. Each new LoRA module minimizes the residual error from previous adaptations, enabling COLA to achieve high-rank expressiveness without increasing the computational cost per iteration.\nMELORA [65] introduces a parallelization approach to rank augmentation. The core idea is to train multiple small LoRA modules concurrently and concatenate their outputs to form a higher-rank adaptation. By assembling a mini-ensemble of low-rank adapters, MELORA constructs an equivalent block-diagonal matrix that collectively has a higher rank."}, {"title": "3.3 Training Process Improvements", "content": "While LoRA has demonstrated remarkable success in parameter-efficient fine-tuning, optimizing its training dynamics remains crucial for maximizing adaptation performance. In this section, we discuss recent advancements aimed at improving the training process, especially learning rates, dropout strategies, and scaling factors.\nLearning Rate. In standard LoRA fine-tuning, a uniform learning rate is typically applied to both low-rank matrices A and B. However, Hayou et al. [68] observe that this practice leads to suboptimal performance, especially as the model width increases. The issue lies in the updates to A and B contribute differently to the learning dynamics. To address this limitation, Hayou et al. [68] propose LoRA+, a method that assigns different learning rates to matrices A and B. Their theoretical analysis in the infinite-width limit reveals that for efficient learning, the magnitudes of feature updates from both A and B should be $\\Theta(1)$. This necessitates scaling the learning rates such that $\\eta_B = \\Theta(1)$ and $\\eta_A = \\Theta(n^{-1})$, where n denotes the model width. In practice, LoRA+ introduces a fixed ratio $\\lambda = \\eta_B/\\eta_A > 1$,"}, {"title": "Dropout Strategies.", "content": "Despite the reduced number of trainable parameters in LoRA-based models, overfitting remains a concern, particularly when fine-tuning small or specialized datasets. Traditional dropout techniques may not suffice to mitigate overfitting in this context. Wang et al. [69] highlight this vulnerability and propose a comprehensive framework to address it through dropout along three dimensions: dropping position, structural pattern, and compensation measure. The dropping position specifies where the noise is introduced, such as in the attention logits, weights, or hidden representations. The structural pattern defines the granularity of unit deactivation, encompassing element-wise, column-wise, or span-wise patterns. The compensation measure aims to minimize the discrepancy between training and inference phases by incorporating techniques like normalized rescaling or Kullback-Leibler divergence loss. Building on this framework, the authors present HiddenKey [69], a dropout method that combines column-wise dropout of attention logits with element-wise dropout of hidden representations, supplemented by a KL divergence loss."}, {"title": "Scaling Factor.", "content": "In LoRA, a scaling factor $\\gamma_r = \\alpha/r$ is applied. However, as Kalajdzievski [70] points out, this scaling factor can cause gradient collapse when increasing the adapter rank, resulting in slowed learning and diminished performance for higher-rank adapters. To overcome this limitation, Kalajdzievski [70] proposes rsLoRA, which redefines the scaling factor to be $\\gamma_r = \\alpha/\\sqrt{r}$. This adjustment ensures that the adapters are rank-stabilized, meaning that both the forward and backward pass maintain stable magnitudes relative to the rank, even as it becomes large. Theoretically derived in the infinite-width limit, this scaling factor prevents gradient collapse, enabling stable learning across different adapter ranks.\nBy adaptive learning rates to the distinct roles of LORA matrices, mitigating overfitting through structured dropout, and preventing gradient collapse with rank-stabilized scaling, these methods enhance both the efficiency and effectiveness of LoRA fine-tuning. We next examine the theoretical foundations underlying LoRA's performance."}, {"title": "3.4 Theoretical Foundations", "content": "While the practical advantages of LoRA are evident, understanding its underlying principles from a theoretical perspective is crucial. This section addresses key questions regarding its effectiveness, optimal rank selection, roles of update matrices, and induced behavioral changes in theoretical aspects.\nQ1: Why does LoRA work effectively? LoRA achieves competitive performance with full fine-tuning while updating only a small subset of parameters. This phenomenon can be understood through the Neural Tangent Kernel (NTK) theory\u00b9. Malladi et al. [71] show that LoRA approximately preserves the kernel of the original model during fine-tuning. Specifically, with high probability [71],\n$\\{P_{(i, j) \\in [N]}[|K_{LORA}^{ (SGD)}(i, j) - K^{(SGD)}(i, j)| \\geq \\frac{c^2}{3}] \\leq \\delta$\n$\\{K_{LORA}^{ (SGD)}(i, j) - K^{(SGD)}(i, j)| \\geq \\frac{c^2}{3}] \\leq 8$\nwhere $K_{LORA}^{ (SGD)}$ and $K^{(SGD)}$ are the kernels induced by LoRA and full fine-tuning respectively, N is the number of examples in the dataset, c is an upper bound on the L2 norms of gradients and inputs, \u03b5 is the approximation error, and d is the probability bound given by $4N^2 exp(-(\u03b5^2 - \u03b5^3)r/4)$, where r is the rank used in LoRA.\nAlthough LoRA restricts updates to a low-rank subspace, it effectively targets the gradients most responsible for significant transformations in network behavior based on the Equation (7). By focusing on these critical gradients, LoRA preserves the model's ability to generalize, ensuring that the network remains sensitive to essential input variations while being highly parameter-efficient.\nQ2: How many ranks are required for optimal LoRA performance? The rank in LoRA fine-tuning is crucial for understanding the expressivity of adaptation and maintaining computational efficiency.\nZeng and Lee [72] conducted a comprehensive study on the expressive power of LoRA across different architectures. (i) For fully connected neural networks, that LoRA can adapt any model f to accurately represent a smaller target model $f_\\star$ if the LoRA-rank r satisfies:\nr \u2265 width of f \u00d7 depth of f depth of f\n(ii) For Transformer networks, they demonstrate that any model can be adapted to a target model of the same size with rank=(embedding_size/2) LoRA adapters. These findings provide a theoretical foundation for determining the minimum rank necessary for effective adaptation across different architectures.\nComplementing this work, Jang et al. [73] analyzed LORA training in the NTK regime, yielding several key insights: (i) They proved that full fine-tuning (without LoRA) admits a low-rank solution of rank $r \\leq \\sqrt{N}$, where N is the number of training data points. (ii) Using LoRA with rank r\u2265 \u221aN eliminates spurious local minima, facilitating efficient global minima discovery. This result suggests a lower bound for the LoRA rank to ensure optimization stability. (iii) They provided generalization guarantees for LoRA-adapted models, demonstrating that the generalization error is bounded by O(1/\u221aN). This bound offers reassurance about the performance of LoRA-adapted models on unseen data.\nThese theoretical analyses offer valuable guidance for hyperparameter tuning in LoRA applications.\nQ3: What are the roles of update matrices A and B? Zhu et al. [46] provide a comprehensive analysis of the distinct roles played by matrices A and B in LoRA. Their work reveals an inherent asymmetry in these matrices, which has important implications for fine-tuning efficiency and model generalization.\nThe authors [46] demonstrate that A primarily functions as a feature extractor from the input, while B projects these features towards the desired output. This asymmetry suggests that fine-tuning B alone can be more effective than fine-tuning A. Notably, their analysis shows that a randomly initialized A can perform nearly as well as a fine-tuned one, challenging the conventional practice of updating both"}, {"title": "Q4: What behavioral changes does LoRA induce in the model?", "content": "Koubbi et al. [74] analyzed the dynamics of attention matrices, demonstrating that LoRA-induced low-rank modifications maintain short-term stability in token clustering while facilitating significant long-term divergence in learned representations.\nLORA updates attention matrices (Q, K, V) with low-rank matrices as $(Q, K, V)$, introducing controlled perturbations:\n$Q = \\tilde{Q} + Q_\\alpha A_Q E, K = \\tilde{K} + K_\\alpha A_K E, V = \\tilde{V} + V_\\alpha A_V E$\nToken dynamics under LoRA are described by:\n$X_i(t) = \\sum_{j=1}^{n} P_{ij}(t)Vx_j(t),$\nwhere attention weights $P_{ij}(t)$ are based on the softmax of the Query and Key matrices.\nLORA maintains short-term stability of token clustering, with the Wasserstein distance $W_2(\u00b5_t, v_t)$ between perturbed and unperturbed token distributions remaining bounded:\n$W_2(\\mu_t, v_t)^2 \\leq 2C_1(R_t)^2. e^{2C_t e^{3Kt}}$\nA key result is the identification of a phase transition, where tokens bifurcate into new clusters after a critical time $T^*(\u03b4)$, governed by the eigenvalue gap $\\Delta(\\tilde{V})$ of the Value matrix. This shows how LoRA fine-tunes models without catastrophic forgetting, preserving token structure early in training while allowing controlled divergence.\nThese theoretical foundations of LoRA show its effectiveness, from its competitive performance explained by the NTK theory to its ability to prevent catastrophic forgetting through controlled token dynamics. The insights into optimal rank selection and the asymmetry of update matrices offer practical guidelines for improvements."}, {"title": "4 FRONTIERS", "content": "Building upon the technical foundations discussed above, which establish the core components and mechanisms of LoRA, this section explores frontier developments that extend the capabilities of LoRA in novel directions. These frontier developments leverage and combine its fundamental principles to enable new functionalities, tackle more complex tasks, and address challenges in model adaptation."}, {"title": "4.1 Advanced Architecture", "content": "While the original LoRA method significantly enhanced the efficiency of fine-tuning and demonstrated performance comparable to full fine-tuning, it had limitations in flexibility, generalization, and handling multiple diverse tasks simultaneously. To address these limitations, researchers have developed advanced LoRA architectures to further improve performance, parameter efficiency, and generalization ability."}, {"title": "4.1.1 LoRA Composition", "content": "One major innovation in advanced LoRA architectures is the dynamic composition of multiple LoRA modules to enhance adaptability and generalization across diverse tasks.\nOptimization-based Composition. LoRAHub [75] leverages CMA-ES [76] gradient-free optimization to determine optimal coefficients for combining LoRA modules. Through few-shot learning, it autonomously selects and integrates modules for new tasks without requiring manual expertise or gradient computation. Similarly, LoRA-Flow [77] introduces dynamic fusion weights to adjust the impact of different LoRAs at each generation step, determined by a fusion gate with minimal parameters. This approach outperforms baselines with static task-level fusion weights across various\nRetrieval-based Composition. LoraRetriever [78] implements dynamic retrieval and composition of LoRA modules based on input prompts. It first embeds task-specific LoRAs into a shared space using instruction fine-tuning on a subset of tasks, then retrieves relevant modules using cosine similarity. The framework supports both module fusion and mixture strategies while maintaining efficient batch processing.\nBatch-oriented Composition. FLORA [79] enables each example in a minibatch to utilize unique low-rank adaptation weights through efficient matrix operations. This design significantly improves throughput and reduces latency compared to traditional batched approaches, particularly beneficial when serving diverse user requests in production environments.\nBy enabling models to select and combine multiple LORA modules based on the task or input, these methods overcome the limitations of standard LoRA in handling diverse tasks and improve overall performance."}, {"title": "4.1.2 Generalized Framework", "content": "Another advancement involves extending the LoRA architecture itself to capture both task-specific and general features more effectively.\nDual-branch Framework. Hydra [80] presents a more generalized formulation by integrating both parallel and sequential LoRA branches within the model. The parallel branch learns task-specific features, similar to standard LoRA, while the sequential branch linearly combines pre-trained features. This dual branch enables Hydra to capture both task-specific adaptations and leverage general pre-trained knowledge, offering a comprehensive adaptation mechanism that improves performance across tasks.\nMulti-PEFT United Framework. GLORA [81] further make a generalization by unifying various parameter-efficient fine-tuning methods beyond LoRA. It introduces trainable support tensors to scale and shift weights, features, and biases, effectively subsuming methods like LoRA, adapter tuning, and prompt tuning within a single framework. GLORA employs evolutionary search to determine optimal layer-wise configurations of these tensors, which can take scalar, vector, or low-rank matrix forms. Through structural re-parameterization, GLORA incurs no additional inference cost while providing greater flexibility than previous PEFT methods.\nThese generalized architectures enhance the expressive power of LoRA by incorporating additional mechanisms for capturing diverse features and facilitating more effective fine-tuning across tasks."}, {"title": "4.1.3 Gradient Boosting with LoRA", "content": "Gradient Boosting with LoRA (GBLORA) combines weak learners through iterative LoRA module training to minimize residual errors. After T boosting iterations, the fine-tuned model is expressed as:\n$M^T(x) = M_0 + \\sum_{t=1}^T \\beta_t B_\\omega(t)(x)$,\nwith cumulative model updates:\n$M^{(t)}(x) = M^{(t-1)}(x) + \\eta B_{\\omega(t)}(x).$\nwhere \u03b7 controls the contribution of each LoRA booster $B_{\\omega(t)}(x)$. The weak learner principle enables GBLORA to achieve strong performance with low-rank updates. XG-BLORA [66] established convergence guarantees and expressiveness bounds, demonstrating how increased boosting iterations can compensate for lower ranks. This framework unifies various matrix merging methods like ReLoRA [62], COLA [63], and MeLoRA [65] within the GB paradigm."}, {"title": "4.1.4 Mixture of Experts with LoRA", "content": "Another important branch in the development of advanced LORA architectures is the combination of LoRA with Mixture of Experts (MoE). MoE is a neural network architecture where multiple \"expert\" sub-networks specialize in different input patterns [82]. A gating mechanism routes inputs to the most appropriate experts, allowing the model to handle a wide range of tasks efficiently [83]. Given the input x, the MoE model computes\ny = \u2211Gi(x)Ei(x)\nwhere y is the output, G\u2081 is a gating function, E\u2081 is an expert, and n is the number of experts.\nBy integrating LoRA with MoE, models learn multiple pairs of low-rank matrices (LoRA experts) instead of a single pair, with a router determining the weights or selection of experts based on the inputs. During fine-tuning, the pre-trained LLM weights remain fixed, while the LoRA experts and the router are trained, leveraging the parameter efficiency of LoRA and the specialization capabilities of MoE.\nResearch on LoRA-MoE methods can be broadly categorized into three groups based on their primary objectives: (1) enhancing performance and parameter efficiency, (2) preserving knowledge during fine-tuning, and (3) adapting to multi-task learning. While these categories highlight different focuses, many approaches address multiple objectives simultaneously.\n(1) Efficiency-oriented Design. Methods in this category aim to match full fine-tuning performance with minimal parameter overhead.\nZadouri et al. [84] introduced MoV and MoLoRA, aiming to achieve full fine-tuning performance while updating less than 1% of parameters and improving generalization to unseen tasks. MoV and MoLoRA utilize (IA)\u00b3 vectors and LORA adapters as experts, respectively, employing a soft merging strategy where all experts contribute to the output weighted by router probabilities.\nBuilding on these concepts, Luo et al. [85] proposed MoELORA, treating LoRA modules as experts within an MoE framework. MoELORA incorporates multiple LoRA experts and a gating network, employing top-k routing and"}, {"title": "4.2 LoRA for Continual Learning", "content": "The parameter-efficient nature of LoRA allows for incrementally updating models on new tasks while mitigating catastrophic forgetting [98], [99]. Several key advantages motivate the use of LoRA for Continual Learning (CL): (1) reduced computational costs compared to full fine-tuning, (2) natural isolation of task-specific knowledge, and (3) flexible combination of task-specific adaptations. Existing LoRA-based continual learning methods can be broadly categorized into three approaches: regularization-based method, task arithmetic-based method, and ensemble-based techniques.\nRegularization-based approaches that leverage parameter constraints on LoRA updates as the primary mechanism to prevent catastrophic forgetting, focusing on preserving critical model parameters. O-LoRA [98] addresses catastrophic forgetting by constraining new task updates to be orthogonal to the subspace of previous tasks. It leverages the insight that LoRA parameters effectively capture the gradient subspace of a task. O-LoRA incrementally learns new"}, {"title": "4.3 LORA for Unlearning", "content": "LORA facilitates the targeted removal of specific knowledge from foundation models without necessitating extensive retraining. This section categorizes and examines methodologies employing LoRA for unlearning", "categories": "modular decomposition methods"}]}