{"title": "Exploring Loss Landscapes through the Lens of Spin Glass Theory", "authors": ["Hao Liao", "Wei Zhang", "Zhanyi Huang", "Zexiao Long", "Mingyang Zhou", "Xiaoqun Wu", "Rui Mao", "Chi Ho Yeung"], "abstract": "In the past decade, significant strides in deep learning have led to numerous groundbreaking applications. Despite these advancements, the understanding of the high generalizability of deep learning, especially in such an over-parametrized space, remains limited. Successful applications are often considered as empirical rather than scientific achievements. For instance, deep neural networks' (DNNs) internal representations, decision-making mechanism, absence of overfitting in an over-parametrized space, high generalizability, etc., remain less understood. This paper delves into the loss landscape of DNNs through the lens of spin glass in statistical physics, i.e. a system characterized by a complex energy landscape with numerous metastable states, to better understand how DNNs work. We investigated a single hidden layer Rectified Linear Unit (ReLU) neural network model, and introduced several protocols to examine the analogy between DNNs (trained with datasets including MNIST and CIFAR10) and spin glass. Specifically, we used (1) random walk in the parameter space of DNNs to unravel the structures in their loss landscape; (2) a permutation-interpolation protocol to study the connection between copies of identical regions in the loss landscape due to the permutation symmetry in the hidden layers; (3) hierarchical clustering to reveal the hierarchy among trained solutions of DNNs, reminiscent of the so-called Replica Symmetry Breaking (RSB) phenomenon (i.e. the Parisi solution) in analogy to spin glass; (4) finally, we examine the relationship between the degree of the ruggedness of the loss landscape of the DNN and its generalizability, showing an improvement of flattened minima.", "sections": [{"title": "1 Introduction", "content": "The rapid development of deep learning in the past decade has inspired lots of extraordinary applica-tions and achieved remarkable success in various fields, from machine vision [1], speech recognition [2], natural language processing [3], reinforcement learning [4], to modeling animals and humans in neuroscience [5, 6], psychology [7, 8] and education [9]. Despite the increasing prevalence of applications employing deep learning, comprehension of the underlying mechanisms driving its exceptional performance remains limited. For instance, the internal representations of deep neural networks (DNNs), the mechanisms by which they achieve effective decision-making, the absence"}, {"title": "2 Related Works", "content": "Deep neural networks (DNNs) are extensively utilized across various domains [26, 27, 28], prompting significant efforts to understand their underlying mechanisms. DNNs are characterized by overpa-rameterized systems with more learnable parameters than the number of training samples. Contrary"}, {"title": "2.1 The Loss Landscape of DNNS", "content": "to conventional wisdom in statistical learning theory and non-convex optimization, training DNNs rarely results in overfitting or poor local minima. Instead, they tend to generalize well, effectively inferring unknown data. This phenomenon is largely attributed to the complex loss landscape in the weight parameter space, which is a critical factor in the exceptional performance of DNNs. Therefore, research has focused on understanding the loss landscape as a foundational step towards comprehend-ing deep neural networks (DNNs) [29, 30]. Given the high dimensionality and nonlinearity of the loss landscape, various assumptions are often made during analysis, such as linear activations [31], full-rank weight matrices [32], or shallow network architectures [33].\nRegarding generalization, some researchers posit that flat minima contribute positively to generaliza-tion [34, 35], a notion supported by the prevalence of zero eigenvalues in the Hessian matrix at good minima [36]. However, others contend that measures of flatness lack robustness due to parameter symmetries, and that re-parameterizing weights can enhance generalization even to non-flat minima [37, 38]. Moreover, studies have shown that under certain limited assumptions, the loss landscape contains many degenerate global minima, which the training algorithm is likely to locate [39, 40]. Despite these efforts to understand the generalization of DNNs through the loss landscape, conclusive results have yet to be achieved.\nBased on the static characteristics of the loss landscape, one can also analyze the training dynamics to reveal how deep neural networks (DNNs) operate. Although training DNNs constitutes an NP-hard non-convex optimization problem [41, 42], some studies have shown that the probability of a monotonically decreasing path from the initial weight configuration to the global optimum is high [43]. Others argue that proper initialization of Gradient Descent (GD) leads to implicit regularization [44], and that Stochastic Gradient Descent (SGD) may identify one of many degenerate minima [45]. SGD based on local entropy has been introduced to leverage the conjecture that good minima are characterized by flatness [46]. Additionally, analyzing loss landscapes has been used to enhance the robustness of training algorithms against adversarial examples, thereby reducing the risk of misclassification [20, 21]. Thus, a more fundamental and comprehensive understanding of the loss landscape can improve model performance and mitigate the risks associated with deep learning applications."}, {"title": "2.2 Spin Glass Theory", "content": "The statistical physics community has a long history of researching artificial neural networks (ANNs) [20, 21, 22]. ANNs are often analyzed using methodologies developed in the studies of spin glass [18, 19], as the training of ANNs is also an optimization problem subject to a set of fixed input and outputs, similar to identifying the group state in spin glass with quenched coupling disorders [15, 47]. Early advancements in understanding ANNs through statistical physics include studies on associative memory in Hopfield models [48, 15], perceptron storage capacity and generalization [49, 17], and learning dynamics in perceptrons and two-layer neural networks, such as committee machines [22, 50].\nUnlike conventional studies, which focuses on performance and solving specific instances, the use of physics tools aims to reveal the underlying mechanisms of ANNs, thus unraveling the typical behavior across instances. Recently, the statistical physics community has sparkled a new wave of focus on DNN analysis, although assumptions such as random weights or random data are often employed to facilitate theoretical analyses [23, 24, 25]. These analytical physical insights often inspire practical applications. For instance, Onsager reactions have been incorporated to improve DNN training [51], and robust ensembles describing well-configured density regions have inspired new entropy-based training algorithms [46, 52]. Clearly, statistical physics tools continue to contribute significantly to the fundamental understanding of DNNs and their applications."}, {"title": "3 Methods", "content": ""}, {"title": "3.1 Random Walk and Interpolation on the Loss Landscape", "content": "The loss landscape of DNNs is highly complex and non-convex, which may have multiple local extrema or saddle points. Their presence lead to different trained solutions of DNNs, which are different locally optimal solutions in the loss landscape, under different initialization conditions, thereby adversely affecting the model's generalizability. Various strategies have been proposed to address this issue, including designing simpler and more stable loss functions, employing better initialization methods, enhancing gradient descent algorithms, and using regularization techniques. The choice of method should be tailored to the specific model and data context.\nTo study the characteristics of the loss landscape, one can conduct random walks in the parameter space from the different trained trained obtained by training the DNN with specific dataset; w denotes a vector characterizing the parameters in the DNN, i.e. a point in the parameter space, which is the domain underlying the loss landscape. To characterize the random walk, for each Wtrained, we start with wo = Wtrained and draw randomly a vector \u2206w(t) from a distribution N(Aw) at each time step t. The parameter configuration at time t + 1 is then given by\n$w_{t+1} = w_t + \\Delta w(t)$\n\nThe process follows a Markov Chain Monte Carlo (MCMC) procedure and are detailed in Algorithm Al in Appendix A.3. Through Eq. 1, the parameter configuration randomly move away from the original trained solution wo, and the evaluation of the loss function at each wt at time t reveal the landscape along a random path in the parameter space.\nOther than random walks, another way to probe the loss landscape in the interpolation between configurations in the parameter space. Since Goodfellow et al. introduced the method to interpolate between parameter configurations for studying DNN loss landscape [53], many studies followed. It relies on the linear interpolation between two trained parameter configurations w\u2081 and w2, i.e. two points in the loss landscape, such that a point w(a) lying on the straight line between them is given by\n$\\w(a) = a w_1 + (1 - a) w_2, \\forall a \\in [0, 1]$\nwhere a controls the movement on this straight line. To probe the loss landscape, one may employ different settings for w\u2081 and w2, e.g. two trained solutions. In the subsequent analyses, we will introduce a protocol to create w2 based on w1 by swapping nodes in the hidden layers of the DNNs."}, {"title": "3.2 Analogy with Spin Glass", "content": "As a disordered system exhibiting interesting and long-lasting metastability, spin glass has its impor-tant and unique research value in statistical physics. Many analytical techniques were established to specifically analyze spin glass. These techniques provide new tools and perspectives for under-standing information processing problems, including neural networks, error correcting codes, image restoration, and optimization problems. There are many variant of spin glass, one representative model is the Sherrington-Kirkpatrick (SK) model, which has a fully-connected structure, i.e. interac-tion exists between any pair of spins i and j, with a coupling strength Jij satisfying the Gaussian distribution. The Hamiltonian of the SK model of spin glass, analogous to the loss function in DNNs, is given by\n$H \\alpha \\sum_{(ij)} J_{ij} s_i s_j,$\nwhere (ij) denotes all pairs i and j, and si = \u00b11 corresponds to spin i.\nThe first analytical solution to the SK model results in a regime with a negative entropy when temperature is close to zero, contradicting the physical expectation that a system with Ising spins (i.e. either +1 or -1) should always have a positive entropy. The concept of Replica Symmetry Breaking (RSB) introduced by Parisi [54] offers a new perspective on the analytical solution to the SK model, which is effectively a way to characterize the structure of the energy landscape with numerous local minima."}, {"title": "3.3 Permutation Symmetry and the Distance among Parameter Configurations", "content": "Many feedforward neural networks possess the property that interchanging two units in the hidden layers and their associated link weight does not alter the network's output from specific input. Such networks are termed permutation neural networks, and this property implies a symmetrical parameter space, i.e. copies in identical regions in the loss landscape, facilitating the search for suitable weights for specific applications.\nOther than the permutated solutions, multiple close-to-optimal solutions should naturally exist similar to the energy landscape of spin glass. To better understand the relationship among these close-to-optimal solutions, we utilize the Hierarchical Clustering Algorithm A3 in Appendix A.3 to group and identify the hierarchy among the solutions. This algorithm uses a \"bottom-up\" clustering strategy and produces a tree, called a Dendrogram Graph, which shows how the data points are combined. There are two ways to compute the distance between two parameter configuration in parameter space. The first one is the Euclidean distance given by\n$Dist(w^1, w^2) := \\sqrt{\\sum_{i=1}^{n} (w_i^1 - w_i^2)^2}$\nThe second way is to compute the cluster distance between the clusters that the two points belong to, namely d(u, v) given by\n$d(u, v) = \\sqrt{\\frac{|v|+|s|}{T} d^2(v, s) + \\frac{|v|+|t|}{T} d^2(v, t) - \\frac{|s|+|t|}{T} d^2(s, t)}$\nwhere u is a new cluster formed by merging clusters s and t, and v is an unused cluster; T = |v| + |s| + |t|.\nThe computational complexity of the Hierarchical Clustering Algorithm A3 is O(n\u00b3), where n is the number of configurations. This algorithm is a deterministic clustering algorithm and is guaranteed to converge to the target cluster number k. It generally converges slower than the k-means algorithm. In addition, since this algorithm is a greedy algorithm, it may fall into a local optimal solution, leading to the problem of over-clustering."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Experimental Setup", "content": "Datasets We employed four datasets of image classification widely used for benchmarking DNNs, including MNIST, fashion-MNIST, CIFAR-10 and CIFAR-100. The details of the datasets can be found in table A1 of Appendix A.1.\nNetwork Architecture The Deep Neural Networks (DNNs) in this study consist of three layers. Parameters for the input and hidden layers are detailed in Table A2 of Appendix A.2. The DNNs are categorized into two sets based on the input layer configurations. Specifically, (1) the input layer dimensions for FC1 and FC2 are 784 and 3,072, respectively. (2) The hidden layers consist of 64, 128, 256, 512, and 1024 nodes. For example, 'FC1 \u2013 64' refers to a network with an input layer of 784 and a hidden layer of 64 nodes. Conversely, 'FC2 \u2013 128\u2032 denotes a network with an input layer of 3, 072 and a hidden layer of 128 nodes. The ReLU function activates the hidden layers, and the output layer, containing 10 neurons, is activated by the Softmax function."}, {"title": "4.2 Results of Random Walks on the Parameter Space", "content": "To obtain Wtrained, we utilized the Adam optimization algorithm to train the neural networks listed in Table A2. Subsequently, we employed Algorithm A1 of Appendix A.3 to generate new weight configurations. This involves conducting a new round of training and subsequently documenting the resulting training loss and accuracy.\nFigure 2 shows the dependence of training loss and the training accuracy on twalk, i.e. the time step of random walk. Similar trends were observed in the test set. As expected, the loss function increases more rapidly in cases with large o, but the results also revealed some interesting behaviors during the random walks, e.g. the walks traverse an initial flat region at smaller twalk values, potentially indicative of the minima's width, and reach a plateau of high loss when \u03c3 = 0.05 and \u03c3 = 0.07.These preliminary results hint at the potential to extend this approach to more complex DNNs with multiple hidden layers."}, {"title": "4.3 Results on permutation-interpolation", "content": "According to Theorem A.1 in Appendix A.4, swapping two neurons and their associated links in the hidden layer multiple times produces a transformed parameter configuration w, loss, accuracy), but with the sameloss and accuracy. We first swap neurons in the hidden layers by R times, which we call the permutation stage. Subsequently, the loss landscape was examined through interpolating between the original configurations and the transformed configurations, which we call the interpolation stage\nFigure 3 shows the dependence of test lost on DNN architecture and the number of neuron swap. With the same number of swap, an increase in the depth of the hidden layer results in a smaller loss barrier at the intermediate peak. Conversely, with the same number of hidden layer nodes, more swaps results in a higher loss barrier. These results indicate that more swaps reduce the similarity between the original and the new parameter configurations, enlarging the distance between them in the loss landscape and increasing the loss barrier. In contrast, more hidden layers tend to smooth the loss landscape, leading to a flatter surface and a lower loss barrier."}, {"title": "4.4 Results on loss landscape optimization", "content": "Here, we analyze a set of different solutions trained for the same DNN by the same dataset, but from different initial conditions. Specifically, we trained a single hidden layer ReLU model on the MNIST dataset using the SGD optimization algorithm, with weights initially randomized from a glorot uniform distribution, repeated for 100 or 200 times. These solutions were subsequently reordered using Algorithm A3 in Appendix A.3 to mitigate the effect of permutation symmetry in the hidden layers. We then compute the Euclidean distance between every pair of the trained solutions as in Eq.(4), and show its distribution in Fig. 4. A Gaussian distribution was best-fitted to the distribution"}, {"title": "4.5 Extreme value flatness analysis in loss landscape", "content": "The stochastic gradient descent (SGD) method and its variants are the algorithm of choice for many deep learning tasks. These methods operate in small-batch mode, where a portion of the training data is sampled to calculate an approximation of the gradient. It has been observed in practice that the quality of the model, as measured by its generalization ability, decreases when using larger batches.\nSB (Small mini-Batch) denotes small batch training, which is fixed at 256, and LB (Large mini-batch) denotes large batch training, which is set to 10% of the samples in the training data set. Adam optimization algorithm is used to train the neural network model, and two sets of solutions OSB and OLB are obtained respectively. All experiments were performed 5 times from different starting points, and the mean and standard deviation were recorded. Then use the linear interpolation algorithm for any pair of OSB and OLB:\n$\\theta = (1 - a) * \\theta_{SB} + a* \\theta_{LB}, a \\in [-1,2]$\nFigure 6 shows the loss function and accuracy of the training and test data sets in the loss landscape along the path containing two solutions. The solid line represents the training data set, the dotted line represents the test data set. a = 0 represents the minimum value of the small batch, and a = 1 represents a large batch minimum. It can be seen that the line is significantly flatter at the minimum value of the small batch than at the minimum value of the large batch, and as the number of hidden layer nodes increases, the minimum value of the small batch becomes flatter. Besides, the test accuracy at a = 0 is higher than at a = 1. Therefore, it can be concluded that flat minima have better generalization performance.\nPlease note that additional experiments are detailed in Appendix A.5, where further analyses and supplementary results are provided to support the findings discussed above."}, {"title": "5 Conclusion", "content": "Although deep learning has many successful application cases in various fields, its theoretical foundation remains elusive. To this end, this paper delves into the loss landscape of single hidden layer ReLU neural network, drawing analogy with energy landscape in spin glass in statistical physics. Our experiments demonstrated that the existence of multiple extrema in the loss landscape, and empirically established that flatter local minima correlate with improved generalizability. However, this study's focus on the most basic model of DNNs, which limits the generalizability of our findings to more complex or different network architectures. Furthermore, the empirical nature of our results and the complex methodologies employed, such as the MCMC random walk and permutation-interpolation protocols, may pose challenges in reproducibility and practical applications. These techniques, while innovative, require a deep understanding, which could hinder their adoption in broader research context. In conclusion, by shedding light on the underpinnings of the loss landscape in deep neural networks, this work contributes to a deeper understanding of DNNs through a new perspective and a new set of tools."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Information of Datasets", "content": "\u2022 The MNIST data set is an image classification data set commonly used for machine learning. It is a dataset of handwritten digit images and has more than 60,000 training images and more than 10,000 test images, each of which is a 28 \u00d7 28 pixel grayscale image representing a number between 0 and 9. These numbers are dimensionally normalized and centered within a fixed-size image with pixel values between 0 and 255, representing the brightness of the image.\n\u2022 The Fashion-MNIST is an image classification dataset that contains 60, 000 training images and 10, 000 test images, each image is a 28 \u00d7 28 grayscale image. Its content includes 10 categories of clothing, namely: T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, Ankle boot.\n\u2022 The CIFAR-10 dataset is a commonly used dataset for image classification and contains 50, 000 training images and 10,000 test images. Each image is a color image with the size of 32 x 32 x 3. There are 10 categories of images, namely: cars, birds, cats, frogs, deer, dogs, airplanes, trucks, boats and horses.\n\u2022 CIFAR-100 is a new and more comprehensive image classification dataset, which contains 50, 000 training images and 10,000 test images. Each image is a color image with the size of 32 x 32 x 3. Compared with the CIFAR-10 dataset, the CIFAR-100 dataset contains more than 100 different categories. The 100 categories in the CIFAR-100 dataset are divided into 20 supercategories and 100 subcategories, with each supercategory contains 5 subcategories."}, {"title": "A.2 Neural Network parameters", "content": "The Deep Neural Networks (DNNs) utilized in this study are structured with three layers. The parameters for both the input and hidden layers are detailed in Table A2. The DNNs are categorized into two sets based on different configurations of the input layers. Specifically, the input layer dimensions for FC\u2081 and FC2 are 784 and 3,072, respectively. The hidden layers consist of 64, 128, 256, 512, and 1024 nodes. The ReLU function is employed to activate the hidden layers, while the output layer, containing 10 neurons, is activated by the Softmax function."}, {"title": "A.3 Pseudocode", "content": ""}, {"title": "A.4 Theorems and Proofs", "content": "The set of all weights in a permutation network is denoted as the weight vector w, with w \u2208 R, where q is the total number of real-valued adjustable parameters. The set of all permutations of w forms the non-Abelian group Sq (the q-th symmetry group), referred to as S. This group contains q! elements. For a permutation neural network, T is defined as the set of all non-singular linear weight transformations (e.g., reversible linear mappings from R\u00ba to R\u00ba that preserve the network's input/output transformation function. By definition, T must include the weights of pairs of hidden layer units in the permutation network but can also encompass additional transformations. For instance, in a backpropagation network, reversing the signs of all weights of a hidden layer unit, reversing the signs of the input weights associated with that unit's output in the next layer, and adjusting their bias weights by the initial values of these input weights is a transformation that leaves the network's input/output functions unchanged. This transformation is not a permutation but is included in T. Finally, T for any permutation neural network is a subset of the general linear group GL(q, R), which comprises all reversible linear transformations from R\u00ba to R\u00ba. Based on these definitions, Theorem A.1 can be established:\nTheorem A.1. If T is a proper subgroup of GL(q, R), then the number of elements in T satisfies #T > \u03a01 Mi!, where M\u2081 represents the number of neural units in the i-th hidden layer, and K represents the number of hidden layers.\nProof. To prove that T is a subgroup, it is necessary to prove that T satisfies closure property under identity elements, inverse elements and multiplication operations. Since transformation of"}, {"title": "A.5 Additional experimental results", "content": "The training and testing curve represented by FC1 512 is shown in Figure A1.\nThe result shown in Figure A2 is that based on Algorithm A1, only walking paths with loss L[w(t)] less than L[\u0175trained]. In each time step tbatch-walk, we conduct 40 individual random walk trials staring from the previous w(tbatch-walk-1), and compute the fraction that among the 40 newly drawn w, their loss is less than the original trained loss L[\u0175trained]. As we can see, this fraction decreases as tbatch-walk increases, which may imply that the random walk trajectories are approaching the"}]}