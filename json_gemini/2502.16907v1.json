{"title": "MambaFlow: A Novel and Flow-guided State Space Model for Scene Flow Estimation", "authors": ["Jiehao Luo", "Jintao Cheng", "Xiaoyu Tang", "Qingwen Zhang", "Bohuan Xue", "Rui Fan"], "abstract": "Scene flow estimation aims to predict 3D motion from consecutive point cloud frames, which is of great interest in autonomous driving field. Existing methods face challenges such as insufficient spatio-temporal modeling and inherent loss of fine-grained feature during voxelization. However, the success of Mamba, a representative state space model (SSM) that enables global modeling with linear complexity, provides a promising solution. In this paper, we propose MambaFlow, a novel scene flow estimation network with a mamba-based decoder. It enables deep interaction and coupling of spatio-temporal features using a well-designed backbone. Innovatively, we steer the global attention modeling of voxel-based features with point offset information using an efficient Mamba-based decoder, learning voxel-to-point patterns that are used to de-voxelize shared voxel representations into point-wise features. To further enhance the model's generalization capabilities across diverse scenarios, we propose a novel scene-adaptive loss function that automatically adapts to different motion patterns. Extensive experiments on the Argoverse 2 benchmark demonstrate that MambaFlow achieves state-of-the-art performance with real-time inference speed among existing works, enabling accurate flow estimation in real-world urban scenarios.", "sections": [{"title": "I. INTRODUCTION", "content": "SCENE flow is a vector field that describes the motion of 3D points between two frames, representing the 3D counterpart of optical flow [1] in 2D images. The scene flow estimation task takes consecutive point cloud frames from sensors as input and output motion vectors for each point, providing autonomous perception systems with essential motion information for accurate environmental perception and decision-making [2], [3]. Moreover, real-time scene flow estimation is also well regarded as a promising solution for enhancing the performance of downstream tasks such as 3D object detection [4], depth estimation [5] and moving object segmentation [6], thus largely enhancing the overall intelligence of autonomous systems.\nExisting scene flow estimation methods have made progress but still face several challenges. Current approaches [7]\u2013[13] either directly concatenate consecutive point cloud frames for implicit temporal features extraction, leading to insufficient utilization of temporal information, or pursue efficiency by projecting point clouds into 2D pseudo-images, resulting in spatial feature loss. Jund et al. [11] proposed an end-to-end learning framework that concatenates consecutive point cloud frames and employs a flow embedding layer for feature extraction. Zhang et al. [12] drew inspiration from optical flow approaches and introduced a Gated Recurrent Unit (GRU) decoder for point cloud feature refinement. While these methods partially alleviate the feature loss, their reliance on only two consecutive frames for temporal information consistently limits model performance.\nTemporal information plays a dominant role in scene flow estimation. This is effectively demonstrated by recent work Flow4D [14], which leverages more than two consecutive frames to achieve significantly improved performance on the Argoverse 2 benchmark. To reduce computational complexity, Flow4D decomposes 4D convolution into parallel temporal and spatial branches with 1D and 3D convolutions respectively. While this decomposition achieves computational efficiency, it sacrifices the holistic modeling of spatio-temporal correlations. The assumption that temporal and spatial features can be processed independently ignores their inherent coupling in motion dynamics. Moreover, since temporal and spatial information characterizes fundamentally different aspects of motion dynamics and geometric structures, such simplified processing prevents effective feature interaction and temporal guidance.\nTo address the above challenges, we propose a novel multi-branch architecture for feature extraction that achieves deep coupling of spatio-temporal features. Specifically, our method incorporates a learnable fusion mechanism that dynamically weights temporal and spatial features while prioritizing temporal cues. This enables effective modeling of complex motion patterns across diverse scenarios, significantly enhancing the learning of motion dynamics from point clouds compared to existing methods.\nAlthough our spatio-temporal coupling approach enables comprehensive feature extraction, the adopted voxel-based encoding introduces inherent feature loss, as points within the same voxel grid become indistinguishable. Recent works [15], [16] demonstrate that transformer architectures can effectively learn point-wise correlations through global attention modeling. However, their quadratic inference complexity challenges real-time requirements. Studies on SSM [17]\u2013[19] suggest a promising alternative with linear-complexity global attention modeling. Drawing inspiration from recent advances in SSM for point cloud processing [20]-[22], we propose a Mamba-based decoder that steers global attention modeling of voxel-based features through point offset information. It achieves refined devoxelization by adaptively learning voxel-to-point patterns, which enables effective transformation from shared voxel representations to point-wise features, representing our core contribution to scene flow estimation.\nAs observed in [11], [12], scene flow estimation methods often exhibit limited generalization in autonomous driving scenarios due to the severe imbalance between dynamic and static point distributions, where over 90% of the point cloud exhibits zero displacement. Drawing insights from self-supervised approaches [11], [12], we propose a scene-adaptive loss function that leverages motion statistics derived from point displacement distributions.\nThrough extensive experiments, we demonstrate that our proposed feature extraction architecture integrated with the Mamba-based decoder, guided by scene-adaptive loss supervision, significantly enhances the model's dynamics-awareness and scene adaptability, achieving state-of-the-art performance on the Argoverse 2 benchmark among published works while maintaining real-time inference speed of 17.30 FPS. The main contributions of this paper are:\n1) We propose MambaFlow, a novel SSM-based architecture for scene flow estimation that addresses voxelization feature loss through voxel-to-point pattern learning. To our knowledge, this represents the first application of state space modeling to scene flow estimation.\n2) We propose a multi-branch backbone for deep coupling of spatio-temporal features, with an adaptive fusion strategy that prioritizes temporal information for complex motion patterns.\n3) We propose a scene-adaptive loss function that leverages point displacement distributions to automatically distinguish between static and dynamic points without empirical thresholds."}, {"title": "II. RELATED WORK", "content": "Scene flow estimation in autonomous driving, while sharing similarities with object registration methods like DifFlow3D [23] and [24] that achieve millimeter-level precision on small-scale datasets like ShapeNet [25] and FlyingThings3D [26], faces unique challenges when processing scenes from datasets like Argoverse 2 [27] and Waymo [28] containing 80k-177k points per frame, where substantial downsampling compromises practicality [11]. For such large-scale data, voxel-based encoding with efficient feature refinement emerges as the preferred approach to balance efficiency and feature preservation.\nMany existing methods adopt flow embedding-based approaches [9], [11], [12], [29], establishing soft correspondences by concatenating spatial features from consecutive frames. However, this indirect feature correlation with two-frame input has been increasingly recognized as suboptimal for capturing temporal dynamics. Following successful practices in 3D detection [30], scene flow estimation has evolved towards multi-frame approaches. For example, Flow4D [14] processes five frames for more robust estimation. Most recently, EulerFlow [31] reformulates scene flow as a continuous space-time PDE. However, to effectively exploit temporal dynamics while avoiding the computational complexity of continuous streams, we follow Flow4D's framework by adopting five consecutive frames as input with voxel-based encoding.\nRecently, self-supervised methods have gained popularity due to the difficulty of obtaining scene flow ground truth. Se-Flow [10] addresses key challenges through efficient dynamic classification and internal cluster consistency, while ICP-Flow [9] incorporates rigid-motion assumptions via histogram-based initialization and ICP alignment. However, precise motion estimation remains critical for autonomous driving safety, and state-of-the-art self-supervised methods like EulerFlow still show limitations in static background estimation, which could be catastrophic for autonomous driving systems. Although reducing annotation costs is appealing, the investment in labeled datasets is justified and necessary given the safety-critical nature of autonomous driving. We maintain a supervised architecture while incorporating insights from self-supervised approaches and proposed a scene-adaptive loss function, which automatically extracts motion statistics from velocity histograms for better generalization without empirical thresholds."}, {"title": "B. State Space Model", "content": "SSMs [17]\u2013[19] have gained attention as an efficient alternative to attention mechanisms and Transformer architectures, particularly for capturing long-term dependencies through hidden states. The Structured State Space Sequence (S4) [17] efficiently models long-range dependencies through diagonal parameterization, addressing computational bottlenecks in attention-based approaches. Building upon S4, researchers developed enhanced architectures such as S5 [18] and H3 [19]. Notably, Mamba [32] represents a significant breakthrough by introducing a data-dependent selective scan mechanism and integrating hardware-aware parallel scanning algorithms, establishing a novel paradigm distinct from CNNs and Transformers while maintaining linear computational complexity.\nThe linear complexity capability of Mamba has inspired extensive exploration in both vision [33], [34] and point cloud processing [20]\u2013[22] domains. In 3D point cloud domain, recent works have demonstrated significant advances in adapting Mamba for various tasks. Mamba3D [20] introduced local norm pooling for geometric features and a bidirectional SSM design for enhanced both local and global feature representation. PointMamba [21] is one of the pioneers in applying state space models to point cloud analysis by utilizing space-filling curves for point tokenization with a non-hierarchical Mamba encoder, establishing a simple yet effective baseline for 3D vision applications. MambaMos [22] further explored SSM's potential in point cloud sequence modeling by adapting Mamba's selective scan mechanism for motion understanding, demonstrating that SSM's strong contextual modeling capabilities are particularly effective for capturing temporal correlations in moving object segmentation. These successes in achieving linear complexity while maintaining robust feature learning suggest promising potential for achieving fine-grained devoxelization in scene flow estimation.\nBuilding upon these insights, we try to integrate the Mamba architecture into the the scene for estimation network for to maintains real-time performance while avoiding the quadratic complexity of transformer-based approaches."}, {"title": "III. PROBLEM STATEMENT", "content": "Consider two consecutive point cloud frames Pt and Pt+1 acquired at time instants t and t + 1, with vehicle ego-motion transformation matrix Tt,t+1. The scene flow estimation task aims to predict the motion vector Mt,t+1(p) = (\u2206x, y, z)T for each point p in Pt. We employ the End Point Error (EPE) as the evaluation metric, as defined in Eq. (1):\n$EPE(p) = ||M(p) \u2013 Mgt(p)||_2$ (1)\nwhere M(p) and Mgt(p) denote the predicted and ground truth scene flow, respectively.\nThe core objective of scene flow estimation is to minimize the average EPE, which can be expressed as Eq. (2):\n$min \\frac{1}{P_t} \\sum_{p \\in P_t} ||M(p) - M_{gt}(p)||_2$ (2)\nwhere Pt denotes the number of points in Pt. A method must address both dynamic objects in static environments and global scene changes induced by ego-motion. Such challenges necessitate effective integration of local features and global context for accurate 3D motion pattern capture."}, {"title": "IV. METHODOLOGY", "content": "Mamba [32] is a sequence modeling framework based on SSM that introduces a selective scan mechanism to model complex state spaces through time-varying characteristics. Through a time scale parameter \u2206, it makes the state transition matrix A and input projection matrix B input-dependent for selective feature filtering. The continuous system is discretized via Zero-Order Hold, which can be expressed as Eq. (3):\n$\\bar{A} = e^{\\Delta A}$\n$\\bar{B} = (e^{\\Delta A} - I) A^{-1} \\Delta B$ (3)\nAfter discretization, the linear ordinary differential equations representing an SSM can be rewritten as Eq. (4):\n$h_t= \\bar{A}h_{t-1}+ \\bar{B}x_t$\n$Y_t = Ch_t$ (4)\nwhere ht denotes the hidden state, xt and yt denote the input and output sequences respectively, and C is output projection matrix."}, {"title": "B. MambaFlow Pipeline", "content": "1) Overall Architecture: As shown in Figure 2, MambaFlow adopts an end-to-end architecture and generates scene flow estimation through three main stages: Spatio-temporal sampling and encoding, Spatio-temporal Deep Coupling Network, and MambaFlow Decoder.\nIn the Spatio-temporal Sampling and Encoding stage, N consecutive LiDAR scans are first transformed into 3D voxel representations through a voxel feature encoder, with all frames warped to the coordinate system of the time step t + 1. The subsequent temporal fusion stage concatenates these 3D voxel representations along the temporal dimension, generating a 4D spatio-temporal feature tensor.\nNext, the 4D tensor is processed by the Spatio-temporal Coupling Network. This network adopts a U-Net architecture with stacked Spatio-temporal Deep Coupling Block, which progressively learns multi-scale feature representations through deep hierarchical modeling. The network consists of a five-level encoder with stacking depths [2,2,2,2,2] and a four-level decoder with stacking depths [1,1,1,1], coupling multi-scale contextual information through sparse convolution operations. The fused features are residually combined with the input features to enhance feature representation.\nFor the devoxelization stage, the decoder is used to processes the extracted 3D voxel features from time step t + 1. The voxel features are first serialized into sequences following space-filling curves. Through multiple cascaded FlowSSM modules, which incorporate discretized point offset information into state space for the global modeling of voxel-wise features, the decoder progressively refines voxel-wise features to point-wise features. Finally, the refined feature sequence is deserialized and fed into an MLP head to generate point-wise scene flow estimation.\n2) Input Representation and Voxelization: To balance prediction performance and computational efficiency, we follow Flow4D [14] by sampling five consecutive point cloud frames as input.\nGiven consecutive point cloud frames Pt, that can be represented as Eq. (5):\n$P_{\\tau} = \\{p_i \\in R^3\\}_{i=1}^{N_{\\tau}}, \\quad \\tau \\in \\{t - 3, t - 2, t - 1, t, t + 1\\}$ (5)\nwhere pi = (xi, Yi, zi)T denotes the point coordinates and N\u03c4 denotes the number of points at time step \u03c4.\nFollowing previous studies, we first warp all point clouds to the perspective of time step t + 1 using the known pose transformation matrices. For each transformed point cloud, we first employ an MLP to extract point-wise features F point, followed by a voxel feature encoder that aggregates these features into voxel-wise features F voxel. To form spatio-temporal representations, we extend F voxel with a temporal dimension and concatenate them along the time axis, yielding a 4D voxel tensor F4D that encodes both spatial and temporal information.\n3) Serialization: Since our SSM-based decoder processes sequential data, we adopt Z-order space-filling curves as our serialization strategy inspired by [15]. We define a mapping function \u03a8 that projects 3D point cloud Po to sequence Po while preserving spatial locality. The serialization and deserialization process can be expressed as Eq. (6):\n$P' = \\Psi(P_o)$\n$P_o = \\Psi^{-1}(P')$ (6)"}, {"title": "C. Spatio-temporal Deep Coupling Block", "content": "1) Feature Extraction Stage: As shown in Figure 3, the proposed spatio-temporal deep coupling block (STDCB) consists of multi sparse convolutions branches, incorporating a cascaded Soft Feature Selection Mechanism to achieve adaptive feature interaction. Specifically, STDCB first sparsifies F4D to obtain a sparse 4D tensor Fsparse, which is then processed by three parallel branches. Drawing inspiration from the design of Flow4D [14], we adopt a convolution with kernel size (3 \u00d7 3 \u00d7 3 \u00d7 1) to extract geometric structures, and two convolution with kernel size (1 \u00d7 1 \u00d7 1 \u00d7 3) to capture local motion patterns and cross-timestep dependencies through different receptive fields. The process of initially feature extraction can be formulated as Eq. (7):\n$F_{spatial} = \\Phi_{3 \\times 3 \\times 3 \\times 1}(F_{sparse})$\n$F_{temporal} = \\Phi_{1 \\times 1 \\times 1 \\times 3}(F_{sparse})$\n$F_{ctemporal} = \\Phi_{1 \\times 1 \\times 1 \\times 3, dilation=1}(F_{sparse})$ (7)\nwhere Fspatial, Ftemporal and Fctemporal denote the features extracted from spatial, local temporal, and cross-timestep temporal branches respectively. \u03a6 denotes the convolution kernel. For the cross-timestep, we employ a dilated convolution operation where a dilation factor of 1 is applied to expand the receptive field.\nThe decomposed design significantly reduces computational complexity compared to full 4D convolutions while preserving the ability to capture both spatial and temporal information.\n2) Soft Feature Selection Mechanism: Given two feature tensors as input, Soft Feature Selection Mechanism designates one as the main branch Fmain and the other as the auxiliary branch Faux according to their roles in the specific task. Their relative importance is then computed through a Gated Unit. The attention weights can be formulated as Eq. (8):\n$\\alpha = \\sigma(LR(BN(\\Phi_p(Concat(F_{main}, F_{aux})))))$ (8)\nwhere \u03c3 denotes the sigmoid activation, LR denotes the LeakyReLU, BN denotes the batch normalization operation, \u03a6p denotes the point-wise convolution, and Concat denotes feature concatenation along the channel dimension.\nThe attention weights are then used to adaptively combine the two branches, where \u03b1 is multiplied with the main branch and (1 - \u03b1) with the auxiliary branch. This weighting scheme allows flexible control of feature importance: when \u03b1 is large, the main branch features are emphasized, while smaller \u03b1 values give more weight to the complementary features from the auxiliary branch.\n3) Temporal Gated Block: After obtaining features from three parallel branches, we first fuse temporal features by employing Soft Feature Selection Mechanism (SFSM) with consecutive temporal features as the main branch and cross-timestep temporal features as the auxiliary branch to complement cross-step temporal information, which can be formulated as Eq. (9):\n$F'_{temporal} = SFSM(F_{temporal}, F_{ctemporal})$ (9)\nThe fused temporal features then guide spatial feature learning through the Temporal Gated Block, where the attention weights \u03b2 are computed as Eq. (10):\n$\\beta = \\sigma(\\Phi_p(ReLU(\\Phi_p(F'_{temporal}))))$\n(10)\nThe spatial features are modulated by temporal attention through gating and residual connection, which can be formulated as Eq. (11):\n$F_{spatial} = F_{spatial} \\cdot (1 + \\beta)$ (11)\nwhere \u2299 denotes element-wise multiplication.\nFinally, F spatial is adaptively combined with F temporal using SFSM, followed by a residual connection with F sparse and a point-wise convolution to fuse the features. The final output can be formulated as Eq. (12):\n$F^{sparse} = \\Phi_{p}(Concat(SFSM(F'_{temporal}, F'_{spatial}); F_{sparse}))$ (12)"}, {"title": "D. MambaFlow Decoder", "content": "The overall structure of the MambaFlow Decoder is shown in Figure 4. During the decoding stage, we first extract the 3D voxel features F voxel at time step t from F 4D. For each point in a voxel grid, we assign the corresponding voxel features as their initial point-wise representations, denoted as F coarse, where all points in the same voxel share identical feature values.\nMeanwhile, we preserve both point-wise features F point and point offset information P offset during encoding stage. Considering that P offset only have 3 channels, which may cause information imbalance when directly used as decoder input, we extend their feature dimension to a matching dimension of F voxel through a point offset encoder, yielding point-wise offset features F offset. The concatenation of F voxel and F point along the channel dimension forms the initial input features Fcoarse to the first decoding layer, with F point serving as guiding information for feature refinement.\nThe key component of our decoder is FlowSSM, a variant of SSM. As shown in Algorithm 1, it incorporates discretized point offset information into state space for global modeling of voxel-wise features. To enable sequence modeling, we first serialize both coarse features and offset features using Z-order space-filling curves, which can be formulated as Eq. (13):\n$F'_{coarse} = \\Psi(F^{tcoarse})$\n$F'_{offset} = \\Psi(P_{offset})$ (13)\nThe decoder then progressively refines these features through multiple cascaded FlowSSM layers, where each layer conditions the state space matrices on F'offset for adaptive feature refinement. After N iterations, the refined feature sequence is first deserialized to obtain refined point-wise features F refined, then concatenated with Foffset. Finally, the scene flow estimation Mt,t+1(p) is generated through deserialization and an MLP head."}, {"title": "E. Scene-adaptive Loss", "content": "In scene flow estimation, supervised methods relying on manually labeled data often show limited generalization ability to real-world scenes. Inspired by the the loss function design of FastFlow3D [11], DeFlow [12] divides points into three subsets {P1, P2, P3} using velocity thresholds of 0.4 m/s and 1.0 m/s, and computes the total loss by averaging the endpoint errors within each subset, which can be formulated as Eq. (18):\n$L = \\frac{1}{3} \\sum_{i=1}^{3} \\frac{1}{\\left| P_i \\right|} \\sum_{p \\in P_i} || \\Delta M(p) - \\Delta M_{gt}(p) ||_2$ (14)\nHowever, empirically determined velocity thresholds can misclassify slow-moving objects as static points. Considering the severe imbalance where over 90% of points exhibit zero displacement in autonomous driving scenarios, we propose a scene-adaptive loss function that automatically determines the displacement threshold for effective model training.\nSpecifically, we first divide the displacement range [rmin, rmax] of a point cloud frame into K equal-width bins, where r denotes the point displacement and rmin is set to 0 considering that zero-displacement points are in the majority of all points. We define wj as the proportion of points whose displacement magnitudes fall into the j-th bin bj, which can be formulated as Eq. (15):\n$w_j = \\frac{N_j}{n}$ (15)\nwhere nj denotes the number of points in j-th bin and n is the total number of points.\nWe then adaptively select the first bin whose scale is below 1/K as the displacement threshold \u03b1, which can be expressed as Eq. (16):\n$\\alpha = min\\{ j : w_j < \\frac{1}{K} \\}$ (16)\nBy setting K = 100, we can effectively capture the inherent displacement distribution of each scene and focus the loss on the truly dynamic portion of the point cloud. With the threshold \u03b1 indicating the bin index, we divide the point p into two categories {PStatic, PDynamic} based on their displacement r(p) relative to the lower bound of the \u03b1-th bin r\u03b1, as defined in Eq. (17):\n$P_{Static} = \\{ p \\in P | r(p) \\leq r_{\\alpha} \\}$\n$P_{Dynamic} = \\{ p \\in P | r(p) > r_{\\alpha} \\}$ (17)\nThe total loss function is a weighted average of these two types of losses, which can be formulated as Eq. (18):\n$L_{total} = \\frac{1}{|P_{Static}|} \\sum_{p \\in P_{Static}} ||\\Delta M(p) - \\Delta M_{gt}(p)||_2 + \\frac{1}{|P_{Dynamic}|} \\sum_{p \\in P_{Dynamic}} ||\\Delta M(p) - \\Delta M_{gt}(P)||_2$ (18)\nwhere |Pstatic| and |PDynamic| denote the number of the static and dynamic point sets respectively, ||\u00b7||2 denotes the L2 norm."}, {"title": "A. Experiment Setups", "content": "1) Datasets: The proposed method is evaluated on the Argoverse 2 dataset [27], a large-scale autonomous driving benchmark containing 1,000 diverse scenarios. Each sequence in this dataset spans 15 seconds and encompasses 30 distinct object classes in complex urban environments. The dataset provides comprehensive sensor data, including LiDAR scans, detailed 3D annotations, and HD maps, making it particularly suitable for evaluating scene flow estimation in real-world urban scenarios.\n2) Metrics: We employ two metrics to evaluate our method. First, we use the standard End Point Error (EPE), which measures the L2 distance between predicted and ground truth scene flow vectors. However, as EPE tends to be dominated by large objects and fails to reflect performance on smaller, safety-critical objects like pedestrians, we also adopt Bucket Normalized EPE [13]. This metric evaluates each object category separately and normalizes the error by object speed, enabling more balanced assessment across different object types and motion patterns.\n3) Implementation details: Our model is implemented in PyTorch with a two-phase training strategy. The first phase focuses on training the backbone network with scene-adaptive loss without the decoder to learn robust feature representations. In the second phase, we integrate the decoder and fine-tune the entire architecture with a lower learning rate to preserve the pre-trained backbone knowledge while optimizing decoder performance.\nWe conduct distributed training on 8 NVIDIA RTX 4090 GPUs. The first phase runs for 30 epochs with an initial learning rate of 2e-4 and batch size of 5 per GPU. The second phase continues for 50 epochs with an initial learning rate of 2e-6 and batch size of 3 per GPU, while maintaining other hyperparameters."}, {"title": "B. Scene Flow Estimation Performance", "content": "1) Quantitation Analysis: We compare our proposed MambaFlow with various supervised and self-supervised scene flow methods. Table I presents the comparative results evaluated through the official Argoverse 2 test server. The proposed method achieves state-of-the-art performance in all EPE metrics, demonstrating the most accurate point-level motion estimation across all published works. Specifically, MambaFlow surpasses Flow4D by significant margins in all three metrics, showing a 14.7% improvement in average EPE, 8.9% in foreground dynamic estimation, and 68.1% in background static prediction.\nFor object-level evaluation using Bucketed Normalized EPE, our method achieves competitive performance in dynamic object estimation, particularly excelling in rigid objects such as cars and other vehicles. MambaFlow's balanced performance across both static and dynamic scenarios, especially its superior point-level accuracy as demonstrated by 3-way EPE metrics, validates its effectiveness as a comprehensive scene flow estimation solution.\nAs shown in Table II, we further compare our method with supervised methods FastFlow3D, DeFlow, and Flow4D on the Argoverse 2 validation set. MambaFlow achieves best performance across all metrics, demonstrating an 8.7% lower mean Dynamic Normalized EPE, 14.6% lower 3-way mean EPE, and 6.4% higher Dynamic IoU compared to the previous state-of-the-art Flow4D.\n2) Qualitative Analysis: As shown in Figure 5, we visualize scene flow predictions from different methods alongside ground truth across multiple scenarios, where our method demonstrates superior discrimination between moving and static objects, producing minimal flow errors for various structures including buildings, pedestrians, and parked vehicles across multiple scenarios. This capability is crucial for reliable scene understanding in autonomous driving scenarios.\nFurthermore, our method exhibits remarkable capability in object-level scene flow estimation. For instance, in the first row (yellow circles), where two pedestrians are walking in opposite directions, MambaFlow accurately captures their distinct motion patterns despite the lack of explicit ground truth annotations for this scenario. In contrast, Flow4D shows only minimal response to these opposing movements, while DeFlow fails to detect this complex interaction entirely. This demonstrates our method's superior sensitivity to fine-grained motion patterns at the object level."}, {"title": "C. Ablation Study", "content": "The component analysis in Table III demonstrates the contribution of each proposed module. In method (i), for both mean Dynamic EPE and mean Static EPE metrics, incorporating STDCB alone brings improvements of 5.1% and 5.4% respectively, validating its effectiveness in temporal-spatial feature coupling. The application of MambaFlow decoder yields improvements of 3.1% and 1.1% respectively, highlighting its capability in feature recovery. Meanwhile, utilizing Scene-adaptive Loss alone for supervision training improves mean Dynamic EPE and mean Static EPE by 1.7% and 15.2% respectively, demonstrating its robust scene adaptation capability. In method (ii), when validating pairwise combinations of components, the model's performance shows substantial improvements, particularly with the synergistic effect of STDCB and scene-adaptive loss, which yields improvements of 7.3% and 13.0% in mean Dynamic EPE and mean Static EPE respectively. By combining all components, the proposed MambaFlow achieves the best performance."}, {"title": "D. Evaluation of Consumption", "content": "As shown in Table V, our method achieves superior performance with only 3.1M parameters, representing a 32.6% reduction in parameter count compared to Flow4D. Instead of using STDB with redundant convolution operations as in Flow4D, STDCB eliminates duplicate feature extraction processes based on our observation and experimental validation that features extracted in earlier stages already contain sufficient information for subsequent fusion. This architectural optimization also improves computational efficiency, enabling our method to achieve 17.30 FPS compared to Flow4D's 14.68 FPS.\nIn terms of memory usage, our method achieves a better balance between performance and resource efficiency with moderate memory usage of 2.04 GiB. This slight increase in memory overhead is well compensated by the substantial performance improvements. With a more compact model size of 12.598 MB compared to Flow4D's 18.412 MB, MambaFlow is particularly suitable for deployment in resource-constrained scenarios while maintaining state-of-the-art performance."}, {"title": "VI. CONCLUSIONS", "content": "In this paper, we present MambaFlow, a novel scene flow estimation approach based on SSM. Through deep coupling of temporal and spatial information, our method achieves comprehensive spatio-temporal feature extraction with temporal guidance. Most importantly, we introduce a Mamba-based decoder that enables refined devoxelization by learning distinct voxel-to-point patterns, effectively preserving fine-grained motion details. Extensive experiments demonstrate that our approach achieves state-of-the-art performance on the Argoverse 2 benchmark while maintaining real-time inference capability, validating the effectiveness of SSM for scene flow estimation."}]}