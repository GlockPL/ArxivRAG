{"title": "A Taxonomy of AgentOps for Enabling Observability of Foundation Model based Agents", "authors": ["LIMING DONG", "QINGHUA LU", "LIMING ZHU"], "abstract": "The ever-improving quality of LLMs has fueled the growth of a diverse range of downstream tasks, leading to an increased demand for AI automation and a burgeoning interest in developing foundation model (FM)-based autonomous agents. As AI agent systems tackle more complex tasks and evolve, they involve a wider range of stakeholders, including agent users, agentic system developers and deployers, and AI model developers. These systems also integrate multiple components such as AI agent workflows, RAG pipelines, prompt management, agent capabilities, and observability features. In this case, obtaining reliable outputs and answers from these agents remains challenging, necessitating a dependable execution process and end-to-end observability solutions. To build reliable AI agents and LLM applications, it is essential to shift towards designing AgentOps platforms that ensure observability and traceability across the entire development-to-production life-cycle. To this end, we conducted a rapid review and identified relevant AgentOps tools from the agentic ecosystem. Based on this review, we provide an overview of the essential features of AgentOps and propose a comprehensive overview of observability data/traceable artifacts across the agent production life-cycle. Our findings provide a systematic overview of the current AgentOps landscape, emphasizing the critical role of observability/traceability in enhancing the reliability of autonomous agent systems.", "sections": [{"title": "1 INTRODUCTION", "content": "The ever-improving quality of Large Language Models (LLMs) and their remarkable capabilities to understand and generate human-like reasoning and content have sparked the growth of a diverse range of downstream tasks using foundation models (FMs). However, LLMs exhibit limitations, particularly in understanding and performing complex chains of tasks. This has triggered an increasing demand for AI automation, leading to the rapid development of LLM applications, particularly FM-based autonomous agents\nWith the iteration of various large language model APIs and the open-sourcing of various AI agent frameworks, FM-based autonomous agents have gained extensive attention, research, and application in academia and industry. FM-based agents have achieved good results in many tasks, such as text"}, {"title": "1.1 New challenges of agentic systems", "content": "FM-based autonomous agents downstream tasks can produce varied outputs, from structured data to unstructured formats like language and images, or even interact directly with tools via user interfaces. Therefore, FM-based autonomous agents upstream systems are complex and often inscrutable. Their ability to learn from vast datasets to perform a wide range of tasks makes AI agentic systems pose new challenges[29]:\n\u2022 Lack of control in decision planning. Current FM-based autonomous agents approaches delegate decision-making to the agents, determining when and what actions to perform based on previous tasks and automatic/human/cross feedback, often with minimal validation. Due to a vast action space and varied feedback, agents can easily become confused and perform sub-optimal actions [36].\n\u2022 AI agentic system input and behaviour are more complex. Most AI assistants are designed for single-iteration tasks, such as \u201csummarize this text\u201d. A significant challenge in these applications is ensuring the accuracy and trustworthiness of the outputs produced by LLMs. Consequently, LLM models have always necessitated careful monitoring of both the model outputs and the data pipelines to operate reliably. In contrast, compound AI agentic systems exhibit considerably more complex behavior for each input. For example, upon receiving a user-defined goal to perform specific tasks, an agent may require multiple iterations to complete the more complex, multi-hop tasks, complicating the understanding of their decision-making process and the debugging of specific incorrect actions [19].\n\u2022 Compliance with the EU AI Act through Observability. As global recognition of the need for AI governance increases, it is expected that regulations similar to the EU AI Act will continue to shape the future of responsible AI development and deployment[20]. The EU AI Act4 sets specific requirements to ensure the traceability and observability of high-risk AI systems. The first requirement, mandates that high-risk AI systems must \u201ctechnically allow for the automatic recording of events (logs) over the lifetime of the system\u201d, (Article 12). Additionally, the second set of requirements emphasizes the need for high-risk AI systems to \u201c...ensure a level of traceability of the functioning of a high-risk AI system that is appropriate to the intended purpose of the system\u201d, (Articles 79(1), 72, and 26(5)). Ensuring compliance with the EU AI Act necessitates the implementation of comprehensive observability solutions[14]. These solutions must provide robust monitoring and logging capabilities to track the functioning and decision-making processes of AI systems throughout their life-cycle."}, {"title": "1.2 Concept", "content": "\u2022 Agent. LLM-based agent refers to developing AI systems that can act autonomously, make decisions and perform complex tasks with minimal human intervention[4].\n\u2022 Agentic System. LLM-based agents can be defined as GenAI systems that serve a user's goals by performing actions that interact with agentic systems external to the LLM itself [28]. Such systems incorporate LLMs as modules in a control flow designed to solve tasks through the use of external tools, planning, memory, and the execution of multiple, iterative steps of processing actions [26, 37]."}, {"title": "AgentOps", "content": "With the rapid evolution of LLM performance, both large-scale software enterprises and early-stage startups are increasingly transitioning their business projects to LLM-based applications, i.e. agent projects. This shift has created a growing demand for AI agent system infrastructure platforms, giving rise to the concept of AgentOps.\nAgentOps serves as a DevOps/MLOps-like ent-to-end platform, encompassing the development, evaluation, testing, deployment, and monitoring of agentic system for the operational management of agent projects in production environments [8].\n\u2022 Agent Observability. Currently, the development of AgentOps platforms in the industry is in its early stages. Observability is crucial for LLM-based agents, and its importance grows with the complexity of the agentic system. It should be integrated into the AgentOps platform from the beginning to support the agent system's reliability, rather than added later as an afterthought[13]. Observability tools provide full visibility into the entire production process by tracing every step in the chain of events.\n\u2022 Traceable Artifacts. In the era of Observability 2.0[24], data is stored in arbitrarily wide structured log events, often with trace and span IDs appended. Users can visualize events over time as traces, or slice and dice the data to zoom in on individual events or zoom out to a bird's-eye view. Therefore, platform engineers need to understand what data or artifacts should be recorded and traced in the AgentOps platform during the production of the agent system to improve the system's observability and traceability. To maintain consistent terminology, we refer to those data as agent traceable artifacts, which are by-product data generated throughout the agent's development to production life-cycle."}, {"title": "1.3 Research Goal", "content": "It is crucial to design AgentOps platforms that support developers in building more sophisticated and efficient agentic systems by integrating a comprehensive observability solution for AgentOps. Therefore, this paper aims to analyze and summarize the data (i.e. artifacts) that should be tracked throughout the entire life-cycle of agent production to enable AgentOps observability. Our study seeks to address the following research question:\n\u2022 RQ: What data/artifacts should be traced in AgentOps?\nThis review aims to accomplish several key objectives. First, by investigating existing AgentOps tools and providing an overview of the latest observability features, it seeks to support the design of more effective AgentOps platforms. Additionally, by summarizing a comprehensive overview of traceable data and artifacts in agentic systems, it aims to help researchers and practitioners gain a deeper understanding of the entire agent project life-cycle. This understanding of data generated during the agent production process will enhance the observability and traceability of agentic systems, ultimately contributing to the overall reliability of agent systems output."}, {"title": "2 METHODOLOGY", "content": "To achieve this goal, we conducted a multivocal review following established guidelines [10], focusing on the search for existing AgentOps tools across gray literature, open-source project platforms, and other relevant sources."}, {"title": "2.1 Data Collection", "content": "Given that the understanding of the AgentOps concept and the exploration of associated tools in the marketplace are still in an early stage, we broadened our review to search for and identify observability tools related to the entire agent life-cycle, from \"dev\u201d to \u201cops\u201d."}, {"title": "2.1.1 Data Sources", "content": "This review identified AgentOps-relevant tools from multiple sources, including GitHub open-source projects searching5, the AI Automation Market Map6, and the Emerging AI Agent Infrastructure7 tool stack.\n\u2022 GitHub. Our initial data source involved searching the keyword \u201cAgentops\u201d on GitHub, where we found an \"AgentOps\u201d project designed primarily for monitoring AI agent cost management through the AgentOps dashboard and session replay analysis. We also found a GitHub project called \u201cAwesome-LLMOps\u201d 9, a curated list of top LLMOps tools for developers. Although our study does not specifically focus on LLM model projects, some tools listed in \u201cAwesome-LLMOps\u201d are designed for monitoring the observability of LLM applications, which also involve AI agent projects. Therefore, the tool list from \u201cAwesome-LLMOps\u201d has been included as one of the data sources for identifying AgentOps-relevant tools.\n\u2022 Grey Literature. The second data source is grey literature, such as AI agent tool stacks summarized by venture capital firms and software investors. Notable examples include Insight Partners' AI Automation Market Map and Madrona's list of early-stage AI agent infrastructure tools."}, {"title": "2.1.2 Selection Criteria", "content": "For this paper, we focused on tools related to the entire agent lifecycle, from development (\u201cdev\u201d) to operations (\u201cops\u201d). We conducted the tool selection process[17] from the multi data sources[10] mentioned above. To identify AgentOps-relevant tools that support agent creation, development, and monitoring, the following criteria were applied:\ni) Relevance to AgentOps: The tool could support tasks related to either the development or operations of AI agents, covering key phases of the agentops life-cycle, including development tools such as agent-building frameworks and operational tools like agent observability platforms.\nii) Available online doc: The tool's website offers well-structured documentation that is readily available online."}, {"title": "2.2 AgentOps-Relevant Tools", "content": "We identified a collection of AgentOps-relevant tools (see Fig.1) that support developers in building, evaluating, and especially monitoring the observability of AI agents."}, {"title": "2.2.1 AgentOps Tool", "content": "First of all, we were fortunate to find a project called AgentOps on GitHub. AgentOps aims to help developers build, evaluate, and monitor AI agents, facilitating the transition from prototype to production. Within AgentOps, the main activities, such as LLM interactions, tool usage, and actions, are categorized as \u201cevents\u201d executed by agents. It is beneficial to define and track how much time each action (or step) takes. Agents primarily initiate LLM calls, which can lead to API or tool calls. Actions encompass other significant procedures, such as executing functions and taking screenshots. AgentOps provides a comprehensive dashboard for tracking agent performance, session replays, and custom performance reporting. After setting up AgentOps, multiple execution of agent is recorded in each session and the data is automatically recorded for developers.\nThere is no doubt that the AgentOps platform is still a work in progress. Therefore, in our paper, we further reviewed the main toolset features related to the other major phases of agent development. This is beneficial to our paper as it provides readers with a more systematic and comprehensive understanding of the observability data throughout the AgentOps life-cycle."}, {"title": "2.2.2 LLM Applications Observability Tool", "content": "It is relatively easy to build a prototype of an LLM application, but the performance of these prototypes is often insufficient for production use. Therefore, it is necessary to integrate an observability platform to help developers bridge the gap between prototype and production. Such a platform would support developers in closely testing, evaluating, monitoring, debugging, analyzing, and iterating on LLM applications.\nWe have collected a range of LLM application observability platforms, offering comprehensive coverage across the tool stack\u2014from language model-specific tracing tools (such as Langtrace, LangSmith, Langfuse) to broader AI system performance monitoring solutions (such as Arize, Datadog). These LLM applications also involve LLM-agent projects and are likely to serve as reference platforms for the design of AgentOps platforms to enhance agent system observability in the near future. This is why we included them in our review."}, {"title": "2.2.3 Agent Building Framework", "content": "The observability tools provided by AgentOps are often integrated with frameworks that support the construction of autonomous agents, allowing developers to build, customize, manage, and run these agents efficiently. For instance, the SuperAGI framework offers a detailed view of agent architecture, workflows, tools, and various data sheets, providing valuable insights that directly contribute to addressing our research questions. Another example is Crew AI, a multi-agent framework designed to facilitate collaboration among role-playing AI agents, which proves helpful for collecting data in more complex agentic systems. Additionally, Dify is an open-source platform for building AI applications. By integrating LLMOps, it streamlines the development process for generative AI solutions, making it accessible to both developers and non-technical users."}, {"title": "2.3 Key Features", "content": "We summarized the key features (as shown in Table 1) of identified AgentOps relevant tool."}, {"title": "2.3.1 Agent Creation", "content": "Agent development tools, such as autonomous Al frameworks, enable developers to build, manage, and run autonomous agents by translating human goals into computational actions[35]. When creating agents, these tools extend agent capabilities by adding toolkits from marketplaces to agent workflows, connecting to multiple knowledge databases to enhance performance, and integrating customized fine-tuned models for specific business use cases."}, {"title": "2.3.2 Prompt Management", "content": "Prompt versioning and management allow developers to store and track different versions of prompts, making it invaluable for testing, optimizing[5], and reusing prompts across various stages of agent production. The Prompt playground enables developers to edit, import, and test various prompt templates with different models, helping to compare performance before deployment. Consistent monitoring of prompts is also crucial for maintaining the health and security of agentic systems, especially in detecting issues such as code injection risks and secret leaks within prompts."}, {"title": "2.3.3 Evaluation and Testing", "content": "Evaluation is crucial to ensure that agents perform their intended functions effectively and safely in various environments. The typical evaluation process involves creating a suitable evaluation dataset, defining clear criteria and metrics, and conducting thorough testing based on these predefined metrics. It is important to benchmark the agent's performance against standard leaderboards or comparable systems. Specific to agentic system, evaluation goes beyond simply assessing the correctness of the final output. It is equally important to track the agent's execution steps and evaluate intermediate outputs to ensure the reliability of the final result. LangSmith 10 introduces two additional dimensions of agent evaluation:\n\u2022 Step-by-Step Evaluation: Assess each individual step the agent takes in isolation, such as determining whether it selects the appropriate tool.\n\u2022 Trajectory Evaluation: Examine whether the agent followed the expected sequence of actions, including the series of tool calls, to arrive at the final answer. This ensures that the decision-making process is sound, not just the outcome."}, {"title": "2.3.4 Feedback", "content": "Human feedback is a great source to evaluate the quality of an agent's output. Feedback is collected as a score and attached to an execution trace or an individual LLM generation. Langfuse11 defined different types of feedback that can be collected that vary in quality, detail, and quantity.\n\u2022 Explicit Feedback: Directly prompt the user to give feedback, this can be a rating, a like, a dislike, a scale or a comment. While it is simple to implement, quality and quantity of the feedback is often low.\n\u2022 Implicit Feedback: Measure the user's behavior, e.g., time spent on a page, click-through rate, accepting or rejecting final output. This type of feedback is more difficult to implement but is often more frequent and reliable."}, {"title": "2.3.5 Monitoring", "content": "Developers can continuously monitor agent performance and enhance observability throughout the agent's execution process by closely tracking its outputs. This involves keeping track of monitoring metrics (e.g., latency and cost[31]), associating feedback with agent runs to evaluate performance, and debugging issues by diving into specific traces and spans where errors occurred. Monitoring also helps identify the root causes of unexpected results, errors, or latency issues, allowing developers to optimize performance based on real-time feedback."}, {"title": "2.3.6 Tracing", "content": "AgentOps is designed to support developers in transitioning from prototype to production, ensuring that the work doesn't stop once the agent is created and initial tests are passed. Within the observability tool, agents execute increasingly complex tasks and iterative runs, such as chains, tool-assisted agents, and advanced prompts. By adding traces to agent production life-cycle[3, 22], AgentOps captures the entire process\u2014from the moment a user sends a query to the final response-helping developers understand each step and identify the root causes of any issues."}, {"title": "3 STUDY RESULT", "content": "AgentOps is a comprehensive platform that covers the entire life-cycle of agent development, including execution, evaluation, testing, and post-deployment tracing and monitoring. To clarify the data generated throughout the AgentOps, we conducted an in-depth analysis of the platform's key components and features, summarizing the data items (organized as traceable artifacts in our paper) that can be recorded. The following results present a comprehensive overview of the traceable artifacts within the AgentOps life-cycle."}, {"title": "3.1 Agent Creation Registry", "content": "Creating new agents, enabling users to extensively customize agents to suit their specific requirements. We refer to the collection of data items related to agent creation as artifact called an Agent Creation Registry (or card), drawing inspiration from prior work on documenting AI systems, such as Agent Cards [3], Datasheets [11], and Model Cards [15, 25].\n\u2022 Agent Identity. Developers begin by assigning a unique ID and name to the agent. This allows for clear identification and makes it easier to manage and track the agent throughout its life-cycle.\n\u2022\u2022 Goals. These are user-defined desired outcomes or objectives that guide the agent's overall behavior and actions. Instructions. Users can provide actionable guidelines that direct the agent on how to achieve the defined goals, assisting in making appropriate decisions.\n\u2022 Agent Input Data. Developers can incorporate input data from multiple sources (see Fig.3), which the agent will process during its operation.\n\u2022 Prompt. Prompts serve as the foundational element for the agent's decision-making and behavior, incorporating multiple layers of information, such as goals, instructions, and contextual data. These prompts guide the agent's actions and responses, shaping its overall performance. For a detailed breakdown of the structure of prompts registry, please refer to Fig.4.\n\u2022 LLM Model/Toolkits. When creating an agent, users select the LLM model for function calls and choose the necessary toolkits that the agent will use to interact with its environment and perform tasks.\n\u2022 Agent Role Type. Users can either create custom agent roles or modify existing ones. Agent roles are classified based on job responsibilities:\nWorker. Agents can take on roles such as \u201cResearcher\u201d or \u201cWriter\u201d,functioning as part of a team with specific skills and responsibilities.\nDevelopers can also create Coordinator/Supervisor agents, under which multiple worker agents operate. These subordinate agents follow the instructions of the supervisor.\n\u2022 Guardrails. Constraints, or guardrails, are predefined during the agent creation process to ensure the agent operates within specific guidelines and environmental boundaries. Developers can modify these constraints as needed to fine-tune the agent's behavior and performance, allowing flexibility in setting targets, rules, and corresponding actions."}, {"title": "3.2 Enhancing Context", "content": "Context construction refers to the process of gathering relevant information from additional input data to provide the model with the necessary background knowledge [13]. Techniques such as In-Context Learning and Retrieval-Augmented Generation (RAG) are used to enrich input data, enabling the model to process it more accurately."}, {"title": "3.2.1 In-Context Learning", "content": "In-Context Learning is a form of continual learning that allows a model to dynamically adapt to new information without the need for retraining or fine-tuning. It typically involves the model interpreting a task description and generating the corresponding output based on a given example and structured prompt instruction. This approach enables the model to incorporate real-time data, ensuring it remains current. A key advantage of In-Context Learning is that it avoids the computational cost of fine-tuning, thereby saving significant resources and time."}, {"title": "3.2.2 RAGs", "content": "The training data for LLMs is typically based on publicly available information, and each training session requires substantial computational resources. As a result, LLMs often lack access to private domain knowledge and may not include the most up-to-date information from the public domain. To address this limitation, a common solution is to use RAG. RAG utilizes a user's query (e.g., through keyword search or text vector search) to match the most relevant external memory sources (source documents, such as documents, tables, or chat history). After retrieving the relevant content, RAG reorganizes and integrates this information into the prompt as additional context. This process allows for the dynamic inclusion of domain-specific knowledge stored in a knowledge base, which can be incorporated into the agent's input as retrieval context. A knowledge base consists of documents that can be uploaded by developers or operations teams, or synchronized from various data sources such as web pages, GitHub, or databases."}, {"title": "3.3 Prompt", "content": "Creating effective prompt templates is critical for optimizing agent performance. To effectively manage and version prompts, the collection of following prompt-related information could recorded as a prompt registry (or card) (see Fig.4).\n\u2022 Prompt Identity. Including standard fields such as ID, Name, and Version.\n\u2022 Prompt Templates Type. The Prompt Playground provides developers with the ability to edit, import, and test various prompt templates. LangSmith offers a choice between two types of prompt templates12: chat-style prompts and instructional prompts. Chat-style prompts are designed for conversational agents, accepting a list of messages as input and responding with"}, {"title": "3.4 Guardrails", "content": "Constraints act as specific guardrails within which an agent must operate, ensuring that it adheres to defined guidelines and functions appropriately within the intended environment. These guardrails are typically pre-defined during the agent creation process, but users also have the flexibility to instantiate specific guardrails by setting targets, rules, and corresponding actions [30] (see Fig.5)."}, {"title": "3.5 Agent Execution", "content": "3.5.1 Planning. Planning involves devising step-by-step actions from a given task. The necessary inputs for agent planning15 include the agent goal, agent role, entire task description, expected task output, tools, etc. Developers then observe an output that includes multiple subtask queues and specific actions, which represent the agent's step-by-step logic generated during the planning phase (see Fig.6).\n\u2022 Task. Tasks are specific assignments completed by agents. They provide all necessary details for execution, such as a description, the responsible agent, required tools, and more, enabling a wide range of action complexities.\n\u2022 Tool. A tool is a skill or function that agents can utilize to perform various actions. The intermediate step outputs of agent is usually the LLM call. The LLM call often contains tool calls, indicating what action the agent should take next. This tool call can be used to determine agent selects a tool( e.g., from existing toolkit) to use, extracts the right parameters from the user query and agent task, then activate the tool call event.\n\u2022 Action. An action module enables the agent to utilize external tools. Agent actions are typically triggered by user requests to support the agent's step-by-step planning tasks. Key elements of an agent action call include the action trigger, which causes the agent to perform an action (e.g., a user query). The action execution involves the actual task or function the agent performs, such as running a tool, browsing a website, or generating a response. Input data or parameters guide the execution of the action (e.g., a search term or dataset). The outcome or response produced by the action may be returned to the user or utilized in subsequent planning steps and actions."}, {"title": "3.5.2 Reasoning", "content": "Reasoning is the process of using existing knowledge to draw conclusions, make predictions, or construct explanations. Single agents generally show limited effectiveness in reasoning tasks. However, multi-agent discussions utilize multiple LLMs as agents to collectively discuss and"}, {"title": "3.5.3 Memory", "content": "The memory module stores and recalls past interactions. The agentic system comprises both short-term and long-term memory[9]. Short-term memory temporarily stores recent interactions and outcomes, enabling agents to recall and utilize information relevant to their current context during execution. For example, agents can maintain context over a conversation or task sequence, resulting in more coherent and relevant responses. Long-term memory preserves valuable insights and learnings from past executions, allowing agents to build and refine their knowledge over time. This enables agents to remember what they did right and wrong across multiple executions."}, {"title": "3.5.4 Workflow", "content": "Workflows reduce system complexity by breaking down complex tasks into smaller steps (nodes), reducing reliance on prompt engineering and model inference capabilities, and enhancing the performance of agent for complex tasks. Nodes are the key components of an agent workflow. By connecting nodes with different functionalities, agent developer can execute a series of operations within the workflow."}, {"title": "3.6 Evaluation and Feedback", "content": "3.6.1 Evaluation. Developers can create an evaluation template to systematically assess the quality of an agent's output. The evaluation process typically involves the following artifacts:"}, {"title": "Evaluation Dataset", "content": "To run evaluations, developers need to create or upload a dataset that serves as the test set for conducting either automated or human evaluations."}, {"title": "Evaluation Template", "content": "To generate a template for evaluation, developers must define the evaluation criteria and select appropriate aggregation metrics. This involves writing an experiment test script to specify the task and assess the agent's effectiveness."}, {"title": "Evaluator Input", "content": "Evaluators are functions that measure the performance of the agent or LLM application. Typically, evaluators require several inputs, including the agent's output, the reference answer (such as the expected output or ground truth), the agent's input (e.g., prompt, task description), and any other relevant data."}, {"title": "Evaluator Output", "content": "After the evaluation is complete, developers can view the results, which are usually presented as key-value pairs that map to the evaluation metrics, along with numeric scores. These results allow developers to display and compare evaluation runs to understand the agent's effectiveness across different models and benchmarks. Additionally, the evaluation results often provide explanations or comments on why the output met or failed the evaluation criteria."}, {"title": "3.6.2 Feedback", "content": "In many applications, particularly for LLM-based agents, collecting user feedback is crucial for understanding how the agent performs in complex, real-world tasks. A simple feedback mechanism, such as a thumbs-up or thumbs-down button, can be valuable for evaluating the final responses of a single LLM call."}, {"title": "Human Feedback Form", "content": "In a formal user-facing feedback form, developers can define feedback tags and create a list of categories (e.g., Toxicity, Human Correctness, Answer Relevance). Each category is associated with a score. When providing feedback, users select one of these categories and assign a score, which is logged in both the value and score fields."}, {"title": "Feedback Loop", "content": "For instance, LangSmith allows developers to manually annotate traces with feedback, enabling the logging of both value and score for each agent run. This feature is particularly useful for dynamically updating the evaluation test set and ground truth by incorporating human feedback from each run. This process facilitates a more detailed inspection of the agent's performance and allows for continuous updates to the dataset used for evaluation"}, {"title": "3.7 Tracing", "content": "Once agent developer have deployed their agent, they can trace its execution process to understand and monitor its operations. This tracing provides a comprehensive view of all the steps involved in processing a request helping to evaluate the agent's performance and identify any issues. By reviewing each run (e.g., chains and calls) in the agent's entire execution process, you can pinpoint problematic requests and trace errors back to their root cause."}, {"title": "Session", "content": "Sessions group multiple traces to represent a sequence of operations, such as the execution of an AI agent workflow. A session encapsulates a single execution instance of workflow, bringing together all agents, LLMs, actions, and related components under one complete view. Session tracking allows for session-level visualization, analytics, and automatic issue detection. A session is defined by a unique session ID, which groups related traces and provides valuable data such as total execution time, token cost, and the success or failure state of the session. The AgentOps dashboard offers detailed insights at the session level, including metrics like costs, token counts, errors, and more. Adding a User ID along with the session ID enables further grouping, filtering, and visualization of user-related traces, interactions, metrics, and costs."}, {"title": "Traces", "content": "Identifying groups of traces where agent is underperforming is essential for improving its effectiveness. A trace refers to the detailed recording of a request's execution path through various system components and services. In an AI application, tracing reveals the entire process from when a user submits a query to when the final response is returned, including the actions the system takes, the documents retrieved, and the final prompt sent to the model. It also shows how much time each step takes and its associated cost, if measurable."}, {"title": "Span (Run)", "content": "A trace consists of one or more spans. The first span represents the root span. Each root span represents a request from start to finish. The spans beneath the parent provide deeper context for what occurs during a request, detailing the steps that make up the request. LLM spans represent a call to an LLM where inputs and outputs are expressed as text. LLM spans typically do not have child spans, as they are standalone operations representing a direct call to an LLM (e.g., a call to OpenAI or Llama). Agent spans represent a dynamic sequence of operations where a large language model determines and executes actions based on inputs. For example, an agent span might represent a series of reasoning steps controlled by a ReAct agent. Agent spans are often the root span for traces representing autonomous or reasoning agents. Workflow spans represent any static sequence of operations. Workflows group together an LLM call with supporting contextual operations, such as tool calls, data retrievals, and other tasks. For instance, a workflow span could represent a process that takes an arXiv paper link and returns a summary, involving a tool call to fetch the paper, text processing tasks, and an LLM summarization. Tool spans represent a standalone step in a workflow or agent that involves a call to an external program or service, such as a web API or database. Task spans represent a standalone step in a workflow or agent that does not involve a call to an external service, such as a data sanitization step before a prompt is submitted to an LLM. Retrieval spans are a subcategory of tool spans and represent a vector search operation where documents are returned from an external knowledge base. For example, a retrieval span could trace a similarity search to a vector store to collect relevant documents for augmenting a user prompt on a given topic. Embedding spans represent a call to an LLM for generating an embedding. For instance, an embedding span might represent a call to OpenAI to get an ada-2 embedding for retrieval purposes. Chain spans represent the starting point or a link between different LLM application steps. For example, a chain span might represent the beginning of a request to an LLM application or the connection between a retriever and an LLM call."}, {"title": "3.8 Monitoring", "content": "Monitoring enables developers to continuously improve the effectiveness, engagement, and cost-efficiency of agent operations."}, {"title": "Monitoring Metrics", "content": "When discussing monitoring, most people think of metrics. The specific metrics to track depend on what aspect of the agent system you want to monitor. In general, there are three types of metrics that developers can track: common model metrics, quality performance metrics, and error issues.\nCommon metrics include token usage, cost, and latency.\nQuality metrics (e.g., Toxicity, Human Correctness, Answer Relevance) can be assessed through user feedback, model-based scoring, human-in-the-loop evaluations, or custom scoring systems."}, {"title": "Error metrics", "content": "can trace errors in the execution process, such as latency or privacy issues, and link these errors back to their causal traces, allowing for detailed investigation down to the span level."}, {"title": "Monitoring Dimensions", "content": "Monitoring can be measured and broken down at various levels: session-level, trace-level, span-level, and across different users, models, and prompt versions. For example, developer can track how changes to the agent's prompt affect the metrics mentioned above."}, {"title": "4 THREATS TO VALIDITY", "content": "Tool Selection Limitations: Due to the rapid proliferation of various tools and AI platforms, it is possible that not all relevant AgentOps tools were identified. To address this limitation, we selected tools from multiple data sources. The identified tools include both open-source AgentOps tools, such as AgentOps and Langfuse, as well as commercial observability platforms like Datadog. We will continue to monitor the development of the AgentOps tool stack and provide updates in our future work.\nData Coverage Limitations: The comprehensive overview of traceable artifacts throughout the AgentOps life-cycle provided in this work may not encompass all possible data attributes related to AI agents. To ensure broader coverage of important traceable data across the entire life-cycle of an agent and enrich on the data attributes outlined in our paper, we have drawn on some relevant academic literature to support our findings as well. However, existing AgentOps/LLMps platforms do not specifically discussed key aspects of agent execution steps, such as artifacts in planning, reasoning, and memory. To address this gap, we will conduct an extensive academic literature review focused on these artifacts. Some potentially valuable data, such as trace links across the AgentOps life-cycle (from user requests to final output) and interactions between different steps, will be further investigated in depth in our future work."}, {"title": "5 CONCLUSION", "content": "LLM-based agents are receiving increasing attention across various fields, but the reliability of their outputs poses significant challenges for developers. The introduction of the AgentOps platform is designed to streamline the agent development process, making it more efficient, observable, and reliable. In this study, we explored various AgentOps tools and provided an overview of key features within AgentOps platforms. We also presented a comprehensive overview of traceable artifacts throughout the agent production life-cycle, aiming to address critical issues related to project tracing and monitoring. Our paper offers platform developers a clearer understanding of the agent life-cycle and serves as a valuable resource for improving error monitoring and debugging within agent systems. Future work will focus on building"}]}