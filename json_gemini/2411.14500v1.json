{"title": "Exploring Accuracy-Fairness Trade-off in Large Language Models", "authors": ["Qingquan Zhang", "Qiqi Duan", "Bo Yuan", "Yuhui Shi", "Jialin Liu"], "abstract": "Large Language Models (LLMs) have made significant strides in the field of artificial intelligence, showcasing their ability to interact with humans and influence human cognition through information dissemination. However, recent studies have brought to light instances of bias inherent within these LLMs, presenting a critical issue that demands attention. In our research, we delve deeper into the intricate challenge of harmonising accuracy and fairness in the enhancement of LLMs. While improving accuracy can indeed enhance overall LLM performance, it often occurs at the expense of fairness. Overemphasising optimisation of one metric invariably leads to a significant degradation of the other. This underscores the necessity of taking into account multiple considerations during the design and optimisation phases of LLMs. Therefore, we advocate for reformulating the LLM training process as a multi-objective learning task. Our investigation reveals that multi-objective evolutionary learning (MOEL) methodologies offer promising avenues for tackling this challenge. Our MOEL framework enables the simultaneous optimisation of both accuracy and fairness metrics, resulting in a Pareto-optimal set of LLMs. In summary, our study sheds valuable lights on the delicate equilibrium between accuracy and fairness within LLMs, which is increasingly significant for their real-world applications. By harnessing MOEL, we present a promising pathway towards fairer and more efficacious Al technologies.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have demonstrated an exceptional promising ability to perform a wide range of tasks (Zhao et al., 2023; Chang et al., 2024), from natural language understanding (NLU) (Devlin et al., 2019; Bang et al., 2023) to natural language generation (NLG) (Qin et al.,\n2023). Recently, LLMs are directly accessible and usable by the general public, integrating seamlessly into daily interactions through applications like personal assistants (Chang et al., 2024). The impact of LLMs spans a variety of fields, including chatbots (Glaese et al., 2022), medical diagnose (Wang et al., 2023), financial advisory (Xing, 2024) and healthcare (Singhal et al., 2023). This widespread accessibility not only aids in various tasks but also subtly shapes and influences people's thoughts and perceptions when people interact with LLMs (Blodgett et al., 2020; Kumar et al., 2023; Obermeyer et al., 2019; De-Arteaga et al., 2019). Therefore, when LLMs convey biased information or experience hallucinations, they can inadvertently spread incorrect ideas or lead to poor decisions for people, potentially causing societal harm and unfairness (Blodgett et al., 2020; Deshpande et al., 2020).\nAmong these societal issues LLMs pose, the ethical concern of fairness is particularly critical (Blodgett et al., 2020; Deshpande et al., 2020; Kumar et al., 2023). With the widespread use of LLMs today, the issue has become an urgent priority to prevent reinforcing existing biases and to ensure equitable outcomes for all users (Blodgett et al., 2020; Deshpande et al., 2020; Weidinger et al., 2021).\nIn the literature of LLMs, there is often an excessive focus on either model performance or fairness metrics (Chu et al., 2024). Our study reveals that many of these approaches may result in a disproportionate sacrifice of one for the other, highlighting the conflicts between these two objectives. It is crucial to find an LLM with a trade-off that correctly balances model performance and fairness, making it suitable and practical for real-world applications. Moreover, the requirements for this balance may change for various reasons, such as specific industry standards, regulatory compliance, and user demographics. Therefore, a set of trade-offs considering both objectives is more desirable to cater to a wider range of needs and scenarios (Zhang\net al., 2023).\nFurthermore, we advocate reformulating the fairness-aware LLM training process as a multi-objective learning task. Our investigation reveals that multi-objective evolutionary learning (MOEL) offers promising avenues for tackling this challenge. By incorporating these methodologies, we can create a framework that simultaneously optimises accuracy and fairness metrics, resulting in a Pareto-optimal set of LLMs, as shown in Fig. 1.\nThe MOEL framework is verified to provide a diverse set of LLMs, each representing a different trade-off between accuracy and fairness. This flexibility allows stakeholders to choose models that best fit their specific needs and contexts, promoting more equitable and effective deployment of LLMs across various applications. By embracing this approach, we can ensure that LLMs contribute positively to society while mitigating the risks associated with biased or unfair outcomes.\nThe remainder of this paper is organized as follows. Section 2 introduces the background of methods for mitigating LLM unfairness and MOEL. Section 3 explains our fairness-aware Pareto framework (denoted as FaPareto) to balance fairness and accuracy. Next, Section 4 presents the experimental studies, including experimental settings and the trade-off analysis of Pareto LLMs. Finally, Section 5 concludes the paper and discusses future work."}, {"title": "2 Background", "content": "First, we review the literature on mitigating LLM's unfairness. Then, we introduce the basics of multi-objective evolutionary learning."}, {"title": "2.1 Mitigating LLM's Unfairness", "content": "The existing methods for mitigating unfairness in LLM can be divided into four categories according to the stage at which they intervene in the processing pipeline (Chu et al., 2024), including pre-processing, in-training, intra-processing and post-processing. Pre-processing strategies target the adjustment of input data for LLMs, such as training datasets and prompts. In-training methods involve modifying the parameters of LLMs through model training, such as altering the loss function and adding auxiliary modules. Intra-processing approaches aim to reduce bias in pre-trained or fine-tuned models at the inference stage without further training, including model editing and decoding modification. Post-processing techniques, such as chain-of-thought and rewriting, focus on altering the model's outputs to address and mitigate biases.\nPre-processing and in-processing techniques are commonly used approaches. In pre-processing methods, an effective method is counterfactual data augmentation (CDA) (Zmigrod et al., 2019; Qian et al., 2022; Chen et al., 2024a), aiming to balance datasets by swapping out data's protected attributes. Additionally, the study (Chen et al., 2024a) demonstrates that adjusting the ratio of data in different groups, through methods such as undersampling and oversampling, is capable of enhancing the fairness of LLMs. In-processing methods often involve loss functions or constraints to ensure that the trained objective function considers both performance and fairness (Yang et al., 2023; Zayed et al., 2023).\nNevertheless, achieving a suitable balance between model performance and fairness remains a major challenge for these methods (Chu et al., 2024). This balance can be determined by various factors, including industry standards and regulatory compliance. Achieving the desired balance based on these methods often requires manually adjusting the trade-off parameter (Zayed et al., 2023), which is difficult and easily leads LLMs to overemphasize either performance or fairness. On the other hand, the requirements for the balance may shift due to changes in industry standards, regulatory compliance or user demographics. More efforts are needed to manually re-adjust the trade-off parameter. However, the process demands considerable time and financial resources, mandating advanced hardware for each cycle.\nA set of trade-offs that can well balance the performance and fairness of LLMs is more attractive. It can lead to more efficient utilisation of computational resources and better alignment with evolving standards and regulations."}, {"title": "2.2 Multi-Objective Evolutionary Learning", "content": "Multi-objective Evolutionary Learning (MOEL) is a family of algorithms to deal with multi-objective learning tasks in machine learning (ML) (Abbass, 2003; Minku and Yao, 2013). Multiple objectives can be simultaneously optimised throughout the model training process, where a set of models is evolved by multi-objective evolutionary algorithms (MOEAs) (Abbass, 2003; Zhang et al., 2023, 2024)."}, {"title": "3 Mitigating LLM's Unfairness via MOEL", "content": "In this section, we first present the problem definition of mitigating LLM unfairness. Next, the overall framework we proposed to mitigate the unfairness of LLMs via MOEL is presented. Then, two key fairness-aware components of the framework are introduced, including fairness-guided diverse LLM generation and fairness-aware evaluation via multi-objective optimisation."}, {"title": "3.1 Problem Definition", "content": "The problem of mitigating LLM unfairness can be formulated as a multi-objective learning task due to the conflicts between accuracy and fairness.\n$\\min F(\\theta) = \\{f_1(\\theta),..., f_n(\\theta)\\} ^T,$\nwhere $\\theta \\in \\Theta_{LLM}$ represents the parameters of the model, and $\\Theta_{LLM}$ is the parameter space. The function $f_i(\\theta)$ corresponds to the $i$th objective function of an LLM $\\theta$, which relates to either accuracy or fairness performance metrics.\nDifferent from single-objective optimization, which typically yields a single optimal solution, the presence of conflicting objectives in multi-objective problems results in a set of optimal parameters $\\theta^*$. This set, typically identified by Pareto-optimal solutions, represents the best possible trade-offs where no objective can be improved without compromising another. The Pareto Front of the problem in Eq. 1 can be formulated as $\\{F(\\theta^*) : \\nexists \\theta' \\in \\Theta_{LLM}, f_i(\\theta') \\leq f_i(\\theta^*), \\forall i \\in \\{1, ..., n\\}, \\exists j \\in \\{1, ..., n\\}, f_j(\\theta') < f_j(\\theta^*)\\}$."}, {"title": "3.2 Overall Framework", "content": "Our proposed framework, detailed in Algorithm 1 and also in Figure 1, is designed to evolve a population of LLMs to achieve trade-offs between accuracy and fairness. The process starts with a set of pre-trained LLMs $\\mathcal{M} = \\{M_1,..., M_x\\}$, model evaluation criteria $\\mathcal{E}$, a training dataset $\\mathcal{D}_{train}$, and a validation dataset $\\mathcal{D}_{val}$.\nInitially, the LLM population $\\mathcal{M}$ are fine-tuned using the training data $\\mathcal{D}_{train}$ to update their parameters (line 1 in Algorithm 1). These tuned models are then evaluated using the criteria $\\mathcal{E}$ on the validation dataset $\\mathcal{D}_{val}$ to obtain objective values (line 3 in Algorithm 1), which are used to assess the fitness of each model (line 4 in Algorithm 1).\nThe main loop of the framework runs until a termination criterion is met. In each iteration, a mating pool pool is created by selecting promising models based on their fitness values (line 6 in Algorithm 1). New models $\\mathcal{M}'$ are generated from this mating pool using the fairness-guided diversity generation (FGDG) strategy. As illustrated in Figure 1, crossover and mutation are two parts of FGDG, which will be detailed in Section 3.3. These new models are subsequently fine-tuned on the training dataset $\\mathcal{D}_{train}$ (line 8 in Algorithm 1). Fairness-aware evaluation via multi-objective optimisation is conducted in two steps. First, the tuned models are evaluated using the criteria $\\mathcal{E}$ on the validation dataset $\\mathcal{D}_{val}$ (line 11 in Algorithm 1) to obtain each LLM's objective values. Next, their fitness values are updated to consider both the original and new objective values. A Pareto selection process combines the original and new models, selecting the best models for the next generation based on their fitness values.\nThis iterative process ensures that the population of LLMs evolves over time, balancing the trade-offs between accuracy and fairness. The final output is a set of models Archive that can be selected by decision-makers according to their preferences."}, {"title": "3.3 Fairness-Guided Diversity Generation for LLMS", "content": "A significant challenge in previous work (Zhang et al., 2021, 2023; Yuan et al., 2024; Gui et al., 2023) has been the difficulty in effectively transferring the strengths of parent models when scaling to LLMs, leading to limited performance of the offspring models. To address this issue, we propose FGDG, a framework designed to leverage the characteristics of LLMs and incorporate advanced techniques, ensuring that offspring models not only inherit valuable traits from their parent models but also explore new and improved possibilities. The framework consists of two key components: a crossover strategy and a mutation strategy.\nLLM merge methods (Wortsman et al., 2022; Ilharco et al., 2023; Yadav et al., 2024; Yang et al., 2024) are used as the crossover strategy in FGDG. These methods combine multiple base LLMs into a new model, effectively integrating the knowledge from each parent (Yang et al., 2024). Specifically, this integration enhances the performance of the merged LLMs, whether in terms of tasks the base models were originally trained on or new tasks they had not previously encountered. This approach effectively meets the requirements of our crossover implementation, supporting the overall objectives of maintaining both performance and fairness in the merged LLMs.\nTo further improve the diversity and exploration of LLMs, we incorporate Gaussian noise as the mutation strategy. Studies (Wu et al., 2022) have shown that introducing Gaussian noise during LLM training helps models escape local optima, thereby boosting their performance. This makes it a suitable mutation operator for our framework."}, {"title": "3.4 Fairness-Aware Evaluation via Multi-Objective Optimisation", "content": "Evaluation within a fairness-aware framework involves two main aspects: objective evaluation (lines 3 and 10 in Algorithm 1) and fitness evaluation (lines 4 and 11 in Algorithm 1). The aim of these steps is to assess and distinguish the quality of LLMs in terms of their performance and fairness.\nObjective evaluation focuses on assessing specific metrics, such as accuracy and fairness, tailored to the particular needs identified by decision-makers within the domain of application. One key advantage of our approach is that these selected metrics only require clear definitions for their computation; they are not constrained by needing to be differentiable, convex, or adhere to other mathematical criteria. For example, accuracy could be assessed by employing measures like precision, recall, and F1 score, whereas fairness might be evaluated using approaches such as statistical parity or equal opportunity (Barocas et al., 2023; Mehrabi et al., 2021; Caton and Haas, 2024).\nFitness evaluation, however, involves ranking LLMs based on the outcomes of their objective evaluations. This is typically done using a multi-objective optimiser, like MOEAS (Fonseca and Fleming, 1995; Deb, 2008; Li et al., 2015), which assigns each LLM a fitness value by evaluating their performance considering the defined objectives, often balancing various trade-offs. The optimiser ranks the LLMs and those with higher fitness values indicating better performance and alignment with desired outcomes are selected or advanced for subsequent processes. Higher-ranking LLMs are used to construct a mating pool for FGDG (line 6 in Algorithm 1) or select the best LLM set for the next generation (line 12 in Algorithm 1). This ranking process identifies which LLMs are most suitable for tasks, taking into account both performance metrics and ethical considerations like fairness.\nIn summary, our framework leverages objective and fitness evaluations to create and maintain LLMs that are both high-performing and fair, using clearly defined metrics and multi-objective optimisation to guide the LLM evolution process."}, {"title": "4 Experimental Studies", "content": "In this section, we first introduce the detailed experimental settings. Then, we analyse the experimental results to explore the accuracy and fairness trade-off in LLMs and finally verify the effectiveness of our framework in dealing with the trade-off."}, {"title": "4.1 Experimental Settings", "content": "Tasks. Accurate and fair occupation classification is crucial for fair online recruiting and professional opportunities. For our preliminary study, the commonly used BiasBios dataset (De-Arteaga et al., 2019) is selected due to its relevance in the growing field of automated hiring, where gender bias in occupation classification can have significant negative consequences. Prior research has identified (De-Arteaga et al., 2019; Chen et al., 2024a) a notable gender bias within BiasBios. To facilitate\na more detailed analysis, we concentrated on the occupations of teacher and surgeon, framing them as a binary (two-class) classification task.\nMetrics. Accuracy and the true positive rate gender gap $\\Delta_{TPR}$ (De-Arteaga et al., 2019) are selected as accuracy and fairness metrics, respectively.\n$\\Delta_{TPR}$ quantifies the disparity in true positive rates (TPRs) between female $(f)$ and male $(m)$ subjects for each occupation $(y)$, which is defined as:\n$\\Delta_{TPR} = |TPR_{f,y} - TPR_{m,y}|.$\nThe true positive rate for a given gender $(g)$ and occupation $(y)$ is expressed as:\n$TPRg,y = P[\\hat{Y} = y|G = g, Y = y]$,\nwhere $\\hat{Y}$ represents the predicted occupation, Y represents the actual occupation, and G denotes the binary gender of the individual data.\nPerformance measure. The widely used indicator in the multi-objective optimisation (Li and Yao, 2019), hypervolume (HV) (Shang et al., 2021; Zitzler et al., 2001), is employed to evaluate the performance of a set of LLMs obtained through our framework. HV is used to evaluate the overall performance of the obtained LLM sets, considering both convergence and diversity. A larger HV value indicates better performance.\nLLM model. The pre-trained BERT-Base-Uncased (Devlin et al., 2019) model is considered as the individual model for our framework. BERT-Base-Uncased is widely used for natural language understanding tasks due to its robust performance and versatility.\nComparison methods. Six common methods for mitigating LLM's unfairness are compared to verify the effectiveness of our framework: vanilla, CDA (Qian et al., 2022), oversampling, undersampling, oversampling with CDA (Chen et al., 2024a), and undersampling with CDA (Chen et al., 2024a). The implementation of these methods including hyper-parameters was based on open-source code available at GitHub repository \u00b9 associated with the work (Chen et al., 2024a). Each of these methods involves training a single model, offering a specific balance between $\\Delta TPR$ and accuracy. This highlights the benefits of our framework in achieving a balance between accuracy and fairness, as well as the advantages of providing a diverse set of LLMs."}, {"title": "4.2 Trade-off Analysis of Pareto LLMs", "content": "For a comprehensive analysis of Pareto LLMs, three aspects are considered. First, we assessed the overall performance of LLM sets obtained by our framework in each trial using the HV metric. Second, we summarized all the LLMs evolved by our framework and identified the non-dominated LLMs to illustrate trade-offs between accuracy and the fairness metric $\\Delta TPR$. Third, we compared our method with six methods mentioned in Section 4.1 to verify its effectiveness.\nEvolution of LLMs. Figure 2 illustrates the convergence curves of HV values, quantifying the quality of the optimization process. The HV curves of our method show a consistent upward trend as the evolutionary process progresses. This indicates that our method effectively improves the performance of the LLMs over time, empirically demonstrating reliable optimization.\nSpecifically, the increasing trend on HV suggest that our framework steadily enhances both the accuracy and fairness of the models. As the evolutionary process continues, the models not only become more accurate in their predictions but also exhibit reduced bias, as indicated by the improved fairness metric $\\Delta TPR$ (Chen et al., 2024a; De-Arteaga et al., 2019). This consistent improvement across multiple trials highlights the reliability and effectiveness of our method.\nAccuracy-Fairness Trade-off. To comprehensively analyse the relationship between accuracy and fairness, all the LLMs are evaluated and recorded. In the objective space of error and fairness, we plotted all the LLMs (represented by red points) and highlighted the Pareto Front (represented by red line). To unify both objectives as minimization tasks, we plotted error instead of accuracy, where error is defined as $Error = \\frac{1}{Accuracy}$.\nThe resulting plot reveals a clear trade-off between the two objectives. Specifically, as one moves along the Pareto Front, improving the fairness metric often comes at the cost of increased error, and vice versa. To quantify this trade-off, we calculated the Pearson correlation coefficient for the LLMs on the Pareto Front, which is -0.81. This significant negative correlation indicates a strong conflict between the two objectives. In practical terms, it means that efforts to enhance the fairness of the model typically result in a reduction in accuracy, as reflected by an increase in error.\nComparisons with State-of-the-arts. The results depicted in Figure 4 highlight the effectiveness of our method compared to the six state-of-the-arts. Our method is represented by the red points and the corresponding Pareto Front, demonstrating superior performance in the trade-off between error and the fairness metric $\\Delta TPR$.\nFirst, our method achieves a lower error rate while maintaining or improving fairness (as indicated by lower $\\Delta TPR$ values). Regarding dominance on the Pareto Front, the red Pareto Front points indicate that our method finds solutions that dominate those from all other methods. None of the other methods produces points on the Pareto\nFront, illustrating that our method achieves a better trade-off between the two objectives. This suggests that our approach effectively balances accuracy and fairness, outperforming the other methods.\nMoreover, we randomly selected the results from three trials and plotted the obtained Pareto Fronts, as shown in Figure 5. In general, each trial conducted using our method offers a diverse set of trade-offs between error and fairness, allowing decision-makers to understand the characteristics of the task at hand. For example, how much loss in one metric (e.g., accuracy) is required to gain in another (e.g., fairness), which provides valuable insights for decision-making. This flexibility enables quick selection of the most appropriate LLM based on real-world requirements."}, {"title": "5 Conclusion", "content": "In this study, we have proposed a novel framework, FaPareto, for mitigating unfairness in LLMs\nthrough multi-objective evolutionary learning. Our approach can better balance the trade-offs between accuracy and fairness, demonstrating significant improvements over existing methods. By evolving a population of LLMs with fairness-guided diversity generation and fairness-aware evaluation, we have achieved models that not only provide accurate predictions but also exhibit reduced bias.\nOur experiments have demonstrated the potential of our method, showing consistent improvements in both accuracy and the fairness metric $\\Delta TPR$ across multiple trials. We observed a clear trade-off between accuracy and fairness, with a strong negative correlation indicating that enhancing fairness typically leads to a reduction in accuracy. This has highlighted the inherent challenge in optimizing these two objectives simultaneously. Furthermore, our framework's ability to provide a diverse set of trade-offs offers valuable insights for decision-making, allowing for the selection of the most appropriate model based on specific requirements.\nIn future work, we plan to test our approach on a broader range of LLMs and apply it to additional tasks, such as natural language generation. This will allow us to further validate the effectiveness and generalizability of our framework."}]}