{"title": "STATIC IR DROP PREDICTION WITH ATTENTION U-NET\nAND SALIENCY-BASED EXPLAINABILITY", "authors": ["Lizi Zhang", "Azadeh Davoodi"], "abstract": "There has been significant recent progress to reduce the computational effort of static IR drop analysis\nusing neural networks, and modeling as an image-to-image translation task. A crucial issue is the\nlack of sufficient data from real industry designs to train these networks. Additionally, there is no\nmethodology to explain a high-drop pixel in a predicted IR drop image to its specific root-causes.\nIn this work, we first propose a U-Net neural network model with attention gates which is specifically\ntailored to achieve fast and accurate image-based static IR drop prediction. Attention gates allow\nselective emphasis on relevant parts of the input data without supervision which is desired because of\nthe often sparse nature of the IR drop map. We propose a two-phase training process which utilizes a\nmix of artificially-generated data and a limited number of points from real designs. The results are,\non-average, 18% (53%) better in MAE and 14% (113%) in F1 score compared to the winner of the\nICCAD 2023 contest (and U-Net only [1]) when tested on real designs. Second, we propose a fast\nmethod using saliency maps which can explain a predicted IR drop in terms of specific input pixels\ncontributing the most to a drop. In our experiments, we show the number of high IR drop pixels can\nbe reduced on-average by 18% by mimicking upsize of a tiny portion of PDN's resistive edges.", "sections": [{"title": "1 Introduction", "content": "Static IR drop analysis of power delivery network (PDN) is a crucial task to accelerate IC design closure in modern\ntechnology nodes. Today's PDN is a large-sized 3D resistive network spanning across many metal layers. Due to\nparasitics in the PDN, voltage drop is induced between the power pads and cells in the design. A common goal of the\nanalysis is to identify the nodes of the PDN where the voltage drop is higher than an acceptable threshold which is\ntypically referred to as the \u2018hotspots'. Fast IR drop analysis is crucial to identify and guide many rounds of optimizations\noften needed to remove the hotspots, and accelerate design closure.\nStatic IR drop analysis depends on: (1) topology of the PDN and resistance value of each branch in the network; (2)\nlocation of power pads on the PDN; (3) location and amount of current sources representing the cells or modules in\nthe design. The underlying computation is based on solving a system of linear equations with millions to billions of\nvariables [2, 3]. A commercial tool can take several hours to perform one round of analysis for modern designs.\nEarlier work for PDN analysis trade off accuracy for speed by utilizing spatial locality [4], hierarchical methods\n[5], Krylov-subspace methods [6, 7], and multi-grid techniques [2]. More recently, machine learning (ML) -based\ntechniques provide significantly faster and more accurate solutions [8, 9, 10]. But some are limited to incremental\nanalysis and require retraining for each new design [8, 9]. The work [10] is based on Convolutional Neural Networks\n(CNNs) but assumes similar PDN resistance from each cell to power pads, and configuring sizes of tiles on the layout.\nMost recently, [1] models the problem as an image-to-image translation task. Each of the three inputs is expressed as\nan image. A U-Net model which is a convolutional encoder-decoder NN is used to generate a predicted IR drop map.\nThis model naturally handles 2D spatially distributed data without the need to set any tile size, and does not require a\nretraining for each design.\nDespite the impressive success of ML-based approaches, an important, unresolved challenge is lack of sufficient data\nfrom real designs to train the NNs. Recently, [11] created a large training dataset for this purpose. It utilized adversarial\nmodels to artificially generate data which had realistic current maps. Open-source static IR drop solvers were then\nused to generate golden ground-truth. However, there has not been a systematic evaluation of how NNs trained with\nartificially-generated data perform on real designs. The ICCAD 2023 contest featured a problem on this issue [12].\nFinally, an issue with [1] is lack of explainability of a predicted hotspot to its root-causes. As an example, consider that\nthe output generated by [1] is an (2D) image but the PDN is a 3D structure. Predicting hotspots at specific pixels in a\n2D image cannot be traced back to specific edges and layers. Neither can the hotspots be traced to other sources such as\nspecific locations on an input current density map or specific power pads. The degree of contribution of each of these\ncannot be determined. This explainability is a highly desired next step to guide optimizations to reduce the hotspots.\nIn this work, to address the above issues, we first propose AttUNet, a neural network model based on U-Net embedded\nwith attention gates. Attention gates allow selective emphasis on relevant parts of the input data without supervision.\nThis allows the model to focus on useful information during the learning process. Use of attention gates is motivated by\nthe often sparse nature of the IR drop maps. In addition, the U-Net architecture is designed for the task of single-image\nto single-image prediction. However, IR drop is a multi-image to single-image prediction task. To handle this issue, we\nalso embed the U-Net architecture with a new preprocessing convolutional block which introduces an initial per-image\nfilter.\nNext, we leverage a pretrain-finetune strategy. We first train AttUNet using a large volume of artificially-generated\ndata. A few data points from real design are then used in a final fine-tuning. We apply image transformation to augment\nthe training data to improve the robustness of the training.\nFinally, we propose a procedure to generate saliency maps [13] for a predicted IR drop map. These maps allow\nmeasuring degree of contribution of each pixel of each input image to any pixel in the output image. They are found via\nbackward propagation of the output image and computing gradients with respect to each input pixel. A main advantage\nof saliency maps is the very low computational effort to generate them (e.g., seconds) without the need to modify the\nmodel. In contrast, existing techniques for explainability of NNs often require modifying the model, e.g., by adding a\nnew layer which compromises the performance [14, 15, 16].\nThe summary of our contributions is listed below:\n\u2022 We propose AttUNet, a new variation of UNet with embedded attention gates, tailored for the IR drop problem\nwhich is by nature a multi-image to single-image prediction task.\n\u2022 We adapt a 'pretrain-finetune' transfer learning strategy which is able to avoid overfitting and thus handle the\nissue of lack of sufficient data from real designs. We apply a data augmentation step to improve the robustness\nof training.\n\u2022 We propose a procedure to generate saliency maps for a predicted IR drop image which allows measuring the\ncontribution of each input feature to predicted high-drop pixels.\nIn our simulations we use the ICCAD 2023 contest setup which provides a hundred of artificially-generated data points\nand few points of real designs. When measuring performance on real designs only, we show AttUNet is on-average\n18% (53%) better in MAE and 14% (113%) in F1 score compared to the winner of the recent ICCAD 2023 contest (and\nU-Net only [1]). We also use saliency maps for predicted high-drop pixels to guide PDN optimization. We show the\nnumber of predicted high IR drop pixels can be reduced on-average by 18% by upsizing a tiny portion of resistive edges\nof the PDN."}, {"title": "2 Proposed Methodology", "content": "Figure 1 shows the overall flow of our approach. The inputs to the problem are (1) current map which has the distribution\nof current sources (representing modules/cells) in the design; (2) PDN density map which includes the topology of the\nPDN and resistance values of each connection (via/metal layer); (3) map of effective distance to power pads which has\nthe locations of all voltage sources (power pads) in the design.\nThe AttUNet neural network receives these inputs and generates a predicted IR drop map as an image-to-image\ntranslation task. The first step shown in Figure 1(a) is to represent the inputs as images. We use 3 images representing\nthe 3 inputs to the problem as in [1]. Additionally, we translate the spice file describing all the layers into multiple\nimages corresponding to each individual layer. These are explained in Section 2.1. Next, the AttUNet model is trained\nusing the two-step pretrain-finetune strategy which is shown in Figure 1(b). Before training, we first augment input\nimages by applying image transformations to increase the training size which helps improve the robustness of the"}, {"title": "2.1 Image-based Inputs", "content": "The first step prior to inference or training is to translate the inputs to image-based format. We explain how this is\ndone including resizing and normalization operations relative to the setup in the recent ICCAD 2023 contest [12]\nwhich we used in our experiments. For each design, 3 image-based and 1 spice file describe the problem inputs. The 3\nimage-based inputs are described in detail in [1, 12].\nThey correspond to the current map, PDN density map, and effective distance to power pads. (Current map shows\nlocations of current sources. PDN density map reflects the spacing between power stripes per unit area across all metal\nlayers. Map of effective distance to power pads reflects the distance of each PDN node to all power pads, given by\n$\\sum_{i=1}^{N} d_i^{-1}$ where N is the number of pads and $d_i$ is distance to the $i^{th}$ pad.) These 3 inputs are expressed in matrix\nformat where each entry in a respective matrix represents a pixel for an image-based representation. An entry in any of\nthese matrices represents the value of the corresponding entity in the lowermost metal layer in 1\u00b5m \u00d7 1\u00b5m area of the\nchip.\nConverting Spice File into Additional Image-Based Inputs: In addition to the above 3 image-based inputs, in this work,\na spice file is added to describe detailed information across each individual metal and via layer. It contains locations of\nPDN node, value of resistances between the nodes, current source nodes and their values, and voltage source nodes. We\nextract multiple image-based files from this single spice file, where each new file correspond to the data of a specific\nmetal or via layer. These are encoded in a similar matrix format. Each entry (pixel) represents a lumped resistance in\na 1\u00b5m \u00d7 1\u00b5m area. Specifically, for the ICCAD 2023 contest setup, we extract image-based files corresponding to\nresistances of layers M1, M4, M7, M8, M9, and via layers M14, M47, M78, M89."}, {"title": "2.2 U-Net Shaped Network with Attention Gate", "content": "We give a brief overview of the U-Net architecture which was also used in [1] for IR drop prediction. We discuss its\nlimitations to motivate for our new embedded blocks. Figure 2 shows structure of AttUNet. Added blocks compared to\nU-Net are highlighted.\nU-Net structure is well known for its great performance in image segmentation tasks. It has the ability to extract and\nexploit features to generate an image-based output. A U-Net consists of 4 major parts: encoder, bottleneck, decoder\nand skipping connection. Encoder, also called downsampling path, utilizes a sequence of double-convolutional blocks,\neach followed by batch normalization, a rectified linear unit (ReLU) and a max pooling layer. The bottleneck connects\nthe encoder to the decoder. It is a double-convolutional block that receives feature maps which have a high number of\nchannels (i.e., 512 in our implementation).\nThe decoder is also composed of double-convolutional blocks, but instead of having a max pooling layer at the end, it\nhas an upsampling layer at its beginning. A skipping connection exists between each encoder and decoder to pass global\ninformation of inputs to layers that are closer to outputs. Skipping connections allow the model to have a high-level\nview of the features and corresponding location information. The final layer is a 1 \u00d7 1 convolution that is used to map\nfeatures to the desired number of classes.\nThere are several limitations of the U-Net model that make it less effective when dealing with IR-drop prediction.\nFirstly, it is originally designed for problems with single-image input. (The input is one image with multiple channels.)\nIn double-convolutional blocks, all channels are added together to generate new feature maps, which is effective for\nsingle-image problem because these channels have great similarity among each other and share the same features such\nas corners, edges, etc. However, for IR-drop prediction, the inputs to the model are multiple images and the images are\nvery different from each other. Addition across channels entangles different features together and extracts inappropriate\nfeatures by the model which can degrade the prediction performance.\nMoreover, U-Nets works best for image labeling and image segmentation, where the outputs are relatively simple (e.g.,\na single value). In contrast, in IR-drop prediction, a value should be predicted for each pixel in the output image and for\nthe full-chip scale which is normally more than 10,000 pixels."}, {"title": "2.3 Data Augmentation and Model Training", "content": "We use a 'pretrain-finetune' strategy for training AttUNet as shown in Figure 1(b). First, pretrain is done using large\nvolume of artificially-generated data. Next, finetune is done using limited data from real designs. (The provided dataset\nin the ICCAD 2023 contest contains 120 test cases in total, of which 100 are artificially-generated explained in [11, 12],\nand the remaining 20 are real. ) Each test case is represented as image-based inputs as discussed in Section 2.1, and is\nalso accompanied by an image-based golden output voltage file."}, {"title": "Data Augmentation", "content": "As shown in Figure 1(b), we first augment the training data by applying multiple transformations\nto each image-based input which help improve the robustness of the model [19] especially when training data is not\nsufficient. Specifically, we apply the following five operations to each image-based input: vertical and horizontal\nflipping and three (counter-clockwise) rotations as shown in Figure 4 for a sample effective distance map. Next, a new\ntestcase is generated by applying one of the five operations to an existing testcase; For instance, vertically flipping all\nthe image-based representations within a testcase generates a new augmented testcase. This process results in a sixfold\nincrease in the number of testcases, and enhances the diversity and robustness of the dataset [19]. It is applied to both\nartificially-generate data in pretrain phase, as well as real data in finetune phase.\nWe note, horizontal and vertical flippings only rearrange the elements in matrix representation of each input. These\ndo not alter the validity of the results when solving the linear system of equations describing the IR-drop problem.\nSimilarly, rotation operations essentially prompt the network to view the images from different angles, ensuring the\nresults remain valid. The rotation angles are restricted to 90\u00b0, 180\u00b0, 270\u00b0 to avoid cropping and centering operations\nwhich may change the resulting IR-drop."}, {"title": "Model Training", "content": "To train AttUNet, a transfer learning approach is employed. First, the pretraining phase aims to\nmaximize the utilization of generative fake data. (The fake data contains meaningful input-output information because\naccording to [11], an actual open-source IR drop solver is used to generate its golden ground-truth.) Next, finetuning is\nconducted only based on real data.\nThe learning rate and drop rate of the pretrain and finetune phases are listed in Table 1. For the pretraining phase, we\nfirst use a relatively high learning rate of 0.0005 and a high dropout rate from 0.3 to 0.5. For the fine-tuning phase\nwhich is based on real data, we adjust to a finer learning granularity, ranging from 0.0005 to 0.00001. Also, a cosine\nannealing learning rate with restart is applied in the finetune phase which can be expressed as follows:\n$\\eta_t = \\eta_{min} + \\frac{1}{2} (\\eta_{max} - \\eta_{min}) (1 + cos(\\frac{T_{cur}}{T_{max}}\\pi))$\nwhere $\\eta_{max}$ is set to be the initial learning rate 0.0005, $\\eta_{min}$ is the lowest learning rate 0.00001. The parameter $T_{cur}$ is\nthe number of current epochs and $T_{max}$ is the number of total training epochs (600). As shown in Figure 5, learning\nrate oscillates with a decreasing frequency with increase in epochs. This provides a balance between rapid exploration\nof the parameter space at higher learning rates and finer optimization in regions of interest at lower learning rates.\nCosine annealing helps the model converge faster during the early stages of training [20]: By starting with a higher\nlearning rate and gradually reducing it, the optimization process becomes more efficient. The cyclical nature of learning\nrate also allows the model to escape local minima to better explore the loss landscape.\nAs a final consideration during training, we employ a custom loss function. We note, the goal of IR-drop analysis is to\npredict the hotspot locations. Underestimating these is undesirable. Therefore, we define a custom loss function while\ntraining the model, which motivates the model to err on the side of overestimating the IR-drop, even if it results in a\nlarger error. The loss function is set to punish more when a predicted value is less than the actual value. which can be"}, {"title": "2.4 Fast Explainability with Saliency Maps", "content": "A predicted IR-drop map can be used to identify high-drop areas (which are individual pixels in the generated image-\nbased output). A desired next step is to understand which specific inputs to the neural network are most responsible for\nthese high-drop areas. This would be identifying specific image-based inputs, and specific pixels within each image\nsince our problem is a multi-image to single-image prediction task. Among these identified input pixels, we would also\nbe able to compare how much is the contribution of each one to the predicted high-drop pixels at the output.\nThese diagnosis help guide necessary optimizations to create a 'cooler' IR-drop map. For example, if specific pixels in\nthe current map are found to have the highest contribution then it suggests that changing the floorplan of modules or\nplacement of cells may be most appropriate to reduce the current demand at those locations. If the highest contributors\nare specific PDN edges on specific metal layers, then slight upsizing of these edges may be most appropriate. Such a\nexplainability has not been explored in any prior work for static IR-drop prediction and is a natural and important next\nstep.\nHowever, AttUNet, like most deep neural networks, operates more as a 'black box' which makes it difficult to\ncomprehend the reasoning behind having specific output predictions. Existing techniques for adding explainability to\na deep neural network often require changing the network structure, for example by adding extra layer(s) which can\nin turn compromise the performance [14, 15, 16]. However, saliency maps are available tools which allow gaining\nsome insights into model behavior very quickly (e.g., seconds in our problem). In particular, for our problem which is\nan image-to-image translation task, we show the insight gained by saliency maps can be helpful for diagnosing the\npredicted high-drop pixels.\nFigure 1(d) shows the process of generating the saliency maps. The first step is identifying high-drop pixels from\nthe predicted IR-drop map. This relies on the designer to know how much IR-drop is considered acceptable given\nthe desired specifications. Next, a back-propagation is done from this high-drop-only output map to each individual\npixel on the input side. Finally, a gradient is computed with respect to each input. Since all image-based inputs are\nnormalized to [0,1] range, the corresponding gradients are comparable across the pixels of different inputs. In the end a\nsaliency map is generated for each image-based input, as shown in Figure 1(d).\nFormally, let $F_k : \\mathbb{R}^{C \\times h \\times w} \\rightarrow \\mathbb{R}$ denote the function describing how the model generates the $k^{th}$ pixel in the output\nimage from the $C$ (single-channel) input images. Each image (inputs or the output) has a height $h$ and width $w$\nin AttUNet. Let $F_k(X) = y_k, X \\in \\mathbb{R}^{C \\times h \\times w}$ denote all input images as a 3D matrix representing a collection of\nindividual single-channel 2D inputs. Also $y_k \\in \\mathbb{R}$ is a pixel $k$ of the generated output image.\nDue to the complex nature of the neural network, $F_k$ is a highly non-linear function of $X$. However, given an input $X_o$,\n$F_k (X_o)$ can be approximated with a linear function in the neighborhood of $X_o$ by computing the first-order Taylor\nexpansion [13]:\n$F_k(X) \\approx w^T X + b$\nwhere $w$ is the derivative of $F_k$ with respect to the input $X$ at $X_0$:\n$w = \\frac{\\partial F_k(X)}{\\partial X}|_{X_0}$\nThe magnitude of elements of $w$ defines the importance of the corresponding pixels of $X$ for the $k^{th}$ pixel in output\nimage.\nThe saliency map $S \\in \\mathbb{R}^{C \\times h \\times w}$ is computed in the similar way. For a subset of output pixels {$y_k$}, $k = 1, 2, . . ., K$\n(for example representing the predicted high-drop pixels), a (combined) saliency map $S$ is generated for input $X$\nrepresenting all image-based inputs. This is done by computing the average gradient regarding the subset of output\npixels:\n$S = \\frac{1}{K} \\sum_{k=1}^{K} \\frac{\\partial F_k}{\\partial X}$\nThe above will have dimension $C \\times h \\times w$ and further be broken into individual saliency maps representing each\ninput-based image."}, {"title": "3 Experimental Results", "content": "The AttUNet model is implemented using Python 3.9 and under Pytorch 2.0.1 framework and is trained on two NVIDIA\nGeForce RTX 2080Ti GPUs. Table 1 describes the model hyperparameters and training parameters. Layer IDs are\nshown in Figure 2.\nWe used the data provided by the ICCAD 2023 contest [12]. As mentioned earlier, 120 test cases were provided, 20 of\nwhich corresponded to real designs and the rest were artificially-generated based on [11]. All 100 artificial data were\nused in the pre-train phase. From the 20 real designs, 10 were used in the fine-tune phase. The remaining 10 designs\nwere used to test the model. To be comparable to the contest results, we use the same train/test split as the contest, that\nis using the same ten real chip data as testing dataset, and the remaining data for training.\nAs explained earlier, we pretrain AttUNet on augmented fake data and fine-tune on augmented real data. The\naugmentation enlarges the dataset by 6 folds, resulting in 800 pretrain data points and 80 fine-tune data points. The\npretraining run time is 17.4 hours and fine-tuning is 4.6 hours. It is important to note that this is a one-time cost; the\ntrained model can be applied directly to different chip designs and does not need any modifications. The average\ninference time across the designs in the testing dataset is 5.37 seconds.\nWe compare AttUNet with the UNet model in [1] which is named as IREDGe. We implemented this model based on\nthe hyperparameters in [1]. For a fair comparison using the contest setup, we trained IREDGe using the same pretrain\nand fine-tune stages and same training data. For IREDGe, we used all training parameters reported in [1] except the"}, {"title": "3.1 Prediction Quality", "content": "Table 2 compares our F1 score and MAE to [1] and the contest winner. The bold entries indicate the best MAE and F1\nscore in each row. First, AttUNet significantly outperforms IREDGe (on-average 113.3% in F1 score and 52.7% in\nMAE) under the contest setup. Furthermore, AttUNet outperforms ConvNeXtV2 in many cases. It is better in 9 out\nof 10 designs in at least one metric (MAE or F1 score). It achieves the lowest average MAE (0.110 mV) and highest\naverage F1 score (0.520) among all test cases which is about 18.1% improvement in MAE and 14.3% in F1 score\ncompared to ConvNeXtV2. Notable improvements are in test cases 13 and 14, in which ConvNeXtV2 has a 0.000 F1\nscore, indicating an inability to highlight the severe IR drop regions. AttUNet has F1 scores of 0.895 and 0.881 for\nthese.\nFigure 6 shows the output of AttUNet in testcase 9. As shown in (a) and (b), our prediction closely aligns with the\nground truth in spotting the high-drop areas. Figures (c) and (d) show the map and distribution of MAE. Location of the"}, {"title": "3.2 Training Efficiency", "content": "Table 3 shows the average training (testing) MAE and F1 scores of IREDGE, AttUNet at different training stages.\nAfter fine-tuning, AttUNet achieves an MAE of 0.0520 mV and F1 score of 0.541. The MAE is significantly smaller\ncompared to IR-drop values which normally range from 1-10 mV. Compared to IREDGE, AttUNet has better training\nand testing performances in terms of both average MAE and F1 score, indicating the effectiveness of learning and fitting\nthe patterns and relationships present in the training data.\nGiven the inadequacy of real data, pre-training with generative fake data serves as a preventive measure against the\npotential risk of over-fitting. Generative errors (gaps between training MAE and testing MAE) decreases from 0.31 to\n0.06 after fine-tuning, showing a great reduction in over-fitting risks. Also, F1 score after fine-tuning is at the same\nlevel in training and testing datasets. In comparison, IREDGe shows much bigger generative errors, which are 0.91 after\npre-training and 0.10 after fine-tuning, indicating a significant possibility of over-fitting.\nFigure 7 shows the output of AttUNet in the early stage of fine-tuning, specifically after 50 epochs. When the model is\npre-trained (middle figure), the output shows clear boundaries that successfully separate regions with high IR-drop. In\ncontrast, the model without pre-training (shown in right) lacks these distinctive features."}, {"title": "3.3 Explainability with Saliency Maps", "content": "In this experiment, we first generate a saliency map for each image-based input. Recall, this requires identifying\nhigh-drop pixels from the predicted IR-drop image at the output. Specifically, for each testcase, we identify the\nmaximum predicted voltage drop across all pixels which we denote by $D_{rmax}$. We then extract high-drop output pixels\nas those with a drop higher than $D_{rth} = 0.9 \\times D_{rmax}$.\nOnce saliency maps are obtained, we identify the top $K$ pixels from the input images which have the highest contribution\nto these high-drop output pixels. To identify the top $K$ input pixels, we first compute an average saliency for each input\nimage by averaging the top $K$ pixels. The input image with the highest contribution is the one with highest average\nsaliency. Next, within that image we identify the top $K$ pixels which have the highest saliency values.\nNext we show how these top $K$ input pixels can guide optimization to generate a new IR drop map with fewer high-drop\npixels. The optimization that we considered in this experiment is upsizing the PDN wire branches corresponding to the\ntop $K$ input pixels. Therefore, we only considered the saliency maps of 5 metal layers (M1, M4, M7, M8, M9) and 4\nvia layers (M14, M47, M7M, M89). Other optimizations such as change in floorplan was not possible due to lack of\nmore detailed layout information. Also, our emphasis in this experiment is to only show the impact of saliency maps.\nExploring the best optimization is outside the scope of this work.\nTable 4 shows the results for $K = 100, 300, 500$. We note these values of $K$ represent a tiny portion of each $512 \\times 512$\ninput image. Our optimization mimics upsizing some PDN wire widths (thus reducing their resistances) which\ncorrespond to the top $K$ pixels. Upsizing is mimiced by reducing the corresponding pixel values (in the corresponding\nimage) by 10%. For example, in testcase 7 the top contributor layer is M1. It has 1574 high-drop pixels in the predicted\noutput image. We identify only $K$ pixels in the image input corresponding to M1 for optimization.\nNext, we feed this slightly-optimized input back to AttUNet to predict a new IR-drop map. Using the same $D_{rth}$ as\nearlier, we then identify the number of high-drop pixels in the optimized map. We report percentage reduction in the\nnumber of high-drop pixels in Table 4. On-average the number of predicted high-drop pixels are reduced by 18.48%,\n37.28%, 49.05% for k = 100, 300, 500, respectively.\nTo show identifying these top $K$ pixels for optimization is not trivial, we compare with an alternative approach in which\nall input pixels corresponding to the same locations of the predicted high-drop pixels are reduced by 10% across all\nlayers (all input images corresponding to all the 9 layers). For example, for testcase 7, we scale 1547 pixels in each of 9\ninput images at the pixels where the high-drop was predicted. As can be seen in the last column of the table (labeled\n'w/o saliency'), the number of predicted IR drop pixels actually increase on-average. Therefore saliency maps help to\nmeaningfully reduce the number of high-drop pixels and by a significant amount, and using only a tiny portion of the\nPDN edges."}, {"title": "4 Conclusions", "content": "We presented a computer-vision based approach to predicting static IR drops in power delivery networks (PDNs) using\nthe AttUNet model, an advanced variant of the U-Net architecture enhanced with attention mechanisms. Our findings\ndemonstrate that AttUNet significantly surpasses previous models and 2023 ICCAD contest winner in prediction\nquality. The use of vector concatenation-based attention gates allows AttUNet to selectively prioritize significant areas"}]}