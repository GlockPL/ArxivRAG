{"title": "Repository Structure-Aware Training Makes SLMs Better Issue Resolver", "authors": ["Zexiong Ma", "Shengnan An", "Zeqi Lin", "Yanzhen Zou", "Bing Xie"], "abstract": "Language models have been applied to various software development tasks, but the performance varies according to the scale of the models. Large Language Models (LLMs) outperform Small Language Models (SLMs) in complex tasks like repository-level issue resolving, but raise concerns about privacy and cost. In contrast, SLMs are more accessible but underperform in complex tasks. In this paper, we introduce ReSAT (Repository Structure-Aware Training), construct training data based on a large number of issues and corresponding pull requests from open-source communities to enhance the model's understanding of repository structure and issue resolving ability. We construct two types of training data: (1) localization training data, a multi-level progressive localization data to improve code understanding and localization capability; (2) code edit training data, which improves context-based code editing capability. The evaluation results on SWE-Bench-verified and RepoQA demonstrate that ReSAT effectively enhances SLMs' issue-resolving and repository-level long-context understanding capabilities.", "sections": [{"title": "1 Introduction", "content": "Language Models (LMs) have been applied to various software development tasks (such as code completion (Cop, 2022), code generation (Chen et al., 2021; Li et al., 2022; Ma et al., 2024; Jiang et al., 2024), and program repair (Jin et al., 2023)), and many LM-based s have been integrated into real-world development processes. Language models can be categorized into two types based on their scales: (1) Large Language Models (LLMs), with a parameter size of 100B or more, which are typically commercial, closed-source models (cha, 2022; Cla, 2024; gpt, 2023). (2) Small Language Models (SLMs), with a parameter size of 13B or less, which are usually open-source models (Team, 2024; Guo et al., 2024; Team et al., 2024; Touvron et al., 2023; Lozhkov et al., 2024). LLMs have stronger abilities but are usually only accessible via API, raising concerns about privacy leakage. SLMs, on the other hand, can be deployed on consumer-grade GPUs (Team et al., 2024) but have poorer performance in more realistic, repository-level software development tasks (i.e., issue resolving (Jimenez et al., 2024)).\nDue to the poor performance of SLMs on repository-level tasks (Liu et al., 2023), mainstream repository-level automatic programming assistants (Cognition, 2024) all employ LLMs. SWE-Bench (Jimenez et al., 2024) is a benchmark to evaluate the ability of automatic programming assistants to resolve issues. Many works have employed agent-based or pipeline-based systems to utilize LMs for repository-level issue resolving, and all the top-performing works on the SWE-Bench leaderboard employ LLMs (gpt, 2023; Cla, 2024).\nIssue resolving requires language models to understand repository-level code and model long-range dependency. Given a lengthy code context and an issue, LMs are required to locate the code segment relevant to the issue, and generate code edits for the segments. Due to the limitations of model size and training data volume, neither the pre-training nor the instruction-tuning (Luo et al., 2023) phases have endowed SLMs with the capability to perform such complex tasks effectively. There are rich structural information in open-source repositories, which is not utilized during training process of SLMs. Consequently, a pressing research question arises: Could we leverage the structural information in open-source repositories to enhance the repository-level understanding and issue-resolving capabilities of SLMs?\nIn this paper, we propose Repository Structure-Aware Training (ReSAT), to enhance the repository-level code understanding and issue re-"}, {"title": "2 Background", "content": "In this section, we will provide a brief introduction to SWE-Bench, and introduce two existing issue resolving frameworks.\n2.1 SWE-Bench\nSWE-Bench (Jimenez et al., 2024) is a benchmark designed to test the capabilities of language models (LMs) in solving real-world software engineering problems. It consists of tasks derived from real GitHub issues and their corresponding pull requests across 12 popular python repositories. LMs or LM-based programming assistants are provided with an issue description and a repository, and expected to generate code edits to the repository that resolve the issue. Each issue is associated with test cases that can be executed in a Docker environment, and the evaluation is based on whether the edited repository can pass the test cases. SWE-Bench differs from traditional code generation benchmarks (Chen et al., 2021; Peng et al., 2024; Austin et al., 2021) by focusing on realistic software engineering tasks that require repository-level code understanding,"}, {"title": "3 Approach", "content": "In this section, we will introduce the details of Repository Structure-Aware Training, including data scraping, localization data construction, code edit data construction and training details.\n3.1 Data Scraping\nWe leverage open-source repositories to construct structure-aware training data, addressing SLMs' lack of repository-level code understanding and issue resolving ability. As shown in Figure 2(1), to construct ReSAT training data, we first select high-quality open-source repositories based on their download numbers, star count and Pull Request (PR) count. Then we scrape the resolved issues and corresponding PRs from the selected repositories.\nRepository Selection. We select the most popular and active open-source python projects to scrape data and construct the ReSAT training dataset. Following previous work (Jimenez et al., 2024), we assume that package quality is positively correlated with the number of downloads, scrape the top 5,000 most downloaded PyPI packages from the Top-pypi-packages website \u00b9, and filter out packages that do not contain corresponding GitHub repository on PyPI webset or without licenses that allow for free software use. To avoid data leakage, we exclude repositories that appeared in the test set of SWE-Bench and RepoQA. To ensure the repositories are sufficiently active and could provide a substantial number of resolved PRs, we filter out repositories with fewer than 1,000 stars or fewer than 1,000 Pull Requets (PRs). In the end, we obtain 229 open-source repositories.\nIssues and PRs Scraping. For the selected repositories, we scrape their resolved issues and the corresponding PRs to construct the ReSAT training data. We utilze GhApi \u00b2 to scrape all the PRs in the repositories. Given a PR, we use regular expressions to extract the issue numbers mentioned in the PR title and commit messages, which are considered as the issues resolved by the PR. We filter out PRs that are not merged to the main branch and those do not mention any issues. As a result, we obtain 44,088 PRs in total to construct the ReSAT dataset.\n3.2 Localization Data Construction\nWhen resolving an issue, the most challenging part is accurately locating the code snippet that requires modification within a large repository. As shown in Figure 2(2), inspired by the design of the localization module in previous work (Xia et al., 2024; Jimenez et al., 2024; Yang et al., 2024), we construct a multi-level localization dataset: (1) File"}, {"title": "3.2.1 File Localization", "content": "File localization training enhances the SLMs' understanding of the high-level architecture of the repository, enabling it to perform an initial rough localization based on the issue description. We first clone the repository locally, then employ os.walk \u00b3, to extract the repository file structure. We utilize the issue and repository structure as inputs, with the full names of the modified files in the PR as outputs. To improve the quality of the training data, we excluded non-Python files and test scripts from both the repository structure in the input and the filenames in the output. Template 1 shows the data template \u2074 used for file localization training."}, {"title": "3.2.2 Function Localization", "content": "Function localization training can improve the SLMs' performance on fine-grained localization"}, {"title": "3.2.3 Line Localization", "content": "Line localization training enhances the SLMs' ability to accurately locate the specific lines of code that require to be modified to resolve the issue. In earlier stages, the identified functions may still be relatively long. Moreover, the file and function localization phases are based only on the project and file structure, lacking detailed code content information. We employ line localization training to correct the errors in function localization caused by missing information and leverage code details for precise locating. Specifically, we extract the modified lines in the PR and their corresponding functions. Line localization takes the issue description and function content as inputs and outputs the modified lines from the PR. Template 3 shows the data template used for line localization training."}, {"title": "3.3 Code Edit Data Construction", "content": "Code Edit training can enhance the SLMs' ability to modify code snippets based on the issue. The input for Code Edit consists of the issue and the localized code snippet, while the output is the code edits of the corresponding PR. Following previous work (Xia et al., 2024; Yang et al., 2024), we employ the Search/Replace Edit format. The Search/Replace format consists of two main parts: 1) Search: the original code snippet that need to modify, and 2) Replace: the new code snippet after editing. Compared to directly generating edits in diff format, the Search/Replace format is easier to generate, and can be converted into the diff format through post-processing (e.g., employing difflib \u2076). Template 4 shows the data template used for code edit training."}, {"title": "3.4 Training Details", "content": "To further improve the quality of the training data, we filter out PRs that do not modify Python files and samples with a context length greater than 32k tokens. In the end, we construct 80,260 training samples from the 229 open-source repositories. We fine-tune CodeQwen1.5-7B-Chat and Deepseek-Coder-6.7B-Instruct through FastChat (Zheng et al., 2023) framework. We set the max length of tokenizer for both models as 32k, and apply linear rope scaling to Deepseek-Coder-6.7B-Instruct to scale up its max length. The training process is conducted on 8x 80G A100 GPUs with full sharding strategy and CPU offload strategy implemented by Pytorch FSDP \u2077. We also utilize flash-attention-2 (Dao, 2024) to reduce memory overhead and speed up the training process. We set the global batch size to 128 and train for 2 epochs. We apply cosine learning rate decay with a maximum learning rate of 5e-6 and 3% warm-up steps. The entire training process takes about 11 hours for each model."}, {"title": "4 Evaluation", "content": "4.1 Datasets\nTo evaluate the effectiveness of ReSAT on repository-level code understanding and issue resolving, our experiments are mainly conducted on two datasets: RepoQA and SWE-Bench-verified. RepoQA (Liu et al., 2024) is a benchmark designed to evaluate the ability of LMs to understand"}, {"title": "4.2 Models", "content": "In our experiments, we apply ReSAT training to two open-source code SLMs: Deepseek-Coder-6.7B-Instruct and CodeQwen1.5-7B-Chat.\nDeepseek-Coder (Guo et al., 2024). We employ Deepseek-Coder-6.7B-Instruct, a code SLM released by the deepseek team. It has demonstrated impressive performance on basic code generation tasks, with 78.6% accuracy on HumanEval and 73.2% accuracy on MBPP. Deepseek-Coder model is pre-trained on repository-level code corpus by employing a window length of 16k and an extra fill-in-the-blank (Guo et al., 2024) task, making it support repository-level code completion and infilling. But it still performs poorly on repository-level code understanding and issue resolving tasks, successfully resolving only 0.22% issues in the original SWE-Bench.\nCodeQwen (Team, 2024). We employ CodeQwen1.5-7B-Chat, a specialized code SLM built upon the Qwen1.5-7B language model. It has outperformed larger models in basic code generation tasks, achieving 83.5% accuracy on HumanEval and 77.7% accuracy on MBPP. It possesses the capability to understand and generate long-context code with up to 64k tokens, but its performance still remains suboptimal on repository-level tasks, successfully resolving only 0.89% issues in the original SWE-Bench."}, {"title": "4.3 Inference Framework", "content": "To directly reflect the ability of ReSAT-trained SLMs to solve issues in real-world applications, we apply ReSAT-trained SLMs on two existing issue resolving frameworks.\nAgentless (Xia et al., 2024), a straightforward LM-based framework for repository-level issue resolving. It contains two parts: (1) Localization: LMs are utilized to identify the code snippets that are responsible for the issue by hierarchically narrowing down candidate files, classes, functions, and even lines of code. (2) Code edit generation: LMs are required to generate potential edits to resolve the issue.\nRAG-SWE (Jimenez et al., 2024), first retrieves the files that are most similar to the issue description, then uses the file content and the issue as prompt for LMs to generate code edits. Following previous work (Jimenez et al., 2024), we utilize BM-25 as our retriever, and retrieve the top-3 most similar files from the repositories. Then we apply the same generation phase as Agentless."}, {"title": "4.4 Metrics", "content": "When we evaluate issue resolving frameworks with ReSAT-trained SLMs on SWE-Bench-verified dataset, to fairly and thoroughly evaluate impact of ReSAT on issue resolving, we apply four metrics: % Resolved, % FileHit, % FuncHit, and % Line-Hit. % Resolved is the proportion of test samples that the issue resolving frameworks can successfully generate code edits based on issues and pass all test cases. % FileHit refers to the proportion of files modified in the PRs that are successfully predicted by the SLMs during file localization. % FuncHit refers to the proportion of functions modified in the PRs that are successfully predicted by the SLMs during Function localization. % Line-Hit refers to the proportion of lines modified in the PRs that are successfully predicted by the SLMs"}, {"title": "4.5 Main Results", "content": "ReSAT enhances issue resolving performance. Table 1 shows the evaluation results on SWE-Bench-verified before and after ReSAT training. For example, when employing the Agentless framework, ReSAT training improves the % Resolved for Deepseek-Coder and CodeQwen by 4.8% and 6.4%. ReSAT training also improves the File, Function, and Line-level localization performance. When employing the Agentless framework, ReSAT improves Deepseek-Coder's %FileHit, %FuncHit, and %LineHit by 4.6%, 23.2%, and 10.8%. The %FileHit performance gap between RAG-SWE and Agentless with ReSAT-CodeQwen (24.8% v.s 53.4%), also demonstrates that employing ReSAT-trained SLMs for localization yields higher accuracy, which further leads to Agentless successfully resolving more issues than RAG-SWE after ReSAT training.\nReSAT enhances repository-level code understanding performance. As shown in Table 2, after ReSAT training, both models demonstrate improved performance on RepoQA. CodeQwen's average accuracy increases from 62.8 to 65.4, and Deepseek-Coder's average accuracy improves from 10.6 to 15.0. The results in Table 2 also indicate that training exclusively on ReSAT data in Python can still improve performance in other languages. After ReSAT training, SLMs achieve higher accuracy in most languages."}, {"title": "4.5.1 Ablation Study", "content": "To explore the impact of ReSAT's localization and code edit data on repository-level code understanding and issue-resolving capabilities, we conduct ablation experiments on ReSAT's training data using CodeQwen.\nFigure 3 shows the evaluation results of CodeQwen trained on single dataset. The results on SWE-Bench-verified indicate that both types of training data have a positive effect on the issue-resolving capability of SLMs. Compared to original CodeQwen, training with only code edit data and localization data improves the % Resolved by 1.0 and 4.0. Combining both types of data results in stronger issue-resolving capabilities than using single data, using only code edit data and localization data reduced the % Resolved by 5.4 and 2.4. Localization Hits on SWE-Bench-verified and Accuracy on RepoQA indicate that localization data effectively enhances SLMs' repository-level code understanding. Localization data has greater impact on issue-resolving performance compared to code edit data, further supporting the hypothesis that training focused on repository structure understanding can effectively improve issue-resolving capabilities."}, {"title": "4.6 Case Study", "content": "We illustrate the issue-resolving performance of ReSAT-trained CodeQwen through two examples."}, {"title": "5 Related Work", "content": "5.1 Issue Resolving with LMs.\nThe rapid development of language models (LMs) has made automated issue resolving possible. There are two types of automatic issue resolving systems: agent-based and pipeline-based.\nAgent-based systems (Wu et al., 2023; Yang et al., 2024; Chen et al., 2024; Hong et al., 2023; Ishibashi and Nishimura, 2024; Luo et al., 2024) equip LMs with tools for decision-making and iterative actions. Yang et al. (2024) proposed the SWE-Agent and designed an Agent-Computer Interface (ACI) that is more suitable for LMs than"}, {"title": "5.2 Training-Data Synthesis for LMs.", "content": "After the pre-training of LMs, a large amount of human-annotated question-answer data is required for instruction tuning, which incurs significant human labor costs. Some work has attempted to generate training data through data synthesis. Wang et al. (2022) proposed Self-Instruct, which maintains an instruction pool. Instruction examples are randomly selected from the pool, and LMs are prompted to generate new instructions based on the examples, thereby continuously expanding the diversity of instructions in the training data. Xu"}, {"title": "6 Conclusion", "content": "In conclusion, we propose ReSAT, a repository structure-aware training approach. ReSAT constructs two types of training data: localization data and code edit data. Evaluation results on SWE-Bench-verified and RepoQA demonstrate that ReSAT effectively enhances SLMs' issue-resolving and repository-level long-context understanding capabilities. In future work, we will further expand the scale of ReSAT data and improve the efficiency of leveraging open-source repositories, continuing to enhance the performance of SLMs-based automatic programming assistants."}, {"title": "7 Limitations", "content": "Due to limitations in computational resources, we only conducted training on two models with 7 billion parameters. We believe that ReSAT can also enhance the performance of other LMs with different parameter sizes on repository-level tasks, we leave it for the furture work.\nSince SWE-Bench only consists of Python issue-resolving tasks, our experiments only verified that ReSAT improves issue-resolving capabilities for Python repositories. Given that projects in other programming languages can also provide the structural information required by ReSAT, we believe that ReSAT can improve issue-resolving performance in other languages. Moreover, our results on RepoQA also show that ReSAT training with Python improves performance in other languages.\nDespite the effectiveness of ReSAT in enhancing the issue-resolving capabilities of SLMs, there remains a significant gap compared to LLMs like GPT-40. Future work should further enhance the repository structure understanding and code editing capabilities of SLMs to bridge this gap.\nReSAT training requires a certain amount of computational resources. Considering the improvement ReSAT brings to the issue-resolving capabilities of SLMs, we believe this consumption is worthwhile. Future work should focus on more environmentally-friendly research, exploring how to achieve improvements in issue-resolving capabilities with less computational resource consumption."}]}