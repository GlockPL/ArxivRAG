{"title": "MAPWise: Evaluating Vision-Language Models for Advanced Map Queries", "authors": ["Srija Mukhopadhyay", "Abhishek Rajgaria", "Prerana Khatiwada", "Vivek Gupta", "Dan Roth"], "abstract": "Vision-language models (VLMs) excel at tasks requiring joint understanding of visual and linguistic information. A particularly promising yet under-explored application for these models lies in answering questions based on various kinds of maps. This study investigates the efficacy of VLMs in answering questions based on choropleth maps, which are widely used for data analysis and representation. To facilitate and encourage research in this area, we introduce a novel map-based question-answering benchmark, consisting of maps from three geographical regions (United States, India, China), each containing 1000 questions. Our benchmark incorporates 43 diverse question templates, requiring nuanced understanding of relative spatial relationships, intricate map features, and complex reasoning. It also includes maps with discrete and continuous values, encompassing variations in color-mapping, category ordering, and stylistic patterns, enabling comprehensive analysis. We evaluate the performance of multiple VLMs on this benchmark, highlighting gaps in their abilities and providing insights for improving such models.", "sections": [{"title": "Introduction", "content": "Vision-Language Models (VLMs) have demonstrated impressive capabilities in tasks requiring joint understanding of visual information and natural language. They have achieved significant success in areas like visual question answering (VQA) (Salaberria et al., 2023; Chaudhry et al., 2020), image generation (Zhao et al., 2024), and multimodal sentiment analysis (Yi et al., 2024). However, when applied to map-based question answering, the reasoning abilities of these models remain largely unexplored (Chang et al., 2022).\nChoropleth maps, which use varying shades or colors to represent geographical data, present a unique challenge (Chang et al., 2022). While humans can readily grasp the spatial patterns and information conveyed by these color variations, their interpretation poses a significant challenge for visual language models and other analytical tools. This difficulty arises from the inherent challenge of translating visual data represented by different colors or shades into simpler, tabular formats.\nThis research addresses this gap by analyzing the performance of VLMs in answering questions related to choropleth maps representing different geographical regions (Figure 1). We aim to answer the following research questions:\n(RQ1) How effectively can VLMs answer questions about Choropleth maps of different geographical regions?\n(RQ2) What prompting strategies can improve the performance of models for Map Visual Question Answering (Map-VQA)?\n(RQ3) What biases are present in these models with regards to Map VQA?\n(RQ4) How effectively do these models attend to the provided map when performing visual question-answering tasks?"}, {"title": "The MAPWise Dataset", "content": "This section details the creation process of the MAPWise dataset, including data gathering, manual question creation, and dataset validation."}, {"title": "Dataset Creation", "content": "Data Sources. The MAPWise dataset was created using data from three countries: India, USA, and China. We have meticulously chosen reliable sources to gather socioeconomic and demographic statistics for each country, as described below.\ni) For India, we sourced data from the Reserve Bank of India's \"Handbook of Statistics on Indian States.\" This resource provides extensive data across various periods, including details such as state-wise cold storage capacity, rural population figures, and the area of non-food grains like cotton.\nii) For USA, the primary data source was the \"Kaiser Family Foundation\", which specializes in healthcare statistics. This includes information on health insurance coverage for adults without dependent children, age-adjusted suicide rates, and weekly COVID-19 vaccine allocations.\niii) For China, we obtained data from the \"National Bureau of Statistics of China.\" This source provides data such as household consumption expenditure, urban unemployment rates and natural growth rate.\nMap Variations. The dataset consists of maps representing data in two primary forms: discrete, where the legend is divided into distinct groups and continuous, where the legend is distributed over a spectrum. The maps also include variations in the presence or absence of annotations, which provide additional contextual information. Our dataset also includes maps with black-and-white textured patterns or hatches for discrete data, different colormap variations (light, dark, and gradient scales), and varying paper background colors (white and grey). These variations test the models' capability to handle diverse visual presentations. We generated maps with annotations, without annotations, and with hatching for each country using the Plotly library.\nQuestion Generation. To create a comprehensive and insightful benchmark, we designed question templates with varying levels of difficulty, ranging from simple yes/no questions to more complex region association questions that required reasoning based on relative locations.\nThe dataset includes three major question types: Binary questions, which require a simple yes or no answer based on the map; Direct Value Extraction questions, which ask for a specific numerical or nominal value related to a particular region or the legend; and Region Association questions, which involve identifying or counting regions meeting some specific criteria, often requiring geospatial reasoning and reasoning about relative regions."}, {"title": "Experimental Evaluation", "content": "This section outlines our experimental setup: we selected a mix of closed-source and open-source Vision-Language Models (VLMs) and Multimodal Large Language Models (MLLMs) for a comprehensive analysis. These models were tested with various prompting techniques, and we developed an evaluation metric to assess different answer types."}, {"title": "Baseline Models", "content": "Closed-Source MLLMs. For analysis on closed source models, we used Gemini 1.5 Flash (Gemini, 2024) and GPT-40 (OpenAI, 2024). These models are known for their advanced features and proprietary implementations.\nOpen-Source VLMs. We selected CogAgent, InternLM XComposer 2, Idefics 2, and Qwen VL. CogAgent-VQA (Hong et al., 2023) is an 18-billion-parameter VLM specializing in GUI understanding and navigation. InternLM-XComposer2 (Dong et al., 2024), an adaptation of InternLM2-7B (Cai et al., 2024), excels in producing high-quality long-text multimodal content and reasoning within visual-language understanding contexts. QwenVL (Bai et al., 2023b), a generalist 7-billion-parameter VLM built on top of Qwen-LM (Bai et al., 2023a), leverages adapted visual encoders and general and multi-task pretraining. These models were chosen due to their accessibility and contributions to the research community, each offering distinct approaches to processing and interpreting visual information."}, {"title": "Prompting Strategies", "content": "We evaluated the baseline models under two distinct prompting settings:\n1. Zero-Shot Chain-of-Thought Prompting (COT). We leverage the Chain-of-Thought (Wei et al., 2023) prompting, presenting the VLM with a map and a question, prompting it to reason through the steps leading to its final answer.\n2. Explicit Extraction and Reasoning (EER). Here, we created a custom prompt that explicitly outlined the reasoning steps the model should follow to answer the specific question. This prompt was broken down into four distinct reasoning steps:\n Extraction of Regions. The model was prompted to identify the regions whose data was required to answer the question.\n Extraction of Relevant Places. Next, the model was instructed to extract the specific locations or places associated with the identified regions.\n - Extraction of Values from Legend. The model was then directed to extract the values corresponding to those regions from the map's legend.\n Reasoning based on Extracted Values. Finally, the model was prompted to reason based on the extracted values to arrive at the final answer.\nThis approach helped break down the reasoning process into smaller, more manageable steps, preventing the model from becoming overwhelmed and guiding it towards a more focused and structured reasoning process.\nDuring the evaluation, all models were given the same prompt in order to fairly and consistently assess their ability to reason. The prompts used have been presented in the Appendix."}, {"title": "Evaluation Details", "content": "The evaluation process adapts to various answer types within in the dataset by employing tailored metrics and criteria for each specific answer type. Additionally, normalization was applied wherever necessary to ensure consistency and accuracy in the assessment.\nFor binary yes/no and integer count answers, we implemented an exact match criterion and accuracy as the evaluation metric. For single-word answers, as some questions have multiple applicable responses, we employed the recall metric for better evaluation. For state names, a valid answer could be either a two-digit state code or the full state name. For ranges, we first normalized the ranges to absolute values (e.g. 1k to 1000) and then compared them. For discrete maps, only exact match was expected, whereas for continuous maps, we gave a full score of 1 for exact match and a partial score of 0.5 for overlapping responses.\nFor list type answer, we used precision and recall metrics because predicted lists often contained irrelevant states (false positives) and missed relevant states (false negatives).\nFor rank-type answers, we prompted the model to assign ranks to states based on map values. However, due to the difficulty in accurately distinguishing shades, models frequently assigned states to wrong shades, resulting in multiple states sharing the same rank despite differing shades. Additionally, for some questions, ground truth involved multiple states in the same rank because of states having identical shades or patterns. To evaluate this, we designed a \"Rank-wise Precision (RWP)\u201d method, computing precision for each rank and then averaging across all ranks. We also evaluated other ranking metrics, including Mean Reciprocal Rank (MRR) and Mean Average Precision (MAP), as detailed in Appendix C.\nNote for Open Source VLMs. Smaller models, like QwenVL, CogAgent, and InternLM, faced challenges in producing answers in the desired format. To address this, we used an \"LLM as an Extractor\" approach, using Gemini 1.5 Flash to extract answers from their outputs. Manual verification of 150 samples confirmed that Gemini primarily acted as a extracting and formatting tool, preserving the original model's answer in 138 cases. In the remaining 12 cases, the original model had not clearly answered the question, for which Gemini reported \"Answer cannot be extracted\"."}, {"title": "Results and Analysis", "content": "MAPWise: A Challenging Benchmark. The MAPWise dataset presents a compelling benchmark for evaluating the reasoning abilities of current Vision-Language Models (VLMs). As shown in Table 3, models consistently perform significantly worse than the human baseline, particularly with questions requiring intricate reasoning, such as counting or providing a list of regions where the difference in scores is close to 50% on average. This substantial performance gap highlights a significant limitation in the reasoning capabilities of existing VLMs, underscoring the need for further research to bridge this gap.\nModel Performance Comparison. While model performance varied across different answer types and countries, GPT-40 consistently emerged as the top performer in most categories, closely followed by Gemini 1.5 Flash (as shown in Table 3). Notably, Gemini demonstrated superior performance on hatched maps (as seen in Table 4), likely due to its stronger legend resolution and data extraction capabilities. However, GPT-4o's robust reasoning skills generally led to better scores across other task types."}, {"title": "Prompt Effectiveness", "content": "While most models consistently perform better with the standard Chain-of-Thought (COT) prompt compared to the Explicit Extraction and Reasoning (EER) prompt (as evident in Table 5), a notable exception is Gemini 1.5 Flash, which performs comparably or even better with the EER prompt. This suggests that Gemini possesses particularly strong instruction-following capabilities. Smaller, open-source models likely struggle with following the complex, stepwise instructions within the EER prompt. However, analysis of responses from larger models reveals that they implicitly adopt a methodology similar to EER, demonstrating impressive progress in their reasoning abilities and mimicking human-like thinking."}, {"title": "Biases in Model Prediction", "content": "This section analyzes the performance variations of models across different map and question variants. While these observations are often influenced by question type, we highlight the most prominent insights."}, {"title": "Map Variants", "content": "Discrete vs. Continuous Maps. While it is challenging to directly compare model performance on continuous and discrete maps due to the differing question types, a general trend emerges: models tend to perform better on discrete maps (as shown in Table 7). This trend is particularly pronounced for questions involving counting and extracting ranges, suggesting that models might struggle with accurately extracting legend ranges and color resolution in continuous maps. Interestingly, models performed significantly better on single-word answers within the continuous category. This may be attributed to the simplicity of these questions, as the task itself is inherently challenging for humans."}, {"title": "Maps with and without annotations.", "content": "As shown in Table 6, models generally exhibited similar performance on maps with and without annotations, with only a slight improvement observed for annotated maps in some cases. Surprisingly, we also found instances where models performed better on maps without annotations. This suggests that while annotations can be beneficial, they are not a critical factor in building models for understanding maps."}, {"title": "Colored Maps vs. Hatched Maps.", "content": "All models consistently performed better on colored maps compared to hatched maps, demonstrating a preference for colored depictions of data (as seen in Table 7). This trend is notable, as even models like GPT-4o experienced significant score drops on hatched maps, highlighting a lack of robustness. Impressively, Idefics displayed the least performance decline, suggesting a more robust ability to accurately extract data from these visually complex maps."}, {"title": "Country-Wise Performance", "content": "Table 8 presents model performance across different countries. While a consistent pattern is difficult to discern, a notable trend emerges: open-source models generally demonstrate consistent performance across countries, while closed-source models exhibit greater variation. The exact cause of this variation remains unclear, but potential contributing factors include biases in the training data."}, {"title": "Analysis across Question and Answer Types", "content": "Table 8 reveals that models generally performed best on questions requiring a binary answer, followed by single-word answers, highlighting their strong data extraction capabilities. Closed-source models like Gemini and GPT also excelled at questions expecting a range; however, smaller models struggled in this domain, likely due to limited reasoning or color extraction skills. Models encountered the most difficulty with tasks requiring a count or listing, which demand complex reasoning, external knowledge, and geospatial understanding. These questions proved challenging not only for models but also for humans (as shown in Table 9). For questions concerning relative regions, models struggled with single-word or count-based answers, further highlighting the complexity of these tasks, which require external knowledge, relative region extraction, and complex reasoning. Smaller models, in particular, struggled in this category (as seen in the Appendix G tables for relative regions)."}, {"title": "Human Evaluation and Baseline", "content": "We conducted a human evaluation of the MapWise dataset to establish a human baseline and to compare the performance of models against human evaluators. The MapWise dataset is particularly challenging as it demands careful identification of subtle shades and patterns, as well as nuanced understanding of spatial geographical relationships.\nWe conducted the human evaluation on a uniformly sampled set of 150 unique questions, spanning 75 maps and 40 templates. We ensured an approximately equal distribution of each answer type and map type, further ensuring the proper representation of continuous and discrete maps and relative region-type questions. This approach was followed for all three countries to capture all diverse scenarios within the dataset. We employed majority voting for result verification of the three independent annotators.\nAs shown in Table 9, the less-than-perfect human performance highlights the complexity of the task and offers a realistic benchmark against which model performance can be compared. Several common challenges contribute to the dataset's complexity, even for human evaluators. These include confusing color shades, particularly in continuous maps, numerous range groups in discrete maps, difficulty in understanding patterns for hatched maps and the challenge of accurately interpreting values for regions with smaller areas."}, {"title": "Experiments with Counterfactual data", "content": "We performed additional analysis to evaluate models which are trained extensively on large datasets, under conditions where their internal factual knowledge was limited. To carry out our analysis, we created three types of counterfactual data that forced the models to rely exclusively on the provided maps. Figure 3 shows examples of our counterfactual maps.\nFor the counterfactual dataset generation, we first uniformly sampled a subset of 240 unique Questions from USA dataset, spreading over 90 Maps and 26 Templates. We also ensured approximately equal distribution of each answer type. Using the sampled dataset as a representative sample (consisting of original names and values), we applied the following modifications to create our countefactual dataset:\nImaginary names. States were assigned imaginary names, generated using GPT-4. (e.g., Alabama was renamed Aquilis, Arkansas became Davina, etc.) The first two letters of these imaginary names were used as state codes for an annotated map of the US.\nShuffled names. The names of different US states were randomly shuffled while retaining the values of each geographical region. Annotated maps with these shuffled state codes were generated (e.g. Alabama became Montana, Arkansas became Idaho).\nJumbled values. The values corresponding to each of the different US states were shuffled, keeping the legend fixed. As a result, several question answer pairs needed to be re-evaluated.\nAdjustments to the prompts were made in accordance with the specific requirements of each counterfactual dataset. For example when dealing with imaginary names, the following instruction was included: \"The map in the image represents fictional names for each state as specified in the following dictionary. Use this dictionary while analyzing the map\". A corresponding dictionary was provided for reference within the prompt. Table 11 presents the results for Gemini, GPT, Idefics and InternLM, evaluated using the zero-shot COT prompt (Appendix A for contains results for the remaining models and the EER prompt). At a high level, it is evident that the closed source model consistently outperformed the open-source models across all three types of counterfactual datasets."}, {"title": "Related Work", "content": "Visual Question Answering (VQA) has attracted significant attention in computer vision and natural language processing due to its interdisciplinary challenges, as explored by Antol et al. (2015); Goyal et al. (2017); Bazi et al. (2023); Hartsock and Rasool (2024); Zhang et al. (2024). The introduction of Visual Question Rewriting (VQR) by Wei et al. (2021) has further advanced our understanding of how visual information can enhance question-answering systems. Similarly, Wu (2023) introduced visual quizzing, which involves reasoning with both images and their related questions.\nMap Question Answering (MQA) and Chart Question Answering (CQA) have also emerged as challenging extensions of VQA, requiring the interpretation of visual data representations such as charts and maps. Datasets like ChartQA(Kafle et al., 2018; Kahou et al., 2017) focus on interpreting structured data charts, while Chang et al. (2022) introduced MapQA for choropleth map question answering, highlighting the need for robust VQA systems. MapQA's U.S. focus study and template questions limit its scope. Our dataset on the other hand includes a diverse set of countries, map types and complex questions which were manually curated to create an effective benchmark to evaluate model performances.\nEnhancing Visual Question Answering. Despite these advances, gaps remain in Chart (CQA) and Map Question Answering (MQA), particularly in handling complex reasoning, numeric answers, and out-of-vocabulary terms. Existing systems often struggle with these challenges, and synthetic datasets may limit their real-world applicability (Bhaisaheb et al., 2023; Chaudhry et al., 2020). Our research addresses these issues by building on Chang et al. (2022) with more diverse maps, challenging questions, and benchmarking state-of-the-art multimodal and visual-language models."}, {"title": "Conclusion and Future Work", "content": "This paper introduces MAPWise, a new large-scale dataset tailored for understanding choropleth maps in three diverse countries: the United States, China, and India. Looking ahead, there are many promising areas for further research based on what we found and from the existing studies. Future studies could broaden the scope of datasets by including different types of maps. Inspired by previous work (Fan et al., 2024), we could complement our dataset by exploring fictional maps or more detailed maps that include features such as rivers and roads. This expansion would help evaluate how well VLMs generalize across diverse geographical contexts. Further research is needed to identify and mitigate biases inherent in map interpretation. Techniques like dataset perturbation, which introduces variations in map features and contexts, could provide deeper insights and help mitigate biases effectively.\nTo improve how data is extracted, integrating external knowledge sources in future would be a promising strategy. Models that use knowledge graphs, like RAG networks filled with detailed information about state borders and regional relationships, could also improve how well Vision Language Models (VLMs) reason through map-based tasks. Another future direction would be improving how VLMs are trained to recognize colors more accurately and integrating additional datasets, training on auxiliary data such as charts, to improve their ability to interpret and process map-related information effectively."}, {"title": "Limitations", "content": "While our study has yielded interesting observations, it's crucial to acknowledge its limitations. We focused exclusively on choropleth maps, which represent data using color gradients. While these maps are effective for visualizing regional data, they lack the detailed features and interactive elements found in more advanced mapping systems like Google Maps.\nAdditionally, our study does not include rank-based questions specifically tailored for the United States. Therefore, our findings and methods may not fully generalize to these more complex mapping systems and their unique challenges. Moreover, we were limited to maps from only three countries, and the manual question creation process restricted the size of our dataset."}, {"title": "Ethics Statement", "content": "We, the authors, ensure that our research meets the highest ethical standards in both research and publication. We have carefully addressed all ethical considerations for responsible and fair use of computational linguistics methods. To help others replicate our results, we are sharing all necessary details, including code, available datasets (used according to their ethical guidelines), and other resources. This allows the research community to verify and build on our work. Our claims are backed by our experimental results. We provide detailed information on annotations, dataset splits, models, and methods used for reproducibility."}, {"title": "Acknowledgement", "content": "Our work is sponsored by the Army Research Office and is accomplished under Grant Number W911NF-20-1-0080. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Office or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation herein. This work was partially funded by ONR Contract N00014-19-1-2620. We would also like to thank Nirupama Ratna, Arqam Patel and Jay Gala for their extensive help and support during the process of creating our dataset. Additionally, we would like to extend our deepest gratitude to Manish Shrivastava for his guidance during the project."}, {"title": "Prompting Strategies", "content": "We evaluated the baseline models under two distinct prompting settings:\n1. Zero-Shot Chain-of-Thought Prompting (COT). We leverage the Chain-of-Thought (Wei et al., 2023) prompting, presenting the VLM with a map and a question, prompting it to reason through the steps leading to its final answer.\n2. Explicit Extraction and Reasoning (EER). Here, we created a custom prompt that explicitly outlined the reasoning steps the model should follow to answer the specific question. This prompt was broken down into four distinct reasoning steps:\n Extraction of Regions. The model was prompted to identify the regions whose data was required to answer the question.\n Extraction of Relevant Places. Next, the model was instructed to extract the specific locations or places associated with the identified regions.\n - Extraction of Values from Legend. The model was then directed to extract the values corresponding to those regions from the map's legend.\n Reasoning based on Extracted Values. Finally, the model was prompted to reason based on the extracted values to arrive at the final answer.\nThis approach helped break down the reasoning process into smaller, more manageable steps, preventing the model from becoming overwhelmed and guiding it towards a more focused and structured reasoning process.\nDuring the evaluation, all models were given the same prompt in order to fairly and consistently assess their ability to reason. The prompts used have been presented in the Appendix."}, {"title": "Evaluation Details", "content": "The evaluation process adapts to various answer types within in the dataset by employing tailored metrics and criteria for each specific answer type. Additionally, normalization was applied wherever necessary to ensure consistency and accuracy in the assessment.\nFor binary yes/no and integer count answers, we implemented an exact match criterion and accuracy as the evaluation metric. For single-word answers, as some questions have multiple applicable responses, we employed the recall metric for better evaluation. For state names, a valid answer could be either a two-digit state code or the full state name. For ranges, we first normalized the ranges to absolute values (e.g. 1k to 1000) and then compared them. For discrete maps, only exact match was expected, whereas for continuous maps, we gave a full score of 1 for exact match and a partial score of 0.5 for overlapping responses.\nFor list type answer, we used precision and recall metrics because predicted lists often contained irrelevant states (false positives) and missed relevant states (false negatives).\nFor rank-type answers, we prompted the model to assign ranks to states based on map values. However, due to the difficulty in accurately distinguishing shades, models frequently assigned states to wrong shades, resulting in multiple states sharing the same rank despite differing shades. Additionally, for some questions, ground truth involved multiple states in the same rank because of states having identical shades or patterns. To evaluate this, we designed a \"Rank-wise Precision (RWP)\u201d method, computing precision for each rank and then averaging across all ranks. We also evaluated other ranking metrics, including Mean Reciprocal Rank (MRR) and Mean Average Precision (MAP), as detailed in Appendix C.\nNote for Open Source VLMs. Smaller models, like QwenVL, CogAgent, and InternLM, faced challenges in producing answers in the desired format. To address this, we used an \"LLM as an Extractor\" approach, using Gemini 1.5 Flash to extract answers from their outputs. Manual verification of 150 samples confirmed that Gemini primarily acted as a extracting and formatting tool, preserving the original model's answer in 138 cases. In the remaining 12 cases, the original model had not clearly answered the question, for which Gemini reported \"Answer cannot be extracted\"."}, {"title": "Results and Analysis", "content": "MAPWise: A Challenging Benchmark. The MAPWise dataset presents a compelling benchmark for evaluating the reasoning abilities of current Vision-Language Models (VLMs). As shown in Table 3, models consistently perform significantly worse than the human baseline, particularly with questions requiring intricate reasoning, such as counting or providing a list of regions where the difference in scores is close to 50% on average. This substantial performance gap highlights a significant limitation in the reasoning capabilities of existing VLMs, underscoring the need for further research to bridge this gap.\nModel Performance Comparison. While model performance varied across different answer types and countries, GPT-40 consistently emerged as the top performer in most categories, closely followed by Gemini 1.5 Flash (as shown in Table 3). Notably, Gemini demonstrated superior performance on hatched maps (as seen in Table 4), likely due to its stronger legend resolution and data extraction capabilities. However, GPT-4o's robust reasoning skills generally led to better scores across other task types."}, {"title": "Prompt Effectiveness", "content": "While most models consistently perform better with the standard Chain-of-Thought (COT) prompt compared to the Explicit Extraction and Reasoning (EER) prompt (as evident in Table 5), a notable exception is Gemini 1.5 Flash, which performs comparably or even better with the EER prompt. This suggests that Gemini possesses particularly strong instruction-following capabilities. Smaller, open-source models likely struggle with following the complex, stepwise instructions within the EER prompt. However, analysis of responses from larger models reveals that they implicitly adopt a methodology similar to EER, demonstrating impressive progress in their reasoning abilities and mimicking human-like thinking."}, {"title": "Biases in Model Prediction", "content": "This section analyzes the performance variations of models across different map and question variants. While these observations are often influenced by question type, we highlight the most prominent insights."}, {"title": "Map Variants", "content": "Discrete vs. Continuous Maps. While it is challenging to directly compare model performance on continuous and discrete maps due to the differing question types, a general trend emerges: models tend to perform better on discrete maps (as shown in Table 7). This trend is particularly pronounced for questions involving counting and extracting ranges, suggesting that models might struggle with accurately extracting legend ranges and color resolution in continuous maps. Interestingly, models performed significantly better on single-word answers within the continuous category. This may be attributed to the simplicity of these questions, as the task itself is inherently challenging for humans."}, {"title": "Maps with and without annotations.", "content": "As shown in Table 6, models generally exhibited similar performance on maps with and without annotations, with only a slight improvement observed for annotated maps in some cases. Surprisingly, we also found instances where models performed better on maps without annotations. This suggests that while annotations can be beneficial, they are not a critical factor in building models for understanding maps."}, {"title": "Colored Maps vs. Hatched Maps.", "content": "All models consistently performed better on colored maps compared to hatched maps, demonstrating a preference for colored depictions of data (as seen in Table 7). This trend is notable, as even models like GPT-4o experienced significant score drops on hatched maps, highlighting a lack of robustness. Impressively, Idefics displayed the least performance decline, suggesting a more robust ability to accurately extract data from these visually complex maps."}, {"title": "Country-Wise Performance", "content": "Table 8 presents model performance across different countries. While a consistent pattern is difficult to discern, a notable trend emerges: open-source models generally demonstrate consistent performance across countries, while closed-source models exhibit greater variation. The exact cause of this variation remains unclear, but potential contributing factors include biases in the training data."}, {"title": "Analysis across Question and Answer Types", "content": "Table 8 reveals that models generally performed best on questions requiring a binary answer, followed by single-word answers, highlighting their strong data extraction capabilities. Closed-source models like Gemini and GPT also excelled at questions expecting a range; however, smaller models struggled in this domain, likely due to limited reasoning or color extraction skills. Models encountered the most difficulty with tasks requiring a count or listing, which demand complex reasoning, external knowledge, and geospatial understanding. These questions proved challenging not only for models but also for humans (as shown in Table 9). For questions concerning relative regions, models struggled with single-word or count-based answers, further highlighting the complexity of these tasks, which require external knowledge, relative region extraction, and complex reasoning. Smaller models, in particular, struggled in this category (as seen in the Appendix G tables for relative regions)."}, {"title": "Human Evaluation and Baseline", "content": "We conducted a human evaluation of the MapWise dataset to establish a human baseline and to compare the performance of models against human evaluators. The MapWise dataset is particularly challenging as it demands careful identification of subtle shades and patterns, as well as nuanced understanding of spatial geographical relationships.\nWe conducted the human evaluation on a uniformly sampled set of 150 unique questions, spanning 75 maps and 40 templates. We ensured an approximately equal distribution of each answer type and map type, further ensuring the proper representation of continuous and discrete maps and relative region-type questions. This approach was followed for all three countries to capture all diverse scenarios within the dataset. We employed majority voting for result verification of the three independent annotators.\nAs shown in Table 9, the less-than-perfect human performance highlights the complexity of the task and offers a realistic benchmark against which model performance can be compared. Several common challenges contribute to the dataset's complexity, even for human evaluators. These include confusing color shades, particularly in continuous maps, numerous range groups in discrete maps, difficulty in understanding patterns for hatched maps and the challenge of accurately interpreting values for regions with smaller areas."}, {"title": "Experiments with Counterfactual data", "content": "We performed additional analysis to evaluate models which are trained extensively on large datasets, under conditions where their internal factual knowledge was limited. To carry out our analysis, we created three types of counterfactual data that forced the models to rely exclusively on the provided maps. Figure 3 shows examples of our counterfactual maps.\nFor the counterfactual dataset generation, we first uniformly sampled a subset of 240 unique Questions from USA dataset, spreading over 90 Maps and 26 Templates. We also ensured approximately equal distribution of each answer type. Using the sampled dataset as a representative sample (consisting of original names and values), we applied the following modifications to create our countefactual dataset:\nImaginary names. States were assigned imaginary names, generated using GPT-4. (e.g., Alabama was renamed Aquilis, Arkansas became Davina, etc.) The first two letters of these imaginary names were used as state codes for an annotated map of the US.\nShuffled names. The names of different US states were randomly shuffled while retaining the values of each geographical region. Annotated maps with these shuffled state codes were generated (e.g. Alabama became Montana, Arkansas became Idaho).\nJumbled values. The values corresponding to each of the different US states were shuffled, keeping the legend fixed. As a result, several question answer pairs needed to be re-evaluated.\nAdjustments to the prompts were made in accordance with the specific requirements of each counterfactual dataset. For example when dealing with imaginary names, the following instruction was included: \"The map in the image represents fictional names for each state as specified in the following dictionary. Use this dictionary while analyzing the map\". A corresponding dictionary was provided for reference within the prompt. Table 11 presents the results for Gemini, GPT, Idefics and InternLM, evaluated using the zero-shot COT prompt (Appendix A for contains results for the remaining models and the EER prompt). At a high level, it is evident that the closed source model consistently outperformed the open-source models across all three types of counterfactual datasets."}]}