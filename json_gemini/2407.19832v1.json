{"title": "ML-Mamba: Efficient Multi-Modal Large Language Model Utilizing Mamba-2", "authors": ["Wenjun Huang", "Jianguo Hu"], "abstract": "Multimodal Large Language Models (MLLMs) have attracted much attention due to their multifunctionality. However, traditional Transformer architectures incur significant overhead due to their secondary computational complexity. To address this issue, we introduce ML-Mamba, a multimodal language model that utilizes the latest and efficient Mamba-2 model for inference. Mamba-2 is known for its linear extension and fast processing of long sequences. We replace the Transformer based backbone with a pre-trained Mamba-2 model and explore methods for integrating 2D visual selective scanning mechanisms into multimodal learning. We also try various visual encoders and Mamba-2 model variants. Our extensive experiments conducted in various multimodal benchmark tests have demonstrated the competitive performance of ML-Mamba and highlighted the potential of state space models in multimodal tasks. The experimental results show that: (1) ML-Mamba achieves performance comparable to state-of-the-art methods such as TinyLaVA and MobileVLM v2 through its linear sequential modeling, while also having faster inference speed; (2) ML-Mamba performs well in visual hallucinations and spatial relationship judgment in closed set benchmark tests; (3) ML-Mamba achieves performance comparable to LLaVA while reducing the number of parameters by 40%.(4) Compared to the multimodal model using the original Mamba model, the Mamba-2 based large-scale multimodal language model has stronger inference performance and effectiveness.", "sections": [{"title": "Introduction", "content": "The emergence of Large Language Models (LLMs) has profoundly changed the landscape of natural language understanding tasks. Unlike early methods that relied on medium-sized task specific models, recent advances have shifted towards using general large-scale models, especially after the success of systems such as ChatGPT. It has been proven that expanding the scale of language models and increasing data volume can bring many advantages, including enhancing the performance of different tasks and improving the sample efficiency of out of distribution generalization [19].\nHowever, traditional LLMs are limited to interacting through language, which limits their adaptability to handling more diverse tasks. Multi modal understanding that integrates visual and textual infor-mation is crucial for improving the ability of models to effectively respond to real-world challenges. Therefore, researchers are actively expanding large-scale language models to integrate multimodal information processing capabilities. Visual language models (VLMs) such as GPT-4 [40], LLaMA adapter [11], and LLaVA [35, 34] have been developed to enhance LLM's visual comprehension ability. These VLMs are fundamental models for handling a range of tasks, including visual question answering (VQA), image captioning, and visual content generation.\nDespite achieving success, previous research has mainly focused on reducing the parameters of language models while preserving the Transformer architecture. However, this method does not solve the inherent problem of low computational efficiency in Transformer's self attention mechanism, which is quadratic with sequence length. To address this bottleneck, the latest research work has designed a new architecture (Mamba-2), whose core layer is an improvement of Mamba selective SSM. The state space model (SSM) has been widely studied as an effective alternative solution. SSM combines elements of Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs), providing linear scaling of sequence length and effective training and inference. It is 2-8 times faster and continues to compete with Transformers in language modeling.\nTo this end, this article proposes a new perspective, which is to directly use the state space model (SSM) as the backbone. Specifically, we use the Mamba-2 language model as the basic model of our VLM. In this article, we introduce ML-Mamba, a work that applies state space models to multimodal learning tasks. Our method utilizes a pre-trained Mamba-2 language model as the backbone, replacing traditional Transformer based models such as LLaMA [50] or Vicuna [4]. We further enhanced ML-Mamba through a novel multimodal connector called Mamba-2 Scan Connector (MSC) architecture, which includes a Mamba-2 visual selective scanning module (MVSS) and a SwiGLU module specifically designed for 2D causal modeling of enriched visual sequences. The MVSS module explores two different scanning mechanisms: bidirectional scanning mechanism (BSM) and cross scanning mechanism (CSM). In addition, we investigated the combination of different visual encoders, variants of pre-trained Mamba-2 language models, and multimodal connectors to optimize the integration of visual and linguistic information.\nExtensive experiments conducted on various multimodal learning benchmarks have demonstrated the effectiveness of ML-Mamba. Our model achieves competitive performance with other similarly sized small MLLMs, and even outperforms large MLLMs on some popular benchmark tests (such as LLaVA v1.5 [34] versions 7B and 13b).\nIn summary, our contributions are as follows:"}, {"title": "Related Work", "content": "In recent years, significant breakthroughs have been made in natural language processing tasks [22, 25], characterized by large model scales, typically containing billions of parameters, and training using massive datasets. GLM [9], LLaMA [50], Alpaca [48], Vicuna [4] and other instruction fine-tuning versions have emerged one after another, with the goal of being comparable to the proprietary InstructGPT model without public access. At the same time, due to the significant computational requirements of large language models, research trends have shifted towards exploring the possibility of smaller scale models, such as Stable LM [2], TinyLaMA [55], and Phi [17, 30], which have parameter sizes below 3 billion but can achieve comparable results to large models through high-quality data and feasible training methods."}, {"title": "State Space Models (SSMS)", "content": "State Space Models (SSMs) have demonstrated excellent performance in areas such as long sequence modeling, image generation, and reinforcement learning. A notable feature of SSMs is their ability to perform efficient autoregressive inference like Recurrent Neural Networks (RNNs) while also being able to process entire input sequences in parallel like attention-based Transformers, thus enabling efficient training. Despite their efficiency, SSMs achieve good results in various sequence modeling tasks. Specifically, Albert et al. [16] proposed a structured state space sequence model for time series analysis. Goel et al. [12] applied SSMs to audio generation and achieved satisfactory performance. Additionally, the H3 model [10] was introduced to bridge the gap between SSMs and Transformers in language modeling.\nIn recent months, a new selective state space model called Mamba [15] has been proposed as a strong competitor to the Transformer architecture. Compared to LLMs of the same capacity, language models based on Mamba have shown competitive performance, faster inference speeds, and the ability to scale linearly over time with constant memory usage. In May 2024, the latest Mamba architecture (Mamba-2) [8] was introduced, featuring an improved core layer of the Mamba selective SSM, which is 2-8 times faster while continuing to compete with Transformers in language modeling."}, {"title": "Multimodal Large Language Model (MLLM)", "content": "The Multi Modal Large Language Model (MLLM) combines visual and linguistic information and has achieved significant success in various fields [52, 53, 38]. However, the basis of these models is usually a known Transformer network, resulting in a square level computational complexity [23]. In order to improve the efficiency of the base model, ML-Mamba is proposed, which is an MLLM with linear computational complexity. Specifically, ML-Mamba integrates the efficient Mamba-2 language model into visual modalities and explores different modal fusion strategies to create effective multimodal Mamba-2 [8]. Experiments have shown that ML-Mamba not only competes with current computationally efficient MLLMs such as LLaVA Phi, TinyLaVA, and MobileVLM v2, but also runs faster due to its linear sequence modeling characteristics. Interestingly, the results of the closed set prediction benchmark test show that ML-Mamba performs well in overcoming visual illusions and spatial relationship judgments, even comparable to LLaVA in performance with only 40% of its parameters.\nIn terms of MLLMs for instruction tuning, recent research [24] has questioned the necessity of the pre alignment phase in MLLM training, pointing out that directly fine-tuning the entire LLM backbone and projector may be sufficient. In line with this, ML-Mamba only underwent a small amount of alignment training and then fine-tuned it on a large combination dataset containing visual multi-turn dialogues and visual alignment instructions."}, {"title": "Mamba in the field of vision", "content": "The successful application of Mamba in natural language processing (NLP) has inspired its adoption in visual applications. Vision Mamba (Vim) [59] utilizes Vim blocks composed of pure Mamba layers: each Vim block models bidirectional representations using forward and backward scanning, and alleviates direction sensitivity issues in Mamba. Another approach, VMamba [36] utilizes Visual State Space (VSS) blocks that integrate Mamba and 2D convolutional layers, supported by a pyramid architecture similar to Swin Transformer [37]: each VSS block first models 2D local information through 2D deep convolution as a token mixer, and then processes 2D global information horizontally and vertically through a cross scan module. Mamba ND [29] further extends the functionality of Mamba to multidimensional data including images and videos. LocalMamba [20] segments the input image into multiple local windows and executes a state space model (SSM) in various directions within these windows to enhance local processing capabilities. EfficientVMamba [43] introduced an efficient 2D scanning technique that reduces computational requirements by performing atrous sampling on feature map blocks. In addition to these newly designed Mamba architectures, our work also draws inspiration from VL-Mamba [44], a multimodal large language model based on state space models, which has shown great potential for long-sequence modeling with fast inference and linear scaling in sequence length. Compared with these newly designed Mamba architectures, our architecture closely follows Mamba's design ideas in the field of vision, enhancing the extraction of visual features with the latest Mamba-2 module. Our main goal with the Mamba-2 based architecture is to enhance multimodal representation and inference capabilities."}, {"title": "Method", "content": "In this section, we first introduce the basic concepts of State Space Models (SSMs) (Sec. 3.1). Subsequently, we provide a detailed description of the proposed ML-Mamba method (Sec. 3.2), which mainly comprises a visual encoder, a multi-modal connector called the Mamba-2 Scan Connector (MSC), an MLP projector, and the Mamba-2 large language model."}, {"title": "Mamba Preliminaries", "content": "The Mamba architecture inherits from state space sequence models [16], which models a 1-D function or sequence $x(t) \\in R \\rightarrow y(t) \\in R$ at time t via expanded hidden states $h_t \\in R^N$. The hidden state is evolved through time driven by parameters A, B, C following linear ordinary differential equations (ODES):\n$h'(t) = Ah(t) + Bx(t)$,\n$y(t) = Ch(t)$.\nTo discretize parameters in this continuous system, a common solution is to introduce a time scale parameter A to transform continuous A, B to discrete A, B using zero-order hold (ZOH) model [41]:\n$A = exp(A\\Delta)$,\n$B = (\\Delta A)^{-1}(exp(A\\Delta) - I) \\cdot \\Delta B$.\nBy applying such transformation, we can rewrite Eq. 1 as:\n$h'(t) = Ah_{t-1}+ Bx_{t}$,\n$Y_t = Ch_t$."}, {"title": "ML-Mamba Model", "content": "The architecture of Mamba consists of four main components: a pre-trained visual encoder, a randomly initialized multi-modal connector called Mamba-2 Scan Connector (MSC), and a pre-trained large language model (Mamba-2 LLM). With an image as input, visual features are first extracted through the visual encoder. The extracted sequence of visual features is then fed into the multi-modal connector (MSC), whose output is mapped to the LLM using a multi-layer perceptron (MLP) projector. The output vector from the visual projector is then combined with tokenized text queries and input into the Mamba-2 LLM. Finally, the Mamba-2 LLM generates the corresponding response."}, {"title": "Overall Architecture", "content": "The architecture of Mamba consists of four main components: a pre-trained visual encoder, a randomly initialized multi-modal connector called Mamba-2 Scan Connector (MSC), and a pre-trained large language model (Mamba-2 LLM), as shown in Fig. 2. With an image as input, visual features are first extracted through the visual encoder. The extracted sequence of visual features is then fed into the multi-modal connector (MSC), whose output is mapped to the LLM using a multi-layer perceptron (MLP) projector. The output vector from the visual projector is then combined with tokenized text queries and input into the Mamba-2 LLM. Finally, the Mamba-2 LLM generates the corresponding response."}, {"title": "Vision Encoder", "content": "We integrate DINOv2 [42] and SigLIP [54] to serve as our vision backbone. The rationale behind this fusion is that combining the low-level spatial features captured by DINOv2 with the semantic features provided by SigLIP enhances performance on downstream tasks [49, 24]. Given an input image $X \\in R^{C\\times H\\times W}$, the vision encoder divides the image into $N_2 = HW/P^2$ patches of equal size, where $P^2$ represents the patch size. Both vision encoders process the patchified image as an input token sequence and concatenate their outputs to form compact visual representations $V_{img} \\in R^{N\\times D_v}$:\n$V_{img} = [\\varphi_{SigLIP} (X_v); Y_{DINOV2} (\\Chi_v)],$\nThese outputs are then channeled to a dedicated task-specific head, with $D_v$ representing the dimen-sionality of the tokens generated as described above."}, {"title": "MultiModal Connector", "content": "Multimodal connectors act between visual features and language models to ensure seamless integra-tion of visual and linguistic information. In this study, we explored a novel multimodal connector called Mamba-2 Scan Connector (MSC) architecture aimed at addressing the challenge of unclear causal relationships in computer vision. The traditional state space model (SSM) is typically used to process sequence data with causal relationships, such as language sequences, but this approach is clearly not applicable to non causal visual sequences generated by visual encoders."}, {"title": "Mamba-2 Large Language Model", "content": "The Mamba-2 language model [8] serves as the primary language processing component responsible for understanding and generating text. The workflow design of the visual encoder and multimodal connector ensures that visual information can be effectively transmitted to the Mamba-2 language model, enabling the model to process and understand complex multimodal data.\n$R = f_L(V_{out}, f_r(Q)).$"}, {"title": "Training Process", "content": "The recent research [24] indicates that for existing training paradigms based on LLaVA [6, 34, 57] (i.e., training only the projection layer in the pre-alignment stage and fine-tuning the LLM backbone in one iteration each), we decided to first use a 558K subset of the LAION-CC-SBU dataset to align the Mamba-2 Scan Connector (MSC) and the projector. During the fine-tuning stage, we simultaneously optimized the Mamba-2 Scan Connector (MSC), the projector, and the Mamba LLM. This comprehensive training effort was executed on 8 NVIDIA Tesla A100 GPUs. We conducted fine-tuning over two randomly sampled epochs using a composite dataset, which includes:\n1. LVIS-Instruct-4V [51]: This dataset contains 220K images with visual alignment and context-aware instructions generated by GPT-4V.\n2. The Mixed Dataset Used in LLaVA v1.5: This dataset includes a total of 655K visual multi-round dialogue samples. It encompasses academic VQA samples [13, 21, 26, 46], as well as visual instruction tuning data from LLaVA Instruction [45] and ShareGPT [35].\nOverall, the fine-tuning dataset consists of approximately 875K images and their corresponding multi-turn dialogue data, as well as pure text dialogue data."}, {"title": "Experiment", "content": "We conducted a comprehensive experimental evaluation of ML-Mamba through four aspects: bench-marking evaluation: We used six commonly used visual language model (VLM) benchmarks to evaluate the effectiveness of the proposed method. These benchmarks include four open-ended visual question answering tasks that require different reasoning abilities, as well as two closed set prediction tasks that involve determining spatial relationships of objects and detecting visual illusions."}, {"title": "Experimental Setup", "content": "Table 1 details the hyperparameters of the ML-Mamba model. For the visual encoder part, DINOv2 adopts the same ViT structure as in its original paper, namely a ViT-Large model with 304M parameters, pretrained on the LVD-142M dataset. SigLIP uses a slightly larger shape-optimized version than ViT-Large. The resolution of the input images is set to 384x384, with the number of visual tokens being 729.\nThe backbone of the LLM is initialized using the pretrained weights from the Mamba-2 model, while the multimodal connectors (MSC) and projectors are always randomly initialized. We chose an open-source model weight from the Huggingface platform to initialize our model as the LLM backbone for our proposed model.\nThe entire training process took approximately 31 hours on 8 NVIDIA A100 80GB GPUs. During training, we used Pytorch's fully shared data parallel framework [56] and adopted automatic mixed precision with FP32 and BF16 for distributed training. The batch size was set to 64. We used the AdamW [39] optimizer and updated the network parameters using a learning rate with cosine decay. The learning rate was set to $2 \\times 10^{-5}$, the decay factor was 0.1, and the warm-up ratio was 0.03. The model was trained for 2 epochs with supervised fine-tuning."}, {"title": "Results", "content": "In addition, we further evaluated the model on six carefully designed metrics, particularly VizWiz [18] and VQAv2 [13], for assessing general visual reasoning ability. VizWiz includes common sense questions and unanswerable questions, requiring the model to avoid incorrect answers to evaluate its reliability. GQA evaluates spatial understanding and multi-step reasoning in real-world images. The issues in TextVQA are related to the text in the image, evaluating the model's optical character recognition (OCR) and inference capabilities. POPE provides a benchmark for evaluating object hallucinations and is a binary classification task that prompts the model to answer whether the object exists. We also introduced two closed set prediction benchmarks consisting of VSR [33] and POPE [32]. VSR evaluates the model's ability to understand spatial relationships between different images, while POPE evaluates the VLM's ability to avoid severe illusion problems. VSR and POPE calculate scores based on the probability of providing the correct answer.\nWe evaluated VizWiz, VQAv2, and TextVQA using validation sets, while using the recommended test dev partition for GQA, zero sample test partition for VSR, and evaluation partition for POPE.\nTo demonstrate the effectiveness of the model, we compared it with a VLM of the same scale with approximately 3B parameters, or with a larger VLM containing twice the number of parameters. As shown in Table 2, although Although ML-Mamba has only about 40% of its parameters, it performs well on multiple benchmarks compared to LLaVA v1.5 7B, and even outperforms all models on POPE.\nCompared with VLM with similar parameter numbers, ML-Mamba consistently achieved better performance than LLaVA Phi in VQAv2, GQA, VQAT, POPE and VizWiz. While VL-Mamba performs better on VQAv2, our ML-Mamba outperforms VL-Mamba on GQA, VQAT, and POPE. MobileVLM is another parallel work aimed at producing small-scale LLMs, and is therefore also introduced in experiments. In summary, these results indicate that ML-Mamba matches the per-formance of state-of-the-art models at the same level (~3B) on multiple benchmarks and remains competitive when compared to larger scale models (7B and above).\nWe present some examples to illustrate the qualitative results of ML-Mamba. As shown in Fig. 7, ML-Mamba effectively understands the user's questions and responds accurately."}, {"title": "Reasoning speed", "content": "In order to evaluate the efficiency advantage of the ML-Mamba model, especially the speed improve-ment brought by its linear sequence modeling, we conducted a detailed inference speed comparison experiment. In the experiment, we compared ML-Mamba with two baseline models of the same scale parameters, TinyLaVA 3B and MobileVLM v2 3B.\nAll models were evaluated in the same hardware environment, namely a single Nvidia A100 PCIe 80GB GPU. Each model receives the same example image as input, with a unified image resolution of 336 \u00d7 336 pixels, and is processed by a CLIP encoder. For TinyLaVA, the model receives 576 image markers processed by the projector; MobileVLM v2 reduces the number of image labels to 144 through LDP blocks. In contrast, ML-Mamba uses dual encoders to process images with a resolution of 384 \u00d7 384, resulting in an increase in the actual number of image labels processed to 729.\nIn the experiment, all models received the same question: \"Provide a detailed description of the image.\" and set the number of output labels to 256. The total time is the entire process from image encoding until the complete answer is generated.And we calculated the average number of tokens generated per second by $Eval_{avg} = 256/T_{total}$.\nThe results from Fig. 1 and Table 3 demonstrated that although the number of image markers processed by ML-Mamba significantly increased, it still exhibited extremely fast inference speed. Compared to MobileVLM v2, although the latter has undergone multiple lightweight optimizations, the time required for ML-Mamba to complete inference is only about 30% of the former. This indicates that ML-Mamba not only maintains high speed while processing larger data, but also, thanks to the characteristics of its RNN like model, its memory usage does not significantly increase with the increase of image marker length, as such models maintain a fixed size hidden state to store historical information during the inference process.\nThe excellent performance of the ML-Mamba model in inference speed proves its advantage in linear sequence modeling, especially when dealing with a large number of image labels. Compared to Transformer based models, ML-Mamba demonstrates significant speed improvements, providing strong support for multimodal tasks that require rapid response."}, {"title": "Ablation Study", "content": "Table 4 presents the results of ablation experiments evaluating the effectiveness of different language model variants. We conducted experiments on three different variants, namely Mamba-2 with parameters of 780m, 1.3b, and 2.7b, trained on the Pile dataset (containing 300B tokens). Specifically, we constructed a baseline model using the same variant of DINOv2+SigLIP as the visual encoder, Mamba-2 language model as the backbone of a large language model, and a regular MLP multimodal connector without a 2D visual selection scanning module. We can see that as the model size and number of training tokens increase, Mamba2-2.7B outperforms other variants on all benchmarks. Therefore, we chose Mamba2-2.7B for other experiments."}, {"title": "Effects of Language Model Variants", "content": "Table 4 presents the results of ablation experiments evaluating the effectiveness of different language model variants. We conducted experiments on three different variants, namely Mamba-2 with parameters of 780m, 1.3b, and 2.7b, trained on the Pile dataset (containing 300B tokens). Specifically, we constructed a baseline model using the same variant of DINOv2+SigLIP as the visual encoder, Mamba-2 language model as the backbone of a large language model, and a regular MLP multimodal connector without a 2D visual selection scanning module. We can see that as the model size and number of training tokens increase, Mamba2-2.7B outperforms other variants on all benchmarks. Therefore, we chose Mamba2-2.7B for other experiments."}, {"title": "Effects of Different Visual Encoders", "content": "Recent research has found that although language image models similar to CLIP can provide rich semantic information, they may lose detailed information about the image itself. Therefore, we further introduce DINOv2 as a supplementary encoder and connect the visual representations of these two encoders for subsequent LLM. As shown in Table 5, the introduction of DINOv2 signifi-cantly improved the model performance in six benchmark tests. This result suggests a meaningful principle when selecting a visual encoder for downstream tasks. Therefore, we ultimately chose DINOv2+SigLIP as the visual encoder to construct our model and used it for further experiments. Through this combination, we can achieve better performance on multiple benchmarks."}, {"title": "Ablation on different multimodal connector structures", "content": "We also explored the impact of different architectures of multi-mode connectors. We evaluated three different MMC variants: MLP, MSC-MLP (Basic), and MSC-MLP (Advanced). As shown in Table 6, by comparing these three architectures, we observed that MSC-MLP (Advanced) performed relatively better on most benchmark tests, especially on VQA, demonstrating the effectiveness of combining MSC modules with swiGLU. Note that these models use DINOv2+SigLIP as the visual encoder, Mamba2-2.7B as the language model, and a bidirectional selective scanning mechanism."}, {"title": "Under different scanning mechanisms", "content": "We compared the bidirectional scanning mechanism (BSM) and cross scanning mechanism (CSM) in MMC modules. As shown in Table 7, although BSM and CSM perform similarly in some benchmark tests, such as scoring 76.6 in one test, BSM shows superior performance in most benchmark tests. This highlights its advantages in handling 2D visual information for multimodal learning tasks."}, {"title": "Limitation", "content": "The training of ML-Mamba relies on specific multimodal datasets, which may have biases or incomplete coverage in certain aspects. Developing more comprehensive and diverse datasets, as well as improving data preprocessing and augmentation techniques, will help enhance the generalization ability and applicability of ML-Mamba in different scenarios.\nML-Mamba currently faces challenges in running on mobile devices, especially in meeting the memory usage requirements of these devices. In order to make ML-Mamba run more smoothly on devices such as smartphones or tablets, further optimization, especially for low memory environments, is necessary."}, {"title": "Conclusion", "content": "This article introduces a novel multimodal learning model, ML-Mamba, which utilizes the latest state space model (SSM) Mamba-2 to solve multimodal learning tasks. It uses a pre-trained Mamba-2 language model as the language model and introduces the multimodal connector Mamba-2 Scan Connector (MSC) module to bridge the gap between 2D non-causal image information and the inherent causal modeling ability of SSM. By conducting comprehensive experiments and ablation studies, ML-Mamba performed well in multimodal benchmark testing, demonstrating its effectiveness and the potential of SSM in multimodal learning. On the other hand, ML-Mamba addresses the efficiency bottleneck of existing multimodal large language models by using models with linear computational complexity. This significantly improves computational efficiency and excels in visual illusion and spatial relationship judgment while reducing the number of parameters. These advancements open new possibilities for deploying high-performance AI models in environments that process visual information at high frequencies."}]}