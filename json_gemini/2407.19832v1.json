{"title": "ML-Mamba: Efficient Multi-Modal Large Language Model Utilizing Mamba-2", "authors": ["Wenjun Huang", "Jianguo Hu"], "abstract": "Multimodal Large Language Models (MLLMs) have attracted much attention\ndue to their multifunctionality. However, traditional Transformer architectures\nincur significant overhead due to their secondary computational complexity. To\naddress this issue, we introduce ML-Mamba, a multimodal language model that\nutilizes the latest and efficient Mamba-2 model for inference. Mamba-2 is known\nfor its linear extension and fast processing of long sequences. We replace the\nTransformer based backbone with a pre-trained Mamba-2 model and explore\nmethods for integrating 2D visual selective scanning mechanisms into multimodal\nlearning. We also try various visual encoders and Mamba-2 model variants. Our\nextensive experiments conducted in various multimodal benchmark tests have\ndemonstrated the competitive performance of ML-Mamba and highlighted the\npotential of state space models in multimodal tasks. The experimental results show\nthat: (1) ML-Mamba achieves performance comparable to state-of-the-art methods\nsuch as TinyLaVA and MobileVLM v2 through its linear sequential modeling,\nwhile also having faster inference speed; (2) ML-Mamba performs well in visual\nhallucinations and spatial relationship judgment in closed set benchmark tests;\n(3) ML-Mamba achieves performance comparable to LLaVA while reducing the\nnumber of parameters by 40%.(4) Compared to the multimodal model using the\noriginal Mamba model, the Mamba-2 based large-scale multimodal language model\nhas stronger inference performance and effectiveness.", "sections": [{"title": "Introduction", "content": "The emergence of Large Language Models (LLMs) has profoundly changed the landscape of natural\nlanguage understanding tasks. Unlike early methods that relied on medium-sized task specific models,\nrecent advances have shifted towards using general large-scale models, especially after the success\nof systems such as ChatGPT. It has been proven that expanding the scale of language models and\nincreasing data volume can bring many advantages, including enhancing the performance of different\ntasks and improving the sample efficiency of out of distribution generalization [19].\n\nHowever, traditional LLMs are limited to interacting through language, which limits their adaptability\nto handling more diverse tasks. Multi modal understanding that integrates visual and textual infor-\nmation is crucial for improving the ability of models to effectively respond to real-world challenges.\nTherefore, researchers are actively expanding large-scale language models to integrate multimodal\ninformation processing capabilities. Visual language models (VLMs) such as GPT-4 [40], LLaMA\nadapter [11], and LLaVA [35, 34] have been developed to enhance LLM's visual comprehension\nability. These VLMs are fundamental models for handling a range of tasks, including visual question\nanswering (VQA), image captioning, and visual content generation.\n\nDespite achieving success, previous research has mainly focused on reducing the parameters of\nlanguage models while preserving the Transformer architecture. However, this method does not solve\nthe inherent problem of low computational efficiency in Transformer's self attention mechanism,\nwhich is quadratic with sequence length. To address this bottleneck, the latest research work has\ndesigned a new architecture (Mamba-2), whose core layer is an improvement of Mamba selective\nSSM. The state space model (SSM) has been widely studied as an effective alternative solution.\nSSM combines elements of Recurrent Neural Networks (RNNs) and Convolutional Neural Networks\n(CNNs), providing linear scaling of sequence length and effective training and inference. It is 2-8\ntimes faster and continues to compete with Transformers in language modeling.\n\nTo this end, this article proposes a new perspective, which is to directly use the state space model\n(SSM) as the backbone. Specifically, we use the Mamba-2 language model as the basic model of our\nVLM. In this article, we introduce ML-Mamba, a work that applies state space models to multimodal\nlearning tasks. Our method utilizes a pre-trained Mamba-2 language model as the backbone, replacing\ntraditional Transformer based models such as LLaMA [50] or Vicuna [4]. We further enhanced ML-\nMamba through a novel multimodal connector called Mamba-2 Scan Connector (MSC) architecture,\nwhich includes a Mamba-2 visual selective scanning module (MVSS) and a SwiGLU module\nspecifically designed for 2D causal modeling of enriched visual sequences. The MVSS module\nexplores two different scanning mechanisms: bidirectional scanning mechanism (BSM) and cross\nscanning mechanism (CSM). In addition, we investigated the combination of different visual encoders,\nvariants of pre-trained Mamba-2 language models, and multimodal connectors to optimize the\nintegration of visual and linguistic information.\n\nExtensive experiments conducted on various multimodal learning benchmarks have demonstrated\nthe effectiveness of ML-Mamba. Our model achieves competitive performance with other similarly\nsized small MLLMs, and even outperforms large MLLMs on some popular benchmark tests (such as\nLLaVA v1.5 [34] versions 7B and 13b).\n\nIn summary, our contributions are as follows:"}, {"title": "Related Work", "content": ""}, {"title": "Large Language Models (LLMs)", "content": "In recent years, significant breakthroughs have been made in natural language processing tasks [22,\n25], characterized by large model scales, typically containing billions of parameters, and training\nusing massive datasets. GLM [9], LLaMA [50], Alpaca [48], Vicuna [4] and other instruction\nfine-tuning versions have emerged one after another, with the goal of being comparable to the\nproprietary InstructGPT model without public access. At the same time, due to the significant\ncomputational requirements of large language models, research trends have shifted towards exploring\nthe possibility of smaller scale models, such as Stable LM [2], TinyLaMA [55], and Phi [17, 30],\nwhich have parameter sizes below 3 billion but can achieve comparable results to large models\nthrough high-quality data and feasible training methods."}, {"title": "State Space Models (SSMS)", "content": "State Space Models (SSMs) have demonstrated excellent performance in areas such as long sequence\nmodeling, image generation, and reinforcement learning. A notable feature of SSMs is their ability to\nperform efficient autoregressive inference like Recurrent Neural Networks (RNNs) while also being\nable to process entire input sequences in parallel like attention-based Transformers, thus enabling\nefficient training. Despite their efficiency, SSMs achieve good results in various sequence modeling\ntasks. Specifically, Albert et al. [16] proposed a structured state space sequence model for time series\nanalysis. Goel et al. [12] applied SSMs to audio generation and achieved satisfactory performance.\nAdditionally, the H3 model [10] was introduced to bridge the gap between SSMs and Transformers\nin language modeling.\n\nIn recent months, a new selective state space model called Mamba [15] has been proposed as a strong\ncompetitor to the Transformer architecture. Compared to LLMs of the same capacity, language\nmodels based on Mamba have shown competitive performance, faster inference speeds, and the\nability to scale linearly over time with constant memory usage. In May 2024, the latest Mamba\narchitecture (Mamba-2) [8] was introduced, featuring an improved core layer of the Mamba selective\nSSM, which is 2-8 times faster while continuing to compete with Transformers in language modeling."}, {"title": "Multimodal Large Language Model (MLLM)", "content": "The Multi Modal Large Language Model (MLLM) combines visual and linguistic information and\nhas achieved significant success in various fields [52, 53, 38]. However, the basis of these models\nis usually a known Transformer network, resulting in a square level computational complexity [23].\nIn order to improve the efficiency of the base model, ML-Mamba is proposed, which is an MLLM\nwith linear computational complexity. Specifically, ML-Mamba integrates the efficient Mamba-2\nlanguage model into visual modalities and explores different modal fusion strategies to create effective\nmultimodal Mamba-2 [8]. Experiments have shown that ML-Mamba not only competes with current\ncomputationally efficient MLLMs such as LLaVA Phi, TinyLaVA, and MobileVLM v2, but also"}, {"title": "Mamba in the field of vision", "content": "The successful application of Mamba in natural language processing (NLP) has inspired its adoption\nin visual applications. Vision Mamba (Vim) [59] utilizes Vim blocks composed of pure Mamba\nlayers: each Vim block models bidirectional representations using forward and backward scanning,\nand alleviates direction sensitivity issues in Mamba. Another approach, VMamba [36] utilizes Visual\nState Space (VSS) blocks that integrate Mamba and 2D convolutional layers, supported by a pyramid\narchitecture similar to Swin Transformer [37]: each VSS block first models 2D local information\nthrough 2D deep convolution as a token mixer, and then processes 2D global information horizontally\nand vertically through a cross scan module. Mamba ND [29] further extends the functionality of\nMamba to multidimensional data including images and videos. LocalMamba [20] segments the input\nimage into multiple local windows and executes a state space model (SSM) in various directions\nwithin these windows to enhance local processing capabilities. EfficientVMamba [43] introduced\nan efficient 2D scanning technique that reduces computational requirements by performing atrous\nsampling on feature map blocks. In addition to these newly designed Mamba architectures, our work\nalso draws inspiration from VL-Mamba [44], a multimodal large language model based on state\nspace models, which has shown great potential for long-sequence modeling with fast inference and\nlinear scaling in sequence length. Compared with these newly designed Mamba architectures, our\narchitecture closely follows Mamba's design ideas in the field of vision, enhancing the extraction of\nvisual features with the latest Mamba-2 module. Our main goal with the Mamba-2 based architecture\nis to enhance multimodal representation and inference capabilities."}, {"title": "Method", "content": "In this section, we first introduce the basic concepts of State Space Models (SSMs) (Sec. 3.1).\nSubsequently, we provide a detailed description of the proposed ML-Mamba method (Sec. 3.2), which\nmainly comprises a visual encoder, a multi-modal connector called the Mamba-2 Scan Connector\n(MSC), an MLP projector, and the Mamba-2 large language model."}, {"title": "Mamba Preliminaries", "content": "The Mamba architecture inherits from state space sequence models [16], which models a 1-D function\nor sequence $x(t) \\in \\mathbb{R} \\rightarrow y(t) \\in \\mathbb{R}$ at time $t$ via expanded hidden states $h_t \\in \\mathbb{R}^N$. The hidden state is\nevolved through time driven by parameters $A, B, C$ following linear ordinary differential equations\n(ODES):\n\n$h'(t) = Ah(t) + Bx(t),$\n$y(t) = Ch(t).$ (1)\n\nTo discretize parameters in this continuous system, a common solution is to introduce a time scale\nparameter $\\Delta$ to transform continuous $A, B$ to discrete $\\bar{A}, \\bar{B}$ using zero-order hold (ZOH) model [41]:\n\n$\\bar{A} = \\exp(\\Delta A),$\n$\\bar{B} = (\\Delta A)^{-1}(\\exp(\\Delta A) - I) \\cdot \\Delta B.$ (2)\n\nBy applying such transformation, we can rewrite Eq. 1 as:\n\n$h'(t) = \\bar{A}h_{t-1}+ \\bar{B}x_t,$\n$Y_t = C h_t.$ (3)"}, {"title": "ML-Mamba Model", "content": ""}, {"title": "Overall Architecture", "content": "The architecture of Mamba consists of four main components: a pre-trained visual encoder, a\nrandomly initialized multi-modal connector called Mamba-2 Scan Connector (MSC), and a pre-\ntrained large language model (Mamba-2 LLM), as shown in Fig. 2. With an image as input, visual\nfeatures are first extracted through the visual encoder. The extracted sequence of visual features\nis then fed into the multi-modal connector (MSC), whose output is mapped to the LLM using a\nmulti-layer perceptron (MLP) projector. The output vector from the visual projector is then combined\nwith tokenized text queries and input into the Mamba-2 LLM. Finally, the Mamba-2 LLM generates\nthe corresponding response."}, {"title": "Vision Encoder", "content": "We integrate DINOv2 [42] and SigLIP [54] to serve as our vision backbone. The rationale behind\nthis fusion is that combining the low-level spatial features captured by DINOv2 with the semantic\nfeatures provided by SigLIP enhances performance on downstream tasks [49, 24]. Given an input\nimage $X \\in \\mathbb{R}^{C \\times H \\times W}$, the vision encoder divides the image into $N_2 = HW/P^2$ patches of\nequal size, where $P^2$ represents the patch size. Both vision encoders process the patchified image\nas an input token sequence and concatenate their outputs to form compact visual representations\n$V_{img} \\in \\mathbb{R}^{N \\times D_v}$:\n\n$V_{img} = [\\varphi_{SigLIP} (X_v); \\Upsilon_{DINOV2} (\\Chi_v)],$ (5)\n\nThese outputs are then channeled to a dedicated task-specific head, with $D_v$ representing the dimen-\nsionality of the tokens generated as described above."}, {"title": "MultiModal Connector", "content": "Multimodal connectors act between visual features and language models to ensure seamless integra-\ntion of visual and linguistic information. In this study, we explored a novel multimodal connector\ncalled Mamba-2 Scan Connector (MSC) architecture aimed at addressing the challenge of unclear\ncausal relationships in computer vision. The traditional state space model (SSM) is typically used to\nprocess sequence data with causal relationships, such as language sequences, but this approach is\nclearly not applicable to non causal visual sequences generated by visual encoders."}, {"title": "Mamba-2 Large Language Model", "content": "The Mamba-2 language model [8] serves as the primary language processing component responsible\nfor understanding and generating text. The workflow design of the visual encoder and multimodal\nconnector ensures that visual information can be effectively transmitted to the Mamba-2 language\nmodel, enabling the model to process and understand complex multimodal data.\n\n$R = f_L(V_{out}, f_r(Q)).$ (9)"}, {"title": "Training Process", "content": "The recent research [24] indicates that for existing training paradigms based on LLaVA [6, 34, 57]\n(i.e., training only the projection layer in the pre-alignment stage and fine-tuning the LLM backbone\nin one iteration each), we decided to first use a 558K subset of the LAION-CC-SBU dataset to\nalign the Mamba-2 Scan Connector (MSC) and the projector. During the fine-tuning stage, we\nsimultaneously optimized the Mamba-2 Scan Connector (MSC), the projector, and the Mamba LLM.\nThis comprehensive training effort was executed on 8 NVIDIA Tesla A100 GPUs. We conducted\nfine-tuning over two randomly sampled epochs using a composite dataset, which includes:\n\n1.  LVIS-Instruct-4V [51]: This dataset contains 220K images with visual alignment and context-\naware instructions generated by GPT-4V.\n2.  The Mixed Dataset Used in LLaVA v1.5: This dataset includes a total of 655K visual multi-\nround dialogue samples. It encompasses academic VQA samples [13, 21, 26, 46], as well as visual\ninstruction tuning data from LLaVA Instruction [45] and ShareGPT [35].\n\nOverall, the fine-tuning dataset consists of approximately 875K images and their corresponding\nmulti-turn dialogue data, as well as pure text dialogue data."}, {"title": "Experiment", "content": "We conducted a comprehensive experimental evaluation of ML-Mamba through four aspects: bench-\nmarking evaluation: We used six commonly used visual language model (VLM) benchmarks to\nevaluate the effectiveness of the proposed method. These benchmarks include four open-ended visual\nquestion answering tasks that require different reasoning abilities, as well as two closed set prediction\ntasks that involve determining spatial relationships of objects and detecting visual illusions.\n\n*   Efficiency evaluation: We conducted a comparative evaluation of ML-Mamba and other\n    Transformer based models at similar model sizes to validate our model's improvement in\n    efficiency.\n*   Ablation study: We further explored some design choices in the model structure through\n    ablation studies to determine which components have a significant impact on model perfor-\n    mance.\n*   Comparison of answer generation quality: We have provided specific examples to demon-\n    strate the comparison of our model with other models in terms of answer generation quality.\n    Through these experiments, we comprehensively evaluated the performance and advantages\n    of ML-Mamba."}, {"title": "Experimental Setup", "content": "Table 1 details the hyperparameters of the ML-Mamba model. For the visual encoder part, DINOv2\nadopts the same ViT structure as in its original paper, namely a ViT-Large model with 304M\nparameters, pretrained on the LVD-142M dataset. SigLIP uses a slightly larger shape-optimized\nversion than ViT-Large. The resolution of the input images is set to 384x384, with the number of\nvisual tokens being 729.\n\nThe backbone of the LLM is initialized using the pretrained weights from the Mamba-2 model,\nwhile the multimodal connectors (MSC) and projectors are always randomly initialized. We chose\nan open-source model weight from the Huggingface platform to initialize our model as the LLM\nbackbone for our proposed model.\n\nThe entire training process took approximately 31 hours on 8 NVIDIA A100 80GB GPUs. During\ntraining, we used Pytorch's fully shared data parallel framework [56] and adopted automatic mixed\nprecision with FP32 and BF16 for distributed training. The batch size was set to 64. We used the\nAdamW [39] optimizer and updated the network parameters using a learning rate with cosine decay.\nThe learning rate was set to 2 \u00d7 10\u20135, the decay factor was 0.1, and the warm-up ratio was 0.03. The\nmodel was trained for 2 epochs with supervised fine-tuning."}, {"title": "Results", "content": "In addition, we further evaluated the model on six carefully designed metrics, particularly VizWiz [18]\nand VQAv2 [13], for assessing general visual reasoning ability. VizWiz includes common sense\nquestions and unanswerable questions, requiring the model to avoid incorrect answers to evaluate\nits reliability. GQA evaluates spatial understanding and multi-step reasoning in real-world images.\nThe issues in TextVQA are related to the text in the image, evaluating the model's optical character\nrecognition (OCR) and inference capabilities. POPE provides a benchmark for evaluating object\nhallucinations and is a binary classification task that prompts the model to answer whether the\nobject exists. We also introduced two closed set prediction benchmarks consisting of VSR [33] and\nPOPE [32]. VSR evaluates the model's ability to understand spatial relationships between different\nimages, while POPE evaluates the VLM's ability to avoid severe illusion problems. VSR and POPE\ncalculate scores based on the probability of providing the correct answer.\n\nWe evaluated VizWiz, VQAv2, and TextVQA using validation sets, while using the recommended\ntest dev partition for GQA, zero sample test partition for VSR, and evaluation partition for POPE.\n\nTo demonstrate the effectiveness of the model, we compared it with a VLM of the same scale with\napproximately 3B parameters, or with a larger VLM containing twice the number of parameters. As\nshown in Table 2, although Although ML-Mamba has only about 40% of its parameters, it performs\nwell on multiple benchmarks compared to LLaVA v1.5 7B, and even outperforms all models on\nPOPE.\n\nCompared with VLM with similar parameter numbers, ML-Mamba consistently achieved better\nperformance than LLaVA Phi in VQAv2, GQA, VQAT, POPE and VizWiz. While VL-Mamba\nperforms better on VQAv2, our ML-Mamba outperforms VL-Mamba on GQA, VQAT, and POPE.\nMobileVLM is another parallel work aimed at producing small-scale LLMs, and is therefore also\nintroduced in experiments. In summary, these results indicate that ML-Mamba matches the per-\nformance of state-of-the-art models at the same level (~3B) on multiple benchmarks and remains\ncompetitive when compared to larger scale models (7B and above).\n\nWe present some examples to illustrate the qualitative results of ML-Mamba. As shown in Fig. 7,\nML-Mamba effectively understands the user's questions and responds accurately."}, {"title": "Reasoning speed", "content": "In order to evaluate the efficiency advantage of the ML-Mamba model, especially the speed improve-\nment brought by its linear sequence modeling, we conducted a detailed inference speed comparison\nexperiment. In the experiment, we compared ML-Mamba with two baseline models of the same scale\nparameters, TinyLaVA 3B and MobileVLM v2 3B.\n\nAll models were evaluated in the same hardware environment, namely a single Nvidia A100 PCIe\n80GB GPU. Each model receives the same example image as input, with a unified image resolution\nof 336 \u00d7 336 pixels, and is processed by a CLIP encoder. For TinyLaVA, the model receives 576\nimage markers processed by the projector; MobileVLM v2 reduces the number of image labels to 144\nthrough LDP blocks. In contrast, ML-Mamba uses dual encoders to process images with a resolution\nof 384 \u00d7 384, resulting in an increase in the actual number of image labels processed to 729.\n\nIn the experiment, all models received the same question: \"Provide a detailed description of the\nimage.\" and set the number of output labels to 256. The total time is the entire process from image\nencoding until the complete answer is generated.And we calculated the average number of tokens\ngenerated per second by $Eval_{avg} = \\frac{256}{T_{total}}.$\n\nThe results from Fig. 1 and Table 3 demonstrated that although the number of image markers\nprocessed by ML-Mamba significantly increased, it still exhibited extremely fast inference speed.\nCompared to MobileVLM v2, although the latter has undergone multiple lightweight optimizations,\nthe time required for ML-Mamba to complete inference is only about 30% of the former. This"}, {"title": "Ablation Study", "content": ""}, {"title": "Effects of Language Model Variants", "content": "Table 4 presents the results of ablation experiments evaluating the effectiveness of different language\nmodel variants. We conducted experiments on three different variants, namely Mamba-2 with\nparameters of 780m, 1.3b, and 2.7b, trained on the Pile dataset (containing 300B tokens). Specifically,\nwe constructed a baseline model using the same variant of DINOv2+SigLIP as the visual encoder,\nMamba-2 language model as the backbone of a large language model, and a regular MLP multimodal\nconnector without a 2D visual selection scanning module. We can see that as the model size and\nnumber of training tokens increase, Mamba2-2.7B outperforms other variants on all benchmarks.\nTherefore, we chose Mamba2-2.7B for other experiments."}, {"title": "Effects of Different Visual Encoders", "content": "Recent research has found that although language image models similar to CLIP can provide rich\nsemantic information, they may lose detailed information about the image itself. Therefore, we\nfurther introduce DINOv2 as a supplementary encoder and connect the visual representations of\nthese two encoders for subsequent LLM. As shown in Table 5, the introduction of DINOv2 signifi-\ncantly improved the model performance in six benchmark tests. This result suggests a meaningful\nprinciple when selecting a visual encoder for downstream tasks. Therefore, we ultimately chose\nDINOv2+SigLIP as the visual encoder to construct our model and used it for further experiments.\nThrough this combination, we can achieve better performance on multiple benchmarks."}, {"title": "Ablation on different multimodal connector structures", "content": "We also explored the impact of different architectures of multi-mode connectors. We evaluated\nthree different MMC variants: MLP, MSC-MLP (Basic), and MSC-MLP (Advanced). As shown in\nTable 6, by comparing these three architectures, we observed that MSC-MLP (Advanced) performed\nrelatively better on most benchmark tests, especially on VQA, demonstrating the effectiveness of\ncombining MSC modules with swiGLU. Note that these models use DINOv2+SigLIP as the visual\nencoder, Mamba2-2.7B as the language model, and a bidirectional selective scanning mechanism."}, {"title": "Under different scanning mechanisms", "content": "We compared the bidirectional scanning mechanism (BSM) and cross scanning mechanism (CSM) in\nMMC modules. As shown in Table 7, although BSM and CSM perform similarly in some benchmark\ntests, such as scoring 76.6 in one test, BSM shows superior performance in most benchmark tests.\nThis highlights its advantages in handling 2D visual information for multimodal learning tasks."}, {"title": "Limitation", "content": "The training of ML-Mamba relies on specific multimodal datasets, which may have biases or\nincomplete coverage in certain aspects. Developing more comprehensive and diverse datasets, as well\nas improving data preprocessing and augmentation techniques, will help enhance the generalization\nability and applicability of ML-Mamba in different scenarios.\n\nML-Mamba currently faces challenges in running on mobile devices, especially in meeting the\nmemory usage requirements of these devices. In order to make ML-Mamba run more smoothly on\ndevices such as smartphones or tablets, further optimization, especially for low memory environments,\nis necessary."}, {"title": "Conclusion", "content": "This article introduces a novel multimodal learning model, ML-Mamba, which utilizes the latest state\nspace model (SSM) Mamba-2 to solve multimodal learning tasks. It uses a pre-trained Mamba-2\nlanguage model as the language model and introduces the multimodal connector Mamba-2 Scan\nConnector (MSC) module to bridge the gap between 2D non-causal image information and the\ninherent causal modeling ability of SSM. By conducting comprehensive experiments and ablation\nstudies, ML-Mamba performed well in multimodal benchmark testing, demonstrating its effectiveness\nand the potential of SSM in multimodal learning. On the other hand, ML-Mamba addresses the\nefficiency bottleneck of existing multimodal large language models by using models with linear\ncomputational complexity. This significantly improves computational efficiency and excels in\nvisual illusion and spatial relationship judgment while reducing the number of parameters. These\nadvancements open new possibilities for deploying high-performance AI models in environments\nthat process visual information at high frequencies."}]}