{"title": "TituLLMs: A Family of Bangla LLMs with Comprehensive Benchmarking", "authors": ["Shahriar Kabir Nahin", "Rabindra Nath Nandi", "Sagor Sarker", "Quazi Sarwar Muhtaseem", "Md Kowsher", "Apu Chandraw Shill", "Md Ibrahim", "Mehadi Hasan Menon", "Tareq Al Muntasir", "Firoj Alam"], "abstract": "In this paper, we present TituLLMs, the first large pretrained Bangla LLMs, available in 1b and 3b parameter sizes. Due to computational constraints during both training and inference, we focused on smaller models. To train TituLLMs, we collected a pretraining dataset of approximately ~ 37 billion tokens. We extended the Llama-3.2 tokenizer to incorporate language- and culture-specific knowledge, which also enables faster training and inference. There was a lack of benchmarking datasets to benchmark LLMs for Bangla. To address this gap, we developed five benchmarking datasets. We benchmarked various LLMs, including TituLLMs, and demonstrated that TituLLMs outperforms its initial multilingual versions. However, this is not always the case, highlighting the complexities of language adaptation. Our work lays the groundwork for adapting existing multilingual open models to other low-resource languages. To facilitate broader adoption and further research, we have made the TituLLMs models and benchmarking datasets publicly available.", "sections": [{"title": "1 Introduction", "content": "The rapid advancements in large language models (LLMs) have reshaped the field of artificial intelligence, showcasing remarkable versatility across numerous tasks (Brown et al., 2020; Ouyang et al., 2022; Achiam et al., 2023; Chowdhery et al., 2023). These models demonstrate not only an ability to perform various NLP tasks but also an intriguing potential for self-assessment and continuous improvement (Liu et al., 2023b; Fu et al., 2023; Chiang et al., 2023).\nDespite these advancements, LLM development-both open and closed\u2014has predominantly focused on multilingual models, with a stronger emphasis on high-resource languages (Achiam et al., 2023; Touvron et al., 2023). While some models have extended coverage to medium- and low-resource languages (Le Scao et al., 2023; \u00dcst\u00fcn et al., 2024; Yang et al., 2024; Team, 2024), their representation remains limited. Although some initiatives have aimed to train language-centric LLMs (Sengupta et al., 2023; Team et al., 2025), these efforts remain scarce due to the high costs associated with computational resources and data collection. Consequently, recent research has shifted towards adapting existing LLMs for new languages (Levine et al., 2024; Team et al., 2025).\nSimilarly, in the context of benchmarking LLMs, most efforts have primarily focused on high-resource languages (Bang et al., 2023; Ahuja et al., 2023), while low-resource languages, such as Bangla, have received limited attention (Kabir et al., 2024; Zehady et al., 2024; Bhattacharjee et al., 2023). Zehady et al. (2024) developed LLMs for Bangla using Llama, leveraging only the Bangla subset of CulturaX (Nguyen et al., 2024), which consists of 12.4 million diverse Bangla news articles. They further fine-tuned their model on 172k instruction samples from subsets of the Alpaca (Taori et al., 2023) and OpenOrca (Lian et al., 2023) datasets, which were translated from English. Their models were benchmarked on 120 queries across nine different generation tasks.\nAddressing this gap is crucial, as linguistic and cultural diversity significantly impact language understanding and generation. Therefore, in this study, we focus on adapting existing LLMs (e.g., Llama) by expanding the model's vocabulary and performing continual pretraining. This process required extensive data collection from diverse sources. Given the relatively low availability of digital content in Bangla, we also developed synthesized datasets to supplement our training data.\nBenchmarking LLM capabilities for Bangla remains challenging due to the lack of specialized datasets, particularly in areas such as world knowledge and commonsense reasoning. Although some efforts have been made to generate such datasets through translation (Lai et al., 2023), they remain limited in scope. To address this gap, we have developed several native and translated datasets. Compared to Zehady et al. (2024), our pretraining corpus is significantly larger (~ 37b tokens) and is benchmarked on five different datasets covering world knowledge and commonsense reasoning, with a total dataset size of 132k entries.\n\u2022 We developed and released two models, TituLLMs, adapted from Llama 3.2, which will enable future research.\n\u2022 We provide a complete data collection recipe for pretraining LLMs including sources, approaches to synthetic data generation.\n\u2022 We extended tokenizer to ingest language specific knowledge.\n\u2022 We developed five datasets to benchmark LLMs capabilities in terms of world knowledge, commonsense reasoning, and reading comprehension. Such datasets will serve as a first step to Benchmark LLMs for Bangla.\n\u2022 We proposed a novel translation techniques that helps to develop high quality benchmarking dataset.\n\u2022 Using the benchmaked datasets we benchmark various LLMs including TituLLMs comparing performance across models to assess understanding of Bangla language.\nOur study reveals several interesting findings:\nVocabulary Extension: We explore the impact of vocabulary extensions on the base Llama Tokenizer by increasing the number of new tokens from 32K to 96K in increments of 16K. We found that average tokens per word (TPW) decreases as the number of tokens increases up to a certain point, after which it declines only minimally. Therefore, when adding new tokens, we must also consider the fertility rate to balance the trade-off between training and inference.\nCommonsense Capability: TituLLMs demonstrates strong commonsense knowledge but has limited capability in world knowledge (e.g., Bangla MMLU). Further training with instruction fine-tuning may enhance its performance in this area."}, {"title": "2 Pretraining Data", "content": "Pretraining data for Bangla is very limited compared to very high quality data available for English and other high resource languages (Penedo et al., 2023; Soldaini et al., 2024). Hence, we needed collect pretraining dataset for training TituLLMs. We have compiled a substantial Bangla raw dataset from a diverse range of sources, encompassing both formal and informal linguistic styles. The dataset is primarily derived from three key sources: web documents, books, and synthetically generated text. The synthetically generated data includes translated data, transliterated data, audio transcripted data, etc. An outline of our data collection and preprocessing pipeline is show in Figure 5. The final high-quality dataset amounts to approximately 268 GB, with 22 GB designated as a validation set, sampled proportionally to maintain the heterogeneity of the data. In total, the corpus contains ~ 37 billion tokens, optimized for training and evaluation across various Bangla language processing applications. In Table 3 (in Appendix) and in Figure 3, we report the distribution of tokens for different sources."}, {"title": "2.1 Web Documents", "content": "We curated the Common Crawl (Raffel et al., 2020) dataset and followed multiple steps to extract and clean the final dataset, as illustrated in Figure 5. Below, we briefly discuss each step.\nSQL Query Engine: Using Amazon Athena,2 we queried the vast Common Crawl dataset to isolate Bangla-specific HTML data and URLs. We applied filtering based on content language, URL patterns indicative of Bangla domains (e.g., .gov.bd), and host information, covering data from 2017 to 2024.\nText Extraction: We used Trafilatura (Barbaresi, 2021) tool for its effectiveness in extracting structured, clean text from HTML. This step preserved"}, {"title": "2.2 Books", "content": "We have compiled a diverse collection of open-source Bangla books, primarily in PDF format, spanning a broad temporal range from historical to contemporary works. Below we briefly discuss the steps taken to extract the text from the book collection.\nRaw Text: For digitally available texts, we directly extract the machine-readable content, requiring minimal processing due to the already high quality of these formats.\nPDF Images: To digitize a vast collection of non-digital and older texts from books, we utilize two leading Optical Character Recognition (OCR) systems: Google OCR\u00b3 and Tesseract\u2074. We used Tesseract to reduce the cost. These texts, often derived from sources that have deteriorated over time or originated in non-digital formats, pose significant challenges in terms of quality and legibility. To extract high-quality text, we implement carefully designed techniques comprising several steps.\nText extraction using Google OCR: For the majority of books, we utilized Google OCR to extract"}, {"title": "2.3 Synthetic Data", "content": "Due to the low representation of digital content in Bangla, we have developed a large-scale synthetic dataset for Bangla, which include transcription, translation and transliterated data.\nTranscribed Text: We collected conversational and spoken language data transcribed using the Bangla Automatic Speech Recognition (ASR) system (Nandi et al., 2023). This system enables us to capture various colloquial and regional linguistic variations in Bangla. We collected approximately 56k hours of speech data from diverse online sources. All collected speech data were transcribed using the ASR system.\nTranslation Data: To collect English-to-Bangla translated data, we trained an NLLB-based (600M-Distilled) model (Team et al., 2022) with the goal of developing a smaller, language-pair-specific (en-bn) model. We decided to train a translation model because our observations indicate that currently available multilingual models, such as Llama-3.1-8B-Instruct, have limited capability for Bangla-specific generation tasks. However, they have shown superior performance in English-specific generation.\nFor training the en-bn machine translation (MT) model, we collected open-source translation data from various platforms, including BanglaNMT (Hasan et al., 2020a) and Samanantar (Ramesh et al., 2022). Furthermore, we generated synthetic bn-en translation pairs using Bangla news sources and Wikipedia as source data, employing Llama-3.1-8B-Instruct (Touvron et al., 2023) for target data generation. We selected Llama for this task due to its superior English-language capabilities.\nUsing this approach, we created a dataset comprising approximately 60 million translation pairs, which we then used to train the NLLB model. On our in-house test dataset, the BLEU score of this model is 37.6. Once the model has been trained, we have used it to translate a corpus of English news articles 5 into Bangla.\nTransliteration Data: The use of romanized text is very common in everyday communication for Bangla (Fahim et al., 2024). To address this, we have developed a Bangla-to-Romanized Bangla dataset by training an NLLB-based (600M-Distilled) model. For model training, we collected transliteration pairs from the Sangraha dataset and generated additional synthetic transliteration pairs using the GPT-4 model (Achiam et al., 2023). We then used this dataset to train the NLLB-based transliteration model. The BLEU score for this model is 65.1, as evaluated on an in-house test dataset. We then used this model to create the transliteration dataset by selecting a small subset of collected Bangla Wikipedia articles.\nConversational Data: To enhance the model with conversational capabilities, we enriched our dataset by incorporating conversational data. We have crawled topics (e.g., \"Rabindranath Tagore's contributions to Bengali art\") from Wikipedia and Banglapedia on which we generated conversations between two agents. To achieve this we have developed an agentic system where two agents interacted each other on a given topic."}, {"title": "2.4 Sangraha Dataset", "content": "Additionally, we enriched our dataset by integrating the open-source Sangraha dataset (Khan et al., 2024). Sangraha is the largest high-quality, cleaned Indic language corpus. We incorporated the Bangla portion of the Sangraha dataset into our training set."}, {"title": "3 Pretraining", "content": "We developed a custom tokenizer for Bangla text using Tiktoken, which employs Byte Pair Encoding (BPE) (Sennrich, 2015) for tokenization. To train this tokenizer, we sampled 48 GB of data from our pretraining Bangla corpus. Additionally, we modified the original Tiktoken codebase to enhance its efficiency and better accommodate the morphological complexities of Bangla. After training multiple tokenizers on the same subset, we merged each newly trained tokenizer with the existing Llama-3.2 tokenizer. This merging process aimed to preserve the strengths of the original tokenizer while integrating domain-specific vocabulary and improving segmentation for Bangla. To evaluate the performance of each merged tokenizer, we computed the average tokens per word (TPW) on a separate 1 GB sample from the original corpus. Table 4, in Appendix, summarizes the TPW values for both the original Llama-3.2 tokenizer and the newly trained tokenizers. A lower TPW generally indicates more efficient segmentation, which reduces sequence lengths and may enhance downstream model performance.\nWe trained five tokenizers with different vocabulary sizes, as presented in Table 4. Each of these tokenizers was then merged with the Llama-3.2 tokenizer to create five new tokenizers. Notably, the Llama-3.2 tokenizer exhibits a very high TPW value, which affects its performance for Bangla. In contrast, the newly developed tokenizers demonstrate significantly lower TPW values.\nThe table also shows that increasing the vocabulary size of the new tokenizers generally results in a lower TPW count. However, the relationship between vocabulary size and TPW is not strictly linear. While TPW decreases with larger vocabularies, the reduction becomes less significant for tokenizers with very large vocabulary sizes."}, {"title": "3.1 Tokenizer Training", "content": "We developed a custom tokenizer for Bangla text using Tiktoken, which employs Byte Pair Encoding (BPE) (Sennrich, 2015) for tokenization. To train this tokenizer, we sampled 48 GB of data from our pretraining Bangla corpus. Additionally, we modified the original Tiktoken codebase to enhance its efficiency and better accommodate the morphological complexities of Bangla. After training multiple tokenizers on the same subset, we merged each newly trained tokenizer with the existing Llama-3.2 tokenizer. This merging process aimed to preserve the strengths of the original tokenizer while integrating domain-specific vocabulary and improving segmentation for Bangla. To evaluate the performance of each merged tokenizer, we computed the average tokens per word (TPW) on a separate 1 GB sample from the original corpus. Table 4, in Appendix, summarizes the TPW values for both the original Llama-3.2 tokenizer and the newly trained tokenizers. A lower TPW generally indicates more efficient segmentation, which reduces sequence lengths and may enhance downstream model performance.\nWe trained five tokenizers with different vocabulary sizes, as presented in Table 4. Each of these tokenizers was then merged with the Llama-3.2 tokenizer to create five new tokenizers. Notably, the Llama-3.2 tokenizer exhibits a very high TPW value, which affects its performance for Bangla. In contrast, the newly developed tokenizers demonstrate significantly lower TPW values.\nThe table also shows that increasing the vocabulary size of the new tokenizers generally results in a lower TPW count. However, the relationship between vocabulary size and TPW is not strictly"}, {"title": "3.2 Model Architecture", "content": "We have modified llama-3.2-1b and llama-3.2-3b models according to the merged tokenizers. As too many new tokens will increase the model's size and the training complexity, we modified the models according to llama-3.2-plus-48K tokenizer. We added extra embedding vectors in the embedding layer and modified the lm-head according to the vocabulary size."}, {"title": "3.3 Pretraining", "content": "After modifying the models, we pre-trained them on our full dataset using LlamaFactory (Zheng et al., 2024). Both models were trained with a context length of 4096, with packing enabled for maximum efficiency. Training for one epoch required 1750 H100 GPU hours."}, {"title": "4 Evaluation", "content": "For evaluation, we utilized the lm-evaluation-harness. We used normalized accuracy as a metric. Our assessment focuses on key aspects such as knowledge and reasoning."}, {"title": "4.1 Evaluation Setup", "content": "For evaluation, we utilized the lm-evaluation-harness. We used normalized accuracy as a metric. Our assessment focuses on key aspects such as knowledge and reasoning."}, {"title": "4.2 Banchmarking Datasets", "content": "We benchmarked TituLLMs alongside other popular LLMs using five newly prepared evaluation datasets. Below, we describe the development process for each dataset. Table 1 presents the distribution and splits of each dataset.\nBangla MMLU: We curated multiple-choice questions from various opensource educational websites and textbooks, inspired by the original MMLU dataset (Hendrycks et al., 2020). The dataset includes multiple-choice questions from different Bangladeshi exams, such as job exams, the Bangladesh Civil Service Exam, and undergraduate admission exams. In Figure 7, we report category wise distributions.\nCommonsenseQA Bangla (CSQA): We translated the CommonsenseQA dataset (Talmor et al., 2018) into Bangla using our custom translation-based approach, Expressive Semantic Translation (EST). This method generates multiple translations for a"}, {"title": "5 Results and Discussion", "content": "We evaluate each model in 0-shot and 5-shot settings to assess their few-shot adaptability. Table 2 presents the detailed results for all models, including the TituLLMs variants. Table 2 shows the accuracy of various models with less than or equal to 3b parameters and the GPT-davinci-002 model.\nBangla MMLU: In the 0-shot setting, both the TituLLM-1b and TituLLM-3b models score 0.25, placing them in the mid-range relative to other 1b-3b models in the Bangla MMLU benchmark. Neither model shows gains when moving to the 5-shot setting (both remain at 0.25), suggesting that additional examples do not substantially improve performance for this specialized knowledge benchmark. It is possible that the domain-specific knowledge required for MMLU-like tasks is not adequately captured by our model. The primary reason behind this can be the lack of extensive pertaining. We have trained our model with only ~ 37b tokens for one epoch. As a result, the model could not capture the full knowledge base (Hoffmann et al., 2022). Another reason behind this can be diversity in datasets. For example, Llama and Qwen models are trained on a high volume of English datasets that have helped these models to have a better knowledge base.\nBoolQ: The BoolQ dataset measures the performance of the model for yes/no question-answering in Bangla. TituLLM-1b achieves 0.53 in the 0-shot setting but drops slightly to 0.51 in the 5-shot setting. In contrast, TituLLM-3B moves from 0.53 (0-shot) to 0.54 (5-shot). However, Llama-3b and Qwen-2.5-1.5b have done much better in this task. As the context length for all BoolQ data was as large as a News document, TituLLM's performance may drop for long contexts. It suggests further pertaining to the model should be done with long contexts (Chowdhery et al., 2022; Kaplan et al., 2020).\nCSQA, OBQA, and PIQA: Commonsense reasoning tasks often challenge smaller-scale language models. The accuracy of the 3B variant of TituLLM, starts at 0.28 (0-shot) and exhibits a more pronounced jump to 0.33 (5-shot) which is the maximum among all models. TituLLM-1b has also shown decent performance on the CSQA dataset.\nOBQA requires both textbook knowledge and reasoning. Similar to CSQA, TituLLM-3b shows superior performance in this dataset 0.35. Both the dataset's results suggest that TituLLM's reasoning capability is better than other base models.\nPIQA tests physical commonsense knowledge. TituLLM-3b model shows better performance in this task too with an accuracy of 0.60. By observing the results on the CSQA, OBQA, and PIQA datasets we can say that the model has captured Bangla Language specific reasoning well in spite of being trained with a smaller dataset than others but the results from MMLU and BoolQ shows the impact of limited training.\nPerformance of tokenizer: The superior performance of our models in reasoning tasks is mainly an impact of our extended tokenizer. To justify this, we can observe the results of the BongLLaMa models. These models are continual pretrained models with existing open-source Bangla text corpus. If only the dataset could improve the performance then that would be reflected in BongLLaMA models. But we can see they are performing similarly to Llama models. To have an interpretation of our extended tokenizer's performance we can look into Figure-4. The figure shows Llama tokens and TituLLM tokens for a simple sentence in Bangla with two of the most common words. We can see that Llama tokenizer splits the text into character and byte levels. On the other hand, TituLLM tokenizes the sentence into word or subword levels. As a result, TituLLM can deliver more meaningful tokens than Llama for Bangla text. This is an important advantage of TituLLM that not only enables Tit-uLLM to perform better with smaller datasets but also ensures low latency during inference."}, {"title": "6 Related Work", "content": "Pretraining: Pretraining LLMs on Bangla has involved the development of specialized models like BongLLaMA (Zehady et al., 2024), which has been adapted from Llama to better understand and generate Bangla text. The pretraining phase typically leverages large-scale Bangla corpora to improve the model's foundational understanding of the language's syntax and semantics. For instance, Zehady et al. (2024) focused on developing a robust model by pretraining on diverse Bangla data sources, significantly improving the model's performance on native text. Similar efforts have been made in previous research, such as BanglaBERT (Bhattacharjee et al., 2022) and SahajBERT (Diskin et al., 2021), where models underwent extensive pretraining on curated Bangla datasets to better capture linguistic nuances.\nEnhancing Tokenization: The evolution of token adaptation in NLP has progressed from linguistic cues and statistical methods (Creutz and Lagus, 2006; Luong et al., 2013; Zhang et al., 2023) to phrase-level segmentation (Koehn et al., 2007, 2003). The rise of deep learning shifted the focus to subword-level segmentation, enhancing the handling of rare words (Sennrich, 2015; Kudo, 2018; Kudo and Richardson, 2018). More recent efforts emphasize integrating specialized vocabularies into pre-trained LLMs, prioritizing tokenization quality and cost-effectiveness (Ahia et al., 2023; Zhang et al., 2023, 2024; Tejaswi et al., 2024). Liu et al. (2023a) propose a model-agnostic approach for adapting extended vocabularies to LLMs by integrating task-specific vocabularies, prioritizing new tokens, and initializing their embeddings using averaged subword representations. Cui et al. (2023) extend Llama's existing vocabulary by incorporating an additional 20k Chinese tokens, enhancing its ability to understand and generate Chinese text. Chiappe and Lennon develop an adaptive tokenization algorithm that implements a dynamic tokenization"}, {"title": "Cross-Lingual Model Adaptation", "content": "Cross-lingual transfer enables models trained in one language to adapt to others without retraining from scratch. Key adaptation techniques include embedding initialization, transliteration, and vocabulary extension. Jaavid et al. (2024) used transliteration to convert non-Latin languages into Latin scripts for better knowledge transfer. Zhao et al. (2024) trained a model on millions of target-language tokens without vocabulary extension, achieving performance comparable to models trained on billions of tokens. However, tokenization mismatches reduced inference efficiency. Studies by Csaki et al. (2023); Cui et al. (2023); Raffel et al. (2020); Lin et al. (2024) found that vocabulary extension improves performance while reducing computational inefficiencies. Tejaswi et al. (2024) further explored language-specific LLMs, highlighting trade-offs in adaptation for low-resource languages. Their findings emphasize that while vocabulary expansion enhances efficiency, selecting the right base model and vocabulary size is crucial."}, {"title": "Benchmarking and Evaluation", "content": "Evaluating LLMs requires benchmarking datasets that assess a wide range of capabilities and tasks. For Bangla, most existing datasets focus on standard NLP tasks. The BanglaNLG benchmark dataset (Bhattacharjee et al., 2023) addresses this by integrating six distinct datasets designed to evaluate various aspects of natural language generation (NLG) in Bangla, including Machine Translation, Text Summarization, Question Answering, Multi-turn Dialogue, News-Headline Generation, and Cross-lingual Summarization. Beyond NLG, the Region-Specific Native-QA dataset (Hasan et al., 2024) was developed to assess the question-answering capabilities of leading LLMs, such as GPT-40, GPT-4, Gemini, Llama-3, and Mistral. By focusing on regionally relevant queries, this dataset ensures that models are tested in real-world Bangla language contexts. For a broader evaluation of LLMs across multiple tasks, BenLLM (Kabir et al., 2024) provides the most comprehensive comparison of model performance. This study benchmarks LLMs against other pretrained models using datasets from diverse sources, offering insights into their strengths and limitations across various NLP tasks.\nThere is a significant lack of benchmarking datasets for evaluating LLMs. To address this gap, our study developed five benchmarking datasets, each designed to assess different capabilities, including world knowledge and commonsense reasoning."}, {"title": "7 Conclusion", "content": "In this study, we present the first pretrained Bangla LLMs, Titulm, trained on ~ 37b tokens by adapting Llama-3.2 models. We extended the tokenizer to incorporate language- and culture-specific knowledge, which also enable faster training and inference. Pretraining data collection remains challenging for languages with low digital representation. To address this, we provide a comprehensive approach, including raw web data collection, translation, and synthetic data generation. Given the lack of LLM-based benchmarking datasets, we developed five datasets comprising 137k samples, covering both knowledge and reasoning. The benchmarking dataset includes manually curated samples as well as a novel translation-based (EST) approach. Using these datasets, we benchmarked various LLMs, including Titulm, demonstrating its superior performance in reasoning tasks without instruction tuning. Future work includes collecting larger pretraining datasets and fine-tuning with instruction-based datasets. We have made our models publicly available, and to support reproducibility, we plan to release training recipes and benchmarking datasets."}, {"title": "8 Limitations", "content": "There are two limitations in this work. Firstly, despite the improvements observed in the 3b variant, the model's performance on long contexts remains suboptimal, suggesting the need for further enhancement in handling extended sequences. Secondly, while the current models are trained solely on Bangla text, their performance could benefit from incorporating larger, English-centric datasets. This could facilitate better knowledge leveraging and potentially improve low-resource language performance, indicating a direction for future research. Since there is a lack of instruction tuning data in Bangla, we do not explore the full potential of instruction tuning, which could have further improved the models' performance on specialized tasks and domain adaptation."}, {"title": "Ethical Consideration", "content": "We do not anticipate any ethical concerns in this study. All datasets used were collected from publicly available sources, ensuring compliance with ethical research standards. No personally identifiable information (PII) was gathered or utilized in the development of our models. While we do not foresee any potential risks arising from the outcomes of this study, we strongly encourage users of the released models to adhere to responsible AI usage guidelines. This includes avoiding the generation or dissemination of harmful, misleading, or biased content and ensuring that the models are employed in ethical and socially beneficial applications."}, {"title": "C Expressive Semantic Translation (EST)", "content": "The EST method innovatively enhances neural machine translation by infusing expressiveness and contextual relevance through an iterative refinement process utilizing LLMs. The EST method encompasses several pivotal steps:\nInitial Translation: A standard translation model M converts text from a source language L\u2081 into a preliminary translation to in the target language L2, which often lacks expressiveness and contextual depth.\nEnhanced Linguistic Refinement: Employing multiple LLMs, the initial translation to is refined into diverse candidate translations that exhibit greater naturalness and idiomatic correctness in L2.\nQuality Diversity: This phase synthesizes the best elements from the candidate translations through a prompt-based evaluation method, aiming to construct a translation that faithfully represents the nuances of L2."}, {"title": "C.1 Expressive Semantic Translation", "content": "The EST method innovatively enhances neural machine translation by infusing expressiveness and contextual relevance through an iterative refinement process utilizing LLMs. The EST method encompasses several pivotal steps:\nInitial Translation: A standard translation model M converts text from a source language L\u2081 into a preliminary translation to in the target language L2, which often lacks expressiveness and contextual depth.\nEnhanced Linguistic Refinement: Employing multiple LLMs, the initial translation to is refined into diverse candidate translations that exhibit greater naturalness and idiomatic correctness in L2.\nQuality Diversity: This phase synthesizes the best elements from the candidate translations through a prompt-based evaluation method, aiming to construct a translation that faithfully represents the nuances of L2."}, {"title": "C.2 Evaluation", "content": "Data Selection: The dataset that has been utilized here for evaluation was taken from (Hasan et al., 2020b) where a customized sentence segmenter was used for Bangla and two novel methods, namely aligner ensembling and batch filtering, were used to develop a high-quality parallel corpus of Bangla and English with 2.75 million sentence pairings. The 1000 pairs that comprise up the test set of this data were created with extensive quality control and used in this assessment.\nEvaluation metrics: We employ the BLEU Score (Papineni et al., 2002), SacreBLEU Score (Post, 2018), BERT Score (F1) (Zhang et al., 2020), and an LLM-Based Evaluation.\nLLM-based evaluation: This approach uses a large language model (GPT-40) to assess translations qualitatively. The LLM is instructed by the given prompt to assess a Bangla translation against a reference text using the following criteria: accuracy (measures translation accuracy and semantic richness), fluency, readability, and faithfulness. A score between 1 and 10 is then assigned by the LLM, along with a rationale for each translation. Finally, an average score is computed for each translation method. LLM-based evaluation is more flexible and human-like than token-based methods since it may assess more semantic variations and fluency.\nResult:\nOur comparison of the EST method against industry-standard models like Google Translation API and advanced systems like GPT-40 and Gemini highlights EST's superior performance. As shown in Table 5, EST leads with remarkable"}, {"title": "D Conversation Data Generation Prompt", "content": "Our methodology for generating Bangla conversational texts involves two specialized roles: the Junior Content Writer and the Senior Content Writer as highlighted in Figure 6. The Junior initiates dialogues based on culturally significant topics. The Senior meticulously reviews these texts to ensure grammatical precision and enhance quality. This structured approach enables replicable, high-standard conversation generation for NLP research."}, {"title": "E Benchmarking Datasets", "content": "In Figure 6, we present the overall distribution of the benchmarking dataset, which consists of approximately 132k entries.\nThe dataset is composed of multiple subsets of the benchmarking set, including Bangla MMLU (87,869 entries), Piqa BN (17,177 entries), CommonsenseQA BN (10,962 entries), OpenBookQA BN (5,944 entries), and BoolQ BN (1,976 entries). This distribution highlights the significant dominance of the Bangla MMLU subset within the overall evaluation dataset.\nTable 7 provides a detailed breakdown of the Bangla MMLU dataset, which contains 116,503 questions spanning multiple educational categories. These include University Admission (62,906), Higher Secondary (32,512), Job Exams (12,864), Medical Admission (4,779), and Engineering Admission (3,442). The dataset reflects a diverse range of question types relevant to various levels of academic and professional assessments, making it a comprehensive benchmark for evaluating LLMs in Bangla educational contexts."}, {"title": "Document Unigram Entropy", "content": "Calculates the entropy of the unigram distribution, measuring lexical variety using the formula:\n\u03a3(x/total * log(x/total)"}]}