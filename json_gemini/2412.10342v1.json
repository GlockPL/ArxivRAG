{"title": "Iris: Breaking GUI Complexity with Adaptive Focus and Self-Refining", "authors": ["Zhiqi Ge", "Juncheng Li", "Xinglei Pang", "Minghe Gao", "Kaihang Pan", "Wang Lin", "Hao Fei", "Wenqiao Zhang", "Siliang Tang", "Yueting Zhuang"], "abstract": "Digital agents are increasingly employed to automate tasks in interactive digital environments such as web pages, software applications, and operating systems. While text-based agents built on Large Language Models (LLMs) often require frequent updates due to platform-specific APIs, visual agents leveraging Multimodal Large Language Models (MLLMs) offer enhanced adaptability by interacting directly with Graphical User Interfaces (GUIs). However, these agents face significant challenges in visual perception, particularly when handling high-resolution, visually complex digital environments. This paper introduces Iris, a foundational visual agent that addresses these challenges through two key innovations: Information-Sensitive Cropping (ISC) and Self-Refining Dual Learning (SRDL). ISC dynamically identifies and prioritizes visually dense regions using a edge detection algorithm, enabling efficient processing by allocating more computational resources to areas with higher information density. SRDL enhances the agent's ability to handle complex tasks by leveraging a dual-learning loop, where improvements in referring (describing UI elements) reinforce grounding (locating elements) and vice versa, all without requiring additional annotated data. Empirical evaluations demonstrate that Iris achieves state-of-the-art performance across multiple benchmarks with only 850K GUI annotations, outperforming methods using 10x more training data. These improvements further translate to significant gains in both web and OS agent downstream tasks.", "sections": [{"title": "1. Introduction", "content": "Digital agents [16, 20, 34, 39, 40] are designed to automate tasks within interactive digital environments, such as web pages [11, 27, 36], software applications [10, 28], and operating systems [29, 30, 34]. These agents leverage environmental information and user intent to offer both indirect assistance (e.g., answering questions) and direct support (e.g., executing actions). Traditional textual agents [11, 37, 43], built on Large Language Models (LLMs) [1, 31-33], depend on predefined APIs or code snippets such as HTTP requests or structured data formats (HTML/XML). Since APIs and code vary across platforms and evolve over time, maintaining these agents requires frequent updates, limiting their scalability. In contrast, visual agents [9, 17, 20, 39], powered by Multimodal Large Language Models (MLLMs) [1, 4, 14, 15, 23, 26, 31], show promise in becoming more versatile. By mimicking human interaction methods-perceiving the Graphical User Interface (GUI) and performing actions within it-these agents can naturally achieve compatibility across a wide range of platforms.\nWhile the GUI as a universal interface simplifies the development of versatile digital agents, it simultaneously introduces significant challenges in both visual perception and functional understanding. These challenges appear in common GUI interfaces where visually prominent elements with simple functions coexist with visually subtle ones that require complex interactions. For instance, Fig. 1a shows how current training data focuses on large input boxes with straightforward text entry functions, while missing critical UI components like small menu buttons that control complex navigation hierarchies. When handling such cases, agents often fail at both visual detection due to the elements' subtle appearance and task execution due to their complex functionalities. These difficulties can be attributed to two key factors:\n\u2022 Architectural Limitations in Processing Heterogeneous GUI Information: Digital tasks often involve high-resolution interfaces (e.g., 1920x1080) that exhibit significant heterogeneity - combining densely packed UI elements with large, empty background areas. This uneven distribution of information creates dual challenges: visually, models struggle to maintain both fine-grained perception of dense regions and contextual awareness of the entire screen; functionally, they face difficulties in understanding complex dependencies between elements, especially within a limited computational budget. Current model architectures lack the flexibility to efficiently allocate computational resources based on this heterogeneous nature of GUI interfaces.\n\u2022 Annotation-Biased Training for GUI Understanding: The training process of digital agents often relies on annotated data that favors simple patterns - both visually (large, prominent UI components) and functionally (common operations like 'OK' or 'Cancel' buttons). This leads to limited understanding of both complex visual layouts and intricate functional relationships. Moreover, obtaining comprehensive annotations that cover both dense information regions and their functional interactions requires substantial manual effort and domain expertise, constraining the scalability of visual agents.\nTo address these challenges, in this paper, we introduce a foundational visual agent focused on GUI understanding and grounding, with Iteratively self-Refining and Information-Sensitive understanding capabilities, named Iris. Our approach addresses the challenges from two perspectives: 1) From the architectural perspective, Iris introduces an information-sensitive model design that efficiently processes high-resolution UI images with varying complexities, enhancing both visual perception and functional understanding; 2) From the training perspective, Iris develops a dual-learning strategy that iteratively refines the model's visual and functional knowledge using unlabeled data, enabling comprehensive GUI understanding without extensive annotations. To implement the information-sensitive architecture, we first propose Information-Sensitive Cropping (ISC), a dynamic cropping method that adapts to the distribution of visual information on the screen. Unlike previous methods [13, 38, 39] that segment images into equally sized sub-images and allocate the same number of tokens to each part, ISC employs a fast edge detection [7] algorithm to identify regions with high information density and selectively crop them. These cropped regions are enlarged, allowing the agent to allocate more tokens to represent detailed areas. Conversely, low-information areas are encoded with fewer tokens, reducing computational overhead. By balancing the token allocation based on the information content of each region, ISC enables the agent to efficiently process both high-resolution context and fine-grained details, enhancing its visual perception capabilities.\nTo enhance Iris's robustness in challenging scenarios, we introduce a Self-Refining Dual Learning (SRDL) strategy that leverages the synergistic relationship between two complementary tasks: referring (generating UI element descriptions) and grounding (locating elements from descriptions). This dual approach creates a positive feedback loop-improvements in referring lead to better understanding of visual and functional characteristics, which enhances grounding accuracy, while more precise grounding provides richer spatial and contextual information that yields more accurate descriptions. A key advantage of SRDL is its ability to autonomously identify and learn from challenging elements without additional annotated data, addressing the common training bias toward easily identifiable UI components. The process begins by detecting difficult cases where Iris struggles, identified through either visual complexities (revealed by ISC information patterns) or functional challenges (indicated by historical performance). For these cases, Iris initiates a dual-learning cycle: it generates element descriptions and attempts to locate them based on these descriptions, using any inconsistencies between predicted and actual locations as feedback for iterative refinement. This self-reinforcing process not only enhances Iris's ability to handle complex UI tasks independently but also develops a deeper understanding of UI characteristics that transcends simple object detection while reducing dependence on labeled training data.\nEmpowered by ISC and SRDL, Iris demonstrates that enhanced foundational GUI understanding capabilities directly improve performance on complex sequential decision-making tasks in multimodal agent scenarios. As shown in Fig. 1b, our method achieves significant improvements through complementary contributions: ISC delivers a 300% efficiency gain by reducing processing time from 3s to 1s, while SRDL enhances accuracy by 10% through"}, {"title": "3. Method", "content": "In this section, we present the design and implementation details of Iris, a foundational visual agent focused on GUI understanding and grounding. We first introduce the preliminary concepts and task formulation in Sec. 3.1, setting the foundation for the referring and grounding tasks. We then describe two core components that distinguish Iris: Information-Sensitive Cropping (ISC) in Sec. 3.2, which dynamically segments high-resolution screens for efficient processing, and Self-Refining Dual Learning (SRDL) in Sec. 3.3, which iteratively enhances the model's performance without external annotations.\n3.1. Preliminary and Task Formulation\nOur approach focuses on enhancing GUI understanding through two complementary tasks: referring and grounding. Each UI element is characterized by two key attributes: Position and Description. A UI element's position is defined by its bounding box coordinates p = (x1, Y1, x2,Y2), where (x1, y1) and (x2, y2) denote the top-left and bottom-right corners. The element's description D encompasses both its visual appearance (e.g., element type, displayed text) and functional role in the interface.\nBased on these attributes, we formulate two core tasks:\nReferring: Given a screen image and position p, generate a comprehensive description D of the UI element at that location. Let R(\u00b7) denote the referring function:\nD = R(p) (1)\nGrounding: Given a screen image and description D, locate the corresponding UI element's position. Let G(\u00b7) denote the grounding function:\np = G(D) (2)\nTogether, referring and grounding serve as the core tasks in the development of digital agents that can effectively understand and interact within GUI environments.\n3.2. Information-Sensitive Cropping\nThe core goal of Information-Sensitive Cropping (ISC) is to dynamically segment a high-resolution screen image into smaller, variable-sized sub-images based on the distribution of visual information. This adaptive approach ensures that each sub-image captures a relatively balanced amount of meaningful information, avoiding the pitfalls of uniform cropping strategies that can either neglect critical details or waste computational resources on irrelevant regions. The ISC process is structured into three key steps: information detection, adaptive cropping, and uniform resizing. Each step is described below.\nInformation Detection. We employ edge detection to identify visually significant regions, leveraging the observation"}, {"title": "3.3. Self-Refining Dual Learning", "content": "This section introduces Self-Refining Dual Learning, which allows Iris to autonomously discover and learn from difficult samples without relying on additional annotations. By leveraging the dual nature of referring and grounding tasks, the agent engages in self-play, iteratively refining its performance through a synergy-driven learning loop.\nDual-Learning Loop for Referring and Grounding. The core idea of the dual-learning loop is to exploit the complementary nature of referring (description generation) and grounding (element localization). Given a GUI image, Iris first enumerates all UI elements by prompting itself with \"What are the UI elements in this image?\", treating each element's name as its basic description. For each element, it performs grounding to locate the element's position p. It then performs referring to re-generate the description from the grounded position.\nGiven that a UI element's description can vary, we focus on the consistency of its position p to determine convergence. If the grounded position from successive iterations remains stable, the output is considered converged, and the resulting sample can be added to the training set.\nThe dual-learning loop can be represented as follows. Let R() denote the referring function, G(\u00b7) the grounding function, D a description, and p a position. The objective is to ensure that:\nSim (G (R (p)), p) > \u03c4 (3)\nwhere Sim(,) is a similarity function (e.g., Intersection over Union, IoU) used to compare positions, and \u03c4 is the convergence threshold. If the similarity exceeds the threshold, the sample is treated as a self-annotated example and added to the training phase.\nWhile effective, this strategy may concentrate learning on simpler samples and overlook difficult cases. Furthermore, it does not account for the model's historical performance, limiting its ability to address specific weaknesses. To overcome these limitations, we introduce two targeted hard case mining methods: Visual Hard Case Mining and Functional Hard Case Mining.\nVisual Hard Case Mining. Visual hard cases are identified by analyzing the information matrix M obtained from the ISC process. We use spectral entropy, derived from Fourier transform analysis, to quantify the density and complexity of visual information. Higher spectral entropy indicates dense visual content, which often corresponds to more challenging tasks for the agent.\nGiven an information matrix M\u2208 {0,1}n\u00d7m, we first compute its 2D discrete Fourier transform (DFT):\nF(u, v) = \\sum_{i=0}^{n-1}\\sum_{j=0}^{m-1} M(i, j)\u00b7e^{-2\\pi i(\\frac{ui}{n} + \\frac{vj}{m})} (4)\nwhere F(u, v) is the Fourier spectrum representing the frequency components of the matrix. We then shift the zero-frequency component to the center using the 'fftshift' operation for better interpretability.\nThe spectral energy of each frequency component is given by |F(u, v)|2. To quantify the distribution of energy across different frequencies, we define the spectral entropy H as:\nH = \\sum_{k}Pk log(pk), Pk = \\frac{|F(u, v)|^2}{\\sum_{u, v} |F(u, v)|^2} (5)\nwhere pk is the normalized energy of the k-th frequency component. High entropy values indicate a complex, information-dense visual region, which is likely to be challenging for the model.\nUsing the spectral entropy scores, we identify the training images with the highest visual complexity as hard cases. Specifically, only those images with entropy values exceeding a predefined threshold Hmin are selected. These high-entropy images are prioritized for additional training using the dual-learning loop described earlier, ensuring that the model focuses on challenging scenarios. By targeting complex visual patterns, the model improves its ability to handle interfaces with dense or intricate UI components, enhancing both referring and grounding performance.\nFunctional Hard Case Mining. Functional hard cases are identified based on the model's past performance. Specifically, we focus on samples where the model exhibits poor accuracy in interpreting functional descriptions. To enhance the model's robustness in such cases, we use a description augmentation strategy that generates new, similar descriptions based on the problematic ones.\nLet Dhard represent the set of functional descriptions where the model has struggled. For each description Di \u2208 Dhard, we prompt a language model (e.g., GPT) to generate variations:\nGenerate(Di) = {D(1), D(2),..., D(n)} (6)\nThese augmented descriptions are then used as inputs to the dual-learning loop, creating synthetic functional hard cases. By iterating over these generated descriptions, the model incrementally improves its understanding of difficult functional concepts.\nSummary. By leveraging the dual-learning loop and targeted hard case mining, Iris can autonomously discover and learn from difficult samples, thereby enhancing its robustness and adaptability in handling diverse GUI environments."}, {"title": "4. Experiments", "content": "We present comprehensive experimental results to evaluate Iris's effectiveness across different aspects. Our evaluation focuses on three main areas: (1) GUI grounding capability on specialized benchmarks (Sec. 4.2), demonstrating Iris's enhanced visual perception and functional understanding; (2) agent performance on real-world tasks (Sec. 4.3), showing Iris's practical utility in diverse applications; (3) ablation studies (Sec. 4.4) examining the contribution of each component.\n4.1. Training Details\nWe follow the same training process as SeeClick[9], initializing from Qwen-VL[4] with identical pretrained datasets: 850K GUI-specific data and 150K general vision-language instructions from the LLaVA[26] dataset. However, we incorporate two key enhancements:\n\u2022 Enhanced Visual Training: We implement Information-Sensitive Cropping (ISC) during the initial training phase to improve visual perception capabilities.\n\u2022 Self-Refining Stage: Following the initial training, we conduct an additional SRDL stage where Iris is trained on approximately 3M self-annotated GUI samples generated through our dual-learning process.\nDetailed hyperparameters and training configurations are provided in Appendix Sec. 8.\n4.2. Evaluation on GUI Grounding Benchmarks\nWe evaluate Iris on two specialized GUI grounding benchmarks: ScreenSpot [9] and GroundUI [42], which test the model's ability to understand and interact with diverse interface elements across mobile, desktop, and web platforms. Details of the benchmarks and comparison models are provided in Appendix Sec. 9.\nOn the ScreenSpot benchmark, Iris achieves state-of-the-art performance with an average accuracy of 74.6%, outperforming previous models by significant margins (Tab. 1). Detailed analysis reveals several key findings:\n\u2022 Consistent Improvement over SeeClick: Despite using the same amount of annotated training data (850K), Iris consistently outperforms SeeClick across all evaluated metrics, demonstrating the effectiveness of our ISC and SRDL enhancements in improving both visual perception and functional understanding capabilities.\n\u2022 Resolution-Dependent Performance Gain: The performance improvements are more pronounced on web and desktop platforms compared to mobile. This pattern can be attributed to the inherent challenges of higher-resolution interfaces: web and desktop screens typically contain more complex layouts and denser information. Traditional approaches like SeeClick, which rely on uniform image scaling, inevitably lose fine-grained details in these complex interfaces. In contrast, our ISC method adaptively processes information-rich regions, effectively preserving critical visual details in high-resolution interfaces while maintaining computational efficiency.\n\u2022 Competitive Performance with Limited Annotations: Compared to UGround, which utilizes more than 10 times our training data (10M vs 850K), Iris achieves superior performance in most categories and comparable overall accuracy. This demonstrates SRDL's effectiveness in identifying and learning from challenging cases, matching the performance of models trained on much larger annotated datasets.\nSimilarly, on the GroundUI benchmark, Iris achieves sota performance across all categories, outperforming previous models by significant margins (Tab. 2)."}, {"title": "4.4. Ablation Study", "content": "Overall Component Analysis: ISC and SRDL are Complementary. We conduct comprehensive ablation studies to analyze the effectiveness of our two key components: Information-Sensitive Cropping (ISC) and Self-Refining Dual Learning (SRDL). As shown in the figure, both components contribute uniquely to Iris's performance. The full Iris model achieves the optimal balance, reaching approximately 75% accuracy at 1.0s processing time. This represents a significant improvement over baseline methods like SeeClick (53% accuracy at 0.5s) and CogAgent (48% accuracy at 2.0s). ISC provides a 300% efficiency improvement, while SRDL contributes a 10% accuracy gain. The complementary nature of these components allows Iris to achieve performance comparable to UGround (shown at 3.0-3.5s) but with significantly faster processing time."}, {"title": "5. Conclusion", "content": "This paper introduces Iris, a foundational visual agent that advances GUI understanding through two key innovations: Information-Sensitive Cropping (ISC) and Self-Refining Dual Learning (SRDL). ISC enables efficient processing of high-resolution interfaces by adaptively allocating computational resources based on information density, achieving a 300% efficiency improvement over traditional approaches. Rather than treating all regions equally, ISC employs edge detection to identify visually dense areas and dynamically adjusts token allocation, allowing for detailed processing of complex UI elements while efficiently handling sparse regions. SRDL enhances model robustness through autonomous discovery and learning from challenging cases, delivering a 10% accuracy gain without requiring additional manual annotations. Evaluations on multiple benchmarks, ranging from basic GUI grounding to complex multimodal agent tasks, demonstrate that Iris achieves state-of-the-art performance while balancing both accuracy and efficiency, setting a new standard for GUI-based digital agents."}, {"title": "6. ISC Implementation Details", "content": "6.1. Motivation\nInformation detection in GUI interfaces presents unique challenges due to the coexistence of information-dense regions (e.g., menu bars, toolbars) and sparse areas (e.g., backgrounds, margins). Our implementation aims to efficiently identify these varying density regions while maintaining real-time processing capabilities.\n6.2. Technical Approach\nOur information detection approach builds on three key insights:\n1. Edge Significance: GUI elements typically have distinct boundaries that separate them from backgrounds and other elements. By detecting these edges, we can identify regions containing meaningful interface components.\n2. Adaptive Processing: Rather than applying uniform processing across the entire interface, we use adaptive histogram equalization to enhance local contrasts. This helps identify subtle UI elements while maintaining sensitivity to prominent features.\n3. Density Preservation: The detected edges are deliberately dilated to create connected regions that better represent the actual information density of UI components. This prevents fragmentation of logical interface elements and helps maintain semantic grouping.\n6.3. Edge Detection Pipeline\nThe edge detection [7] process consists of four sequential stages:\n1. Pre-processing:\n\u2022 Convert the input image to grayscale to focus on structural information\n\u2022 Apply adaptive histogram equalization to enhance local contrasts\n\u2022 This stage ensures that both prominent and subtle interface elements are captured\n2. Noise Reduction:\n\u2022 Apply Gaussian smoothing to reduce image noise\n\u2022 This step prevents false edge detection from texture and compression artifacts\n\u2022 The smoothing preserves significant boundaries while eliminating minor variations\n3. Gradient Computation:\n\u2022 Calculate intensity gradients in both horizontal and vertical directions\n\u2022 Compute gradient magnitude and direction at each pixel\n\u2022 Strong gradients indicate potential edges in the interface\n4. Edge Formation:\n\u2022 Apply non-maximum suppression to thin edge responses\n\u2022 Use hysteresis thresholding to connect edge segments\n\u2022 The result is a binary edge map highlighting significant UI boundaries\n6.4. Working Mechanism\nThe detection process operates by:\n1. Converting the input GUI screenshot to a representation that emphasizes structural information\n2. Enhancing local contrasts to identify both prominent and subtle interface elements\n3. Detecting and strengthening edges to create a cohesive information density map\n4. Producing a binary matrix that indicates regions of high information content\nThe resulting information matrix serves as a guide for the Information-Sensitive Cropping (ISC) process described in the main paper, enabling efficient allocation of computational resources based on the actual distribution of interface elements."}, {"title": "7. ISC Efficiency Analysis", "content": "7.1. ISC Computational Complexity\nThe ISC pipeline consists of two main stages:\n1. Information Detection\nEdge detection has a complexity of O(W \u00d7 H), where W and H are the image width and height. This includes:\n\u2022 Grayscale conversion, adaptive histogram equalization, and edge detection operations\n\u2022 Each operation has linear complexity in terms of image pixels\n2. Adaptive Cropping\nFor sliding windows of size k:\n\u2022 Step size is k/4\n\u2022 Density calculation per position: O(k2)\n\u2022 Number of windows: O((W \u00d7 H)/(k/4)2)\n\u2022 Window sizes increase geometrically from kmin to max(W, H)\nTotal complexity: O(W \u00d7 H)"}, {"title": "7.2. MLLM Inference Complexity Comparison", "content": "Let H be the number of attention heads and d be the dimension of each head in the MLLM. The computational complexity can be analyzed as follows:\n1. Without ISC\nFor standard MLLM processing with direct input of full resolution images:\n\u2022 Input image size: W \u00d7 H\n\u2022 Visual token count: Nfull = [WXH/p2], where p is the patch size\n\u2022 Each self-attention operation: O(Nfull2 \u00b7d)\n\u2022 Across all heads: O(Nfull2 \u00b7H\u00b7d)\n\u2022 Total complexity: Tstandard = O((W \u00d7 H/p2)2 . H . d)\n2. With ISC\nUsing ISC-based processing with adaptive sub-image selection:\n\u2022 ISC preprocessing: O(W \u00d7 H)\n\u2022 Maximum sub-images: M (fixed constant)\n\u2022 Tokens per sub-image: Nsub = (S/p)2, where S is the target size\n\u2022 Attention complexity per sub-image: O(Nsub2\u00b7H\u00b7d)\n\u2022 Total complexity: Tisc = O(W \u00d7 H + M \u00b7 Nsub2\u00b7H\u00b7d)\nNote that M and S can be flexibly adjusted according to computational budget and model performance requirements. The key efficiency gain comes from the fact that TISC scales linearly with input resolution (W \u00d7 H), while Tstandard scales quadratically. This difference becomes particularly significant for high-resolution GUI interfaces where (W \u00d7 H) \u226b (M \u00b7 S\u00b2)."}, {"title": "7.3. ISC Efficiency Benefits", "content": "This results in significant efficiency improvements:\n\u2022 ISC reduces quadratic dependency on input resolution to linear\n\u2022 Total computation becomes bounded by a constant factor of sub-image count\n\u2022 The theoretical speedup ratio Tstandard/TISC increases with input resolution\n\u2022 For typical GUI resolutions (1920\u00d71080), ISC achieves approximately 300% speedup in practice"}, {"title": "8. Iris Training Details", "content": "Following SeeClick [9], we employed the same training data (850K GUI-specific data and 150K general vision-language instructions from the LLaVA [26] dataset) for continual pre-training of Qwen-VL-Chat [4], only with a different image processing pipeline, where SeeClick resizes the input image to a fixed resolution of 448x448 but we use our ISC pipeline to select the most relevant sub-images. Then we had another Self-Refining Dual Learning stage to further improve the model's understanding of complex GUI interfaces. Based on the training history of the first stage, we further self-labeled another 3M data, as introduced in 3.3. Then we had another round of training with the new data mixed with the original training data, with all other training hyper-parameters kept the same. The optimizer we used is AdamW [22] and we picked a cosine annealing scheduler with an init learning rate of 3e-5 and a global batch size of 64."}, {"title": "9. GUI Grounding Benchmarks Details", "content": "9.1. SeeClick Benchmark\nWe evaluate our model on ScreenSpot, a comprehensive benchmark proposed by Cheng et al. [9]. ScreenSpot contains 610 interface screenshots with 1,272 annotated instructions spanning various platforms (iOS, Android, macOS, Windows, and web). The instruction set is divided into 502 mobile, 334 desktop and 436 web instructions. Each instruction is paired with precise bounding box coordinates marking the target UI element.\nThe data collection process of ScreenSpot follows rigorous protocols where:\n\u2022 Screenshots are captured during daily use by experienced annotators\n\u2022 Instructions are written in natural language describing common operations\n\u2022 Both text elements and UI widgets/icons are annotated\n\u2022 Platform distribution is balanced to cover:\nMobile: iOS and Android mobile interfaces\nDesktop: macOS and Windows operating systems\nWeb: Development, shopping, forum and tool websites\n9.2. GroundUI Benchmark\nFor additional validation, we utilize the GroundUI benchmark from AgentStudio [42] which curates data from multiple existing datasets:\nDataset Composition:\nEach sample consists of:\n\u2022 An interface screenshot\n\u2022 A single-step natural language instruction\n\u2022 Ground truth bounding box coordinates\nFor efficient evaluation, we use GroundUI-1K, a carefully selected subset that maintains the diversity of the full dataset:\n\u2022 400 web samples covering various website types:\nDevelopment tools and environments\nE-commerce and shopping platforms\nForum and community websites\nUtility and productivity tools\n\u2022 300 desktop samples from:\nSystem settings and configurations\nCommon productivity applications\nBuilt-in utilities\n\u2022 300 mobile samples featuring:"}, {"title": "10. Android In The Wild (AITW)", "content": "AITW [29] is a comprehensive Android automation dataset containing over 30K instructions and 700K episodes across five categories: General, Install, GoogleApps, Single, and WebShopping. Each sample contains an instruction and corresponding action trajectory with screenshots.\nFor evaluation, we follow SeeClick's approach in using a screen-wise matching score that considers both the correctness of action type and its value (e.g., click coordinates or typed text). This scoring mechanism has been shown to correlate well with human-judged task completion rates. We utilize their standard metrics while ensuring fair comparisons across different models."}, {"title": "10.2. Mind2Web", "content": "Mind2Web [11] is a web automation benchmark involving real-world websites, consisting of over 2000 open-ended tasks collected from 137 different websites. Each task includes high-level instructions and corresponding human action trajectories.\nFollowing SeeClick, we evaluate using three key metrics:\n\u2022 Element Accuracy (Ele.Acc): For visual-based methods like Iris, we consider a prediction correct if the predicted click coordinates fall within the ground truth element's bounding box\n\u2022 Operation F1 (Op.F1): For click operations, this is equivalent to accuracy. For type and select operations, it calculates the token-level F1 score of predicted values\n\u2022 Step Success Rate (Step SR): A step is considered successful only if both the predicted element and operation are correct\nWe maintain strict evaluation criteria where predictions falling outside the target element's bounding box are considered incorrect, even if they are near the target. This ensures consistency with previous evaluations while providing clear metrics for assessing localization accuracy."}, {"title": "9.3. Compared Baselines", "content": "Following previous works, we compare our approach with several state-of-the-art vision-language models:\nOpen-source Models:\n\u2022 PaliGemma [6]: A 3B parameter versatile vision-language model with flexible resolution support up to 896x896\n\u2022 MiniCPM [21]: A small yet efficient multimodal model built with scalable training strategies\n\u2022 CogVLM2 [19]: An improved vision-language model with enhanced image and video understanding capabilities\n\u2022 Qwen-VL-Chat [4]: A vision-language model with open-ended dialogue abilities\n\u2022 MiniGPT-v2 [8]: A 7B parameter model for multi-task vision-language learning\nProprietary Models:\n\u2022 Gemini-1.0/1.5 [31]: Google's multimodal models with progressive improvements in visual understanding and reasoning\n\u2022 GPT-40/GPT-4V [2]: OpenAI's large language model with strong zero-shot visual capabilities.\n\u2022 Claude [3]: Anthropic's multimodal model demonstrating strong performance in language tasks with visual inputs\nGUI-Specialized Models:\n\u2022 Fuyu [5]: A model architecture which takes naive image as input\n\u2022 CogAgent [20]: A 140M-sample GUI-pretrained model focused on computer interaction tasks\n\u2022 UGround [18]: A universal visual grounding model trained on 10M GUI samples\n\u2022 SeeClick [9]: A GUI-focused model pretrained on 850K samples with specialized grounding capabilities\nFor fair comparison, we evaluate all models using the same click accuracy metric - considering a prediction successful only when the predicted coordinates fall within the ground truth element's bounding box. This directly measures each model's ability to precisely locate and interact with UI elements based on natural language instructions."}, {"title": "10. Agent Benchmark Details", "content": "We evaluate Iris on two comprehensive agent benchmarks following the same settings as SeeClick [9]: AITW [29] for mobile OS interactions and Mind2Web [11] for web navigation."}]}