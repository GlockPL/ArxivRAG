{"title": "Co-Activation Graph Analysis of Safety-Verified and Explainable Deep Reinforcement Learning Policies", "authors": ["Dennis Gross", "Helge Spieker"], "abstract": "Deep reinforcement learning (RL) policies can demonstrate unsafe behaviors and are challenging to interpret. To address these challenges, we combine RL policy model checking-a technique for determining whether RL policies exhibit unsafe behaviors with co-activation graph analysis a method that maps neural network inner workings by analyzing neuron activation patterns to gain insight into the safe RL policy's sequential decision-making. This combination lets us interpret the RL policy's inner workings for safe decision-making. We demonstrate its applicability in various experiments.", "sections": [{"title": "1 INTRODUCTION", "content": "Deep Reinforcement Learning (RL) has improved various industries (Liu et al., 2024; Ji et al., 2024; Wang et al., 2024), enabling the creation of agents that can outperform humans in sequential decision-making tasks (Mnih et al., 2015).\nIn general, an RL agent aims to learn a near-optimal policy to achieve a fixed objective by taking actions and receiving feedback through re-wards and state observations from the environment (Sutton and Barto, 2018). Each state is described in terms of features, which can be con-sidered characteristics of the current environment state (Strehl et al., 2007). We call a policy \u0430 \u0442\u0435\u0442\u043e-ryless policy if it only decides based on the current state (Sutton and Barto, 2018).\nA neural network (NN) commonly represents the policy that, given the observation of the environment state as input, yields values that indicate which action to choose (Mnih et al., 2013). These values are called Q-values (Watkins and Dayan, 1992), repre-senting the expected cumulative reward an agent pol-icy expects to obtain by taking a specific action in a particular state.\nUnfortunately, trained policies can exhibit unsafe behavior (Gross et al., 2022) like col-lisions (Ban and Li, 2024), as rewards often do not fully capture complex safety require-ments (Vamplew et al., 2022), and are hard to interpret because the complexity of NNs hides cru-cial details affecting decision-making (Bekkemoen, 2024).\nTo resolve the issues mentioned above, formal verification methods like model check-ing (Baier and Katoen, 2008) have been proposed to reason about the safety of RL policies (Wang et al., 2020; Hasanbeig et al., 2020; Br\u00e1zdil et al., 2014; Hahn et al., 2019) and explainable RL methods to interpret trained RL policies (Milani et al., 2024).\nModel checking is not limited by the proper-ties that rewards can express. Instead, it sup-ports a broader range of properties that can be expressed by probabilistic computation tree logic (PCTL) (Hansson and Jonsson, 1994). PCTL for-malizes reasoning about probabilistic systems, such as Markov decision processes (MDPs). It enables the specification of (safety) properties that relate to the probability of events occurring over discrete time steps, as applicable in our RL setting.\nExplainable RL involves methods that make RL policies interpretable, such as clarifying how the pol-icy makes decisions (Sieusahai and Guzdial, 2021). Local explanations clarify decision-making for spe-cific states, while global explanations offer a holistic view of the policy and its actions (Milani et al., 2024).\nSome research combines safety with explainabil-ity by creating simpler surrogate models of poli-cies (Schilling et al., 2023), pruning neural network interconnections and re-verifying the pruned network to identify which connections influence safety prop-erties (Gross and Spieker, 2024b). Other approaches use external systems to explain failures and propose alternative actions, enhancing the safety of trained RL policies (Gross and Spieker, 2024a).\nUnfortunately, there remains a gap between local and global explanations, as, to the best of our knowl-edge, no current methodology offers nuanced safety explanations for RL policies within specific regions of the environment.\nCo-activation graph analysis (Horta et al., 2021; Selani and Tiddi, 2021; Horta et al., 2023; Horta and Mileo, 2021) can be such methodology. While co-activation graph analysis was successfully applied in classification tasks, no work has applied it to RL nor in the context of safety.\nIn general, co-activation graph analysis explores how NN classifiers learn by extracting their acquired knowledge (Horta et al., 2021). The method creates a graph in which the nodes represent neurons, and the weighted connections show the statistical correlations between their activations. Correlations are derived by applying the trained NN classifier to a labeled dataset (labels come from external knowledge) and measur-ing the relationships between neuron activations.\nHowever, the main challenge in RL is identifying and integrating the missing external knowledge into the co-activation graph analysis to extract valuable in-formation from the trained NN policies.\nIn this work, we tackle the problem of generat-ing external knowledge via safety verification and ex-plainable RL methods to allow co-activation graph analysis in the context of RL safety. This approach creates a new category of explainable RL methods, which we call semi-global safety explanations. We achieve this through the following steps.\nFirst, we create the unlabeled dataset containing the states of the environment that are reachable by the trained RL policy and for which a user-specific safety property holds. In more detail, given a model-based RL environment, a user-specified safety prop-erty, and a trained RL policy, the formal model of the interactions between the RL environment and trained RL policy is built and verified in the following way.\nWe query for an action for every state reachable via the trained policy relevant to the given safety prop-erty. Only states reachable via that action are ex-panded in the underlying environment. The resulting formal model is fully deterministic, with no open ac-tion choices. It is passed to the model checker Storm for verification, yielding the exact safety property and all its relevant states Gross et al. (2022).\nSecond, we label the whole state dataset with the safety property as the label and compare it with other labeled state datasets (such as another safety property labeled dataset), or we label each state individually in the dataset via an explainable metric (for instance, if the state is critical or not critical for the trained policy (Milani et al., 2024; Vouros, 2023)) or another user-specified metric.\nFinally, we investigate the neuron activations of the trained RL policy for the labeled datasets via co-activation graph analysis methods (Horta et al., 2021; Selani and Tiddi, 2021; Horta et al., 2023; Horta and Mileo, 2021) to gain insights into the trained NN policy inner-workings by analyzing the neuron co-activations per labeled dataset and com-pare.\nOur experiments show that RL co-activation graph analysis is a valuable tool for interpreting NNs in RL policies, especially for safety applications. It offers insights into neuron importance and feature rankings, and it identifies densely connected neuron clusters, or functional modules, within the network. This reveals how different parts of the neural network contribute to safe decision-making, enhancing our understand-ing of the model's behavior in critical areas and fu-eling human curiosity in the pursuit of explainable AI (Hoffman et al., 2023, 2018; Miao et al., 2018).\nTherefore, our main contribution is a framework that allows us to apply co-activation graph analysis specifically for RL safety interpretations."}, {"title": "2 Related Work", "content": "In this section, we review work related to our ap-proach. First, we position our method within the broader field of explainable techniques for NNs. Next, we examine research focused on the formal ver-ification of RL policies. Finally, we discuss studies integrating explainability with formal verification of RL policies, highlighting where our approach con-tributes within this combined framework."}, {"title": "2.1 Explainable NN Methods", "content": "Drawing inspiration from neuroscience, which uses network analysis and graphs to understand the brain, Horta et al. (2021) explore how NNs learn by extract-ing the knowledge they have acquired. They devel-oped a co-activation graph analysis in the context of classification tasks. The authors suggest that this co-activation graph reflects the NN's knowledge gained during training and can help uncover how the NN functions internally. In this graph, the nodes represent neurons, and the weighted connections show the sta-tistical correlations between their activations. These correlations are derived by applying the trained NN classifier to a labeled dataset (labels come from exter-nal knowledge) and measuring the relationships be-tween neuron activations. This method enables iden-"}, {"title": "2.2 Formal Verification of RL Policies", "content": "Various studies use model checking to verify that RL policies do not exhibit unsafe behav-ior (Eliyahu et al., 2021; Kazak et al., 2019; Corsi et al., 2021; Dr\u00e4ger et al., 2015; Zhu et al., 2019; Jin et al., 2022; Gross et al., 2022). We build on top of the work of Gross et al. (2022) and augment their tool to support co-activation RL policy graph analysis (Cassez et al., 2005; David et al., 2015)."}, {"title": "2.3 Formal Verification and Explainability", "content": "In the context of MDPs, there exists work (Elizalde et al., 2007, 2009) that analyzes the feature importance (a type of explainability) of MDPs manually and automatically. However, we focus on the inner workings of trained RL policies for states that satisfy user-specified safety properties.\nIn the context of classification tasks, work exists that extends the PCTL language by itself to support more trustworthiness of explanations (Termine et al., 2021). However, we focus on sequential decision-making of RL policies.\nIn the context of explainable and verified RL, ex-isting work iteratively prunes trained NN policies to interpret the feature importance for safety at a global level (Gross and Spieker, 2024b). We support vari-ous graph algorithms applied to the inner workings of NN policies, including a way to measure the feature importance. Additionally, some work leverages large language models to identify safety-critical states and apply counterfactual reasoning to explain why the RL policy violated a safety property while proposing al-ternative actions (Gross and Spieker, 2024a). How-ever, this approach provides only local explanations via an external large language model."}, {"title": "3 Background", "content": "First, we introduce probabilistic model checking. Second, we present the basics for explainable RL. Finally, we give an introduction to co-activation graph analysis."}, {"title": "3.1 Probabilistic Model Checking", "content": "A probability distribution over a set X is a function \u03bc: X \u2192 [0,1] with \u2211x\u2208x\u03bc(x) = 1. The set of all dis-tributions on X is Distr(X).\nDefinition 1 (MDP). A MDP is a tuple M = (S, so, Act, Tr, rew, AP,L) where S is a finite, nonempty set of states; so \u2208 S is an initial state; Act is a finite set of actions; Tr: S\u00d7Act \u2192 Distr(S) is a partial probability transi-tion function; rew: S \u00d7 Act \u2192 R is a reward function; AP is a set of atomic propositions; L: S \u2192 2AP is a labeling function.\nWe employ a factored state representation where each state s is a vector of features (f1, f2,..., fa) where each feature fi \u2208 Z for 1 \u2264 i \u2264 d (state dimension).\nDefinition 2. A memoryless deterministic policy \u03c0 for an MDP M is a function \u03c0: S \u2192 Act that maps a state s \u2208 S to action a \u2208 Act.\nApplying a policy \u03c0 to an MDP M yields an in-duced DTMC D where all non-determinism is re-solved."}, {"title": "3.2 Explainable Reinforcement Learning", "content": "The standard learning goal for RL is to learn a pol-icy \u03c0 in an MDP such that \u03c0maximizes the accumu-lated discounted reward (Bekkemoen, 2024), that is, \u0395[\u03a3\u039f\u03a5R], where y with 0 \u2264 y \u2264 1 is the discount factor, R\u2081 is the reward at time t, and N is the total number of steps.\nTo approximate the optimal policy \u03c0* concerning the objective, RL algorithms employ NN, which con-tains multiple layers of neurons, as function approxi-mators (Mnih et al., 2013).\nExplainability methods are used to understand trained RL policies (Milani et al., 2024). Global"}, {"title": "3.3 Co-activation Graph Analysis", "content": "The co-activation values are extracted over a set of inputs S. A co-activation value W between a neuron i in layer k and a neuron j in layer l is defined as the correlation of the activation values A of the two neurons for a set of inputs S (Selani and Tiddi, 2021, see Equation 1).\n$W = Corr(A(i,k,S),A(j,k,S))$ (1)\nA co-activation graph is an undirected network where each node represents a neuron from the NN, and the connection weights indicate co-activation val-ues (Horta et al., 2021).\nPageRank. In graph theory, centrality measures quantify the importance of nodes within a graph. The graph's domain and the chosen centrality met-ric determine the importance. In the context of the co-activation graph, centrality measures can highlight neurons critical for the NN's perfor-mance (Horta et al., 2021)."}, {"title": "4 Methodolodgy", "content": "Our methodology consists of two main steps: gener-ating a labeled dataset based on safety properties or other explainable RL or user-specified methods and applying co-activation graph analysis on the labeled dataset to interpret the NN policy. The steps are de-tailed in the first two subsections, followed by a limi-tation analysis of our methodology."}, {"title": "4.1 Labeled Dataset Generation", "content": "In the first step, we create a dataset of states reflect-ing a user-specified safety property. Given an MDP of the RL environment, a trained RL policy \u03c0, and a desired safety property, we first incrementally build the induced DTMC of the policy \u03c0, and the MDP M as follows.\nFor every reachable state s via the trained policy \u03c0, we query for an action a = \u03c0(s). In the underly-ing MDP M, only states s' reachable via that action a \u2208 A(s) are expanded. The resulting DTMC D in-duced by M and \u03c0 is fully deterministic, with no open action choices, and is passed to the model checker Storm for verification, yielding the exact results con-cerning satisfying the safety property or violating it, and the states S belonging to the specific safety prop-erty (Gross et al., 2022).\nNow, we have two options to proceed.\nOption 1 We label the entire dataset by associat-ing each state s\u2208 S with the specific safety property. In addition to this labeling, we also create alterna-tive labeled datasets for comparative analysis. For instance, we may label the dataset according to a dif-ferent safety property or specify a particular metric of interest (such as states with specific properties). This enables us to explore policy behavior variations under different metrics.\nOption 2 By introducing other metrics, we can also classify each state \u2208 S individually. For example, we may categorize each states as \"critical\" or \"non-critical\" based on the policy's outputs. These met-rics can also be user-defined, allowing customization to reflect the policy's inner workings for the initial safety property."}, {"title": "4.2 Co-Activation graph Analysis", "content": "We apply co-activation graph analysis on the labeled dataset to interpret the RL policy's neural network's internal structure and decision-making process. For"}, {"title": "4.3 Advantages and Limitations", "content": "RL policy co-activation graph analysis without safety properties is also possible for model-free RL environ-ments (without rigorous model checking) by collect-ing states 5 for S by executing the policy in the envi-ronment and labeling them just via local RL explana-tion methods.\nOur explainable RL safety method with co-activation graph analysis supports memoryless NN policies within modeled MDP environments, limited by its model checking for large state space and tran-sition counts (Gross et al., 2022). The co-activation graph analysis works with any layer in the NN archi-tecture and supports labeled datasets of different sizes that can be found in labeled datasets for classification tasks (Horta et al., 2021)."}, {"title": "5 Experiments", "content": "In this section, we evaluate our proposed method and show that it is applicable in the context of explain-able RL safety. We begin by introducing the RL en-vironments used in our experiments. Next, we de-scribe the trained RL policies. We then explain the technical setup. After that, we apply our method for co-activation graph analysis in various RL safety set-tings. The first setting uses co-activation graph analy-sis for two different safety properties, the second set-ting uses it for an explainable RL method in the con-text of a specific safety property. The final subsection summarizes additional observations of applying co-activation graph analysis in RL safety.\nEnvironments In the experiments, we use a taxi and a cleaning robot environment that are described below.\nThe taxi agent has to pick up passengers and trans-port them to their destination without running out of fuel. The environment terminates as soon as the taxi agent does the predefined number of jobs or runs out of fuel. After the job is done, a new guest spawns ran-domly at one of the predefined locations (Gross et al., 2022). For the first job, the passenger location and destination is always the same, afterwards the passen-ger location and destination is set to four locations randomly.\nS = {(x, y, passenger_loc_x, passenger_loc_y, passenger_dest_x, passenger dest_y, fuel, done, on board, jobs_done, done), ...}\nAct = {north, east,south, west,pick-up,drop}\nPenalty = {0, if passenger successfully dropped.\n21, if passenger got picked up.\n21+ |x - passenger_dest_x|+ |y - passenger_dest_y|, if passenger on board.\n21+ |x - passenger_loc_x|+ |y - passenger-loc-y, otherwise"}, {"title": "5.1 Comparative Analysis of Trained RL Policies Across Different Safety Scenarios", "content": "In this experiment, we show, using the taxi environ-ment, that it is possible to use co-activation graph analysis with model checking to gain safety insights into the trained RL policy and observe NN inner workings for different safety properties.\nSetup In this experiment, we compare the co-activation graphs of a policy that differs for the datasets associated with the safety property of fin-ishing with a reachability probability of 1 one job (P=1( jobs = 1)) vs. finishing two jobs (P=1(\u25ca jobs = 2)). This can give us, for instance, in-sights into what features are more relevant in the be-ginning of the taxi policy execution compared to later steps in the environment.\nExecution We first create the two labeled datasets by building the formal model for each and verify each model with the corresponding PCTL queries. The la-beled dataset for safety property P=1(\u25ca jobs = 1) has 12 data points and the labeled dataset for safety prop-erty P=1(\u25ca jobs = 2) has 206 data points.\nWe apply the PageRank algorithm to rank all neu-rons in the neural network and use the Louvain com-munity detection algorithm to identify neuron com-munities and calculate modularity values."}, {"title": "5.2 Critical vs. Non-Critical State Analysis for a Specific Safety Scenario", "content": "In this experiment, we target a specific safety property for applying co-activation graph analysis. We catego-rize the states using a local explainable RL method to label the dataset. This approach demonstrates that integrating model checking and established local ex-plainable RL methods with co-activation graph anal-ysis can yield deeper insights into neural network policy decision-making. Experiments are again per-formed on the taxi environment.\nSetup We focus on a dataset of states linked to the safety property of completing two jobs with a reacha-bility probability of 1 (P=1(\u25ca jobs = 2)). Labeling is performed through critical state classification, where a threshold of 100 is set for the distance between the highest and lowest predicted Q-values of the policy. Each state is labeled as critical if this Q-value distance"}, {"title": "5.3 Additional Observations", "content": "Next to the two different ways to apply co-activation graph analysis in the context of RL safety, we made"}, {"title": "6 Conclusion", "content": "In this paper, we introduced a methodology that inte-grates RL policy model checking (Gross et al., 2022) with co-activation graph analysis to improve the ex-plainable safety of RL policies. By generating labeled datasets through model checking and local explain-able RL methods, we extended co-activation graph analysis (Horta and Mileo, 2019) to apply it within RL safety. Our approach enables examining NN poli-cies by analyzing neuron activation patterns in states associated with specific safety properties and local ex-plainable RL method results.\nFor future work, we plan to examine how co-activation graph analysis can be applied within multi-agent RL settings (Zhu et al., 2024) or to be used for safe NN policy pruning (Gross and Spieker, 2024b)."}]}