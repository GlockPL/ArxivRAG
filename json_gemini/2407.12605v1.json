{"title": "Continuous reasoning for adaptive container image distribution in the cloud-edge continuum", "authors": ["Damiano Azzolini", "Stefano Forti", "Antonio Ielo"], "abstract": "Cloud-edge computing requires applications to operate across diverse infrastruc-tures, often triggered by cyber-physical events. Containers offer a lightweight deployment option but pulling images from central repositories can cause delays. This article presents a novel declarative approach and open-source prototype for replicating container images across the cloud-edge continuum. Considering resource availability, network QoS, and storage costs, we leverage logic program-ming to (i) determine optimal initial placements via Answer Set Programming (ASP) and (ii) adapt placements using Prolog-based continuous reasoning. We evaluate our solution through simulations, showcasing how combining ASP and Prolog continuous reasoning can balance cost optimisation and prompt decision-making in placement adaptation at increasing infrastructure sizes.", "sections": [{"title": "1 Introduction", "content": "Cloud-edge computing paradigms (e.g., fog [1] and edge [2] computing) have been getting increasing attention both from academia and industry as a solution to deal with the considerable amounts of data coming from the Internet of Things (IoT) [3]. All such paradigms aim at placing (part of the) computation suitably closer to where IoT data are produced to aggregate and filter them along a pervasive contin-uum of heterogeneous infrastructure resources at the interplay between IoT devices and cloud datacentres. Such an approach requires to deploy application components (e.g., microservices and serverless functions) in a context- and QoS-aware manner in order to improve their response times, data-locality, energy demand, security, and trust [4]. Indeed, cloud-edge computing is particularly well-suited for latency-sensitive and bandwidth-hungry IoT applications that require prompt reactions to sensed cyber-physical events, e.g., virtual reality, remote surgery, and online gaming [5].\nMuch research (e.g., surveyed in [6]) has focussed on the decision-making process related to placing application components onto cloud-edge resources according to a large variety of constraints (e.g., hardware, IoT, and QoS) and optimisation targets (e.g., operational cost, resource usage, and battery lifetime), also in response to cyber-physical stimuli. Many proposals foresee deploying application components through containers, being a lightweight and scalable virtualisation technology that runs both on powerful cloud servers and resource-constrained edge devices [7, 8]. To run a container, one must first pull its image from the repository stored in an image registry, i.e., a server that acts as a distribution hub for container images, allowing users to push and pull images to and from the repository.\nHowever, as highlighted in [9, 10], very few works considered the problems related to distributing container image repositories over cloud-edge registries to further im-prove application performance by reducing application start-up times. On one hand, downloading images from a central cloud repository is not always viable in short times, especially for devices closer to the edge of the Internet that might incur intermittent connectivity or scarce bandwidth availability. On the other hand, some edge appli-cation components require fast start-ups, being deployed to the task in response to cyber-physical events that trigger them, or to specific end-users mobility patterns. This is especially true in Function-as-a-Service edge settings, where short-running containerised stateless functions run only when needed [11].\nHereinafter, we precisely consider the possibility of distributing container images across cloud-edge registry nodes to speed up on-demand image downloads to any other node and, therefore, containers' start-up times. In the above settings, deciding onto which infrastructure nodes to place different container images is an interesting and challenging problem [9, 10]. Image placement should guarantee that images can be downloaded from any infrastructure node within a specific time frame. In doing so, it should replicate images onto a subset of infrastructure nodes in a QoS-aware and context-aware manner, without exceeding the capacity of each node and containing operational costs related to leasing the needed storage resources. Indeed, if relying on a central cloud repository might suffer from too high transfer times, replicating all images on all available nodes might be economically unsustainable or unfeasible due to edge resource constraints.\nThe above scenario is characterised by large-scale deployments, high network churn, and frequent image repository updates. Decision-making should scale to quickly dis-tribute tens of images over hundreds of cloud-edge registry nodes, encompassing their high heterogeneity. Edge nodes can temporarily join and leave the network either due"}, {"title": "2 Considered Problem", "content": "In this section, we illustrate a formal model of the considered cloud-edge settings and formally define the problems we will tackle, while proving their NP-hard nature.\nFirst, we model base container images as follows.\nDefinition 1. A container image is a triple (i, s, m) where i denotes the unique image identifier, s the size of the image in Megabytes (MB), and m the maximum amount of time in seconds tolerated to download such image onto a target node.\nWe model registry nodes, their available storage and storage cost, and the Quality of Service of end-to-end links between them in terms of latency and bandwidth. We assume that the considered nodes host a container image registry, i.e., they can be used to distributed container image repositories across the cloud-edge network.\nDefinition 2. A registry node is a triple (n,h,c) where n denotes the unique node identifier (e.g., its IP address), h the available amount of storage it features expressed in MB, and c the monthly cost per used unit of storage (MB) at such node.\nDefinition 3. A link is a 4-tuple (n, n', l,b) where l and b denote the end-to-end la-tency in milliseconds and bandwidth in Mbps, respectively, between the nodes identified by n and n'.\nIt is now possible to formally define image placements. Note that one image can be stored onto multiple nodes, i.e., we foresee the possibility to replicate an image onto multiple infrastructure nodes.\nDefinition 4. Let I be a set of container images and N be a set of available infras-tructure nodes. An image placement is a mapping p : I' \u2192 P(N) that maps each image of I' \u2286 I to a subset of infrastructure nodes N \u2286 N. If I' = I, then the placement is total, it is partial otherwise.\nAssuming a cap on the number of replicas for all images, we now define the eligi-bility criteria for image placement. An eligible image placement must (1) not exceed the cap on the number of replicas for any placed image, (2) enable all infrastructure nodes to download any image within its associated maximum time frame (including transmission and propagation times) or to have it already stored, and (3) not exceed storage capacity at any infrastructure node. These conditions correspond to the three conditions in the following definition of eligible placement.\nDefinition 5. Let I be a set of container images, N be a set of available infrastructure nodes, p : I' \u2192 P(N) be an image placement over I' \u2286 I, R \u2208 N be a maximum image"}, {"title": "2.2 Problem Statements and Complexity", "content": "In this section we formally define the problems we address in this paper. The first (decision) problem we consider in this article is precisely how to determine eligible image placements in Cloud-Edge landscapes. We define it as follows.\nProblem 1 (Image Placement, IPP). An instance of the image placement problem is a 4-tuple (N, L, I, R) where N is a set of available infrastructure nodes, L is the set of end-to-end links between such nodes, I is a set of container images, and R is a maximum replica factor. Determine (if it exists) a total eligible placement p : I \u2192 P(N) as per Definition 5.\nWe prove that Problem 1 is NP-hard by sketching a polynomial time reduction of the NP-hard [16] problem of bin-packing (BPP) to IPP. Bin packing can be stated as follows. Let O be a finite set of items each of size s(o) \u2208 N. Determine if there exists a partition of items in O over K\u2208 N bins without exceeding their capacity B\u2208 N.\nLemma 2.1. IPP \u2208 NP-hard.\nProof. Given an instance (O, K, B) of BPP it is possible to reduce it to an instance (N, L, I, R) of IPP as follows:\n1. populate N with K nodes of the form (k, B,0) with k = 1, 2, ..., K, size B, and null cost,\n2. populate L with a complete set of end-to-end links between pairs of nodes i, j\u2208 N of the form (i, j, 0, \u221e), so that nodes reach each other with null latency and infinite bandwidth,\n3. populate I with a container image (o, s(o), \u221e) \u2208 I for each object o \u2208 O, where the size of the object becomes the size of the image and the maximum tolerated transfer time is infinite, and\n4. set the maximum replica factor R to 1, i.e. placing each image to a single node only.\nDetermining a solution to the above instance of IPP would partition images in I (i.e., item in O) over K nodes (i.e., bins) without exceeding their capacity B. Thus, it would solve the original instance of BPP. The above steps reduce an instance of BPP to an instance of IPP, incurring a polynomial time complexity of O(K2), due to step 2, which requires creating a link for each pair of nodes in N. This proves that IPP is at least as difficult as BPP, which is known to be NP-hard. Hence, IPP is also NP-hard.\nSuch a result means that no poly-time algorithm is currently known to solve IPP (unless P = NP is proven). However, checking whether a placement p is eligible, according to Definition 5, can be performed naively in O(|I|N\u00b2). This consideration along with the previous lemma proves that IPP \u2208 NP-complete. The second problem we will be studying is to continuously adapt a previously eligible total placement whenever changes in the underlying network topology or image requirements occur that affect its eligibility.\nProblem 2 (Continuous Image Placement, CIPP). Let (N, L, I, R) and (N', L', I', R') be two image placement problem instances and p : I \u2192 P(N) be a total eligible place-ment for the first problem instance. Let Iok be a maximal proper subset of I' such that p is eligible over Iok, and I\u043a\u043e = I' \\ \u0406\u043e\u043a. Determine an eligible total placement p' : I' \u2192 P(N') (if any) such that p' (i) = p(i), \u2200i \u2208 \u04c0ok.\nSaid otherwise, given infrastructure or image requirements changes that make a previously total eligible placement p only partially eligible, we aim at extending p over Iok into a total eligible placement p' over I, by (possibly) only modifying the placement of images in Iko\u00b7\nLast, following Definition 7, we can easily define the optimisation problem corresponding to Problem 1.\nProblem 3 (Optimal Image Placement, OIPP). Let (N, L, I, R) be a placement prob-lem instance, determine whether it exists an eligible cost-optimal image placement p: I \u2192 P(N) as per Definition 5 and Definition 7."}, {"title": "3 Methodology", "content": "As solving CIPP and OIPP implies solving IPP, Problems 2 and 3 are also NP-hard.\nIn this section we propose declarative solutions to all the problems introduced in Section 3.1. In particular, we will detail a knowledge representation for our problems, two Prolog iterative deepening solutions to solve IPP and CIPP, respectively, and an ASP encoding to efficiently solve OIPP. Summing up, we show how to combine Prolog continuous reasoning with the ASP encoding to implement adaptive QoS-, context-and cost-aware container image distribution in cloud-edge landscapes. We assume readers to be familiar with basics of logic programming. For a complete treatment of the Prolog and ASP, we refer them to [17] and [18]."}, {"title": "3.1 Knowledge Representation", "content": "We encode a problem instance as a set of facts over the predicates node/3, image/3, and link/4, which can be mapped one to one to Definitions 1, 2, and 3 of Section 2, respec-tively. In particular, container images are denoted by facts like image(ImgId, Size, Max), where ImgId is a unique image identifier, Size is the image size expressed in MB, and Max is the maximum tolerated download latency for the image at hand expressed in ms. Similarly, infrastructure nodes are declared as facts of the form node(NodeId, Storage, Cost) where NodeId is a unique node identifier, Storage the available node storage ca-pacity expressed in MB, and Cost the unit storage cost per MB. Links between nodes are declared as facts of the form link(NodeId1, NodeId2, Latency, Bandwidth) where NodeId1 and NodeId2 are the considered link endpoints, and Latency and Bandwidth the average end-to-end latency in ms and bandwidth in Mbps featured by such a link. Finally, the maximum image replica factor is denoted by maxReplicas(R) where R is an integer."}, {"title": "3.2 Solving IPP with Prolog", "content": "Let us now discuss the Prolog encoding to solve the IPP (Problem 1) task.\nPlacement Strategy Given a (non-empty) list [I|Is] of images to be placed, a list of candidate placement Nodes, a maximum replica factor R, and a partial eligible"}, {"title": "3.3 Solving CIPP with Prolog", "content": "Based on the above, we now describe a solution to CIPP (Problem 2). Predicate cr/7 (lines 49-58) inputs a list [I|Is] of images to be placed, a list of target Nodes, and the current image placement P with its associated allocation Alloc. By recurring on the list of images to be placed, it determines a portion NewPok of P, which constitutes a partial eligible placement of [I|Is] over Nodes, and collects in the KO list all images that need to be migrated. The first clause of cr/7 retrieves the placement of image I from the current placement P and appends it to the partial eligible placement Pok being build by creating TmpPOk (lines 50). Then, cr/7 checks whether for TmpPOk Equation 1, Equation 2 and Equation 3 still hold (lines 51-53). In such a case, TmpPOk is passed to the recursive call, moving to the next image. Whenever such conditions do not hold true, the second clause of cr/7 adds image I to the list of KO images (line 55) and recurs to the next image without extending the partial eligible placement being build (line 56). It is worth noting that this case also handles new images to be placed, which can be continuously added to the image repository, by including them in the KO list. The recursion ends when the list of images to be checked is empty (line 57).\nPredicate crPlacement/5 retrieves the current Placement and its allocation Alloc (line 60) and exploits cr/7 to identify a partial eligible PPlacement and the list of KOImages within Placement (line 60). Finally, it relies on placement/6 to determine a complete eligible NewPlacement that extends the identified partial one (line 61). If no previous placement exists or it is not possible to fix the current one via continuous reasoning, predicate crPlacement/5 attempts to determine a complete eligible InitPlacement from scratch (line 62-63).\nLastly, predicate declace/2 (lines 64\u201368) can be called periodically over a changing knowledge base to continuously determine needed updates of the image Placement and its associated Cost. It simply retrieves input data for crPlacement/5 via predicates imagesToPlace/1, networkNodes/1 and via the fact maxReplicas/1 (lines 65-67), and last it calls crPlacement/5 stopping at the first result through once/1 (line 68).\nNote that predicates imagesToPlace/1 and networkNodes/1 (lines 65 and 66, respec-tively) sort their output to drive backtracking via a combination of a fail-first and a fail-last heuristics [19]. Particularly, Images are sorted (and will be explored) by size in descending order so as to try to place larger images first, in that they might be supported by fewer candidate nodes (fail-first). Nodes are instead sorted by increas-ing cost and decreasing average outgoing bandwidth and hardware capabilities, thus pursuing the two-fold objective of containing deployment costs and avoiding filling up less capable nodes too early in the search (fail-last)."}, {"title": "3.4 Solving OIPP with ASP", "content": "We now introduce an ASP encoding (according to the GRINGO language [20]) to solve the OIPP problem (Problem 3). We follow the Guess, Check & Optimise paradigm, thus defining programs that (i) generate the search space of possible image placements, (ii) prune invalid ones, and (iii) minimise an objective function to rank valid solutions.\nThe ASP input follows the format introduced in Section 3.1. Figure 2 lists the complete ASP encoding. The first requirement is for each image to be deployed on at least one (and at most R) nodes as per Equation 1 of Definition 5. This is succintly encoded at line 75, where the placement(I, N) atom models that each image I is placed onto a non-empty set of at most R nodes N.\nThe rules at lines 76-81 account for the computation of transfer times, where transfer_time(I, Src, Dst, T) models that it is possible to transfer image I from node Src to node Dst in T time units. The first rule (line 76-77) states that if the image Img is placed on node Src its transfer time towards Src itself is null. The second rule computes transfer times in the general case as in the Prolog code (lines 19-24) and relies on the @-term compute_transfer_time, which executes a Python function to compute transfer times of images between pairs of nodes \u2013 since ASP does not natively support floating point operations. Last, the rule transfer_ok(Img, Dst) models that the image Img can be pulled from node Dst within its maximum transfer time.\nWe now define constraints to prune invalid image placements generated by the above choice rule. First, each image's transfer time must meet the specified maximum latency requirements as per Equation 2 of Definition 5. Thus, our ASP enconding discards placements that do no allow a node to pull an image within its maximum transfer time (line 86). Furthermore, the images placed on the same node should not exceed its capacity as per Equation 3 of Definition 5. This is achieved by discarding solutions that do not meet this constraint (line 87-88). Here, TS is the sum (computed with the aggregate #sum) of the sizes Size over all the pairs (Size, Img) for an image Img with size Size (line 87) placed on the node N with maximum capacity Cap (line 88). The above lines determine all solutions to IPP.\nFinally, we aim at finding a cost-optimal image placement, i.e., one that minimise storage costs as per Definition 7. We can do so by adding the weak constraint at line 89-90. Weak constraints enable defining a preference criterion over a logic program's answer sets, akin to an objective function in an optimisation problem. The weak con-straint in our program expresses the following: when there is a placement of an image Img of size Size on a node X that has monthly a cost per MB of Cost, such placement contributes to the cost of the answer set as the additive term Cost * Size once for each distinct pair of the tuple (Img, X). Optimal answer sets are those with minimal costs."}, {"title": "3.5 Functioning of declace", "content": "We assume that declace periodically checks the state of the image placement exploiting Prolog continuous reasoning in combination with ASP-based optimisation. Figure 3 sketches the overall functioning of declace. ASP is used to solve OIPP when (i) an initial image placement needs to be determined, or (ii) the heuristic continuous rea-soning, invoked via declace/2, fails to fix the current placement after an infrastructure or repository change. Informally, the ASP encoding acts as a backup for the second clause of crPlacement/5 of Section 3.3.\nSuch behaviour, which we will thoroughly assess in the next section, suitably alter-nates ASP optimisation and continuous reasoning adaptation steps, balancing between prompt adaptation and cost optimisation of found placements. On one hand, it ensures optimal initial placements and optimal re-placements when the current status cannot be recovered. On the other hand, it features faster decision-making in the adaptation phase at runtime, by focussing only on those images that need to be migrated to reduce service disruption and avoid unnecessary image transfers. Indeed, ASP solves OIPP exactly with the disadvantage of possibly incurring longer execution times over small problem instances than our Prolog heuristic, which solves problem instances reduced via continuous reasoning, taming their NP-hard complexity."}, {"title": "4 Experimental Assessment", "content": "In this section, we report and discuss the results of our experiments10 with the pro-totypes illustrated in Section 3. In the following, we will assess, compare and contrast via simulation the performance of both the individual components of declace and of their combination as per the workflow depicted in Figure 3."}, {"title": "4.1 Assessment of ASP and Iterative Deepening", "content": "In this first experiment, we compare the performance of our iterative deepening solu-tion and ASP solutions to IPP and OIPP, respectively. Particularly, we consider the execution times of both solutions and the cost of their output placements."}, {"title": "4.2 Assessment of declace adaptive behaviour", "content": "In this second experiment, we will assess the performance of declace in continuously adapting the placement of container images over cloud-edge resources, i.e., in solving CIPP over lifelike scenarios at an increasing number of nodes and images to place. As before, we focus on execution times and placement costs achieved by declace against an ASP-computed baseline."}, {"title": "5 Related Work", "content": "Despite being interesting and challenging, the problem of distributing container im-ages in Cloud-Edge landscapes has only been investigated in a few recent works, viz., [9, 10, 24-26], none of which considers runtime image placement adaptation. Indeed, most efforts have been devoted to the (online or offline) problem of plac-ing multiservice applications onto specific target nodes in a context- and QoS-aware manner as surveyed, for instance, in [6]. Among these, Forti et al. [12] and Herrera et al. [15] proposed to exploit continuous reasoning to speed up decision-making at runtime, when selecting suitable placements for multiservice applications.\nFocussing on Docker image distribution in cloud datacentres, Kangjin et al. [27] proposed a peer-to-peer extension of the Docker Image Registry system. Similarly, yet targeting cloud-edge computing settings, the authors of [24] and [25] devise fully decentralised container registries based on local agents compliant with the Docker reg-istry API. Also Zhang et al. [26] propose a container image distribution architecture based on peer-to-peer pre-caching of images onto alive edge nodes. Although missing QoS-, cost- and context-aware strategies to decide on container image placement, the above works show that distributing images around the network can lead to substan-tially lower container pull times, improved image availability, and reduced network traffic. They might constitute the enabling technology for declace to distribute images.\nDarrous et al. [9] first investigated the problem of QoS-aware image placement in cloud-edge settings by proposing two placement strategies based on k-center optimisa-tion. While their problem model is similar to ours, they do not consider storage costs and only focus on reducing container retrieval times. They employ a Python-based solver for k-center problems and propose sorting images to be placed by decreasing size as we do. They assess their proposal against a best-fit and a random strategy over networks up to 50 nodes. Finally, Theodoropoulos et al. [10] formulate the image placement problem as a minimum vertex cover problem and solve it through a re-inforcement learning approach. They assess their proposal against a greedy strategy. Execution times of the proposed reinforcement strategies are two orders of magnitude slower than those of the greedy strategy but obtain on average a better vertex cover.\nDifferently from declace, both [9] and [10] focus on initial image placement and do not consider the possibility of adapting such placement in response to runtime in-frastructural or image repository changes, which might turn useful in ever-changing cloud-edge landscapes, working in continuity with CI/CD release pipelines. To the best of our knowledge, our work is the first employing continuous reasoning for adaptively solving the cloud-edge image placement problem at runtime while reducing execu-tion times, by focussing only on those images that need to be re-allocated. Thanks to its declarative nature, our approach features better readability, maintainability and extensibility than procedural solutions like [9], and better interpretability and incre-mentality of the obtained solutions, which can be difficult to obtain when relying on learning approaches, like [10]."}, {"title": "6 Concluding Remarks", "content": "In this article, we have formally modelled the (optimal and continuous versions of the) problem of adaptive placement of container images in cloud-edge setting and proved its decisional version is NP-hard. Then, we have proposed a declarative pipeline to solve such a problem in a QoS-, context- and cost-aware manner. We have implemented our methodology through logic programming in the open-source prototype declace, which combines ASP to determine cost-optimal initial placements and Prolog for adapting image placements at runtime via continuous reasoning. We have thoroughly assessed our prototype by simulating its functioning over lifelike data. The incremental ap-proach of continuous reasoning reduces the size of the problem instances to be solved and the number of image migrations. This makes runtime decision-making faster, also avoiding unneeded data transfers. The synergistic use of Prolog and ASP balances initial cost optimisation with prompt runtime adaptation. The usage of logic program-ming to define our methodology makes it more concise (~ 100 lines of code), readable and extensible than imperative solutions. Future work on this line may include:\n\u2022 the extension of declace to account for image layers, dependencies between non-base images, and associated transfer delays, and with with probabilistic reasoning [28] to"}]}