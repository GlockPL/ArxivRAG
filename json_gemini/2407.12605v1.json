{"title": "Continuous reasoning for adaptive container image distribution in the cloud-edge continuum", "authors": ["Damiano Azzolini", "Stefano Forti", "Antonio Ielo"], "abstract": "Cloud-edge computing requires applications to operate across diverse infrastructures, often triggered by cyber-physical events. Containers offer a lightweight deployment option but pulling images from central repositories can cause delays. This article presents a novel declarative approach and open-source prototype for replicating container images across the cloud-edge continuum. Considering resource availability, network QoS, and storage costs, we leverage logic programming to (i) determine optimal initial placements via Answer Set Programming (ASP) and (ii) adapt placements using Prolog-based continuous reasoning. We evaluate our solution through simulations, showcasing how combining ASP and Prolog continuous reasoning can balance cost optimisation and prompt decision-making in placement adaptation at increasing infrastructure sizes.", "sections": [{"title": "1 Introduction", "content": "Cloud-edge computing paradigms (e.g., fog [1] and edge [2] computing) have been getting increasing attention both from academia and industry as a solution to deal with the considerable amounts of data coming from the Internet of Things (IoT) [3]. All such paradigms aim at placing (part of the) computation suitably closer towhere IoT data are produced to aggregate and filter them along a pervasive contin-uum of heterogeneous infrastructure resources at the interplay between IoT devicesand cloud datacentres. Such an approach requires to deploy application components(e.g., microservices and serverless functions) in a context- and QoS-aware manner inorder to improve their response times, data-locality, energy demand, security, andtrust [4]. Indeed, cloud-edge computing is particularly well-suited for latency-sensitiveand bandwidth-hungry IoT applications that require prompt reactions to sensedcyber-physical events, e.g., virtual reality, remote surgery, and online gaming [5].Much research (e.g., surveyed in [6]) has focussed on the decision-making processrelated to placing application components onto cloud-edge resources according to alarge variety of constraints (e.g., hardware, IoT, and QoS) and optimisation targets(e.g., operational cost, resource usage, and battery lifetime), also in response to cyber-physical stimuli. Many proposals foresee deploying application components throughcontainers, being a lightweight and scalable virtualisation technology that runs both onpowerful cloud servers and resource-constrained edge devices [7, 8]. To run a container,one must first pull its image from the repository stored in an image registry, i.e., aserver that acts as a distribution hub for container images, allowing users to push andpull images to and from the repository.However, as highlighted in [9, 10], very few works considered the problems relatedto distributing container image repositories over cloud-edge registries to further im-prove application performance by reducing application start-up times. On one hand,downloading images from a central cloud repository is not always viable in short times,especially for devices closer to the edge of the Internet that might incur intermittentconnectivity or scarce bandwidth availability. On the other hand, some edge appli-cation components require fast start-ups, being deployed to the task in response tocyber-physical events that trigger them, or to specific end-users mobility patterns.This is especially true in Function-as-a-Service edge settings, where short-runningcontainerised stateless functions run only when needed [11].Hereinafter, we precisely consider the possibility of distributing container imagesacross cloud-edge registry nodes to speed up on-demand image downloads to any othernode and, therefore, containers' start-up times. In the above settings, deciding ontowhich infrastructure nodes to place different container images is an interesting andchallenging problem [9, 10]. Image placement should guarantee that images can bedownloaded from any infrastructure node within a specific time frame. In doing so,it should replicate images onto a subset of infrastructure nodes in a QoS-aware andcontext-aware manner, without exceeding the capacity of each node and containingoperational costs related to leasing the needed storage resources. Indeed, if relying ona central cloud repository might suffer from too high transfer times, replicating allimages on all available nodes might be economically unsustainable or unfeasible dueto edge resource constraints.The above scenario is characterised by large-scale deployments, high network churn,and frequent image repository updates. Decision-making should scale to quickly dis-tribute tens of images over hundreds of cloud-edge registry nodes, encompassing theirhigh heterogeneity. Edge nodes can temporarily join and leave the network either dueto failures (e.g., power disruption, node crash, reboot) or to network QoS degradation(e.g., network congestion, increased jitter due to node mobility) [12].Image repositories are continuously updated by developers as soon as new featuresare released or newly discovered bugs are fixed. As for 2018, operating system (OS)images such as Ubuntu and Alpine received monthly updates, while non-OS imagessuch as Nginx and MongoDB received up to ten updates per month [13]. Hence,decision-making should run continuously to adapt image placements in response tochanges in the infrastructure (e.g., node/link crashes, link QoS degradation) or in theimage repository to be distributed (e.g., image addition/removal, increase of storagerequirements), that may negatively affect image transfer times. Hence, adaptationShould also migrate as few images as possible to avoid unnecessary data transfers. Tothe best of our knowledge, no prior work exists tackling these adaptation aspects.In this article, we propose a novel declarative methodology - and its open-sourceprototype\u00b9 declace to solve the problem of adaptively distributing base containerimages along a cloud-edge infrastructure in a QoS-, context- and cost-aware manner.Our prototype extensively relies on logic programming. It combines a cost-optimalimage placement strategy, written in Answer Set Programming (ASP), to determineinitial image placements and a heuristic iterative deepening placement strategy, writ-ten in Prolog, to adapt such initial placements via continuous reasoning\u00b2 when needed.Thanks to the declarative nature of logic programming, declace is more concise (~ 100lines of code), readable, and extensible compared to imperative solutions.While initial placements tolerate the longer decision-making times of our ASPapproach in favour of minimising operational placement costs (i.e., it solves theproblem by finding the global minimum), runtime adaptation calls for prompt decision-making to minimise disruption of the image distribution service, even at the priceof sub-optimal placements. By employing heuristic continuous reasoning to solve thecontainer image placement problem at runtime, this work enables the adaptation ofdistributed image repositories to changing cloud-edge environments and image re-quirements. As we will show through our experiments, continuous reasoning reduces the size of the problem instances to be solved at runtime and contains the number ofimages to be migrated when adapting to changing conditions. This permits handlingvarious infrastructure sizes and speeds up decision-making.The rest of this article is organised as follows. After formally defining the decisionand optimisation problems of interest for this work (Section 2), we detail the Prologand ASP solutions to such problems and how they are combined into the declaceprototype tool (Section 3). We then describe the results of the experimental assessmentof declace through simulation over lifelike data (Section 4). Finally, after discussingSome closely related work (Section 5), we conclude by pointing to directions for futurework (Section 6)."}, {"title": "2 Considered Problem", "content": "In this section, we illustrate a formal model of the considered cloud-edge settings andformally define the problems we will tackle, while proving their NP-hard nature.2.1 Formal ModelFirst, we model base container images\u00b3 as follows.Definition 1. A container image is a triple (i, s, m) where i denotes the unique imageidentifier, s the size of the image in Megabytes (MB), and m the maximum amountof time in seconds tolerated to download such image onto a target node.Example 1. The latest version of the alpine, ubuntu and nginx images are modeledby the following triples:(alpine, 8 MB, 30 s), (ubuntu, 69 MB, 60 s), (nginx, 192 MB, 120 s)enforcing that alpine can be downloaded from any network node within 30 seconds,ubuntu within 60 seconds, and nginx within 120 seconds.We model registry nodes, their available storage and storage cost, and the Qualityof Service of end-to-end links between them in terms of latency and bandwidth. Weassume that the considered nodes host a container image registry, i.e., they can beused to distributed container image repositories across the cloud-edge network.Definition 2. A registry node is a triple (n,h,c) where n denotes the unique nodeidentifier (e.g., its IP address), h the available amount of storage it features expressedin MB, and c the monthly cost per used unit of storage (MB) at such node.Definition 3. A link is a 4-tuple (n, n', l,b) where l and b denote the end-to-end la-tency in milliseconds and bandwidth in Mbps, respectively, between the nodes identifiedby n and n'.Example 2. Consider the cloud-edge infrastructure of Figure 1, composed of onecloud and five edge nodes. As an example, the cloud node can be modelled via the tuple(cloud, 2048 MB, 0.9 \u20ac/MB)For the sake of readability, Figure 1 only shows direct (wired or wireless) linksbetween nodes. We assume that all other end-to-end links are computed by selectingShortest paths based on latency, and that the end-to-end bandwidth corresponds to theminimum bandwidth along the path. Then, the links between cloud and edge1 andbetween cloud and edge5 can be modelled as follows(cloud, edge1, 20 ms, 50 Mbps), (cloud, edge5, 55 ms, 5 Mbps)with traffic from cloud to edge5 routed through edge2.It is now possible to formally define image placements. Note that one image can bestored onto multiple nodes, i.e., we foresee the possibility to replicate an image ontomultiple infrastructure nodes.Definition 4. Let I be a set of container images and N be a set of available infras-tructure nodes. An image placement is a mapping p : I' \u2192 P(N) that maps each imageof I' \u2286 I to a subset of infrastructure nodes N \u2286 N. If I' = I, then the placement istotal, it is partial otherwise.Example 3. Considering the images of Example 1 as I and the infrastructure ofExample 2 as N and L, a possible partial placement p' is defined by:p'(alpine) = {cloud, edge2} p' (ubuntu) = {edge5}as a placement is not defined for image nginx. Thus, a total placement p is defined by:p(alpine) = {edge2} p(ubuntu) = {edge2, edge5} p(nginx) = {edge2, edge3, edge5}since it maps all images onto a target node.Assuming a cap on the number of replicas for all images, we now define the eligi-bility criteria for image placement. An eligible image placement must (1) not exceedthe cap on the number of replicas for any placed image, (2) enable all infrastructurenodes to download any image within its associated maximum time frame (includingtransmission and propagation times) or to have it already stored, and (3) not exceedstorage capacity at any infrastructure node. These conditions correspond to the threeconditions in the following definition of eligible placement.Definition 5. Let I be a set of container images, N be a set of available infrastructurenodes, p : I' \u2192 P(N) be an image placement over I' \u2286 I, R \u2208 N be a maximum imagereplica factor, and L be the set of links between nodes in N. The image placement pis an eligible placement over I' if and only if:\u2200i\u2208 I'. |p(i)|\u2264 RV(n,-,-) \u2208 N. V(i,s,m) \u2208 I'. 3 (n',-,-) \u2208 p(i) :(1)n=n' V ((n',n,l,b) \u2208L ^ 8 \u00d7 s/b+l/1000 \u2264 m) (2)V(n, h, -) \u2208 N. I'm = { (i, s, -) \u2208 I' | p(i) = n } \u21d2 \u03a3 s\u2264h(3)(-,-) InNote that Definition 5 is incremental and enables reasoning on partial eligibleimage placements while extending them to total eligible placements. Such an aspect iscrucial for continuous reasoning and will be key to the Prolog code presented next. The8 constant factor in Equation 2 is needed to convert b's Mbps to MB/ms and correctlysum them with the milliseconds of l, converted to seconds by dividing by 1000.Eligible placements can then be associated with their monthly cost as follows.Definition 6. Let I be a set of container images, N be a set of available infrastructurenodes, and p: I \u2192 P(N) be an eligible image placement. The cost of p is defined as:cost(p) = \u03a3\u03a3 S\u00d7Ci\u2208I (-,s,-)\u2208I(-,-,c)\u2208p(i)Lastly, we provide a notion of optimality of an eligible placement based on itsassociated cost.Definition 7. An eligible image placement p : I \u2192 P(N) is cost-optimal if no othereligible image placement p' : I \u2192 P(N) exists such that p \u2260 p'> cost(p') < cost(p).Example 4. Retaking Example 3 and setting R = 3, the first partial placement p' isnot eligible as, for instance, it is not possible to transfer image ubuntu from node edge5to node cloud within 60 seconds (as it requires over 110 seconds). Readers can insteadverify that the total placement p of Example 3 is eligible as it meets all conditions (1),(2), and (3) of Definition 5. Besides, as we will programmatically check in Section 3,placement p, with a cost of \u20ac308, is also cost-optimal in the described scenario.2.2 Problem Statements and ComplexityIn this section we formally define the problems we address in this paper. The first(decision) problem we consider in this article is precisely how to determine eligibleimage placements in Cloud-Edge landscapes. We define it as follows.Problem 1 (Image Placement, IPP). An instance of the image placement problemis a 4-tuple (N, L, I, R) where N is a set of available infrastructure nodes, L is theset of end-to-end links between such nodes, I is a set of container images, and R isa maximum replica factor. Determine (if it exists) a total eligible placement p : I \u2192P(N) as per Definition 5.We prove that Problem 1 is NP-hard by sketching a polynomial time reduction ofthe NP-hard [16] problem of bin-packing (BPP) to IPP. Bin packing can be stated asfollows. Let O be a finite set of items each of size s(o) \u2208 N. Determine if there existsa partition of items in O over K\u2208 N bins without exceeding their capacity B\u2208 N.Lemma 2.1. IPP \u2208 NP-hard.Proof. Given an instance (O, K, B) of BPP it is possible to reduce it to an instance(N, L, I, R) of IPP as follows:1. populate N with K nodes of the form (k, B,0) with k = 1, 2, ..., K, size B, andnull cost,2. populate L with a complete set of end-to-end links between pairs of nodes i, j\u2208 Nof the form (i, j, 0, \u221e), so that nodes reach each other with null latency and infinitebandwidth,3. populate I with a container image (o, s(o), \u221e) \u2208 I for each object o \u2208 O, wherethe size of the object becomes the size of the image and the maximum toleratedtransfer time is infinite, and4. set the maximum replica factor R to 1, i.e. placing each image to a single node only.Determining a solution to the above instance of IPP would partition images in I (i.e.,item in O) over K nodes (i.e., bins) without exceeding their capacity B. Thus, it wouldsolve the original instance of BPP. The above steps reduce an instance of BPP to aninstance of IPP, incurring a polynomial time complexity of O(K2), due to step 2, whichrequires creating a link for each pair of nodes in N. This proves that IPP is at leastas difficult as BPP, which is known to be NP-hard. Hence, IPP is also NP-hard.\u03a0Such a result means that no poly-time algorithm is currently known to solve IPP(unless P = NP is proven). However, checking whether a placement p is eligible,according to Definition 5, can be performed naively in O(|I|N\u00b2). This considerationalong with the previous lemma proves that IPP \u2208 NP-complete. The second problemwe will be studying is to continuously adapt a previously eligible total placementwhenever changes in the underlying network topology or image requirements occurthat affect its eligibility.Problem 2 (Continuous Image Placement, CIPP). Let (N, L, I, R) and (N', L', I', R')be two image placement problem instances and p : I \u2192 P(N) be a total eligible place-ment for the first problem instance. Let Iok be a maximal proper subset of I' suchthat p is eligible over Iok, and I\u043a\u043e = I' \\ \u0406\u043e\u043a. Determine an eligible total placementp' : I' \u2192 P(N') (if any) such that p' (i) = p(i), \u2200i \u2208 \u04c0ok.Said otherwise, given infrastructure or image requirements changes that make apreviously total eligible placement p only partially eligible, we aim at extending pover Iok into a total eligible placement p' over I, by (possibly) only modifying theplacement of images in Iko\u00b7Last, following Definition 7, we can easily define the optimisation problemCorresponding to Problem 1.Problem 3 (Optimal Image Placement, OIPP). Let (N, L, I, R) be a placement prob-lem instance, determine whether it exists an eligible cost-optimal image placementp: I \u2192 P(N) as per Definition 5 and Definition 7."}, {"title": "3 Methodology", "content": "As solving CIPP and OIPP implies solving IPP, Problems 2 and 3 are also NP-hard.In this section we propose declarative solutions to all the problems introduced inSection 3.1. In particular, we will detail a knowledge representation for our problems,two Prolog iterative deepening solutions to solve IPP and CIPP, respectively, and anASP encoding to efficiently solve OIPP. Summing up, we show how to combine Prologcontinuous reasoning with the ASP encoding to implement adaptive QoS-, context-and cost-aware container image distribution in cloud-edge landscapes. We assumeReaders to be familiar with basics of logic programming. For a complete treatment ofthe Prolog and ASP, we refer them to [17] and [18].3.1 Knowledge RepresentationWe encode a problem instance as a set of facts over the predicates node/3, image/3, andlink/4, which can be mapped one to one to Definitions 1, 2, and 3 of Section 2, respec-tively. In particular, container images are denoted by facts like image (ImgId, Size, Max),where ImgId is a unique image identifier, Size is the image size expressed in MB, andMax is the maximum tolerated download latency for the image at hand expressed in ms.Similarly, infrastructure nodes are declared as facts of the form node (NodeId, Storage,Cost) where NodeId is a unique node identifier, Storage the available node storage ca-pacity expressed in MB, and Cost the unit storage cost per MB. Links between nodesare declared as facts of the form link (NodeId1, NodeId2, Latency, Bandwidth) whereNodeId1 and NodeId2 are the considered link endpoints, and Latency and Bandwidth theaverage end-to-end latency in ms and bandwidth in Mbps featured by such a link. Fi-nally, the maximum image replica factor is denoted by maxReplicas (R) where R is aninteger.Example 5. A portion of the problem instance described in Examples 1-4 andsketched in Figure 1 is encoded as follows:image (alpine, 8,30).image (ubuntu, 69,60).image (nginx, 192, 120).node (cloud, 1024000,0.7).node (edge1, 64000,0.7).node (edge2, 32000,0.4).node (edge3,8000,0.5).node (edge4, 4000,0.7).node (edge5, 2000,0.4).link(edge, edge1,20,50).link (edge1, edge0, 20,50).link(edge, edge2, 25,70).link (edge2, edge0, 25,70).link(edge1, edge2,5,100).link (edge2, edge1,5,100).link (edge3, edge1,15,10).link (edge1, edge4,10,30).link (edge1, edge4,10,30).link(edge4, edge1,10,30).link(edge2, edge5,30,5).link(edge2, edge5,30,5).maxReplicas (3).For the sake of readability, we only reported the direct links sketched in Figure 1.3.2 Solving IPP with PrologLet us now discuss the Prolog encoding to solve the IPP (Problem 1) task.Placement Strategy Given a (non-empty) list [I|Is] of images to be placed, alist of candidate placement Nodes, a maximum replica factor R, and a partial eligiblePPlacement, predicate imagePlacement/5 (lines 1-4) determines a total eligible Placementby relying on subsequent evaluations of predicate replicaPlacement/5 (line 2). Whilerecurring on the image list, replicaPlacement/5 extends the previous partial placementPPlacement into a new eligible partial placement TmpPPlacement which also includesimage I at hand (line 3). Recursion ends when the list of images to be placed has beenfully consumed, viz., it is empty (line 4).imagePlacement([I|Is], Nodes, PPlacement, Placement, R) :-replicaPlacement (I, Nodes, PPlacement, TmpPPlacement, R),imagePlacement (Is, Nodes, TmpPPlacement, Placement, R).imagePlacement([],Placement, Placement, _).In turn, given an image I, list of Nodes, a maximum replica factor R and a par-tial eligible placement PPlacement, predicate replicaPlacement/5 (lines 5-13) extendsPPlacement into a new partial eligible NewPPlacement by adding at most R replicas of im-age I. Until the maximum replica factor has not been exceeded (line 6, viz., Equation 1of Definition 1) and the transfer time condition for image I in PPlacement does nothold (line 7, viz., Equation 2 of Definition 5), replicaPlacement/5 selects a candidatedeployment node N (line 8) onto which I has not been placed yet (line 9). It thenchecks whether node N can support all images that it hosts as per PPlacement and thenew replica of image I (line 10, viz., Equation 3 of Definition 5). If this holds, theassociation at(I,N) is added to PPlacement in the recursive call (line 11). The recur-sion ends when the transfer times condition holds for image I by returning the newlyconstructed eligible partial Placement (line 13).replicaPlacement (I, Nodes, PPlacement, NewPPlacement, R) :-R > 0, NewR is R - 1,\\+ transferTimesOk (I, Nodes, PPlacement),image (I, Size, _), member(N, Nodes),\\+ member(at(I,N), PPlacement),storageOk (PPlacement, N, Size),replicaPlacement (I, Nodes, [at (I, N) | PPlacement], NewPPlacement, NewR).replicaPlacement (I, Nodes, Placement, Placement, _) :-transferTimesOk(I, Nodes, Placement).Predicates transferTimesOk/3 and storage0k/3 are in charge of checking conditionsimposed by Equation 2 and Equation 3 of Definition 5, respectively.Given an image I, a list of nodes [N|Ns], transferTimesOk/3 (lines 14-20) recursivelychecks that for each node in the list there exists a node M (possibly M =N) from which it ispossible to download I within a transfer time of at most MaxT (lines 17-18), as specifiedin the correspondingimage/3 fact (line 16). Predicate transferTime/4 (line 19-24) simplycomputes the transfer time T of an Image from a source node Src to a destinationnode Dst following Equation 2 of Definition 5. We use here the cut operator ! (line16). Such an operator prevents unwanted backtracking, thus avoiding determiningunneeded solutions. Indeed, as per Equation 2 of Definition 4, the availability of oneimage source is sufficient for each destination node.transferTimesOk (I, [N|Ns], P) :-dif(P,[]), member (at (I,M), P), image (I,_, MaxT),transferTime (I,M,N,T), T < MaxT, !, % one source is enoughtransferTimesOk(I, Ns, P).transferTimesOk(, [], _).transferTime (Image, Src, Dst, T) :-dif(Src, Dst), node (Src, _,_), node (Dst, _,_),image (Image, Size, _),link(Src, Dst, Latency, Bandwidth),Tis Size 8 / Bandwidth + Latency / 1000.transferTime(_, N, N, 0).Given a partial eligible Placement, a node N and the Size of a new image to beplaced on N, predicate storage0k/3 checks whether node N can also host the new imagewithout exceeding its capacity. It first retrieves the node Storage capacity (line 26) andsums up resources allocated at N by the partial placement through predicate usedHW/3(line 27, 29-30). Finally, it checks Equation 3 of Definition 5 for the partial Placementextended with the new image replica (line 28).storageOk (Placement, N, Size) :-node (N, Storage, _),usedHw(Placement, N, UsedHw),Storage UsedHw >= Size.usedHw(P, N, TotUsed) :-findall(S, (member(at(I,N), P), image(I,S,_)), Used), sum_list(Used, TotUsed).The above 30 lines of code, mimicking the incremental nature of Definition 5,already solve the image placement problem (viz., Problem 1) stated in Section 2 bydetermining a total eligible placement of a set of images onto a target infrastructure.Example 6. Querying the imagePlacement/5 predicate over the input of Example 5returns as a first result the eligible image placement P?- imagePlacement([alpine, ubuntu,nginx], [cloud, edge1, edge2, edge3, edge4, edge5],[], P, 3).P = [at(nginx, edge5), at (nginx, edge3), at (nginx, cloud), at (ubuntu, edge5),at (ubuntu, edge1), at (ubuntu, cloud), at(alpine, cloud)].which, going back to the model of Section 2.1, corresponds to the complete eligibleplacement p defined as followsp(alpine:latest) = {cloud}p(ubuntu:latest) = {cloud, edge1, edge5} p(nginx:latest) = {cloud, edge3, edge5}Iterative Deepening To limit the amount of memory used by Prolog's backtrack-ing and make our program more efficient, imagePlacement/5 is wrapped in the id/6predicate (lines 31-36) that performs iterative deepening [19]. At each step, if thereplica factor R has not reached the maximum replica factor RMax (line 32), id/6 ex-ploits imagePlacement/5 to determine a total eligible Placement with at most R replicasof each image (line 33). If such a placement is not found, the value of R is incremented(line 35) and id/6 is called recursively (line 36).id(Images, Nodes, PP, Placement, R, RMax) :-R =< RMax,imagePlacement (Images, Nodes, PP, Placement, R), !.id(Images, Nodes, PP, Placement, RMax) :-R =< RMax, NewR is R+1,id(Images, Nodes, PP, Placement, NewR, RMax).The placement/6 (lines 37-41) predicate relies on id/6 (setting the replica factorfrom 1 to RMax) to determine a total eligible Placement, possibly starting from a partialeligible placement PPlacement. It then computes its Cost by relying on predicate cost/2(lines 39, 42-43) and its storage Allocation via predicate allocatedStorage/2 (lines 40,44-45). Storage allocation is simply represented as a list of pairs (N, Storage) thatkeeps track of the Storage used by each image at each node N. Finally, predicatestorePlacement/3 (lines 41, 46-49) stores the found Placement, its storage Allocation,and its Cost in the knowledge base as a fact of the formplaced Images (Placement, Allocation, Cost).that will later be used by our continuous reasoning approach. If a placedImages/3 factalready exists, it is retracted (line 47), and then the new one is asserted (line 48).placement (Images, Nodes, MaxR, PPlacement, Placement, Cost) :-id(Images, Nodes, PPlacement, Placement, 1, MaxR),cost (Placement, Cost),allocatedStorage (Placement, Allocation),storePlacement (Placement, Allocation, Cost).cost (Placement, Cost) :-findall(CS, (member(at(I,N), Placement), image(I,S,_), node(N,_,C), CS isC*S), CSs), sum_list(CSs, Cost).allocatedStorage (P, Alloc) :-findall((N,S), (member (at(I,N), P), image(I,S,_)), Alloc).storePlacement (Placement, Alloc, Cost) :- (placedImages/3 -> retract (placedImages/3); true),assert(placed Images (Placement, Alloc, Cost)).Example 7. Querying id/6 over the knowledge base of Example 5 returns the sameplacement P of Example 6, along with its associated cost of \u20ac437.?- placement ([alpine, ubuntu, nginx], [cloud, edge1, edge2, edge3, edge4, edge5], 3, [],P, 1, Cost).P = [at(nginx, edge5), at (nginx, edge3), at(nginx, cloud), at (ubuntu, edge5),at (ubuntu, edge1), at(ubuntu, cloud), at(alpine, cloud)].Cost = 437.0As a side effect, it also asserts a fact placedImages(P,Alloc, Cost) within the knowledgebase. Such fact can be retrieved as follows?- placedImages (P, Alloc, Cost).P = [at(nginx, edge5), at (nginx, edge3), at(nginx, cloud), at (ubuntu, edge5),at (ubuntu, edge1), at(ubuntu, cloud), at(alpine, cloud)],Alloc = [(edge5, 192), (edge3, 192), (cloud, 192), (edge5, 69), (edge1, 69),(cloud, 69), (cloud, 8)],Cost 437.03.3 Solving CIPP with PrologBased on the above, we now describe a solution to CIPP (Problem 2). Predicate cr/7(lines 49-58) inputs a list [I|Is] of images to be placed, a list of target Nodes, andthe current image placement P with its associated allocation Alloc. By recurring onthe list of images to be placed, it determines a portion NewPok of P, which constitutesa partial eligible placement of [I|Is] over Nodes, and collects in the KO list all imagesthat need to be migrated. The first clause of cr/7 retrieves the placement of image Ifrom the current placement P and appends it to the partial eligible placement Pok beingbuild by creating TmpPok (lines 50). Then, cr/7 checks whether for TmpPOk Equation 1,Equation 2 and Equation 3 still hold (lines 51-53). In such a case, TmpPOk is passed tothe recursive call, moving to the next image. Whenever such conditions do not holdtrue, the second clause of cr/7 adds image I to the list of KO images (line 55) andrecurs to the next image without extending the partial eligible placement being build(line 56). It is worth noting that this case also handles new images to be placed, whichcan be continuously added to the image repository, by including them in the KO list.The recursion ends when the list of images to be checked is empty (line 57).cr([I|Is], Nodes, MaxR, P, POk, NewPOk, Alloc, KO) :-findall(at(I,N), member(at(I,N), P), INs), append(INs, POk, TmpPOk),length(INs, IReplicas), IReplicas =< MaxR,transferTimesOk(I, Nodes, TmpPOk),image (I, Size, _), storageOk (I, Size, TmpPOk, Alloc), !,cr(Is, Nodes, MaxR, P, TmpPOk, NewPOk, Alloc, \u041a\u041e).cr([I|Is], Nodes, MaxR, P, POk, NewPOk, Alloc, [I|KO]) :-cr(Is, Nodes, MaxR, P, POk, NewPOk, Alloc, KO).cr([],,Pok, Pok,, []).Predicate crPlacement/5 retrieves the current Placement and its allocation Alloc (line60) and exploits cr/7 to identify a partial eligible PPlacement and the list of KOImageswithin Placement (line 60). Finally, it relies on placement/6 to determine a completeeligible NewPlacement that extends the identified partial one (line 61). If no previousplacement exists or it is not possible to fix the current one via continuous reasoning,predicate crPlacement/5 attempts to determine a complete eligible InitPlacement fromscratch (line 62-63).crPlacement(Images, Nodes, MaxR, NewPlacement, Cost) :-placed Images (Placement, Alloc, _),cr(Images, Nodes, MaxR, Placement, [], OkPlacement, Alloc, KOImages),dif (KOImages, Images),placement (KOImages, Nodes, MaxR, OkPlacement, NewPlacement, Cost).Lastly, predicate declace/2 (lines 64\u201368) can be called periodically over a changingknowledge base to continuously determine needed updates of the image Placement andits associated Cost. It simply retrieves input data for crPlacement/5 via predicatesimagesToPlace/1, networkNodes/1 and via the fact maxReplicas/1 (lines 65-67), and lastit calls crPlacement/5 stopping at the first result through once/1 (line 68).Note that predicates imagesToPlace/1 and networkNodes/1 (lines 65 and 66, respec-tively) sort their output to drive backtracking via a combination of a fail-first and afail-last heuristics [19]. Particularly, Images are sorted (and will be explored) by sizein descending order so as to try to place larger images first, in that they might besupported by fewer candidate nodes (fail-first). Nodes are instead sorted by increas-ing cost and decreasing average outgoing bandwidth and hardware capabilities, thuspursuing the two-fold objective of containing deployment costs and avoiding filling upless capable nodes too early in the search (fail-last).declace (Placement, Cost) :-imagesToPlace (Images),networkNodes (Nodes),maxReplicas (Max),crPlacement (Images, Nodes, Max, Placement, Cost), !.The above 68 lines of code, which already constitute the core of declace, heuris-tically solve the continuous image placement problem (viz., Problem 2) stated inSection 2 by determining a total eligible placement of a set of images onto a tar-get infrastructure, starting from a previous partial eligible placement. Note that theproposed solution adapts the current placement in response to changes in the imagerepository to be distributed (i.e., image addition-removal and storage requirementschanges), in the target infrastructure (i.e., node/link crash, link QoS degradation),and in the maximum replica factor.Example 8. Querying predicate declace/2 over the knowledge base of Example 5 givesthe following result?- declace (P, Cost).P = [at(alpine, edge2), at (ubuntu, edge5), at (ubuntu, edge2), at(nginx, edge3),at (nginx, edge5), at (nginx, edge2)],B = 308.0.which corresponds to the eligible placement of Example 4, costs \u20ac308, and replicatesalpine onto one node, ubuntu onto two nodes, and nginx onto three nodes.Assume that node edge3 fails. Querying again predicate declace/2 over the updatedknowledge base adapts the previous placement as follows?- declace (PNew, Cost).PNew = [at(nginx, edge5), at(nginx, edge2), at(alpine, edge2), at (ubuntu, edge5),at (ubuntu, edge2)],Cost = 212.0.which simply removes the nginx image deployed at edge3. Similarly, adding a newimage to place image(redis, 149,60) to the repository and querying declace/2 returnsthe following placement3.4 Solving OIPP with ASPWe now introduce an ASP encoding (according to the GRINGO language [20]) to solvethe OIPP problem (Problem 3). We follow the Guess, Check & Optimise paradigm,thus defining programs that (i) generate the search space of possible image placements,(ii) prune invalid ones, and (iii) minimise an objective function to rank valid solutions.The ASP input follows the format introduced in Section 3.1. Figure 2 lists thecomplete ASP encoding. The first requirement is for each image to be deployed on atleast one (and at most R) nodes as per Equation 1 of Definition 5. This is succintlyencoded at line 75, where the placement(I, N) atom models that each image I is placedonto a non-empty set of at most R nodes N.The rules at lines 76-81 account for the computation of transfer times, wheretransfer_time(I, Src, Dst, T) models that it is possible to transfer image I from nodeSrc to node Dst in T time units. The first rule (line 76-77) states that if the image Img isplaced on node Src its transfer time towards Src itself is null. The second rule computesTransfer times in the general case as in the Prolog code (lines 19-24) and relies on the@-term compute_transfer_time, which executes a Python function to compute transfertimes of images between pairs of nodes \u2013 since ASP does not natively support floatingpoint operations. Last, the rule transfer_ok(Img, Dst) models that the image Img canbe pulled from node Dst within its maximum transfer time.We now define constraints to prune invalid image placements generated by theabove choice rule. First, each image's transfer time must meet the specified maximumLatency requirements as per Equation 2 of Definition 5. Thus, our ASP encondingdiscards placements that do no allow a node to pull an image within its maximumTransfer time (line 86). Furthermore, the images placed on the same node should notexceed its capacity as per Equation 3 of Definition 5. This is achieved by discardingsolutions that do not meet this constraint (line 87-88). Here, TS is the sum (computedwith the aggregate #sum) of the sizes Size over all the pairs (Size, Img) for an imageImg with size Size (line 87) placed on the node N with maximum capacity Cap (line 88).The above lines determine all solutions to IPP.Finally, we aim at finding a cost-optimal image placement, i.e., one that minimiseStorage costs as per Definition 7. We can do so by adding the weak constraint at line89-90. Weak constraints enable defining a preference criterion over a logic program'sAnswer sets, akin to an objective function in an optimisation problem. The weak constraint in our program expresses the following: when there is a placement of an imageImg of size Size on a node X that has monthly a cost per MB of Cost, such placementContributes to the cost of the answer set as the additive term Cost * Size once for eachDistinct pair of the tuple (Img, X). Optimal answer sets are those with minimal costs.3.5 Functioning of declaceWe assume that declace periodically checks the state of the image placement exploitingProlog continuous reasoning in combination with ASP-based optimisation. Figure 3Sketches the overall functioning of declace. ASP is used to solve OIPP when (i) anInitial image placement needs to be determined, or (ii) the heuristic continuous reaSoning, invoked via declace/2, fails to fix the current placement after an infrastructureor repository change. Informally, the ASP encoding acts as a backup for the secondClause of crPlacement/5 of Section 3.3.Such behaviour, which we will thoroughly assess in the next section, suitably alterNates ASP optimisation and continuous reasoning adaptation steps, balancing betweenPrompt adaptation and cost optimisation of found placements. On one hand, it ensuresOptimal initial placements and optimal re-placements when the current status cannotBe recovered. On the other hand, it features faster decision-making in the adaptationPhase at runtime, by focussing only on those images that need to be migrated to reduceService disruption and avoid unnecessary image transfers. Indeed, ASP solves OIPPExactly with the disadvantage of possibly incurring longer execution times over smallProblem instances than our Prolog heuristic, which solves problem instances reducedVia continuous reasoning, taming their NP-hard complexity."}, {"title": "4 Experimental Assessment", "content": "In this section, we report and discuss the results of our experiments10 with the pro-totypes illustrated in Section 3. In the following, we will assess, compare and contrastvia simulation the performance of both the individual components of declace and oftheIr combination as per the workflow depicted in Figure 3.4.1 Assessment of ASP and Iterative DeepeningIn this first experiment, we compare the performance of our iterative deepening solu-tion and ASP solutions to IPP and OIPP, respectively. Particularly, we consider theExecution times of both solutions and the cost of their output placements.Setup We consider the set of images of Table 1, and we group them into threeSubsets, each containing four images: I1= {busybox, memcached, nginx, mariadb},I2= {alpine, traefik, httpd, postgres}, and I3= {ubuntu, redis, rabbitmq, mysql}. NoteThat the three subsets are homogeneous in that they contain one small image (withMaximum transfer time Tmax= 15 seconds), two medium images (Tmax= 30 andTmax= 60 seconds, respectively), and one large image (Tmax= 120 seconds).We then generate random infrastructures at varying number of nodes n. EachInfrastructure is generated according to a Barabasi-Albert topology [21], with preferEntial attachment parameter m = 3 by relying on the Python's NetworkX library [22].The Barabasi-Albert yields graphs with inhomogeneous degree distributions that areOften observed in real-life networks [21], characterised by hubs with a high degree (e.g.Cloud nodes), while most of the nodes have a lower degree (e.g. edge noes), such asComputer networks and the Internet [23]. As the first nodes in the network are likelyTo be connected to many other nodes in Barabasi-Albert topologies, we generate inFrastructures with three extra nodes and we then remove the first three nodes fromThe obtained graph to avoid that they can host all image replicas.On top of the generated network topology, each node gets assigned its storage acCording to a uniform discrete distribution: 10% of the nodes get 2000 MB, 50% ofThe nodes get 4000 MB and, 40% of the nodes get either 8000 MB or 16000 MB.Links in the generated network are assigned a latency in between 1 and 10 ms, and aBandwidth in between 25 and 1000 Mbps. Last, end-to-end links are computed by deTermining shortest paths between nodes based on latency through Dijkstra algorithm,And constraining them to the minimum bandwidth value along the path.This first experiment consists of trying to place from scratch the set of imagesI1, I\u2081 U I2 and I\u2081 U I2 U I3 onto 1000 randomly generated infrastructures per eachInfrastructure size n \u2208 {25, 50, 75, 100, 125, 150}. We set the maximum amount ofReplicas R = 15. For all runs, we measure execution times (with a timeout of 45Seconds) and output placement costs.Results Figure 4, 5, and 6 show the average execution times and placement costsObtained over 1000 random instances when placing I\u2081 (4 images), I\u2081UI2 (8 images) andI\u2081 U I2 U I3 (12 images) onto varying infrastructure sizes. We only consider instancesFor which both ASP and iterative deepening can succeed, i.e., we do not account forProblem instances that are labelled as unsatisfiable by our ASP solution.Figure 4a shows that iterative deepening is faster than ASP in placing 4 images forInfrastructure of at least 75 nodes. Particularly, it is at least 1-2 seconds faster thanASP for infrastructure sizes above 100 nodes; this corresponds to iterative deepeningBeing between 2% (25 nodes) and 43% (150 nodes) faster than ASP in determiningEligible placements. For what concerns cost, instead, iterative deepening determinesSolutions that are on average more expensive by a factor between 4% (25 nodes) and18% (150 nodes) than the optimal solution identified by ASP (Figure 4b). In ourExperiments, this constitutes a difference of at most 45 euro per month for 150 nodes.Similarly, Figure 5a shows that iterative deepening is faster than ASP in placing 8Images, already for infrastructures larger than 50 nodes. This corresponds to executionTimes that are between 11% (50 nodes) and 58% (150 nodes) faster, in favour ofIterative deepening. Again, such a performance increase is paid at the price of solutionsThat are sub-optimal by a percentage between 7% (25 nodes) and 21% (150 nodes)(Figure 5b). This turns out in execution times of at most 3 seconds for iterativeDeepening and of almost 8 seconds for ASP on average, with a cost difference of atMost 115 euro per month on average.Then, Figure 6a shows that iterative deepening is always faster than ASP in placIng 12 images. In this latter case, which represents more complex problem instances,Iterative deepening is between 20% (25 nodes) and 81% (150 nodes) faster than ASPIn determining solutions that are between 5% (25 nodes) and 15% (150 nodes) farFrom optimal (Figure 6b). This corresponds to at most 5-second execution times forIterative deepening, with a cost increase of at most 150 euro per month on average,Compared to ASP execution times that reach 22 seconds on average.Note that, while ASP always determines eligible placements if any exists - theHeuristic approach incurs in some instances that cannot be solved within the timeout.Such number increases along with the size of the considered problem instances (from0.1% 8 images, 50 nodesUp to 11.8% of the instances - 12 images, 150 nodes).Discussion These results confirm the analyses of Section 3.5:\u2022 Iterative deepening is in general faster than ASP (by a factor between 2% and81%) in solving the considered container image placement problem both at varyingInfrastructure sizes and number of images to be placed,\u2022 ASP always finds exact solutions to OIPP, while heuristic iterative deepening deTermines solutions that are between 4% and 18% far from optimal, and shows anIncreasing failure rate while handling larger problem instances,\u2022 Iterative deepening stays closer to the optimal when placing fewer images irrespecTively of the size of the target infrastructure, as per Figures 4 and 5.Finally, the above experimental results allow us to estimate reasonable timeoutThresholds for both our ASP encoding (which did not exceed 25 seconds on average)And heuristic iterative deepening solution (which did not exceed 6 seconds on average).4.2 Assessment of declace adaptive behaviourIn this second experiment, we will assess the performance of declace in continuouslyAdapting the placement of container images over cloud-edge resources, i.e., in solvingCIPP over lifelike scenarios at an increasing number of nodes and images to place. AsBefore, we focus on execution times and placement costs achieved by declace againstAn ASP-computed baseline.Setup We consider again the set of images reported in Table 1, and group them asBefore into the same subsets of images to place, viz. I1, I\u2081 U I2 and I\u2081 U I2 U I3. ForEach amount of images to place, we generate a random Barabasi-Albert infrastructureAgain setting parameter m = 3. Each node is initially assigned 4000 MB of storage,While end-to-end links are generated with the same QoS profiles and routing procedureAs in the previous experiment. Differently from the previous experiment, since weAre interested in evaluating our methods in an dynamic scenario, we do not generateMultiple random infrastructures but simulate life-like network events by modifying anInitial random infrastructure and set of images for 1000 epochs. At each epoch, theFollowing changes in the infrastructure can happen:\u2022 node failure: each node in the network can fail (and be removed from theInfrastructure for one\u00b9\u00b9 epoch.) with a probability of 5%,\u2022 link QoS and node storage variations: with a probability of 0.5 each node storage,And each link latency and bandwidth profile\u00b9\u00b2 change by a uniformly sampled factorOf \u00b115%, and\u2022 image requirements variations: with a probability of 0.1, each image storageRequirement change by a uniformly sampled factor of \u00b15%.Link and node attributes are directly affected (depending on the type of change),While node-to-node metrics are re-computed as we sketched in the previous experimentAnd are indirectly affected by the simulated network changes. Thus, each simulationStep yields a network infrastructure and image requirements that slightly differ fromThe previous configuration. On each step of the simulation, we run declace (accordingTo the workflow in Figure 3) and the vanilla version of our ASP enconding to collectTheIr respective execution times. Continuous reasoning Prolog steps are executed with6-second timeouts, while ASP-based reasoning steps are executed with a timeout of 25Seconds (both in declace and in the vanilla version), as per our previous experiment.Results Figure 7, Figure 8, and Figure 9 show the results of the three simulations weRun, aggregating average execution times and placement costs at varying infrastructureSizes and for 4, 8 and 12 images, respectively.As per Figure 7, declace and ASP show similar performances both in terms ofExecution times and obtained placement costs, with declace slightly worse on bothMetrics. Note that, despite this, the optimal ASP solution guarantees a cost-optimalPlacement but does not consider the implicit cost paid to discard (a possibly large partOf) the current image placement (intuitively, the \"cost of moving images around\").On the other hand, continuous reasoning attempts to contain this cost by identifyingAnd migrating only those images that do not currently satisfy their transfer time andStorage requirements, in view of evolving infrastructure conditions.On the other hand, Figure 8 already shows the benefits of continuous reasoningIn reducing decision-making times for infrastructures of at least 50 nodes. Indeed, asPer Figure 8a, declace shows an average consistent reduction in the range of 1.4-1.9Seconds, corresponding to a reduction in the execution times that ranges from 11%(150 nodes) to 21% (75 nodes). For what concerns cost, Figure 8b shows a slightIncrease in the range between 3% (150 nodes) and 19.8% (75 nodes).Finally, Figure 9 shows how more complex settings can benefit from continuousReasoning on any of the considered infrastructure sizes. Particularly, as per Figure 9a,Declace improves on execution times by a factor between 19.2% (150 nodes) and 85%(25 nodes). On the other hand, cost increases by a factor between 3.9% (150 nodes)And 23.9% (25 nodes).Discussion Summing up, we obtained the following experimental evidence:\u2022 execution times lower than ASP: as for problem instances with at least 8 imagesAnd 75 nodes, declace always performs better than ASP by reducing execution timesOf a portion between 11% and 85% and is inversely proportional to the size of theTarget infrastructure (i.e., the larger the infrastructure, the lower the gain in termsOf execution times),\u2022 limited cost increase: as for problem instances with at least 8 images and 50 nodes,The advantages in terms of execution times brought by declace correspond to a costIncrease between 3% and 24%, also inversely proportional to the size of the targetInfrastructure,\u2022 avoidance of unnecessary migrations: since continuous reasoning focuses on comPuting feasible placements by migrating only images that violate constraints inDefinition 5, which turns out in a considerable reduction in terms of execution timesWhen the current network conditions still support the previously enacted placement.Overall, the above results show that interleaving ASP with continuous reasoning basedOn iterative deepening achieves a positive trade-off between execution times and theCost of the found solution placement."}, {"title": "5 Related Work", "content": "Despite being interesting and challenging, the problem of distributing container im-ages in Cloud-Edge landscapes has only been investigated in a few recent works,viz., [9, 10, 24-26], none of which considers runtime image placement adaptation.Indeed, most efforts have been devoted to the (online or offline) problem of plac-ing multiservice applications onto specific target nodes in a context- and QoS-awaremanner as surveyed, for instance, in [6]. Among these, Forti et al. [12] and Herreraet al. [15] proposed to exploit continuous reasoning to speed up decision-making atruntime, when selecting suitable placements for multiservice applications.Focussing on Docker image distribution in cloud datacentres, Kangjin et al. [27]Proposed a peer-to-peer extension of the Docker Image Registry system. Similarly,Yet targeting cloud-edge computing settings, the authors of [24] and [25] devise fullyDecentralised container registries based on local agents compliant with the Docker regIstry API. Also Zhang et al. [26] propose a container image distribution architectureBased on peer-to-peer pre-caching of images onto alive edge nodes. Although missingQoS-, cost- and context-aware strategies to decide on container image placement, theAbove works show that distributing images around the network can lead to substanTially lower container pull times, improved image availability, and reduced networkTraffic. They might constitute the enabling technology for declace to distribute images.Darrous et al. [9] first investigated the problem of QoS-aware image placement inCloud-edge settings by proposing two placement strategies based on k-center optimisaTion. While their problem model is similar to ours, they do not consider storage costsAnd only focus on reducing container retrieval times. They employ a Python-basedSolver for k-center problems and propose sorting images to be placed by decreasingSize as we do. They assess their proposal against a best-fit and a random strategyOver networks up to 50 nodes. Finally, Theodoropoulos et al. [10] formulate the imagePlacement problem as a minimum vertex cover problem and solve it through a reInforcement learning approach. They assess their proposal against a greedy strategy.Execution times of the proposed reinforcement strategies are two orders of magnitudeSlower than those of the greedy strategy but obtain on average a better vertex cover.Differently from declace, both [9] and [10] focus on initial image placement andDo not consider the possibility of adapting such placement in response to runtime inFrastructural or image repository changes, which might turn useful in ever-changingCloud-edge landscapes, working in continuity with CI/CD release pipelines. To the bestOf our knowledge, our work is the first employing continuous reasoning for adaptivelySolving the cloud-edge image placement problem at runtime while reducing execuTion times, by focussing only on those images that need to be re-allocated. Thanks toIts declarative nature, our approach features better readability, maintainability andExtensibility than procedural solutions like [9], and better interpretability and increMentality of the obtained solutions, which can be difficult to obtain when relying onLearning approaches, like [10]."}, {"title": "6 Concluding Remarks", "content": "In this article, we have formally modelled the (optimal and continuous versions of the)Problem of adaptive placement of container images in cloud-edge setting and proved itsDecisional version is NP-hard. Then, we have proposed a declarative pipeline to solveSuch a problem in a QoS-, context- and cost-aware manner. We have implemented ourMethodology through logic programming in the open-source prototype declace, whichCombines ASP to determine cost-optimal initial placements and Prolog for adaptingImage placements at runtime via continuous reasoning. We have thoroughly assessedOur prototype by simulating its functioning over lifelike data. The incremental apProach of continuous reasoning reduces the size of the problem instances to be solvedAnd the number of image migrations. This makes runtime decision-making faster, alsoAvoiding unneeded data transfers. The synergistic use of Prolog and ASP balancesInitial cost optimisation with prompt runtime adaptation. The usage of logic programMing to define our methodology makes it more concise (~ 100 lines of code), readableAnd extensible than imperative solutions. Future work on this line may include:\u2022 the extension of declace to account for image layers, dependencies between non-baseImages, and associated transfer delays, and with with probabilistic reasoning [28] to"}]}