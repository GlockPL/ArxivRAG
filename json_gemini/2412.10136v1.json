{"title": "Can LLMs Convert Graphs to Text-Attributed Graphs?", "authors": ["Zehong Wang", "Sidney Liu", "Zheyuan Zhang", "Tianyi Ma", "Chuxu Zhang", "Yanfang Ye"], "abstract": "Graphs are ubiquitous data structures found in numerous real-world applications, such as drug discovery, recommender systems, and social network analysis. Graph neural networks (GNNs) have become a popular tool to learn node embeddings through message passing on these structures. However, a significant challenge arises when applying GNNs to multiple graphs with different feature spaces, as existing GNN architectures are not designed for cross-graph feature alignment. To address this, recent approaches introduce text-attributed graphs, where each node is associated with a textual description, enabling the use of a shared textual encoder to project nodes from different graphs into a unified feature space. While promising, this method relies heavily on the availability of text-attributed data, which can be difficult to obtain in practice. To bridge this gap, we propose a novel method named Topology-Aware Node description Synthesis (TANS), which leverages large language models (LLMs) to automatically convert existing graphs into text-attributed graphs. The key idea is to integrate topological information with each node's properties, enhancing the LLMs' ability to explain how graph topology influences node semantics. We evaluate our TANS on text-rich, text-limited, and text-free graphs, demonstrating that it enables a single GNN to operate across diverse graphs. Notably, on text-free graphs, our method significantly outperforms existing approaches that manually design node features, showcasing the potential of LLMs for preprocessing graph-structured data, even in the absence of textual information. The code and data are available at https://github.com/Zehong-Wang/TANS.", "sections": [{"title": "1 Introduction", "content": "Graph-structured data are prevalent in many real-world domains, including chemistry, social networks, and recommendation systems (Gilmer et al., 2017; Wang et al., 2023; Zhao et al., 2023; Zhang et al., 2024). Graph neural networks (GNNs) (Kipf and Welling, 2017; Hamilton et al., 2017; Veli\u010dkovi\u0107 et al., 2018; Gilmer et al., 2017) have emerged as powerful tools for processing such data by learning node representations that capture both the structural and feature properties of graphs. In each GNN layer, the model first applies a linear transformation to project the input node features or previous layer embeddings into a new space. Then, through message passing (Gilmer et al., 2017), the model aggregates information from neighboring nodes to update node embeddings."}, {"title": "2 Backgrounds", "content": "Preliminary. We analyze why existing GNNs struggle to handle multiple graphs with different feature spaces. To understand this limitation, we first review the message-passing process in GNNs. Given a graph G = (V,E) with a node set V and edge set E, where each node v \u2208 V is associated with a feature vector x \u2208 Rd, a GNN encoder takes the graph as input and learns node embeddings Z = \u03c6(V, E) through message passing. A typical GNN layer is defined as follows:\nz(l)i = \u03c3(W1z(l\u22121)i +W2\u2211j\u2208N(i)z(l\u22121)j|N(i)|)\nwhere N(i) denotes the neighbors of node i, |N(i)| is the number of neighbors, z(l) is the node embedding at layer l, and W1, W2 are learnable weight matrices. In the first layer, the model applies a linear transformation to project the input node features, meaning the dimensions of the transformation matrix are fixed based on the input features."}, {"title": "3 Method: TANS", "content": "The core idea behind TANS is to leverage topological information as auxiliary knowledge to enhance LLM-generated descriptions for each node. Our method is versatile, applying not only to graphs without textual descriptions but also improving the quality of graphs with existing textual data.\nChallenges. We identify two key challenges in developing TANS: (1) How to identify the most relevant topological information for describing each node? (2) How to effectively integrate LLMs to interpret and utilize this topological information?\nOverview. The framework of TANS is illustrated in Figure 2, and it consists of four main steps: (1) Compute topological properties for each node, (2) Use these properties to generate basic node descriptions, (3) Leverage LLMs to predict node roles and explain the reasoning behind these predictions, and (4) Treat the LLM-generated output as the final node descriptions. Before detailing each step, we first present relevant use cases to demonstrate the practicality of our method."}, {"title": "3.1 Application Scenarios", "content": "We define three types of graphs based on the amount of textual information associated with each node: text-rich, text-limit, and text-free graphs. TANS is designed to handle all of these scenarios, while the most relevant baselines, TAPE and KEA, are limited to specific cases, as shown in Table 1.\nText-Rich Graphs. Each node contains abundant textual descriptions that provide sufficient information for downstream tasks. While TAPE and KEA perform well in this scenario, TANS further enhances the node descriptions by incorporating topological information.\nText-Limit Graphs. Nodes have only sparse textual descriptions, which may lack sufficient detail for downstream tasks. TANS supplements this limited information with topological knowledge, making it more effective than baselines.\nText-Free Graphs. No textual descriptions are available for nodes, leaving topological information as the only resource for downstream tasks. TANS excels in this scenario, where other methods are not applicable."}, {"title": "3.2 Step 1: Graph Properties", "content": "To balance effectiveness in describing node-level characteristics and computational efficiency, we select the following five graph properties.\nDegree Centrality. This property measures the number of directly connected nodes for a target node, capturing its localized importance or influence (Zhang and Luo, 2017). It helps LLMs determine whether a node is central or peripheral within the graph:\nCD(v) = deg(v) = |N(v)|,\nwhere deg indicates the node degree.\nBetweenness Centrality. This property measures how frequently a node lies on the shortest paths between other nodes, highlighting its role in facilitating communication or information flow within the graph (Zhang and Luo, 2017). This helps LLMs to identify nodes that act as key intermediaries for generating more informative descriptions:\nCB(v) = \u2211s\u2260v\u2260teV \u03c3st(v)Ost,\nwhere ost is the total number of shortest paths from node s to node t, and \u03c3st(v) is the number of those paths that pass through v (excluding endpoints).\nCloseness Centrality. This property measures how close a node is to all other nodes in the graph by calculating the average distance from a given node to every other node. It reflects the node's global centrality and how efficiently information can spread from it across the graph (Zhang and Luo, 2017). Thus, it helps LLMs to capture the global influence of nodes:\nCC(v) = N-1\u2211\u03c5\u2208V,\u03c5\u2260\u03c5 d(u, v),\nwhere N is the number of nodes, and d(u, v) is the shortest distance between nodes u and v.\nClustering Coefficient. This property measures the likelihood that a node's neighbors are also connected to each other, indicating the formation of triangle-like structures in the graph. It provides LLMs with insights into the local transitivity of the network (Saram\u00e4ki et al., 2007), which is crucial for understanding the cohesiveness of a node's neighborhood:\nCtri = 2T(\u03c5)deg(v)(deg(v) \u2013 1),\nwhere T(v) is the number of triangles that include node v, and deg(v)(deg(v)\u22121) represents the maximum possible number of triangles around node v. A value of Ctri = 1 indicates that all of neighboring nodes are fully connected, while Ctri = 0 suggests that the node is isolated in its neighborhood.\nSquare Clustering Coefficient. Similar to the clustering coefficient, this property measures the tendency of nodes to form square-like structures rather than triangle-like ones. By capturing more complex interactions among nodes, it provides LLMs with a deeper understanding of the correlations within a node's neighborhood (Zhang et al., 2008)."}, {"title": "3.3 Step 2: Generate Basic Node Descriptions", "content": "We generate basic node descriptions using the computed node properties, which are then fed into LLMs for inference. These descriptions are composed of four components, as shown in Table 2.\nPrompt 1: Prefix. This part provides basic information about the graph, including its type and the type of nodes, helping LLMs detect key properties and interpret the following content more effectively. For example, in citation graphs, the LLMs might prioritize textual descriptions since they provide rich information about papers (He et al., 2024). In contrast, in social networks, topological features like the clustering coefficient or degree centrality may be more relevant (Zhang and Luo, 2017), reflecting close friendships or node popularity, respectively.\nPrompt 2: (Optional) Node Text. This component incorporates the original node textual descriptions (if applicable), enabling the method to handle graphs that have inherent textual data. It also helps capture neighborhood information more effectively when describing the target node.\nPrompt 3: (Optional) Neighbor Text. This component stores the textual descriptions of neighboring nodes. We randomly select k = 5 neighbors to provide additional context. This is especially important for text-limited graphs, where original descriptions may be insufficient, and 1-hop neighborhood information has proven to be informative (Han et al., 2023; Ju et al., 2024), as supported by our experimental results in Table 9.\nPrompt 4: Node Property. This component appends the pre-processed node properties to the prompt. Unlike methods that input the entire graph for inference (Guo et al., 2023; Wang et al., 2024a), our approach explicitly injects topological knowledge into the LLMs, making them more controllable. We also provide the ranking of nodes based on these properties, ensuring that the LLMs better understand their relative importance."}, {"title": "3.4 Step 3: Prompting LLMS", "content": "The basic node descriptions from the previous step are fed into an LLM for inference. To ensure robustness and transferability, we aim to generate descriptions that are consistent across different graphs, so that the resulting textual embeddings remain close in the feature space. In our experiment, we use the public GPT-4o-mini interface for prompting.\nPrompt 5: Suffix. To achieve this, we avoid outputting overly specific or uninterpretable knowledge. Instead, we aim for general descriptions by providing the potential node classes and having the LLM analyze the correlation between the basic descriptions and these classes, generating the top-k predictions along with corresponding explanations. The specific prompt format is shown in Table 2. If the number of classes exceeds 3, we set k = 3; otherwise, k is set to 1."}, {"title": "3.5 Step 4: Explanations", "content": "We use the LLM-generated output as the final node descriptions, which explain why a node likely belongs to certain classes. For text-rich and text-limited graphs, we append the generated descriptions to the original text and then use a textual encoder to produce the node embeddings. For text-free graphs, the generated text serves as the node description, and we similarly apply a textual encoder for embedding."}, {"title": "4 Experiments", "content": "4.1 Experimental Setup\nDataset. We use five graph-structured datasets in our experiments: Cora, Pubmed, USA, Europe, and Brazil, with statistics provided in Table 3. Cora and Pubmed are citation networks (He et al., 2024), where nodes represent papers and edges represent citations. Each node contains paper titles and abstracts, and the classes correspond to paper types. These graphs can be either text-rich (using both titles and abstracts) or text-limited (using only titles or abstracts). USA, Europe, and Brazil are text-free airport datasets (Ribeiro et al., 2017), where nodes represent airports and edges represent flight connections. The goal is to classify airports based on their activity levels.\nBaselines. We compare several feature alignment methods. For graphs with textual descriptions, the primary baselines are TAPE (He et al., 2024) and KEA (Chen et al., 2024a), as discussed in related works. In our experiments, we append the generated texts to the original node descriptions rather than using their original, more complex training paradigms, as our focus is on aligning feature spaces across graphs for cross-graph learning. We also evaluate models that rely solely on original textual descriptions or node features. For text-free graphs, where TAPE and KEA are not applicable, we compare against methods that generate node features from graph topologies, such as Node Degree (Ribeiro et al., 2017), Eigenvector (Dwivedi et al., 2023), and Random Walk (Dwivedi et al., 2022). For these methods, we set the number of feature dimensions to 32, which we found empirically to provide good performance.\nEvaluation Protocol. We run each experiment 30 times with different random seeds to reduce the impact of randomness. The node classification results are reported on the test set, using the model that performs best on the validation set. We use accuracy as the evaluation metric and employ GCN (Kipf and Welling, 2017), GAT (Veli\u010dkovi\u0107 et al., 2018), and MLP as backbone models. For the textual encoder, we follow (Chen et al., 2024b) to use MiniLM (Wang et al., 2020) otherwise specifically indicated. The hyper-parameters are presented in Appendix B.1."}, {"title": "4.2 Single-Graph Learning", "content": "Setting. We evaluate model performance by training from scratch, following the settings from (Chen et al., 2024a). Two evaluation settings are used: low-labeling and high-labeling. In the low-label setting, we randomly select 20 nodes per class for training, 30 nodes per class for validation, and use the remaining nodes for testing. For the smaller Brazil dataset, we use 10 nodes per class for training and 20 for validation. In the high-label setting, we randomly split the nodes into 60%/20%/20% for training, validation, and testing.\nText-Rich Graphs. Table 4 presents the performance of our models on Cora and Pubmed under low-label and high-label settings, using GCN, GAT, and MLP backbones. The results show that node features generated by advanced textual encoders outperform the original features. While methods like KEA and TAPE improve performance with additional textual information, our approach achieves superior results. This is likely due to the incorporation of graph topological properties, which provide a deeper understanding of node roles within the graph. Notably, our method shows the largest improvement with MLPs, demonstrating the advantage of using topological information to enhance node feature quality.\nText-Limit Graphs. Table 7 shows the performance of our models in the text-limit setting using the GCN backbone under low-label conditions. Since TAPE and KEA cannot be applied here, we compare the performance of (1) using only titles, (2) using only abstracts, and (3) using titles combined with our method. As expected, the performance in the text-limit setting is lower than in the text-rich setting. However, our method improves performance by about 5% on average compared to the best baselines, highlighting the effectiveness of incorporating topological properties to enhance node descriptions in graphs with limited text.\nText-Free Graphs. We report model performance on USA, Europe, and Brazil using the GCN backbone in Table 5 and the MLP backbone in Table 15. Our method significantly outperforms existing approaches that use graph properties to generate node features. We attribute this improvement to two factors: (1) Our method utilizes a larger set of graph topological properties, which better describe node characteristics, and (2) LLMs analyze the relationship between the prompts and potential classes, where LLM's inherent knowledge helps infer node classes based on the provided textual descriptions, offering additional information. These results highlight the potential of our method to generate effective node descriptions even for graphs without initial textual data, enabling a unified model to process multiple graphs."}, {"title": "4.3 Cross-Graph Learning", "content": "Setting. We consider two cross-graph learning settings: domain adaptation and pretrain & finetune. In domain adaptation, we train the model on a source graph and evaluate its performance on a target graph, with 20% of the data used for validation and 80% for testing.\nPretrain & Finetune. In this setting, we use text-rich Cora and Pubmed, with results shown in Table 8. The table shows that simply applying SVD to the original node features results in significantly lower performance. Additionally, existing methods perform poorly in the transfer learning setting, sometimes even worse than using the original textual descriptions. This may be due to the excess information provided by the generated text, which limits generalization and obscures shared patterns across graphs. In contrast, our TANS incorporates topological information to generate node descriptions, proving more robust in transfer learning. One possible reason is that the topological properties capture shared structures across citation networks, leading to more reliable text generation\u00b9.\nDomain Adaptation. The results for domain adaptation across the text-free graphs USA, Brazil, and Europe are presented in Table 6, using GCN as the backbone. Our method achieves the highest average performance of 58.40, significantly outper\u00b9 Although we observe negative transfer in some experiments, this is a common issue in graph learning (Wang et al., 2024d) and beyond the scope of this paper, as our focus is on demonstrating the potential of using LLMs for feature alignment in cross-graph training."}, {"title": "4.4 Ablation Study", "content": "Prompts in Generating Node Descriptions. We analyze the impact of different prompt components, specifically focusing on the role of neighboring node textual information. When this information is excluded, only node properties and optional node descriptions are used. We conduct an ablation study by creating a variant that omits the neighborhood textual descriptions. The averaged results across three backbones are shown in Table 9, with full results in Table 16. The results demonstrate that our method improves performance on text-attributed graphs, whether using both topological properties and neighborhood descriptions or just topological properties. Notably, on text-limited graphs, the inclusion of neighborhood information is crucial, as the model's performance drops from 76.21 to 72.92 when neighborhood descriptions are removed. This highlights the importance of incorporating topological and neighborhood information to enhance node descriptions.\nTextual Encoders. To evaluate the robustness of the generated textual descriptions, we tested four different textual encoders: MiniLM (Wang et al., 2020), Albert (Lan et al., 2020), Roberta (Liu et al., 2020), and MPNet (Song et al., 2020). The average performance on the text-rich Cora dataset with a low-label setting and GCN backbone is shown in Table 10, with full results in Table 17. Our method consistently achieves the best performance, demonstrating the robustness and high quality of the generated textual descriptions."}, {"title": "5 Discussion", "content": "Expanding to More Graph-Related Tasks. In our experiments, the proposed TANS achieves desirable performance on node classification tasks for citation and airport networks, demonstrating the potential of LLMs in understanding node properties based on graph topology. This success motivates us to explore the potential of LLMs in understanding edge and graph properties, extending our method to edge-level and graph-level tasks. We plan to investigate these extensions in future work.\nConverting Basic Attributed Graphs. Our proposed TANS converts existing graphs into text-attributed graphs, facilitating feature alignment in graph preprocessing. Although our focus is on graphs classified by their associated textual descriptions, we believe that attributed graphs, where node features are generated through feature engineering, can also be converted into text-attributed graphs due to the inherent semantics of each feature dimension. For instance, Cora uses a 1433-dimensional one-hot encoding, with each dimension corresponding to a keyword, and Pubmed uses a 500-dimensional TF-IDF vector, where each dimension represents a keyword. By leveraging these inherent semantics, we can convert original node features into textual descriptions. Whether these converted graphs are classified as text-rich or text-limited will depend on the specific case. We plan to explore this conversion process in future work."}, {"title": "6 Conclusion", "content": "In this work, we explore the ability of LLMs to convert existing graphs to text-attributed graphs by generating node descriptions in graphs, regardless of whether the graphs contain textual information. Our proposed TANS enables LLMs to incorporate graph topological information when generating node descriptions, allowing for the alignment of node features across graphs. Experimental results demonstrate the superiority of our method across text-rich, text-limited, and text-free graphs in training from the scratch, domain adaptation, and transfer learning settings."}, {"title": "7 Limitations", "content": "One limitation of our work is the exclusion of large-scale graphs (with more than 100,000 nodes) from our experiments. Applying TANS to such large graphs is expensive, as generating textual descriptions requires querying GPT for each node, which significantly increases time and cost. This limitation also restricted our ability to conduct more extensive ablation studies on prompt design. However, the ablation studies we performed still provide meaningful insights into how our method works, and future work could explore more efficient template designs to further optimize the process.\nAdditionally, we used GPT-4o-mini for querying, which may have a lower capacity compared to GPT-4o. Despite this, our experimental results were still highly desirable. It remains uncertain whether GPT-4o would significantly outperform GPT-4o-mini, and further investigation into this aspect could be part of future research."}, {"title": "8 Ethical Considerations", "content": "Our method serves as a tool for generating textual descriptions for graph-structured data using LLMs. However, there is potential for the generated content to include biased or harmful information. To mitigate this risk, more careful prompt design, including clear instructions and guidelines, can help steer the LLMs toward generating positive and accurate content. Additionally, users must be mindful of ethical concerns such as bias in the data and ensure responsible use of the tool in different applications."}, {"title": "A More Related Works", "content": "Graph Neural Networks. Graph neural networks (GNNs) (Liu et al., 2023; Wang et al., 2024b,e; Liu et al., 2024b) are effective in various graph learning tasks by utilizing the message passing framework. For example, GCN (Kipf and Welling, 2017) leverages the Laplacian matrix for message passing, MPNN (Gilmer et al., 2017) formally defines the message passing framework, GraphSAGE (Hamilton et al., 2017) extends it to inductive learning, and GAT (Veli\u010dkovi\u0107 et al., 2018) introduces attention mechanisms. Further works (Wang et al., 2019, 2023; Zhang et al., 2024) have extended message passing to various graph types and applications. However, a key limitation of message passing GNNs is their inability to handle graphs with different feature spaces (Liu et al., 2024a; Wang et al., 2024c), highlighting the need for effective feature alignment methods across graphs.\nManually Designed Node Features. Another approach involves manually designing node features using topological information. For example, node degrees can be represented using one-hot encoding to describe node properties (Ribeiro et al., 2017). Additionally, methods such as the eigenvector of the graph Laplacian or random walk-based techniques like node2vec (Grover and Leskovec, 2016) can be used to generate node embeddings based solely on topological properties."}, {"title": "B Experimental Setting", "content": "B.1 Hyper-parameters\nWe follow the hyper-parameters described in Appendix B.2 of Chen et al. (2024a) and perform 500 runs of hyper-parameter tuning using a Bayesian searcher for each method, reporting the best performance. We set the number of attention heads to 1 without searching this parameter. The hyper-parameters we searched are listed in Table 11. The parameters we used in our model are presented in Table 12, 13, and 14.\nB.2 Pretrain & Finetune Setting\nIn the transfer learning setting, the original node features of Cora (1,433 dimensions) and Pubmed (500 dimensions) cannot be directly used, as a single GNN cannot handle graphs with different feature dimensions. However, by using LLMs to encode the textual descriptions of nodes, we can naturally align the node features across different graphs. In this setting, we analyze the transfer learning performance on these datasets in the text-rich low-label scenario. The key difference from basic learning, where a model is trained from scratch, is that we use a pretrained model to initialize the parameters. During pretraining, we fix the number of epochs to 100 and the learning rate to 0.001.\nB.3 Domain Adaptation Setting\nOur proposed method converts basic graphs into text-attributed graphs, enabling the alignment of feature spaces across graphs. We evaluate our method in a classic cross-graph learning setting: domain adaptation. Domain adaptation transfers knowledge from a source graph to a target graph without fine-tuning, meaning the model is pretrained on the source graph and directly applied for inference on the target graph. In our experiments, we use the source graph for training, with 20% of the nodes in the target graph randomly selected for validation and the remaining 80% for testing. We evaluate our approach using three text-free airport graphs\u2014USA, Brazil, and Europe\u2014due to their aligned label spaces.\nC Additional Experimental Results\nAdditional experimental results are provided as follows. Table 15 presents the results using the MLP encoder on text-free graphs. The complete ablation study results are shown in Table 16, and the full results for different textual encoders are presented in Table 17."}, {"title": "D Additional Ablation Studies", "content": "D.1 Advanced Encoder\nWe also compare our methods to TAPE and KEA on an advanced graph encoder, OFA (Liu et al., 2024a). OFA proposes a graph foundation model that leverages LLMs to align node features across graphs. This model does not generate additional textual information. Instead, it employs a template-based method to help language models better encode the original node text (e.g., \"Feature node. Node title: <paper title>, node abstract: <paper abstract>...\"). We consider OFA as a \"backbone\" model, similar to GCN, as it can serve as the base encoder for TAG-based methods. We conducted additional experiments on the low-label Cora dataset (text-rich graph), using OFA as the backbone, as shown in Table 18. It is worth noting that OFA's performance is lower than GCN's, which is likely because OFA is designed for handling cross-domain and cross-task graphs, whereas GCN is optimized for solving single tasks individually.\nD.2 Impacts of Topological Information on Prompt Design\nWe provide additional experimental results to demonstrate how incorporating topological properties enhances model performance. In particular, we simply remove the corresponding prompt to analyze the impact of topologies. Using the Cora dataset in the low-label setting, we evaluate both text-rich and text-limited scenarios. As shown in Table 19, removing topological information leads to a drop in performance. Regarding the relative importance of different topological properties, we consider their significance may vary depending on the dataset and graph type. For instance, in social networks, properties like clustering coefficient may be more important as they capture triangular patterns that indicate strong friendship relationships. Similarly, other graphs may favor different topological patterns. To account for this variability, we aim to provide a comprehensive set of topological features, allowing the LLMs to automatically identify and leverage the patterns that are most beneficial for the specific dataset and task.\nD.3 Impacts of Topological Information on Node Features\nIt is possible to directly use topological properties, such as degree, centrality, and clustering coefficients, as node features (or append them to the original node features). However, this approach is limited to single-graph training and cannot be effectively extended to cross-graph training unless the original node features are either aligned or excluded entirely.\nWe conducted additional experiments to compare the effectiveness of using LLM-generated node descriptions versus directly using topological properties as features. These experiments include (1) single-graph training with a low-label setting using a GCN backbone (Table 20) and (2) cross-graph training (domain adaptation) with a low-label setting using the same backbone (Table 21). We established a baseline, Topology Properties as Features (TPF), where the topological properties used in prompt design were concatenated as node features. These features underwent normalization for training stability.\nD.4 Advanced Knowledge Transfer\nWe conduct experiments on transfer learning across text-rich and text-free datasets, as well as across different domains (e.g., citation networks and airport networks). This experiments may better demonstrate the potential of TANS in jointly handling text-attributed and text-free graphs. Note that citation networks (Cora, Pubmed) and airport networks (USA, Europe) differ significantly in their underlying structures and semantics, which naturally leads to lower transfer performance across domains. However, we observe that the performance drop is marginal in some cases, likely due to shared structural patterns such as the importance of high-degree nodes. The results are presented in Table 22. These results demonstrate that TANS can support transfer learning across both similar and dissimilar domains, achieving reasonable performance even when transferring between fundamentally different types of graphs."}, {"title": "E Results on Larger Graphs", "content": "Following (He et al., 2024), we conduct experiments on ogbn-products dataset to evaluate the model performance on relatively larger graphs. Note that we follow (He et al., 2024) to conduct subgraph sampling to manage computational costs. Specifically, for the ogbn-products dataset (~2,500,000 nodes), we sample a smaller graph with 54,000 nodes and conduct experiments on this reduced graph. Note that we did not provide results for KEA as no processed text data was available. As shown in Table 23, our proposed method, TANS, consistently outperforms TAPE on this relatively large graph. This can be attributed to TANS's ability to leverage topological information in graphs by utilizing LLMs for inference. We aim to include experiments on large-scale graphs, e.g., ogbn-arxiv (~150,000 nodes) in our future work."}, {"title": "F Case Studies", "content": "We present case studies in the following pages. Our findings show that incorporating topological information significantly influences the generated answers and improves the quality of the generated texts. Furthermore, providing neighborhood information allows the LLMs to adjust their predictions, leading to more accurate results. This highlights the importance of leveraging both node-specific and neighborhood data for improved performance in using LLMs to synthesize node descriptions."}]}