{"title": "HyperbolicLR: Epoch insensitive learning rate\nscheduler", "authors": ["Tae-Geun Kim"], "abstract": "This study proposes two novel learning rate schedulers: the Hyperbolic Learning Rate Sched-\nuler (HyperbolicLR) and the Exponential Hyperbolic Learning Rate Scheduler (ExpHyperbol-\nicLR). These schedulers attempt to address the inconsistent learning curves often observed in\nconventional schedulers when adjusting the number of epochs. By leveraging the asymptotic\nbehavior of hyperbolic curves, the proposed schedulers maintain more consistent learning\ncurves across varying epoch settings. The HyperbolicLR algorithm directly applies this prop-\nerty to the epoch-learning rate space, while the ExpHyperbolicLR maps this concept onto\nthe exponential space of epochs and learning rates. To evaluate the performance of these\nschedulers, first we found the optimal hyperparameters for each scheduler on a small num-\nber of epochs, fixed these values, and compared their performance as the number of epochs\nincreased. Our experimental results on various deep learning tasks and architectures demon-\nstrate that both HyperbolicLR and ExpHyperbolicLR maintain more consistent performance\nimprovements compared to conventional schedulers as the number of epochs increases. These\nfindings suggest that our hyperbolic-based learning rate schedulers offer a more robust and\nefficient approach to training deep neural networks, especially in scenarios where computa-\ntional resources or time constraints limit extensive hyperparameter searches.", "sections": [{"title": "1. Introduction", "content": "Recent steady progression of deep learning has prompted researchers to actively apply these tech-\nniques across diverse fields \u2013 such as computer vision, natural language processing, biomedical en-\ngineering, and even high energy physics [1]\u2013[5]. While there are several factors contributing to the\nsuccess of deep learning, including hardware development and the creation of effective model architec-\ntures, the advancement of optimization techniques is also a crucial factor. In the optimization process,\nchoosing the appropriate optimizer and searching an optimal learning rate are certainly important\n[6]\u2013[8], yet learning rate scheduling is now regarded as a almost essential process [9], [10].\nLearning rate scheduling is the process of appropriately changing the learning rate during training to\nenable more efficient learning. Various learning rate schedulers have been developed and widely used\nin deep learning research and applications.\nThese diverse learning rate schedulers assist deep learning researchers by enabling more efficient op-\ntimization, but they also add another layer of complexity by requiring the exploration of not only the\nmodel's hyperparameters but also the scheduler's hyperparameters. This additional hyperparameter\noptimization becomes increasingly time-consuming and costly, especially for large neural network\nmodels, making deep learning research more complex."}, {"title": "2. Related Work and Background", "content": ""}, {"title": "2.1. Learning Rate Scheduling in Deep Learning", "content": "The learning rate is one of the most crucial hyperparameters in training deep learning models. If the\nlearning rate is too high, the model may diverge or oscillate around the optimal point. Conversely, if\nthe learning rate is too low, the model might get stuck in a local minimum or take an excessively long\ntime to converge. Therefore, finding or appropriately adjusting the learning rate significantly impacts\nthe model's performance and convergence speed.\nLearning rate scheduling is a technique that dynamically adjusts the learning rate during the training\nprocess. Generally, it starts with a high learning rate and gradually decreases it to address issues such\nas getting stuck in local minima or overshooting the optimal point. However, there are also methods\nlike cyclic learning rate scheduling [9], which periodically changes the learning rate, or warm restart\n[10], which linearly restores the learning rate before continuing the decay process.\nThe significance of learning rate scheduling is underscored by the following considerations [12]:\n(a) Enhanced accuracy and faster training: Learning rate scheduling improves model accuracy while\nreducing overall training time.\n(b) Improved adaptability during training: Variable learning rates allow the training process to adjust\nthe learning speed and direction according to the needs of different phases, maintaining optimal\nlearning efficiency and ensuring effective convergence.\n(c) Increased training stability: By decreasing the learning rate during plateau phases and increasing it\nduring faster learning phases, learning rate scheduling helps maintain training stability, prevent-\ning problems such as oscillation or divergence, and ensuring smoother convergence.\nAs can be seen, learning rate scheduling plays a crucial role in improving the performance, efficiency,\nand stability of deep learning models. Therefore, developing and applying effective learning rate sched-\nuling techniques is one of the key challenges in deep learning research."}, {"title": "2.2. Commonly Used Learning Rate Schedulers", "content": "Let $N_0$ be the set of non-negative integers and $\\mathcal{P}$ be the set of specific hyperparameters required for\nlearning rate scheduling. A learning rate scheduler can be expressed as a function $f$ mapping from\n$N_0 \\times \\mathcal{P}$ to $[0,\\infty)$ as follows:\n$n_n = f(n; \\mathcal{P})$   (1)\nHere, $n \\in N_0$ represents the current epoch, $\\mathcal{P} \\in \\mathcal{P}$ is a tuple of hyperparameters and $n_n$ is the learning\nrate at the current epoch. Using this notation, we will describe some of the most widely used learning\nrate schedulers.\n(a) Polynomial learning rate scheduler (PolynomialLR) [13], [14]: PolynomialLR decays the learning\nrate from the initial value using a polynomial function.\n$f(n; \\eta_{init}, N, p) = \\eta_{init} \\times \\left(1 - \\frac{n}{N}\\right)^p$  (2)\nwhere $\\eta_{init}$ is the initial learning late, $N$ is the total number of epochs, and $p$ is the exponent of\nthe polynomial function.\n(b) Cosine annealing learning rate scheduler (CosineAnnealingLR) [10]: CosineAnnealingLR decays\nthe learning rate from an initial value $\\eta_{max}$ to a minimum learning rate $\\eta_{min}$ using a cosine curve.\n$f(n; \\eta_{min}, \\eta_{max}, N) = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})\\left(1 + cos\\left(\\frac{n}{N}\\pi\\right)\\right)$ (3)\nAs with PolynomialLR, $N$ is the total number of epochs.\n(c) Exponential learning rate scheduler (ExponentialLR) [15]: ExponentialLR decays the learning rate\nexponentially with a constant decay rate $\\lambda$. Unlike the two schedulers explained earlier, this sched-\nuler does not depend on the total number of epochs.\n$f(n; \\eta_{init}, \\gamma) = \\eta_{init} \\times \\gamma^n$ (4)\nwhere $\\eta_{init}$ is the initial learning rate, and $\\gamma$ is the exponential decay factor.\nThese learning rate schedulers are widely used in training deep learning models as they effectively\nimprove model performance [16], [17]. However, each of these learning rate schedulers has its own\nset of problems. In this study, we will particularly focus on the learning curve decoupling problem, a\ncommon issue shared by schedulers that depend on the total number of epochs, such as PolynomialLR\nand CosineAnnealingLR."}, {"title": "2.3. The Learning Curve Decoupling Problem", "content": "This section introduces a potential issue with learning rate schedulers that we have termed the \"Learn-\ning curve decoupling problem.\u201d This problem has not been explicitly addressed in previous literature\nand was observed in the behavior of well-known learning rate schedulers when training durations\nwere altered. We define the Learning curve decoupling problem as follows: For a given learning rate\nscheduler, when only the total number of training epochs is changed while keeping other hyperpara-\nmeters fixed, the learning rate change pattern significantly differs. This leads to a separation of the two\nlearning rate curves and, consequently, inconsistent optimization behavior, increasing the complexity\nof hyperparameter optimization.\nTo formalize this, we propose a metric for the Learning curve difference. Consider a learning rate\nscheduler $f(n; N, \\mathcal{P})$, where we fix other hyperparameters $\\mathcal{P}$ and change the total number of epochs\nfrom $N_1$ to $N_2$. Let $l_1$ and $l_2$ be the loss curves obtained from training with $N_1$ and $N_2$ epochs, re-\nspectively. We define the Learning curve difference as the following relative difference between these\ntwo curves:\n$\\Delta(l_1, l_2) = \\frac{1}{N}\\sum_{n=0}^{N-1} \\frac{|l_1(n)-l_2(n)|}{l_1(n) + l_2(n)},$ where $N = min(N_1, N_2)$ (5)\nHowever, this metric may indicate a significant difference between two learning curves even when\ntheir trends are similar but contain noise. To address this, we propose a smoothed metric:\n$\\Delta_S(l_1, l_2) = \\frac{1}{N} \\sum_{n=0}^{S-1} \\frac{|S(l_1)(n) - S(l_2)(n)|}{S(l_1)(n) + S(l_2)(n)}$ (6)\nHere, $S$ is a smoothing operator, which could be a exponential moving average [18], Savitzky-Golay\nfilter [19], or similar.\nThe Learning curve difference as defined above will approach 0 if the two curves are nearly identical\nand approach 1 if they are completely decoupled. However, since the entire learning curve can vary\ndepending on the task and dataset, it's more informative to make relative comparisons within the same\ntask and dataset to determine whether a particular scheduler exhibits more or less learning curve de-\ncoupling than others."}, {"title": "2.4. Hyperbolic Curves and Their Properties", "content": "Hyperbolic curves are a family of curves defined by the equation:\n$\\frac{x^2}{a^2} - \\frac{y^2}{b^2} = 1$ (7)\nwhere $a$ and $b$ are positive constants. These curves have two branches, each of which is symmetric\nabout the x-axis and approaches the lines $y = \\pm(\\frac{b}{a})x$ as asymptotes [11]. For the purpose of learning\nrate scheduling, we will focus on the branch where $x \\leq -a$ and $y \\geq 0$, which can be represented by\nthe equation:\n$y = \\frac{b}{a}\\sqrt{x^2 - a^2}$ (8)\nOne of the key properties of hyperbolic curves is that the slope of the curve converges to its asymp-\ntotes as the distance from the vertex increases. This property can be demonstrated by examining the\nderivative of the equation:\n$\\frac{dy}{dx} = \\frac{b}{a} \\frac{x}{\\sqrt{x^2 - a^2}}$ (9)\nTaking the limit as x approaches negative infinity, we find:\n$\\lim_{x\\rightarrow\\infty} \\frac{dy}{dx} = \\lim_{x\\rightarrow\\infty} \\frac{b}{a} \\frac{x}{\\sqrt{x^2 \u2014 a^2}} = \\frac{b}{a}$ (10)\nThis result indicates that when $x < -a$, the slope of the curve $dy/dx$ approaches a constant value\nof $-b/a$. In other words, the rate of change of the curve becomes stable and predictable as we move\nfurther away from the vertex. This convergence property of hyperbolic curves makes them a promis-\ning foundation for developing learning rate schedulers that maintain a consistent initial learning rate\nchange rate, regardless of the total number of epochs. By harnessing this property, it is possible to\ncreate a more robust and flexible approach to learning rate scheduling, enabling deep learning models\nto achieve optimal performance with minimal hyperparameter tuning."}, {"title": "3. Proposed Learning Rate Schedulers", "content": ""}, {"title": "3.1. HyperbolicLR", "content": "We introduce HyperbolicLR, a novel learning rate scheduler based on the hyperbolic curve. The learn-\ning rate at epoch $n$ is defined as:\n$f_h(n; \\eta_{init}, \\eta_{inf}, N, U) = \\eta_{init} + (\\eta_{init} \u2014 \\eta_{inf})(h(n; N, U) \u2013 h(0; N, U))$ (11)\nwhere $\\eta_{init}$ denotes the initial learning rate, $\\eta_{inf}$ represents the infimum of the learning rate, $N$ is the\ntotal number of epochs minus one, and $U$ is the upper bound of $N$. The function $h(n; N, U)$ is given by:\n$h(n; N, U) = \\sqrt{\\frac{N-n}{U}(2 - \\frac{N+n}{U})} (U \\geq N)$  (12)\nThis function represents a portion of a hyperbolic curve with vertex at $(N, 0)$, center at $(U, 0)$, and\nasymptote slope of $-1/U$, as illustrated in the Figure 2.\nThus, HyperbolicLR starts at $\\eta_{init}$ and decreases along the hyperbolic curve, ending at a learning rate\nthat is always greater than or equal to $\\eta_{inf}$ at the $N$-th epoch. (For proof, see Appendix B.) The variable\n$N$ can be adjusted within the range less than or equal to $U$. When $U$ and $N$ are equal, the curve coin-\ncides with the hyperbola's asymptote, resulting in a simple linear decay.\nA key property of HyperbolicLR is that the asymptote slope $-1/U$ does not depend on $N$, which\ncorresponds to the total number of epochs. Therefore, in the early stages of training when $n \\ll N$,\nthe change rate of the learning rate will be similar to the asymptote, meaning it won't change signif-\nicantly even if $N$ is altered. This characteristic suggests that HyperbolicLR could potentially address\nthe learning curve decoupling problem by maintaining a consistent learning pattern across varying\nepoch numbers."}, {"title": "3.2. ExpHyperbolicLR", "content": "Building upon the concept of HyperbolicLR, we propose ExpHyperbolicLR, which extends the hyper-\nbolic approach to the exponential space. The learning rate at epoch $n$ for ExpHyperbolicLR is defined\nas:\n$f_{EH} (n; \\eta_{init}, \\eta_{inf}, N, U) = \\eta_{init} \\times exp\\left(ln\\frac{\\eta_{init}}{\\eta_{inf}} \\times (h(n; N,U) \u2013 h(0; N, U))\\right)$ (13)\nwhere the hyperparameters are consistent with those in HyperbolicLR. Notably, ExpHyperbolicLR can\nbe expressed in terms of HyperbolicLR:\n$f_{eh}(n; \\eta_{init}, \\eta_{inf}, N, U) = exp(f_H(n; ln \\eta_{init}, ln \\eta_{inf}, N, U))$  (14)\nThe monotonically increasing nature of the exponential function ensures that ExpHyperbolicLR retains\nthe fundamental properties of HyperbolicLR. However, when $N = U$, it becomes a linear function in\nthe exponential space, equivalent to ExponentialLR rather than exhibiting linear decay.\nExpHyperbolicLR decreases the learning rate exponentially, making its initial rate of decrease faster\nthan HyperbolicLR. This could make it a better choice in environments where overfitting occurs easily."}, {"title": "3.3. Comparison with Existing Learning Rate Schedulers", "content": "Figure 3 illustrates the behavior of PolynomialLR, CosineAnnealingLR, and our proposed Hyperboli-\ncLR and ExpHyperbolicLR schedulers for different total epoch settings ($N = 250, 500, 750, 1000$). As\nevident from the top row, both PolynomialLR and CosineAnnealingLR exhibit significant variations\nin their learning rate decay patterns as the total number of epochs changes. In contrast, our proposed\nHyperbolicLR and ExpHyperbolicLR (bottom row) demonstrate more consistent decay patterns across\ndifferent epoch settings, particularly in the early stages of training."}, {"title": "To quantitatively measure the initial learning rate change, which we claimed to be less sensitive to\nchanges in $N$ for HyperbolicLR due to the properties of hyperbolic curves, we introduce a new metric\ncalled the Initial Learning Rate Integral (ILRI):", "content": ""}, {"title": "$ILRI = \\int_{0}^{no_{0.8}} (\\eta_n - 0.8 \\times \\eta_{init}) dn$", "content": " (15)\nwhere $n_n$ is the learning rate at epoch n, and $n_{0.8}$ is the epoch at which the learning rate reaches 80% of\nits initial value. Although epochs are typically integers and $n_n$ is a sequence rather than a continuous\nfunction, we use cubic Hermite splines to interpolate and treat epochs as real numbers for more precise\nmeasurements.\nTo compare how the initial rate of change in HyperbolicLR and ExpHyperbolicLR differs from existing\nschedulers, we measured the ILRI values for each scheduler with N set to 250, 500, 750, and 1000. Using\n$N=1000$ as a baseline, we calculated the relative ILRI percentage difference as follows:\n$\\Delta_{ILRI}(N; 1000) = \\frac{|ILRI_N \u2013 ILRI_{1000}|}{ILRI_{1000}} \u00d7 100(\\%)$  (16)\nFor PolynomialLR and CosineAnnealingLR, regardless of hyperparameter values, we can theoretically\nderive that this value is always $(1 \u2013 N/1000) \\times 100(\\%)$, which we also verified numerically. For Hy-\nperbolicLR and ExpHyperbolicLR, we fixed $U=1000$ and, after extensive experimentation, adopted\nhyperparameters that reduce the initial learning rate by three orders of magnitude, as these showed\nthe highest versatility.\nThe results are summarized in the Table 1."}, {"title": "4. Experimental Setup", "content": "This section details our experimental methodology, encompassing dataset preparation, model archi-\ntectures, and training procedures for both image classification and time series prediction tasks."}, {"title": "4.1. Datasets", "content": ""}, {"title": "4.1.1. CIFAR-10 for Image Classification", "content": "We employed the CIFAR-10 dataset [20] for our image classification experiments. This dataset com-\nprises 60, 000 number of 32 \u00d7 32 color images across 10 classes. We utilized 50,000 images for training\nand the remaining 10,000 for validation.\nData augmentation techniques applied to the training set included:\n1. Random cropping to 32 \u00d7 32 pixels with 4-pixel padding\n2. Random horizontal flipping\n3. Per-channel normalization (mean: 0.4914, 0.4822, 0.4465; standard deviation: 0.2023, 0.1994, 0.2010)\nThe validation set underwent only normalization."}, {"title": "4.1.2. Custom Oscillation Dataset for Time Series Prediction", "content": "For our time series prediction task, we constructed a custom dataset incorporating both simple and\ndamped harmonic oscillations. The dataset was generated using the following ordinary differential\nequation:\n$m\\ddot{u} + c\\dot{u} + ku = 0$ (17)\nwhere $m=1$, $k=200$, and $c = \\zeta\\cdot 2\\sqrt{mk}$. We varied the damping ratio $\\zeta$ with values 0, 0.01, and\n0.02 to simulate different oscillatory behaviors. To generate the data, we numerically solved the ODE\nusing the Newmark-beta method [21] over the interval $t \\in [0, 10]$ with a step size of $10^{-3}$. The initial\nconditions were set as $u(0) = 0.1$, $\\dot{u}(0) = 0$, and $\\ddot{u}(0) = -20$.\nWe employed a sliding window approach to prepare the data for the prediction task. For each $\\zeta$ value,\nwe generated 10,001 data points and applied the sliding window technique with a history of 100 time\nsteps and a prediction horizon of 20 time steps. The resulting sequences from all three $\\zeta$ values were\nthen combined into a single dataset.\nThis process yielded a total of 29,646 input-output pairs, with each input sequence comprising 100\nconsecutive time steps used to predict the subsequent 20 time steps. The final dataset had an input size\nof (29646, 100, 1) and a label size of (29646, 20, 1).\nTo ensure a random distribution of oscillation patterns across different damping ratios, we shuffled the\ncombined dataset before splitting it into training and validation sets. We used an 80-20 split, resulting\nin 23,716 samples for training and 5,930 samples for validation.\nPrior to training, we normalized the data to the range [0, 1] to ensure consistent scale across different\noscillation parameters. This custom dataset provides a controlled environment to evaluate the perfor-\nmance of various learning rate schedulers on a challenging time series prediction task with known\nunderlying dynamics, while also testing the model's ability to generalize across different damping\nratios."}, {"title": "4.1.3. Custom Integral Dataset for Operator Learning", "content": "For operator learning task, we aimed to learn an operator $G$ defined for continuous functions $u \\in\nC[0, 1]$ and arbitrary real numbers $y \\in [0, 1]$ as follows:\n$G(u)(y) = \\int_0^y u(x)dx$  (18)\nTo accomplish this, we employed the DeepONet approach [22]. Training a DeepONet requires two dis-\ntinct inputs: values of the input function $u$ at fixed sensor points $x_i$, denoted as $u(x_i)$, and domain\nvalues $y$ of the operator-mapped function $G(u)$. The corresponding labels are the values of $G(u)(y_i)$.\nFor the target points $y_i$, we uniformly partitioned the interval [0, 1] into 100 points. To generate inputs\ncorresponding to the input functions, we employed a Gaussian Random Field approach with a squared\nexponential kernel. To enhance the diversity of our dataset, we introduced variability in the length\nscale parameter $l$, uniformly sampling it from the range [0.1, 0.4]. This modification expands upon the\nfixed length scale typically used in similar studies [22], potentially enabling our model to generalize\nacross a broader spectrum of input functions.\nFor each randomly selected length scale parameter $l$, we uniformly divided the [0, 1] interval into 100\nsensor points. We then generated a Gaussian Random Field $X_i$ corresponding to each point $x_i$, con-\nsidering this as a discrete representation $u$ of a continuous function $u$:\n$u = [X_0, X_1, ..., X_{98}, X_{99}] = [u(0), u(x_1), ..., u(x_{98}), u(1)]$  (19)\nWe generated 10,000 such functions. To match the size, we replicated the vector $y$ of target points $y_i$\n10,000 times. Consequently, the input sizes are (10000, 100) for $u$ and (10000, 100) for $y$.\nTo optimize the learning process, we implemented a tailored normalization strategy for the generated\ninput vectors $u$. We applied a linear transformation such that $\\mu - \\sigma$ maps to -1 and $\\mu + \\sigma$ maps to 1,\nwhere $\\mu$ and $\\sigma$ represent the mean and standard deviation of the input data, respectively. This linear\nscaling ensures that approximately 68% of the data points fall within the [-1, 1] interval, while values\noutside this range are mapped proportionally. This approach preserves the inherent variability in the\ndata more effectively than standard min-max normalization, making it particularly suitable for data\ngenerated from a Gaussian Random Field."}, {"title": "4.2. Model Architectures", "content": "Figure 6 illustrates the architectures of our SimpleCNN and LSTM Sequence-to-Sequence models used\nfor CIFAR-10 classification and oscillation prediction tasks, respectively.\n4.2.1. SimpleCNN for CIFAR-10\nAs shown in the red solid box of Figure 6, our SimpleCNN architecture for CIFAR-10 classification con-\nsists of a series of convolutional layers followed by fully connected layers. Each convolutional block,\nrepeated $L_c$ times, comprises a convolutional layer, ReLU activation, and max pooling. This structure\nallows for hierarchical feature extraction from the input images.\nFollowing the convolutional blocks, the network includes $L_F + 1$ fully connected layers. The first $L_F$\nlayers are followed by ReLU activation, while the final layer produces the output logits for classifica-\ntion. This architecture balances feature extraction capabilities with model complexity, suitable for the\nCIFAR-10 dataset.\n4.2.2. LSTM Sequence-to-Sequence for Oscillation Prediction\nThe LSTM Sequence-to-Sequence model, depicted in the blue solid box of Figure 6, consists of an en-\ncoder (E) and a decoder (D), both utilizing LSTM layers. The encoder processes the input sequence\n$u(t_1), u(t_2), ..., u(t_i)$, representing historical oscillation data. The final hidden state $h$ of the encoder\nis used to initialize the decoder.\nThe decoder operates autoregressively, generating predictions $\\hat{u}(t_{i+1}), \\hat{u}(t_{i+2}), ..., \\hat{u}(t_{i+p})$ for future\ntime steps. It takes its own previous output as input for the next prediction, starting with an initial\ninput of 0. This design challenges the model to maintain long-term coherence in its predictions, crucial\nfor accurately forecasting oscillatory behavior.\n4.2.3. DeepONet for Operator Learning\nAs illustrated in the green dashed box of Figure 6, we employed the DeepONet architecture as proposed\nby [22] for our operator learning task. Both the branch and trunk networks consist of $L$ repeated layers\nof MLP followed by GELU activation, with a final MLP layer without activation. The outputs of these\nnetworks, $b = [b_1, b_2, ..., b_p]$ from the branch net and $t = [t_1, t_2, ..., t_p]$ from the trunk net (where $p$\nis a hyperparameter), are combined through an inner product to yield the operator output $\\hat{G}(u)(y)$.\n4.2.4. TraONet for Operator Learning\nTo enhance the efficiency of operator learning, we developed and utilized TraONet (Transformer Op-\nerator Network), a novel operator network that incorporates Transformer encoder and decoder models\nalongside the traditional DeepONet structure. As shown in the orange solid box of Figure 6, TraONet\ndraws inspiration from the Transformer architecture introduced in [2]. However, we adapted the struc-\nture for operator learning by removing the masking in the decoder's masked multi-head attention\ncomponent. Both the encoder and decoder parts of TraONet are repeated $L$ times to achieve the desired\ndepth of representation.\nThis novel approach combines the strengths of attention mechanisms with the proven effectiveness of\nDeepONet, potentially offering improved performance in capturing complex operator relationships.\nThe specific configurations for all models presented-SimpleCNN, LSTM Sequence-to-Sequence, Deep-\nONet, and TraONet-including the number of layers, hidden units, attention heads (for TraONet), and\nother architectural details, were determined through our comprehensive hyperparameter optimization\nprocess. This process ensures that each model is optimally tuned for its respective task, whether it be\nimage classification, time series prediction, or operator learning. We will elaborate on the hyperpara-\nmeter optimization methodology and present the resulting optimal configurations for each model in\nthe subsequent sections, providing a detailed view of our experimental setup and the rationale behind\nour architectural choices."}, {"title": "4.3. Experimental Design and Evaluation Protocol", "content": "The primary focus of our experimental design is to evaluate the performance of various learning rate\nschedulers across diverse tasks and model architectures, while simultaneously investigating whether\noptimizations performed at lower epoch counts can consistently maintain performance as the number\nof epochs increases. This approach addresses the learning curve decoupling problem, where scheduler\nperformance may diverge significantly with extended training durations. Given the extensive range of\nhyperparameters for both models and schedulers, we have devised a consistent experimental protocol\napplicable across all tasks.\nOur methodology comprises three distinct phases:\n1. Model Hyperparameter Optimization: We optimize model hyperparameters using a fixed scheduler.\n2. Scheduler Hyperparameter Optimization: Utilizing the optimal model hyperparameters obtained in\nthe previous phase, we then optimize the scheduler hyperparameters, again over a small epoch\ncount.\n3. Performance Evaluation Across Varying Epoch Settings: We assess each scheduler's performance us-\ning the optimized hyperparameters from the previous phases, progressively increasing the number\nof epochs and measuring validation loss or accuracy.\nTo ensure consistency and fairness, we employ the AdamW optimizer [23] throughout all experiments,\nutilizing $\u03b2_1=0.9$, $\u03b2_2=0.999$, $\u03b5=10^{-8}$ and $\u03bb=0.01$. Furthermore, to mitigate the impact of random\ninitialization, we conduct each training session five times using seeds obtained from random.org [24],\nreporting the mean performance metrics.\n4.3.1. Model Hyperparameter Optimization\nThe hyperparameters of our models primarily consist of discrete values, such as the number of layers\nand nodes. Consequently, we employed a grid search within predefined ranges to optimize these hy-\nperparameters. For consistency across tasks, we utilized the ExponentialLR scheduler, which has the\nfewest hyperparameters, fixing the decay factor at 0.9 for all experiments over 50 epochs. The initial\nlearning rate was task-specific: $10^{-2}$ for CIFAR-10 and the oscillation dataset, and $5 \u00d7 10^{-3}$ for the\nintegral dataset. Table 2 presents the grid search ranges for each model architecture along with the\noptimal values determined through our comprehensive search."}, {"title": "4.3.2. Scheduler Hyperparameter Optimization", "content": "Following the optimization of model hyperparameters, we focused on optimizing the scheduler hyper-\nparameters. Unlike the discrete model parameters, scheduler hyperparameters are primarily continu-\nous values (e.g., learning rate, power, decay factor, infimum learning rate and etc.). To effectively opti-\nmize these continuous parameters, we employed the Tree-structured Parzen Estimator (TPE), a variant\nof Bayesian optimization [25], [26].\nOur primary objective was to assess whether optimizations performed at lower epoch counts could\nconsistently maintain performance as the number of epochs increased. Therefore, we fixed the number\nof epochs at 50 for this optimization phase. For each task and scheduler combination, we conducted\n25 trials using TPE, selecting the optimal values from these trials.\nThe initial learning rate optimization range was consistent across all schedulers, spanning from\n$10^{\u22124}$ to $5 \u00d7 10^{\u22122}$ on a logarithmic scale. Other hyperparameters were optimized within scheduler-\nspecific ranges. The detailed optimization ranges for each scheduler's hyperparameters are presented\nin Table 3."}, {"title": "4.3.3. Performance Evaluation", "content": "To assess the efficacy of our optimized scheduler configurations across varying training durations, we\nconducted a series of experiments using the optimal hyperparameters presented in Table 4. For each\ntask, we progressively increased the number of epochs from 50 to 200 in increments of 50, measuring\nthe validation loss for all models and accuracy for the SimpleCNN model on the CIFAR-10 task.\nWe evaluated the relative performance improvement per 50-epoch interval, calculating the mean and\nstandard deviation to determine which scheduler demonstrated the most consistent and substantial\nperformance gains."}, {"title": "5. Results and Analysis", "content": ""}, {"title": "5.1. Overall Performance", "content": "Figure 7 presents the performance of all schedulers across the four model architectures studied. The\nresults demonstrate that"}, {"title": "HyperbolicLR: Epoch insensitive learning rate\nscheduler", "authors": ["Tae-Geun Kim"], "abstract": "This study proposes two novel learning rate schedulers: the Hyperbolic Learning Rate Sched-\nuler (HyperbolicLR) and the Exponential Hyperbolic Learning Rate Scheduler (ExpHyperbol-\nicLR). These schedulers attempt to address the inconsistent learning curves often observed in\nconventional schedulers when adjusting the number of epochs. By leveraging the asymptotic\nbehavior of hyperbolic curves, the proposed schedulers maintain more consistent learning\ncurves across varying epoch settings. The HyperbolicLR algorithm directly applies this prop-\nerty to the epoch-learning rate space, while the ExpHyperbolicLR maps this concept onto\nthe exponential space of epochs and learning rates. To evaluate the performance of these\nschedulers, first we found the optimal hyperparameters for each scheduler on a small num-\nber of epochs, fixed these values, and compared their performance as the number of epochs\nincreased. Our experimental results on various deep learning tasks and architectures demon-\nstrate that both HyperbolicLR and ExpHyperbolicLR maintain more consistent performance\nimprovements compared to conventional schedulers as the number of epochs increases. These\nfindings suggest that our hyperbolic-based learning rate schedulers offer a more robust and\nefficient approach to training deep neural networks, especially in scenarios where computa-\ntional resources or time constraints limit extensive hyperparameter searches.", "sections": [{"title": "1. Introduction", "content": "Recent steady progression of deep learning has prompted researchers to actively apply these tech-\nniques across diverse fields \u2013 such as computer vision, natural language processing, biomedical en-\ngineering, and even high energy physics [1]\u2013[5]. While there are several factors contributing to the\nsuccess of deep learning, including hardware development and the creation of effective model architec-\ntures, the advancement of optimization techniques is also a crucial factor. In the optimization process,\nchoosing the appropriate optimizer and searching an optimal learning rate are certainly important\n[6]\u2013[8], yet learning rate scheduling is now regarded as a almost essential process [9], [10].\nLearning rate scheduling is the process of appropriately changing the learning rate during training to\nenable more efficient learning. Various learning rate schedulers have been developed and widely used\nin deep learning research and applications.\nThese diverse learning rate schedulers assist deep learning researchers by enabling more efficient op-\ntimization, but they also add another layer of complexity by requiring the exploration of not only the\nmodel's hyperparameters but also the scheduler's hyperparameters. This additional hyperparameter\noptimization becomes increasingly time-consuming and costly, especially for large neural network\nmodels, making deep learning research more complex."}, {"title": "2. Related Work and Background", "content": ""}, {"title": "2.1. Learning Rate Scheduling in Deep Learning", "content": "The learning rate is one of the most crucial hyperparameters in training deep learning models. If the\nlearning rate is too high, the model may diverge or oscillate around the optimal point. Conversely, if\nthe learning rate is too low, the model might get stuck in a local minimum or take an excessively long\ntime to converge. Therefore, finding or appropriately adjusting the learning rate significantly impacts\nthe model's performance and convergence speed.\nLearning rate scheduling is a technique that dynamically adjusts the learning rate during the training\nprocess. Generally, it starts with a high learning rate and gradually decreases it to address issues such\nas getting stuck in local minima or overshooting the optimal point. However, there are also methods\nlike cyclic learning rate scheduling [9], which periodically changes the learning rate, or warm restart\n[10], which linearly restores the learning rate before continuing the decay process.\nThe significance of learning rate scheduling is underscored by the following considerations [12]:\n(a) Enhanced accuracy and faster training: Learning rate scheduling improves model accuracy while\nreducing overall training time.\n(b) Improved adaptability during training: Variable learning rates allow the training process to adjust\nthe learning speed and direction according to the needs of different phases, maintaining optimal\nlearning efficiency and ensuring effective convergence.\n(c) Increased training stability: By decreasing the learning rate during plateau phases and increasing it\nduring faster learning phases, learning rate scheduling helps maintain training stability, prevent-\ning problems such as oscillation or divergence, and ensuring smoother convergence.\nAs can be seen, learning rate scheduling plays a crucial role in improving the performance, efficiency,\nand stability of deep learning models. Therefore, developing and applying effective learning rate sched-\nuling techniques is one of the key challenges in deep learning research."}, {"title": "2.2. Commonly Used Learning Rate Schedulers", "content": "Let $N_0$ be the set of non-negative integers and $\\mathcal{P}$ be the set of specific hyperparameters required for\nlearning rate scheduling. A learning rate scheduler can be expressed as a function $f$ mapping from\n$N_0 \\times \\mathcal{P}$ to $[0,\\infty)$ as follows:\n$n_n = f(n; \\mathcal{P})$ (1)\nHere, $n \\in N_0$ represents the current epoch, $\\mathcal{P} \\in \\mathcal{P}$ is a tuple of hyperparameters and $n_n$ is the learning\nrate at the current epoch. Using this notation, we will describe some of the most widely used learning\nrate schedulers.\n(a) Polynomial learning rate scheduler (PolynomialLR) [13], [14]: PolynomialLR decays the learning\nrate from the initial value using a polynomial function.\n$f(n; \\eta_{init}, N, p) = \\eta_{init} \\times \\left(1 - \\frac{n}{N}\\right)^p$ (2)\nwhere $\\eta_{init}$ is the initial learning late, $N$ is the total number of epochs, and $p$ is the exponent of\nthe polynomial function.\n(b) Cosine annealing learning rate scheduler (CosineAnnealingLR) [10]: CosineAnnealingLR decays\nthe learning rate from an initial value $\\eta_{max}$ to a minimum learning rate $\\eta_{min}$ using a cosine curve.\n$f(n; \\eta_{min}, \\eta_{max}, N) = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})\\left(1 + cos\\left(\\frac{n}{N}\\pi\\right)\\right)$ (3)\nAs with PolynomialLR, $N$ is the total number of epochs.\n(c) Exponential learning rate scheduler (ExponentialLR) [15]: ExponentialLR decays the learning rate\nexponentially with a constant decay rate $\\lambda$. Unlike the two schedulers explained earlier, this sched-\nuler does not depend on the total number of epochs.\n$f(n; \\eta_{init}, \\gamma) = \\eta_{init} \\times \\gamma^n$ (4)\nwhere $\\eta_{init}$ is the initial learning rate, and $\\gamma$ is the exponential decay factor.\nThese learning rate schedulers are widely used in training deep learning models as they effectively\nimprove model performance [16], [17]. However, each of these learning rate schedulers has its own\nset of problems. In this study, we will particularly focus on the learning curve decoupling problem, a\ncommon issue shared by schedulers that depend on the total number of epochs, such as PolynomialLR\nand CosineAnnealingLR."}, {"title": "2.3. The Learning Curve Decoupling Problem", "content": "This section introduces a potential issue with learning rate schedulers that we have termed the \"Learn-\ning curve decoupling problem.\u201d This problem has not been explicitly addressed in previous literature\nand was observed in the behavior of well-known learning rate schedulers when training durations\nwere altered. We define the Learning curve decoupling problem as follows: For a given learning rate\nscheduler, when only the total number of training epochs is changed while keeping other hyperpara-\nmeters fixed, the learning rate change pattern significantly differs. This leads to a separation of the two\nlearning rate curves and, consequently, inconsistent optimization behavior, increasing the complexity\nof hyperparameter optimization.\nTo formalize this, we propose a metric for the Learning curve difference. Consider a learning rate\nscheduler $f(n; N, \\mathcal{P})$, where we fix other hyperparameters $\\mathcal{P}$ and change the total number of epochs\nfrom $N_1$ to $N_2$. Let $l_1$ and $l_2$ be the loss curves obtained from training with $N_1$ and $N_2$ epochs, re-\nspectively. We define the Learning curve difference as the following relative difference between these\ntwo curves:\n$\\Delta(l_1, l_2) = \\frac{1}{N}\\sum_{n=0}^{N-1} \\frac{|l_1(n)-l_2(n)|}{l_1(n) + l_2(n)},$ where $N = min(N_1, N_2)$ (5)\nHowever, this metric may indicate a significant difference between two learning curves even when\ntheir trends are similar but contain noise. To address this, we propose a smoothed metric:\n$\\Delta_S(l_1, l_2) = \\frac{1}{N} \\sum_{n=0}^{S-1} \\frac{|S(l_1)(n) - S(l_2)(n)|}{S(l_1)(n) + S(l_2)(n)}$ (6)\nHere, $S$ is a smoothing operator, which could be a exponential moving average [18], Savitzky-Golay\nfilter [19], or similar.\nThe Learning curve difference as defined above will approach 0 if the two curves are nearly identical\nand approach 1 if they are completely decoupled. However, since the entire learning curve can vary\ndepending on the task and dataset, it's more informative to make relative comparisons within the same\ntask and dataset to determine whether a particular scheduler exhibits more or less learning curve de-\ncoupling than others."}, {"title": "2.4. Hyperbolic Curves and Their Properties", "content": "Hyperbolic curves are a family of curves defined by the equation:\n$\\frac{x^2}{a^2} - \\frac{y^2}{b^2} = 1$ (7)\nwhere $a$ and $b$ are positive constants. These curves have two branches, each of which is symmetric\nabout the x-axis and approaches the lines $y = \\pm(\\frac{b}{a})x$ as asymptotes [11]. For the purpose of learning\nrate scheduling, we will focus on the branch where $x \\leq -a$ and $y \\geq 0$, which can be represented by\nthe equation:\n$y = \\frac{b}{a}\\sqrt{x^2 - a^2}$ (8)\nOne of the key properties of hyperbolic curves is that the slope of the curve converges to its asymp-\ntotes as the distance from the vertex increases. This property can be demonstrated by examining the\nderivative of the equation:\n$\\frac{dy}{dx} = \\frac{b}{a} \\frac{x}{\\sqrt{x^2 - a^2}}$ (9)\nTaking the limit as x approaches negative infinity, we find:\n$\\lim_{x\\rightarrow\\infty} \\frac{dy}{dx} = \\lim_{x\\rightarrow\\infty} \\frac{b}{a} \\frac{x}{\\sqrt{x^2 \u2014 a^2}} = \\frac{b}{a}$ (10)\nThis result indicates that when $x < -a$, the slope of the curve $dy/dx$ approaches a constant value\nof $-b/a$. In other words, the rate of change of the curve becomes stable and predictable as we move\nfurther away from the vertex. This convergence property of hyperbolic curves makes them a promis-\ning foundation for developing learning rate schedulers that maintain a consistent initial learning rate\nchange rate, regardless of the total number of epochs. By harnessing this property, it is possible to\ncreate a more robust and flexible approach to learning rate scheduling, enabling deep learning models\nto achieve optimal performance with minimal hyperparameter tuning."}, {"title": "3. Proposed Learning Rate Schedulers", "content": ""}, {"title": "3.1. HyperbolicLR", "content": "We introduce HyperbolicLR, a novel learning rate scheduler based on the hyperbolic curve. The learn-\ning rate at epoch $n$ is defined as:\n$f_h(n; \\eta_{init}, \\eta_{inf}, N, U) = \\eta_{init} + (\\eta_{init} \u2014 \\eta_{inf})(h(n; N, U) \u2013 h(0; N, U))$ (11)\nwhere $\\eta_{init}$ denotes the initial learning rate, $\\eta_{inf}$ represents the infimum of the learning rate, $N$ is the\ntotal number of epochs minus one, and $U$ is the upper bound of $N$. The function $h(n; N, U)$ is given by:\n$h(n; N, U) = \\sqrt{\\frac{N-n}{U}(2 - \\frac{N+n}{U})} (U \\geq N)$ (12)\nThis function represents a portion of a hyperbolic curve with vertex at $(N, 0)$, center at $(U, 0)$, and\nasymptote slope of $-1/U$, as illustrated in the Figure 2.\nThus, HyperbolicLR starts at $\\eta_{init}$ and decreases along the hyperbolic curve, ending at a learning rate\nthat is always greater than or equal to $\\eta_{inf}$ at the $N$-th epoch. (For proof, see Appendix B.) The variable\n$N$ can be adjusted within the range less than or equal to $U$. When $U$ and $N$ are equal, the curve coin-\ncides with the hyperbola's asymptote, resulting in a simple linear decay.\nA key property of HyperbolicLR is that the asymptote slope $-1/U$ does not depend on $N$, which\ncorresponds to the total number of epochs. Therefore, in the early stages of training when $n \\ll N$,\nthe change rate of the learning rate will be similar to the asymptote, meaning it won't change signif-\nicantly even if $N$ is altered. This characteristic suggests that HyperbolicLR could potentially address\nthe learning curve decoupling problem by maintaining a consistent learning pattern across varying\nepoch numbers."}, {"title": "3.2. ExpHyperbolicLR", "content": "Building upon the concept of HyperbolicLR, we propose ExpHyperbolicLR, which extends the hyper-\nbolic approach to the exponential space. The learning rate at epoch $n$ for ExpHyperbolicLR is defined\nas:\n$f_{EH} (n; \\eta_{init}, \\eta_{inf}, N, U) = \\eta_{init} \\times exp\\left(ln\\frac{\\eta_{init}}{\\eta_{inf}} \\times (h(n; N,U) \u2013 h(0; N, U))\\right)$ (13)\nwhere the hyperparameters are consistent with those in HyperbolicLR. Notably, ExpHyperbolicLR can\nbe expressed in terms of HyperbolicLR:\n$f_{eh}(n; \\eta_{init}, \\eta_{inf}, N, U) = exp(f_H(n; ln \\eta_{init}, ln \\eta_{inf}, N, U))$ (14)\nThe monotonically increasing nature of the exponential function ensures that ExpHyperbolicLR retains\nthe fundamental properties of HyperbolicLR. However, when $N = U$, it becomes a linear function in\nthe exponential space, equivalent to ExponentialLR rather than exhibiting linear decay.\nExpHyperbolicLR decreases the learning rate exponentially, making its initial rate of decrease faster\nthan HyperbolicLR. This could make it a better choice in environments where overfitting occurs easily."}, {"title": "3.3. Comparison with Existing Learning Rate Schedulers", "content": "Figure 3 illustrates the behavior of PolynomialLR, CosineAnnealingLR, and our proposed Hyperboli-\ncLR and ExpHyperbolicLR schedulers for different total epoch settings ($N = 250, 500, 750, 1000$). As\nevident from the top row, both PolynomialLR and CosineAnnealingLR exhibit significant variations\nin their learning rate decay patterns as the total number of epochs changes. In contrast, our proposed\nHyperbolicLR and ExpHyperbolicLR (bottom row) demonstrate more consistent decay patterns across\ndifferent epoch settings, particularly in the early stages of training."}, {"title": "To quantitatively measure the initial learning rate change, which we claimed to be less sensitive to\nchanges in $N$ for HyperbolicLR due to the properties of hyperbolic curves, we introduce a new metric\ncalled the Initial Learning Rate Integral (ILRI):", "content": ""}, {"title": "$ILRI = \\int_{0}^{no_{0.8}} (\\eta_n - 0.8 \\times \\eta_{init}) dn$", "content": " (15)\nwhere $n_n$ is the learning rate at epoch n, and $n_{0.8}$ is the epoch at which the learning rate reaches 80% of\nits initial value. Although epochs are typically integers and $n_n$ is a sequence rather than a continuous\nfunction, we use cubic Hermite splines to interpolate and treat epochs as real numbers for more precise\nmeasurements.\nTo compare how the initial rate of change in HyperbolicLR and ExpHyperbolicLR differs from existing\nschedulers, we measured the ILRI values for each scheduler with N set to 250, 500, 750, and 1000. Using\n$N=1000$ as a baseline, we calculated the relative ILRI percentage difference as follows:\n$\\Delta_{ILRI}(N; 1000) = \\frac{|ILRI_N \u2013 ILRI_{1000}|}{ILRI_{1000}} \u00d7 100(\\%)$ (16)\nFor PolynomialLR and CosineAnnealingLR, regardless of hyperparameter values, we can theoretically\nderive that this value is always $(1 \u2013 N/1000) \\times 100(\\%)$, which we also verified numerically. For Hy-\nperbolicLR and ExpHyperbolicLR, we fixed $U=1000$ and, after extensive experimentation, adopted\nhyperparameters that reduce the initial learning rate by three orders of magnitude, as these showed\nthe highest versatility.\nThe results are summarized in the Table 1."}, {"title": "4. Experimental Setup", "content": "This section details our experimental methodology, encompassing dataset preparation, model archi-\ntectures, and training procedures for both image classification and time series prediction tasks."}, {"title": "4.1. Datasets", "content": ""}, {"title": "4.1.1. CIFAR-10 for Image Classification", "content": "We employed the CIFAR-10 dataset [20] for our image classification experiments. This dataset com-\nprises 60, 000 number of 32 \u00d7 32 color images across 10 classes. We utilized 50,000 images for training\nand the remaining 10,000 for validation.\nData augmentation techniques applied to the training set included:\n1. Random cropping to 32 \u00d7 32 pixels with 4-pixel padding\n2. Random horizontal flipping\n3. Per-channel normalization (mean: 0.4914, 0.4822, 0.4465; standard deviation: 0.2023, 0.1994, 0.2010)\nThe validation set underwent only normalization."}, {"title": "4.1.2. Custom Oscillation Dataset for Time Series Prediction", "content": "For our time series prediction task, we constructed a custom dataset incorporating both simple and\ndamped harmonic oscillations. The dataset was generated using the following ordinary differential\nequation:\n$m\\ddot{u} + c\\dot{u} + ku = 0$ (17)\nwhere $m=1$, $k=200$, and $c = \\zeta\\cdot 2\\sqrt{mk}$. We varied the damping ratio $\\zeta$ with values 0, 0.01, and\n0.02 to simulate different oscillatory behaviors. To generate the data, we numerically solved the ODE\nusing the Newmark-beta method [21] over the interval $t \\in [0, 10]$ with a step size of $10^{-3}$. The initial\nconditions were set as $u(0) = 0.1$, $\\dot{u}(0) = 0$, and $\\ddot{u}(0) = -20$.\nWe employed a sliding window approach to prepare the data for the prediction task. For each $\\zeta$ value,\nwe generated 10,001 data points and applied the sliding window technique with a history of 100 time\nsteps and a prediction horizon of 20 time steps. The resulting sequences from all three $\\zeta$ values were\nthen combined into a single dataset.\nThis process yielded a total of 29,646 input-output pairs, with each input sequence comprising 100\nconsecutive time steps used to predict the subsequent 20 time steps. The final dataset had an input size\nof (29646, 100, 1) and a label size of (29646, 20, 1).\nTo ensure a random distribution of oscillation patterns across different damping ratios, we shuffled the\ncombined dataset before splitting it into training and validation sets. We used an 80-20 split, resulting\nin 23,716 samples for training and 5,930 samples for validation.\nPrior to training, we normalized the data to the range [0, 1] to ensure consistent scale across different\noscillation parameters. This custom dataset provides a controlled environment to evaluate the perfor-\nmance of various learning rate schedulers on a challenging time series prediction task with known\nunderlying dynamics, while also testing the model's ability to generalize across different damping\nratios."}, {"title": "4.1.3. Custom Integral Dataset for Operator Learning", "content": "For operator learning task, we aimed to learn an operator $G$ defined for continuous functions $u \\in\nC[0, 1]$ and arbitrary real numbers $y \\in [0, 1]$ as follows:\n$G(u)(y) = \\int_0^y u(x)dx$ (18)\nTo accomplish this, we employed the DeepONet approach [22]. Training a DeepONet requires two dis-\ntinct inputs: values of the input function $u$ at fixed sensor points $x_i$, denoted as $u(x_i)$, and domain\nvalues $y$ of the operator-mapped function $G(u)$. The corresponding labels are the values of $G(u)(y_i)$.\nFor the target points $y_i$, we uniformly partitioned the interval [0, 1] into 100 points. To generate inputs\ncorresponding to the input functions, we employed a Gaussian Random Field approach with a squared\nexponential kernel. To enhance the diversity of our dataset, we introduced variability in the length\nscale parameter $l$, uniformly sampling it from the range [0.1, 0.4]. This modification expands upon the\nfixed length scale typically used in similar studies [22], potentially enabling our model to generalize\nacross a broader spectrum of input functions.\nFor each randomly selected length scale parameter $l$, we uniformly divided the [0, 1] interval into 100\nsensor points. We then generated a Gaussian Random Field $X_i$ corresponding to each point $x_i$, con-\nsidering this as a discrete representation $u$ of a continuous function $u$:\n$u = [X_0, X_1, ..., X_{98}, X_{99}] = [u(0), u(x_1), ..., u(x_{98}), u(1)]$ (19)\nWe generated 10,000 such functions. To match the size, we replicated the vector $y$ of target points $y_i$\n10,000 times. Consequently, the input sizes are (10000, 100) for $u$ and (10000, 100) for $y$.\nTo optimize the learning process, we implemented a tailored normalization strategy for the generated\ninput vectors $u$. We applied a linear transformation such that $\\mu - \\sigma$ maps to -1 and $\\mu + \\sigma$ maps to 1,\nwhere $\\mu$ and $\\sigma$ represent the mean and standard deviation of the input data, respectively. This linear\nscaling ensures that approximately 68% of the data points fall within the [-1, 1] interval, while values\noutside this range are mapped proportionally. This approach preserves the inherent variability in the\ndata more effectively than standard min-max normalization, making it particularly suitable for data\ngenerated from a Gaussian Random Field."}, {"title": "4.2. Model Architectures", "content": "Figure 6 illustrates the architectures of our SimpleCNN and LSTM Sequence-to-Sequence models used\nfor CIFAR-10 classification and oscillation prediction tasks, respectively.\n4.2.1. SimpleCNN for CIFAR-10\nAs shown in the red solid box of Figure 6, our SimpleCNN architecture for CIFAR-10 classification con-\nsists of a series of convolutional layers followed by fully connected layers. Each convolutional block,\nrepeated $L_c$ times, comprises a convolutional layer, ReLU activation, and max pooling. This structure\nallows for hierarchical feature extraction from the input images.\nFollowing the convolutional blocks, the network includes $L_F + 1$ fully connected layers. The first $L_F$\nlayers are followed by ReLU activation, while the final layer produces the output logits for classifica-\ntion. This architecture balances feature extraction capabilities with model complexity, suitable for the\nCIFAR-10 dataset.\n4.2.2. LSTM Sequence-to-Sequence for Oscillation Prediction\nThe LSTM Sequence-to-Sequence model, depicted in the blue solid box of Figure 6, consists of an en-\ncoder (E) and a decoder (D), both utilizing LSTM layers. The encoder processes the input sequence\n$u(t_1), u(t_2), ..., u(t_i)$, representing historical oscillation data. The final hidden state $h$ of the encoder\nis used to initialize the decoder.\nThe decoder operates autoregressively, generating predictions $\\hat{u}(t_{i+1}), \\hat{u}(t_{i+2}), ..., \\hat{u}(t_{i+p})$ for future\ntime steps. It takes its own previous output as input for the next prediction, starting with an initial\ninput of 0. This design challenges the model to maintain long-term coherence in its predictions, crucial\nfor accurately forecasting oscillatory behavior.\n4.2.3. DeepONet for Operator Learning\nAs illustrated in the green dashed box of Figure 6, we employed the DeepONet architecture as proposed\nby [22] for our operator learning task. Both the branch and trunk networks consist of $L$ repeated layers\nof MLP followed by GELU activation, with a final MLP layer without activation. The outputs of these\nnetworks, $b = [b_1, b_2, ..., b_p]$ from the branch net and $t = [t_1, t_2, ..., t_p]$ from the trunk net (where $p$\nis a hyperparameter), are combined through an inner product to yield the operator output $\\hat{G}(u)(y)$.\n4.2.4. TraONet for Operator Learning\nTo enhance the efficiency of operator learning, we developed and utilized TraONet (Transformer Op-\nerator Network), a novel operator network that incorporates Transformer encoder and decoder models\nalongside the traditional DeepONet structure. As shown in the orange solid box of Figure 6, TraONet\ndraws inspiration from the Transformer architecture introduced in [2]. However, we adapted the struc-\nture for operator learning by removing the masking in the decoder's masked multi-head attention\ncomponent. Both the encoder and decoder parts of TraONet are repeated $L$ times to achieve the desired\ndepth of representation.\nThis novel approach combines the strengths of attention mechanisms with the proven effectiveness of\nDeepONet, potentially offering improved performance in capturing complex operator relationships.\nThe specific configurations for all models presented-SimpleCNN, LSTM Sequence-to-Sequence, Deep-\nONet, and TraONet-including the number of layers, hidden units, attention heads (for TraONet), and\nother architectural details, were determined through our comprehensive hyperparameter optimization\nprocess. This process ensures that each model is optimally tuned for its respective task, whether it be\nimage classification, time series prediction, or operator learning. We will elaborate on the hyperpara-\nmeter optimization methodology and present the resulting optimal configurations for each model in\nthe subsequent sections, providing a detailed view of our experimental setup and the rationale behind\nour architectural choices."}, {"title": "4.3. Experimental Design and Evaluation Protocol", "content": "The primary focus of our experimental design is to evaluate the performance of various learning rate\nschedulers across diverse tasks and model architectures, while simultaneously investigating whether\noptimizations performed at lower epoch counts can consistently maintain performance as the number\nof epochs increases. This approach addresses the learning curve decoupling problem, where scheduler\nperformance may diverge significantly with extended training durations. Given the extensive range of\nhyperparameters for both models and schedulers, we have devised a consistent experimental protocol\napplicable across all tasks.\nOur methodology comprises three distinct phases:\n1. Model Hyperparameter Optimization: We optimize model hyperparameters using a fixed scheduler.\n2. Scheduler Hyperparameter Optimization: Utilizing the optimal model hyperparameters obtained in\nthe previous phase, we then optimize the scheduler hyperparameters, again over a small epoch\ncount.\n3. Performance Evaluation Across Varying Epoch Settings: We assess each scheduler's performance us-\ning the optimized hyperparameters from the previous phases, progressively increasing the number\nof epochs and measuring validation loss or accuracy.\nTo ensure consistency and fairness, we employ the AdamW optimizer [23] throughout all experiments,\nutilizing $\u03b2_1=0.9$, $\u03b2_2=0.999$, $\u03b5=10^{-8}$ and $\u03bb=0.01$. Furthermore, to mitigate the impact of random\ninitialization, we conduct each training session five times using seeds obtained from random.org [24],\nreporting the mean performance metrics.\n4.3.1. Model Hyperparameter Optimization\nThe hyperparameters of our models primarily consist of discrete values, such as the number of layers\nand nodes. Consequently, we employed a grid search within predefined ranges to optimize these hy-\nperparameters. For consistency across tasks, we utilized the ExponentialLR scheduler, which has the\nfewest hyperparameters, fixing the decay factor at 0.9 for all experiments over 50 epochs. The initial\nlearning rate was task-specific: $10^{-2}$ for CIFAR-10 and the oscillation dataset, and $5 \u00d7 10^{-3}$ for the\nintegral dataset. Table 2 presents the grid search ranges for each model architecture along with the\noptimal values determined through our comprehensive search."}, {"title": "4.3.2. Scheduler Hyperparameter Optimization", "content": "Following the optimization of model hyperparameters, we focused on optimizing the scheduler hyper-\nparameters. Unlike the discrete model parameters, scheduler hyperparameters are primarily continu-\nous values (e.g., learning rate, power, decay factor, infimum learning rate and etc.). To effectively opti-\nmize these continuous parameters, we employed the Tree-structured Parzen Estimator (TPE), a variant\nof Bayesian optimization [25], [26].\nOur primary objective was to assess whether optimizations performed at lower epoch counts could\nconsistently maintain performance as the number of epochs increased. Therefore, we fixed the number\nof epochs at 50 for this optimization phase. For each task and scheduler combination, we conducted\n25 trials using TPE, selecting the optimal values from these trials.\nThe initial learning rate optimization range was consistent across all schedulers, spanning from\n$10^{\u22124}$ to $5 \u00d7 10^{\u22122}$ on a logarithmic scale. Other hyperparameters were optimized within scheduler-\nspecific ranges. The detailed optimization ranges for each scheduler's hyperparameters are presented\nin Table 3."}, {"title": "4.3.3. Performance Evaluation", "content": "To assess the efficacy of our optimized scheduler configurations across varying training durations, we\nconducted a series of experiments using the optimal hyperparameters presented in Table 4. For each\ntask, we progressively increased the number of epochs from 50 to 200 in increments of 50, measuring\nthe validation loss for all models and accuracy for the SimpleCNN model on the CIFAR-10 task.\nWe evaluated the relative performance improvement per 50-epoch interval, calculating the mean and\nstandard deviation to determine which scheduler demonstrated the most consistent and substantial\nperformance gains."}, {"title": "5. Results and Analysis", "content": ""}, {"title": "5.1. Overall Performance", "content": "Figure 7 presents the performance of all schedulers across the four model architectures studied. The\nresults demonstrate that, in nearly all cases, the use of learning rate schedulers leads to improved per-\nformance compared to using no scheduler. This underscores the importance of learning rate scheduling\nin optimizing deep learning models.\nFor the image classification task using SimpleCNN, CosineAnnealingLR initially showed the highest\naccuracy at 50 epochs. However, as the number of epochs increased, PolynomialLR consistently out-\nperformed other schedulers. Notably, at 200 epochs, HyperbolicLR exhibited a sharp performance in-\ncrease, nearly matching PolynomialLR's accuracy. ExpHyperbolicLR showed comparable performance\nto CosineAnnealingLR at 200 epochs, while ExponentialLR consistently underperformed relative to\nother schedulers.\nThe LSTM Seq2Seq model for time series prediction demonstrated a stark contrast between scheduled\nand non-scheduled learning rates. At 200 epochs, the validation loss difference was approximately 470-\nfold in favor of scheduled approaches. PolynomialLR and CosineAnnealingLR initially showed strong\nperformance but exhibited instability, with performance decreases at 150 epochs. In contrast, Hyper-\nbolicLR and ExpHyperbolicLR displayed steady improvements across all epochs, with HyperbolicLR\nshowing the most pronounced performance gains, ultimately matching PolynomialLR's performance\nat 200 epochs.\nThe operator learning task using DeepONet revealed an interesting phenomenon where non-exponen-\ntial decay models (PolynomialLR, CosineAnnealingLR, and HyperbolicLR) experienced significant loss\ndivergence after 100 epochs. This unexpected behavior warrants further investigation and may indicate\na need for careful scheduler selection in certain operator learning contexts. Conversely, ExponentialLR\nand ExpHyperbolicLR demonstrated consistent performance improvements, with ExpHyperbolicLR\nachieving the best performance from 150 epochs onward.\nInterestingly, the TraONet model, also applied to the operator learning task but utilizing a transformer-\nbased architecture, showed robust performance across all schedulers. HyperbolicLR emerged as the\ntop performer after 100 epochs, with a particularly significant margin at 200 epochs. ExpHyperbolicLR\nconsistently ranked second in"}, {"title": "5.2. Consistency of Improvement", "content": "Table 5 presents the mean (\u03bc) and standard deviation (\u03c3) of the percentage increase in validation per-\nformance (loss for most models", "as": "n$\\frac{L_{N_i} - L_{"}]}]}