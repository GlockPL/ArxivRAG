{"title": "HyperbolicLR: Epoch insensitive learning rate scheduler", "authors": ["Tae-Geun Kim"], "abstract": "This study proposes two novel learning rate schedulers: the Hyperbolic Learning Rate Scheduler (HyperbolicLR) and the Exponential Hyperbolic Learning Rate Scheduler (ExpHyperbolicLR). These schedulers attempt to address the inconsistent learning curves often observed in conventional schedulers when adjusting the number of epochs. By leveraging the asymptotic behavior of hyperbolic curves, the proposed schedulers maintain more consistent learning curves across varying epoch settings. The HyperbolicLR algorithm directly applies this property to the epoch-learning rate space, while the ExpHyperbolicLR maps this concept onto the exponential space of epochs and learning rates. To evaluate the performance of these schedulers, first we found the optimal hyperparameters for each scheduler on a small number of epochs, fixed these values, and compared their performance as the number of epochs increased. Our experimental results on various deep learning tasks and architectures demonstrate that both HyperbolicLR and ExpHyperbolicLR maintain more consistent performance improvements compared to conventional schedulers as the number of epochs increases. These findings suggest that our hyperbolic-based learning rate schedulers offer a more robust and efficient approach to training deep neural networks, especially in scenarios where computational resources or time constraints limit extensive hyperparameter searches.", "sections": [{"title": "1. Introduction", "content": "Recent steady progression of deep learning has prompted researchers to actively apply these techniques across diverse fields \u2013 such as computer vision, natural language processing, biomedical engineering, and even high energy physics. While there are several factors contributing to the success of deep learning, including hardware development and the creation of effective model architectures, the advancement of optimization techniques is also a crucial factor. In the optimization process, choosing the appropriate optimizer and searching an optimal learning rate are certainly important, yet learning rate scheduling is now regarded as a almost essential process. Learning rate scheduling is the process of appropriately changing the learning rate during training to enable more efficient learning. Various learning rate schedulers have been developed and widely used in deep learning research and applications. These diverse learning rate schedulers assist deep learning researchers by enabling more efficient optimization, but they also add another layer of complexity by requiring the exploration of not only the model's hyperparameters but also the scheduler's hyperparameters. This additional hyperparameter optimization becomes increasingly time-consuming and costly, especially for large neural network models, making deep learning research more complex."}, {"title": "2. Related Work and Background", "content": "Moreover, many existing learning rate schedulers exhibit sensitivity to the number of epochs. This sensitivity can create significant differences in learning curves between small and large numbers of epochs, leading to what we term the \"learning curve decoupling problem.\" The practical implication of this problem is significant: hyperparameters optimized for a small number of epochs may not yield optimized performance for a large number of epochs. This discrepancy can lead to suboptimal model performance and increased complexity in the hyperparameter optimization process, especially when scaling up training to larger epochs.\nTo address this epoch sensitivity issue, we focused on hyperbolic curves. Unlike polynomial or trigonometric functions, hyperbolic curves have the unique property of converging to an asymptote as they move away from the vertex [11]. We expected that by utilizing this property, we could obtain a learning rate scheduler with higher flexibility, allowing small changes while not significantly altering the learning rate change pattern even as the number of epochs increases.\nThe main contributions of this paper are as follows:\n\u2022 We propose HyperbolicLR and ExpHyperbolicLR, two novel learning rate schedulers based on hyperbolic curves that maintain a consistent initial learning rate change, regardless of the number of epochs.\n\u2022 We provide detailed mathematical formulations and analyses of the proposed schedulers, highlighting their properties and advantages over existing methods.\n\u2022 We conduct extensive experiments on various deep learning architectures and datasets to evaluate the performance of HyperbolicLR and ExpHyperbolicLR, comparing them to widely used learning rate schedulers.\n\u2022 We make our code publicly available to facilitate further research and reproducibility.\nThe rest of the paper is organized as follows. Section 2 reviews related work on learning rate scheduling and provides background information on hyperbolic curves. Section 3 presents the proposed HyperbolicLR and ExpHyperbolicLR schedulers, along with their mathematical formulations and properties. Section 4 describes the experimental setup, datasets, and architectures used for evaluation. Section 5 presents and discusses the experimental results, comparing the performance of the proposed schedulers to existing methods. Finally, Section 6 concludes the paper and outlines future research directions."}, {"title": "2.1. Learning Rate Scheduling in Deep Learning", "content": "The learning rate is one of the most crucial hyperparameters in training deep learning models. If the learning rate is too high, the model may diverge or oscillate around the optimal point. Conversely, if the learning rate is too low, the model might get stuck in a local minimum or take an excessively long time to converge. Therefore, finding or appropriately adjusting the learning rate significantly impacts the model's performance and convergence speed.\nLearning rate scheduling is a technique that dynamically adjusts the learning rate during the training process. Generally, it starts with a high learning rate and gradually decreases it to address issues such as getting stuck in local minima or overshooting the optimal point. However, there are also methods like cyclic learning rate scheduling [9], which periodically changes the learning rate, or warm restart [10], which linearly restores the learning rate before continuing the decay process.\nThe significance of learning rate scheduling is underscored by the following considerations [12]:"}, {"title": "2.2. Commonly Used Learning Rate Schedulers", "content": "Let No be the set of non-negative integers and P be the set of specific hyperparameters required for learning rate scheduling. A learning rate scheduler can be expressed as a function f mapping from No \u00d7 P to [0,\u221e) as follows:\n$$nn = f(n; P)$$\nHere, n \u2208 No represents the current epoch, P \u2208 P is a tuple of hyperparameters and nn is the learning rate at the current epoch. Using this notation, we will describe some of the most widely used learning rate schedulers.\n(a) Polynomial learning rate scheduler (PolynomialLR) [13], [14]: PolynomialLR decays the learning rate from the initial value using a polynomial function.\n$$f(n; Ninit, N, p) = Ninit \u00d7 (1 - \\frac{n}{N})^p$$\nwhere Ninit is the initial learning late, N is the total number of epochs, and p is the exponent of the polynomial function.\n(b) Cosine annealing learning rate scheduler (CosineAnnealingLR) [10]: CosineAnnealingLR decays the learning rate from an initial value Nmax to a minimum learning rate min using a cosine curve.\n$$f(n; Nmin, Nmax, N) = \\frac{1}{2} nmin + \\frac{1}{2} (Nmax - min) (1 + cos(\\frac{n}{N}))$$\nAs with PolynomialLR, N is the total number of epochs.\n(c) Exponential learning rate scheduler (ExponentialLR) [15]: ExponentialLR decays the learning rate exponentially with a constant decay rate \u5165. Unlike the two schedulers explained earlier, this scheduler does not depend on the total number of epochs.\n$$f(n; Ninit, Y) = Ninit \u00d7 yn$$\nwhere Ninit is the initial learning rate, and y is the exponential decay factor.\nThese learning rate schedulers are widely used in training deep learning models as they effectively improve model performance [16], [17]. However, each of these learning rate schedulers has its own set of problems. In this study, we will particularly focus on the learning curve decoupling problem, a common issue shared by schedulers that depend on the total number of epochs, such as PolynomialLR and CosineAnnealingLR."}, {"title": "2.3. The Learning Curve Decoupling Problem", "content": "This section introduces a potential issue with learning rate schedulers that we have termed the \"Learning curve decoupling problem.\u201d This problem has not been explicitly addressed in previous literature and was observed in the behavior of well-known learning rate schedulers when training durations were altered. We define the Learning curve decoupling problem as follows: For a given learning rate scheduler, when only the total number of training epochs is changed while keeping other hyperparameters fixed, the learning rate change pattern significantly differs. This leads to a separation of the two learning rate curves and, consequently, inconsistent optimization behavior, increasing the complexity of hyperparameter optimization.\nTo formalize this, we propose a metric for the Learning curve difference. Consider a learning rate scheduler f(n; N, P), where we fix other hyperparameters P and change the total number of epochs from N\u2081 to N2. Let 11 and 12 be the loss curves obtained from training with N\u2081 and N2 epochs, respectively. We define the Learning curve difference as the following relative difference between these two curves:\n$$\u0394(1,2) = \\frac{1}{N}\\sum_{n=0}^{N-1} \\frac{|l_1(n)-l_2(n)|}{l_1(n) + l_2(n)},  where N = min(N_1, N_2)$$\nHowever, this metric may indicate a significant difference between two learning curves even when their trends are similar but contain noise. To address this, we propose a smoothed metric:\n$$\u0394_S(l_1, l_2) = \\frac{1}{N}\\sum_{n=0}^{S-1} \\frac{|S(l_1)(n) - S(l_2)(n)|}{S(l_1)(n) + S(l_2)(n)}$$\nHere, S is a smoothing operator, which could be a exponential moving average [18], Savitzky-Golay filter [19], or similar.\nThe Learning curve difference as defined above will approach 0 if the two curves are nearly identical and approach 1 if they are completely decoupled. However, since the entire learning curve can vary depending on the task and dataset, it's more informative to make relative comparisons within the same task and dataset to determine whether a particular scheduler exhibits more or less learning curve decoupling than others."}, {"title": "2.4. Hyperbolic Curves and Their Properties", "content": "Hyperbolic curves are a family of curves defined by the equation:\n$$\\frac{x^2}{a^2} - \\frac{y^2}{b^2} = 1$$\nwhere a and b are positive constants. These curves have two branches, each of which is symmetric about the x-axis and approaches the lines y = \u00b1(\\frac{b}{a})x as asymptotes [11]. For the purpose of learning rate scheduling, we will focus on the branch where x \u2264 \u2212a and y \u2265 0, which can be represented by the equation:\n$$y= \\frac{b}{a} \\sqrt{x^2-a^2}$$\nOne of the key properties of hyperbolic curves is that the slope of the curve converges to its asymptotes as the distance from the vertex increases. This property can be demonstrated by examining the derivative of the equation:\n$$\\frac{dy}{dx} = \\frac{b}{a} \\frac{x}{\\sqrt{x^2 - a^2}}$$\nTaking the limit as x approaches negative infinity, we find:\n$$lim_{x\u2192\u221e} \\frac{dy}{dx} = lim_{x\u2192\u221e} \\frac{b}{a} \\frac{x}{\\sqrt{x^2 \u2014 \u0430^2}} = \\frac{b}{a}$$\nThis result indicates that when x < \u2212a, the slope of the curve dy/dx approaches a constant value of -b/a. In other words, the rate of change of the curve becomes stable and predictable as we move further away from the vertex. This convergence property of hyperbolic curves makes them a promising foundation for developing learning rate schedulers that maintain a consistent initial learning rate change rate, regardless of the total number of epochs. By harnessing this property, it is possible to create a more robust and flexible approach to learning rate scheduling, enabling deep learning models to achieve optimal performance with minimal hyperparameter tuning."}, {"title": "3. Proposed Learning Rate Schedulers", "content": ""}, {"title": "3.1. HyperbolicLR", "content": "We introduce HyperbolicLR, a novel learning rate scheduler based on the hyperbolic curve. The learning rate at epoch n is defined as:\n$$fh(n; Ninit, Ninf, N,U) = Ninit + (Ninit \u2014 Ninf)(h(n; N, U) \u2013 h(0; N, U))$$\nwhere Ninit denotes the initial learning rate, ninf represents the infimum of the learning rate, N is the total number of epochs minus one, and U is the upper bound of N. The function h(n; N, U) is given by:\n$$h(n; N,U) = \\sqrt{\\frac{N-n}{U} (2 - \\frac{N+n}{U})} (U \u2265 N)$$\nThis function represents a portion of a hyperbolic curve with vertex at (N, 0), center at (U, 0), and asymptote slope of -1/U, as illustrated in the Figure 2.\nThus, HyperbolicLR starts at Ninit and decreases along the hyperbolic curve, ending at a learning rate that is always greater than or equal to ninf at the N-th epoch. (For proof, see Appendix B.) The variable N can be adjusted within the range less than or equal to U. When U and N are equal, the curve coincides with the hyperbola's asymptote, resulting in a simple linear decay.\nA key property of HyperbolicLR is that the asymptote slope -1/U does not depend on N, which corresponds to the total number of epochs. Therefore, in the early stages of training when n \u00ab N, the change rate of the learning rate will be similar to the asymptote, meaning it won't change significantly even if N is altered. This characteristic suggests that HyperbolicLR could potentially address the learning curve decoupling problem by maintaining a consistent learning pattern across varying epoch numbers."}, {"title": "3.2. ExpHyperbolicLR", "content": "Building upon the concept of HyperbolicLR, we propose ExpHyperbolicLR, which extends the hyperbolic approach to the exponential space. The learning rate at epoch n for ExpHyperbolicLR is defined as:\n$$fEH (n; Ninit, Ninf, N, U) = Ninit X exp (\\frac{ln \\frac{Ninit}{Ninf}}{} \u00d7 (h(n; N,U) \u2013 h(0; N, U)))$$"}, {"title": "3.3. Comparison with Existing Learning Rate Schedulers", "content": "Figure 3 illustrates the behavior of PolynomialLR, CosineAnnealingLR, and our proposed HyperbolicLR and ExpHyperbolicLR schedulers for different total epoch settings (N = 250, 500, 750, 1000). As evident from the top row, both PolynomialLR and CosineAnnealingLR exhibit significant variations in their learning rate decay patterns as the total number of epochs changes. In contrast, our proposed HyperbolicLR and ExpHyperbolicLR (bottom row) demonstrate more consistent decay patterns across different epoch settings, particularly in the early stages of training."}, {"title": "4. Experimental Setup", "content": "This section details our experimental methodology, encompassing dataset preparation, model architectures, and training procedures for both image classification and time series prediction tasks."}, {"title": "4.1. Datasets", "content": ""}, {"title": "4.1.1. CIFAR-10 for Image Classification", "content": "We employed the CIFAR-10 dataset [20] for our image classification experiments. This dataset comprises 60, 000 number of 32 \u00d7 32 color images across 10 classes. We utilized 50,000 images for training and the remaining 10,000 for validation.\nData augmentation techniques applied to the training set included:\n1. Random cropping to 32 \u00d7 32 pixels with 4-pixel padding\n2. Random horizontal flipping\n3. Per-channel normalization (mean: 0.4914, 0.4822, 0.4465; standard deviation: 0.2023, 0.1994, 0.2010)\nThe validation set underwent only normalization."}, {"title": "4.1.2. Custom Oscillation Dataset for Time Series Prediction", "content": "For our time series prediction task, we constructed a custom dataset incorporating both simple and damped harmonic oscillations. The dataset was generated using the following ordinary differential equation:\n$$m\u00fc + c\u1ee7 + ku = 0$$\nwhere m = 1, k = 200, and c = \u03da\u00b7 2\u221amk. We varied the damping ratio ( with values 0, 0.01, and 0.02 to simulate different oscillatory behaviors. To generate the data, we numerically solved the ODE using the Newmark-beta method [21] over the interval t \u2208 [0, 10] with a step size of 10\u22123. The initial conditions were set as u(0) = 0.1, \u00f9(0) = 0, and \u00fc(0) = -20."}, {"title": "4.1.3. Custom Integral Dataset for Operator Learning", "content": "For operator learning task, we aimed to learn an operator G defined for continuous functions u \u2208 C[0, 1] and arbitrary real numbers y \u2208 [0, 1] as follows:\n$$G(u)(y) = \\int_{0}^{y} u(x)dx$$\nTo accomplish this, we employed the DeepONet approach [22]. Training a DeepONet requires two distinct inputs: values of the input function u at fixed sensor points xi, denoted as u(xi), and domain values y of the operator-mapped function G(u). The corresponding labels are the values of G(u)(yz).\nFor the target points y\u2081, we uniformly partitioned the interval [0, 1] into 100 points. To generate inputs corresponding to the input functions, we employed a Gaussian Random Field approach with a squared exponential kernel. To enhance the diversity of our dataset, we introduced variability in the length scale parameter l, uniformly sampling it from the range [0.1, 0.4]. This modification expands upon the fixed length scale typically used in similar studies [22], potentially enabling our model to generalize across a broader spectrum of input functions.\nFor each randomly selected length scale parameter l, we uniformly divided the [0, 1] interval into 100 sensor points. We then generated a Gaussian Random Field X\u2081 corresponding to each point x, considering this as a discrete representation u of a continuous function u:\n$$u = [X_0, X_1, ..., X_{98}, X_{99}] = [u(0), u(x_1), ..., u(x_{98}), u(1)]$$\nWe generated 10,000 such functions. To match the size, we replicated the vector y of target points Yi 10,000 times. Consequently, the input sizes are (10000, 100) for u and (10000, 100) for y.\nTo optimize the learning process, we implemented a tailored normalization strategy for the generated input vectors u. We applied a linear transformation such that \u03bc \u03c3maps to -1 and \u03bc + \u03c3 maps to 1, where \u03bc and o represent the mean and standard deviation of the input data, respectively. This linear scaling ensures that approximately 68% of the data points fall within the [-1, 1] interval, while values outside this range are mapped proportionally. This approach preserves the inherent variability in the data more effectively than standard min-max normalization, making it particularly suitable for data generated from a Gaussian Random Field."}, {"title": "4.2. Model Architectures", "content": "Figure 6 illustrates the architectures of our SimpleCNN and LSTM Sequence-to-Sequence models used for CIFAR-10 classification and oscillation prediction tasks, respectively."}, {"title": "4.2.1. SimpleCNN for CIFAR-10", "content": "As shown in the red solid box of Figure 6, our SimpleCNN architecture for CIFAR-10 classification consists of a series of convolutional layers followed by fully connected layers. Each convolutional block, repeated Le times, comprises a convolutional layer, ReLU activation, and max pooling. This structure allows for hierarchical feature extraction from the input images.\nFollowing the convolutional blocks, the network includes LF + 1 fully connected layers. The first LF layers are followed by ReLU activation, while the final layer produces the output logits for classification. This architecture balances feature extraction capabilities with model complexity, suitable for the CIFAR-10 dataset."}, {"title": "4.2.2. LSTM Sequence-to-Sequence for Oscillation Prediction", "content": "The LSTM Sequence-to-Sequence model, depicted in the blue solid box of Figure 6, consists of an encoder (E) and a decoder (D), both utilizing LSTM layers. The encoder processes the input sequence u(t\u2081), u(t\u2082), ..., u(t\u2081), representing historical oscillation data. The final hidden state h of the encoder is used to initialize the decoder.\nThe decoder operates autoregressively, generating predictions \u00fb(ti+1), \u00fb(ti+2), ..., \u00fb(ti+p) for future time steps. It takes its own previous output as input for the next prediction, starting with an initial input of 0. This design challenges the model to maintain long-term coherence in its predictions, crucial for accurately forecasting oscillatory behavior."}, {"title": "4.2.3. DeepONet for Operator Learning", "content": "As illustrated in the green dashed box of Figure 6, we employed the DeepONet architecture as proposed by [22] for our operator learning task. Both the branch and trunk networks consist of L repeated layers of MLP followed by GELU activation, with a final MLP layer without activation. The outputs of these networks, b = [b1, b2, ..., b] from the branch net and t = [t1, t2, ..., tp] from the trunk net (where p is a hyperparameter), are combined through an inner product to yield the operator output \u011c(u)(y)."}, {"title": "4.2.4. TraONet for Operator Learning", "content": "To enhance the efficiency of operator learning, we developed and utilized TraONet (Transformer Operator Network), a novel operator network that incorporates Transformer encoder and decoder models alongside the traditional DeepONet structure. As shown in the orange solid box of Figure 6, TraONet draws inspiration from the Transformer architecture introduced in [2]. However, we adapted the structure for operator learning by removing the masking in the decoder's masked multi-head attention component. Both the encoder and decoder parts of TraONet are repeated L times to achieve the desired depth of representation.\nThis novel approach combines the strengths of attention mechanisms with the proven effectiveness of DeepONet, potentially offering improved performance in capturing complex operator relationships.\nThe specific configurations for all models presented-SimpleCNN, LSTM Sequence-to-Sequence, Deep-ONet, and TraONet-including the number of layers, hidden units, attention heads (for TraONet), and other architectural details, were determined through our comprehensive hyperparameter optimization process. This process ensures that each model is optimally tuned for its respective task, whether it be image classification, time series prediction, or operator learning. We will elaborate on the hyperparameter optimization methodology and present the resulting optimal configurations for each model in the subsequent sections, providing a detailed view of our experimental setup and the rationale behind our architectural choices."}, {"title": "4.3. Experimental Design and Evaluation Protocol", "content": "The primary focus of our experimental design is to evaluate the performance of various learning rate schedulers across diverse tasks and model architectures, while simultaneously investigating whether optimizations performed at lower epoch counts can consistently maintain performance as the number of epochs increases. This approach addresses the learning curve decoupling problem, where scheduler performance may diverge significantly with extended training durations. Given the extensive range of hyperparameters for both models and schedulers, we have devised a consistent experimental protocol applicable across all tasks.\nOur methodology comprises three distinct phases:\n1. Model Hyperparameter Optimization: We optimize model hyperparameters using a fixed scheduler.\n2. Scheduler Hyperparameter Optimization: Utilizing the optimal model hyperparameters obtained in the previous phase, we then optimize the scheduler hyperparameters, again over a small epoch count.\n3. Performance Evaluation Across Varying Epoch Settings: We assess each scheduler's performance using the optimized hyperparameters from the previous phases, progressively increasing the number of epochs and measuring validation loss or accuracy.\nTo ensure consistency and fairness, we employ the AdamW optimizer [23] throughout all experiments, utilizing \u03b2\u2081 = 0.9, \u03b2\u2082 = 0.999, \u03b5 = 10\u22128 and X = 0.01. Furthermore, to mitigate the impact of random initialization, we conduct each training session five times using seeds obtained from random.org [24], reporting the mean performance metrics."}, {"title": "4.3.1. Model Hyperparameter Optimization", "content": "The hyperparameters of our models primarily consist of discrete values, such as the number of layers and nodes. Consequently, we employed a grid search within predefined ranges to optimize these hyperparameters. For consistency across tasks, we utilized the ExponentialLR scheduler, which has the fewest hyperparameters, fixing the decay factor at 0.9 for all experiments over 50 epochs. The initial learning rate was task-specific: 10-2 for CIFAR-10 and the oscillation dataset, and 5 \u00d7 10\u00af\u00b3 for the integral dataset. Table 2 presents the grid search ranges for each model architecture along with the optimal values determined through our comprehensive search."}, {"title": "4.3.2. Scheduler Hyperparameter Optimization", "content": "Following the optimization of model hyperparameters, we focused on optimizing the scheduler hyperparameters. Unlike the discrete model parameters, scheduler hyperparameters are primarily continuous values (e.g., learning rate, power, decay factor, infimum learning rate and etc.). To effectively optimize these continuous parameters, we employed the Tree-structured Parzen Estimator (TPE), a variant of Bayesian optimization [25], [26].\nOur primary objective was to assess whether optimizations performed at lower epoch counts could consistently maintain performance as the number of epochs increased. Therefore, we fixed the number of epochs at 50 for this optimization phase. For each task and scheduler combination, we conducted 25 trials using TPE, selecting the optimal values from these trials.\nThe initial learning rate optimization range was consistent across all schedulers, spanning from 10\u22124 to 5 \u00d7 10-2 on a logarithmic scale. Other hyperparameters were optimized within scheduler-specific ranges. The detailed optimization ranges for each scheduler's hyperparameters are presented in Table 3."}, {"title": "4.3.3. Performance Evaluation", "content": "To assess the efficacy of our optimized scheduler configurations across varying training durations, we conducted a series of experiments using the optimal hyperparameters presented in Table 4. For each task, we progressively increased the number of epochs from 50 to 200 in increments of 50, measuring the validation loss for all models and accuracy for the SimpleCNN model on the CIFAR-10 task.\nWe evaluated the relative performance improvement per 50-epoch interval, calculating the mean and standard deviation to determine which scheduler demonstrated the most consistent and substantial performance gains."}, {"title": "5. Results and Analysis", "content": ""}, {"title": "5.1. Overall Performance", "content": "Figure 7 presents the performance of all schedulers across the four model architectures studied. The results demonstrate that, in nearly all cases, the use of learning rate schedulers leads to improved performance compared to using no scheduler. This underscores the importance of learning rate scheduling in optimizing deep learning models.\nFor the image classification task using SimpleCNN, CosineAnnealingLR initially showed the highest accuracy at 50 epochs. However, as the number of epochs increased, PolynomialLR consistently outperformed other schedulers. Notably, at 200 epochs, HyperbolicLR exhibited a sharp performance increase, nearly matching PolynomialLR's accuracy. ExpHyperbolicLR showed comparable performance to CosineAnnealingLR at 200 epochs, while ExponentialLR consistently underperformed relative to other schedulers.\nThe LSTM Seq2Seq model for time series prediction demonstrated a stark contrast between scheduled and non-scheduled learning rates. At 200 epochs, the validation loss difference was approximately 470-fold in favor of scheduled approaches. PolynomialLR and CosineAnnealingLR initially showed strong performance but exhibited instability, with performance decreases at 150 epochs. In contrast, HyperbolicLR and ExpHyperbolicLR displayed steady improvements across all epochs, with HyperbolicLR showing the most pronounced performance gains, ultimately matching PolynomialLR's performance at 200 epochs.\nThe operator learning task using DeepONet revealed an interesting phenomenon where non-exponential decay models (PolynomialLR, CosineAnnealingLR, and HyperbolicLR) experienced significant loss divergence after 100 epochs. This unexpected behavior warrants further investigation and may indicate a need for careful scheduler selection in certain operator learning contexts. Conversely, ExponentialLR and ExpHyperbolicLR demonstrated consistent performance improvements, with ExpHyperbolicLR achieving the best performance from 150 epochs onward.\nInterestingly, the TraONet model, also applied to the operator learning task but utilizing a transformer-based architecture, showed robust performance across all schedulers. HyperbolicLR emerged as the top performer after 100 epochs, with a particularly significant margin at 200 epochs. ExpHyperbolicLR consistently ranked second in performance, except at 100 epochs. These results suggest that transformer-based architectures may be more resilient to scheduler choice, though still benefiting from optimized scheduling strategies.\nIt is worth noting that ExponentialLR, while initially performing adequately, showed rapidly diminishing improvements as epochs increased across tasks. This limitation was most evident in the TraONet experiment, where its performance at 200 epochs (1.1541 \u00d7 10\u22125 validation loss) fell below that of using no scheduler (1.0926 \u00d7 10\u22125). This behavior likely stems from ExponentialLR's fixed decay rate, which may reduce the learning rate too aggressively in later epochs. These observations highlight the potential limitations of fixed-rate exponential decay in extended training scenarios."}, {"title": "5.2. Consistency of Improvement", "content": "Table 5 presents the mean (\u03bc) and standard deviation (\u03c3) of the percentage increase in validation performance (loss for most models, accuracy for SimpleCNN) per 50-epoch interval as the total number of epochs increases from 50 to 200. The percentage increase is calculated as:\n$$\\frac{LN_i - LN_{i+1}}{LN_i} \u00d7 100(%) $$\nwhere N\u2081 = 50 \u00d7 i for 1 \u2264 i \u2264 4, and LN represents the validation metric (loss or accuracy) at Ni epochs.\nFor the SimpleCNN model on the CIFAR-10 dataset, HyperbolicLR demonstrated the highest mean improvement while maintaining the lowest standard deviation, indicating both superior performance enhancement and consistency. PolynomialLR showed the second-highest mean improvement but exhibited considerably higher variability. ExpHyperbolicLR matched HyperbolicLR's consistency but with a lower mean improvement. It is worth noting, however, that the overall magnitudes of improvement and standard deviations were relatively small across all schedulers for this task. This observation suggests that the impact of learning rate scheduling, while still beneficial, was less pronounced for the SimpleCNN model on CIFAR-10 compared to other model-task combinations in our study.\nIn the case of LSTM Sequence-to-Sequence modeling for oscillation prediction, HyperbolicLR and ExpHyperbolicLR exhibited the most substantial performance improvements. ExponentialLR displayed remarkably consistent performance enhancement. Notably, PolynomialLR and the no-scheduler case showed negative mean improvements, indicating instability in the learning process. ExponentialLR, HyperbolicLR, and ExpHyperbolicLR all demonstrated significantly more consistent performance improvements compared to PolynomialLR and CosineAnnealingLR.\nThe DeepONet results for operator learning require careful interpretation due to the previously mentioned loss divergence issues during training. PolynomialLR, CosineAnnealingLR, and HyperbolicLR yielded results that cannot be considered representative of successful training. Despite these challenges, ExpHyperbolicLR and ExponentialLR both demonstrated consistent performance improvements, with ExpHyperbolicLR achieving the highest performance gains.\nFor the TraONet architecture, also applied to operator learning, HyperbolicLR and ExpHyperbolicLR significantly outperformed other schedulers in terms of both magnitude and consistency of improvement. Interestingly, ExponentialLR underperformed even the no-scheduler baseline in this context.\nIn summary, HyperbolicLR and ExpHyperbolicLR demonstrated the most consistent performance improvements across almost all scenarios. HyperbolicLR, in particular, showed the highest performance gains in all cases except for DeepONet. PolynomialLR and CosineAnnealingLR frequently exhibited less consistent improvements than even the no-scheduler baseline. While ExponentialLR showed good consistency, its magnitude of improvement was generally lower than that of other schedulers."}, {"title": "5.3. Regression Analysis", "content": "Table 6 presents the results of power regression analysis (y = exp(A)xB) performed on the validation metrics across epochs for each scheduler. The DeepONet results were excluded from this analysis due to the previously discussed training instability issues, which resulted in statistically insignificant regressions (p > 0.2) for all schedulers except ExponentialLR and ExpHyperbolicLR.\nAcross all models, HyperbolicLR and ExpHyperbolicLR consistently demonstrated the highest R\u00b2 values and lowest p-values, indicating superior goodness of fit and statistical significance. This suggests that these schedulers maintain the most consistent and predictable performance improvements as training progresses.\nHyperbolicLR exhibited the highest absolute B values (power coefficient) across all tasks, indicating the steepest rate of improvement over epochs. This aligns with our earlier observations of HyperbolicLR's strong performance gains, particularly in later epochs.\nExponentialLR generally showed statistically significant regressions. However, its B values were consistently lower than those of HyperbolicLR and ExpHyperbolicLR, suggesting a slower rate of improvement over time.\nPolynomialLR and CosineAnnealingLR displayed variable performance across tasks. While they showed strong statistical significance for SimpleCNN and TraONet (p < 0.05, R\u00b2 > 0.95), their performance on the LSTM Seq2Seq task was notably poor (R\u00b2 < 0.85, p > 0.05). This inconsistency suggests that these schedulers may be more sensitive to task-specific characteristics.\nThe no-scheduler baseline showed statistically significant regression only for the SimpleCNN model. For LSTM Seq2Seq and TraONet, the regressions were not statistically significant, indicating that the performance improvements without scheduling were not reliably predictable."}, {"title": "5.4. Learning Curve Decoupling Analysis", "content": "Table 7: Average Smoothed Learning Curve Differences (\u22065) for each scheduler and model combination. Lower values indicate less decoupling between learning curves of different epoch settings.\nTo quantify the learning curve decoupling problem, we calculated the average Smoothed Learning Curve Differences for each scheduler and model combination, as presented in Table 7. These values were derived by averaging the pairwise differences between learning curves of 50, 100, 150, and 200 epochs. A lower value indicates greater consistency in learning curves across different epoch settings, suggesting less decoupling and potentially more robust performance as training duration increases. It's worth noting that No-scheduler and ExponentialLR are not included in this analysis as they inherently maintain consistent learning curves across different epoch settings, resulting in zero learning curve differences.\nThe results demonstrate the effectiveness of our proposed hyperbolic-based learning rate schedulers in mitigating the learning curve decoupling problem across diverse deep learning tasks and architectures. Among the adaptive schedulers, ExpHyperbolicLR consistently exhibited the lowest average Smoothed"}, {"title": "5.5. Summary of Findings", "content": "Our comprehensive evaluation of learning rate schedulers across various deep learning tasks and architectures has yielded several important findings:\n1. Performance: HyperbolicLR and ExpHyperbolicLR demonstrated consistently good performance across most scenarios. While PolynomialLR showed superior final performance in CNN and LSTM models", "Improvement": "Our proposed schedulers exhibited more consistent performance improvements across increasing epoch numbers. HyperbolicLR", "Predictability": "Regression"}]}