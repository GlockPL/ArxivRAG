{"title": "The Mathematics of Artificial Intelligence", "authors": ["Gabriel Peyr\u00e9"], "abstract": "This overview article highlights the critical role of mathematics in artificial intelligence (AI), emphasizing that mathematics provides tools to better understand and enhance AI systems. Conversely, AI raises new problems and drives the development of new mathematics at the intersection of various fields. This article focuses on the application of analytical and probabilistic tools to model neural network architectures and better understand their optimization. Statistical questions (particularly the generalization capacity of these networks) are intentionally set aside, though they are of crucial importance. We also shed light on the evolution of ideas that have enabled significant advances in AI through architectures tailored to specific tasks, each echoing distinct mathematical techniques. The goal is to encourage more mathematicians to take an interest in and contribute to this exciting field.", "sections": [{"title": "Supervised Learning", "content": "Recent advancements in artificial intelligence have mainly stemmed from the development of neural networks, particularly deep networks. The first significant successes, after 2010, came from supervised learning, where training pairs (x\u00b2, y\u00b2); are provided, with x representing data (e.g., images or text) and y\u00b2 corresponding labels (typically, classes describing the content of the data). More recently, spectacular progress has been achieved in unsupervised learning, where labels y; are unavailable, thanks to techniques known as generative AI. These methods will be discussed in Section 4.\nEmpirical Risk Minimization. In supervised learning, the objective is to construct a function $f_\\theta(x)$, dependent on parameters $\\theta$, such that it approximates the data well: $y^i \\approx f_\\theta(x^i)$. The dominant paradigm involves finding the parameters $\\theta$ by minimizing an empirical risk function, defined as:\n$\\min E(\\theta) := \\sum_i l(f_\\theta(x^i), y^i)$,\nwhere $l$ is a loss function, typically $l(y, y') = ||y \u2013 y'||^2$ for vector-valued $(y^i)$;. Optimization of $E(\\theta)$ is performed using gradient descent:\n$\\theta_{t+1} = \\theta_t - \\tau\\nabla E(\\theta_t)$,\nwhere is the step size. In practice, a variant known as stochastic gradient descent [23] is used to handle large datasets by randomly sampling a subset at each step t. A comprehensive theory for the convergence of this method exists in cases where fe depends linearly on 0, as E(0) is convex. However, in the case of deep neural networks, E(0) is non-convex, making its theoretical analysis challenging and still largely unresolved. In some instances, partial mathematical analyses provide insights into the observed practical successes and guide necessary modifications to improve convergence. For a detailed overview of existing results, refer to [2].\nAutomatic Differentiation. Computing the gradient \u2207E(0) is fundamental. Finite difference methods or traditional differentiation of composite functions would be too costly, with complexity on the order of O(DT), where D is the dimensionality of 0 and T is the computation time of fo. The advances in deep network optimization rely on the use of automatic differentiation in reverse mode [13], often referred to as \u201cbackpropagation of the gradient,\" with complexity on the order of O(T)."}, {"title": "Two-Layer Neural Networks", "content": "Multi-Layer Perceptrons. A multi-layer network, or multi-layer perceptron (MLP) [25], computes the function $f_\\theta(x) = x_L$ in L steps (or layers) recursively starting from $x_0 = x$:\n$x_{l+1} = \\sigma(W_lx_l + b_l)$,\nwhere $W_l$ is a weight matrix, $b_l$ a bias vector, and $\u03c3$ a non-linear activation function, such as the sigmoid $\u03c3(s) = \\frac{1}{1+e^{-s}}$, which is bounded, or ReLU (Rectified Linear Unit), $\u03c3(s) = \\max(s, 0)$, which is unbounded. The parameters to optimize are the weights and biases $\u03b8 = (W_l, b_l)l$. If \u03c3 were linear, then fe would remain linear regardless of the number of layers. The non-linearity of $\u03c3$ enriches the set of functions representable by fo, by increasing both the width (dimension of intermediate vectors xk) and the depth (number L of layers).\nTwo-Layer Perceptrons and Universality. The mathematically best-understood case is that of L = 2 layers. Denoting $W_1 = (u_k)_{k=1}^n$ and $W_2 = (v_k)_{k=1}^n$ as the n rows and columns of the two matrices (where n is the network width), we can write fo as a sum of contributions from its n neurons:\n$f_\\theta(x) = \\frac{1}{n} \\sum_{k=1}^n v_k \\sigma(\\langle u_k, x \\rangle + b_k)$.\nThe parameters are $\u03b8 = (u_k, v_k, b_k)_{k=1}^n$. Here, we added a normalization factor 1/n (to later study the case n \u2192 +\u221e) and ignored the non-linearity of the second layer. A classical result by Cybenko [8] shows that these functions fo can uniformly approximate any continuous function on a compact domain. This result is similar to the Weierstrass theorem for polynomial approximation, except that (4) defines, for a fixed n, a non-linear function space. The elegant proof by Hornik [15] relies on the Stone-Weierstrass theorem, which implies the result for $\u03c3 = cos$ (since (4) defines an algebra of functions when n is arbitrary). The proof is then completed by uniformly approximating a cosine in 1-D using a sum of sigmoids or piecewise affine functions (e.g., for $\u03c3$ = ReLU)."}, {"title": "Mean-Field Representation", "content": "This result is, however, disappointing, as it does not specify how many neurons are needed to achieve a given approximation error. This is impossible without adding assumptions about the regularity of the function to approximate. Barron's fundamental result [4] introduces a Banach space of regular functions, defined by the semi-norm $||f||_B := \\int |\\hat{f}(w)||w|dw$, where $\\hat{f}$ denotes the Fourier transform of f. Barron shows that, in the L2 norm, for any n, there exists a network fo with n neurons such that the approximation error on a compact \u03a9 of radius R is of the order\n$|| f - f_\\theta||_{L^2(\\Omega)} = O(\\frac{R||f||_B}{\\sqrt{n}})$. This result is remarkable because it avoids the \"curse of dimensionality\": unlike polynomial approximation, the error does not grow exponentially with the dimension d (although the dimension affects the constant $||f||_B$).\nBarron's proof relies on a \u201cmean-field\u201d generalization of (4), where a distribution (a probability measure) \u03c1 is considered over the parameter space (u, v, b), and the network is expressed as:\n$F(x) := \\int v \\sigma(\\langle u, x \\rangle + b) d\\rho(u, v, b)$.\nA finite-size network (4), $f_\\theta = F_{\\hat{\\rho}}$, is obtained with a discrete measure $\\rho = \\frac{1}{n} \\sum_{k=1}^n \\delta_{(u_k,v_k,b_k)}$. An advantage of the representation (5) is that F depends linearly on p, which is crucial for Barron's proof. This proof uses a probabilistic Monte Carlo-like method (where the error decreases as $1/\\sqrt{n}$ as desired): it involves constructing a distribution p from f, then sampling a discrete measure p\u02c6 whose parameters (uk, Uk, bk)k are distributed according to \u03c1."}, {"title": "Wasserstein Gradient Flow", "content": "In general, analyzing the convergence of the optimization (2) is challenging because the function E is non-convex. Recent analyses have shown that when the number of neurons n is large, the dynamics are not trapped in a local minimum. The fundamental result by Chizat and Bach [7] is based on the fact that the distribution pt defined by gradient descent (2), as t\u2081 \u2192 0, follows a gradient flow in the space of probability distributions equipped with the optimal transport distance. These Wasserstein gradient flows, introduced by [16] and studied extensively in the book [1], satisfy a McKean-Vlasov-type partial differential equation:\n$\\partial_t \\rho_t + div(\\rho_t V(\\rho)) = 0$,\nwhere $x \u2192 V(\u03c1)(x)$ is a vector field depending on \u03c1, which can be computed explicitly from the data (x\u00b2, y\u00b2). The result by Chizat and Bach can be viewed both as a PDE result (convergence to an equilibrium of a class of PDEs with a specific vector field V(p)) and as a machine learning result (successful training of a two-layer network via gradient descent when the number of neurons is sufficiently large)."}, {"title": "Very Deep Networks", "content": "The unprecedented recent success of neural networks began with the work of [17], which demonstrated that deep networks, when trained on large datasets, achieve unmatched performance. The first key point is that to achieve these performances, it is necessary to use weight matrices Wk that exploit the structure of the data. For images, this means using convolutions [18]. However, this approach is not sufficient for extremely deep networks, with the number of layers L reaching into the hundreds."}, {"title": "Residual Networks", "content": "The major breakthrough that empirically demonstrated that network performance increases with L was the introduction of residual connections, known as ResNet [14]. The main idea is to ensure that, for most layers xe, the dimensions of xe and xe+1 are identical, and to replace (3) with L steps:\n$x_{l+1} = x_l + \\frac{1}{L} U_l^T \\sigma (V_lx_l + b_l)$,\nwhere $U_l, V_l \u2208 R^{n\u00d7d}$ are weight matrices, with n being the number of neurons per layer (which, as in (4), can be increased to enlarge the function space). The intuition for the success of (7) is that this formula allows, unlike (3), for steps that are small deformations near identity mappings. This makes the network $f_\\theta(x_0) = x_L$ obtained after L steps well-posed even when L is large, and it can be rigorously proven [20] that this well-posedness is preserved during optimization via gradient descent (2)."}, {"title": "Neural Differential Equation", "content": "As Lapproaches +\u221e, (7) can be interpreted as a discretization of an ordinary differential equation:\n$\\frac{dx_s}{ds} = U \\sigma (V_s x_s + b_s)$,\nwhere $s \u2208 [0,1]$ indexes the network depth. The network fo(xo) := X1 maps the initialization Xxs=0 to the solution Xs=1 of (8) at times 1. The parameters are 0 = (Us, Vs, bs) s\u2208[0,1]. This formalization, referred to as a neural ODE, was initially introduced in [6] to leverage tools from adjoint equation theory to reduce the memory cost of computing the gradient \u2207E(0) during backpropagation. It also establishes a connection to control theory, as training via gradient descent (2) computes an optimal control e that interpolates between the data x\u00b2 and labels y\u00b2. However, the specificity of learning the- ory compared to control theory lies in the goal of computing such control using gradient descent (2). To date, no detailed results exist on this. Nonetheless, in his thesis, Raphael Barboni [3] demonstrated that if the network is initialized near an interpolating network, gradient descent converges to it."}, {"title": "Generative AI for Vector Data", "content": "Self-Supervised Pre-Training. The remarkable success of large generative AI models for vector data, such as images (often referred to as \u201cdiffusion models\"), and for text (large language models or LLMs), relies on the use of new large-scale network architectures and the creation of new training tasks known as \u201cself-supervised\u201d. Manually defining the labels y' through human intervention is too costly, so these are calculated automatically by solving simple tasks. For images, these involve denoising tasks, while for text, they involve predicting the next word. A key advantage of these simple tasks (known as pre-training tasks) is that it becomes possible to use a pre-trained network fo generatively. Starting from an image composed of pure noise and iterating on the network, one can randomly generate a realistic image [27]. Similarly, for text, starting from a prompt and sequentially predicting the next word, it is possible to generate text, for example, to answer questions [5]. We will first describe the case of vector data generation, such as images, and address LLMs for text in the following section.\""}, {"title": "Sampling as an Optimization Problem", "content": "Generative AI models aim to generate (or \"sample\") random vectors x according to a distribution \u03b2, which is learned from a large training dataset (x\u00b2). This distribution is obtained by \u201cpushing forward\" a reference distribution a (most commonly an isotropic Gaussian distribution a = N(0, Id)) through a neural network fo. Specifically, if X ~ a is distributed according to a, then fo(X) ~ \u03b2 follows the law \u00df, which is denoted as $(f_\\theta)#a = \u03b2$, where # represents the pushforward operator (also known as the image measure in probability theory).\nEarly approaches to generative AI, such as \u201cGenerative Adversarial Networks\u201d (GANs) [12], attempted to directly optimize a distance D between probability distributions (e.g., an optimal transport distance [22]):\n$\\min D((f_\\theta)#a, \u03b2)$.\nThis problem is challenging to optimize because computing D is expensive, and \u1e9e must be approximated from the data (x\u00b2)i."}, {"title": "Flow-Based Generation", "content": "Recent successes, particularly in the generation of vector data, involve neural networks fo that are computed iteratively by integrating a flow [21], similar to a neural differential equation (8):\n$f_\\theta(x_0) := x_1 \\text{ where } \\frac{dx_s}{ds} = g_\\theta(x_s, s)$,\nwhere $g_\\theta$: (x,s) \u2208 Rd \u00d7 R \u2192 Rd. The input space includes an additional temporal dimension s, compared to fo. The most effective neural networks for images are notably U-Nets [24].\nThe central mathematical question is how to replace (9) with a simpler optimization problem when fe is computed by integrating a flow (10). An extremely elegant solution was first proposed in the context of diffusion models [27] (corresponding to the specific case a = N(0, Id)) and later generalized under the name \u201cflow matching\" [19].\nThe main idea is that if xo ~ a is initialized randomly, then xs, the solution at time s of (10), follows a distribution as that interpolates between a = a and a1, which is desired to match \u03b2. This distribution satisfies a conservation equation:\n$\\partial_t a_s + div(a_s v_s) = 0$,\nwhere the vector field is defined as $v_s(x) := g_\\theta(x, s)$. This equation is similar to the evolution of neuron distributions during optimization (6), but it is simpler because vs does not depend on the distribution as, making the equation linear in as."}, {"title": "Denoising Pre-Training", "content": "The question is how to find a vector field $v_s(x) = g_\\theta(x, s)$ such that a\u2081 = \u03b2, i.e., the final distribution matches the desired one. If vs is known, the distribution as is uniquely determined. The key idea is to reason in reverse: starting from an interpolation as satisfying a = a and a1 = \u03b2, how can we compute a vector field vs such that (11) holds? There are infinitely many possible solutions (since vs can be modified by a conservative field), but for certain specific interpolations as, remarkably simple expressions can be derived.\nOne example is the interpolation obtained via barycentric averaging: take a pair ~ a and X1 ~ \u03b2, and define as as the distribution of (1 \u2212 s)xo + SX1. It can be shown [19] that an admissible vector field is given by a simple conditional expectation:\n$v_s(x) = E_{x_0\\sim a, x_1 \\sim \\beta}(x_1 \u2013 x_0 | (1 \u2013 s)x_0 + s x_1 = x)$.\nThe key advantage of this formula is that the conditional mean corresponds to a linear regression, which can be approximated using a neural network $v_s \\approx g_\\theta(\\cdot, s)$. The expectation over X1 ~ \u03b2 can be replaced by a sum over training data (x\u00b2), leading to the following optimization problem:\n$\\min E(\\theta) := \\int_0^1 E_{x_0\\sim a} \\sum_i ||x_0 \u2013 g_\\theta((1 \u2013 s)x_0 + sx^i, s)||^2 ds$.\nThis function E(0) is an empirical risk function (1), and similar optimization techniques are used to find an optimal \u03b8. In particular, stochastic gradient descent efficiently handles both the integral fo and the expectation Exo~a.\nThe problem (12) corresponds to an unsupervised pre-training task: there are no labels y', but an artificial supervision task is created by adding random noise xo to the data x\u00b2. This task is called denoising. We will now see that, for textual data, a different pre-training task is used."}, {"title": "Generative AI for Text", "content": "Tokenization and Next-Word Prediction. Generative AI methods for text differ from those used for vector data generation. The neural network architectures are different (they involve transformers, as we will describe), and the pre-training method is based not on denoising but on next-word prediction [28]. It is worth noting that these transformer neural networks are now also used for image generation [9], but specific aspects related to the causal nature of text remain crucial. The first preliminary step, called \u201ctokenization\u201d, consists of transforming the input text into a sequence of vectors $X = (x[1], . . ., x[P])$, where the number P is variable (and may increase, for instance, when generating text from a prompt). Each token x[p] is a vector that encodes a group of letters, generally at the level of a syllable. The neural network x = fo(X) is applied to all the tokens and predicts a new token x. During training, a large corpus of text (X\u00b2); is available. If we denote by X the text Xi with the last token x\u00b2 removed, we minimize an empirical risk for prediction:\n$\\min E(\\theta) := \\sum_i l(f_\\theta(\\hat{X^i}), x^i)$,\nwhich is exactly similar to (1). When using a pre-trained network fe for text generation, one starts with a prompt X and iteratively adds a new token in an auto-regressive manner: $X \\leftarrow [X, f_\\theta(X)]$.\nTransformers and Attention. The large networks used for text generation tasks are Transformer networks [10]. Unlike ResNet (7), these networks fe no longer operate on a single vector x, but on a set of vectors $X = (x[1], . . ., x[P])$ of size P. In Transformers, the ResNet layer (7), operating on a single vector, is replaced by an attention layer, where all tokens interact through a barycentric combination of vectors (Vx[q]), where V is a matrix:\n$A_w(X)_p := \\sum_q M_{p,q}(V_x[q]), \\text{ where } M_{p,q} := \\frac{e^{\\langle Qx[p], Kx[q] \\rangle}}{\\sum_l e^{\\langle Qx[p], Kx[l] \\rangle}} \\text{ with } w := (Q, K, V)$.\nThe coefficients Mp,q are computed by normalized correlation between x[p] and x[q], depending on two parameter matrices Q and K. A Transformer fo(X) is then defined similarly to ResNet (7), iterating L attention layers with residual connections:\n$X_{l+1} = X_l + \\frac{1}{L} A_{w_l} (X_l)$.\nThe parameters of the Transformer $f_\\theta(X_0) = X_L$ with L layers are $\u03b8 = (\u03c9_l = (Q_l, K_l, V_l))_{l=1}^L$.\nThis description is simplified: in practice, a Transformer network also integrates nor-malization layers and MLPs operating independently on each token. For text applications, attention must be causal, imposing Mi,j = 0 for j > i. This constraint is essential for recursively generating text and ensuring the next-word prediction task is meaningful."}, {"title": "Mean-Field Representation of Attention", "content": "The attention mechanism (13) can be viewed as a system of interacting particles, and (14) describes the evolution of tokens across depth. As with ResNet (8), in the limit L \u2192 +\u221e, one can consider a system of coupled ordinary differential equations:\n$\\frac{dX_s}{ds} = A_w(X_s)$.\nA crucial point is that for non-causal Transformers, the system $X = (x_s[p])_p$ is invariant under permutations of the indices. Thus, this system can be represented as a probability distribution $\u03bc_s := \\frac{1}{p} \\sum_p \\delta_{x_s [p]}$ over the token space. This perspective was adopted in Michael Sander's thesis [26], which rewrites attention as:\n$A_w(x) := \\frac{\\int e^{\\langle Qx, Kx' \\rangle} V x' d\\mu(x')}{\\int e^{\\langle Qx, Kx' \\rangle} d \\mu (x')}$ where $\u03c9 := (Q, K, V)$.\nThe particle system (15) then becomes a conservation equation for advection by the vector field Aw\u03c2 (\u03bc):\n$\\partial_t \\mu_s + div (\\mu_s A_w( \\mu_s)) = 0$.\nSurprisingly, this yields a McKean-Vlasov-type equation, similar to the one describing the training of two-layer MLPs (6), but with the velocity field Aw(\u03bc)(x) replacing V(\u03c1)(u, v,b). However, here the evolution occurs in the token space x rather than in the neuron space (u,v,b), and the evolution variable corresponds to depths, not the optimization time t.\nUnlike (6), the evolution (16) is not a gradient flow in the Wasserstein metric [26]. Nonetheless, in certain cases, this evolution can be analyzed, and it can be shown that the measure \u03bcs converges to a single Dirac mass [11] as s \u2192 +\u221e: the tokens tend to cluster. Better understanding these evolutions, as well as the optimization of parameters 0 = (Qs, Ks, Vs)s\u2208[0,1] via gradient descent (2), remains an open problem. This problem can be viewed as a control problem for the PDE (16)."}, {"title": "Conclusion", "content": "Mathematics plays a critical role in understanding and improving the performance of deep architectures while presenting new theoretical challenges. The emergence of Trans-formers and generative AI raises immense mathematical problems, particularly for better understanding the training of these networks and exploring the structure of \"optimal\" networks. One essential question remains: whether an LLM merely interpolates train-ing data or is capable of genuine reasoning. Moreover, issues of resource efficiency and privacy in AI system development demand significant theoretical advancements, where mathematics will play a pivotal role. Whether designing resource-efficient models, en-suring compliance with ethical standards, or exploring the fundamental limits of these systems, mathematics is poised to be an indispensable tool for the future of artificial intelligence."}]}