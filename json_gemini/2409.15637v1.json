{"title": "Synatra: Turning Indirect Knowledge into Direct Demonstrations for Digital Agents at Scale", "authors": ["Tianyue Ou", "Frank F. Xu", "Aman Madaan", "Jiarui Liu", "Robert Lo", "Abishek Sridhar", "Sudipta Sengupta", "Dan Roth", "Graham Neubig", "Shuyan Zhou"], "abstract": "LLMs can now act as autonomous agents that interact with digital environments and complete specific objectives (e.g., arranging an online meeting). However, accuracy is still far from satisfactory, partly due to a lack of large-scale, direct demonstrations for digital tasks. Obtaining supervised data from humans is costly, and automatic data collection through exploration or reinforcement learning relies on complex environmental and content setup, resulting in datasets that lack comprehensive coverage of various scenarios. On the other hand, there is abundant knowledge that may indirectly assist task completion, such as online tutorials that were created for human consumption. In this work, we present Synatra, an approach that effectively transforms this indirect knowledge into direct supervision at scale. We define different types of indirect knowledge, and carefully study the available sources to obtain it, methods to encode the structure of direct demonstrations, and finally methods to transform indirect knowledge into direct demonstrations. We use 100k such synthetically-created demonstrations to finetune a 7B CodeLlama, and demonstrate that the resulting agent surpasses all comparably sized models on three web-based task benchmarks Mind2Web, MiniWoB++ and WebArena, as well as surpassing GPT-3.5 on WebArena and Mind2Web. In addition, while synthetic demonstrations prove to be only 3% the cost of human demonstrations (at $0.031 each), we show that the synthetic demonstrations can be more effective than an identical number of human demonstrations collected from limited domains.", "sections": [{"title": "1 Introduction", "content": "AI agents that operate within a digital environment (e.g., a browser in a computer) intelligently to accomplish complex tasks (e.g., \u201cCreate my July online shopping expense report.\") have the potential to improve the productivity across a broad swath of tasks performed by humans every day [2, 33, 6, 58]. However, agents still lack the ability to complete tasks with a high degree of reliability, partly due to a paucity of training data for such agentic tasks. Typically, supervised finetuning is a standard way to adapt large language models (LLMs) to tasks such as text generation or classification, large-scale demonstration collections for digital agents are not readily available.\nFor AI agents, demonstrations typically involve specifying a sequence of actions and observations that results in successful task completion, as shown on the right of Figure 1. Existing works that automatically collect demonstrations (1) set up environments for an agent to interact with, (2) run a baseline agent within this environment, and (3) employ scoring functions to remove low quality demonstrations [9] or perform relabeling [1, 26]. All three of these requirements limit applicability to a variety of practical applications. Setting up an environment that is representative of the actual"}, {"title": "2 Problem Formulation", "content": "2.1\nControlling Digital Agents through Natural Language\nAn agent interacts with a computer environment E = (S, A, O, T) with state space S, action space A, observation space O and the environment dynamics T : S \u00d7 A \u2192 S. While this framework can be applied to different types of task, in this work, we consider a web browser as the unified entry point to access different web applications. We follow WebArena [58] to define the observation space as the contents on screen, represented as in text-based accessibility tree format. We use the same universal action space as in WebArena. This action space resembles the keyboard and mouse operations of a computer (e.g., click, type), and is applicable to arbitrary web applications. Appendix A lists all valid actions. The environment dynamics (e.g., the effect of clicking a button) and the states (e.g., the database status) are decided by the implementation of each web application.\nGiven the natural language intent i, at each time step t, an agent issues an action at \u2208 A based on St. The environment state is updated to st+1 with new observation Ot+1. This process ends when the agent predicts the stop action. We follow existing works [6, 47, 58, 54] to represent st with current observation and all previous actions in the form of (i, a1, ..., at\u22121, 0t). A benchmark (e.g., WebArena) supplies a scoring function r(i, sn) that examines the final state and returns 1 if the desired goal state is satisfied, and 0 otherwise."}, {"title": "2.2 Definition of Direct Demonstrations and Indirect Knowledge", "content": "We consider the expected action at given st as a form of direct demonstration, i.e., (st, at). This allows an agent to directly learn how to predict the next action under a given state. On the other hand, indirect knowledge is broadly defined as resources that can benefit the task execution, but is not in the format of state and expected action tuple. We mainly focus on three types of indirect knowledge:\n1. Procedural knowledge details the sequence of steps (a1, a2,..., am) required to complete a specific task i. Unlike an action at in trajectories, the steps in procedural knowledge are ungrounded, they lack a direct association with any particular observation and are not tied to specific action spaces. For instance, the tutorial in Figure 1 instructs the user to \"login with your credentials\u201d without providing the concrete PayPal login page and the input fields to type.\n2. Environment knowledge T that describe the effects of applying different actions in some hypothetical states. Example knowledge includes verbal descriptions such as \u201c... after clicking the cancel button, you will see a pop up window ...\u201c.\n3. Ungrounded observations o that are not associated with particular tasks or trajectories. In the context of web-based tasks, an observation could be any random web page with different content and status (e.g., a product page with a query in the search field)."}, {"title": "3 Scalable Demonstration Synthesis for Digital Agents", "content": "In this section, we first introduce our design choices on the canonical formalization of trajectories, which account for the structural nature of procedures. Then, we delve into the sources for acquiring indirect knowledge, and the mechanisms for re-purposing this knowledge into direct supervision."}, {"title": "3.1 Trajectories as Programs", "content": "Existing works demonstrate that representing task-related procedures as programs is beneficial for several reasons. This includes benefits from the structural nature of programs compared to free-form text [56, 23], and the flexibility of using tools [4, 10, 41]. Inspired by these observations, we represent a Python function that interleaves natural language planning articulated in comments and actions as API calls, as shown on the right. The pink background represents the prompt, while the blue background corresponds to the model's response format.\nThe planning process includes both task-level planning, which breaks the task into multiple sub-tasks, and action-level planning, which explains the low-level goals of each executable action. The model's generation consists of chain-of-thought (CoT, [43, 47]) reasoning that analyzes the objective, previous actions, and current observations. Since CoT reasoning includes detailed information about the current step that may not be relevant for future steps, we further design the model response to include an action summary. This summary serves as a description of the predicted action that is added"}, {"title": "3.2 Synthesizing from Text Procedural Knowledge with Generative Environment", "content": "The Internet offers fairly extensive procedural knowledge that describes how to perform high-level tasks by breaking down the task into detailed lower-level steps, such as how-tos and tutorials.\nSource We use wikiHow\u00b3 as our main source for these tutorials due to its comprehensive coverage of diverse tasks as well as its consistent format. Each article consists of a high-level task description and step-by-step instructions. We performed a filtering step and only kept the articles that involve navigation through the graphical user interface (GUI) of a computer or a mobile phone. We prompted GPT-3.5-turbo with six examples mixing the target articles (e.g., How to redeem an Amazon gift card online\u2074) and non-target articles (e.g., How to make a pizza) to perform the classification of all wikiHow articles. The prompt is shown in Appendix C. As a result, we obtained 25k articles that can be used to perform data synthesis.\nSynthesis Approach We want to bridge two gaps to re-purpose (a1, a2,..., a'n) for task i into (a1, ..., at-1, Ot). When re-purposing a sequence of actions (a1, a2, ..., a'n) for task i into a new sequence (a1, ..., at \u2212 1, ot), many challenges arise. First, the action descriptions provided in tutorials are not constrained to specific action spaces. Instead, they are presented as free-form natural language (NL) expressions, which can lead to ambiguity. For instance, various verbs such as \u201center,\" \"input,\u201d and others may all correspond to the same underlying action, type. Second, NL descriptions are often abstract, omitting concrete actions. For example, the process of \u201clogging in\" involves a series of actions, including typing in a username and password, but these specific actions may not be explicitly mentioned. Finally, the steps outlined in tutorials are ungrounded, meaning they are not directly associated with observable states or outcomes. Tutorials typically employ generic descriptions to accommodate various instances of conceptually similar tasks. For example, as illustrated in Figure 1, the tutorial merely instructs to \"enter the keyword\u201d without addressing any specific scenario.\nBased on these findings, we propose an iterative approach that first uses an LLM to rewrite an article into a hypothetical trajectory in the format shown in \u00a73.1, then we leverage a generative model to synthesize the intermediate observation between two consecutive actions. First, in the rewriting step, we ask the assistant LM to perform: (1) propose a hypothetical concrete scenario relevant to the task (2) perform basic parsing such as translating \"enter the keyword [...]\" into type (\"search bar\u201d, \u201cAmazon Prime\u2019); (3) categorize actions into groups that reflect the sub-task structures outlined by coding blocks. These tasks mainly demand a LLMs's creativity, language processing ability, and event understanding respectively. An example of rewriting a how-to article into a trajectory in program format is showed in Appendix E. The detailed prompt for the rewriting step is shown in Appendix D.\nBecause the previous procedures still only result in a sequence of ungrounded actions, we next leverage the assistant LM to generate the observations between randomly sampled consecutive actions. We use the consecutive actions of type (\"search bar\u201d, \u201cAmazon Prime", "Inc": "id=156) in Figure 1 as an example. There are mainly two requirements for generated observations. First, the observation must reflect the outcomes of past actions. In the example, this corresponds to a page with a user logged in, and a search input field filled with \"Amazon Prime\". Second, the observation encodes the necessary elements to perform the next action. In the example, this corresponds to a payment history list with a payment to Amazon. We prompt the assistant LM with the action sequence to generate a HTML snippet that fulfills the above requirements. Since the next action requires the concrete element to interact with, we ask the model to insert a tag of id=\"next-action-target-element\u201d in the corresponding HTML node to indicate the grounding.5 This step mainly requires a model's coding capabilities, particularly in web development. However, we find that it is not necessary for the LLM to generate HTML with high fidelity and"}, {"title": "3.3 Synthesizing from Random Observations", "content": "While the procedure in the previous section results in real procedures, the LLM-based method generates simplified observations. To compensate for this, we also perform data synthesis with real observations, and use synthesis to generate simplified procedures. We show that these two sources can compensate each other by examining the generated data in \u00a74 and comparing the actual web-based task performance in \u00a76.3.\nSource We utilize ClueWeb [27] as our data source, which comprises HTML snapshots of more than 10 billion web pages. Our initial analysis indicates that a random sampling approach would likely lead to a homogeneous distribution dominated by less interactive pages, such as news articles. In contrast, more complex web-based tasks typically require interactions with various web elements to advance the tasks. To diversify the sampled web pages, we employed a temperature sampling approach to select pages based on their content categories. In general, web pages from higher frequency top-level domains in ClueWeb are typically more interactive, such as Amazon and Reddit, while domains with lower frequency are less interactive, such as news article. We use a temperature sampling with T = 0.6 so that the sample probability of choosing a page in domain i, Pi = \\frac{p_i^k}{\\sum_{j=1}^{k} p_j^k}, where pi is the original probability of choosing a page in domain i, and k is all the available domains. In doing so, we up-sample more interactive sites while maintaining diversity on more rare sites. More details are listed in Appendix J.\nSynthesis Approach We treat each sampled web page as an intermediate observation at time step t, aiming to synthesize the specific task i, the previous actions a1, ..., at-1 and the subsequent action at consisting of an action, a corresponding target element in the observation, and a natural language summary of the action. We first convert a web page into its corresponding accessibility tree at the beginning of each node, and sample a segment to present to the assistant LM. We follow the WebArena convention of assigning a unique ID to each node in the tree to ease the challenge of referencing the nodes. To increase the diversity of the tasks, we first instruct the model to brainstorm k task categories relevant to the web domain. Then the model randomly selects three of these categories and develops them into concrete scenarios with past actions leading up to the current observation and the next action to take. The prompt is in Appendix F and an example generation is in Appendix G."}, {"title": "3.4 Data Filtering", "content": "To ensure the quality of the training set, we apply a two-part filtering pipeline. In the first part, we ensure that data samples are both complete and coherent. For a sample to pass this filter, it must include all required components in the correct format. These components include: (1) an action that falls within the defined action space, (2) a valid and meaningful action target element in the corresponding web page, (3) NL texts that are well formed, e.g. without the use of \u201c\u2026\u201d in the texts as an abbreviation by the generative model, (4) overall comprehensive generation without placeholders from the prompt (e.g., ).\nTo further eliminate accurately formatted but unresponsive actions, we apply a second filtering step using next state prediction. Here, we use the LLM to predict the next state, Ot+1, based on the current state ot and action at in our synthesized data. If the model predicts that Ot+1 = ot, the action is deemed to have no impact, and we filter out the action accordingly."}, {"title": "4 Data Statistics", "content": "We assess the distribution of history lengths, task objectives, and observations in the form of accessibility trees. The first statistic reflects general task complexity, as longer trajectories typically indicate"}, {"title": "5 Experimental Setup", "content": "Agent training We finetune CodeLlama-7b [31] with 99, 920 web-navigation instruction examples, sourced from WikiHow articles and ClueWeb web pages 6. Training details can be found in Appendix K.\nWe select CodeLlama-7b after comparing Llama-2, Llama-3, Llama-3-Instruct, CodeLlama, CodeLlama-Instruct, DeepSeek Coder, and Mistral by finetuning and evaluating on Mind2Web's train and test set."}, {"title": "6 Results", "content": "6.1\nMain Results\nTable 1 presents the performance of various models across three web-based task benchmarks. Overall, Synatra-CodeLlama achieves the best performance among models of comparable size. Notably, Synatra-CodeLlama significantly outperforms its base model CodeLlama-instruct-7b. While CodeLlama-instruct-7b fails to complete any tasks in WebArena, Synatra-CodeLlama successfully executes 6.28% of them. Furthermore, Synatra-CodeLlama elevates the performance of CodeLlama-instruct-7b from 6.62% to 15.85% in Mind2Web (a 139.42% relative improvement) and from 23.04% to 38.20% in MiniWoB++. More encouragingly, Synatra-CodeLlama demonstrates superior performance on Mind2Web and WebArena compared to GPT-3.5. It also outperforms Lemur-chat-70b, which is finetuned with interactive data and is ten times larger, across all three benchmarks. The results suggest that our data synthesis approach is effective in helping the model predict the next action (as in Mind2Web) and performing simple tasks with a few steps (as in MiniWoB++). The synthesized data can also guide the model towards executing real-world complex tasks more accurately. Consequently, Synatra-CodeLlama has potential applications in suggesting individual steps in browser copilot scenarios.\nSynatra-CodeLlama surpassed the performance of all open-source model finetuned with interactive data. Among these models, AgentLM, CodeActAgent and AgentFlan include demonstrations to perform web-based tasks in their instruction finetuning dataset. However, we find that these models may not serve as capable agents to perform web-based tasks due to the special design choice encoded in the finetuned models. For instance, AgentLM and CodeActAgent use Regex expression to match interactive element on a web page and require carefully selected in-context examples to showcase which are the proper Regex expression for different examples. However, Regex expressions only work for simple web pages with a few elements as in MiniWoB++, while it is prohibitive to do pattern matching in complex web pages as in Mind2Web and WebArena. As a result, when we experiment with the more generic action space which is suitable for all three benchmarks without in-context examples, we see these models have a significant performance degradation. On the other"}, {"title": "6.2 Analysis", "content": "What does the model learn from the synthetic data? Our analysis suggests that the baseline acquires essential knowledge at multiple levels from synthetic data. The error rates for different error types are shown in Table 2. At the low level, synthetic data helps the model emit valid actions. For example, while approximately 95% of actions predicted by the base model CodeLlama-instruct-7b involve interacting with non-existent elements on the web page, Synatra reduces this error to just 0.7%. This low error rate is closer to that of larger models such as Llama3-chat-70b and GPT-4, which are better at following complex instructions with multiple requirements. Additionally, the synthetic data improves the model's ability to understand web pages more accurately. To assess this, we measure the ratio of invalid actions, including both invalid click and type, where the predicted action attempts to interact with an element that is not clickable (e.g., a piece of text) or not typable (e.g., a button). We observe that models in the 7b-8b range have high error rates in interpreting basic web elements. For instance, the strong 8b model Llama3-chat-8b mistakenly click a non-clickable element over 15% of the time, while Synatra reduces this error to under 6% approaching the performance of GPT-4. Finally, at task completion level, Synatra shows a stronger ability to track progress accurately. Specifically, when filling out forms on web pages, Synatra-CodeLlama is 74% less likely than GPT-4-turbo to repeatedly input the same words into the same field, a common error in models. This indicates that our synthetic data enables the model to more precisely recognize completed actions and identify the remaining steps needed to achieve the goal. More qualitative study can be found in Appendix I."}, {"title": "6.3 Ablations", "content": "In this section, we perform an ablation to validate the design choices of our data synthesis approach.\nModel performance improves with more synthetic data To assess the impact of scaling our synthetic data, we trained three CodeLlama 7B models with 18k, 50k, and 100k samples respectively, while keeping all other parameters constant. We then evaluated the models on WebArena. As shown on the right, the success rates on WebArena steadily increase as the synthetic training data scales from 18k to 100k samples. This highlights the potential of scaling synthetic data with our approach.\nRepresenting trajectories as programs is beneficial To verify if the programs format is helpful, we convert 30k trajectories to the NL format similar to setting in WebArena [58] and we compare its performance with the model trained with the exact data, but in our program format. The results are shown in Figure 4a. We can see that performance drops on both MiniWoB++ and WebArena when using the NL format. We hypothesize that program is potentially a more natural format to represent a sequence of actions in the form of API calls, and the chain-of-thought reasoning can be embedded as code comments. Our observation also echo the observations of using program representation for non-programming tasks [23, 29, 41], while our experiments further contributes insights towards finetuning setups for interactive tasks.\nDifference sources of indirect knowledge complement each other Our indirect knowledge primarily comes from two sources: tutorials and randomly sampled web pages. In the former source,"}, {"title": "7 Related Work", "content": "Learning from Indirect Supervision Due to the costly nature of human supervision, many digital agent learning works explore learning from existing yet indirect knowledge sources [8, 35, 12, 55], reinforcement learning optimization that learn from environment feedback [37, 21, 32, 39]. These work mostly focus on simplistic web agents inside synthetic environments, instead of realistic web agents that require complex observation of real-world websites and long-horizon nature of web navigation trajectories. More recently, LLM self-improvement from environment rewards with prompts during inference time [34, 49, 28, 44, 14, 52] has been applied on creating more complex digital agents in the wild. Our work fills the gap of training with synthetic data generated from existing resources, i.e., indirect supervision, on complex web navigation tasks.\nPrompting Approaches for AI Agents Prompting approaches attempt to design dedicated processes that benefit task execution. Existing methods include performing reasoning about the current statues before proceeding to next actions [43, 47, 22], deliberate search and planning to look ahead and find better solutions [48, 13], and self-verification to assure progressing on the right track [18, 34, 24]. Our focus on instruction tuning data generation without human supervision can hopefully enable synthetic data generation recipes for various kinds of instruction prompts for agent tasks.\nData Generation for Interactive Agents Many works on improving interactive agents, especially those with open-source models, design some ways of generating training data to adapt general-purpose LLMs to agent-specific tasks. For example, Gur et al. [11], uses hand-crafted instruction templates to generate tasks and collect training data after executing in the environment. Lai et al. [19]'s data generation is either synthesizing simple tasks, e.g., single web page understanding, next action prediction, or using a human-in-the-loop approach to assist complex trajectory generation. Fully synthetic data generation for complex scenarios such as long-horizon web navigation has not been systematically studied before. Our work aim to generate more realistic data without human intervention while preserving real-world web navigation complexity and trajectory lengths."}, {"title": "8 Conclusion", "content": "We propose a data synthesis approach Synatra. Empirically we showcase that finetuning with data generated from our approach can improve existing general-purpose LLMs to perform better on digital agent tasks. Since Synatra can synthesize trajectories given a single, static piece of indirect knowledge (e.g., a single web page snapshot), we argue that when equipped with a capable LLM, a regular tutorial designed for human consumption and a random observation, can secretly also be a trajectory. We show that even considering the high cost of calling state-of-the-art LLMs such as OpenAI GPT-4, synthesizing is more cost-effective than collecting human demonstrations of similar quantity for model training.\nWhile in this paper we only test the method in limited settings, we believe that it is a scalable and orthogonal recipe usable for: (1) other prompting techniques that may perform better on certain tasks during the synthesis; (2) scaling up with more indirect knowledge resources for more diverse synthetic data; (3) finetuning base LLMs of different sizes and configurations. Admittedly, Synatra also have limitations, such as requiring a strong LLM for data generation that may cost significant money and processing time, and the dependency on source web page data quality. We detail the limitations in Appendix M and broader impacts in Appendix L."}, {"title": "L Broader Impacts", "content": "The broader impacts of this work extend across several domains. Firstly, the approach has the potential to democratize the development of digital agents by making the training process more affordable and accessible. Organizations with limited resources can utilize existing public data to train competent agents without the need for expensive data collection efforts. This could lead to a more widespread adoption and innovation in AI applications, particularly in regions or sectors that previously could not afford such technology.\nSecondly, by enabling digital agents to perform more complex tasks effectively, this work can significantly enhance productivity and efficiency in various industries. For example, customer service, online troubleshooting, and data management tasks could be accelerated, allowing human workers to focus on more creative or complex problem-solving tasks.\nFurthermore, the technology has implications for accessibility, as it could help develop more intuitive and user-friendly interfaces for people with disabilities or those who are not tech-savvy. Agents trained with a diverse range of demonstrations can offer more personalized and context-aware assistance, improving user experience across digital platforms.\nLastly, the ethical and societal implications of this technology also constitute a critical area of impact. While the technology can lead to significant efficiencies and capabilities, it also raises questions about the potential for job displacement, and the need for robust guidelines to ensure that the deployment of such agents aligns with ethical standards. These broader impacts underscore the importance of interdisciplinary approaches to the development and governance of AI technologies, ensuring they contribute positively to society."}, {"title": "M Limitations", "content": "One major limitation to the work is the potential variability in the quality and relevance of the indirect knowledge sources. We attempted to mitigate this through use of high-quality sources such as WikiHow and broad sources such as ClueWeb with intelligent sampling strategies, but still the danger of unrepresentative data remains.\nAnother concern is the generalizability of the synthesized demonstrations. While the approach allows for the generation of a large volume of training data, the synthetic nature of these demonstrations may not fully capture the complexity and nuances of real human interactions with digital environments. As a result, agents trained on this data may still struggle with unexpected or less typical scenarios not covered in the training data.\nFurthermore, there is the risk of overfitting to the specific formats and tasks represented in the indirect knowledge sources. If the diversity of these sources is limited, the agents may not develop the flexibility needed to handle a broad range of tasks across different platforms or environments.\nLastly, the reliance on large language models and complex synthesis processes might introduce significant computational costs and environmental impacts. The energy consumption and carbon footprint associated with training such large models are concerns that need to be addressed to ensure sustainable development in AI technologies. These limitations highlight the need for ongoing research, improved data curation methods, and the development of more robust models that can better generalize from synthetic training environments to real-world applications."}, {"title": "I Case Study", "content": "We conducted a detailed examination of instances where Synatra-CodeLlama successfully completes tasks that GPT-4-turbo fails to accomplish on WebArena. Two key patterns emerged from these cases, which we outline here. Our analysis focuses on hard-level tasks requiring multiple steps to complete.\nSynatra-CodeLlama identifies details more effectively In several scenarios, Synatra-CodeLlama outperforms GPT-4-turbo by detecting and utilizing detailed information such as hidden links and buttons on the page, whereas GPT-4-turbo focuses only on the most prominent components. In the scenario illustrated in Figure 6, both agents are tasked with displaying issues labeled as \u201cbug.\u201d GPT-4-turbo immediately attempts to use the search box, but lacks the knowledge of how to search effectively in this case and guesses the keyword, which does not yield results. In contrast, Synatra-CodeLlama pays closer attention to the content displayed on the page and accurately identifies a link that lists all issues labeled as \u201cbug.\u201d\nSynatra-CodeLlama demonstrates clearer logical flow between steps In the example shown in Figure 8, both agents are filling out a form and have correctly entered the start and end dates. However, GPT-4-turbo incorrectly predicts that the next step is to re-enter the starting date, while Synatra-CodeLlama accurately interprets the status of the web page (\u201cwith the date range set to include the entirety of Q1 2023\u201d), plans accordingly (\u201cwe are ready to generate the refund report\u201d), and executes the correct action. This example illustrates that data generated by Synatra enhances"}]}