{"title": "Proof Recommendation System for the HOL4 Theorem Prover", "authors": ["Nour Dekhil", "Adnan Rashid", "Sofi\u00e8ne Tahar"], "abstract": "We introduce a proof recommender system for the HOL4 theorem prover [1]. Our tool is\nbuilt upon a transformer-based model [2] designed specifically to provide proof assistance in\nHOL4. The model is trained to discern theorem proving patterns from extensive libraries of\nHOL4 containing proofs of theorems. Consequently, it can accurately predict the next tactic(s)\n(proof step(s)) based on the history of previously employed tactics. The tool operates by\nreading a given sequence of tactics already used in a proof process (in our case, it contains at\nleast three tactics), referred to as the current proof state, and provides recommendations for\nthe next optimal proof step(s).", "sections": [{"title": "Introduction", "content": "Figure 1 depicts the major steps taken to develop the proof recommendation tool. The\ninitial block (highlighted in blue) refers to the construction of a HOL4 proofs dataset. In the\ndataset construction phase, we are abstracting the proof scripts to only include the tactics used\nto prove a theorem or a lemma. This process involves initially identifying the theorems or\nlemmas within each sml file, followed by recording the tactics used to prove each one of them."}, {"title": "Methodology and experiments", "content": "We created large proof sequences datasets (Datasets 1-5) from five HOL4 theories [3-7]\ndeveloped by the Hardware Verification Group (HVG) of Concordia University alongside an\nalready available dataset created using the real arithmetic theory of HOL4 (Dataset 6) [8]. For\nexperimental purposes, we combined all datasets into Dataset 7. Our objective is to predict\nthe subsequent tactic from a sequence of previously employed tactics. To accomplish this, we\napproach this challenge as a multi-label classification task using language models. To facilitate\nthis, we restructure the dataset into pairs of current proof states and possible future tactics.\nMore details on the datasets used for classification are given in Table 1.\nWe experimented with various transformer-based language models, such as BERT [9],\nROBERTa [10], and T5 [11] for these datasets to identify the most effective model based on our\nevaluation. After splitting the restructured datasets into a 90-10 ratio for training and testing,\nwe proceeded to train the selected models (block highlighted in yellow) using a grid search of\nhyperparameters optimization. Given the multitude of possible tactics available at each proof\nstate, we chose to provide multiple recommendations for the next proof step. To assess the\naccuracy of these recommendations (block highlighted in green), we use the n-correctness rate,\nwhich measures the likelihood that a correct tactic from the testing dataset is among the top-n\nrecommended tactics, where n signifies the number of recommended tactics evaluated against\nthe correct tactic. We found out that RoBERTa demonstrated superior performance across\nmost cases for n = 7. As a result, we deploy it into our proof recommendation tool (block\nhighlighted in grey).\nWith the aim of efficiently predicting the next tactic (k = 1, where k represents the number\nof future tactics to predict) for the majority of theory datasets, we also challenged our tool by\nattempting to predict two future tactics. Table 2 provides further details of the experimental\nresults for RoBERTa in predicting one future tactic (k = 1) and two future tactics (k = 2).\nAfter examining the performance results across different datasets, it seems that the variations\narise from the diversity and patterns unique to each dataset, as well as the range of tactics\nemployed. Specifically, Datasets 1-5 exhibit a uniformity in their proof structures, originating\nfrom one application project written by a single person, thus making the proofs more homo-\ngeneous and consistent in style. However, Dataset 6, came from HOL4 libraries containing a\ndiverse range of theorems regarding different mathematical concepts, presents proofs with het-\nerogeneous patterns, making them challenging to predict. Additionally, we observed a decrease\nin performance when attempting to predict two future tactics, which may be attributed to the\nexpansive space of possibilities and resulting in increased uncertainty."}, {"title": "Conclusion", "content": "In the recent past, several studies have integrated artificial intelligence into theorem prover\ntools (e.g., PVS and Coq), particularly for predicting future-proof steps. For instance, in the\nstudy reported in [12], accuracies ranging from 50% to 70% were achieved for the top 3-5\nrecommendations, while the work in [13] achieved 87% accuracy for the top 3, and the one\nin [14] reported 54.3% accuracy for the top 10. In comparison, our tool surpasses results\nreported in these studies, achieving accuracies of 77.3%, 89.88%, and 93.7% for the top 3, 7,\nand 10 next tactic recommendations, respectively, measured on the combined Dataset 7. The\ncurrent tool version is available to try online [15]. In the future, we plan to expand it to include\nmore HOL4 theories and enhance its interfacing with HOL4. In addition, we are investigating\nits potential to automatically generate complete proofs, considering the need for optimization\ngiven the exponential growth in combination possibilities with the proof sequence length. To\naddress this, we plan to use some advanced tree search algorithms."}]}