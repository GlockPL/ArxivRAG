{"title": "Proof Recommendation System for the HOL4 Theorem Prover", "authors": ["Nour Dekhil", "Adnan Rashid", "Sofi\u00e8ne Tahar"], "abstract": "We introduce a proof recommender system for the HOL4 theorem prover [1]. Our tool is built upon a transformer-based model [2] designed specifically to provide proof assistance in HOL4. The model is trained to discern theorem proving patterns from extensive libraries of HOL4 containing proofs of theorems. Consequently, it can accurately predict the next tactic(s) (proof step(s)) based on the history of previously employed tactics. The tool operates by reading a given sequence of tactics already used in a proof process (in our case, it contains at least three tactics), referred to as the current proof state, and provides recommendations for the next optimal proof step(s).", "sections": [{"title": null, "content": "We created large proof sequences datasets (Datasets 1-5) from five HOL4 theories [3-7] developed by the Hardware Verification Group (HVG) of Concordia University alongside an already available dataset created using the real arithmetic theory of HOL4 (Dataset 6) [8]. For experimental purposes, we combined all datasets into Dataset 7. Our objective is to predict the subsequent tactic from a sequence of previously employed tactics. To accomplish this, we approach this challenge as a multi-label classification task using language models. To facilitate this, we restructure the dataset into pairs of current proof states and possible future tactics. More details on the datasets used for classification are given in Table 1."}, {"title": null, "content": "We experimented with various transformer-based language models, such as BERT [9], ROBERTa [10], and T5 [11] for these datasets to identify the most effective model based on our evaluation. After splitting the restructured datasets into a 90-10 ratio for training and testing, we proceeded to train the selected models (block highlighted in yellow) using a grid search of hyperparameters optimization. Given the multitude of possible tactics available at each proof state, we chose to provide multiple recommendations for the next proof step. To assess the accuracy of these recommendations (block highlighted in green), we use the n-correctness rate, which measures the likelihood that a correct tactic from the testing dataset is among the top-n recommended tactics, where n signifies the number of recommended tactics evaluated against the correct tactic. We found out that RoBERTa demonstrated superior performance across most cases for n = 7. As a result, we deploy it into our proof recommendation tool (block highlighted in grey)."}, {"title": null, "content": "With the aim of efficiently predicting the next tactic (k = 1, where k represents the number of future tactics to predict) for the majority of theory datasets, we also challenged our tool by attempting to predict two future tactics. Table 2 provides further details of the experimental results for RoBERTa in predicting one future tactic (k = 1) and two future tactics (k = 2). After examining the performance results across different datasets, it seems that the variations arise from the diversity and patterns unique to each dataset, as well as the range of tactics employed. Specifically, Datasets 1-5 exhibit a uniformity in their proof structures, originating from one application project written by a single person, thus making the proofs more homogeneous and consistent in style. However, Dataset 6, came from HOL4 libraries containing a diverse range of theorems regarding different mathematical concepts, presents proofs with heterogeneous patterns, making them challenging to predict. Additionally, we observed a decrease in performance when attempting to predict two future tactics, which may be attributed to the expansive space of possibilities and resulting in increased uncertainty."}, {"title": null, "content": "In the recent past, several studies have integrated artificial intelligence into theorem prover tools (e.g., PVS and Coq), particularly for predicting future-proof steps. For instance, in the study reported in [12], accuracies ranging from 50% to 70% were achieved for the top 3-5 recommendations, while the work in [13] achieved 87% accuracy for the top 3, and the one in [14] reported 54.3% accuracy for the top 10. In comparison, our tool surpasses results reported in these studies, achieving accuracies of 77.3%, 89.88%, and 93.7% for the top 3, 7, and 10 next tactic recommendations, respectively, measured on the combined Dataset 7. The current tool version is available to try online [15]. In the future, we plan to expand it to include more HOL4 theories and enhance its interfacing with HOL4. In addition, we are investigating its potential to automatically generate complete proofs, considering the need for optimization given the exponential growth in combination possibilities with the proof sequence length. To address this, we plan to use some advanced tree search algorithms."}]}