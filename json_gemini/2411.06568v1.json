{"title": "Learning Loss Landscapes in Preference Optimization", "authors": ["Carlo Alfano", "Silvia Sapora", "Jakob Nicolaus Foerster", "Patrick Rebeschini", "Yee Whye Teh"], "abstract": "We present an empirical study investigating how specific properties of preference datasets, such as mixed-quality or noisy data, affect the performance of Preference Optimization (PO) algorithms. Our experiments, conducted in MuJoCo environments, reveal several scenarios where state-of-the-art PO methods experience significant drops in performance. To address this issue, we introduce a novel PO framework based on mirror descent, which can recover existing methods like Direct Preference Optimization (DPO) and Odds-Ratio Preference Optimization (ORPO) for specific choices of the mirror map. Within this framework, we employ evolutionary strategies to discover new loss functions capable of handling the identified problematic scenarios. These new loss functions lead to significant performance improvements over DPO and ORPO across several tasks. Additionally, we demonstrate the generalization capability of our approach by applying the discovered loss functions to fine-tuning large language models using mixed-quality data, where they outperform ORPO.", "sections": [{"title": "1 Introduction", "content": "Learning from human feedback is a paradigm that enables the alignment of complex agents to human preferences, and has been successfully applied to Large Language Models (Team et al., 2023; Achiam et al., 2023). In particular, fine-tuning pretrained LLMs with human preferences has become a popular strategy to adapt them to specific tasks and to improve their safety and helpfulness.\nMost LLM alignment pipelines begin with a supervised fine-tuning (SFT) step, which involves supervised next-token prediction on a dataset of high-quality responses and leads to a reference policy. The reference policy is further optimized using the human preference data, typically through either Reinforcement Learning from Human Feedback (RLHF) (Christiano et al., 2017) or Direct Preference Optimization (DPO) (Rafailov et al., 2024), or one of their several variants. RLHF consists of learning a reward model consistent with human preferences and then using Reinforcement Learning (RL) techniques such as REINFORCE (Sutton et al., 1999) and Proximal Policy Optimisation (PPO) (Schulman et al., 2017) to maximize the total expected reward. In contrast, DPO and its variations, e.g. odds ratio preference optimization (ORPO) (Hong et al., 2024), bypass explicit reward models entirely and optimize directly on preference data, implicitly learning the reward.\nWhile RL-based methods offer stronger theoretical guarantees and often lead to higher performance (Song et al., 2024; Xu et al., 2024), offline approaches such as DPO have gained traction due to their simplicity and the ability to leverage preexisting high-quality datasets. In contrast to PPO, where data collection and labeling are performed iteratively after each update, DPO and its modifications allow for more efficient training, avoiding the high computational costs, need of additional sample labelling and complexity of RL methods. Specifically, PPO requires careful parameter tuning (Yuan et al., 2023) and involves simultaneous training of multiple models (the reward model, language model, and critic), which can be prohibitive in most hardware setups."}, {"title": "2 Preliminaries", "content": "Let $\\mathcal{M} = (\\mathcal{S}, \\mathcal{A}, P,r,\\mathcal{T}, \\mu)$ denote an episodic Markov Decision Process, where $\\mathcal{S}$ and $\\mathcal{A}$ are respectively the state and action spaces, $P(s' | s,a)$ is the transition probability from state $s$ to $s'$ when taking action $a$, $r(s, a) \\in [0, 1]$ is the reward function, $\\mathcal{T}$ is the maximum episode length, and $\\mu$ is a starting state distribution. A policy $\\pi \\in (\\Delta(\\mathcal{A}))^{\\mathcal{S}}$, where $\\Delta(\\mathcal{A})$ is the probability simplex over $\\mathcal{A}$, represents the behavior of an agent on an MDP, whereby at state $s \\in \\mathcal{S}$ the agents takes actions according to the probability distribution $\\pi(\\cdot | s)$. Let $\\tau = \\{(s_t,a_t)\\}_{t=0}^{\\mathcal{T}-1}$ denote a trajectory of length $\\mathcal{T}$ and, with a slight overload of notation, let $\\pi(\\tau) = \\prod_{t=0}^{\\mathcal{T}-1} \\pi(a_t |s_t)$ and $r(t) = r(s_t, a_t)$. Lastly, let $\\pi(\\cdot|\\tau)$ be the product distribution of $\\pi(\\cdot|s_0), ..., \\pi(\\cdot|s_{N-1})$.\nLet $\\mathcal{D} = \\{(s_0, \\tau^{\\omega}, \\tau^{\\iota})\\}_{i=1}^I$ be a preference dataset, where each tuple $(s_0, \\tau^{\\omega}, \\tau^{\\iota})$ consists of a starting state $s_0$ and two trajectories with starting state $s_0$. Each pair of trajectories is ranked by a judge, who determines a chosen trajectory $\\tau^{\\omega}$ (\u201cwin\") and a rejected trajectory $\\tau^{\\iota}$ (\u201close\"), based on the cumulative rewards $r(\\tau_{\\omega})$ and $r(\\tau_{\\iota})$. We assume the judge ranks trajectories according to the Bradley-Terry model (Bradley & Terry, 1952), whereby the probability of choosing $\\tau^{\\omega}$ over $\\tau^{\\iota}$ is defined as\n$P(\\tau_{\\omega} \\succ \\tau_{\\iota}) = \\frac{\\exp(r(\\tau_{\\omega})/\\eta)}{\\exp(r(\\tau_{\\omega})/\\eta) + \\exp(r(\\tau_{\\iota})/\\eta)} = \\sigma((r(\\tau_{\\omega}) - r(\\tau_{\\iota}))/\\eta),$ (1)\nwhere $\\sigma$ is the sigmoid function and $\\eta$ is a temperature parameter. Our objective is to exploit the dataset $\\mathcal{D}$ to find a policy $\\pi^*$ that maximizes the expected cumulative reward of an episode, that is\n$\\pi^* \\in \\underset{\\pi}{\\operatorname{argmax}} \\mathbb{E}_{\\tau \\sim (\\mu,\\pi,P)}r(\\tau) := \\underset{\\pi}{\\operatorname{argmax}} \\mathbb{E}_{s_0\\sim\\mu,a_t\\sim\\pi(\\cdot|s_t),s_{t+1}\\sim P(\\cdot|s_t,a_t)}\\sum_{t=0}^{\\mathcal{T}-1}r(s_t,a_t).$ (2)\nWe consider an offline setting, where we do not have access to either the transition probability $P$, the reward function $r$, or the MDP $\\mathcal{M}$."}, {"title": "2.1 Preference optimization", "content": "There are several methods in the literature to optimize the objective in (2) using a preference dataset $\\mathcal{D}$. We provide here a short review of the main pipelines (Ouyang et al., 2022). Two common independent preliminary steps are reward modelling (RM) and supervised fine tuning (SFT). RM aims to obtain an estimate of the true reward function, and is usually framed as a maximum likelihood estimation problem for a Bradley-Terry preference model, i.e. find\n$\\underset{\\theta}{\\operatorname{argmax}} \\sum \\log \\sigma(r_{\\theta}(\\tau_{\\omega}) - r_{\\theta}(\\tau_{\\iota})/\\eta),$ (3)\nfor a parametrized reward class $\\{r_{\\theta} : \\theta \\in \\Theta\\}$ and for $\\eta \\geq 0$. SFT is an initial alignment phase, where the starting policy $\\pi_0$ is trained to imitate high-quality demonstration data. In particular, the starting policy $\\pi_0$ is updated to minimize the cross-entropy loss $l(\\pi, (s_0, \\tau_{\\omega}, \\tau_{\\iota})) = \\log(\\pi(\\tau_{\\omega}))$, typically doing one epoch over the dataset. We call reference policy $\\pi_{ref}$ the policy obtained at the end of this procedure. The new objective we want to optimize is\n$\\pi^* \\in \\underset{\\pi}{\\operatorname{argmax}} \\mathbb{E}_{\\mathcal{D}_{\\pi_0},\\tau \\sim (\\pi,P)} \\sum_{t=0}^{\\mathcal{T}-1} \\mathbb{E}_{a \\sim \\pi(\\cdot|s_t)} [r(s_t, a)] - \\beta D_{KL}(\\pi(\\cdot|\\tau), \\pi_{ref}(\\cdot|\\tau));$ (4)\nwhere the $D_{KL}$ represents the KL-divergence and is introduced to prevent the policy from moving too far away from the dataset distribution. The expressions in (3) and (4) can be optimized sequentially, first obtaining a reward estimate in (3) and then optimizing (4) with PPO using the reward estimate in place of the true reward. Alternatively, the optimization problems in (3) and (4) can be solved implicitly using DPO or ORPO. We proceed to discuss each of these methods.\nPPO PPO is recognized as one of the preferred methods for optimizing (4) when the necessary computing resources are available, as demonstrated by its success in training state of the art models like GPT-4 (Achiam et al., 2023) and Claude (Antropic, 2023). However, it presents a complex pipeline where one needs to effectively train a reward model, perform SFT and then optimize (4), where each phase has a different set of hyper-parameters. Additionally, storing the reward model, the reference agent and the current agent in memory is impractical in most setups and often requires sacrificing other aspects, such as batch-size. Besides it computational costs, PPO is known to be prone to reward overoptimization (Coste et al., 2024).\nDPO Direct Preference Optimization (DPO) bypasses the need for an explicit reward model by using the agent itself to implicitly represent the reward model. It consists in optimizing the objective\n$\\pi^* \\in \\underset{\\pi}{\\operatorname{argmax}} \\mathbb{E}_{(s_0, \\tau_{\\omega},\\tau_{\\iota}) \\sim \\mathcal{D}} \\log \\sigma\\bigg(\\beta \\bigg(\\log \\frac{\\pi(\\tau_{\\omega})}{\\pi_{ref}(\\tau_{\\omega})} - \\log \\frac{\\pi(\\tau_{\\iota})}{\\pi_{ref}(\\tau_{\\iota})}\\bigg)\\bigg),$ (5)\nwhich is obtained by plugging the theoretical solution of (4) in the maximum likelihood problem in (3). Refer to Appendix A for details. Thanks to its simplicity, DPO has been widely adopted to fine-tune LLMs as an alternative to PPO (Yuan et al., 2024; Jiang et al., 2024).\nA known issue of DPO is that it pushes probability mass away from the preference dataset and to unseen responses, which can cause the final policy to deviate significantly from the reference policy, even when the reference policy aligns well with human preferences. In contrast, PPO can leverage the generalization capabilities of the (learned) reward model to generate responses beyond the preference dataset distribution, while the KL-divergence penalty can provide additional regularization. To mitigate the risks described above, DPO is usually only applied for a few epochs.\nORPO ORPO is a more recent algorithm that aims to further simplify the training pipeline and, concurrently, to address the distribution shift issue present in DPO. ORPO merges the SFT and DPO steps into one, which optimizes the unified objective\n$\\pi^* \\in \\underset{\\pi}{\\operatorname{argmax}} \\mathbb{E}_{(s_0, \\tau_{\\omega},\\tau_{\\iota}) \\sim \\mathcal{D}} \\log \\pi(\\tau_{\\omega}) + \\Lambda \\log \\sigma \\bigg(\\log (odds(\\tau_{\\omega})) - \\log (odds(\\tau_{\\iota}))\\bigg)$ (6)\nwhere $odds(\\tau) = \\frac{\\pi(\\tau)}{1 - \\pi(\\tau)}$. ORPO gets rid of the need for a reference model by adding an SFT term to the preference optimization objective function, which substitutes the role of the SFT"}, {"title": "2.2 Mirror Maps", "content": "We review the concept of mirror map, which will be needed when describing our methodology. For a convex set $\\mathcal{X} \\subset \\mathbb{R}^{|\\mathcal{A}|}$, a mirror map $h : \\mathcal{X} \\rightarrow \\mathbb{R}$ is defined as a strictly convex, continuously differentiable and essentially smooth function that satisfies $\\nabla h(\\mathcal{X}) = \\mathbb{R}^{|\\mathcal{A}|}$. Essentially, a mirror map is a function whose gradient allows bijective mapping between the primal space $\\mathcal{X}$ and the dual space $\\mathbb{R}^{|\\mathcal{A}|}$. The specific class of mirror maps that we are going to use is the $\\omega$-potential mirror map class, to which most mirror maps considered in the literature belong.\nDefinition 2.1 ($\\omega$-potential mirror map Krichene et al. (2015)). For $u \\in (-\\infty, +\\infty]$, $\\omega \\leq 0$, an $\\omega$-potential is defined as an increasing $C^1$-diffeomorphism $\\phi : (-\\infty, u) \\rightarrow (\\omega, +\\infty)$ such that\n$\\underset{x \\rightarrow u}{\\operatorname{lim}} \\phi(x) = +\\infty, \\qquad \\underset{x \\rightarrow -\\infty}{\\operatorname{lim}} \\phi(x) = \\omega, \\qquad \\int_{-\\infty}^u \\phi^{-1}(x)dx < \\infty$.\nFor any $\\omega$-potential $\\phi$, we define the associated mirror map $h_{\\phi}$ as\n$h(\\pi(\\cdot|s)) = \\sum_{a \\in \\mathcal{A}} \\int_{\\omega}^{\\pi(a|s)} \\phi^{-1}(x)dx$.\nWhen $\\phi(x) = e^{x-1}$ we recover the negative entropy mirror map, while we recover the $l_2$-norm when $\\phi(x) = 2x$ (refer to Appendix B). Mirror maps in this class are simple to implement in practice, where $\\mathcal{A}$ is often large, as they can be parametrized by a scalar function instead of a multi-dimentional one. Additionally, the same $\\omega$-potential $\\phi$ can be used to generate mirror maps for different action spaces, allowing the insights obtained for one action space to easily generalize to others. An $\\omega$-potential mirror map $h_{\\phi}$ induces a Bregman divergence (Bregman, 1967), which is defined as\n$D_{h_{\\phi}}(\\pi(\\cdot|s), \\pi'(\\cdot|s)) := h_{\\phi}(\\pi(\\cdot|s)) - h_{\\phi}(\\pi'(\\cdot|s)) - \\langle \\nabla h_{\\phi}(\\pi'(\\cdot|s)), \\pi(\\cdot|s) - \\pi'(\\cdot|s) \\rangle,$\nwhere $D_{h_{\\phi}}(\\pi(\\cdot|s), \\pi'(\\cdot|s)) \\geq 0$ for all $x, y \\in \\mathcal{Y}$. When $\\phi(x) = e^{x-1}$, $D_{h_{\\phi}}$ is equivalent to the KL-divergence, while we recover the Euclidean distance when $\\phi(x) = 2x$ (refer to Appendix B). When the Bregman divergence is employed as a regularization term in optimization problems, tuning the mirror map allows us to control the geometry of the updates of the parameters to be optimized, determining when to take large or small updates based on the current value of the parameters."}, {"title": "3 Methodology", "content": "We develop a new framework for preference optimization based on mirror maps, which generalizes DPO and ORPO. We start by replacing the KL-divergence penalty term in the objective in (4) with a more general Bregman divergence, that is, we aim to solve the problem\n$\\pi^* \\in \\underset{\\pi}{\\operatorname{argmax}} \\mathbb{E}_{s \\sim \\mathcal{D}, \\tau \\sim (\\pi, P)} \\sum_{t=0}^{\\mathcal{T}-1} \\mathbb{E}_{a \\sim \\pi(\\cdot|s_t)} [r(s_t, a)] - \\beta D_h(\\pi(\\cdot|\\tau), \\pi_{ref}(\\cdot|\\tau))$ (7)\nwhere $D_h$ is the Bregman divergence induced by a mirror map $h$. This new objective allows us to enforce different types of regularization, which, as we show later in the paper, can be tailored to account for specific properties of the preference dataset. Following the same intuition used to obtain the DPO objective, we have the following result.\nTheorem 3.1. Let $h_{\\phi}$ be a 0-potential mirror map and $\\pi^*$ be a solution to the optimization problem in (7). If $\\pi_{ref}(a|s) > 0$ for all $s \\in \\mathcal{S}, a \\in \\mathcal{A}$, we have that\nr(\\tau) = \\phi^{-1}(\\pi^*(\\tau)) - \\phi^{-1}(\\pi_{ref}(\\tau)) + c(s_0), (8)\nwhere $c(s_0)$ is a normalization constant that depends only on $s_0$."}, {"title": "3.1 Learning mirror maps", "content": "To search the space of PO algorithms we have defined, we employ a neural network parametrization for both $\\phi$ and $\\phi^{-1}$, which we optimize using evolutionary strategies.\nSimilarly to Alfano et al. (2024), we parameterize both $\\phi$ and $\\phi^{-1}$ as a one layer neural network with 126 hidden units and non-negative kernels, where the activation functions are equally split among:\nx, (x)_+, x^2, x^3, (x)^{1/2}, (x)^{1/3}, \\log((x)_+), e^x, \\tanh(x), \\log(\\frac{clip(x)}{1 - clip(x)}) ,\nwhere $(x)_+ = \\max(x,0)$ and $clip(x) = \\max(\\min(x,1),0)$. The non-negative kernels and the increasing activation functions guarantee the monotonicity of $\\phi$ and $\\phi^{-1}$, while the several different activation functions facilitate expressing complex functions. To ensure that we are able to recover the ORPO objective, we add $a \\log(x)$ and $b \\log(x) - b \\log(1 - x)$ to the final outputs of $\\psi$ and $\\phi^{-1}$, respectively, where $a, b \\geq 0$. In case we want to take into account training progress, we give a second input to the neural network, i.e. $x \\cdot n/N$, where $n$ is the current epoch and $N$ is the total number of epochs through the dataset. To ensure that monotonicity is preserved, we lower bound the weights associated to the second input with the negative of the respective weights associated to the first input.\nTo search for the best $\\phi$ and $\\phi^{-1}$ within this class, we employ the OpenAI-ES strategy (Salimans et al., 2017). Denote by $\\zeta$ the parameters of $\\phi$ and $\\phi^{-1}$ and by $\\pi_{\\zeta}$ the final policy obtained optimizing the objective in (10) when using the parametrized $\\psi$ and $\\phi^{-1}$. Lastly, let $F(\\zeta)$ be the expected cumulative reward of $\\pi_{\\zeta}$, i.e. $F(\\zeta) = \\mathbb{E}_{\\tau \\sim (\\mu, \\pi_{\\zeta}, P)}r(\\tau)$. We estimate the gradient $\\nabla F(\\zeta)$ as\n$\\mathbb{E}_{\\epsilon \\sim \\mathcal{N}(0,I_d)} [\\frac{1}{\\sigma} (F(\\zeta + \\sigma \\epsilon) - F(\\zeta - \\sigma \\epsilon))],$\nwhere $\\mathcal{N}(0, I_d)$ is the multivariate normal distribution, $d$ is the number of parameters, $F$ is an estimate of $F$, and $\\sigma > 0$ is a hyperparameter regulating the variance of the perturbations. We then use Adam (Kingma & Ba, 2015) to update the parameters $\\zeta$ using the estimated gradient."}, {"title": "4 Experiments", "content": "We carry out our experiments on continuous reinforcement learning tasks in MuJoCo and on LLM fine-tuning. In particular, due to computational efficiency constraints, we discover mirror maps in MuJoCo and report their performance when evaluated for an LLM finetuning task."}, {"title": "4.1 Environments", "content": "We consider three different preference optimization settings which we describe below. Each setting includes a starting policy, a set of preference datasets with different properties and an evaluation method for the trained policy. We apply ES to the first two settings and test the discovered objectives on the last one.\nHopper Our objective in Hopper is to learn a policy from scratch, i.e. with a randomly initialised policy, using a preference dataset. On Hopper, we train four reference agents of different skill levels, specifically, of respective expected cumulative reward (or value) of 900, 1200, 1800, and 2100 (the expert agent). Each dataset consists of 5120 rows, each with two trajectories of length 1000 starting from the same state. We generate a dataset by comparing trajectories by the expert agent (average return 2100), with other trajectories by one of our reference agents. A Bradley-Terry judge ranks each pair of trajectories, based on their true reward. The trained policy is then evaluated on the standard Hopper environment. To test the efficacy and robustness of our loss functions, we analyse a few variations of these standard datasets that represent common issues of real world data.\n\u2022 Base dataset: we compare a trajectory from the expert agent with one of a reference agent.\n\u2022 Noisy dataset: we flip a varying portion of the rankings in the base dataset.\n\u2022 Shuffled dataset: the comparisons are not always between the expert agent and a second agent. Namely, 25% of the comparisons are between two trajectories from the expert agent, 50% of the comparisons are between a trajectory of the expert agent and one of a reference agent, and 25% of the comparisons are between two trajectories of a reference agent.\n\u2022 Poor judge dataset: the judge is more likely to flip labels when the two trajectories have closer reward values. This is implemented as an increase in the temperature of the Bradley-Terry judge.\nAnt To further investigate a scenario similar to fine-tuning a large language model (LLM), we examine a setting in which a pre-trained agent is required to modify its behavior to achieve its goal with a stylistic constraint. Specifically, in the Ant environment, we take an agent that has been pre-trained on the standard Ant goal of moving forward, and enforce the objective of avoiding the use of one of its legs. This is accomplished by introducing the Three-legged-ant (TLA) environment, a modified version of Ant where utilizing the fourth leg results in significant penalties. We train one agent in the original Ant environment, achieving a reward of 6000, and another in the TLA environment, which achieves a reward of 3900. For comparison, the agent trained in the original Ant environment achieves a reward of 1700 when evaluated in the TLA environment. The dataset generation follows the same protocol as described for the Hopper environment, where trajectories are collected from both the Ant and TLA agents. However, in this setting the number of rows for each dataset is 1280, to account for the fact that we do not want the agent to learn from scratch, but to adjust its policy to a new instruction. The trained policy is then evaluated on the TLA environment.\nLLM tuning Finally, we evaluate one of our discovered objective functions on a real-world LLM fine-tuning task. To simulate a scenario involving mixed data quality, we fine-tune the gemma-7b model on a modified version of the dpo-mix-7k dataset, where half of the responses, selected at random, are replaced with responses generated by gemma-2b, a model that typically produces lower-quality responses compared to those originally present in DPO-mix. This approach replicates the shuffled dataset described for the MuJoCo experiments, aiming to simulate the challenges of training with datasets of varying quality, a common issue in fine-tuning tasks where it is difficult and costly to ensure uniformly high-quality data."}, {"title": "4.3 Results: Transfer", "content": "We show that PO objectives learnt with our methodology can transfer to other domains. In particular, the static objective discovered on the shuffled TLA dataset tranfers to both the shuffled Hopper dataset, where it obtains a final policy value of 1735\u00b139, and to the LLM-tuning task with shuffled data defined above. We train the base LLM on the modified DPO-mix dataset with both ORPO and the discovered objective. We observe that the first methodology achieves 57% accuracy on the test set while the latter achieves 62%. Additionally, we compare the two trained models with AlpacaEval (Li et al., 2023), and obtain a 53% winrate for the model trained with the discovered objective."}, {"title": "5 Related Work", "content": "Automatic Discovery of Preference Optimization Loss Functions Several works in the literature have shown that it is possible to discover machine learning algorithms that outperform algorithms manually designed by researchers (Oh et al., 2020; Lu et al., 2022; Jackson et al., 2024; Alfano et al., 2024). An approach particularly relevant to our method is DiscoPOP by Lu et al. (2024), which leverages an LLM to discover objective functions for LLM tuning. They consider a different space of objective functions from us, as they replace the log-sigmoid in (5) with a generic loss function, following the framework built by Tang et al. (2024). Additionally, instead of searching over a space of parametrized functions, they ask the LLM to generate loss functions in code space. This distinction suggests that our approaches could be complementary, as the model discovered by DiscoPOP could be paired with our learned mirror map. Lastly, DiscoPOP optimizes its objective function directly on the final task, whereas we adopt a two-stage process-optimizing the loss function on a separate task (MuJoCo) and later transferring it to the LLM setting. This transferability underscores the broader applicability of our approach.\nGeneralisations of DPO A generalization of DPO alternative to ours is f-DPO, developed by Wang et al. (2023), which consists in replacing the KL-divergence in (2) with an f-divergence and then apply the same heuristic as DPO to obtain the final objective function. We note that the KL-divergence is the only f-divergence to be also a Bregman divergence, and vice-versa. They empirically demonstrate that different f-divergences lead to different balances between alignment performance and generation diversity, highlighting the trade-offs inherent to this class of algorithms. Huang et al. (2024) further explore this class of PO algorithm and individuate an f-divergence for which f-DPO is robust to overoptimization."}, {"title": "6 Conclusion", "content": "We have introduced a novel framework for Policy Optimization algorithms, as well as a methodology for the automatic discovery of PO algorithms using evolution strategies. Through a systematic evaluation across diverse settings in MuJoCo environments, we demonstrated that our discovered objective functions consistently match or exceed the performance of existing methods, particularly in noisy and mixed-quality datasets where the ORPO baseline struggles. The introduction of temporally aware objective functions further improved performance, allowing the optimization process to vary and adapt during training. Analysing the landscape of the discovered objectives, we give an intuition that justifies improved performance as well as guidance in the design of new PO algorithms. Our results also indicate that the discovered objectives generalize beyond simple reinforcement learning environments, showing promising performance when transferred to LLMs, thereby confirming the broader applicability of our approach."}, {"title": "A Proof of Theorem 3.1", "content": "We provide here a proof for our main result, i.e. Theorem 3.1. The proof to obtain the DPO objective in (5) follow by taking $\\phi = e^x$\nTheorem A.1 (Theorem 3.1). Let $h_{\\phi}$ be a 0-potential mirror map and $\\pi^*$ be a solution to the optimization problem in (7). If $\\pi_{ref}(a|s) > 0$ for all $s \\in \\mathcal{S}, a \\in \\mathcal{A}$, we have that\nr(\\tau) = \\phi^{-1}(\\pi^*(\\tau)) - \\phi^{-1}(\\pi_{ref}(\\tau)) + c(s_0), (11)\nfor all trajectories $\\tau$, where $c(s_0)$ is a normalization constant that depends only on $s_0$.\nProof. We use the KKT conditions to solve (7), i.e.\n$\\pi^* \\in \\underset{\\pi}{\\operatorname{argmax}} \\mathbb{E}_{\\mathcal{D}_{\\pi_0},\\tau \\sim (\\pi,P)} \\sum_{t=0}^{\\mathcal{T}-1} \\mathbb{E}_{a \\sim \\pi(\\cdot|s_t)} [r(s_t, a)] - \\beta D_h(\\pi(\\cdot|\\tau), \\pi_{ref}(\\cdot|\\tau))\nWe use the stationarity condition to obtain the equation\n$\\nabla_{\\pi(\\tau)} \\bigg( \\sum_{t=0}^{\\mathcal{T}-1} \\mathbb{E}_{a \\sim \\pi(\\cdot|s_t)} [r(s_t, a)] - \\beta D_h(\\pi(\\cdot|\\tau), \\pi_{ref}(\\cdot|\\tau)) \\bigg)$\n$= -\\lambda \\bigg(\\sum_{\\tau':s_0 \\in \\tau'} \\pi(\\tau') - 1\\bigg) + \\alpha(\\tau) \\pi(\\tau)$\n= r(\\tau) - \\beta \\phi^{-1}(\\pi(\\tau)) + \\phi^{-1}(\\pi_{ref}(\\tau)) - \\lambda + \\alpha(\\tau) = 0,$\nfor all initial states $s_0 \\in \\mathcal{S}$ and for all trajectories $\\tau$ starting from $s_0$. Rearranging, we obtain that\n$\\pi(\\tau) = \\phi(r(\\tau) + \\phi^{-1}(\\pi_{ref}(\\tau)) - \\lambda + \\alpha(\\tau)).$\nSince $0 \\notin \\text{dom } \\phi^{-1}$, due to the definition of a 0-potential, and $\\pi_{ref}(\\tau) > 0$, we have that $\\pi(\\tau) > 0$ for all trajectories $\\tau$. Invoking the complementary slackness condition, whereby $\\alpha(\\tau)\\pi(\\tau) = 0$ for all trajectories $\\tau$, we have that $\\alpha(\\tau) = 0$ for all trajectories $\\tau$. Therefore, we have that\nr(\\tau) - \\beta \\phi^{-1}(\\pi(\\tau)) + \\phi^{-1}(\\pi_{ref}(\\tau)) - \\lambda = 0\nThe theorem statement is obtained by rearranging the last equation and denoting $c(s_0) = \\lambda$"}, {"title": "B Further discussion of $\\omega$-potentials", "content": "We show here two examples of Bregman divergence induced by an $\\omega$-potential mirror map, that is when $\\phi(x) = e^{x-1}$ and when $\\phi(x) = x$. If $\\phi(x) = e^{x-1}$, the associated mirror map is defined as\n$h_{\\phi}(\\pi(\\cdot|s)) = \\sum_{a \\in \\mathcal{A}} \\int_{\\omega}^{\\pi(a|s)} \\phi^{-1}(x)dx = \\sum_{a \\in \\mathcal{A}} \\int_{-\\infty}^{\\pi(a|s)} (\\log(x) + 1)dx$\n$= \\sum_{a \\in \\mathcal{A}} \\pi(a | s) \\log(\\pi(a | s)) - \\pi(a | s) + \\pi(a | s)$\n$= \\sum_{a \\in \\mathcal{A}} \\pi(a | s) \\log(\\pi(a | s)),$\nwhich is the negative entropy. Plugging this expression in the definition of Bregman divergence we obtain\n$D_h(x,y) = h(x) - h(y) - \\langle \\nabla h(y), x - y \\rangle$\n$= \\sum_{a \\in \\mathcal{A}} x_a \\log(x_a) - \\sum_{a \\in \\mathcal{A}} y_a \\log(y_a) - \\sum_{a \\in \\mathcal{A}} (\\log(y_a) - \\frac{y_a}{y_a}) (x_a - y_a)$\n$= \\sum_{a \\in \\mathcal{A}} x_a \\log(\\frac{x_a}{y_a}),$"}, {"title": "C Hyper-parameters", "content": "We give the hyper-parameters we use for training in Tables 4 and 5."}]}