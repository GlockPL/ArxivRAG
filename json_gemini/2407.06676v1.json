{"title": "Games played by Exponential Weights Algorithms", "authors": ["Maurizio D'Andrea", "Fabien Gensbittel", "J\u00e9r\u00f4me Renault"], "abstract": "This paper studies the last-iterate convergence properties of the exponential weights algorithm with constant learning rates. We consider a repeated interaction in discrete time, where each player uses an exponential weights algorithm characterized by an initial mixed action and a fixed learning rate, so that the mixed action profile \\(p^t\\) played at stage t follows an homogeneous Markov chain. At first, we show that whenever a strict Nash equilibrium exists, the probability to play a strict Nash equilibrium at the next stage converges almost surely to 0 or 1. Secondly, we show that the limit of \\(p^t\\), whenever it exists, belongs to the set of \"Nash Equilibria with Equalizing Payoffs\". Thirdly, we show that in strong coordination games, where the payoff of a player is positive on the diagonal and 0 elsewhere, \\(p^t\\) converges almost surely to one of the strict Nash equilibria. We conclude with open questions.", "sections": [{"title": "1 Introduction", "content": "Many machine learning algorithms used for prediction or decision-making are designed to optimize the behavior of a single agent facing an unknown environment. However, with the increasing use of these algorithms in various fields and the complexity of the problems at hand, interaction between these algorithms, designed for an agent unconscious of other players, have become common. This raises a natural question: where will these interactions lead?\nOur paper contributes to the large literature on learning algorithms in games. Precisely, we analyze the day to day behavior of the exponential weights (EW) algorithm with constant learning rates, when applied independently by all the players in a finite game.\nThe EW algorithm ([15, 10, 5, 3]) is one of the most popular and widely studied algorithm with applications in various domains and contexts: computational geometry, optimization, operations research, online statistical decision-making, machine learning (we refer to [4, 14, 2] for a general account on the subject). The idea behind EW is the following: at each stage \\(t > 0\\), each player i chooses an action at random according to their mixed strategy \\(p_i^t\\) and then observe their random vector payoff (one coordinate for each possible action of that player) which depends on the realized actions of the other players. The probability to play each action at stage \\(t + 1\\) for player i is proportional to the exponential of the sum of the payoffs he would have obtained by playing that same action at all past stages multiplied by a learning rate \\(\\eta_i\\)."}, {"title": "2 Model", "content": "Throughout the paper we fix a finite normal form game \\(G = (N, (A_i)_{i\\in N}, (u_i)_{i\\in N})\\), where \\(N = \\{1, ..., n\\}\\) is the set of players. For each i in N, \\(A_i\\) is the finite set of actions of player i and \\(u_i : \\prod_{j=1}^n A_j \\rightarrow \\mathbb{R}\\) is the payoff function of player i.\nNotations: \\(A = \\prod_{i=1}^n A_i\\) is the set of actions profiles of all players and for i in N, \\(A_{-i}\\) is the set \\(\\prod_{j\\in N\\backslash\\{i\\}} A_j\\) of action profiles of all players different from i. As usual, we write \\(\\Delta(A_i)\\) to represent the set of mixed actions of player i (probability distributions over \\(A_i\\)). \\(\\Delta = \\prod_{i\\in N} \\Delta(A_i)\\) is the set of mixed action profiles and the payoff functions are extended linearly to \\(\\Delta\\). We denote by \\(\\text{int}(\\Delta) = \\{p \\in \\Delta, \\forall i \\in N, \\forall a_i \\in A_i, p_i(a_i) > 0\\}\\) the relative interior of \\(\\Delta\\). We identify \\(p \\in A\\) with the product probability over \\(\\prod_{i\\in N} A_i\\) defined by \\(p(a) = \\prod_{i\\in N} p_i(a_i)\\). We also identify each pure action \\(a_i \\in A_i\\) with the Dirac mass at \\(a_i\\).\nThe game G will be played at stages \\(t = 0,1,...\\), and we assume that each player i uses an Exponential Weights Algorithm characterized by a learning rate \\(\\eta_i > 0\\) and"}, {"title": "3 Strict Nash Equilibria", "content": "Recall that a strict Nash equilibrium of G is a (necessarily pure) action profile a in A such that: for each i in N and \\(b_i \\neq a_i\\) in \\(A_i\\), \\(u_i(b_i, a_{-i}) < u_i(a)\\)."}, {"title": "4 Nash Equilibria with Equalizing Payoffs", "content": "It is well known that in a Nash equilibrium p, if for some player i two pure actions \\(a_i, b_i\\) in \\(A_i\\) are played with positive probability under \\(p_i\\), then the expected payoffs \\(u_i(a_i, p_{-i})\\) and \\(u_i(b_i, p_{-i})\\) coincide. In the following definition, we require this equality between payoffs to hold not only in expectation, but also almost surely.\nDefinition 4.1. A Nash Equilibrium with Equalizing Payoffs, or NEEP for short, is a Nash equilibrium p in \\(\\Delta\\) such that for each player i, for all \\(a_i, b_i\\) in the support of \\(p_i\\), for all \\(a_{-i}\\) in the support of \\(p_{-i}\\):\n\\(u_i(a_i, a_{-i}) = u_i(b_i, a_{-i}).\\)\nWe denote by \\(\\mathcal{P}\\) the set of NEEP."}, {"title": "5 Coordination Games", "content": "Recall the game in Example 3.5:\nHere we have 2 NEEP which are strict Nash Equilibria, and we showed that almost surely the EW process converges to one of them. Consider the extension to 3 actions per player, that is the game H given by:\nAn obvious conjecture is that here the play will almost surely converge to one of the 3 strict NE. Applying corollary 3.7, we get that almost surely either the play will converge to one of the 3 strict NE, or \\(d(p^t, Z) \\rightarrow 0\\), where\n\\(Z = \\{p \\in \\Delta, p((T,l)) = p((M,m)) = p((B,r)) = 0\\}.\\)\nZ is here a connected set, union of 6 segments:\n\\(Z =[(T, m), (T, r)] \\cup [(T, r), (M,r)] \\cup [(M, r), (M, l)]\\cup [(M, l), (B, l)] \\cup [(B, l), (B, m)] \\cup [(B, m), (T, m).\\)\nTo prove the conjecture we need to show that \\(d(p^t, Z) \\rightarrow 0\\) cannot happen with positive probability, and this is surprisingly not so simple. Formally, it will be a consequence of the following Theorem 5.2."}, {"title": "6 Simulations", "content": "Consider once again the game of Example 3.5:"}, {"title": "7 Open questions", "content": "We have shown that the Markov chain generated by the EW algorithm with constant learning rates converges almost surely to the set of strict Nash equilibria in the class of strict coordination games. Outside of this class, we know that the only possible limit points are the Nash Equilibria with Equalizing Payoff and that whenever a strict Nash equilibrium exists, then the probability to play this strict Nash equilibrium converges almost surely to 0 or 1. These results raise several interesting open questions and research directions for future works.\n1. What are the possible limits of the EW process? We have shown in Theorem 4.2 that any limit has to be a Nash Equilibrium with Equalizing Payoff. However, in example 4.3 only NEEP with maximal support can be obtained as a limit of the process. Does this property generalize to any game? In particular, can a pure, but not strict, Nash equilibrium be a limit of a EW process ?\n2. We have shown that the almost sure convergence of the EW process is not guaranteed. For instance, it does not hold for zero-sum games like Matching Pennies, which do not have a Payoff Equalizing Nash Equilibria. As a consequence existence is not granted for generic games. An interesting case is the one of common payoffs games: does EW always converge for games with common payoffs ? We have checked this convergence holds for common payoff games with 2 players and 2 actions for each player, but we do not know if the convergence extends to any number of actions and players.\n3. Computing the probability to converge to a given NEEP is difficult, it would be nice to have a non trivial example where an explicit formula can be obtained. More generally, in the class of strict coordination games, our results imply that there exists a random time T such that the players always play one of the strict Nash equilibria after stage T. Is it possible to have any quantitative estimate to bound T? Is it true that T is integrable?"}, {"title": "8 Convergence in Example 5.4", "content": "In this appendix, we will provide further details on the proof of convergence in example 5.4:\nWrite \\(p^t = (x^t, y^t)\\). In the first part, the proof is not very different from the proof of Theorem 5.2. In fact, introducing\nwe can show that if \\(M_0\\) is large enough, for all \\(t > 0\\) we have: \\(1_{Z_t \\geq M_0} E[Z_{t+1}|F_t] < Z_t1_{Z_t \\geq M_0}\\). Moreover, as in the proof of Theorem 5.2 we have\n\\(P_{p_0} (\\lim Z_t = \\infty) = 0.\\)\nNotice that if \\(p^t (SNE) \\rightarrow 0\\), since \\(P_{p_0}(\\lim_{t\\rightarrow\\infty} Z_t = \\infty) = 0\\), we get that \\(p^t = (x^t, y^t)\\) converges to the set\n\\(D := \\{(x, y) \\in \\Delta : x_3 = 1, y_3 = 0\\} \\cup \\{(x, y) \\in \\Delta : y_2 = 1, x_2 = 0\\}.\\)\nHowever, it implies that \\(y_3^t\\rightarrow 0\\), then by Lemma 2.2 it simple to show that \\(x_3^t \\rightarrow 0\\). Hence, the unique option is that \\(p^t\\) converges to the pure strategy (T, m) but it is in contradiction with Theorem 4.2, so \\(P_{p_0} (\\lim_{t\\rightarrow\\infty} p^t(SNE) = 0) = 0\\). It concludes the proof and shows that also in the Example 5.4 the dynamics induced by the EW converge a.s. to a SNE."}]}