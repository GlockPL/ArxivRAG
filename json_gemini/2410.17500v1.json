{"title": "Learning Fair and Preferable Allocations through Neural Network", "authors": ["Ryota Maruo", "Koh Takeuchi", "Hisashi Kashima"], "abstract": "The fair allocation of indivisible resources is a fundamental problem. Existing research has developed various allocation mechanisms or algorithms to satisfy different fairness notions. For example, round robin (RR) was proposed to meet the fairness criterion known as envy-freeness up to one good (EF1). Expert algorithms without mathematical formulations are used in real-world resource allocation problems to find preferable outcomes for users. Therefore, we aim to design mechanisms that strictly satisfy good properties with replicating expert knowledge. However, this problem is challenging because such heuristic rules are often difficult to formalize mathematically, complicating their integration into theoretical frameworks. Additionally, formal algorithms struggle to find preferable outcomes, and directly replicating these implicit rules can result in unfair allocations because human decision-making can introduce biases. In this paper, we aim to learn implicit allocation mechanisms from examples while strictly satisfying fairness constraints, specifically focusing on learning EF1 allocation mechanisms through supervised learning on examples of reported valuations and corresponding allocation outcomes produced by implicit rules. To address this, we developed a neural RR (NRR), a novel neural network that parameterizes RR. NRR is built from a differentiable relaxation of RR and can be trained to learn the agent ordering used for RR. We conducted experiments to learn EF1 allocation mechanisms from examples, demonstrating that our method outperforms baselines in terms of the proximity of predicted allocations and other metrics.", "sections": [{"title": "1 INTRODUCTION", "content": "The fair allocation of indivisible resources is a fundamental problem in mechanism design, extensively studied in both computer science and economics [1, 3, 5]. Fair division problems aim to allocate indivisible items fairly among agents who have individual preferences or valuations for the resources. Research has predominantly focused on the fair division of goods, where n agents assign non-negative values on m indivisible items [8]. Examples of good allocation include course assignments for students [28] and the general goods allocation approach employed by Spliddit [17], one of the most successful applications of fair division principles.\nResearchers have developed numerous allocation mechanisms or algorithms to address fair division problems under various fairness concepts. One well-known fairness criterion is envy-freeness (EF) [16], where no agent believes that another agent received a better allocation [1]. However, EF allocations do not always exist, as demonstrated by a simple counterexample: when there are two agents and a single good, any allocation results in envy from the unallocated agent. To address this, Lipton et al. [25] and Budish [6] proposed a relaxed fairness notion called envy-freeness up to one good (EF1), meaning that envy can be eliminated by removing a single good from the envied agent's allocation. EF1 allocations always exist and can be computed in polynomial time using the round robin (RR) mechanism [7]. In this algorithm, an order of agents is defined, and each agent, in turn, selects their most preferred item from the remaining items. Other algorithms and variants of fairness notions have also been explored, as described in surveys [1, 4].\nExpert algorithms without mathematical formulations are used in real-world resource allocation problems to find preferable outcomes for users. Although their goodness is not formally proven, these algorithms can use implicit or empirical knowledge in various domains [20]. For example, health providers allocate clinical resources based on subconscious knowledge, such as work ethics [23, 35]. As in the case of divide-and-choose, which appeared in Bible and was later shown to have good properties in EF, there is a possibility that these algorithms also have some good properties. Even if we can acquire expert knowledge, it is not immediately amenable to formalization. On the other hand, existing formal algorithms cannot skim off the top of implicit knowledge of experts.\nDesigning mechanisms that strictly satisfy good properties with replicating expert knowledge is challenging for three reasons. First, because these heuristics are difficult to formalize into precise mathematical expressions, translating them into theoretical frameworks is problematic. Second, formal algorithms can select an allocation that strictly satisfies good properties, but struggle to find one from a set of candidates that is preferable to experts. Third, human judgments can introduce biases, potentially leading to unfair allocations [19, 24], and thus directly mimicking such implicit rules can reproduce or increase undesired biases.\nIn this paper, we study how to learn implicit allocation rules from examples of allocation results while strictly satisfying fairness constraints. We significantly extended the general idea introduced by Narasimhan et al. [27] within the context of automated mechanism design (AMD). In their work, they proposed a framework that learns mechanisms from examples while ensuring constraint satisfaction. Given example pairs of reported valuations and allocations based on implicit rules, our goal is to train a parameterized fair allocation mechanism by capturing the relationship between inputs (valuations) and outputs (allocations). We optimize the parameters through supervised learning, minimizing the discrepancy between predicted and actual outcomes. This approach extends the prior work [27], adapting it to our problem, which addresses both the reproduction of implicit rules and the enforcement of fairness constraints. Instead of formalizing expert knowledge, our approach extracts their rules as a parameter through learning from examples."}, {"title": "2 RELATED WORK", "content": "The study of fair allocation of indivisible items has led to the development of various algorithms based on different fairness and efficiency notions. In addition to EF, other fairness concepts include the maximin share (MMS), which ensures that each agent receives an allocation at least as valuable as the least-valued subset of items they could obtain, assuming they divide the entire set and select the least valued subset for themselves [6]. Amanatidis et al. [2] proposed an approximation algorithm for MMS allocations. Pareto efficiency is another key efficiency notion, meaning that no alternative allocation can make some agents strictly better off without making any other agent strictly worse off. Caragiannis et al. [7] demonstrated that allocations maximizing Nash welfare yield both EF1 and Pareto-efficient allocations. However, these formal algorithms are static and cannot learn from examples.\nOur work contributes to the literature on AMD, a field first introduced by Conitzer and Sandholm [9, 10]. AMD focuses on automatically designing mechanisms by solving optimization problems, where objective functions correspond to social objectives and constraints model incentive properties [9, 10, 33]. For instance, in auction settings, the problem is often framed as maximizing expected revenue while satisfying conditions such as incentive compatibility and individual rationality [10-15, 29, 31, 34, 36, 39]. AMD also has many other applications, including facility location [18], data market design [32], and contract design [38]. However, these methods are typically fixed to explicit mathematical optimization problems and do not consider fitting implicit rules from examples.\nIn differentiable economics [14], researchers have designed neural networks to solve AMD problems. Pioneered by Duetting et al. [14], who introduced RegretNet, a line of AMD research focused on solving revenue-optimal auction problems using neural networks [11, 15, 29, 31, 36, 39]. However, the current study focuses on fair allocations. Mishra et al. [26] explored AMD for fair allocations using a neural network, but their method only approximately satisfies EF1, whereas ours rigorously satisfies the condition."}, {"title": "3 PRELIMINARIES", "content": "We use [n] to denote the set {1, ..., n} for $n \\in \\mathbb{N}$. A row vector is represented as $x = [x_1,...,x_d]$, and a matrix as $X = [X_{ij}]_{ij}$. The d-dimensional all-one vector and all-zero vector are denoted by $1_d$ and $0_d$, respectively. The element-wise product of two vectors, x and y, is written as $x \\odot y = [x_1y_1,...,x_dy_d]$. We define $x \\geq y \\Leftrightarrow x_i \\geq y_i, (\\forall i)$. A zero matrix with n rows and m columns is denoted by $O_{n,m}$. For a matrix $X \\in \\mathbb{R}^{n \\times m}$, X[i] denotes the i-th row vector, $X[:, j]$ denotes the j-th column vector, and $X[a: b, c : d] = [X_{ij}]_{i=a,...,b,j=c,...d}$ denotes the sub-matrix selected by row and column ranges. We use $1[\\cdot]$ to denote the indicator function. We study the standard setting of fair division of a set of indivisible goods $[m] = \\{1, 2, ..., m\\}$ among a set of agents $[n] = \\{1, 2, ..., n\\}$. A bundle is a subset of goods. An agent i has a valuation function $v_i : 2^{[m]} \\rightarrow \\mathbb{R}_{\\geq o}$ that assigns a non-negative real value to a bundle. We assume that the valuation function is additive; that is, we define $v_i(S) := \\sum_{j \\in S} v_{ij}$ for each bundle $S \\subseteq [m]$ where $v_{ij} := v_i(\\{j\\})$. A valuation profile $(v_1, . . ., v_n)$ is a collection of valuation functions, and we represent it by a matrix $V := (v_{ij})_{i\\in[n],j\\in[m]} \\in \\mathbb{R}_o^{n \\times m}$. We denote an integral allocation by $A = (A_1, ..., A_n)$, where $A_i \\subseteq [m]$ is the bundle allocated to agent i, and each good is allocated to exactly one agent, i.e., $A_i \\cap A_j = \\emptyset$ for all $i \\neq j$ and $\\cup_{i\\in[n]}A_i = [m]$. We also represent an integral allocation $(A_1, ..., A_n)$ by the matrix $A \\in \\{0, 1\\}^{n \\times m}$, where $A_{ij} = 1 [j \\in A_i]$. A fractional allocation is one in which some goods are allocated fractionally among agents. Unless otherwise specified, the term \"allocation\" refers to an integral allocation without explicitly using the word \u201cintegral.\u201d\nWe focus on envy-freeness (EF) and its relaxation as fairness concepts. An allocation is EF if every agent values their own allocation at least as highly as they value any other agent's allocation."}, {"title": "4 PROBLEM SETTING", "content": "Given an implicit allocation mechanism g, our goal is to find an allocation mechanism that approximates g, subject to EF1 constraint.\nWe formally define our problem by following the framework established in prior work [27]. Because g is an implicit rule, akin to a human heuristic, its explicit formulation of g is unavailable. Instead, we assume access to g through a dataset $S := \\{(V^1, A^1), ..., (V^L, A^L)\\}$, where $V^1, . . ., V^L$ are examples of valuation profiles sampled from an unknown distribution over the set of all valuation profiles, and $A^1 = g(V^1),..., A^L = g(V^L)$ are the corresponding allocation outcomes determined by g. Given the dataset S, our goal is to find the EF1 allocation mechanism that best approximates g:\n$\\min_{f \\in F_{EF1}} \\sum_{r=1}^L d(A^r, f(V^r))$,\nwhere $F_{EF1}$ is the set of all EF1 allocation mechanisms, and d(A, A') is a function that calculates the discrepancy between two allocation outcomes A and A'. Because the set of all EF1 allocation mechanisms is not explicitly identifiable, we follow existing research [27] and focus on searching over a subset of EF1 allocation mechanisms. Technically, we consider a parameterized subset of all the EF1 allocation mechanisms $F := \\{f_\\theta | \\theta \\in \\Theta\\} \\subset F_{EF1}$, where $\\Theta$ is a parameter from the parameter space. The problem is then solved by searching for $f_{\\theta^*}$ corresponding to the optimal $\\theta^*$: \n$f_{\\theta^*} := \\text{argmin}_{\\theta \\in \\Theta} \\sum_{r=1}^L d(A^r, f_\\theta(V^r)).$\nIn other words, we optimize the parameter $\\theta$ by minimizing the discrepancy between the predicted and the actual allocations."}, {"title": "5 PROPOSED METHOD", "content": "To solve problem in Equation (1), we propose a parameterized family of mechanisms $F$ based on RR [7], one of the EF1 allocation algorithms. RR allocates goods in multiple rounds, where, in each round, agents choose their most preferred goods from the remaining available items, following a specific order. RR's output depends on the order of agents, so we propose modeling $f_\\theta$ in Equation (1) through a neural network, where $\\theta$ as a learnable parameter. Specifically, we aim to construct a neural network that computes the agent order parametrically and then executes RR according to the order. By doing so, we can optimize the agent order by backpropagating errors through RR. Creating such a model is not straightforward because RR is a discrete procedure and is not directly suitable for gradient-based training. To address this challenge, we first propose a differentiable relaxation of RR, which incorporates the agent order. We then develop a neural network that integrates this relaxed RR, along with a sub-network to parametrize the agent order.\nWe describe our proposed method as follows. First, we briefly review the RR algorithm. Next, we present the differentiable relaxation of RR. Finally, we describe our neural network architecture, which incorporates the relaxed version of RR and a component for parametric computation of agent orders as the two building blocks."}, {"title": "5.1 Round Robin", "content": "RR consists of multiple rounds, during each of which agents 1,...,n sequentially pick their most preferred goods according to a predefined order. The entire procedure of RR is detailed in Algorithm 1. It can be shown through a straightforward proof that RR always produces an EF1 allocation.\nProposition 5.1 (Caragiannis et al. [7]). RR computes an EF1 allocation.\nPROOF. Consider two distinct agents i and j. Without loss of generality, assume i < j. Because i picks goods before agent j in each round, agent i receives more valuable goods than those allocated to agent j. As a result, agent i has no envy toward agent j. Envy may exist from j toward i. Now, consider the moment when i selects the first good o in the initial round. If we treat the execution of RR process for the remaining goods as a new process, then j picks before i in each subsequent round, eliminating j's envy toward i. As a result, j's envy dissipates once o is removed from $A_i$.\nRR can produce different EF1 allocations depending on the initial order of the agents. In other words, permuting the indices of the agents can result in different allocations as shown in Example 5.1."}, {"title": "5.2 Differentiable Relaxation of RR", "content": "To search over a subset of EF1 allocations as described in Equation (1), we aim to represent $f_\\theta$ using a neural network based on RR. Specifically, we seek to construct a neural network that parameterically computes the agent order and executes RR within the network, allowing us to tune the parameters to optimize the agent order. To achieve this, we must consider the differentiable relaxation of RR, enabling error backpropagation from predicted allocations. Without such relaxation, the discrete nature of RR makes it unsuitable as a direct network layer.\nFirst, we develop the differentiable relaxation of a single round of RR. Then, we introduce SoftRR, an algorithm for differentiable relaxation of RR."}, {"title": "5.2.1 Differentiable Relaxation of One Round", "content": "We first formally define one round. Consider a scenario with n agents and m goods. Let $V = (v_{ij})_{i\\in[n],j\\in[m]}$ represent the valuation of the n agents over the m available goods. The allocation obtained after one round is denoted as $r: V\\rightarrow A$. The resulting allocation from r is:\n$r(V) = [1 [j = \\text{argmax}_{j' \\in C_i} \\{v_{ij'}\\}]]_{i\\in[n],j\\in[m]}$,\nwhere the argmax operator breaks ties in favor of the earlier index. The set $C_i$ is defined as\n$C_i := \\begin{cases} [m] & (\\text{if } i = 1) \\\\ C_{i-1} \\backslash \\{\\text{argmax}_{j'\\in C_{i-1}} \\{v_{ij'}\\} \\} & (\\text{otherwise}) \\end{cases}$.\nThat is, $C_i$ represents the set of available goods left for the agent i after agents 1, 2, . . ., i \u2212 1 have selected their most preferred goods. Using $C_i$, Equation (2) calculates the resulting allocation obtained from one round.\nNext, we present the differentiable relaxation of the function r(.). The computation is detailed in Algorithm 2. Our relaxed function, denoted as srt, includes a temperature parameter \u03c4 > 0. Intuitively, we replace the argmax operator with a softmax that incorporates the temperature parameter \u03c4. The set $C_i$ is represented by a vector c where each element $c_j \\in [0, 1]$ satisfies $c_j \u2248 1$ if $j \\in C_i$, and $c_j \u2248 0$ otherwise. To simulate the argmax operator over the remaining goods, we apply softmax to the expression $(V[i] \u2013 min(V[i]) \u00b7 1 + 1) \\odot c$. The term (-min(V[i]) 1 + 1) distinguish between remaining goods and those already taken: the j-th element becomes approximately greater than 1 if $c_j \u2248 1$, while it remains close to zero if $c_j \u2248 0$.\nThe parameter \u03c4 controls the approximation precision. We formally prove that srt converges to r in the limit as \u03c4 \u2192 +0.\nProposition 5.3. Let $V \\in \\mathbb{R}^{n \\times m}$ be a valuation profile. Assume n\u2264 m and assume there are no ties in any row of V. That is, $\u2200 i \\in [n], \u2200 j, j'\\in [m], j \\neq j' \\Rightarrow V_{ij} \\neq V_{ij'}$. Then,\n$\\lim_{\\tau \\rightarrow +0} \\text{srt}(V) = r(V)$.\nPROOF. We set the following two loop invariants for the for-loop:\n(L1) After the i-th iteration, R[i] = r(V) [i].\n(L2) After the i-th iteration, for all $j \\in [m]$, $c_j = 1 [j \\in C_{i+1}]$.\nThese invariants hold trivially before the for-loop.\nNow, consider entering the i-th iteration, assuming (L1) and (L2) hold for the (i - 1)-th iteration. Let $z = (V[i] - min(V[i])1+1) \\odot c$."}, {"title": "5.2.2 Differentiable Relaxation of RR", "content": "Using srt() as a building block, we propose SoftRR, the algorithm that makes RR differentiable and enables it for backpropagation. The pseudo-code for this process is described in Algorithm 3.\nIntuitively, SoftRR, approximates the original RR by converting the multiple rounds into a single round with copied agents. To achieve this, we repeat V by $k = \\lceil m/n \\rceil$ times along the row direction using the function, defined as\n$V_{\\text{rep}} = \\text{repeat}(V, k) := \\begin{bmatrix} V & & \\\\ & \\ddots & \\\\ & & V \\end{bmatrix} (k \\text{ times}).$\nWe apply srt to $V_{\\text{rep}}$. The operations in Lines 1 to 3 simulate each agent i being replicated into k distinct agents, with each of k agents receiving goods individually. Finally, we sum the k allocations to consolidate them into a single agent in Lines 4 and 5.\nSoftRR incorporates the temperature parameter t from srt. We achieve results analogous to those in Proposition 5.3 as $\\tau\\rightarrow +0$.\nProposition 5.4. Let $V \\in \\mathbb{R}^{n \\times m}$ be a valuation profile. Assume m mod n = 0 and no ties exist in any row of V. Then,\n$\\lim_{\\tau\\rightarrow +0} \\text{SoftRR}(V) = RR(V)$.\nPROOF. Let $k = m/n \\in \\mathbb{N}$. Because $V_{\\text{rep}} \\in \\mathbb{R}^{kn \\times m}$ has no ties in any of its rows, $R_{\\text{rep}} = \\text{srt}(V_{\\text{rep}})$ computes the allocation result of one round for kn distinct agents and m goods exactly in the limit of $\\tau\\rightarrow +0$, by Proposition 5.3. Hence, Lines 4 and 5 produces the final allocation, which is identical to that of RR.\nWe present an example of the convergence of SoftRR with respect to t in Figure 1. We independently sampled $v_{i,j} \\sim U[0, 1]$, generating a valuation profile $V \\in \\mathbb{R}^{10 \\times 20}$. As the parameter \u03c4 decreases, SoftRR, converges to the output of RR."}, {"title": "5.3 NeuralRR", "content": "Since SoftRR is differentiable, we can optimize the agent order via backpropagation. Using SoftRR as a building block, we propose NRR, a novel neural network architecture that models $f_\\theta$ in Equation (1). Given a valuation profile as input, NRR computes the agent order parameterically, and then executes SoftRR to produce a fractional allocation. The architecture is shown in Figure 2.\nOur architecture models RR induced by a permutation parameterically computed from the input valuation. Specifically, NRR models $RR_{\\pi_\\theta(V)}(V)$ ($V$), where the input valuation profile V is transformed into a permutation $\\hat{\\pi} = \\pi_\\theta(V)$ by a sub-network $\u03c0_\\theta$ with learnable parameters \u03b8. The resulting allocation is then obtained through SoftRR using $\\hat{\\pi}$ and V. Because SoftRR supports backpropagation, the architecture can learn the agent order by minimizing the output errors and search over $F_{RR}$ to find the solution in Equation (1).\nThe computation of NRR proceeds as follows. First, the sub-network $\u03c0_\\theta (V)$ computes a permutation matrix $P \\in \\mathbb{R}^{n \\times n}$ representing $\\hat{\\pi}$ from the valuation profile V. To do this, $\u03c0^\\theta$ applies singular value decomposition to V to obtain agent-specific low-rank embeddings. Then, the row-wise minimum and maximum values of V are concatenated to the embeddings. These min and max values are explicitly calculated, as they represent fundamental features not easily captured by permutation-invariant models with a fixed-dimensional latent spaces, such as DeepSets [37, 40]. Next, each row is fed into a multi-layer perceptron to project it into a 1-dimensional space, resulting in an n-dimensional vector. To break ties among the values of this vector, we apply the tie-breaking function TieBreak(a) := a+rank(a), which adds a \u2208 $R^n$ to rank(a) := $[\\#\\{j | a_j < a_i \\text{ or } a_j = a_i \\text{ and } j < i\\}\\}]_{i}.\\textsuperscript{1}$ Finally, the permutation matrix P is computed using SoftSort [30]: SoftSort, (a) = $(-\\frac{1}{\u03c4'} (\\text{sort}(a) - 1_n) \\odot 1_n)^2$, where $X^2 := [X_{ij}^2]_{i,j}$ represents the element-wise square, and the softmax is applied row-wise. 1 has the same dimension as a. SoftSort provides a continuous relaxation of argsort operator, computing a permutation matrix that sorts the input vector a. After computing P, we multiply it by the input valuation to reorder the agents. This gives the allocation result SoftRR(PV). Next, we compute $P^T \\text{SoftRR}(PV)$ to restore the original agent order, and normalize each column to obtain the final matrix $\u00c2 \\in \\mathbb{R}^{n \\times m}$. Because \u00c2 is normalized column-wise, all goods are fractionally allocated to the agents.\nNRR operates differently during training and inference. Specifically, during training, NRR produces fractional allocations, whereas during inference, it generates integral allocations because we use hard computations for both P and SoftRR. As a result, during inference, NRR is equivalent to $RR_{\\pi_\\theta(V)}(V)$, and thus it rigorously satisfies EF1 for all parameters \u03b8. While NRR relaxes the problem in Equation (1) by outputting fractional allocations during training, it still adheres to the original problem because it can be used as an EF1 allocation mechanism at any point during training."}, {"title": "5.4 Loss Function", "content": "To train NRR, we use a column-wise cross-entropy loss function. Specifically, we define the loss function d in Equation (1) as\n$d(A, \u00c2) := \\sum_{j=1}^m \\text{CE} (A[:, j], \u00c2[:, j])$,\nwhere $\\text{ICE}(y, \\hat{y})$ represents the cross-entropy loss between the one-hot vector y and a probability vector $\\hat{y}$. We back-propagate this\n$\\textsuperscript{1}$This function is not strictly differentiable with respect to a, but we detach the term rank(a) from the computational graph and treat it as a constant. error through SoftRR and optimize the parameters using a standard gradient decent method."}, {"title": "6 EXPERIMENTS", "content": "We conducted experiments to compare the effectiveness of our proposed method with baseline methods in learning EF1 allocation mechanisms from examples."}, {"title": "6.1 Experimental Setting", "content": "Synthetic Data. We synthesized datasets for good allocations by modeling i's valuation of good jas $v_{ij} = \\mu_i + \\varepsilon_{i,j}$, where $\u03bc_i$ and $\u03b5_{i,j}$ denote the average valuation of agent i and random error, respectively. We sampled $\u03bc_i$ and $\u025b_{i,j}$ independently and identically from $U[1, 2]$ and $U[0, 0.01]$, respectively. This low-rank valuation model is commonly used to represent human preferences as seen in recommender systems [22]. We generated 100 samples for the training, validation, and test datasets.\nWe synthesized allocation results using the maximum utilitarian welfare (MUW) rule as an implicit allocation rule. This mechanism was selected for comparison with other methods across several metrics in addition to proximity to the correct allocation results. The MUW rule outputs the allocation that maximizes the sum of agent valuations:\n$\\text{MUW}(V) := \\text{argmax}(\\text{UW}(V, A) | A \\in \\{0, 1\\}^{n \\times m} \\text{ is an allocation}\\}$,\nwhere $\\text{UW}(V, A) := \\sum_{i=1}^n v_i (A_i)$ denotes the utilitarian welfare achieved by allocation A. We computed the solutions using the Gurobi optimizer [21]. For training and validation data, we set the number of agents n = 15 and the number of goods m = 5, or n = 30 and m = 10. We tested models trained on n = 15, m = 5 using datasets consisting of n = 15 agents and m = 5k (1 \u2264 k \u2264 6). For models trained on n = 30 and m = 10, we tested them using datasets with n = 30 agents and m = 5k (2 \u2264 k \u2264 12).\nBaselines. We use two models as baselines for comparison. The first is the original RR, which does not reorder the agents. The second is EEF1NN, a fully convolutional neural network proposed by Mishra et al. [26]. EEF1NN takes a valuation matrix as input and outputs a fractional allocation during training or an integral allocation during inference. Because EEF1NN does not inherently satisfy EF1, Mishra et al. [26] introduced a loss function that includes the EF violation. In line with this, we define the loss function by adding"}, {"title": "6.1.3 Evaluation Metrics", "content": "We used three evaluation metrics to assess the performance of the models.\nFirst, we computed Hamming distance (HD):\n$\\text{HD} (A, \\hat{A}) := \\frac{1}{2nm} \\sum_{i=1}^n \\sum_{j=1}^m |A_{ij} \u2013 \\hat{A}_{ij}|$.\nUsing HD, we evaluated the ability of NRR to predict allocations that are close to the correct ones. We multiplied by 1/(2m) for normalization, because the maximum value is 2m. Second, following existing work [26], we evaluated the ratio of the number of predicted allocations that are EF1 to the total number of test samples. We denote this ratio by\n$\\text{EF1Ratio} := |\\{\\hat{A} | \\hat{A} \\text{ is EF1}\\}|/(\\text{# Test instances})$.\nThird, we calculated welfare loss. Because we used MUW to generate the target allocations, we evaluated how much welfare the predicted allocations gained. Formally, we defined the utilitarian welfare loss (UWLoss) as\n$\\text{UWLoss}(V, \\hat{A}) := 1 \u2013 \\text{UW}(V, \\hat{A})/\\text{MUW}(V)$.\nNote that we did not train the models incorporating the welfare loss. This is because our motivation is to test whether we can learn implicit allocation rules through examples, which we instantiated as MUW in this experiment. That is, we cannot explicitly formulate the implicit rules in practice, and thus cannot a priori know if such rules consider the welfare function. However, our model is independent of loss functions, and therefore we can also incorporate such welfare metrics into the loss function in Equation (1)."}, {"title": "6.1.4 Hyper-parameters", "content": "For NRR, we fixed the rank to 3 for the agent low-rank embedding dimension and trained for 20 epochs with a batch size of 4. We selected the two temperature parameters \u03c4, \u03c4' for both SoftRR and SoftSort from {1.0, 0.1, 0.01}, which yielded the minimum average HD on the validation data. We excluded hyper-parameters that did not result in a decrease in training loss. For EEF1NN, we trained it for 20 epochs with batch size of 4 and set \u03bb = 1.0, which is the midpoint of the range [0.1, 2.0] considered in prior research [26]."}, {"title": "6.2 Results", "content": "Figure 3 shows the evaluation metrics for various numbers of agents and goods.\nFor the results with n = 15 agents, our proposed method yielded a lower HD compared to RR and EEF1NN. Notably, the difference between NRR and RR was most pronounced in the ranges where m < n and n < m < 2n, with the difference being larger in the m < n case than in n < m < 2n. RR and NRR are EF1 allocation mechanisms by construction, and EF1Ratio remained 1.0. In contrast, EEF1NN failed to output EF1 allocations in nearly all cases as reported in prior work [26]. UWLoss exhibited a similar pattern to HD. The results for n = 30 showed similar trends to those for n = 15, with the performance gap between RR and NRR narrowing compared to the n = 15 case.\nWe examined the agent orders learned by NRR, as described in Figure 4. For each valuation profile V in the test dataset, we compared the predicted order P = $\u03c0^\u03b8$(V) in NRR with the order based on the mean valuation $[m^{-1} \\sum_j v_{ij}]_{i}$. Specifically, we calculated Kendall's \u03c4 between the two orders for test instances and compared it to the fixed order 1, 2, ..., n used in RR. The results confirmed that, on average, NRR computed orders closer to mean valuations than the fixed order used by RR. Furthermore, we visualized an example of the two orders. We analyzed the changes in agent orders from the mean valuation order to the learned order by plotting points (x, y), where the agent ranked at position x in the mean valuation order is positioned at rank y in the learned order. In these examples, the learned order closely aligned with the mean valuation, specifically at the higher and lower ranks."}, {"title": "6.3 Discussion", "content": "The experimental results demonstrate that our model outperforms the baselines across all the three evaluation metrics.\nThe improvements in HD and UWLoss can be attributed to both the data characteristics and NRR's ability to optimize the agent order. Because valuations were generated as $v_{ij} = \\mu_i + \\varepsilon_{ij}$, each training allocation typically favors the agent $i^* = \\text{argmax}_i \\{\\mu_i\\}$ with the highest average valuation. Thus, optimizing the agent order to place $i^*$ near the top helps approximate RR effectively for the MUW rule. Specifically, when $kn < m < (k + 1)n$ for some k \u2208 N, the optimized order can allocate (k+1) goods to $i^*$, while a random order may only allocate k goods. Conversely, when m = kn for some k \u2208 N, executing RR with random orders can yield the same Hamming distance because it allocates k goods to $i^*$ regardless of the agent order. Additionally, as k increases, the difference between k+1 and k diminishes, making HD between the two orders converge. UWLoss correlates with the characteristics of HD because as more goods are allocated to $i^*$, UWLoss increases. This relationship is illustrated in Figure 5. We created a valuation profile V where n = 15 and i* = 7 with $v_{i,1} > ...> v_{i,m}$ for all i. When $m \u2260 kn$ for any k, RR induced by the highest mean valuation gives more goods to i* than RR induced by the original agent indices, while both give equal number of goods when n = m. Note that we did not include RR based on the mean valuations because the experiments assume we cannot know in advance that the MUW rule is the implicit rule; therefore, we cannot predefine the order.\nAs shown in Figure 4, NRR can learn the order based on mean valuation, resulting in similar performance to the order. The performance gap between RR and NRR narrowed for n = 30 compared to n = 15, as NRR's order estimation ability declined when n = 30. Further improvements for larger n values are left for future work."}, {"title": "7 CONCLUSION AND LIMITATION", "content": "We studied learning EF1 allocation mechanisms through examples based on implicit rules. We first developed SoftRR for differentiable relaxation of RR, and proposed a neural network called NRR based on SoftRR. We conducted experiments with synthetic data and compared NRR to baselines. Experimental results show that our architecture can learn implicit rules by optimizing agent orders. Improvement for larger number of agents are left future work."}]}