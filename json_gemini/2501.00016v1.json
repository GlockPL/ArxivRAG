{"title": "Predicting Crack Nucleation and Propagation in Brittle Materials Using Deep Operator Networks with Diverse Trunk Architectures", "authors": ["Elham Kiyani", "Manav Manav", "Nikhil Kadivar", "Laura De Lorenzis", "George Em Karniadakis"], "abstract": "Phase-field modeling reformulates fracture problems as energy minimization problems and enables a comprehensive characterization of the fracture process, including crack nucleation, propagation, merging and branching, without relying on ad-hoc assumptions.\nHowever, the numerical solution of phase-field fracture problems is characterized by a high computational cost. To address this challenge, in this paper, we employ a deep neural operator (DeepONet) consisting of a branch network and a trunk network to solve brittle fracture problems. We explore three distinct approaches that vary in their trunk network configurations. In the first approach, we demonstrate the effectiveness of a two-step DeepONet, which results in a simplification of the learning task.\nIn the second approach, we employ a physics-informed DeepONet, whereby the mathematical expression of the energy is integrated into the trunk network's loss to enforce physical consistency. The integration of physics also results in a substantially smaller data size needed for training. In the third approach, we replace the neural network in the trunk with a Kolmogorov-Arnold Network and train it without the physics loss. Using these methods, we model crack nucleation in a one-dimensional homogeneous bar under prescribed end displacements, as well as crack propagation and branching in single edge-notched specimens with varying notch lengths subjected to tensile and shear loading. We show that the networks predict the solution fields accurately and the error in the predicted fields is localized near the crack.", "sections": [{"title": "1. Introduction", "content": "The accurate prediction of fracture phenomena in brittle materials is of crucial importance in practical applications. Griffith's seminal work in 1920 [1] laid the groundwork for brittle fracture theory; according to Griffith, crack propagation is dictated by the balance between the consumption of energy associated to the creation of new surfaces and the release of bulk elastic energy as the crack propagates. More recently, Griffith's criterion was recast in a variational form [2] giving rise to a free discontinuity problem. Regularization of this problem led to the variational phase-field approach to fracture [3, 4], which gained enormous popularity over the past two decades [5] and became an increasingly powerful framework for predicting a variety of fracture problems including thermal [6], drying-induced [7], hydraulic [8] and ductile fracture [9], among many others. The approach introduces a continuous damage or phase-field variable varying between 0, representing intact conditions, and 1, representing fully damaged conditions. The displacement and phase fields are governed by a system of non-linear coupled partial differential equations (PDEs), which arise from the stationarity conditions of the total energy functional, i.e. as necessary conditions for the system to reach a state of minimum energy. This model leads to unprecedented flexibility in the simulation of complex crack behaviors, such as nucleation, branching, and merging, with no need for explicit crack tracking, thereby greatly simplifying implementation. However, it introduces a small length scale which is computationally expensive to resolve in a discretized setting. Moreover, non-convexity of the energy functional requires special attention in the numerics [10].\nMachine learning recently emerged as a powerful set of tools for modeling of complex material behavior including fracture, see the recent review in [11]. In particular, physics-informed neural networks (PINNs) [12], a versatile framework for solving forward and inverse problems [13, 14, 15, 16, 17], were also explored in the context of fracture modeling within the phase-field framework [18, 19], as well as"}, {"title": "2. Phase-field modeling of brittle fracture", "content": "In this section, we briefly describe the phase-field fracture model for isotropic brittle materials under quasi-static loading which we adopt in this work. Let \u03a9 \u2282 Rd, d\u2208 1,2,3 be an open bounded domain denoting an arbitrary body with external boundary \u0393, possibly containing a crack set \u0393c (see Figure 1(a)). We denote the Dirichlet and Neumann portions of the boundary as \u0393D and \u0393N, respectively, such that \u0393D\u222a\u0393N = \u0393 and \u0393D\u2229 \u0393N = \u2205. \u0393D in turn generally consists of a homogeneous Dirichlet boundary, \u0393D,0, and a nonhomogeneous Dirichlet boundary, \u0393D,1. \u0393N,1 denotes the nonhomogeneous Neumann boundary. The displacement of a material point x in the body is denoted by u(x), and we assume infinitesimal deformations so that the strain tensor is given by \u03b5 = sym(\u2207u). The material is linear elastic and isotropic, with the strain energy density function given by\n\u03a8(\u03b5) =  \\frac{\u03bb}{2} tr^2 (\u03b5) + \u03bc\u03b5\u00b7 \u03b5,\nwhere \u03bb and \u03bc are the Lam\u00e9 constants. In the phase-field fracture model, the energy functional for a"}, {"title": "3. Operator learning through DeepONets", "content": "DeepONet is a specialized neural network framework engineered to approximate operators i.e., mappings between infinite-dimensional function spaces that are crucial for solving parametric PDEs [22]. The input to a DeepONet can be initial conditions, boundary conditions, source functions etc. and it maps an input function, denoted here by f \u2208 F, to the corresponding PDE solution g \u2208 G, where F and G are appropriately defined function spaces [22], by approximating the operator G : F \u2192 G.\nThe DeepONet architecture consists of two distinct subnetworks:\nBranch Network: This subnetwork encodes the input functions. To encode a function f, the function is first sampled at a finite set of sensor points {x1,x2,...,xk}. The function values, (f(x1), f(x2),..., f (xk)), are input into the branch network, which transforms them into a latent representation b\u2208 Rp, where p represents the latent dimension. For a set of input functions {f1, f2,..., fm}, the output of the network is B \u2208 Rm\u00d7p where the ith row of B is the latent representation of fi.\nTrunk Network: This subnetwork encodes the spatial coordinates of points at which the output of the operator is to be evaluated. Specifically, it takes a spatial coordinate x as input and generates a coordinate-dependent basis \u2208 Rp. For a set of spatial coordinates {x1,x2,...,xn}, the trunk network generates an output \u03a6\u2208 Rn\u00d7p where the jth row of \u00de is the basis corresponding to xj."}, {"title": "3.1. Two-step DeepONet training method", "content": "Training both the trunk and branch networks simultaneously typically requires solving a complex optimization problem in a high-dimensional space, which is both non-convex and non-linear, making the training process challenging. To address this issue, a two-step training method has been proposed [29], which breaks down the entire optimization problem into two simpler subproblems.\nThe sequential two-step training approach (see Figure 3) begins with the training of the trunk network by minimizing the following loss function with respect to \u03b8\u03c4, A\u03b1, Au, and Av in a two-dimensional problem:\nLTdata (\u03b8\u03c4, A\u03b1, Au, Av) = \u03bb\u03b1 lossa + \u03bbu lossu + \u03bbv lossv\n\u03bb\u03b1 ||\u03b1\u03b8 \u2013 \u03b1True ||L2 + \u03bbu ||u\u03b8 \u2013 uTrue ||L2 + \u03bbv ||v\u03b8 \u2013 vTrue ||L2,\n\u03b1\u03b8 = (\u03a6(\u03b8\u03c4)A\u03b1)T, u\u03b8 = (\u03a6(\u03b8\u03c4)Au)T, v\u03b8 = (\u03a6(\u03b8\u03c4)Av)T.\nwhere (0) is the output of the trunk network, and Aa, Au, and Av \u2208 Rp\u00d7m are trainable matrices. The arrays \u03b1\u03b8, us, and ve represent the intermediate field predictions generated solely by the trunk network. Correspondingly, the arrays \u03b1True, uTrue, and vTrue are m\u00d7n arrays, containing the ground truth data for damage and displacement fields.\nIn addition, \u03bb\u03b1, \u03bb\u03b1, and \u03bb\u03c5 are trainable weights that are adjusted during the training process, refer to Appendix A. After optimization, let \u03b8\u2217\u03c4 , A\u2217\u03b1, A\u2217u, and A\u2217v denote the optimized parameters and matrices, where \u03a6(\u03b8\u2217\u03c4) is assumed to be full rank. To prepare for the next step, either QR decomposition or singular value decomposition (SVD) of \u03a6(\u03b8\u2217\u03c4) must be performed. In this study, we utilize the QR decomposition:"}, {"title": "3.2. Physics-informed two-step DeepONet", "content": "Incorporating physics into the DeepONet framework enhances its ability to model complex systems and makes the network prediction robust, accurate, and adhering to the governing physical laws. In a physics-informed two-step DeepONet, physics is enforced by modifying the trunk network's loss function as follows:\nCT (\u03b8\u03c4, A\u03b1, Au, Av) = Adata lossdata + Aphysics lossphysics,\n= Adata LTdata (\u03b8\u03c4, A\u03b1, Au, Av) + Aphysics E(u\u03b8, \u03b1\u03b8).\nwhere u\u03b8 is the displacement field with components u\u03b8 and v\u03b8, and Adata and Aphysics are weighting factors that balance the importance of physical constraints and the data-driven loss. Subsequently, the branch network is trained using the trunk network's output as described in Section 3.1."}, {"title": "3.3. DeepOK\u0391\u039d", "content": "The DeepOKAN architecture incorporates KANs in the operator learning framework. KANs are based on the Kolmogorov-Arnold representation theorem, which provides a powerful mathematical foundation for approximating multivariate continuous functions. This theorem states that any mul- tivariate function f(x), where x is a point in a bounded domain, can be represented as a finite composition of univariate continuous functions. A generalized KAN architecture is described by the sequence of layer sizes [n1,...,nk+1], where K represents the number of layers in the network. In the"}, {"title": "3.4. Operator network design and training", "content": "The DeepONet framework supports a variety of architectures for the branch and trunk networks. In this work, we use an MLP as a branch network, and investigate the application of two different networks, an MLP and a KAN, as trunk networks (Figure 2). We demonstrate the performance of these networks for two sets of problems: (a) a one-dimensional homogeneous bar with prescribed displacement at the boundaries, and (b) single-edge notched two-dimensional specimens with differ- ent boundary conditions and modeling assumptions. For the bar problem, the prescribed boundary displacement is the input to the branch network. For the problems with the notched specimen, the length of the initial crack-like notch, represented by an initial phase field, is the input to the branch network. Leveraging the monotonically increasing discrete loading procedure, we train separate net- works to represent the solution fields at different loading levels, i.e. for each prescribed displacement Ut, t \u2208 {0,1,2, ...} (with Uo = 0), we train a network that approximates the operator Gt: A\u2192 Gt where Ao is the set of phase fields corresponding to the notches of different sizes and Gt is the set of the solution fields, that is the displacement and phase fields, at time t. Since the input to Gt is ao \u2208 Ao and not the solution fields in the previous loading step, unlike in previous work [32], our approach does not suffer from the problem of error accumulation.\nFurthermore, we employ the robust two-step DeepONet training approach, schematized in Figure 3, to train the DeepONet with an MLP as the trunk network. In the first approach of data-driven two- step DeepONet training, the trunk network loss function includes only the data loss, with no physics- informed component. In the second approach of physics-informed two-step DeepONet training, the trunk network loss function incorporates physics-based loss terms in addition to the data loss. In training the DeepOKAN, we do not employ the two-step training and train the branch and trunk networks concurrently, with the trunk network implemented as a KAN and the branch as an MLP. In all approaches, the networks are trained using the data generated from FE simulations. See Appendix B for the details of the FE computations.\nIn the physics-informed training, the computation of E and its minimization poses challenges due to the nonconvexity and complexity of the energy functional, requiring careful optimization. To efficiently compute and minimize the energy functional, we utilize the following techniques:\n\u2022 Instead of employing automatic differentiation to compute gradients, we mesh the domain and compute the gradients of the fields as well as perform the integration in (1) following the com- putational approach based on the finite element (FE) method described in Appendix C. This approach provides a computationally efficient and geometrically intuitive way to capture spatial variations in field values. Note that the computation of energy and its minimization requires the evaluation of the solution fields across the entire spatial domain simultaneously."}, {"title": "4. Prediction of fracture with data-driven two-step DeepONets", "content": "In the data-driven framework, the trunk network is trained using only the data loss in (11) corre- sponding to the displacement and phase fields. This section presents four case studies to evaluate the effectiveness of the two-step DeepONet in predicting crack nucleation and propagation. We start by studying crack nucleation in a one-dimensional homogeneous bar (Case 1). Subsequently, we study crack propagation in single-edge notched (SEN) specimens subjected to tensile loading (Case 2), and crack propagation with kinking in the same specimens under shear loading (Case 3). We also study crack branching in the same specimens, again under shear loading (Case 4). For the first two cases, the one-dimensional bar and the SEN specimens with tensile loading, FE data is generated using the AT2 damage model, which leads to an immediate onset of damage at the beginning of loading. For the next two cases, the SEN specimens with shear loading leading to crack kinking and branching, we generate FE data using the AT1 damage model, which leads to a linear-elastic behavior with no dam- age in the initial stage of loading. These two examples demonstrate the versatility of the framework in handling both sharp and gradual damaging behaviors, thus encompassing a broader range of fracture mechanics scenarios. For all examples, unless otherwise specified, the networks are trained using the Adam optimizer [44] with a learning rate of 10-4, a mean squared error (MSE) loss function, and the tanh activation function is used throughout the architecture."}, {"title": "4.1. One-dimensional homogeneous bar (Case 1)", "content": "In this section, we study the one-dimensional homogeneous bar shown in Figure 4. Here, we set E = 1.0, Gc = 0.01, and l = 0.1. The data set comprises the phase and displacement fields at 101 equispaced nodes in the discretization of the domain for 50 equispaced applied displacements such that Ut \u2208 {0.01, 0.02,\u2026\u2026,0.5}. The data set is divided into training and test sets, with the data for the first 45 values of Ut used for training and the data for the last 5 values of Ut reserved for testing. A two-step DeepONet architecture is employed, consisting of a branch network, which processes the applied displacement values, and a trunk network, which operates on the coordinates of the nodes. The trunk network has three hidden layers, each containing 101 neurons, while the branch network consists of five hidden layers, each with 101 neurons. The dimension of the outputs of both the networks is 45.\nThe network is trained to predict the phase field \u03b1 and the displacement field u with a learning rate of 10-5."}, {"title": "4.2. Case 2: Crack propagation in an SEN specimen under tensile loading", "content": "In this example, we demonstrate the ability of DeepONet to predict crack propagation by train- ing the network on FE data for SEN specimens with varying initial notch sizes subjected to tensile loading (Figure 8). For this problem, we set E = 1.0, \u03bd = 0.3, Gc = 1.0, and 1 = 0.01. In this case, the branch network processes the phase field associated with the initial notches, while the trunk network receives the spatial coordinates of 33749 points as input. The trunk network is composed of 7 hidden layers, each containing 1001 neurons, while the branch network comprises 9 hidden layers and 1001 neurons. The final layer of each network is configured with 45 neurons. The dataset consists of the displacement and phase fields for 50 specimens, each with a distinct initial notch size, and each subjected to 40 equispaced applied displacements such that Ut \u2208 {0.005,0.01,\u2026\u2026,0.2}. The data for 45 randomly selected specimens are used for training, while the data for the remaining 5 specimens are reserved for testing."}, {"title": "4.3. Case 3: Crack propagation in a SEN specimen under shear loading", "content": "In this example, the ability of the network to predict crack kinking is demonstrated using a net- work trained on FE data for SEN specimens with varying initial notch lengths subjected to shear loading (Figure 11). For this problem also, we set E = 1.0, \u03bd = 0.3, Gc = 1.0, and 1 = 0.01. Here, the branch network processes the phase field associated with the initial notches and the trunk network processes 149383 sensor points. The structure of the trunk network and branch network follows the same architecture as in Case 2. The dataset consists of the displacement and phase fields for 50 speci- mens, each with a distinct initial notch size, and each subjected to 50 equispaced applied displacements such that Ut \u2208 {0.01, 0.02, \u2026\u2026\u2026, 0.5}. Data from 45 randomly selected initial notch sizes are utilized for training, while data from the remaining 5 initial notch sizes are allocated for testing. The phase and displacement fields are predicted for all 50 applied displacements.\nFigure 12 displays the predicted phase and displacement fields at the applied displacement values of Ut = 0.01, Ut = 0.4, and Ut = 0.5 for one of the test specimens. The networks accurately predict the propagation of a crack with kinking. Remarkably, the network also predicts the nucleation of a crack at the bottom right corner of the specimen, as observed from the plot for Ut = 0.4). Furthermore, unlike in the case with tensile loading, the phase field does not exhibit an unrealistic smooth decay at the crack tip. Small errors in the predicted fields suggest that the network accurately approximates the operator."}, {"title": "4.4. Case 4: SEN under shear loading (branching)", "content": "This example demonstrates the ability of the network to predict crack branching. For FE simulation of crack branching under shear loading of the same SEN described in Section 4.3, we purposefully do not apply the decomposition of the strain energy density in (1) such that \u03a8+ = \u03a8 and \u03a8\u2212 = 0. Although this is an artificial example (in the sense that the induced branching is unphysical), it demonstrates the capability of the approach to learn and predict crack branching. The dataset consists of displacement and phase fields for 50 specimens, each with a distinct initial notch size, and each subjected to 50 uniformly spaced applied displacements, Ut \u2208 {0.01,0.02,\u2026, 0.5}. In this case, similar to Case 2 and Case 3, the branch network processes the phase field data corresponding to 45 randomly selected initial notches for training and the remaining 5 samples for testing, while the trunk network processes coordinate points. The architecture of the network is very similar to that in Case 2 and Case 3 utilizing the same number of layers and neurons for both the trunk and branch networks. However, the last layer of each network now consists of 101 neurons.\nThe performance of the two-step DeepONet in predicting crack propagation with branching is illustrated in Figure 13, which displays predicted \u03b1, u, and v. The network predicts the crack path correctly. However, while the fields away from crack are predicted accurately, some error is observed in the vicinity of the crack, pointing to difficulties in learning the sharply varying fields near the crack accurately."}, {"title": "4.5. Accuracy of predictions", "content": "The mean absolute errors (MAEs) using the 5 specimens reserved for testing for the three two- dimensional cases\u2014tensile (Case 2), shear (Case 3), and shear with branching (Case 4)\u2014are pre- sented in Table 2. It is evident that the MAEs for the displacement field components u, v, and the phase field \u03b1 are generally on the order of 10-3 or 10-4, suggesting good generalizability of the trained networks."}, {"title": "4.6. Basis functions", "content": "We also investigate the basis functions obtained during the training of the trunk network. As detailed in Section 3.1, we employ the QR decomposition to derive the orthonormal basis functions for the trunk network during training. However, visualizing these basis functions directly from the"}, {"title": "5. Prediction of fracture with physics-informed two-step DeepONet", "content": "In this section, we discuss the performance of the physics-informed two-step DeepONet for the cases of crack propagation in SEN specimens subjected to tensile loading (Case 2) and shear loading (Case 3). Following the damage models used in data generation, we use the AT2 damage model for the tensile loading case and the AT1 damage model for the shear loading case. Again, the input to the trunk network is spatial coordinates, while the branch network processes the phase field associated with the initial notches. It is worth noting that physics-informed DeepONet is trained using only 10 samples, each with a different initial crack size, and tested on 5 additional samples.\nFigures 16 and 17 compare the behavior of the loss functions in the trunk and branch networks within the physics-informed two-step DeepONet. The plots in Figure 16 illustrate how the trainable weights for lossu help balance the contributions of the losses throughout training, as lossu is rel- atively small compared to the other losses. Figures 17 shows that the initial value of los"}]}