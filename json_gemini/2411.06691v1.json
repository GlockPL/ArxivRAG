{"title": "Autonomous Droplet Microfluidic Design Framework with Large Language Models", "authors": ["Dinh-Nguyen Nguyen", "Raymond Kai-Yu Tong", "Ngoc-Duy Dinh"], "abstract": "Droplet-based microfluidic devices have substantial promise as cost-effective alternatives to current assessment tools in biological research. Moreover, machine learning models that leverage tabular data, including input design parameters and their corresponding efficiency outputs, are increasingly utilised to automate the design process of these devices and to predict their performance. However, these models fail to fully leverage the data presented in the tables, neglecting crucial contextual information, including column headings and their associated descriptions. This study presents \u00b5-Fluidic-LLMs, a framework designed for processing and feature extraction, which effectively captures contextual information from tabular data formats. \u03bc-Fluidic-LLMs overcomes processing challenges by transforming the content into a linguistic format and leveraging pre-trained large language models (LLMs) for analysis. We evaluate our \u03bc-Fluidic-LLMs framework on 11 prediction tasks, covering aspects such as geometry, flow conditions, regimes, and performance, utilising a publicly available dataset on flow-focusing droplet microfluidics. We demonstrate that our \u00b5-Fluidic-LLMs framework can empower deep neural network models to be highly effective and straightforward while minimising the need for extensive data preprocessing. Moreover, the exceptional performance of deep neural network models, particularly when combined with advanced natural language processing models such as DistilBERT and GPT-2, reduces the mean absolute error in the droplet diameter and generation rate by nearly 5- and 7-fold, respectively, and enhances the regime classification accuracy by over 4%, compared with the performance reported in a previous study. This study lays the foundation for the huge potential applications of LLMs and machine learning in a wider spectrum of microfluidic applications.", "sections": [{"title": "1. Introduction", "content": "Droplet-based microfluidics has been recognised as a groundbreaking technology for miniaturising biological and chemical experiments. It has significantly advanced biotechnology 1-5 by enabling techniques such as next-generation sequencing, 6-8 single-cell RNA sequencing, 8-10 droplet digital PCR, 11-14 and liquid biopsies diagnostics. 15 However, the impact of microfluidics remains largely confined to single-use cartridges, integrated bench-top devices, and specialised lab setups. 16,17 In addition, the complex design and fabrication of custom microfluidic devices have limited their widespread adoption and general use.18,19 Moreover, microfluidic design and operation can take months or years of iterative testing to optimise, even if fabrication is outsourced at a high cost.20 To overcome these limitations, machine learning, which predicts patterns and behaviour, has been employed.\nMachine learning models are becoming more popular for predicting performance and automating design in microfluidic droplet generation. For instance, Mahdi et al.21 used machine learning to predict the size of water droplets in glycerine oil from a T-junction setup. The model, trained with 742 data points, accurately predicted droplet size using Reynolds and capillary numbers across different flow rates and fluid properties within one geometry. Furthermore, in Lashkaripour et al.,22 neural networks were used to predict droplet size, generation rate, and flow regime based on design geometry and flow conditions. The neural networks, which were trained on 888 data points with varying capillary numbers, flow rate ratio, and six geometric parameters, accurately predicted the droplet generation regime (95.1% accuracy), size (error < 10 \u00b5m), and generation rate (error < 20 Hz) for droplets ranging from 25 to 250 \u00b5m in size and 5 to 500 Hz in rate. Elsewhere, in Damiati et al.,23 a machine learning model predicted the size of poly(lactic-co-glycolic acid) (PLGA) microparticles produced by flow-focusing droplet generators and dichloromethane solvent evaporation. The model was trained on data from 223 combinations of flow rates, PLGA concentrations, device types, and sizes to predict PLGA particle size (R\u00b2 > 0.94) accurately. Furthermore, Hong et al.24 applied machine learning models to automate the design of concentration gradient generators. A neural network trained on 9 million data points from a verified physics model was able to map desired concentration profiles to inlet settings, achieving an 8.5% error rate. Meanwhile, Ji et al.25 applied machine learning to automate the iterative design of grid micromixers. Neural networks were trained on 4,320 simulated chips to map channel lengths to output concentrations. The designs met outlet concentration targets within 0.01 mol/m\u00b3, compared with simulations, for"}, {"title": "2.1 Data and Tasks", "content": "We utilised a dataset on flow-focusing droplet microfluidics, 22 structured in tabular form and comprising 998 data points. This dataset includes 11 features: orifice width, normalised orifice length, normalised water inlet width, normalised oil inlet width, expansion ratio, aspect ratio, flow rate ratio, capillary number, droplet diameter, generation rate, and regime. Using this dataset, we conducted two regression tasks and one classification task focused on performance prediction, as well as eight regression tasks related to design automation. For the design automation aspect, we present the results of one regression task, with the remaining results in the supplementary file."}, {"title": "2.2 Text Serialisation for Performance Prediction and Design Automation", "content": "Text serialisation is the process of converting structured data into a linear, text-based format that can be easily stored, transmitted, and reconstructed. In LLMs, text serialisation plays a vital role in processing and managing vast amounts of data30. These models rely on serialised text data to train on diverse datasets, often including a mix of natural language, structured information, and metadata. By converting complex input data into a serialised text format, LLMs can uniformly process this information, enabling them to generate meaningful predictions, responses, or completions based on the input. Furthermore, the serialised text is essential for fine-tuning models with specific data, allowing the LLMs to adapt to new tasks or domains by processing serialised input and output pairs. 31,32 Our framework implements manual text serialisation for individual data samples, as shown in Fig. 2. By leveraging the column headers of tabular data, the framework systematically transforms each table row into a sequence of key-value pairs, with the keys originating from the column headers and the values corresponding to the respective data entries. For performance prediction, as shown in Fig. 2A, the eight column headers and their corresponding values are transformed into a paragraph. This text is then input into pre-trained LLMs to generate embeddings. These embeddings are"}, {"title": "2.3 Text Embeddings", "content": "Text embedding is a technique used to convert textual data into a dense, fixed-size vector representation. These vectors capture the semantic meaning of the text by mapping words, phrases, or entire documents into a continuous vector space, where similar pieces of text are represented by vectors that are close to each other. The primary advantage of text embedding lies in its ability to represent complex linguistic relationships and contextual meanings in a numerical format that can be efficiently processed by machine learning algorithms. This makes text embedding an essential component in various natural language processing (NLP) tasks, such as sentiment analysis, text classification, and machine translation.33"}, {"title": "2.4 Large Language Model Selection", "content": "LLMs play a pivotal role in generating high-quality text embeddings by leveraging their understanding of linguistic patterns and contextual relationships. Pre-trained language models, such as BERT,34 GPT-2,35 and their variants, are particularly effective in producing embeddings because they are trained on vast text corpora and can capture nuanced semantic information. These models transform input text into embeddings by processing it through multiple layers of neural networks, where each layer refines the representation by focusing on different aspects of the language, such as syntax, semantics, and context. The resulting embeddings are highly informative and can be used as input for downstream NLP tasks, enhancing the performance of models in applications like question-answering document retrieval and text summarisation.\nIn our study, it was essential to carefully select the language model backbones that would be employed. While there are many potential options, we focused on DistilBERT,36"}, {"title": "2.5 Baseline Machine Learning Models", "content": "To assess the effectiveness of text embeddings compared with non-text embeddings in downstream tasks, we employed a range of standard machine learning models, including XGBoost, 38 LightGBM,39 and support vector machine (SVM).40 Additionally, DNNs, an advanced form of traditional neural networks characterised by the addition of multiple hidden layers, 41 were designed and applied in this study. Subsequently, we optimised the hyperparameters for all baseline models, as well as for their respective combinations with pre-trained language models."}, {"title": "2.6 Evaluation Metrics", "content": "After finalising the models, we performed 10-fold cross-validation 15 times to obtain more robust and reliable mean performance estimates, thereby reducing the variability introduced by random data partitioning.42 For regression tasks, the evaluation utilised four metrics: mean absolute error (MAE), mean squared error (MSE), root mean squared error (RMSE), and the coefficient of determination (R2), along with their associated standard errors. For the classification task, five metrics were employed: accuracy, F1 score, precision, recall, and area under the receiver operating characteristic curve (ROC AUC), each accompanied by standard errors. Here, we present line graphs comparing MAE across models for regression and accuracy across models for classification. Line graphs for the remaining metrics are included in the supplementary file. In addition, we provide tables in the \u2018Results & discussion' section"}, {"title": "3. Results & discussion", "content": "To verify the effectiveness of our \u00b5-Fluidic-LLMs framework, we highlight selected results in this section, with the remaining findings provided in the supplementary file."}, {"title": "3.1 Enhanced Efficiency for Performance Prediction", "content": ""}, {"title": "3.1.1 Performance in Prediction of Droplet Diameter", "content": ""}, {"title": "3.1.2 Performance in Prediction of Droplet Generation Rate", "content": ""}, {"title": "3.1.3 Performance in Prediction of Droplet Regime", "content": ""}, {"title": "3.2 Enhanced Efficiency for Design Automation", "content": ""}, {"title": "4. Conclusion", "content": "In this paper, we present \u00b5-Fluidic-LLMs, an innovative framework designed to process tabular data associated with droplet microfluidics by transforming it into a text-based representation that effectively incorporates essential contextual information, including column descriptions. Our findings underscore the critical role of model selection and language model integration in achieving optimal efficiency of performance prediction and design automation. We emphasise the superior performance of DNN models, particularly when integrated with advanced NLP models such as DistilBERT and GPT-2. In comparison, traditional ensemble methods like LightGBM and XGBoost, even when incorporating state-of-the-art NLP approaches, may struggle to achieve comparable performance levels.\nMultiple research directions can be pursued to further advance and optimise our framework. For example, exploration of various text serialisation methods, regularisation techniques, dataset augmentation, or advanced optimisation algorithms could provide means to enhance model performance and reduce error rates. Furthermore, given the flexibility of our framework to accommodate different language models, it can be utilised with the latest LLMs, such as GPT-4,44,45 PaLM 2,46 Gemini 1.5,47 and LLaMA 3,48 to assess variations in overall performance. Moreover, \u00b5-Fluidic-LLMs is a general framework that can be adapted for use with other microfluidic components, such as micromixers,49 and extended to non-microfluidic structures like 3D-printed lattices50, thereby enabling the automation of intricate design processes. In addition, \u00b5-Fluidic-LLMs can be seamlessly integrated with existing microfluidic computer-aided design tools51-59 to enable more sophisticated and advanced design automation. We anticipate that this work will inspire further research into the application of language technologies across a broader range of microfluidic applications."}, {"title": "Conflicts of interest", "content": "There are no conflicts to declare."}]}