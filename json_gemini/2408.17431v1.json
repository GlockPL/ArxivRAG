{"title": "ADVANCING MULTI-TALKER ASR PERFORMANCE WITH LARGE LANGUAGE MODELS", "authors": ["Mohan Shi", "Zengrui Jin", "Yaoxun Xu", "Yong Xu", "Shi-Xiong Zhang", "Kun Wei", "Yiwen Shao", "Chunlei Zhang", "Dong Yu"], "abstract": "Recognizing overlapping speech from multiple speakers in conversational scenarios is one of the most challenging problem for automatic speech recognition (ASR). Serialized output training (SOT) is a classic method to address multi-talker ASR, with the idea of concatenating transcriptions from multiple speakers according to the emission times of their speech for training. However, SOT-style transcriptions, derived from concatenating multiple related utterances in a conversation, depend significantly on modeling long contexts. Therefore, compared to traditional methods that primarily emphasize encoder performance in attention-based encoder-decoder (AED) architectures, a novel approach utilizing large language models (LLMs) that leverages the capabilities of pre-trained decoders may be better suited for such complex and challenging scenarios. In this paper, we propose an LLM-based SOT approach for multi-talker ASR, leveraging pre-trained speech encoder and LLM, fine-tuning them on multi-talker dataset using appropriate strategies. Experimental results demonstrate that our approach surpasses traditional AED-based methods on the simulated dataset LibriMix and achieves state-of-the-art performance on the evaluation set of the real-world dataset AMI, outperforming the AED model trained with 1000 times more supervised data in previous works.", "sections": [{"title": "1. INTRODUCTION", "content": "Although automatic speech recognition (ASR) [1, 2, 3] has achieved excellent performance in quiet, single-speaker scenarios, it still faces significant challenges in multi-talker conversational scenarios, especially in the case of overlapping speech. To overcome this challenge, a series of multi-talker ASR approaches have been proposed [4, 5, 6, 7, 8]. One of the most representative methods is serialized output training (SOT) [8, 9, 10]. The core idea of SOT is to concatenate the transcriptions of multiple speakers in the order of their speech emission times, separated by a speaker change symbol. Compared to permutation invariant training (PIT) [5, 6, 7], SOT avoids the limitation on the maximum number of speakers, models the dependencies in multi-talker content, and reduces computational complexity, resulting in better performance on multi-talker ASR task.\nHowever, in SOT-style transcriptions, the concatenation of related content from multiple speakers, coupled with the relatively poor grammatical structure of sentences in meeting discussions, necessitates strong long-context awareness and cross utterance modeling. This is precisely what previous SOT methods based on attention-based encoder-decoder (AED) [8], which relied more on encoder performance, lacked, leading to performance bottlenecks. For instance, in [11], despite using 900K hours of large-scale simulated data for pre-training, the word error rate on the AMI [12] meeting corpus still reached 21.2%.\nLarge language models (LLMs) [13, 14, 15, 16], trained on vast amounts of text data, possess unparalleled capabilities in understanding and generating natural language. Their proficiency in long-context awareness makes them exceptionally well-suited for SOT-style transcriptions. Therefore, the combination of LLM and SOT-based multi-talker ASR is a perfect match. A series of LLM-based ASR works [17, 18, 19, 20, 21, 22] have been conducted, which, in contrast to traditional AED methods that focus on encoder performance, tend to treat the speech foundation encoder [23, 24, 25, 26] in LLM-based models as a tool for extracting embedding. The speech embedding then serve as prompt for the LLM, relying on the powerful decoder-only LLM to generate transcription. These studies have shown that this approach can match or slightly outperform traditional AED methods in simple single-speaker ASR tasks [18, 21]. However, in these works, the performance advantage of the LLM-based methods is not particularly pronounced, indicating that LLM-based models, with their powerful decoders, have not fully realized their potential in handling speech tasks in simple scenarios.\nTherefore, in this paper, motivated by the potential of powerful LLMs to handle challenging speech tasks in complex scenarios and the natural compatibility of LLMs with SOT, we propose an LLM-based approach for multi-talker ASR. Similar to previous LLM-based ASR works, we employ a architecture comprising a pre-trained speech encoder, a projector, and an LLM. In previous works, various training strategies have been employed. For example, in [18], low-"}, {"title": "2. METHOD", "content": "rank adaptation (LoRA) [27] was introduced into the LLM to facilitate efficient fine-tuning, and all three components were fine-tuned together in a single stage. In [21], LoRA was not introduced, and the encoder was frozen while training only the projector, which also yielded satisfactory results. In [22], a multi-stage fine-tuning approach was used to better align the modalities of speech and text. In this paper, we compared the aforementioned training strategies on the simulated LibriMix dataset and synthesized the best practices to propose the most suitable strategy, which made our LLM-based method surpass the AED-based approach. On the evaluation set of the real-world meeting corpus AMI, the proposed LLM-based method not only surpasses AED-based methods trained with the same amount of data but also remarkably outperforms the AED model trained on an enormous scale of 900K hours (1000 times more) of supervised data, achieving state-of-the-art. This astounding result demonstrates the immense potential of LLM-based models in handling speech processing tasks in challenging scenarios."}, {"title": "2.1. Serialized Output Training", "content": "Serialized output training (SOT) is an elegant method to address multi-talker ASR. During the training stage, the transcriptions of different speakers are concatenated using a speaker change symbol to create the reference transcription for the overlapping speech. The concatenation order follows the emission time of each speaker, known as first-in first-out (FIFO). For example, as shown in Fig. 1, in the case of three speakers, the reference transcription Y is given as\nR = {r_{1}^{1},\u2026\u2026,r_{N_{1}}^{1}, $, r_{1}^{2},\u2026\u2026, r_{N_{2}}^{2}, $, r_{1}^{3},\u2026\u2026, r_{N_{3}}^{3} }, where r_{i}^{j} represents the i-th token of the j-th speaker, Nj represents the number of tokens in the j-th speaker, and \"$\" represents the speaker change symbol."}, {"title": "2.2. LLM-Based SOT for Multi-Talker ASR", "content": "In previous works, attention-based encoder-decoder (AED) architectures have been employed to implement SOT-based multi-talker ASR. Considering that SOT-style transcription involves concatenating potentially related utterances from multiple speakers, the model requires strong long-context awareness and the ability to model across utterances. Unlike AED architectures that use cross attention to obtain recognition sequences, LLM architectures directly utilize their powerful decoders, which have undergone extensive pre-training, to generate text. Therefore, LLM-based models are likely better suited for this complex and challenging task. Given these considerations, we propose an LLM-based model to further overcome the performance bottlenecks of SOT-based multi-talker ASR.\nAs shown in Fig. 2, the architecture for LLM-based multi-talker ASR mainly consists of a speech encoder, a projector, and an LLM. For each sample, given the overlapped speech signal $S_{olp}$ and the corresponding SOT-style multi-talker transcription $T_{multi}$, a speech encoder is first used to convert the overlapped speech signal into a speech representation, which can be represented as:\n$H^{s} = Encoder(S_{olp})$ (1)\n$H^{s} \u2208 R^{f_{s}\u00d7l_{s}}$ is the speech representation, where $f_{s}$ and $l_{s}$ denote the feature dimension and the length, respectively. $H^{s}$ can be very long, making it difficult for the LLM to process and increasing the computational burden. Therefore, we stack every n consecutive frames in the feature dimension to downsample the representation, denoted as:\n$H^{\\prime s} = Downsampler(H^{s})$ (2)\nwhere $H^{\\prime s} \u2208 R^{(f_{s}\u22c5n)\u00d7l^{\\prime s}}$ is the output after downsampling. The length of $H^{\\prime s}$ is $l^{\\prime s}$, which is more suitable for the LLM. The dimension of the speech representation is expanded by a factor of n. Then, a projector is introduced to convert the"}, {"title": "3. EXPERIMENTS", "content": "We first conducted experiments on the modified simulated dataset LibriMix [28], where each utterance contains only 2 speakers with a time delay between them. Then, we evaluated our model on the real-world meeting scenario dataset AMI [12], where each meeting in the evaluation set contains up to 4 speakers."}, {"title": "3.1. Experiment with LibriMix", "content": ""}, {"title": "3.1.1. Dateset and evaluation metric", "content": "We used LibriMix\u00b9 modified by ESPnet [29] for preliminary experiments. LibriMix is a simulated dataset obtained by mixing single-speaker speech from LibriSpeech [30] with noise from WHAM! [31, 32]. The official LibriMix is used for the source separation task, where the simulation process typically assumes fully-overlapped speech, meaning that speech from different speakers starts at the same time. To make it suitable for the multi-talker ASR task, the original simulation process is modified in the ESPnet pipeline\u00b9 to introduce a random delay ranging from 1 to 1.5 seconds for the mixed speech. The final generated simulated data contains approximately 830 hours of speed-perturbed training set, 8.2 hours of development set, and 7.6 hours of test set, with two speakers in all utterances.\nIn the LibriMix experiment, to compare with the results from ESPnet, we used word error rate (WER) as the evaluation metric. This metric is directly calculated between the predicted and reference SOT-style multi-talker transcriptions."}, {"title": "3.1.2. Model configuration", "content": "We utilized WavLM\u00b2 [25] as the speech encoder because both the Base+ and Large versions of WavLM leverage a substantial amount of overlapped speech data for self-supervised pre-training, making them suitable for the multi-talker ASR task. The LLM module chosen was Vicuna-7B3 [16], a chat model fine-tuned from the pre-trained LLaMA [14, 15] on conversational data collected from ShareGPT users. The downsampling rate n was set to 10, resulting in speech embedding with frames of 200 ms length. Two linear layers acted as projectors with ReLU activation in between, and the hidden size was set to 4096. We used Vicuna tokenizer in all systems."}, {"title": "3.1.3. Training strategy and detail", "content": "In previous works on LLM-based ASR, different training strategies were employed. In [18], the speech encoder, projector, and LoRA adaptor were trained together. In [21], the LORA was not introduced, the speech encoder was frozen, and only the projector was trained. In [22], the three modules were unfrozen in three stages, following the order of projector \u2192 speech encoder \u2192 LoRA. In the LibriMix experiment, we adopted a multi-stage training strategy similar to that in [22]. The benefit of this multi-stage training is that it enhances the model's capacity to align auditory and textual information. A slight difference in our approach is that when using the WavLM model fine-tuned with LibriMix, the training process requires freezing the speech encoder.\nWe used 8 NVIDIA V100 32GB GPUs for training, with a batch size of 2 samples per GPU and a gradient accumulation of 4. The DeepSpeed strategy [33] was used for distributed training. We employed the AdamW optimizer [34] with a learning rate of 0.0001, betas of (0.9, 0.999), epsilon of 1e-08, and weight decay of le-6. A linear warmup scheduler was used, with 2000 warmup steps and a maximum of 100,000 training steps, but training was stopped early if the validation loss did not decrease. We applied this training configuration in each training stage. When training the LLM, we only performed LoRA fine-tuning with alpha = 16 and rank = 16. In all experiments, greedy search was used for decoding."}, {"title": "3.1.4. Experimental results", "content": "Table 1 shows our results comparing various approaches on LibriMix. Sys. {1-3} are the results from ESPnet. Among these, using a conformer as the encoder and the WavLM Large model as upstream achieves better results because the WavLM model has been self-supervised pre-trained on large-scale overlapped speech, making it more suitable for multi-talker scenarios. Sys. {4-5} in Table 1 are the results of fine-tuning the WavLM model using AED approach. The performance of the WavLM Large model is significantly better than that in ESPnet, since the WavLM in the latter is frozen. Sys. {6-8} in Table 1 are the results of the LLM-based approach proposed in this work. When using WavLM Base+ as the speech encoder, the LLM-based method (Sys. 6, Tab. 1) outperforms the AED-based method (Sys. 4, Tab. 1). However, when WavLM Large is used as the encoder, the AED-based method shows a significant performance boost (Sys. 5, Tab. 1), even surpassing the LLM-based method (Sys. 7, Tab. 1), which indicates that AED-based systems are more dependent on encoder performance. Initializing the LLM-based system with the speech encoder fine-tuned on LibriMix using AED method results in the best performance (Sys. 8, Tab. 1). Therefore, in the performance on LibriMix test set, the advantage of the LLM-based system over the AED-based system is not very pronounced (9.0% WER in Sys. 8 vs. 9.2% WER in Sys. 5). This is similar to conclusions drawn from single-speaker ASR studies [18, 21], as LibriMix is simulated data and contains only two speakers per utterance, making it less challenging compared to real conversational scenarios.\nTable 2 shows the performance comparison of different speech encoders with and without LoRA fine-tuning. Similar to the conclusions in [18] and [22], introducing LoRA fine-tuning into the LLM consistently improves performance regardless of the speech encoder used. This indicates that LORA fine-tuning can adapt the LLM output to the style of SOT-based multi-talker transcription. In [21], promising performance can be achieved even without introducing the LoRA adaptor, possibly because the transcription style of the single-talker Librispeech used in [21] is similar to the output of the original LLM.\nTable 3 presents the impact of freezing the speech encoder during training. When the initialized speech encoder is not fine-tuned on LibriMix using the AED method, freezing the encoder results in poor performance because the encoder has not adapted to the LibriMix dataset. However, when using the encoder fine-tuned with LibriMix (Sys. 5, Tab. 1), freezing the encoder during training results in better performance. This is likely because the fine-tuned encoder already has excellent representation extraction capabilities on LibriMix and does not require further adjustment.\nFig. 3 shows the comparison of training curves in the first training stage, where only the projector module is trained, using either a fine-tuned encoder or a non-fine-tuned encoder. When using the fine-tuned encoder, the model quickly converges to a very high accuracy. In contrast, using the original WavLM model results in slower and less complete convergence. This indicates that if we have a high-quality encoder, simply aligning the modality of speech representations with the LLM can directly achieve a relatively good performance. Conversely, for an unadapted encoder, merely training the projector to perform alignment is insufficient, which is similar to the conclusion in Table 3.\nTable 4 presents the comparison between single-stage and multi-stage training strategies. The results show that, regardless of whether the speech encoder is frozen, multi-stage training outperforms single-stage training. This indicates that multi-stage training helps the model better align auditory and textual information."}, {"title": "3.2. Experiment with AMI", "content": ""}, {"title": "3.2.1. Experimental settings", "content": "To evaluate the LLM-based multi-talker ASR approach in a more realistic setting, we conducted experiments on real-world corpus AMI. The AMI meeting corpus includes approximately 95 hours of real-world meeting recordings, with the training, validation, and evaluation sets comprising 76.9, 8.9, and 8.7 hours, respectively. Each meeting involves 3 to 5 participants. The audio in the AMI corpus was recorded using an 8-channel microphone array, known as multiple distant microphones (MDM). Typically, the first channel is used for monaural ASR evaluation, referred to as the single distant microphone (SDM) setting. Additionally, the AMI corpus includes near-field single-speaker audio recorded by independent headset microphones (IHM) worn by each participant.\nIn this work, we conducted experiments using the SDM setting. However, in the original SDM, the audio is segmented by oracle timestamps into utterances containing only a single speaker. To evaluate SOT-based multi-talker ASR, we followed the approach in [11] to use utterance group-based evaluation. An utterance group is defined as a set of utterances connected by speaker overlap regions. Correspondingly, SOT-style transcriptions are generated in the order of the emission time of each speaker."}, {"title": "3.2.2. Experimental results", "content": "The overall experimental results on the AMI-SDM evaluation set are presented in Table 5. Sys. {1-3} are from previous work, all relying on large-scale supervised data for pre-training. As shown by the experimental results, in terms of the average cpWER metric, the LLM-based approach (Sys. 5, Tab. 5) not only outperforms the AED-based method using the same amount of data (Sys. 4, Tab. 5) but also remarkably surpasses the models in Sys. {1-3} that were trained with large-scale supervised data. It is worth mentioning that Sys. 1 in Table 5 was trained using 900k hours of supervised data, which is 1000 times more than what we used. This demonstrates that for SOT-based multi-talker ASR task, having a robust, large-scale pre-trained decoder is more important, as it provides strong capabilities in long-context awareness and cross-utterance modeling. This is precisely the advantage of LLM-based architectures over traditional AED-"}, {"title": "4. CONCLUSIONS", "content": "In this paper, we pioneer an LLM-based multi-talker ASR approach. In the evaluation, the proposed method achieves state-of-the-art results on both the simulated data LibriMix and the real-world data AMI, even outperforming existing methods trained with 1000 times more supervised data on the AMI-SDM evaluation set. The experimental results demonstrate that LLM-based architectures, which emphasize decoder performance and possess strong capabilities in understanding long contexts and modeling across utterances, outperform AED-based structures that focus more on encoder performance in SOT-based multi-talker ASR task. The LLM-based method has a much larger advantage on real data AMI than on simulated data LibriMix, which further highlights the potential of LLM-based models in handling speech processing tasks in complex and challenging scenarios."}]}