{"title": "HREF: HUMAN RESPONSE-GUIDED EVALUATION OF INSTRUCTION FOLLOWING IN LANGUAGE MODELS", "authors": ["Xinxi Lyu", "Yizhong Wang", "Hannaneh Hajishirzi", "Pradeep Dasigi"], "abstract": "Evaluating the capability of Large Language Models (LLMs) in following instruc-tions has heavily relied on a powerful LLM as the judge, introducing unresolvedbiases that deviate the judgments from human judges. In this work, we reevaluatevarious choices for automatic evaluation on a wide range of instruction-followingtasks. We experiment with methods that leverage human-written responses andobserve that they enhance the reliability of automatic evaluations across a widerange of tasks, resulting in up to a 3.2% improvement in agreement with humanjudges. We also discovered that human-written responses offer an orthogonal per-spective to model-generated responses in following instructions and should beused as an additional context when comparing model responses. Based on theseobservations, we develop a new evaluation benchmark, Human Response-GuidedEvaluation of Instruction Following (HREF), comprising 4,258 samples across 11task categories with a composite evaluation setup, employing a composite evalu-ation setup that selects the most reliable method for each category. In addition toproviding reliable evaluation, HREF emphasizes individual task performance andis free from contamination. Finally, we study the impact of key design choicesin HREF, including the size of the evaluation set, the judge model, the baselinemodel, and the prompt template. We host a live leaderboard that evaluates LLMSon the private evaluation set of HREF12.", "sections": [{"title": "1 INTRODUCTION", "content": "Automatic evaluations of instruction following abilities in Large Language Models (LLMs) hasrecently received significant attention (Zheng et al., 2023; Li et al., 2023; Lin et al., 2024; Li et al.,2024; Chiang et al., 2024). To make evaluation efficient and enable rapid iteration over modelingchoices during development, prior work has approximated human judgments of the model responsequality using by using a poweful language model as a judge (LLM-as-a-Judge). Although modeljudges have been shown to exhibit biases due to superficial features, such as the length of responses,prior work has indicated that such biases can be addressed (Dubois et al., 2024) to improve thereliability of these judgments. However, the analysis of such biases and the corresponding debiasingtechniques developed in prior work are based on a distribution of tasks that is not representative ofthe full range of applications of instruction-tuned language models.\nIn this work, we reevaluate various choices for automatic evaluation on a wider range of instructionfollowing tasks (Section 2). We choose a task distribution closely aligned with those typically usedto train instruction-tuned models (Ouyang et al., 2022), and measure the agreement between humanand model judges by comparing LLM-as-a-Judge and embedding-based similarity approaches. Weexperiment with using human-written reference responses in the process-by including them as addi-tional context in the LLM-as-a-Judge or measuring embedding similarity between model responsesand human responses-and observe that they can and observe that they enhance the reliability ofautomatic evaluation across many tasks, resulting in up to a 3.2% improvement in agreement withhuman judges (Section 3.2). Our analysis also provides insights into how human-written responsesare helpful. We discovered that human-written responses offer an orthogonal perspective to model-"}, {"title": "2 EMPIRICAL BASIS FOR THE EVALUATION SETUP", "content": "In this section, we describe our experiment settings to explore how human-written response can beutilized to improve the reliability of evaluating the instruction-following capability of LLMs. Specif-ically, we construct a dataset for evaluating the evaluation methods, collecting human annotations"}, {"title": "2.1 HUMAN AGREEMENT SET CONSTRUCTION", "content": "In this subsection, we describe a dataset containing instructions, responses from models and humans,along with human annotated preferences. We refer to this dataset as the human agreement setwhich will be a subset of the final dataset described in Section 4."}, {"title": "2.1.1 INSTRUCTIONS AND RESPONSES COLLECTION", "content": "We construct a dataset of instructions, each associated with a human-written response, two candidatemodel responses, and multiple human judgment annotations indicating which model response ispreferred.\nTask Selection. Prior benchmarks for evaluating instruction following include sets of instructionsthat are representative of real user interactions with publicly hosted language models. While evalu-ating on such datasets can inform how the model would perform in practice, the input distributions tend to be heavily skewed towards a small set of tasks as shown by (Lin et al., 2024; Chiang et al.,2024; Li et al., 2024). Consequently, the decisions regarding the evaluation setup, though based onrigorous human agreement experiments, may be biased towards a small number of tasks. In contrast,we begin with a taxonomy of 11 instruction-following tasks and build a dataset of instructions specif-ically targeting these tasks. Specifically, we select 8 tasks from the InstructGPT taxonomy (Ouyanget al., 2022)-Brainstorming, Open QA, Closed QA, Extraction, Generation, Rewriting, Summa-rization, Classification, and 3 additional tasks focused on scientific text understanding-Fact Check-ing, Multi-Document Synthesis, and Reasoning Over Numerical Data."}, {"title": "2.1.2 HUMAN ANNOTATION COLLECTION.", "content": "We collected 4 human preference annotations for each instance in our human agreement set follow-ing the procedure described below. Importantly, the annotators are shown only the instructions andthe two model responses per each instance, and not the human-written responses.\nAnnotator Selection. We recruited native English speakers from the U.S., the U.K., and Canada,who have a Bachelor's degrees or above, and a prior approval rating over 99% from Prolific (First,2014). We further screened annotators using a qualification test that required them to correctlyannotate at least 9 out of 10 instances with easily distinguishable model response pairs. We assignthe qualification task to 50 participants, and recruited 16 of them as our final group of annotatorsand paid them $16 / hour.\nAnnotation Guidelines and Interface. We used the annotation guidelines from Li et al. (2023)with the following modifications: We slightly modified checklist of judging aspects, included twoexample annotations, and importantly allowed the annotators to choose \"tie\" when both the model"}, {"title": "2.2 EVALUATION METHODS", "content": "We evaluate a set of pairwise evaluation methods (Zheng et al., 2023), i.e., those that select thebetter response between two candidate model responses, based on their agreement with the humanjudgments we collected.\nLLM-as-a-Judge involves prompting a powerful LLM to judge the better response between a pairof responses from two models. This is the method that prior work have prominently adopted. Weexperiment with Llama-3.1-7B-Instruct, Llama-3.1-70B-Instruct (Dubey et al., 2024), GPT-4, andGPT-4-Turbo (Achiam et al., 2023) as the judge model in our experiments. Note that we allow the methods to judge 'tie' between the two modelresponses."}, {"title": "2.3 COMPUTING HUMAN AGREEMENT", "content": "Following Li et al. (2023), we use the Leave-One-Out (LOO) agreement rate to evaluate the agree-ment between a method's output and the 4 annotations for each sample. Concretely, we compute thefrequency with which the evaluation method's output matches the mode of each combination of 3out of 4 human annotations, then average the results across all 4 possible combinations. We reportthe human agreement rate as the average of LOO agreement rate over the all response pairs. To cal-culate the agreement rate within the human annotator themselves, we treat the remaining annotationas the \"model\" prediction for each combination of 3 annotations and perform the same calculation."}, {"title": "3 RESULTS", "content": "In this section, we present the results from the experiment described in Section 2, and we provideadditional insights into why human-written responses are helpful in improving the evaluation meth-ods."}, {"title": "3.1 MAIN RESULTS", "content": "Table 2 presents the results of the human agreement analysis.\nHuman agreement rates varies across task categories. Tasks such as Brainstorming, Open QA,Summarization, and Multi-Document Synthesis, tend to have responses that vary in multiple dimen-sions, including general content, level of details, tone, etc. We observe that both the inner-agreementrate among human annotators and the agreement rates across all evaluation methods are lower withinthese task categories, indicating that humans apply divergent standards for judging LLM responsesand weights various dimension of such open-ended responses differently. Conversely, categories thattends to have easily verifiable answers, including Close QA, Extraction, Classification, and Reason-ing Over Numerical Data, appears to have higher agreements. Note that although Rewrite containsmany open-ended instructions, a large portion of the instructions are verifiable as they ask for spe-cific tone or format of the response. These findings highlight the importance to evaluate LLMs onspecific task categories.\nLlama-3.1-70B-Instruct is the best judge. Llama-3.1-70B-Instruct outperforms GPT-4 by 6%and GPT-4-Turbo by 1.5% without human responses, achieving the closest agreement rate comparedto the human. It also outperforms GPT-4 by 4.2%, GPT-4-Turbo by 1.3%, and even humans by 0.9%using human responses on average.\nHuman-written responses improve agreement with human judgments. Across all models ex-cept Llama-3.1-7B-Instruct, embedding human-written responses into the prompts and using themas additional context frequently improves agreement with human judgments. The performance dropwith Llama-3.1-7B-Instruct is likely because LLMs have to reach a certain capability threshold sothat they understand how to properly utilize the human-written responses. In task categories CloseQA, Extraction, Generation, Rewriting, Classification, Multi- Document Synthesis, and ReasoningOver Numerical Data, using human-written responses brings an increment of 4.8% on average inagreement with human for using Llama-3.1-70B-Instruct as the judge. For OpenQA, Summariza-tion, and Fact Checking, we observe that human-written response improves agreement with humanjudgement for GPT-4 and GPT-4-Turbo but not for Llama models. This suggests that the capabilityof properly leverage human-written responses as additional context is inconsistent across differentmodels for these task categories. We also see that RoBERTa-Large is able to deliver the highestagreement rate with human on Open QA and Fact Checking. These results show that, despite thatthe annotators who write the human response and the ones who annotate the preference are twodifferent groups, a human-written response can help improve the judgment by serving as an addi-tional context or a comparable reference. We will talk about the insights around the usefulness ofhuman-written responses in the following Section 3.2.\nChoosing the best method for each category. With the new set of evaluation methods that lever-age human-written responses, we are provided with the option to select the best evaluation methodsfor each task categories and compose the final composite methods. Overall, the resulting compositemethod with Llama-3.1-70B-Instruct achieves 1.5% higher in human agreement rate than only using"}, {"title": "3.2 ANALYSIS: LEVERAGING HUMAN REFERENCES", "content": "In order to understand the unique value of human-written responses, we compare them directlyagainst model-generated response proposed in Zheng et al. (2023).\nHuman-written responses are more useful than model-generated responses with LLM-as-a-Judge. We use generate responses from GPT-4-Turbo for the instructions in the human agreementset and repeat the experiments in Section 2 with model-generated responses.  demonstratesa comparison between using human-written responses and model-generated responses. We observethat with LLM-as-a-judge methods, human-written responses display higher agreement rates thanmodel-generated responses across all judge models. This demonstrates that references written byhumans are consistently more useful than those generated by even the strongest LLMs. Withembedding-based evaluation methods (ROBERTa and Rouge), using model-generated responses dis-play higher agreements than human-written responses. This is due to the fact that model-generatedresponses are syntactically and stylistically more similar to each other than to human-written ones,likely biasing these simpler evaluation methods.\nWhy not directly compare against human responses? We experimented with a setup where weprompt each LLM judge in Section 2 to directly compare model responses with human responses.  shows that, surprisingly, all the judge models strongly prefer model responses over humanresponses despite their judgments being more aligned with those of human annotators when usinghuman responses as additional context. This is likely because that the judge models strongly preferthe stylistic characteristics of model-generated responses. However, humans may prefer the style ofhuman-written responses and other impactful dimensions, such as correctness, which are overlookedby the judge models. This demonstrates that human-written responses are much more effective asadditional context or additional reference for comparing model responses, rather than servingas the sole reference for direct comparison in evaluating response quality."}, {"title": "4 NEW BENCHMARK: HREF", "content": "Based on the insights that human-written responses significantly improves the evaluation of LLMs'instruction-following capability, we construct a new evaluation benchmark, Human Response-guided Evaluation of instruction Following (HREF). See  for an overview of the com-parison between HREF and similar existing benchmarks. We release two evaluation sets in additionto the human agreement set we used for experiments described in Section 2: a private evaluation setand a public development set.\nPublic Development Set We adopt a subset of the No Robots (Rajani et al., 2023) test split as thedevelopment set, which contains 430 human-written instruction and response pairs covering 8 out ofthe same 11 task categories as the evaluation set (See ). The remaining three scientific text"}, {"title": "4.1 PRIVATE EVALUATION SET", "content": "Instruction and Human Response Collection. We hire human experts to write instructions andcorresponding responses specifically targetting the taxonomy of tasks shown in Table 1. This resultsin 4,258 high quality instruction-response pairs.  Left shows the resulting distribution of theinstructions.\nBaseline Response Generation. We generate a baseline response for each instruction to be com-pared against by a target model using the open model Llama-3.1-405B-Instruct-FP8 using greedydecoding. We compare this model with other choices for baseline models in Section 5.1."}, {"title": "4.2 EVALUATION DETAILS", "content": "Pipeline. For a target model, we first generate its response to each instruction to compare againstthe baseline model response using HREF, and consider it a as win if HREF either prefers the targetmodel response or selects a tie. To obtain the final expected win rate, we compute the frequency ofwins for the target model across all data points.\nMethod Details. Following the observation from Section 5.1, we use the composite method withLlama-3.1-70B-Instruct as the judge model.\nDecoding Strategy. For reproducibility, we choose greedy decoding for these models. We findthat this choice does not significantly impact the evaluation results\u2014we find a high correlation (0.98Spearman and 0.99 Pearson) between the results obtained from using greedy decoding and thoseobtained from using a temperature of 1.0 on our development set."}, {"title": "4.3 RESULTS ON CURRENT LLMS", "content": "We evaluate 37 LLMs with a variety of model families and sizes on HREF as the initial benchmark. presents the results ranked by their total expected win rates, along with their expected winrates in each of the 11 categories. \nIn general, LLMs with larger sizes display higher expected win rates, and such trends holds con-sistently within the same model family. For example, Llama-2-70b-chat-hf holds a higher expectedwin rate than Llama-2-13b-chat-hf on average. Also note that model expected win rates vary acrossdifferent categories. For example, while Mistral-Large-Instruct-2407 a high average expected winrate among the models that we evaluate, it performs poolly in Open QA. This demonstrates the im-portance of focusing into the evaluation on individual task and underscores the advantage of HREFin providing task-centric evaluation."}, {"title": "Correlation with evaluation on the development set.", "content": "We also evaluate the same group of LLMs on our development set with 8 categories (See Section 4.1), additionally with several GPT models.See the full results in Appendex A. We observe similar trends to those seen in the test dataset.To validate that model developers can expect a reasonable transfer of their results from the publicdevelopment set to the private evaluation set, we calculate the correlation of the expected win ratesbetween these two sets and observe high correlations: a Spearman correlation of 0.98 and a Pearsoncorrelation of 0.99."}, {"title": "4.4 STATISTICAL SIGNIFICANCE", "content": "To ensure the reliability of our evaluation set in distinguishing between models, we evaluate HREF'scapability of statistically distinguishing among a diverse set of models of reasonable size. Specifi-cally, we sample from a pool of 13 models following Li et al. (2023) but use a set of more recent anddiverse models . For each pair of models, we apply a paired t-test to evaluate the null hypothesisthe the preference predictions from the pair of models have identical expected values, and we mea-sure the resulting p-values. We perform this analysis on both of the evaluation set and developmentset.\nCapacity of the development and test sets.\nFigure 6 Left shows that with fewer than 2000 sam-ples in the evaluation set, the p-values at 90th quantile falls below 0.05, which suggests that ourevaluation set is able to statistically significantly distinguish between 90% of the model pairs. Simi-larly,  right suggests that our development set is able to statistically significantly distinguishbetween 80% of the model pairs."}, {"title": "5 DISCUSSION ON DESIGN CHOICES", "content": "In this section, we discuss the specific design choices and the advantages they bring to HREF,including the choice of the judge model for LLM-as-a-Judge, and choice of the baseline model,and the choice of the prompt template."}, {"title": "5.1 CHOICE OF THE JUDGE AND BASELINE MODELS", "content": "Unlike prior work (Li et al., 2023; Chiang et al., 2024; Zheng et al., 2023; Li et al., 2024; Lin et al.,2024), we choose Llama-3.1-70B-Instruct as the LLM judge and Llama-3.1-405B-Instruct-FP8 asour baseline model instead of GPT models. In this section, we discuss the rationale behind suchchoices.\nHigh Human Agreement Rate with the Judge Model. Llama-3.1-70B agrees with human judg-mements the most on HREF as discussed in Section 3.\nA Less Length-biased Judge Model. Previous work (Dubois et al., 2024; Lin et al., 2024; Liet al., 2024) has observed that the judge LLMs strongly prefer longer responses and has adoptedlength normalization methods to account for such bias. We quantify the length bias of various judgemodels on our human agreement set, by measuring the difference between each judge's frequencyof preferring longer responses versus the frequency of preferring shorter responses. We refer to thisdifference as the length bias rate. Since we explicitly control for response length while samplingresponses in the human agreement set (see Section 2.1.1), we expect a model with no length biasto have a length bias rate close to 0% on our dataset.  shows that Llama-3.1-70B-Instructhas the lowest length bias rate among all the four judge models that we experiment with. The use ofhuman written responses further lowers its length bias rate to 1.4%. As a result, we chose not to addany length debiasing controls."}, {"title": "Relevance of HREF.", "content": "As the size of the model pool and the strength of the models in the poolincrease, the chance that a model pair will be indistinguishable (t-test with a p-value less than 0.05)will also increase. In other words, a larger evaluation set will be needed to distinguish more andstronger models. Hence, as the community keeps developing stronger models, we expect HREF,with the largest evaluation set among similar benchmarks, to remain relevant for longer."}, {"title": "5.2 CHOICE OF THE PROMPT TEMPLATE", "content": "Unlike prior work such as AlpacaEval, we directly transform the guidelines we provide to humanannotators into the prompt we provide to the judge LLMs, and we show the reasoning behind suchchoice here. We structure each prompt template into two components: a guideline and a list ofdemonstration examples. We interchange these components with those from AlpacaEval and com-pare the 4 resulting prompt templates using Llama-3.1-70B-Instruct with human-written responseson our development set as shown in .  shows that using a different set of exam-ples (Prompt B), dropping the examples (Prompt C), or completely changing the prompt (PromptD) negatively impacts agreement with human annotators compared to aligning the model promptwith the guidelines provided to human annotators (Prompt A). These results imply that ensuringthe consistency between the guidelines given to human annotators and the prompts for LLMs"}, {"title": "6 RELATED WORK", "content": "To evaluate the capability of post-trained LLMs in instruction-following, prior work has constructsbenchmarks in several ways.\nInstruction Source. Prior work have chosen to source instructions from real-world users. Chat-botArena (Chiang et al., 2024) is a benchmark that constantly collects instructions from the onlinecommunity users by directly prompting for the user's inputs. ArenaHard (Li et al., 2024) automati-cally curates instructions from those collected by Chatbot Area. These benchmarks possess sets ofinstructions that closely matches human's common interest in terms of instruction categories, butthey are also heavily skewed towards OpenQA and Generation as a result. Another widely recog-nized benchmark is AlpacaEval (Li et al., 2023; Dubois et al., 2024), which is consist of syntheticallygenerated instructions generated using human-written template (Wang et al., 2022). WildBench (Linet al., 2024) also collect instructions from the user in the wild. MT-Bench, with task-specific instruc-tions created by human experts, is the most similar to our work, but it is restricted by the small sizeof the instruction size. Our work have collected instructions covering a wider range of tasks with amuch larger evaluation set.\nEvaluating Instruction-Following Models. When evaluating a LLM' responses to a instruction,prior work either directly grade the response with a score, or perform a pairwise comparison withthe response form another LLM (Zheng et al., 2023). Chatbot Arena (Chiang et al., 2024) promptsthe same user who creates the instruction to also do a pairwise comparison between responses fromtwo models (i.e., selecting the better response), and the benchmark's evaluation results are treated asground-truth and compared against by several other benchmarks (Li et al., 2023; Lin et al., 2024).However, such evaluation requires extensive human feedback, which is expensive to collect formajority of the benchmarks. LLM-as-a-judge, acting as a proxy for human annotators, has beenwidely adopted by many benchmarks in both single response grading and pairwise comparison.However, prior work use closed API models, which lacks transparency and consistency in theirjudgment. Our work is uses LLM-as-a-judge with public models and shows the benefits that brings.\nReference Guided Evaluation Comparing text embeddings to a human-written reference answeris widely used in traditional NLP tasks, especially summarization (Zhang et al., 2019; Lin, 2004;Papineni et al., 2002; Banerjee & Lavie, 2005), but it is less clear how to properly utilize the ref-erence answer to evaluation more open-ended instruction-following. AlpacaEval (Li et al., 2023)has found that including model-generated responses in the prompt when using LLM-as-a-Judge isbeneficial in following instruction related to math. Our work adopt an combination of comparingtext embeddings to human-written responses and using human-written responses with LLM-as-a-"}, {"title": "LIMITATIONS", "content": "Multi-turn Evaluation. Multi-turn evaluation is not the focus of work, and HREF is only suitablefor single-turn instruction following evaluation. We suggest using benchmarks like WildBench formulti-turn evaluation.\nAbsolute Rating. Our work focuses solely on improving pairwise evaluation, which requires theuse of a baseline model. We recognize that there might be circumstances where an independentabsolute score can be useful, and we leave the topic of improving the accuracy of absolute rating ofan LLM in instruction-following for future work."}, {"title": "B.1 PROBLEM DEFINITION", "content": "We denote HREF's evaluation dataset as D, with each element being $(i_n, o_B, o_H)$, denoting theinstruction, the baseline model response, and the human written reference response respectively.\nGiven a target LLM T, HREF aims to estimate the rate that human would consider the responsesfrom T are at least as good as the baseline model B in following instructions, which we formally"}, {"title": "B.2 LLM-AS-A-JUDGE WITH OPTIONAL HUMAN REFERENCE", "content": "We proposes the evaluation method, LLM-as-a-judge with human reference, as one of the methodsto estimates $p(i_n, o_T, o_B)$. Specifically, we embed $i_n, o_T, o_B, o_H$ into a prompt template as theinput to a separate judge model $J$ formally:\n$p(i_n, o_T, o_B, o_H) = J(i_n, o_T, o_B, o_H)$\nNote that when not using a reference, the defination is the same except that $o_H$ will not be an inputto J."}, {"title": "B.3 ROBERTA EMBEDDING: COMPARING TEXT EMBEDDINGS WITH HUMAN REFERENCE", "content": "We also proposes to compare the cosine similarity between the text embeddings of $o_T$ and $o_H$against $o_B$ and $o_H$. Formally,\n$p(i_n, o_T, o_B, o_H) = \\begin{cases} 1 & \\text{if } sim(o_T, o_H) < sim(o_B, o_H) \\\\ 0 & \\text{otherwise.} \\end{cases}$\nwhere\n$sim(o_x, o_y) = \\frac{Embed(o_x) \\cdot Embed(o_y)}{||Embed(o_x)|| ||Embed(o_y)||}\nwith $Embed(o_y)$ represents some embeddings of $o_y$."}]}