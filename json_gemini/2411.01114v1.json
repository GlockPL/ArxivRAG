{"title": "INFANT AGENT: A TOOL-INTEGRATED, LOGIC-DRIVEN AGENT WITH COST-EFFECTIVE API USAGE", "authors": ["Bin Lei", "Yuchen Li", "Yiming Zeng", "Tao Ren", "Yi Luo", "Tianyu Shi", "Zitian Gao", "Zeyu Hu", "Weitai Kang", "Qiuwu Chen"], "abstract": "Despite the impressive capabilities of large language models (LLMs), they currently exhibit two primary limitations, I: They struggle to autonomously solve the real world engineering problem. II: They remain challenged in reasoning through complex logic problems. To address these challenges, we developed the INFANT AGENT, integrating task-aware functions, operators, a hierarchical management system, and a memory retrieval mechanism. Together, these components enable large language models to sustain extended reasoning processes and handle complex, multi-step tasks efficiently, all while significantly reducing API costs. Using the INFANT AGENT, GPT-40's accuracy on the SWE-bench-lite dataset rises from 0.33% to 30%, and in the AIME-2024 mathematics competition, it increases GPT-4o's accuracy from 13.3% to 37%.", "sections": [{"title": "1 INTRODUCTION", "content": "LLMs have achieved remarkable advancements across various domains, primarily due to their powerful pattern recognition and contextual understanding capabilities (Naveed et al., 2023). Trained on extensive datasets, LLMs can generate coherent and high-quality outputs, tackle complex tasks, and demonstrate strong adaptability across a wide range of applications (Yang et al., 2024b). However, despite their impressive capabilities, LLMs still face two significant limitations (Hadi et al., 2024):\n\n1.  LLMs themselves struggle with interaction with the physical world, which limits their capability to autonomously address certain engineering problems.\n\n2.  LLMs often struggle with multi-step logical reasoning, which limits their ability to solve complex logic problems and hinders their capacity for innovation.\n\nTo mitigate these limitations and further unlock the potential of LLMs, we developed the INFANT AGENT. It is a fully autonomous, multi-agent workflow that integrates step-by-step reasoning, tool invocation, environment interaction, feedback adjustments, and evaluation summaries. Each step in this process is autonomously determined by the Agent itself. The entire workflow begins with the user's input and then enters an infinite loop, where the Agent autonomously determines every step. The process continues until the Agent concludes that the task is complete or the system reaches its budget restrictions ."}, {"title": "2 RELATED WORK", "content": "The use of agents has become increasingly important as a means to automate and optimize complex tasks, particularly those that involve multi-step processes or require interaction with external resources (Qian et al., 2024; Abbasian et al., 2023). Agents offer the potential to enhance efficiency, reduce human intervention, and manage intricate workflows (Buhler & Vidal, 2005; Yan et al., 2001).\n\nAgents for General Task: AutoGPT (Gravitas, 2024), BabyAGI (Nakajima, 2024), AgentGPT (Team, 2024a), and AutoGen (Microsoft, 2024) are designed to tackle a broad range of general tasks by breaking down user queries into actionable components. These agents typically perform operations such as decomposing user questions, browsing online resources, and providing feedback. Among these, AutoGPT distinguishes itself by autonomously linking to external sources, while AgentGPT requires user input for certain steps, ensuring user-guided interactions. AutoGen, on the other hand, supports collaboration among multiple agents, facilitating more efficient task execution through cooperative problem-solving.\n\nSoftware Automation Agents: MetaGPT (Hong et al., 2023) and Aider (Team, 2024b) focus on automating the software development pipelines. Both follow a structured cycle of writing, running, debugging, and refining code. MetaGPT is designed specifically for end-to-end automated software development, offering an integrated solution for code generation and testing. In contrast, Aider assists developers by auto-completing code, identifying bugs, and providing optimizations within the user's daily workflow, making it suitable for enhancing productivity in practical development scenarios. Devin (AI, 2024), OpenHands, and SWE-Agent (Yang et al., 2024a) are specialized in managing complex code file operations, demonstrating the capability to solve real-world code issues, such as those encountered on GitHub. These agents are tailored to handle intricate file manipulation tasks and interact with large codebases, showcasing their utility in software maintenance.\n\nBy leveraging specialized agents, these approaches seek not only to automate routine and complex workflows but also to enhance the efficiency and scalability of task execution. They empower users to handle sophisticated problem-solving requirements across various domains, from software development to scientific research. The Agents' abilities to manage, prioritize, and execute multiple tasks simultaneously allows for streamlined operations, leading to optimized performance and enabling users to achieve higher productivity and accuracy in diverse environments."}, {"title": "3 INFANT AGENT", "content": null}, {"title": "3.1 Overall Architecture", "content": "In Figure 2, we illustrate the overall architecture of INFANT AGENT. All its operations can be contained within an infinite loop. In each loop, as long as it determines that the user's request has not yet been fulfilled, it will sequentially perform the following actions: reasoning and analysis, task scheduling, task execution, results evaluation, and summarization. Except for the task execution step, which is handled by the hand-level Agent, all other operations are managed by the brain-level Agent. In the next loop iteration, the actions executed by the brain-level Agent are summarized in the form of dialogue history and used as input for the next iteration. The entire process is fully automated by the INFANT AGENT, and once it determines that the user's request is complete, the infinite loop will break. The specific functionalities of each operation are introduced as follows:\n\nInput: We parse the user's input and extract a mandatory requirement based on the user's request. In subsequent operations, we continually remind the Agent that its response must satisfy this mandatory requirement. This acts as a constraint for the Agent, thereby encouraging it to respond aligned with the user's expectations. The mandatory requirements vary depending on the scenario. For example, in coding tasks, the mandatory requirement might be unit tests, while in writing tasks, it could be writing preferences.\n\nReasoning: By default, we employ the conventional chain-of-thought (Wei et al., 2022) approach for reasoning. Each time, the Agent is required to only return one step of analysis. Additionally, similar to previous works, for complex problems, the reasoning process may trigger multi-round voting or reflection.\n\nTask: If the Agent determines that a task needs to be executed, the brain-level Agent will assigns it to the hand-level Agent. The output of this step includes specific task objective, detailed steps, and expected outcome. For example, in a coding task, if the brain-level Agent wants to debug in the file test.py, the task description would be:\n\nPlease open the file test.py, add a print statement at line ..., and then run python test.py to provide"}, {"title": "3.2 Hierarchical Agent Collaboration System", "content": "One challenge in building an agent that can adapt to various situations is that as the number of command format prompts and few-shot examples increases in in-context learning, the instruction-selecting capability of LLMs tends to diminish. To address this issue, INFANT AGENT employs a hierarchical collaboration structure. As shown in Figure 3, agents are divided into brain-level and hand-level agents. The brain-level agents handle all the reasoning, while the hand-level agents are responsible for executing tasks by invoking different tools, such as file editing, web browsing, and code execution. Each hand-level agent can be designed with prompts or trained on carefully curated datasets specifically tailored to its task, which not only significantly reduces token usage but also nearly eliminates all incorrect command invocations. For example, in our experiments, we tested a pure code task where browser-related commands were mixed with code commands. In 13.9% of cases, the agent incorrectly invoked browser-related commands. However, with the hierarchical structure, the percentage of incorrect browser command invocations dropped to 0%. For detailed experimental information, see Experiment 4.4."}, {"title": "3.3 Memory Retrieval Mechanism", "content": "n Figure 4, we designed a memory retrieval mechanism to prevent excessive API cost consumption. The specific explanation for each step is as follows:\n\nStorage: All responses (string) generated by the model will be parsed in a specific way and uniformly stored in historical memory as instances in sequence. For example, when we ask the model to analyze a coding problem, a sample response is as follows:\n\nI will help you to analyze this problem. To solve this problem, we need to merge multiple sorted linked lists into a single sorted linked list.\n\nIt will be parsed as a Class:\n\nclass Analysis:\ncontent: str = Merge multiple sorted linked lists into a single sorted linked list.\n\nThis allows us to extract key information and apply it in various forms across different scenarios, such as evaluation and summarization, thereby prompting the model to generate responses in different formats accordingly.\n\nRetrieval: Before generating a new response, the Agent's historical memory is retrieved. The reasoning and task execution parts are separated: Input, Analysis, and Summary memories are retrieved during the Reasoning process; Action and Observation memories are retrieved during the Execution process; and Task memories are retrieved during both the Reasoning and Execution processes. Memory retrieval is designed to help the Agent categorize memories, facilitating the assignment of tasks to different levels of Agents in the next step.\n\nGeneration: Closed-source models are used solely as the brain of our Agent. For simple and repetitive tasks, smaller and cheaper, or open-source models are utilized for execution, serving as the hands of the Agent. Reasoning, summarization, and evaluation tasks are managed by the brain-level agent, while task execution is handled by the hand-level agent. The execution phase, particularly the observation step, consumes the most tokens since it involves reading files and other operations that contain a lot of textual information. By outsourcing the execution process to the hand-level agent, we can significantly reduce reliance on expensive closed-source models."}, {"title": "3.4 Token computational analysis", "content": "We perform a analysis of input tokens and output tokens before and after applying the memory extraction mechanism. We assume that for each question, there are n analyses per turn, m action-observation pairs, and k turns. Based on Figure 2, we have:\n\nThe input tokens before applying the memory extraction:\n\n$Token_{in_{bef}} = k(3 + 2m + n) Token_{input}$\n\n$+ Tokensumy\\sum_{i=0}^{k-1}((k - i - 1)(3 + 2m + n))$\n\n$ + Tokeneval\\sum_{i=0}^{k-1} ((k \u2013 i)(3 + 2m + n) \u2013 \u043f \u2013 2m \u2013 2)$\n\n$ + + Token_{usk}() Tokentask\\sum_{i=0}^{k-1} ((k \u2013 i)(3 + 2m + n) \u2013 n \u2212 1)$\n\n$+Token_{analysis} \\sum_{i=0}^{k-1}\\sum_{j=0}^{n-1} ((3+2m+n)k-(3+2m)i-j-1)$\n\n$ + Token_{action} \\sum_{i=0}^{k-1} \\sum_{j=0}^{m} ((3-i)(3+2m+n)-2n-2-2j)$\n\n$ +Token_{obs} \\sum_{i=0}^{k-1}\\sum_{j=0}^{m} ((3-i)(3+2m+n)-2n-2-2j-1)$\n\nThe output tokens before applying the memory extraction mechanism are:\n\n$Token_{out_{bef}} = k\u00d7(Tokensumy+Tokeneval+Tokentask)$\n\n$+nk \u00d7 Token_{analysis} + mk \u00d7 (Token_{action} + Token_{obs})$\n\nThe input tokens after applying the memory extraction mechanism are:\n\n$Token_{in_{aft}} = k(2+n)Token_{input}$\n\n$+ Tokensumy\\sum_{i=0}^{k-1}((k - i) (2 + n) \u2013 n \u2212 2)$\n\n$ + Tokentask \\sum_{i=0}^{k-1}((k \u2013 i)(2 + n) \u2013 n \u2212 1)$\n\n$+ Token_{analysis} \\sum_{i=0}^{k-1} \\sum_{j=1}^{n} ((k - i) (2 + n) \u2013 j)$\n\nThe output tokens after applying the memory extraction mechanism are:\n\n$Token_{out_{aft}} = k(Tokensumy + Tokentask)$\n\n$+ nk(Token_{analysis})$\n\nWhere: $Token_{input}$ represents the token count of the request made by the user. $Tokensumy$ is the average token count for the summarization step. $Tokeneval$ refers to the average token count for the evaluation step. $Tokentask$ indicates the average token count for each task, $Token_{analysis}$ denotes the average token count for each analysis step, $Token_{action}$ represents the average token count for each action during execution, and $Token_{obs}$ is the average token count for each observation step during execution.\n\nAccording to the sampling of 100 different test cases, we obtained the average values of each variable as follows:\nn = 2.53, m = 3.78, k = 5.64, Tokeninput = 359, Tokensumy = 784, Tokeneval = 7.54, Tokentask = 754, Tokenanalysis = 148, Tokenaction = 227, Tokenobs = 1994. Substituting into the above formulas, we find that applying the memory extraction mechanism for a single user request can save 79.81% of input tokens and 83.06% of output tokens. This theoretical derivation aligns closely with the experimental test results, with specific experimental tests detailed in Experiment 4.5."}, {"title": "3.5 Execution Tools", "content": "While LLMs are highly capable in natural language processing, our practical experiments show that even some of the most advanced models, like GPT-40 and Claude 3.5 Sonnet, frequently make fundamental mistakes, such as sequence misalignment, particularly when processing large files. This issue is not unique to individual models; many state-of-the-art agents, including Cursor (Team, 2024c) and OpenHands, encounter similar challenges. These errors suggest that LLMs, despite their language strengths, still struggle with accuracy in detailed, sequence-dependent tasks within extensive datasets.\n\nTo address this issue, we first enhanced the original file-editing commands of OpenHands and SWE-Agent, with specific differences shown in Figure 5. We added two new parameters, start_line_string and end_line_string, to the original SWE-Agent editing command edit_file(file_name, start_line, end_line, new_content). These parameters require that the line number of start_line must correspond to start_line_string, and the line number of end_line must match end_line_string. If they do not match, the Agent will automatically issue prompt commands to guide the LLM in adjusting the original edit_file() command until it matches correctly.\n\nWe made this improvement because we found that LLMs have a strong understanding of text but slightly less proficiency in discriminate numbers. As a result, they can often identify the correct editing location but may struggle with specifying the correct line number. This enhancement to the file-editing command improved the accuracy of SWE-Agent's file-editing function from 72.9% to 96.8%. The specific experimental details are provided in Experiment 4.6.\n\nAdditionally, we customized two commands specifically for code tasks: replace_function(file_name, function_to_replace, new_code), which replaces a specific function in a given file based on its signature, and Trace_code_switch(True/False), which enables the Agent to track essential functions called during execution, regardless of whether they run successfully or encounter errors. This tracing capability helps the Agent pinpoint potential issues in code logic by identifying functions where problems may arise."}, {"title": "4 EXPERIMENT", "content": "In this section, we tested INFANT AGENT's performance across four key datasets: SWE-bench-lite (Yang et al., 2024a), AIME2024, GPQA-diamond (Rein et al., 2023), and Codeforce contests (Codeforces, 2024). SWE-bench-lite evaluates the agent's ability to address real-world engineering problems, while AIME2024 and Codeforce test its skills in handling complex logical tasks. Additionally, GPQA Diamond requires strong logical reasoning, autonomous online information retrieval, and code execution for calculations. In the ablation studies, we examined improvements from the Hierarchical Agent Collaboration System in command accuracy, token savings from the Memory Retrieval Mechanism, and accuracy enhancements in the new file-editing commands compared to the original ones."}, {"title": "4.1 SWE-bench-lite", "content": "Dataset Description: SWE-bench (Yang et al., 2024a) is a dataset consisting of 2, 294 software engineering problems drawn from real GitHub issues and corresponding pull requests across 12 popular Python repositories. The input for this dataset is the description of a GitHub issue raised by a real user, and the agent is required to automatically generate a Patch file to resolve the GitHub issue. Since the API cost for testing the full SWE-bench dataset could be quite high, an official test subset, SWE-bench-lite, is provided.\n\nExperiment Setup: In the testing process, we initialized each level of agents with GPT-40 (temperature = 0.7) and used evaluation conditions consistent with the official SWE-bench leaderboard: all submissions are Pass@1, do not use hints_text, and are in the unassisted setting. The maximum number of iterations was set to 100, with up to 3 self-correction attempts. Automated linting prompts were enabled after code edits, the maximum timeout in the sandbox was set to 120s, and the maximum cost per iteration was capped at 10 dollars.\n\nExperiment Analysis: The performance of INFANT AGENT on the SWE-bench is shown in Table 1. Based on the data in Table 1, it is evident that most of the leading agent architectures remain closed-source, with limited technical details available. Among open-source agents, the performance of INFANT AGENT is only 0.67% lower than AutoCodeRover, surpassing all other open-source agents.\n\nWhen compared to specialized code-focused agents, such as MentatBot and AutoCodeRover, INFANT AGENT demonstrates a broader range of application scenarios, indicating its versatility beyond code-specific tasks. Additionally, when compared to generalized AI agents like OpenDevin and Aider, INFANT AGENT achieves superior accuracy. Using the 40 model for initialization, INFANT AGENT achieves an accuracy that is 8 percentage points higher than Open-Devin.\n\nThe comparison between RAG models and agent-based systems shows a clear advantage for agents in SWE-bench performance. While RAG models can leverage retrieval for knowledge-intensive tasks, they lack the structured command execution and adaptability seen in agents. Agents, such as INFANT AGENT, perform significantly better due to their ability to manage complex workflows and apply task-specific actions, which RAG models are not equipped to handle effectively. This results in higher accuracy and more reliable task completion for agents, highlighting their superiority over RAG models for engineering and iterative problem-solving tasks.\n\nThis analysis highlights INFANT AGENT's competitive performance, particularly among open-source and generalized AI agents, suggesting its effectiveness in both code-specific and broader engineering tasks."}, {"title": "4.2 \u0391\u0399\u039cE & Codeforce", "content": "Dataset Description: The American Invitational Mathematics Examination (AIME) is the second exam in the series of exams used to challenge bright students on the path toward choosing the team that represents the United States at the International Mathematics Olympiad (IMO). To prevent data contamination, we used the o1 model testing standard and selected 2024 exam questions to test INFANT AGENT.\n\nCodeforces is a website that hosts competitive programming contests. Similarly, to prevent data contamination, we selected the most recent four Codeforces contests after the release of o1: contests 969 Div1, 970, 971, and 972 to test INFANT AGENT.\n\nExperiment Setup: During the testing phase, we initialized the brain-level agents with GPT-40 and the hand-level agents with Qwen2.5-72B-Instruct (Hui et al., 2024).\n\nModel setting: The temperature for GPT-40 was set to 0.7, while inference for Qwen2.5 72B instruct was conducted using the vllm package with the following settings: temperature = 1.0, top-p = 1.0, top_k = -1, tensor_parallel_size = 8, kv_cache_allocation = 0.95, max_tokens = 9632, and max_retry = 3.\n\nAgent setting: max_iterations = 100, with up to sel f_correction_times = 3, sandbox_timeout = 120s, max_cost = $10. Results are uniformly recorded as Pass@1.\n\nExperiment Analysis: As shown in Table 2, we compared the accuracy of different models on these two datasets using various prompting methods and benchmarked them against the current state-of-the-art reasoning model, the ol series. Results show that supported by the INFANT AGENT workflow, the combination of 40 and the open-source Qwen2.5-72B-Instruct achieves the same accuracy as ol-preview on the AIME2024 dataset, with nearly half the API cost. In addition, though o1-mini's performance significantly surpasses other methods, INFANT AGENT still solved two problems that 01-mini could not: AIME-2024-I-15 and AIME-2024-II-14. This situation did not occur with other methods. On the Codeforces dataset, while its accuracy is slightly lower than 01-preview, the API cost is reduced by almost 90%."}, {"title": "4.3 GPQA Diamond", "content": "Dataset Description: The GPQA Diamond (Rein et al., 2023) dataset contains 198 PhD-level questions covering various fields, including Organic Chemistry, Quantum Mechanics, Astrophysics, Genetics, and more. Solving these problems requires the agent not only to have deep logical reasoning abilities but also to be able to retrieve information from the web and to write and execute code for scientific computations.\n\nExperiment Setup: We initialized the brain-level agent of INFANT AGENT with GPT-40 and Claude-3.5-Sonnet, and consistently used Qwen2.5-72B-Instruct to initialize the hands-level agent. The litellm package was used to standardize the output format for both Claude-3.5-Sonnet and GPT-40. All other experimental parameters were kept consistent with the setup used in Experiments 4.2. For the test data format, we used the random function to shuffle one correct answer and three incorrect answers, presenting them in choice format below the question description.\n\nExperiment Analysis:"}, {"title": "4.4 Error Command Correction Test", "content": "We tested the correction capability of the Hierarchical Agent Collaboration System for agent command misjudgments.\n\nDataset Description: We selected a pure code task dataset, LiveCodeBench. It is a comprehensive and contamination-free evaluation benchmark for LLMs focused on code, which continuously gathers new problems over time. In theory, this dataset requires only code generation, without any browser-related operations.\n\nExperiment Setup: We selected two types of commands from the INFANT AGENT command library: file-editing commands and browser commands. To evaluate command generation accuracy, we compared the frequency of unintended browser command generation for this task using two distinct prompting methods, hierarchical prompting and flat prompting. In the flat prompting approach, we provided the model with a 1-shot example containing a mix of file-editing and browser commands. This analysis was conducted using both open-source and closed-source models to assess performance across different model types. For closed-source models, we used GPT-40, while for open-source models, we used Qwen2.5-72B-instruct. The LiveCodeBench timeframe was set from 1/1/2024 to 11/1/2024. During testing, all model temperatures were set to 0.0, and results were recorded using pass@1 scores.\n\nExperiment Analysis: In Table 4, under the hierarchical prompting structure, the model did not generate any browser commands, whereas under the flat prompting structure, the model generated over 10% browser commands. This misdirected the model's reasoning, leading to a decrease in accuracy. For Qwen2.5, the model's accuracy dropped by 10.7%, a level of precision loss that is unacceptable."}, {"title": "4.5 API Token Savings from Memory Retrieval", "content": "In this section, we compared the API token cost before and after memory retrieval and its impact on accuracy. To ensure a sufficient level of task complexity, we selected 50 test samples from SWE-bench-lite in which INFANT AGENT required over 70 iterations. We then tested scenarios with and without memory retrieval. We keep all the experiment setups the same as the Experiment 4.4."}, {"title": "4.6 File Editing Accuracy Improvement", "content": "When executing file-editing commands, the agent must accurately generate line numbers; otherwise, misalignment errors may occur, leading to incorrect edits. In this section, we tested the error-correction capability of our new file-editing command compared to the original SWE-Agent file-editing command.\n\nWe selected the same 50 test cases as in Experiment 4.5. However, this time we focused specifically on all file-editing commands within these cases. Since sequential file edits do not inherently trigger errors, we manually reviewed a total of 351 file-editing commands across these 50 test cases.\n\nExperiment Analysis: The results in the Figure 6 demonstrate that the Infant Agent edit_file() method achieves a substantial improvement in accuracy at the cost of a slight increase in average call rounds. Specifically, the Infant Agent method reached an impressive 97% accuracy compared to 73% for the original SWE-Agent method, highlighting a significant enhancement in generating precise line numbers and content for file edits, which reduces the occurrence of sequencing errors and incorrect edits. Although the Infant Agent method required an average of 5.1 call rounds compared to 2.7 for SWE-Agent, this trade-off in additional calls enables a marked increase in accuracy, making Infant Agent more effective and reliable for file editing tasks."}, {"title": "5 CONCLUSION", "content": "We present three major contributions: INFANT AGENT, an advanced agent that can perform deep logical reasoning, invoke tools, and engage in self-reflection; a hierarchical agent collaboration system to address output inefficiencies caused by an excessive number of built-in commands or overly lengthy few-shot examples; and a memory retrieval mechanism, which reduces API token costs by 80% compared to using the full memory for each inference, thereby optimizing resource efficiency. Together, these innovations significantly enhance the Infant Agent's adaptability, cost-effectiveness, and capability to handle complex tasks."}, {"title": "6 FUTURE WORK", "content": "1.  We plan to expand the current Agent framework from a text modality to a multimodal one. Two potential technical approaches: 1. Moving the mouse at the pixel level (Research, 2023), 2. First performing image parsing, then locating (Kang et al., 2025; 2024).\n\n2.  Train a File-Editing model.\n\n3.  Verify step by step and enhance GPT's error-correction capability through reinforcement learning (Lightman et al., 2023).\n\n4.  Teach model how to use tools instead of using long prompts (Lei et al., 2024)."}, {"title": "A PIPELINE DEMONSTRATION EXAMPLE", "content": "To illustrate the actual operational logic of Infant Agent, we selected the following example and included terminal screenshots from the program's runtime to demonstrate Infant Agent's workflow:\n\nFirst, the user submits a query to the agent:"}]}