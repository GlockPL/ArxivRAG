{"title": "Retrieval Augmented Time Series Forecasting", "authors": ["Kutay Tire", "Ege Onur Taga", "M. Emrullah Ildiz", "Samet Oymak"], "abstract": "Retrieval-augmented generation (RAG) is a central component of modern LLM systems, particularly in scenarios where up-to-date information is crucial for accurately responding to user queries or when queries exceed the scope of the training data. The advent of time-series foundation models (TSFM), such as Chronos, and the need for effective zero-shot forecasting performance across various time-series domains motivates the question: Do benefits of RAG similarly carry over to time series forecasting? In this paper, we advocate that the dynamic and event-driven nature of time-series data makes RAG a crucial component of TSFMs and introduce a principled RAG framework for time-series forecasting, called Retrieval Augmented Forecasting (RAF). Within RAF, we develop efficient strategies for retrieving related time-series examples and incorporating them into forecast. Through experiments and mechanistic studies, we demonstrate that RAF indeed improves the forecasting accuracy across diverse time series domains and the improvement is more significant for larger TSFM sizes.", "sections": [{"title": "Introduction", "content": "The success of large language models (LLM) has motivated a broader push toward developing foundation models for other modalities. Time-series analysis, in particular, stands to directly benefit from recent advancements in sequence modeling techniques. Indeed, there has been significant progress in new time-series architectures [57, 54, 58, 24, 33, 27, 56, 7, 43, 28], tokenization strategies [33, 5, 1, 39, 49], and more recently, time-series foundation models such as Chronos [1]. These advances hold the premise to enhance accuracy, robustness, and few-shot learning capabilities of future time-series models. On the other hand, there is a notable shift from standalone models to compound AI systems [20, 35, 22, 21, 17] where LLMs are integrated with external databases and advanced prompting strategies to accomplish complex tasks.\nIn particular, retrieval augmented generation (RAG) [22], has become a key component of LLM pipelines during recent years [23]. In essence, RAG aims to facilitate factual and up-to-date generation by retrieving query-related documents from external databases. Notably, RAG also mitigates the need for retraining the model to incorporate fresh data or fine-tuning it for individual application domains. In the context of time-series forecasting, we expect RAG to be beneficial for several reasons. First, time-series data is inherently dynamic, heterogeneous, and context-dependent, making it challenging to forecast accurately without access to relevant external context. Second,"}, {"title": "Problem Setup", "content": "Let us first introduce the basic notation. We use lower-case and upper-case bold letters (e.g., $\\mathbf{a}, \\mathbf{A}$) to represent vectors and matrices, respectively; $a_i$ denotes the $i$-th entry of a vector $\\mathbf{a}$. Let $x \\in \\mathbb{R}^L$ denote a univariate time series of length $L$. Let $f(x) \\in \\mathbb{R}^H$ represent the forecast of the time series given model $f$ and input $x$. We also use $x[j, C]$ to denote the sub-series of length $C$ that starts from time $j$ and ends at time $j + C - 1$. ($f(x)$) $[j, H]$ is defined similarly. A sub-series that approximately repeats within a longer time-series is also referred to as time-series motif. Given a motif $m \\in \\mathbb{R}^C$, we say that $x[j, j + C - 1]$ matches $m$ if $x[j, j + C - 1] = m$."}, {"title": "Time-series Retrieval Problem", "content": "Retrieval augmented forecasting is inherently related to the model's capability to identify motifs in the time-series, and utilizing this to make inference. This motivates our Time-Series Retrieval (TS-R) task which measures the ability to match the current motif (i.e. query) to an earlier similar motif (i.e. key). The model is then asked to retrieve the context surrounding the earlier motif and output it as its prediction."}, {"title": "TS-R problem", "content": "Let $m$ be the motif at the end of the time-series sequence i.e. $m = x[L - C + 1, L] \\in \\mathbb{R}^C$. Suppose that there is a unique matching motif in the history, namely, we have $x[t, t + C - 1] = m$ for a unique timestamp $t < L - C + 1$. Let $Y \\in \\mathbb{R}^H$ be a motif followed by $x[t, t + C - 1]$. We say that a model $f$ solves TS-R problem if- for all $m \\in \\mathbb{R}^C$ and $Y \\in \\mathbb{R}^H$, we have $f(x) = Y$."}, {"title": "Synthetic Retrieval Experiment", "content": "In practice, time-series data are noisy, unlike the idealized setting of Definition 1. To assess Chronos' time-series retrieval behavior under noisy conditions, we investigate the following experimental"}, {"title": "Methodology", "content": "The time-series retrieval problem discussed in Section 2 and the experimental results shown in Figure 2 demonstrate that transformer-based time-series models are well-equipped for retrieval-augmented forecasting. However, these assume the existence of a retrieval mechanism and a time-series model. Here, we describe the design choices and implementation details of the RAF framework."}, {"title": "Indexing and Database Formation", "content": "To retrieve the best matching time series as described in Section 2, we construct a database specific to each data domain (dataset) because different data domains exhibit distinct characteristics. Consequently, we allocate 20% of the time series in each dataset for testing and use the remaining 80% to form the database through a random but fixed split for all evaluations. From each dataset-specific database, we then retrieve the best matches."}, {"title": "Matching and Similarity Metric", "content": "The selection of the best-matching time series to the original time series is based on embedding similarity. In this approach, we first obtain the embedding of the original time series using the encoder of the chosen model. Then, we identify the top-n best matches by calculating the $l_2$ norm between the original time series and the time series in our allocated database. The $l_2$ norm is given by $||m - y||_{l_2} = \\sqrt{\\sum_{i=1}^n(m_i - y_i)^2}$, where $m$ and $y$ represent the vectors corresponding to the embeddings of the original time series and the time series retrieved from the database, respectively."}, {"title": "Instance Normalization", "content": "To mitigate the distribution shift effects between training and testing data, we apply instance normalization [50, 19]. We normalize each time series instance $x^{(i)}$ with zero mean and unit standard deviation. For the baseline approach, we normalize each $x^{(i)}$ before prediction and the mean and deviation are added back to the output prediction. On the other hand, original time series $x^{(i)}$ and the retrieved time series $x'^{(i)}$ are normalized separately before being input into the model in the form of retrieval query formation. The mean and deviation of $x^{(i)}$ are then added back to the output prediction at the end."}, {"title": "Retrieval Query Formation", "content": "The initial step in query formation is identifying the top-1 best-matching time series from our database using our similarity metric. The goal is to find a time series that closely matches the context (historical pattern - motif) of the time series we are working with. This retrieved time series serves as the retrieved context. After identifying the retrieved context, we focus on its future portion, which is the segment immediately following the retrieved context. The length of this segment, termed the retrieved future, corresponds exactly to the prediction length we aim to forecast. This combination of the retrieved context and retrieved future forms what we call the retrieved time series. The retrieved time series, which includes both the retrieved context and retrieved future, undergoes instance normalization. This step ensures that the data from the retrieved series is scaled appropriately, eliminating any discrepancies that may arise due to varying magnitudes in different time series. The same normalization process is also applied to the original context to maintain consistency. Once both the original context and the retrieved time series are normalized, a smooth transition between them must be ensured. To do this, the ending point of the retrieved time series is aligned with the beginning point of the original context. This alignment prevents any abrupt changes or discontinuities that might negatively affect the performance of the Chronos models. After the alignment, we concatenate the retrieved time series and the normalized original context. This combined, augmented time series serves as the input for the Chronos models."}, {"title": "Time series models", "content": "We utilize Chronos Mini and Base as our evaluation models. Similar to LLMs [1], Chronos is probabilistic, allowing for multiple future trajectories to be generated by autoregressively sampling from the predicted distribution, $p_\\theta(Z_{C+h+1} | Z_{1:C+h})$."}, {"title": "Experiments", "content": "This section provides an overview of the datasets, baselines, and evaluation metrics, followed by our main results and evaluations. The code is available at: https://github.com/kutaytire/ Retrieval-Augmented-Time-Series-Forecasting."}, {"title": "Benchmark Datasets", "content": "To evaluate the performance of RAF, we have analyzed two distinct benchmark datasets: (a) Benchmark I datasets with long context and prediction lengths (6 datasets), and (b) Benchmark II datasets with short context and prediction lengths (5 datasets). In selecting the datasets, we adhere to the list used in the zero-shot and in-domain evaluation of Chronos to ensure that Chronos has no prior exposure to the data during training, thus guaranteeing a fair evaluation. For both Benchmarks I and II, we use the last H observations of each time series as the test set, following [1]. The prediction length $H \\in \\{10, 15, 20\\}$ and context length $C \\in \\{50, 75, 100, 150\\}$ are used for Benchmark I, while $H \\in \\{3, 4, 5\\}$ and $C \\in \\{10, 15, 18, 21\\}$ are used for Benchmark II. Both benchmarks exhibit a wide range of characteristics, such as dataset size and frequency, making them valuable benchmarks that closely resemble real-world scenarios. The specifications and lists of the datasets are provided in Appendix E."}, {"title": "Baselines and Evaluation Metrics", "content": "Our experiments compare Chronos models augmented with RAF to the baseline Chronos models without retrieval. We also compare both approaches for zero-shot forecasting (Tables 1 and 2) as well as forecasting after the model is fine-tuned on the target dataset (Table 3). For a fair comparison, we evaluated RAF against the baseline across two different benchmark datasets, as explained earlier. For each dataset, we evaluated the results using four different context lengths, $C \\in \\{50, 75, 100, 150\\}$ for Benchmark I and $C \\in \\{10, 15, 18, 21\\}$ for Benchmark II, across three different prediction lengths: $H \\in \\{10, 15, 20\\}$ for Benchmark I and $H \\in \\{3, 4, 5\\}$ for Benchmark II. We then compared the average results across these three prediction lengths for each benchmark.\nFor evaluation, we assessed RAF based on both probabilistic and point forecast performance, as recommended by [1]. We utilized the weighted quantile loss (WQL) to measure the quality of our probabilistic forecasts. WQL measures how closely the predicted distribution aligns with the"}, {"title": "Main Results", "content": "We present our primary results based on two sets of datasets: Benchmark I, consisting of 6 datasets, and Benchmark II, consisting of 5 datasets. On average, RAF outperforms the baseline approach across both benchmarks when tested on both Chronos Base and Chronos Mini. Additionally, the fine-tuning of the models with time series retrieval demonstrates superior performance in Benchmark I as seen in Table 3."}, {"title": "Conclusion", "content": "In this paper, we have introduced the RAF framework for time-series foundation models which leverages retrieval-augmentation and fine-tuning to enhance forecast accuracy. By incorporating external, domain-specific knowledge during inference through retrieval, RAF provides substantial improvements in both probabilistic and point forecasting tasks. Furthermore, we have explored two variations: Naive RAF, which uses TSFMs as black boxes without modifying their weights, and Advanced RAF, which fine-tunes models for enhanced retrieval integration.\nWe have evaluated the performance of both Naive and Advanced RAF on 11 diverse datasets, grouped into two distinct benchmarks, each with varying context and prediction lengths. Our experimental results demonstrate that both Naive and Advanced RAF outperform the standard baseline approach on average. These findings highlight the flexibility and robustness of the RAF framework in improving time-series forecasting performance, particularly in scenarios that require adapting to varying historical data contexts and forecasting needs. Importantly, our study has also revealed that model size matters: Chronos Mini fails to solve simple synthetic retrieval tasks and larger models benefit more from retrieval-augmented forecasting both in synthetic and real experiments. As a future perspective, we propose expanding the RAF framework to handle multi-channel predictions and retrieve multiple samples from external sources. Combining these retrieved samples in a structured manner could further enhance the models' forecasting capabilities, particularly in complex, multi-variate time-series scenarios, and improve adaptability across diverse data domains."}, {"title": "Theoretical Results", "content": "In Section 2, we asserted that a two layer transformer architecture can solve the TS-R problem with mild assumptions employed by various transformer-based time-series architectures. In the below theorem, we prove that with patching, a 2-layer transformer architecture can indeed solve the TS-R problem. Note that our proof is based on the literature on the nearest neighbor retrieval, where a previous line of work [36, 11] has shown that softmax attention can implement nearest neighbor retrieval."}, {"title": "TS-R Problem", "content": "Setting. We use the definitions in Section 2. Given $x \\in \\mathbb{R}^L$ denoting a univariate time series of length $L$, and with $C$ denoting the context length, we extract patches of size $C$ with a sliding window of size 1. Furthermore, we assume $H < C$. Without loss of generality, from here on we assume $H = C$ as we can trim the output after the retrieval. Overall, we get a patched input sequence $X = [x_1, x_2, \\dots, x_{L-C+1}]$. Moreover, we embed each $x_i$ to $z_i := \\left[ \\frac{x_i}{\\|x_i\\|_2}, \\|x_i\\|_2 \\right] \\in \\mathbb{R}^{C+1}$. That is, we embed each $x_i$ so that we store the direction and the magnitude in seperate dimensions (there is a clear bijective mapping that is inverse of this embedding, denote by $g$). Thus, with this mapping, we define the $X := [z_1, z_2, \\dots z_{L-C+1}]$. Based on $X$, we define the token embedding matrix as $\\mathbf{X} := [\\mathbf{x_1}, \\mathbf{x_2}, \\dots, \\mathbf{x}_{L-C+1}]^T \\in \\mathbb{R}^{(L-C+1)\\times(C+1)}$. Note that based on $\\mathbf{X}$, we can recover each $x_i$.\nMoreover, let $p_i$ be fixed positional encodings, denoting the positions of the tokens. Define $\\mathbf{X}_{PE} := [\\mathbf{x_1} + \\mathbf{p_1} \\dots \\mathbf{x}_{L-C+1} + \\mathbf{p}_{L-C+1}]^T$. For the ease of notation, we use $\\mathbf{X} = \\mathbf{X}_{PE}$ from here on."}, {"title": null, "content": "Assumption 1 We assume that the positional encodings $(p_i)_{i=1}^{L-C+1}$ have unit $l_2$ norm and are unique. Moreover, we assume that positional encodings are orthogonal to tokens $\\langle p_i x_j \\rangle = 0$ (if there are no such token positional encodings, without loss of generality, we can just concatenate $x_i$ with 0 vectors of required size). In addition to that, we assume that retrieved token positions are rotated versions of the value positions. That is, there is a unitary matrix $R$ such that $\\mathbf{p}_{i+C} = R \\mathbf{p}_i$ for all $1 \\leq i \\leq L - 2C + 1$.\nAs in the retrieval, given a matching motif patch, the value is in C forward patches, we put above rotational assumption with rotation value as C. Note that this idea is also employed in one of the most popular positional embedding strategies, namely Rotational Positional Encoding (RoPE) [47]. Moreover, denote the projection matrix associated to the token embeddings via $\\mathbf{\\Phi}$, where $\\mathbf{\\Phi}_1 = \\mathbf{11}$ and $\\mathbf{\\Phi}_\\perp p_i = 0$. This means $\\mathbf{\\Phi}^\\perp = I - \\mathbf{\\Phi}$, implying that $\\mathbf{\\Phi}^{\\perp}x_i = 0$ and $\\mathbf{\\Phi}^{\\perp}p_i = p_i$.\nAttention model. We consider a 2-layer attention as $\\mathbf{X}_{tr} = \\mathbf{X}_{0:L-C,:}$, that is the truncated version of $X$ where we remove the last row. $N$ is the diagonal normalization matrix that normalizes each row of $\\mathbf{X}_{tr}$ to be unit norm. Moreover, In the first attention layer, we write $x = f_1(x_{L-C+1}) = \\mathbf{X}S(N\\mathbf{X}_{tr}W_1 \\Phi^\\perp)$ and for the second attention layer we write $f_2(x) = \\mathbf{X}S(N\\mathbf{X}_{tr}W_2 \\Phi^\\perp)$."}, {"title": "TS-R Problem", "content": "Consider the TS-R problem as described in Section 2 Definition 2. Moreover, assume the setting, assumptions and the attention model above. That is, given a time series of length $L$, i.e. $x \\in \\mathbb{R}^L$, that is patched and ending with motif $x_{L-C+1}$ of size $C$ and has a unique matching motif in the time series, followed by $Y$ of size $C = H$. Moreover:\n1. Set $W_1 = c. \\mathbf{\\Phi}$\n2. Set $W_2 = c. \\mathbf{\\Phi}^\\perp R \\mathbf{\\Phi}^\\perp$\nAs $c \\rightarrow \\infty$, we have $g(f_2(f_1(x_{L-C+1}))) \\rightarrow \\Upsilon$."}, {"title": "Synthetic Retrieval Experiment", "content": "Here, we provide details about our synthetic retrieval experiment, as illustrated in Figure 2. We introduce randomness in our experimental setup through Q, a randomly sampled orthonormal matrix for each s. Consequently, each data generation process involves $L^2$ learnable parameters, ensuring that for time series of length $C < L$, the data remains essentially random for Chronos. This effect is clearly observable in the non-RAF results presented in Figure 2. However, when RAF is employed, even the smaller Chronos models with retrieval capabilities exhibit significantly enhanced performance, despite the context length being relatively small compared to $L^2$.\nNote that since Chronos is stochastic, we sampled forecasts from Chronos 20 times and took their mean values for each query, an approach also suggested by the Chronos paper. We assessed the retrieval performances under varying signal-to-noise ratios, as depicted in Figure 2. For each signal, we sampled many time series signals and averaged the metrics for all of them. From Figure 2, it is clear why Chronos-base outperforms Chronos-mini on retrieval tasks. In noisy settings, we generally observe that larger models perform retrieval better than smaller models, a trend also evident in experiments with real data."}, {"title": "Experimental Details", "content": "The experimental details for the fine-tuning process are given in Table 4. The table provides a summary of the hyper-parameter settings used for fine-tuning two approaches: Baseline and Naive RAF. Both approaches employ a data split of 70% for database formation (used only for Naive RAF), 10% for validation on which the approaches are fine-tuned, and 20% for testing. The Chronos Mini and Chronos Base models are used in both setups, with a prediction length of 10. However, there are key differences between the two approaches: the Baseline approach uses a context length of 75, while Naive RAF uses a longer context length of 160 due to the concatenation of the retrieved context and retrieved future. Still, the approaches are evaluated on the same samples during the test time. Additionally, the minimum past time steps considered are 30 for Baseline and 60 for Naive RAF. This represents the minimum number of time steps that must be retained prior to the introduction of NaN values during the training process.\nDespite these differences, other hyper-parameters remain consistent between the two approaches. Both approaches fine-tune Chronos Mini for 400 epochs and Chronos Base for 1000 epochs, with a learning rate of 0.00001. Each approach generates 20 samples during fine-tuning, as maintained in [1], and employs a linear learning rate scheduler. The optimizer for both of them is AdamW, which incorporates weight decay for regularization. Gradient accumulation occurs after each step (set to 1), and a dropout probability of 0.2 is applied to both approaches. The experiments were conducted using NVIDIA A100 40GB and L40S 48GB GPUs. Finally, for the reproducibility of the results, the seed for dataset splitting is fixed at 42 throughout every experiment."}, {"title": "Extended Results", "content": "Chronos Mini Results on Benchmark I"}, {"title": "Chronos Base Results on Benchmark I", "content": null}, {"title": "Aggregate Relative MASE Scores", "content": null}, {"title": "Qualitative Results", "content": null}, {"title": "Datasets", "content": "Benchmark I Datasets\nWeather dataset ([10]) contains hourly time series data with 3010 series for rainfall, recorded at various weather stations across Australia.\nTraffic dataset ([10]) consists of 862 hourly time series representing road occupancy rates on freeways in the San Francisco Bay area, covering the period from 2015 to 2016."}]}