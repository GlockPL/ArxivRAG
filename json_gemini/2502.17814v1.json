{"title": "An Overview of Large Language Models for Statisticians", "authors": ["Wenlong Ji", "Weizhe Yuan", "Emily Getzen", "Kyunghyun Cho", "Michael I. Jordan", "Song Mei", "Jason Weston", "Weijie J. Su", "Jing Xu", "Linjun Zhang"], "abstract": "Large Language Models (LLMs) have emerged as transformative tools in artificial intelligence (AI), exhibiting remarkable capabilities across diverse tasks such as text generation, reasoning, and decision-making. While their success has primarily been driven by advances in computational power and deep learning architectures, emerging problems in areas such as uncertainty quantification, decision-making, causal inference, and distribution shift require a deeper engagement with the field of statistics. This paper explores potential areas where statisticians can make important contributions to the development of LLMs, particularly those that aim to engender trustworthiness and transparency for human users. Thus, we focus on issues such as uncertainty quantification, interpretability, fairness, privacy, watermarking and model adaptation. We also consider possible roles for LLMs in statistical analysis. By bridging AI and statistics, we aim to foster a deeper collaboration that advances both the theoretical foundations and practical applications of LLMS, ultimately shaping their role in addressing complex societal challenges.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have emerged as transformative tools in artificial intelligence (AI), exhibiting remarkable capabilities across diverse tasks from text generation to dialog to complex reasoning. As these models increase in scale and sophistication, an important question arises: in the era of LLMs, how can statisticians play a role in guiding the design and deployment of large-scale AI models?"}, {"title": "2 Background and Fundamentals of LLMs", "content": "The development of LLMs has been a landmark event in the field of natural language processing, representing a quantum leap in the ability of machines to understand human language. This section provides a comprehensive overview of the historical development of LLMs, beginning with foundational concepts in representation learning which illustrate how models encode language into numerical forms. We then transition into advances in language modeling, involves predicting the next word in a sequence of texts, allowing for coherent and fluent text generation. Finally, we explore various architectures that have been pivotal in shaping the current landscape of LLMs."}, {"title": "2.1 Historical Development of LLMs", "content": null}, {"title": "2.1.1 Representation Learning", "content": "The first step in an LLM is to transform natural language into a format that computers can understand-specifically vectors, matrices, and tensors. The symbolic lexical units (i.e., words) that comprise sentences require effective numerical representation strategies, known as word embeddings. Grounded on the distributional hypothesis, which posits that linguistic items that occur in similar contexts have similar meanings [Har54], such methods have gradually become more sophisticated over time.\nEarly attempts at capturing word meanings involved bag-of-words representations, which represent documents as sparse vectors as in TF-IDF [Spa72], with a dimension for each element of the dictionary. Many unsupervised [Hof01; LD02] and supervised methods [Joa98; Bai+09; WBU11] involved learning (dense) word embeddings, but struggled with complex semantic and syntactic nuances of words in challenging NLP tasks. Neural methods first grew to prominence with the advent of Word2vec [Mik+13]"}, {"title": "2.1.2 Language Modeling", "content": "Early stages of language models (LMs), such as n-gram models [Jel98; GL04; Ros00], were mostly statistical regression models, relying on the Markov assumption to predict the next word from the most recent context. Challenges for these models included the need to tackle data sparsity [CG96] and the exponentially growing number of transition probabilities as n increases.\nNeural language models (NLMs) [Ben+03; Mik+10; SDG06] tackle data sparsity by mapping words to low-dimensional vectors and predicting subsequent words using neural networks. One of the earliest and most influential neural language models is based on a Recurrent Neural Network (RNN) architecture, which was first introduced by [Elm90; Jor86] and later popularized by [Mik+10]. RNNs are particularly well-suited for modeling sequential data, such as text, but can sometimes struggled with vanishing gradients and capturing long-term dependencies in the input sequence. To address these limitations, variants of RNNs were introduced, including Long Short-Term Memory (LSTM) networks [HS97] and Gated Recurrent Units (GRUs) [Chu+14]. A major advance came with the introduction of the attention mechanism [BCB15], allowing the model to focus on specific parts of the input sequence which can be a large number of positions away when generating each output token. More detail on the attention mechanism can be found in \u00a72.2. This innovation quickly led to a number of developments including stacking network layers of attention [SWF+15] and use of position embeddings [CW08; SWF+15] which showed strong performance without the use of position recurrence, culminating in the introduction of the Transformer architecture [Vas+17].\nThe Transformer, introduced by [Vas+17], revolutionized NLP by enabling deeper and more efficient language model training. Unlike LSTMs, Transformers capture global dependencies between input and output regardless of distance while crucially allowing for significantly more parallelization, enhancing their scalability. This led to the ability to train Transformer-based pre-trained models on large amounts of data, as exhibited by GPT [Rad+18] BERT [Dev+19], XLNET [Yan+19], RoBERTa [Liu+19] and T5 [Raf+20]. The results were striking. These models learn general representations of language through pre-training on large text corpora with language modeling objectives, allowing for effective fine-tuning on specific NLP tasks.\nMost LLMs today are built on the Transformer architecture, which has led to steady improvement in performance on downstream tasks by scaling both the number of model parameters and the volume of training data [Hof+22]. There are numerous popular families of LLMs available, such as the Llama [Tou+23b; Tou+23a; Dub+24a], Mistral [Jia+23; Jia+24b], GPT [Bro+20; Ope+24a], Claude families\u00b9, and DeepSeek [Bi+24; Liu+24a; Liu+24b], each offering models of varying sizes. As illustrated in Table 1, LLMs can be categorized based on their hosting requirements from small models that can run on a laptop, to medium-sized models that require a server cluster, and large proprietary models accessible via API. Whether one requires a lightweight model for personal use or a powerful model for enterprise-level tasks, there are currently LLM solutions available.\nStatisticians interested in research on actual LLMs can begin by leveraging resources and tools that lower the barrier to entry while addressing the computational challenges associated with these models. Please refer to Appendix B for more details."}, {"title": "2.2 Architectures of Pre-trained Language Models", "content": "As we have discussed, the evolution of neural language model architectures progressed from sequence-based models such as convolutional [Kim14] and recurrent [HS97] models to the more advanced Transformer models [Vas+17]. Further success came from Pre-trained Language Models (PTMs), which"}, {"title": "3 Training Pipelines of LLMs", "content": "Between 2017 and 2019, a paradigm shift occurred in the learning of NLP models. The traditional fully supervised learning paradigm began to be replaced by a two-step process: pre-training and fine-tuning [Pet+18; Rad+18; Dev+19]. In this new paradigm, a model with a fixed architecture is first pre-trained as a language model using unlabeled web data through self-supervised learning. This pre-trained language model is then adapted to various downstream tasks by introducing additional parameters and fine-tuning these parameters using task-specific objective functions. The pre-training and fine-tuning paradigm offers several advantages. Firstly, pre-training on a large text corpus enables the model to learn universal language representations, which can be beneficial for many downstream tasks. Secondly, pre-training provides a superior model initialization, which often results in better generalization performance and faster convergence on the target task. Lastly, pre-training can serve as a form of regularization, helping to prevent overfitting, particularly when dealing with small datasets [Erh+10]."}, {"title": "3.1 LLM Pre-training", "content": null}, {"title": "3.1.1 Pre-training Objective", "content": "The choice of pre-training tasks plays a pivotal role in learning universal language representations. These tasks should ideally be challenging and have substantial training data. In this section, we provide a brief overview of one of the most widely used pre-training tasks.\nStandard Language Modeling. Standard language modeling objectives focus on training the model to learn the probability P(x) of texts from a training corpus [Rad+19]. Typically, text prediction occurs in an autoregressive manner, predicting the tokens in the sequence one at a time, often from left-to-right, although other orders are possible as well. Formally, a language model is parameterized by a set of parameters \u03b8 and learns a parameterized mapping from the context x<t to the next-token xt. The goal of the model is to predict the next-token in the sequence given the context, and this is achieved by minimizing the conditional probability P\u03b8(xt|x<t). The loss function of standard language modeling objectives over a sequence of tokens x = x1,x2,\u2026\u2026,xT is \\(L_{SLM} = \\sum_{t=1}^T \\log P_\\theta(x_t|x_{<t})\\). Due to their simplicity, efficiency, scalability, and proven performance on a wide range of tasks, language modeling objectives have become the preferred choice for pre-training LLMs.\nIn addition to the standard language modeling objective, other primary training objectives such as corrupted text reconstruction [Raf+20] and full text reconstruction [Lew+20] are also widely utilized. Beyond these primary objectives, auxiliary objectives have been developed to enhance the model's performance on specific downstream tasks. Common auxiliary objectives include next sentence prediction [Dev+19], sentence order prediction [Lan+20], discourse relation prediction [Sun+20], and token-passage prediction [Liu+20b]. These auxiliary objectives are strategically employed to provide additional training signals that help refine the model's understanding and generation capabilities, thereby improving its applicability and effectiveness across a variety of NLP tasks."}, {"title": "3.1.2 Pre-training Data", "content": "The choice of pre-training datasets also plays a crucial role in the development and capabilities of LLM models. These datasets, often composed of vast amounts of text data from various sources, serve as the foundation on which LLMs learn the intricacies of language. By training on such massive corpora, LLMs acquire general language understanding and the ability to generate coherent text.\nData Source Pre-training data can be broadly categorized into two types: general pre-training corpora and domain-specific pre-training corpora. General pre-training corpora are broad datasets, covering categories such as webpages, language texts, books, academic materials, code, parallel corpora, social media, and encyclopedia [Liu+24f]. Webpages are a major source, offering extensive multilingual content that often requires significant cleaning, as seen in derivatives like RefinedWeb [Pen+24] from Common Crawl\u00b2. Language texts are sourced from large corpora like the American and British National Corpora, \u00b3,\u2074 often focusing on specific languages or domains like finance. Books provide high-quality, lengthy texts from sources such as Project Gutenberg\u2075 improve models' understanding of complex language. Academic Materials, such as those in arXiv, contribute specialized scholarly content. Code data from repositories like The Stack [Koc+23] and Github\u2077 is essential for programming tasks. Parallel corpora data, involving bilingual text pairs from resources like ParaCrawl [Ba\u00f1+20] is crucial for translation tasks. Social Media data from platforms such as StackExchange\u2078 and Reddit\u2079 helps models learn conversational dynamics, while Encyclopedia data, particularly from Wikipedia, \u00b9\u2070 strengthens models' general knowledge. Interestingly, there are some unexpected phenomena relating to how certain corpora can enhance the abilities of LLMs. For example, code data not only is essential for programming tasks, but also significantly enhances non-code performance when included in pre-training. Specifically, [Ary+24] found that initializing models with code pre-trained data led to a"}, {"title": "3.1.3 Scaling Laws", "content": "With the introduction of increasingly large language models [Bro+20; Rae+21; Smi+22], understanding computational efficiency has become crucial. The compute and energy costs for training these models are substantial [Rae+21; Tho+22], escalating with model size. In practical scenarios, the training compute budget is often predetermined, depending on the availability of accelerators and the duration of their use. Given that training large models is usually feasible only once, accurately estimating optimal hyperparameters for a given compute budget is essential. Kaplan et al. [Kap+20] provided initial insights into computational efficiency for language models, discovering a power law relationship between the number of parameters in an autoregressive LM and its performance. They suggested that with a tenfold increase in computational budget, model size should increase by 5.5X, while the number of training tokens should only increase by 1.8X. This led to a trend of training larger models to achieve performance gains. Scaling laws have been shown to apply across various data modalities, including language, images [Che+20], and videos [WTU20], as well as multimodal modeling [Tsa+19] and even mathematical problem solving [Sax+19].\nBeyond model size and training tokens, [Tay+23] derived scaling laws for different inductive biases and model architectures, revealing significant variations in scaling coefficients among models. They found that among ten architectures, the vanilla Transformer exhibited the best scaling behavior, despite not having the highest absolute performance at each compute region.\nIn 2022, the Chinchilla scaling law [Hof+22] shifted the focus from model size to the number of training tokens for computational efficiency. This law suggests that training slightly smaller models on larger datasets is often more efficient than the previous approach [Kap+20], which favored larger models on smaller datasets. Compared to Kaplan's study, key differences introduced by the Chinchilla study include: (1) a different learning rate schedule for all models, unlike [Kap+20], which did not account for the impact of these hyperparameters on the loss; and (2) the inclusion of larger scale models. The Chinchilla model, with 70B parameters trained on 1.4T tokens (approximately 20 tokens per parameter), outperformed its much larger counterpart, Gopher [Rae+21].\nThe \"Chinchilla efficient\" model size and training dataset size, along with the achievable test loss, can be determined as follows:\n\\begin{align}\nN_{opt} (C) &= 0.6 C^{0.45} \\\\\nD_{opt} (C) &= 0.3 C^{0.55} \\\\\nL_{opt} (C) &= 1070 C^{-0.154} + 1.7,\n\\end{align}\nwhere \\(N_{opt}\\) represents the optimal number of model parameters, \\(D_{opt}\\) denotes the optimal number of training tokens, and \\(L_{opt}\\) indicates the optimal final pre-training loss achievable under a fixed FLOPs"}, {"title": "3.2 LLM Prompting", "content": "For the largest language models, the paradigm has shifted for typical users from traditional supervised learning to prompt-based learning, often called prompt engineering [Liu+23]. In supervised learning, we use labeled data consisting of input-output pairs D = (xi, yi)i=1\u2026N to tune the model parameter \u03b8 so that we can predict the output for an input x that is not in the training data using P\u03b8(y|x). However, in prompt engineering, the model parameter \u03b8 is fixed, and instead, one tunes a template t which is combined with x to form a new input x' that achieves good performance when using P\u03b8(y|x'). In other words, prompt engineering learning involves optimizing a template t that is used to generate a new input x' from the original input x, rather than optimizing the model parameters directly. This approach allows us to adapt the model to new inputs without requiring retraining of the model. We describe two main prompt-based learning techniques as follows.\nVanilla Prompt Engineering Vanilla prompt engineering involves the development of effective input prompts for LLMs to generate better outputs [Liu+23]. Traditionally, prompts were manually crafted based on intuitive templates, a process that requires considerable expertise and may not always yield optimal results. To overcome this limitation, automated methods have been introduced, categorizing prompts into discrete and continuous types. Discrete prompts, also known as hard prompts, involve natural language phrases and can be discovered through methods like prompt mining from large corpora [Jia+20], paraphrasing existing seed prompts [YNL21], gradient-based search over tokens [Wal+19], and using LLMs to generate prompts based on inputs [GFC21]. Continuous prompts, or soft prompts, operate within the embedding space of the model and do not require human-interpretable language. Prefix-tuning is an example of this approach [LL21], where continuous task-specific vectors are prepended to inputs, allowing the LM to perform the task more effectively without altering its parameters. In addition, some methods, such as P-tuning [Liu+24e] and PTR [Han+22], enhance hard prompt templates by incorporating some tunable embeddings, rather than relying solely on purely learnable prompt templates. These approaches blend the structure of hard prompts with the flexibility of trainable soft tokens, improving prompt performance and adaptability."}, {"title": "3.3 LLM Supervised Fine-Tuning (SFT)", "content": "Even with extensive pre-training, LLMs may not excel at specific tasks without further adjustment. To achieve improvements of this kind, a process known as Supervised Fine-Tuning (SFT), which involves fine-tuning with labeled data, is often necessary. For instance, the BERT paper [Dev+19] demonstrated the effectiveness of fine-tuning the model on 11 distinct tasks. Although more recent LLMs can perform tasks through in-context learning [Bro+20] or zero-shot prompting [Liu+23] without prior fine-tuning, they still stand to gain from fine-tuning tailored to specific tasks or datasets. A notable example is OpenAI's GPT-3.5 Turbo, which, despite its smaller size compared to GPT-4, could achieve superior performance when fine-tuned with task-specific data.\u00b9\u00b9\nSFT does not need to be based on a single task. Indeed, to further improve LLMs' performance and address data scarcity, researchers are also increasingly adopting Multi-Task Learning (MTL) [Car98] for NLP tasks. This approach trains models on multiple related tasks simultaneously, broadening the training dataset and reducing overfitting risks [CZY24]. MTL not only captures generalized and task-"}, {"title": "3.3.1 Instruction Tuning", "content": "The transformative idea that any NLP task can be converted into a text-to-text format has significantly aligned with advances in generative language models [Raf+20]. This task paradigm shift allows for the fine-tuning of language models across a broad spectrum of NLP tasks using an unified data format [Mis+22], thereby ensuring a uniform training objective. A pivotal development in this area is \"instruction tuning\" where a language model is fine-tuned on a collection of tasks described via instructions [Wei+22b]. This method has been shown to improve the zero-shot performance of language models on unseen tasks. Models such as FLAN [Wei+22b] and TO [San+22] exemplify this approach. These models are trained to process a variety of NLP tasks through instructional prompts, setting new performance benchmarks and demonstrating an impressive ability to generalize to tasks they were not explicitly trained on. This evolution towards instruction-based task execution highlights the critical role of multi-task learning in increasing the robustness and adaptability of language models [CZY24].\nThe significance of instruction tuning is further highlighted in the InstructGPT paper [Ouy+22], which utilizes a novel dataset comprising prompts crafted by labelers and those submitted to early InstructGPT models via the OpenAI API. This dataset includes a wide spectrum of tasks such as brainstorming, rewriting, open-ended question answering, and more, reflecting the diverse and user-centric nature of modern NLP applications. Labelers demonstrate the desired responses to prompts, providing training data for SFT using GPT-3. The InstructGPT findings emphasize the necessity of moving beyond traditional NLP tasks to include user-centric tasks like brainstorming, which are not adequately captured by traditional NLP datasets. This expansion not only broadens the scope of tasks that models are trained on but also enhances their ability to perform effectively on real-world, user-driven tasks, marking a significant shift towards more adaptive and user-focused language models."}, {"title": "3.3.2 Parameter-Efficient Fine-Tuning", "content": "Given the sheer size of LLMs, a common approach to fine-tuning is to modify a small fraction of the model's parameters while leaving most of them unmodified. This approach, called \"Parameter-Efficient Fine-Tuning\" (PEFT), focuses on selectively tuning a limited number of parameters to achieve the desired performance gains without completely modifying the entire model. The PEFT strategies can be broadly classified into three types [Han+24b]: (1) Additive fine-tuning, which injects new trainable modules or parameters into the original model architecture. (2) Selective fine-tuning, which trains only a subset of model parameters during fine-tuning. (3) Reparameterized fine-tuning, which constructs a low-dimensional reparameterization of the original model parameters for training.\nAdditive Fine-Tuning Additive fine-tuning strategies, such as adapters [Hou+19; He+22b] and soft prompts [LL21; Liu+24e], introduce only a minimal number of trainable parameters that are strategically positioned within the model architecture. Adapters are small layers inserted within Transformer blocks, consisting of a down-projection matrix, a nonlinear activation function, and an up-projection matrix. These layers act as computational bottlenecks, refining the model's output while leveraging the existing pre-trained parameters. On the other hand, soft prompts involve appending adjustable vectors at the beginning of the input sequence, enhancing the model's ability to utilize the rich information within the continuous embedding space. This method adjusts the initial conditions of the model's input processing, allowing for fine-tuned performance improvements without extensive retraining of the core model components. Both approaches maintain the original model architecture unmodified while providing targeted enhancements for specific tasks.\nSelective Fine-Tuning Unlike additive PEFT, selective PEFT fine-tunes only a specific subset of the existing parameters within a model. This is achieved by applying a binary mask to the model's parameters, where each element of the mask is either 0 or 1, indicating whether the corresponding parameter should be updated during fine-tuning. Only the selected parameters are adjusted based on the gradients of the loss function, using a predefined learning rate. This method allows for targeted improvements on downstream tasks by optimizing a limited number of model parameters, thereby maintaining the overall efficiency and scalability of the model. Techniques such as Diff pruning [GRK21],"}, {"title": "3.4 System 2 Prompting and Chain-of-Thought", "content": "\"System 2 prompting\" refers to prompts that elicit a deliberate reasoning-like process in AI models that takes the form of the generation of intermediate steps before arriving at a final response [WS23; Yu+24]. This contrasts with \"System 1 prompting,\" where a model directly produces a response without intermediate steps. Inspired by human cognitive processes, System 2 prompting is designed to handle complex reasoning tasks that System 1 might struggle with, by using techniques like Chain-of-Thought [Wei+22a], Tree-of-Thoughts [Yao+23], Graph-of-Thoughts [Bes+24], Branch-Solve-Merge [Sah+24], System 2 Attention [WS23], Rephrase and Respond [Den+24] and others. These techniques aim to improve performance in areas like multi-step reasoning [Ran+24], mathematical problem solving [Wei+22a], and commonsense reasoning [Zha+24h]. While System 2 methods can lead to more accurate and interpretable outcomes, they typically involve higher computational costs and latency [Ope+24b].\nInference-Time Scaling Law In addition to the established scaling laws for training LLMs, recent shifts in focus have highlighted the significance of inference-time scaling laws, particularly following the introduction of OpenAI's o1 model [Ope+24b], which is designed to extend various computational steps before generating responses. This can be achieved, for example, by (1) generating multiple candidate responses and selecting the best using methods such as automatic verifiers [Bro+24], reward models [Nak+22], or self-consistency [Wan+23d; Che+23b], or (2) enhancing the reasoning process within a single trial by introducing more intermediate thinking steps like reflection and revision [Ope+24b; Qin+24; Hua+24b]. For instance, [Bro+24] demonstrated that across multiple tasks and models, the coverage defined as the fraction of problems solved by any attempt significantly scales with the number of samples across four orders of magnitude. Complementing this, [Sne+24] showed that optimizing inference-time computation through a combination of (1) searching against dense, process-based verifier reward models, and (2) adaptively updating the model's response distribution based on the test-time prompt can yield greater performance improvements than merely scaling model parameters. Furthermore, [Dee+25; Tea+25] observed that by directly optimizing for outcome-based rewards, the system can self-evolve and scale its inference time without external intervention, high-"}, {"title": "3.5 LLM Reinforcement Learning & Preference Optimization", "content": "Current strong LLMs can be prompted to perform a variety of tasks. However, these models sometimes exhibit unintended behaviors, such as making up facts or producing biased or toxic content [Wei+21; Bom+22; Ken+21]. This issue has been framed as a lack of \u201calignment\u201d of the models, where alignment in LLMs is defined in terms of desiderata such as \"ensuring that models are helpful, honest, and harmless\" [Ask+21]."}, {"title": "3.5.1 Approaches to Aligning LLMs", "content": "To tackle the issue of aligning LLMs, researchers have developed two distinct categories of approaches: (1) Reward-based methods, which involve training a reward model using preference data and subsequently optimizing the model's behaviors to maximize the reward received; and (2) Reward-free methods, which eliminate the reward model altogether and instead directly utilize human preferences to train the LLM.\nReward-based methods Many leading proprietary LLMs, including GPT-4 [Ope+24a] and Claude 3,\u00b9\u00b2 employ reward-based methods for alignment, specifically Reinforcement Learning from Human Feedback (RLHF) [Sti+20]. The RLHF pipeline usually includes three phases: (1) supervised fine-tuning (SFT); (2) reward modeling and (3) RL optimization.\n1. SFT Phase: RLHF typically starts by fine-tuning a pre-trained LM with supervised learning, using high-quality data across a large and diverse set of instruction following tasks. This process aims to establish a well-prepared initial model, denoted as \\(\\pi_{SFT}\\), which serves as a good foundation for subsequent training stages.\n2. Reward Modeling Phase: During this phase, the SFT model is prompted with input x to produce pairs of answers (y1, y2) ~ \\(\\pi_{SFT}(y|x)\\). These pairs are then evaluated by human labelers who indicate their preference between the two, represented as \\(y_w \\succ y_l |x\\), where \\(y_w\\) and \\(y_l\\) denotes the preferred and dispreferred response among (y1, y2) respectively. The preferences are assumed to be generated by some latent reward model \\(r^*(y, x)\\), which we do not have access to. A common approach to modeling these preferences is the Bradley-Terry (BT) model [BT52], which posits that the human preference distribution p* can be expressed as:\n\\[P^* (y_1 \\succ y_2|x) = \\frac{\\exp(r^*(x, y_1))}{\\exp(r^*(x, y_1)) + \\exp(r^*(x, y_2))}.\\]\nGiven a static dataset of comparisons D = \\{(x^{(i)}, y_w^{(i)}, y_l^{(i)})\\}_{i=1}^N sampled from p*, we can parametrize a reward model \\(r_\\theta(x, y)\\) and estimate its parameters via maximum likelihood. This setup is treated as a binary classification problem, where the negative log-likelihood loss is defined as:\n\\[L_R(r_\\theta, D) = -E_{(x,y_w,y_l) \\sim D} [\\log \\sigma(r_\\theta(x, y_w) - r_\\theta(x,y_l))], \\]\nand where \u03c3 is the logistic function. In the context of LMs, the network \\(r_\\theta(x, y)\\) is often initialized from the SFT model \\(\\pi_{SFT}(y|x)\\) with an additional linear layer on top of the final Transformer layer that produces a single scalar prediction for the reward value [Zie+20]. To ensure a reward function with lower variance, it is common practice to normalize the rewards, such that \\(E_{x,y \\sim D} [r_\\theta(x, y)] = 0\\) for all x. Additionally, having a separate reward model offers the advantage of utilizing it for rejection sampling during inference time. This process involves generating multiple responses"}, {"title": "3.6 LLM Self-Alignment", "content": "Aligning LLMs using human feedback is often bottlenecked by the size and quality of human-annotated data. As models reach or surpass human-level intelligence in particular domains, it is expected that future models may require feedback that goes beyond what humans can provide in order to provide an adequate training signal. Leveraging the LLM itself to provide such feedback, in particular to create high-quality data for instruction-finetuning, called \"synthetic data,\" has become a promising and scalable solution."}, {"title": "3.6.1 Synthetic Data Generation", "content": "Current alignment methods in synthetic data generation often involve several key components: (a) Instructions, (b) Responses, and (c) Feedback on those responses."}, {"title": "4 Designing Trustworthy LLMs by Statistical Methods", "content": "As LLMs increasingly permeate various aspects of society, ensuring their trustworthiness has become a critical challenge. Trustworthiness encompasses a range of dimensions, including interpretability, accountability, and algorithmic fairness. Statistical methods offer a rigorous and systematic approach to address these challenges, providing tools to analyze, enhance, and monitor the behavior of LLMs. This section explores how statistical techniques can contribute to the design of trustworthy LLMS across several key areas. First, we discuss mechanistic interpretability, which aims to uncover how LLMs make predictions and generate outputs. Next, we examine uncertainty quantification, a vital"}, {"title": "4.1 Uncertainty Quantification", "content": "While LLMs produce human-like responses with impressive accuracy across various tasks, they are also prone to hallucination [Ji+23; RSD23], raising concerns about their reliability. Quantifying uncertainty is crucial for addressing these limitations, as it allows models to provide not just answers but also confidence in their outputs, enabling users to make more informed decisions.\nUncertainty Metrics While uncertainty estimation and calibration are well-established for traditional machine learning models [Abd+21; Gaw+23], the emergence of LLMs has introduced new challenges and demands. Unlike fixed-dimensional outputs typical of traditional models, LLM responses are often complex, requiring uncertainty metrics that can operate on sentence-level outputs. This complexity necessitates innovative approaches to quantify uncertainty effectively. Classical metrics, such as entropy, can be directly calculated on the probability distribution of next-token prediction and averaged over all tokens [MG21]. To incorporate the special structure of language model, existing approaches further considered semantic features such as semantic similarity [Fom+20; LLS22], semantic equivalence [KGF23] and token importance [Dua+24b; Bak+24], as well as internal signals in language models like logits and hidden states [Kad+22; Che+24a; Liu+24c] into the metric design. In general, they can be easily computed in a white-box setting where the underlying representation and prediction distribution are available, while for black-box models, some can be calculated via repeated sampling of the response [Kad+22; LTS24; CM24]. Overall, these approaches aim to develop robust uncertainty metrics capable of appropriately assessing the confidence of LLM-generated responses in a meaningful and scalable manner.\nConformal Prediction in LLMS Conformal Prediction (CP) [VGS05; AB23] has emerged as a versatile framework for distribution-free statistical inference. CP constructs confidence sets for predictions based on the empirical distribution of residuals, ensuring validity without assumptions about the underlying model or data. Its flexibility and computational efficiency have made it an appealing tool for LLMs despite challenges like large output spaces and non-exchangeable token sequences. To address these issues, works such as [Kum+23; Ren+23] have restricted the output space, applying CP to tasks like multiple-choice question answering and robot actions, while [RGG23] calibrated nucleus sampling to improve token-level predictions. Other methods, such as [UZM24], adapt CP to non-exchangeable settings by leveraging latent representations for nearest-neighbor searches, resulting in more precise prediction sets. Beyond improving accuracy, CP has been extended to control quantities like toxicity and hallucination risks [Zol+24; Yad+24; MH24; CGC24], enabling safer and more aligned LLM applications. CP methods have also been leveraged for evaluating LLM performance. [Ye+24a] applied CP to benchmark LLMs on five natural language processing tasks, measuring average confidence set sizes for multiple-choice questions to quantify uncertainty. In machine translation, [Gio23] and [ZM24] used CP to assess translation quality, providing calibrated confidence estimates for both human and machine evaluations. Additionally, [Sch+21; Sch+22b] proposed confident early exiting methods for Transformers, where intermediate layers assess uncertainty to speed up inference while maintaining consistency with the full model.\nHallucination Detection Recently, there has been a growing trend toward adopting uncertainty estimation methods to address hallucination detection in LLMs. The core idea is that the logits and hidden states of LLMs encapsulate information about the model's confidence in its generated output, which can be leveraged to identify hallucinations. For example, [AM23; Slo+23; Che+24a] use activa-"}, {"title": "4.2 LLM Watermarking", "content": "The capability of LLMs to generate human-like text has raised significant concerns regarding potential misuse. For instance, StackOverflow, a leading question-and-answer platform for programmers, implemented a temporary ban on AI-generated responses to prevent the dissemination of misleading information [Vin22", "Zel+19": "academic integrity [MML23", "Shu+23": ".", "KKO24": "whereas others exploited structural properties of LLMs for detection [Ipp+20; GSR19; Mit+23", "Tul+24": "."}]}