{"title": "Predictive Coding Networks and Inference Learning: Tutorial and Survey", "authors": ["BJ\u00d6RN VAN ZWOL", "RO JEFFERSON", "EGON L. VAN DEN BROEK"], "abstract": "Recent years have witnessed a growing call for renewed emphasis on neuroscience-inspired approaches in artificial intelligence research, under the banner of NeuroAI. This is exemplified by recent attention gained by predictive coding networks (PCNs) within machine learning (ML). PCNs are based on the neuroscientific framework of predictive coding (PC), which views the brain as a hierarchical Bayesian inference model that minimizes prediction errors from feedback connections. PCNs trained with inference learning (IL) have potential advantages to traditional feedforward neural networks (FNNs) trained with backpropagation. While historically more computationally intensive, recent improvements in IL have shown that it can be more efficient than backpropagation with sufficient parallelization, making PCNs promising alternatives for large-scale applications and neuromorphic hardware. Moreover, PCNs can be mathematically considered as a superset of traditional FNNs, which substantially extends the range of possible architectures for both supervised and unsupervised learning. In this work, we provide a comprehensive review as well as a formal specification of PCNs, in particular placing them in the context of modern ML methods, and positioning PC as a versatile and promising framework worthy of further study by the ML community.", "sections": [{"title": "1 INTRODUCTION", "content": "Neuroscience-inspired approaches to Machine Learning (ML) have a long history within artificial intelligence research [33, 49, 77]. Despite the remarkable empirical advances in ML capabilities in recent years, biological learning is still superior in many ways, such as flexibility and energy efficiency [49]. As such, recent years have seen a growing call for renewed emphasis on neuroscience-inspired approaches in AI research, known as NeuroAI [1, 103]. This is exemplified by the rising popularity of the predictive coding (PC, also known as predictive processing) framework in computational neuroscience [21, 27, 73] and its recent entrance into the field of machine learning [55, 99]. PC represents a Bayesian perspective on how the brain processes information, emphasizing the role of probabilistic models and minimization of prediction errors in perception and learning [21, 26, 27, 73]. At its core, it suggests that the brain continually generates predictions about sensory input and updates these predictions based on incoming sensory data. By removing the predictable components, this reduces redundancy in the information processing pipeline [35].\nCentral to PC is the concept of hierarchical prediction: the lowest level of the hierarchy represents sensory data, and each higher level attempts to predict neural activity in the layer below [73]. Prediction errors, arising from discrepancies between actual and predicted activity, are propagated upward through the hierarchy, while the predictions from higher levels are propagated downwards via feedback connections; see fig. 1. In the influential work by Rao & Ballard (1999), the authors showed that such a predictive coding network (PCN) can learn statistical regularities in the input and explain several neural responses in the visual cortex [12, 21, 61, 88, 98]. Since then, the PC framework has found wide adoption across neuroscience, psychology, and philosophy [16, 87]. It is often seen as part of 'active inference', a broader umbrella term which aims to provide an integrated perspective on brain, cognition, and behavior [69].\nAlthough PC was partly inspired by ML methods [17, 73], within the ML community it appears to have gone mostly unnoticed until recently. In 2017, Whittington & Bogacz applied PCNs to supervised learning tasks, revealing notable properties that connect them to traditional feedforward neural networks (FNNs). First, PCNs become equivalent to FNNs during inference (testing) [22, 89, 99]. Second, PCN's training algorithm, called inference learning (IL) can be related to backpropagation (BP), the workhorse training algorithm of modern deep learning. These properties sparked a surge of interest in PCNs [54, 57, 83, 84, 90], suggesting IL as a more biologically plausible alternative for training deep learning architectures.\nUntil recently, implementations of PCNs were more computationally intensive than equivalent BP-trained networks, helping to explain the lack of wider adoption of IL in ML applications. However, recent work has achieved marked improvements in performance, showing it can achieve higher efficiency than BP with sufficient parallelization [5, 85]. This is because IL's computations use only locally available information, such that the serial updates inherent to BP can be avoided. If sufficiently parallelized, this means that computation time no longer scales with depth in PCNs [85]. As FNNs become increasingly large (deep), this could provide a marked advantage compared to BP, and further suggests PCNs are a promising candidate for use on neuromorphic hardware [55, 85].\nWhile recent work has compared PCNs to FNNs, in particular focusing on supervised learning, fundamentally PCN is a probabilistic (Bayesian) model, naturally formulated for unsupervised learning. In fact, even before it was used within ML, the PCN of Rao & Ballard was phrased in terms familiar to the ML community [51, 73]. Specifically, it is defined by a graphical model with latent (unobserved) variables, trained using generalized Expectation Maximization [7, 24], a general procedure for maximum likelihood in latent variable models (also used, e.g., in k-means clustering) [7, 18, 59]. Its objective (i.e., cost function) is the complete data log-likelihood or variational free energy when seen as a variational inference procedure [7, 25, 27]. Hence, from this perspective, PCNs are most appropriately compared not to FNNs, but to techniques associated with generative modeling (e.g., variational autoencoders, diffusion models [72]) and classic latent variable models (e.g., probabilistic PCA [7], factor analysis [30]). In other words, PC can be seen both as a learning algorithm contrasted with BP, and as a probabilistic latent variable model comparable to generative models.\nIn PCNs, the difference between supervised and unsupervised learning comes down to the direction of predictions within the network. In the supervised setting, these flow from data to the labels, while in the unsupervised setting, they flow towards the data (fig. 1). Recent work [81] showed how this direction can be extended to a broader notion of topology by defining IL on any graph, known as PC graphs. This allows the training of non-hierarchical, brain-like structures, going beyond hierarchical to heterarchical prediction. This means PCNs effectively encompass FNNs, allowing the definition of a more general class of neural networks. While we are unaware of a formal proof in the literature, one expects that these networks share with FNNs the appealing property of being universal function approximators, but they also allow a new collection of structures untrainable with BP to be studied. Although the study of such structures is in its infancy, this means they form a superset of traditional ANNs (fig. 2). This understanding of PC graphs as generalized ANNs follows from [81], but has not been previously phrased in this way.\nThe different perspectives on PCNs for ML (fig. 3) are warranted by their mathematical structure, as outlined in [10, 26, 27, 51]. Substantial progress has been made since these works, however, both theoretically and in applying PCNs to larger datasets. Although some of this progress was surveyed in [54, 55], and more recently in [79, 94], these works favor broad coverage over mathematical detail. Additionally, the only tutorial on PCNs dates from 2017 [8], before most work on PC in ML was published. Hence, there is a need for a more comprehensive formal specification, and a tutorial aimed at ML practitioners. Our work aims to provide both in the form of a combined tutorial/survey paper, as well as to clarify some minor mathematical errors in the literature.\nWe focus on PC as a neuro-inspired branch of ML, meaning we choose not to discuss the large body of work on PC in neuroscience, psychology, philosophy, active inference, and robotics (see aforementioned references and [43, 69] for reviews). Specifically, we do not discuss the neurologically-relevant issue of biological plausibility, often discussed in the PC literature (see e.g. [29, 91]).\nIn summary, the contributions of this work are as follows:\n\u2022 Provide a comprehensive formal specification of modern PCNs, clarifying some mathematical issues in earlier work.\n\u2022 Summarize and provide structure for recent theoretical and empirical results on PCNs, integrating different perspectives (fig. 3), as both a unified basis for future work, and an accessible starting-point for machine learning practitioners.\n\u2022 Explain connections with existing methods in ML, some of which have remained unexposed in recent literature. In particular, we highlight how the structure of PCNs and PC graphs forms a mathematical superset of ANNs. This follows from earlier work [81], but to our knowledge was not yet pointed out as a general conception of PCNs.\nOur aim is to provide a technical reference while remaining accessible, providing a useful starting point for researchers seeking to understand PCNs and pursue future work in this area.\nThe three perspectives in fig. 3 define the structure of this work. While interconnected, making a distinction facilitates an understanding of PCNs. Section 2 interprets PCNs as generalized ANNs (fig. 2),which should provide a familiar context for many ML researchers. Section 3 discusses PC as probabilistic latent variable models, and section 4 as a learning algorithm, focusing on theoretical results on IL as compared to BP. The literature on PCNs is organized in Table 1 by network type (section 2), which we survey throughout this work. We conclude in section 5, with appendices including an extended discussion of section 3 (appendix A), complementary proofs (appendix B.2), and a discussion of computational complexity (appendix C)."}, {"title": "2 PCNS AS GENERALIZED ANNS", "content": "The theory of PC as a machine learning algorithm is often derived from variational inference in a hierarchical Gaussian latent variable model. The resulting equations bear a lot of similarity to those of artificial neural networks (ANNs). In particular, PCNs of a particular type, sometimes called discriminative PCNs, are in an important sense equivalent to feedforward neural network during inference (testing). Since we believe ANNs to be more familiar context for most ML researchers and practitioners than variational inference, we will here take ANNs as a starting point, with the aim of making the generalization to PCNs more transparent.\nThus, we review briefly the theory of the simplest ANN: a feedforward neural network (FNN; also known as a multilayer perceptron) in a supervised learning context, as taught in introductory ML classes, after which we review discriminative PCNs. To understand similarities and differences between both networks, we discuss in detail their training procedure, learning algorithm, testing procedure, and structure.\nProblem Setup. We are given a dataset of N labelled samples {(xn),yn)}Nn=1 split into a training set and a test set, where x(n) \u2208 X is a datapoint and y(n) \u2208 Y its corresponding label [7], defined on input and output domains X, Y, respectively.\n2.1 FNNs\n2.1.1 Training Procedure. FNNs are defined by a set of activation nodes organized in a hierarchy of L layers: al \u2208 Rnl+1, where l = 0, ..., L is the layer and nl the number of nodes in that layer (a constant value of 1 is added to account for the bias). Layers are connected by a set of weight matrices wl \u2208 Rnl\u00d7(nl\u22121+1) in the following way [7, 30]:\nal = f(wl\u22121al\u22121), (1)\nwhere throughout, the biases bl are absorbed as an additional column in the weight matrix (as is commonly done [30]). We will call this equation the activity rule since it states how activation nodes are computed. The function f : Rnl \u2192 Rnl here is a nonlinear activation function (e.g. sigmoid, ReLU). Setting the bottom layer a0 to a datapoint x(n), the final layer aL is taken as the predicted label \\hat{y} = \\hat{y}(w,x^{(n)}). We also define the argument of f as the pre-activation zl = wl\u22121al\u22121. Then, notice that the neural network is simply a function NN : X \u2192 Y, such that \\hat{y} = NN(x(n)) and\nNN(x) = (aL \u25e6 aL\u22121 \u25e6 ... \u25e6 a0)(x). (2)\nThus, an FNN is a function composition of linear and nonlinear operations defined through (1). The produced output \\hat{y} is compared to the true label y(n) using a loss function:\nL(w) = N\u2211n=1 Ln(\\hat{y},y^{(n)}),\nwhere the dependence on weights is through \\hat{y} (note that the sum here is over the datapoints, to simplify the comparison with PCNs; typically, with mini-batch learning, one sums over the batches instead). In practice, the loss is often chosen based on properties of the task, e.g., the Mean Squared Error (MSE) loss Ln(y(n), \\hat{y}) = (\\hat{y} \u2212 y(n))2 is useful for regression-type learning. Then, the optimal weights are those that minimize the loss function, which defines the learning rule for ANNs. Since L is generically a complicated non-convex function of the weights, determining an optimal set of weights can only be done numerically with gradient descent:\nw\u2217 = argminwL(w) \u21d2 \u0394wl = \u2212\u03b1 \u2202L\u2202wl,\n(3)\nwhere the weights are updated iteratively using \u0394wl, where \u03b1 is the learning rate. For better training efficiency, in practice one uses stochastic gradient descent (SGD), often with optimizers such as Adam [39], though this will not be important for our purposes.\n2.1.2 Learning Algorithm. Calculating these weights efficiently is done using backpropagation or backprop (BP), which is the standard workhorse algorithm used in deep learning. In BP, the error in layer l is the derivative of the loss w.r.t. the activation al, i.e.\n\u03b4l = \u2202L\u2202al (4)\nIt can be shown (appendix B.1) that the right-hand side of (3) is\n\u0394wl = \u03b1\u03b4l+1 \u2297 f'(wl al)(al)T (5)\nwhere\n\u03b4l = {\\hat{aL}\u2212y(n)l=L(wl)T\u03b4l+1 \u2299 f'(wl al)l<L(6)\nIt can be observed how errors are computed from the final to the first layer, explaining the name backpropagation of errors. Pseudocode is shown in alg. 1.\nAlgorithm 1 Learning {(xn),yn} with BP\nRequire: : a0 = x(n) .\n1: for l = 0 to L \u2212 1 do\n2: al+1 = f(wl al)\n3: end for\n// Forward pass\n4: \u03b4L = \u2202L\u2202al\n5: for l = L \u2212 1 to 0 do\n6: \u03b4l = (wl)T\u03b4l+1 \u2299 f'(wl al)\n7: wl \u2190 wl \u2212 \u03b1 \u2202L\u2202w\n8: end for\n// Backward pass\n// Weight update\n2.1.3 Structure. Fig. 4 shows the typical way to visualize an FNN. Connections are defined by (1). As such, we can say that this equation defines the structure of the network.\n2.1.4 Testing Procedure. Finally, after a network has been trained, testing the network is done by using the activity rule (1) to obtain predicted outputs \\hat{y} from datapoints x(n).\n2.2 Discriminative PCNS\n2.2.1 Direction in PCNs. Before explaining PCNs in detail we briefly mention a convention issue. The literature contains conflicting notions of direction in PCNs, which can cause confusion and are worth discussing at the outset. There are two connected issues. The first is mathematical: should local predictions \u03bcl be defined as \u03bcl = f(wl\u22121al\u22121) (as in this section) or \u03bcl = f(wl+1al+1) (implied by fig. 1)? The second issue is terminological: how should the words forward/backward be used?\nTypically in the neuroscientific PC literature [26, 73], predictions are \u03bcl = f(wl+1al+1), and data is clamped to l = 0 (fig. 1). This means that predictions go towards data (and errors go away from data). In this 'PC convention', the word forward is typically chosen to mean away from the data, which is the direction of errors.\nHowever, in the supervised learning context, for a PCN to be equivalent to FNNs during testing, it turns out that one needs to have predictions go away from data. With the conventions above, this means one would have to clamp data to the highest layer l = L, which is confusing from the perspective of standard ML. This also leads to ambiguity as to whether \u2018forward\u2019 should mean towards increasing layer number, or away from the data.\nThe alternative when comparing PCNs to FNNs is to keep data clamped to l = 0 but change the direction of local prediction to \u03bcl = f(wl\u22121al\u22121), as is done by many recent works [5, 6, 11, 36, 54, 56, 57, 71, 76, 84, 91, 106]. Forward then means away from the data (equivalently, towards higher layers) as in the PC convention above, except that this is now the direction of predictions. In other words, rather than swapping the data clamping from l = 0 to L, we instead swap the directions of predictions and errors relative to fig. 1, so that \u2013 when comparing PCNs to FNNs specifically \u2013 predictions go forwards and errors go backwards.\nThe disadvantage of this choice is that \u03bcl = f(wl+1al+1) with data at l = 0 is remains the more sensible convention for PCNs when seen as Bayesian/generative models (section 3) [60, 64, 73, 82]; it is only in the comparison with FNNs where some inversion (either of clamping or of directions) is required. Hence, in this work we use both conventions depending on the use of the network. We define discriminative PCNs as \u03bcl = f(wl\u22121al\u22121) (for supervised learning), and generative PCNs as \u03bcl = f(wl+1al+1) (for unsupervised learning; this reflects also the use of these terms in [36, 55]). As for terminology, we can then unambiguously use \u2018forward\u2019 to mean away from the data in all cases, consistent with standard ML. One must simply bear in mind that for discriminative PCNs, \u2018forward\u2019 reflects the direction of predictions, while for generative PCNs it refers to the direction of errors.\n2.2.2 Training Procedure. As with FNNs, we define activity nodes al. In line with the fundamental notion of PC, the layer al\u22121 now tries to \u2018predict' this activity. The local prediction is defined as\n\u03bcl = f(wl\u22121al\u22121) (7)\n(not to be confused with the \u2018global' prediction \\hat{y} of the network as a whole). Note that predictions here go towards lower layers, which is opposite to what is suggested by fig. 1. We discuss this convention in detail in section 2.2.1. The weights and biases are defined just like in FNNs. The discrepancy between actual and predicted activity is\n\u03b5l = al \u2212 \u03bcl (8)\ni.e. the prediction error. The objective function in PC is the sum of squared prediction errors:\nE(a,w) = 12 \u2211l(\u03b5l)2, (9)\noften called the energy function. Training now happens by setting the bottom layer a0 to the datapoint x(n), like in FNNs, and additionally fixing the final layer aL to the correct label y(n). The activity rule is not defined as in (1); instead, the updated values of the hidden (unclamped) activity nodes (i.e. l = 1, ..., L \u2212 1) are the minimum of the energy function:\n\\hat{a}l = argminE(a,w).\nal(10)\nThis is called the inference phase, since we are inferring the appropriate activation values for hidden nodes, given the clamped nodes (see section 3 for a detailed discussion of the probabilistic interpretation). Note that we use a hat for the optimized values where E is at a minimum.\nTypically, finding the minimum \\hat{a} requires gradient descent (an important exception is the testing of discriminative PCNs, as will be discussed below). Taking the derivative using (9), one obtains:\n\u0394al = \u2212\u03b3 \u2202E\u2202al (11)\n= \u2212\u03b3 (\u03b5l \u2212 (wl)T(\u03b5l+1 \u2299 f'(wl al))).\nHere, \u03b3 is the inference rate: a step size required by gradient descent (i.e., the learning rate for activation nodes). As such, to find \\hat{a} during training, one does T iterations of (11) until convergence. The weights are now found at the minimum of the same energy function, which defines a learning rule similar to FNNs:\nw\u2217 = argminwE(a,w) \u21d2 \u0394wl = \u2212\u03b1 \u2202E\u2202wl, (12)\nagain using a hat for the values after minimizing, and a step-wise gradient descent updates which can be done using SGD or Adam. The fact that (11) is trained to convergence before the weight update is performed suggests that training a PCN is more computationally costly than FNNs. However, this added cost can be at least partially circumvented in practice, as we discuss in section 4. Additionally, note that since the energy function is a sum over layers, the update rule does not require backpropagating errors. Instead, taking the derivative in (9) simply gives:\n\u0394wl = \u03b5l+1 \u2299 f'(wl \\hat{a}l)(al)T, (13)\ni.e., the weight updates can be computed using the error and activation nodes in neighboring layers, meaning PCNs are local both in space and time. We discuss the advantages of this property below.\n2.2.3 Learning Algorithm. By virtue of the use of the energy function for both activation updates and weight updates, training PCNs is not done by backprop, but a different learning algorithm. Pseudocode is shown in alg. 2. This is sometimes called inference learning (IL) [6, 82?] or, rarely, prospective configuration [91], but is often simply referred to as \u2018predictive coding' [81, 85]. Importantly, it is an instance of expectation maximization (EM) (see section 3). We choose the term IL in this work. Note that unlike BP, IL requires initializing hidden nodes before inference. This is often done using a 'feedforward pass', setting al(0) = \u03bcl. This causes the error throughout the network to be zero except at the final layer. Other initializations can also be used [22].\nAlgorithm 2 Learning {(xn),yn} with IL.\nRequire: : a0 = x(n), aL = y(n)\nRequire: : al(0) = \u03bcl for l = 1, . . ., L \u2212 1\n// Clamp data\n// Feedforward initialization\n1: for t = 0 to T do\n2: for each l do\n3: al(t + 1) = al(t) \u2212 \u03b3 \u2202E\u2202a\n// Inference\n// Activation update\n4: end for\n5: end for\n6: for each l do\n7: wl \u2190 wl \u2212 \u03b1 \u2202E\u2202w\n// Weight update\nImportantly, recent work introduced incremental PC [85] using the insight that EM admits the use of partial steps, meaning that the minimization of E w.r.t. activations does not have to be complete for the algorithm to converge. Thus, a minor change in alg. 2 gives alg. 3, which we to as incremental IL in this work.\nAlgorithm 3 Learning {(xn),yn} with Incremental IL.\nRequire: : a0 = x(n), aL = y(n)\nRequire: : al(0) = \u03bcl for l = 1, . . ., L \u2212 1\n1: for t = 0 to T do\n2: for each l do\n3: al(t + 1) = al(t) \u2212 \u03b3 \u2202E\u2202a\n4: wl \u2190 wl \u2212 \u03b1 \u2202E\u2202w\n5: end for\n// Clamp data\n// Feedforward initialization\n// Activation update\n// Weight update\n6: end for\n2.2.4 Structure. A PCN is typically visualized as in fig. 5. The grey connections are the predictions defined by (7), together with error computations, (8) (similar to those in FNNs, see fig. 4). The red connections are defined by (11).\n2.2.5 Testing Procedure. A PCN can be tested by clamping the lowest layer a0 to the data, and computing updates for all the other layers, now including the final layer aL, using the activity rule (10). Interestingly, computing these updates is much simpler than during training. With the output nodes now unclamped, (10) has an analytical solution. One can show [89, 99] (see appendix B.2) that with a0 = x(n), the minimum of E is:\n\\hat{a}1 = f(w0x(n)), \\hat{a}2 = f(w1 \\hat{a}1), ..., \\hat{a}L = f(wL\u22121\\hat{a}L\u22121). (14)\nIn other words, during testing, a prediction of the network can be computed using a single pass through the network, without requiring gradient-based optimization of activation nodes of (11). In other words, we can even say that testing is equivalent in PCNs and FNNs, or that during testing the PCN \u2018becomes' an FNN in the sense of (2). As such, important results for neural networks such as universal approximation theorems [72] also hold for discriminative PCNs. For these networks, the difference w.r.t. FNNs is only in their learning algorithm. We emphasize that this holds only for discriminative PCNs, with the local prediction defined as in (7), and data clamped to a0. If this is changed, as will be done in later sections, then this equivalence no longer holds.\n2.3 FNNs vs Discriminative PCNS\nHere, we summarize the similarities and differences between FNNs and PCNs, reiterating the training & testing procedures, the learning algorithm, and structure.\n2.3.1 Training & Testing Procedures. table 2 summarizes the important steps and computations for FNNs (with MSE loss) and discriminative PCNs, during training and testing. Notice the equality between the MSE loss and PCN's energy function during testing, and their similarity during training. This is a result of fixing the final layer of the PCN aL the label y(n), and labelling the prediction of the last layer \u03bcL = f(wL\u22121aL\u22121) as \\hat{y}. The term \u2211L\u22121l=1(\u03b5l)2 is called the internal error [56] or residual error [57].\n2.3.2 Learning Algorithms. Table 3 shows the key steps involved in BP and IL. Conceptually, the main difference is the forward (1 \u2192 L) and backward (L \u2192 1) sweeps of BP, absent in IL. In contrast, updates in IL (for both activations and weights) use only local information. Updates are local in time: all layers could in principle be updated in parallel (see fig. 6). In contrast, in BP, weight updates in lower layers cannot be done before computations in all higher layers have been completed. BP thus involves waiting times; it is not local in time. If this parallelization of computation can be realized, this difference implies a potential speed-up of IL compared to BP (see appendix C). It should be noted that the learning time in practice involves several additional factors, discussed further in section 4.\nA final note on learning concerns weight initialization. It is well-known in machine learning that in deep networks, correctly initializing weights is required for networks to be trainable [30, 72? ? ]. This has not yet been studied in relation to PC except [22], which only showed that weights ought to be sufficiently small at initialization. Practical uses so far (e.g., [91]) and publicly available implementations [96] use a value of 0.05, similar to typical values chosen for ANNs [70], but this choice lacks a theoretical basis, and more optimal initialization schemes may exist.\n2.3.3 Structure. A typical, perhaps universal representation of the structure of an FNN is a diagram like in fig. 4. The connections in this diagram are unambiguously defined by the activity rule (1). The same applies to PCNs: the red arrows are defined by (11). It is not yet clear whether fig. 5 (or similar diagrams) can be considered a universal representation of PCNs, since the grey connections and the arrows are then only visualized, but not rigorously part of the definition. We return to this issue in section 2.6.\n2.3.4 Computational Complexity. Since testing a FNN and PCN is equivalent, their time complexity during this phase is equal. As for the training phase, by studying the computations in table 3, the time complexity of a single weight update can be computed (reviewed in detail in appendix C). The results of these computations are summarized in table 4. With matrix multiplications being the most costly computations and defining M as the complexity for the largest weight multiplication, the complexity can be shown to be O(LM) for BP, O(TLM) for IL [85? ] and O(LM) for Incremental IL (see also appendix C). That is, BP and incremental IL have the same time complexity per weight update, with standard IL a factor of T more costly.\nHowever, as mentioned above, the computations in IL enjoy both temporal and spatial locality, meaning the computations in different layers during inference and learning could be parallelized. Ignoring overhead, this implies that IL's time complexity would decrease to O(TM), and incremental IL would have O(M), which is faster than BP. In this case, total training time no longer scales with the depth of the network, which is a highly desirable feature. A first implementation of this kind was provided by [85]. However, their algorithm included a substantial computational overhead, such that it is only faster in networks where L \u2265 128. At the same time, BP is heavily optimized by dedicated libraries that employ GPU acceleration, whereas no similarly comprehensive library yet exists for IL. In addition, note that the time complexity per weight update is not the only factor that determines training time in practice. The second key factor is the number of epochs until convergence, which depends on the optimizer, dataset, and other hyperparameters used. These can impact the learning properties of BP and IL in different ways. We discuss some of these topics in section 4.\n2.3.5 Empirical results. As is well known to deep learning practitioners, many factors affect performance of FNNs, which is no different with discriminative PCNs. Exponential growth of the search space with hyperparameters makes a systematic comparison intractable, meaning existing works compare FNNs and PCNs only over a small range of hyperparameters. Interestingly, discriminative PCNs often perform extremely similar to FNNs [91, 99], but works by [22, 23] also make clear that care needs to be taken in importing knowledge and intuitions from BP-trained networks since IL does introduce meaningful changes related to its mathematical properties. With this remark we summarize the advantages and limitations that have been observed so far.\nIn terms of improvements of PCNs over FNNs, most noteworthy are those observed in [91] and [6]. Ref. [91] observes minor but statistically significant improvements for online learning (learning with batch size 1) and data efficiency (learning with less than 300 data points per class), on the order of 2% in the best case. Gains in classification accuracy of this order is also observed in [6], who additionally observe that the use of optimizer greatly affects results, exhibiting differences between BP and IL. Specifically, IL combined with SGD sometimes converges to poor local minima, while BP does not (with Adam, IL again improves to the level of BP). Somewhat more impressive are the improvements observed for continual learning tasks and concept drift tasks observed in [91], with gains of 5-20%. Overall, both works generally observe faster convergence measured as number of epochs. The improvement is typically small but sometimes substantial. The recent work [36] finds evidence confirming this; in section 4, we discuss theoretical insights that help explain these results. On larger architectures (e.g., for CNNs), [6] observe that IL with SGD sometimes converges to shallow local minima, such that BP outperforms IL by a large margin (order of 10%). However, this difference is decreased to 2% by using Adam.\nAlthough IL often appears to converge in fewer epochs, its total computation time (as measured by wall-clock time) is often cited to be larger than BP, although few works have studied this extensively. Reasons for this include standard IL's increased computation per weight update (improved with incremental IL), and an increased computational overhead compared to heavily optimized BP libraries. For standard IL, then, three main counteracting 'forces' appear to be at play: (1) decreased number of epochs required, but (2) increased computation per weight update, and (3) increased overhead. With (2) largely removed in incremental IL, it remains to be studied how (1) and (3) balance out in practice.\nIn sum, the existing literature suggests that IL performs roughly as well as BP when measured by accuracy on typical classification tasks. IL sometimes performs worse, but this appears to be largely remedied by choice of other optimizers, where we stress again that relatively little effort has been directed towards optimization of PCNs. For specific tasks (continual learning, concept drift, and marginally so with little data and online learning), IL appears to outperform traditional ANNs trained with BP. On the matter of computation time, IL appears to converge in fewer epochs, but more extensive comparisons in wall-clock time remain to be done.\n2.4 Extensions\nIt is straightforward to extend discriminative PCNs to more complex architectures such as convolutional neural networks [6, 83], recurrent neural networks [83], variational autoencoders and transformers [71]. Other objective functions may also be considered [57, 71].\n2.4.1 Convolutional Layers. Convolutional layers make the replacement\nal+1j = f(\u2211iWlai, j + b) \u2192 al+1z,j = f(\u2211x\u2211yWxyalai+x,j+y + blz). (15)\nwhere Wl is the kernel, a matrix of size ke\u00d7ke with ke the kernel size. For each (now two-dimensional) output neuron zl+1z,j one sums over x and y, the neurons in the local receptive field. W, then, is a weight matrix shared by all neurons, together with a single bias bl. Layer-dependent stride se (which allows moving the local receptive field in larger steps) and padding of zeros pe can be introduced by setting x \u2192 sex \u2212 pe and y \u2192 sey \u2212 pe. Importing this to PCNs, we see that we need only change the pre-activations zl in the relations al = f(zl) (ANNs) and \u03bcl = f(zl) (PCNs) respectively. Pooling layers (which reduce the dimensionality of layers by downsampling) can similarly be written by changing only zl.\n2.4.2 Other Objective Functions, Transformers and VAEs. So far, the objective function considered was E = \u2211(\u03b5l)2, which corresponded to the MSE loss in FNNs, Ln = (\\hat{y} \u2212 y(n))2. Generalizing this to any layer-dependent energy one may write E = \u2211l El, where El may now be generalized to other functions. For instance, instead of an MSE-like error,[71] derives cross-entropy-like error:\nEl = \u2211ka lk log (\u03bclkalk), (16)\nwhich is appropriate for binary output variables. This can be understood and generalized further by understanding PCNs as a probabilistic latent variable model, cf. section 3. Even more generally, one may write:\nEl = g(al, al\u22121) (17)\nwhere g is some function [57].\nIn [71], PC is extended to train transformers and VAEs. Training transformers requires attention layers which include the softmax function. These require summing over all nodes in a layer, and are hence not included in PC as presented above, which considered only layer-dependent activations. Considering PC for non- Gaussian distributions (cf. section 3) however [71] makes this extension possible. Using a similar framework, the same authors also train variational autoencoders (VAEs) [40] with IL.\n2.5 Generative PCNS\nIn the supervised learning context, discriminative models approximate the posterior probability distribution of labels given datapoints, p(y|x) [7]. If equipped with a softmax function at the final layer, this is precisely what the model above does (hence the term discriminative PCN). Making only a minor change to the approach discussed above, one instead obtains a model that approximates p(x|y): a generative model, from which synthetic points in the input space can be generated. With IL, such a model can be used both for supervised learning, as well as unsupervised learning, both of which we discuss below.\nIn general, it should be noted that testing a generative model is less straightforward than testing discriminative models, since they can be used for several purposes (e.g. density estimation, sampling/generation, latent representation learning) [72].\n2.5.1 Supervised Learning. Structure. To obtain a generative PCN, one changes the direction of the local predictions in the network: \u03bcl = f(wl\u22121al\u22121) becomes \u03bcl = f(wl+1al+1).\nTraining Procedure. The training procedure and learning algorithm (IL), are unchanged: (10) is run until converged, followed by a weight update. The changed local prediction results in slight changes to update rules: in (10) and (12), l + 1 becomes l \u2212 1.\nTesting Procedure. Clamping the final layer aL to a label, with the reversed prediction direction one can derive the reverse of (14): a synthetic datapoint is created at a0 in a single feedforward pass (without requiring several iterations of (10)).\n2.5.2 Unsupervised Learning. Unsupervised learning is concerned with finding patterns and structure in unlabeled data [7]. This means a change in the problem setup of section 2: we no longer have a dataset {(xn),yn}Nn=1, but only {x(n)}Nn=1. For traditional ANNs, an FNN may be adapted for unsupervised learning by using a special autoencoder architecture, encoding input data into a lower-dimensional representation followed by a decoder which reconstructs the original input. PCNs do not require a special architecture of this sort, but only change the direction of prediction to \u03bcl = f(wl+1al+1) i.e. the use of a generative PCN. Indeed, PCNs were originally conceived in this way [73] (also see section 3).\nTraining Procedure. Again, one needs to only make a minor change to the approach described above. Simply keep the final layer aL unclamped during training, keeping data x(n) clamped to a0, and run IL. The model functions like an autoencoder: higher layers take the role of the latent space, which during training encodes increasingly abstract features of the data clamped to the lowest layer. As such, the PCN takes the role of both encoder and decoder.\nTesting Procedure. During testing, one can sample the latent space as a decoder using a noise vector and ancestral sampling. Seeing the PCNs as a hierarchical probabilistic model (cf. section 3), each layer has a (Gaussian) conditional probability distribution. The idea of ancestral sampling is to generate a sample from the root variable(s) (the top layer) and then sample from the subsequent conditional distributions based on this [72]. Finally, a synthetic datapoint at the bottom is obtained [64].\n2.5.3 Hierarchical PCNs. Discriminative and generative PCNs can together be called hierarchical PCNs, defined by the local prediction \u03bcl = f(wl\u00b11al\u00b11). The supervised and unsupervised learning that can be done with generative PCNs can then be called the training modes of PCNs. These are illustrated in fig. 7. Intuitively, we can see the difference between discriminative and generative PCNs in the direction of predictions: in the discriminative model, predictions flow from data to labels, while errors flow from labels to data. In generative models, this is reversed. This is also visualized in fig. 7.\nIt should be noted that testing can also be \u2018reversed' in hierarchical PCNs, which we call backwards testing. If one has trained the discriminative PCN, one can clamp labels and find the minimum energy with iterative inference, producing a synthetic image at the bottom. Conversely, a trained generative PCN can classify images by iterative inference, using a clamped image instead of a clamped label. Thus, confusingly, discriminative PCNs can be used for generative tasks, and generative PCNs can be used for discriminative tasks. However, in practice, neither works better than their forward-tested counterpart, meaning generative PCN are the natural choice for generation, as discriminative PCNs are for classification. This justifies the naming convention used here and elsewhere in the literature.\n2.5.4 Empirical results. Compared to discriminative PCNs, generative PCNs remain underexplored the literature. This is perhaps surprising considering the original conception of PC as an unsupervised generative model. At the same time, as was mentioned, metrics for what counts as a 'good' generative model are wide and varied [72], making it a more involved field of study in general. Indeed, the works in the literature are disparate and focus on different aspects.\nAlthough not used for machine learning tasks, we mention [73] here as the first generative PCN trained in unsupervised mode. Then, [82] used generative PCN as an associative memory model. It was tested on the task of reconstructing from corrupted and partial data, using up to 500 datapoints. They find good performance, outperformining autoencoders, and both Hopfield networks [34] and modern Hopfield networks [42] in most cases. Another set of unsupervised tasks was considered by [64], and extended in [65]. This work used a set of models dubbed neural generative coding (NGC). This framework is equal in spirit to PCNs as presented in section 2, but differs in notation/terminology as well as some structural aspects. The authors study the network's performance on reconstruction & sampling ability (as measured by the likelihood) on black and white images. They find that NGC is competitive with VAEs and GANs on these tasks, and works well on downstream classification.\nNext, a smaller study [38] considers the performance of generative PCNs on standard classification tasks, training the model in supervised mode with backwards testing. Similar to [5], it is observed that the weights of different layers are updated at different rates, which causes model accuracy to worsen after it has peaked. They propose that regularizing weights remedies this pathology.\n2.6 PC Graphs\nThe previous section showed how the structure of discriminative PCNs, could be extended by changing the definition of local prediction from \u03bcl = f(wl\u22121al\u22121) to \u03bcl = f(wl+1al+1). This can be taken one step further: PCNs can be naturally generalized to arbitrary graphs, called PC graphs by [81]. These are trained using IL, but dispense with the hierarchical structure of layers. These networks can be understood as a superset of both discriminative and generative PCNs, and can flexibly be used for a variety of tasks.\n2.6.1 Structure. PC graphs are defined by a collection of N activation nodes {ai}Ni=1, and error nodes {\u03b5i}Ni=1, with ai, \u03b5i \u2208 R, and \u03b5i = ai \u2212 \u03bci. The local prediction is defined as:\n\u03bci = \u2211j\u2260i f(wij aj),\nwhere wij \u2208 R, and the sum is over all the other nodes, meaning self-connections are left out (wii = 0), as in [81]. This defines a fully-connected PC graph. If we choose to include self-connections one simply takes the sum over all values of i. In vector notation, one can write \u03bc = f(wa).\nA small (fully-connected) PC graph is illustrated in fig. 8, where each activation node ai and error node \u03b5i has been grouped in a vertex vi. Importantly, graphs with different connectivity/topology can be obtained by multiplying the weight matrix with a mask or adjacency matrix. In this way, one obtains the architectures discussed in earlier sections: the discriminative PCN, as well as the generative PCN, depending on which weights are masked. This is visualized in fig. 9.\n2.6.2 Training Procedure. Depending on the task, different nodes in the PC graph can be clamped during training. For instance, in a standard supervised mode, the data {xi}Ki=1 (with dimensionality K) is clamped to a subset of activation nodes {ai}Ki=1, and the label {yi}M1i=1 (dimensionality M) is clamped to a second subset {ai}Mi=1. With simplified notation we can write this as:\n{x} = {a} \u2282 {aI} {y} = {a} \u2282 {aZ}. (16)\nFor unsupervised learning, only data is clamped: {x} = {a} \u2282 {aI}. Following this, IL can be used like earlier, where the key computations change only changing slightly. For example, the energy function becomes E = 12 \u2211i(\u03b5i)2 = 12 \u03b5T\u03b5; other equations are shown in table 5. Observe that the equations are obtained by simply omitting the layer l in the equations of section 2.2.\n2.6.3 Testing Procedure. Depending on how the network was trained and the desired application, different nodes can be clamped also during testing. After supervised training, classification/generation of synthetic datapoints can be done by clamping a datapoint/label respectively, running inference until convergence, and considering the produced output (a label/datapoint).\n2.6.4 Empirical results. PC graphs have only been used in [81], with models trained using supervised mode only. They train models for classification, generation, reconstruction, denoising, and associative memory tasks (using different adjacency matrices). An interesting result is that for classification using a fully connected model, PC graphs perform much better than other fully connected architectures such as Boltzmann machines and Hopfield networks, up to 30% better on MNIST. At the same time, the fully connected model does not perform comparably to hierarchical networks trained with either BP or IL. This might be expected, considering that depth is suggested to be critical for performance in neural network learning [72]. At the same time, such statements are based mostly on empirical evidence, and lack a definitive theoretical basis.\nFor the other tasks considered, only proofs of concepts are provided to demonstrate their flexibility. Unsupervised learning is also not considered.\n2.7 Generalized ANNS\nAt this point it is useful to compare PC graphs with PCNs discussed in section 2. Fig. 9 illustrated how different adjacency matrices lead to the architectures discussed in earlier sections. Given the result of (14) that FNNs are equivalent to discriminative PCNs during testing, we can see them as a subset of hierarchical PCNs, defined by the local prediction \u03bcl = f(wl\u00b11al\u00b11). In turn, hierarchical PCNs can be seen as a subset of PC graphs, with a particular choice of adjacency matrix. This is illustrated in fig. 10, as well as fig. 2.\nThus, it becomes clear that formally, PCNs and PCGs can be seen as types of \u2018generalized ANNs' that go beyond hierarchical structures, by virtue of the use of IL as opposed to BP. This is very interesting, for at least two reasons. First, as observed by [81], PC graphs allow one to train non-hierarchical structures with a brain-like topology. Speculatively, such networks could, if better understood, share some of the advantages that biological brains have over ANNs, such as vastly superior energy efficiency and parallelization. Second, from a very general perspective, topological considerations have strongly contributed to several advances in machine learning in the past. A prominent example is that of residual networks and skip connections [104], which have allowed training of much deeper networks, and can improve performance on a variety of tasks [72]. In a sense, the very notion of depth in deep learning is a topological feature. As such, we consider this an important avenue for further work."}, {"title": "3 PCNS AS PROBABILISTIC LATENT VARIABLE MODELS", "content": "Section 2 introduced the PCN as a type of generalized ANN. In this section we discuss a second, complementary, perspective: PCN as a probabilistic latent variable model. Conceptualizing PCNs in this way allows a principled derivation of equations in the previous section, and brings to light connections to other methods well-known in machine learning (e.g., energy-based models and linear factor models). It also reveals a number of assumptions and modeling choices, providing a basis for improvements and possible future developments.\nWe start our step-by-step derivation from maximum likelihood estimation in probabilistic (Bayesian) models, discussing how expectation maximization (EM) implements maximum likelihood for models with latent variables, and how different generative models lead to PC as formulated by [73] or to multi-layer PC as described in the previous section.\nA Brief History of PCNs. The original model of Rao & Ballard [73] was conceived as a hierarchical model with Gaussian relations between layers. This formulation contained the ingredients typically associated with PC, chief among them the minimization of prediction errors in a hierarchical fashion. Work by Friston [24, 26, 28] showed how the computational steps in PC (minimization of the energy with respect to activations and weights) are an instance of EM, also understood as a variational inference procedure using a variational free energy (i.e., the evidence lower bound) as the objective function. This perspective explains why EM maximizes the likelihood of the data, and can be used to derive generalized objective functions (see appendix A).\nWe remark that, unlike most works in the PC literature, we emphasize the role of EM to explain IL, deferring to appendix A the derivations involving the variational free energy. We choose this focus because the two steps of IL (inference and learning) correspond precisely to the two steps of EM (expectation and maximization), and because EM is perhaps more familiar to machine learners than the thermodynamics-inspired free energy approach.\n3.1 Expectation Maximization\nGiven data x \u2208 Rnx (an observed variable), EM is a general method for maximizing the (log-)likelihood in models with latent variables z \u2208 Rnz, or equivalently, minimizing the negative log-likelihood,\nNLL(\u03b8) = \u2212 ln p\u03b8(x) (17)\nFor some joint distribution p\u03b8(x, z), also called the generative model, it can be shown that the following two steps will minimize the NLL (see appendix A):\n\\bar{z} = argmaxz p\u03b8(x, z) (E-step) (18)\n\u03b8 = argmax\u03b8 p\u03b8(x, z) (M-step) (19)\nHere, ln p\u03b8(x, z) is also called complete data log-likelihood, since it represents the likelihood function calculated using the full set of observed and latent variables \u2013 to distinguish it from the likelihood calculated only with observed data [7]. For convenience, we will label the negative complete data log-likelihood as E, following:\nE(x, z) = \u2212 ln p\u03b8(x, z) (20)\nwhich we will choose to minimize, as opposed to maximizing ln p\u03b8(x, z). This, with a change of notation, gives IL, exactly as presented in section 2. It has furthermore been shown (cf. appendix A, ref. [59]) that partial E-steps and M-steps will also minimize the NLL:\n\u0394z \u221d \u2202E\u2202z (partial E-step) (21)\n\u0394\u03b8 \u221d \u2202E\u2202\u03b8 (partial M-step) (22)\nwhich is indeed how IL is implemented in practice.\n3.2 Generative Models\nHaving described the general process for minimizing the NLL, we can now proceed with modeling p\u03b8(x, z).\n3.2.1 PCNs of Rao & Ballard. Here we derive the model of Rao & Ballard, i.e., PC with two layers. First factorize the generative model as:\np\u03b8(x, z) = p(x|z)p(z) . (23)\nTo obtain the original results of Rao & Ballard, one assumes that both distributions on the right-hand side may be well-approximated by Gaussians. Hence for p(x|z) we take\np(x|z) = N(x; \u03bcx, \u03a3x) = 1\u221a((2\u03c0)nx det \u03a3x) exp (\u221212(x \u2212 \u03bcx)T(\u03a3x)\u22121(x \u2212 \u03bcx)) , (24)\nand likewise for the prior,\np(z) = N(z; \u03bcz, \u03a3z) . (25)\nFurthermore, take \u03bcx = f(Wxz) and \u03bcz = f(Wzzp) with f, Wx, Wz defined as before, and zp a parameter for the prior (either fixed or learnable). \u03a3x \u2208 Rnx\u00d7nx, \u03a3z \u2208 Rnz\u00d7nz are covariance matrices. Thus, \u03b8 = {Wx, Wz, \u03a3x, \u03a3z} are the parameters that define the generative model. For convenience, we also define the errors \u03b5x = x \u2212 \u03bcx and \u03b5z = z \u2212 \u03bcz as in section 2. The model is illustrated schematically in fig. 11.\nWith the decomposition (23), the energy (20) is\nE(x, z) = \u2212 ln p(x|z) \u2212 ln p(z) = 12 ((\u03b5x)T (\u03a3x)\u22121 \u03b5x + (\u03b5z)T (\u03a3z)\u22121 \u03b5z + ln [det \u03a3x det \u03a3z]) + const., (26)\nwhere on the second line we have inserted the Gaussian ans\u00e4tze (24), (25), and the constant \u2013 which is irrelevant to the optimization problem \u2013 is 12(nx + nz) ln 2\u03c0. Choosing diagonal bases so that \u03a3 = \u03c32I, and disregarding the constant and the factor of 1/2, we have\nE(x, z) = 1\u03c32x (\u03b5x)2 + 1\u03c32z (\u03b5z)2, (27)\nwhich is the energy function used by Rao & Ballard [73] (with variances taken as constants). With a model for E in hand, we can now apply EM, cf. (A.9) and (A.10).\n3.2.2 Multi-layer PC. We now extend the previous section to a model with L layers (vectors) of latent variables {zl}L\u22121l=1 in a hierarchical structure. It is convenient to label the observed variables x as x = z0 \u2208 Rn0. Thus our generative model is p\u03b8(x, {z}) = p\u03b8(z0, ..., zL) = p\u03b8({z}). The hierarchical structure then translates to the statement that each layer is conditionally independent given the layer above, i.e.,\np\u03b8({z}) = p\u03b8(zL) \u220fL\u22121l=0 p\u03b8(zl|zl+1), (28)\nso that the energy becomes\nE({z}) = \u2212 ln p\u03b8({z}) = \u2212 ln p\u03b8(zL) \u2212 \u2211L\u22121l=0 ln p\u03b8(zl|zl+1). (29)\nAs in the simple model above, we assume a multidimensional Gaussian for the conditional distribution of each layer, as well as for the prior:\np\u03b8(zl|zl+1) = N(zl; \u03bcl, \u03a3l) p(zL) = N(zL; \u03bcL, \u03a3L) (30)\nwith mean \u03bcl = f(Wl+1zl+1) and covariance matrix \u03a3l \u2208 Rnl\u00d7nl, where Wl \u2208 Rnl\u00d7nl+1 and \u03bcL = f(WL+1zp). These parameters, \u03b8 = {WL, \u03a3L}L+1l=0, define the generative model, which is illustrated in fig. 12. As before, we define the errors \u03b5l = zl \u2212 \u03bcl. We then have\n\u2212 ln p(zl|zl+1) = 12 ((\u03b5l)T (\u03a3l)\u22121 \u03b5l + ln det \u03a3l + nl ln 2\u03c0)\nsuch that E becomes\nE({z}) = \u2211Ll=1 ((\u03b5l)T (\u03a3l)\u22121 \u03b5l + ln det \u03a3l) + const, (32)\nwhere the constant is \u2211Ll=0 nl ln 2\u03c0. Dropping this, and choosing a diagonal basis so that \u03a3 = I, we obtain\nE({z}) = 12 \u2211Ll=0(\u03b5l)2, (33)\nwhich is the energy of Section 2; cf. also (27). Applying EM to this then yields IL as presented in section 2.\n3.3 Learning in PCNs Revisited\n3.3.1 Discriminative and Generative PCNs. At this point we can provide a number of interesting interpretations of earlier sections. Notice that in the derivation above, the model had observed variables at layer l = 0. This corresponds to generative PCN, as discussed in section 2.5. If trained in supervised mode, we can understand the clamping of the label y(n) to the final layer zL as follows: y(n) becomes a parameter of the prior (e.g. its mean). If trained in unsupervised mode, it is treated just like the other hidden variables. Interestingly then, for discriminative PCN, we see that the clamped data x(n) is a parameter for the prior, and the label is now the observed variable. This is shown in table 6.\n3.3.2 Precision Matrices. In the context of predictive coding, the covariance matrices \u03a3l are often interpreted as the uncertainty at each layer around the mean \u03bcl, with their inverse (\u03a3l)\u22121 =: \u03a0 sometimes called precision matrices [51, 57? ]. Within neuroscience and psychology, they have been used in a wide range of models [54], where it has been suggested that they implement a type of attention [20]. By up-weighting the error, increasing the precision of a variable would serve as a form of gain modulation. From the ML perspective, instead of setting them to identity matrices, one can add minimization of E w.r.t. \u03a0l as an additional part of the M-step, seeing them simply as additional parameters of the generative model. However, their practical utility remains unclear, with varying results. In particular, [60] do not find advantages of additionally learning for generative modeling tasks, while [64] do appear to find some benefit, suggesting they act as a type of lateral modulation that induces a useful form of sparsity in the model. Comparisons with VAEs are made in [71], with a different theoretical interpretation by [57] (see section 4 for further discussion).\n3.4 Connections to Other Latent Variable Models\nPCNs share various properties with other probabilistic models in machine learning. We briefly discuss some of these connections here, leaving more extensive comparisons (e.g., on training methods, sampling) for future work.\n3.4.1 Linear Factor Models. With f as a linear function, the two-layer PCN model discussed above is equivalent to a linear factor model. This is illustrated in fig. 13. Specifically, if we take \u03bcz = 0, \u03a3z = I, and \u03a3x = diag \u03c32 with \u03c32 = [\u03c321, \u03c322, ..., \u03c32n], then we obtain exactly factor analysis. If additionally take \u03a3x = \u03c32I, i.e. the variances of are all equal to each other, we obtain probabilistic PCA [30]. Like in PCNs, parameters in this model can be learned using the EM algorithm [7].\n3.4.2 VAEs and Diffusion Models. VAEs share many properties with PCNs, as discussed in [50, 51, 60, 71]. They make use of a similar graphical model as two-layer PCNs (and linear factor models, fig. 13). VAEs are trained using variational free energy as an upper bound for the NLL, cf. appendix A for a treatment of PCNs in these terms. While they use different optimization techniques, these can be seen as design choices for solving the same inference problem. A number of key differences can be identified. First, the conditional p(x|z) is parametrized by a linear function, followed by a nonlinearity (i.e.f (W z)). In contrast, VAEs use a deep neural network (trained with BP). Moreover, in the variational inference perspective, whereas PCNs use a variational posterior of the form q(z), VAEs use q\u03c8(z|x), i.e. with an explicit conditioning on data (visualized by dotted lines in fig. 13). This is parameterized by a separate neural network through parameters \u03c8, which are learned jointly with \u03b8 using BP. For further comparisons, see [50, 51].\nAdding multiple layers to VAEs results in hierarchical VAEs, which can similarly be compared to multi-layer PCNs. Diffusion models are also shown in fig. 13, which share (most of) their graphical structure with hierarchical VAEs. Like VAEs, diffusion models factorize the variational posterior using conditional distributions, i.e. q(xl|xl+1), but this is not parametrized by separate parameters \u03c8. I.e. the variational model is not learned, but fixed. These differences are summarized in table 7.\n3.4.3 Energy-Based Models (EBMs). PCNs are often cited to belong to the class of EBMs [56, 57, 91]. However, in these works the term \u2018EBM' is used in a different way than is typical in ML. The typical use of EBM [44, 95] is based on the observation that any probability distribution function can be parametrized by:\np\u03b8(x) = exp(\u2212E\u03b8(x))Z(\u03b8) , (34)\nwhere E\u03b8(x) is the energy function, and Z(\u03b8) is the partition function, i.e., the sum of all possible states. This use comes from physics, where it is called a Boltzmann distribution, corresponding to a system at thermal equilibrium described by a canonical ensemble. The prototypical example of EBMs are Boltzmann machines, which have E\u03b8(x) = \u2212xT Wx [2, 48].\nIn contrast, in [56, 57, 91] an EBM is instead defined through (21) and (22) [56, 57, 91]. This definition has also been used by [86], and encompasses continuous Hopfield networks and networks trainable by Equilibrium Propagation. Indeed, PCNs have much in common with these techniques, cf. [56] and section 4.\nRegarding EBMs as defined by (34), comparisons of PCNs with this class of models has not yet been done to the authors' knowledge. We briefly mention some differences here, comparing to Boltzmann machines. If one includes latent variables z, one may write p\u03b8(x) = \u2211z p\u03b8(x, z) (with a sum instead of an integral because nodes in Boltzmann machines are typically discrete), with [48]:\np(x, z) = exp(\u2212E\u03b8(x, z))Z(\u03b8) . (35)\nNotably, Boltzman machines are characterized by all-to-all connectivity between nodes. Now, observe that the generative model of PCNs, by definition of E (cf. (20)), can be written as\np\u03b8(x, z) = exp(\u2212E\u03b8(x, z)) . (36)\nwhich is similar to (34). However, note that E in this equation is not an \u2018energy' due to the lack of normalization in Z. Nonetheless, comparing EBMs to PCNs one may first observe that E is different. Moreover, they are trained in different wat PCNs minimize \u2013 ln p\u03b8(x, z), whereas EBMs typically minimize the NLL itself \u2013 not by using EM, but taking derivatives of the NLL w.r.t. model parameters, calculated using MCMC techniques, e.g. Gibbs sampling [30]."}, {"title": "4 PCNS AND INFERENCE LEARNING", "content": "Where section 2 gave a general introduction to PCNs and IL", "57": "and trust region methods [36", "99": ".", "76": ".", "90": ".", "feedforward pass": "nitialization of hidden nodes is done (see section 2.2.3). Alg. 4 shows pseudocode for Z-IL.\nAlgorithm 4 Learning {(xn)", "loss.\nRequire": "\u03bc = f(wl+1al+1)\nRequire: : aL = x(n)", "y(n)\nRequire": "al(0) = \u03bcl(0) for l = 1", "1\nRequire": "\u03b3 = 1\n// Upward predictions\n// Clamp data\n// Feedforward-pass initialization\n1: for t = 0 to L do\n// Note: T = L\n2: for each l do\n3: al(t + 1) = al(t) \u2212 \u03b3 \u2202E\u2202a\n// Note: l = t\n4: wL=t(t + 1) = wL=t(t) \u2212 \u03b1 \u2202E\u2202w\n5: end for\n6: end for\nSince it has been known that other algorithms (Equilibrium Propagation [86", "100": "also approximate BP", "56": "showed that these various approximations can be understood as a consequence of certain properties of energy-based models (EBMs)", "106": "."}, {"57": "is that PC can be understood to interpolate between BP and a third algorithm for training neural networks", "45": ".", "natural": "egime of IL as presented in section 2", "22": "showed a useful result concerning the inference phase. Using a dynamical systems perspective"}, {"22": "gave theoretical assurance that this is indeed the case. Then", "91": "which according to the authors lies in reduced weight interference. \u2018Catastrophic interference' is a well-known pathology of BP to abruptly and drastically forget previously learned information upon learning new information [41"}, {"91": "this is reduced in IL", "6": "showed that IL approximates implicit SGD"}, {"6": "observe for IL. The authors also develop a variant of PC in which updates become equal to implicit SGD", "5": "where the authors observe that implicit SGD is sensitive to second-order information", "91": ".", "36": "which related IL to trust region (TR) methods. These are a well-known method in optimization", "102": "."}, {"102": ".", "36": "that IL's inference phase of PC can be understood as solving a TR problem on the BP loss"}, {"36": "show that these properties transfer to IL", "5": ".", "91": ".", "57": "are concerned with the convergence in PCN's energy landscape. The authors showed that that when activity nodes are initialized with the so-called \u2018energy gradient bound'"}, {"57": "for a derivation):\n|\u2202L\u2202al|\u2265|\u2202E\u2202al|.(37)\nwhich implies that the gradient of L is greater than the negative gradient of the residual energy. At the beginning of inference this always holds when using feedforward-pass initialization of hidden activities, since then \u2202E/\u2202al = 0. Indeed, [57"}]}