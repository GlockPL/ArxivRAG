{"title": "MATCHED: Multimodal Authorship-Attribution To Combat Human Trafficking in Escort-Advertisement Data", "authors": ["Vageesh Saxena", "Gijs Van Dijck", "Benjamin Bashpole", "Gerasimos Spanakis"], "abstract": "Human trafficking (HT) remains a critical issue, with traffickers increasingly leveraging online escort advertisements (ads) to advertise victims anonymously. Existing detection methods, including Authorship Attribution (AA), often center on text-based analyses and neglect the multimodal nature of online escort ads, which typically pair text with images. To address this gap, we introduce MATCHED, a multimodal dataset of 27,619 unique text descriptions and 55,115 unique images collected from the Backpage escort platform across seven U.S. cities in four geographical regions. Our study extensively benchmarks text-only, vision-only, and multimodal baselines for vendor identification and verification tasks, employing multitask (joint) training objectives that achieve superior classification and retrieval performance on in-distribution and out-of-distribution (OOD) datasets. Integrating multimodal features further enhances this performance, capturing complementary patterns across text and images. While text remains the dominant modality, visual data adds stylistic cues that enrich model performance. Moreover, text-image alignment strategies like CLIP and BLIP2 struggle due to low semantic overlap and vague connections between the modalities of escort ads, with end-to-end multimodal training proving more robust. Our findings emphasize the potential of multimodal AA (MAA) to combat HT, providing LEAs with robust tools to link ads and disrupt trafficking networks.", "sections": [{"title": "1 Introduction", "content": "Human trafficking (HT) is a widespread crime that exploits vulnerable individuals of all ages and genders (EUROPOL, 2020; UNDOC, 2020). Among its various forms, sex trafficking is notably prevalent, with traffickers coercing victims into commercial sex through violence, threats, deception, and debt bondage (ILO, 2012). Women and girls are particularly affected, especially within the commercial sex industry (ILO, 2012). With the rise of digital platforms, traffickers increasingly exploit online advertisements (ads) to take advantage of anonymity, with approximately 65% of HT victims in the United States being advertised through online escort services (POLARIS, 2020). The sheer volume of online ads makes manual tracking a daunting task, causing many trafficking cases to go undetected (POLARIS, 2018).\nDirect detection of HT instances through end-to-end classification techniques demonstrates assuring performances (Alvari et al., 2016; Tong et al., 2017; Alvari et al., 2017), but reliance on expert-generated labels risks overfitting, failing to generalize beyond training data. To address this challenge, LEAs and researchers have developed a list of HT indicators for identifying suspicious ads (Ibanez and Suthers, 2014; Ibanez and Gazan, 2016; Lugo-Graulich and Meyer, 2021). However, these indicators can only be studied in a group of ads linked to individuals or trafficking networks. While traditional methods suggest connecting these ads through phone numbers and email addresses (Chambers et al., 2019), recent research demonstrates that only 37% of ads contain such identifiers (Saxena et al., 2023a). Other supervised (Nagpal et al., 2015; Li et al., 2022a; Liu et al., 2023) and unsupervised techniques (Rabbany et al., 2018; Nair et al., 2022; Vajiac et al., 2023) often rely on explicit similarities, like name, identical phrases, or near duplicates, to connect these escort ads, limiting their effectiveness, particularly when vendors alter details to evade detection. Another research direction suggests using AA approaches, offering a holistic approach by identifying unique language patterns and stylistic features consistent across ads from the same vendor or vendor group. Existing AA approaches in NLP have demonstrated considerable success by examining these subtle written expressions, allowing for stronger connections between ads even when explicit markers differ (Ardakani, 2020; Saxena et al., 2023a).\nHowever, a key limitation in AA research for linking escort ads is the underutilization of multimodal data-specifically, the integration of images with text-even though most escort ads include a title, ad description, and multiple images. Combining textual and visual data can provide a richer context for AA, as images often reveal stylistic consistencies, locations, or poses that uniquely characterize a vendor's profile. Incorporating images alongside text thus strengthens the \"anchors\" needed for linking ads. In larger trafficking networks, vendors may reuse images with varying writing patterns in text ads, providing a visual link for AA models to leverage. Conversely, vendors might pair similar text descriptions with different escort images. Additionally, existing research expects a minimum of five ads per vendor for effective AA (Saxena et al., 2023a). Since most ads already include multiple images, adopting Multimodal AA (MAA) can capitalize on these data sources, improving performance beyond single-modality approaches for vendors with lower ad frequencies. Through this work, we aim to support LEAs in building AA-driven knowledge graphs and enable targeted investigations across extensive collections of raw escort ads by making the following contributions:\n(i) MATCHED Dataset and Comprehensive Benchmarking: We present MATCHED, a novel dataset for MAA, featuring 27,619 unique text descriptions and 55,115 images from Backpage escort ads collected across seven U.S. cities between December 2015 and April 2016. Extensive experiments establish benchmarks across text, vision, and multimodal domains, with evaluations conducted on in-distribution and Out-Of-Distribution (OOD) datasets. MATCHED provides a robust foundation for future research in MAA. Due to the dataset's sensitivity, the anonymized metadata is attached via DataverseNL, while the full dataset will remain restricted. Code for the experiments is available on MATCHED.\n(ii) Enhanced Performance through Multitask Training: We propose a joint multitask framework that simultaneously optimizes vendor identification and verification, outperforming traditional single-task models by 1.61% (text) and 1.52% (vision) on macro-F1 score for classification and 1.68% (text) and 6.75% (vision) on R-Precision for retrieval task. Although these gains may seem subtle, this dual-focus approach empowers LEAs to identify known vendors and discover emerging ones in OOD ads, enhancing their investigative capabilities.\n(iii) Advancements in Model Performance through Multimodal Training: Traditional AA methods rely heavily on textual data, often ignoring valuable stylistic cues from images and excluding vendors with fewer ads. Our multimodal approach integrates text and image data, improving performance even for vendors with limited postings. Pairing a single text description with multiple images (e.g., one text with five images produces five samples) expands the training set and enriches feature representation. While text remains the dominant modality, incorporating images with text enhances text-only results by 5.43% on retrieval R-Precision, marginally improves vision-only results by 0.75% on retrieval R-Precision, and increases classification macro-F1 by 32.62%-ultimately providing a more comprehensive and robust AA framework."}, {"title": "2 Related Research", "content": "AA in NLP has evolved from basic stylometric analysis (Bhargava et al., 2013; Ramnial et al., 2016) to advanced models that detect distinct linguistic patterns for identifying authorship across text segments (Fabien et al., 2020; Ai et al., 2022; Wegmann et al., 2022). AA's applications include forensic linguistics, where it attributes authorship in forensic contexts (Iqbal et al., 2008; Nirkhi and Dharaskar, 2013; Fobbe, 2021), to cybersecurity and cybercrime, where AA tracks malicious actors and identifies criminal activity across platforms (Zhang et al., 2019; Saxena et al., 2023b). However, a unique challenge emerges in applying AA within online criminal markets: conventional models struggle to efficiently capture the specialized jargon, coded language, and noise of illicit environments like illegal marketplaces, necessitating models that recognize these nuanced linguistic shifts (Choshen et al., 2019; Manolache et al., 2022). This limitation underscores the need for fine-tuned models that capture the nuanced linguistic shifts and stylistic patterns unique to these illicit activities. Addressing this gap, Ardakani (2020) propose supervised neural networks for AA on Backpage escort ads, demonstrating the potential to uncover stylistic consistencies even when explicit identifiers are altered. Building on this, Saxena et al. (2023a) leverage state-of-the-art transformer-based models for vendor identification and verification, showcasing their ability to link ads across escort markets spanning 41 cities effectively.\nIn addition to text, images in criminal markets may reveal recurring stylistic patterns that can help identify vendors' accounts (Cotogni et al., 2024). Vision-based AA approaches can leverage visual patterns such as backgrounds, lighting, or object placement-that complement linguistic cues, especially when text data is sparse or inconsistent (Wang et al., 2018). MAA approaches, on the other hand, combine text and images, enhancing accuracy by merging stylistic patterns across media. They can link and create features that account for linguistic and visual patterns, creating a more comprehensive vendor profile (Zhang et al., 2019)."}, {"title": "3 Dataset", "content": "Lugo-Graulich and Meyer (2021) provides compelling evidence that Backpage escort advertisements are frequently associated with HT activities. Motivated by this finding, we focus our experiments on Backpage ads and curate a dataset of 28,513 advertisements, including 27,619 unique text descriptions and 55,115 unique images linked to 3,549 vendors. Approximately 56% of these images feature an escort's face, while the remainder display only parts of their body (without any faces). Following Saxena et al. (2023a), we utilize Chambers et al. (2019) to extract phone numbers and apply NetworkX (Hagberg et al., 2008) to form vendor communities. Each community is assigned a unique vendor label, establishing the ground truth for AA tasks across the dataset. The ads are collected from seven major U.S. cities-Chicago, Houston, Detroit, Dallas, San Francisco, New York, and Atlanta-representing four geographic regions: South, Midwest, West, and Northeast. These zones group ads from relevant cities, with average text sequence lengths of 125, 118, 113, and 132 tokens, respectively. This organization ensures geographic diversity and allows for a more nuanced AA analysis across different regions.\nThe south region dataset, which contains the most text and image ads, is selected as the primary dataset for model training and in-distribution evaluation. We use the remaining Midwest, West, and Northeast datasets as OOD datasets to assess our model's capability to adapt to new distributions. Notably, many vendors in our dataset appear across multiple geographic regions. As a result, the OOD datasets include ads from vendors present in the training dataset (South) and additional vendors unique to each region. Further dataset details can be found in the appendix A.2 and A.3."}, {"title": "4 Experimental Setup", "content": "In our approach to vendor identification and verification tasks, we conduct our experimental setup through a closed-set classification task using the south region dataset, where the model learns to identify vendors from a pre-defined candidate set and an open-set metric learning task, where we leverage the ad representations (embeddings) from the trained classifier to perform similarity searches that retrieve all relevant ads from a vendor's collection. Before establishing the multimodal benchmark, we establish individual baselines in the text-only and vision-only modalities for a fair comparison. With practical utility in mind, the baselines are determined based on the performance of the trained model on vendor identification and verification objectives, providing a point of reference for identifying vendors in LEA databases and connecting them to new, emerging vendors.\n(i) Vendor Identification - A Closed-Set Classification Task: For the vendor identification, we perform a multi-class classification using pre-trained backbones on the South region dataset using cross-entropy (CE) loss (Juola and Baayen, 2005). Following Ai et al. (2022), we also experiment with a multitask joint objective combining CE, Supervised Contrastive (SupCon) (Ye et al., 2023; Huertas-Tato et al., 2024), and Triplet losses (Hu et al., 2020; Tyo et al., 2021) to enhance feature discrimination by aligning representations of the same vendor while separating those of different vendors.\n(ii) Vendor Verification - An Open-Set Metric Learning Task: Similar to Wegmann et al. (2022), we employ contrastive learning for a metric learning task (Kaya and Bilge, 2019) using Triplet and SupCon losses on the South region dataset. The objective is to bring together ad representations from the same vendor while pushing apart representations from different vendors.\n(iii)(A) Text-Only Baselines: Following Saxena et al. (2023a), we establish text-only baselines using the Style-Embedding (Wegmann et al., 2022) and DeCLUTR-small (Giorgi et al., 2021) backbones. For vendor identification, we fine-tune these models with CE, CE+Triplet, and CE+SupCon objectives. For vendor verification, we apply Triplet and SupCon objectives.\n(iii)(B) Vision-Only Baselines: In the absence of prior vision-only baselines on similar datasets, we fine-tune VGG-16 (Simonyan and Zisserman, 2015), ResNet-50 (He et al., 2015), DenseNet-121 (Huang et al., 2018), InceptionNetV3 (Szegedy et al., 2015), EfficientNetV2 (Tan and Le, 2021), ConvNext-small (Woo et al., 2023), and ViT-base-patch16-244 (Dosovitskiy et al., 2021) backbones with CE, CE+Triplet, and CE+SupCon objectives for the vendor identification task on the South Dataset. Once again, we employ Triplet and SupCon objectives on these backbones for the vendor verification task.\n(iii)(C) Multimodal Baselines: We evaluate two widely-used multimodal architectures, Visual-BERT (Li et al., 2019) and ViLT (Kim et al., 2021), fine-tuned on the South dataset for vendor identification using CE loss. Building on insights from unimodal experiments, we also design a custom backbone, \"DeCLUTR-ViT\", which combines DeCLUTR for text and ViT for images, leveraging their strong performance in respective modalities. To integrate text and image representations, we explore four fusion strategies: concatenation (Gallo et al., 2018; Li et al., 2024), mean pooling (Sleeman et al., 2022), self-attention (Kiela et al., 2020; Gan et al., 2024), and adaptive auto fusion via a neural network (Sahu and Vechtomova, 2021), enabling nuanced cross-modal interactions by combining complementary signals. Additionally, we perform image-text alignment pre-training tasks using the DeCLUTR-ViT backbone on the combined dataset from all regions, applying three alignment strategies: Image-Text Contrastive (ITC, or CLIP) (Radford et al., 2021), ITC+ITM (Image-Text Contrastive and Image-Text Matching) (Villegas et al., 2024), and BLIP2 (Li et al., 2023). These alignment techniques ensure that text and images from the same ad are represented closely in the latent space, particularly when a single text ad is associated with multiple images. Finally, we fine-tune the aligned models on the South dataset for vendor identification using CE and CE+SupCon objectives, effectively combining alignment and fusion strategies to enhance performance.\n(iv) Evaluation: To address the class imbalance in our dataset (Appendix Figure 2b(C)), we prioritize the Macro-F1 metric for the classification task. Additionally, we assess classification, metric-learning, and text-image alignment baselines through a retrieval task, linking query ads to relevant ads from the same vendor. The dataset is split into training (\"documents\") and test (\"queries\") sets, with embeddings generated by trained models used to compute cosine similarity via FAISS-based search (Johnson et al., 2019). Text-only and vision-only baselines extract embeddings directly from their respective encoders, while multimodal baselines combine text and vision embeddings from the DeCLUTR-ViT backbone using mean pooling. For ITC+ITM and BLIP2-based baselines, we take these vision embeddings from the QFormer encoder. Retrieval tasks are categorized as text-to-text, image-to-image, or multimodal based on whether query and document embeddings are derived from text, vision, or pooled multimodal representations. This framework evaluates the models' ability to link ads to vendors, including unseen ones, offering robust insights into performance across tasks.\nThe retrieval task is evaluated using R-Precision@X, which measures precision when the number of retrieved items equals the number of relevant ads per vendor, with higher scores reflecting more accurate representations of vendor activity (Saxena et al., 2023a). Additionally, Mean Reciprocal Rank (MRR@10) evaluates the average ranking position of the first ten correctly retrieved ads for each query, with scores closer to 1 indicating higher relevance ranking, thereby reducing manual search efforts for LEAs (Striebel et al., 2024). Lastly, Macro-F1@X independently calculates F1 scores for each vendor class and averages them, ensuring equal weight for all vendors regardless of sample size. In Macro-F1@X and R-Precision@X, X represents the cutoff, defined as the number of relevant items per vendor."}, {"title": "5 Results", "content": "This section evaluates text-only, vision-only, and multimodal baselines for vendor identification and verification tasks. The classification task leverages CE, CE+Triplet, and CE+SupCon objectives to enhance feature discrimination among vendor classes. For metric learning, Triplet and SupCon contrastive losses are employed to cluster representations of the ads from the same vendor while distinguishing those of different vendors. Retrieval performance is assessed using similarity search and metrics such as MRR@10, R-Precision@X, and Macro-F1@X across the South, Midwest, West, and Northeast datasets. The Zero-Shot (ZS) average reflects model performance across all datasets without task-specific training. In contrast, the OOD average score evaluates how models trained on the South dataset generalize to unseen ads and vendors in the Midwest, West, and Northeast datasets. Since classification models cannot handle unseen vendors in OOD scenarios, OOD performance is reported solely through retrieval metrics. For detailed quantitative insights and performance comparisons, we direct our readers to Appendix Tables 4-11.\n(i) Classification task: Consistent with the prior findings (Saxena et al., 2023a), the DeCLUTR outperforms the Style-Embedding backbone with CE loss amongst the text-baselines by about 12%. Furthermore, as shown in text-baseline results in Table 2, the DeCLUTR model performs the best when trained with the CE+SupCon joint objective.\nFor the vision baseline, ResNet-50 with CE loss achieves the highest macro-F1 score (0.6394) among classifiers, followed by EfficientNetV2 (0.6285), DenseNet-121 (0.6262), ConvNext-small (0.6215), and ViT-base-patch16 (0.6141). Despite its slight underperformance in classification tasks, our analysis in appendix table 6 reveals that ViT-base-patch16 outperforms all other models in retrieval tasks for both in-distribution and OOD datasets. This finding aligns with prior research (Gkelios et al., 2021; El-Nouby et al., 2021), which highlights ViT's ability to produce rich, contextualized embeddings that capture global relationships and stylistic patterns, even across diverse visual data (e.g., images with or without faces), making ViT-base-patch16 the most suitable backbone for our task. Finally, as shown in Table 2, the ViT baseline trained with the CE+Triplet objective achieves the best macro-F1 score of 0.6378, with CE+SupCon closely following at 0.6294.\nThe multimodal DeCLUTR-ViT backbone, trained end-to-end with mean pooling as the fusion technique, achieves the highest macro-F1 score (0.9670) on the South region dataset, surpassing VisualBERT and ViLT. Notably, the DeCLUTR-ViT baselines pre-trained with text-image alignment strategies, such as CLIP, ITC+ITM, or BLIP2, underperform when fine-tuned for the authorship identification task, though the BLIP2-pretrained backbone comes closest to matching DeCLUTR-ViT's performance (0.9420). When trained with the joint CE+SupCon objective, the DeCLUTR-ViT backbone demonstrates exceptional robustness in capturing multimodal relationships. This strong performance may also be attributed to the dataset's structure, where each text ad is paired with multiple images and vice versa, ensuring the model encounters diverse combinations during training.\n(ii) Retrieval Task: Since the metric-learning task employs triplet and SupCon losses, its effectiveness is assessed by the model's ability to cluster ad representations based on stylometric patterns from the same vendor. Similarly, the joint-objective classifiers also incorporate classification and metric-learning losses, enabling direct comparison between the baselines through a retrieval task.\nFigure 1(A) compares the text-to-text retrieval performance of text-only pre-trained (\u25cf), fine-tuned, and multimodal baselines. Fine-tuning on the South region dataset significantly improves performance across all metrics. Among text-only baselines, the DeCLUTR backbone trained with the joint CE+SupCon objective (\u2588) outperforms the CE-only baseline (\u25a0) and performs on-par with the SupCon-only baseline (\u2666) on OOD avg score, while surpassing it on the training dataset. Given the consistent performance of the DeCLUTR backbone with CE+SupCon objective on the classification and retrieval task, we establish it as the benchmark for text-only modality. This benchmark is further compared against the text representations from the multimodal DeCLUTR-ViT backbone trained end-to-end with CE+SupCon (+) and the fine-tuned DeCLUTR-ViT backbone, pre-trained for text-image alignment task using BLIP2 objective (\u0394). The multimodal backbone trained end-to-end with CE+SupCon consistently outperforms all baselines on training and OOD datasets.\nFigure 1(B) highlights image-to-image retrieval performance, comparing vision-only pre-trained (\u25cf), fine-tuned, and multimodal baselines. Fine-tuning on image ads also improves retrieval performance. Amongst vision-only baselines, the ViT backbone trained with the CE+SupCon objective (\u2666) achieves superior performance over other baselines on both training and OOD datasets, establishing itself as the benchmark for the vision-only modality. Despite the better performance of the ViT backbone with CE+Triplet objective (\u25a0) on classification, it underperforms on the retrieval task. We further compare this vision benchmark against the vision representations from the multimodal DeCLUTR-ViT backbone trained end-to-end with CE+SupCon (1) and the fine-tuned DeCLUTR-ViT backbone, pre-trained for text-image alignment task using BLIP2 objective (\u0394). The end-to-end multimodal backbone with CE+SupCon objective consistently outperforms other baselines on OOD datasets. However, it underperforms the fine-tuned BLIP2 baseline on R-Precision and Macro-F1 metrics for the training dataset.\nFigure 1(C) compares retrieval performance among multimodal baselines, evaluating the multimodal representation from the end-to-end multimodal DeCLUTR-ViT backbone trained end-to-end with CE+SupCon (1) and the fine-tuned DeCLUTR-ViT backbone, pre-trained for text-image alignment task using BLIP2 objective (+). The end-to-end multimodal backbone with CE+SupCon objective consistently outperforms the other baseline across the training and OOD datasets. Our analysis (Appendix Table 8) indicates that the low performance of the text-image alignment strategies can be attributed to the lack of semantic similarity between images and text, as images in escort ads often do not directly reflect the context of the accompanying text.\nTakeaways: Key takeaways from our results are:\n(i) MATCHED Dataset: Fine-tuning on our dataset significantly improves task performance, emphasizing its importance and revealing the limitations of pre-trained checkpoints in adapting to its unique linguistic and stylistic patterns.\n(ii) Benchmarks: Given the dual objective of vendor identification and verification, we establish DeCLUTR-small and ViT-base-patch16-244 backbones with CE+SupCon joint objective as our text- and vision-only benchmarks. The multimodal benchmark integrates these backbones with mean pooling as the fusion technique.\n(iii) Benefits of Joint Objective: The CE+SupCon joint objective consistently outperforms or matches other objectives on both in-distribution and OOD datasets, demonstrating its robustness and generalization capability. Furthermore, employing joint objectives allows AA models to address closed-set vendor identification and open-set vendor verification tasks effectively.\n(iv) Effects of Multimodal Integration: Integrating multimodal features significantly enhances AA performance across all tasks, outperforming text-only and vision-only benchmarks. Multimodal setups effectively leverage complementary features from both modalities, capturing richer and more comprehensive authorship patterns.\n(v) Text-Image Alignment Challenges: Text-image alignment strategies, inspired by CLIP and BLIP2 research, struggle to connect text to image pairs in our dataset due to a lack of semantic similarity, as images in escort ads often do not directly reflect the context of the accompanying text.\n(vi) Modality-Specific Performance: In the multimodal setup, combining text and vision features improves vision retrieval performance compared to the vision-only backbone, though it remains unreliable. However, incorporating vision features into the text modality significantly enhances text retrieval performance, with text consistently delivering the best results across text, vision, and multimodal representations. This highlights the superiority of the text representations from the DeCLUTR-ViT backbone, making it the most effective option for retrieval tasks on our dataset."}, {"title": "6 Discussion & Further Insights", "content": "This research introduces a novel multimodal dataset and conducts extensive benchmarking to demonstrate that multitask joint objectives and multimodal data integration enhance the performance of AA tasks on both in-distribution and OOD datasets. By linking escort ads through these techniques, we aim to assist researchers, investigators, and LEAs study HT indicators. Given the space constraints of the main manuscript, we prioritized presenting the most critical claims and experimental results. However, additional insights and detailed analyses are provided in the Appendix.\nSpecifically, data analysis, preprocessing steps, and a datasheet following (Gebru et al., 2021) are detailed in Appendix sections A.2 and A.3. Given the sensitive nature of our research, we decided not to release any models publicly and, therefore, haven't attached any model cards for our research. Appendix A.4 outlines the training setup and computational considerations, while Appendix A.5 presents comprehensive performance metrics for all baselines. Key insights into model behavior and learning dynamics are discussed in Appendix A.6, and the practical application of AA tasks in building knowledge graphs for investigative purposes is explored in Appendix A.7"}, {"title": "7 Conclusion", "content": "Through this research, we demonstrate the potential of MAA in addressing the complexities of vendor identification and verification within online escort markets. Using our novel MATCHED dataset, we extensively benchmark text-only, vision-only, and multimodal approaches, showcasing the advantages of CE+SupCon multitask training objectives. Our analysis reveals that this dual-objective consistently outperforms single-task approaches across in-distribution and OOD datasets, enabling LEAs to identify known vendors while linking emerging ones in new markets. Additionally, multimodal integration significantly enhances model performance by capturing complementary patterns across text and images. While text remains the dominant modality, integrating image data along text descriptions adds stylistic cues that enrich the model's capabilities. Among text, vision, and multimodal representations, text representations from the DeCLUTR-ViT backbone emerge as the most effective for retrieval tasks, achieving the best results across all modalities. While pre-trained text-image alignment strategies like CLIP and BLIP2 fail to establish meaningful cross-modal connections due to low semantic overlap and ineffective use of stylistic features, end-to-end multitask training is a more robust approach for leveraging multimodal data in AA tasks. Finally, the performance gap between pre-trained checkpoints and fine-tuned baselines highlights the importance of domain-specific adaptations and task-specific training, providing a strong foundation for future research. By addressing real-world challenges and emphasizing scalability, we aim to equip LEAs with actionable tools to uncover and disrupt trafficking networks effectively."}, {"title": "8 Limitations", "content": "Assumption: Similar to existing research, our research assumes that each class label corresponds to a distinct vendor during the classification task, vendors across the South and OOD datasets enabling the model to leverage domain knowledge effectively. However, our qualitative analysis identifies cases where the trained classifier misclassifies ads, likely due to similarities in writing style and content, suggesting the possibility that multiple vendors might belong to the same entity. While we lack definitive ground truth to confirm this hypothesis, it represents a notable challenge in ensuring label accuracy. We recognize that improving the quality of vendor labels would likely lead to enhanced benchmark performance and more robust model evaluations.\nDataset Limitations and Generalization Challenges: Our research utilizes escort ads collected from the Backpage platform between December 2015 and April 2016. These ads, sourced from seven U.S. cities and categorized into four geographical regions, exhibit significant vendor overlap across regions, as shown in appendix figure 2a. Manual inspection also reveals the presence of near-duplicate ads that are challenging to identify and remove due to extensive noise and variability within the dataset. While we aim to evaluate the OOD generalization capabilities of our trained models, a more realistic scenario would involve incorporating ads from multiple escort platforms and broader geographical areas to better simulate cross-platform generalization performance. Finally, our dataset covers only briefly from December 2015 to April 2016. Extending the collection period would allow us to understand how vendors change their writing styles to evade detection, providing deeper insights into temporal dynamics that our current work does not address.\nSelective Feature Extraction and Fine-Tuning: In this work, we extract text and vision representations exclusively from the final layers of our models, which may not fully capture nuanced features learned at earlier layers. Representations extracted from intermediate layers could yield different or potentially better outcomes. Additionally, while fine-tuning pre-trained text-image alignment models, we fine-tune all layers uniformly, which may not be optimal. Techniques like Centered Kernel Alignment (CKA) (Kornblith et al., 2019) can provide insights into which layers learn the most relevant features, enabling more informed decisions about representation extraction and selective layer freezing during fine-tuning. Addressing these concerns is currently beyond the scope of this research, but we plan to explore these aspects in future work.\nComputational Constraints: While our research employs relatively large model architectures and advanced training strategies, it is limited by the computing resources available to us. Larger model architectures could potentially enhance performance across classification and retrieval tasks. However, when applied to text-image alignment tasks, the computational demands of scaling these models exceeded our resource capacity. As a result, we opted for smaller, more efficient architectures that fit within our computational constraints, ensuring a fair and balanced comparison across baselines. Similarly, our research relies heavily on contrastive learning objectives, and prior studies (Gao et al., 2021; Vaessen and van Leeuwen, 2024) highlight the benefits of larger batch sizes for such tasks. However, to maintain consistency and fairness among baselines, we limited our batch size to 32, as larger sizes led to memory errors, particularly with text-image alignment models. This computational limitation also influenced our decision to forego fine-tuning pre-trained CLIP and BLIP2 checkpoints, as the memory requirements for fine-tuning BLIP2 architecture caused GPU crashes. These decisions reflect deliberate trade-offs made to ensure the reproducibility and fairness of our experimental comparisons while working within resource limitations.\nExplainability: Although this research does not explicitly address the explainability or interpretability of our models, we recognize their critical role in fostering trust among researchers, investigators, and law enforcement agencies. Previous studies (Saxena et al., 2023a) have explored explainability in AA through local feature attribution techniques applied to text ads. However, while numerous frameworks exist for explainability in unimodal data (Ribeiro et al., 2016; Lundberg and Lee, 2017; Kokhlikyan et al., 2020), these methods cannot be directly extended to the multimodal AA context. Additionally, research highlights the limitations of existing explainability techniques, including their susceptibility to adversarial attacks, network sparsity, and inconsistencies in results (Das and Rad, 2020; Krishna et al., 2022; Saxena et al., 2023b). We aim to address these challenges in future work by developing a robust explainability framework tailored specifically for multimodal AA scenarios. Such a framework will help uncover the contributions of textual and visual features in decision-making processes, ensuring transparency and reliability in the application of multimodal AA models."}, {"title": "9 Ethical Considerations", "content": "Generative Models: Vendors can potentially leverage advanced Large Language Models (LLMs), like ChatGPT, to generate text ads with varying linguistic styles, making them harder to trace back to individual authors. While many publicly available LLMs restrict content generation for illegal purposes, vendors could fine-tune open-source LLMs or develop models tailored to evade AA by mimicking diverse stylistic patterns. This poses a significant challenge to our AA systems, which rely on detecting unique stylometric signatures. Similarly, the advent of vision-based generative models enables vendors to create or manipulate images to obscure identifiable stylistic cues. These advancements in generative technologies could diminish the effectiveness of both text- and vision-based AA models. In the future, we plan to recollect and analyze updated datasets to ensure our AA systems can differentiate between content generated by automated systems and authentic user-generated data, adapting to these emerging threats.\n9.1 Data Protocols\nWe collect our dataset from the Backpage Escort Markets spanning seven U.S. cities, posted between December 2015 and April 2016. Following ethical guidelines outlined by Krotov et al. (2020), which presents a framework of seven principles for responsible web scraping, we ensured our approach complied with these standards. The Backpage website's use policy does not explicitly prohibit data scraping.\n9.2 Privacy Considerations and Potential Risks\nIn undertaking this research, we recognize the significant privacy concerns associated with using data from escort advertisements, particularly given that individuals within these ads may be at risk. However, the prevalence of human trafficking, a grave societal issue that affects countless lives, drives our commitment to contribute positively to anti-trafficking efforts. We believe our intentions align with the broader ethical imperative to support the fight against exploitation and to aid LEA in identifying and disrupting trafficking networks.\nTo address privacy concerns, we have extensively tried to mask personal identifiers within the dataset. Following methods from Saxena et al. (2023a), we mask phone numbers, email addresses, post IDs, dates, and links in text data, transforming them into generalized formats such as \"<EMAILID-23>\" or \"<LINK>,\" which minimizes the risk of reverse engineering and personal identification (please refer appendix section A.2 for more details). At the same time, we explored various entity recognition tools to mask names (Li et al., 2022a; Liu et al., 2023) and locations, the inherent noise in the data led to inaccuracies, with some false positives in entity predictions. Since research indicates that individuals in these ads often use pseudonyms (Carter et al., 2021; Lugo-Graulich) and Backpage ads are no longer publicly accessible after the 2016 seizure, we find it unlikely that masked text data could be misused for individual identification.\nPrivacy risks are more challenging to mitigate for the image data, as AA relies on preserving stylistic cues. Although we initially considered blurring faces to protect identities, we ultimately decided against it to avoid introducing biases that could compromise the authenticity of stylometric patterns. This decision was made after careful consideration of the potential impact on the accuracy and integrity of the AA task. Many ads already feature images with blurred or cropped"}]}