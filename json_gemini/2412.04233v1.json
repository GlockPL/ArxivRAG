{"title": "HyperMARL: Adaptive Hypernetworks for Multi-Agent RL", "authors": ["Kale-ab Abebe Tessera", "Arrasy Rahman", "Stefano V. Albrecht"], "abstract": "Balancing individual specialisation and shared behaviours is a critical challenge in multi-agent reinforcement learning (MARL). Existing methods typically focus on encouraging diversity or leveraging shared representations. Full parameter sharing (FuPS) improves sample efficiency but struggles to learn diverse behaviours when required, while no parameter sharing (NoPS) enables diversity but is computationally expensive and sample inefficient. To address these challenges, we introduce HyperMARL, a novel approach using hypernetworks to balance efficiency and specialisation. HyperMARL generates agent-specific actor and critic parameters, enabling agents to adaptively exhibit diverse or homogeneous behaviours as needed, without modifying the learning objective or requiring prior knowledge of the optimal diversity. Furthermore, HyperMARL decouples agent-specific and state-based gradients, which empirically correlates with reduced policy gradient variance, potentially offering insights into its ability to capture diverse behaviours. Across MARL benchmarks requiring homogeneous, heterogeneous, or mixed behaviours, HyperMARL consistently matches or outperforms FuPS, NoPS, and diversity-focused methods, achieving NoPS-level diversity with a shared architecture. These results highlight the potential of hypernetworks as a versatile approach to the trade-off between specialisation and shared behaviours in MARL.", "sections": [{"title": "1 INTRODUCTION", "content": "Balancing individual and shared behaviours is critical to collective intelligence. Individual-level specialisation in a population helps improve the performance of these systems, whether in biological fitness or decision-making processes [35, 36, 45]. However, shared behaviours, i.e. behaviours consistent among agents, play a crucial role in ensuring adaptability and efficiency [23, 43]. This trade-off between specialised and shared behaviours is also pertinent in the context of Multi-Agent Reinforcement Learning (MARL), where a balance between these behaviours is essential.\nAlthough the importance of balancing individual and shared behaviours in MARL is well-recognised [5, 25], achieving this balance is challenging. On the one end of the spectrum, when we want individual agents to specialise and behave differently, a straightforward approach is for each agent to have their own policy and critic networks, which is referred to as no parameter sharing (NoPS) [28]. This allows each agent to develop specialised policies tailored to specific tasks or roles. While NoPS enables diverse behaviours, it is computationally expensive and scales poorly as the number of agents increases, as it requires N sets of parameters for N agents. Furthermore, NoPS is sample inefficient [10], as agents learn independently without sharing information with one another.\nOn the other end of the spectrum, full parameter sharing (FuPS) [16, 18, 37]2 is a widely used approach when similar behaviour across agents is desired. In FuPS, all agents share the same policy and critic parameters, which improves sample efficiency by allowing agents to learn from each other's experiences [10, 38]. Additionally, FuPS enhances memory efficiency, requiring fewer parameters, and is computationally efficient due to the parallelisation of the agents' learning process. Although parameter-shared networks conditioned on unique agent IDs are theoretically capable of representing diverse behaviours [20], they often exhibit limited expressive capacity in practice. This limitation leads to underperformance compared to NoPS in scenarios that require diverse behaviours [10, 17, 24, 25]. Christianos et al. [10] attributed this underperformance to the inherent challenges of using a single neural network to represent multiple diverse agent behaviours effectively.\nBalancing the efficiency of parameter sharing with the ability to learn diverse behaviours is a fundamental challenge in MARL. Intrinsic reward methods based on mutual information (MI) promote diversity by modifying the learning objective to encourage diverse policies [22, 25]. However, this can result in suboptimal behaviour, as agents may prioritize distinguishing themselves in states rather than optimising task performance. Furthermore, it has been shown that these methods can be outperformed by simpler approaches like FuPS and NoPS [17]. Role-based methods, like ROMA and RODE, [41, 42] assign agents to subtasks but rely on MI objectives for diversity, with limited practical success [11]. Architectural methods [24] require careful tuning of hyperparameters, such as pruning rates per layer and have not shown statistically significant"}, {"title": "2 BACKGROUND", "content": "We formulate the fully cooperative multiagent systems addressed in our work as a Dec-POMDP [30]. A Dec-POMDP is a tuple, $(I, S, {A^{i}}_{i\\in I}, R, {O^{i}}_{i\\in I}, {\\Omega^{i}}_{i\\in I}, P, p_{0}, \\gamma)$, where I denotes the set of agents, S is the global state space, $A^{i}$ is the action space for agent i, $R: S \\times A \\times S \\rightarrow R$ is the shared reward function, $O^{i}$ is the observation space for agent i, $\\Omega^{i} : A \\times S \\times O \\rightarrow [0, 1]$ is the observation function for agent i, $P: S \\times A \\times S \\rightarrow [0, 1]$ is the state transition function, $p_{0}$ is the initial state distribution, and $\\gamma$ is the discount factor.\nIn this setting, agents receive partial observations and select actions based on their decentralized policies $\\pi^{i}(a^{i}|\\tau^{i})$, where $\\tau^{i}$ is the action-observation history of agent i. The goal is to learn a joint policy $\\pi = (\\pi^{1}, ..., \\pi^{n})$ that maximizes the expected return $E_{\\tau \\sim \\pi} [G(\\tau)]$, where the trajectory $\\tau = (s_{0}, a_{0}, s_{1}, ...)$ is generated by the joint policy, with $s_{0} \\sim p_{0}$, and $G(\\tau) = \\sum_{t=0} \\gamma^{t} R(s_{t}, a_{t}, s_{t+1})$."}, {"content": "We formulate the fully cooperative multiagent systems addressed in our work as a Dec-POMDP [30]. A Dec-POMDP is a tuple, $(I, S, \\{A^{i}\\}_{i\\in I}, R, \\{O^{i}\\}_{i\\in I}, \\{\\Omega^{i}\\}_{i\\in I}, P, p_{0}, \\gamma)$, where I denotes the set of agents, S is the global state space, $A^{i}$ is the action space for agent i, $R: S \\times A \\times S \\rightarrow R$ is the shared reward function, $O^{i}$ is the observation space for agent i, $\\Omega^{i} : A \\times S \\times O \\rightarrow [0, 1]$ is the observation function for agent i, $P: S \\times A \\times S \\rightarrow [0, 1]$ is the state transition function, $p_{0}$ is the initial state distribution, and $\\gamma$ is the discount factor.\nIn this setting, agents receive partial observations and select actions based on their decentralized policies $\\pi^{i}(a^{i}|\\tau^{i})$, where $\\tau^{i}$ is the action-observation history of agent i. The goal is to learn a joint policy $\\pi = (\\pi^{1}, ..., \\pi^{n})$ that maximizes the expected return $E_{\\tau \\sim \\pi} [G(\\tau)]$, where the trajectory $\\tau = (s_{0}, a_{0}, s_{1}, ...)$ is generated by the joint policy, with $s_{0} \\sim p_{0}$, and $G(\\tau) = \\sum_{t=0} \\gamma^{t} R(s_{t}, a_{t}, s_{t+1})$."}, {"title": "2.1 Specialised Policies and Environments", "content": "Specialisation plays a key role in MARL, yet remains under-defined, hindering consistent evaluation. To address this, we define specialised environments and specialised policies.\nDefinition 2.1. An environment is specialised if:\n(1) The optimal joint policy $\\pi^{*}$ consists of at least two distinct agent policies, i.e., $\\exists i, j \\in I$ such that $\\pi^{i} \\neq \\pi^{j}$, where I is the set of agents.\n(2) Any permutation $o$ of the policies in $\\pi^{*}$ produces an expected return that is weakly lower or equivalent to that of the original joint policy:\n$E_{\\tau \\sim \\pi o} [G(\\tau)] \\leq E_{\\tau \\sim \\pi^{*}} [G(\\tau)]$,\nwith strict inequality when agents' policies are not symmetric.3\nFor example, in a specialised environment like a football game, agents adopt distinct roles-e.g., \"attackers\" and \"defenders\"-to optimise team performance. Permuting their roles (policies) results in suboptimal outcomes, demonstrating the need for complementary behaviours.\nIn such environments, agents develop specialised policies when they learn distinct, complementary behaviours necessary for the optimal joint policy. By definition, heterogeneous agents-those with different action or observation spaces-are naturally specialised, as their distinct capabilities require unique policies. However, homogeneous agents, which share identical capabilities, can also develop specialised policies by adopting distinct roles that are necessary for the team's success. We work with specialised environments in Sections 3.1 and 5.2."}, {"title": "2.2 Quantifying Team Diversity", "content": "We quantify policy diversity using System Neural Diversity (SND) [6], which measures behavioural diversity based on differences in policy outputs:\n$SND (\\{\\pi^{i}\\}_{i\\in I}) = \\frac{2}{n(n-1)|O|} \\sum_{i=1}^{n} \\sum_{j=i+1}^{n} \\sum_{o\\in O} D(\\pi^{i}(o_{t}), \\pi^{j}(o_{t})),$   (1)\nwhere n is the number of agents, O is a set of observations typically collected via policy rollouts, $\\pi^{i}(o_{t})$ and $\\pi^{j}(o_{t})$ are the outputs of policies i and j for observation ot, and D is our distance function."}, {"title": "3 CHALLENGES IN BALANCING SPECIALISED AND SHARED BEHAVIOURS", "content": "Balancing specialised agent behaviours and shared policy representations is challenging in MARL. In this section, we introduce two simple matrix games that illustrate the challenges of achieving this balance, even in basic settings. Through analysis and empirical validation, we demonstrate the limitations of both fully shared and fully independent policies, highlighting the need for more nuanced approaches in MARL."}, {"title": "3.1 Specialisation and Synchronisation Game", "content": "The Specialisation Game, inspired by the XOR game [17] and VMAS's Dispersion [4], is a stateless, two-player matrix game where each agent selects one of two food items. Payoffs are 0.5 for matching choices and 1 for different choices, as shown in the payoff matrix in Table 1.\nThis game has two symmetric Nash equilibria, both along the anti-diagonal of the payoff matrix. It differs from the XOR game by assigning non-zero payoffs for matching actions. The game can be extended to n agents, with payoffs of 1 for unique actions and 1/n for matching actions. This satisfies the conditions of a Specialised Environment as defined in Definition 2.1.\nSynchronisation Game This is the inverse of the Specialisation Game, where players aim to choose the same food item. Payoffs are 1 for matching choices and 0.5 for different choices, resulting in Nash equilibria along the diagonal. In the n-agent version, matching actions receive a payoff of 1, while different choices receive 1/n."}, {"title": "3.2 Limits of Shared Policies", "content": "Next, we provide a formal proof showing that a shared policy cannot learn the optimal behaviour in the 2-player Specialisation game, following from the proof that a shared policy cannot learn the optimal policy in the XOR game.\nTHEOREM 3.1. A shared policy cannot learn the optimal behaviour for the two-player Specialisation Game.\nPROOF. Let \u03c0 be a shared policy for both agents, and let $a = P(a_{i} = 0)$ represent the probability of any agent choosing action 0. The expected return of \u03c0 for each agent is:\n$E[R(\\pi)] = P(a_{0} = 0, a_{1} = 0) \\cdot 0.5 + P(a_{0} = 0, a_{1} = 1) \\cdot 1$    (2)\n$+ P(a_{0} = 1, a_{1} = 0) \\cdot 1 + P(a_{0} = 1, a_{1} = 1) \\cdot 0.5$    (3)\n$= 0.5a^{2} + 2a(1 - a) + 0.5(1 - a)^{2}$    (4)\n$= -a^{2} + a + 0.5$    (5)\n$= -(a - 0.5)^{2} + 0.75$    (6)\nThus, $E[R(\\pi)] \\leq 0.75 < 1$ for all $a \\in [0, 1]$, with the maximum at a = 0.5. Therefore, a shared policy cannot achieve the optimal return of 1, confirming the need for specialised behaviour to optimise rewards."}, {"title": "3.3 Empirical Validation using Function Approximation", "content": "To empirically validate the challenges in balancing specialisation and shared behaviours when using function approximation, we conduct empirical experiments on the Specialisation and Synchronisation Games, extending the analysis to n = 2, 4, and 8 agents. We compare three approaches: independent policies (NoPS), fully shared policies (FuPS), and shared policies conditioned on one-hot encoded agent IDs (FuPS+ID One-Hot), using REINFORCE [44]. All variants used single-layer neural networks for the policy, and we controlled for the number of parameters across NoPS and FuPS approaches. Hyperparameters were kept constant across experiments, with results averaged over five seeds. We provide the full hyperparameters in Table 4 in Appendix B.\nThe results in Table 2 show that while NoPS consistently learns the optimal policy in the Specialisation Game, FuPS struggles, especially with n = 8 agents. Conversely, in the Synchronisation Game, FuPS performs better 4, while NoPS struggles with sample efficiency as the number of agents increases (detailed sample efficiency plots in 13 in Appendix C). FuPS+ID shows promising results in balancing specialised and shared representations, but its performance deteriorates at larger scales, suggesting that ID conditioning alone is insufficient for fully distinct behaviours.\nThe results demonstrate that while shared policies face significant challenges in environments requiring agent specialisation, independent policies struggle with efficiency in environments demanding synchronisation. In Section 5.2, we extend this analysis to temporal MARL environments, where modern MARL algorithms like IPPO and MAPPO encounter similar issues.\nThese findings underscore the need for more nuanced methods that can effectively leverage shared knowledge while still enabling"}, {"title": "4 HYPERMARL: HYPERNETWORKS FOR ENABLING SPECIALISATION IN MARL", "content": "As shown in the previous section, balancing specialised and shared behaviours is a challenging task. To address this challenge, we introduce HyperMARL, a simple yet effective approach to enable agent specialisation in MARL through agent-conditioned hypernetworks, trained in a fully end-to-end manner."}, {"title": "4.1 Hypernetworks for MARL", "content": "Hypernetworks [19] are neural networks that generate the weights of another neural network (the target network) based on an input context vector. They have proven effective in meta-learning and multi-task learning within single-agent RL settings [2, 3].\nIn HyperMARL, hypernetworks are used to enable specialisation among agents. Specifically, a hypernetwork h takes a context vector e and outputs the weights for both the policy and critic networks:\n$\\theta = h_{\\psi}(e)$ and $\\phi = h_{\\psi}(e)$   (7)\nwhere $h_{\\psi}$ and $h_{\\psi}$ are the hypernetworks for the policy and critic, respectively, and e is either the agent's one-hot encoded ID or a learned embedding.\nThis design allows the hypernetwork to produce agent-specific policies and critics as:\n$\\theta^{i} = h_{\\psi}(e^{i})$ and $\\phi^{i} = h_{\\psi}(e^{i})$   (8)\nwhere $e^{i}$ represents the context vector for agent i, and $\\theta^{i}$, $\\phi^{i}$ denote the agent-specific policy and critic parameters, as illustrated in Figure 1. This mechanism allows a single hypernetwork to generate unique policy and critic parameters for each agent, enabling individual specialisation when needed. In tasks that require uniform behaviour across agents, the hypernetwork can adapt by learning similar agent embeddings or producing similar policies for different agent IDs. This flexibility ensures that both specialised and uniform behaviours are captured within the same framework."}, {"title": "4.2 Linear Hypernetworks", "content": "Linear hypernetworks with one-hot agent IDs effectively create separate parameters for each agent. Given a one-hot agent ID, $1^{i} \\in R^{1\\times n}$, a linear hypernetwork $h_{\\psi}$ generates agent-specific parameters $\\theta^{i}$ as:\n$\\theta^{i} = h_{\\psi}(1^{i}) = 1^{i} \\cdot W + b$  (9)\nwhere $W \\in R^{n\\times m}$ is the weight matrix (m being the number of parameters for each agent's policy and n is the number of agents), and $b \\in R^{1\\times m}$ is a bias vector. Since $1^{i}$ is one-hot encoded, each $\\theta^{i}$ corresponds to a specific row of W plus the shared bias b. If there is no shared bias term, this effectively replicates training of separate policies for each task (in our case for each agent) [2] since there are no shared parameters and gradient updates are independent."}, {"title": "4.3 Non-linear Hypernetworks for Expressiveness", "content": "Multilayer perceptron (MLP) hypernetworks extend linear hyper-networks, by including one or more non-linear layers, as follows:\n$\\theta = h_{\\psi}(e) = f(g(e))$ and $\\phi = h_{\\psi}(e) = f(g(e))$  (10)\nwhere e is the context vector, $f_{1}$ represents the linear transformation of the output layer, and $g_{1, 2}$ represents the preceding layers, which contain non-linear activation functions such as ReLU.\nThese non-linear transforms provide additional flexibility, allowing the hypernetworks to generalise beyond linear mappings of the context to the policy and critic weights. This could be beneficial when agents need to learn intricate specialisation patterns.\nHowever, unlike linear hypernetworks with one-hot agent IDs, non-linear hypernetworks do not guarantee distinct weights for each agent. Additionally, they increase the total number of trainable parameters, necessitating a careful balance between expressiveness and computational efficiency."}, {"title": "4.4 Decoupling Agent and State-based Gradients in HyperMARL", "content": "HyperMARL decouples gradients arising from state-action dynamics and agent-specific features by leveraging hypernetworks. This decoupling allows agents to learn unique behaviours while mitigating interference during training.\nIn HyperMARL, the policy for agent i is defined as:\n$\\pi^{i}(a|o^{i}) = \\pi_{\\theta^{i}}(a|o^{i})$, where $\\theta^{i} = h_{\\psi}(e^{i})$,   (11)\nwhere $h_{\\psi}$ is the policy hypernetwork parameterized by weights \u03c8, and $e^{i}$ is the embedding for agent i.\nFor a shared policy, the policy gradient can be expressed as follows:\n$\\nabla_{\\theta}J(\\theta) = E_{\\tau \\sim \\pi} [\\sum_{t=0}^{T} \\sum_{i=1}^{I} A(\\theta, a_{t}^{i}) \\nabla_{\\theta} log \\pi_{\\theta} (a_{t}^{i}|o_{t}^{i})]$  (12)\nwhere t is the trajectory sampled from the joint policy \u03c0, $A(o_{t}^{i}, a_{t}^{i})$ is the advantage for agent i and I is our set of agents.\nIn HyperMARL, policy parameters $\\theta_{i}$ are defined by the hypernetwork $h_{\\psi}$. The policy gradient with respect to \u03c8 becomes:\n$\\nabla_{\\psi}J(\\psi) = E_{\\tau \\sim \\pi_{\\theta}} [\\sum_{t=0}^{T} \\sum_{i=1}^{I} A(o_{t}^{i}, a_{t}^{i}) \\nabla_{\\psi} log \\pi_{\\theta_{i}} (a_{t}^{i}|o_{t}^{i})]$  (13)\nApplying the chain rule:\n$\\nabla_{\\psi} log \\pi_{\\theta_{i}} (a_{t}^{i}|o_{t}^{i}) = \\nabla_{\\theta_{i}} log \\pi_{\\theta_{i}} (a_{t}^{i}|o_{t}^{i}). \\nabla_{\\psi} h(e^{i})$ (14)\nwhere $\\nabla_{\\psi}h(e^{i}) = \\nabla_{\\psi}\\theta^{i}$, since $\\theta^{i} = h(e^{i})$.\nSubstituting this into the policy gradient:\n$\\nabla_{\\psi}J(\\psi) = E_{\\tau \\sim \\pi_{\\theta}} [\\sum_{t=0}^{T} \\sum_{i=1}^{I} A(o_{t}^{i}, a_{t}^{i}) \\nabla_{\\theta_{i}} log \\pi_{\\theta_{i}} (a_{t}^{i}|o_{t}^{i}) . \\nabla_{\\psi} h(e^{i})]$  (15)\nThis formulation separates the gradient into two components:\n*   $\\nabla_{\\theta_{i}} log \\pi_{\\theta_{i}} (a_{t}^{i}|o_{t}^{i})$: State-dependent gradients - captures how state-action dynamics affect the policy. This term is independent across agents because i is unique for each agent.\n*   $\\nabla_{\\psi}h^{i}(e^{i})$: Agent-specific gradient \u2013 captures how the hypernetwork parameters \u03c8 influence the policy parameters $\\theta^{i}$ via the agent embedding $e^{i}$.\nDecoupling these gradients reduces interference during training, as the state-based gradients remain independent, enabling agents to adapt unique parameters for their policies. This approach has shown benefits in Meta-RL [34], where hypernetworks reduced policy gradient variance, leading to more stable training. Similarly, in Section 5.2.3, we observe lower gradient variance with HyperMARL. This could provide insight into the benefits of using hypernetworks in MARL, especially for learning diverse behaviour using a shared policy."}, {"title": "4.5 Initialisation", "content": "HyperMARL's hypernetwork initialisation ensures the generated weights for each agent match the distribution of those in a standard network without hypernetworks. For example, PPO commonly uses orthogonal initialisation [15], so HyperMARL generates orthogonal initial weights for each agent with the same scaling/gain. This approach is adaptable to any initialisation strategy."}, {"title": "4.6 Agent Embedding", "content": "For our linear hypernetworks, we use one-hot encoded agent IDs as they result a mix of agent independent weights and shared weights as discussed in Section 4.2. For our MLP hypernetworks, we used learned agent embeddings $e^{i}$, initialised orthogonally. These embeddings are learned parameters, optimized during training independently of observations. This setup allows agents to learn distinct or similar embeddings as needed, enabling specialisation while training in an end-to-end fashion."}, {"title": "5 EXPERIMENTS", "content": "We evaluate the effectiveness of HyperMARL in various MARL environments, testing its ability to handle homogeneous, heterogeneous, and mixed behaviours."}, {"title": "5.1 Experimental Setup", "content": "5.1.1 MARL Algorithms. We compare HyperMARL with strong baselines IPPO [12] and MAPPO [46]. IPPO conditions each agent's actor and critic on individual observations, while MAPPO uses a centralized critic conditioned on the global state or concatenated agent observations. We use NoPS and FuPS variants of these methods, with all FuPS variants being conditioned on one-hot encoded agent IDs.\nFor the experiments in Sections 5.2 and 5.3, we use feedforward architectures, while for Section 5.4, we use recurrent GRU networks [9]. In all cases, HyperMARL generates the actor and critic weights based on the agent's embedding. For recurrent PPO variants, HyperMARL maintains the same architecture as for feedforward methods, generating only the actor and critic feedforward weights, not the GRU weights.\nAs described in Section 4.6, we use one-hot encoded IDs for linear hypernetworks and learned embeddings for MLP hypernetworks. All hyperparameters match the baselines unless otherwise stated, with all the hyperparameters in Appendix B."}, {"title": "5.1.2 Environments.", "content": "We test our methods in the following environments:\n*   Dispersion (VMAS) [4]: n agents and n food particles are randomly placed in a 2D space. Agents observe their relative distance to all food particles and and receive a reward for collecting food and incur a step penalty. The optimal policy requires each agent to go to different food particles, i.e. learn heterogeneous behaviour, and hence this environment is a strong failure case for parameter-sharing methods. We use the discrete version of this environment, with n = 4 agents. This can be seen as a temporal and stateful version of the Specialisation Game from Section 3.1.\n*   Navigation (VMAS) [4]: Agents are spawned at random locations and must navigate to their assigned goals and receive dense rewards based on the relative distance to their goals. We test three settings: all agents sharing the same goal (homogeneous behaviour), different goals (heterogeneous behaviour), and a mix of shared and individual goals. We experiment with 2, 4, and 8 agents. We use the same setting of this environment from the DiCo paper [5], i.e. continuous control.\n*   SMAX (JaxMARL) [32]: We test on four maps from JaxMARL'S SMAX, a Jax-accelerated version of SMAC [33] and SMACv2 [13]. Although SMAX has simplified dynamics compared to SMAC, it presents a significant challenge due to its high-dimensional observation space, intricate unit interactions, and sophisticated heuristic agents. We test on both SMACv1-style maps (2s3z, 3s5z) and SMACv2-style maps, which include randomized ally/enemy units and initial positions (smacv2_10, smacv2_20)."}, {"title": "5.1.3 Training and Evaluation.", "content": "*   Training: For Section 5.2, we run 5 seeds and train for 20 million steps. For Sections 5.3 and 5.4, we train for 10 million timesteps as per the baselines.\n*   Evaluation: In Section 5.2, we evaluate every 100k steps across 32 episodes. In Section 5.3, following [5], we evaluate every 120k steps for 200 episodes. In Section 5.4, evaluation occurs every 500k steps across 32 episodes."}, {"title": "5.1.4 Measuring Policy Diversity.", "content": "We measure team diversity using the System Neural Diversity (SND) metric [Equation 1, [6]] with Jensen-Shannon distance. SND ranges from 0 (identical policies across all agents) to 1 (maximum diversity). We collect a dataset of observations from IPPO-NoPS and IPPO-FuPS policies checkpointed at 5 and 20 million training steps. Each policy is rolled out"}, {"title": "5.2 Learning Diverse Behaviour (Dispersion)", "content": "5.2.1 Performance on Dispersion. From the results, in Figures 2a and 2b we see that both parameter sharing variants (IPPO-FuPS, MAPPO-FuPS - ()) fail to learn the diverse policies required to solve the Dispersion task, while their non-parameter sharing counterparts (IPPO-NoPS, MAPPO-NoPS \u2013 ()) are able to converge to the optimal policy. Although MAPPO-FuPS does appear to be trending towards the optimal mean episode return, its slower rate of convergence suggests that it would require substantially more samples to achieve competitive performance. These results confirm that parameter-sharing methods struggle to learn the diverse policies in environments requiring a high degree of agent specialisation.\nIn comparison, both our linear and MLP hypernetworks (,\u2022) match the performance of NoPS across mean return and sample efficiency. In some cases, such as IPPO-MLP hypernetworks, we observe higher confidence intervals during training than NoPS, indicating more variability. This could be attributed to the learning of useful agent embeddings. However, at convergence (Figures 2c"}, {"title": "5.2.2 Diversity of Policies Learned.", "content": "We measure policy diversity as discussed in Section 5.1.4. Figure 3 illustrates that IPPO-FuPS and MAPPO-FuPS exhibit lower diversity compared to their NoPS counterparts, aligning with performance results. Notably, both our linear and MLP hypernetworks achieve diversity levels comparable to NoPS methods, despite utilizing a shared architecture. Linear hypernetworks enforce weight and gradient separation via one-hot encoded agent IDs, but retain some parameter sharing through shared biases. In contrast, MLP hypernetworks dynamically learn agent ID embeddings and apply non-linear functions to generate weights, facilitating end-to-end learning of diverse policies, without enforcing any separation of weights and gradients."}, {"title": "5.2.3 Gradient Variance.", "content": "To investigate HyperMARL's effect on training stability, we compute the average policy gradient variance by calculating the variance across all actor parameters at each update and averaging it over the training period. As shown in Figure 4, hypernetworks in both IPPO and MAPPO exhibit lower mean policy gradient variance than FuPS. This reduction, which aligns with the hypernetworks' ability to learn diverse behaviors, may stem from decoupling agent-specific and state-based gradients"}, {"title": "5.3 Balancing Diverse and Shared Behaviours (Navigation)", "content": "We evaluate HyperMARL against DiCo [5] in the Navigation environment [4], using DiCo's best diversity targets for n = 2 agents (SNDdes = 1.2 for different goals, SNDdes = 0 for the same goals) and matching its hyperparameters. Since DiCo only tested with n = 2 agents, we sweep diversity levels for more agents, and for all methods we sweep the same learning hyperparameters (details in Tables 10 and 11 in Appendix B). While better diversity levels may exist for more agents, finding the right level becomes challenging, even when known for the same task with fewer agents.\nFigure 5 presents IQM of the mean reward across different goal setups. HyperMARL consistently demonstrates strong performance across all configurations, outperforming DiCo and achieving comparable or better performance than the NoPS and FuPS baselines in most scenarios. In scenarios that require diverse behaviours, such as mixed or different goal configurations (Figures 5c and 5b), IPPO-FuPS shows competitive results with two and four agents, closely matching the performance of NoPS and HyperMARL. We hypothesize that this is due to the dense reward structure in the Navigation environment, which contrasts with the sparse reward signals found in environments like Dispersion, where agents are rewarded only when collecting food.\nAs we scale to n = 8 agents, where more complex specialisation patterns are necessary, we observe that HyperMARL outperforms all other methods. This highlights its ability to effectively handle increased agent diversity and coordination challenges, adapting well to larger, more complex environments."}, {"title": "5.4 SMAX", "content": "From the results in Figure 6, we see across simple maps (2s3z), hard maps (3s5z) and SMACv2-style maps (smacv2_10 and smacv2_20), that HyperMARL matches the performance of the baseline FuPS implementation from JaxMARL. These results are consistent with prior findings in SMAC [17, 46], which identified FuPS as the optimal architecture for such environments. Our findings suggest that HyperMARL is similarly effective in settings requiring homogeneous behaviours, large observation spaces, and coordination among many agents, further demonstrating its applicability across diverse multi-agent scenarios."}, {"title": "6 ABLATIONS", "content": "We perform ablation studies to evaluate the impact of some design choices in our MLP hypernetworks. Specifically, we compare our HyperMARL approach with the MAPPO-MLP hypernetwork and MAPPO FuPS feedforward baselines (with one-hot agent IDs) from Section 5.2. The ablations focus on three key variations: (1) HyperMARL (Small), which reduces the hidden layer size of the hypernetworks from 64 to 32; (2) HyperMARL w/ One-Hot IDs, which replaces learned ID embeddings with one-hot encoded IDs; and (3) HyperMARL w/ Hyperfan Init, where the simple initialisation from Section 4.5 is substituted with Hyperfan initialisation [7].\nThe results in Figure 7 show that network capacity affects hypernetwork performance. HyperMARL (Small) converges more slowly but ultimately reaches similar performance to the baseline HyperMARL. A similar pattern is seen with HyperMARL w/ Hyperfan Init, which also converges slower but matches the final performance. In contrast, HyperMARL w/ One-Hot IDs converges faster than the baseline, suggesting that learned ID embeddings require more training time, whereas one-hot embeddings can be effective immediately. However, learned embeddings might offer more robustness across different kinds of tasks.\nThis demonstrates that while design choices influence sample efficiency, they do not degrade final performance, with all variants consistently outperforming the FuPS method with one-hot agent IDs."}, {"title": "7 RELATED WORK", "content": "Hypernetworks in RL and MARL. Hypernetworks have proven effective in meta-learning, multi-task learning, and continual learning in single-agent settings [2, 3, 21, 34]. In MARL, QMIX [31] used hypernetworks conditioned on a global state to generate the"}, {"title": "8 SCALABILITY AND PARAMETER EFFICIENCY", "content": "Hypernetworks generate weights for the target network", "trends": "n*   NoPS and linear hypernetworks: Parameter count grows linearly with the number of agents.\n*   FuPS: More efficient", "hypernetworks": "Scale better with larger populations, since they only require embeddings of fixed size for each new agent.\nTo reduce parameter count, techniques like shared hypernetworks, chunked hypernetworks [8, 40"}]}