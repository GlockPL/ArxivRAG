{"title": "One for All: A General Framework of LLMs-based Multi-Criteria Decision Making on Human Expert Level", "authors": ["Hui Wang", "Fafa Zhang", "Chaoxu Mu"], "abstract": "Multi-Criteria Decision Making (MCDM) is widely applied in various fields, using quantitative and qualitative analyses of multiple levels and attributes to support decision makers in making scientific and rational decisions in complex scenarios. However, traditional MCDM methods face bottlenecks in high-dimensional problems. Given the fact that Large Language Models (LLMs) achieve impressive performance in various complex tasks, but limited work evaluates LLMs in specific MCDM problems with the help of human domain experts, we further explore the capability of LLMs by proposing an LLM-based evaluation framework to automatically deal with general complex MCDM problems. Within the framework, we assess the performance of various typical open-source models, as well as commercial models such as Claude and ChatGPT, on 3 important applications, these models can only achieve around 60% accuracy rate compared to the evaluation ground truth. Upon incorporation of Chain-of-Thought or few-shot prompting, the accuracy rates rise to around 70%, and highly depend on the model. In order to further improve the performance, a LoRA-based fine-tuning technique is employed. The experimental results show that the accuracy rates for different applications improve significantly to around 95%, and the performance difference is trivial between different models, indicating that LoRA-based fine-tuned LLMs exhibit significant and stable advantages in addressing MCDM tasks and can provide human-expert-level solutions to a wide range of MCDM challenges.", "sections": [{"title": "1 Introduction", "content": "Multi-Criteria Decision Making (MCDM) is designed to address complex decision-making problems that involve multiple criteria. Quite a lot of work indicates that MCDM techniques can help decision makers select the optimal option among many alternatives by comprehensively evaluating multiple criteria. Therefore, MCDM becomes an invaluable tool in a wide range of fields. Over the years, numerous techniques have been developed to address MCDM problems. It is particularly effective in applications such as investment analysis, project evaluation, economic efficiency evaluations, and employee performance evaluations. Traditional methods in MCDM include the Analytic Hierarchy Process (AHP), which decomposes a problem into multiple hierarchical levels and constructs a judgment matrix to calculate the weights of elements at each level. Another widely used method is the Technique for Order Preference by Similarity to Ideal Solution (TOPSIS), which evaluates alternatives by calculating their geometric distances from the ideal and negative ideal solutions. In addition, the VIKOR method constructs a loss function to compute a comprehensive score for each alternative, selecting the one closest to the ideal solution. This approach is particularly effective in addressing decision-makers' preferences and trade-offs. The Fuzzy Comprehensive Evaluation (FCE) method, rooted in fuzzy mathematics, is well suited to handle uncertainty and vagueness in decision making. Integrates the results of the evaluation using fuzzy matrices to process and interpret fuzzy information effectively. In addition, several studies have investigated the combination of AHP and FCE for MCDM, utilizing their complementary strengths. These methods have been extensively applied in fields such as project evaluation, investment decision making, resource allocation, and supply chain evaluation. Using these tools, decision makers are empowered to make more informed and rational decisions in complex and uncertain environments.\nWith the rapid development of Large Language Models (LLMs), groundbreaking advancements have been achieved in various technological domains. Models such as ChatGPT and LLaMA, using their advanced natural language processing capabilities, are redefining human-machine interaction, automated content generation, and decision support at multiple levels. In traditional MCDM, decision-making processes heavily rely on human experts or decision makers to assess problems and evaluate solutions. These methods involve quantifying criteria, comparing alternatives, analyzing trade-offs, and ultimately making final selections. Such processes often require extensive data collection, expert judgment, and a comprehensive evaluation of multiple criteria. However, the emergence of LLMs is revolutionizing the operational approach of MCDM methods. With their powerful computational and learning capabilities, LLMs can process and analyze large amounts of textual data in extremely short time frames. This allows them to provide more precise, efficient, and comprehensive information support for decision-making processes.\nLLMs such as LLaMA, Qwen, and ChatGLM demonstrate strong performance in natural language processing tasks. However, directly applying these pre-trained models to specific domains may not yield optimal results. To address this limitation, the performance of LLMs in specialized domains can be significantly improved by fine-tuning. Fine-tuning introduces adaptive adjustments that effectively capture domain-specific information, thereby improving the accuracy and applicability of the models. Recent studies have demonstrated that fine-tuning methods leveraging domain-specific knowledge can substantially enhance model capabilities and performance in targeted applications. However, there is still no single general MCDM framework that can be used as a human expert to deal with different MCDM tasks automatically.\nTherefore, in this paper, we propose a general MCDM framework that can serve as human experts on different MCDM applications. To validate the efficiency of the proposed framework, we tested three applications: supply chain evaluation, air quality evaluation, and customer satisfaction evaluation. The main contributions of this paper are summarized as follows:\n1.  We propose and implement a human-expert-level LLM-based MCDM assessment framework that can evaluate multiple decision-making scenarios more efficiently by utilizing advanced natural language processing techniques to improve the automation and intelligence of handling MCDM tasks.\n2.  The performance of several open-source LLMs, such as Qwen2, ChatGLM4, and Llama3, as well as commercial models such as Claude and ChatGPT, has been systematically evaluated. The experimental results demonstrate that LLMs do not show good performance in processing MCDM scenarios, indicating the requirements of combining other techniques to improve the ability.\n3.  To improve the performance of the models in MCDM tasks, we employ the LoRA fine-tuning technique. With a small amount of data for tuning, the models achieved notable improvements across multiple MCDM datasets, significantly reducing the performance gap between different LLMs."}, {"title": "2 Related Work", "content": "Traditional MCDM methods have been widely applied in areas such as supplier evaluation and environmental impact assessment. These methods typically rely on the judgments of human experts involving the weighing and comparison of multiple criteria. Although effective in practical applications, traditional MCDM methods can be time-consuming and resource-intensive, particularly when dealing with large datasets and complex scenarios, due to their heavy reliance on manual input. In recent years, LLMs have emerged as a promising solution to address these challenges. Several studies have explored the use of LLMs as virtual judges to automate the assessment and decision-making process. For example, Wang and Wu proposed the application of ChatGPT for supplier evaluation, demonstrating that its performance was highly consistent with the results of manual expert evaluations. Similarly, Svoboda et al. introduced a multi-criteria decision analysis framework for cybersecurity that combines AHP with GPT-4. This framework automates decision making while improving both the efficiency and reliability of decisions by using GPT-4 as a virtual expert. Furthermore, Dong et al. introduced linguistic uncertainty estimation to improve evaluation consistency for high-confidence samples. Their findings showed that in certain tasks, the performance of LLM-based evaluations matched or even exceeded that of human experts. These studies highlight a more reliable and scalable approach to leveraging LLMs for personalized evaluations, offering innovative opportunities to automate and enhance decision-making processes. However, a single general MCDM framework should be explored to serve as a general expert in solving different MCDM tasks to reduce the dependence on domain knowledge."}, {"title": "3 METHODOLOGY", "content": "In this study, we compare the traditional AHP-FCE method with LLMs to explore their applications in MCDM problems. Three data sets were used for the experiments: a supplier evaluation dataset, a customer satisfaction assessment dataset, and an air quality evaluation data set. Each data set contains multiple dimensions for evaluation. The primary objective of this study is to evaluate the importance and weights of these dimensions using data sets and to compare the results with those obtained from traditional AHP-FCE methods. The comparison aims to verify whether LLMs can effectively assist or even replace traditional MCDM methods in practice.\nAHP is a decision-making method that decomposes elements related to decision making into multiple levels, such as objectives, criteria, and alternatives, and performs both qualitative and quantitative analysis. Depending on the nature of the problem and the ultimate goal, AHP breaks the problem down into various factors. By analyzing the interrelationships and dependencies among these factors, it aggregates and synthesizes them at different levels to create a multilevel analytical structure. Ultimately, the problem is solved by determining the relative importance weights of the lowest-level elements in relation to the highest-level objectives. FCE is a comprehensive evaluation method based on fuzzy mathematics, using the affiliation theory of fuzzy sets to transform qualitative evaluations into quantitative ones. Conducting an overall evaluation of objects or phenomena constrained by multiple factors, FCE is particularly effective in dealing with fuzzy and difficult-to-quantify problems, providing clear and systematic results, and integrating both qualitative and quantitative factors, expanding the amount of information available, and enhancing the reliability of evaluation outcomes. Consequently, FCE is widely applied in state evaluation. In summary, AHP and FCE each have distinct advantages and are commonly employed in complex decision-making and comprehensive evaluation processes to ensure scientific and rational decision-making. The combination of AHP and FCE synthesizes both qualitative and quantitative analysis, improving the accuracy of weight determination and enhancing the model's adaptability. This integrated approach results in more scientific, comprehensive, and reliable decision-making and evaluation.\nThe AHP-FCE based method consists of the following steps.\n1.  Clarifying the indicators for the object to be evaluated and determining the set of factors for the object to be evaluated.\n2.  Determine the set of weights \\(W_i\\) using the AHP method.\n\\[W_i = (W_1, W_2,..., W_n)  \\tag{1}\\]\n3.  Create a collection of rubrics.\n\\[V_i = \\{U_1, U_2,..., V_w \\} \\tag{2}\\]\n4.  Create a fuzzy composite rating matrix R.\n\\[\nR = \n\\begin{pmatrix}\nr_{11} & r_{12} & \\cdots & r_{1n} \\\\\nr_{21} & r_{22} & \\cdots & r_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nr_{m1} & r_{m2} & \\cdots & r_{mn}\n\\end{pmatrix} \\tag{3}\n\\]"}, {"title": "3.1 Evaluation with LLMs", "content": "As shown in Fig. 1, this paper presents a framework for the integration of LLM and MCDM. The framework employs two approaches during the model preparation phase: one involves using direct prompts through APIs for design, while the other utilizes open-source models that are fine-tuned on small datasets. The language models prepared through these two methods are then evaluated on the MCDM dataset to generate results. In the final step, the effectiveness of language models in automating and improving decision-making accuracy is validated by comparing the evaluation results of the LLMs with those obtained from expert evaluations. We evaluated three datasets using the LLMs, focusing on its application to MCDM tasks. Three different approaches are employed: zero-shot, few-shot, chain of thought. Furthermore, for pre-trained open source models, we employed LoRA fine-tuning to improve the performance.\nZero-shot and few-shot prompts\nZero-shot and few-shot are two common task settings to evaluate a model's ability to reason and make decisions under varying amounts of information. Zero-shot refers to a scenario where the task description is provided directly without any examples, and the model is required to perform reasoning and decision analysis based solely on the given description. In contrast, few-shot involves providing a small number of task-specific examples to help the model understand the task's requirements and decision criteria. These examples serve as guidance, allowing the model to better understand how to perform the task and improving its performance in MCDM tasks. In the context of MCDM, zero-shot and few-shot provide distinct approaches to solving decision-making problems. The zero-shot setting evaluates the model's generalization ability, while the few-shot setting allows the model to better comprehend and address complex decision-making tasks with the help of a small number of examples.\nChain of Thought prompt\nChain-of-Thought (CoT) prompting is a method of guiding a language model to reason step by step as it generates an answer, rather than directly providing the final answer. This approach is particularly suitable for problems requiring multi-step reasoning or complex computation. Using CoT prompting, the model is encouraged to simulate human thought processes, allowing it to break down complex problems and generate more accurate and logical answers.\nFine-tuned Model with LORA\nLORA is an efficient fine-tuning method for large-scale pre-trained language models. As model sizes grow, traditional full-model fine-tuning becomes increasingly impractical due to the immense number of parameters that need adjustment, leading to extremely high computational and storage costs. LoRA addresses this challenge by enabling efficient model adaptation without the need to retrain all model parameters, significantly reducing resource requirements while maintaining performance."}, {"title": "4 EXPERIMENTS", "content": "To fully validate our supplier evaluation methodology, we employed several major LLMs. For commercial APIs, we selected two representative models: ChatGPT-3.5-turbo and Claude-3-sonnet-20240229. For open source models, we used LLaMA3-8B, Qwen2-7B, and ChatGLM4-9B. In addition, we developed LoRA-tuned versions of these open-source models to explore their optimization potential for different tasks. To systematically evaluate the performance of these models, we adopted several standard evaluation metrics, including precision, recall, and the F1 score. By comprehensively analyzing these metrics, we can thoroughly assess the performance differences between the LLM-based MCDM method and traditional expert evaluations. As shown in Table 1, the AHP-FCE method is used to determine the weights of each dimension and criterion within each dimension. For supplier evaluation, as an example, the results are categorized into four criteria: Good, Fair, Average, and Poor. For the other two tested applications, the datasets include the final expert evaluation, which can be used as the groundtruth for comparison."}, {"title": "4.1 Datasets", "content": "First, in this part, we introduce the tested datasets.\nData Co-Supply Chain Dataset: The Data Co-Supply Chain Dataset provides multidimensional data, including suppliers' delivery timeliness, product quality, and supply capability. This data set enables the analysis of supplier reliability and performance, offering decision support for supply chain management. It ensures the selection of suppliers that consistently deliver high-quality products and good customer service, ultimately optimizing supply chain efficiency.\nCustomer Feedback and Satisfaction Dataset: The Customer Feedback and Satisfaction Dataset is a synthetic data set designed to analyze and predict customer satisfaction, providing data scientists with insights into customer behavior and data-driven support to formulate business strategies.\nAir Quality and Pollution Evaluation: Air Quality and Pollution Evaluation involves a comprehensive analysis to reduce sources of pollution, improve air quality and protect public health and environmental sustainability. This is achieved by monitoring and analyzing the distribution of various air pollutants, population densities, and industrial areas."}, {"title": "4.2 \u0391\u03a1\u0399 Models Evaluation", "content": "As shown in Table 2, the performance of different API models on Supplier Evaluation, Customer Satisfaction Evaluation, and Air Quality Evaluation was compared using evaluation metrics such as Precision, Recall, and F1 score. Few-shot Claude-3-sonnet achieved the highest scores in all three metrics in both the Supplier Evaluation and Customer Satisfaction Evaluation tasks. In the Air Quality Evaluation task, the few-shot Claude-3-sonnet + CoT performed the best on all three metrics."}, {"title": "4.3 Open-Source Models Evaluation", "content": "The superior performance of Claude-3-sonnet + CoT can be attributed to the integration of the CoT inference enhancement method, which enhances its reasoning capabilities. However, the primary factor driving the model's exceptional performance lies in the few-shot learning approach. Fine-tuning the model with a small number of examples significantly improved its adaptability and accuracy across all tasks. In particular, Claude-3-sonnet with few shots achieved the highest metrics in supplier evaluation and customer satisfaction assessment, showcasing its ability to effectively learn task-specific patterns even with limited training data. When combined with CoT, the few-shot Claude-3-sonnet + CoT demonstrated further improvements, particularly in the air quality evaluation task, where it achieved the highest scores in precision, recall and F1 score. Although CoT provided additional reasoning support, the foundation of this performance was established through the fine-tuning process. This highlights that few-shot learning played a critical role in enabling the model to process complex decision-making scenarios with improved accuracy and efficiency.\nHowever, we see that the F1 scores are around 55% for pure APT models, and the F1 scores rise to around 75% while employing few-shot and CoT prompts. Indicting that the performance of basic LLMs in solving MCDM is not as good as experts, and different prompt methods on different models achieve significantly different performance.\nIn Fig. 2, we analyze three open-source models LLaMA3-8B, Qwen2-7B, and ChatGLM4-9B using the same prompt methods applied to API models. As shown in Table 2, the size of the model does have a significant impact on its performance. Both API models and open-source models experience significant improvements using CoT and few-shot learning. Specifically, air quality evaluation performance increased from 0.228 to 0.704, highlighting the potential of models to process multidimensional data more effectively when coupled with the right learning strategies. We see both API and open-source models exhibit the same general performance trend with the integration of CoT and few-shot, the outcomes vary across different tasks. This consistency across model types offers robust support and a valuable reference for future applications in other MCDM tasks."}, {"title": "5 Conclusions", "content": "This paper proposed a human-expert-level framework to deal with MCDM tasks through proper prompt design and fine-tuning with a small amount of data. The performance of LLMs has improved significantly from around 55% to 75% and finally stabilized around 95%, achieving impressive evaluation performance. These findings demonstrate that LLMs can not only address the limitations of traditional MCDM methods in handling complex multidimensional tasks, but also highlight its great potential and advantages in decision analysis. Traditional MCDM methods, such as AHP and FCE, although widely used, are heavily based on manually set weights and expert experience. In this paper, three pre-trained models are optimized using fine-tuning techniques, and the results show performance comparable to human expert evaluators. This paper not only provides strong empirical evidence for the application of LLMs in MCDM but also offers valuable insights for the development of intelligent decision-making systems with LLMs.\nFor future work, applying the proposed framework to more real applications is promising. In addition, it is also interesting to generate and tag fine-tuning MCDM data on LLMs, since although fine-tuning does not need much data, for some applications, collecting and tagging real data is costly."}, {"title": "6 Appendix", "content": "Figure 5 demonstrates examples of air quality evaluation from three datasets, utilizing Zero-shot, Few-shot, and Chain of Thought methods. Each example evaluates air quality based on input environmental data and assigns a final air quality rating (Good, Moderate, Poor, or Hazardous). The Zero-shot method directly outputs the rating based on the input data, the Few-shot method references similar examples before evaluation, and the Chain of Thought method analyzes the impact of each environmental factor step-by-step to derive the final rating. These examples illustrate the application and outcomes of different methods in air quality evaluation.\nIn data analysis in the field of supply chain management, two different methods of prompts are used for supplier evaluation. The first method is prompts with weights, where different weights are assigned based on dimensions and specific criteria to highlight the importance of certain factors. This approach allows optimizing the model's response to a specific problem by precisely adjusting the weights. Another approach is to use generic prompts without templates and to look at supplier performance holistically without preset weights. This approach shows its strengths when dealing with complex issues that require a broad perspective and flexibility in response. The results of validating the model with prompts that require refinement are shown in Fig. 4 to demonstrate whether the results obtained are better than the performance without weights."}]}