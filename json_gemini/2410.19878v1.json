{"title": "Parameter-Efficient Fine-Tuning in Large Models: A Survey of Methodologies", "authors": ["Luping Wang", "Sheng Chen", "Linnan Jiang", "Shu Pan", "Runze Cai", "Sen Yang", "Fei Yang"], "abstract": "The large models, as predicted by scaling raw forecasts, have made groundbreaking progress in many fields, particularly in natural language generation tasks, where they have approached or even surpassed human levels. However, the unprecedented scale of their parameters brings significant computational and storage costs. These large models require substantial computational resources and GPU memory to operate. When adapting large models to specific downstream tasks, their massive parameter scale poses a significant challenge in fine-tuning on hardware platforms with limited computational power and GPU memory. To address this issue, Parameter-Efficient Fine-Tuning (PEFT) offers a practical solution by efficiently adjusting the parameters of large pre-trained models to suit various downstream tasks. Specifically, PEFT adjusts the parameters of pre-trained large models to adapt to specific tasks or domains, minimizing the introduction of additional parameters and the computational resources required. This review mainly introduces the preliminary knowledge of PEFT, the core ideas and principles of various PEFT algorithms, the applications of PEFT, and potential future research directions. By reading this review, we believe that interested parties can quickly grasp the PEFT methodology, thereby accelerating its development and innovation.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, large pre-trained models, commonly re- ferred to as \"large models\", have emerged as a significant advancement in the field of artificial intelligence. Due to their outstanding performance and versatility in various application contexts, these models have attracted plenty of attention and provoked much discussion. These models have impressive computing capabilities and extensive data resources, allowing them to excel in tackling intricate jobs. Within the field of natural language processing (NLP), notable interest is given to Large Language Models (LLMs). These models demonstrate remarkable ingenuity in text generation [1][2], machine translation [3][4], personalized chatbots [5][6][7], text summarization [8], sentiment analysis [9], and question- answering systems [10].\nNevertheless, the development of large models faces sig- nificant challenges and controversies. These models require substantial computational resources and data support, which can potentially jeopardize the environment and compromise privacy protection [11]. Despite their impressive performance in specific tasks, these models still have limitations and error rates that need continuous optimization and improvement [12][13][14]. When directly using large models for specific tasks, their performance often falls below desired levels. Consequently, fine-tuning large models has become a crucial method for enhancing model performance.\nPEFT is a transfer learning method specifically developed to adapt the parameters of the large pre-trained models to suit new tasks and scenarios. This approach involves dynamically adjusting the model to enhance its effectiveness in performing certain tasks, taking into account the distinct features and requirements of the target task. The fine-tuning process typically entails improving the model architecture [15], optimizing parameters [16][17], and adapting learning strategies [18], among other considerations, to achieve better performance in new tasks. As the field of deep learning continues to evolve, techniques for optimizing and fine-tuning large models have also made significant advancements. Notable PEFT ap- proaches include LoRA [19], adapter tuning [20], prefix-tuning [16], prompt-tuning [17], P-tuning [21], BitFit [22] and others. However, despite the significant achievements of large model fine-tuning techniques across several fields, there are always challenges and difficulties that need to be resolved. Overfitting mitigation, optimizing fine-tuning efficiency, and striking a learning balance between pre-training and fine-tuning tasks are a few examples of issues that need more investigation.\nIn recent years, hundreds of articles on PEFT have been published, with some studies offering informative overviews of the most prevalent approaches. Below is a comparative analysis of these studies.\nDing, Ning, et al. [23] introduce a theoretical abstraction for Delta Tuning, which is analyzed from the viewpoints of optimization and optimum control. This abstraction offers a unified approach to describe the current parameter-efficient fine-tuning methods which provides a distinct perspective for future investigations. Nonetheless, while the study pre- dominantly concentrates on NLP applications, the general- izability and efficacy of these methods in diverse domains merit additional investigation. Lialin, V, et al. [24] provide a comprehensive analysis and classification that covers a broad range of methods and compares approximately 30 approaches across five dimensions: storage efficiency, memory efficiency, computational efficiency, accuracy, and inference overhead. However, while the article primarily focuses on detailed meth- ods with practical efficiency for fine-tuning multibillion-scale"}, {"title": "II. PRELIMINARY", "content": "A. Large Language Models\n1) Background: LLMs refer to neural language models with a large number of parameters, typically over billions of parameters. These models are built on the transformer architecture [28] and are pre-trained on vast text corpora [29]. Prior to the emergence of LLMs, the advent of transformers revolutionized the development approach for neural language models, shifting from end-to-end training to a pre-train then fine-tune paradigm. Under the pre-train fine-tune paradigm, pre-trained models can be repeatedly utilized, significantly enhancing the scalability of neural language models. Con- sequently, the scale of parameters is continuously growing larger. For instance, OpenAI's GPT-1 possessed 120 million parameters, while GPT-2 boasted 1.5 billion parameters. This number surged to 175 billion for GPT-3 and soared to 1.76 trillion for the latest GPT-4 [30]. With the evident increase in neural network parameter scales, the concept of large language models has been developed.\n2) Emergent abilities: Research suggests that the rapid expansion of the parameter scale may lead to emergent abilities [31], which are formally defined as abilities that are not present in small models but arise in large models, constituting one of the most prominent characteristics distinguishing LLM from previous PLM. To conclude, emerging abilities can be categorized into threefolds.\nIn-context learning. In-context learning [31][32], known as ICL defined in GPT-3 [33], illustrates the ability of LLMs to acquire new task capabilities based on a small set of examples in context. Importantly, this process does not require additional training or gradient updates, indicating that the LLM is capable of completing new tasks with only prompts. In addition, [31] reveals that ICL is associated with both the LLM and the downstream task.\nInstruction following. Natural language descriptions, known as instructions, are essential for fine-tuning LLMs. Instruction tuning organizes fine-tuning datasets in the format of natural language descriptions (instructions). Research [34] shows that with instruction tuning, LLMs are enabled to follow task instructions for new tasks without using explicit examples, demonstrating better generalization capability across inputs of various tasks. [35] discovered that to achieve evident efficacy, instruction tuning should be conducted on a relatively large- scale LLM, e.g., over 60B parameters.\nStep-by-step reasoning. Constrained by parameter size, PLMs often struggle to solve tasks requiring intricate reason- ing. In contrast, scaling up in parameter size equips language models with the Chain-of-Thought (CoT) [31]. CoT enhances language models' performance on tasks involving logic, calcu- lation, and decision making by structuring the input prompt to human reasoning. Thanks to CoT, LLMs are enabled to tackle tasks that demand intermediate reasoning steps to derive the final answer, akin to constructing a step-by-step prompt that invokes a thinking and inference process within the model.\n3) Scaling Laws of LLMs: Thanks to the exceptional scal- ability of the transformer architecture [28], language models also exhibit high scalability. Scaling laws for LLMs describe how the model grows and performs as the volume of training data increases.\nIn general, a scaling law includes four parameters, which also characterize a language model: denoted as: (1) Parameters count $N$. The number of parameters of an LLM is often associated with the number of transformer layers and the hidden size, except for some MoE LLMs. (2) Data size $D$. In LLM, this refers to the number of tokens for training. (3) Computation cost $C$. This is typically measured in terms of time and computational resources. (4) Loss $L$. The performance of training is usually evaluated by the training loss. There are two representative scaling laws for transformer LLMs.\nThe Kaplan scaling law Proposed by Kaplan [36], the law examines the statistical relations between the parameters $C$, $N$, $D$ and $L$ over a wide range of values, models and data tokens. The relationships can be expressed through the following equations:\n$L(N) = (\\frac{N_c}{N})^{\\alpha_N}, \\alpha_N \\sim 0.076, N_c \\sim 8.8 \\times 10^{13}$ (1)\n$L(D) = (\\frac{D_c}{D})^{\\alpha_D}, \\alpha_D \\sim 0.095, D_c \\sim 5.4 \\times 10^{13}$ (2)\n$L(C) = (\\frac{C_c}{C})^{\\alpha_C}, \\alpha_C \\sim 0.050, C_c \\sim 3.1 \\times 10^{8},$ (3)\nwhere the loss $L$ is influenced by parameters $N$, $D$, and $C$, shedding light on decision-making processes when computa- tional resources are limited.\nThe Chinchilla scaling law Proposed by DeepMind [37], the law provides guidelines for compute-optimal training of LLMs, specifically when computational resources are limited. Through rigorous experiments spanning a wide range of model sizes from 70M to 16B and dataset sizes from 5B to 500B tokens, they derived a scaling law with different coefficients compared to Kaplan's, as shown below:\n$L(N, D) = E + \\frac{A}{N^{\\alpha}} + \\frac{B}{D^{\\beta}}$, (4)\nwhere $E$ denotes the loss of an ideal generative process on the test data. Furthermore, claimed by the research, the constants in this formula are $\\alpha = 0.34$, $\\beta = 0.28$, $A = 406.4$, $B = 410.7$, $L_o = 1.69$. Moreover, there is a general constraint that model the relationship between $C$ and $(N, D)$: $C = 6ND$, which means that it costs six FLOPs per parameter to train one token. Then it can be computed that an optimal model size and data size selection can be denoted as:\n$N_{opt} = C^{0.6} \\cdot 10^{0.45}$ (5)\n$D_{opt} = C^{0.3} \\cdot 10^{0.55}$ (6)\n$L_{opt} = 1070 \\cdot C^{-0.154} + 1.7$ . (7)\nB. Prevalent LLMs\n1) The GPT Family: Generative Pre-trained Transformers (GPT) constitute a series of decoder-only Transformer-based"}, {"title": "C. Multimodal Large Language Models", "content": "1) MLLM: Background: Multimodal Large Language Model (MLLM), is an extension of LLM which adopts multi- modal information as input such as text, sound, video, etc, to enable multiple dimensional reasoning and text generation.\nBefore the emergence of MLLM, significant research ef- forts were dedicated to multi-modality. These efforts can generally be categorized into representative and generative paradigms. An exemplary work in the representative paradigm is CLIP [48], which serves as a foundational contribution. This process yields a visual encoder [49][50] and a text encoder, effectively establishing a bridge for downstream multimodal tasks. In contrast, generative frameworks [51][52] approach multimodal tasks by transforming them into sequence-to- sequence tasks. MLLM distinguishes itself from previous multimodal research in two key aspects. (1) Composition: MLLM is comprised of at least one LLM with billion-scale parameters. (2) Training techniques: MLLM introduces and incorporates novel training techniques derived from LLM to enhance multimodal performance.\n2) MLLM: Architecture: Figure 1 illustrates the mainstream architecture of multimodal large language models, typically composed of three modules: a multimodal encoder, an LLM, and a modal connector.\nMultimodal Encoder. This module incorporates non-text inputs, such as images or audio, and encoding the raw infor- mation into a more compact representation. It is noteworthy that the encoder is aligned with one or several encoders in advance to ensure associated meanings are preserved. It is more advisable to directly adopt and fine-tune a pre-trained"}, {"title": "D. Instruction Tuning", "content": "Instruction tuning in large language models has undergone significant development, evolving from initial efforts in multi- task fine-tuning without explicit instruction prompts to so- phisticated techniques leveraging diverse tasks and templates. Early work focused on improving downstream task perfor- mance through large-scale multi-task fine-tuning [54], [55], [56], [57], while other efforts [58], [59], [60] converted a range of NLP tasks into a single generative question answering format using prompt instructions. The instruction tuning began in 2020 with the release of several task collections, including Natural Instructions [61], Flan 2021 [62], and PromptSource [63]. These collections aggregated large NLP datasets and provided templatized instructions for zero-shot prompting, enabling models to generalize to unseen instructions. MetaICL [64] emphasized few-shot prompting without explicit instruc- tions, using input-output examples to teach tasks in-context. Research confirmed the benefits of task and template diversity, with some studies highlighting the advantages of inverting inputs and outputs to create new tasks [64]. The subsequent phase saw the expansion and combination of resources, with collections like SuperNatural Instructions [65] and OPT-IML [66] integrating more datasets and tasks. This phase also introduced multilingual instruction tuning, as seen in xP3 [67], and incorporated Chain-of-Thought training prompts in Flan 2022 [68]. These expanded collections included most tasks from previous resources, establishing a strong foundation for future open-source work. Current and future research is exploring new directions, such as synthetic data generation for creative and open-ended dialogue tasks [69], [70], [71], [72] and integrating human feedback on model responses [34], [73], [74], [41], [75]. These approaches are viewed as complementary to foundational instruction tuning methods, driving further advancements in the field.\nA recent advance in instruction tuning is the potential to complement or replace few-shot in-context learning with parameter-efficient fine-tuning. Compared to instruction tun- ing, parameter-efficient fine-tuning can achieve performance comparable to full parameter tuning while being computa- tionally more cost-effective. Previous studies [76], [62], [77],"}, {"title": "E. Alignment Tuning and RLHF", "content": "Despite the emergent abilities brought by increasing param- eters of language models, hallucination exhibit to become a challenge for LLMs to produce satisfying response. To address this issue, alignment tuning is applied to align the models with specific human preferences. There are three primary targets for alignment tuning, respectively presented as helpfulness, honesty and harmlessness. From the targets' names, it can be concluded that the alignment criteria are closely associated with human's recognition, making it difficult to formulate them as optimization objectives for LLMs. Therefore, human feedback is widely adopted as an assistance to reinforce LLMs' performance.\nRLHF [79], [80] emerged as a method to fine-tune language models using human feedback, aiming to align the LLMs with human preferences, and consequently enhancing alignment performance.\nGenerally, an RLHF system[34] comprises three key com- ponents: a pre-trained language model, a reward model learned from human feedback, and a reinforcement learning algorithm to train the language model. Figure 2 shows the three key steps."}, {"title": "F. Dataset for LLM training and tuning", "content": "A critical component of the development and deployment of LLMs is the datasets used at various stages of their lifecycle, which significantly influence their capabilities and performance. In this section, we delve into the datasets that are instrumental in the pre-training, tuning, and other auxiliary phases of LLMs. The pre-training phase is where an LLM absorbs the foundational knowledge from a diverse array of textual data. This stage is pivotal, as it sets the stage for the model's general understanding of language. The datasets used in pre-training are vast and varied, encompassing everything from the sprawling expanse of the internet to curated col- lections of literature and encyclopedias. Tuning is a process where the LLM is fine-tuned on specific tasks or domains. This phase refines the model's abilities, enabling it to perform with greater precision and relevance in targeted applications. Tuning datasets are often more specialized and may include annotated examples that guide the model towards desired behaviors and outputs. Lastly, other datasets serve a multitude of purposes throughout the LLM's development. These may include evaluation datasets that help assess the model's perfor- mance, auxiliary datasets that broaden its knowledge base, or datasets that are used to imbue the model with specific skills or competencies.\nCommonly Used Datasets for Pre-training. In the realm of LLMs, the pre-training phase is instrumental in establishing a robust foundation upon which the model's linguistic prowess is built. LLMs, with their exponentially larger parameter counts, necessitate an extensive and diverse corpus of training data that spans a multitude of topics and linguistic expressions. This data not only serves as the bedrock for the model's comprehension of language but also influences its ability to generalize and adapt to new contexts and tasks. To meet these requirements, a variety of comprehensive and accessible datasets have been curated and made available for the research community. In this section, we embark on an overview of the datasets that are pivotal in the pre-training of LLMs. We categorize these datasets based on the type of content they provide, which can be broadly divided into seven distinct groups: Webpages, Books, Code, Social Media, Wikipedia, and a diverse array of other sources. Each of these categories contributes unique elements to the model's knowledge base, ensuring a well-rounded understanding of human language and its myriad uses.\nCommonly Used Datasets for tuning. The fine-tuning phase of LLMs is a critical juncture where models are honed to perform with greater precision on specific tasks or domains. This process leverages a variety of datasets designed to target the skills and knowledge that the model needs to develop. In this table, we show some of the key datasets used for fine-tuning LLMs, focusing on their characteristics, the tasks they support, and their impact on the models' performance.\nOther Datasets. Beyond the foundational pretraining and targeted tuning phases, the realm of LLMs is enriched by a multitude of other datasets. These datasets, while not central to the initial training or fine-tuning processes, are indispens- able for expanding the model's expertise and enhancing its performance across a spectrum of nuanced language tasks. They embody the diversity of linguistic challenges and provide a testing ground for models to demonstrate adaptability and depth in understanding.In the following sections, we will explore these other datasets in detail, highlighting their unique characteristics, the specific tasks they target, and the value they bring to the comprehensive development of LLMs."}, {"title": "III. PEFT TAXONOMY", "content": "PEFT techniques are typically divided into three primary categories: Additive PEFT (III-A), which introduces addi-"}, {"title": "A. Additive PEFT", "content": "Full-parameter fine-tuning is computationally expensive and could adversely affect the model's capacity to generalize. To address this, additive PEFT methods add a small set of train- able parameters to a pre-trained model, carefully integrated into its architecture. When fine-tuning for particular down- stream tasks, it is only these extra components or parameters are adjusted, keeping the original pre-trained model parameters unchanged. This approach significantly reduces the need for storage, memory, and computation. Based on where and how these additional trainable parameters are incorporated into the model's architecture, there are primarily three types of additive PEFT techniques: Adapter, Soft Prompt, and Scale and Shift. We will delve into some of the principal studies on these techniques.\n1) Adapter: Adapter methods enable parameter-efficient fine-tuning by inserting small adapter layers into pre-trained models, which learn task-specific transformations while keep- ing the base model frozen. These adapters, typically consisting of a down-projection, a non-linear activation function and an up-projection layer (the standard adapter shown in Figure 5 (a)), adapt the representations to downstream tasks with min- imal overhead. For example, in Sequential Adapter [15], two serial adapters are inserted after the attention layer and the feed-forward layer in transformer blocks. Residual Adapter [112] dynamically adapts a pre-trained language model, such as GPT-2, to various downstream tasks using low- rank residual adapters and task embeddings, with the adapter module formulated as:\n$Adapter(H_i) = (ReLU(LN(H_i)))W_D W_U + H_i$, (8)\nwhere $H_i$ is the hidden representation of the $i^{th}$ layer, $W_U$ and $W_D$ are the adapter parameters, and LN denotes layer nor- malization. AdapterDrop [113] dynamically removes adapters"}, {"title": "B. Reparameterized PEFT", "content": "Reparameterization is a technique for improving the training efficiency and performance of a model by transforming its pa- rameters. In the context of PEFT, the transformation involves low-rank parameterization, which entails constructing a low- rank learnable parameter matrix to adapt to specific down- stream tasks. During training, only the low-rank parameter matrix is fine-tuned, and at inference time, the learned matrix is combined with the pre-trained parameters to ensure that inference speed is not affected.\n1) Low-rank Decomposition: LoRA (Low-rank Adapta- tion) [19] introduces low-rank trainable matrices $A \\in R^{d \\times r}$ and $B \\in R^{r \\times k}$ to update the pre-trained weight matrix $W_0 \\in R^{d \\times k}$ via $\\Delta W = BA$, where $W = W_0 + \\Delta W$ is used for inference without additional latency. KronA [140] is a Kronecker product-based adapter module for efficient fine-tuning of Transformer-based pre-trained language models (PLMs). The tuned weight matrix $W_{tuned}$ is computed as the original PLM weight matrix $W$ plus a scaled Kronecker product of two learnable matrices $A_k$ and $B_k$:\n$W_{tuned} = W + s[A_k \\otimes B_k]$, (18)\nwhere $s$ is a scaling factor, and $\\otimes$ denotes the Kronecker product operator.\n2) LORA Derivatives:"}, {"title": "C. Selective PEFT", "content": "Contrary to Additive PEFT, Selective PEFT selects a very small subset of the pre-trained model's parameters for fine- tuning to adapt to specific downstream tasks through a param- eter masking matrix. Depending on the way the parameters are masked, Selective PEFT can be divided into unstructured masking and structured masking."}, {"title": "D. Hybrid PEFT", "content": "Due to the significant performance differences of different types of PEFT methods on various tasks, many studies aim to enhance model performance by combining the advantages of different types of PEFT methods. These research efforts are summarized as Hybrid PEFT methods.\nUniPELT [167] operates on the principle of dynamically ac- tivating the most suitable parameter-efficient language model tuning (PELT) submodules for a given task through a gating mechanism, which is mathematically represented as $h' = G_A h_A + h_r$, where $h'$ is the final output, $h_A$ is the output of the adapter submodule, $h_r$ is the direct input to the adapter, and $G_A$ is the gating function that modulates the contribution of the adapter submodule based on the specific data and task setup. S4 [168] discovers design patterns by grouping layers in a spindle pattern, uniformly allocating trainable parameters, tuning all groups, and assigning tailored strategies to different groups, consistently outperforming existing fine- tuning strategies across various NLP tasks and models. MAM Adapter [115] is a unified framework for parameter-efficient transfer learning methods by reframing them as modifications to specific hidden states in pretrained models, which can be mathematically represented as $h \\leftarrow (1 - \\lambda(x))h + \\lambda(x)\\Delta h$, where $h$ is the original hidden representation, $\\lambda(x)$ is a gating scalar, and $\\Delta h$ is the modification vector computed by a function $f$ applied to the input $x$. LLM-Adapters [169] discusses the use of different adapters such as Series Adapters, Parallel Adapters, and LoRA (Low-Rank Adaptation), which are incorporated into the model's architecture at optimal locations. NOAH [170] employs neural architecture search to automatically design optimal \"prompt modules\" for large vision models, tailored to each downstream dataset, enhanc- ing transfer learning, few-shot learning, and domain gen- eralization. AUTOPEFT [171] automates the configuration selection for PEFT of large pre-trained language models. It employs a multi-objective Bayesian optimization approach to discover a set of Pareto-optimal configurations that bal- ance task performance with parameter efficiency, significantly outperforming existing PEFT methods with minimal training costs. S\u00b3Delta-M [172] automatically searches for an optimal trainable structure within pre-trained models by using a unified framework of various Delta Tuning methods. It employs bi- level optimization and a shifted global sigmoid function to control sparsity, achieving high performance with minimal"}, {"title": "E. Quantization PEFT", "content": "Quantization is another widely used and studied technique aimed at improving computational efficiency and reducing memory usage. We summarize the PEFT methods that use and research quantization technology, as Quantization PEFT.\nBI-Adapter [174] introduces a novel method for low- precision adapter training in vision models. It utilizes the observation that adapter parameters converge to flat minima, suggesting robustness to precision reduction. The method employs a quantization-aware training strategy, minimizing the quantization error by clustering weight parameters into Gaus- sian distributions. Specifically, weights $w$ are standardized $w' = \\frac{w-\\mu}{\\sigma}$, quantized, and then de-standardized to backpropa- gate gradients effectively. This approach significantly reduces model size with minimal impact on performance, addressing storage and transmission inefficiencies in multi-task learning.\nPEQA [175] involves a two-step process: first, decomposing the parameter matrix of each fully-connected layer into a low- bit integer matrix and quantization scales, and second, fine- tuning only the quantization scale while keeping the integer matrix frozen, which can be mathematically represented as:\n$W = (S_0 + \\Delta S) \\cdot (clamp(\\frac{W_0}{S_0}, -2^{b-1}, 2^{b-1}-1))$, (29)\nwhere the notation $A \\odot B$ denotes the element-wise product of matrices $A$ and $B$. The symbol $[.]$ represents the rounding function, which rounds its argument to the nearest integer. The function $clamp(., a, b)$ signifies the clamping operation that constrains its input within the range $[a, b]$. Here, $W_0$ denotes the original weight matrix, $s_0$ represents the initial scale factor, and $z_0$ is the zero-point value. The variable $\\Delta S \\in R^{n \\times 1}$ signifies the gradient update of $s_0$, obtained through adaptation to a downstream task, and $b$ indicates the bit-width. QLORA [176], a quantized version of LoRA, utilizes 4-bit NormalFloat (NF4) precision for quantizing pretrained models, enhanced by double quantization and a paged optimizer to prevent the gradient checkpointing mem- ory spikes. The NF4 is an information theoretically optimal quantization data type for normally distributed data, delivering enhanced empirical performance over 4-bit Integer and Float representations. While QLoRA converts the FP16 pretrained weights $W$ to the NF4 precision to enable LLM finetuning on a reduced number of GPUs, the auxiliary weights of the LoRA matrix re-quantize the final weights back to FP16 post- finetuning. Therefore, QA-LORA (Quantization-Aware Low- Rank Adaptation) [178] addresses the imbalance between quantization and adaptation by employing group-wise oper- ations, which increase the flexibility of low-bit quantization while reducing that of the adaptation process. The algorithm is straightforward to implement and provides two key ben- efits: during fine-tuning, LLM weights are quantized (e.g.,"}, {"title": "F. Multi-task PEFT", "content": "The previously introduced PEFT methods were mainly designed for single downstream task. This section focuses on PEFT for multi-task learning.\n1) Adapter-based: AdapterFusion [182] employs a two- stage approach to transfer learning, where it first extracts knowledge into task-specific adapters and then composes this knowledge in a separate step to exploit multi-task represen- tations without destructive interference. AdaMix [183] inte- grates multiple adaptation modules within each Transformer layer of a pre-trained language model, enabling efficient tuning with a mixture of these modules while maintaining most of the model's weights unaltered. PHA [184] leverages an instance-dense retriever and a prototypical hypernetwork to efficiently generate task-specific adapter layers by retrieving prototype embeddings and feeding them into the hypernet- work, enabling sample-efficient multi-task learning and new task generalization. AdapterSoup [185] improves the gen- eralization of pretrained language models to new domains by averaging the weights of adapters trained on different domains, without the need for additional training or increasing inference cost. MerA [186] efficiently incorporates pretrained adapters into a single model through model fusion, align- ing the parameters via optimal transport based on weights and activations to enhance performance in few-shot learning scenarios. Hyperformer [187] integrates hypernetwork-based adapter layers into a transformer model, enabling the model to share knowledge across tasks while adapting to each indi- vidual task through task-specific adapters generated by shared hypernetworks.\n2) Soft Prompt-based: SPoT (Soft Prompt Transfer) [188] leverages soft prompts to adapt pre-trained language models efficiently. It first trains a soft prompt $p$ on one or more source tasks, where $p \\in R^{d}$ represents a sequence of continuous vectors with dimensionality $d$. This learned prompt is then used to initialize the prompt for a target task, facilitating transfer learning. SPoT significantly improves upon the per- formance of prompt tuning and matches or outperforms full model fine-tuning while using significantly fewer task-specific parameters. ATTEMPT (ATTEntional Mixtures of Prompt Tuning) [189] leverages pre-trained soft prompts $P_1, ..., P_t$ for different high-resource tasks and a new target prompt $P_{target}$. An attention module $G$ computes attention scores be- t+1 tween input $X$ and each prompt token to produce an instance- wise prompt $P_{instance} = \\sum_{j=1}^{t+1} a_j P_j$, where $a_j$ represents the attention weight for prompt $P_j$. Only $P_{target}$ and $G$ are updated during training, keeping the original language model frozen. This approach is parameter-efficient and flexible for multi-task learning. MPT (Multitask Prompt Tuning) [190] is a method for efficient transfer learning of large language models across multiple downstream tasks. The core idea is to distill knowl- edge from multiple task-specific source prompts into a single transferable prompt, $P^*$, which is then adapted to each target task with minimal additional parameters. The prompt for each source task is decomposed into a shared matrix $P^*$ and a low- rank task-specific matrix $W_k = u_kv$, where $u_k$ and $v_k$ are task-specific vectors. This decomposition is learned through a knowledge distillation process that minimizes the KL- divergence between teacher and student prompts, $L_{Logits}$, and an additional mean squared loss on the hidden states, $L_{Hidden}$. The total training loss is $L_{Total} = L_{PLM} + \\lambda(L_{Logits} + L_{Hidden})$, where $L_{PLM}$ is the task loss and $\\lambda$ balances the distillation impact. The innovation lies in leveraging cross-task knowledge within a parameter-efficient framework, which outperforms full finetuning with far fewer task-specific parameters. IPT (Intrinsic Prompt Tuning) [191] is a method to reparameterize the adaptation of pre-trained language models to various tasks within a low-dimensional intrinsic task subspace. The key idea is to decompose the soft prompts $P$ for multiple NLP tasks"}, {"title": "IV. APPLICATIONS OF PEFT", "content": "A. PEFT in Vision Models\nOver the past decade", "Classification": "VP [197", "198": "adapts pre-trained vision Transformers for downstream tasks by introducing task-specific", "170": "automatically searches for the optimal design of prompt modules for large vision models through Neural Architecture Search (NAS). NOAH encompasses three prompt modules: Adapter, LoRA, and VPT, each inserted into Transformer blocks. The search space includes parameters like embedding dimensions $D = \\{5, 10, 50, 100\\}$ and depths $L = \\{3, 6, 9, 12\\}$, determining the range of applications. An AutoFormer-based one-shot NAS algorithm is employed to select the best configuration for each downstream dataset. Convpass [199"}]}