{"title": "The Vision of Autonomic Computing: Can LLMs Make It a Reality?", "authors": ["Zhiyang Zhang", "Fangkai Yang", "Xiaoting Qin", "Jue Zhang", "Qingwei Lin", "Gong Cheng", "Dongmei Zhang", "Saravan Rajmohan", "Qi Zhang"], "abstract": "The Vision of Autonomic Computing (ACV), proposed over two decades ago, envisions computing systems that self-manage akin to biological organisms, adapting seamlessly to changing environments. Despite decades of research, achieving ACV remains challenging due to the dynamic and complex nature of modern computing systems. Recent advancements in Large Language Models (LLMs) offer promising solutions to these challenges by leveraging their extensive knowledge, language understanding, and task automation capabilities. This paper explores the feasibility of realizing ACV through an LLM-based multi-agent framework for microservice management. We introduce a five-level taxonomy for autonomous service maintenance and present an online evaluation benchmark based on the Sock Shop microservice demo project to assess our framework's performance. Our findings demonstrate significant progress towards achieving Level 3 autonomy, highlighting the effectiveness of LLMs in detecting and resolving issues within microservice architectures. This study contributes to advancing autonomic computing by pioneering the integration of LLMs into microservice management frameworks, paving the way for more adaptive and self-managing computing systems. The code will be made available at https://aka.ms/ACV-LLM.", "sections": [{"title": "INTRODUCTION", "content": "As we enter the modern era, computing infrastructure is becoming increasingly distributed and large-scale, presenting significant challenges for human management. This complexity underscores the necessity for developing systems capable of self-management. This vision, reminiscent of the Vision of Autonomic Computing (ACV) [27] proposed two decades ago, envisions computing systems that can manage themselves according to an administrator's goals, integrating new components as seamlessly as biological cells.\nRealizing an autonomic system with self-management poses significant challenges. Early approaches to autonomic systems often relied on rule-based mechanisms and predefined policies, which, while effective to some extent, struggled to adapt to the increasingly dynamic and complex environments seen in modern computing systems [23, 33]. Over the years, significant strides have been made towards achieving this vision through the development of self-adaptive and self-managing systems [15, 36]. Despite significant efforts and progress over the past two decades, the realization of ACV is still elusive due to numerous grand challenges outlined in the ACV paper, many of which hinge on breakthroughs in AI.\nRecent advancements in AI, particularly through Large Language Models (LLMs), offer promising new avenues to address these challenges. LLMs' extensive knowledge, language understanding, and task automation capabilities [7, 45, 46] represent a significant leap forward in our ability to create truly autonomic systems. Successful demonstrations of LLMs in tasks such as anomaly detection and incident mitigation [60] illustrate their potential to provide the contextual understanding and adaptive decision-making necessary to achieve the goals of ACV. This prompts an intriguing question: Can LLMs make the Vision of Autonomic Computing a reality?\nAddressing this question is complex due to the broad scope of ACV. To systematically assess the feasibility of achieving ACV using LLMs, evaluation studies in concrete settings are necessary. This work proposes to study this feasibility within the context of microservice self-management, a popular architecture for managing cloud services. Specifically, we propose an LLM-based multi-agent self-management framework for microservices and assess its performance using a live online evaluation benchmark built on the known microservice demo project Sock Shop [1].\nOur proposed LLM-based microservice management system employs a hierarchical multi-agent architecture. High-level group manager handles declarative tasks that span multiple service components, such as optimizing end-to-end latency to under 200 ms. In contrast, low-level autonomic agents focus on specific tasks within their managed service components. To evaluate our system, we introduce a five-level taxonomy of autonomous service maintenance, emphasizing Self-Optimization and Self-Healing. We then design specific evaluation tasks within the Sock Shop microservice, employing chaos engineering techniques to deliberately introduce faults and observe how our management system resolves these issues. Our findings demonstrate that the LLM-based multi-agent framework achieves Level 3 autonomy in our five-level taxonomy. While it effectively detects issues and performs specific imperative tasks, there are opportunities for further enhancements in root cause analysis and issue mitigation capabilities.\nOur contributions can be summarized as follows:\n\u2022 We advance the domain of autonomic computing for microservice management through an LLM-based multi-agent framework. To the best of our knowledge, we are the first research work to explore microservice self-management using LLM-based agents.\n\u2022 We establish a taxonomy consisting of five levels for autonomous service maintenance. We also present an online evaluation benchmark designed to assess tasks corresponding to each level of autonomy within the microservice demo project Sock Shop.\n\u2022 We conduct a rigorous evaluation of our LLM-based microservice management framework using the aforementioned benchmark."}, {"title": "BACKGROUND AND RELATED WORK", "content": "Autonomic Computing. The goal of autonomic computing is to develop self-managing systems which reduces the complexity and cost of IT management that increases system reliability and performance. It is inspired by the biological autonomic nervous system, which autonomously regulates functions such as heart rate and body temperature, thereby reducing cognitive load. The ACV paper identified four key objectives of self-management in autonomic systems [12, 27]:\n\u2022 Self-Configuration: Systems can configure and re-adjust themselves to meet the agreed objectives.\n\u2022 Self-Optimization: Continuously monitor themselves and seek opportunities to improve performance and costs.\n\u2022 Self-Healing: The ability to autonomously recover from failures and even predict them.\n\u2022 Self-Protection: Against malicious attacks or cascading failures.\nIt also proposed the Monitor, Analyze, Plan, Execute, and Knowledge (MAPE-K) loop [27] as a structured approach to autonomic computing, providing a clear and systematic methodology for implementing self-managing capabilities. The MAPE-K model forms the foundation of self-adaptive and self-management systems, enabling continuous adaptation and optimization through feedback loops [5, 12, 40, 55] and creating autonomous systems to handle complex and dynamic environments.\nSubsequent works have explored various aspects of autonomic systems, leading to significant advancements in self-adaptive systems, such as handling uncertainties and runtime variabilities [4, 14, 24, 28, 61], as well as self-protecting ability in terms of security risks [20]. Autonomic computing principles have also been applied beyond software systems, such as in robotics [8], autonomous driving systems [6], and the development of digital twins [18]. These systems require sophisticated self-adaptation mechanisms to cope with dynamic and unpredictable environments.\nInitially, autonomic systems often relied on rule-based approaches and predefined policies to achieve self-management. While effective in certain contexts, these approaches were limited in their ability to handle complex, dynamic scenarios requiring adaptive decision-making and contextual understanding. Recent advancements in AI and machine learning (ML) have introduced new possibilities. Leveraging Al and ML to provide deeper insights (such as event correlation and predictive analytics) and automation capabilities has further enhanced the self-managing capabilities of IT operations, leading to the development of a field known as AIOps.\nAutonomic Management of Cloud-Native Applications. Cloud computing has emerged as a critical component of modern distributed computing systems, offering scalable resources, cost efficiency, and flexibility through on-demand access to computing power, storage, and applications. This shift has led to the widespread adoption of cloud-native applications, leveraging architectures such as microservices. However, managing these applications poses significant challenges, including ensuring security, navigating complex microservices architectures, maintaining observability, managing resource allocation, and ensuring reliable performance and scalability [17, 40, 50, 58].\nTo address these challenges, numerous management tools have been developed. The Cloud Native Computing Foundation (CNCF) [10] hosts various projects aimed at fostering innovation and collaboration within the cloud-native community, such as Kubernetes [11] and Prometheus [47]. Kubernetes, for example, automates deployment, scaling, and management of containerized applications, offering features like service discovery and automated rollouts. Despite these capabilities, Kubernetes lacks comprehensive high-level management features aligned closely with user intent, such as built-in middleware or advanced configuration management systems. On the research side, recent advancements, such as the AMOCNA [29\u201331] framework, extend traditional autonomic management approaches like the MAPE-K loop [5] to enhance autonomy in cloud-native environments. However, these frameworks often rely on rule-based systems, limiting their adaptability in handling complex scenarios requiring contextual understanding and adaptive decision-making.\nTo address these limitations, recent works have further extended the traditional MAPE-K loop to incorporate AI and ML advancements [43, 51], proposing a novel self-adaptation approach [32, 52].\nLLM-based Kubernetes and Cloud Service Management. In the era of LLMs, integrating LLMs into Kubernetes management represents a promising avenue to overcome limitations in traditional approaches. For instance, GenKubeSec [39] applies LLM-based methods to detect and remediate Kubernetes configuration file misconfigurations with precision and detailed insights, surpassing the capabilities of rule-based tools. Similarly, the open-source project K8sGPT [26] provides natural language-based diagnostics and issue triaging for Kubernetes clusters. Despite these advancements, a cohesive integration of LLMs across all facets of self-management in Kubernetes remains an ongoing area of development.\nSimilarly, current cloud service management systems are starting to integrate LLMs for AIOps to ensure the high availability and reliability of large-scale cloud infrastructures [57, 59, 60]. The key AIOps tasks within cloud service management include data collection and pre-processing, root cause analysis (RCA), and incident mitigation. Cloud platforms consist of numerous services with diverse configurations, posing challenges for traditional AIOps models in terms of cross-service and cross-task flexibility. LLMs offer a solution by efficiently interpreting unstructured data such as service logs and troubleshooting guides, extracting essential information from large data volumes [25, 34]. Once an anomaly or incident is detected, RCA is crucial for identifying the underlying causes. Leveraging information extracted and summarized by LLM-based data pre-processing, LLMs can comprehend and localize incidents effectively [2, 9]. Incident mitigation follows RCA, where LLMs assist in automating mitigation steps previously managed by on-call engineers (OCEs) [3, 48]. These LLM-based solutions empower each AIOps task by reducing human effort and increasing automation. However, despite these advancements in each AIOps task, the entire AIOps process is not yet fully self-managed, requiring additional efforts for managing large-scale cloud service systems. The concrete realization of ACV in our paper offers a demonstration that unifies and coordinates LLM-based AIOps task solutions to achieve a self-managed cloud service system with high efficiency, availability and reliability."}, {"title": "SERVICE MANAGEMENT WITH LLM-BASED MULTI-AGENTS", "content": "In this section, we demonstrate the application of ACV in service management using LLM-based agents, realizing the general architectural considerations discussed in the ACV paper.\nArchitecture Overview. The ACV paper emphasizes that autonomic management elements operate at various levels, from individual service components to service groups fulfilling specific features, and up to entire business functions. This concept aligns with the design of LLM-based multi-agent system [19, 54], where a hierarchical architecture is naturally suited to manage services at these different levels. Considering the varied complexity of management requests at different levels, we propose a two-level autonomic management architecture in this paper. As shown in Figure 1, this hierarchical service management framework provides flexibility for the maintainers to manage the system at different appropriate levels. For complex management requests, such as \u201censure end-to-end latency below 20 ms\u201d, which involve multiple services, the high-level LLM-based group manager processes the request by breaking it down into simpler low-level tasks that can be handled by the respective low-level autonomic agents. Conversely, simpler requests can be sent directly to the low-level autonomic agents. For example, a request to \u201cscale replicas to 3\u201d can be directed straight to the corresponding low-level autonomic agent, bypassing the high-level group manager. The detailed working mechanism of this two-level architecture will be discussed later in this section.\nLow-Level Autonomic Agent. Traditionally, a system consists of elements with limited autonomic capabilities responsible for basic service functions, such as a pod in Kubernetes. These elements operate reactively, executing tasks like responding to API calls upon request. By integrating an autonomic layer to manage each element, these managed elements can enhance their functionality significantly. They can monitor their own health, analyze potential issues, and develop mitigation plans to resolve problems autonomously.\nBefore the advent of LLMs, autonomic capabilities were achieved through explicit development of Monitor, Analyze, Plan, Execute, and Knowledge modules [27]. However, LLMs streamline these functions into two core modules: Planner and Executor. The Planner generates execution steps to achieve specific goals, including monitoring and analysis, which maybe passing down from the high-level group manager (discussed in the below section) or requested directly by maintainers. These steps are often executable maintenance code (e.g., kubectl commands in Kubernetes), leveraging the coding capabilities and general knowledge of LLMs. The Executor carries out these steps, and the results are sent back to the Planner to verify goal achievement. If the goal is unmet, another self-correction cycle is initiated. This Plan-Execute feedback loop simplifies the traditional design of self-management agents, facilitating the development of autonomic elements at scale and across different hierarchical levels.\nHigh-Level Group Manager. Low-level autonomic agents can interact through natural upstream and downstream dependencies or form a high-level service group managed by a high-level autonomic group manager. This high-level group manager is often necessary for complex tasks requiring coordinated group actions, such as resolving complex service incidents in cloud environments. It is also easier and flexible to express goal-oriented terms in natural language at the high-level service group, such as \u201creduce end-to-end service latency to 20 ms\u201d.\nThe LLM-based agent in the high-level group manager also uses the Plan-Execute feedback mechanism. Unlike the lower-level autonomic elements, the Planner in the high-level group manager breaks down complex management requests into sub-tasks and generates detailed, step-by-step plans. Each step corresponds to a specific low-level autonomic agent and includes executable code, such as assigning sub-tasks and analyzing collected metrics. The Executor then carries out these steps by executing the code. Feedback from the low-level autonomic agents is sent back to the high-level Planner to determine if the goals have been met. The Planner can adjust the plan based on this feedback and track the progress of the execution. This proactive approach allows the system to maintain optimal performance and reduce maintenance time, minimizing the need for direct human intervention.\nThree Working Mechanisms. We define three working mechanisms with our hierarchical service management framework, each designed for particular task management and execution scenarios: a) Low-Level Autonomic Agent Working Alone; b) Multiple Low-Level Autonomic Agents Collaborating Under a Manager; c) Intra-Communication among Low-Level Autonomic Agents. To be more specific:\nLow-Level Autonomic Agent Working Alone. In this mode, the maintainer directly assign requests or tasks to a single low-level autonomic agent, requesting it to answer maintenance-related inquiries or take actions to fulfill maintenance requests. The agent operates independently to complete the assigned task. This mechanism is straight-forward and effective for simple, isolated tasks that do not require coordination with other low-level autonomic agents.\nMultiple Low-Level Autonomic Agents Collaborating Under a Manager. In this mode, the high-level group manager either receives tasks from the maintainer or issues are raised by the low-level autonomic agents themselves. The manager first decomposes the task based on the received message, generate a plan, and then assigns the sub-tasks to the relevant low-level autonomic agents for execution. The execution results from each agent are collected, and the manager may modify or proceed to the next steps based on these outcomes. If the original plan cannot be followed, it is adjusted accordingly. This iterative process continues until the task is either completed or deemed unachievable. This mechanism is suitable for complex tasks that require collaborative efforts among multiple low-level autonomic agents.\nIntra-Communication among Low-Level Autonomic Agents. When a low-level autonomic agent encounters an issue it cannot resolve independently, it can seek assistance from other agents without involving of the high-level group manager. This mechanism facilitates internal communication among low-level autonomic agents, allowing them to collaborate and attempt to fix the issue by directly communicating with each other.\nTo facilitate these working mechanisms, we use a message queue system as middleware for message passing and storage. This approach unifies the basic framework of all three working mechanisms, enhancing overall service availability and reliability. Additionally, the message queue system decouples message passing through asynchronous message queues, improving system robustness and flexibility. Note that we implement the first two mechanisms in the example below for simplicity and better illustration.\nExample Implementation on Sock Shop. There are several microservce demos such as Online Boutique [16] and Death Star [13]. We select Sock Shop [1] as the microservices demo application due to its comprehensive example of a real-world e-commerce application, modular microservices design, and Kubernetes-native setup. It offers a simple yet accessible start point to demonstrate sophisticated microservices and Kubernetes concepts.\nSock Shop is a microservices demo application that emulates an e-commerce website for selling socks, designed to help users understand Kubernetes and microservices architecture. It consists of various microservices managed by Kubernetes, each handling different aspects of the e-commerce site, such as the website frontend, product catalog, orders, and payment (shown in Figure 2 left). Each microservice is containerized using Docker, ensuring consistent and isolated deployment across different environments. These microservices are deployed and orchestrated by Kubernetes, which offers certain capabilities for managing them. However, Kubernetes relies on additional tools like Prometheus [47] to perform tasks like monitoring and metrics collection. It also requires other essential declarative actions include auto-scaling, resource management, alerting, fault troubleshooting, and performance optimization. Currently, these actions are mostly performed manually by maintainers, requiring significant effort and domain expertise.\nAs shown in Figure 2 right, we demonstrate the application of LLM-based autonomic management on the Sock Shop microservice demo project in Kubernetes. This autonomic management framework is based on the previously discussed LLM-based two-level hierarchical framework. Briefly, the high-level group manager oversees the entire group of microservices, interprets high-level tasks from maintainers, and breaks down these tasks into executable steps assigned to low-level autonomic agents. The high-level group manager also handles issues raised by low-level autonomic agents. Note that the high-level group manager does not directly operate any microservice; instead, each low-level autonomic agent manages its corresponding microservice, responding to tasks assigned by the high-level group manager, such as monitoring and maintenance.\nTo be more specific, each Sock Shop containerized microservice is managed by a low-level LLM-based autonomic agent, making it an LLM-enhanced service component. Each agent is responsible for managing its own component. For example, the Front-end component is converted into the Front-end agent, enhanced by the LLM-based autonomic agent. Messages containing sub-tasks or maintainer requests are passed to the Front-end agent. Through the iterative process with the Planner and Executor, maintenance tasks are completed within the agent. Responses and feedback are then sent to the high-level group manager through the manager's message queue. If there is an unresolved problem, an issue is sent to the manager's message queue, seeking assistance from the high-level group manager (shown in the red line in Figure 2). The high-level group manager analyzes the message, makes necessary adjustments to the plan, and then transmits the revised steps to the message queues of the corresponding low-level autonomic agents, coordinating collaboration among various agents."}, {"title": "EVALUATION BENCHMARK", "content": "After introducing the hierarchical LLM-based autonomic management framework and presenting an example ACV implementation, we now evaluate its performance to address the central question of this paper: how close are we to achieving ACV with LLMs? To answer this, we introduce a taxonomy categorizing tasks by their levels of autonomy and define specific task cases using Sock Shop."}, {"title": "Autonomous Levels in Service Maintenance", "content": "Inspired by the six levels of autonomous driving and the categorization of Personal LLM Agents' duties based on intelligence levels in [37], we propose a taxonomy of five autonomy levels for service maintenance, depicted in Figure 3. The basic L1 and L2 levels represent the foundational maintenance capabilities of autonomic management agents, such as understanding users' intent and possessing necessary service maintenance knowledge (e.g., writing correct maintenance code using kubectl commands). At Level 1 (\"Simple Step Following\"), we assess whether the agent can determine correct operational commands to fulfill specific imperative instructions, such as scaling replicas to three. Level 2 (\"Deterministic Task Automation\u201d) additionally requires the agent to possess the planning capability by decomposing a complex task into smaller executable steps, such as checking the service health status by generating a series of metric collection and understanding steps. L1 and L2 levels are characterized by imperative tasks, focusing on fulfilling specified tasks. Higher autonomy levels require the agent to fulfill declarative tasks, proactively performing actions to achieve predefined goals or states. These high-level goals align with the four self-management objectives outlined in the ACV paper. For simplicity, this work focus on tasks related to Self-Optimization and Self-Healing, leaving the other two areas for future research.\nAchieving Self-Optimization and Self-Healing necessitates three key capabilities in the autonomic management agent: \"Proactive Issue Detection\", \"Automatic Root cause Analysis\", and \"Full Self-Maintenance\" with the generation and execution of mitigation solutions. These capabilities correspond to L3, L4, and L5 levels in our taxonomy shown in Figure 3. With this five-level taxonomy established, we proceed to design specific evaluation tasks tailored to each level within the Sock Shop context. These tasks will serve as benchmarks to quantitatively assess the capabilities of current LLMs in advancing towards the realization of ACV."}, {"title": "Online Live Evaluation Benchmark", "content": "In traditional benchmarking, evaluations are typically performed against a specific dataset in an offline manner. However, to assess whether an agent can achieve the complete Detection-RCA-Mitigation maintenance cycle, we need an Online Live Evaluation Benchmark that operates within a functional service environment. To create this environment, we deploy the Sock Shop service using Kubernetes and simulate traffic to ensure it functions as a live system. This setup allows us to introduce various tasks, such as metric collection and health checks, to evaluate the basic L1 and L2 capabilities. For evaluating L3, L4, and L5 capabilities, we further employ Chaos Engineering techniques [44] to intentionally inject faults or induce performance issues.\nTable 1 lists 16 evaluation tasks for L1 and L2 levels, distinguishing between 12 tasks for low-level and 4 tasks for high-level group manager. Low-level tasks are directly applied to agents that manage individual service component, while high-level tasks involve the manager agent, allowing us to examine individual and collaborative task handling. For this study, we focus on the Catalogue for low-level evaluation and include the high-level group manager and Front-end for high-level evaluation tasks. These tasks encompass basic Deployment and Creation Management (DCM) operations as well as Runtime Management (RM) activities, reflecting common microservice management operations. Traffic load levels are specified to ensure tasks, such as reducing latency, are meaningful.\nIn contrast, L3, L4, and L5 evaluations involve injecting specific faults or issues to trigger the evaluation. We introduce three types of faults and performance issues:\n\u2022 Pod Failure: Replace Catalogue pod image with a fake and non-functional one.\n\u2022 CPU Stress: Occupy 100% of Catalogue pod's CPU resources.\n\u2022 Rising Traffic: Gradually increase traffic, leading to high resource usage and extended service latency. This pattern is applied directly to Catalogue for low-level tasks and to Front-end for high-level tasks.\nWhile many other faults and issues exist in microservice management, these three are representative and pertinent for this study. We then evaluate the system's ability to perform self-management operations to meet predefined Service Level Objectives (SLOs) under these injected conditions. Within the Sock Shop context, we define the following SLOs:\n(1) All service components maintain a healthy READY state.\n(2) CPU and memory usage for each component remains under 50% of allocated resources.;\n(3) The P99 latency for each component is stable, with an average P99 latency below 200ms.\nEvaluation of L3/L4/L5 tasks are also distinguished by both low-level autonomic agents and high-level group manager, with the above SLOs communicated to these agents based on task requirements. Given that L3, L4, and L5 tasks are typically executed in a unified Detection-RCA-Mitigation sequence, we implement a combined task to evaluate all three levels simultaneously. A detailed description of these tasks is provided in Section A.2 in the Appendix."}, {"title": "Evaluation Metric", "content": "Our evaluation focuses on two main types of metrics: efficiency and quality. For efficiency metrics, we monitor the number of steps taken in each evaluation, with each step defined as a single call to the base LLM. We also track the number of communication rounds between high-level group manager and low-level autonomic agents for tasks initiated by the high-level group manager. Steps that result in execution errors are included as well, as these errors reflect the base LLM's ability to generate correct service maintenance code.\nOn the quality side, we measure the task fulfillment rate at various levels. For L1 and L2 tasks, we assess whether the assigned tasks are completed successfully. For L3 tasks, we evaluate the agent's ability to correctly judge if the system meets the SLOs. In L4 tasks, we determine if the management agents can identify the correct root causes. Finally, for L5 tasks, we check whether injected faults or issues are successfully mitigated. Additionally, for tasks assigned to high-level group manager, we evaluate the high-level agent's ability to correctly delegate tasks to low-level autonomic agents."}, {"title": "EXPERIMENT", "content": "Setup. Sock Shop is deployed on a Minikube [42] cluster with 6 cores and 16 GB of memory, utilizing an Intel Xeon Gold 6338 CPU @ 2 GHz server. The deployment includes an enabled metric server and a Prometheus server, configured according to the Sock Shop deployment files. Load simulation is applied using Locust [38], with traffic levels detailed in Table 8 in the Appendix. The faults of Pod Failure and CPU Stress are injected by utilizing the Chaos Mesh tool [41].\nAgent-based Management System. The agent-based management system introduced in Section 3 is implemented by leveraging the multi-agent framework AutoGen [56]. It incorporates agent groups consisting of LLM-based agents and non-LLM-based code executors for each microservice component, serving as low-level autonomic agents. Similarly, the high-level group manager comprises a LLM-based agent and a non-LLM-based code executor. Communication among these agents is facilitated by RabbitMQ [49], and GPT-4 Turbo is adopted as the underlying LLM.\nThe prompts for low-level autonomic agents and high-level group manager are given in Sections C in the Appendix. Most of instructions in them are generally applicable to other microservices managed by Kubernetes. However, in the prompt for low-level autonomic agents, we include instructions that is tailored specifically to the Sock Shop service. Those instructions are effective at eliminating inefficient self-correcting steps. For example, agents might use the wrong label selector \u201c-1 app Catalogue\u201d instead of the correct \u201c-1 name=Catalogue\u201d to identify Catalogue. While agents can correct such errors through several self-correction rounds, these mistakes significantly disrupt the execution flow since many kubectl commands depend on accurate label selectors. Similarly, querying Prometheus metrics correctly is often challenging because it requires precise input of metric names and filters, which are service-specific and not typically known to LLMs. Although these specific instructions reduce generality, they are necessary, especially for low-level autonomic agents managing elements with domain-specific information that LLMs lack.\nExperiment Procedure. As detailed in Section 4.2, tasks are categorized for management agents at both low and high levels. Given the distinct characteristics of L1/L2 and L3/L4/L5 tasks, we establish four distinct experiment configurations, each outlined comprehensively in Table 2. Overall, the procedure of each experiment run involves the environment setup (i.e., deploying Sock Shop and ensuring stable traffic), sending task requirement to corresponding management agents and finally evaluating the task performance. All experiments are randomly repeated three times to reduce statistical fluctuation. For L3/L4/L5 tasks managed by low-level autonomic agents, three consecutive evaluations are allowed to account for potential missed detections in initial trials. Conversely, for L3/L4/L5 tasks managed by high-level agents, a single evaluation suffices as multiple agents collaborate to detect issues."}, {"title": "Results for Tasks Applied to the Low-Level Autonomic Agent", "content": "Table 3 presents the experimental results for the L1 and L2 tasks applied on the low-level autonomic agent of Catalogue. Details include the pass status, the number of steps taken, and any steps with execution errors. The key findings are summarized as follows:\n\u2022 The L1 and L2 tasks demonstrated high task completion rates, achieving 100% for L1 and 87% for L2. One of the failed L2 experiments was the Latency Reduction, which failed because an ineffective action was made by editing another service configuration. The other unsuccessful experiment, CPU Reduction, failed due to misguided actions, particularly the reduction of CPU requests and limits, resulting in non-operational pods.\n\u2022 On average, the L1 task required approximately 5 steps to complete, while the L2 task required 8 steps, indicating that L2 tasks often involve additional planning steps. While core operations often required only 1 or 2 steps, those extra steps were related to precautionary checks before and after taking actions.\n\u2022 Code execution errors were minimal, and the system generally self-corrected these errors, underscoring the robustness of the LLM-based management system.\nThese results indicate that low-level LLM-based autonomic agents are highly effective in performing basic service maintenance tasks in Kubernetes, establishing a solid foundation for advancing towards higher levels of autonomy.\nThe results for L3, L4, and L5 tasks applied to the low-level autonomic agent of Catalogue are summarized in Table 4. Detailed evaluations, including failure reasons, are provided in Table 10."}, {"title": "Results for Tasks Applied to the High-Level Group Manager", "content": "We next present the results of applying tasks to the high-level group manager, focusing on the task completion rate, task assignment to low-level autonomic agents, and how these agents fulfill sub-tasks. Table 5 summarizes the results for L2 tasks applied to the high-level group manager, with detailed results in Table 11. Besides the number of steps and the overall task pass status, Table 5 also indicates the number of communication rounds between high-level and low-level autonomic agents and the accuracy of task assignment by the high-level agent. Key observations include:\n\u2022 On average, it took about 3 rounds to complete each L2 task. These rounds typically involve: i) the first round of collecting metrics to determine task necessity; ii) the second round where low-level autonomic agents perform the actual task; iii) the final round of summarizing and reporting results, and terminating the task. For instance, Figure 4 illustrates the sequence for the Latency Reduction-Group task. Upon receiving the task to reduce P99 latency of Front-end and Catalogue to under 400 ms, the high-level group manager first gathered P99 latency metrics for both services. After finding that the latencies were about 400 ms and 700 ms respectively, the manager assigned the task of reducing latency to under 200 ms. Low-level autonomic agents carry out the required actions and reported back, enabling the high-level group manager to summarize and terminate the task.\n\u2022 Each task took approximately 31 steps to complete, significantly higher than the 8 steps required for L2 tasks handled by low-level autonomic agents (Table 3). The increase in steps is due to the higher complexity of tasks involving multiple low-level autonomic agents (e.g., Latency Reduction-Group task) and additional planning and communication steps required in the hierarchical management architecture. Notably, the steps taken by low-level autonomic agents during task execution (e.g., the round for the latency reduction task) were comparable to those in Table 3. Thus, since a hierarchical management structure introduces extra planning and communication costs, directly assigning maintenance requests to specific low-level autonomic agents might sometimes be more efficient.\n\u2022 The high-level management system demonstrates strong task assignment capabilities, achieving an accuracy of 0.87 for L2 tasks. Analysis of task completion reveals that while latency reduction tasks exhibit high success rates, tasks aimed at CPU reduction suffer substantial accuracy declines. Detailed examination of failure instances, detailed in Table 11, highlights common issues such as erroneous CPU request reductions (mirroring the L2 Latency Reduction task failures in Table 3) and horizontal scaling actions insufficient for task fulfillment.\nThe results for the L3, L4, and L5 tasks are summarized in Table 6, with detailed information provided in Table 12. These tables list the number of rounds and steps performed, as well as the task completion rates at each level and overall, including whether the injected faults were resolved. Key findings include:\n\u2022 The average number of rounds required was approximately 3, similar to the L2 task. The procedure for these rounds typically followed the similar pattern of metric collection, task execution, and result summary. However, additional rounds of task execution were sometimes necessary if the previous ones did not complete the task.\n\u2022 On average, 45 steps were required, significantly more than the 31 steps needed for L2 tasks. This increase may be due to the greater complexity and deeper analysis required for each round of the L3/L4/L5 sub-tasks.\n\u2022 The overall task completion rate decreased from 0.9 (L3) to 0.5 (L4) and 0.42 (L5), consistent with the pattern observed in low-level autonomic agent tasks. However, the actual completion rates were higher than those for the corresponding low-level tasks, suggesting that including a hierarchical management structure may improve the overall task completion rate. This observation further confirms our previous proposition that attaining Level 3 autonomy is feasible with the existing LLM-based agent management system."}, {"title": "Failure Analysis", "content": "Lastly, we present an in-depth analysis of failed cases in tasks related to L3 (incorrect detection), L4 (incorrect root cause analysis), L5 (mitigation failure), and erroneous high-level task assignment. The detailed results, categorized by failure type and potential improvement directions, are shown in Table 7. These failures stem from issues such as instruction-following errors (e.g., task omission), hallucinations, deficiencies in reasoning capabilities, and insufficient domain knowledge.\nTo address these issues, we propose two primary strategies. First, we can employ a superior base LLM, either a state-of-the-art general LLM or a domain-specific LLM fine-tuned with service maintenance knowledge. Second, we can enhance our existing agent framework by integrating additional modules. These modules might include mechanisms for incorporating domain knowledge using retrieval-augmented generation techniques [35] or the addition of critic agents to mitigate hallucinations and bolster reasoning capabilities."}, {"title": "DISCUSSION", "content": "This study systematically evaluates the feasibility of realizing autonomic computing using LLM-based agents. Our comprehensive experiments"}, {"title": "The Vision of Autonomic Computing: Can LLMs Make It a Reality?", "authors": ["Zhiyang Zhang", "Fangkai Yang", "Xiaoting Qin", "Jue Zhang", "Qingwei Lin", "Gong Cheng", "Dongmei Zhang", "Saravan Rajmohan", "Qi Zhang"], "abstract": "The Vision of Autonomic Computing (ACV), proposed over two decades ago, envisions computing systems that self-manage akin to biological organisms, adapting seamlessly to changing environments. Despite decades of research, achieving ACV remains challenging due to the dynamic and complex nature of modern computing systems. Recent advancements in Large Language Models (LLMs) offer promising solutions to these challenges by leveraging their extensive knowledge, language understanding, and task automation capabilities. This paper explores the feasibility of realizing ACV through an LLM-based multi-agent framework for microservice management. We introduce a five-level taxonomy for autonomous service maintenance and present an online evaluation benchmark based on the Sock Shop microservice demo project to assess our framework's performance. Our findings demonstrate significant progress towards achieving Level 3 autonomy, highlighting the effectiveness of LLMs in detecting and resolving issues within microservice architectures. This study contributes to advancing autonomic computing by pioneering the integration of LLMs into microservice management frameworks, paving the way for more adaptive and self-managing computing systems. The code will be made available at https://aka.ms/ACV-LLM.", "sections": [{"title": "INTRODUCTION", "content": "As we enter the modern era, computing infrastructure is becoming increasingly distributed and large-scale, presenting significant challenges for human management. This complexity underscores the necessity for developing systems capable of self-management. This vision, reminiscent of the Vision of Autonomic Computing (ACV) [27] proposed two decades ago, envisions computing systems that can manage themselves according to an administrator's goals, integrating new components as seamlessly as biological cells.\nRealizing an autonomic system with self-management poses significant challenges. Early approaches to autonomic systems often relied on rule-based mechanisms and predefined policies, which, while effective to some extent, struggled to adapt to the increasingly dynamic and complex environments seen in modern computing systems [23, 33]. Over the years, significant strides have been made towards achieving this vision through the development of self-adaptive and self-managing systems [15, 36]. Despite significant efforts and progress over the past two decades, the realization of ACV is still elusive due to numerous grand challenges outlined in the ACV paper, many of which hinge on breakthroughs in AI.\nRecent advancements in AI, particularly through Large Language Models (LLMs), offer promising new avenues to address these challenges. LLMs' extensive knowledge, language understanding, and task automation capabilities [7, 45, 46] represent a significant leap forward in our ability to create truly autonomic systems. Successful demonstrations of LLMs in tasks such as anomaly detection and incident mitigation [60] illustrate their potential to provide the contextual understanding and adaptive decision-making necessary to achieve the goals of ACV. This prompts an intriguing question: Can LLMs make the Vision of Autonomic Computing a reality?\nAddressing this question is complex due to the broad scope of ACV. To systematically assess the feasibility of achieving ACV using LLMs, evaluation studies in concrete settings are necessary. This work proposes to study this feasibility within the context of microservice self-management, a popular architecture for managing cloud services. Specifically, we propose an LLM-based multi-agent self-management framework for microservices and assess its performance using a live online evaluation benchmark built on the known microservice demo project Sock Shop [1].\nOur proposed LLM-based microservice management system employs a hierarchical multi-agent architecture. High-level group manager handles declarative tasks that span multiple service components, such as optimizing end-to-end latency to under 200 ms. In contrast, low-level autonomic agents focus on specific tasks within their managed service components. To evaluate our system, we introduce a five-level taxonomy of autonomous service maintenance, emphasizing Self-Optimization and Self-Healing. We then design specific evaluation tasks within the Sock Shop microservice, employing chaos engineering techniques to deliberately introduce faults and observe how our management system resolves these issues. Our findings demonstrate that the LLM-based multi-agent framework achieves Level 3 autonomy in our five-level taxonomy. While it effectively detects issues and performs specific imperative tasks, there are opportunities for further enhancements in root cause analysis and issue mitigation capabilities.\nOur contributions can be summarized as follows:\n\u2022 We advance the domain of autonomic computing for microservice management through an LLM-based multi-agent framework. To the best of our knowledge, we are the first research work to explore microservice self-management using LLM-based agents.\n\u2022 We establish a taxonomy consisting of five levels for autonomous service maintenance. We also present an online evaluation benchmark designed to assess tasks corresponding to each level of autonomy within the microservice demo project Sock Shop.\n\u2022 We conduct a rigorous evaluation of our LLM-based microservice management framework using the aforementioned benchmark."}, {"title": "BACKGROUND AND RELATED WORK", "content": "Autonomic Computing. The goal of autonomic computing is to develop self-managing systems which reduces the complexity and cost of IT management that increases system reliability and performance. It is inspired by the biological autonomic nervous system, which autonomously regulates functions such as heart rate and body temperature, thereby reducing cognitive load. The ACV paper identified four key objectives of self-management in autonomic systems [12, 27]:\n\u2022 Self-Configuration: Systems can configure and re-adjust themselves to meet the agreed objectives.\n\u2022 Self-Optimization: Continuously monitor themselves and seek opportunities to improve performance and costs.\n\u2022 Self-Healing: The ability to autonomously recover from failures and even predict them.\n\u2022 Self-Protection: Against malicious attacks or cascading failures.\nIt also proposed the Monitor, Analyze, Plan, Execute, and Knowledge (MAPE-K) loop [27] as a structured approach to autonomic computing, providing a clear and systematic methodology for implementing self-managing capabilities. The MAPE-K model forms the foundation of self-adaptive and self-management systems, enabling continuous adaptation and optimization through feedback loops [5, 12, 40, 55] and creating autonomous systems to handle complex and dynamic environments.\nSubsequent works have explored various aspects of autonomic systems, leading to significant advancements in self-adaptive systems, such as handling uncertainties and runtime variabilities [4, 14, 24, 28, 61], as well as self-protecting ability in terms of security risks [20]. Autonomic computing principles have also been applied beyond software systems, such as in robotics [8], autonomous driving systems [6], and the development of digital twins [18]. These systems require sophisticated self-adaptation mechanisms to cope with dynamic and unpredictable environments.\nInitially, autonomic systems often relied on rule-based approaches and predefined policies to achieve self-management. While effective in certain contexts, these approaches were limited in their ability to handle complex, dynamic scenarios requiring adaptive decision-making and contextual understanding. Recent advancements in AI and machine learning (ML) have introduced new possibilities. Leveraging Al and ML to provide deeper insights (such as event correlation and predictive analytics) and automation capabilities has further enhanced the self-managing capabilities of IT operations, leading to the development of a field known as AIOps.\nAutonomic Management of Cloud-Native Applications. Cloud computing has emerged as a critical component of modern distributed computing systems, offering scalable resources, cost efficiency, and flexibility through on-demand access to computing power, storage, and applications. This shift has led to the widespread adoption of cloud-native applications, leveraging architectures such as microservices. However, managing these applications poses significant challenges, including ensuring security, navigating complex microservices architectures, maintaining observability, managing resource allocation, and ensuring reliable performance and scalability [17, 40, 50, 58].\nTo address these challenges, numerous management tools have been developed. The Cloud Native Computing Foundation (CNCF) [10] hosts various projects aimed at fostering innovation and collaboration within the cloud-native community, such as Kubernetes [11] and Prometheus [47]. Kubernetes, for example, automates deployment, scaling, and management of containerized applications, offering features like service discovery and automated rollouts. Despite these capabilities, Kubernetes lacks comprehensive high-level management features aligned closely with user intent, such as built-in middleware or advanced configuration management systems. On the research side, recent advancements, such as the AMOCNA [29\u201331] framework, extend traditional autonomic management approaches like the MAPE-K loop [5] to enhance autonomy in cloud-native environments. However, these frameworks often rely on rule-based systems, limiting their adaptability in handling complex scenarios requiring contextual understanding and adaptive decision-making.\nTo address these limitations, recent works have further extended the traditional MAPE-K loop to incorporate AI and ML advancements [43, 51], proposing a novel self-adaptation approach [32, 52].\nLLM-based Kubernetes and Cloud Service Management. In the era of LLMs, integrating LLMs into Kubernetes management represents a promising avenue to overcome limitations in traditional approaches. For instance, GenKubeSec [39] applies LLM-based methods to detect and remediate Kubernetes configuration file misconfigurations with precision and detailed insights, surpassing the capabilities of rule-based tools. Similarly, the open-source project K8sGPT [26] provides natural language-based diagnostics and issue triaging for Kubernetes clusters. Despite these advancements, a cohesive integration of LLMs across all facets of self-management in Kubernetes remains an ongoing area of development.\nSimilarly, current cloud service management systems are starting to integrate LLMs for AIOps to ensure the high availability and reliability of large-scale cloud infrastructures [57, 59, 60]. The key AIOps tasks within cloud service management include data collection and pre-processing, root cause analysis (RCA), and incident mitigation. Cloud platforms consist of numerous services with diverse configurations, posing challenges for traditional AIOps models in terms of cross-service and cross-task flexibility. LLMs offer a solution by efficiently interpreting unstructured data such as service logs and troubleshooting guides, extracting essential information from large data volumes [25, 34]. Once an anomaly or incident is detected, RCA is crucial for identifying the underlying causes. Leveraging information extracted and summarized by LLM-based data pre-processing, LLMs can comprehend and localize incidents effectively [2, 9]. Incident mitigation follows RCA, where LLMs assist in automating mitigation steps previously managed by on-call engineers (OCEs) [3, 48]. These LLM-based solutions empower each AIOps task by reducing human effort and increasing automation. However, despite these advancements in each AIOps task, the entire AIOps process is not yet fully self-managed, requiring additional efforts for managing large-scale cloud service systems. The concrete realization of ACV in our paper offers a demonstration that unifies and coordinates LLM-based AIOps task solutions to achieve a self-managed cloud service system with high efficiency, availability and reliability."}, {"title": "SERVICE MANAGEMENT WITH LLM-BASED MULTI-AGENTS", "content": "In this section, we demonstrate the application of ACV in service management using LLM-based agents, realizing the general architectural considerations discussed in the ACV paper.\nArchitecture Overview. The ACV paper emphasizes that autonomic management elements operate at various levels, from individual service components to service groups fulfilling specific features, and up to entire business functions. This concept aligns with the design of LLM-based multi-agent system [19, 54], where a hierarchical architecture is naturally suited to manage services at these different levels. Considering the varied complexity of management requests at different levels, we propose a two-level autonomic management architecture in this paper. As shown in Figure 1, this hierarchical service management framework provides flexibility for the maintainers to manage the system at different appropriate levels. For complex management requests, such as \u201censure end-to-end latency below 20 ms\u201d, which involve multiple services, the high-level LLM-based group manager processes the request by breaking it down into simpler low-level tasks that can be handled by the respective low-level autonomic agents. Conversely, simpler requests can be sent directly to the low-level autonomic agents. For example, a request to \u201cscale replicas to 3\u201d can be directed straight to the corresponding low-level autonomic agent, bypassing the high-level group manager. The detailed working mechanism of this two-level architecture will be discussed later in this section.\nLow-Level Autonomic Agent. Traditionally, a system consists of elements with limited autonomic capabilities responsible for basic service functions, such as a pod in Kubernetes. These elements operate reactively, executing tasks like responding to API calls upon request. By integrating an autonomic layer to manage each element, these managed elements can enhance their functionality significantly. They can monitor their own health, analyze potential issues, and develop mitigation plans to resolve problems autonomously.\nBefore the advent of LLMs, autonomic capabilities were achieved through explicit development of Monitor, Analyze, Plan, Execute, and Knowledge modules [27]. However, LLMs streamline these functions into two core modules: Planner and Executor. The Planner generates execution steps to achieve specific goals, including monitoring and analysis, which maybe passing down from the high-level group manager (discussed in the below section) or requested directly by maintainers. These steps are often executable maintenance code (e.g., kubectl commands in Kubernetes), leveraging the coding capabilities and general knowledge of LLMs. The Executor carries out these steps, and the results are sent back to the Planner to verify goal achievement. If the goal is unmet, another self-correction cycle is initiated. This Plan-Execute feedback loop simplifies the traditional design of self-management agents, facilitating the development of autonomic elements at scale and across different hierarchical levels.\nHigh-Level Group Manager. Low-level autonomic agents can interact through natural upstream and downstream dependencies or form a high-level service group managed by a high-level autonomic group manager. This high-level group manager is often necessary for complex tasks requiring coordinated group actions, such as resolving complex service incidents in cloud environments. It is also easier and flexible to express goal-oriented terms in natural language at the high-level service group, such as \u201creduce end-to-end service latency to 20 ms\u201d.\nThe LLM-based agent in the high-level group manager also uses the Plan-Execute feedback mechanism. Unlike the lower-level autonomic elements, the Planner in the high-level group manager breaks down complex management requests into sub-tasks and generates detailed, step-by-step plans. Each step corresponds to a specific low-level autonomic agent and includes executable code, such as assigning sub-tasks and analyzing collected metrics. The Executor then carries out these steps by executing the code. Feedback from the low-level autonomic agents is sent back to the high-level Planner to determine if the goals have been met. The Planner can adjust the plan based on this feedback and track the progress of the execution. This proactive approach allows the system to maintain optimal performance and reduce maintenance time, minimizing the need for direct human intervention.\nThree Working Mechanisms. We define three working mechanisms with our hierarchical service management framework, each designed for particular task management and execution scenarios: a) Low-Level Autonomic Agent Working Alone; b) Multiple Low-Level Autonomic Agents Collaborating Under a Manager; c) Intra-Communication among Low-Level Autonomic Agents. To be more specific:\nLow-Level Autonomic Agent Working Alone. In this mode, the maintainer directly assign requests or tasks to a single low-level autonomic agent, requesting it to answer maintenance-related inquiries or take actions to fulfill maintenance requests. The agent operates independently to complete the assigned task. This mechanism is straight-forward and effective for simple, isolated tasks that do not require coordination with other low-level autonomic agents.\nMultiple Low-Level Autonomic Agents Collaborating Under a Manager. In this mode, the high-level group manager either receives tasks from the maintainer or issues are raised by the low-level autonomic agents themselves. The manager first decomposes the task based on the received message, generate a plan, and then assigns the sub-tasks to the relevant low-level autonomic agents for execution. The execution results from each agent are collected, and the manager may modify or proceed to the next steps based on these outcomes. If the original plan cannot be followed, it is adjusted accordingly. This iterative process continues until the task is either completed or deemed unachievable. This mechanism is suitable for complex tasks that require collaborative efforts among multiple low-level autonomic agents.\nIntra-Communication among Low-Level Autonomic Agents. When a low-level autonomic agent encounters an issue it cannot resolve independently, it can seek assistance from other agents without involving of the high-level group manager. This mechanism facilitates internal communication among low-level autonomic agents, allowing them to collaborate and attempt to fix the issue by directly communicating with each other.\nTo facilitate these working mechanisms, we use a message queue system as middleware for message passing and storage. This approach unifies the basic framework of all three working mechanisms, enhancing overall service availability and reliability. Additionally, the message queue system decouples message passing through asynchronous message queues, improving system robustness and flexibility. Note that we implement the first two mechanisms in the example below for simplicity and better illustration.\nExample Implementation on Sock Shop. There are several microservce demos such as Online Boutique [16] and Death Star [13]. We select Sock Shop [1] as the microservices demo application due to its comprehensive example of a real-world e-commerce application, modular microservices design, and Kubernetes-native setup. It offers a simple yet accessible start point to demonstrate sophisticated microservices and Kubernetes concepts.\nSock Shop is a microservices demo application that emulates an e-commerce website for selling socks, designed to help users understand Kubernetes and microservices architecture. It consists of various microservices managed by Kubernetes, each handling different aspects of the e-commerce site, such as the website frontend, product catalog, orders, and payment (shown in Figure 2 left). Each microservice is containerized using Docker, ensuring consistent and isolated deployment across different environments. These microservices are deployed and orchestrated by Kubernetes, which offers certain capabilities for managing them. However, Kubernetes relies on additional tools like Prometheus [47] to perform tasks like monitoring and metrics collection. It also requires other essential declarative actions include auto-scaling, resource management, alerting, fault troubleshooting, and performance optimization. Currently, these actions are mostly performed manually by maintainers, requiring significant effort and domain expertise.\nAs shown in Figure 2 right, we demonstrate the application of LLM-based autonomic management on the Sock Shop microservice demo project in Kubernetes. This autonomic management framework is based on the previously discussed LLM-based two-level hierarchical framework. Briefly, the high-level group manager oversees the entire group of microservices, interprets high-level tasks from maintainers, and breaks down these tasks into executable steps assigned to low-level autonomic agents. The high-level group manager also handles issues raised by low-level autonomic agents. Note that the high-level group manager does not directly operate any microservice; instead, each low-level autonomic agent manages its corresponding microservice, responding to tasks assigned by the high-level group manager, such as monitoring and maintenance.\nTo be more specific, each Sock Shop containerized microservice is managed by a low-level LLM-based autonomic agent, making it an LLM-enhanced service component. Each agent is responsible for managing its own component. For example, the Front-end component is converted into the Front-end agent, enhanced by the LLM-based autonomic agent. Messages containing sub-tasks or maintainer requests are passed to the Front-end agent. Through the iterative process with the Planner and Executor, maintenance tasks are completed within the agent. Responses and feedback are then sent to the high-level group manager through the manager's message queue. If there is an unresolved problem, an issue is sent to the manager's message queue, seeking assistance from the high-level group manager (shown in the red line in Figure 2). The high-level group manager analyzes the message, makes necessary adjustments to the plan, and then transmits the revised steps to the message queues of the corresponding low-level autonomic agents, coordinating collaboration among various agents."}, {"title": "EVALUATION BENCHMARK", "content": "After introducing the hierarchical LLM-based autonomic management framework and presenting an example ACV implementation, we now evaluate its performance to address the central question of this paper: how close are we to achieving ACV with LLMs? To answer this, we introduce a taxonomy categorizing tasks by their levels of autonomy and define specific task cases using Sock Shop."}, {"title": "Autonomous Levels in Service Maintenance", "content": "Inspired by the six levels of autonomous driving and the categorization of Personal LLM Agents' duties based on intelligence levels in [37], we propose a taxonomy of five autonomy levels for service maintenance, depicted in Figure 3. The basic L1 and L2 levels represent the foundational maintenance capabilities of autonomic management agents, such as understanding users' intent and possessing necessary service maintenance knowledge (e.g., writing correct maintenance code using kubectl commands). At Level 1 (\u201cSimple Step Following\u201d), we assess whether the agent can determine correct operational commands to fulfill specific imperative instructions, such as scaling replicas to three. Level 2 (\u201cDeterministic Task Automation\u201d) additionally requires the agent to possess the planning capability by decomposing a complex task into smaller executable steps, such as checking the service health status by generating a series of metric collection and understanding steps. L1 and L2 levels are characterized by imperative tasks, focusing on fulfilling specified tasks. Higher autonomy levels require the agent to fulfill declarative tasks, proactively performing actions to achieve predefined goals or states. These high-level goals align with the four self-management objectives outlined in the ACV paper. For simplicity, this work focus on tasks related to Self-Optimization and Self-Healing, leaving the other two areas for future research.\nAchieving Self-Optimization and Self-Healing necessitates three key capabilities in the autonomic management agent: \u201cProactive Issue Detection\u201d, \u201cAutomatic Root cause Analysis\u201d, and \u201cFull Self-Maintenance\u201d with the generation and execution of mitigation solutions. These capabilities correspond to L3, L4, and L5 levels in our taxonomy shown in Figure 3. With this five-level taxonomy established, we proceed to design specific evaluation tasks tailored to each level within the Sock Shop context. These tasks will serve as benchmarks to quantitatively assess the capabilities of current LLMs in advancing towards the realization of ACV."}, {"title": "Online Live Evaluation Benchmark", "content": "In traditional benchmarking, evaluations are typically performed against a specific dataset in an offline manner. However, to assess whether an agent can achieve the complete Detection-RCA-Mitigation maintenance cycle, we need an Online Live Evaluation Benchmark that operates within a functional service environment. To create this environment, we deploy the Sock Shop service using Kubernetes and simulate traffic to ensure it functions as a live system. This setup allows us to introduce various tasks, such as metric collection and health checks, to evaluate the basic L1 and L2 capabilities. For evaluating L3, L4, and L5 capabilities, we further employ Chaos Engineering techniques [44] to intentionally inject faults or induce performance issues.\nTable 1 lists 16 evaluation tasks for L1 and L2 levels, distinguishing between 12 tasks for low-level and 4 tasks for high-level group manager. Low-level tasks are directly applied to agents that manage individual service component, while high-level tasks involve the manager agent, allowing us to examine individual and collaborative task handling. For this study, we focus on the Catalogue for low-level evaluation and include the high-level group manager and Front-end for high-level evaluation tasks. These tasks encompass basic Deployment and Creation Management (DCM) operations as well as Runtime Management (RM) activities, reflecting common microservice management operations. Traffic load levels are specified to ensure tasks, such as reducing latency, are meaningful.\nIn contrast, L3, L4, and L5 evaluations involve injecting specific faults or issues to trigger the evaluation. We introduce three types of faults and performance issues:\n\u2022 Pod Failure: Replace Catalogue pod image with a fake and non-functional one.\n\u2022 CPU Stress: Occupy 100% of Catalogue pod's CPU resources.\n\u2022 Rising Traffic: Gradually increase traffic, leading to high resource usage and extended service latency. This pattern is applied directly to Catalogue for low-level tasks and to Front-end for high-level tasks.\nWhile many other faults and issues exist in microservice management, these three are representative and pertinent for this study. We then evaluate the system's ability to perform self-management operations to meet predefined Service Level Objectives (SLOs) under these injected conditions. Within the Sock Shop context, we define the following SLOs:\n(1) All service components maintain a healthy READY state.\n(2) CPU and memory usage for each component remains under 50% of allocated resources.;\n(3) The P99 latency for each component is stable, with an average P99 latency below 200ms.\nEvaluation of L3/L4/L5 tasks are also distinguished by both low-level autonomic agents and high-level group manager, with the above SLOs communicated to these agents based on task requirements. Given that L3, L4, and L5 tasks are typically executed in a unified Detection-RCA-Mitigation sequence, we implement a combined task to evaluate all three levels simultaneously. A detailed description of these tasks is provided in Section A.2 in the Appendix."}, {"title": "Evaluation Metric", "content": "Our evaluation focuses on two main types of metrics: efficiency and quality. For efficiency metrics, we monitor the number of steps taken in each evaluation, with each step defined as a single call to the base LLM. We also track the number of communication rounds between high-level group manager and low-level autonomic agents for tasks initiated by the high-level group manager. Steps that result in execution errors are included as well, as these errors reflect the base LLM's ability to generate correct service maintenance code.\nOn the quality side, we measure the task fulfillment rate at various levels. For L1 and L2 tasks, we assess whether the assigned tasks are completed successfully. For L3 tasks, we evaluate the agent's ability to correctly judge if the system meets the SLOs. In L4 tasks, we determine if the management agents can identify the correct root causes. Finally, for L5 tasks, we check whether injected faults or issues are successfully mitigated. Additionally, for tasks assigned to high-level group manager, we evaluate the high-level agent's ability to correctly delegate tasks to low-level autonomic agents."}, {"title": "EXPERIMENT", "content": "Setup. Sock Shop is deployed on a Minikube [42] cluster with 6 cores and 16 GB of memory, utilizing an Intel Xeon Gold 6338 CPU @ 2 GHz server. The deployment includes an enabled metric server and a Prometheus server, configured according to the Sock Shop deployment files. Load simulation is applied using Locust [38], with traffic levels detailed in Table 8 in the Appendix. The faults of Pod Failure and CPU Stress are injected by utilizing the Chaos Mesh tool [41].\nAgent-based Management System. The agent-based management system introduced in Section 3 is implemented by leveraging the multi-agent framework AutoGen [56]. It incorporates agent groups consisting of LLM-based agents and non-LLM-based code executors for each microservice component, serving as low-level autonomic agents. Similarly, the high-level group manager comprises a LLM-based agent and a non-LLM-based code executor. Communication among these agents is facilitated by RabbitMQ [49], and GPT-4 Turbo is adopted as the underlying LLM.\nThe prompts for low-level autonomic agents and high-level group manager are given in Sections C in the Appendix. Most of instructions in them are generally applicable to other microservices managed by Kubernetes. However, in the prompt for low-level autonomic agents, we include instructions that is tailored specifically to the Sock Shop service. Those instructions are effective at eliminating inefficient self-correcting steps. For example, agents might use the wrong label selector \u201c-1 app Catalogue\u201d instead of the correct \u201c-1 name=Catalogue\u201d to identify Catalogue. While agents can correct such errors through several self-correction rounds, these mistakes significantly disrupt the execution flow since many kubectl commands depend on accurate label selectors. Similarly, querying Prometheus metrics correctly is often challenging because it requires precise input of metric names and filters, which are service-specific and not typically known to LLMs. Although these specific instructions reduce generality, they are necessary, especially for low-level autonomic agents managing elements with domain-specific information that LLMs lack.\nExperiment Procedure. As detailed in Section 4.2, tasks are categorized for management agents at both low and high levels. Given the distinct characteristics of L1/L2 and L3/L4/L5 tasks, we establish four distinct experiment configurations, each outlined comprehensively in Table 2. Overall, the procedure of each experiment run involves the environment setup (i.e., deploying Sock Shop and ensuring stable traffic), sending task requirement to corresponding management agents and finally evaluating the task performance. All experiments are randomly repeated three times to reduce statistical fluctuation. For L3/L4/L5 tasks managed by low-level autonomic agents, three consecutive evaluations are allowed to account for potential missed detections in initial trials. Conversely, for L3/L4/L5 tasks managed by high-level agents, a single evaluation suffices as multiple agents collaborate to detect issues."}, {"title": "Results for Tasks Applied to the Low-Level Autonomic Agent", "content": "Table 3 presents the experimental results for the L1 and L2 tasks applied on the low-level autonomic agent of Catalogue. Details include the pass status, the number of steps taken, and any steps with execution errors. The key findings are summarized as follows:\n\u2022 The L1 and L2 tasks demonstrated high task completion rates, achieving 100% for L1 and 87% for L2. One of the failed L2 experiments was the Latency Reduction, which failed because an ineffective action was made by editing another service configuration. The other unsuccessful experiment, CPU Reduction, failed due to misguided actions, particularly the reduction of CPU requests and limits, resulting in non-operational pods.\n\u2022 On average, the L1 task required approximately 5 steps to complete, while the L2 task required 8 steps, indicating that L2 tasks often involve additional planning steps. While core operations often required only 1 or 2 steps, those extra steps were related to precautionary checks before and after taking actions.\n\u2022 Code execution errors were minimal, and the system generally self-corrected these errors, underscoring the robustness of the LLM-based management system.\nThese results indicate that low-level LLM-based autonomic agents are highly effective in performing basic service maintenance tasks in Kubernetes, establishing a solid foundation for advancing towards higher levels of autonomy.\nThe results for L3, L4, and L5 tasks applied to the low-level autonomic agent of Catalogue are summarized in Table 4. Detailed evaluations, including failure reasons, are provided in Table 10."}, {"title": "Results for Tasks Applied to the High-Level Group Manager", "content": "We next present the results of applying tasks to the high-level group manager, focusing on the task completion rate, task assignment to low-level autonomic agents, and how these agents fulfill sub-tasks. Table 5 summarizes the results for L2 tasks applied to the high-level group manager, with detailed results in Table 11. Besides the number of steps and the overall task pass status, Table 5 also indicates the number of communication rounds between high-level and low-level autonomic agents and the accuracy of task assignment by the high-level agent. Key observations include:\n\u2022 On average, it took about 3 rounds to complete each L2 task. These rounds typically involve: i) the first round of collecting metrics to determine task necessity; ii) the second round where low-level autonomic agents perform the actual task; iii) the final round of summarizing and reporting results, and terminating the task. For instance, Figure 4 illustrates the sequence for the Latency Reduction-Group task. Upon receiving the task to reduce P99 latency of Front-end and Catalogue to under 400 ms, the high-level group manager first gathered P99 latency metrics for both services. After finding that the latencies were about 400 ms and 700 ms respectively, the manager assigned the task of reducing latency to under 200 ms. Low-level autonomic agents carry out the required actions and reported back, enabling the high-level group manager to summarize and terminate the task.\n\u2022 Each task took approximately 31 steps to complete, significantly higher than the 8 steps required for L2 tasks handled by low-level autonomic agents (Table 3). The increase in steps is due to the higher complexity of tasks involving multiple low-level autonomic agents (e.g., Latency Reduction-Group task) and additional planning and communication steps required in the hierarchical management architecture. Notably, the steps taken by low-level autonomic agents during task execution (e.g., the round for the latency reduction task) were comparable to those in Table 3. Thus, since a hierarchical management structure introduces extra planning and communication costs, directly assigning maintenance requests to specific low-level autonomic agents might sometimes be more efficient.\n\u2022 The high-level management system demonstrates strong task assignment capabilities, achieving an accuracy of 0.87 for L2 tasks. Analysis of task completion reveals that while latency reduction tasks exhibit high success rates, tasks aimed at CPU reduction suffer substantial accuracy declines. Detailed examination of failure instances, detailed in Table 11, highlights common issues such as erroneous CPU request reductions (mirroring the L2 Latency Reduction task failures in Table 3) and horizontal scaling actions insufficient for task fulfillment.\nThe results for the L3, L4, and L5 tasks are summarized in Table 6, with detailed information provided in Table 12. These tables list the number of rounds and steps performed, as well as the task completion rates at each level and overall, including whether the injected faults were resolved. Key findings include:\n\u2022 The average number of rounds required was approximately 3, similar to the L2 task. The procedure for these rounds typically followed the similar pattern of metric collection, task execution, and result summary. However, additional rounds of task execution were sometimes necessary if the previous ones did not complete the task.\n\u2022 On average, 45 steps were required, significantly more than the 31 steps needed for L2 tasks. This increase may be due to the greater complexity and deeper analysis required for each round of the L3/L4/L5 sub-tasks.\n\u2022 The overall task completion rate decreased from 0.9 (L3) to 0.5 (L4) and 0.42 (L5), consistent with the pattern observed in low-level autonomic agent tasks. However, the actual completion rates were higher than those for the corresponding low-level tasks, suggesting that including a hierarchical management structure may improve the overall task completion rate. This observation further confirms our previous proposition that attaining Level 3 autonomy is feasible with the existing LLM-based agent management system."}, {"title": "Failure Analysis", "content": "Lastly, we present an in-depth analysis of failed cases in tasks related to L3 (incorrect detection), L4 (incorrect root cause analysis), L5 (mitigation failure), and erroneous high-level task assignment. The detailed results, categorized by failure type and potential improvement directions, are shown in Table 7. These failures stem from issues such as instruction-following errors (e.g., task omission), hallucinations, deficiencies in reasoning capabilities, and insufficient domain knowledge.\nTo address these issues, we propose two primary strategies. First, we can employ a superior base LLM, either a state-of-the-art general LLM or a domain-specific LLM fine-tuned with service maintenance knowledge. Second, we can enhance our existing agent framework by integrating additional modules. These modules might include mechanisms for incorporating domain knowledge using retrieval-augmented generation techniques [35] or the addition of critic agents to mitigate hallucinations and bolster reasoning capabilities."}, {"title": "DISCUSSION", "content": "This study systematically evaluates the feasibility of realizing autonomic computing using LLM-based agents. Our comprehensive experiments, conducted with the microservice Sock Shop in Kubernetes, highlight several key topics that merit further discussion.\nAlternative LLM-based Management System with Agents. We utilize a hierarchical multi-agent system to decompose tasks at different levels, employing the Plan-Execution feedback mechanism in each agent for autonomic task management. Other agent-based solutions could also be considered, such as integrating critic agents to oversee the entire process, implementing a memory mechanism to store short/long-term maintenance history, and including a lifelong self-learning module to continuously enhance agent performance [48, 53, 56", "22": "and metastable failures [21", "16": "and"}]}]}