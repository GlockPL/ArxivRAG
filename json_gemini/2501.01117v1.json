{"title": "Robust COVID-19 Detection from Cough Sounds using Deep Neural Decision\nTree and Forest: A Comprehensive Cross-Datasets Evaluation", "authors": ["Rofiqul Islam", "Nihad Karim Chowdhury", "Muhammad Ashad Kabir"], "abstract": "This research presents a robust approach to classifying COVID-19 cough sounds using cutting-edge machine\nlearning techniques. Leveraging deep neural decision trees and deep neural decision forests, our methodology demon-\nstrates consistent performance across diverse cough sound datasets. We begin with a comprehensive extraction of fea-\ntures to capture a wide range of audio features from individuals, whether COVID-19 positive or negative. To determine\nthe most important features, we use recursive feature elimination along with cross-validation. Bayesian optimization\nfine-tunes hyper-parameters of deep neural decision tree and deep neural decision forest models. Additionally, we\nintegrate the synthetic minority over-sampling technique during training to ensure a balanced representation of pos-\nitive and negative data. Model performance refinement is achieved through threshold optimization, maximizing the\nROC-AUC score. Our approach undergoes a comprehensive evaluation in five datasets: Cambridge (asymptomatic\nand symptomatic), Coswara, COUGHVID, Virufy, and the combined Virufy with the NoCoCoDa dataset. Consis-\ntently outperforming state-of-the-art methods, our proposed approach yields notable AUC scores of 0.97, 0.98, 0.92,\n0.93, 0.99, and 0.99, alongside remarkable precision scores of 1, 1, 0.72, 0.93, 1, and 1 across the respective datasets.\nMerging all datasets into a combined dataset, our method, using a deep neural decision forest classifier, achieves an\naccuracy of 0.97, AUC of 0.97, precision of 0.95, recall of 0.96, F1-score of 0.96, and specificity score of 0.97. Also,\nour study includes a comprehensive cross-datasets analysis, revealing demographic and geographic differences in the\ncough sounds associated with COVID-19. These differences highlight the challenges in transferring learned features\nacross diverse datasets and underscore the potential benefits of dataset integration, improving generalizability and\nenhancing COVID-19 detection from audio signals.", "sections": [{"title": "1. Introduction", "content": "COVID-19 is a highly contagious disease caused by the SARS-CoV-2 virus. It can cause serious illness, death, and\neconomic disruption. The surge in COVID-19 cases burdened medical diagnostic laboratories and healthcare facilities,\nunderscoring the limitations of conventional diagnostic methods, such as clinical examinations, CT scans, and PCR\ntests [1]. Although PCR is a highly accurate method for COVID-19 detection, its cost and time requirements render it\ninaccessible to a significant portion of the population [2]. Furthermore, the utility of CT scan imaging for COVID-19\ndiagnosis is constrained by the potential overlap of symptoms with influenza, delayed detection of lung manifestations\nin the early stages of infection, and restrictions on its use, notably for infants and pregnant women [3, 4].\nStudies have indicated the potential of using the human voice as a primary diagnostic tool for conditions associ-\nated with voice production, including the detection of pathological voice [5], pertussis [6], asthma [7], and respiratory\ndiseases [8]. These instances underscore the promising capabilities of the human voice in diagnosing diseases, partic-\nularly those related to voice production. Researchers have explored acoustic analysis to extend COVID-19 detection\nbeyond voice-related conditions. They have investigated various modalities, including cough sounds, breathing pat-\nterns, and speech or voice, encompassing vowels. The initial two modalities present indicators for the symptoms of\nCOVID-19, namely persistent coughing and breathlessness. We opted for the first modality due to the substantial\nvolume of available cough data. Numerous universities around the world, such as the University of Cambridge in\nthe UK [9], the Massachusetts Institute of Technology (MIT) in the United States [10], and the \u00c9cole Polytechnique\nF\u00e9d\u00e9rale de Lausanne (EPFL) in Switzerland [11], are actively involved in researching the application of machine\nlearning (ML) techniques for the diagnosis of COVID-19 using cough sound data.\nTo expedite research focused on detecting COVID-19 through cough sound analysis, we introduce an approach\nthat leverages deep neural decision trees (DNDT) and deep neural decision forests (DNDF). Initially, audio samples\nare preprocessed to extract various audio characteristics from cough sounds of individuals with confirmed positive and\nnegative results of the COVID-19 test. Subsequently, we use Recursive Feature Elimination with Cross-Validation\n(RFECV) in combination with the Extra-Trees classifier to identify the most critical features for our classifiers. Fol-\nlowing this, Bayesian optimization (BO) is used to fine-tune the hyper-parameters of our proposed method. To enhance\nthe classification performance of our models, we incorporate threshold moving (TM) techniques to ascertain the opti-\nmal threshold value. The dataset prominently displays an imbalance, notably with a limited representation of positive\ninstances for COVID-19, potentially posing a detrimental effect on the performance of the ML classifier. In response\nto this imbalance, we have implemented the synthetic minority over-sampling technique (SMOTE) [12] during the\ntraining process. This strategic inclusion aims to rectify the dataset imbalance, thereby enhancing the performance of\nthe ML classifier.\nThe evaluation of our classification models extends across multiple datasets, including Cambridge [9], Virufy [10],\nCOUGHVID [11], Coswara [13], and Virufy merged with NoCoCoDa [14]. Additionally, we conduct a comprehen-\nsive cross-datasets study (CDS), in which our proposed approach is first trained on one dataset and then evaluated"}, {"title": "2. Related work", "content": "Numerous researchers have concentrated on detecting COVID-19 using sounds, such as coughs, breath, voice,\nand speech [15, 16, 17, 18, 19, 20, 21, 22, 23]. In this study, we focus on research related to cough sounds. Many re-\nsearchers have invested significant efforts in developing datasets, including Cambridge [9], Virufy [10], COUGHVID\n[11], Coswara [13], Novel Coronavirus Cough Database (NoCoCoDa) [14], Cough against COVID [24], AI4COVID-\n19 [25], MIT-Covid-19 [26], IATos [27], Sarcos [28], ComParE [29], COVID-19 Cough [30], COVID-19 Sounds\n[31], and DiCOVA Challenge [32]. Next, we investigate studies exclusively focusing on using cough sounds to\nclassify COVID-19. We explore these studies' methodologies, findings, and implications to gain a comprehensive\nunderstanding of the role and effectiveness of cough-based approaches in COVID-19 detection.\nThere are two classifications of COVID-19 datasets: publicly available and inaccessible. Publicly available\ndatasets include Virufy [10], COUGHVID [11], Coswara [13], IATos [27], and others. Datasets that are not publicly\navailable include Cambridge [9], Novel Coronavirus Cough Database (NoCoCoDa) [14], Cough against COVID [24],\nAI4COVID-19 [25], MIT-Covid-19 [26], Sarcos [28], ComParE [29], COVID-19 Cough [30], COVID-19 Sounds\n[31], DICOVA Challenge [32], and others.\nA comparison of our research and previous studies on COVID-19 identification using cough sound analysis is\nshown in Table 1. In the merged dataset (M) and cross-datasets study (CDS), a '-' symbol indicates that only one\ndataset is used. We perform a comprehensive CDS, where we initially train our proposed method on a single dataset\nand then evaluate its performance on several other datasets. We repeat this procedure by training the model on each\nremaining dataset one by one and assessing its results on the other datasets. Moreover, a '' signifies that methods\nsuch as merged dataset (M), cross-dataset study (CDS), feature selection (FS), hyper-parameter tuning (HT), and\nthreshold moving (TM) are applied, while a 'X' indicates they are not implemented. It is worth noting that some\nresearchers limited their analyses to specific datasets, such as Cambridge [9, 33, 34, 35], Coswara [36, 37, 38, 39, 40],\nCOUGHVID [41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54], and Virufy [55, 56, 57, 58, 59, 60, 61]. A\nfew others use only their proprietary datasets [25, 62, 63, 64, 65, 66]. Furthermore, in addition to using well-known\ndatasets like Cambridge, Coswara, COUGHVID, and Virufy, some research included the use of other publicly and\nnon-publicly accessible datasets [24, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80] to validate the robustness\nof their classification models.\nNumerous studies have evaluated the effectiveness of their suggested techniques using several datasets. Of these,\na few studies used two datasets [81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99], while\nothers evaluated from three [10, 100, 101, 102, 103, 104, 105, 106, 107] or four datasets [108, 109, 110, 111, 112].\nNevertheless, our work is noteworthy because it is the only one that fully incorporates each of the five well-known\nCOVID-19 cough datasets: Cambridge, Coswara, COUGHVID, Virufy, and Virufy merged with NoCoCoDa. Also,\nsome researchers choose to take a combined strategy, combining two or more datasets to strengthen the validity of\ntheir research. Among these studies, several combined two datasets [83, 87, 89, 95, 108], while several investigated"}, {"title": "3. Research questions", "content": "In our pursuit of advancing the cutting-edge in cough-based COVID-19 detection, we present a ML-based ar-\nchitecture tailored to analyze cough sounds. Furthermore, our research focuses on pinpointing the most successful\ntechniques for precise COVID-19 detection using cough data. As a result, we have devised a series of research\nquestions (RQs) dedicated to the field of cough-based COVID-19 detection:\n\u2022 RQ1: How do different training strategies impact the classification performance of detecting COVID-19 from\ncough sounds?\nWe provide various training strategies to enhance the effectiveness of the proposed method. The significance\nof these training techniques is elucidated in Section 4.7, with a detailed analysis presented in Section 5.2.\n\u2022 RQ2: Should the construction of detection models consider any demographic or geographic variations in cough\nsounds associated with COVID-19?\nYes, demographic and geographic variations in cough sounds related to COVID-19 exist. These variations\nshould be considered when developing detection models to ensure they are effective and generalizable across\ndifferent populations. We explore the specifics of this cross-datasets investigation and its resulting findings in\nSection 5.4."}, {"title": "4. Methodology", "content": "Inspired by the advancements in ML-based audio applications, we have created a comprehensive ML framework\ncapable of taking cough samples and making direct predictions of binary classification labels, hinting at the potential"}, {"title": "4.1. Dataset description", "content": "In this section, we describe the datasets we use to validate and evaluate the effectiveness of our COVID-19 clas-\nsification models based on cough sound analysis. We use five different datasets in our experiment: Cambridge [9],"}, {"title": "4.1.1. Cambridge dataset", "content": "The University of Cambridge has developed an online platform and mobile app that allows people to submit\nrecordings of their coughs, inhalations, and voices while reciting a specific phrase. The Cambridge dataset [9] is\ndivided into two groups, asymptomatic and symptomatic, to distinguish between people who have tested positive\nfor COVID-19 and those who have not. We acknowledge the limitations imposed by the authors of the Cambridge\ndataset, which is only available under a bilateral legal agreement for research purposes and not for commercial use.\n\u2022 Asymptomatic: To distinguish between people who have tested positive for COVID-19 and those who have\ntested negative, the Cambridge Asymptomatic dataset contains 141 cough samples from COVID-19 positive\npeople and 298 cough samples from people who have tested negative for COVID-19. The people in the dataset\nhave no notable medical conditions, do not smoke, and are asymptomatic (show no symptoms).\n\u2022 Symptomatic: To distinguish between people who have tested positive for COVID-19 and people who have\ntested negative, both with a cough but no other medical conditions or smoking history, the Cambridge Symp-\ntomatic dataset contains 54 cough samples from COVID-19 positive people and 32 cough samples from people\nwho have tested negative for COVID-19."}, {"title": "4.1.2. Coswara dataset", "content": "The Coswara dataset [13] is a publicly available dataset of cough samples developed by the Coswara project, a\ncollaboration between the Indian Institute of Science and the Indian Institute of Technology Palakkad. It was collected"}, {"title": "4.1.3. COUGHVID dataset", "content": "The COUGHVID dataset [11] was collected by researchers at the Embedded System Laboratory (ESL) in Switzer-\nland. We preprocessed and labeled the samples into two groups: those from healthy individuals (COVID-19 negative)\nand those from individuals with notable cough variations (COVID-19 positive). The COUGHVID dataset contains\n1,360 cough samples, with 680 samples from COVID-19 positive individuals and 680 samples from COVID-19 neg-\native individuals."}, {"title": "4.1.4. Virufy dataset", "content": "The Virufy COVID-19 open cough dataset [10] is the first publicly available dataset of cough sounds from COVID-\n19 patients. The sounds were recorded in a hospital with the patient's consent, under the supervision of a physician,\nand in accordance with standard operating procedures. The Virufy dataset contains 121 cough samples from 16\npatients, with 48 samples from COVID-19 positive patients and 73 samples from COVID-19 negative patients."}, {"title": "4.1.5. NoCoCoDa dataset", "content": "The NoCoCoDa dataset [14] is a collection of cough sounds from COVID-19 patients recorded during interviews\nand news programs. It contains 73 cough sounds from 10 participants who attended 13 interviews. To provide a more\ncomprehensive dataset for experiments, we combine the NoCoCoDa dataset with the Virufy dataset, which contains\ncough samples from both COVID-19 positive and negative people. The combined dataset contains 194 samples, with\n121 from COVID-19 positive people and 73 from COVID-19 negative people."}, {"title": "4.1.6. Combined dataset", "content": "We consolidate the Cambridge (both asymptomatic and symptomatic), Coswara, COUGHVID, Virufy, and No-\nCoCoDa datasets to form a combined dataset. This unified dataset comprises 3,398 samples, including 1,181 cough\nsamples from COVID-19-positive individuals and 2,217 cough samples from COVID-19-negative individuals."}, {"title": "4.2. Feature extraction", "content": "To maintain consistency with a common standard in audio applications, we capture the acoustic signal used for\nfeature extraction at a frequency of 22 kHz. We then compute five spectral feature types from the sampled audio:\nMFCCs, Mel-Scaled Spectrogram, Tonal centroid, Chromagram, and Spectral contrast. Figure 2 presents an overview\nof our feature extraction process. The Python library librosa [118] is used to extract them."}, {"title": "4.2.1. Mel-Frequency Cepstral Coefficients (MFCCs)", "content": "MFCCs have demonstrated their effectiveness in distinguishing between dry and wet coughs [119] and are well-\nregarded as valuable spectral features for audio analysis. From the audio signal, we extract 40 MFCC features. The\nprocess of extracting MFCCs encompasses several key steps. Initially, audio signals are divided into frames, each\nsubject to a windowing function to mitigate noise from sudden changes at its start and end. Subsequently, the Fast\nFourier Transform (FFT) is applied to compute the power spectrum of each frame post-windowing. This power\nspectrum is further manipulated using a filter bank design based on the Mel scale, as depicted in Equation 1, to obtain\nMel-scaled filters from the original frequency (f). Ultimately, the Discrete Cosine Transform (DCT) is used to derive\na set of MFCCs (MFCC coefficients) for each frame from the audio input, following the transformation of the power\nspectrum into a logarithmic scale.\n$f_{mel} = 2595log_{10}(1 + \\frac{f}{700})$\n(1)"}, {"title": "4.2.2. Mel-Scaled Spectrogram", "content": "The Mel-Scaled Spectrogram is a widely adopted technique in ML for audio analysis, serving as a prevalent\nmethod for feature extraction from audio data. This process involves converting the power spectrogram into the Mel\nscale domain using a set of Mel filters. To generate a Mel-scaled Spectrogram, the initial step is to divide the signal\ninto small frames using windowing. A window size of 2048 samples and a hop length of 512 samples are typically set"}, {"title": "4.2.3. Tonal centroid", "content": "The tonal centroid, a feature used in audio analysis, is created by projecting a 12-bin chroma vector onto a six-\ndimensional vector using a transformation matrix, as denoted by Equation 2 [120].\n$I_n(d) = \\frac{1}{||C_n||} \\sum_{l=0}^{11} \\phi(d, l)c_n(l), 0 \\leq d \\leq 5, 0 \\leq l \\leq 11$\n(2)\nwhere $I_n$ is the 6-dimensional tonal centroid vector, computed by multiplying the transformation matrix $\\phi$ by the\nchroma vector c during the specified time frame n and dividing the resulting vector by the L1-norm of the chroma\nvector to ensure proper scaling of the values."}, {"title": "4.2.4. Chromagram", "content": "In the realm of ML applied to audio analysis, chromagrams serve as fundamental input features. We extracted\n12 chromagram features from an audio signal. Generating a chromagram from an acoustic signal involves using the\nfrequency power spectrum derived from the Short-Time Fourier Transform (STFT). The STFT is computed by using a\nsliding window over the audio signal and performing the Fourier transform for each window, effectively representing\nthe audio stream as a time-frequency wave. Subsequently, the power spectrum is derived by squaring the magnitudes\nof the STFT coefficients.\nThe chromagram itself is derived from the power spectrum of an acoustic signal by mapping the frequency bins.\nIn this context, a specific hop length of 512 and a window size of 2048 are chosen, creating 12 chroma bins. Finally,\nthe feature vector is compiled by obtaining the normalized energy of each chroma bin for every frame in the audio\nsignal."}, {"title": "4.2.5. Spectral contrast", "content": "Spectral contrast features find application in ML for audio analysis. From the audio signal, we extract seven\nspectral contrast features. The procedure for deriving spectral contrast features from an audio signal encompasses\nseveral sequential stages. Firstly, a Fast Fourier Transform (FFT) is applied to the digital audio clips, capturing the\nspectral distribution of the audio signal. Subsequently, the frequency spectrum is partitioned into a collection of sub-\nbands using octave band filters. The number of these sub-frequency bands is standardized at 6. The evaluation of\nspectral valleys, peaks, and their disparities is performed within each sub-band, as described in Equations 3, 4 and\n5 [121]. The initial spectral contrast values are then transformed into a logarithmic representation. Lastly, using a\nKarhunen-Loeve transform, the Log-frequency contrast values are projected into an orthogonal subspace.\n$Peakk = 108 log \\frac{1}{\\alpha N} \\sum_{i=1}^{\\alpha N} X_{k,i}$\n(3)\n$Valleyk = 108 log \\frac{1}{\\alpha N} \\sum_{i=1}^{\\alpha N} X_{k,N-i+1}$\n(4)\n$SCk = Peakk - Valleyk$\n(5)\nwhere N represents the overall count within the k-th sub-frequency band, k ranging from 1 to 6, and $\\alpha$ is invariant\nwith a range of 0.02 to 0.2."}, {"title": "4.3. Feature selection", "content": "We extract 193 features from each audio signal, 40 MFCCs, 128 mel-scaled spectrogram, 6 tonal centroid, 12\nchromagram, and 7 spectral contrast features. However, it is worth noting that not all of these features are optimal.\nOne method that is often used in ML is feature selection. Combining the Recursive Feature Elimination with Cross-\nValidation (RFECV) technique [115] and the Extra-Trees Classifier is an efficient method for feature dimensionality\nreduction (optimal feature selection). Figure 3 provides an overview of the RFECV method with the Extra Trees clas-\nsifier for selecting optimal features. The most appropriate features to be chosen automatically are found using RFECV,\nwhich uses cross-validation and feature significance weights. Less significant characteristics are removed iteratively,\nand cross-validation is used to assess the model's performance. Following the use of the RFECV technique and\nExtra-Trees Classifier, we obtain optimal features of 71, 182, 33, 172, 46, and 188 for the Cambridge asymptomatic,\nCambridge symptomatic, Coswara, COUGHVID, Virufy, and Virufy merged with NoCoCoDa datasets, respectively.\nWe use the Extra-Trees estimator to obtain information about each feature's significance. This method's main objec-\ntive is to use the RFECV and Extra-Trees classifier to examine feature importance and minimize feature dimensions."}, {"title": "4.4. Hyper-parameter tuning", "content": "Hyper-parameters control the learning process of an ML model, such as the number of trees, depth, used features\nrate, and epochs. Hyper-parameter tuning is finding the best values for these hyper-parameters to optimize the model's\nperformance on a given dataset. Bayesian Optimization (BO) is a popular and effective technique for hyper-parameter\ntuning. It works by building a probabilistic model of the relationship between the hyper-parameters and the model's\nperformance. It then uses this model to select the next set of hyper-parameters to find the values that lead to the\nbest performance. BO uses a model to make informed decisions about which hyper-parameter values to test next,\nleveraging past results to make more efficient choices. This approach requires fewer iterations to find the optimal\nhyper-parameter combination than the more brute-force methods of grid search (GS) and random search (RS) [117].\nThis efficiency is a key advantage of BO in hyper-parameter tuning. The performance of DNDT and DNDF for\nclassification is significantly impacted by hyper-parameters. In BO, we define hyper-parameter space for hyper-\nparameters of our classifiers. Extracted input features and their labels are given to the Bayesian Optimization function\nto obtain the most effective hyper-parameter values. The default hyper-parameters used for all datasets in both DNDT\nand DNDF classifiers are presented in Table 3. Additionally, Table 4 reports the optimized hyper-parameters for\nvarious datasets in both DNDT and DNDF classifiers."}, {"title": "4.5. Classification models", "content": ""}, {"title": "4.5.1. Deep neural decision tree", "content": "The deep neural decision tree (DNDT) is a classifier structured as a tree [122], encompassing both decision and\nprediction nodes. Decision nodes, positioned within the tree but not at its leaves, serve as points where the tree\nassesses data features or conditions to make determinations. The decisions at each node guide the path a sample takes\nthrough the tree. On the other hand, prediction nodes are the leaf nodes of the tree, where the final prediction is\ngenerated. These nodes serve as the terminal points for predictions. Each prediction node corresponds to a specific\nclass or outcome the classifier aims to predict. To classify a sample, the DNDT guides it to a leaf node, using a\nprobability distribution to make the final prediction. The final prediction for a sample is determined by Equation 6,\n$Pr[y|x, \\theta, \\pi] = \\sum_{l \\in L} \\pi_{ly} \\mu_l(x|\\theta)$\n(6)\nWhere $\\pi_{ly}$ is the likelihood of a sample arriving at leaf node l to get placed in class y and routing function $\\mu_l(x|\\theta)$ is\nthe likelihood of a sample x will arrive at leaf node l.\nTo explicitly define the routing function, we introduce two binary relations determined by the tree's structure:\nl < n, which holds true if l is part of the left subtree of node n, and n > l, which holds true if l is part of the right\nsubtree of node n. Using these relations, $\\mu_l$ is expressed as shown in Equation 7.\n$\\mu_l(x) = \\prod_{n \\in N} [d_n(x; \\Theta)]^{\\mathbb{I}_{l < n}} [\\overline{d_n(x; \\Theta)}]^{\\mathbb{I}_{n < l}}$\n(7)\nWhere $\\mathbb{I}_P$ denotes an indicator function that activates when the condition P is satisfied, and $d_n$ represents the decision\noutput at the n-th node, calculated as shown in Equation 8, indicating a split or routing decision.\n$d_n(x; \\Theta) = \\sigma(f_n(x; \\Theta))$\n(8)\nThe sigmoid function $\\sigma$ outputs a value between 0 and 1, used for binary decisions, while $f_n$ is a learned function\nat the n-th node, often derived from a fully connected layer, that processes input x to determine the split decision.\nAdditionally, $\\overline{d_n(x; \\Theta)}$ is the complement of $d_n(x; \\Theta)$, calculated as 1 - $d_n(x; \\Theta)$, representing the alternative path or\ndecision when the initial $d_n$ decision is not chosen."}, {"title": "4.5.2. Deep neural decision forest", "content": "The deep neural decision forest (DNDF) [122] is a classifier consisting of multiple DNDTs trained simultaneously.\nThe DNDF produces its final output by averaging the individual outputs from each of the trees within the forest. The\noutput of the DNDF is represented by Equation 9.\n$P_F[y|x] = \\frac{1}{k} \\sum_{h=1}^k P_h[y|x]$\n(9)"}, {"title": "4.6. Threshold moving", "content": "In binary classification, the threshold used to determine class labels based on predicted probabilities is crucial for\nthe model's performance. To improve class label assignment, we use the threshold moving technique, as relying on\nthe default threshold of 0.50 often leads to suboptimal results. With this method, the ROC-AUC score is considered\nwhile determining the optimal threshold for a binary classifier. In medicine, the ROC-AUC score is a commonly used\nassessment metric, particularly for evaluating the effectiveness of diagnostic procedures [123]. ROC-AUC score-based\nthreshold optimization is achieved by cross-validation tests. We calculate ROC-AUC scores over a threshold value\nrange of 0.1 to 1, using 0.001 increments. The best threshold is then determined by selecting the one that produced\nthe highest ROC-AUC score."}, {"title": "4.7. Training strategies", "content": "We introduce five strategies to evaluate the effectiveness of different components of our proposed method: strategy\n1, strategy 2, strategy 3, strategy 4, and strategy 5. Table 5 shows the combinations of different training strategies used\nin each strategy. Strategy 1 exclusively relies on our trained classifiers. In strategy 2, we only use the threshold moving\ntechnique (to select the optimal threshold based on the ROC-AUC score) with a trained classifier. In strategy 3, we\nuse both the threshold moving technique and the feature dimension reduction technique (to determine the key features\nusing the RFECV method and Extra-Trees classifier) with a trained classifier. In strategy 4, we use the threshold\nmoving technique, optimal feature selection using the RFECV method and the Extra-Trees classifier, and Bayesian\nOptimization (to select the best hyper-parameters). In strategy 5, we use the threshold moving technique, optimize\nfeature selection using the RFECV method, use Bayesian Optimization, and apply SMOTE to balance the data of the\nminority class in an imbalanced dataset."}, {"title": "5. Results and discussion", "content": "In this section, we present the results of our experiments on identifying COVID-19 by analyzing cough sounds.\nWe first outline the evaluation metrics used to assess the performance of our proposed methods. Next, we present the\nclassification performance of four strategies, which helps us to select the best strategy based on performance. Then,\nwe describe the optimal feature selection techniques for selecting the most important features. Finally, we evaluate\nthe effectiveness of our proposed method using state-of-the-art methods on different datasets."}, {"title": "5.1. Evaluation metrics", "content": "In our experimental evaluation, we use 10-fold stratified cross-validation to evaluate the performance using six\nstandard evaluation metrics: ROC-AUC, Accuracy (Acc.), Precision, Recall/Sensitivity, Specificity (Spec.), and F1\nscore. The definitions of Precision, Recall/Sensitivity, Accuracy, Specificity, and F1 score are provided below:\n$Accuracy = \\frac{TP + TN}{TP + FP + FN + TN}$\n$Precision = \\frac{TP}{TP + FP}$\n$Recall/Sensitivity = \\frac{TP}{TP + FN}$\n$Specificity = \\frac{TN}{TN + FP}$\n$F1 Score = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}$\nHere, True Positive, False Positive, True Negative and False Negative are each represented as TP, FP, TN and FN,\nrespectively. AUC, which stands for \u201cArea Under the Curve\u201d, signifies the area under the ROC curve, a probability\ncurve. ROC, or the \"Receiver Operating Characteristic\", is determined by the TPR (true positive rate) as a fraction of\nthe FPR (false positive rate).\n$TPR = \\frac{TP}{TP + FN}$\n$FPR = \\frac{FP}{FP+ TN}$"}, {"title": "5.2. Classification performance of five strategies", "content": "All five strategies use DNDT and DNDF classifiers, and strategy 2 through strategy 5 incorporate the threshold\nmoving technique. Strategies 3, 4, and 5 implement the RFECV method, using the Extra-Trees classifier to identify\nthe most relevant features. Strategies 4 and 5 further integrate Bayesian Optimization to fine-tune Hyper-parameters\nfor the trained classifiers. Additionally, in strategy 5, SMOTE is applied during the training phase to address data\nimbalance by generating synthetic samples for both positive and negative instances of cough related to COVID-19.\nThe AUC-based classification performance of all five strategies is outlined in Table 6."}, {"title": "5.3. Comparative analysis of performance with state-of-the-art methods", "content": "Table 8 provides a comparative analysis of our innovative approaches for COVID-19 diagnosis from cough sam-\nples in contrast to contemporary methods. Our methodologies encompass several pivotal components", "9": "Soltanian and\nBorna [55", "109": ".", "categories": "Asymptomatic and Symptomatic. Studies that fail to adopt these specific categories\nor use alternative dataset categorizations are also excluded from the comparison.\nOur proposed method demonstrates constant superiority over state-of-the-art techniques on a wide range of datasets,\nconfirming its strong performance in COVID-19 diagnosis. On the Cambridge Asymptomatic dataset, Aytekin et al.\n[81"}]}