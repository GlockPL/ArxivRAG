{"title": "HDR: Talking to Machines: do you read me?", "authors": ["Lina Mar\u00eda ROJAS BARAHONA"], "abstract": null, "sections": [{"title": "Introduction", "content": "In this dissertation I would like to guide the reader to the research on dialogue but more precisely the research I have conducted during my career since my PhD thesis. Starting from modular architectures with machine learning/deep learning and reinforcement learning to end-to-end deep neural networks. Besides my work as research associate, I also present the work I have supervised in the last years. I proposed four PhD thesis Conventions industrielles de formation par la recherche (CIFRE) that Orange accepted to fund. Therefore, I could co-supervise four PhD candidates: Timothy Garwood supervised by Claire Gardent at CNRS, Thibault Cordier supervised by Fabrice Lefevre at the University of Avignon, Sebastien Montella supervised by Alexis Nasr at the university of Aix-Marseille, L\u00e9o Jacqmin supervised by Benoit Favre at the University of Aix-Marseille. During 5 years I was head of the industrial research project on dialogue, Dialogue in NAtural Language (DIANA), which gave me the opportunity of supervising the work of the young researcher Quentin Brabant, other experimented researchers, a developer as well as students in internship and apprenticeship. The deliverables of DIANA project gather open-sourced datasets and neural models as well as scientific publications.\nI review briefly the state of the art and highlight the open research problems on conversational agents in Chapter 2. Afterwards, I present my contribution to Task-Oriented Dialogues (TOD) in"}, {"title": "Preliminary Approaches", "content": "of the world. Desires, in turn, represent how the agent would like the world to be in the future;\nwhile intentions are the structured plan the agent has decided to perform. The agent interacts\nwith the world by performing actions and by perceiving aspects of it, including changes which\nresult from its own actions. Perceptions will influence the beliefs of the agent, while actions\nmay change aspects of the world. This model was at the origin of modern Natural Language\nUnderstanding (NLU), in which the aim is to detect user's intentions or intents. However, the term\nintent usually means a dialogue-act with a set of concepts or a combination of them in semantic\nlabels.\nAll these inherent characteristics in natural dialogue make implementing automated systems a\nvery difficult task."}, {"title": "Preliminary Approaches", "content": "A range of approaches emerged in the history of Dialogue Systems (DSs), they were classified in conformity with their Dialog Manager (DM) (Allen et al., 2001; Churcher, 1997). According to this classification, ordered by increasing complexity, the simplest of these is the finite-state scripts, also called dialogue grammars, followed by slot-filling, plan-based and agent-based models. In a finite-state script the dialogue is represented as a script of prompts for the user. In slot-filling, questions are asked in order to enable the system to fill the necessary slots to perform a task. Conversely, plan-based theories claim that utterances infer acts that are part of a plan, thus, the system tries to identify users' underlying plan, collaborates in accomplishing that plan and responds appropriately. Agent-based models are at the highest level of complexity. They consider planning, executing, and monitoring operations in a dynamically changing world, possibly involving multi-modality. Examples of agent-based models are: the logic-based approaches, which uses inference engines of a higher complexity that in some cases are semi-decidable (in some cases the system will never halt), as well as reinforcement learning approaches, which need a large number of interactions to converge."}, {"title": "Task Oriented Dialogue Systems", "content": "Conversational agents have gained great interest in both academy and industry in the last decades.\nTypically, available conversational agents have been designed for the task of information-seeking.\nThese agents act as a natural language interface to a database. First, the system tries to fill the\nconstrains to query a database by inquiring the user. Then it retrieves the items that fulfill users'\nconstraints and finally it communicates the results to the user in natural language. For instance, a\nperson could call the system to check train timetables, she would provide the departure and arrival\ncity as well as the departure date and time. Then, the system would inform her about the available\ntrains.\nTOD main goal is to complete a task in collaboration with the user (Pieraccini et al., 1992;\nYoung, 2002; Rieser and Lemon, 2011; Young et al., 2013a). Examples of tasks are to search\ninformation about a restaurant, to reserve a hotel or to buy train tickets."}, {"title": "Definitions", "content": "As introduced in Section 2.1, dialogue-acts or communicative acts are the actions performed by the speakers when uttering sentences (e.g. Informing, Asking, Confirming, Greeting, etc.) (Austin, 1975b). A domain is formally defined in an ontology as a list of slots with their valid values. The most common task, the information seeking task, is usually modelled as a slot-filling data-query problem in which the system requests constraints to the user and proposes items that fulfil those constraints in a database. The action or intention is composed by a predicate: the dialogue-act, and a set of arguments: the slot-value pairs. For instance, let us suppose the user has uttered \u201cI would like a restaurant in the center of town please\u201d, this will be translated in the semantic form: inform(type = restaurant, area = center). This semantic representation is usually called flat-semantics because there is not hierarchy in the concepts of the ontology."}, {"title": "Statistical Dialogue Systems", "content": "One approach to automatic dialogue is to use Reinforcement Learning (RL) to select the system's action (Levin et al., 2000; Litman et al., 2000). As in a Chess game, a dialogue involves two players, in which each of them takes turns to play. The system should decide its move by considering the environment (the other player's moves) and the rewards is either win or lose the game. Dialogue is then formulated as an optimisation problem, in which the environment is the user action and the user's feedback is the reward. The final goal of the system is then to maximise the accumulated reward at long run (Rieser and Lemon, 2011). The optimal policy,\\( \\pi \\), is a function that takes as argument the current state \\( s \\) and returns the optimal action \\( a \\).\nMarkov decision process\nDialogue can be formalised as a Markov Decision Process (MDP), which is a tuple \\( M = (S, A, T, \\gamma, R) \\) where:\n\\begin{itemize}\n    \\item S: A set of possible states that represent the dynamic environment.\n    \\item A: A set of possible actions.\n    \\item T: \\( S \\times A \\times S \\rightarrow [0, 1] \\) is a transition probability function. For any action \\( a \\in A(s) \\) taken\n    in a state \\( s \\in S \\), the probability of transiting to the next state \\( s' \\) is given by \\( T(s, s') \\).\n    \\item \\( \\gamma \\): A discounting factor in the range of [0, 1], which controls the prediction horizon of the\n    algorithm.\n    \\item R: The reward function that specifies the reward gained at every state. It contains the\n    information that guides the agent towards the goal. \\( R \\) is a function of the state that is\n    bounded in absolute value by \\( R_{max} \\).\n\\end{itemize}\nA stationary policy is a map \\( \\pi : S \\rightarrow A \\) and the discounted infinite-horizon expected reward\nfor starting in state s and following policy \\( \\pi \\) thereafter is given by the value function \\( V^{\\pi}(s) \\) that\nsatisfies the following Bellman Equation:\n\\[\nV^*(s) = R(s) + \\sum_{s'} T(s, \\pi(s), s')V^* (s')\n\\tag{2.1}\n\\]"}, {"title": "Task Oriented Dialogue Systems", "content": "The discounted infinite-horizon expected reward for starting in state s, taking action a and follow-ing policy \\( \\pi \\) thereafter is given by the Q-function \\( Q^{\\pi}(s, a) \\) that satisfies the following equation:\n\\[\nQ^{\\pi} (s, a) = R(s) + \\gamma \\sum_{s'} T(s, a, s')V^{\\pi} (s')\n\\tag{2.2}\n\\]\nA policy \\( \\pi \\) is optimal in M if, for all \\( s \\in S \\):\n\\[\n\\pi(s) = \\arg\\max_{a\\in A} Q^{\\pi} (s, a)\n\\tag{2.3}\n\\]\nLikewise \\( Q^*(s, a, R) \\) is the optimal Q-function of the optimal policy \\( \\pi^* \\) for a known reward func-tion R.\nPartially Observable Markov decision process\nDialogues can be modelled as an optimisation problem with Partially Observable Markov decision process (POMDP)s. It simulates the inherent dynamic behaviour of human conversations while deals with the uncertainty of spoken language (Roy et al., 2000; Williams and Young, 2007; Young et al., 2013b).\nA POMDP can be seen as a continuous-space Markov decision process (MDP) in terms of policy optimisation where the states are the belief states, which is partially observable. POMDPS have been proposed for spoken dialogue systems because the system is never sure about the user beliefs because of speech recognition errors due to noisy or spoken language disfluencies and hesitations (Roy et al., 2000; Young et al., 2013a). Since the state is uncertain, it is called the belief state b(s). An example of a POMDP dialogue system is presented in Section 2.3.2. The task of predicting the b(s) at a given time t is known as the task of Dialogue State Tracking (DST). The policy learning algorithm receives as input the b(s) and returns the optimal policy \\( \\pi^* \\) and a given time.\nThe belief state b\u2081 is a vector encoding a probability distribution over the different goals, dialogue acts and concepts that are discussed in the dialogue. In the same way, the dialogue action at is a vector encoding a probability distribution over the possible agent dialogue actions.\nHierarchical Reinforcement Learning\nMDP models have been proven to be inefficient for solving complex tasks. These models have trouble overcoming the cold start problem and/or suffer from the curse of dimensionality (Barto and Mahadevan, 2003). This pattern was also observed with models proposed recently (Mnih et al., 2013; Duan et al., 2016). To overcome this issue, (Parr and Russell, 1998) proposed to specify a hierarchy of tasks and to reuse parts of the state space across many sub-tasks, which can greatly improve both learning speed and agent performance.\nThe notion of temporal abstraction, in which a policy can be decomposed into sub-policies by calling temporally extended sub-tasks was first proposed by (Sutton et al., 1999). In order to consider hierarchical architectures with temporally extension, we have to generalise the MDP to the semi-Markov Decision Process (SMDP) (Parr and Russell, 1998) where actions can take a variable amount of time to complete. This creates a division between primitive actions that span over only one action and composite actions that involve an execution of a sequence of primitive"}, {"title": "Introduction", "content": "As introduced in Section 2.3, task-oriented dialogue systems search to accomplish a task.\nThis task can be for instance, information seeking, in which the system search for items in a\ndatabase according to the constraints obtained through natural language interaction. Thus, these\nconstrains can be given by the user (\u2018I am looking for a restaurant') or can be enquired by the\nsystem ('which price range?', 'where about?'). Once the desired items are retrieved, the system\ninforms the results back to the user. This kind of dialogue has rich interactions with dialogue\nacts such as: informing, requesting, clarifying, rectifying. State-of-the-art systems are centred on\nthe information seeking task for a variety of domains: hotels, restaurants, touristic attractions,\ntrains, flights, taxis, etc. Unfortunately, these dialogues are very specific and have difficulties to\ngeneralise to new domains and to more complex tasks (i.e. beyond information seeking).\nI present in this Chapter my contributions to TOD dialogues (Figure 2.1). I start by presenting\nmy work on NLU in Section 3.1 and on Dialogue Management in Section 3.2. My contributions\nto NLU concerns the definition of an annotation schema for the French corpus EmoSpeech, in\nwhich 12 distinct dialogues were integrated in a serious game. The dialogues involved various\ncharacters representing the system and the player and they were triggered at different levels of\nthe game quest. Dialogues were modelled as an information seeking task, in which the system\nis always providing information to the player. The set of dialogue acts and goals are introduced\nin Section 3.1, as well as the models that were trained. Data augmentation through paraphrases\nvia back translation, dictionaries, lexical resources and distributional semantics is also presented"}, {"title": "NLU in a Serious Game", "content": "Machine Learning (ML) approaches such as logistic regression classifiers and conditional random fields were the state-of-the art back in 2011. I could collect a corpus through Wizard-of-Oz experiments, define an annotation scheme and train Logistic Regression (LR) and Support Vector Machines (SVM) multi-class classifiers for the task of NLU. These models were integrated within distinct dialogues in a serious game. I will start by describing briefly this work that gave origin to four publications (Rojas-Barahona et al., 2012b,c; Rojas-Barahona and Gardent, 2012; Gardent and Rojas-Barahona, 2013).\nThe serious game is a multiplayer quest where the players (3 teenagers) seek to build a video game joystick in order to free their uncle trapped in the game. To build this joystick, the players must explore a factory and achieve 17 mandatory goals (find the plans, get the appropriate mould, retrieve some raw material from the storing shed, etc). In addition, they can increase their score by achieving optional goals which, when reached, provide them with extra information about the industry (therefore increasing their knowledge). In total, the players can achieve up to 28 goals by conducting 12 separate subdialogs in various parts of the virtual world. That is, dialogs in the game are long dialogs involving multiple players in various settings."}, {"title": "Language Understanding", "content": "3.2). Taking into account the game goals, the total number of categories to be learned is 27.\nWhen learning on subdialogs, the number of categories to be learned is smaller but so is the size\nof the training set. The features for the machine learning models were bag of words, in which\nstop words were filtered out, utterances were deaccented and converted to lower-case. In addition,\nwe experimented with various context length using as features the 0 to 4 previous dialogue acts.\nSubdialog identifiers were also used when training the classifier on the whole dialogue. More\ndetails are given in (Rojas-Barahona et al., 2012a).\nWe also experimented using tf*idf filtering to limit the impact of frequent uninformative\nwords. Moreover, we experimented penalising those categories with more training instances,\nsince the data was highly skewed. Dialogue acts that relate to optional goals were often not fol-\nlowed up by the players resulting in data sparseness."}, {"title": "Results", "content": "Table 3.3 shows the results for the 6 main configurations: training on the whole dialog or on sub-dialogs, with and without tf*idf filtering and using LR, SVM or SVM with penalisation. The best results are obtained using the LR classifier on the whole dataset with tf*idf filtering. Penalising improved slightly the accuracy of the SVM when classifing without tf*idf filtering or when having a reduced context (0 or 2 previous acts in Table 3.4).\nImpact of the tf*idf filtering. Globally, the tf*idf filtering has a positive impact leading to an increase in accuracy ranging from 2.81 to 11.52 points. For the SVM classifier, the tf*idf filtering consistently lead to better results. However, for the LR classifier the filtering adversely impacts performance on short subdialogs (6 and 7), where one unique goal is being discussed. We conjecture that for these cases, the tf*idf filtering removed words which helped the classifier distinguish between turns about the unique goal from other turns. SVM with penalisation yields worse results with the tf*idf filtering than without, thus suggesting overfitting. In the next section we present how can we exploiting synonyms to improve generalisation.\nImpact of contextual features. Having a notion of context is crucial for correctly interpreting dialog acts. As mentioned above, we use the dialog acts of the previous turns to model context. However the further back we look into the previous turns, the more features there will be to train on. In other words, depending on the number of previous turns considered, the data to learn from will be more or less sparse. We experimented with 3 setups: a null context, the dialog acts of the two previous turns and the dialog acts of the four previous acts. Table 3.4 shows the results."}, {"title": "Language Understanding", "content": "and the test data; augmenting the training data with automatically acquired paraphrases; and substituting unknown words with synonyms or its distributional neighbours at run-time.\nFor Lemmatisation, we used the French version of Treetagger\u00b3 to lemmatise both the training and the test data. Lemmas without any filtering were used to train classifiers. We then compare performance with and without lemmatisation. As we shall see, the lemma and the POS tag pro-vided by TreeTagger are also used to lookup synonym dictionaries and EuroWordNet when using synonym handling at run-time.\nWe were among the first to exploit automatically acquired paraphrases and to use these not only to increase the size of the training corpus but also to better balance it. We proceed as follows.\nFirst, we generated paraphrases using a pivot machine translation approach where each user utterance in the training corpus (around 3610 utterances) was translated into some target language and back into French. Using six different languages (English, Spanish, Italian, German, Chi-nese and Arabian), we generated around 38000 paraphrases. We used Google Translate API for translating.\nSecond, we eliminate from these paraphrases, words that are likely to be incorrect lexical translations by removing words with low normalised term frequency (< 0.001) across translations i.e., lexical translations given by few translations and/or translation systems. We then preprocessed the paraphrases in the same way the utterances of the initial training corpus were preprocessed i.e., utterances were unaccented, converted to lower-case and stop words were removed, the remaining words were filtered with TF*IDF. After preprocessing, duplicates were removed.\nThird, we added the paraphrases to the training data seeking to improve the balance between dialog acts per dialog. The process to balance data was guided by the deviation of the category with lowest examples compared to the standard deviation. If the deviation is lower than the stan-dard deviation then we add paraphrases by keeping as much as possible the data balanced after replacement. We invite the interested reader to find more details about the algorithm proposed for balancing data in the paper (Gardent and Rojas-Barahona, 2013)."}, {"title": "Language Understanding", "content": "Substituting Synonyms for Unknown Words A word is unknown, if it is a well-formed French word\u2075 and if it does not appear in the training corpus. When an unknown word w is detected in a player utterance at run-time, we search for a word w' which occurs in the training data and is either a synonym of w or a distributional neighbour. After disambiguation, we substitute the unknown word for the synonym."}, {"title": "Results and Discussion", "content": "Table 3.5 summarises the results obtained in four main configurations: (i) with and without para-phrases; (ii) with and without synonym handling; (iii) with and without lemmatisation; and (iv) when combining lemmatisation with synonym handling. We also compare the results obtained when evaluating using 10-fold cross validation on the training data (H-H dialogs) vs. evaluating the performance of the system on H-C interactions.\nOverall Impact The largest performance gain is obtained by a combination of the three tech-niques namely, data expansion, synonym handling and lemmatisation (+8.9 points for the cross-"}, {"title": "Language Understanding", "content": "We found that lexical resources are only useful when combined with lemmatisation. This is unsurprising since synonym dictionaries and Eu-roWordNet only contain lemmas. Indeed when distributional neighbours are used, lemmatisation has little impact (e.g., 65.11% usingdistributional neighbours without lemmatisation on the H-H corpus without paraphrases vs. 66.41% when using lemmatisation).\nAnother important issue when searching for a word synonym concerns lexical disambigua-tion: the synonym used to replace an unknown word should capture the meaning of that word in its given context. We tried using a language model trained on the training corpus to choose between synonym candidates (i.e., selecting the synonym yielding the highest sentence proba-bility when substituting that synonym for the unknown word) but did not obtain a significant improvement. In contrast, it is noticeable that synonym handling has a higher impact when using EuroWordNet as a lexical resource. Since EuroWordNet contain categorial information while the synonym dictionaries we used do not, this suggests that the categorial disambiguation provided by TreeTagger helps identifying an appropriate synonym in EuroWordNet.\nFinally, it is clear that the lexical resources used for this experiment are limited in coverage and quality. We observed in particular that some words which are very frequent in the training data (and thus which could be used to replace unknown words) do not occur in the synonym dictionaries. For instance when using paraphrases and dictionaries (fourth row and fourth column in Table 3.5) 50% of the unknown words were solved, 17% were illformed and 33% remained unsolved. To compensate this deficiency, we tried combining the three lexical resources in various ways (taking the union or combining them in a pipeline using the first resource that would yield a synonym). However the results did not improve and even in some cases worsened due probably to the insufficient lexical disambiguation. Interestingly, the results show that paraphrases always improves synonym handling presumably because it increases the size of the known vocabulary thereby increasing the possibility of finding a known synonym.\nIn sum, synonym handling helps most when (i) words are lemmatised and (ii) unknown words can be at least partially (i.e., using POS tag information) disambiguated. Moreover since data expansion increases the set of known words available as potential synonyms for unknown words, combining synonym handling with data expansion further improves accuracy."}, {"title": "Language Understanding", "content": "new with respect to the training data i.e., content words for which neither a synonym nor a lemma can be found in the expanded training data.\nWhile classifiers are routinely trained on dialog data to model the dialog management pro-cess, the impact of such basic factors as lemmatisation, automatic data expansion and synonym handling has remained largely unexplored. The empirical evaluation described here suggests that each of these factors can help improve performance but that the impact will vary depending on their combination and on the evaluation mode. Combining all three techniques yields the best re-sults. We conjecture that there are two main reasons for this. First, synonym handling is best used in combination with POS tagging and lemmatisation because these supports partial lexical seman-tic disambiguation. Second, data expansion permits expanding the set of known words thereby increasing the possibility of finding a known synonym to replace an unknown word with."}, {"title": "Spoken Language Understanding and Few-Shot Learning", "content": "The following work was published in Coling 2016 (Rojas Barahona et al., 2016). At that time the task of Spoken Language Understanding (SLU), namely semantic decoding, was seen as a sequence tagging problem with models trained and tested on datasets with word-level annotations (T\u00fcr et al., 2013; Mesnil et al., 2015; Yao et al., 2013; Sarikaya et al., 2011; Deoras and Sarikaya, 2013; Sarikaya et al., 2014). Nevertheless, spoken language understanding from unaligned data, in which utterances are annotated with an abstract semantics, faces the additional challenge of not knowing which specific words are relevant for extracting the semantics. This problem was tackled in (Zhou and He, 2011), by using conditional random fields (CRFs) driven by finely-tuned hand-crafted features. Other discriminative approaches that deal with unaligned data use some form of delexicalisation or mapping of the input to known ontological concepts (Henderson et al., 2012, 2014a). The main disadvantage of delexicalisation is the difficulty in scaling it, not only to larger and more complex dialogue domains but also to handle the many forms of language variation.\nWe proposed a semantic decoder that learns from unaligned data (Figure 3.2) and that exploits rich semantic distributed word representations instead of delexicalisation. The semantic decoder predicts the dialogue act and the set of slot-value pairs from a set of n-best hypotheses returned by an ASR. The prediction is made in two steps. First, a deep learning architecture is used for the joint prediction of dialogue acts and the presence or absence of slots. Second, the same architec-ture is reused for predicting the values of the slots that were detected by the first joint-classifier. The deep architecture combines sentence and context representations. A CNN (Collobert et al., 2011) is used to generate the sentence representation, while a LSTM network (Hochreiter and Schmidhuber, 1997) is used to generate the context representation. A non-linear function then combines the top layers of these neural networks and distinct Softmax layers are used to predict the dialogue act and slots in the first joint model. In the second model, a single Softmax predicts the possible values for each slot."}, {"title": "Evaluation", "content": "We split the task of semantic decoding into two steps: (i) training a joint model for predicting the dialogue act and presence or absence of slots and (ii) predicting the values for the most probable slots detected in (i). As shown in Figure 3.3, we use the same deep learning architecture in both steps for combining sentence and context representations to generate the final hidden unit that feeds one or many softmax layers. In the first step, as shown in the Figure, there are distinct softmax layers for the joint optimisation of the dialogue act and each possible slot. In the second step there is a single softmax layer that predicts the value of each specific slot. In the following we explain this architecture in more detail."}, {"title": "Why is human conversation difficult?", "content": "Who has not experienced the frustration of talking to an automatic system in a call centre. Typi-cally, these systems struggle to understand. They are repetitive because they are unable to rectify misunderstandings. Users then must start all over from scratch. In the worst-case users need to call again, then they try hard to fool the system until the call is finally answered by a human. Con-versations with automatic systems are unnatural because they do not deal correctly with misunder-standings, they do not adapt to novel situations and they constraint humans' great communication skills.\nDialogue-Acts and discourse obligations: A dialogue can be seen as a sequence of turns, in which every speaker takes a turn to speak and to contribute to the conversation. The philosopher (Austin, 1975a) stated that speakers perform actions while conversing and named these actions speech acts. Examples of these actions are : informing, requesting, offering, promising, answer-ing, persuading, convincing, etc. The research community nowadays call these actions Dialogue Acts or Communicative Acts (Bunt et al., 2010). Adjacency-pairs are pairs of dialogue-acts in conversation. For example, after a question in a conversation, the speaker is waiting for an an-swer. After an offer the speaker is waiting for an acceptance or a rejection. These represent discourse obligations in human conversation.\nCoreferences and Ambiguity: Moreover, humans can refer to concepts that were mentioned previously in the conversation. Indeed, a fluent conversation avoid repetitions. For instance, if you are talking about the president of France, you can say \u201cEmmanuel Macron\u201d the first time you mentioned him, later you can choose to say \u201cthe president\u201d. Moreover, if you want to further give your opinion about a recent political proposition he has made, you can say \u201cI disagree with his retirement policy\u201d. These are well studied linguistic phenomena that made conversation difficult.\nAnother aspect is the ambiguity, the same sentence can mean different things in different contexts. \"It's cold in here\u201d can be understood as a request to close the window inside a closed room or it may mean \u201cIt's cool in here\u201d in summer. In winter instead it could mean \u201cI can't stand the weather; it is too cold\".\nGrounding: speakers are always checking that they are following each other. For instance, let us suppose you are receiving instructions about where to place a box in a room. If there are many similar boxes, and the instruction is \u201cmove the box to the right of the desk", "which one?": "Then the instructor will provide more precise information such as", "box": "This coordination or mutual agreement is known as grounding, mutual knowledge or shared knowledge(Clark and Brennan, 1991).\nPlanning: The model Beliefs, Desires and Intentions (BDI) as the primary mental attitudes of an agent was first introduced by Bratman (Bratman, 1987). The beliefs are the agent's model"}, {"title": "Data Collection, Annotation and Frameworks", "content": "Most of my work concerns task-oriented dialogue, which involves the different components pre-sented in Section 2.3. I worked on NLU, DM and more recently on Natural Language Gener-ation (NLG). I have adapted neural models for distinct dialogue components that are presented in Chapter 3. Since the past five years, I have also explored open-domain Conversational Ques-tion Answering (CQA) (Section 2.6) which I present in Chapter 4, with predictive and generative(encoder-decoder) models for question rewriting, reading-comprehension and knowledge-graph question generation. As an extension of the last approach, I have also explored graph-embeddings (Section 4.4) and graph verbalisation with language models (Montella et al., 2023). The contri-butions published the second half of this year or the work in progress are not included in this document.\nMy work has always followed the state-of-the-art at the time of publication. For a detail com-parison of each contribution summarised in this manuscript with the related work of its time, we invite the curious reader to check the corresponding publications. To provide a brief positioning of some contributions, the work on data collection, NLU, dialogue management and human eval-uation I made for the EmoSpeech corpus was the first of its kind: a set of dialogues (12 distinct types of dialogue) in a Serious Game and in French. Moreover, we were among the first to propose in 2013 data-augmentation with back-translation and distributional representations for balancing biased models (Gardent and Rojas-Barahona, 2013).\nThe work on inverse reinforcement learning followed on the seminal work of Ng et al. (2000). It differed from previous work Paek and Pieraccini (2008); Chandramohan et al. (2011); El Asri et al. (2012); Boularias et al. (2010) because of the Bayesian Inverse Reinforcement Learning (IRL) algorithm that was applied for learning the tutor (i.e. system) reward function from experts. Furthermore, experts were taken from twelve distinct types of conversations in a serious game that were Human-Human (i.e. The EmoSpeech corpus) and not Machine-Machine (not from simulated conversations).\nI proposed to enrich a subset of the corpus CoQA (Reddy et al., 2019) releasing the cor-pus CoQAR\u00b2 with up to three out-of-context question paraphrases per question in conversations(Chapter 4, Section 4.2). Unlike previous work, these paraphrases were made by professional English native annotators instead of using crowd-sourcing, guaranteeing fair earning and working conditions. We compared CoQAR to CANARD (Elgohary et al., 2019) that provided only one question rewriting per question in QuAC (Choi et al., 2018b). I experimented with RoBERTa for answer extraction in both datasets CoQA and CoQAR with a without rewriting. Surprisingly solv-ing the context through rewritten questions confuses RoBERTa, which is already good to solve co-references by itself, specially in short dialogue contexts as in these datasets.\nI also utilised contextual embeddings (DistilBERT (Sanh et al., 2020), TransformersXL (Dai et al., 2019)) for estimating the reward function in long dialogues. I pointed out at that time the limitation of BERT-like models to deal with long contexts, which, besides the notable improve-ments of recent years, is still an open research problem. We also explored neural generation models such as BART (Lewis et al., 2019) and T5 (Raffel et al., 2020) for contextual question generation, question rewriting (Section 4.2) and graph verbalisation (Montella et al., 2022, 2023). I explored together with Sebastien Montella and Johannes Heinecke structural adapters for graph verbalisation (Montella et al., 2023) just after low rank emerged (Hu et al., 2021) as a recom-"}, {"title": "Positioning My Contributions in the State-of-the-Art", "content": "mended way to optimally fine-tune LLMs.\nAfter the advent of Large Language Models (LLMs), I am now questioning the performance of these models in complex tasks that required planning, such as dialogue. First, we need an eval-uation methodology to assess the performance of LLMs in these tasks. Then we need to compare LLMs based reasoning (Wei et al., 2022; Yao et al., 2023, 2022) with Reinforcement Learn-ing (RL) approaches, and explore recent trends for learning complex strategies such as algorithm distillation (Laskin et al., 2022). I talk about these research paths in Chapter 6."}, {"title": "Contributions to Task-Oriented Dialogues", "content": "As introduced in Section 2.3", "which price range?": "where about?", "as": "informing", "domains": "hotels"}, {"title": "HDR: Talking to Machines: do you read me?", "authors": ["Lina Mar\u00eda ROJAS BARAHONA"], "abstract": null, "sections": [{"title": "Introduction", "content": "In this dissertation I would like to guide the reader to the research on dialogue but more precisely the research I have conducted during my career since my PhD thesis. Starting from modular architectures with machine learning/deep learning and reinforcement learning to end-to-end deep neural networks. Besides my work as research associate, I also present the work I have supervised in the last years. I proposed four PhD thesis Conventions industrielles de formation par la recherche (CIFRE) that Orange accepted to fund. Therefore, I could co-supervise four PhD candidates: Timothy Garwood supervised by Claire Gardent at CNRS, Thibault Cordier supervised by Fabrice Lefevre at the University of Avignon, Sebastien Montella supervised by Alexis Nasr at the university of Aix-Marseille, L\u00e9o Jacqmin supervised by Benoit Favre at the University of Aix-Marseille. During 5 years I was head of the industrial research project on dialogue, Dialogue in NAtural Language (DIANA), which gave me the opportunity of supervising the work of the young researcher Quentin Brabant, other experimented researchers, a developer as well as students in internship and apprenticeship. The deliverables of DIANA project gather open-sourced datasets and neural models as well as scientific publications.\nI review briefly the state of the art and highlight the open research problems on conversational agents in Chapter 2. Afterwards, I present my contribution to Task-Oriented Dialogues (TOD) in"}, {"title": "Preliminary Approaches", "content": "of the world. Desires, in turn, represent how the agent would like the world to be in the future;\nwhile intentions are the structured plan the agent has decided to perform. The agent interacts\nwith the world by performing actions and by perceiving aspects of it, including changes which\nresult from its own actions. Perceptions will influence the beliefs of the agent, while actions\nmay change aspects of the world. This model was at the origin of modern Natural Language\nUnderstanding (NLU), in which the aim is to detect user's intentions or intents. However, the term\nintent usually means a dialogue-act with a set of concepts or a combination of them in semantic\nlabels.\nAll these inherent characteristics in natural dialogue make implementing automated systems a\nvery difficult task."}, {"title": "Preliminary Approaches", "content": "A range of approaches emerged in the history of Dialogue Systems (DSs), they were classified in conformity with their Dialog Manager (DM) (Allen et al., 2001; Churcher, 1997). According to this classification, ordered by increasing complexity, the simplest of these is the finite-state scripts, also called dialogue grammars, followed by slot-filling, plan-based and agent-based models. In a finite-state script the dialogue is represented as a script of prompts for the user. In slot-filling, questions are asked in order to enable the system to fill the necessary slots to perform a task. Conversely, plan-based theories claim that utterances infer acts that are part of a plan, thus, the system tries to identify users' underlying plan, collaborates in accomplishing that plan and responds appropriately. Agent-based models are at the highest level of complexity. They consider planning, executing, and monitoring operations in a dynamically changing world, possibly involving multi-modality. Examples of agent-based models are: the logic-based approaches, which uses inference engines of a higher complexity that in some cases are semi-decidable (in some cases the system will never halt), as well as reinforcement learning approaches, which need a large number of interactions to converge."}, {"title": "Task Oriented Dialogue Systems", "content": "Conversational agents have gained great interest in both academy and industry in the last decades.\nTypically, available conversational agents have been designed for the task of information-seeking.\nThese agents act as a natural language interface to a database. First, the system tries to fill the\nconstrains to query a database by inquiring the user. Then it retrieves the items that fulfill users'\nconstraints and finally it communicates the results to the user in natural language. For instance, a\nperson could call the system to check train timetables, she would provide the departure and arrival\ncity as well as the departure date and time. Then, the system would inform her about the available\ntrains.\nTOD main goal is to complete a task in collaboration with the user (Pieraccini et al., 1992;\nYoung, 2002; Rieser and Lemon, 2011; Young et al., 2013a). Examples of tasks are to search\ninformation about a restaurant, to reserve a hotel or to buy train tickets."}, {"title": "Definitions", "content": "As introduced in Section 2.1, dialogue-acts or communicative acts are the actions performed by the speakers when uttering sentences (e.g. Informing, Asking, Confirming, Greeting, etc.) (Austin, 1975b). A domain is formally defined in an ontology as a list of slots with their valid values. The most common task, the information seeking task, is usually modelled as a slot-filling data-query problem in which the system requests constraints to the user and proposes items that fulfil those constraints in a database. The action or intention is composed by a predicate: the dialogue-act, and a set of arguments: the slot-value pairs. For instance, let us suppose the user has uttered \u201cI would like a restaurant in the center of town please\u201d, this will be translated in the semantic form: inform(type = restaurant, area = center). This semantic representation is usually called flat-semantics because there is not hierarchy in the concepts of the ontology."}, {"title": "Statistical Dialogue Systems", "content": "One approach to automatic dialogue is to use Reinforcement Learning (RL) to select the system's action (Levin et al., 2000; Litman et al., 2000). As in a Chess game, a dialogue involves two players, in which each of them takes turns to play. The system should decide its move by considering the environment (the other player's moves) and the rewards is either win or lose the game. Dialogue is then formulated as an optimisation problem, in which the environment is the user action and the user's feedback is the reward. The final goal of the system is then to maximise the accumulated reward at long run (Rieser and Lemon, 2011). The optimal policy,\\( \\pi \\), is a function that takes as argument the current state \\( s \\) and returns the optimal action \\( a \\).\nMarkov decision process\nDialogue can be formalised as a Markov Decision Process (MDP), which is a tuple \\( M = (S, A, T, \\gamma, R) \\) where:\n\\begin{itemize}\n    \\item S: A set of possible states that represent the dynamic environment.\n    \\item A: A set of possible actions.\n    \\item T: \\( S \\times A \\times S \\rightarrow [0, 1] \\) is a transition probability function. For any action \\( a \\in A(s) \\) taken\n    in a state \\( s \\in S \\), the probability of transiting to the next state \\( s' \\) is given by \\( T(s, s') \\).\n    \\item \\( \\gamma \\): A discounting factor in the range of [0, 1], which controls the prediction horizon of the\n    algorithm.\n    \\item R: The reward function that specifies the reward gained at every state. It contains the\n    information that guides the agent towards the goal. \\( R \\) is a function of the state that is\n    bounded in absolute value by \\( R_{max} \\).\n\\end{itemize}\nA stationary policy is a map \\( \\pi : S \\rightarrow A \\) and the discounted infinite-horizon expected reward\nfor starting in state s and following policy \\( \\pi \\) thereafter is given by the value function \\( V^{\\pi}(s) \\) that\nsatisfies the following Bellman Equation:\n\\[\nV^*(s) = R(s) + \\sum_{s'} T(s, \\pi(s), s')V^* (s')\n\\tag{2.1}\n\\]"}, {"title": "Task Oriented Dialogue Systems", "content": "The discounted infinite-horizon expected reward for starting in state s, taking action a and follow-ing policy \\( \\pi \\) thereafter is given by the Q-function \\( Q^{\\pi}(s, a) \\) that satisfies the following equation:\n\\[\nQ^{\\pi} (s, a) = R(s) + \\gamma \\sum_{s'} T(s, a, s')V^{\\pi} (s')\n\\tag{2.2}\n\\]\nA policy \\( \\pi \\) is optimal in M if, for all \\( s \\in S \\):\n\\[\n\\pi(s) = \\arg\\max_{a\\in A} Q^{\\pi} (s, a)\n\\tag{2.3}\n\\]\nLikewise \\( Q^*(s, a, R) \\) is the optimal Q-function of the optimal policy \\( \\pi^* \\) for a known reward func-tion R.\nPartially Observable Markov decision process\nDialogues can be modelled as an optimisation problem with Partially Observable Markov decision process (POMDP)s. It simulates the inherent dynamic behaviour of human conversations while deals with the uncertainty of spoken language (Roy et al., 2000; Williams and Young, 2007; Young et al., 2013b).\nA POMDP can be seen as a continuous-space Markov decision process (MDP) in terms of policy optimisation where the states are the belief states, which is partially observable. POMDPS have been proposed for spoken dialogue systems because the system is never sure about the user beliefs because of speech recognition errors due to noisy or spoken language disfluencies and hesitations (Roy et al., 2000; Young et al., 2013a). Since the state is uncertain, it is called the belief state b(s). An example of a POMDP dialogue system is presented in Section 2.3.2. The task of predicting the b(s) at a given time t is known as the task of Dialogue State Tracking (DST). The policy learning algorithm receives as input the b(s) and returns the optimal policy \\( \\pi^* \\) and a given time.\nThe belief state b\u2081 is a vector encoding a probability distribution over the different goals, dialogue acts and concepts that are discussed in the dialogue. In the same way, the dialogue action at is a vector encoding a probability distribution over the possible agent dialogue actions.\nHierarchical Reinforcement Learning\nMDP models have been proven to be inefficient for solving complex tasks. These models have trouble overcoming the cold start problem and/or suffer from the curse of dimensionality (Barto and Mahadevan, 2003). This pattern was also observed with models proposed recently (Mnih et al., 2013; Duan et al., 2016). To overcome this issue, (Parr and Russell, 1998) proposed to specify a hierarchy of tasks and to reuse parts of the state space across many sub-tasks, which can greatly improve both learning speed and agent performance.\nThe notion of temporal abstraction, in which a policy can be decomposed into sub-policies by calling temporally extended sub-tasks was first proposed by (Sutton et al., 1999). In order to consider hierarchical architectures with temporally extension, we have to generalise the MDP to the semi-Markov Decision Process (SMDP) (Parr and Russell, 1998) where actions can take a variable amount of time to complete. This creates a division between primitive actions that span over only one action and composite actions that involve an execution of a sequence of primitive"}, {"title": "Introduction", "content": "As introduced in Section 2.3, task-oriented dialogue systems search to accomplish a task.\nThis task can be for instance, information seeking, in which the system search for items in a\ndatabase according to the constraints obtained through natural language interaction. Thus, these\nconstrains can be given by the user (\u2018I am looking for a restaurant') or can be enquired by the\nsystem ('which price range?', 'where about?'). Once the desired items are retrieved, the system\ninforms the results back to the user. This kind of dialogue has rich interactions with dialogue\nacts such as: informing, requesting, clarifying, rectifying. State-of-the-art systems are centred on\nthe information seeking task for a variety of domains: hotels, restaurants, touristic attractions,\ntrains, flights, taxis, etc. Unfortunately, these dialogues are very specific and have difficulties to\ngeneralise to new domains and to more complex tasks (i.e. beyond information seeking).\nI present in this Chapter my contributions to TOD dialogues (Figure 2.1). I start by presenting\nmy work on NLU in Section 3.1 and on Dialogue Management in Section 3.2. My contributions\nto NLU concerns the definition of an annotation schema for the French corpus EmoSpeech, in\nwhich 12 distinct dialogues were integrated in a serious game. The dialogues involved various\ncharacters representing the system and the player and they were triggered at different levels of\nthe game quest. Dialogues were modelled as an information seeking task, in which the system\nis always providing information to the player. The set of dialogue acts and goals are introduced\nin Section 3.1, as well as the models that were trained. Data augmentation through paraphrases\nvia back translation, dictionaries, lexical resources and distributional semantics is also presented"}, {"title": "NLU in a Serious Game", "content": "Machine Learning (ML) approaches such as logistic regression classifiers and conditional random fields were the state-of-the art back in 2011. I could collect a corpus through Wizard-of-Oz experiments, define an annotation scheme and train Logistic Regression (LR) and Support Vector Machines (SVM) multi-class classifiers for the task of NLU. These models were integrated within distinct dialogues in a serious game. I will start by describing briefly this work that gave origin to four publications (Rojas-Barahona et al., 2012b,c; Rojas-Barahona and Gardent, 2012; Gardent and Rojas-Barahona, 2013).\nThe serious game is a multiplayer quest where the players (3 teenagers) seek to build a video game joystick in order to free their uncle trapped in the game. To build this joystick, the players must explore a factory and achieve 17 mandatory goals (find the plans, get the appropriate mould, retrieve some raw material from the storing shed, etc). In addition, they can increase their score by achieving optional goals which, when reached, provide them with extra information about the industry (therefore increasing their knowledge). In total, the players can achieve up to 28 goals by conducting 12 separate subdialogs in various parts of the virtual world. That is, dialogs in the game are long dialogs involving multiple players in various settings."}, {"title": "Language Understanding", "content": "3.2). Taking into account the game goals, the total number of categories to be learned is 27.\nWhen learning on subdialogs, the number of categories to be learned is smaller but so is the size\nof the training set. The features for the machine learning models were bag of words, in which\nstop words were filtered out, utterances were deaccented and converted to lower-case. In addition,\nwe experimented with various context length using as features the 0 to 4 previous dialogue acts.\nSubdialog identifiers were also used when training the classifier on the whole dialogue. More\ndetails are given in (Rojas-Barahona et al., 2012a).\nWe also experimented using tf*idf filtering to limit the impact of frequent uninformative\nwords. Moreover, we experimented penalising those categories with more training instances,\nsince the data was highly skewed. Dialogue acts that relate to optional goals were often not fol-\nlowed up by the players resulting in data sparseness."}, {"title": "Results", "content": "Table 3.3 shows the results for the 6 main configurations: training on the whole dialog or on sub-dialogs, with and without tf*idf filtering and using LR, SVM or SVM with penalisation. The best results are obtained using the LR classifier on the whole dataset with tf*idf filtering. Penalising improved slightly the accuracy of the SVM when classifing without tf*idf filtering or when having a reduced context (0 or 2 previous acts in Table 3.4).\nImpact of the tf*idf filtering. Globally, the tf*idf filtering has a positive impact leading to an increase in accuracy ranging from 2.81 to 11.52 points. For the SVM classifier, the tf*idf filtering consistently lead to better results. However, for the LR classifier the filtering adversely impacts performance on short subdialogs (6 and 7), where one unique goal is being discussed. We conjecture that for these cases, the tf*idf filtering removed words which helped the classifier distinguish between turns about the unique goal from other turns. SVM with penalisation yields worse results with the tf*idf filtering than without, thus suggesting overfitting. In the next section we present how can we exploiting synonyms to improve generalisation.\nImpact of contextual features. Having a notion of context is crucial for correctly interpreting dialog acts. As mentioned above, we use the dialog acts of the previous turns to model context. However the further back we look into the previous turns, the more features there will be to train on. In other words, depending on the number of previous turns considered, the data to learn from will be more or less sparse. We experimented with 3 setups: a null context, the dialog acts of the two previous turns and the dialog acts of the four previous acts. Table 3.4 shows the results."}, {"title": "Language Understanding", "content": "and the test data; augmenting the training data with automatically acquired paraphrases; and substituting unknown words with synonyms or its distributional neighbours at run-time.\nFor Lemmatisation, we used the French version of Treetagger\u00b3 to lemmatise both the training and the test data. Lemmas without any filtering were used to train classifiers. We then compare performance with and without lemmatisation. As we shall see, the lemma and the POS tag pro-vided by TreeTagger are also used to lookup synonym dictionaries and EuroWordNet when using synonym handling at run-time.\nWe were among the first to exploit automatically acquired paraphrases and to use these not only to increase the size of the training corpus but also to better balance it. We proceed as follows.\nFirst, we generated paraphrases using a pivot machine translation approach where each user utterance in the training corpus (around 3610 utterances) was translated into some target language and back into French. Using six different languages (English, Spanish, Italian, German, Chi-nese and Arabian), we generated around 38000 paraphrases. We used Google Translate API for translating.\nSecond, we eliminate from these paraphrases, words that are likely to be incorrect lexical translations by removing words with low normalised term frequency (< 0.001) across translations i.e., lexical translations given by few translations and/or translation systems. We then preprocessed the paraphrases in the same way the utterances of the initial training corpus were preprocessed i.e., utterances were unaccented, converted to lower-case and stop words were removed, the remaining words were filtered with TF*IDF. After preprocessing, duplicates were removed.\nThird, we added the paraphrases to the training data seeking to improve the balance between dialog acts per dialog. The process to balance data was guided by the deviation of the category with lowest examples compared to the standard deviation. If the deviation is lower than the stan-dard deviation then we add paraphrases by keeping as much as possible the data balanced after replacement. We invite the interested reader to find more details about the algorithm proposed for balancing data in the paper (Gardent and Rojas-Barahona, 2013)."}, {"title": "Language Understanding", "content": "Substituting Synonyms for Unknown Words A word is unknown, if it is a well-formed French word\u2075 and if it does not appear in the training corpus. When an unknown word w is detected in a player utterance at run-time, we search for a word w' which occurs in the training data and is either a synonym of w or a distributional neighbour. After disambiguation, we substitute the unknown word for the synonym."}, {"title": "Results and Discussion", "content": "Table 3.5 summarises the results obtained in four main configurations: (i) with and without para-phrases; (ii) with and without synonym handling; (iii) with and without lemmatisation; and (iv) when combining lemmatisation with synonym handling. We also compare the results obtained when evaluating using 10-fold cross validation on the training data (H-H dialogs) vs. evaluating the performance of the system on H-C interactions.\nOverall Impact The largest performance gain is obtained by a combination of the three tech-niques namely, data expansion, synonym handling and lemmatisation (+8.9 points for the cross-"}, {"title": "Language Understanding", "content": "new with respect to the training data i.e., content words for which neither a synonym nor a lemma can be found in the expanded training data.\nWhile classifiers are routinely trained on dialog data to model the dialog management pro-cess, the impact of such basic factors as lemmatisation, automatic data expansion and synonym handling has remained largely unexplored. The empirical evaluation described here suggests that each of these factors can help improve performance but that the impact will vary depending on their combination and on the evaluation mode. Combining all three techniques yields the best re-sults. We conjecture that there are two main reasons for this. First, synonym handling is best used in combination with POS tagging and lemmatisation because these supports partial lexical seman-tic disambiguation. Second, data expansion permits expanding the set of known words thereby increasing the possibility of finding a known synonym to replace an unknown word with."}, {"title": "Spoken Language Understanding and Few-Shot Learning", "content": "The following work was published in Coling 2016 (Rojas Barahona et al., 2016). At that time the task of Spoken Language Understanding (SLU), namely semantic decoding, was seen as a sequence tagging problem with models trained and tested on datasets with word-level annotations (T\u00fcr et al., 2013; Mesnil et al., 2015; Yao et al., 2013; Sarikaya et al., 2011; Deoras and Sarikaya, 2013; Sarikaya et al., 2014). Nevertheless, spoken language understanding from unaligned data, in which utterances are annotated with an abstract semantics, faces the additional challenge of not knowing which specific words are relevant for extracting the semantics. This problem was tackled in (Zhou and He, 2011), by using conditional random fields (CRFs) driven by finely-tuned hand-crafted features. Other discriminative approaches that deal with unaligned data use some form of delexicalisation or mapping of the input to known ontological concepts (Henderson et al., 2012, 2014a). The main disadvantage of delexicalisation is the difficulty in scaling it, not only to larger and more complex dialogue domains but also to handle the many forms of language variation.\nWe proposed a semantic decoder that learns from unaligned data (Figure 3.2) and that exploits rich semantic distributed word representations instead of delexicalisation. The semantic decoder predicts the dialogue act and the set of slot-value pairs from a set of n-best hypotheses returned by an ASR. The prediction is made in two steps. First, a deep learning architecture is used for the joint prediction of dialogue acts and the presence or absence of slots. Second, the same architec-ture is reused for predicting the values of the slots that were detected by the first joint-classifier. The deep architecture combines sentence and context representations. A CNN (Collobert et al., 2011) is used to generate the sentence representation, while a LSTM network (Hochreiter and Schmidhuber, 1997) is used to generate the context representation. A non-linear function then combines the top layers of these neural networks and distinct Softmax layers are used to predict the dialogue act and slots in the first joint model. In the second model, a single Softmax predicts the possible values for each slot."}, {"title": "Evaluation", "content": "We split the task of semantic decoding into two steps: (i) training a joint model for predicting the dialogue act and presence or absence of slots and (ii) predicting the values for the most probable slots detected in (i). As shown in Figure 3.3, we use the same deep learning architecture in both steps for combining sentence and context representations to generate the final hidden unit that feeds one or many softmax layers. In the first step, as shown in the Figure, there are distinct softmax layers for the joint optimisation of the dialogue act and each possible slot. In the second step there is a single softmax layer that predicts the value of each specific slot. In the following we explain this architecture in more detail."}, {"title": "Why is human conversation difficult?", "content": "Who has not experienced the frustration of talking to an automatic system in a call centre. Typi-cally, these systems struggle to understand. They are repetitive because they are unable to rectify misunderstandings. Users then must start all over from scratch. In the worst-case users need to call again, then they try hard to fool the system until the call is finally answered by a human. Con-versations with automatic systems are unnatural because they do not deal correctly with misunder-standings, they do not adapt to novel situations and they constraint humans' great communication skills.\nDialogue-Acts and discourse obligations: A dialogue can be seen as a sequence of turns, in which every speaker takes a turn to speak and to contribute to the conversation. The philosopher (Austin, 1975a) stated that speakers perform actions while conversing and named these actions speech acts. Examples of these actions are : informing, requesting, offering, promising, answer-ing, persuading, convincing, etc. The research community nowadays call these actions Dialogue Acts or Communicative Acts (Bunt et al., 2010). Adjacency-pairs are pairs of dialogue-acts in conversation. For example, after a question in a conversation, the speaker is waiting for an an-swer. After an offer the speaker is waiting for an acceptance or a rejection. These represent discourse obligations in human conversation.\nCoreferences and Ambiguity: Moreover, humans can refer to concepts that were mentioned previously in the conversation. Indeed, a fluent conversation avoid repetitions. For instance, if you are talking about the president of France, you can say \u201cEmmanuel Macron\u201d the first time you mentioned him, later you can choose to say \u201cthe president\u201d. Moreover, if you want to further give your opinion about a recent political proposition he has made, you can say \u201cI disagree with his retirement policy\u201d. These are well studied linguistic phenomena that made conversation difficult.\nAnother aspect is the ambiguity, the same sentence can mean different things in different contexts. \"It's cold in here\u201d can be understood as a request to close the window inside a closed room or it may mean \u201cIt's cool in here\u201d in summer. In winter instead it could mean \u201cI can't stand the weather; it is too cold\".\nGrounding: speakers are always checking that they are following each other. For instance, let us suppose you are receiving instructions about where to place a box in a room. If there are many similar boxes, and the instruction is \u201cmove the box to the right of the desk", "which one?": "Then the instructor will provide more precise information such as \"the yellow rectangular box", "1991).\nPlanning": "The model Beliefs, Desires and Intentions (BDI) as the primary mental attitudes of an agent was first introduced by Bratman (Bratman, 1987). The beliefs are the agent's model"}, {"title": "Data Collection, Annotation and Frameworks", "content": "Most of my work concerns task-oriented dialogue, which involves the different components pre-sented in Section 2.3. I worked on NLU, DM and more recently on Natural Language Gener-ation (NLG). I have adapted neural models for distinct dialogue components that are presented in Chapter 3. Since the past five years, I have also explored open-domain Conversational Ques-tion Answering (CQA) (Section 2.6) which I present in Chapter 4, with predictive and generative(encoder-decoder) models for question rewriting, reading-comprehension and knowledge-graph question generation. As an extension of the last approach, I have also explored graph-embeddings (Section 4.4) and graph verbalisation with language models (Montella et al., 2023). The contri-butions published the second half of this year or the work in progress are not included in this document.\nMy work has always followed the state-of-the-art at the time of publication. For a detail com-parison of each contribution summarised in this manuscript with the related work of its time, we invite the curious reader to check the corresponding publications. To provide a brief positioning of some contributions, the work on data collection, NLU, dialogue management and human eval-uation I made for the EmoSpeech corpus was the first of its kind: a set of dialogues (12 distinct types of dialogue) in a Serious Game and in French. Moreover, we were among the first to propose in 2013 data-augmentation with back-translation and distributional representations for balancing biased models (Gardent and Rojas-Barahona, 2013).\nThe work on inverse reinforcement learning followed on the seminal work of Ng et al. (2000). It differed from previous work Paek and Pieraccini (2008); Chandramohan et al. (2011); El Asri et al. (2012); Boularias et al. (2010) because of the Bayesian Inverse Reinforcement Learning (IRL) algorithm that was applied for learning the tutor (i.e. system) reward function from experts. Furthermore, experts were taken from twelve distinct types of conversations in a serious game that were Human-Human (i.e. The EmoSpeech corpus) and not Machine-Machine (not from simulated conversations).\nI proposed to enrich a subset of the corpus CoQA (Reddy et al., 2019) releasing the cor-pus CoQAR\u00b2 with up to three out-of-context question paraphrases per question in conversations(Chapter 4, Section 4.2). Unlike previous work, these paraphrases were made by professional English native annotators instead of using crowd-sourcing, guaranteeing fair earning and working conditions. We compared CoQAR to CANARD (Elgohary et al., 2019) that provided only one question rewriting per question in QuAC (Choi et al., 2018b). I experimented with RoBERTa for answer extraction in both datasets CoQA and CoQAR with a without rewriting. Surprisingly solv-ing the context through rewritten questions confuses RoBERTa, which is already good to solve co-references by itself, specially in short dialogue contexts as in these datasets.\nI also utilised contextual embeddings (DistilBERT (Sanh et al., 2020), TransformersXL (Dai et al., 2019)) for estimating the reward function in long dialogues. I pointed out at that time the limitation of BERT-like models to deal with long contexts, which, besides the notable improve-ments of recent years, is still an open research problem. We also explored neural generation models such as BART (Lewis et al., 2019) and T5 (Raffel et al., 2020) for contextual question generation, question rewriting (Section 4.2) and graph verbalisation (Montella et al., 2022, 2023). I explored together with Sebastien Montella and Johannes Heinecke structural adapters for graph verbalisation (Montella et al., 2023) just after low rank emerged (Hu et al., 2021) as a recom-"}, {"title": "Positioning My Contributions in the State-of-the-Art", "content": "mended way to optimally fine-tune LLMs.\nAfter the advent of Large Language Models (LLMs), I am now questioning the performance of these models in complex tasks that required planning, such as dialogue. First, we need an eval-uation methodology to assess the performance of LLMs in these tasks. Then we need to compare LLMs based reasoning (Wei et al., 2022; Yao et al., 2023, 2022) with Reinforcement Learn-ing (RL) approaches, and explore recent trends for learning complex strategies such as algorithm distillation (Laskin et al., 2022). I talk about these research paths in Chapter 6."}, {"title": "Hierarchical Imitation Learning", "content": "This section also presents the work of the PhD candidate Thibault Cordier, who I co-supervised together with Dr.Tanguy Urvoy and Professor Fabrice Lefevre. This work has been published in Cordier et al. (2022).\nWe explore Graph Neural Network (GNN) for learning the policy in multi-domain and multi-task environments, in which several domains and tasks can be evoked in the same conversation.\nIn practice, real applications like personal assistants or chatbots must deal with multiple tasks: the user may first want to find a hotel (first task), then book it (second task). Moreover, the tasks may cover several domains: the user may want to find a hotel (first task, first domain), book it (second task, first domain), and then find a restaurant nearby (first task, second domain).\nOne way of handling this complexity is to rely on a domain hierarchy which decomposes the decision-making process;another way is to switch easily from one domain to another by scaling up the policy.\nAlthough structured dialogue policies can adapt quickly from a domain to another Chen et al. (2020), covering multiple domains remains a hard task because it increases the dimensions of the state and action spaces while the reward signal remains sparse. A common technique to circumvent this reward scarcity is to guide the learning by injecting some knowledge through a teacher policy 15.\nWe study how structured policies like graph neural networks (GNN) combined with some degree of imitation learning (Imitation Learning (IL)) can be effective to handle multi-domain scenarios.\nWe provide large scale experiments in a dedicated framework (Zhu et al., 2020) in which we analyse the performance of different types of policies, from multi-domain policy to generic policy, with different levels of imitation learning.\nDialogue State / Action Representations\nOne way of standardising the slot representation into a common feature space is to use Domain Independent Parametrisation (DIP) (Wang et al., 2015) parametrisation. We adopt DIP as state and action representations, which are not reduced to a flat vector but to a set of sub-vectors: one corresponding to the domain parametrisation (or domain representation), the others to the slots parametrisation (or slot representations). For any active domain, the input to the domain represen-tation is the concatenation of the previous domain user and system actions (see examples of the output below, and a formal definition in Section 3.2.4), the number of entities fulfilling the user's constraints in the database, the booleans indicating if the dialogue is terminated and whether an offer has been found / booked. The output corresponds to action scores such as REQMORE, OF-FER, BOOK, GREAT, etc. Regarding the slot representation, its input is composed of the previous slot-dependent user and system actions (see output below), the booleans indicating if a value is known and whether the slot is needed for the find / book tasks. Its output are actions scores such"}, {"title": "Graph Neural Network", "content": "Assuming that sub-policies associated with the slots are the same, a better alternative is to use the graph neural network layer (GNN) presented in Figure 3.8b. This structure assumes that the state and action representations"}]}]}