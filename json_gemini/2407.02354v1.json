{"title": "HDR: Talking to Machines: do you read me? Part I: Scientific contributions", "authors": ["Lina Mar\u00eda ROJAS BARAHONA"], "abstract": "I presented in this dissertation a selected number of contributions I made to the areas of task- oriented dialogue systems, conversational question answering and graph embeddings.\nThe contributions to task-oriented dialogue were in the fields of NLU, SLU and DM. Par- ticularly, I explored classical machine learning techniques for NLU, convolutional and recurrent neural networks to deal with noisy inputs for SLU, as well as data-augmentation techniques. Although not mentioned in this work, I am currently supervising with Benoit Favre from the Uni- versity of Aix-Marseille a PhD thesis on DST, recently we competed in the challenge DSTC-11 and we were awarded the first and second place (Jacqmin et al., 2023). Regarding the DM, I explored Inverse Reinforcement Learning, Deep Reinforcement Learning, Imitation Learning and Structured Policy Learning.\nThe contributions to conversational QA regard the annotation of existing datasets with in- formation about ellipsis and coreferences, and with question rewriting to transform in-context questions into out-of-context questions. The generative models released with these annotations for the tasks of answer extraction, question generation and question rewriting; as well as the models implemented for predicting ellipsis and coreferences were also presented. I also briefly introduced the corpus KGConv grounded in Wikidata. Moreover, I presented our work on graph embeddings in the hyperbolic space. Finally, I summarised the released resources such as datasets and frameworks, in which I contributed to their creation, annotation and development.\nThis document consolidates my own contributions as young researcher, the work of two PhD candidates supervised jointly with academics under the CIFRE convention. I could also collabo- rate with academics in two ITN projects: Conversational Brains (COBRA) and NL4XAI. As head of the project DIANA during 5 years, I could also define the main workpackages of the project and supervise a dynamic team of researchers, developers, students in internship and apprenticeship.\nI would like to focus my future research in proposing a framework to evaluate LLMs per- formance in complex task-oriented dialogue. Moreover, I would like to explore interpretability, retrieval augmentation and semantically control to understand LLMs internally, to support ground- ing and to generate factual information. I would like to explore recent reasoning LLMs approach (e.g., ReAct, algorithm distillation, etc.) to study LLMs capabilities to make long-term decisions. It is wort noting that these research paths also cover multi-modal interactions.", "sections": [{"title": "Introduction", "content": "In this dissertation I would like to guide the reader to the research on dialogue but more precisely the research I have conducted during my career since my PhD thesis. Starting from modular architectures with machine learning/deep learning and reinforcement learning to end-to- end deep neural networks. Besides my work as research associate, I also present the work I have supervised in the last years. I proposed four PhD thesis Conventions industrielles de formation par la recherche (CIFRE) that Orange accepted to fund. Therefore, I could co-supervise four PhD candidates: Timothy Garwood supervised by Claire Gardent at CNRS, Thibault Cordier supervised by Fabrice Lefevre at the University of Avignon, Sebastien Montella supervised by Alexis Nasr at the university of Aix-Marseille, L\u00e9o Jacqmin supervised by Benoit Favre at the University of Aix-Marseille. During 5 years I was head of the industrial research project on dialogue, Dialogue in NAtural Language (DIANA), which gave me the opportunity of supervising the work of the young researcher Quentin Brabant, other experimented researchers, a developer as well as students in internship and apprenticeship. The deliverables of DIANA project gather open-sourced datasets and neural models as well as scientific publications.\nI review briefly the state of the art and highlight the open research problems on conversational agents in Chapter 2. Afterwards, I present my contribution to Task-Oriented Dialogues (TOD) in"}, {"title": "Preliminary Approaches", "content": "of the world. Desires, in turn, represent how the agent would like the world to be in the future; while intentions are the structured plan the agent has decided to perform. The agent interacts with the world by performing actions and by perceiving aspects of it, including changes which result from its own actions. Perceptions will influence the beliefs of the agent, while actions may change aspects of the world. This model was at the origin of modern Natural Language Understanding (NLU), in which the aim is to detect user's intentions or intents. However, the term intent usually means a dialogue-act with a set of concepts or a combination of them in semantic labels.\nAll these inherent characteristics in natural dialogue make implementing automated systems a very difficult task."}, {"title": "Preliminary Approaches", "content": "A range of approaches emerged in the history of Dialogue Systems (DSs), they were classified in conformity with their Dialog Manager (DM) (Allen et al., 2001; Churcher, 1997). According to this classification, ordered by increasing complexity, the simplest of these is the finite-state scripts, also called dialogue grammars, followed by slot-filling, plan-based and agent-based models. In a finite-state script the dialogue is represented as a script of prompts for the user. In slot-filling, questions are asked in order to enable the system to fill the necessary slots to perform a task. Conversely, plan-based theories claim that utterances infer acts that are part of a plan, thus, the system tries to identify users' underlying plan, collaborates in accomplishing that plan and re- sponds appropriately. Agent-based models are at the highest level of complexity. They consider planning, executing, and monitoring operations in a dynamically changing world, possibly involv- ing multi-modality. Examples of agent-based models are: the logic-based approaches, which uses inference engines of a higher complexity that in some cases are semi-decidable (in some cases the system will never halt), as well as reinforcement learning approaches, which need a large number of interactions to converge."}, {"title": "Task Oriented Dialogue Systems", "content": "Conversational agents have gained great interest in both academy and industry in the last decades. Typically, available conversational agents have been designed for the task of information-seeking. These agents act as a natural language interface to a database. First, the system tries to fill the constrains to query a database by inquiring the user. Then it retrieves the items that fulfill users' constraints and finally it communicates the results to the user in natural language. For instance, a person could call the system to check train timetables, she would provide the departure and arrival city as well as the departure date and time. Then, the system would inform her about the available trains.\nTOD main goal is to complete a task in collaboration with the user (Pieraccini et al., 1992; Young, 2002; Rieser and Lemon, 2011; Young et al., 2013a). Examples of tasks are to search information about a restaurant, to reserve a hotel or to buy train tickets."}, {"title": "Definitions", "content": "As introduced in Section 2.1, dialogue-acts or communicative acts are the actions performed by the speakers when uttering sentences (e.g. Informing, Asking, Confirming, Greeting, etc.) (Austin, 1975b). A domain is formally defined in an ontology as a list of slots with their valid values. The most common task, the information seeking task, is usually modelled as a slot-filling data-query problem in which the system requests constraints to the user and proposes items that fulfil those constraints in a database. The action or intention is composed by a predicate: the dialogue-act, and a set of arguments: the slot-value pairs. For instance, let us suppose the user has uttered \u201cI would like a restaurant in the center of town please\u201d, this will be translated in the semantic form: inform(type = restaurant, area = center). This semantic representation is usually called flat-semantics because there is not hierarchy in the concepts of the ontology."}, {"title": "Statistical Dialogue Systems", "content": "One approach to automatic dialogue is to use Reinforcement Learning (RL) to select the system's action (Levin et al., 2000; Litman et al., 2000). As in a Chess game, a dialogue involves two players, in which each of them takes turns to play. The system should decide its move by consid- ering the environment (the other player's moves) and the rewards is either win or lose the game. Dialogue is then formulated as an optimisation problem, in which the environment is the user action and the user's feedback is the reward. The final goal of the system is then to maximise the accumulated reward at long run (Rieser and Lemon, 2011). The optimal policy,\u3160, is a function that takes as argument the current state s and returns the optimal action a.\nDialogue can be formalised as a Markov Decision Process (MDP), which is a tuple M = (S, A, T, \u03b3, R) where:\n\u2022 S: A set of possible states that represent the dynamic environment.\n\u2022 A: A set of possible actions.\n\u2022T:S\u00d7A\u00d7S \u2192 [0, 1] is a transition probability function. For any action a \u2208 A(s) taken in a state s \u2208 S, the probability of transiting to the next state s' is given by T(s, a, s').\n\u2022 \u03b3: A discounting factor in the range of [0, 1], which controls the prediction horizon of the algorithm.\n\u2022 R: The reward function that specifies the reward gained at every state. It contains the information that guides the agent towards the goal. R is a function of the state that is bounded in absolute value by Rmax.\nA stationary policy is a map \u03c0 : S \u2192 A and the discounted infinite-horizon expected reward for starting in state s and following policy \u03c0 thereafter is given by the value function V\"(s) that satisfies the following Bellman Equation:\nV*(s) = R(s) + \\sum_{s'} T(s, \u03c0(s), s')V* (s') (2.1)"}, {"title": "Task Oriented Dialogue Systems", "content": "The discounted infinite-horizon expected reward for starting in state s, taking action a and follow- ing policy \u03c0 thereafter is given by the Q-function Q\" (s, a) that satisfies the following equation:\nQ\" (s, a) = R(s) + \u03b3\\sum_{s'} T(s, a, s')V* (s') (2.2)\nA policy \u03c0 is optimal in M if, for all s \u2208 S:\n\u03c0(s) = arg max_{a \u2208 A} Q* (s, a) (2.3)\nLikewise Q*(s, a, R) is the optimal Q-function of the optimal policy \u03c0* for a known reward func- tion R.\nDialogues can be modelled as an optimisation problem with Partially Observable Markov decision process (POMDP)s. It simulates the inherent dynamic behaviour of human conversations while deals with the uncertainty of spoken language (Roy et al., 2000; Williams and Young, 2007; Young et al., 2013b).\nA POMDP can be seen as a continuous-space Markov decision process (MDP) in terms of policy optimisation where the states are the belief states, which is partially observable. POMDPS have been proposed for spoken dialogue systems because the system is never sure about the user beliefs because of speech recognition errors due to noisy or spoken language disfluencies and hesitations (Roy et al., 2000; Young et al., 2013a). Since the state is uncertain, it is called the belief state b(s). An example of a POMDP dialogue system is presented in Section 2.3.2. The task of predicting the b(s) at a given time t is known as the task of Dialogue State Tracking (DST). The policy learning algorithm receives as input the b(s) and returns the optimal policy \u03c0* and a given time.\nThe belief state bt is a vector encoding a probability distribution over the different goals, dialogue acts and concepts that are discussed in the dialogue. In the same way, the dialogue action at is a vector encoding a probability distribution over the possible agent dialogue actions.\nMDP models have been proven to be inefficient for solving complex tasks. These models have trouble overcoming the cold start problem and/or suffer from the curse of dimensionality (Barto and Mahadevan, 2003). This pattern was also observed with models proposed recently (Mnih et al., 2013; Duan et al., 2016). To overcome this issue, (Parr and Russell, 1998) proposed to specify a hierarchy of tasks and to reuse parts of the state space across many sub-tasks, which can greatly improve both learning speed and agent performance.\nThe notion of temporal abstraction, in which a policy can be decomposed into sub-policies by calling temporally extended sub-tasks was first proposed by (Sutton et al., 1999). In order to consider hierarchical architectures with temporally extension, we have to generalise the MDP to the semi-Markov Decision Process (SMDP) (Parr and Russell, 1998) where actions can take a variable amount of time to complete. This creates a division between primitive actions that span over only one action and composite actions that involve an execution of a sequence of primitive"}, {"title": "End-to-End Dialogues", "content": "Three approaches to end-to-end dialogue system are identify: retrieval-based, generative-based and the combination of both. These approaches are truly end to end, which means the model learns through gradient-based optimisation. Retrieval-based approach treats dialogue as an infor- mation retrieval problem (Lowe et al., 2015; Wu et al., 2018), in which there is a set of candidate responses from which one is selected as system response, here dialogue is evaluated as an infor- mation retrieval problem (in terms of precision/recall). Generative-based models (Vinyals and Le, 2015; Sordoni et al., 2015; Serban et al., 2016; Goo and Chen, 2018) on the contrary use natural language generation to generate the system response and dialogue is evaluated as as a generation problem (in terms of comparison with multi-references for instance, with BLEU score). Both ap- proaches have been used for chit-chats or casual conversations. Retrieval-based models however have been also used for task-oriented solutions(Lowe et al., 2015). The combination of both, can use generation to paraphrase the retrieved answer. Another way is to compare generative and re- trieved responses. The interested reader can find more details about recent end-to-end approaches in (Ni et al., 2022).\nYet these approaches neglect the fact that dialogue is dynamic and highly dependent on the environment. Moreover, they do not consider the metrics usually used for evaluating dialogues such as task completion and user satisfaction. Generative approaches usually generate fluent answers, which are sometimes incoherent with the dialogue context. They can generate hallu- cinations, distortions or repetitions. Retrieval-based approaches are limited to a list of candidate responses, which needs to be created in advance, yielding answers without any context agreement. To overcome these limitations, (Sankar and Ravi, 2019) proposes a combination of reinforcement- learning and generative models. The successful ChatGPT is an example of generative model, carefully trained to follow instructions that also learns to rank its responses."}, {"title": "Pre-trained Language Models", "content": "With the success of Language Models, Deep Learning and Transformers (Vaswani et al., 2017), recent solutions proposed pre-trained models with self-supervision. These models outperformed the state-of-the-art in different Natural Language processing (NLP) tasks. They have been initially trained on a large quantity of texts. For instance, BERT has been trained on 800M words and 2,500M words of the BooksCorpus and Wikipedia respectively. GPT-3 is a huge pre-trained model with 175B of parameters, trained on large quantity of data.\nRecently, these pre-trained models have been also trained on conversations. For example, BlenderBot (Shuster et al., 2022) is an encoder decoder which has been trained on 1.5B of Reddit comments. DialoGPT is just a decoder that has trained on 147M of dialogues extracted from Red- dit. (Santra et al., 2021). ConveRT has been trained on 727M of dialogs. ChatGPT is presumably the result of finetuning GPT3 with carefully curated instructions with reinforcement learning for correctly rank the generated responses (i.e. learning to rank) (Ouyang et al., 2022).\nThe trend nowadays is to initialise deep learning models with a pre-trained one and then fine- tune them for a specialised task. However, fine-tuning with little data will often degrade the initialised weights. Therefore, recent optimisation for fine-tuning are : prompting (Lester et al., 2021), prefixes (Li and Liang, 2021) and adapters (Hu et al., 2021).\nDespite very promising, the great limitation is that pre-trained models are still static. If there are changes in the language or in the World, these changes will not be reflected in the model. Think about the BERT models pre-trained before the Covid pandemic, they do not contain any representation for the bunch of new vocabulary that emerged during the pandemics: Covid-19, Moderna, Pfizer, sanitary vaccination pass, test PCR, etc. It is worth noting that training these large models is computationally costly and they require Hyper-performance Computing (HPC). A research path is how to keep these models up to date avoiding catastrophic forgetting and reducing the carbon impact necessary during training."}, {"title": "Conversational Question Answering", "content": "Research on conversational question answering has gained increasing interest (Saha et al., 2018; Reddy et al., 2018; Choi et al., 2018b). It consists in sequences of question-answer pairs related to a document or to a knowledge graph. Complex sequential question answering (CSQA) con- tains open-domain conversations that treat complex linguistic phenomena such as co-references, ellipses, incompleteness (or under specification) as well as logical, comparative and quantita- tive reasoning (Saha et al., 2018). Two corpora containing discussions about a paragraph of a Wikipedia document have been made public, namely question answering in context (QuAC) (Choi et al., 2018b) and a conversational question answering challenge (CoQA) (Reddy et al., 2018). In addition, a workshop devoted to this topic has first created in 2017, search conversational artificial intelligence (SCAI), with the participation of academics and industrials\u00b9.\nParagraph-based Question-Answering can be seen as a problem of reading comprehension, in which given a candidate document and a question, it finds the correct answer in the document. These systems use attention mechanisms (Seo et al., 2017) and memory networks (Weston et al., 2014)."}, {"title": "Positioning My Contributions in the State-of-the-Art", "content": "Most of my work concerns task-oriented dialogue, which involves the different components pre- sented in Section 2.3. I worked on NLU, DM and more recently on Natural Language Gener- ation (NLG). I have adapted neural models for distinct dialogue components that are presented in Chapter 3. Since the past five years, I have also explored open-domain Conversational Ques- tion Answering (CQA) (Section 2.6) which I present in Chapter 4, with predictive and generative (encoder-decoder) models for question rewriting, reading-comprehension and knowledge-graph question generation. As an extension of the last approach, I have also explored graph-embeddings (Section 4.4) and graph verbalisation with language models (Montella et al., 2023). The contri- butions published the second half of this year or the work in progress are not included in this document.\nMy work has always followed the state-of-the-art at the time of publication. For a detail com- parison of each contribution summarised in this manuscript with the related work of its time, we invite the curious reader to check the corresponding publications. To provide a brief positioning of some contributions, the work on data collection, NLU, dialogue management and human eval- uation I made for the EmoSpeech corpus was the first of its kind: a set of dialogues (12 distinct types of dialogue) in a Serious Game and in French. Moreover, we were among the first to propose in 2013 data-augmentation with back-translation and distributional representations for balancing biased models (Gardent and Rojas-Barahona, 2013).\nThe work on inverse reinforcement learning followed on the seminal work of Ng et al. (2000). It differed from previous work Paek and Pieraccini (2008); Chandramohan et al. (2011); El Asri et al. (2012); Boularias et al. (2010) because of the Bayesian Inverse Reinforcement Learning (IRL) algorithm that was applied for learning the tutor (i.e. system) reward function from experts. Furthermore, experts were taken from twelve distinct types of conversations in a serious game that were Human-Human (i.e. The EmoSpeech corpus) and not Machine-Machine (not from simulated conversations).\nI proposed to enrich a subset of the corpus CoQA (Reddy et al., 2019) releasing the cor- pus CoQAR\u00b2 with up to three out-of-context question paraphrases per question in conversations (Chapter 4, Section 4.2). Unlike previous work, these paraphrases were made by professional English native annotators instead of using crowd-sourcing, guaranteeing fair earning and working conditions. We compared CoQAR to CANARD (Elgohary et al., 2019) that provided only one question rewriting per question in QuAC (Choi et al., 2018b). I experimented with RoBERTa for answer extraction in both datasets CoQA and CoQAR with a without rewriting. Surprisingly solv- ing the context through rewritten questions confuses RoBERTa, which is already good to solve co-references by itself, specially in short dialogue contexts as in these datasets.\nI also utilised contextual embeddings (DistilBERT (Sanh et al., 2020), TransformersXL (Dai et al., 2019)) for estimating the reward function in long dialogues. I pointed out at that time the limitation of BERT-like models to deal with long contexts, which, besides the notable improve- ments of recent years, is still an open research problem. We also explored neural generation models such as BART (Lewis et al., 2019) and T5 (Raffel et al., 2020) for contextual question generation, question rewriting (Section 4.2) and graph verbalisation (Montella et al., 2022, 2023). I explored together with Sebastien Montella and Johannes Heinecke structural adapters for graph verbalisation (Montella et al., 2023) just after low rank emerged (Hu et al., 2021) as a recom-"}, {"title": "Positioning My Contributions in the State-of-the-Art", "content": "mended way to optimally fine-tune LLMs.\nAfter the advent of Large Language Models (LLMs), I am now questioning the performance of these models in complex tasks that required planning, such as dialogue. First, we need an eval- uation methodology to assess the performance of LLMs in these tasks. Then we need to compare LLMs based reasoning (Wei et al., 2022; Yao et al., 2023, 2022) with Reinforcement Learn- ing (RL) approaches, and explore recent trends for learning complex strategies such as algorithm distillation (Laskin et al., 2022). I talk about these research paths in Chapter 6."}, {"title": "Contributions to Task-Oriented Dialogues", "content": "As introduced in Section 2.3, task-oriented dialogue systems search to accomplish a task. This task can be for instance, information seeking, in which the system search for items in a database according to the constraints obtained through natural language interaction. Thus, these constrains can be given by the user (\u2018I am looking for a restaurant') or can be enquired by the system ('which price range?', 'where about?'). Once the desired items are retrieved, the system informs the results back to the user. This kind of dialogue has rich interactions with dialogue acts such as: informing, requesting, clarifying, rectifying. State-of-the-art systems are centred on the information seeking task for a variety of domains: hotels, restaurants, touristic attractions, trains, flights, taxis, etc. Unfortunately, these dialogues are very specific and have difficulties to generalise to new domains and to more complex tasks (i.e. beyond information seeking).\nI present in this Chapter my contributions to TOD dialogues (Figure 2.1). I start by presenting my work on NLU in Section 3.1 and on Dialogue Management in Section 3.2. My contributions to NLU concerns the definition of an annotation schema for the French corpus EmoSpeech, in which 12 distinct dialogues were integrated in a serious game. The dialogues involved various characters representing the system and the player and they were triggered at different levels of the game quest. Dialogues were modelled as an information seeking task, in which the system is always providing information to the player. The set of dialogue acts and goals are introduced in Section 3.1, as well as the models that were trained. Data augmentation through paraphrases via back translation, dictionaries, lexical resources and distributional semantics is also presented in Section 3.1.1. Moreover, the task of spoken language understanding is studied by using deep neural models and few-shot learning through risk minimisation in Section 3.1.2. Finally, I present the work of the PhD candidate Sebastien Montella on graph embeddings in Section 4.4.\nThe contributions on Dialogue management are presented in Section 3.2. I first present my own work on inverse reinforcement learning to find the implicit reward followed by humans in the EmoSpeech dialogue corpus. I also present a reward estimation by using deep learning. Finally, I present the work of the PhD candidate Thibault Cordier on hierarchical imitation learning."}, {"title": "NLU in a Serious Game", "content": "Machine Learning (ML) approaches such as logistic regression classifiers and conditional random fields were the state-of-the art back in 2011. I could collect a corpus through Wizard-of-Oz ex- periments, define an annotation scheme and train Logistic Regression (LR) and Support Vector Machines (SVM) multi-class classifiers for the task of NLU. These models were integrated within distinct dialogues in a serious game. I will start by describing briefly this work that gave origin to four publications (Rojas-Barahona et al., 2012b,c; Rojas-Barahona and Gardent, 2012; Gardent and Rojas-Barahona, 2013).\nThe serious game is a multiplayer quest where the players (3 teenagers) seek to build a video game joystick in order to free their uncle trapped in the game. To build this joystick, the players must explore a factory and achieve 17 mandatory goals (find the plans, get the appropriate mould, retrieve some raw material from the storing shed, etc). In addition, they can increase their score by achieving optional goals which, when reached, provide them with extra information about the industry (therefore increasing their knowledge). In total, the players can achieve up to 28 goals by conducting 12 separate subdialogs in various parts of the virtual world. That is, dialogs in the game are long dialogs involving multiple players in various settings."}, {"title": "Language Understanding", "content": "3.2). Taking into account the game goals, the total number of categories to be learned is 27. When learning on subdialogs, the number of categories to be learned is smaller but so is the size of the training set. The features for the machine learning models were bag of words, in which stop words were filtered out, utterances were deaccented and converted to lower-case. In addition, we experimented with various context length using as features the 0 to 4 previous dialogue acts. Subdialog identifiers were also used when training the classifier on the whole dialogue. More details are given in (Rojas-Barahona et al., 2012a).\nWe also experimented using tf*idf filtering to limit the impact of frequent uninformative words. Moreover, we experimented penalising those categories with more training instances, since the data was highly skewed. Dialogue acts that relate to optional goals were often not fol- lowed up by the players resulting in data sparseness."}, {"title": "Spoken Language Understanding and Few-Shot Learning", "content": "The following work was published in Coling 2016 (Rojas Barahona et al., 2016). At that time the task of Spoken Language Understanding (SLU), namely semantic decoding, was seen as a sequence tagging problem with models trained and tested on datasets with word-level annotations (T\u00fcr et al., 2013; Mesnil et al., 2015; Yao et al., 2013; Sarikaya et al., 2011; Deoras and Sarikaya, 2013; Sarikaya et al., 2014). Nevertheless, spoken language understanding from unaligned data, in which utterances are annotated with an abstract semantics, faces the additional challenge of not knowing which specific words are relevant for extracting the semantics. This problem was tackled in (Zhou and He, 2011), by using conditional random fields (CRFs) driven by finely-tuned hand-crafted features. Other discriminative approaches that deal with unaligned data use some form of delexicalisation or mapping of the input to known ontological concepts (Henderson et al., 2012, 2014a). The main disadvantage of delexicalisation is the difficulty in scaling it, not only to larger and more complex dialogue domains but also to handle the many forms of language variation.\nWe proposed a semantic decoder that learns from unaligned data (Figure 3.2) and that exploits rich semantic distributed word representations instead of delexicalisation. The semantic decoder predicts the dialogue act and the set of slot-value pairs from a set of n-best hypotheses returned by an ASR. The prediction is made in two steps. First, a deep learning architecture is used for the joint prediction of dialogue acts and the presence or absence of slots. Second, the same architec- ture is reused for predicting the values of the slots that were detected by the first joint-classifier. The deep architecture combines sentence and context representations. A CNN (Collobert et al., 2011) is used to generate the sentence representation, while a LSTM network (Hochreiter and Schmidhuber, 1997) is used to generate the context representation. A non-linear function then combines the top layers of these neural networks and distinct Softmax layers are used to predict the dialogue act and slots in the first joint model. In the second model, a single Softmax predicts the possible values for each slot.\nWe evaluated our models on two datasets DSTC2 (Henderson et al., 2014b) and In-car (Tsi- akoulis et al., 2012) using accuracy, f-measure and the Item Cross Entropy (ICE) score (Thomson et al., 2008). Our models outperform previous proposed models, without using manually designed features and without any pre-processing of the input (e.g., stop words filtering, delexicalisation). They do this by exploiting distributed word representations and we claim that this allows semantic decoders to be built that can easily scale to larger and more complex dialogue domains."}, {"title": "Dialogue Manager", "content": "My work on dialogue management regards learning the reward function. First, I explored inverse reinforcement learning to infer the reward function from human conversations on the EmoSpeech dataset (Rojas-Barahona and Cerisara, 2014). Second, I trained a predictor of the interaction quality to infer the reward function in the PyDial dialogue framework (Rojas-Barahona, 2020). Last but not least, I co-supervised a PhD thesis on imitation learning to solve the problem of the scarce reward signal in dialogue systems. Besides imitation learning (Cordier et al., 2020), we also explored graph neural networks for handling policies in multi-domain and multi-task environments (Cordier et al., 2022). Furthermore, we use both imitation and graph neural policies for few-shot learning (Cordier et al., 2023)."}, {"title": "Bayesian Inverse Reinforcement Learning", "content": "Inverse reinforcement learning (IRL) was defined in (Ng et al., 2000) as the problem of recovering the reward function from experts' demonstrations. It learns an optimal reward, which leads to a decision policy that follows as closely as possible the examples provided by experts, while maximising the expected accumulated reward in the long run.\nIn (Ramachandran and Amir, 2007) we used Bayesian Inverse Reinforcement Learning (BIRL) to infer human behaviour in the context of the Emospeech serious game (Section 3.1.1), given evidence in the form of stored dialogues provided by experts, who played the role of several con- versational agents in the game. We also reduce the computational complexity in large state spaces by using the approach proposed by (Michini and How, 2012). Instead of designing in advance the reward function to \u201cproperly instruct players\u201d, which is a difficult and subjective task, we rather propose to learn it from humans.\nWe evaluated BIRL in terms of policy loss (Michini and How, 2012) and is compared against two baselines. The first one uses random rewards, while the second one exploits corpus-estimated locally optimal rewards (i.e., supervised learning). The results show that the proposed approach converges relatively quickly and consistently outperforms both baselines. This suggests that tak- ing into account the dynamic properties of the environment leads to virtual characters that better reproduce the behaviour of experts. Qualitatively, our models have thus learned to adequately inform users and provide help when needed."}, {"title": "States, Actions and Transitions", "content": "As shown in Table 3.1, there are 12 distinct conversations in the game between 7 virtual characters (VC) and 3 players. Each of these dialogues talks about mandatory and optional goals. The player either asks for information about these goals or asks for help. Accordingly, the virtual character either informs about the goals or provides help. It can also handle out of domain topics, misunderstandings or request information (see example of dialogue in Figure 3.1).\nWe designed coarse-grained states containing user and system contributions to the dialogue; either by explicitly asking about the domain specific tasks (i.e. the dialogue goals) or by producing general dialogue acts (e.g., greeting, asking for help, acknowledgments, etc). A binary variable that indicates whether the dialogue has finished is also included. With this state representation we have 32 states for the shortest dialogue (the first dialogue in Table 3.1), and 432 states for the longest dialogue (i.e., the third dialogue in Table 3.1 with 5 goals).\nState variables\n1. Has any of the characters ended the dialogue with a farewell action? : 1 for setting a terminal state, 0 otherwise."}, {"title": "Imitation Learning", "content": "This section introduces the work of the PhD candidate Thibault Cordier, who I co-supervised together with Dr.Tanguy Urvoy and Professor Fabrice Lefevre. This work explores imitation learning for learning the policy on single domains. It has been published in the NeurIPS workshop Human in the loop dialogue systems (Cordier et al., 2020).\nDeep RL (DRL) (Li, 2018) has achieved significant success on many complex decision- making problems and in particular in conversational AI (Gao et al., 2018). The ability to learn from few interactions is essential in dialogue applications because human interactions are scarce and costly. Unfortunately, standard RL algorithms usually require a large amount of interactions with the environment to reach good performances. One solution to speedup the learning process is to guide"}]}