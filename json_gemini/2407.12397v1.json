{"title": "Mamba-PTQ: Outlier Channels in Recurrent Large Language Models", "authors": ["Alessandro Pierro", "Steven Abreu"], "abstract": "Modern recurrent layers are emerging as a promising path toward edge deployment of foundation models, especially in the context of large language models (LLMs). Compressing the whole input sequence in a finite-dimensional representation enables recurrent layers to model long-range dependencies while maintaining a constant inference cost for each token and a fixed memory requirement. However, the practical deployment of LLMs in resource-limited environments often requires further model compression, such as quantization and pruning. While these techniques are well-established for attention-based models, their effects on recurrent layers remain underexplored.\nIn this preliminary work, we focus on post-training quantization for recurrent LLMs and show that Mamba models exhibit the same pattern of outlier channels observed in attention-based LLMs. We show that the reason for difficulty of quantizing SSMs is caused by activation outliers, similar to those observed in transformer-based LLMs. We report baseline results for post-training quantization of Mamba that do not take into account the activation outliers and suggest first steps for outlier-aware quantization.", "sections": [{"title": "1. Introduction", "content": "Attention-based models, also known as Transformers (Vaswani et al., 2023), constitute the current state-of-the-art backbone for large language models (LLMs) (Brown et al., 2020). However, their powerful modeling capabilities come with significant computational requirements, resulting in high inference costs and limiting the deployment on edge and low-power devices. Novel recurrent neural network (RNN) architectures, informed mainly by recent work on state space models (SSMs) (Gu et al., 2020; 2022), are now emerging as promising alternatives for sequence modeling tasks, either in isolation (Poli et al., 2023; Gu & Dao, 2023; Peng et al., 2023) or as hybrid models interleaving recurrent and attention blocks (De et al., 2024; Lieber et al., 2024; Botev et al., 2024). In particular, RNNs compress the input sequence into a finite-dimensional representation, decoupling the computational and memory cost of each token's forward pass from the sequence's length. Hence, they provide better scalability to long context scenarios than vanilla self-attention, which scales quadratically with sequence length.\nHowever, similarly to Transformers, deploying recurrence-based LLMs at scale or in resource-constrained environments requires advanced model optimization techniques, such as quantization, pruning, and knowledge distillation. While applying these techniques starts to be well understood in the context of attention-based LLMs, model optimization for recurrent and hybrid architectures remains an important yet underexplored topic. In this paper, we focus on quantization and analyze its impact on Mamba (Gu & Dao, 2023) model family, drawing connections to previous work on quantized LLMs."}, {"title": "2. Quantization and outlier channels in LLMs", "content": "Quantization is a compression technique that reduces the numerical precision of a model's weights and activations to integer datatypes in order to facilitate inference (Jacob et al., 2017). We adopt symmetric per-tensor quantization: given a tensor x and a bit precision n, its quantized representation is computed as:\n$X_n = \\frac{(2^{n-1} - 1)x}{max |x|} = [s_x x]$\nwhere the quantization scale $s_x$ is a scalar. The benefits of quantization for inference efficiency are twofold. Firstly, weight quantization reduces the memory footprint of the model, which is especially beneficial in the memory-bound regime of autoregressive generation. Secondly, when both weights and activations are quantized, matrix multiplications can be offloaded to the integer processing units, which typically offer higher throughput and energy efficiency than floating point units.\nMost state-of-the-art techniques for quantizing LLMs are based on the empirical observation of outlier channels (Bondarenko et al., 2021), a small percentage of model dimensions with a dynamic range that is consistently larger than the rest. This phenomenon complicates activation quantization since the large abs max values from the outlier channels deteriorate the effective bit precision of the remaining channels. A possible solution would be maintaining a different quantization scale for each channel, which is not hardware-friendly on current GPU architectures (Xiao et al., 2024). Various strategies have been proposed to circumvent this issue. For instance, some methods treat outlier channels separately, either by maintaining them in floating point format (Dettmers et al., 2022) or by representing them with two integer channels each (Zhang et al., 2024). Other approaches modify the transformer architecture to prevent the emergence of outliers (Bondarenko et al., 2023), while some partially shift the quantization difficulty to the weights, thereby mitigating the impact of outliers (Xiao et al., 2024).\nWe make the first steps towards post-training quantization for recurrent LLMs, focusing on the Mamba (Gu & Dao, 2023) model family. We analyse the activation patterns of Mamba to assess the presence of outliers, which we define as those channels having an absolute maximum activation beyond six standard deviations from the layer mean, following prior practice (Bondarenko et al., 2021). Figure 1 reports the pre-activations of the linear block of a layer from Mamba-130m (similar results were observed for the other"}, {"title": "3. Method", "content": "3.1. Mamba model\nExisting state space models are described by the following dynamics:\n$x_k = Ax_{k-1} + Bu_k$\n$y_k = Cx_k + Du_k$\nwhere A is the recurrent matrix, B is the input matrix, C is the output matrix, and D is the residual matrix from the input to the output. State space models of this form must treat every input token equally, as the input matrix B is fixed, such that the SSM cannot focus on or ignore specific tokens. This is a major shortcoming compared to transformer architectures, whose attention mechanism allows for such interactions and thus limits the performance of SSMs, especially on language tasks.\nThe key innovation of Mamba over previous SSMs is its ability to perform such content-based reasoning. By letting Mamba's parameters depend on the input, the model effectively gains the ability to filter out irrelevant information so that the relevant context can be compressed more efficiently into the hidden state. Specifically, the parameters for Mamba's SSM block are obtained by:\n$B_t, A_t, C_t = W_{proj} u_t$\n$\\Delta_t = \\sigma_+ (W_{dt} \\Delta_t)$\n$A_t = exp(- exp(A_{log} \\Delta))$\n$B_t = A_t B_t$\nwhere $u_t$ is the input to the SSM block, $W_{proj}$, $A_{log}$ and $W_{dt}$ are time-invariant weight matrices, $\\o_+$ denotes the soft-plus function and denotes element-wise multiplication. The weight matrices $B_t$ and $C_t$ are thus directly dependent on the input $u_t$, whereas the recurrent matrix $A_t$ is dependent on the input $u_t$ only through the input-dependent timescale parameter $\\Delta_t$. The hidden state $h_t$ and output $y_t$ of the SSM block is then computed as:\n$h_t=A_th_{t-1} + B_t U_t$\n$Y_t = C_th_t + D_t U_t$\nAs shown in Figure 1, each layer in the Mamba architecture also includes additional gating, nonlinearities, normalization, causal convolution, and linear blocks.\n3.2. Baseline quantization\nIn order to quantize Mamba, we distinguish between Mamba's pre-trained weights and its activations. Importantly, due to the input-dependent parameterization, we consider only input-independent parameters as weights, such as $A_{log}$, while we consider input-dependent parameters like $A_t$ as activations.\nWe adopt symmetric, per-tensor quantization for weights and activations as described in section 2, using the absolute maximum (absmax) of the tensor for calibration.\nFor our experiments using naive quantization on the activations, we quantize the output from all linear layers (including the matrices $B_t, A_t, C_t$ from Equation 4), but we do not quantize the effective weight matrices $A_t, \\Delta_t, B_t$. We further do not quantize the output from the SSM block\n3.3. Outlier-aware quantization (e.g., SmoothQuant)\nThe naive absmax quantization is sensitive to outliers. A large value in the tensor x will yield a small scale $s_x = \\frac{max|x|}{2^{n-1}-1}$, thus leading to larger rounding errors for the same n-bit quantization precision. As discussed in section 2, outliers (particularly in activations) are the subject of research in LLM quantization.\nMost notably, the SmoothQuant method proposed by Xiao et al. (Xiao et al., 2024) exploits the fact that outliers exist in activations but not in the weights. SmoothQuant smooths the activation outliers by partially taking them into the preceding weights. Because activation outliers typically persist in the same activation channels, a weight matrix with per-channel quantization can absorb part of the quantization difficulty from the subsequent activations. As such, SmoothQuant introduces a per-channel smoothing factor $s \\in R^{C_i}$ where $C_i$ is the dimension of the activations X and, equivalently, the number of output channels of the weight matrix W. This smpoothing factor is used to scale the weights and activations:\n$(Xdiag(s)^{-1}) (diag(s)W) = XW$\nThe aim is to choose a smoothing factor s so that $X= Xdiag(s)^{-1}$ is easy to quantize. However, simply choosing $s_j = max(|X_j|$ where $j = 1,..., C.$ to minimize the difficulty in quantization activations, will push all these difficulties into the weights. On the other hand, we can choose $s_j = 1/max(|W_j|)$ to move all the quantization difficulty from the weights into the activations. The authors propose a new hyperparameter, the migration strength a, to control how much difficulty we want to migrate from activations to weights, using the equation:\n$S_j = \\frac{max(|X;|)^\\alpha}{max(|W_j|)^{1-\\alpha}}$\nwhere a smaller $\\alpha$ will leave more difficulty with the activations, and a larger $\\alpha$ will migrate more difficulty to the weights. The authors suggest to use a default value of $\\alpha$ = 0.5 and a larger $\\alpha$ for models where activation outliers are more significant such that more quantization difficulty is moved into the weights."}, {"title": "4. Experiments", "content": "4.1. Experimental setup\nWe assess the impact of different quantization configurations on the zero-shot accuracy of six downstream tasks: LAMBADA (Paperno et al., 2016), HellaSwag (Zellers et al., 2019), PIQA (Bisk et al., 2019), WinoGrande (Sakaguchi et al., 2019), RTE (Wang et al., 2019), and COPA (Roemmele et al.). We explicitly neglect perplexity benchmarking, since prior work noted how it may not be informative of the actual task performance degradation (Sun et al., 2021).\nWe run three different experimental conditions:\n1.  To assess the importance of outlier channels, we analyze the impact of removing outlier channels on downstream task accuracy.\n2.  We then analyze the effect of naive quantization of only the pre-trained weights on downstream task accuracy."}, {"title": "5. Discussion", "content": "In this preliminary work, we make the first steps towards post-training quantization of Mamba, in order to inform future edge deployments of recurrent LLMs based on selective state space models such as Mamba. We have shown that the difficulty of quantizing Mamba is caused by activation outliers, similar to those observed in transformer-based LLMs. We presented baseline results for post-training quantization of Mamba that does not take into account the activation outliers and a first proposal for outlier-aware quantization of Mamba.\n5.1. Future work\nAs this area is under rapid development, several opportunities exist to extend this work. Firstly, a similar analysis could be performed on other recurrent LLMs, such as the RWKV family (Peng et al., 2023), the novel Mamba-2 architecture (Dao & Gu, 2024), or hybrid models such as Griffin (De et al., 2024) and RecurrentGemma (Botev et al., 2024). Secondly, additional work should be done to convert the SSM dynamics fully to integer operations, as previously demonstrated by (Blouw et al., 2021), and explore the use of quantized activations. Lastly, it will be interesting to see how quantized recurrent LLMs perform at the edge in energy-constrained scenarios for real-time multimodal processing (Shrestha et al., 2024), as the specific of the hardware architecture could provide additional guidance on model compression requirements."}, {"title": "A. Additional results", "content": "Herein we present all additional experimental results from the experiments presented in this paper.\nA.1. Impact of removing outlier channels\nTable 2 shows the accuracy on all evaluated tasks for the Mamba-130m model and Mamba-2.8B model, with different rows indicating the outlier removal specific to particular layers, or across the entire model.\nA.2. Impact of quantization on downstream task accuracy\nTable 3 shows the accuracy on all evaluated tasks for all Mamba models and all quantization configurations."}]}