{"title": "JustLogic: A Comprehensive Benchmark for Evaluating Deductive Reasoning in Large Language Models", "authors": ["Michael K. Chen", "Xikun Zhang", "Dacheng Tao"], "abstract": "Logical reasoning is a critical component of Large Language Models (LLMs), and substantial research efforts in recent years have aimed to enhance their deductive reasoning capabilities. However, existing deductive reasoning benchmarks, which are crucial for evaluating and advancing LLMs, are inadequate due to their lack of task complexity, presence of prior knowledge as a confounder, and superficial error analysis. To address these deficiencies, we introduce JustLogic, a synthetically generated deductive reasoning benchmark designed for rigorous evaluation of LLMs. JustLogic is (i) highly complex, capable of generating a diverse range of linguistic patterns, vocabulary, and argument structures; (ii) prior knowledge independent, eliminating the advantage of models possessing prior knowledge and ensuring that only deductive reasoning is used to answer questions; and (iii) capable of in-depth error analysis on the heterogeneous effects of reasoning depth and argument form on model accuracy. Our experimental results on JustLogic reveal that most state-of-the-art (SOTA) LLMs perform significantly worse than the human average, demonstrating substantial room for model improvement. All code and data are available at https://github.com/michaelchen-lab/JustLogic", "sections": [{"title": "1. Introduction", "content": "Deductive reasoning is a crucial capability for large language models (LLMs). It refers to the process of creating logically valid arguments, where conclusions necessarily follow from the premises. In other words, if an argument's premises are true, the conclusion must also be true. Recent state-of-the-art (SOTA) LLMs (Achiam et al., 2023; Dubey et al., 2024; Jiang et al., 2023) have exhibited outstanding performance and consistent improvement across various reasoning benchmarks, including HelloSwag (Zellers et al., 2019), ARC Challenge (Clark et al., 2018) and WinoGrande (Sakaguchi et al., 2021). However, we argue that the existing benchmarks are insufficient and often ineffective for evaluating LLMs' true deductive reasoning capabilities.\nWe identify three major problems with the existing benchmarks. First, they lack complexity, as measured on two dimensions: natural language complexity, i.e. how arguments are linguistically expressed, and argument complexity, i.e. the structure of the argument itself. Manually curated datasets, such as FOLIO (Han et al., 2022) and LogiQA 2.0 (Liu et al., 2020; 2023a) exhibit high natural language complexity but low argument complexity, while synthetic datasets like CLUTRR (Sinha et al., 2019) and ProofWriter (Tafjord et al., 2020) exhibit the opposite. Simplicity in either dimension makes these benchmarks prone to overfitting and memorization, thus allowing models to perform well despite underlying weaknesses in logical reasoning. A more detailed analysis can be found in Section 3.4. Second, existing benchmarks often fail to test deductive reasoning in isolation, as models can benefit from prior knowledge. To empirically validate this claim, we developed a novel test for prior knowledge independence, which measures the influence of prior knowledge on reasoning benchmarks. As detailed in Section 5.1, prior knowledge can substantially increase accuracy, even in datasets not intended to require commonsense or domain knowledge, e.g. FOLIO and LogiQA 2.0. Thus, high accuracy may not reflect strong reasoning capabilities. Third, many existing benchmarks provide superficial error analysis, leaving key questions unanswered: At what reasoning depth does the model start to fail? How does the model compare to humans at different argument depths? Which argument forms is the model particularly weak at? These insights are essential for understanding the depth and robustness of a model's deductive reasoning, yet many benchmarks provide them. Section 5.3 demonstrates the importance and usefulness of comprehensive error analysis.\nDue to these issues, LLMs' deductive reasoning abilities remain ambiguous. In response to the critical need for a reliable benchmark to support ongoing research efforts,"}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Existing reasoning datasets for Large Language Models", "content": "Reasoning benchmarks are a vital part of LLM evaluation. Some benchmarks measure deductive reasoning in conjunction with natural language inference (NLI), inductive reasoning, and commonsense knowledge: HellaSwag (Zellers et al., 2019) tasks machines to select the most likely follow-up of an event description, WinoGrande (Sakaguchi et al., 2021) is a pronoun resolution task, and MuSR (Sprague et al., 2023) tasks machines to solve fictional problems, such as murder mysteries. Other benchmarks measure reasoning on domain knowledge: AI2 Reasoning Challenge (ARC) (Yadav et al., 2019) contains grade-school science questions, while Massive Multitask Language Understanding (MMLU) (Hendrycks et al., 2020) contains questions across 57 subjects in STEM, humanities, and more. Finally, math-specific benchmarks include GSM-8K (Cobbe et al., 2021) and DROP (Dua et al., 2019).\nThe aforementioned benchmarks explicitly evaluate skills beyond reasoning and do not specifically define the type of reasoning involved, e.g. inductive, deductive, and analogical. As such, benchmarks that solely test for deductive reasoning have seen a considerable increase in interest. They can be classified into two broad categories: synthetic and manually curated. Synthetic datasets include (i) CLUTRR (Sinha et al., 2019), where a machine must infer the relationship"}, {"title": "2.2. Reasoning in Large Language Models", "content": "As LLMs continue to increase in size, their performance on various reasoning-related benchmarks has improved dramatically. For example, in 2024, Gemini Ultra (Team et al., 2023) achieved 90.0% on MMLU when the SOTA model in 2020, UnifiedQA 11B (Khashabi et al., 2020), achieved a mere 48.9%. In 2023, GPT-4 achieved 96.4% on ARC when the SOTA model in 2020, GPT-3 (Brown, 2020), achieved 53.2%.\nThe advent of prompting techniques played an important role in developing LLMs' reasoning abilities. In-context learning (Dong et al., 2022) provides LLMs with instructions and examples in the input prompt to guide its response. Chain-of-thought prompting (Wei et al., 2022) prompts LLMs to generate a series of intermediate reasoning steps before arriving at the final answer. Self-consistency decoding (Wang et al., 2022) chooses the most consistent answer after sampling multiple chain-of-thought outputs. Least-to-most prompting (Zhou et al., 2022) decomposes a complex problem into simpler subproblems, which are then solved sequentially.\nAs mentioned above, LLMs are conventionally tested on datasets that combine reasoning with other skills. Moreover, existing logical reasoning-specific datasets possess major limitations that call into question the reliability of their evaluations. In response, JustLogic aims to robustly and accurately evaluate the deductive reasoning abilities of LLMs."}, {"title": "3. Dataset Construction", "content": "JustLogic is a programmatically generated dataset designed to evaluate a model's ability of deductive reasoning, specifically its capability to form logically valid arguments. A logically valid argument is one where the conclusion necessarily follows from the premise(s); in other words, given the premises are true, the conclusion must also be true.\nIn order to test this, JustLogic presents a model with a paragraph consisting of premises, followed by a query statement. Based solely on the premises and assuming they are all true, the model needs to determine whether the query statement is true, false, or uncertain. In line with the open-world assumption, the \"Uncertain\" answer refers to cases where the premises neither support nor contradict the query statement.\nThe following outlines the process for generating each instance in the dataset:\n1. Step 1: Generate an argument structure\n2. Step 2: Add natural language statements to the argument structure\n3. Step 3: Generate a query statement"}, {"title": "3.1. Step 1: Generate argument structure", "content": "Argument structures are composed of one or more valid argument forms, derived from propositional logic; argument forms are made up of a series of logical forms, which we define as symbolic representations of statements. Specifically, the seven distinct argument forms in our dataset are constructed with the following four logical forms: (i) basic (x), (ii) negation (\u00acx), (iii) conditional (x \u2192 y), and (iv) disjunction (x V y). While there is a theoretically infinite number of possible argument forms, complex argument forms can be derived by combining simpler ones. Therefore, we explicitly define the eight most fundamental forms (Johnson, 2006): modus ponens, modus tollens, hypothetical syllogism, disjunctive syllogism, reductio ad absurdum, constructive dilemma, and disjunctive elimination. \nThe function to create an argument structure accepts an intended argument depth as input. It first generates a random conclusion and an argument form to support it. If the intended depth has not been reached, one or more premises will become subconclusions, which are supported by new, randomly generated argument forms, thus increasing the argument's depth. This process continues until the target depth is achieved."}, {"title": "3.2. Step 2: Adding natural language", "content": "Once the argument structure is generated, it serves as the skeleton of the paragraph, and the next step is to convert the statements in logical form into natural language. Each statement consists of one or more logical forms, i.e. variable, negation, conditional, and disjunction. In natural language, these forms can be expressed in a variety of ways. For example, a conditional can be expressed as both \"If x, then y.\" and \"Given that x, y is true.\", where variables x and y are simple propositions. To emulate the diversity of natural language, we manually create a list of expressions for each logical form with the help of GPT-4 (Achiam et al., 2023) and human feedback. \nThe variable(s) within each expression is ultimately replaced by randomly selected generic, real-world sentences from GenericsKB-Best (Bhakthavatsalam et al., 2020). The GenericsKB-Best dataset is chosen for its vast collection of simple propositions (1,020,868 sentences) without conditionals, disjunctions, etc.\nNotably, the statements are generally factually inaccurate despite being drawn from real-world data. This is intentional. Real-world propositions allow us to generate sentences with diverse grammatical structures that closely emulate human-written arguments. However, factually accurate arguments enable models to bypass deductive reasoning with their prior real-world knowledge, which is experimentally demonstrated in Section 5.1. By using real-world yet factually inaccurate statements, we combine realism and prior knowledge independence.\nThere is a potential concern that factually inaccurate statements may confuse models and therefore lead to artificially low performance. Our empirical study in Appendix G proves that this concern does not hold true in practice."}, {"title": "3.3. Step 3: Generate query statement", "content": "The LLM's task is to determine whether the given query statement is true, false, or uncertain based on the premises provided. If we assign the query statement to be the negation of the conclusion, i.e. \"It is not true that Japan is in Asia\u201d, then the answer is false. If the query statement is the same as the conclusion, then the answer is true. If the query statement is unrelated to the premises, then the answer is uncertain."}, {"title": "3.4. Dataset Complexity", "content": "In the context of deductive reasoning datasets, complexity is defined as the variety and comprehensiveness of instances. It can be further divided into two dimensions: natural language complexity and argument complexity. In this section, we highlight the significance of both aspects and how JustLogic compares against other logical reasoning datasets.\nNatural language complexity. Human language is complex. Statements and arguments of similar meanings can be presented in a variety of ways. Therefore, it is insufficient for models to reason solely with symbols, e.g. x and y, and basic natural language sentences, e.g. \"Some birds are yellow.\"; they must be capable of reasoning with real-world vocabulary and diverse sentence structures to be useful in practical contexts.\nWe measure natural language complexity with (i) reading difficulty, as measured by the Flesch-Kincard readability test (Kincaid, 1975), and (ii) lexical diversity, as measured by vocabulary & domain size. A domain is defined as any topic of interest, such as golf, computers, or traveling; Vocabulary size refers to the number of unique words in the dataset.\nArgument complexity. Argument complexity refers to the diversity of argument structures used in the dataset. A sufficiently high argument complexity is important because humans use a range of argument forms to reason, beyond just conditionals and modus ponens. Moreover, a real-world argument is typically composed of multiple argument forms, due to the inherent complexity of real-life scenarios.\nWe evaluate a dataset's argument complexity based on two metrics: (i) the range of reasoning depth, and (ii) the number of unique argument structures. The upper limit of both metrics is calculated based on the theoretical maximum without any additional human input, rather than the highest depth used in experiments in existing works. \nIn summary, JustLogic combines the best aspects of both dataset construction methods, incorporating the argument complexity of synthetic datasets and the natural language complexity of manually curated ones."}, {"title": "3.5. Future-proofing JustLogic", "content": "As the reasoning abilities of LLMs continue to improve, we expect LLMs to solve the existing JustLogic dataset eventually. To maintain JustLogic's relevance, we leverage its synthetic nature to increase complexity with minimal human input. Argument complexity can be adjusted by increasing the (i) range of argument depth and (ii) number of distinct argument forms to >7. Natural language complexity can be adjusted by (i) increasing the number of expressions for each logical form and (ii) integrating more complex knowledge bases than GenericsKB. Importantly, these changes are programmatically achievable with minimal man-hours.\nImportantly, JustLogic can also effectively tackle benchmark leakage (Xu et al., 2024), whereby test sets are unintentionally included in LLMs' pertaining data, thus artificially inflating their performance through memorization. Should JustLogic's test set be leaked, a new test set of similar difficulty can be trivially generated, thereby mitigating this problem."}, {"title": "4. Experimental Setup", "content": "We first investigate the influence of prior knowledge on evaluating deductive reasoning with JustLogic and other existing benchmarks using our prior knowledge independence test. Next, several SOTA LLMs of various sizes are evaluated using JustLogic. Finally, an in-depth error analysis of the LLMs' results is conducted.\nJustLogic contains 7000 instances, equally split amongst reasoning depths ranging from 1 to 7. It is then divided into train/validation/test sets (70%/15%/15%). Train and validation sets facilitate in-context learning and model fine-tuning if required, while the test set is used for evaluation. Note that the number of instances and range of reasoning depths can be easily adjusted using JustLogic's open-source dataset generation program."}, {"title": "4.1. Prior Knowledge Independence Test", "content": "The task for deductive reasoning benchmarks is typically framed as CQO \u2192 A: Given a context C, consisting of n premises (P = {P1, P2, ..., Pn}), a question Q, and m answer options (O = {01, 02, ..., 0m}), determine the correct answer A. To assess the influence of prior knowledge on determining answer A, the prior knowledge independence test is framed as QO \u2192 A. No context C is provided, and the prompt instructs the LLM to answer the question based on prior knowledge alone.\nIf prior knowledge is not useful, the LLM should be unable to answer question Q without C, and the accuracy for the prior knowledge independence test should approximate random probability. Benchmarks exhibiting such accuracies are deemed prior knowledge independent.\nAny LLM capable of using prior knowledge can be used for this test. However, models with larger parameter sizes, and thus more extensive prior knowledge, are more likely to exhibit notable differences in accuracies. For our experiment, we use GPT-4. The test is conducted on both JustLogic and existing benchmarks, including CLUTRR, ProofWriter, LogiQA 2.0, and FOLIO."}, {"title": "4.2. Evaluation of LLMs' Deductive Reasoning", "content": "Our task follows the conventional formulation: CQO \u2192 A. Question Q is \"Is the statement S true, false, or uncertain?\u201d, followed by the query statement; there are 3 answer options, where O = {true, false, uncertain}. All prompts begin with a preamble, which includes (i) the requirements of the task at hand, (ii) a list of argument forms in propositional logic, and (iii) the available answer options.\nWe evaluated various models of different sizes, including Llama3-8B (Dubey et al., 2024), Llama3-70B, GPT-4, GPT-40, OpenAI 01 and OpenAI 01-preview (Jaech et al., 2024). Given that prompt quality significantly impacts LLM accuracy, a range of prompting techniques are tested: zero-shot, few-shot, and chain-of-thought (CoT) (Wei et al., 2022). OpenAI's reasoning models had strict rate limits at the time of writing. As such, 42 random samples in the test set are used for OpenAI 01 and OpenAI 01-preview. To ensure fairness, the selected subset has the same proportion of reasoning depth and classes (True, False, and Uncertain) as the entire test set.\nWe also measured human performance. 18 anonymous participants, recruited from Amazon Mechanical Turk (Amazon, 2005), are given a random subset of questions. This is because deductive reasoning questions, especially those"}, {"title": "5. Results", "content": ""}, {"title": "5.1. Prior Knowledge Independence Test", "content": "The results of JustLogic and four other benchmarks are shown ; note that lower accuracy relative to the benchmark's random probability indicates that prior knowledge is more detrimental to answering the question, thereby demonstrating that the benchmark is more prior knowledge independent. Thus, the smaller the || between model accuracy and random probability, the better. The As of CLUTRR and ProofWriter are relatively low, while those of LogiQA 2.0 and FOLIO are nontrivially higher. This is because the former are synthetic datasets, while the latter are manually curated. When a question is code-generated, it generally bears no correlation with reality, e.g. \u201cIs it true, false, or uncertain that Gary is not red."}, {"title": "5.2. Evaluation of LLMs' Deductive Reasoning", "content": "As shown , the best-performing model by a large margin is OpenAI 01-preview with an accuracy of 81.0%. The second and third-best models, GPT-40 and Llama3-70B, achieved 65.6% and 64.6% respectively. Surprisingly, OpenAI 01 (64.3%) performs substantially worse than OpenAI ol-preview due to developer-imposed limits on test-time compute; OpenAI 01-preview is a better representation of SOTA reasoning models' capabilities. Models with larger parameter sizes generally perform better than smaller models, assuming the same prompting methods are used. \nMoreover, the improvements offered by increasing model size pale in comparison to those offered by better prompting methods. Using chain-of-thought prompting, Llama3-8B achieved higher performance (57.8%) than zero-shot Llama3-70B (53.1%).\nHuman performance (73.0%) is significantly higher than all models besides OpenAI 01-preview, while the human ceiling (100.0%) outperforms all models. The non-trivial gap between the human ceiling and the best-performing model (81.0%) shows that models still have significant room for improvement."}, {"title": "5.3. Error Analysis", "content": "Figure 3 illustrates the model accuracy by argument form (left) and by reasoning depth (right). It shows the statistics for (i) Llama3-8B with CoT prompting, chosen for its superior performance amongst smaller LLMs, (ii) Llama3-70B with CoT prompting, chosen due to its superior performance amongst medium-sized LLMs, and (iii) OpenAI 01-preview, chosen for its overall highest performance. Note that OpenAI 01-preview is excluded from the argument form analysis due to insufficient samples;\nThe accuracies of some argument forms are evidently better than others. For example, hypothetical syllogism and constructive dilemma achieve considerably higher performance than modus tollens and reductio ad absurdum. We hypothesize that these forms appear less frequently in the models' training data.\nAs for reasoning depth, model accuracies generally decrease as depth increases, consistent with expectations that accuracy declines as the complexity of questions increases. Interestingly, Llama3-70B performs comparably to OpenAI o1-preview for instances with a depth of 1, but Llama3-70B sees a sharp decline in performance once depth is increased, while OpenAI ol-preview only sees a moderate decline;"}, {"title": "6. Conclusion", "content": "Deductive reasoning is one of the key challenges in LLM research. In response to the lack of reliable benchmarks, we present JustLogic, a natural language deductive reasoning dataset that is (i) highly complex, (ii) prior knowledge independent, and (iii) capable of in-depth error analysis. These qualities are enabled by JustLogic's dataset construction method: argument structures are synthetically generated, and natural language is programmatically incorporated via expression templates and a knowledge base. We empirically justify JustLogic's merits: most LLMs underperform the human average and all significantly underperform the human ceiling. We demonstrate that JustLogic is a highly challenging, future-proof benchmark that is reliable and insightful for evaluating logical reasoning in LLMs."}, {"title": "A. Argument Forms", "content": ""}, {"title": "B. Sample texts from deductive reasoning benchmarks", "content": "Beyond metrics like vocabulary size and number of domains, the degree of natural language complexity can be straightforwardly determined by manually inspecting the linguistic patterns of a given benchmark.\nEvidently, JustLogic exhibits significantly greater natural language complexity than CLUTRR, ProofWriter, ProntoQA-OOD, and SimpleLogic, because the latter benchmarks programmatically generate every sentence, while JustLogic extracts its sentences from GenericsKB, a natural language text database. Thus, the former benchmarks rely on a limited number of grammar templates, reducing their linguistic complexity. JustLogic exhibits similar levels of complexity to FOLIO. LogiQA 2.0 is more complex because it is human-curated and not backed by a formal logic system (unlike how JustLogic is backed by propositional logic). Without a formal logic system, LogiQA 2.0's argument complexity suffers, as shown , which compromises its ability to evaluate deductive reasoning in LLMs."}, {"title": "C. Prior Knowledge Independence Test", "content": "A sample prompt for the prior knowledge independence test, based on the example in Figure 1"}, {"title": "D. Experiment Implementation Details", "content": "The hyperparameters for the Llama3 models are decided largely based on the recommendations in the original paper Dubey et al. (2024)"}, {"title": "E. Performance of OpenAI 01 and o1-preview", "content": "OpenAI 01 (64.3%) performs substantially worse than OpenAI 01-preview (81.0%) on JustLogic. These results are unintuitive, given that o1 is released after 01-preview.\nFigure 5, showing OpenAI ol's and ol-preview's accuracy over various depths, reinforces our analysis.\nGiven that the reduced compute on OpenAI ol is potentially a cost-saving measure, our results on OpenAI 01-preview is likely a better representation of OpenAI reasoning models' capabilities."}, {"title": "F. Qualitative Analysis of Failure Modes", "content": "To identify exactly how JustLogic is challenging for existing LLMs, we conducted a qualitative analysis to identify the 4 major failure modes of various models' responses to JustLogic questions. We primarily rely on analyzing the chain-of-thought responses to investigate how they produced the wrong answers. High-level explanations and abbreviated examples are provided for the failure modes.\n(1) Logical inconsistency. Models sometimes produce arguments that rely on premises, implicit or otherwise, that contradicts earlier parts of their chain-of-thought. This causes models to generate incoherent arguments, and ultimately arrive at the wrong conclusion. For example, the following is a truncated response from OpenAI 01-preview:\nThis line of reasoning suggests that MVR implies MV S. For this to be the case, R \u2192 S must be true. However, earlier in the reasoning chain, ol-preview says \"we cannot definitively state R \u2192 S\". This argument is therefore logically inconsistent.\n(2) Wrong application of argument forms. Models sometimes identify the correct argument form to solve the question. However, mistakes are made when applying the form to the specific question context. In the truncated response below by Llama3-70B, disjunctive syllogism is, in fact, the appropriate argument form, but the opposite conclusion should have been made: the statement, pain can be severe and make a person unable to perform normal activities, does not hold.\n(3) Using the wrong argument form. Sometimes, models attempt to use argument forms that logically cannot be applied to the context. Such mistakes often derail the response, making it entirely incoherent. In the following example of an output by Llama3-70B, modus tollens simply cannot be applied here; reductio ad absurdum should have been used instead.\n(4) False interpretation of facts. Models sometimes misinterpret the natural language facts entirely."}, {"title": "G. Impact of Factual Accuracy on Model Performance", "content": "Given that JustLogic randomly chooses sentences from GenericsKB to add to each instance's argument structure, the final conclusion may be factually accurate or inaccurate in the real world. For example, if the conclusion is \"It is not true that Japan is in Asia.\", then the conclusion is factually inaccurate.\nTo study these concerns, we conduct the following empirical study. If the above concerns are true, we expect factually inaccurate conclusions to perform worse than factually accurate ones. Because all GenericsKB sentences are factually accurate, we can straightforwardly deduce each conclusion's factual accuracy. \nThese results reject the hypothesis that factually inaccurate conclusions perform worse than factually accurate ones; there is no consistent trend between both conclusion types. In fact, when depth=1, factually inaccurate conclusions exhibit higher performance! This trend is somewhat reversed when depth is 7 or less, but OpenAI 01-preview is a notable exception.\nThere are two reasons for these results. First, our prompt explicitly instructs models to answer the question only using the paragraph provided and without using prior knowledge."}, {"title": "H. Future Works", "content": "While JustLogic already achieves higher or similar natural language complexity to existing deductive reasoning benchmarks, linguistic complexity can be further enhanced to emulate human-written prose, e.g. news articles and fiction stories. Notably, LLMs can be introduced in Step 2 of JustLogic's dataset construction process, whereby instead of randomly selecting sentences from GenericsKB, an LLM can generate fictional statements and scenarios, e.g. \"John's favorite food is hamburgers.\". \nError analysis using JustLogic can also be further explored.\nJustLogic can be scaled to incorporate more question types related to logical reasoning."}]}