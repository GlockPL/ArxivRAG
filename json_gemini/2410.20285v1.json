{"title": "SWE-SEARCH: ENHANCING SOFTWARE AGENTS WITH MONTE CARLO TREE SEARCH AND ITERATIVE REFINEMENT", "authors": ["Antonis Antoniades", "Albert \u00d6rwall", "Kexun Zhang", "Yuxi Xie", "Anirudh Goyal", "William Wang"], "abstract": "Software engineers operating in complex and dynamic environments must con-\ntinuously adapt to evolving requirements, learn iteratively from experience, and\nreconsider their approaches based on new insights. However, current large lan-\nguage model (LLM)-based software agents often rely on rigid processes and tend\nto repeat ineffective actions without the capacity to evaluate their performance or\nadapt their strategies over time. To address these challenges, we propose SWE-\nSearch, a multi-agent framework that integrates Monte Carlo Tree Search (MCTS)\nwith a self-improvement mechanism to enhance software agents' performance on\nrepository-level software tasks. SWE-Search extends traditional MCTS by incor-\nporating a hybrid value function that leverages LLMs for both numerical value\nestimation and qualitative evaluation. This enables self-feedback loops where\nagents iteratively refine their strategies based on both quantitative numerical eval-\nuations and qualitative natural language assessments of pursued trajectories. The\nframework includes a SWE-Agent for adaptive exploration, a Value Agent for\niterative feedback, and a Discriminator Agent that facilitates multi-agent debate\nfor collaborative decision-making. Applied to the SWE-bench benchmark, our\napproach demonstrates a 23% relative improvement in performance across five\nmodels compared to standard open-source agents without MCTS. Our analysis\nreveals how performance scales with increased search depth and identifies key\nfactors that facilitate effective self-evaluation in software agents. This work high-\nlights the potential of self-evaluation driven search techniques to enhance agent\nreasoning and planning in complex, dynamic software engineering environments.", "sections": [{"title": "1 INTRODUCTION", "content": "Software engineering is a complex and iterative process involving exploration, problem-solving,\nand decision-making under uncertainty. Tasks such as debugging, feature development, and code\nrefactoring require continuous assessment of different approaches, frequent backtracking, and the\nincorporation of new information. While machine learning has made progress in automating parts\nof this workflow, replicating the adaptive and strategic behavior of human engineers remains a significant challenge.\nThis is due to the inherently non-linear and iterative nature of software engineering, where engi-\nneers dynamically explore various solutions, refine strategies based on feedback, and collaborate\nto identify the most effective path forward. Current large language model (LLM)-based software\nagents, while powerful, often struggle with complex, long-\nhorizon tasks that require adaptive strategies and flexible reassessment over time. These agents can\nbecome trapped in repetitive patterns, limiting their effectiveness in tackling more intricate software\nengineering problems."}, {"title": "2 RELATED WORK", "content": "To address these challenges, we introduce SWE-Search, a multi-agent system that replicates the\nadaptability, iterative learning, and collaborative decision-making of human engineers. SWE-Search\nis designed to address three critical needs in software engineering:\nFlexible Exploration and Adaptation: Engineering problems often require exploring multiple ap-\nproaches and adapting strategies based on evolving information. SWE-Search's\nSWE-Agent operates in a flexible state space, allowing it to fluidly transition between actions such\nas planning, searching, and editing. This design mirrors the way engineers backtrack and adjust their\napproach dynamically, ensuring the agent can revise its course when faced with new challenges or\ninformation, and points towards the direction of more general, open-ended systems.\nIterative Learning through Feedback: Effective engineering relies heavily on continuous testing and\nrefinement. To replicate this, SWE-Search integrates a Monte Carlo Tree Search (MCTS) planning module paired with a Value Agent. The MCTS module balances exploration\nand exploitation to guide the agent through complex solution spaces. The Value Agent augments this\nprocess by providing both utility estimates and qualitative feedback, allowing the agent to iteratively\nimprove its decision-making based on past experiences, similar to how engineers refine their work\nthrough feedback and debugging.\nCollaborative Decision-Making: Complex problems often benefit from diverse perspectives. In SWE-Search, once a\nset of potential solutions is generated, the Discriminator Agent facilitates a multi-agent debate. Each\nagent advocates for different solutions by presenting arguments, which are critically evaluated by a\njudge agent. This process mirrors real-world engineering collaboration, where teams deliberate to\nrefine and select the most robust solutions.\nThe architecture of SWE-Search is designed to automate software engineering tasks through these\nadaptive, feedback-driven, and collaborative processes. The SWE-Agent serves as the system's\nproblem solver, operating in a dynamic environment where it can backtrack and adapt its actions\nas necessary. The MCTS Planning Module efficiently guides exploration and exploitation, ensuring\nthat the agent balances the need for innovation with the need to focus on promising solutions. The\nValue Agent provides continual feedback, offering both quantitative assessments and qualitative\ninsights, helping the agent refine its strategy iteratively. Finally, the Discriminator Agent ensures\nthat the final decision is rigorously vetted through a multi-agent debate, simulating the collaborative\ndecision-making processes commonly found in engineering teams.\nWe evaluate SWE-Search on the SWE-bench benchmark, a comprehensive dataset from real-world\nopen-source repositories. SWE-bench tests agents' ability to resolve software issues by generating\ncode patches that fix failing tests. SWE-Search demonstrates a 23% relative performance improve-\nment across five models compared to standard open-source agents, highlighting the effectiveness\nof strategic search and iterative self-evaluation. Through detailed analysis, we explore how perfor-\nmance scales with increased search depth and identify key factors that enhance self-assessment in\nsoftware agents. Our work demonstrates the potential of MCTS and iterative learning to improve\nagent reasoning and planning in dynamic, complex domains like software engineering, introducing\na new paradigm for autonomous software development."}, {"title": "2 RELATED WORK", "content": "Search methods Various search approaches have been applied to Large Language Models (LLMs)\nto facilitate System 2 thinking in non-linear reasoning structures. A critical feature of these approaches is their ability to\nbacktrack. Unlike greedy processes, search algorithms explore multiple branches at\neach step, potentially escaping paths that lead to dead ends. These methods differ in their strategies\nfor exploring and memorizing possible choices, and in their heuristics for switching between them.\nBreadth-first search maintains all possible search paths, incurring significant memory\nand computational costs. Depth-first search, in contrast, prioritizes the most\npromising path in a more greedy manner. When applied to LLMs, these methods demonstrate a\ntrade-off between diversity and quality in text generation. The A* algorithm combines aspects of breadth-first and greedy search to find optimal solutions using a"}, {"title": "3 METHODOLOGY", "content": "SWE-Search is a multi-agent system designed to tackle complex software engineering tasks by inte-\ngrating dynamic planning, value estimation, and deliberative decision-making. The core motivation\nbehind this method is to emulate the sophisticated, iterative workflows of human software engi-\nneers, where exploration, planning, and collaboration are crucial to solving intricate problems. By\nleveraging the strengths of Monte Carlo Tree Search (MCTS) for planning, a Value Agent for utility\nestimation and feedback, and a Discriminator Agent for final decision-making through debate, SWE-\nSearch provides a comprehensive, adaptive framework capable of navigating and solving real-world\nsoftware engineering challenges.\nSWE-Search consists of four primary components that work in synergy:\nSWE-Search Framework and Action Agent: Building on the moatless-tools framework, SWE-Search operates in a dynamic code environment with a flexible state-space and a git-\nlike commit tree structure. This design facilitates efficient backtracking to previous states, enabling\nthe Action Agent to explore diverse solution trajectories. The adaptable state-space enhances the\nsystem's ability to exploit the MCTS module effectively.\nSearch Algorithm: The core of SWE-Search's exploration strategy is based on a Monte Carlo Tree\nSearch (MCTS) which uses a heuristic-based selection process similar to AlphaZero, specifically tailored for software engineering tasks. This modified MCTS algorithm effec-\ntively balances exploration and exploitation, helping the agent explore a diverse set of solutions and\nconverge quickly on the most promising strategies.\nValue (Function) Agent: To approximate the utility of each observation, we employ an LLM-based\nvalue function, which in addition to outputting a value, also generates an explanation in natural\nlanguage. This explanation can be leveraged to improve subsequent actions from parent nodes,\nenabling iterative self-improvement of the search process.\nDiscriminator Agent: In the final stage of SWE-Search, the Discriminator Agent evaluates the solu-\ntions generated by the search process. Inspired by multi-agent debate frameworks this agent engages in a structured debate, where multi-\nple agents argue for or against the proposed solutions. The debate process not only surfaces diverse\nperspectives but also leads to a more rigorously justified final decision.\nThis system architecture combines the strengths of dynamic action selection, strategic planning, and\ncollaborative deliberation, creating a comprehensive tool capable of handling the complexity and\niterative nature of software engineering tasks."}, {"title": "3.1 PROBLEM FORMULATION", "content": "The task of the SWE agent can be formalized as a tuple \\(M = (S,C,A,V,P, P_o, \\rho)\\). Here, \\(S\\)\nrepresents the state space, encompassing all possible states such as the current context of the files\nthe agent is working on and the overall status of the codebase. The context space, denoted as \\(C\\),\nincludes metadata about the repository and the initial problem description. The value function \\(V\\)\nassigns a utility score to each state-action pair \\(O(a, t)\\), guiding the agent's decisions.\nThe environment's dynamics are defined by a context-dependent transition function \\(P : S\\times A\\times\nC\\rightarrow\\Delta(S)\\), which models the evolution of the repository's state after each action. The initial state\ndistribution, \\(p_o: C \\rightarrow \\Delta(S)\\), specifies how the initial state depends on the given context, while\n\\(\\rho\\in\\Delta(C)\\) defines the distribution over contexts.\nGiven an initial context \\(c \\sim \\rho\\) and an initial state \\(s_o \\sim p_o(\\cdot | c)\\), the SWE agent executes its policy\n\\(\\pi: S\\times C \\rightarrow \\Delta(A)\\), which selects actions based on the current state and context. At each time\nstep \\(t\\), the agent takes an action \\(a_t \\sim \\pi(s_t, c)\\) and receives a corresponding reward \\(R(s_t, a_t, c)\\).\nThe environment then transitions to a new state \\(s_{t+1} \\sim P(\\cdot | s_t, a_t, c)\\), and the agent continues to\nobserve this updated state. Over time, this process generates a trajectory \\(r := \\{s_t, a_t, r_t\\}_{t=0}^{T-1}\\) as the\nagent interacts with the environment.\nThe agent's objective is to maximize the cumulative reward over the trajectory, which is captured by\nthe value function \\(v(s_t, a_t, \\{s_i\\}_{i=0...t-1}, \\{a_i\\}_{i=0...t-1})\\). This value function depends not only on the current\nstate and action but also on the history of previous states and actions, which deviates from the\nassumptions of a Markovian process. Formally, the agent seeks to maximize the expected cumulative\nreward, defined as: \\(\\underset{\\pi}{\\text{max}} \\mathbb{E}_\\tau[\\sum_{t=0}^{T}R(s_t, a_t, c) | c \\sim \\rho; \\pi]\\).\nThis optimization captures the agent's (in-context) process, as it adjusts its policy \\(\\pi\\) to achieve the\nhighest expected return across multiple trajectories, considering both current and historical informa-\ntion."}, {"title": "3.2 SWE-SEARCH FRAMEWORK AND ACTION AGENT", "content": "The SWE-Search Action Agent builds on the moatless-tools framework. Its action\nspace, \\(A\\), is organized as a two-tier hierarchy, comprising both action types and their corresponding"}, {"title": "3.3 VALUE (FUNCTION) AGENT", "content": "specific actions. Formally, this can be expressed as \\(A = \\{(t,a) | t \\in T, a \\in A_t\\}, where \\(T\\) repre-\nsents the set of action types (e.g., Search, Plan, Edit), and \\(A_t\\) is the set of possible actions\ncorresponding to each type \\(t\\). These actions range from tool invocations and code modifications to\nthe generation of structured text. To enhance the agent's effectiveness in search-driven tasks, we\nintroduced the following modifications:\nOne key modification we implemented is the expansion of the Plan state, allowing it to transition\nflexibly to any other state, rather than being limited to transitioning only to Edit. This change\nis motivated by the need to enable more dynamic and adaptive problem-solving behaviors within\nthe agent. In the context of software engineering, rigid state transitions can be overly restrictive,\nforcing the agent into predetermined pathways that may not always align with the complexities of\nreal-world scenarios. For instance, during code modification tasks, an agent might recognize mid-\nprocess that further planning, additional searches, or different types of analysis are necessary before\nproceeding with edits. Restricting transitions only to editing would artificially constrain the agent,\npotentially leading it to suboptimal actions or causing it to become stuck in unproductive loops.\nBy allowing transitions to any state, we empower the agent to adapt to new information as it arises\n(Fig. 2), exploring a wider variety of trajectories. This enhanced flexibility reflects the iterative and\noften non-linear nature of real software engineering workflows, where engineers frequently revisit\nplanning, testing, and research phases before committing to edits.\nSecond, the agent is empowered to execute any tests within the codebase at its discretion, as well\nas to create and implement new tests. The results of these tests are incorporated into both the value\nfunction and the agent's subsequent decision-making process. It is crucial to highlight that the tests\nrequired to resolve a given instance (i.e., fail-to-pass tests) are not explicitly revealed to the agent.\nHowever, the agent can leverage any pre-existing tests within the repository, simulating the behavior\nof a real-world software engineer. I"}, {"title": "3.3 VALUE (FUNCTION) AGENT", "content": "The role of the Value Agent extends beyond simply estimating the expected utility of a given state-\naction pair \\(O_n(S_n, a_n)\\). In addition to calculating the value \\(U_n\\), the Value Agent generates a written\nexplanation, denoted as \\(\\varepsilon_t\\). This explanation serves a dual purpose: it provides transparency into\nthe decision-making process and functions as feedback for the Action Agent, which can leverage\nthis explanation when re-expanding from the parent node of \\(O_n\\) (see Figure 1, hindsight feedback).\nThis approach enables the system to iteratively refine its decision-making process, mirroring how\na human software engineer continuously re-evaluates their approach based on new information to\nimprove their problem-solving strategy.\nThe input to the value function consists of all state-action pairs up to and including the current state\nbeing evaluated, alongside specific instructions on how to assess the state. This allows the Value\nAgent to contextualize the decision within the trajectory, accounting for the sequence of actions and\nstates leading up to the present. The final output of the value function can be formalized as:\n\\begin{equation}\n(U_t, \\mathcal{E}_t) = V (S_t, a_t, \\{S_i\\}_{i=0...t-1}, \\{a_i\\}_{i=0...t-1})\n\\end{equation}\nHere, \\(v_t\\) represents the expected utility of the current state-action pair, while \\(\\mathcal{E}_t\\) is the accompanying\nexplanation.\nIn practice, the Value Agent is tasked with analyzing the entire trajectory leading up to the current\nstate-action pair, providing not only the required utility estimate \\(v_t\\), but also a detailed explanation\n\\(\\mathcal{E}_t\\). This explanation is critical for the agent's overall performance, as it offers insight into the rea-\nsoning behind utility estimates, which in turn informs the Action Agent's future decisions. We have\nobserved that one of the key factors driving the effectiveness of the Value Agent lies in the clarity\nand specificity of these explanations. A well-articulated explanation can illuminate the strengths and\nlimitations of different state types (e.g., Search, Edit, Plan), helping the Action Agent better\nunderstand which types of states are more promising or risky to pursue.\nThis approach aligns with the practices of other SWE agents, and has been validated by the authors of\nSWE-bench, who confirmed its legitimacy as long as the fail-to-pass tests remain concealed from the model."}, {"title": "3.4 SEARCH ALGORITHM", "content": "By providing detailed feedback on the potential utility of different actions and contextualizing them\nwithin the broader trajectory, the Value Agent enables more informed and strategic decision-making\nby the Action Agent. This integration of both quantitative and qualitative feedback leads to improved\nperformance and more adaptive behavior throughout the task (Fig. 4a).\nOur search tree is structured with nodes representing states \\(S_t\\) and edges representing actions \\(A_t\\).\nThe search algorithm employed is a modified Monte Carlo Tree Search (MCTS), specifically adapted\nfor the tasks of the SWE-Agent. Unlike prior approaches for web agents that utilize language models\nin the selection process we deliberately choose not to rely on\nlanguage models for node selection. Instead, we adopt a more straightforward heuristic-based selec-\ntion function, similar to the approach used in AlphaZero This decision\nis driven by the need for interpretability, efficiency, and the focus on tasks where heuristic-based\nexploration suffices to guide the agent effectively through complex software engineering environ-\nments.\nAt the core of our algorithm is a modified Upper Confidence Bound for Trees (UCT) selection crite-\nrion which determines the next node to expand. This criterion balances\nexploitation of known high-reward actions with exploration of less-visited states. We introduce\nadditional terms to encourage strategic exploration early in the search process, and to penalize over-\nexploration at later stages when convergence on the optimal solution is desired. The modified UCT\nfunction is expressed as:\n\\begin{equation}\nUCT(s, a) = exploitation + exploration + early\\_depth\\_bonus \u2013 late\\_depth\\_penalty\n\\end{equation}\nThis can be expressed more formally as:\n\\begin{equation}\nUCT(s, a) s, a) = V(s, a) + C\\sqrt{\\frac{ln N(s)}{N(s,a)}} + \\alpha e^{-\\beta(d-1)} \u2013 \\gamma \\sqrt{d}\n\\end{equation}\n\\(V(s, a)\\) is the value estimate of the state-action pair, \\(N(s, a)\\) is the number of times the state-action\npair \\((s, a)\\) has been visited, \\(N(s)\\) is the visit count of state \\(s\\), \\(d\\) is the depth of the node in the search\ntree, and \\(C\\), \\(\\alpha\\), \\(\\beta\\), and \\(\\gamma\\) are constants that control the balance between exploration, exploitation, and\ndepth-dependent rewards and penalties.\nThis formulation is inspired by the way software engineers explore potential solutions to a task. In\npractice, an engineer's search process can be broken down into the following key phases, which our\nalgorithm mirrors:\nEarly Exploration: Initially, an engineer explores a wide variety of potential approaches to fully\nunderstand the problem and identify promising strategies. This is encouraged in our algorithm by\nthe early_depth_bonus, represented by the term \\(\\alpha e^{-\\beta(d-1)}\\), which rewards exploration at shallow\ndepths, simulating the early phases of wide exploration.\nConvergence and Exploitation: As the engineer gains more information and narrows down the op-\ntions, the focus shifts to exploiting the most effective solution paths. This transition is handled by\nthe standard UCT exploitation term \\(V(s, a)\\) and is further reinforced by the late_depth_penalty\n(\\(-\\gamma\\sqrt{d}\\)), which discourages over-exploration as the agent delves deeper into the search tree.\nQuick Abandonment of Poor Strategies: Software engineers are also adept at abandoning poor strate-\ngies when new information indicates that a particular approach is not viable. We capture this be-\nhavior by implementing a simple heuristic rule that abandons nodes associated with consecutive low\nrewards, ensuring that the agent does not waste resources on unproductive trajectories.\nAt each step, the node with the highest UCT value is selected for expansion, formalized as:\n\\begin{equation}\ns^* = \\underset{(s,a)}{\\text{arg max}} UCT(s,a)\n\\end{equation}"}, {"title": "3.4.1 DISCRIMINATOR AGENT", "content": "The final stage of SWE-Search involves the Discriminator Agent, whose role is to evaluate the can-\ndidate solutions generated by the search process and select the one most likely to resolve the issue\nat hand. This module accepts up to five final solutions produced by the search and engages in a\nmulti-agent debate to determine the most promising option. Drawing inspiration from recent work\non persuasive multi-agent debates the Discriminator\nleverages the collective reasoning of multiple agents to ensure a more robust final selection. Config-\nuration and hyperparameter details can be found in Table 2.\nIn this stage, agents are presented with the original problem statement and candidate solutions. They\nengage in a structured debate to determine the most effective solution, supporting their choices with\nlogical reasoning and evidence from the search process. This debate encourages a thorough explo-\nration of trade-offs between solutions, potentially uncovering strengths or weaknesses not evident\nduring individual searches. Finally, a judge agent evaluates the arguments and selects the solution\ndeemed most likely to resolve the issue. This process simulates the collaborative decision-making\nin software engineering teams, where diverse perspectives lead to a more thorough evaluation of\ncandidate solutions, ultimately increasing the likelihood of identifying the most optimal outcome.\nThe discriminator process not only enhances the robustness of the final solution but also adds trans-\nparency, as the reasoning behind the choice is clearly articulated and evaluated. This ensures that\nthe selected solution is well-reasoned and thoroughly vetted before implementation."}, {"title": "4 EXPERIMENTS", "content": "Benchmark For our experiments, we utilize SWE-bench Lite, a curated subset of the official\nSWE-bench, containing 300 instances. This dataset is specifically designed to be self-contained and\nfocuses primarily on evaluating functional bug fixes, providing a controlled environment to assess\nthe performance of our system.\nEvaluation Metrics We use two metrics: resolve rate (Pass@1) and Pass@5. Resolve rate is the\npercentage of issues successfully resolved, measuring overall effectiveness. Pass@5 is the percent-"}, {"title": "4.1 EXPERIMENTAL RESULTS", "content": "age of issues where a correct solution is found within five attempts. This allows us to assess the\nefficiency of the search in identifying successful bug fixes within a limited number of iterations.\nBaselines Software agents leverage diverse tools, architectures, and models, leading to variability\nin their performance on subsets of the SWE-bench Lite dataset. For compar-\nison, we build upon the moatless-tools framework, a high-performing open-source\nagent commonly used in research settings. To isolate the impact of our\nsearch approach, we adapt moatless-tools as our baseline, referred to as moatless-adapted. This\nallows us to fairly compare the performance of SWE-Search against moatless-adapted across var-\nious models, including two closed-source models (GPT-40, GPT-40-mini) and three open-source\nmodels. We also reference official moatless-tools GPT-40\nresults on SWE-bench Lite to ensure a fair and consistent comparison.\nImplementation Details For consistency, we use identical prompts across all models. In SWE-\nSearch, we limit each node to a maximum of three expansions and cap the total search iterations at\n100. Further details on model hyperparameters can be found in Appendix, 2."}, {"title": "4.1.1 SWE-SEARCH SURPASSES ALL CORRESPONDING BASE AGENTS AND ENABLES\nSMALLER, OPEN SOURCE MODELS TO APPROACH GPT-40", "content": "On average, SWE-Search outperforms the baseline agent across all five models, achieving a 23%\nrelative improvement (Table 1). Notably, SWE-Search with Qwen-2.5-72B-Instruct exceeds the\nperformance of GPT-40 using the original Moatless-v1 framework, and closely matches its perfor-\nmance when compared with the Moatless-adapted agent, with only a slight difference (\u2206 = \u22121%).\nInterestingly, all five models demonstrate significant improvement when utilizing the proposed ap-\nproach, with consistent gains across different models."}, {"title": "4.1.2 SEARCH ENABLES AGENTS TO MAKE BETTER USE OF MORE FLEXIBILITY", "content": "To prevent goal divergence, most agents, including moatless-tools, rely on strict transition rules,\nwhere state transitions follow predetermined sequences (e.g., Search \u2192 Identify, Plan \u2192 Edit). In\nmoatless-adapted, we introduce a more flexible transition logic that allows a Plan state to transition\ninto any other state type. This added flexibility has both advantages and drawbacks. On the positive\nside, it enables the agent to autonomously correct its trajectory without external feedback, particu-\nlarly when the necessary adjustments span only a limited portion of the task. However, this increased\nflexibility also introduces the risk of the agent becoming trapped in infinite loops. Without a high-\nlevel control mechanism to detect and mitigate these situations, the agent may fail to recover from\nsuch loops. This trade-off is evident in the modest performance difference between Moatless-v1 and\nmoatless-adapted, with a slight performance improvement of only 1.4% (Table 1)."}, {"title": "4.1.3 IMPACT OF HINDSIGHT FEEDBACK ON AGENT PERFORMANCE", "content": "One key advantage of utilizing LLMs as general value functions is their dual ability to provide both\nquantitative value estimates and qualitative assessments in natural language. These qualitative in-"}, {"title": "4.2 IMPORTANCE OF COMPREHENSIVE STATE INFORMATION FOR VALUE FUNCTION\nPERFORMANCE", "content": "The effectiveness of SWE-Search hinges on the value\nfunction's ability to accurately differentiate between\ndesirable and undesirable states, and to provide ac-\ntionable feedback that drives improvement. However,\nour experiments revealed that the value function some-\ntimes failed to recognize critical decision points in the\nsearch tree. It frequently misinterpreted the purpose\nof certain actions, leading to the undervaluation of ef-\nfective strategies by assigning low rewards. As shown"}, {"title": "5 DISCUSSION AND CONCLUSION", "content": "In this paper, we introduced SWE-Search, a general framework that integrates Monte Carlo Tree\nSearch (MCTS) and qualitative feedback to enhance the performance of software engineering agents."}, {"title": "5 DISCUSSION AND CONCLUSION", "content": "The proposed approach demonstrated improvements over different baseline models, highlighting the\npotential of search-based methods in software engineering tasks.\nOne of the key advantages of search-based approaches, as demonstrated in our work, is their ability\nto scale performance with increased inference-time compute. This flexibility allows the system to\nadapt to problems that require higher computational resources, such as discovering software vul-\nnerabilities or even generating large codebases from scratch. Future research should focus on two\nmain directions: (a) investigating how search agents scale with computational resources, and (b)\nexpanding the application of software agent search to a broader range of complex use cases.\nGiven that search techniques like MCTS closely resemble the problem-solving processes of human\nsoftware engineers, we expect these methods to become increasingly prevalent in agent-driven sys-\ntems. As the nature of software engineering tasks evolves, system architectures will need to become\nmore fluid and adaptable, fully leveraging the potential of search-based techniques. This evolution\nwill likely lead to the development of larger, more general agentic systems capable of tackling a\nwide array of software engineering challenges."}, {"title": "A REPRODUCIBILITY", "content": "All models and data used in our work are publicly available. We additionally provide hyperparameter\ndetails in Appendix 2. The code will be released as a public repository upon publication."}, {"title": "B ADDITIONAL IMPLEMENTATION DETAILS", "content": "Moatless-adapted is an extended version of the moatless-tools library with support for a tree struc-\nture, the ability to revert to earlier versions of the codebase, and the capability to run tests.\nThe standard implementation of moatless-tools is based on a finite state machine structure where a\nstate holds information about file context and properties set in the configuration or from previous\nstates. It can then transition to a new state when an action is executed. The request that initiates the\naction is created by an LLM. This follows a linear structure where one state can transition to another\nstate. In moatless-adapted, this model is extended so that a state can expand by using actions to\ncreate more states. The connections between states are then represented in a tree structure with\nnodes.\nEach state has a file context associated with it. This file context will be included in the prompt\nsent to an LLM. To limit the size of the prompt, files are divided into \"spans,\" where a span could\nbe, for example, a section of code (e.g., imports), a class, or a function. These are identified by\nspan IDs. Thus, the LLM sees a limited part of the code at a time but can request more context by\nsearching for or adding files and spans. The file context therefore changes over time, and a specific\nstate of file context is linked to a specific state. In the standard implementation of moatless-tools,\nchanges to the codebase are made linearly, and each change is saved directly to the file system. In\nmoatless-adapted, however, there is a need to be able to revert to earlier states and thus return to a\nprevious version of the codebase. To handle this, the code is stored in a git repository where each\nchange is committed, and each state has a reference to a commit as well as the current patch of the\ndiff from the initial commit that existed before starting. This way, one can go back to an earlier state\nby specifying the state ID, and the commit that was current at that time will be checked out.\nThe test files present in the file context are run each time the Plan state is initiated, and the test\nresults are provided to the state. The tests are then run in Docker images built via the SWE-bench\nlibrary. To use this approach in a benchmark where a larger number of instances should be able to\nrun simultaneously, a solution is used where these images are run as pods in a Kubernetes cluster.\nMoatless-tools communicates with the testbed by applying patches and running commands via an\nAPI. When a new instance starts, a pod is created which is then reset at each run, applying the\ncurrent patch and running tests according to the test command specified in the SWE-bench library.\nIt's important to add here that the agent is not aware of the PASS_TO_PASS or FAIL_TO_PASS\ntests in the SWE-bench harness, but only knows how to run the tests. This corresponds to a real\nengineering environment where each project can have its own test commands."}, {"title": "C MCTS HYPERPARAMETERS", "content": "The Monte Carlo Tree Search (MCTS) algorithm used in this study employs several hyperparame-\nters."}]}