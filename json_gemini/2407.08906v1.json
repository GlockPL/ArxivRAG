{"title": "AirSketch: Generative Motion to Sketch", "authors": ["Hui Xian Grace Lim", "Xuanming Cui", "Yogesh S Rawat", "Ser-Nam Lim"], "abstract": "Illustration is a fundamental mode of human expression and communication. Certain types of motion that accompany speech can provide this illustrative mode of communication. While Augmented and Virtual Reality technologies (AR/VR) have introduced tools for producing drawings with hand motions (air drawing), they typically require costly hardware and additional digital markers, thereby limiting their accessibility and portability. Furthermore, air drawing demands considerable skill to achieve aesthetic results. To address these challenges, we introduce the concept of AirSketch, aimed at generating faithful and visually coherent sketches directly from hand motions, eliminating the need for complicated headsets or markers. We devise a simple augmentation-based self-supervised training procedure, enabling a controllable image diffusion model to learn to translate from highly noisy hand tracking images to clean, aesthetically pleasing sketches, while preserving the essential visual cues from the original tracking data. We present two air drawing datasets to study this problem. Our findings demonstrate that beyond producing photo-realistic images from precise spatial inputs, controllable image diffusion can effectively produce a refined, clear sketch from a noisy input. Our work serves as an initial step towards marker-less air drawing and reveals distinct applications of controllable diffusion models to AirSketch and AR/VR in general.", "sections": [{"title": "1 Introduction", "content": "Hand gestures are an essential element in communication[42]. In particular, iconic hand motions (i.e. air drawing) can depict visual aspects of an object. This form of expression is frequently used to visually supplement verbal communication and is used in various practical applications, including conceptual discussions, overcoming language barriers, and aiding in visual design.\nPopular AR/VR tools like Google's Tiltbrush [19] and HoloARt [5] create visuals via hand motions, but are inconvenient. These applications generally require head-mounted displays and digital hand markers, which are costly and hinder portability. Furthermore, their weight and temperature make them unsuitable for continuous use [59, 71, 12], and so are inconvenient for spontaneous usage. Yet, these devices provide accurate positioning, stabilization, and varied brush controls and are crucial to producing high-quality drawings.\nCan we generate sketches from hand motions without additional sensors or markers? In order to enhance accessibility and convenience, we aim to generate sketches from hand motions videos captured using any standard camera embedded in devices like smartphones or smart glasses.\nOne could clearly deploy hand tracking algorithms [9, 73] to turn these hand motion videos into sketches. However, creating air drawings with a hand tracking algorithm alone presents several challenges. These include the user's drawing ability, physical fatigue, and inaccuracies in hand tracking. Noise in hand tracking can severely distort a sketch, rendering it almost unrecognizable.\nThe objective, therefore, is to generate clean sketches that faithfully represent the user's intent, from highly noisy and distorted hand motion input. This task requires the model to possess a deep understanding of shape and object priors, enabling it to discern and correct deformed motion cues while filtering out undesirable noise. We refer to this task as Generative Motion to Sketch.\nThere are many approaches to this task, with different architecture and data modalities. The input modality might include learned video representations, coordinate sequences from a hand tracking algorithm, or a rasterized image. Depending on the modality, the task may also be reformulated as video-to-sketch, sequence-to-sequence, image-to-image, or a combination thereof. This diversity introduces interesting opportunities for rich exploration of all these diverse approaches.\nWe explore the use of controllable image Diffusion Models (DM) in generating sketches from motion. Existing work such as ControlNet [75] and T2IAdapter [43] generate photo-realistic images given spatially-precise conditioning images. We explore a different use case by using controllable DMS to \"reconstruct\u201d clean sketches from severely distorted and noisy input images obtained with a hand tracking algorithm. We propose a simple, augmentation-based, self-supervised training procedure and construct two air drawings datasets for evaluation purposes.\nOur experiments show that with our augmentation-based training, controllable image DMs are able to recognize and interpret correct visual cues from noisy tracking images, some of which even appear to be nearly unrecognizable to the human eye, and generate sketches faithfully and aesthetically, while being robust to unseen objects. Moreover, we show that through simple augmentations, the model is capable of sketch-completion and text-instructed stroke styling. Finally, we conduct ablations to investigate 1) the effects of different augmentations, 2) the contribution of text prompts to sketch generation and 3) the effect of different level of input 'chaos' on the quality of resulting generations.\nIn summary, in this paper:\n1. We conduct a pilot study of AirSketch, sketch generation from marker-less air drawing, and provide two air drawing datasets for evaluation.\n2. We propose a controllable DM approach that generates faithful and aesthetic sketches from air-drawn tracking images with a self-supervised, augmentation-based training procedure, and conduct ablation studies to prove its effectiveness and robustness.\n3. We explore a different way of using spatially-conditioned DMs and reveal some of their interesting properties in the context of sketch generation. We hope our experiments shed new light on understanding the properties of controllable DMs and facilitate future research."}, {"title": "2 Related Works", "content": "2.1 Sketching in AR/VR\nThere are many methods for drawing in AR and VR. Applications such as Google's Tilt Brush [19], Mozilla's A-Painter [44], Oculus' Quill [47], and HoloARt [5] display user strokes as lines or tubes that can extend in all three dimensions. Many sketching applications such as these require a combination of VR/AR headsets and controllers to use. Since drawing freehand with six degrees of freedom makes it difficult to draw steady lines and surfaces, applications like AdaptiBrush [58] and Drawing on Air [27] use trajectory-based motion of ribbons to render strokes predictably and reliably. Just-a-Line [20] and ARCore Drawing [25] are smartphone-based AR drawing applications, where users draw lines by moving a smartphone in the air [20], or drawing on a smartphone screen [25].\n2.2 Sketch Applications\nRepresenting Sketches. A sketch can be represented in both vector and pixel space, and with varying levels of abstraction. A sketch can be represented as a rasterized image [39, 74, 49, 36, 31], a sequence of coordinates [22, 55, 37], a set of Bezier curves [66, 17], or a combination [8]. These representation methods are suited for different tasks. For example, the rasterized image representation is commonly used in Sketch-Based Image Retrieval (SBIR) in order to compare sketches with images, while the coordinate sequence representation is often used for sketch generation. On the other hand, a sketch can also depict the same object at varying abstraction levels, thereby imposing further challenges on downstream tasks, especially sketch-based retrieval in 2D [36, 31, 61] and 3D [68, 40, 13, 30].\nSketch Generation. Most existing sketch generation methods adopt the vector representation [22, 55, 37], and view sketch generation as an auto-regressive problem on coordinate sequences. These sequences are typically represented by lists of tuple (dx, dy, p), where dx and dy represent the offset distances of x and y from the previous point, and p is a one-hot vector indicating the state of the pen. Sketch-RNN [22] uses a Variational Autoencoder (VAE) with a bi-directional RNN as the encoder and an auto-regressive RNN as the decoder. Sketch-Bert [37] follows the BERT [14] model design and training regime. Instead of auto-regressive sketch generation, SketchGAN [38] takes in a rasterized sketch image and uses a cascade GAN to iteratively complete the sketch.\nNonetheless, these methods generate sketches in the input sketch modality by taking in a sketch and reconstructing the exact same sketch, or by predicting endings given incomplete sketches. In contrast, our work considers the task for generating sketches from hand motions. Specifically, we adopt the image representation for sketch generation as opposed to using coordinate sequences. This offers several advantages: it 1) maintains constant computation complexity with regards to sketch length, 2) is drawing-order invariant, which is especially favorable in our case as we consider extremely noisy sketches as input, and 3) allows us to utilize large pretrained image generative models.\n2.3 Image Diffusion Models\nDiffusion Probabilistic Model. First introduced by Sohl-Dickstein et al. [63], diffusion proba-bilistic models have been widely applied in image generation [23, 64, 28]. To reduce computational cost, Latent Diffusion Models [56] project input images to lower dimension latent space, where diffusion steps are performed. Text-to-image diffusion models [46, 54, 53, 60, 50] achieve text control over image generation by fusing text embeddings obtained by pre-trained language models such as CLIP [51] and T5 [52] into diffusion UNet [57] via cross attention.\nControllable Image Generation. Beyond text, various recent works have focused on allowing more fine-grained controls over the diffusion process. This can be done via prompt engineering [72, 35, 77], manipulating cross-attention [6, 10, 26, 70], or training additional conditioning adapters [43, 75]. ControlNet [75] learns a trainable copy of the frozen UNet encoder and connects it to the original UNet via zero convolution. T2IAdapter [43] trains a lightweight adapter to predict residuals that are added to the UNet encoder at multiple scales.\nIn particular, both ControlNet and T2IAdapter take spatially-precise conditional images such as canny edge, depth, and skeleton images extracted from the original image. Koley et al. [32] considers"}, {"title": "2.4 Hand tracking and gesture recognition", "content": "Hand tracking and gesture recognition are used for several purposes, including communication and interaction in an AR environment [41] [33] and VR [1], for image-based pose estimation in sign language recognition [7] [3] [4], and many others [34] [11]. Many of these require depth-sensing hardware, such as work done by Oikonomidis et. al. using a Kinect sensor [48], or use deep learning for pose estimation [67] [18], making it difficult to integrate them into lightweight systems.\nHand pose estimation such as MediaPipe Hands [73] and OpenPose [9] take in RGB images or video and return the coordinates of 21 landmarks for each hand detected, and MediaPipe Hands is lightweight enough to integrate into even mobile devices."}, {"title": "3 Air-Drawing Dataset", "content": "In order to thoroughly evaluate our model, we need datasets with sketch-video pairs. Popular sketch datasets include Sketchy [62], TUBerlin [16], and Quick, Draw! [21]. There are also hand motion datasets that associate hand motions with semantic meaning, such as Sign Language MNIST [65], How2Sign [15], and the American Sign Language Lexicon Video Dataset [45]. However, there are no datasets that associate sketches with air drawing hand motions, prompting us to create our own sketch-video pair datasets for evaluation purposes.\nSynthetic Air-Drawing Dataset. We use samples from the Quick, Draw! dataset [21] as the intended ground-truth sketch; each stroke is represented as a sequence of timestamps and coordinates. A 3D human arm asset is animated in the Unity engine [29] using inverse kinematics and rotation constraints, see Figure 2 (left). While following stroke coordinates, the index finger is extended, and when the stroke ends, the hand is in a closed fist. The videos have an aspect ratio of 1920 by 1080 pixels and were recorded at 60 frames per second. We choose 50 common object categories from Quick, Draw! dataset, each with 100 sketches to form our synthetic dataset with a total of 5000 sketch-video pairs. This synthetic Air-Drawing dataset simulates the scenario where users have perfect drawing ability, and the errors are solely introduced by the tracking algorithm.\nReal Air-Drawing Dataset. A human user attempts to replicate sketches from the Quick, Draw! dataset by moving their index finger through the air. The videos were recorded with aspect ratio of 1280 by 720 pixels and at 30 frames per second. We take 10 samples per category used in the synthetic dataset, resulting in 500 video-sketch pairs."}, {"title": "4 Methods", "content": "Our method trains a controllable DM to recover the clean sketch from a noisy one, which we discuss in Sections 4.1 and 4.2. As input at training time, we simulate noisy sketches produced from the direct application of a hand tracking algorithm, discussed in Section 4.3. Finally, we evaluate our model using two different datasets: a dataset of 3D animated hand motion videos, and a small dataset of real hand motion videos. The creation of these two datasets is discussed in Section 3.\n4.1 Preliminary: Controllable DMs\nDiffusion models involve a forward and inverse diffusion process. Given an input image $x_0$, the forward process gradually adds noise to $x_0$ to form a noisy input $x_t$ at time step t as:\n$x_t = \\sqrt{\\bar{a}_t}x_0 + (\\sqrt{1 - \\bar{a}_t})\\epsilon, \\epsilon \\sim \\mathcal{N}(0, I)$\nwhere $\\bar{a}_t := \\Pi_{i=1}^t a_i$, and $a_t = 1 - \\beta_t$ is determined by a pre-defined noise scheduler [23].\nThe reverse process trains a denoising UNet $\\epsilon_{\\theta}(.)$ to predict the noise $\\epsilon$ added to input $x_t$. In the context of controllable generation, as in ControlNet [75] and T2IAdapter [43], with a set of conditions including a text prompt $c_t$ and an additional condition $c_f$, the overall loss can be defined as:\n$\\mathcal{L} = \\mathbb{E}_{x_0,t,c_t,c_f, \\epsilon\\sim\\mathcal{N}(0,1)} [||\\epsilon - \\epsilon_{\\theta}(x_t, t, c_t, c_f) ||^2],$\n4.2 Training Controllable DMs for Sketch Recovery\nWe adopt ControlNet [75] as our primary controlling approach. Our training procedure is illustrated in Figure 3. Due to the lack of sketch-video pair datasets, we devise a self-supervised, augmentation-based training procedure. During training, for each sketch image, we randomly sample combinations of augmentations $\\mathcal{A}(.)$ and apply to $x_0$ to get the distorted view $\\mathcal{A}(x_0)$. It is then used as the input to ControlNet's conditioning adapter. Hence, the loss function 2 can be re-written as:\n$\\mathcal{L} = \\mathbb{E}_{x_0,t,c_t, c_f, \\epsilon\\sim\\mathcal{N}(0,1)} [||\\epsilon - \\epsilon_{\\theta}(x_t, t, c_t, \\mathcal{A}(x_0))||^2].$\nTherefore, unlike regular controllable DMs where the conditioning adapter takes in edge-like maps and predicts spatial-conditioning signals to be injected to the UNet, our adapter learns both the spatial-conditioning signals and a mapping from the distorted to the clean input: $\\mathcal{A}(x_0) \\rightarrow x_0$.\n4.3 Sketch Augmentations\nWe categorize the prevalent errors from air drawings into three types: 1) user-induced artifacts such as hand jitters and stroke distortions, 2) hand tracking errors such as inaccurate hand landmark predictions, unintended strokes, and 3) aesthetic shortcomings related to the user's drawing proficiency."}, {"title": "5 Experiments", "content": "Datasets and Implementation Details. In training, we use a subset of 100 categories from the Quick, Draw! dataset [21]. Because a large portion of Quick, Draw! sketches are not well-drawn, we first calculate the CLIP Image-Text similarity between each sketch and their corresponding category and select the top 5% from each category, resulting in 60K sketches. Note that the sketches used for training are mutually exclusive with the sketches used for generating synthetic and real tracking images used during evaluation. To test for generalizability, we select 10 categories with similar statistics 2 from the rest and exclude them from training.\nWe primarily use Stable Diffusion XL [50] (SDXL) in our experiments, and adhere to the original ControlNet training and inference procedures. During both training and inference phases, we use text prompts in the format of \"a black and white sketch of a <category>\" to guide the model generation. We also finetune SDXL on the Quick, Draw! dataset with Low-Rank Adaptation [24] (LoRA) in order to \"equip\" the model with the basic ability to generate sketches in the appropriate style.\nEvaluation Metrics. We primarily focus on using faithfulness, or the similarity between the generated sketch and the ground-truth sketch, as our model performance. Due to the versatility of sketches, we adopt multiple metrics to ensure comprehensive measurements. On the pixel-level, we use SSIM [69] to measure detailed local structural similarity, and Chamfer Distance [2] (CD) for global comparison, as CD is less sensitive to local density mismatch. Taking a perceptual perspective, we adopt LPIPS [76], CLIP [51] Image-to-Image similarity (I2I), and CLIP Image-to-Text similarity (I2T) between sketches and their corresponding text prompts to measure \"recognizability\u201d.\nWe benchmark our model on the similarity between the ground-truth sketch and the hand tracking image. We then train a ControlNet on sketches but without any augmentation as our second baseline.\n5.1 Results and Analysis\nFaithfulness. In Figure 1 and Figure 6 we can clearly observe the ControlNet trained with aug-mentations successfully identifies the visual cues from the noisy tracking image, removes artifacts, and generates the clean sketches, while being aesthetic and semantically coherent. Unsurprisingly, ControlNet trained without augmentations fails to make any improvement from the tracking.\nTable 1 shows quantitative results for measuring the faithfulness of generated sketch images to ground-truth. For both synthetic and real datasets, we observe a noticeable performance gain. For example, with the real dataset and SDXL, SSIM increases by 10% in SSIM, LPIPS decreases by 6%, and CD decreases by 21%.\nGeneralizability. From Figure 5 we can see that our model generalizes well to categories which are not seen during training, suggesting that the trained ControlNet learns a robust category-agnostic"}, {"title": "5.2 Ablations", "content": "Role of Augmentations. In Table 2 we provide similarity scores between ground-truth and gener-ated sketches when different combinations of augmentations are applied in training. We observe that local augmentations plays an important role in detailed local similarity: with only local augmentation applied, SSIM increases more than other metrics, with a 10% improvement to the baseline, yet increases by only 4% and 0% when only false strokes and structural augmentations are applied. Con-versely, false strokes and structural augmentation have larger effects on the global similarity of the generated sketch: CD decreases by 9% and 3% when only false strokes and structural augmentations are applied, respectively.\nIn Figure 6 we provide visual results when different combinations of sketch augmentations are applied during training. We observe that local augmentations are indeed crucial to removing jitters and correcting deformed lines, while false stroke augmentations ensure that the model does not falsely follow the spatial-conditions introduced by these false strokes. The structural augmentations are less significant, likely because structural artifacts are not as common as local artifacts and false strokes.\nEffect of Text Prompts. Input tracking image conditions are extremely noisy and may not even possess obvious visual cues about the nature of the intended sketch. It is then important to investigate the effect of text guidance on the generated output and examine if our augmentation-based training significantly contributes to the generation, or if it is guided purely by text prompts.\nIn Table 3 we show the similarity scores between the ground-truth and generated sketches with or without prompt on seen or unseen categories. When no augmentation is applied during training, there"}, {"title": "6 Conclusions", "content": "In this paper, we tackle the problem of marker-less air drawing by leveraging a spatially-controlled diffusion model. We devise a simple augmentation-based data-free training procedure to learn a"}, {"title": "A Appendix", "content": "A.1 Details for Sketch Augmentations\nImplementation. Below we provide implementation details for each augmentation:\n\u2022 Local Augmentation includes 3 types of sub-augmentations: stroke distortion, random spikes, and jitters. For stroke distortion, we generate a number of wave functions each with random frequency, shift, and amplitude. We aggregate them to form a random curve, whose value is used to distort the ground-truth sketch. Spikes are implemented in two modes sharp spikes and smooth spikes. Sharp spikes are determined by the width and height which are randomly sampled from a normal distribution; smooth spikes are implemented with Bezier curve, and the control points are randomly sampled from a uniform distribution within predefined range along the gradient of the ground-truth sketch. Jitters are simply small perturbations sampled from a pre-defined normal distribution and added to random locations along the ground-truth sketch.\n\u2022 Structural Augmentation also include 3 types of sub-augmentations: sketch-wise distortion & relocation, stroke-wise misplacement, and stroke-wise resize. For sketch-wise distortion& relocation, we randomly shrink and change aspect ratio of the whole sketch and reposition the sketch in the canvas. For stroke-wise misplacement/resize, we randomly move/resize each stroke within a pre-defined range.\n\u2022 False Strokes includes two types: transitional and random false strokes. For transitional false strokes, we simply draw lines between the transition of each stroke; for random false strokes, we randomly draw a number of extra lines on the canvas.\nIn the above augmentations, misplacement and stroke resize under structural augmentation are mutually-exclusively applied. This is because it is highly probable that the combination of the two will completely destroy the visual cues of the resulting sketch image. For all other augmentations, we set a 50% chance for each to be applied to each sketch sample during training."}, {"title": "A.2 Training Details", "content": "Held-out categories. Because different object categories have different drawing difficulty, sketch and tracking samples belonging to different categories exhibit large variance in statistics (CLIP Image-Text similarity between ground-truth sketch and corresponding text, CLIP Image-Image similarity, Chamfer Distance, or SSIM between ground-truth and tracking image). To allow for fair comparison of the performance between seen and held-out categories, we first partition categories into 10 clusters using K-Means Clustering, and randomly sample one category from each cluster as the held-out categories. The held-out categories are:\n{car, face, cow, snail, diamond, candle, angel, cat, grapes, sun}\nTraining configurations. All training is conducted on two Nvidia H100 GPUs. For finetuning the diffusion model with LORA, we set LORA rank to 4, per device batch size to 16, learning rate to 5e-5, gradient accumulation steps to 4, and train for 6K steps on the Quick, Draw! dataset. For our augmentation-based ControlNet training, we set per device batch size to 8, learning rate to 2e-5, gradient accumulation steps to 4, and the proportion of empty prompts to 25%."}, {"title": "A.3 Comparison on Egocentric Hand Tracking Algorithm", "content": "An example of hand landmarking with MediaPipe, OpenPose, and NRSM is shown in Figure 9. We can see that only MediaPipe's hand landmarker comparatively accurate and consistent, while OpenPose often fails to detect landmarks, and NSRM is less accurate. However, even though MediaPipe seems to accurately predict hand landmarks, the resulting tracking image, as shown in Figure 1 and Figure 12, still contains large amount of noise."}, {"title": "A.4 Sketch Completion & Text-conditioned styling.", "content": "With simple augmentations, we can also perform other sketch-based tasks such as sketch completion and text-conditioned styling. Unlike prior works [38, 37] on sketch completion that require sophis-ticated architectures or training procedures, we could achieve sketch completion by simply adding random erasure to the set of augmentations during training. In Figure 10, we show two types of sketch completion: completion with small missing line segments (column 1&5), and completion with whole strokes missing (column 3&7). In addition, as shown in Figure 11, we can also utilize the text-conditioning capacity of DMs to prompt for different stroke styles, such as different color and thickness, therefore alleviating the need for fine-grained style control on the user's part."}, {"title": "A.5 Visual Comparison between Controllable DMs", "content": "In Figure 12 we compare the generated sketches between ControlNet and T2IAdapter, when trained under our augmentation-based procedure. We can observe that both ControlNet and T2IAdapter generate visually coherent sketches and generally follow the input tracking image. Nonetheless, by"}]}