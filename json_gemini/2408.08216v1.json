{"title": "The Dawn of KAN in Image-to-Image (I2I) Translation: Integrating Kolmogorov-Arnold Networks with GANs for Unpaired I2I Translation", "authors": ["Arpan Mahara", "Naphtali D. Rishe", "Liangdong Deng"], "abstract": "Image-to-Image translation in Generative Artificial Intelligence (Generative AI) has been a central focus of research, with applications spanning healthcare, remote sensing, physics, chemistry, photography, and more. Among the numerous methodologies, Generative Adversarial Networks (GANs) with contrastive learning have been particularly successful. This study aims to demonstrate that the Kolmogorov-Arnold Network (KAN) can effectively replace the Multi-layer Perceptron (MLP) method in generative AI, particularly in the subdomain of image-to-image translation, to achieve better generative quality. Our novel approach replaces the two-layer MLP with a two-layer KAN in the existing Contrastive Unpaired Image-to-Image Translation (CUT) model, developing the KAN-CUT model. This substitution favors the generation of more informative features in low-dimensional vector representations, which contrastive learning can utilize more effectively to produce high-quality images in the target domain. Extensive experiments, detailed in the results section, demonstrate the applicability of KAN in conjunction with contrastive learning and GANs in Generative AI, particularly for image-to-image translation. This work suggests that KAN could be a valuable component in the broader generative AI domain.", "sections": [{"title": "I. INTRODUCTION", "content": "Generative AI is prevalent across various research fields, including images, texts, videos, and more. It has been developed to achieve generative outcomes such as text-to-text [1], text-to-image [2], image-to-text [3], text-to-video [4], video-to-video [5], and image-to-image generation or translation [6]. The present study primarily focuses on image-to-image translation, a subdomain of Generative AI.\nThe popularity and success of Generative AI can be largely attributed to the flexibility and expressiveness of Multi-layer Perceptrons (MLPs) [7]\u2013[9]. Despite their significant contri-butions to the field of Deep Learning (a branch of Generative AI), MLPs have certain limitations, such as the inability to optimize univariate functions effectively and their lower accuracy compared to splines in low-dimensional spaces.\nRecently, the Kolmogorov-Arnold Network (KAN) [10], based on the Kolmogorov-Arnold representation theorem [11], has been proposed as a potential replacement for MLPs. KAN combines the strengths of MLPs and splines, offer-ing improved accuracy and interpretability. The authors have demonstrated KAN's better performance compared to MLPS in small-scale AI applications and suggested its potential in broader applications.\nImage-to-image translation is a well-researched domain where images from domain A are translated to domain B using generative mechanisms to achieve various outcomes such as facial attribute manipulation [12], medical image analysis [13], and geospatial analysis [14]. Among various mechanisms like VAEs [15] and diffusion models [16], GANS have been widely used to achieve image-to-image translation. Initially, the Pix2Pix model [6] utilized paired images of domains A and B in a supervised manner for image-to-image translation. However, due to the impracticality of obtaining paired images, the CycleGAN model [17] was introduced to perform translation in an unsupervised manner. Several other promising GANs equipped with unsupervised principles, such as GCGAN [18], CUT [19], DCLGAN [20], and StarGAN [21], have since been proposed. Among these, CUT is partic-ularly notable for its accuracy, computational efficiency, and time complexity due to its uni-directional training. This model combines Generative Adversarial training with mutual infor-mation maximization using contrastive learning to generate high-quality images in the target domain.\nA noteworthy aspect of this mechanism is its use of con-trastive learning, inspired by the SIMCLR [22] framework, where features processed from different layers of the generator are fed into a two-layer MLP to enhance feature representation. This process allows for high-quality image generation in the target domain.\nUnderstanding the importance of KAN, to demonstrate and potentially prove the applicability of KAN in Generative AI, specifically in image-to-image translation, we first propose a novel customization of KAN and construct an efficient two-layer KAN, which we then use to replace the two-layer MLP in CUT, giving rise to the KAN-CUT model. Our overall contributions in the present study are:\n\u2022 Reformulated the KAN architecture to improve efficiency by avoiding the expansion of the input tensor and remov-"}, {"title": "II. RELATED WORK", "content": "In unpaired image-to-image translation within the GANs domain, CycleGAN [17] was a foundational work where images from domains A and B were trained on the cyclic principle. This principle dictates that if b is an image generated by generator G with a as an input, then we should be able to obtain the original image a when we feed the generated image b to another generator F. The ultimate goal is to obtain a mapping of each instance from A and B in the absence of paired images, so that a previously unseen instance image from domain A can be accurately translated to another instance of an image in domain B. While the model was very successful and is still being applied in various research endeavors, it has limitations such as being restrictive, computationally expensive, and having high time complexity due to its cyclic nature.\nTo address limitations related to time complexity and com-putational expense, several different works in GANs have been presented that make the training process single-directional, thus eliminating the need for auxiliary generators and discrimi-nators. Notable examples include GCGAN [18] and CUT [19]. GCGAN leverages geometric consistency to impose the struc-tural correspondence between the input and output images, ensuring that the generated image maintains the geometrical structure of the input image.\nContrastive Learning, a methodology that favors achiev-ing numerous Machine Learning (ML) and Deep Learning tasks such as classification, segmentation, simulation, and generation, operates with a self-supervised mechanism in the absence of supervised data. SIMCLR, a contrastive framework presented in [22], was a remarkable work in Contrastive Learning based on image data that obtained better results than previous semi-supervised and self-supervised tasks, and comparable performance to well-known supervised models like ResNet50 [23] in the ImageNet dataset. In their work, the authors showcased that simple data augmentation and applying contrastive learning to the patches of augmented images can help determine similar features in embedding space, and maximizing mutual information can lead to the final goal without the need for human-labeled data. More importantly, the authors of SIMCLR [22] found that instead of directly applying contrastive learning to features or vectors in lower dimensions, processing the features with a two-layer MLP before applying contrastive learning yielded better results.\nInspired by SIMCLR, [19] proposed the CUT model, which utilized contrastive learning at the patch level, integrated with GAN in a uni-directional fashion, to address the limitations of CycleGAN. This model successfully achieved better perfor-mance based on quantitative evaluation along with lower time complexity. The most important procedure in this method is that during training, patches drawn from the input image and target domain image are processed through network layers and propagated to a two-layer MLP to obtain enriched features or vectors, following [22]. In the obtained enriched vectors, contrastive learning is carried out, and maximization of in-formation among corresponding patches is done to maintain qualitative and selective image generation. It can be seen that the two-layer MLP is a crucial component that helps generate enriched vectors or features used for image-to-image translation.\nSome subsequent works focused on improving the quality of generation while still incorporating bi-directional training with the need for two generators and two discriminators but with additional changes in the principle. Examples of these works include DualGAN [24] and DCLGAN [20]. DualGAN introduced a dual learning mechanism inspired by natural lan-guage translation. It utilizes a primary GAN to convert images from the input domain to the output domain and a secondary GAN to reverse this conversion. This architecture ensures consistent and accurate mappings between domains by using reconstruction loss, which enhances the quality and robustness of the generated images. Similarly, DCLGAN [20] shows that reverse mapping can be achieved without depending on the generated images, leading to non-restrictive mapping, unlike the mechanism of CycleGAN [17]. It aligns with CycleGAN [17] by using two generators and two discriminators and bi-directional training, additionally applying contrastive learning, similar to CUT [19], by maximizing mutual information among patches in both directions. DCLGAN can be considered an upgraded version of CUT. These GANs, successful in unpaired image-to-image translation, were mainly constructed to function between two domains. However, there are scenarios where generation is needed among multiple domains, such as generating images from various attributes or styles. StarGAN [21] was proposed to achieve multi-domain translation using a single generator and discriminator. StarGAN [21] was pro-posed to achieve translation across multiple domains using a unified generator and discriminator. StarGAN acquires map-pings among various domains by conditioning the generator on domain labels, allowing it to flexibly translate an input image to any desired target domain.\nKANS [10], recently proposed, have been applied to various Machine Learning and Deep Learning tasks, such as image classification [25], image segmentation and generation [26], time-series analysis [27], and graph-based learning [28]. Even"}, {"title": "III. PROPOSED APPROACH", "content": "In this section, we first detail the architecture of the Kolmogorov-Arnold Network (KAN), followed by our novel changes and enhancements to the architecture. We then pro-ceed to integrate the customized efficient KAN into the domain of image-to-image translation by replacing the two-layer Multi-layer Perceptron (MLP) with a two-layer KAN in the Contrastive Unpaired Image-to-Image Translation (CUT) [19] model, resulting in the KAN-CUT model.\nKolmogorov-Arnold Networks (KANs) have been discussed as a significant advancement in machine learning, often re-ferred to as Machine Learning 2.0 among researchers. At a high level, our approach facilitates the generation of better-informed features in lower dimensions where contrastive learn-ing can be performed. Due to the lack of previous research on KANs in representation learning, it is not straightforward to deduce their relevance for understanding or generating feature vectors in embedding space better than the well-established MLPs. To build our initial confidence, we conducted a sim-ulation where data had three different labels (categories) with various features in a simulated embedding space. After training simple MLP and KAN models separately on the given simulated data, we visualized the results using t-SNE [29] and observed that KANs performed better in clustering similar data points in the feature representation, as depicted in Fig. 1.\nBefore exploring how KAN can be used in generative tasks, we first discuss the fundamentals of KAN. Unlike MLP, which is based on the Universal Approximation Theorem, KAN is based on the Kolmogorov-Arnold Representation Theorem [11]. This theorem states that if f is a multivariate continuous function on a bounded domain, then f can be simplified into a finite composition of continuous single-variable functions and the binary operation of addition:\n$$f(x) = f(x_1,...,x_n) = \\sum_{q=1}^{2n+1} \\Phi_q(\\sum_{p=1}^{n} \\varphi_{q,p}(x_p))$$\nwhere $\\varphi_{q,p}: [0,1] \\rightarrow \\mathbb{R}$ and $\\Phi_q: \\mathbb{R} \\rightarrow \\mathbb{R}$.\nOur main goal is to translate or generate images from one domain, say domain A, to another domain, say domain B. For this, we seek a function g such that g(A) yields B' \u2248 B. According to [10], we can approximate g by learning a discrete number of one-dimensional functions, parameterized by B-spline curves with tunable coefficients of local B-spline functions.\nKAN is comparable to MLP, but instead of learnable weights, it has learnable activation functions on edges, and summation on the resultant learned function's output is per-formed at the nodes. As explained in [10], a KAN layer with $n_{in}$-dimensional inputs and $n_{out}$-dimensional outputs can be represented as a matrix of 1D functions:\n$$\\Phi = {\\varphi_{q,p}}, p = 1,2,......, n_{in}, q = 1,2......,n_{out},$$\nwhere each function's $\\varphi_{q,p}$ parameters are trainable. In the Kolmogorov-Arnold theorem, the internal functions constitute a KAN layer with $n_{in} = n$ and $n_{out} = 2n + 1$, while the external functions form another KAN layer with $n_{in} = 2n+1$ and $n_{out} = 1$. Thus, the Kolmogorov-Arnold representations in Eq. (2.1) are constructed by composing two KAN layers.\nB-Splines\nB-splines are piecewise polynomial curves constructed from a sequence of lower-order polynomial segments. They are defined by a set of control points $P_i$ and a knot vector u that determines the influence of each control point on the B-spline curve.\nThe basis function for B-splines of order 0 (piecewise constant) is defined as:\n$$M_{i,0}(u) =\\begin{cases}\n1 & \\text{if } u_i \\leq u < u_{i+1} \\\\\n0 & \\text{otherwise}\n\\end{cases}$$\nFor higher-order basis functions (piecewise linear, quadratic, etc.), the recursive definition is:\n$$M_{i,k}(u) = \\frac{u - u_i}{u_{i+k} - u_i} M_{i,k-1}(u) + \\frac{u_{i+k+1} - u}{u_{i+k+1} - u_{i+1}} M_{i+1,k-1}(u)$$\nwhere k = 1, ..., d, and d is the degree of the B-spline.\nThe B-spline curve is then defined by:\n$$c(u) = \\sum_{i=0}^{m} P_i M_{i,d}(u)$$\nwhere $P_i$ are the control points, $M_{i,d}(u)$ are the basis functions of degree d, and u is the parameter.\nIn this context, $u_i$ represents the elements of the knot vector, and m corresponds to the total number of control points minus one.\nWe have demonstrated the importance of the two-layer MLP in the image-to-image translation domain, particularly in applications involving contrastive learning and generative adversarial networks, as mentioned in Section II. We have also explored the concept of the KAN network and its potential applicability. Below, we will introduce our novel changes and refinements of the KAN network, which we will later apply to generative mechanisms.\nEfficient Two-Layer KAN\nFor a layer with in_features inputs and out_features outputs, the original implementation expands the input to a tensor of the shape (batch_size, out_features, in_features) to apply the activation functions. As mentioned above in Subsection A, all the activation functions are linear combinations of a fixed set of basis functions, B-spline curves. Following the work of [30], for efficient processing, we reconstructed the computation process by refraining from the additional step of expanding the input. Instead, we apply learnable activation functions directly to the input tensor and then combine the results linearly. Mathematically, if i is the input, and B(i) represents the B-spline basis functions applied to the input, the reformulated procedure of applying the activation function $\\sigma$ on the input can be expressed as:\n$$\\sigma(i) = \\sigma_l(i) + B(i)$$\nwhere $\\sigma_l(i)$ is the base activation function applied to the input i.\nThis modification simplifies the computation to a straight-forward matrix multiplication, efficiently supporting both for-ward and backward passes. Similarly, regarding regularization, L1 regularization [31] is applied in MLP to achieve sparsity and improve model interpretability. In the original KAN [10], L1 regularization was adapted for learnable activation func-tions rather than linear weights since there are no learnable weights in terms of KAN. Specifically, the L1 norm was defined as the average magnitude of these activation func-tions over their inputs. This approach required the activation functions to be sparsified by their L1 norm. Additionally, an entropy regularization was necessary to further enforce sparsity. In our implementation of KAN, we align with the original approach by applying L1 regularization to the B-spline activation functions (\u2018spline_activation') to enforce sparsity and control overfitting. The main difference is the removal of the tensor expansion step and additional entropy regular-ization, making the process more efficient.\nTo obtain an optimizable layer, the original KAN imple-mentation [10] used a residual connection mechanism and expressed the activation function $\\varphi(x)$ as the combination of the basis function $b(x)$ and the spline function, given as:\n$$\\varphi(x) = w_b b(x) + w_s spline(x)$$\nwhere $w_b$ and $w_s$ are learnable weights, but they can be considered redundant since they can be merged into $b(x)$ and $spline(x)$ [10].\nInstead of the additive operation, we applied concatenation and processed it with Gated Linear Units (GLUs) [32], in a novel way, to enrich features while maintaining the same number of parameters to avoid an increase in computation, as detailed in the subsection below.\n1) Integration of Basis and Spline Functions via Concate-nation and Gated Linear Units (GLUs): In our Kolmogorov Arnold Network (KAN) layer implementation, we aim to enhance expressive power while maintaining the same feature dimensionality. To obtain this, we propose the following approach:\n1) Concatenation: Compute the outputs of $b(x)$ and $spline(x)$, then concatenate them:\n$$concatenated\\_output = concat(w_b \u00b7 b(x), w_s \u00b7 spline(x))$$\n2) Gated Linear Units (GLUs): Apply GLU to the con-catenated output for non-linear transformation while maintaining the original feature dimensionality:\n$$\\phi(x) = (W\u00b7concatenated\\_output+b)\\odot \\rho(V\u00b7concatenated\\_output+c)$$\nwhere $\\odot$ denotes element-wise multiplication and $\\rho$ represents the penalized tanh function with our customized implementation:\n$$\\rho_{\\alpha}(x) = tanh(x) + \\alpha\u00b7 max(-x, 0)$$\nHere, $\\alpha$ is a hyperparameter that controls the penalty applied to the negative part of the input x. It is worth mentioning that, while the sigmoid function is generally used in GLU, we integrated the penalized tanh as shown above, as it yielded better results during experiments.\nWith these steps, we make different use of the residual connection mechanism, unlike the original KAN implemen-tation, and implement the activation function $\\phi(x)$ as the concatenation of the basis function and the spline function. The construction ends with non-linear transformations to maintain the initial dimension of the concatenated inputs."}, {"title": "D. Formulation of KAN-CUT Model", "content": "Conforming to the principle of Generative Adversarial Networks (GANs), we integrate two neural network mod-els: a generator and a discriminator. The generator's role is translating (generating) from input domain A to output domain B, while the discriminator determines whether the provided image is genuine or artificial. Following recent literature in image-to-image translation [17], [19], [20], we have implemented ResNet-based encoder-decoder architecture of the generator.\nDuring the translation of an image a in A to an image b in B, not only should the entire image share information, but the individual patches within the image should also share information while transforming domain-dependent informa-tion in the generated image. For instance, let's say we are translating an image from a cat to a dog. The generated dog image should share local information in the patches, meaning the eyes and nose should appear in the same locations as they were in the cat image (see Fig. 3, where we depict the appearance of the eyes of a cat and a dog with blue-colored squares), while the obtained domain-dependent features such as fur patterns and facial structure should be unique to a dog (see Fig. 3, where we highlight fur patterns with red-colored squares). We can achieve this effect of the images sharing patch-level information by maximizing information between corresponding patches while minimizing information in the non-corresponding patches. Since our work is based on an unsupervised mechanism, it is not straight forward to determine which patches are similar or corresponding and how to maximize the information. Thanks to the predictive mechanism of contrastive learning [22], [33], we can use contrastive loss to successfully select the correct positive patch from a given set of patches in which there exists one positive and several negatives for a given query. More specifically, fol-lowing [19], working in a vector representation, we construct an (N+1)-way classification problem, where the probability of positive being selected over negatives is achieved with cross-entropy loss given as:\n$$L(q, p, {n_i}) = -log \\frac{exp((q,p)/\\tau)}{exp((q,p)/\\tau) + \\sum_{i=1}^N exp((q,n_i)/\\tau)}$$\nwhere $q, p\\in \\mathbb{R}^K$ and $n_i \\in \\mathbb{R}^{N\\times K}$ are K-dimensional vectors corresponding to query, positive, and N negatives, respectively. \\tau is a temperature parameter to scale the distances between vectors. Following this procedure, we can select the correct query and positive vector from the set of vectors and maximize the mutual information between them in the embedding space. It is clear that we can obtain the selection and maximization of mutual information between vectors.\nNow, the question arises: how can we obtain the vectors or features where we can apply contrastive estimation?\nAs mentioned in [19], since images are processed in the generator G, we can utilize features from intermediate layers of the generator's encoder, $G_{enc}$. Inspired by [19] and [22], we select X layers from the encoder and pass them to our in-novative two-layer efficient KAN instead of a two-layer MLP (see Fig. 4 illustrating the difference in layer implementation between KAN and MLP), producing a more informative stack of features. For instance, let's say we feed image a to the generator G and generate image G(a). To obtain stacks of features from which the positive feature or vector is chosen, we select the x-th layer from the encoder through which a is being processed and pass it to the two-layer KAN network $K_\\iota$ to produce a stack of features {$z_x$}$_x$ = {$K_x(G_{enc}(a))$}$_x$ (as depicted with the red-colored arrow in Fig. 5).\nIn each layer, there are several spatial locations quantified by $S_x$, where s$\\in$ {1, . . ., $S_\\pounds$}. We refer to the positive feature as $z_s^\\oplus \\in \\mathbb{R}^{C_x}$ and the negative features as $z_\\s^\\ominus \\in \\mathbb{R}^{(S_x-1)\\times C_#}$, where $C_x$ denotes the channel count at each layer. To denote a query vector or feature in this instance, we feed the generated image G(a) to the same encoder, select the x-th layer, and pass it to the two-layer KAN network $K_\\iota$ to produce another stack of features, {$z_x'$}$_x$ = {$K_x(G_{enc}(G(a)))$}$_x$ (as depicted with green-colored arrow in Fig. 5). From this feature stack, we refer to a corresponding query point as $z_s'$. It is worth mentioning that negatives vectors are drawn from the same input image, following [19]. Once we have the query, positive, and negative vectors, we can perform constrastive learning using the loss function given above in equation (1). The above procedure is performed for an instance of a spatial location and from a layer; however, we need to do this for all spatial locations and layers. For that, we use PatchNCE loss as presented in [19], which can be expressed as:\n$$\\mathcal{L}_{PatchNCE}(G, K, A) = \\mathbb{E}_{a\\sim A} \\sum_{x=1}^X \\sum_{s=1}^{S_x} \\mathcal{S}(z_s', z_s^{\\oplus}, {z_\\s^\\ominus})$$\n1) Adversarial Loss for Generation of Realistic-Looking Images: We can achieve mutual information maximization at the patch level with the above procedure using contrastive learning and the two-layer efficient KAN. For high-quality image generation, we use adversarial loss from generative adversarial networks (GANs). The initial work in GANs was introduced in [35], and this architecture is considered as Vanilla GAN. The authors introduced the use of a sigmoid cross-entropy loss for GANs. This loss can sometimes cause vanishing gradients when data samples fall within the correct classification boundary but remain distant from the actual data distribution. To mitigate this problem, [36] proposed Least Squares Generative Adversarial Networks (LSGANs), replacing the binary cross-entropy loss with a least squares loss. Following principles of [35] and [36], to guarantee the generation of authentic images perceived by humans in domain B', such that B' \u2248 B for the given input images from domain A, we utilize an adversarial loss based on the least squares loss. The adversarial loss, also known as LSGAN loss, comprises two main components: the loss for the generator and the loss for the discriminator. These losses guide the back-"}, {"title": "V. CONCLUSION, LIMITATION, AND FUTURE WORK", "content": "GANs, being among the most prominent generative models, have played a major role in the success of Image-to-Image (I2I) generation (translation), a subdomain of Generative AI. Despite their success, there are several areas requiring further research, particularly in terms of accuracy, computational efficiency, and knowledge transferability. Focusing primarily on accuracy and demonstrating the efficacy of Kolmogorov-Arnold Networks (KAN) [10] in the image-to-image transla-tion domain, we propose the KAN-CUT model. This novel model replaces the two-layer MLP in the CUT model [19] with a two-layer, efficient, and customized KAN.\npropose and demonstrate the applicability of KAN in image-to-image translation, potentially paving the way for numerous applications of KAN in Generative AI, which we find very promising.\nWhile the present study has been successful, it has some limitations. Our primary goal was to demonstrate the applica-bility of KAN [10] and to showcase its improved performance compared to MLP. We do not claim improvement over the state-of-the-art performance in the image-to-image translation domain. Additionally, the KAN-CUT model's performance has been demonstrated on only two datasets, Horse\u2192Zebra and Cat\u2192Dog, due to time and resource constraints. Experimental validation on other datasets remains an area for future work."}]}