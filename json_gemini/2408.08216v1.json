{"title": "The Dawn of KAN in Image-to-Image (I2I) Translation: Integrating Kolmogorov-Arnold Networks with GANs for Unpaired I2I Translation", "authors": ["Arpan Mahara", "Naphtali D. Rishe", "Liangdong Deng"], "abstract": "Image-to-Image translation in Generative Artificial Intelligence (Generative AI) has been a central focus of research, with applications spanning healthcare, remote sensing, physics, chemistry, photography, and more. Among the numerous methodologies, Generative Adversarial Networks (GANs) with contrastive learning have been particularly successful. This study aims to demonstrate that the Kolmogorov-Arnold Network (KAN) can effectively replace the Multi-layer Perceptron (MLP) method in generative AI, particularly in the subdomain of image-to-image translation, to achieve better generative quality. Our novel approach replaces the two-layer MLP with a two-layer KAN in the existing Contrastive Unpaired Image-to-Image Translation (CUT) model, developing the KAN-CUT model. This substitution favors the generation of more informative features in low-dimensional vector representations, which contrastive learning can utilize more effectively to produce high-quality images in the target domain. Extensive experiments, detailed in the results section, demonstrate the applicability of KAN in conjunction with contrastive learning and GANs in Generative AI, particularly for image-to-image translation. This work suggests that KAN could be a valuable component in the broader generative AI domain.", "sections": [{"title": "I. INTRODUCTION", "content": "Generative AI is prevalent across various research fields, including images, texts, videos, and more. It has been developed to achieve generative outcomes such as text-to-text [1], text-to-image [2], image-to-text [3], text-to-video [4], video-to-video [5], and image-to-image generation or translation [6]. The present study primarily focuses on image-to-image translation, a subdomain of Generative AI.\nThe popularity and success of Generative AI can be largely attributed to the flexibility and expressiveness of Multi-layer Perceptrons (MLPs) [7]\u2013[9]. Despite their significant contributions to the field of Deep Learning (a branch of Generative AI), MLPs have certain limitations, such as the inability to optimize univariate functions effectively and their lower accuracy compared to splines in low-dimensional spaces.\nRecently, the Kolmogorov-Arnold Network (KAN) [10], based on the Kolmogorov-Arnold representation theorem [11], has been proposed as a potential replacement for MLPs. KAN combines the strengths of MLPs and splines, offering improved accuracy and interpretability. The authors have demonstrated KAN's better performance compared to MLPS in small-scale AI applications and suggested its potential in broader applications.\nImage-to-image translation is a well-researched domain where images from domain A are translated to domain B using generative mechanisms to achieve various outcomes such as facial attribute manipulation [12], medical image analysis [13], and geospatial analysis [14]. Among various mechanisms like VAEs [15] and diffusion models [16], GANS have been widely used to achieve image-to-image translation. Initially, the Pix2Pix model [6] utilized paired images of domains A and B in a supervised manner for image-to-image translation. However, due to the impracticality of obtaining paired images, the CycleGAN model [17] was introduced to perform translation in an unsupervised manner. Several other promising GANs equipped with unsupervised principles, such as GCGAN [18], CUT [19], DCLGAN [20], and StarGAN [21], have since been proposed. Among these, CUT is particularly notable for its accuracy, computational efficiency, and time complexity due to its uni-directional training. This model combines Generative Adversarial training with mutual information maximization using contrastive learning to generate high-quality images in the target domain.\nA noteworthy aspect of this mechanism is its use of contrastive learning, inspired by the SIMCLR [22] framework, where features processed from different layers of the generator are fed into a two-layer MLP to enhance feature representation. This process allows for high-quality image generation in the target domain.\nUnderstanding the importance of KAN, to demonstrate and potentially prove the applicability of KAN in Generative AI, specifically in image-to-image translation, we first propose a novel customization of KAN and construct an efficient two-layer KAN, which we then use to replace the two-layer MLP in CUT, giving rise to the KAN-CUT model. Our overall contributions in the present study are:\n\u2022 Reformulated the KAN architecture to improve efficiency by avoiding the expansion of the input tensor and removing additional entropy regularization for L1 normalization.\n\u2022 Enhanced the KAN layer by implementing an activation function that concatenates the basis function and spline function, replacing the original addition operation, and supplemented with additional processing using Gated Linear Units (GLU).\n\u2022 Innovatively replaced the two-layer MLP in the CUT model with our efficient two-layer KAN, creating the KAN-CUT model for unpaired image-to-image translation.\n\u2022 This study marks the first integration of KAN in the image-to-image translation domain (a subdomain of Generative AI), potentially paving the way for numerous applications of KAN in different Generative AI domains to achieve better outcomes."}, {"title": "II. RELATED WORK", "content": "In unpaired image-to-image translation within the GANs domain, CycleGAN [17] was a foundational work where images from domains A and B were trained on the cyclic principle. This principle dictates that if b is an image generated by generator G with a as an input, then we should be able to obtain the original image a when we feed the generated image b to another generator F. The ultimate goal is to obtain a mapping of each instance from A and B in the absence of paired images, so that a previously unseen instance image from domain A can be accurately translated to another instance of an image in domain B. While the model was very successful and is still being applied in various research endeavors, it has limitations such as being restrictive, computationally expensive, and having high time complexity due to its cyclic nature.\nTo address limitations related to time complexity and computational expense, several different works in GANs have been presented that make the training process single-directional, thus eliminating the need for auxiliary generators and discriminators. Notable examples include GCGAN [18] and CUT [19]. GCGAN leverages geometric consistency to impose the structural correspondence between the input and output images, ensuring that the generated image maintains the geometrical structure of the input image.\nContrastive Learning, a methodology that favors achieving numerous Machine Learning (ML) and Deep Learning tasks such as classification, segmentation, simulation, and generation, operates with a self-supervised mechanism in the absence of supervised data. SIMCLR, a contrastive framework presented in [22], was a remarkable work in Contrastive Learning based on image data that obtained better results than previous semi-supervised and self-supervised tasks, and comparable performance to well-known supervised models like ResNet50 [23] in the ImageNet dataset. In their work, the authors showcased that simple data augmentation and applying contrastive learning to the patches of augmented images can help determine similar features in embedding space, and maximizing mutual information can lead to the final goal without the need for human-labeled data. More importantly, the authors of SIMCLR [22] found that instead of directly applying contrastive learning to features or vectors in lower dimensions, processing the features with a two-layer MLP before applying contrastive learning yielded better results.\nInspired by SIMCLR, [19] proposed the CUT model, which utilized contrastive learning at the patch level, integrated with GAN in a uni-directional fashion, to address the limitations of CycleGAN. This model successfully achieved better performance based on quantitative evaluation along with lower time complexity. The most important procedure in this method is that during training, patches drawn from the input image and target domain image are processed through network layers and propagated to a two-layer MLP to obtain enriched features or vectors, following [22]. In the obtained enriched vectors, contrastive learning is carried out, and maximization of information among corresponding patches is done to maintain qualitative and selective image generation. It can be seen that the two-layer MLP is a crucial component that helps generate enriched vectors or features used for image-to-image translation.\nSome subsequent works focused on improving the quality of generation while still incorporating bi-directional training with the need for two generators and two discriminators but with additional changes in the principle. Examples of these works include DualGAN [24] and DCLGAN [20]. DualGAN introduced a dual learning mechanism inspired by natural language translation. It utilizes a primary GAN to convert images from the input domain to the output domain and a secondary GAN to reverse this conversion. This architecture ensures consistent and accurate mappings between domains by using reconstruction loss, which enhances the quality and robustness of the generated images. Similarly, DCLGAN [20] shows that reverse mapping can be achieved without depending on the generated images, leading to non-restrictive mapping, unlike the mechanism of CycleGAN [17]. It aligns with CycleGAN [17] by using two generators and two discriminators and bi-directional training, additionally applying contrastive learning, similar to CUT [19], by maximizing mutual information among patches in both directions. DCLGAN can be considered an upgraded version of CUT. These GANs, successful in unpaired image-to-image translation, were mainly constructed to function between two domains. However, there are scenarios where generation is needed among multiple domains, such as generating images from various attributes or styles. StarGAN [21] was proposed to achieve multi-domain translation using a single generator and discriminator. StarGAN [21] was proposed to achieve translation across multiple domains using a unified generator and discriminator. StarGAN acquires mappings among various domains by conditioning the generator on domain labels, allowing it to flexibly translate an input image to any desired target domain.\nKANS [10], recently proposed, have been applied to various Machine Learning and Deep Learning tasks, such as image classification [25], image segmentation and generation [26], time-series analysis [27], and graph-based learning [28]. Even"}, {"title": "III. PROPOSED APPROACH", "content": "In this section, we first detail the architecture of the Kolmogorov-Arnold Network (KAN), followed by our novel changes and enhancements to the architecture. We then proceed to integrate the customized efficient KAN into the domain of image-to-image translation by replacing the two-layer Multi-layer Perceptron (MLP) with a two-layer KAN in the Contrastive Unpaired Image-to-Image Translation (CUT) [19] model, resulting in the KAN-CUT model.\nKolmogorov-Arnold Networks (KANs) have been discussed as a significant advancement in machine learning, often referred to as Machine Learning 2.0 among researchers. At a high level, our approach facilitates the generation of better-informed features in lower dimensions where contrastive learning can be performed. Due to the lack of previous research on KANs in representation learning, it is not straightforward to deduce their relevance for understanding or generating feature vectors in embedding space better than the well-established MLPs. To build our initial confidence, we conducted a simulation where data had three different labels (categories) with various features in a simulated embedding space. After training simple MLP and KAN models separately on the given simulated data, we visualized the results using t-SNE [29] and observed that KANs performed better in clustering similar data points in the feature representation, as depicted in Fig. 1.\nBefore exploring how KAN can be used in generative tasks, we first discuss the fundamentals of KAN. Unlike MLP, which is based on the Universal Approximation Theorem, KAN is based on the Kolmogorov-Arnold Representation Theorem [11]. This theorem states that if f is a multivariate continuous function on a bounded domain, then f can be simplified into a finite composition of continuous single-variable functions and the binary operation of addition:\n$f(x) = f(x_1,...,x_n) = \\sum_{q=1}^{2n+1}\\Phi_q(\\sum_{p=1}^{n} \\varphi_{q,p}(x_p))$\nwhere $\\varphi_{q,p}: [0,1] \\rightarrow R$ and $\\Phi_q: R \\rightarrow R$.\nOur main goal is to translate or generate images from one domain, say domain A, to another domain, say domain B. For this, we seek a function g such that g(A) yields B' \u2248 B. According to [10], we can approximate g by learning a discrete number of one-dimensional functions, parameterized by B-spline curves with tunable coefficients of local B-spline functions.\nKAN is comparable to MLP, but instead of learnable weights, it has learnable activation functions on edges, and summation on the resultant learned function's output is performed at the nodes. As explained in [10], a KAN layer with $n_{in}$-dimensional inputs and $n_{out}$-dimensional outputs can be represented as a matrix of 1D functions:\n$\\Phi = {\\varphi_{q,p}}, p = 1,2,\\ldots, n_{in}, q = 1,2,\\ldots, n_{out}$ \nwhere each function's $\\varphi_{q,p}$ parameters are trainable. In the Kolmogorov-Arnold theorem, the internal functions constitute a KAN layer with $n_{in} = n$ and $n_{out} = 2n + 1$, while the external functions form another KAN layer with $n_{in} = 2n + 1$ and $n_{out} = 1$. Thus, the Kolmogorov-Arnold representations in Eq. (2.1) are constructed by composing two KAN layers.\nB-Splines\nB-splines are piecewise polynomial curves constructed from a sequence of lower-order polynomial segments. They are defined by a set of control points $P_i$ and a knot vector $u$ that determines the influence of each control point on the B-spline curve.\nThe basis function for B-splines of order 0 (piecewise constant) is defined as:\n$M_{i,0}(u) =\n\\begin{cases}\n1 & \\text{if } u_i \\leq u < u_{i+1} \\\\\n0 & \\text{otherwise}\n\\end{cases}$"}, {"title": "C. Efficient Two-Layer KAN", "content": "For a layer with in_features inputs and out_features outputs, the original implementation expands the input to a tensor of the shape (batch_size, out_features, in_features) to apply the activation functions. As mentioned above in Subsection A, all the activation functions are linear combinations of a fixed set of basis functions, B-spline curves. Following the work of [30], for efficient processing, we reconstructed the computation process by refraining from the additional step of expanding the input. Instead, we apply learnable activation functions directly to the input tensor and then combine the results linearly. Mathematically, if i is the input, and B(i) represents the B-spline basis functions applied to the input, the reformulated procedure of applying the activation function \u03c3 on the input can be expressed as:\n$\\sigma(i) = \\sigma_l(i) + B(i)$\nwhere $\\sigma_l(i)$ is the base activation function applied to the input i.\nThis modification simplifies the computation to a straightforward matrix multiplication, efficiently supporting both forward and backward passes. Similarly, regarding regularization, L1 regularization [31] is applied in MLP to achieve sparsity and improve model interpretability. In the original KAN [10], L1 regularization was adapted for learnable activation functions rather than linear weights since there are no learnable weights in terms of KAN. Specifically, the L1 norm was defined as the average magnitude of these activation functions over their inputs. This approach required the activation functions to be sparsified by their L1 norm. Additionally, an entropy regularization was necessary to further enforce sparsity. In our implementation of KAN, we align with the original approach by applying L1 regularization to the B-spline activation functions (\u2018spline_activation') to enforce sparsity and control overfitting. The main difference is the removal of the tensor expansion step and additional entropy regularization, making the process more efficient.\nTo obtain an optimizable layer, the original KAN implementation [10] used a residual connection mechanism and expressed the activation function \u03c6(x) as the combination of the basis function b(x) and the spline function, given as:\n$\\varphi(x) = w_b b(x) + w_s spline(x)$\nwhere $w_b$ and $w_s$ are learnable weights, but they can be considered redundant since they can be merged into b(x) and spline(x) [10].\nInstead of the additive operation, we applied concatenation and processed it with Gated Linear Units (GLUs) [32], in a novel way, to enrich features while maintaining the same number of parameters to avoid an increase in computation, as detailed in the subsection below.\nIntegration of Basis and Spline Functions via Concatenation and Gated Linear Units (GLUs): In our Kolmogorov Arnold Network (KAN) layer implementation, we aim to enhance expressive power while maintaining the same feature dimensionality. To obtain this, we propose the following approach:\n1) Concatenation: Compute the outputs of b(x) and spline(x), then concatenate them:\nconcatenated_output = concat(wb b(x), ws \u00b7 spline(x))\n2) Gated Linear Units (GLUs): Apply GLU to the concatenated output for non-linear transformation while maintaining the original feature dimensionality:\n$\\phi(x) = (W\\cdot concatenated\\_output+b) \\odot \\rho(V\\cdot concatenated\\_output+c)$\nwhere $\\odot$ denotes element-wise multiplication and $ \\rho $ represents the penalized tanh function with our customized implementation:\n$\\rho_{\\alpha} (x) = tanh(x) + \\alpha \\cdot max(-x, 0)$\nHere, \u03b1 is a hyperparameter that controls the penalty applied to the negative part of the input x. It is worth mentioning that, while the sigmoid function is generally used in GLU, we integrated the penalized tanh as shown above, as it yielded better results during experiments.\nWith these steps, we make different use of the residual connection mechanism, unlike the original KAN implementation, and implement the activation function \u03c6(x) as the concatenation of the basis function and the spline function. The construction ends with non-linear transformations to maintain the initial dimension of the concatenated inputs."}, {"title": "D. Formulation of KAN-CUT Model", "content": "Conforming to the principle of Generative Adversarial Networks (GANs), we integrate two neural network models: a generator and a discriminator. The generator's role is translating (generating) from input domain A to output domain B, while the discriminator determines whether the provided image is genuine or artificial. Following recent literature in image-to-image translation [17], [19], [20], we have implemented ResNet-based encoder-decoder architecture of the generator.\nDuring the translation of an image a in A to an image b in B, not only should the entire image share information, but the individual patches within the image should also share information while transforming domain-dependent information in the generated image. For instance, let's say we are translating an image from a cat to a dog. The generated dog image should share local information in the patches, meaning the eyes and nose should appear in the same locations as they were in the cat image (see Fig. 3, where we depict the appearance of the eyes of a cat and a dog with blue-colored squares), while the obtained domain-dependent features such as fur patterns and facial structure should be unique to a dog (see Fig. 3, where we highlight fur patterns with red-colored squares). We can achieve this effect of the images sharing patch-level information by maximizing information between corresponding patches while minimizing information in the non-corresponding patches. Since our work is based on an unsupervised mechanism, it is not straight forward to determine which patches are similar or corresponding and how to maximize the information. Thanks to the predictive mechanism of contrastive learning [22], [33], we can use contrastive loss to successfully select the correct positive patch from a given set of patches in which there exists one positive and several negatives for a given query. More specifically, following [19], working in a vector representation, we construct an (N+1)-way classification problem, where the probability of positive being selected over negatives is achieved with cross-entropy loss given as:\n$L(q, p, {n_i}) = -log(\\frac{exp((q,p)/\\tau)}{\\sum_{i=0}^{N} exp((q,n_i)/\\tau)})$\nwhere q, p\u2208 RK and $n_i$ \u2208 $R^{N\u00d7K}$ are K-dimensional vectors corresponding to query, positive, and N negatives, respectively. \u03c4 is a temperature parameter to scale the distances between vectors. Following this procedure, we can select the correct query and positive vector from the set of vectors and maximize the mutual information between them in the embedding space. It is clear that we can obtain the selection and maximization of mutual information between vectors.\nNow, the question arises: how can we obtain the vectors or features where we can apply contrastive estimation?\nAs mentioned in [19], since images are processed in the generator G, we can utilize features from intermediate layers of the generator's encoder, Genc. Inspired by [19] and [22], we select X layers from the encoder and pass them to our innovative two-layer efficient KAN instead of a two-layer MLP (see Fig. 4 illustrating the difference in layer implementation between KAN and MLP), producing a more informative stack of features. For instance, let's say we feed image a to the generator G and generate image G(a). To obtain stacks of features from which the positive feature or vector is chosen, we select the x-th layer from the encoder through which a is being processed and pass it to the two-layer KAN network $K_\\iota$ to produce a stack of features {$z_x$}x = {$K_x(G_{enc}(a))$}x (as depicted with the red-colored arrow in Fig. 5).\nIn each layer, there are several spatial locations quantified by Sx, where s \u2208 {1, . . ., S\u00a3}. We refer to the positive feature as $z_\\iota^s$ \u2208 $R^{C_\\#}$ and the negative features as {$z_\\iota^{S\\setminus s}$} \u2208 $R^{(S_x\u22121)\u00d7C_\\#}$, where Cx denotes the channel count at each layer. To denote a query vector or feature in this instance, we feed the generated image G(a) to the same encoder, select the x-th layer, and pass it to the two-layer KAN network K\u03b9 to produce another stack of features, {$z^\\iota_x$}x = {$K_x(G_{enc}(G(a)))$}x (as depicted with green-colored arrow in Fig. 5). From this feature stack, we refer to a corresponding query point as $z^q_\\iota$. It is worth mentioning that negatives vectors are drawn from the same input image, following [19]. Once we have the query, positive, and negative vectors, we can perform constrastive learning using the loss function given above in equation (1). The above procedure is performed for an instance of a spatial location and from a layer; however, we need to do this for all spatial locations and layers. For that, we use PatchNCE loss as presented in [19], which can be expressed as:\n$L_{PatchNCE} (G, K, A) = E_{a \\sim A} \\sum_{x=1}^{X} \\sum_{s=1}^{S_x}(z_\\iota^s, z^p_\\iota, {z_\\iota^{S\\setminus s}})$\nAdversarial Loss for Generation of Realistic-Looking Images: We can achieve mutual information maximization at the patch level with the above procedure using contrastive learning and the two-layer efficient KAN. For high-quality image generation, we use adversarial loss from generative adversarial networks (GANs). The initial work in GANs was introduced in [35], and this architecture is considered as Vanilla GAN. The authors introduced the use of a sigmoid cross-entropy loss for GANs. This loss can sometimes cause vanishing gradients when data samples fall within the correct classification boundary but remain distant from the actual data distribution. To mitigate this problem, [36] proposed Least Squares Generative Adversarial Networks (LSGANs), replacing the binary cross-entropy loss with a least squares loss. Following principles of [35] and [36], to guarantee the generation of authentic images perceived by humans in domain B', such that B' \u2248 B for the given input images from domain A, we utilize an adversarial loss based on the least squares loss. The adversarial loss, also known as LSGAN loss, comprises two main components: the loss for the generator and the loss for the discriminator. These losses guide the backpropagation process through which the neural networks are trained and updated. The general procedure involves training the generator to generate images in the target domain that are identical to real images, whereas the discriminator learns to differentiate real images from generated ones.\nThe adversarial loss can be formulated as:\n$L_{LLSGAN}(D) = \\frac{1}{2} E_{b \\sim B}[(D(b) - 1)^2] + \\frac{1}{2} E_{a \\sim A}[(D(G(a)))^2]$\n$L_{LLSGAN} (G) = \\frac{1}{2} E_{a \\sim A}[(D(G(a)) \u2013 1)^2]$\nIn these equations, D(b) represents the discriminator's decision for a ground truth image b from the target domain, where the label for real images is set to 1. G(a) is the generated image from the input image a from the input domain, where the label for fake images is set to 0. The generator G aims to minimize $L_{LLSGAN}(G)$, making the discriminator believe that the generated images are real by pushing D(G(a)) towards 1. The discriminator D aims to minimize $L_{LLSGAN}(D)$, correctly distinguishing real images from generated ones by pushing D(b) towards 1 and D(G(a)) towards 0. This results in a minimax game between the two, similar to the initial work in [35], promoting the generation of high-quality, realistic images.\nFinal Loss Functions: Following the same principle in [19], we utilize three different loss functions involving only one generator and discriminator. These loss functions include the adversarial loss for generating realistic-looking images, the PatchNCE loss, $L_{PatchNCE}(G, K, A)$, for ensuring patch-level correspondence, and a similar PatchNCE loss, $L_{PatchNCE} (G, K, B)$, to prevent inappropriate translation by the generator, similar to the identity loss presented in [17]. The combined final loss function is formulated as follows:\n$L_{final} = L_{LLSGAN}(G, D, A, B)+\\\\\n\\lambda_{PatchNCE-A}L_{PatchNCE} (G, K, A)+\\lambda_{PatchNCE-B}L_{PatchNCE} (G, K, B)$"}, {"title": "IV. EXPERIMENTS", "content": "For evaluation, we selected two well-known, widely utilized, and publicly accessible datasets for research in image-to-image translation: Horse \u2192 Zebra and Cat \u2192 Dog.\nDatasets\na) Horse Zebra: First introduced in CycleGAN [17], this dataset comprises 1067 horse images and 1344 zebra images, resulting in 2403 training images. Additionally, there are 120 horse images and 140 zebra images, totaling 260 test images.\nb) Cat Dog: First introduced in StarGAN V2 [34], this dataset is a subset of AFHQ and includes 5000 training images and 500 test images for each category.\nEvaluation Metrics\nFor the evaluation metric, we selected the Fr\u00e9chet Inception Distance (FID) [37] score. It is one of the most utilized metrics for assessing the quality of images produced by GANs. The FID score evaluates the distance between feature vectors calculated for real and fake (generated) images, utilizing a pre-trained Inception v3 [38] network. A lower FID score signifies higher similarity to the real images, thus representing better quality of the generated images.\nExperimental Environment and Baselines\na) Experiment Setting: Our experiments were performed in the Python 3.6.8 environment using the PyTorch framework for all facets of training and testing. The computational tasks were carried out on a system comprising NVIDIA A100-PCI GPUs, each featuring 80 GB of HBM2 memory. The computation tasks were facilitated using CUDA version 12.3 and NVIDIA driver version 545.23.08. All the models were trained for 400 epochs using the Adam optimization algorithm [39], set at a learning rate of 0.0001. During the training of our model, KAN-CUT, we chose \\patchNCE-A = 1 and PatchNCE-B = 1.\nb) Baselines: For our comparative analysis, we selected well-known GAN models as our baselines: CycleGAN [17], MUNIT [40], SelfDistance [41], GCGAN [18], CUT [19], and DCLGAN [20]. It is worth noting that all models were trained in our setup except for MUNIT [40], SelfDistance [41], and GCGAN [18], for which the FID scores were recorded from [20], where DCLGAN was proposed.\nResults\nThe quantitative results from each baseline model, including the proposed KAN-CUT, are presented in Table I. It can be seen that KAN-CUT outperforms all the models on both selected datasets, with an FID score of 40.2 on the Horse Zebra dataset, and 59.55 on the Cat Dog dataset.\nAdditionally, our study presents a visualization of the performance of each model in generating zebra images from horse images and dog images from cat images, as depicted in Fig. 6. As mentioned above, we did not run the experiments for MUNIT [40], SelfDistance [41], and GCGAN [18] in our work and could not find the resulting images generated by these models online. Therefore, the visualization comparison was done against CycleGAN, DCLGAN, and CUT. The KAN-CUT model is able to generate higher quality zebra images based on stripes, structure, and color, compared to all these selected baseline models."}, {"title": "V. CONCLUSION, LIMITATION, AND FUTURE WORK", "content": "GANs, being among the most prominent generative models, have played a major role in the success of Image-to-Image (I2I) generation (translation), a subdomain of Generative AI. Despite their success, there are several areas requiring further research, particularly in terms of accuracy, computational efficiency, and knowledge transferability. Focusing primarily on accuracy and demonstrating the efficacy of Kolmogorov-Arnold Networks (KAN) [10] in the image-to-image translation domain, we propose the KAN-CUT model. This novel model replaces the two-layer MLP in the CUT model [19] with a two-layer, efficient, and customized KAN."}]}