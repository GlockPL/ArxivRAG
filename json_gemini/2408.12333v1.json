{"title": "Graph Retrieval Augmented Trustworthiness Reasoning", "authors": ["Ying Zhu", "Shengchang Li", "Ziqian Kong", "Peilan Xu"], "abstract": "Trustworthiness reasoning is crucial in multiplayer games with incomplete information, enabling agents to identify potential allies and adversaries, thereby enhancing reasoning and decision-making processes. Traditional approaches relying on pre-trained models necessitate extensive domain-specific data and considerable reward feedback, with their lack of real-time adaptability hindering their effectiveness in dynamic environments. In this paper, we introduce the Graph Retrieval Augmented Reasoning (GRATR) framework, leveraging the Retrieval-Augmented Generation (RAG) technique to bolster trustworthiness reasoning in agents. GRATR constructs a dynamic trustworthiness graph, updating it in real-time with evidential information, and retrieves relevant trust data to augment the reasoning capabilities of Large Language Models (LLMs). We validate our approach through experiments on the multiplayer game \"Werewolf,\" comparing GRATR against baseline LLM and LLM enhanced with Native RAG and Rerank RAG. Our results demonstrate that GRATR surpasses the baseline methods by over 30% in winning rate, with superior reasoning performance. Moreover, GRATR effectively mitigates LLM hallucinations, such as identity and objective amnesia, and crucially, it renders the reasoning process more transparent and traceable through the use of the trustworthiness graph.", "sections": [{"title": "Introduction", "content": "In multiplayer games with incomplete information, reasoning is crucial for assessing the trustworthiness of players who may conceal their intentions based on their actions, dialogue, and other observable information. Autonomous agents must analyze and evaluate the reliability of available information sources to determine player trust and cooperation (Fig. 1). Currently, key methods supporting such reasoning include symbolic reasoning ( (@; coherence and consistency in neural sequence models with dual system 2021(@) and evidential theory (geometry problem solving with formal language and symbolic reasoning 2021(@), Bayesian reasoning (hierarchy hesitant fuzzy linguistic entropy-based TODIM approach using evidential theory 2020(@; probability to consilience: How explanatory values implement Bayesian reasoning 2021(@; implementations of Bayesian inference 2021(@), and reinforcement learning (RL) (a bayesian multi-hop reasoning framework for knowledge graph reasoning 2021(@; like human: Hierarchical reinforcement learning for knowledge graph reasoning 2020(@; attention-based deep reinforcement learning framework for knowledge graph reasoning 2021(@).\nLarge language model (LLM) is a promising approach for trustworthiness reasoning in such games due to their robust natural language understanding and generation capabilities. They are well-suited for interpreting complex dialogues, inferring different intentions, and detecting hidden motives from context. However, LLMs often face potential risks such as hallucinations and knowledge obsolescence, since their reasoning is based on pre-existing training data. To mitigate these issues, methods such as supervised fine-tuning (SFT), and RL, have been proposed to improve reasoning performance, they often require substantial historical data and reward feedback, which may be scarce in real-world scenarios. Retrieval augmented generation (RAG)(aware knowl-"}, {"title": "Preliminaries", "content": "Multiplayer Game with Incomplete Information\nIn a multi-player game with incomplete information, the game can be described by the following components:\n\u2022 Players: \\(P = \\{P_1,P_2,..., P_n\\}\\), where \\(p_i\\) represents the i-th player.\n\u2022 Types: Each player \\(p_i\\) has a private type \\(\\theta \\in \\Theta_i\\), where \\(\\Theta_i\\) is the set of possible types for player \\(p_i\\).\n\u2022 Actions: In each round t, player \\(p_i\\) chooses an action \\(a^t_i \\in A_i\\), where \\(A_i\\) is the set of available actions for player \\(p_i\\).\n\u2022 Observations: After all players choose their actions, each player \\(p_i\\) receives an observation \\(o^t_i \\in O_i\\), where \\(O_i\\) is the set of possible observations for \\(p_i\\). The observation \\(o^t_i\\) of depends on the joint actions \\(a^t = (a^t_1, a^t_2,..., a^t_n)\\) and possibly other public or private signals.\n\u2022 Beliefs: Each player \\(p_i\\) maintains a belief \\(\\sigma^{t}_i(\\theta_{-i} | h^t)\\), where \\(\\theta_{-i}\\) denotes the types of all other players, and \\(h^t\\) is the history of observations and actions up to round t. The belief of is updated after each round based on Bayes' rule.\n\u2022 Strategies: A strategy for player \\(p_i\\) is a function \\(s_i: H_i \\times O_i \\rightarrow A_i\\), where \\(H_i\\) is the set of possible histories for player \\(p_i\\). The strategy \\(s_i(h, \\theta_i)\\) dictates the action \\(a^t_i\\) a player \\(p_i\\) should take given their history \\(h^t\\) and type \\(\\theta_i\\).\n\u2022 Payoff: The utility or payoff for player \\(p_i\\) at the end of the game is given by a function \\(u_i : A \\times \\Theta \\rightarrow \\mathbb{R}\\), where \\(A = \\prod_{i=1}^{n} A_i\\) and \\(\\Theta = \\prod_{i=1}^{n} \\Theta_i\\). The utility \\(u_i(a, \\theta)\\) depends on the joint actions a and the types \\(\\theta\\) of all players.\n\u2022 Objective: Each player \\(p_i\\) aims to maximize their ex-pected utility \\(E_{\\sigma_i}[u_i(a,\\theta)]\\), where the expectation is taken over the belief distribution \\(\\sigma_i\\).\n\u2022 Game Dynamics: The game proceeds as follows:\nAt the beginning of each round t, each player \\(p_i\\) observes \\(h^t\\) and selects an action \\(a^t_i = s_i(h^t, \\theta_i)\\).\nAfter all actions \\(a^t\\) are chosen, players receive observations \\(o^t\\).\nPlayers update their beliefs \\(\\sigma^{t+1}_i(\\theta_{-i} | h^{t+1})\\) based on the new history \\(h^{t+1}\\) that includes \\(o^t\\) and \\(a^t\\).\nThe game continues for a fixed number of rounds T, or until a stopping condition is met.\nReasoning Task\nIn incomplete information games, players typically need to reason to get more potential information based on the observations in order to make better decisions. Especially in mul-tiplayer games, players need to observe and analyze other players' behavior and historical data in real time. Typically, there is a large amount of data to be processed, and some of it"}, {"title": "Methodology", "content": "In the context of multiplayer games with incomplete information, players often need to make decisions based on partial knowledge about the game state and the intentions of other players. LLMs can generate actions directly from historical interactions, but this approach often leads to hallucinations, where the model produces outputs that are inconsistent or not grounded in the actual game history. Additionally, the reasoning process of LLMs in this context is typically opaque and difficult to trace.\nTo enhance the effectiveness of LLM reasoning, especially in environments where trust and strategic interactions are crucial, it is essential to retrieve the most relevant evidence from historical data. This motivated us to develop a framework where the information observed by agents is structured into a graph-based evidence base. By maintaining this evidence graph, we can retrieve related evidence chains, augment LLM reasoning, and mitigate the issues of hallucination and opacity. This methodology forms the foundation of our proposed GRATR system, which dynamically evaluates trustworthiness among players by leveraging the graph structure to organize and retrieve evidence, ultimately improving decision-making in multiplayer games.\nInitialization of the Evidence Graph\nA directed graph G is initialized as a dynamic evidence base, storing the history of observations and actions h up to round t. This graph G serves as a foundation for the LLM's reasoning process, allowing for the retrieval and utilization of real-time evidence. The graph consists of two fundamental components, i.e., nodes and edges.\nNodes Each node in the graph G represents a player \\(p_j\\) and stores three parameters:\n\u2022 Trustworthiness \\(T^t_i(p_j)\\): The perceived trustworthiness of player \\(p_j\\) by player \\(p_i\\) at time t.\n\u2022 Role Classification \\(R^t_i(p_j)\\): The classification of \\(p_j\\)'s role as perceived by \\(p_i\\), determined by \\(T^t_i(p_j)\\), which is as Table 1, where \\(\\epsilon\\) is the tolerance threshold for neutral judgment. Player \\(p_i\\) has an inherent trustworthiness of 1 toward themselves, i.e., \\(T^t_i(p_i) = 1\\).\n\u2022 Historical Observations \\(h^t(p_j)\\): The history of observations and actions gathered by player \\(p_i\\) about player \\(p_j\\) up to round t."}, {"title": "Update of the Evidence Graph", "content": "When player \\(p_i\\) receives a new observation \\(o^t(p_j)\\) following an action by player \\(p_j\\), the evidence graph \\(G^t\\) must be updated to incorporate this new information. This ensures that Gaccurately represents the current state of trustworthiness among the players at time t.\nPlayer \\(p_i\\) uses the LLM to extract evidence items \\(e^t(p_j, p_k)\\) and their corresponding weights \\(w^t(p_j, p_k)\\) from the observation \\(o^t(p_j)\\) (the related prompts used for LLM interactions are provided in Appendix I). For each directed edge \\(E^t(p_j, p_k)\\) in the graph, the evidence list \\(h^{t+1}(p_j, p_k)\\) associated with the edge \\(E^t(p_j, p_k)\\) is updated by adding the new intention \\(e^t(p_j, p_k)\\):\n\\(h^{t+1}(p_j, p_k) = h^t(p_j, p_k) \\cup \\{e^t(p_j, p_k)\\}\\).\n(1)\nThe sign of \\(w^t(p_j, p_k)\\) indicates the nature of \\(p_j\\)'s intention towards \\(p_k\\): negative for hostility and positive for support, with \\(|w^t(p_j, p_k)|\\) reflecting its strength. Noted that the evidence list \\(h^t(p_j, p_k)\\) is updated with the new observation, and the edge weight \\(T^t(p_j, p_k)\\) is adjusted accordingly during retrieval to maintain an accurate representation of trustworthiness.\nThen, player \\(p_i\\) use the LLM to process this observation and update the trustworthiness of \\(p_k\\). The update depends on several factors: the perceived trustworthiness of"}, {"title": "Graph Retrieval Augmented Reasoning", "content": "During a player \\(p_i\\)'s turn, particularly when deciding on an action involving player \\(p_o\\), the reasoning process is augmented by retrieving and leveraging relevant evidence from the evidence graph G. This graph-based retrieval augments the player's trustworthiness assessment by incorporating historical evidence into the reasoning process. The retrieval process is divided into three key phases: evidence merging, forward retrieval, and backward update, and reasoning.\nEvidence Merging In this phase, the objective is to aggregate and evaluate the various pieces of evidence collected by player \\(p_i\\) over time, specifically related to the interactions between players \\(p_j\\) and \\(p_k\\). Assume that player \\(p_i\\) has n pieces of evidence \\(e^{t_k}(p_j, p_k)\\) towards player \\(p_k\\) in the evidence list \\(h^t(p_j, p_k)\\) associated with the directed edge \\(E^t(p_j, p_k)\\). The evidence is sorted in chronological order, with each piece of evidence having an associated weight \\(w^{t_k}(p_j, p_k)\\) and a temporal importance factor \\( \\rho \\). The updated edge weight \\(T^{t+1}(p_j, p_k)\\) is computed as follows:\n\\(T^{t+1}(p_j, p_k) = tanh(\\sum_{k=1}^n \\rho^{n-k} \\cdot w^{t_k}(p_j, p_k))\n(4)\nwhere the impact of evidence decreases over time, with more recent evidence having greater influence. The tanh function is used to constrain the edge weight \\(T^{t+1}(p_j, p_k)\\) within the interval [-1,1], providing a bounded measure of the trustworthiness between players."}, {"title": "Forward Retrieval", "content": "Given that player \\(p_i\\) holds a trustworthiness value \\(T^t_i(p_1)\\) towards player \\(p_1\\), if there exists a evidence chain \\(C_n: p_o\\rightarrow p_{o-1} \\rightarrow \\cdots \\rightarrow p_1\\), the value \\(V_{C_n}\\) of this evidence chain and the cumulative trustworthiness update \\(u^t(p_o)\\) towards player \\(p_o\\) are computed as follows:\n\\(V_{C_n} = \\sum_{k=1}^{o-1} T^t_i(p_{k+1}) \\cdot T^t_i(p_{k+1}, p_k)\\)   (5)\n\\(u^t(p_o) = T^t_i(p_1) \\prod_{k=1}^{o-1} T^t_i(p_{k+1}, p_k)\\)   (6)\nThe uncertainty associated with the chain \\(C_n\\) is defined by:\n\\(H(C_n) = -u^t(p_o) \\log_2 u^t(p_o)\\)   (7)\nFor the player \\(p_o\\) with m related evidence chains \\(C_1, C_2, ..., C_m\\), the updated trustworthiness \\(T^{t+1}_i(p_o)\\) is given by:\n\\(T^{t+1}_i(p_o) = \\frac{\\sum_{n=1}^m (V_{C_n} - H(C_n)) \\cdot u^t(p_o)}{\\sum_{n=1}^m (V_{C_n} - H(C_n))}\\) (8)\nwhere the trustworthiness update is a weighted sum of the relevant evidence chains, where each chain's weight is determined by its value and associated uncertainty.\nBackward Update Once \\(T^t_i(p_o)\\) is updated, the edge weights associated with the relevant evidence chains need to be updated in reverse:\n\\(T^{t+1}_i(p_o, p_{o-1}) = \\gamma \\frac{T^t_i(p_o)}{T^t_i(p_{o-1})} + T^t_i(p_{o-1})\\)  (9)\nHere, \\( \\gamma \\) represents the learning rate for the backward update, and \\(p_{j-1}\\) is the preceding player in the evidence chain Ci\nReasoning After updating the trustworthiness of player \\(p_i\\) towards \\(p_o\\), A summary and reasoning is made based on the trustworthiness of player \\(p_o\\) and the relevant evidence chains retrieved. Specifically, the trustworthiness of player \\(p_i\\) towards \\(p_o\\) and the evidence chains are combined into a prompt that is sent to LLM, which ultimately returns the summary and reasoning of the player \\(p_o\\). The prompt used are shown in Appendix 3."}, {"title": "Setup", "content": "We implemented our GRATR method using the classic multi-player game \"Werewolf\". We used the framework of the Werewolf environment implemented by Ref (local to global: a graph rag approach to query-focused summarization 2024(@). We set the number of players to 8 in the Werewolf game, with three leaders: the witch, the guard, and the seer, three werewolves, and two villagers. The history message window size K is set to 15. We use the gpt-3.5-turbo model as the backend LLM, and its temperature is set to 0.3.\nWe added Native RAG and Rerank RAG to the baseline LLM which help to enhance reasoning by retrieve history messages to generate two new compared algorithms. We tested GRATR with baseline LLM and LLM enhanced with Native RAG and Rerank RAG on the Werewolf framework and each algorithm was compared in 50 games.\nIn each game, there are four players on each algorithm, three of whom are randomly assigned to the leader and werewolf sides and the remaining one is assigned to the village side. When one side wins, the algorithm corresponding to that side is considered to have won."}, {"title": "Experiment Results", "content": "In group 1, the GRATR algorithm significantly outperforms the baseline LLMs across all evaluated metrics. Specifically, the total win rate achieved by GRATR is 76.0%, markedly higher than the 24.0% attained by the baseline LLM. Moreover, GRATR demonstrates a substantial advantage in the Werewolf's win rate, achieving 72.4% compared to 27.6% for the baseline LLM. Additionally, the GRATR algorithm leads to a notably higher win rate for the leaders (81.0%), in contrast to the 19.0% observed with the baseline LLM. These results highlight the superior effectiveness of the GRATR algorithm in optimizing game outcomes across different roles.\nIn group 2, the win rate of the GRATR algorithm is compared against baseline LLM and LLM enhanced with Native RAG and Rerank RAG in the Werewolf game. The GRATR algorithm demonstrates a clear advantage, achieving a total win rate of 66.5%, which is nearly double that of the LLM enchanced with Native RAG approach at 33.5%. Furthermore, the GRATR algorithm significantly boosts the win rate of the Werewolf identity to 72.0%, compared to the 28.0% recorded for the other. The Leaders' (witch, guard, seer) win rate under GRATR also reflects this trend, standing at 72.0% as opposed to the 28.0% seen with LLM enchanced with Native RAG. This comparison illustrates the effectiveness of the GRATR algorithm in consistently enhancing the win rates across different player roles within the game.\nGroup 3 provides a comparison between the GRATR algorithm and the combination of LLM enhanced Rerank RAG in the Werewolf game. The results clearly indicate the superior performance of the GRATR algorithm across all metrics. Specifically, GRATR achieves a total win rate of 83.7%, significantly higher than the 16.3% recorded by the LLM enhanced Rerank RAG. Similarly, GRATR's Werewolf win rate is 88.5%, while the LLM enhanced Rerank RAG manages only 11.5%. The disparity is most pronounced in the Leaders' win rate, where GRATR reaches a perfect 100%, compared to the 0.0% of the LLM enhanced Rerank RAG. These data suggest that, although Rerank RAG attempts to account for the correlations among evi-"}, {"title": "Game performance", "content": "In this section, we will evaluate and compare the performance of GRATR and other algorithms in Werewolf games. Specifically, the winning party will receive 5 victory point, the vote weight of the werewolf is 0.5 point, the vote weight of the villager is 1 point, and the vote weight of the leader is 1.5 point. During the day, each round of voting correctly votes for the enemy to get the vote point, and votes for teammates to deduct the corresponding vote weight points. The performance score calculation table is as follows:"}, {"title": "Conclusion", "content": "This paper has presented a novel framework called Graph Retrieval Augmented Trustworthiness Reasoning (GRATR). GRATR employs a dynamic trustworthiness relationship graph, updated in real-time with new evidence, to enhance the accuracy and reliability of trustworthiness assessments among players. The framework consists of two main phases: During the player observation phase, evidence is collected to update the nodes and edges of the graph in real-time. During the player's turn, the relevant evidence chain and the trustworthiness of the player's action object are retrieved to improve reasoning and decision-making. The effectiveness of GRATR is demonstrated through experiments in the multiplayer game 'Werewolf', where it outperforms existing methods in key metrics such as game winning rate, overall performance, and reasoning ability. It also effectively solves the illusion of large language models, and enables the traceability and visualization of the reasoning process through time evidence and evidence chains."}, {"title": "Appendix", "content": "Reasoning ability\nTo verify the effectiveness of our algorithm in trustworthiness reasoning, we compared our algorithm with the original algorithm, the original algorithm assisted by Native RAG, and the original algorithm assisted by Rerank RAG in trustworthiness reasoning test on Werewolf.\nSpecifically, we counted and compared the ratio of each algorithm that successfully reasoned about other identities under different identities.\nIn Fig. 4a, \"WE\" means werewolf, \"WI\" means witch, \"GU\" means guard, \"SE\" means seer, \"VI\" means villager. The GRATR algorithm demonstrates a marked improvement in reasoning capabilities over the original algorithm across all roles within the Werewolf game. GRATR achieves an overall success ratio of 0.752, significantly surpassing the original algorithm's 0.450. Notably, GRATR exhibits exceptional performance in the Werewolf role, with a success ratio of 0.846, and in the Seer role, with a success ratio of 0.770, indicating a superior ability to both conceal its identity and accurately infer the identities of other players. Even in the Witch role, where GRATR's success ratio is relatively lower at 0.609, it still substantially outperforms the original algorithm. These findings underscore the enhanced strategic inference capabilities of GRATR across diverse roles in the game.\nIn Fig. 4b, compared to the original algorithm supported by Native RAG, the GRATR algorithm shows a significant improvement in reasoning success in all roles. Specifically, in the werewolf role, GRATR achieves a success rate of 0.868, which is significantly higher than the 0.284 achieved by the original algorithm. This indicates GRATR's superior ability to minimize false inferences, often referred to as 'phantoms', thereby allowing more accurate inferences to be made about other identities. For the villager role, GRATR achieved a success rate of 0.661, compared to 0.292 for the original algorithm. Despite the inherent limitations of the villager role and the lack of special abilities, GRATR's advanced reasoning capabilities allow it to draw more accurate conclusions from minimal information, demonstrating the algorithm's effectiveness in making the most of limited data.\nIn Fig. 4c, the GRATR algorithm shows a significant advantage over the original algorithm augmented with Rerank RAG in reasoning about other identities. GRATR achieves an overall success rate of 0.751, significantly higher than the 0.295 achieved by the original algorithm. This improved performance is evident across all roles: in the werewolf role, GRATR's success rate is 0.868, significantly higher than the original algorithm's 0.297. In the witch role, GRATR records a success rate of 0.686, compared to the original's 0.321, indicating superior effectiveness in exploiting the witch's unique abilities. Similarly, GRATR has a success rate of 0.669 in the Guardian role and 0.771 in the Seer role, both of which are significantly higher than the original algorithm's corresponding success rates of 0.262 and 0.280. In addition, GRATR's success rate of 0.661 in the villager role exceeds the original's 0.321, demonstrating its effec-"}]}