{"title": "Graph Retrieval Augmented Trustworthiness Reasoning", "authors": ["Ying Zhu", "Shengchang Li", "Ziqian Kong", "Peilan Xu"], "abstract": "Trustworthiness reasoning is crucial in multiplayer games with incomplete information, enabling agents to identify potential allies and adversaries, thereby enhancing reasoning and decision-making processes. Traditional approaches relying on pre-trained models necessitate extensive domain-specific data and considerable reward feedback, with their lack of real-time adaptability hindering their effectiveness in dynamic environments. In this paper, we introduce the Graph Retrieval Augmented Reasoning (GRATR) framework, leveraging the Retrieval-Augmented Generation (RAG) technique to bolster trustworthiness reasoning in agents. GRATR constructs a dynamic trustworthiness graph, updating it in real-time with evidential information, and retrieves relevant trust data to augment the reasoning capabilities of Large Language Models (LLMs). We validate our approach through experiments on the multiplayer game \"Werewolf,\" comparing GRATR against baseline LLM and LLM enhanced with Native RAG and Rerank RAG. Our results demonstrate that GRATR surpasses the baseline methods by over 30% in winning rate, with superior reasoning performance. Moreover, GRATR effectively mitigates LLM hallucinations, such as identity and objective amnesia, and crucially, it renders the reasoning process more transparent and traceable through the use of the trustworthiness graph.", "sections": [{"title": "Introduction", "content": "In multiplayer games with incomplete information, reasoning is crucial for assessing the trustworthiness of players who may conceal their intentions based on their actions, dialogue, and other observable information. Autonomous agents must analyze and evaluate the reliability of available information sources to determine player trust and cooperation (Fig. 1). Currently, key methods supporting such reasoning include symbolic reasoning ( (@; coherence and consistency in neural sequence models with dual system 2021(@) and evidential theory (geometry problem solving with formal language and symbolic reasoning 2021(@), Bayesian reasoning (hierarchy hesitant fuzzy linguistic entropy-based TODIM approach using evidential theory 2020(@; probability to consilience: How explanatory values implement Bayesian reasoning 2021(@; implementations of Bayesian inference 2021(@), and reinforcement learning (RL) (a bayesian multi-hop reasoning framework for knowledge graph reasoning 2021(@; like human: Hi-erarchical reinforcement learning for knowledge graph reasoning 2020(@; attention-based deep reinforcement learning framework for knowledge graph reasoning 2021(@).\nLarge language model (LLM) is a promising approach for trustworthiness reasoning in such games due to their robust natural language understanding and generation capabilities. They are well-suited for interpreting complex dialogues, inferring different intentions, and detecting hidden motives from context. However, LLMs often face potential risks such as hallucinations and knowledge obsolescence, since their reasoning is based on pre-existing training data. To mitigate these issues, methods such as supervised fine-tuning (SFT), and RL, have been proposed to improve reasoning performance, they often require substantial historical data and reward feedback, which may be scarce in real-world scenarios.\nRetrieval augmented generation (RAG)(aware knowl-"}, {"title": "Preliminaries", "content": "Multiplayer Game with Incomplete Information\nIn a multi-player game with incomplete information, the game can be described by the following components:\nPlayers: P = {P1,P2,..., Pn}, where pi represents the i-th player.\nTypes: Each player pi has a private type \\theta \u0395 \u0398\u2081, where O is the set of possible types for player pi.\nActions: In each round t, player pi chooses an action a\u2208 Ai, where A\u00bf is the set of available actions for player pi.\nObservations: After all players choose their actions, each player pi receives an observation of \u2208 O\u017c, where Oi is the set of possible observations for pi. The observation of depends on the joint actions at = (a1, a2,..., an) and possibly other public or private signals.\nBeliefs: Each player pi maintains a belief (\u03b8ih), where 0_i denotes the types of all other players, and h is the history of observations and actions up to round t. The belief of is updated after each round based on Bayes' rule.\nStrategies: A strategy for player pi is a function si: Hi\u00d7 Oi \u2192 Ai, where Hi is the set of possible histories for player pr. The strategy si(h,0i) dictates the action a player pi should take given their history h and type 0.\nPayoff: The utility or payoff for player pi at the end of the game is given by a function ui : A \u00d7 \u0472 \u2192 R, where A = \u03a0-1 A\u2081 and \u0398 = \u03a0=1 \u0398\u2081. The utility u\u017c(a, 0) depends on the joint actions a and the types & of all players.\nObjective: Each player pi aims to maximize their ex-pected utility \u0395\u03c3\u2081[u\u2081(a,0)], where the expectation is taken over the belief distribution \u03c3\u03c4.\nGame Dynamics: The game proceeds as follows:\nAt the beginning of each round t, each player pi ob-serves h and selects an action a = si(h, 0i).\nAfter all actions at are chosen, players receive obser-vations o.\n\u03c3\n-1\nPlayers update their beliefs o+1(0\u2212i | ht+1) based on the new history h+1 that includes of and a\nThe game continues for a fixed number of rounds T, or until a stopping condition is met.\nReasoning Task\nIn incomplete information games, players typically need to reason to get more potential information based on the obser-vations in order to make better decisions. Especially in mul-tiplayer games, players need to observe and analyze other players' behavior and historical data in real time. Typically, there is a large amount of data to be processed, and some of it"}, {"title": "Methodology", "content": "In the context of multiplayer games with incomplete infor-mation, players often need to make decisions based on par-tial knowledge about the game state and the intentions of other players. LLMs can generate actions directly from his-torical interactions, but this approach often leads to halluci-nations, where the model produces outputs that are inconsis-tent or not grounded in the actual game history. Additionally, the reasoning process of LLMs in this context is typically opaque and difficult to trace.\nTo enhance the effectiveness of LLM reasoning, espe-cially in environments where trust and strategic interactions are crucial, it is essential to retrieve the most relevant ev-idence from historical data. This motivated us to develop a framework where the information observed by agents is structured into a graph-based evidence base. By maintaining this evidence graph, we can retrieve related evidence chains, augment LLM reasoning, and mitigate the issues of halluci-nation and opacity. This methodology forms the foundation of our proposed GRATR system, which dynamically evalu-ates trustworthiness among players by leveraging the graph structure to organize and retrieve evidence, ultimately im-proving decision-making in multiplayer games.\nInitialization of the Evidence Graph\nA directed graph G is initialized as a dynamic evidence base, storing the history of observations and actions h up to round t. This graph G serves as a foundation for the LLM's reasoning process, allowing for the retrieval and utilization of real-time evidence. The graph consists of two fundamen-tal components, i.e., nodes and edges.\nNodes Each node in the graph G represents a player pj and stores three parameters:\nTrustworthiness T\u2021 (pj): The perceived trustworthiness of player pj by player pi at time t.\nRole Classification R(pj): The classification of pj's role as perceived by pi, determined by T (pj), which is as Table 1, where e is the tolerance threshold for neutral judgment. Player pi has an inherent trustworthiness of 1 toward themselves, i.e., Tt (pi) = 1.\nHistorical Observations h(pj): The history of obser-vations and actions gathered by player pi about player pj up to round t."}, {"title": "Update of the Evidence Graph", "content": "When player pi receives a new observation of (pj) following an action by player pj, the evidence graph G must be up-dated to incorporate this new information. This ensures that Gaccurately represents the current state of trustworthiness among the players at time t.\nPlayer pi uses the LLM to extract evidence items e(pj, Pk) and their corresponding weights w(pj, pk) from the observation of (pj) (the related prompts used for LLM interactions are provided in Appendix I). For each directed edge E(pj, Pk) in the graph, the evidence list ht+1(pj, Pk) associated with the edge E (pj, Pk) is updated by adding the new intention e(Pj, Pk):\nh+1(Pj, Pk) = h(Pj, Pk) \u222a {e(Pj, Pk)}. (1)\nThe sign of w (pj, Pk) indicates the nature of pj's inten-tion towards pk: negative for hostility and positive for sup-port, with w (pj, pk)| reflecting its strength. Noted that the evidence list h(pj, pk) is updated with the new observation, and the edge weight (pj, pk) is adjusted accordingly dur-ing retrieval to maintain an accurate representation of trust-worthiness.\nThen, player pi use the LLM to process this observa-tion and update the trustworthiness of pk. The update de-pends on several factors: the perceived trustworthiness of"}, {"title": "Graph Retrieval Augmented Reasoning", "content": "During a player pi's turn, particularly when deciding on an action involving player po, the reasoning process is aug-mented by retrieving and leveraging relevant evidence from the evidence graph G. This graph-based retrieval augments the player's trustworthiness assessment by incorporating historical evidence into the reasoning process. The retrieval process is divided into three key phases: evidence merging, forward retrieval, and backward update, and reasoning.\nEvidence Merging In this phase, the objective is to aggre-gate and evaluate the various pieces of evidence collected by player pi over time, specifically related to the interac-tions between players pj and pk. Assume that player pi has n pieces of evidence e (pj, Pk) towards player pk in the evidence list ht (pj, pk) associated with the directed edge E(pj, Pk). The evidence is sorted in chronological order, with each piece of evidence having an associated weight w(pj, Pk) and a temporal importance factor p. The updated edge weight +1 (pj, pk) is computed as follows:\n\\tau_{i}^{t+1}(p_{j}, p_{k})=\\tanh \\left(\\sum_{k=1}^{n} \\rho^{n-k} \\cdot w^{t}(p_{j}p_{k})\\right)\n(4)\nwhere the impact of evidence decreases over time, with more recent evidence having greater influence. The tanh function is used to constrain the edge weight 7+1 (pj, Pk) within the interval [-1,1], providing a bounded measure of the trustworthiness between players."}, {"title": "Setup", "content": "We implemented our GRATR method using the classic multi-player game \"Werewolf\". We used the framework of the Werewolf environment implemented by Ref (local to global: A graph rag approach to query-focused summariza-tion 2024(@). We set the number of players to 8 in the Were-wolf game, with three leaders: the witch, the guard, and the seer, three werewolves, and two villagers. The history mes-sage window size K is set to 15. We use the gpt-3.5-turbo model as the backend LLM, and its temperature is set to 0.3. We added Native RAG and Rerank RAG to the baseline LLM which help to enhance reasoning by retrieve history messages to generate two new compared algorithms. We tested GRATR with baseline LLM and LLM enhanced with Native RAG and Rerank RAG on the Werewolf framework and each algorithm was compared in 50 games.\nIn each game, there are four players on each algorithm, three of whom are randomly assigned to the leader and were-wolf sides and the remaining one is assigned to the village side. When one side wins, the algorithm corresponding to that side is considered to have won."}, {"title": "Experiment Results", "content": "different roles.\nIn group 2, the win rate of the GRATR algorithm is com-pared against baseline LLM and LLM enhanced with Native RAG and Rerank RAG in the Werewolf game. The GRATR algorithm demonstrates a clear advantage, achieving a total win rate of 66.5%, which is nearly double that of the LLM enchanced with Native RAG approach at 33.5%. Further-more, the GRATR algorithm significantly boosts the win rate of the Werewolf identity to 72.0%, compared to the 28.0% recorded for the other. The Leaders' (witch, guard, seer) win rate under GRATR also reflects this trend, standing at 72.0% as opposed to the 28.0% seen with LLM enchanced with Native RAG. This comparison illustrates the effectiveness of the GRATR algorithm in consistently enhancing the win rates across different player roles within the game.\nGroup 3 provides a comparison between the GRATR al-gorithm and the combination of LLM enhanced Rerank RAG in the Werewolf game. The results clearly indicate the superior performance of the GRATR algorithm across all metrics. Specifically, GRATR achieves a total win rate of 83.7%, significantly higher than the 16.3% recorded by the LLM enhanced Rerank RAG. Similarly, GRATR's Were-wolf win rate is 88.5%, while the LLM enhanced Rerank RAG manages only 11.5%. The disparity is most pro-nounced in the Leaders' win rate, where GRATR reaches a perfect 100%, compared to the 0.0% of the LLM enhanced Rerank RAG. These data suggest that, although Rerank RAG attempts to account for the correlations among evi-"}, {"title": "Conclusion", "content": "This paper has presented a novel framework called Graph Retrieval Augmented Trustworthiness Reasoning (GRATR). GRATR employs a dynamic trustworthiness relationship graph, updated in real-time with new evidence, to enhance the accuracy and reliability of trustworthiness assessments among players. The framework consists of two main phases: During the player observation phase, evidence is collected to update the nodes and edges of the graph in real-time. Dur-ing the player's turn, the relevant evidence chain and the trustworthiness of the player's action object are retrieved to improve reasoning and decision-making. The effective-ness of GRATR is demonstrated through experiments in the multiplayer game 'Werewolf', where it outperforms existing methods in key metrics such as game winning rate, overall performance, and reasoning ability. It also effectively solves the illusion of large language models, and enables the trace-ability and visualization of the reasoning process through time evidence and evidence chains."}, {"title": "Appendix", "content": "Reasoning ability\nTo verify the effectiveness of our algorithm in trustworthi-ness reasoning, we compared our algorithm with the original algorithm, the original algorithm assisted by Native RAG, and the original algorithm assisted by Rerank RAG in trust-worthiness reasoning test on Werewolf.\nSpecifically, we counted and compared the ratio of each algorithm that successfully reasoned about other identities under different identities.\nIn Fig. 4a, \"WE\" means werewolf, \"WI\" means witch, \"GU\" means guard, \"SE\" means seer, \"VI\" means villager. The GRATR algorithm demonstrates a marked improvement in reasoning capabilities over the original algorithm across all roles within the Werewolf game. GRATR achieves an overall success ratio of 0.752, significantly surpassing the original algorithm's 0.450. Notably, GRATR exhibits excep-tional performance in the Werewolf role, with a success ra-tio of 0.846, and in the Seer role, with a success ratio of 0.770, indicating a superior ability to both conceal its iden-tity and accurately infer the identities of other players. Even in the Witch role, where GRATR's success ratio is relatively lower at 0.609, it still substantially outperforms the original algorithm. These findings underscore the enhanced strategic inference capabilities of GRATR across diverse roles in the game.\nIn Fig. 4b, compared to the original algorithm supported by Native RAG, the GRATR algorithm shows a significant improvement in reasoning success in all roles. Specifically, in the werewolf role, GRATR achieves a success rate of 0.868, which is significantly higher than the 0.284 achieved by the original algorithm. This indicates GRATR's supe-rior ability to minimize false inferences, often referred to as 'phantoms', thereby allowing more accurate inferences to be made about other identities. For the villager role, GRATR achieved a success rate of 0.661, compared to 0.292 for the original algorithm. Despite the inherent limitations of the villager role and the lack of special abilities, GRATR's ad-vanced reasoning capabilities allow it to draw more accu-rate conclusions from minimal information, demonstrating the algorithm's effectiveness in making the most of limited data.\nIn Fig. 4c, the GRATR algorithm shows a significant ad-vantage over the original algorithm augmented with Rerank RAG in reasoning about other identities. GRATR achieves an overall success rate of 0.751, significantly higher than the 0.295 achieved by the original algorithm. This improved performance is evident across all roles: in the werewolf role, GRATR's success rate is 0.868, significantly higher than the original algorithm's 0.297. In the witch role, GRATR records a success rate of 0.686, compared to the original's 0.321, indicating superior effectiveness in exploiting the witch's unique abilities. Similarly, GRATR has a success rate of 0.669 in the Guardian role and 0.771 in the Seer role, both of which are significantly higher than the original al-gorithm's corresponding success rates of 0.262 and 0.280. In addition, GRATR's success rate of 0.661 in the villager role exceeds the original's 0.321, demonstrating its effec-"}]}