{"title": "Bridging the Evaluation Gap: Leveraging Large Language Models for Topic Model Evaluation", "authors": ["Zhiyin Tan", "Jennifer D'Souza"], "abstract": "This study presents a framework for automated evaluation of dynamically evolving topic taxonomies in scientific literature using Large Language Models (LLMs). In digital library systems, topic modeling plays a crucial role in efficiently organizing and retrieving scholarly content, guiding researchers through complex knowledge landscapes. As research domains proliferate and shift, traditional human-centric and static evaluation methods struggle to maintain relevance. The proposed approach harnesses LLMs to measure key quality dimensions-such as coherence, repetitiveness, diversity, and topic-document alignment-without heavy reliance on expert annotators or narrow statistical metrics. Tailored prompts guide LLM assessments, ensuring consistent and interpretable evaluations across various datasets and modeling techniques. Experiments on benchmark corpora demonstrate the method's robustness, scalability, and adaptability, underscoring its value as a more holistic and dynamic alternative to conventional evaluation strategies.", "sections": [{"title": "1. Introduction", "content": "Topic taxonomies of Science have been traditionally used to simplify literature search, to study the structure and dynamics of scientific disciplines, or to facilitate bibliometric research evaluations [1]. These taxonomies, often hierarchical and multidisciplinary, provide a framework for categorizing knowledge and can significantly influence the dissemination and evolution of scientific information [2]. They play a crucial role in the organization of academic databases, directly impact the efficiency of information retrieval systems, and serve as essential tools for structuring and navigating vast repositories within digital library systems. As scientific output continues to grow exponentially, the need for effective and dynamically adaptable taxonomic systems [3, 4] becomes increasingly important, not just for academic researchers but also for policymakers and funding agencies aiming to identify and support pivotal research areas [5].\nAs we shift towards dynamically updatable taxonomies to manage the growing volumes of scientific literature, developing robust evaluation methods is essential to ensure their effectiveness [3]. These methods must verify that taxonomies adapt to the rapid evolution of scientific domains while consistently producing meaningful and coherent topics. Evaluating these dynamic systems involves assessing their accuracy in reflecting current research trends and their capacity to interlink related disciplines seamlessly [6]. This process is crucial for maintaining the integrity and utility of taxonomies in facilitating efficient research discovery and supporting informed decision-making in scientific policy and funding strategies. Human evaluation is laborious and time-consuming; thus, we also need automated evaluation systems to efficiently manage and validate these complex, dynamic structures [7]."}, {"title": "2. Related Work", "content": "Building on the premise of automated evaluation systems, Large Language Models (LLMs) [8, 9] are well-suited to function as evaluators of dynamic taxonomies due to their advanced natural language understanding and generation capabilities [10]. Trained on extensive corpora, these models excel in discerning linguistic patterns and semantic relationships within complex datasets, making them ideal for assessing the coherence and relevance of topics generated by taxonomies. Moreover, LLMs offer a scalable, consistent, and context-sensitive approach to evaluation, overcoming key challenges of traditional methods such as reliance on human annotators or narrowly focused statistical metrics. With the ability to simulate nuanced human-like reasoning, LLMs can evaluate multiple dimensions of topic quality-such as coherence, repetitiveness, diversity, and topic-document alignment-while providing detailed, interpretable feedback.\nIn this work, we propose a novel framework that leverages LLMs as evaluators for topic model outputs, addressing key limitations in existing evaluation methodologies. The contributions of this work are threefold. First, we introduce a comprehensive set of metrics-coherence, repetitiveness, diversity, and topic-document alignment-that collectively capture multiple facets of topic model quality. Second, we design and implement tailored LLM prompts for each metric, ensuring consistency, interpretability, and adaptability across different datasets and topic modeling techniques. Third, we validate the framework through extensive experiments on benchmark datasets 20 Newsgroups (20NG)\u00b9 and a subset of scholarly documents from the International System for Agricultural Science and Technology (AGRIS) 2, demonstrating its robustness and effectiveness. This study not only provides a scalable and holistic solution for topic model evaluation but also paves the way for broader applications of LLMs in addressing dynamic and context-sensitive challenges in natural language processing. The code is available here\u00b3."}, {"title": "2.1. Topic Modeling Approaches and Their Evolution", "content": "The field of topic modeling has evolved significantly, advancing from early matrix factorization techniques to modern probabilistic and neural architectures. One foundational work, Latent Semantic Indexing (LSI) [11], utilized singular value decomposition to uncover conceptual associations between words and documents through co-occurrence patterns, though it lacked a fully generative probabilistic model. Building on these concepts, Probabilistic Latent Semantic Indexing (pLSI) [12] introduced a probabilistic framework where words are generated from a mixture of topics. This probabilistic perspective paved the way for LDA [13], a seminal contribution that introduced a fully generative Bayesian model capable of inferring a corpus-wide set of latent topics and their associated per-document distributions. LDA's flexible yet tractable variational inference techniques (as well as alternative inference algorithms like Gibbs sampling, as demonstrated in [14]) solidified it as a cornerstone in topic modeling research.\nIn more recent years, researchers have embraced neural variational inference frameworks-inspired by VAE [15] -to develop neural topic models (NTMs) such as NVDM [16], GSM/GSB/RSB [17], ProdLDA/AVITM [18], and ETM [19]. These models employ continuous latent representations and deep neural networks to capture richer semantic structures and often rely on embeddings to represent words and documents. Another emerging direction leverages contextualized embeddings and large language models (LLMs) as building blocks or alternatives to traditional topic models. Methods such as clustering-based approaches [20], CombinedTM [21], and BERTopic [22] have demonstrated that directly clustering embeddings of documents and words can yield coherent, diverse topics. More recently, research efforts like [23, 24, 25] propose employing LLMs directly to generate and refine topics, showing promise for overcoming certain limitations of classical topic modeling frameworks."}, {"title": "2.2. Evaluation of Topic Models: Methods and Limitations", "content": "Topic model evaluation has shifted from basic statistical measures to methods that capture human interpretability. Early work used held-out likelihood, introduced in [13] and consistently employed by [26], measure how well a trained model can predict unseen data, although it can be computationally demanding. Similarly, log probability [27] aggregates observed words and documents likelihood, providing an intuitive gauge of model fit but potentially favoring complexity over interpretability. Perplexity normalized held-out likelihood [13, 26, 28, 27, 29, 18, 30, 31, 32, 19]. However, low perplexity does not necessarily translate to coherent or meaningful topics [33], highlighting a fundamental disconnect between statistical quality and human interpretability. To bridge this gap, human-centered evaluations like word intrusion and topic intrusion tasks [33] were introduced, where human judges identify out-of-place words or topics. Human-rated coherence, first explored in [34] and later adopted by [35, 36, 37] asking annotators score topics on an ordinal scale. Although these methods closely reflect human understanding, their reliance on manual annotation constrains scalability and efficiency. In response, automated metrics were then introduced to approximate human judgments. Topic words-based coherence metrics measure how strongly top-ranked topic words co-occur in the underlying data. Early approaches, such as coherence $CUCI$ [38] and coherence $CUMass$ [35], rely on word co-occurrence frequencies and statistics derived from the training corpus. More refined metrics, like NPMI [39], normalize mutual information coherence scores $CNPMI$ to better align with human judgments [36, 40, 18, 30, 31, 41, 42, 22, 43], while coherence $Cv$ [44] uses a variation of $CNPMI$ to compute topic coherence over a sliding window of size and adds a weight to assign more strength to more related words. Moreover, embedding-based coherence used by [45, 46, 30, 21] further improves the match to human judgement [46]. Other metrics assess different aspects of topic quality beyond coherence. Diversity metrics ensure that the discovered topics are distinctive, not redundant or overlapping. For instance, topic diversity [19] counts the proportion of unique top words across topics, while topic redundancy [47] and topic uniqueness [48] measure how frequently top words appear across multiple topics. Similarly, inverted ranked-biased overlap [21] and embedding-based diversity metrics [49, 50] compare ranked word lists or semantic distances to ensure substantial topic variety. Document-level evaluations measure how well topics capture document's content. [51] ask annotators to rate each topic's relevance to a given document. [52] vectorize documents associated with the selected topic and calculate a coherence score based on the document vectors. [53] tests whether an outlier topic can be identified given a document and a few topics. Supervised coverage-based methods [54] match model-generated topics to a fixed, human-defined topics, though these methods are resource intensive. More recently, LLM-based evaluations have emerged as a promising new paradigm. Studies [37, 55] demonstrate that large language models can simulate human reasoning, providing nuanced judgments of topic coherence, word intrusions. [56] proposes a set of metrics to quantify the agreement between keywords generated from documents using LLM and topic words generated from documents by a topic model. By leveraging LLMs' extensive world knowledge and contextual reasoning, this approach overcomes the limitations of statistical co-occurrence, embedding similarities that often fail to capture semantic quality, and avoids the resource intensity of human-centric evaluations. However, current LLM-based methods often focus on a single aspect, such as coherence. In contrast, our approach integrates multiple LLM-based metrics-including coherence, repetitiveness, diversity, and topic-document alignment\u2014into a unified framework. Rather than simply measuring co-occurrence, our framework provides interpretable evidence (e.g., flagged outlier words and identified duplicate concepts) that explains topic flaws. Following [33, 57], topic model evaluations should assess the model's practical capabilities rather than rely on legacy metrics detached from its intended usage. Our novel topic-document alignment metrics explicitly reveal discrepancies between topic words and document content, which is crucial for applications like recommendation, summarization, and classification. By integrating document-level and topic words-based assessments, our comprehensive, adversarially validated framework bridges the gap between statistical measures and human-centered evaluations, offering actionable insights to improve topic model performance."}, {"title": "3. Our Solution: Evaluation Metrics and LLM as Evaluator", "content": "In this section, we present our comprehensive evaluation framework that leverages LLMs as evaluators for topic models. We begin with topic words-based evaluation (Section 3.1), where we introduce metrics to evaluate topic coherence $Crate$ and $Coutlier$, and repetitiveness $Rrate$ and $Rduplicate$, as well include adversarial tests to validate metric robustness. Next, cross-topic evaluation (Section 3.2) focus on topic diversity $Drate$ that captures thematic distinctiveness across topics. Finally, topic-document alignment (Section 3.3) introduces novel metrics irrelevant topic words $Air-topic$ and missing themes $Amissing-theme$ to evaluate the correspondence between topic words and document content. Together, these subsections form an integrated, interpretable, and scalable framework for evaluating topic model performance."}, {"title": "3.1. Topic words-based Evaluation", "content": "We evaluate individual topics by examining two key dimensions: (1) coherence, which measures the semantic consistency of the top-ranked topic words using the metrics $Crate$ and $Coutlier$, and (2) repetitiveness, which assesses potential redundancy within the topic words using the metrics $Rrate$ and $Rduplicate$. This two-pronged evaluation enables us to quantify both the semantic integrity and the diversity of the generated topic words.\nCoherence Topic coherence measures how well the top-ranked topic words form a semantically unified theme. Inspired by [37], we first employ an coherence rating metric, $Crate$, which asks the LLM to assess the overall semantic consistency of the topic words on a 3-point scale (with 1 indicating minimal alignment and 3 indicating strong coherence). While $Crate$ yields an overall numerical score, it does not reveal which and how many specific words are responsible for any lack of coherence. To enhance interpretability, we introduce an auxiliary outlier detection metric, $Coutlier$, that explicitly identifies semantic outlier words. In this procedure, the LLM extracts candidate outliers over 5 iterations, and a word flagged in at least 3 out of 5 iterations is deemed a semantic outlier. We then count the number of outliers as the final evaluation result, and the outlier words themselves are saved for later case studies. In addition, we perform an adversarial test $AdvToutlier$ to validate the reliability of the outlier detection inspired by established word intrusion methodologies [37, 33]. A semantically unrelated term (e.g., \"Shakespeare\") is inserted into the topic list, and the LLM is expected to correctly identify the inserted term. Even if the LLM flags additional words along with the inserted term, the detection is considered successful. Each successful detection is assigned a score of 1 and each unsuccessful attempt a score of 0. The final result is calculated as the percentage of successful detections over the total number of tests."}, {"title": "Repetitiveness", "content": "While coherence focuses on thematic alignment, we introduce a repetitiveness rating metric $Rrate$ to assess whether the perceived coherence is due to redundant topic words. $Rrate$ is rating on a 3-point scale, where a rating of 1 indicates high repetitiveness with significant semantic overlap, and a rating of 3 indicates minimal repetition with diverse and distinctive words. To further elucidate these ratings, we introduce an auxiliary duplicate concept detection metric, $Rduplicate$, which explicitly identifies exact semantic repetitions in the topic word list. $Rduplicate$ is critical as it helps distinguish genuine topic coherence from inflated scores due to redundancy. For each topic word, we compute a binary indicator: if a word has at least one other conceptual repetition in the list, it is assigned a value of 1, otherwise, 0. The final $Rduplicate$ score for a topic is the sum of these indicators, reflecting the number of topic words that have at least one duplicate in the list. In addition, we perform an adversarial test $AdvTduplicate$ to validate the reliability of using the LLM for duplicate concept detection. In this test, we randomly select an anchor word from the topic word list and manually choose a conceptually identical word to serve as its duplicate. We then insert this duplicate into the topic word list. The LLM is expected to identify the inserted duplicate given the anchor word. Each successful detection is scored as 1, while an unsuccessful one is scored as 0."}, {"title": "3.2. Cross-topic Evaluation", "content": "Diversity Diversity quantifies the uniqueness among generated topics by assessing the thematic distinctiveness of their associated top words. Inspired by [50]'s word embedding-based pairwise distance, we exhaustively extract all possible pairs of topics, with each topic represented by its corresponding topic word list. For each pair, the LLM rates the thematic distinctiveness on a 3-point scale, where a rating of 1 denotes partial overlap (low diversity) and a rating of 3 denotes minimal overlap (high distinctiveness). Finally, the average of all pairwise ratings is computed to yield the overall diversity rating, $Drate$."}, {"title": "3.3. Topic-document Alignment", "content": "Document-level evaluation focuses on assessing how effectively topics capture the underlying themes of documents. Early methods relied on human annotations [51, 53] or supervised matching against curated references [54] to measure topic relevance. However, these approaches are resource-intensive and lack scalability. Recent work by [56] introduces a set of metrics that quantify the agreement between keywords generated by LLMs from documents and the topic words produced by topic models. Although these metrics capture similarity, they do not quantify the degree of mismatch between the two sets. This unaccounted discrepancy is critical for evaluating how well a topic model covers less frequent or nuanced themes, which are often key to understanding long-tail phenomena. By incorporating measures of mismatch, we can gain a more complete picture of the model's limitations and identify specific areas where the topic representation may require further improvement. Motivated by these limitations, we propose two novel LLM-based metrics for topic-document alignment: irrelevant topic words detection metric $Air-topic$ and missing themes detection metric $Amissing-theme$. These metrics leverage the contextual understanding of LLMs to assess both overrepresentation (irrelevant topic words) and underrepresentation (missing themes) in topic-document relationships, providing a more comprehensive evaluation of how well topics capture document content.\nIrrelevant Topic Words Detection Metric $Air-topic$ assesses the extent to which a topic contains words that are not relevant to the content of its associated documents. For each topic-document pair, we instruct the LLM to identify topic words that are not explicitly or implicitly related to the document. The number of extraneous words is tallied for each document and then averaged across all pairs, providing a precise measure of overrepresentation.\nMissing Themes Detection Conversely, metric $Amissing-theme$ quantifies the extent to which a topic fails to capture key themes present in the documents. For each topic-document pair, the LLM extracts significant themes from the document that are absent in the topic word list and counts these missing themes. The resulting counts are then averaged across all pairs, yielding a measure of underrepresentation."}, {"title": "4. Experimental Setup", "content": "Datasets The 20NG dataset is a widely used benchmark comprising approximately 20,000 newsgroup posts organized into 20 categories. We adopt the pre-processed version from OCTIS\u2074, which contains 11,415 training and 4,894 test documents. Known for its diverse topics, 20NG has been extensively employed in topic model evaluations [29, 17, 32, 43]. Besides, from a repository of approximately 14 million records of food and agricultural scholarly documents-we extract a subset by excluding non-English documents, those with titles shorter than five tokens or abstracts shorter than 40 tokens, and duplicate records (by DOI, and named it AGRIS. The final dataset comprises 50,067 documents (45,060 for training and 5,007 for testing). For each document, we retain the title and abstract. To support sentence-level analysis, abstracts are segmented using SaT [58], yielding 454,850 training and 50,703 test entries. This granularity allows multiple topics to be assigned to a single document, reflecting the multi-faceted nature of scholarly texts, where a single work often spans diverse thematic areas.\nDomain-Specific Stopword Removal Stopword removal is a critical preprocessing step, particularly for domain-specific data. Stopwords are frequent but low-information terms, and their removal, guided by Zipf's law [59], reduces token counts while preserving vocabulary diversity, optimizing computational efficiency without compromising semantic integrity. Generic stopword lists often overlook contextually irrelevant terms in specialized domains. For AGRIS, we employed an information-theoretic framework [60] to identify and remove domain-specific stopwords. The 20NG dataset, pre-processed by OCTIS, required no further stopword refinement."}, {"title": "4.2. Topic Models", "content": "We evaluated four topic models chosen for their methodological diversity and proven performance, spanning traditional probabilistic approaches to neural and embedding-based methods, enabling a comprehensive comparison. Their key characteristics and implementations are detailed below.\n\u2022 Latent Dirichlet Allocation (LDA) [13]: a foundational probabilistic topic model that represents documents as mixtures of topics, with each topic modeled as a distribution over words. We use the Gensim implementation5.\n\u2022 Product of Experts Latent Dirichlet Allocation (ProdLDA) [18]: a neural adaptation of LDA that leverages variational autoencoders to enhance scalability and improve topic coherence. we adopt the code provided by the TopMost toolkit.\n\u2022 CombinedTM [21]: it integrates contextual embeddings from pre-trained transformers into the LDA framework, effectively capturing semantic nuances through deep neural embeddings. We used the official implementation7.\n\u2022 BERTopic [22]: combines document embeddings with a class-based TF-IDF procedure to generate coherent and interpretable topics. For this study, we configured BERTopic with UMAP for dimensionality reduction and HDBSCAN for clustering, following its standard pipeline.\nFor each model, we conducted an extensive parameter tuning process to identify the optimal settings for two key evaluation metrics: $Cv$ [44] for topic coherence and $Dunique$ [19] for topic diversity. Once the optimal configurations were determined, we obtain the top 10 topic words and the topic-document pairs from each model on both the 20NG and AGRIS datasets with number of topic K = 50 and K = 100. Each configuration was run ten times to account for variability in probabilistic and neural-based outputs. This resulted in ten aggregated sets of results for each model and dataset, ensuring a robust and statistically sound evaluation. This rigorous evaluation protocol not only ensures a fair comparison across the diverse modeling paradigms but also provides comprehensive insights into each model's strengths and limitations in capturing thematic content across varied datasets."}, {"title": "4.3. Evaluation", "content": "Metrics We employ two widely recognized automated metrics as baselines: the coherence metric $Cv$ [44], which measures the semantic consistency of top-ranked topic words, and the diversity metric $Dunique$ [19], which quantifies the proportion of unique words across all topics. To address the limitations of automated metrics, we also use the suite of LLM-based metrics described in Section 3 for a more nuanced evaluation of topic quality. These include: (1) Coherence Metrics: coherence rating metric $Crate$ and outlier detection metric $Coutlier$. (2) Repetitiveness Metrics: repetitiveness rating metric $Rrate$ and duplicate concept detection metric $Rduplicate$. (3) Diversity Metric: diversity rating metric $Drate$. (4) Topic-document Alignment Metrics: irrelevant topic words detection metric $Air-topic$ and missing themes detection metric $Amissing-theme$. For the topic-document alignment metrics, a sample set was constructed by randomly selecting one iteration of the model's output and sampling up to 100 associated documents per topic-yielding 59,499 samples from AGRIS and 38,321 from 20NG-thus ensuring a comprehensive evaluation. These metrics harness LLMs' deep semantic understanding to provide a comprehensive, multi-dimensional evaluation of topic quality.\nLLM as Evaluators We selected three open-source LLMs as evaluators for the proposed metrics: Mistral-7B-Instruct-v0.3\u00ba (referred to as Mistral), Meta-Llama-3.1-8B-Instruct\u00b9\u2070 (referred to as Llama), and Qwen2.5-14B-Instruct\u00b9\u00b9 (referred to as Qwen). These LLMs, chosen for their diverse pretraining corpora and instruction-tuning objectives, exhibit robust semantic understanding. Their complementary strengths ensure reliable, scalable, and reproducible evaluations, while promoting transparency and facilitating replication in the research community.\nEfficiency and Scalability We evaluated the computational efficiency and scalability of our LLM-based evaluation framework. All experiments were conducted on an NVIDIA A100 GPU, with approximately 40 GB allocated for coherence, repetitiveness, and diversity evaluations, and about 70 GB for topic-document alignment. The combined evaluation (three LLMs) time for coherence, repetitiveness, and diversity ranged from 15 minutes for K = 50 to 35 minutes for K = 100, while topic-document alignment evaluation required between 2 and 3 hours. On GPUs with lower performance and memory, reducing the batch size enables smooth operation at the expense of increased processing time, while parallel computing can reduce runtime proportionally to the number of GPUs available. These results demonstrate that our framework is both computationally efficient and scalable, making it suitable for extensive evaluations of topic models."}, {"title": "5. Results and Discussion", "content": "Quantitative Results\nAdversarial Test We sampled 100 topics (each with 10 words) from four topic models applied to the 20NG and AGRIS datasets. For adversarial test of outlier detection $AdvToutlier$, the success rates on 20NG are 77% (Mistral), 81% (Llama), and 90% (Qwen), and on AGRIS, 82% (Mistral), 85% (Llama), and 93% (Qwen). For duplicate concept detection $AdvTduplicate$, success rates on 20NG are, 37% (Mistral), 81% (Llama), and 84% (Qwen), and on AGRIS, 29% (Mistral), 74% (Llama), and 81% (Qwen). These results highlight significant variability among LLM evaluators and underscore the importance of using multiple evaluators to reliably assess topic quality.\nCoherence Tables 1 and 2 indicate that, based on the coherence rating metric $Crate$, BERTopic consistently outperforms the other topic models on both datasets, which aligns with the automated metric $Cv$. With respect to the outlier detection metric $Coutlier$, for K = 50 two of the three LLMs report the fewest outliers for BERTopic, supporting its superior coherence. At K = 100, Qwen maintain its preference for BERTopic while Mistral finds that LDA and CombinedTM are comparable. In contrast, Llama's $Coutlier$ suggest that BERTopic has more outliers. These discrepancies are further analysed in Section 5.3.\nRepetitiveness To discern whether high coherence reflects genuine semantic unity or is driven by redundant topic words, we examine $Rrate$ and $Rduplicate$. A robust coherence result should be accompanied by a high $Rrate$ (indicating minimal repetition) and a low $Rduplicate$ (indicating few duplicate"}, {"title": "Irrelevant Topic Words", "content": "Table 6 presents the evaluation of the irrelevant topic words detection metric $Air-topic$, where lower counts indicate better topic-document alignment. On 20NG, LDA, ProdLDA, and CombinedTM consistently yield lower counts compared to BERTopic, indicating that their topic words are more closely aligned with document content. In AGRIS, at K = 50 two of the three LLM evaluators favor LDA for having the fewest irrelevant words, whereas at K = 100, CombinedTM achieves the lowest count, suggesting its superior ability to capture nuanced document themes-likely due to its effective integration of contextual embeddings.\nMissing Themes Table 7 reports the missing themes detection metric $Amissing-theme$, which quantifies the number of key document themes are omitted from the topic word list, with lower counts indicating better thematic coverage. For 20NG at K = 50, two out of three LLM evaluators rate BERTopic as having the lowest missing theme counts, suggesting that its topic words more comprehensively represent the document themes. At K = 100, Qwen continues to favor BERTopic, while both Mistral and Llama indicate that LDA provides the best coverage. In AGRIS, however, the differences across topic models are minimal. Overall, $Amissing-theme$ provides valuable insight into the extent to which topic models may fail to capture less frequent or nuanced themes from documents, which is vital for understanding long-tail phenomena and enhancing downstream applications.\nDivergent LLM Evaluation Patterns The evaluation results reveal distinct evaluation tendencies among the three LLMs, offering valuable insights for researchers using LLMs as evaluators. In terms of coherence, Qwen consistently flags a higher number of outliers $Coutlier$, suggesting a stricter criterion for semantic consistency, while Mistral reports lower outlier counts, indicative of a more lenient evaluation; Llama's results generally fall between these extremes. For repetitiveness, Qwen detects fewer duplicate concepts $Rduplicate$ compared to Llama, with Mistral's assessments again falling in between-demonstrating variable sensitivity to lexical redundancy across evaluators. In topic-document alignment, Qwen registers higher counts of irrelevant topic words $Air-topic$ yet lower counts of missing"}, {"title": "5.2. Visualization", "content": "Standardization of Metrics To enable fair comparisons across LLM evaluators (Mistral, Llama, Qwen), all metrics are normalized to the [0, 1] range using the following piecewise function:\n$Xnorm = \\begin{cases}\n0.5 + \\frac{X - Xmean}{Xmax - Xmin},& \\text{if higher values indicate better performance,}\\\\\n1 - (0.5 + \\frac{X - Xmean}{Xmax - Xmin}) ,& \\text{if higher values indicate poorer performance.}\n\\end{cases}$\nHere, Xnorm is the normalized score, while Xmean, Xmax, and Xmin are the mean, maximum, and minimum values of X within each evaluator group.\nVisualization and Analysis The radar plots (Figures 1 and 2) show clear discrepancies in Llama's coherence scores. A high coherence rate $Crate$ should imply fewer outliers $Coutlier$, yet Llama rates BERTopic high while flagging more outliers than other models. Similarly, Mistral gives low $Crate$ scores to CombinedTM and LDA but paradoxically finds fewer outliers in their topics."}, {"title": "5.3. Qualitative Analysis", "content": "In this section, we provide a qualitative analysis of representative examples to explore discrepancies and patterns in outlier detection, duplicate concept detection.\nOutlier Detection Discrepancies Outlier detection is a crucial aspect of evaluating topic coherence, as it identifies semantically inconsistent words in a topic's word list. Across the examples, outliers identified by the models often intersect but also reflect unique insights. Compared to the other two models, Mistral is more cautious in detecting outliers in topic words. On the contrary, Qwen is relatively more aggressive in detecting words with unclear semantic pointing from topic words and considering them as outliers.\nDuplicate Concept Detection Contradictions The extracted duplicate pairs often differ significantly among the LLMs, showcasing varying thresholds for identifying conceptual overlap. Mistral treats semantically related nouns (e.g., \"christian\" and \"church\", collective nouns where there is intersection (e.g., \"patient\" and \"adult\"), and nouns that belong to the same category (e.g. \"child\" and \"adult\") as conceptually identical. Llama treats grammatically related words (e.g., the verb \u201csearch\u201d and its potential object \u201c package\u201d), semantically opposite words (e.g., \"disease\u201d and \u201chealth \") as conceptually identical."}, {"title": "6. Conclusion", "content": "In this work, we have introduced a comprehensive framework for evaluating topic models using LLM-based metrics that complements traditional automated metrics by incorporating nuanced measures of coherence, repetitiveness, diversity, and topic-document alignment. We designed specific evaluation protocols-including adversarial tests\u2014to reveal not only the strengths and weaknesses of various topic models but also the intrinsic biases and judgment tendencies of different LLM evaluators. Our experiments on both the 20NG and AGRIS datasets demonstrate that LLMs can provide rich, context-sensitive insights into topic quality, while also highlighting evaluator-specific variations that are crucial for informed application in downstream tasks.\nThese findings illustrate the potential of our framework to expand the boundaries of topic model assessment by emphasizing both interpretability and practical application needs. Future work will focus on further refining these metrics, exploring additional LLM evaluators, and assessing how evaluator biases impact downstream tasks, thereby fostering more robust and actionable topic model assessments."}]}