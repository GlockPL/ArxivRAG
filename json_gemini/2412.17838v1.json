{"title": "Coordinated Power Smoothing Control for Wind Storage Integrated System with Physics-informed Deep Reinforcement Learning", "authors": ["Shuyi Wang", "Huan Zhao", "Yuji Cao", "Zibin Pan", "Guolong Liu", "Gaoqi Liang", "Junhua Zhao"], "abstract": "The Wind Storage Integrated System with Power Smoothing Control (PSC) has emerged as a promising solution to ensure both efficient and reliable wind energy generation. However, existing PSC strategies overlook the intricate interplay and distinct control frequencies between batteries and wind turbines, and lack consideration of wake effect and battery degradation cost. In this paper, a novel coordinated control framework with hierarchical levels is devised to address these challenges effectively, which integrates the wake model and battery degradation model. In addition, after reformulating the problem as a Markov decision process, the multi-agent reinforcement learning method is introduced to overcome the bi-level characteristic of the problem. Moreover, a Physics-informed Neural Network-assisted Multi-agent Deep Deterministic Policy Gradient (PAMA-DDPG) algorithm is proposed to incorporate the power fluctuation differential equation and expedite the learning process. The effectiveness of the proposed methodology is evaluated through simulations conducted in four distinct scenarios using WindFarmSimulator (WFSim). The results demonstrate that the proposed algorithm facilitates approximately an 11% increase in total profit and a 19% decrease in power fluctuation compared to the traditional methods, thereby addressing the dual objectives of economic efficiency and grid-connected energy reliability.", "sections": [{"title": "1. Introduction", "content": "As one of the most popular renewable energy resources, wind power holds substantial potential for meeting future global energy demands while mitigating climate change and environmental pollution. However, the intermittent nature of wind power introduces inherent variability and uncertainty when integrated into power systems. As the wind power penetration level increases, the secure and reliable operation of power systems becomes a significant challenge [1]. In practice, the grid usually requires the active power fluctuation from wind farms to be confined to a specific value within a one-minute time window [2]. Therefore, Wind Power smoothing control (PSC) has emerged as a potential solution. Previous research has established two major categories of Power Smoothing Control for wind farms, including regulation control of wind turbines and indirect power control by Battery Energy Storage System (BESS). The former approach typically involves pitch angle control [3], rotor inertia control [4], and Direct Current (DC)-link voltage control [5], which require a different operation from maximum power point tracking, causing inefficiency and potential damages [6].\nOn the contrary, with a stronger capability of power smoothing, the BESS-based PSC coordinates the active power from BESS and wind turbine [7], providing rapid response to power fluctuation with high operability and little power loss. Recognizing the benefits of such Wind Storage Integrated Systems (WSIS) [8], incentive policies have been introduced to mandate the installation of BESSs from 10% to 30% of wind farms' installed capacity. WSIS facilitates wind power storage, allocating, and smoothing, enhancing delivery stability and energy management flexibility for both the grid and wind farm.\nFor BESS-based power smoothing, traditional model-based methods utilize the physical properties of WSIS to build the models and optimize the power generation. In terms of model completeness, researchers have considered the wind turbine wake model [9], battery degradation model [10], wind speed and power forecasting model [11], and other relevant physics information for PSC problems in WSIS. Moreover, Xiong et al. [12] combine sophisticated wake and battery energy models to optimize the allocation of power flow in BESS. However, challenged by complexity, none of the studies include both the wake effect and battery degradation in the model formulation of the WSIS PSC problem. Such model-based approaches overly rely on the accuracy and temporal scale of the environmental models, necessitating a trade-off between control deviation and computational costs, which pose difficulties for real-time implementation.\nTo address the aforementioned challenges, several model-free methods are applied in WSIS, such as random search [13], adaptive control [14], and Reinforcement Learning (RL). As an emerging data-driven approach, RL allows the agent to learn optimal policy by performing actions and receiving feedback rewards from the environment, making it well-suited for complex, dynamic, and uncertain conditions [15]. Deep RL (DRL), which incorporates RL with deep neural networks [16], has been successfully demonstrated in wind farm and BESS, respectively. For instance, Deep Deterministic Policy Gradient (DDPG) algorithm has been applied to achieve optimal wind power generation [17] and power smoothing [18] in wind farms. Yang et al. [19] implemented the Rainbow algorithm to control the charge/discharge schedule of the BESS to increase their revenues under uncertainties of wind generation and electricity price. Furthermore, with a unitary control frequency, Wang et al. [9] proposed a hybrid power smoothing strategy for WSIS. This approach consists of a model-based power control and an RL-based power optimization. However, there is currently a scarcity of research that fully addresses the WSIS PSC problem using model-free methods, especially satisfying both power maximization and power smoothing. Moreover, previous coordinated PSC studies have overlooked the distinct response frequencies and internal control sequences of wind turbines and BESS, which is a pivotal factor to take into account.\nTo navigate the intricate WSIS environment and enhance the learning process efficiency, multi-agent RL (MARL) and Physics-informed Neural Network (PINN) are introduced. Based on game theory, MARL deals with self-learning and decision-making in a dynamic environment where multiple agents interact, cooperate, or compete with each other based on local observations [20]. As a result, MARL can be introduced to handle asynchronous decision-making and multi-objectives. Furthermore, multi-level MARL explores settings where agents are organized into hierarchies or teams [21], each with different responsibilities and coordination requirements, which allows for much more flexible task allocation. MARL has been applied to manage multiple wind turbines [9] and batteries [22] for decision optimization in smart grid. On the other hand, PINN combines traditional physics-based modeling with machine learning methods [23], which makes it possible to design specialized network architectures that automatically satisfy some of the physical invariants for better accuracy, faster training, and improved generalization. Specifically, several paradigms of PINN, e.g., Physics-informed (PI) loss function, PI initialization, PI design of architecture, and hybrid physics-deep learning models, are summarized in [24]. PINN has found diverse extensions and applications, including dynamic analysis [25] and transfer learning [26]. When cooperating physics priors into the RL, it highlights the different components of the typical RL paradigm, such as state/action space, reward function, and agent networks [27]. Gao et al. [28] proposes a transient voltage control approach, by using the transient constraint on the value function to expedite convergence. However, for dynamic complex and uncertain environments, further exploration is imperative regarding the selection of both the physics prior and the method of RL implementation.\nTo solve the PSC problem in WSIS, this paper proposes a Physics-informed Deep Reinforcement Learning (PIDRL)-based coordinated control framework. The contributions of this paper are summarized as follows:\n\u2022 In light of the distinct control frequencies and sequence of wind turbines and BESS, a bi-level coordinated control framework is derived to address the WSIS PSC problem, which also incorporates wake effect and battery degradation models to achieve a comprehensive and authentic environment.\n\u2022 The proposed coordinated PSC framework is reformulated to a bi-level Markov decision process, and the reward function of MARL is redesigned to derive the real-time optimal strategy with less uncertainty. Specifically, the power generation of the wind farm, power fluctuation to the grid, energy loss within BESS, the degradation cost of BESS, and the penalty of the unsafe actions are selectively constructed as the reward functions of the two agents, to maximize total profitability and ensure power smoothness."}, {"title": "2. Wind Storage Integrated System", "content": "The WSIS primarily consists of a centralized control system, the BESS, wind turbines, and transmission lines that connect the system to the main grid. Power converters and transformers serve as devices for power conversion. The overall structure of the WSIS is shown in Fig. 1."}, {"title": "2.1. Overall Structure", "content": "The WSIS primarily consists of a centralized control system, the BESS, wind turbines, and transmission lines that connect the system to the main grid. Power converters and transformers serve as devices for power conversion. The overall structure of the WSIS is shown in Fig. 1."}, {"title": "2.2. Wind Farm Model", "content": ""}, {"title": "2.2.1. Wind Turbine Model", "content": "The power generated by a single wind turbine can be mathematically formulated using the following equations:\n$P_{turbine} = \\frac{1}{2} \\rho A_r C_p U_{inf}^3$  (1)\n$U_{min} \\leq U_{inf} \\leq U_{max}$ (2)\nwhere $P_{turbine}$ is the power output of the wind turbine, $\\rho$ is the air density, $A_r$ is the swept area of the rotor, and $U_{inf}$ is the free stream speed. The power coefficient, $C_p$, is the ratio of electric power extracted from the wind flow and is explicitly concerning detailed control policy. Furthermore, free stream speed $U_{inf}$ should be in a threshold between the cut-in speed $U_{min}$ and cut-out speed $U_{max}$ for regular operation of the turbine.\nThe Actuator Disk Model (ADM) [29] is used to simulate a wind turbine's aerodynamics, where the wind turbine's rotor is regarded as an actuator disk. Applying ADM to a one-dimensional stream shows that:\n$C_p (a) = 4a (cos (\\beta_y) - a)^2$ (3)\n$\\alpha = 1 - \\frac{U_r}{U_{inf}}$ (4)\nwhere $\\beta_y$ is the yaw angle and $U_r$ is the wind speed at the rotor. $a$ ($0 \\leq a \\leq 0.5$) is the axial induction factor, which is defined as the ratio of the speed deficit behind the rotor to the undisturbed wind speed ahead of the rotor. In axial induction-based control, the axial induction factor of the turbines is tuned by physically controlling the blades' tip speed ratio and pitch angle to recover the flow [30]."}, {"title": "2.2.2. Wake Model", "content": "The wake effect introduces challenges in calculating power output, as the turbulence and reduced wind speeds in the wake impact the downstream wind turbines. The Jensen Park model [31] takes into account the wake expansion and the effect of the rotor on the incoming wind flow. The wind speed $U$ at location $(x, y)$ is defined as:\n$U (x, y, a) = U_{inf} (1 - u_{deficit})$ (5)\nwhere $u_{deficit}$ is the wind deficit percentage compared to the free stream speed $U_{inf}$, represented by:\n$u_{deficit} = \\begin{cases}  \\frac{2 \\alpha}{(1 + kx)^2}, & \\text{if } y \\leq \\frac{D + 2kx}{2} \\\\ 0, & \\text{otherwise}  \\end{cases}$ (6)\nwhere $D$ is the turbine blade's diameter, and $k$ is the wake expansion parameter. Due to its comprehensibility and broad applicability, the Jensen Park model is commonly used in mainstream wind farm simulators."}, {"title": "2.3. Battery Energy Storage System", "content": ""}, {"title": "2.3.1. Battery Energy Model", "content": "For the purpose of simulation, the dynamics of the BESS considering energy loss [32] is modeled as:\n$E^{t+1}_B = E^t_B + [max(P_B^t, 0) \\cdot \\eta_{ch} - \\frac{max(-P_B^t, 0)}{\\eta_{dis}}] \\Delta t$ (7)\nwhere $\\eta_{ch}$ and $\\eta_{dis}$ represent the energy conversion efficiency during the charging and discharging process, respectively. $\\Delta t$ is the duration time of each interval and $E^t_B$ is the battery energy at time t. $P_B^t$'s represent the power flow to BESS,"}, {"title": "2.3.2. Battery Degradation Model", "content": "Battery degradation affects the energy that can be stored and discharged, ultimately impacting the system's reli-ability and economics. A linear programming approach in [34] is used to model and estimate the equivalent battery degradation costs. Firstly, the lifetime throughput $LT,n$ for every depth of discharge $n$ is calculated as:\n$LT,n = E_{max} g_n f_n$ (10)\nwhere $f_n$ is the number of battery cycles to failure, $g_n$ is the Depth of Discharge (DOD) of the battery. Then, the resulting lifetime throughput $L_T$ is obtained by averaging the values of $LT,n$ in the allowable operating range of battery DOD:\n$L_T = \\frac{1}{n} \\sum_{i=1}^n LT,n$ (11)\nThe equivalent battery degradation cost per kWh can be defined as:\n$K_{deg} = \\frac{C_R}{L_T \\sqrt{R_E}}$ (12)\nwhere $C_R$ is the replacement cost of the battery, and $R_E$ is the square root of the roundtrip efficiency of the battery. The unit of $K_{deg}$ is ($/kwh). This modeling approach refers to battery lifetime throughputs under different DOD and SOC and performs averaging to estimate the battery degradation cost in a linearized way. The cost will be incurred every time the battery is discharged:\n$C^t_{deg} = K_{deg} max(-P^t_B, 0) \\cdot \\Delta t$ (13)"}, {"title": "3. DRL-based Coordinated Control Framework", "content": ""}, {"title": "3.1. Bi-level Coordinated Power Smoothing Control", "content": "The objective of the proposed framework is to maximize the total power profit of the WSIS while ensuring the fluctuation of the power output within an acceptable level. The key idea of this framework is based on two assumptions. First, the centralized control system has the ability to control all the turbines simultaneously. Second, the control frequencies for wind turbines and the BESS are set to be 1/f and 1 min\u207b\u00b9, respectively, where f is the real-time control time scale for wind turbines [35]. According to the highest control frequency, the time granularity \u0394t is discretized as one minute.\nThe model of the PSC problem for WSIS is formulated as an objective function (14) that maximizes the total profit of the power generation and the negative degradation cost of the BESS in a time interval of T:\n$\\max_{P_B, \\overrightarrow{\\alpha}} \\sum_{t=0}^T (P_r^t \\cdot P_G^t \\cdot \\Delta t - C_{deg}^t)$ (14)\ns.t.\n$\\overrightarrow{\\alpha} = {(\\alpha_1^t, \\alpha_2^t, ..., \\alpha_n^t)}$ (15)"}, {"title": "3.1.1. Upper-level Wind Farm Control", "content": "According to the separation of the bi-level formulation, the sub-goal of the upper level is to maximize the total profit of wind power generation in a control loop T response to changes in wind velocity and electricity price. Choosing all turbines' axial induction factors \u03b1 as the control variable, the control objective can be written as:\n$\\max \\sum_{t=0}^T P_r^t \\cdot \\sum_{i \\in N_{\\Psi}} P_{turbine,i}^t$ (20)\ns.t.\n$\\overrightarrow{\\alpha}^t = \\overrightarrow{\\alpha}^{t-f} \\text{ if not } t = 0, f, 2f, ...$ (21)\nThe control frequency is guaranteed in Eq. (21). The detailed models for wind turbines are illustrated in Eqs. (1) -(6)."}, {"title": "3.1.2. Lower-level Battery Energy Storage System Control", "content": "The lower-level system is related to control objects in BESS, which are responsible for minimizing the battery degradation cost, electricity purchasing cost, and power fluctuations by controlling the power flow among the wind farm, BESS, and the grid. For simplicity and flexibility, the power fluctuation constraint Eq. (19) is transferred to the objectives as a penalty term that prescribes a high cost for violation of the constraint. Therefore, the problem is represented as:\n$\\min_{P_B} \\sum_{t=0}^T [C_{deg}^t + P_r^t \\cdot (P_B^t - P_W^t) + \\beta \\cdot P_{VG}^t]$ (22)"}, {"title": "3.2. DRL-based Coordinated Control Framework", "content": "To solve the aforementioned problem, a coordinated bi-level WSIS PSC framework is proposed, as depicted in Fig. 2. The following steps outline the detailed procedure. First, the upper-level controller gathers observations from the environment, including the free stream speed and direction measured by the anemometer, as well as the on-grid price for wind power. Subsequently, the free stream velocity will be preprocessed to a suitable threshold to satisfy the cut-in and cut-off speed. If the free stream velocity surpasses the predefined threshold, the wind turbine will automatically shut down, and the upper-level controller will wait until the next decision time to reassess the observations and repeat the steps. When the system observes the current observation and the accumulative reward during the f minutes, data are saved to a database to train the RL agent. The agent then calculates the real-time control strategy using the agent's policy. The action should be checked through a safety module in case some improper operations damage the turbine. Furthermore, the upper-level action is conveyed to the lower-level controller as a guiding signal, which indicates the amount of wind power to allocate.\nBased on the current state of the system, the lower-level module provides the control strategy with a higher frequency, precisely at a one-minute period. After aggregating the observations from both the environment and the upper-level signal from the power meter, the observations undergo normalization to an appropriate scale. The training process of the RL agent is similar to the upper-level controller, with the addition of a physics-informed neural network to expedite training. The strategy derived from the upper-level agent undergoes testing through a safety module to prevent battery charging/discharging actions from exceeding the maximum charging/discharging power limits. It's noteworthy that the upper-level controller generates actions every f minutes, while the lower-level controller operates within the control loop until it reaches the maximum time steps, at which point the lower-level controller terminates the current control loop.\nDue to the stochastic nature of wind, the sequential decision-making involved, and the adherence to the Markov property, the above-mentioned control problem can be reformulated as a Markov Decision Process (MDP). Customize distinct agents for each level within this coordinated control framework."}, {"title": "3.2.1. Upper-level Agent", "content": "The upper-level agent is defined as follows.\nState: The upper-level agent controls the multiple wind turbines in the wind farm. Considering the target of the system, the state includes the incoming free-stream wind speed $U_{inf}^t$, wind direction $\\phi^t$, and electricity price $P_r^t$ in time t, i.e., $s_U^t = (U_{inf}^t, \\phi^t, P_r^t)$. Note that the free stream wind speed $U_{inf}^t$ should satisfy the Eq. (2). If the wind speed $U_{inf}^t$ falls outside the range, the state is invalid.\nAction: The action is defined as the axial induction factors of the turbines, which can be expressed as a vector:\n$\\overrightarrow{\\alpha_t} = (\\alpha_1^t, \\alpha_2^t, ..., \\alpha_n^t)$. To ensure the safety, rather than directly applying $\\alpha(i \\in N_{\\Psi})$ to the environment, a check model is utilized. If the action is infeasible, meaning $\\alpha < 0$ or $\\alpha > \\frac{1}{2}$ for some $i \\in N_{\\Psi}$, the agent is required to adjust the action to $\\overrightarrow{\\alpha_t^*} = (\\alpha_1^*, \\alpha_2^*, ..., \\alpha_n^*)$ according to the following rule.\n$\\alpha_i^* = \\begin{cases}  \\frac{1}{2}, & \\text{if } \\alpha > \\frac{1}{2} \\\\ \\alpha_i^t, & 0 \\leq \\alpha \\leq \\frac{1}{2} \\\\ 0, & \\alpha < 0  \\end{cases}$ (25)\nReward: The target of the upper-level agent is to maximize the wind farm's total profit and keep the action within the safe range. A normal way to deal with constraint is to use the violation as a punishment in the reward function. Therefore, the reward function consists of two parts: the first is the total power generation of the wind turbines, while the second is the penalized term for unsafe actions. The cumulative reward $r_U$ in a control interval will be received after f minutes:\n$r_U = \\sum_{t'=t}^{t+f-1} (P_r^{t'} \\cdot \\sum_{i \\in N_{\\Psi}} P_i^{t'}) - G_U$ (26)\n$G_U = \\kappa \\cdot ||\\overrightarrow{\\alpha_t^*} - \\overrightarrow{\\alpha_t}||$ (27)\nwhere $P^t_i$, $U_{inf}^t$, and $\\phi^t$ are the power generation, incoming wind speed, and wind direction of the turbine i in time t', respectively. \u03ba is the penalized coefficient and $|| \\cdot ||$ denotes the Manhattan distance."}, {"title": "3.2.2. Lower-level Agent", "content": "The lower-level agent is defined as follows.\nState: The lower-level agent allocates the wind power generation based on information including historical power to grid. After the upper-level controller optimizes the turbines, the lower-level agent chooses the action according to the current environment state $s_U^t$, the grid-connected power of the previous time $P_G^{t-1}$, and the signal $g^t$ from the upper level. To smooth the power output to the grid, the upper-level signal $g^t$ is addressed as the total power generation $P_W^t$ for each time step. Although the induction factor \u03b1 remains unchanged, $P_W^t$ still varies as a result of the changing wind velocity. Accordingly, the state is defined as: $s_L^t = [s_U^t, P_G^{t-1}, P_W^t]$.\nAction: The lower-level action $a_L^t$ is defined as the quantity of the charging or discharging power at time step t, defined as a continuous variable $a_L^t = P_B^t$ constrained in Eq. (8), which ultimately determines the power flow to the grid by Eq. (17). Invalid actions will be clipped to the boundary value during the learning process and punished according to the degree of the violation."}, {"title": "3.3. MA-DDPG Algorithm", "content": "To tackle the complexity and information flow of the given bi-level MDP problem, multi-agent DDPG (MA-DDPG) is specially developed to learn the optimal policy. Unlike DDPG, the centralized fully cooperative MA-DDPG algorithm comprises several actors (a policy network $\\mu(s|\\theta^{\\mu})$ with $\\theta^{\\mu}$ as the parameter used to approximate the deterministic policy function) which are used to execute different tasks according to the local observation. Each actor has a corresponding critic (a value network $Q(s, a|\\theta^Q)$ with $\\theta^Q$ as the parameter used to simulate the action-value function) that is used to guide the actions of the actors.\nFor the coordinated control problem in WSIS, MA-DDPG employs a centralized training and decentralized execution architecture. The critics collectively share both actions and observations across all agents, while each actor independently acts based on its state. The actor's parameter $\\theta^{\\mu}$ and the critic's parameter $\\theta^Q$ of agent $k$ ($k \\in {U, L}$) are updated by:\n$L(\\theta^Q) = E [y' - Q^k (s^t, a^t | \\theta^Q)]^2$ (30)\n$y' = r^t + \\gamma \\max_{a^{t+1}} Q_k (s^{t+1}, a^{t+1}|\\theta^{Q'})$ (31)\n$\\nabla_{\\theta^{\\mu}} J_k = E [\\nabla_{a^k} Q_k (s^k, a|\\theta^Q)|_{a^k=\\mu^k(s^k)} \\nabla_{\\theta^{\\mu}} \\mu^k (s^k)]$ (32)\nwhere $L$ is the loss function for value network, $E$ is the expectation of samples in random batch, and $\\gamma$ ($0 < \\gamma < 1$) is the discounted factor. $a^k, s^k, \\mu^k, Q^k$ and $J_k$ are the action, state, policy network, value network and the cumulative discounted reward of the agent k, respectively. $\\mu'(s|\\theta^{\\mu})$ and $Q(s, a|\\theta^{Q'})$ are target networks to slowly update the learned actor and critic network with identical initial parameters [36], which are improved by a soft updating process as follows:\n$\\theta' \\leftarrow \\tau \\theta + (1 - \\tau)\\theta$ (33)\nwhere $ \\tau$ is the hyper-parameter for the target network to update.\nSpecifically, as shown in Fig. 3, the upper-level agent receives the raw states $s_U^t$ and produces its action $a_U^t$ by its actor network $\\mu_U$. $a_U^t$ persists unchanged for the subsequent few time steps due to the decision frequency. For each state $s^t$ ($t' \\in {t, t + 1, ..., t + f - 1}$), $a_U^t$ corresponds to a signal $P_W^t$, which will be transmitted to the lower-level agent together with raw state $s_U^t$ and historical power output $P_G^{t-1}$. The lower-level agent integrates this information as its states\", producing its action $a_L^t$ using the actor network $\\mu_L$ similarly and getting an instant reward $r_L^t$. The lower-level agent terminates either when the episode ends or when the maximum time step $f$ is reached. Subsequently, the upper-level agent obtains the cumulative reward $r_U$. The training process is similar for both agents using Eq. (30)-(32) and their network parameters $\\theta_U$ and $\\theta_L$ are revised. Notably, PINN is designed for the lower-level training procedure, as elucidated in the subsequent section. Following this, the iterative process continues until each agent converges.\""}, {"title": "3.4. Physics-informed Neural Network", "content": "The proposed muti-agent DRL consists of two levels of policies and agents, thus, the interaction between different levels adds additional complexity and training time. Especially for the lower-level agent, its reward encompasses various facets, rendering the allocation of the optimal power flow a huge challenge. Therefore, considering the dynamics of power fluctuation for optimal policy, the PINN-based method is introduced to enhance the efficiency of the training process of the multi-agent DRL.\nFor the physical models of a general dynamic system, PINN incorporates its physical constraints into the neural network via differential equations involving time and other physical inputs. The general form [37] can be expressed as:\n$\\frac{du(t, s)}{dt} + N(u) = 0$ (34)\nwhere $u(t, s)$ represents the potential solution, s is the input state vector, and N(\u00b7) is an operator that can encapsulate a series of mathematical physics.\nWhen applied to this work, the solution is defined as the lower-level action $P^t_B = u(t, s_L)$ with lower-level state $s_L$. Assume that the power transferred to the grid is invariant to simulate the ideal dynamic system. Thus, the changes in power $P_G$ remain zero over time t. The invariable physical equation about the optimal power smoothing objective is derived as:\n$\\frac{\\partial P_G}{\\partial t} = 0$ (35)\nwhere $P_G^t$ is identified as the variable related to the lower-level action $P_B^t$ by Eq. (17). Accordingly, the above equation can be elegantly reformulated as a differential equation related to the solution $P_B^t$:\n$\\frac{\\partial P_G}{\\partial t} + N(P_B^t) = 0$ (36)\nwhich can be regarded as the special case of Eq. (34) for WSIS. Define $f(t, s_L)$ to be given by the left-hand-side of Eq. (36), i.e.,\n$f(t, s_L) := \\frac{\\partial P_G}{\\partial t} + N(P_B^t)$ (37)\nand $u(t, s_L)$ has been approximated by policy network $\\mu_L$. The policy network $\\mu_L$ and physics-informed network $f(t, s_L)$ have the same parameters, while the activation functions are different due to the operator N. Therefore, the physics-informed network $f(t, s_L)$ can be utilized for the training of the lower-level agent. The shared parameters between the neural networks can be learned by minimizing the integrated mean squared error loss:\n$L_{PINN} = w_u L_u + w_f L_f$ (38)\nwhere $w_u$ and $w_f$ are the weights to balance the interplay between the two loss terms. $L_u$ and $L_f$ are the data error and the physical information error of the neural network, respectively. In WSIS, $f$ represents the physical laws of power smoothness:\n$L_f = \\frac{1}{N_f} \\sum_{i=1}^{N_f} |f(t, s_L^i)|^2$ (39)\nwhere $N_f$ is the batch size and ${(t, s_L^i)}$ are sets of experiences in the sampled batch. Hence the actor network is updated by the gradient ascent method:\n$\\nabla J \\approx E_s [w_u \\cdot (\\nabla_a Q(s, a|\\theta^Q) |_{a=\\mu(s)}; \\nabla_\\theta \\mu(s|\\theta^\\mu)) + w_f \\cdot \\nabla f(t, s)|^2]$ (40)\nThe output of PINN and the associated physics-based information of environment states are provided as components to the RL agent's neural network loss function during its training. It is noteworthy that, since $P_G$ has already been calculated for state derivation, there is no need for additional storage or computation of $P_G^t$, and $N(P_B^t)$ in $f(t, s)$. Considering physical laws during model training constrains the range of feasible solutions for neural network parameters, consequently reducing the demand for extensive training data and the size of the neural network. The detailed process of PINN-assisted Multi-agent DDPG (PAMA-DDPG) is shown in Algorithm 1."}, {"title": "4. Experimental Results", "content": ""}, {"title": "4.1. Setup", "content": "To evaluate the effectiveness of the proposed PAMA-DDPG algorithm, we employ the dynamic control-oriented wind farm simulator, WindFarmSimulator (WFSim) [38], for conducting comprehensive case studies. WFSim is a dynamic medium fidelity control-oriented model that predicts the flow velocity vectors in a wind farm using the spa-tially and temporally discretized 2D Navier-Stokes equations, which strike a balance between simulation fidelity and"}, {"title": "4.2. Optimal Results Comparison", "content": "To provide a clear demonstration of the optimal results achieved by the proposed algorithm, MPC's performance is utilized as the baseline, and the relative metrics of other methods are calculated. By comparing the performance of MPC, DDPG, MA-DDPG, and PAMA-DDPG algorithms, we can effectively assess the effectiveness and superiority of the proposed approach.\nFig. 6 exhibits that the proposed PAMA-DDPG algorithm can always reach the maximum total profit and minimum fluctuation in the WFSim environment. On average, PAMA-DDPG demonstrates improvements of roughly +11%, -19%, and -22% in total profit, FS, and VO compared to the MPC method. The base values for MPC are $840.55, 19.01MW, and 23 for total profit, FS, and VO respectively. As the single-agent DDPG control operates at a reduced frequency, it yields unsatisfactory performance. The reason is that the agent cannot react to the environmental"}, {"title": "4.3. Verification of Model Completeness", "content": "Within the proposed framework, we adopt a comprehensive approach by considering the influence of both the wind wake model and the battery degradation model. In order to evaluate the impact of model completeness, two controlled experiments are designed to compare the proposed control strategies.\nFirstly, if the wake model is not considered, the upper layer control problem could have an analytical solution. By utilizing the power generation formula of the wind turbine, since the induction factor A is constrained by 0 and 1/2, the maximum power extraction $P^*$ for the single turbine is determined as $A^* = cos(4y)/3$, while the coefficient of power, denoted as $C$, can be expressed as 16 cos\u00b3 (4y)/27. Secondly, the battery degradation model is directly related to the learning goal for the lower-level controller. Without considering the degradation, the performance feedback obtained from control algorithms may be inaccurate, leading to unreliable operation of the system. To validate it, we test the proposed PAMA-DDPG algorithm in the four testing scenarios with varying model settings.\nThe results depicted in Fig. 7 highlight the positive influence of accurate and comprehensive models on the WSIS system's control objectives, particularly in maximizing total profit and minimizing power fluctuation/violation. Specifically, define the complete model as the baseline, for the model without wake effect, the average total profit, FS, and VO are roughly 85%, 107%, and 108%, respectively. For the model without battery degradation, the average total profit, FS, and VO are 91%, 135%, and 152%. By incorporating a complete environmental model, the PAMA-DDPG algorithm is empowered to make more informed decisions, leading to improved control performance."}, {"title": "4.4. Verification of the Efficiency of PINN", "content": "The lower-level controller utilizes the PINN during the training process. In this test, we compare the changes of rewards, total profit, and fluctuation (FS) against iteration times between the MA-DDPG and PAMA-DDPG algorithms to validate the efficacy of PINN. Due to the significant variability in wind velocity, the direct output remains highly volatile even"}]}