{"title": "Loss-to-Loss Prediction: Scaling Laws for All Datasets", "authors": ["David Brandfonbrener", "Nikhil Anand", "Nikhil Vyas", "Eran Malach", "Sham Kakade"], "abstract": "While scaling laws provide a reliable methodology for predicting train loss across compute\nscales for a single data distribution, less is known about how these predictions should change\nas we change the distribution. In this paper, we derive a strategy for predicting one loss from\nanother and apply it to predict across different pre-training datasets and from pre-training\ndata to downstream task data. Our predictions extrapolate well even at 20x the largest\nFLOP budget used to fit the curves. More precisely, we find that there are simple shifted\npower law relationships between (1) the train losses of two models trained on two separate\ndatasets when the models are paired by training compute (train-to-train), (2) the train loss\nand the test loss on any downstream distribution for a single model (train-to-test), and\n(3) the test losses of two models trained on two separate train datasets (test-to-test). The\nresults hold up for pre-training datasets that differ substantially (some are entirely code and\nothers have no code at all) and across a variety of downstream tasks. Finally, we find that in\nsome settings these shifted power law relationships can yield more accurate predictions than\nextrapolating single-dataset scaling laws.", "sections": [{"title": "1 Introduction", "content": "Scaling laws (Kaplan et al., 2020; Hoffmann et al., 2022) have become a reliable tool for extrapolating\nmodel performance (as measured through, e.g., cross-entropy loss on held-out data), as well as a way to\ndetermine optimal model size given a FLOP budget (Llama 3 Team, 2024). In their standard form, scaling\nlaws essentially predict the training loss for a given model size and dataset size. However, these scaling laws\nare distribution-dependent and only apply to the training distribution that is used to fit the scaling law.\nRelatively little is known about how they change across different pre-training distributions, and how to use\nscaling laws to predict transfer performance on downstream test distributions.\nIn this paper, we take a first step towards understanding how scaling laws change as we change either the\ntraining distribution or the testing distribution. To do this, we propose loss-to-loss prediction, a methodology\nfor predicting the loss on one data distribution from the loss on another. This is useful since once we have a\nfunction that predicts one loss from another, we can take a scaling law fit on the first loss and immediately\ntranslate it to a scaling law for the second loss. Further, if we have a suite of models trained on one dataset"}, {"title": "2 Related work", "content": null}, {"title": "2.1 Scaling laws", "content": "Standard approaches to scaling laws attempt to fit a curve to the optimal number of model parameters N\nand training tokens D to minimize the pre-training loss under a given budget of FLOPs (Hestness et al.,\n2017; Kaplan et al., 2020; Hoffmann et al., 2022; Porian et al., 2024; Abnar et al., 2021; Maloney et al., 2022;\nBordelon et al., 2024a).\nTo fit these curves, it is useful to specify a parametric form of the loss in terms of N and D. Hoffmann et al.\n(2022) assumes this curve takes the following form:\n\n$L(N, D) = E + \\frac{A}{N^{\\alpha}} + \\frac{B}{D^{\\beta}}$\n\nThis formula is inspired by classical upper bounds on a loss decomposition that attributes error to Bayes\nrisk (entropy), approximation error (from having finite parameters), and estimation error (from having finite\ndata) (Bottou and Bousquet, 2007).\nOn the other hand Kaplan et al. (2020) instead assumes that:\n\n$L(N,D) = ((\\frac{A}{N})^{\\alpha/\\beta}+ (\\frac{B}{D})^{\\beta})$"}, {"title": "2.2 Scaling laws for transfer and downstream tasks", "content": "Scaling laws for pre-training loss are useful as a proxy to guide pre-training, but we ultimately care about\ndownstream task performance. Prior work attempting to tackle this issue has found that directly computing\nhard metrics like accuracy can lead to the appearance of emergent behaviors and suggests using softer metrics\nlike cross entropy loss instead (Schaeffer et al., 2024a;b). This is corroborated by Du et al. (2024) which\nnotes that while downstream accuracy can vary smoothly with training loss at some points in the curve, the\nhardness of the accuracy metric means that no progress in accuracy above random chance will be observed\nuntil some \"emergent\" loss level.\nOn the other hand, Gadre et al. (2024) claims that downstream accuracy can be predicted as a function of\ntraining loss with a similar exponential curve to the one we propose for predicting downstream loss. However,\nthey only claim this is predictable when averaging over many tasks and carefully selecting which tasks to use.\nIn this paper when considering downstream tasks we focus on single downstream tasks and find loss to be a\nmore stable metric than accuracy. A detailed discussion of loss versus accuracy is in Appendix A.\nAnother related line of work comes from the distributional robustness literature on \"accuracy on the line\"\n(Miller et al., 2021; Tripuraneni et al., 2021; Awadalla et al., 2022). This phenomena focuses on the relationship\nbetween the accuracy of a single model across two closely related tasks, like different versions of imagenet,\nand finds that accuracy on one will predict accuracy on the other. We consider loss rather than accuracy,\nlanguage modeling rather than vision, and find non-linear fits."}, {"title": "3 Setting", "content": null}, {"title": "3.1 Notation", "content": "We are interested in studying transfer across different training distributions. To formalize this, we will define\ntwo distributions: Po and P\u2081. We will consider Po as the \"source\" and P\u2081 as the target. The goal is to use a\nfunction of the loss on Po to predict the loss on P\u2081. As an example, Po could be FineWeb and P\u2081 could be\nStarcoder or Hellaswag. We use L\u2081 to indicate the loss calculated on distribution Pi (averaged per-token). If\nP\u2081 represents a multiple choice task, we will let L\u2081 be the loss of correct answer when the question is phrased\nas a cloze task (following (Schaeffer et al., 2024b; Madaan et al., 2024)).\nGiven a pre-training distribution Pi, we let \u00ceN,D denote an N parameter model trained on D tokens sampled\nfrom Pi. Our results present comparisons across losses Lo, L\u2081 for models \u00ceN,D, \u00ceN,D when sweeping\ndifferent choices of Po, P1, as well as N, D.\nWhen we refer to a scaling law fit from Equation (4) on distribution Pi, we will append a subscript to the\ncorresponding parameters. For example, the irreducible entropy of the scaling law fit on Po is denoted by Eo."}, {"title": "3.2 Experimental methodology", "content": "To facilitate our analysis, we pre-train models of varying size with varying flop budgets on 6 pre-training\ndatasets: FineWeb (Penedo et al., 2024), FineWeb-edu (Penedo et al., 2024), Proof Pile 2 (Azerbayev\net al., 2023; Computer, 2023; Paster et al., 2023), SlimPajama (Soboleva et al., 2023), SmolLM Corpus\n(Ben Allal et al., 2024), and Starcoder v1 (Li et al., 2023). We train all models using OLMO (Groeneveld\net al., 2024) and generally follow hyperparameter settings from Wortsman et al. (2023); Zhao et al. (2024).\nFull hyperparameters can be found in Appendix E. Importantly, we use a linear warmup and cosine decay\nschedule for every run and only report the final performance (Porian et al., 2024).\nFLOP budgets for our sweep range from 2e17 to 4.84e19 and model sizes range from 20M to 1.7B. The\noptimal model at the largest FLOP budget is roughly 750M (it varies per dataset). The total grid contains\n528 models, or 88 models per dataset. For our extrapolation experiments, we train 6 larger models (one for\neach dataset) at a FLOP budget of 1e21 each of size 3.3B. Full scaling law fits are in Appendix D."}, {"title": "4 Predicting loss across datasets", "content": "In this section, we present the loss-to-loss relationships that for the core observation of the paper. In turn we\nwill present train-to-train, train-to-test, and test-to-test relationships."}, {"title": "4.1 Train-to-train prediction", "content": "Our first main result is to observe a consistent scaling relationship between train losses across datasets.\nExplicitly, we find that by fitting just two parameters K and we can capture and extrapolate the scaling\nrelationship between pairs of training losses as follows:\n\n$L_{1}(\\widehat{f}_{N,D}) \\approx K \\cdot (L_{0}(\\widehat{f}_{N; D}) - E_{0})^{\\kappa} + E_{1}$\n\nNote, this is comparing different losses and different models, but the models are paired when they each\nhave N parameters trained on D tokens. Also, recall that E0, E1 are the irreducible errors from independent\nscaling law fits on Po and P\u2081 respectively. Finally, note that since we are only fitting a slope and exponent,\neach curve is linear on a shifted log-log scale. However, since we are plotting 6 curves in one plot, each with\ndifferent E1, we cannot display them all consistently log-log plot and opt for a linear scale for clarity. Results\nfor fitting these curves can be seen in Figure 2."}, {"title": "Scaling law parameterization.", "content": "Note neither Equation (1) nor Equation (2) provides a parameterization\nwhere the translation defined by Equation (3) gives a valid mapping between scaling laws. As such, in this\nwork we use slightly different functional form that does yield valid scaling law translations, and is essentially\nEquation (2) with an added entropy term. Explicitly, we use:\n\n$L(N, D) = E + ((\\frac{A}{N})^{\\alpha/\\beta}+ (\\frac{B}{D})^{\\beta})$\n\nFull fits of our scaling laws and fits using Equation (1) can be found in Appendix D. We should caveat that\nwhile this formulation leads to valid translations, we are not precluding other formulations. We think it is an\ninteresting open question to precisely pin down the correct formulation for scaling laws.\nNote that under the parametrization in Equation (4), we get the following relationships between parameters\nof the scaling law for L\u2081 and Lo under the translation predicted by Equation (3):\n\n$\\alpha_{1} = \\kappa \\alpha_{0}, \\beta_{1} = \\kappa \\beta_{0}, A_{1} = K^{\\frac{1}{\\alpha_{0}}} A_{0}, B_{1} = K^{\\frac{1}{\\beta_{0}}}.$\n\nIn this way, Equation (3) maps one valid scaling law to another."}, {"title": "Compute optimal models.", "content": "Under the parameterization in Equation (3) for translating between losses,\nthe size of the compute optimal is invariant. To see this, note that the optimal model size for a given flop\n$\\beta$\n$\\alpha+\\beta$\nbudget N*(C) can be expressed as (C) for a = and G = under the assumption that C = 6ND.\nCoupled with the relationships described in Equation (5), this implies that under the transformations induced\nby Equation (3) the function N*(C) is invariant.\nThis implies that for a given FLOP budget, the optimal model size is the same for any data distribution\nwhere this translation relationship holds. This seems like a strong conclusion, but does fit in with common\nempirical practice after Hoffmann et al. (2022) where practitioners often train on approximately 20x more"}, {"title": "Implications.", "content": "In summary, the train-to-train prediction results have a few implications:\n\nSince K, \u03ba are not near 1, different datasets can indeed lead to substantially different returns to scale\nin terms of reductions in loss. However, under our translations the compute optimal model size is\ninvariant to the training distribution.\nEquation (4) is the only formulation of the underlying scaling law that is compatible with the\ntrain-to-train fit given by Equation (3). If we instead used eq. (1), then the transformed scaling law\nafter applying Equation (3) would no longer satisfy the same functional form."}, {"title": "4.2 Train-to-test prediction", "content": "Next, we want to go beyond the train loss and consider translating the train loss to a test loss for the same\nmodel under a different distribution. We now hypothesize that the functional form of the relationship is as"}, {"title": "4.3 Test-to-test prediction", "content": "Next, we can move on to test-to-test prediction which can be seen as a composition of the prior two rules.\nThis now involves three different data distributions: Po the initial training distribution, P\u2081 the target training\ndistribution, and P\u2082 the test distribution that we use to measure loss. Explicitly, we consider:\n\n$L_{2}(\\widehat{f}_{N,D}) \\approx K \\cdot (L_{2}(\\widehat{f}_{N;D}) - E_{20})^{\\kappa} + E_{2/1}$\n\nLike train-to-train, these predictions compare the same loss on different models, but now we are using test\nloss rather than train loss. In this way, test-to-test can be seen as a generalization of train-to-train. Models\nare paired when they use the same number of parameters N and number of training tokens D.\nResults on four downstream losses are shown in Figure 5. Note that now that we are combining three\ndistributions rather than two, there are many more possible combinations. Here we focus on a fixed Po as\nFineWeb-edu and show results across training data P\u2081 and test distributions P2. Further results on other\nsweeps and combinations can be found in Appendix C.\nAgain the fits are usually good and able to extrapolate to models trained with 20x the FLOP budget of\nthe largest one used to fit the curves. The fits are especially good on Hellaswag, but as before the other\ndownstream datasets tend to be substantially noisier. This is magnified now since this evaluation noise affects\nboth the x and y axes when they are both measuring test loss (unlike in train-to-test when only one axis\ndepends on test loss). In the next section we will discuss a practical use case for test-to-test prediction."}, {"title": "4.4 General loss-to-loss prediction", "content": "Having presented three important types of loss-to-loss prediction, we can now hypothesize a generalization\nthat encompasses all three as special cases (along with more types that we have not yet discussed):\n\n$L_{i}(\\widehat{F}_{N,D}) \\approx K \\cdot (L_{k}(\\widehat{f}_{N,D}) - E_{k/\\ell})^{\\kappa} + E_{i/j}$\n\nThe content of the above equation is that it is a prescription for translating losses on distributions i and kas\ncomputed by models fj, fe that were trained on distributions j and l. Since Equation (9) can be composed"}, {"title": "5 Loss-to-loss prediction can outperform independent scaling laws", "content": "Consider the following situation that a practitioner could encounter: after having fit a scaling law and\nperformed a large run on one dataset, they want to know what would happen if they trained on a different\ndataset. They could fit an independent scaling law on the new dataset, but that would not be leveraging the\ncomputation that has already been done. Instead, we can use loss-to-loss prediction. This can allow us to\nget good predictions of the scaling laws and test performance with only a few model runs on the new data\ndistribution since we can leverage information we already have from the original training distribution.\nIn this section we consider two variants of this situation, one where we fit a scaling law on the new distribution\nand one where we predict the test loss of training a large model on the new distribution."}, {"title": "5.1 Translating a scaling law", "content": "For the scaling law setting, we consider the following scenario. There are two pre-training distributions Po\nand P1. Assume that we have already fit a set of 88 small models on Po so as to fit a scaling law. Then, we\nfit only 8 small models on a new distribution P\u2081. We want to get a scaling law on P\u2081.\nWe will consider two approaches illustrated in Figure 6 and Figure 7:\n\n(Ours) Train-to-train translation. We fit a train-to-train curve using the 8 models on P\u2081. From this\nwe can translate the scaling law from Po to P1.\u00b2\n(Baseline) Independent scaling laws. Here we fit an independent scaling law on P\u2081 from only the 8\nmodels we have that are trained on that dataset.\n\nThe point of this experiment is to illustrate how train-to-train fits can unlock an efficient way to fit a new\nscaling law on a new dataset. Note that as we said above, we should caution that under train-to-train\ntranslation the size of the compute optimal model is invariant.\nWe also consider a skyline of fitting a scaling law on P\u2081 with access to all 88 models trained on P\u2081. Then we\ncompute the R\u00b2 of each of the three scaling law models (skyline, ours, and baseline) on the entire set of 88\nmodels trained on P\u2081 to assess the goodness of fit. Results are reported in Table 1."}, {"title": "5.2 Predicting test loss on a large model", "content": "For the test loss setting, we consider the following scenario. There are two pre-training distributions Po and\nP1. Assume that we have already fit a set of 4 small models and one larger model (3.3B parameters and 1e21\nFLOPs) on Po. Then, we consider a new dataset P\u2081 and fit only 8 small models with various budgets on P\u2081.\nWe want to predict what would happen if we train a large model on P\u2081.\nWe will consider the approaches illustrated in Figure 8, plus one additional baseline:\n(Ours) General train-to-test prediction. We fit a train-to-test curve across different training sets\nusing the 8 paired small models. Explicitly, we predict L2(fN,D) from Lo(fNd). This allows us to\nextrapolate using the train loss of the large model trained on Po as an input.\n(Ours) Test-to-test prediction. We fit a test-to-test curve using the 8 paired small models. Explicitly,\nwe predict L\u2082(fN,D) from L\u2082(f,d). This allows us to extrapolate using the test loss of the large\nmodel trained on Po as an input.\n(Baseline) FLOPs-to-test. As a first baseline that does not use information from Po, we can fit a\ncurve from FLOPs to test loss. Since each of the models is near the chinchilla-optimal model size for\nthe FLOP budget, it is reasonable to fit a curve and extrapolate it here.\n(Baseline) Independent scaling law. As before, we can fit a full scaling law to the set of small models\non P\u2081 and extrapolate the predictions.\n(Baseline) Identity. As an even simpler baseline, we can just predict that the test loss when training\non P\u2081 is exactly the same as training on Po."}, {"title": "6 Discussion", "content": "Here we discuss the takeaways of our findings, some limitations, and directions for future work.\nTakeaways.\nLoss-to-loss fits with shifted power laws provide a good description of empirical trends across a\nvariety of pre-training datasets and to downstream tasks. These fits can effectively extrapolate well\nbeyond the scale they were trained on.\nLoss-to-loss prediction is of scientific interest since it provides several insights into the nature of how\ntraining data affects models and how transfer performance scales predictably.\nLoss-to-loss predictions can be practically valuable for translating scaling laws and predicting test\nloss of large models trained on new data.\nLimitations and caveats.\nOur fits rely on estimating the asymptotic entropy of various scaling laws. This is a fundamentally\ndifficult quantity to estimate and we hypothesize that where our fits fail it is often due to poor\nestimates of this quantity. Moreover, we hypothesize that when our fits fail to extrapolate beyond the\n20x results reported in the paper, it is likely due to errors in estimating these irreducible loss terms.\nNote that many of the train-to-test and test-to-test fits have noisier trends, especially at high losses.\nIt is not totally clear if this is pure noise or may be indicative that the power law trend does not\nhold as globally as we hypothesize. Future work could dive into this issue more directly.\nWe only test on a relatively small set of downstream tasks compared to all possible choices. We\nalso focus on multiple choice tasks instead of generative tasks since they have been more extensively\nstudied in prior work and have easier to compute proxy loss metrics.\nOur results hold for our specific choices of hyperparameters and may not hold under some other\nchoices. In particular, we would be interested in checking robustness to pre-training hyperparameters\nlike sequence length, batch size, and learning rate.\nFuture work.\nOne exciting direction is to take the implications of the loss-to-loss relationships further so as to\ndirectly inform data mixing and filtering. Once we have reliable predictions, we can use those to"}, {"title": "A From loss to accuracy", "content": null}, {"title": "A.1 Train-to-error", "content": "We focus on loss-to-loss prediction, but it of course would be useful to be able to predict accuracy. Prior\nwork (Schaeffer et al., 2024a;b; Du et al., 2024) indicates that predicting accuracy from loss can be difficult,\nand we generally agree. However, other work (Gadre et al., 2024) claims that downstream accuracy can be\npredictable in some cases and we want to consider here whether accuracy is predictable in our data with\nmethods similar to those presented in the main text."}, {"title": "A.2 Test-to-error", "content": "For similar reasons, we also found it difficult to fit loss-to-error maps from the downstream CE loss to the\nclassification error. However, while the exact functional form of the dependence is unclear, there is useful\ninformation in the loss-to-error plots in Figure 12. Importantly, there is convergence across pre-training\ndistributions where irrespective of the pre-training distribution there is a relatively consistent relationship\nbetween downstream CE loss and classification error. This is markedly different from the patter we see when\nlooking at train loss where each pre-training dataset yields a different relationship between train and any test\nloss or error."}, {"title": "B Train-to-test downstream", "content": null}, {"title": "C Additional test-to-test results", "content": null}, {"title": "D Scaling law fits", "content": "We follow the methodology from Hoffmann et al. (2022); Besiroglu et al. (2024) for fitting scaling law curves\nand illustrate fits for both Equation (4) and Equation (1)."}, {"title": "E Hyperparameters", "content": null}, {"title": "F Full loss-to-loss parameter fits from Figure 1", "content": null}, {"title": "G Comment on theoretical implications", "content": "There is now a growing body of literature on the theory of loss scaling in large neural networks (see, e.g.,\nBahri et al. (2024); Lin et al. (2024); Sharma and Kaplan (2022); Maloney et al. (2022); Canatar et al.\n(2021); Dohmatob et al. (2024); Hutter (2021); Wei et al. (2022); Michaud et al. (2023); Jain et al. (2024);\nBordelon et al. (2020); Atanasov et al. (2024); Nam et al. (2024); Bordelon et al. (2024b); Paquette et al.\n(2024) and references therein). For example, Lin et al. (2024) derives an expression for the loss scaling at\nfinite model size and dataset size in a sketched linear model and single-pass SGD setting. Bahri et al. (2024)\nand Atanasov et al. (2024) considered a similar problem in an analogous student-teacher network setting, but\nin the asymptotic regimes where either the dataset size or model size was taken to infinity.\nHowever, there is comparatively less theoretical work on understanding the effects of the data distribution on\nthe scaling laws, and on disentangling the two different types of scaling laws in Equation (1) and Equation (4).\nThis is partially because in the asymptotic regime when N \u2192 \u221e or D \u2192 \u221e, both forms given rise to the\nsame scaling in the other variable and because empirically both result in \"reasonable\" fits to the data. Works\nlike Lin et al. (2024) derive bounds which include cross terms involving both N and D, but it remains unclear\nif these cross terms can be interpreted as those coming from the polynomial form of Equation (4).\nIn this work, we use the scaling law in Equation (4) since it yields valid scaling law translations (though\nour results do not necessarily rule out other parametrizations). This leads us to ask if existing theoretical\nmodels prefer the functional form of Equation (4) versus, e.g., Equation (1). In this section, we consider this\nquestion in a simple linear model that has been considered in many previous works to theorize about scaling\nlaws (Bordelon et al., 2020; Maloney et al., 2022; Lin et al., 2024). Our goal here is not to derive a novel\nresult, but rather to show that a simplified version of the train-to-train (in-domain) loss transfer emerges\nin the existing theory, and that the scaling law is qualitatively described by an equation that is roughly\nanalogous to Equation (2). However, we defer analysis of the train-to-test and test-to-test setting for future\nwork, which, to the best of our knowledge, is not captured by any theoretical model studied in the literature."}, {"title": "G.1 Generalized linear model", "content": "As in Canatar et al. (2021); Spigler et al. (2019); Cui et al. (2021); Maloney et al. (2022), we consider data\nxi that has M features whose covariance has a spectrum that exhibits the empirically-motivated power-law\nbehavior\n\n$\\lambda_{i} = \\frac{1}{i^{\\beta+1}}, i: 1,..., M.$"}]}