{"title": "The Devil is in the Prompts: De-Identification Traces Enhance Memorization Risks in Synthetic Chest X-Ray Generation", "authors": ["Raman Dutt"], "abstract": "Generative models, particularly text-to-image (T2I) diffusion models, play a crucial role in medical image analysis. However, these models are prone to training data memorization, posing significant risks to patient privacy. Synthetic chest X-ray generation is one of the most common applications in medical image analysis with the MIMIC-CXR dataset serving as the primary data repository for this task. This study presents the first systematic attempt to identify prompts and text tokens in MIMIC-CXR that contribute the most to training data memorization. Our analysis reveals two unexpected findings: (1) prompts containing traces of de-identification procedures (markers introduced to hide Protected Health Information) are the most memorized, and (2) among all tokens, de-identification markers contribute the most towards memorization. This highlights a broader issue with the standard anonymization practices and T2I synthesis with MIMIC-CXR. To exacerbate, existing inference-time memorization mitigation strategies are ineffective and fail to sufficiently reduce the model's reliance on memorized text tokens. On this front, we propose actionable strategies for different stakeholders to enhance privacy and improve the reliability of generative models in medical imaging. Finally, our results provide a foundation for future work on developing and benchmarking memorization mitigation techniques for synthetic chest X-ray generation using the MIMIC-CXR dataset. The anonymized code is available here.", "sections": [{"title": "Introduction", "content": "High-quality data, often regarded as the \"new gold\"\u00b9, is vital in medical image analysis where large-scale datasets are scarce, hindering clinically viable AI development [6]. Diffusion models [24,11] have proven effective in producing novel, high-fidelity data. In medical imaging, they address data scarcity while mitigating privacy, ethical, and legal challenges in data sharing [28,15]. Their efficacy is"}, {"title": "Related Work", "content": "Memorization in Generative Models: Deep generative models have been shown to exhibit various forms of memorization, including training data extraction [3], content replication [22], and data copying [23]. In the medical domain, [1] found that diffusion models tend to memorize significantly more than GANS [10]. Additionally, [5] emphasized the need for robust mitigation strategies, highlighting the notable memorization in 3D Latent Diffusion Models (LDMs).\nMitigation Mechanisms: Several mechanisms have been developed to mitigate memorization. [23] introduced training and inference-time approaches, such"}, {"title": "Preliminaries", "content": "Diffusion Models\nDiffusion models consist of two phases: forward and reverse diffusion. In the forward process, a data sample is gradually corrupted over T steps by adding Gaussian noise according to a fixed Markov chain. At each step, the noise is injected as:\n$q(x_t | x_{t-1}) = N(x_t; \\sqrt{1 - \\beta_t}x_{t-1}, \\beta_tI)$,\nwhich leads to the closed-form expression\n$x_t = \\sqrt{\\bar{a}_t}x_0 + \\sqrt{1 - \\bar{a}_t}\\epsilon$,\nwhere $\\bar{a}_t = \\prod_{i=1}^{t}(1 - \\beta_i)$\nIn the reverse process, one begins with a sample $x_T \\sim N(0,1)$ and iteratively denoises it to recover $x_0$. At each step, a learned noise estimator $\\epsilon_\\theta(x_t)$ predicts and subtracts the noise, updating the state as\n$x_{t-1} = \\sqrt{a_{t-1}} \\frac{1}{\\sqrt{\\bar{a}_t}} (x_t) + \\sqrt{1 - a_{t-1}}\\epsilon_\\theta(x_t)$,\nwhere $\\epsilon_\\theta$ represents the intermediate estimate of $x_0$.\nEfficient Memorization Detection via Text-Conditional Noise\nA standard T2I stable-diffusion pipeline consists of a text encoder $T_e$, a variational autoencoder (VAE) $V_E$, and a noise predictor (U-Net). As noted in [27], for non-memorized prompts, the generated images are primarily influenced by the initial noise. In such cases, the model follows a denoising track influenced by both the initial noise and text-conditioning. However, for memorized prompts, the model overfits to a fixed denoising track, making the generated image largely independent of the initial noise. In this scenario, the model's predictions become predominantly reliant on text-conditioning.\nThis phenomenon is demonstrated in Fig 1. For a prompt that has been identified as \u201cmemorized\u201d, the generations across multiple seeds show a striking resemblance to one another, indicating independence on the initial noise"}, {"title": "Experiments", "content": "Experimental Setup. A reliable memorization signal necessitates an in-domain latent diffusion model capable of generating high-quality chest X-rays. For this task, we employ the off-the-shelf RadEdit model [16], which integrates a biomedical text encoder [2] and the VAE from SDXL [17]. This model is particularly well-suited to our setup as it includes the MIMIC-CXR dataset in its training corpus. For detecting memorization in prompts, we employ the framework from [27] (Sec 3.2) due to its reliability and efficiency."}, {"title": "Detecting Memorized Prompts in MIMIC-CXR", "content": "Setup: To identify all memorized prompts in the MIMIC-CXR dataset, we begin by extracting the subset of all unique prompts. Using a text-to-image pipeline comprising a pre-trained denoising U-Net ($\\epsilon_\\theta$), a text-encoder ($T_E$) and a VAE ($V_E$), we track and store the text-conditional noise for each unique prompt at every denoising timestep. Finally, we compute the average text-conditional noise across all timesteps to quantify memorization. This gives us a memorization score ($d_{mem}$) for each unique prompt in the dataset.\nResults: Figure 2 illustrates the distribution of the memorization scores for all unique prompts, sorted in descending order for visual clarity. The distribution follows a heavy-tailed pattern, with a small subset of prompts (on the left) exhibiting significantly higher norms, indicating a stronger contribution to memorization. The prompts corresponding to the top 1 percentile of norm values,"}, {"title": "Examining Individual Token Contribution: Traces of De-Identification Enhance Memorization", "content": "Token-Level Analysis: Building on the prompt-level analysis in Section 4.1, we extend our investigation to the token-level. Specifically, we focus on the set of memorized prompts and analyze the contribution of individual tokens toward memorization.\nResults: Our findings consistently show that within memorized prompts, the de-identification marker is the token contributing most significantly to memorization, as illustrated in Figures 3. We hypothesize two key reasons for this phenomenon: (1) The de-identification marker is a distinct and unique token, differing from all other tokens in the MIMIC-CXR text corpus. (2) It appears frequently across the dataset, occurring in 21,373 unique prompts. This high frequency allows the model to learn spurious correlations, leading to the memorization of specific samples. This finding is particularly concerning as de-identification is a standard practice before publicly releasing medical datasets. Our results highlight the need to reassess current de-identification methodologies to prevent unintended memorization in generative models."}, {"title": "Existing Intervention Methods are Ineffective", "content": "In this section, we investigate whether applying memorization mitigation strategies to de-identification traces can effectively reduce memorization. Specifically, we evaluate different inference-time mitigation techniques [23]: (1) Random Word Addition (RWA), where de-identification markers are replaced with random words; (2) Random Number Addition (RNA), where markers are substituted with random numbers; and (3) the complete removal of de-identification markers from the prompt.\nResults: We assess memorization by analyzing multiple generations across different initialization seeds for the same prompt. Memorization is qualitatively indicated by the similarity among generated images. For a quantitative evaluation, we compute the mean L2 distance between 50 generated samples using the same prompt, where a lower L2 distance signifies stronger memorization. Across all mitigation strategies, we observe that the model continues to generate visually similar images. Simply replacing de-identification markers with a random word or number, or even removing them entirely, remains ineffective. Quantitative analysis reinforces this observation. The average L2 distance over"}, {"title": "Discussion and Conclusion", "content": "This section examines potential factors through which de-identification practices may inadvertently heighten the risks of memorization and compromise privacy preservation. We also offer recommendations for medical AI researchers involved in dataset curation, pre-processing, and the training of T2I models with a focus on mitigating memorization.\nWhy Do de-identification Markers Lead to Memorization? The text corpus in MIMIC-CXR exhibits a distinct lexical structure, notably marked by the frequent occurrence of the de-identification token (\"___\"). Introduced during the de-identification process, this token offers no substantive information for text-to-image generation. Instead, it creates a spurious correlation with the corresponding images. As a result, such highly specific tokens can serve as retrieval keys, allowing for the extraction of particular data points that appear as repeated, replicated generations, indicating memorization, as shown in [23].\nRecommendations for Enhancing Privacy Preservation: We propose several actionable strategies for different stakeholders.\nDataset curators should refrain from using a uniform de-identification marker across the entire dataset. By employing a rule-based de-identification approach as in [12], curators can randomize the marker symbols. This method not only enhances the diversity of captions that can mitigate memorization [23] but also helps to minimize the risk of establishing spurious correlations between specific tokens and images.\nModel developers tasked with training T2I models should invest additional effort in pre-processing dataset captions. For example, recaptioning datasets to eliminate redundant tokens can enhance both the quality and diversity of the captions. Additionally, employing an in-domain vision-language model (VLM) [14] can refine the language and augment the information density of the captions. This strategy is expected to improve caption diversity and boost generative performance [21].\nIn summary, our work tackles the challenges of memorization and privacy preservation. By focusing on MIMIC-CXR, the most widely used dataset for T2I generation of chest X-rays, we reveal a critical flaw in the conventional de-identification procedure employed in medical datasets, establishing a clear connection to memorization. Moreover, we demonstrate that removing memorization from trained models is a complex task, with standard mitigation techniques falling short. To address this issue at its source, we offer targeted recommendations for various stakeholders. Finally, we release a list of memorized prompts to support future benchmarking and the development of more effective mitigation strategies."}]}