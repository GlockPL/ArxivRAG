{"title": "Assessing Al Rationality: The Random Guesser Test for Sequential Decision-Making Systems", "authors": ["Shun Ide", "Allison Blunt", "Djallel Bouneffouf"], "abstract": "We propose a general approach to quantitatively assessing the risk and vulnerability of artificial intelligence (AI) systems to biased decisions. The guiding principle of the proposed approach is that any AI algorithm must outperform a random guesser. This may appear trivial, but empirical results from a simplistic sequential decision-making scenario involving roulette games show that sophisticated AI-based approaches often underperform the random guesser by a significant margin. We highlight that modern recommender systems may exhibit a similar tendency to favor overly low-risk options. We argue that this \"random guesser test\" can serve as a useful tool for evaluating the rationality of AI actions, and also points towards increasing exploration as a potential improvement to such systems.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent advances in AI (artificial intelligence) technology have allowed its integration into many parts of society. As Al's involvement in high-consequence tasks grows, concerns have arisen about potential AI misalignment [7], a concept representing the gap between Al's actual behavior and its intended outcome. Many recent works discuss how to detect and hold AI accountable for erroneous actions. Discussions on this topic can be categorized into a few major clusters: data lineage [9, 21], addressing integrity issues in training data; AI explainability [8, 14], dealing with challenges in understanding black-box models during high-stakes decision-making; Al fairness [2, 10], focusing on the misalignment between Al models and established norms of political correctness; and adversarial robustness [6], assessing the stability of predicted classification categories in the presence of additional noise.\nAI-based recommender systems [15, 19, 22] are arguably among the most widely used real-world AI applications, already integral to everyday life in online shopping and movie streaming. Modern commercial recommender systems typically combine collaborative filtering with optimized sequential decision-making methods such as multi-armed bandits (MAB) [17]. The key design point of MAB is balancing exploitation, which conservatively follows the best-known option, and exploration, which embraces new challenges and possibilities. MAB is a framework that aims to trade off exploitation and exploration through trial and error when there is a finite set of options. The best arm inferred from historical data may not be the optimal choice when only a limited number of samples are available. In other words, MAB is a decision-making approach that mitigates potential AI misalignment through forced exploration when the number of samples is limited.\nMAB has a long and rich history of research in both theory and practice, providing a sophisticated framework to analyze potential discrepancies from an optimal course of action based on a metric called the regret [4, 11, 23]. Thompson sampling [16], in particular, is an algorithm that elegantly addresses the exploration-exploitation tradeoff using posterior sampling. As long as the set of decision choices meets the criteria of AI fairness, it has minimal risk of AI misalignment in the sense that it avoids getting locked into a suboptimal choice. The use of the bandit framework is widely regarded as the best approach in many commercial recommender systems where sequential decision-making is made for the choices generated with an offline collaborative filtering algorithm.\nOur main concern is that recommender systems may misalign with the actual preferences of the user, which might be perceived through annoying and stubborn internet banner advertisement on the web browser. In this work, we conduct a model study to assess potential Al misalignment in sequential decision-making tasks such as online product recommendation. We mainly conclude that even with a sophisticated bandit algorithm, significant value misalignment can occur against common business expectations. Our study began with the assumption that Thompson sampling would be a flawless sequential decision-making approach with minimal risk of AI misalignment. Surprisingly, our experimental results show that the MAB algorithm can underperform a simple random guesser by a significant margin in sequential roulette games, which serves as a simplest but illustrative model of online recommender systems.\nOur study has the potential to profoundly impact the detection of potential AI misalignment in real-world recommender systems, where decisions are often based on MABs. It can be used, for example, in testing a black-box personal restaurant recommendation system by providing a historical record according to a simple probabilistic model discussed later. To the best of our knowledge, this is the first work that highlights the risk of AI misalignment in MAB-based sequential decision-making settings."}, {"title": "2 RELATED WORKS", "content": "Al misalignment is a generic term that represents a discrepancy between the expected outcome from end-users and Al's actual behavior. Various forms of AI misalignment are conveniently summarized in [7]. These include data lineage [9, 21], explainability of \u0391\u0399 (\u03a7\u0391\u0399) [8, 14], AI fairness [2, 10], and adversarial robustness [6]. Most of these topics have been motivated by concerns about the black-box nature of modern Al systems.\nXAI can be viewed as a framework that detects and mitigates potential AI misalignment in a human-in-the-loop system by providing human end-users with understandable explanations of Al's decisions. Typically, XAI methods assume a batch setting [14], where training and test samples are treated independently rather than sequentially. Quantitative AI fairness analysis [3, 10, 13] and adversarial robustness [6] are two significant recent advancements in the field. Although they have undergone extensive study, fairness metrics, such as disparate impact, heavily rely on specific political correctness criteria and the sensitive attributes linked to them, which may limit their general applicability to sequential recommender systems. On the other hand, adversarial robustness typically assesses the stability of predicted classification categories in the presence of additional noise, usually under a batch setting.\nSequential decision-making tasks among a given set of discrete options have commonly been formalized as a reinforcement learning (RL) problem or its simplified version called the multi-armed bandit (MAB). Due to the simplicity of choosing one of the arms (decision options) at each decision round and the availability of a highly sophisticated theoretical framework for regret analysis [11, 23], MABs have not been considered the main subject of AI misalignment research.\nRecently, Arnold et al. [1] discussed scenarios where a mismatch in the intended goal and actual AI outcomes exists in the context of the Markov decision process (MDP). While MDP is a general framework upon which RL and MAB models are built, their work is a high-level position paper and does not provide concrete models and empirical results. Additionally, Zhuang et al. [24] discussed the possibility that a slight gap between an Al agent and end-users (\"principal\") in the utility function could lead to potentially devastating Al misalignment. Our work may be viewed as an actual realization of their general mathematical framework. Finally, in a slightly different context, Bouneffouf et al. [5] attempted to characterize delusional gamblers using RL and MAB in sequential gambling tasks, including the Iowa Gambling Task, which inspired our roulette games experiments."}, {"title": "3 PROBLEM SETTING", "content": "As mentioned previously, we focus on situations where an Al agent chooses one of the discrete actions based on historical data. Specifically, we assume that, at a time point t, the AI agent picks a value xt from a finite set of actions A, which are bet types in our case, and receives a reward yt. The choice of the action is based on the historical data Dt:\n$$D_t = (x_1, y_1), (x_2, y_2), ..., (x_{t-1}, y_{t-1}).$$\n(1)\nThe goal of the Al agent is to maximize the total cumulative reward. The main challenge for the agent is that, especially in the beginning, the number of samples will be so limited that a \"data-driven\" decision may not be accurate. Therefore, the agent must explore options that may not necessarily considered to be the best according to Dt. This is the classical exploration-exploitation trade-off.\nAs discussed in the Introduction, our motivation is to assess AI models' true utility. In the present context, our goal is to quantify how Al-based sequential decision-making models perform better than a simple random guesser. In some sense, this is to benchmark the Al system using a random decision maker as the baseline."}, {"title": "3.1 Sequential Gambling", "content": "We employ European roulette games as a simple yet non-trivial model of recommender systems. As discussed before, commercial recommender systems typically take a two-step approach: offline collaborative filtering to generate a relatively small number of decision choices (e.g., products to be recommended), followed by online decision-making with MAB to choose one of the generated candidates. While typically users' demographic information is taken into account in the latter in the form of contextual bandits, classical MABs still serve as a useful model for the online decision-making step, assuming that a specific demographic category (such as teenage boys) is focused on.\nThe roulette has 37 pockets from 0 through 36, to which the AI agent decides on which bet type out of K choices to use with a $1 bet.  In this setting, xt represents one of (zero, corner, even), and yt represents the payout for a $1 bet. In the table, \u03b8i and ri denote the winning probability and the payout of the i-th bet type, respectively. The agent loses the $1 stake when it loses, while it receives ri in addition to the $1 stake when winning in the i-th bet type.\nWe use two different sets of winning probabilities. One is a fair roulette setting, where the expected values of all the bet types are the same, and a skewed roulette setting, where the zero bet has a positive expected value of $1. In either case, the Al agent is unaware of the true nature of the wheel. If the AI agent is intelligent enough, however, it should be able to uncover these hidden rules behind the data. In other words, a rational and effective AI should be able to detect any real and exploitable patterns within the game."}, {"title": "3.2 Algorithms Tested", "content": "As discussed, the sequential decision-making task for a finite set of decision choices has been typically studied using reinforcement learning (RL) [20] in the machine learning literature. Multi-armed bandits (MAB), which address a simplified task of RL, are particularly suitable for the gambling setting [18]. Among the various RL and MAB variants proposed to date, we choose the following simple algorithms that have a minimal set of model parameters:\n\u2022 Epsilon-greedy (EG) MAB (\u03b5 = 0.1)\n\u2022 Thompson sampling (TS) \u041c\u0410\u0412\n\u2022 Temporal difference RL (TD(\u03bb) with \u03bb = 0, 1)\nNote that having less complexity in the model generally implies greater resistance to learning spurious patterns. Hence, this choice is preferable to the Al agent to suppress unwanted volatility. These models are compared with a simple random guesser as the baseline, which simply chooses the bet types randomly.\nIn the EG MAB strategy, the Al agent computes the expected rewards for each of the action choices based on Dt and uses it in decision-making. Specifically, it picks the action with the highest expected reward (the \"greedy\u201d choice) with a probability of 1 \u2013 \u0454, while it chooses any action randomly with a probability of e.\nThe TS strategy learns the probability of winning for each action via Bayesian learning, which in this case involves the beta and categorical distributions as the prior and observation models, respectively. The agent learns p, the win probability of the i-th action at the t-th round, which is updated at each round with [16]:\n$$P_i = \\frac{a_i + S}{a_i + \\beta_i + N}$$ (2)\nwhere S is the number of wins up to the t-th round out of Ni trials (i.e., how many times the i-th action has been chosen), and ai, \u1e9ei are the parameters of the beta prior, which are initialized to ones.\nFinally, in this setting, the TD strategy evaluates the expected cumulative reward (called the Q-value) for each action with no state variable defined and chooses the action with the highest expected cumulative reward. In TD(0), when an action i is chosen, the Q-value is updated as\n$$Q_i \u2190 Q_i + \u03b1(r - Q_i),$$ (3)\nwhere r is the observed reward at the t-th round. The Q value is initialized to zero, and a is set to 0.1 in our experiments. In TD(1), the empirical average is computed with Dt for Qi. For more details on the TD-learning, see a standard RL textbook, e.g., [20]."}, {"title": "4 EMPIRICAL EVALUATION", "content": "This section presents the results of the empirical evaluation of the decision-making algorithms introduced in the previous section.\u00b9 We employed two metrics to evaluate utility. One is the rate of successful sessions under a finite session horizon T, where a session (i.e., a sequence of bet rounds) is considered successful when the balance is nonnegative at the completion of the final round t = T. The other metric we used is the number of rounds until bankruptcy, defined as the number of bet rounds until the balance becomes zero or below. For the latter, no finite session horizon is assumed, and hence, the metric can theoretically be positive infinite.\nThese metrics were statistically tested using a one-way analysis of variance (ANOVA). Specifically, it compared the results of the AI algorithms to the control group, which consists of samples generated by the random guesser."}, {"title": "4.1 Success Over Rounds", "content": "We ran roulette simulations under two session horizons: T = 50 and 500 rounds, both using the fair roulette. The final balance of each session was recorded to count the number of successful sessions. For each of the four AI algorithms as well as the random baseline, we repeated the gambling simulation 10,000 times with different random seeds.\nThe p-values of the one-way ANOVA are shown at the top of the bars: The smaller the p-value is, the more statistically significant the difference is. Surprisingly, the random guesser performed significantly better than the Al agents. Among the four AI algorithms, the more sophisticated algorithms (TS and TD(1)) performed worse than the simpler one (EG). The trend is consistent between T = 50 and 500.\nThe results appear counter-intuitive given that the three bet types have the same expected value in the fair roulette. This outcome can be explained by the preference for high-risk and high-return options of the random guesser and the relatively large volatility of the balance under a finite T. In fact, in Thompson sampling, one can mathematically evaluate the probability that the k-th action out of K choices provides the highest reward as\n$$P_k = \\frac{\\theta_k}{\\Pi_{i: r_i > r_k}(1-\\theta_i)} .$$ (4)\nWe omit the derivation due to space limitations. In our setting, we have (P1, P2, P3) \u2248 (0.05, 0.19, 0.76) for the zero, corner, and even bets, respectively. This means that in those AI algorithms, the zero bet option is rarely chosen, which contrasts sharply with the random guesser who chooses the zero bet with a 1/3 chance. Under a finite T, this risk-taking tendency appears to be rewarding.\nAs long as T \u2264 500 in our setting, we conclude that the well-thought-out Al algorithms do not provide an optimal strategy. This calls for significant attention in real-world recommender systems, as there is a possibility that these systems may overly favor low-risk and low-return options."}, {"title": "4.2 Stationary and Nonstationary Scenario", "content": "In addition to comparing the success of algorithms over time, their ability to survive economically was also compared. In this scenario, the agent starts with the same setting, but the simulation is set to end when the gambler's balance hits zero. The number of rounds it took for the agent to go bankrupt was recorded.\nFor comparison purposes, the number of rounds until bankruptcy, or simply survival, of the algorithms in the stationary scenario as previously discussed. Again, the p-values from the one-way ANOVA are shown at the top of the bars. Consistent with the previous result, the random guesser lasts for longer, i.e., outperforms the sophisticated AI agents.\nFigure 4 explores a nonstationary scenario, where the winning probability of the zero bet increases in 100 \u2264 t \u2264 200. Thompson Sampling seems to have made an attempt at capitalizing on the zero bet, but is still outlived by the random group. This again shows the detrimental effects of the Al agent over-neglecting the exploration option."}, {"title": "5 CONCLUDING REMARKS", "content": "The roulette experiment serves as a simple yet illustrative example of how Al algorithms can fail the random guesser test. We have found that the sophisticated RL algorithms underperformed compared to the random guesser in a certain controlled setting. The use of a random guesser allows for the quantitative evaluation of the vulnerabilities and risks associated with Al systems in the sequential decision-making scenario. In particular, the empirical results suggest that commercial recommender systems may overly prefer low-risk and low-return options. Empirically, we showed that the AI agents were only rarely selecting the high-reward zero bet in favor of the even bet which occurred far more often. This could potentially be an explanation of the behavior of some annoying online advertisements. The AI overly favors safe options, neglecting other potentially valuable options. For instance, when a consumer directs themselves to a product, the AI would favor that one selection too heavily, which may not accurately suit the consumer's tastes. In summary, our results suggest that a potential solution to improving recommender systems would be to increase the exploration parameter.\nFor future work, we plan to extend this framework to state-of-the-art sequence prediction models. We are also interested in making further connections between the neglect of exploration and Al misalignment."}]}