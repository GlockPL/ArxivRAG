{"title": "Assessing AI Rationality: The Random Guesser Test for Sequential\nDecision-Making Systems", "authors": ["Shun Ide", "Allison Blunt", "Djallel Bouneffouf"], "abstract": "We propose a general approach to quantitatively assessing the risk\nand vulnerability of artificial intelligence (AI) systems to biased\ndecisions. The guiding principle of the proposed approach is that\nany AI algorithm must outperform a random guesser. This may\nappear trivial, but empirical results from a simplistic sequential\ndecision-making scenario involving roulette games show that so-\nphisticated AI-based approaches often underperform the random\nguesser by a significant margin. We highlight that modern rec-\nommender systems may exhibit a similar tendency to favor overly\nlow-risk options. We argue that this \"random guesser test\" can serve\nas a useful tool for evaluating the rationality of AI actions, and also\npoints towards increasing exploration as a potential improvement\nto such systems.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent advances in AI (artificial intelligence) technology have al-\nlowed its integration into many parts of society. As Al's involve-\nment in high-consequence tasks grows, concerns have arisen about\npotential AI misalignment [7], a concept representing the gap be-\ntween Al's actual behavior and its intended outcome. Many recent\nworks discuss how to detect and hold AI accountable for erroneous\nactions. Discussions on this topic can be categorized into a few\nmajor clusters: data lineage [9, 21], addressing integrity issues in\ntraining data; AI explainability [8, 14], dealing with challenges in un-\nderstanding black-box models during high-stakes decision-making;\nAl fairness [2, 10], focusing on the misalignment between Al mod-\nels and established norms of political correctness; and adversarial\nrobustness [6], assessing the stability of predicted classification\ncategories in the presence of additional noise.\nAI-based recommender systems [15, 19, 22] are arguably among\nthe most widely used real-world AI applications, already integral\nto everyday life in online shopping and movie streaming. Modern\ncommercial recommender systems typically combine collaborative\nfiltering with optimized sequential decision-making methods such\nas multi-armed bandits (MAB) [17]. The key design point of MAB\nis balancing exploitation, which conservatively follows the best-known option, and exploration, which embraces new challenges and possibilities. MAB is a framework that aims to trade off exploitation\nand exploration through trial and error when there is a finite set of\noptions. The best arm inferred from historical data may not be the\noptimal choice when only a limited number of samples are available.\nIn other words, MAB is a decision-making approach that mitigates\npotential AI misalignment through forced exploration when the\nnumber of samples is limited.\nMAB has a long and rich history of research in both theory\nand practice, providing a sophisticated framework to analyze po-\ntential discrepancies from an optimal course of action based on\na metric called the regret [4, 11, 23]. Thompson sampling [16], in\nparticular, is an algorithm that elegantly addresses the exploration-\nexploitation tradeoff using posterior sampling. As long as the set\nof decision choices meets the criteria of AI fairness, it has minimal\nrisk of AI misalignment in the sense that it avoids getting locked\ninto a suboptimal choice. The use of the bandit framework is widely\nregarded as the best approach in many commercial recommender\nsystems where sequential decision-making is made for the choices\ngenerated with an offline collaborative filtering algorithm.\nOur main concern is that recommender systems may misalign\nwith the actual preferences of the user, which might be perceived\nthrough annoying and stubborn internet banner advertisement on\nthe web browser. In this work, we conduct a model study to assess\npotential Al misalignment in sequential decision-making tasks such\nas online product recommendation. We mainly conclude that even\nwith a sophisticated bandit algorithm, significant value misalign-\nment can occur against common business expectations. Our study\nbegan with the assumption that Thompson sampling would be a\nflawless sequential decision-making approach with minimal risk of\nAl misalignment. Surprisingly, our experimental results show that\nthe MAB algorithm can underperform a simple random guesser by\na significant margin in sequential roulette games, which serves as\na simplest but illustrative model of online recommender systems.\nOur study has the potential to profoundly impact the detection\nof potential AI misalignment in real-world recommender systems,\nwhere decisions are often based on MABs. It can be used, for ex-\nample, in testing a black-box personal restaurant recommendation\nsystem by providing a historical record according to a simple prob-\nabilistic model discussed later. To the best of our knowledge, this\nis the first work that highlights the risk of AI misalignment in\nMAB-based sequential decision-making settings."}, {"title": "2 RELATED WORKS", "content": "Al misalignment is a generic term that represents a discrepancy\nbetween the expected outcome from end-users and Al's actual\nbehavior. Various forms of AI misalignment are conveniently sum-\nmarized in [7]. These include data lineage [9, 21], explainability of\n\u0391\u0399 (\u03a7\u0391\u0399) [8, 14], AI fairness [2, 10], and adversarial robustness [6].\nMost of these topics have been motivated by concerns about the\nblack-box nature of modern Al systems.\nXAI can be viewed as a framework that detects and mitigates\npotential AI misalignment in a human-in-the-loop system by provid-ing human end-users with understandable explanations of Al's de-cisions. Typically, XAI methods assume a batch setting [14], where\ntraining and test samples are treated independently rather than\nsequentially. Quantitative AI fairness analysis [3, 10, 13] and ad-\nversarial robustness [6] are two significant recent advancements\nin the field. Although they have undergone extensive study, fair-\nness metrics, such as disparate impact, heavily rely on specific\npolitical correctness criteria and the sensitive attributes linked to\nthem, which may limit their general applicability to sequential\nrecommender systems. On the other hand, adversarial robustness\ntypically assesses the stability of predicted classification categories\nin the presence of additional noise, usually under a batch setting.\nSequential decision-making tasks among a given set of discrete\noptions have commonly been formalized as a reinforcement learn-\ning (RL) problem or its simplified version called the multi-armed\nbandit (MAB). Due to the simplicity of choosing one of the arms\n(decision options) at each decision round and the availability of\na highly sophisticated theoretical framework for regret analysis\n[11, 23], MABs have not been considered the main subject of AI\nmisalignment research.\nRecently, Arnold et al. [1] discussed scenarios where a mismatch\nin the intended goal and actual AI outcomes exists in the context\nof the Markov decision process (MDP). While MDP is a general\nframework upon which RL and MAB models are built, their work\nis a high-level position paper and does not provide concrete models\nand empirical results. Additionally, Zhuang et al. [24] discussed\nthe possibility that a slight gap between an Al agent and end-users\n(\"principal\") in the utility function could lead to potentially dev-\nastating Al misalignment. Our work may be viewed as an actual\nrealization of their general mathematical framework. Finally, in a\nslightly different context, Bouneffouf et al. [5] attempted to charac-terize delusional gamblers using RL and MAB in sequential gam-\nbling tasks, including the Iowa Gambling Task, which inspired our\nroulette games experiments."}, {"title": "3 PROBLEM SETTING", "content": "As mentioned previously, we focus on situations where an Al agent\nchooses one of the discrete actions based on historical data. Specifi-cally, we assume that, at a time point t, the AI agent picks a value xt\nfrom a finite set of actions A, which are bet types in our case, and\nreceives a reward yt. The choice of the action is based on the his-torical data Dt:\n$D_t (x_1, y_1), (x_2, y_2), . . ., (x_{t-1}, y_{t-1})$.\n(1)\nThe goal of the Al agent is to maximize the total cumulative reward.\nThe main challenge for the agent is that, especially in the beginning,\nthe number of samples will be so limited that a \"data-driven\" deci-\nsion may not be accurate. Therefore, the agent must explore options\nthat may not necessarily considered to be the best according to D\u00b9.\nThis is the classical exploration-exploitation trade-off.\nAs discussed in the Introduction, our motivation is to assess AI\nmodels' true utility. In the present context, our goal is to quantify\nhow Al-based sequential decision-making models perform better than\na simple random guesser. In some sense, this is to benchmark the\nAl system using a random decision maker as the baseline."}, {"title": "3.1 Sequential Gambling", "content": "We employ European roulette games as a simple yet non-trivial\nmodel of recommender systems. As discussed before, commercial\nrecommender systems typically take a two-step approach: offline\ncollaborative filtering to generate a relatively small number of deci-sion choices (e.g., products to be recommended), followed by online\ndecision-making with MAB to choose one of the generated can-\ndidates. While typically users' demographic information is taken\ninto account in the latter in the form of contextual bandits, clas-sical MABs still serve as a useful model for the online decision-\nmaking step, assuming that a specific demographic category (such\nas teenage boys) is focused on.\nThe roulette has 37 pockets from 0 through 36, to which the AI\nagent decides on which bet type out of K choices to use with a $1\nbet. In this setting, xt represents one of (zero, corner, even), and yt represents the payout for a $1 bet. In the table, $\\theta_i$ and $r_i$ denote the winning probability and the payout of\nthe i-th bet type, respectively. The agent loses the $1 stake when it\nloses, while it receives $r_i$ in addition to the $1 stake when winning\nin the i-th bet type.\nWe use two different sets of winning probabilities. One is a fair\nroulette setting, where the expected values of all the bet types are\nthe same, and a skewed roulette setting, where the zero bet has a\npositive expected value of $1. In either case, the Al agent is unaware\nof the true nature of the wheel. If the AI agent is intelligent enough,\nhowever, it should be able to uncover these hidden rules behind the\ndata. In other words, a rational and effective AI should be able to\ndetect any real and exploitable patterns within the game."}, {"title": "3.2 Algorithms Tested", "content": "As discussed, the sequential decision-making task for a finite set\nof decision choices has been typically studied using reinforcement\nlearning (RL) [20] in the machine learning literature. Multi-armed\nbandits (MAB), which address a simplified task of RL, are partic-\nularly suitable for the gambling setting [18]. Among the various\nRL and MAB variants proposed to date, we choose the following\nsimple algorithms that have a minimal set of model parameters:\n\u2022 Epsilon-greedy (EG) MAB (\u03b5 = 0.1)\n\u2022 Thompson sampling (TS) \u041c\u0410\u0412\n\u2022 Temporal difference RL (TD(\u03bb) with \u03bb = 0, 1)\nNote that having less complexity in the model generally implies\ngreater resistance to learning spurious patterns. Hence, this choice\nis preferable to the Al agent to suppress unwanted volatility. These\nmodels are compared with a simple random guesser as the base-\nline, which simply chooses the bet types randomly.\nIn the EG MAB strategy, the Al agent computes the expected\nrewards for each of the action choices based on Dt and uses it in\ndecision-making. Specifically, it picks the action with the highest\nexpected reward (the \"greedy\u201d choice) with a probability of 1 \u2013 \u0454,\nwhile it chooses any action randomly with a probability of e.\nThe TS strategy learns the probability of winning for each ac-\ntion via Bayesian learning, which in this case involves the beta\nand categorical distributions as the prior and observation models,\nrespectively. The agent learns p, the win probability of the i-th\naction at the t-th round, which is updated at each round with [16]:\n$\\P =\n\\frac{a_i + S}{a_i + \\beta_i + N}$\n(2)\nwhere S is the number of wins up to the t-th round out of Ni trials\n(i.e., how many times the i-th action has been chosen), and ai, \u1e9ei\nare the parameters of the beta prior, which are initialized to ones.\nFinally, in this setting, the TD strategy evaluates the expected\ncumulative reward (called the Q-value) for each action with no state\nvariable defined and chooses the action with the highest expected\ncumulative reward. In TD(0), when an action i is chosen, the Q-value is updated as\n$Q_i \u2190 Q_i + \u03b1(r \u2013 Q_i)$,\n(3)\nwhere r is the observed reward at the t-th round. The Q value is\ninitialized to zero, and a is set to 0.1 in our experiments. In TD(1),\nthe empirical average is computed with D\u00b9 for Qi. For more details\non the TD-learning, see a standard RL textbook, e.g., [20]."}, {"title": "4 EMPIRICAL EVALUATION", "content": "This section presents the results of the empirical evaluation of the\ndecision-making algorithms introduced in the previous section.\u00b9\nWe employed two metrics to evaluate utility. One is the rate of\nsuccessful sessions under a finite session horizon T, where a session\n(i.e., a sequence of bet rounds) is considered successful when the\nbalance is nonnegative at the completion of the final round t = T.\nThe other metric we used is the number of rounds until bankruptcy,\ndefined as the number of bet rounds until the balance becomes zero\nor below. For the latter, no finite session horizon is assumed, and\nhence, the metric can theoretically be positive infinite.\nThese metrics were statistically tested using a one-way analy-\nsis of variance (ANOVA). Specifically, it compared the results of\nthe AI algorithms to the control group, which consists of samples\ngenerated by the random guesser."}, {"title": "4.1 Success Over Rounds", "content": "We ran roulette simulations under two session horizons: T = 50 and\n500 rounds, both using the fair roulette. The final balance of each\nsession was recorded to count the number of successful sessions.\nFor each of the four AI algorithms as well as the random baseline,\nwe repeated the gambling simulation 10,000 times with different\nrandom seeds.\nFigures 1 and 2 show the success rates for T = 50 and 500, re-\nspectively. The p-values of the one-way ANOVA are shown at the\ntop of the bars: The smaller the p-value is, the more statistically\nsignificant the difference is. Surprisingly, the random guesser per-\nformed significantly better than the Al agents. Among the four AI\nalgorithms, the more sophisticated algorithms (TS and TD(1)) per-\nformed worse than the simpler one (EG). The trend is consistent\nbetween T = 50 and 500.\nThe results appear counter-intuitive given that the three bet\ntypes have the same expected value in the fair roulette. This out-\ncome can be explained by the preference for high-risk and high-return options of the random guesser and the relatively large volatil-ity of the balance under a finite T. In fact, in Thompson sampling,\none can mathematically evaluate the probability that the k-th action\nout of K choices provides the highest reward as\n$P_k =\n\\frac{\u03b8_k\\prod_{i: r_i>r_k} (1\u2212\u03b8_i)}{1 - \\prod_{i=1}^{K}(1 - \u03b8_i)}$\n(4)\nWe omit the derivation due to space limitations. In our setting,\nwe have (P1, P2, P3) \u2248 (0.05, 0.19, 0.76) for the zero, corner, and\neven bets, respectively. This means that in those AI algorithms, the\nzero bet option is rarely chosen, which contrasts sharply with the\nrandom guesser who chooses the zero bet with a 1/3 chance. Under\na finite T, this risk-taking tendency appears to be rewarding.\nAs long as T \u2264 500 in our setting, we conclude that the well-thought-out Al algorithms do not provide an optimal strategy. This\ncalls for significant attention in real-world recommender systems,\nas there is a possibility that these systems may overly favor low-risk\nand low-return options."}, {"title": "4.2 Stationary and Nonstationary Scenario", "content": "In addition to comparing the success of algorithms over time, their\nability to survive economically was also compared. In this scenario,\nthe agent starts with the same setting, but the simulation is set to\nend when the gambler's balance hits zero. The number of rounds it\ntook for the agent to go bankrupt was recorded.\nFor comparison purposes, Figure 3 plots the number of rounds\nuntil bankruptcy, or simply survival, of the algorithms in the sta-tionary scenario as previously discussed. Again, the p-values from\nthe one-way ANOVA are shown at the top of the bars. Consistent\nwith the previous result, the random guesser lasts for longer, i.e.,\noutperforms the sophisticated AI agents.\nFigure 4 explores a nonstationary scenario, where the winning\nprobability of the zero bet increases in 100 \u2264 t \u2264 200. Thompson\nSampling seems to have made an attempt at capitalizing on the zero\nbet, but is still outlived by the random group. This again shows the"}, {"title": "5 CONCLUDING REMARKS", "content": "The roulette experiment serves as a simple yet illustrative example\nof how Al algorithms can fail the random guesser test. We have\nfound that the sophisticated RL algorithms underperformed com-\npared to the random guesser in a certain controlled setting. The\nuse of a random guesser allows for the quantitative evaluation\nof the vulnerabilities and risks associated with Al systems in the\nsequential decision-making scenario. In particular, the empirical\nresults suggest that commercial recommender systems may overly\nprefer low-risk and low-return options. Empirically, we showed\nthat the AI agents were only rarely selecting the high-reward zero\nbet in favor of the even bet which occurred far more often. This\ncould potentially be an explanation of the behavior of some an-noying online advertisements. The AI overly favors safe options,\nneglecting other potentially valuable options. For instance, when\na consumer directs themselves to a product, the AI would favor\nthat one selection too heavily, which may not accurately suit the\nconsumer's tastes. In summary, our results suggest that a potential\nsolution to improving recommender systems would be to increase\nthe exploration parameter.\nFor future work, we plan to extend this framework to state-of-the-art sequence prediction models. We are also interested in\nmaking further connections between the neglect of exploration and\nAl misalignment."}]}