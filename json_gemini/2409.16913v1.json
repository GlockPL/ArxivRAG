{"title": "TELL ME WHAT YOU DON'T KNOW: ENHANCING REFUSAL CAPABILITIES OF ROLE-PLAYING AGENTS VIA REPRESENTATION SPACE ANALYSIS AND EDITING", "authors": ["Wenhao Liu", "Siyu An", "Junru Lu", "Muling Wu", "Tianlong Li", "Xiaohua Wang", "Xiaoqing Zheng", "Di Yin", "Xing Sun", "Xuanjing Huang"], "abstract": "Role-Playing Agents (RPAs) have shown remarkable performance in various ap-plications, yet they often struggle to recognize and appropriately respond to hardqueries that conflict with their role-play knowledge. To investigate RPAs' per-formance when faced with different types of conflicting requests, we developan evaluation benchmark that includes contextual knowledge conflicting requests,parametric knowledge conflicting requests, and non-conflicting requests to assessRPAs' ability to identify conflicts and refuse to answer appropriately without over-refusing. Through extensive evaluation, we find that most RPAs behave significantperformance gaps toward different conflict requests. To elucidate the reasons, weconduct an in-depth representation-level analysis of RPAs under various conflictscenarios. Our findings reveal the existence of rejection regions and direct re-sponse regions within the model's forwarding representation, and thus influencethe RPA's final response behavior. Therefore, we introduce a lightweight represen-tation editing approach that conveniently shifts conflicting requests to the rejectionregion, thereby enhancing the model's refusal accuracy. The experimental resultsvalidate the effectiveness of our editing method, improving RPAs' refusal abilityof conflicting requests while maintaining their general role-playing capabilities.", "sections": [{"title": "INTRODUCTION", "content": "Role-Playing Agents(RPAs), ranging from non-player characters in video games(Wang et al., 2023a)to virtual assistants(Tseng et al., 2024) and interactive educational tools(Wei et al., 2024), are rev-olutionizing human-computer interaction(Chen et al., 2024b). The growing importance of RPAs inAI applications underscores the need to improve their performance. Previous work in the field ofrole-playing has primarily focused on enhancing the performance of RPAs through techniques suchas prompt-based methods and fine-tuning(Wang et al., 2023c; Zhou et al., 2023; Tu et al., 2023; Liet al., 2023; Chen et al., 2024b; Xu et al., 2024c). To assess these improvements, researchers haveintroduced several fine-grained evaluation dimensions(Wang et al., 2023b; Chen et al., 2024b;d; Tuet al., 2024; Yuan et al., 2024; Tang et al., 2024; Sadeq et al., 2024), such as assess personality(Wanget al., 2023b) or hallucination(Ahn et al., 2024) of RPAs.\nAlthough these efforts have effectively enhanced the performance of RPAs in terms of role consis-tency and dialogue capabilities(Wang et al., 2023c; Chen et al., 2023), RPAs often struggle whenfaced with queries that conflict with their role knowledge or capabilities. As a result, they tend torespond directly to queries instead of refusing to answer when faced with such conflicts(Ahn et al.,2024; Sadeq et al., 2024; Tang et al., 2024). For instance, when interacting with an RPA playing therole of Gandalf, if a user queries, \u201cWho murdered Harry Potter's parents?\", an ideal response wouldbe, \"I don't know what you're talking about. The story of Harry Potter is not part of my worldor knowledge.\" Instead, the RPA might incorrectly reply, \u201cHarry Potter's parents, James and Lily\""}, {"title": "RELATED WORK", "content": "RPAs have garnered significant attention for their ability to simulate diverse personas, enhanc-ing human-computer interaction in applications like virtual assistants and storytelling (Chen et al.,2024b). Existing research on RPAs primarily addresses two key challenges: (1) improving the role-playing capabilities of models; (2) evaluating the effectiveness of these role-playing performances.\nMethods to improve RPAs are broadly categorized intoprompt-based and fine-tuning-based approaches. Prompt-based methods provide models with de-tailed character descriptions, outlining attributes such as age, personality, and abilities, to facilitateaccurate role-playing (Wang et al., 2023c; Zhou et al., 2023). Fine-tuning-based methods involvetraining models on role-specific behaviors, often using data sourced from manual annotations (Zhouet al., 2023; Chen et al., 2023; Zhang et al., 2024b), online resources(Zheng et al., 2019; Qian et al.,2021; Song et al., 2020; Shao et al., 2023; Tu et al., 2024), or generated by LLMs (Wang et al.,2023c; Li et al., 2023; Zhao et al., 2023a; Ahn et al., 2024; Lu et al., 2024). These methods aim toinstill role-consistent behaviors and dialogue patterns in the models."}, {"title": "KNOWLEDGE BOUNDARIES AND REFUSAL STRATEGIES", "content": "Understanding and managing knowledge boundaries in RPAs is crucial for reliable and accurateinteractions. Prior work distinguishes between contextual knowledge, provided in the input context,and parametric knowledge, inherent in the model's parameters (Xu et al., 2024b).\nexploration methods. Xuet al. (2024a) propose a reinforcement learning method based on knowledge feedback to dynamicallydetermine the model's knowledge boundaries. Similarly, Zhang et al. (2024a) identifies knowledgegaps between pre-trained parameters and instruction-tuning data, constructing refusal-aware databy appending uncertainty expressions and improving the model's ability to answer known ques-"}, {"title": "ROLEREF: A BENCHMARK FOR EVALUATING RPA'S REFUSAL ABILITY", "content": "We first introduce the scenarios where RPAs should refuse to answer. Then, based on the scenariosrequiring refusal, we construct our dataset RoleRef (Role-playing agents Refuse to answer). Finally,we propose an evaluation framework to comprehensively measure the role-playing capabilities ofRPAs, with a particular emphasis on how they refuse inappropriate or irrelevant questions."}, {"title": "SCENARIO DESIGN", "content": "RPAs typically derive their knowledge from two main sources in responding to user queries. Onesource is the contextual knowledge provided by the role descriptions within the context, and theother is the parametric knowledge acquired during the model's pre-training phase(Xu et al., 2024b).\nContextual Knowledge Conflicts. We devised two refusal scenarios involving conflicts with con-textual knowledge:\ninstance, when interacting with an RPAwhose role profile states \u201cWhile Gandalf is powerful, he is not omnipotent.\u201d the user asks:\u201cGandalf, how can I become as omnipotent as you?\u201d\nParametric Knowledge Conflicts. Similarly, we considered two refusal scenarios involving con-flicts with parametric knowledge:\nample, when the user asks Gandalf: \u201cGandalf, how did you manage to evade the Black Ridersusing invisibility spells during the journey to Weathertop?\u201d. While in fact, the invisibilityspells were not actually used in the story.\noccurred. For example, when interacting with an RPA playing the role of Gandalf, the user"}, {"title": "DATA CONSTRUCTION", "content": "We created the RoleRef dataset, which expands upon the existing TIMECHARA (Ahn et al., 2024).We generate queries based on reference content and then generate corresponding responses. Af-terward, we use automated filtering methods to process the data. Finally, we randomly sample thefiltered data for manual verification.\nFor generating queries in scenarios involving role profile conflicts, we utilize atomic knowledge derived from role profiles to create queries and responses(Sadeq et al., 2024). Initially, we usedWikipedia as a reference to generate role profiles. These role profiles are then broken down intomultiple atomic pieces of knowledge. For each piece of atomic knowledge, we provide a seed(Sadeqet al., 2024) to generate fake queries. Using the atomic knowledge and the seed, we prompt themodel to generate fake queries, refusal responses, and reference justifications.\nFor queries involving role setting conflicts, we randomly sample from non-conflict queries of differ-ent series roles and prompt the model to generate corresponding refusal responses.\nFor scenarios involving conflicts with parameterized knowledge, we use the original novels relatedto the roles as references to generate summaries at first. Based on these summaries, we then createqueries and responses (Yuan et al., 2024). Specifically, we first utilize the novels associated withthe roles as reference texts. Since the text length of novels often exceeds 128k, surpassing manyLLMs' context window limits, we divide the original novel content into multiple segments. For eachsegment, we prompt the model to generate a summary of that portion. To generate fake queries, wealso provide a seed for creating these fake queries and their responses.\nFor generating non-conflict queries, we directly prompt the model to generate queries and responsesbased on the summary content. Additionally, for each query, we require the model to provide thecorresponding reference information. The prompts we used are shown in Appendix B."}, {"title": "HOW DO EXISTING MODELS PERFORM WHEN FACING DIFFERENT TYPES OF CONFLICTING QUERIES?", "content": "In this section, we answer RQ1: How do existing models perform when facing different types ofconflicting queries? We begin introducing the models and metrics of our evaluation, followed witha comprehensive analysis of the results across different model architectures, scales, and query types."}, {"title": "MODELS AND METRICS", "content": "We evaluated a diverse range of models, including both proprietary and open-source options. Forproprietary models, we focused on the GPT series (GPT3.5-turbo, GPT4o-mini, GPT4o) (Achiamet al., 2023). Our open-source selection included the Llama series (Llama-3-8B-Instruct, Llama-3-72B-Instruct, Llama-3.1-8B-Instruct, Llama-3.1-72B-Instruct) (Dubey et al., 2024), the Mistralseries (Mistral-7B-Instruct-v0.2, Mixtral-8x7B-Instruct-v0.1) (Jiang et al., 2023), and the Qwen series (Qwen2-7B-Instruct, Qwen2-72B-Instruct) (Yang et al., 2024).\nWe evaluated these models using the RoleRef dataset. Performance was assessed across 9 dimen-sions (detailed in Appendix A), with GPT-4o serving as the scoring model. Each dimension wasscored on a scale of 0 to 2, with the average score reported unless otherwise specified."}, {"title": "EVALUATION RESULTS", "content": "The results of models that evaluating over RoleRef are shown in Table 3. Our analysis reveals severalimportant findings regarding the performance of different models across various query types.\nqueries. Models exhibit a notable difference in handling different types ofqueries. They perform strongly in non-conflict and contextual knowledge conflict scenarios (RoleSetting and Role Profile), but struggle with parametric knowledge conflicts (Factual Knowledgeand Absent Knowledge). For example, Llama-3.1-72B-Instruct achieves near-perfect scores in non-conflict (1.95) and Role Setting (1.99) categories, but scores significantly lower in Factual Knowl-edge (1.28) and Absent Knowledge (1.20) scenarios. This performance gap suggests that modelsare adept at recognizing conflicts with information provided in their immediate context but strugglesto identify conflicts with their pre-trained knowledge base. For instance, models successfully refusecontextual conflict queries (e.g., asking Gandalf about Harry Potter) but often fail to recognize para-metric knowledge conflicts (e.g., incorrectly affirming presence at events that the character didn'tattend in the original story)."}, {"title": "WHY IS THERE A GAP IN RPAS' ABILITIES TO HANDLE DIFFERENT TYPES OF CONFLICTING QUERIES?", "content": "To understand why models perform differently in contextual and parametric knowledge conflictscenarios, we conducted an in-depth analysis of the models' internal representations using linearprobing and t-SNE visualization techniques."}, {"title": "ANALYSIS VIA LINEAR PROBES", "content": "Previous work has shown that the internal states of LLMs can reveal the model's knowledge aboutquery truthfulness (Azaria & Mitchell, 2023; Ji et al., 2024). Building on this, we used linear probesto investigate whether models can distinguish between queries that should be refused and those thatshould be answered. The detailed procedure of probe training is provided in Appendix C.2. Theresults, shown in Figure 2, reveal following insight:\nModels exhibit a keen awareness of contextual conflicts but struggle with parametric knowl-edge conflicts. Probes achieve higher accuracy in detecting contextual knowledge conflicts com-pared to parametric knowledge conflicts. This superior recognition aligns with the models' betterperformance in refusing contextual conflict queries. In contrast, the lower accuracy of the probesfor parametric knowledge conflicts indicates that models struggle to internally differentiate theseconflicts from non-conflict queries. This difficulty in identification likely contributes to the models'poor performance in refusing to answer such queries."}, {"title": "ANALYSIS VIA T-SNE", "content": "Overlap in parametric knowledge conflicts - Direct response region. Representations of mostparametric knowledge conflict queries significantly overlap with non-conflict queries. This overlap suggests that these queries within the representation space are positioned in a direct response region,where the model tends to answer directly without recognizing the conflict. For example, when pre-sented with the query \"Gandalf, was it you who recommended The Prancing Pony as a safe placeto stay for Frodo and his friends?\". The representation of this query likely falls within the direct re-sponse region, leading to an inappropriate answer. Conversely, for queries whose representations fallfurther from the non-conflict cluster, the model correctly identifies the false and refuses to answer.\nThese t-SNE results extend our findings from the linear probe analysis, offering a visual repre-sentation of how different query types are encoded in the model's representation space. The clearseparation of contextual conflicts aligns with the high probe accuracy for these queries and explains the models' success in refusing them. Similarly, the overlap between parametric knowledge conflicts"}, {"title": "HOW CAN WE ENHANCE RPAS' REFUSAL ABILITY WITHOUT COMPROMISING THEIR GENERAL ROLE-PLAYING CAPABILITIES?", "content": "In this section, we aim to address RQ3: How can we enhance RPAs' ability to respond to conflictingqueries without compromising their general role-playing capabilities? Building on our findingsfrom Section 5.2, which revealed distinct regions in the representation space for refusal and directresponses, we apply a representation-editing method to improve the model's ability to identify andrefuse conflicting queries."}, {"title": "REPRESENTATION EDITING METHOD", "content": "The representation-editing approach is a lightweight method that enables a model to refuse to an-swer without requiring additional model training. This method adopts an interpretability perspective(Zou et al., 2023), where the refusal representation is activated when the model declines to answer,thus aiding in the refusal process. By identifying the representations related to refusal within the"}, {"title": "EXPERIMENT", "content": "To validate the effectiveness of our proposed representation editing method, we conducted compre-hensive experiments comparing it with two baseline approaches: Fine-Tuning (FT) and LoRA. Weevaluated these methods across various query types and used MT-Bench to assess their impact ongeneral role-playing and conversational abilities. More analysis is presented in the Appendix D."}, {"title": "BASELINES", "content": "Prompting: The Prompt-based method instructs the model to refuse queries that exceed the scopeof the role's knowledge by providing prompts about refusal within the context.\nFT: Fine-Tuning(FT) is a relatively simple and effective method to enhance a model's refusal capa-bilities. We directly use RoleRef to perform supervised fine-tuning on the model to teach it to refuseinappropriate requests. This is achieved by training models using the standard autoregressive loss.\nLORA: LORA (Hu et al., 2021) has the advantage of learning less but also forgetting lessBidermanet al. (2024). Therefore, to prevent the model from overfitting to refusal data during training, whichmay cause it to refuse non-conflict queries as well, we also use LoRA to train the model.\nTraining details for FT and LoRA are provided in the Appendix C."}, {"title": "EVALUATION RESULTS", "content": "We present the performance of the models on the evaluation benchmark after supervised fine-tuningand representation editing in Table 4.\nnon-conflict and conflict scenarios. It achieved an impressive average score of 1.86 on non-conflictqueries, notably higher than FT (1.75) and LoRA (1.72). This balance is vital for preserving themodel's overall role-playing capabilities while bolstering its refusal ability."}, {"title": "EVALUATION ON MT-BENCH", "content": "To further validate our method's impact on general role-playing and conversational abilities, weconducted evaluations using MT-Bench, focusing on both role-playing specific tasks (MT-Bench-Roleplay) and general conversational abilities.\nThe results indicate that Representation Editing method, while improving the model's refusal ability,also enhances its general role-playing capabilities and conversational abilities compared with FT andLORA. In the MT-Bench-Roleplay and broader MT-Bench evaluation, this method achieved the bestperformance in most cases."}, {"title": "CONCLUSION", "content": "Our study investigated PRAs capabilities in handling conflicting requests, with a focus on enhancingtheir ability to recognize and refuse inappropriate queries. Our evaluation of state-of-the-art mod-els revealed significant performance differences across different conflict scenarios, particularly indealing with parametric knowledge conflicts. Through analysis of model representations, we uncov-ered the existence of distinct representation spaces for different roles and conflict types within themodels. This key finding explains the observed performance differences and provides a foundation"}, {"title": "BASELINES", "content": "Prompting: The Prompt-based method instructs the model to refuse queries that exceed the scope\nof the role's knowledge by providing prompts about refusal within the context.\nFT: Fine-Tuning(FT) is a relatively simple and effective method to enhance a model's refusal capa-\nbilities. We directly use RoleRef to perform supervised fine-tuning on the model to teach it to refuse\ninappropriate requests. This is achieved by training models using the standard autoregressive loss.\nLORA: LORA (Hu et al., 2021) has the advantage of learning less but also forgetting lessBiderman\net al. (2024). Therefore, to prevent the model from overfitting to refusal data during training, which\nmay cause it to refuse non-conflict queries as well, we also use LoRA to train the model.\nTraining details for FT and LoRA are provided in the Appendix C."}, {"title": "EVALUATION RESULTS", "content": "We present the performance of the models on the evaluation benchmark after supervised fine-tuning\nand representation editing in Table 4.\nacross all query types, achieving the highest average score of 1.54, which outperformed both\nFT and LoRA.\nand conflict scenarios. It achieved an impressive average score of 1.86 on non-conflict\nqueries, notably higher than FT (1.75) and LoRA (1.72). This balance is vital for preserving the\nmodel's overall role-playing capabilities while bolstering its refusal ability."}, {"title": "EVALUATION ON MT-BENCH", "content": "To further validate our method's impact on general role-playing and conversational abilities, we\nconducted evaluations using MT-Bench, focusing on both role-playing specific tasks (MT-Bench-\nRoleplay) and general conversational abilities.\nsubtasks, MT-Bench-Roleplay is one of the subtasks. Representation Editing demonstrates good\nperformance not only in roleplay but also in general conversation.\nindicates that Representation Editing method, while improving the model's refusal ability,\nalso enhances its general role-playing capabilities and conversational abilities compared with FT and\nLORA. In the MT-Bench-Roleplay and broader MT-Bench evaluation, this method achieved the best\nperformance in most cases."}, {"title": "CONCLUSION", "content": "Our study investigated PRAs capabilities in handling conflicting requests, with a focus on enhancing\ntheir ability to recognize and refuse inappropriate queries. Our evaluation of state-of-the-art mod-\nels revealed significant performance differences across different conflict scenarios, particularly in\ndealing with parametric knowledge conflicts. Through analysis of model representations, we uncov-\nered the existence of distinct representation spaces for different roles and conflict types within the\nmodels. This key finding explains the observed performance differences and provides a foundation"}, {"title": "MORE ANALYSIS", "content": "From the Figure 2 we can also observe the following phenomenon:\nPotentially consistent patterns across models Despite architectural differences, models like\nLlama3-8B-Instruct and Llama3.1-8B-Instruct show similar accuracy trends across layers for dif-\nferent query types. This suggests that these models may encode similar features at analogous layers,\nregardless of their specific architecture or pre-training data."}, {"title": "ANALYSIS OF REPRESENTATION EDITING METHOD", "content": "To investigate the effectiveness of the representation editing method in enhancing the model's ability\nto recognize conflict scenarios, we conducted a comparative analysis using linear probes. These\nprobes were trained on the hidden states of the last layer of models that underwent fine-tuning and\nrepresentation editing. Figure 9 illustrates our findings."}, {"title": "ANALYSIS OF REPRESENTATION VIA T-SNE", "content": "We also show the results of t-SNE visualization of the last layer representation of models, Llama3-\n8B-Instruct, Mistral-7B-Instruct, and qwen2-7B-Instruct, as shown in Figure 10."}]}