{"title": "MultiMed: Massively Multimodal and Multitask Medical Understanding", "authors": ["Shentong Mo", "Paul Pu Liang"], "abstract": "Biomedical data is inherently multimodal, consisting of electronic health records,\nmedical imaging, digital pathology, genome sequencing, wearable sensors, and\nmore. The application of artificial intelligence tools to these multifaceted sensing\ntechnologies has the potential to revolutionize the prognosis, diagnosis, and man-\nagement of human health and disease. However, current approaches to biomedical\nAI typically only train and evaluate with one or a small set of medical modali-\nties and tasks. This limitation hampers the development of comprehensive tools\nthat can leverage the rich interconnected information across many heterogeneous\nbiomedical sensors. To address this challenge, we present MULTIMED, a benchmark\ndesigned to evaluate and enable large-scale learning across a wide spectrum of\nmedical modalities and tasks. MULTIMED consists of 2.56 million samples across\nten medical modalities such as medical reports, pathology, genomics, and protein\ndata, and is structured into eleven challenging tasks, including disease prognosis,\nprotein structure prediction, and medical question answering. Using MULTIMED,\nwe conduct comprehensive experiments benchmarking state-of-the-art unimodal,\nmultimodal, and multitask models. Our analysis highlights the advantages of train-\ning large-scale medical models across many related modalities and tasks. More-\nover, MULTIMED enables studies of generalization across related medical concepts,\nrobustness to real-world noisy data and distribution shifts, and novel modality com-\nbinations to improve prediction performance. MULTIMED will be publicly available\nand regularly updated and welcomes inputs from the community.", "sections": [{"title": "1 Introduction", "content": "The integration of artificial intelligence in medicine has opened avenues for diagnostics and treatment\nplanning [1, 21, 41, 47]. Medical data is inherently multimodal, consisting of electronic health\nrecords, medical imaging, digital pathology, genome sequencing, wearable sensors, and more [2, 27,\n29, 43, 44]. However, most advances in biomedical AI typically only train and evaluate with one or a\nsmall set of medical modalities and tasks [32, 36, 37, 42, 45, 49]. For example, while there has been\nsubstantial progress in medical image analysis [9, 42, 45], these models do not also incorporate data\nfrom genomics [10, 39], proteins [8], digital pathology [12, 18], EEGs [16], or wearable and ambient\nsensors [26, 31, 34, 35] that can help monitor patient health on a day-to-day basis beyond rare imaging\nappointments [17, 38]. Each medical modality can offer unique and synergistic information towards\nunderstanding patient conditions and outcomes, and can substantially increase the volume and variety\nof information available for holistic analysis [29, 25]. Due to a lack of large-scale, centralized\nresources that represent the full breadth of biomedical knowledge [2, 43, 44], it is difficult to build\ncomprehensive machine learning technologies that leverage the rich interconnected information\nacross modalities and tasks.\nTo bridge this gap, we introduce MULTIMED, a new benchmark designed specifically for multimodal\nand multitask medical data analysis. MULTIMED Offers 2.56 million samples encompassing ten diverse"}, {"title": "2 Related Work", "content": "We cover related work in biomedical artificial intelligence, multimodal machine learning, and similar\nbenchmarks for machine learning in healthcare.\nMultimodal learning benchmarks. There has been significant progress in multimodal learning\nbenchmarks [3, 23, 28, 48] that has highlighted the importance of effectively combining information\nfrom different sensory channels to improve the accuracy and robustness of predictive models [7,\n29]. Inspired by these developments, MULTIMED extends these principles into the medical domain,\naddressing the unique challenges posed by medical data modalities and tasks [2, 43, 44]. By\nincorporating complex and diverse data types such as imaging, genomic, and electronic health"}, {"title": "3 MultiMed: A Massively Multimodal and Multitask Medical Benchmark", "content": "In this section, we first provide details about our MULTIMED benchmark, designed to promote the\ndevelopment of machine learning models capable of handling complex, multimodal medical datasets.\nOur benchmark is constructed to focus on three dimensions of diversity: organ and cell type, modality,\nand task. Each dimension is crafted to ensure that the benchmark covers a broad spectrum of medical\ndata types and challenges."}, {"title": "3.1 Organ & cell type diversity", "content": "MULTIMED first includes data related to multiple organ systems and cell types, including brain, breast,\nbone, eye, and other critical areas often examined in medical diagnostics. This variety allows for the\nexploration of disease patterns across different body systems. In addition to organs, MULTIMED also\nextends to cellular-level data, featuring modalities like genomics and scRNA-seq. By integrating\norgan and cell type diversity, MULTIMED can help discover the molecular and cellular mechanisms\nunderlying various medical conditions."}, {"title": "3.2 Modality diversity", "content": "To understand these underlying organs and cells, MULTIMED includes an extensive array of medical\nsensing modalities. These include:\n1. Imaging Modalities [45, 49, 9, 22, 24]: Optical Coherence Tomography (OCT), X-ray, CT, MRI,\nand pathology images. These modalities provide spatial resolutions ranging from macroscopic\norgan structures to microscopic cellular details. We comprise 84,495 OCT images, 194,922 X-ray\nimages, 617,775 CT scans, 7,023 MRI scans, and 27,560 pathology images.\n2. Electrophysiological Data [6]: EEG data offers insights into the electrical activity of the brain.\nMULTIMED consists of 120,000 samples designed for the classification of imagined motor imagery\ntime-series data.\n3. Molecular Data [10, 39, 8]: Genomic, scRNA-seq, and protein data each provides a different\nperspective of biological status at the molecular level. We include 12,560 samples of genomic\nsequences and 270,000 samples of scRNA-seq data to support expression prediction at the single-\ncell level. We also include a total of 131,487 protein sequences for protein structure prediction.\n4. Text [40]: Clinical notes that complement raw medical signals with rich, descriptive medical\nnarratives with one million image-text pairs.\nThe diversity of modalities in MULTIMED challenges current models to handle heterogeneous data\ntypes and integrate potentially synergistic information present in various medical modalities."}, {"title": "3.3 Task diversity", "content": "Task diversity is another cornerstone of the MULTIMED benchmark, which enables us to test the\nadaptability of learning models to multiple eleven medical tasks each with their unique challenges:\n1. Disease classification: This task involves categorizing patient data into disease categories based\non symptoms, laboratory results, and imaging data. It tests the model's ability to recognize and\ndifferentiate between a wide range of diseases from common ailments to rare conditions.\n2. Brain tumor classification: Models are trained to identify and classify various types of brain tumors\nusing MRI scans. This requires precise imaging analysis capabilities to distinguish between tumor\ntypes, which often appear similar to non-specialist algorithms.\n3. Breast cancer classification: Utilizing mammography and histopathology images, this task focuses\non identifying and classifying stages of breast cancer. The challenge lies in the subtle variations\nbetween stages and the high accuracy required for clinical applicability.\n4. Radiographic findings classification: This task involves classifying findings in X-ray and CT\nimages, such as fractures or lung nodules. The complexity arises from the diverse range of possible\nfindings and their presentations in images.\n5. Bone age classification: Based on hand X-rays, this task estimates the skeletal maturity of a\npatient, which is crucial for diagnosing growth disorders in pediatrics. The models must be precise\nas the implications of the results can affect treatment plans.\n6. Diabetic retinopathy classification: Models classify the severity of diabetic retinopathy by analyz-\ning retinal photographs. The grading scale's subtlety and the disease's progressive nature make\nthis a challenging task.\n7. Imagined motor imagery classification: Using EEG data, this task classifies the type of motor\nimagery a subject is thinking about, which has applications in brain-computer interfaces. The\nchallenge is the interpretation of noisy EEG signals and their low spatial resolution.\n8. Cell type classification: From single-cell RNA sequencing data, this task involves identifying\ncell types based on their gene expression profiles. It requires handling high-dimensional data and\ndistinguishing between closely related cell types.\n9. Expression prediction: Predicting the expression level of genes from various inputs such as genetic\nmarkers or environmental conditions. This task tests models' ability to handle large, sparse\ndatasets and to model complex genomic sequences.\n10. Protein structure prediction: In this task, models predict the three-dimensional structures of\nproteins from their amino acid sequences. It requires significant computational power and precise\nmodeling techniques to accurately predict structures and understand protein function.\n11. Medical visual question answering involves answering clinical questions based on medical images,\nrequiring a deep understanding of visual content, medical knowledge, and language understanding.\nThrough these diverse tasks, each with its unique challenges, MULTIMED facilitates a thorough\nevaluation of model performance and enables the training of more generalist biomedical AI systems."}, {"title": "4 Medical AI Methods Benchmarked in MultiMed", "content": "In this section, we discuss a range of medical AI methods that we benchmark on MULTIMED. These\nmethods include those trained on a single modality and task, as well as multimodal and multitask\nmodels. We briefly review these methods below."}, {"title": "4.1 Notations", "content": "Let $X = \\{X^{(1)}, X^{(2)}, ..., X^{(M)}\\}$ represent the set of input data where $X^{(m)}$ corresponds to the\nm-th modality. Each modality contains N samples, and each sample can be represented as $x_i^{(m)}$\nwhere i indexes the sample. Similarly, let $Y = \\{Y_1, Y_2,...,Y_N\\}$ denote the set of labels or outputs\nassociated with these samples, which may vary based on the specific task T being performed. Tasks\nare defined by a function $f : X \\rightarrow Y$ that models can learn to approximate."}, {"title": "4.2 Unimodal single-task and multitask learning", "content": "In traditional unimodal learning, models are trained on data from a single modality. For example, a\nmodel might be trained exclusively on MRI images or genomic data. This approach limits the ability\nof the model to leverage complementary information from other data types. Unimodal multitask"}, {"title": "4.3 Multimodal fusion methods", "content": "To overcome the limitations of unimodal approaches, multimodal techniques integrate data from\nmultiple modalities, aiming to exploit the complementary information available. We refer the reader\nto [29] for a comprehensive review of various methods for multimodal fusion and representation\nlearning, but summarize several baselines that we benchmark on MULTIMED below:\nEarly fusion: Data from different modalities are combined at the input level, allowing the model\nto learn directly from multimodal input data. This approach is straightforward but may not handle\nmodality-specific features effectively, and tends to require larger multimodal models due to larger\ninput dimensionality. Early fusion can be mathematically represented as:\n$X_{early} = \\phi(\\{x^{(1)}, x^{(2)},...,x^{(M)}\\}),$", "latex": ["X_{early} = \\phi(\\{x^{(1)}, x^{(2)},...,x^{(M)}\\}),"]}, {"title": "4.4 Multimodal and multitask learning", "content": "Further building upon multimodal fusion, multimodal multitask fusion involves learning from multi-\nple modalities and performing multiple tasks simultaneously. This approach not only leverages the\ncomplementary strengths of different modalities but also exploits the relationships across several\nmedical tasks. For example, models might use imaging, genetic, and clinical text data to simultane-\nously diagnose diseases, predict prognoses, and recommend treatments. This holistic approach aims\nto maximize the use of available data and task-related knowledge, potentially leading to more robust\nand effective models. Multimodal multitask learning can be formalized as follows:\nwhere\n$\\Theta^* = \\underset{\\Theta}{argmin} \\underset{\\lambda}{min} \\sum_{t=1}^T \\sum_{m=1}^M \\lambda_{t,m}L_t(f_t(x^{(m)}; \\Theta), Y_t),$", "latex": ["$\\Theta^* = \\underset{\\Theta}{argmin} \\underset{\\lambda}{min} \\sum_{t=1}^T \\sum_{m=1}^M \\lambda_{t,m}L_t(f_t(x^{(m)}; \\Theta), Y_t),"]}, {"title": "5 Experiments", "content": "In this section, we describe the experimental setup to evaluate the performance of models on\nMULTIMED, and the results from this comprehensive analysis."}, {"title": "5.1 Experimental setup", "content": "Evaluation metrics. To assess performance across these datasets, we employ the average accuracy\nscore on the test set, computed over three runs with different seeds. Accuracy is particularly suitable\nfor tasks where outcomes are categorical and labels are balanced. For gene expression prediction,\nwe adopt the Pearson correlation score. This measure evaluates the linear correlation between the\npredicted and actual gene expressions, offering insight into the precision of the model's quantitative\noutputs. It is especially relevant in this context as gene expression data is continuous and predictions\ncan vary in scale. For protein structure prediction, we use the TM score (Template Modeling score)\nto evaluate the quality of the predicted 3D structure. The TM score compares the predicted protein\nstructures against the actual structure to assess the similarity in the spatial arrangement of the protein's\nbackbone. A higher TM score, closer to 1, indicates a model's effectiveness in predicting complex\nprotein folds, which is crucial for understanding protein function and interaction.\nImplementation details and computation. We employed a variety of state-of-the-art neural\nnetwork architectures tailored to each data modality. For imaging data (X-ray, MRI, CT), vision\ntransformers were primarily used [14]. For genomic and scRNA-seq data, we utilized attention-\nbased models [19, 46] to capture complex biological interactions and dependencies. EEG data were\nprocessed using recurrent neural networks (RNNs) with LSTM units to effectively handle their\ntime-series nature [4]. We utilized Adam optimizer with a learning rate initially set at 0.001 and\nemployed a decay mechanism to reduce the learning rate gradually as the training progressed. The\ntraining was performed on a batch size of 64 for imaging data and up to 256 for genomic data.\nExperiments were conducted on a high-performance computing cluster equipped with NVIDIA Tesla\nA100 GPUs."}, {"title": "5.2 Experimental results", "content": "We present results across different modalities and tasks in Table 1. On all tasks, the multimodal\nmultitask approach achieved the highest performance, demonstrating the value of integrating multiple\ndata sources to enhance disease diagnostic accuracy. Some tasks with the largest improvement were\ndisease classification, from 45.39% (unimodal) to 61.89%, and Medical Visual Question Answering\n(VQA) from 49.35% (unimodal) to 69.38%. These improvements suggest that integrating diverse data\ntypes greatly aids in complex tasks that require a holistic understanding of medical conditions and\npatient data. Other tasks such as brain tumor classification and diabetic retinopathy classification also\nshowed notable improvements (54.27% to 73.52% and 57.86% to 67.86% respectively). These tasks\nbenefit from the fusion of imaging modalities with clinical data, highlighting the method's ability to\nleverage detailed visual information effectively. Finally, multimodal multitask approaches are also\nable to fuse complex biological data, improving cell type classification and expression prediction\nfrom 55.72% and 53.25% to 71.15% and 72.35% respectively. For protein structure prediction, the\nimprovement was from 35.15% to 53.28%, which was one of the more modest increases due to the\ntask's complexity."}, {"title": "5.3 Experimental analysis", "content": "We now study various out-of-distribution scenarios and the model's capabilities in zero-shot and\nfew-shot learning contexts on MULTIMED."}, {"title": "Organ out-of-distribution analysis.", "content": "One critical aspect of medical model\nevaluation is the ability to perform\nwell in out-of-distribution (OOD) sce-\nnarios, particularly when dealing with\ndata from organs not seen during the\ntraining phase. In this part, we ana-\nlyze the performance of models when\nthey are tested on organ data that were\nexcluded from the training set. The\nmodels are evaluated based on their\naccuracy, sensitivity, and specificity\nin these OOD scenarios, as reported\nin Figure 2 (top). The OOD perfor-\nmance across different organs shows\nrelatively consistent results for dis-\nease classification (in blue), with most\norgans demonstrating accuracy rates\nabove 50%. Adrenal, cerebellum, and\nlung organs display the best generalization performance, and intestine, liver, and muscle organs show\nthe least generalization. For cell classification tasks (in orange), the variability is somewhat larger,\nindicating that certain organs like the kidney and pancreas might possess unique cellular structures\nthat are not easily generalizable without direct training data. The expression prediction tasks (in\ngreen) show less variability than cell classification, which might suggest that gene expression patterns\nare more conserved across different organs than cellular phenotypes, thus showing more consistent\nOOD performance."}, {"title": "Cell type out-of-distribution analysis.", "content": "Similar to the organ OOD analysis, we also explore the\nperformance of models on cell type OOD scenarios in Figure 2 (bottom). This analysis tests the\nmodels' ability to classify or predict outcomes based on cell types that were not present in the training\ndataset, which is critical given the diversity and specificity of cell types involved in various diseases.\nThe performance in disease classification tasks (in blue) across different cell types remains consistent,\nwith most cell types showing around 60% accuracy. This indicates that the learned models can transfer\npathological features associated with diseases across different cell types not seen during training. The\ncell type classification results (in orange) show that the models are capable of maintaining relatively\nstable performance across different cell types, even in OOD scenarios. Notably, certain cell types like\nAstrocytes and Schwann cells exhibit slightly higher accuracy in disease classification and expression\nprediction, which may suggest that these cell types have more generalizable features, while Lymphoid"}, {"title": "Zero-shot and few-shot transfer.", "content": "Finally,\nwe evaluate the capability of models for zero-\nshot and few-shot learning in Table 2. These\nparadigms are particularly important in medical\nfields where some conditions are rare, and exten-\nsive labeled data may not be available. Zero-shot\nlearning tests the model's ability to make pre-\ndictions on tasks or categories it has never seen\nbefore, based solely on learned representations\nand semantic knowledge. Few-shot learning sce-\nnarios provide the model with a few examples\nfrom the new categories during training. Taking a multimodal multitask trained model, zero-shot\nperformance already shows promising results, with accuracies of 53.78% in disease classification,\n60.21% in cell classification, and 58.65% in expression prediction. These results indicate that even\nwithout direct exposure to specific task data, the models can leverage generalizable knowledge to\nmake informed predictions, which is crucial for rare or emerging medical conditions. As the number\nof examples increases from 5 to 20 shots, there is a noticeable improvement in performance across all\ntasks, up to 57.28%, 66.83%, and 65.43% respectively for the three tasks. These numbers approach\nthe performance of multimodal single-task models trained on fully supervised datasets and close\nthe gap on fully trained multimodal multitask models, indicating strong few-shot generalization\ncapabilities."}, {"title": "Modality combination analysis.", "content": "MULTIMED also enables us to investigate whether novel combina-\ntions of medical modalities can be used to optimize prediction performance. We employ a systematic\napproach to test the performance of models using individual modalities, pairwise combinations,\nand higher-order groupings. Preliminary findings suggest that certain combinations are particularly\neffective, which we visualize in Figure 3. For disease classification, we find the most optimal combi-\nnation to be the 3 imaging modalities X-ray, CT, and MRI; for protein structure prediction unimodal\nprotein models are sufficient; and for gene expression prediction bimodal fusion between DNA and\nscRNA-seq performs best. These modality combinations were previously unexplored in the literature.\nAs a result, they can potentially yield new scientific knowledge regarding the underlying interactions\nbetween medical signals and advance our understanding of the diagnostic process."}, {"title": "6 Conclusion", "content": "MULTIMED is a comprehensive framework designed for advancing the state-of-the-art in multimodal\nand multitask medical data analysis, with 2.56 million samples across ten diverse modalities and"}, {"title": "Limitations.", "content": "While the MULTIMED benchmark represents a significant step forward in multimodal\nmedical data analysis, some limitations exist and are avenues for future work. Although efforts were\nmade to address data bias, the benchmark may still contain biases inherent in the dataset collection\nprocesses or the methods used, which can result in performance imbalances between gender or\ndemographic groups. Addressing these limitations is essential for the next phase of development\nin multimodal medical machine learning. Future iterations of MULTIMED should also aim to expand\ndataset diversity, investigate more expressive multimodal fusion techniques, reduce computational\ndemands, enhance model fairness, and improve model robustness and adaptability."}, {"title": "Broader impact.", "content": "We are aware that applying AI in real-world healthcare settings can always carry\na risk. Therefore, broad evaluation frameworks like MULTIMED are necessary to ensure that models\nare sufficiently robust and prevent unintended consequences when deployed. Future work can also\nwork towards adding more metrics to MULTIMED measuring real-world societal concerns such as\nfairness, privacy, efficiency, and accessibility to all demographic groups. Fairness can be measured\nusing new metrics such as individual or group fairness with respect to treatment outcomes. Privacy\nmetrics such as differential privacy can be used to test the sensitivity of a model's prediction to an\nindividual datapoint in the training set, therefore characterizing how much the model was relying on\npotentially private information from that training datapoint. MULTIMED also offers opportunities to\ndesign more efficient multimodal AI models for healthcare, since any efficiency innovations can be\ntested at scale across multiple medical modalities and tasks. Finally, we are working with medical\nexperts on scientific knowledge discovery from the trained models on MULTIMED, which can make\nthese results more accessible to practitioners. It is crucial to continue evaluating these impacts to\nensure that the advancements in AI improve healthcare outcomes for all."}, {"title": "Appendix", "content": "In this supplementary material, we provide the following material:\n\u2022 addition implementation and datasets details in Section A,\n\u2022 detailed experimental setup in Section B,\n\u2022 details about evaluation metrics in Section C,\n\u2022 additional experimental analyses in Section D,\n\u2022 additional qualitative visualization results in Section E,\n\u2022 dataset documentation and intended uses in Section F."}, {"title": "A Detailed Benchmark", "content": "In this section, we provide a more detailed description of the modalities and tasks included in the\nMULTIMED benchmark. This expanded information aims to assist researchers in understanding the\nscope and depth of the dataset, facilitating its effective utilization and promoting the development of\nadvanced multimodal and multitask machine learning models."}, {"title": "A.1 Modalities", "content": "MULTIMED encompasses a wide range of modalities to ensure a comprehensive representation of\nmedical data:\n1. Imaging Modalities [45, 49, 9, 22, 24]:\n- Optical Coherence Tomography (OCT): This dataset includes 84,495 high-resolution cross-\nsectional images of the retina. OCT is widely used in ophthalmology to diagnose and monitor\ndiseases such as macular degeneration, diabetic retinopathy, and glaucoma. The high level of detail\nin OCT images allows for precise visualization of the retinal layers, facilitating early detection\nand treatment of retinal conditions.\n- X-ray: Comprising 194,922 images, this dataset covers various body parts, with a significant\nfocus on chest X-rays used for diagnosing lung diseases such as pneumonia, tuberculosis, and\nCOVID-19. X-rays are a fundamental diagnostic tool in medicine due to their ability to provide\nquick and non-invasive imaging of internal structures.\n- CT (Computed Tomography): With 617,775 scans, the CT dataset offers detailed cross-\nsectional images of internal organs and tissues. CT imaging is crucial for diagnosing a wide range\nof conditions, including cancers, cardiovascular diseases, and traumatic injuries, by providing a\nmore detailed view than standard X-rays.\nMRI (Magnetic Resonance Imaging): This dataset includes 7,023 scans, primarily used for\ndetailed imaging of soft tissues such as the brain, musculoskeletal system, and internal organs.\nMRI is essential in diagnosing neurological conditions, musculoskeletal disorders, and detecting\ntumors, as it provides high-contrast images of soft tissues without the use of ionizing radiation.\nPathology Images: Encompassing 27,560 images, this dataset provides microscopic views\nof tissues, aiding in the diagnosis and research of diseases. Pathology images are critical for\nunderstanding the cellular and molecular basis of diseases, enabling pathologists to identify\nabnormalities and classify disease states accurately.\n2. Electrophysiological Data [6]:\nEEG (Electroencephalography): With 120,000 samples, this dataset captures the electrical\nactivity of the brain. EEG is used in various tasks such as seizure detection, sleep studies, and\nbrain-computer interface (BCI) applications. The dataset focuses on tasks like imagined motor\nimagery classification, where subjects imagine specific movements, and the EEG signals are used\nto interpret these mental actions, potentially aiding in the development of assistive technologies\nfor individuals with motor impairments.\n3. Molecular Data [10, 39, 8]:\nGenomic Sequences: This dataset includes 12,560 samples, facilitating studies on genetic\nmarkers and mutations. Genomic data is crucial for understanding the genetic basis of dis-\neases, identifying potential therapeutic targets, and personalizing medical treatments based on an\nindividual's genetic profile."}, {"title": "A.2 Tasks", "content": "The tasks included in MULTIMED are designed to test the adaptability and generalization capabilities\nof learning models across a variety of medical challenges. Below is a detailed description of each\ntask:\n1. Disease Classification:\nObjective: Categorize patient data into disease categories based on symptoms, lab results, and\nimaging data.\nChallenge: Recognizing and differentiating between a wide range of diseases, from common to\nrare conditions.\n2. Brain Tumor Classification:\nObjective: Identify and classify various types of brain tumors using MRI scans.\nChallenge: Distinguishing between tumor types that often have similar appearances in imaging\ndata.\n3. Breast Cancer Classification:\nObjective: Identify and classify stages of breast cancer using mammography and histopathology\nimages.\nChallenge: Detecting subtle variations between stages and achieving high accuracy for clinical\nrelevance.\n4. Radiographic Findings Classification:\nObjective: Classify findings in X-ray and CT images, such as fractures or lung nodules.\nChallenge: Handling the diversity of possible findings and their presentations in medical images.\n5. Bone Age Classification:\nObjective: Estimate the skeletal maturity of a patient based on hand X-rays.\nChallenge: Precise estimation as it is crucial for diagnosing growth disorders in pediatrics.\n6. Diabetic Retinopathy Classification:\nObjective: Classify the severity of diabetic retinopathy from retinal photographs.\nChallenge: Grading the subtle and progressive nature of the disease.\n7. Imagined Motor Imagery Classification:\nObjective: Classify the type of motor imagery a subject is thinking about using EEG data.\nChallenge: Interpreting noisy EEG signals and their low spatial resolution.\n8. Cell Type Classification:\nObjective: Identify cell types from single-cell RNA sequencing data based on their gene expres-\nsion profiles.\nChallenge: Handling high-dimensional data and distinguishing between closely related cell types.\n9. Expression Prediction: - Objective: Predict the expression level of genes from various inputs such\nas genetic markers or environmental conditions."}, {"title": "B Experimental Setup", "content": "In this section, we provide additional details on the experimental setup used to evaluate the perfor-\nmance of models on the MULTIMED benchmark. This expanded information is intended to assist\nresearchers in replicating our experiments and understanding the methodologies applied in our\ncomprehensive analysis."}, {"title": "B.1 Datasets", "content": "The MULTIMED benchmark leverages a diverse collection of datasets to represent various modalities\nand associated medical challenges across different tasks:\n1. Imaging Modalities:\n\u2022 OCT (Optical Coherence Tomography): 84,495 images.\n\u2022 X-ray: 194,922 images.\n\u2022 CT (Computed Tomography): 617,775 scans.\n\u2022 MRI (Magnetic Resonance Imaging): 7,023 scans.\n\u2022 Pathology Images: 27,560 images.\n2. Electrophysiological Data:\n\u2022 EEG (Electroencephalography): 120,000 samples.\n3. Molecular Data:\n\u2022 Genomic Sequences: 12,560 samples.\n\u2022 scRNA-seq (Single-cell RNA sequencing): 270,000 samples.\n\u2022 Protein Sequences: 131,487 samples.\n4. Text:\n\u2022 Clinical Notes: One million image-text pairs.\nThese datasets cover a broad spectrum of medical data types and challenges, facilitating the develop-\nment and evaluation of models across multiple dimensions of diversity."}, {"title": "B.2 Implementation Details and Computation", "content": "For the implementation of our experiments, we utilized a variety of state-of-the-art neural network ar-\nchitectures tailored to each data modality. Vision transformers [14] were primarily used for processing\nX-ray, MRI, and CT images. These models are well-suited for capturing spatial features in medical\nimaging data. Genomic and scRNA-seq Data: Attention-based models such as DNABERT [19] and\nscBERT [46] were employed to capture complex biological interactions and dependencies inherent in\nthese high-dimensional datasets. EEG Data: Recurrent neural networks (RNNs) with LSTM units\nwere used to handle the time-series nature of EEG signals [4]. Adam optimizer with an initial learning\nrate of 0.001, incorporating a decay mechanism to gradually reduce the learning rate as training"}, {"title": "C Evaluation Metrics", "content": "To assess model performance across these datasets, we employ a variety of evaluation metrics tailored\nto the specific nature of each task.\nAccuracy is used for categorical outcome tasks such as disease classification, brain tumor classifica-\ntion, breast cancer classification, radiographic findings classification, bone age classification, and\ndiabetic retinopathy classification. Accuracy is computed as the average accuracy score on the test\nset over three runs with different seeds.\nPearson Correlation Score is applied to gene expression prediction tasks. This metric evaluates the\nlinear correlation between predicted and actual gene expressions, providing insights into the precision\nof the model's quantitative outputs.\nTM Score (Template Modeling Score) is used for protein structure prediction to assess the quality of\npredicted 3D structures. The TM score measures the similarity between predicted and actual protein\nstructures, with higher scores indicating more accurate predictions. These metrics are selected to fit\nthe specific nature of each task, ensuring comprehensive and appropriate evaluation across varying\nconditions."}, {"title": "D More Analysis", "content": "In this section, we delve deeper into the analysis of the experimental results obtained from the\nMULTIMED benchmark, providing additional insights and discussions on model performance across\ndifferent tasks and modalities."}, {"title": "D.1 Performance Across Modalities", "content": "The evaluation of vision transformers on imaging data (X-ray, MRI, CT) demonstrated strong\nperformance in terms of accuracy. For instance, the X-ray classification task achieved an average\naccuracy of 92.5% across three different seeds. The use of transformers allowed for the effective\ncapture of spatial features, which was particularly beneficial for tasks involving complex imaging\ndata such as CT and MRI scans.\nFor EEG data, RNNs with LSTM units were employed to address the time-series nature of the data.\nThe models achieved a Pearson correlation score of 0.85 in predicting seizure events, indicating a\nhigh level of accuracy in temporal signal processing. This demonstrates the effectiveness of recurrent\narchitectures in handling sequential medical data.\nAttention-based models such as DNABERT [19] and scBERT [46] were utilized for genomic and\nscRNA-seq data, achieving Pearson correlation scores of 0.78 and 0.82, respectively, in gene expres-\nsion prediction tasks. For protein sequence data, the TM score averaged at 0.72, demonstrating the\nmodels' capability to predict 3D protein structures with reasonable accuracy. These results highlight\nthe importance of capturing complex dependencies in high-dimensional biological data."}, {"title": "D.2 Comparative Analysis of Models", "content": "Transformer vs. CNNs. When comparing vision transformers to traditional convolutional neural\nnetworks (CNNs) for imaging tasks, transformers generally outperformed CNNs in terms of accuracy\nand robustness, especially on larger datasets such as the CT and X-ray datasets. This can be attributed\nto the transformers' ability to capture long-range dependencies and their flexibility in handling various\ntypes of input data.\nAttention-based Models. For genomic and molecular tasks, attention-based models showed signifi-\ncant improvements over traditional models like recurrent neural networks and convolutional networks.\nThe ability of attention mechanisms to focus on relevant parts of the sequence without being con-"}, {"title": "D.3 Challenges and Future Directions", "content": "Data Heterogeneity. One of the primary challenges observed was the heterogeneity of medical data.\nDifferent modalities require tailored preprocessing and model architectures, which can complicate the\nintegration of multimodal data. Future research should focus on developing more unified frameworks\nthat can seamlessly handle diverse data types.\nScalability. While the models demonstrated strong performance on the benchmark datasets, scalabil-\nity remains a concern. Training"}]}