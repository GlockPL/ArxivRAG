{"title": "MultiMed: Massively Multimodal and Multitask Medical Understanding", "authors": ["Shentong Mo", "Paul Pu Liang"], "abstract": "Biomedical data is inherently multimodal, consisting of electronic health records, medical imaging, digital pathology, genome sequencing, wearable sensors, and more. The application of artificial intelligence tools to these multifaceted sensing technologies has the potential to revolutionize the prognosis, diagnosis, and management of human health and disease. However, current approaches to biomedical AI typically only train and evaluate with one or a small set of medical modalities and tasks. This limitation hampers the development of comprehensive tools that can leverage the rich interconnected information across many heterogeneous biomedical sensors. To address this challenge, we present MULTIMED, a benchmark designed to evaluate and enable large-scale learning across a wide spectrum of medical modalities and tasks. MULTIMED consists of 2.56 million samples across ten medical modalities such as medical reports, pathology, genomics, and protein data, and is structured into eleven challenging tasks, including disease prognosis, protein structure prediction, and medical question answering. Using MULTIMED, we conduct comprehensive experiments benchmarking state-of-the-art unimodal, multimodal, and multitask models. Our analysis highlights the advantages of training large-scale medical models across many related modalities and tasks. Moreover, MULTIMED enables studies of generalization across related medical concepts, robustness to real-world noisy data and distribution shifts, and novel modality combinations to improve prediction performance. MULTIMED will be publicly available and regularly updated and welcomes inputs from the community.", "sections": [{"title": "1 Introduction", "content": "The integration of artificial intelligence in medicine has opened avenues for diagnostics and treatment planning [1, 21, 41, 47]. Medical data is inherently multimodal, consisting of electronic health records, medical imaging, digital pathology, genome sequencing, wearable sensors, and more [2, 27, 29, 43, 44]. However, most advances in biomedical AI typically only train and evaluate with one or a small set of medical modalities and tasks [32, 36, 37, 42, 45, 49]. For example, while there has been substantial progress in medical image analysis [9, 42, 45], these models do not also incorporate data from genomics [10, 39], proteins [8], digital pathology [12, 18], EEGs [16], or wearable and ambient sensors [26, 31, 34, 35] that can help monitor patient health on a day-to-day basis beyond rare imaging appointments [17, 38]. Each medical modality can offer unique and synergistic information towards understanding patient conditions and outcomes, and can substantially increase the volume and variety of information available for holistic analysis [29, 25]. Due to a lack of large-scale, centralized resources that represent the full breadth of biomedical knowledge [2, 43, 44], it is difficult to build comprehensive machine learning technologies that leverage the rich interconnected information across modalities and tasks.\nTo bridge this gap, we introduce MULTIMED, a new benchmark designed specifically for multimodal and multitask medical data analysis. MULTIMED Offers 2.56 million samples encompassing ten diverse"}, {"title": "2 Related Work", "content": "We cover related work in biomedical artificial intelligence, multimodal machine learning, and similar benchmarks for machine learning in healthcare.\nMultimodal learning benchmarks. There has been significant progress in multimodal learning benchmarks [3, 23, 28, 48] that has highlighted the importance of effectively combining information from different sensory channels to improve the accuracy and robustness of predictive models [7, 29]. Inspired by these developments, MULTIMED extends these principles into the medical domain, addressing the unique challenges posed by medical data modalities and tasks [2, 43, 44]. By incorporating complex and diverse data types such as imaging, genomic, and electronic health"}, {"title": "3 MultiMed: A Massively Multimodal and Multitask Medical Benchmark", "content": "In this section, we first provide details about our MULTIMED benchmark, designed to promote the development of machine learning models capable of handling complex, multimodal medical datasets. Our benchmark is constructed to focus on three dimensions of diversity: organ and cell type, modality, and task. Each dimension is crafted to ensure that the benchmark covers a broad spectrum of medical data types and challenges."}, {"title": "3.1 Organ & cell type diversity", "content": "MULTIMED first includes data related to multiple organ systems and cell types, including brain, breast, bone, eye, and other critical areas often examined in medical diagnostics. This variety allows for the exploration of disease patterns across different body systems. In addition to organs, MULTIMED also extends to cellular-level data, featuring modalities like genomics and scRNA-seq. By integrating organ and cell type diversity, MULTIMED can help discover the molecular and cellular mechanisms underlying various medical conditions."}, {"title": "3.2 Modality diversity", "content": "To understand these underlying organs and cells, MULTIMED includes an extensive array of medical sensing modalities. These include:\n1. Imaging Modalities [45, 49, 9, 22, 24]: Optical Coherence Tomography (OCT), X-ray, CT, MRI, and pathology images. These modalities provide spatial resolutions ranging from macroscopic organ structures to microscopic cellular details. We comprise 84,495 OCT images, 194,922 X-ray images, 617,775 CT scans, 7,023 MRI scans, and 27,560 pathology images.\n2. Electrophysiological Data [6]: EEG data offers insights into the electrical activity of the brain. MULTIMED consists of 120,000 samples designed for the classification of imagined motor imagery time-series data.\n3. Molecular Data [10, 39, 8]: Genomic, scRNA-seq, and protein data each provides a different perspective of biological status at the molecular level. We include 12,560 samples of genomic sequences and 270,000 samples of scRNA-seq data to support expression prediction at the single-cell level. We also include a total of 131,487 protein sequences for protein structure prediction.\n4. Text [40]: Clinical notes that complement raw medical signals with rich, descriptive medical narratives with one million image-text pairs.\nThe diversity of modalities in MULTIMED challenges current models to handle heterogeneous data types and integrate potentially synergistic information present in various medical modalities."}, {"title": "3.3 Task diversity", "content": "Task diversity is another cornerstone of the MULTIMED benchmark, which enables us to test the adaptability of learning models to multiple eleven medical tasks each with their unique challenges:\n1. Disease classification: This task involves categorizing patient data into disease categories based on symptoms, laboratory results, and imaging data. It tests the model's ability to recognize and differentiate between a wide range of diseases from common ailments to rare conditions.\n2. Brain tumor classification: Models are trained to identify and classify various types of brain tumors using MRI scans. This requires precise imaging analysis capabilities to distinguish between tumor types, which often appear similar to non-specialist algorithms.\n3. Breast cancer classification: Utilizing mammography and histopathology images, this task focuses on identifying and classifying stages of breast cancer. The challenge lies in the subtle variations between stages and the high accuracy required for clinical applicability.\n4. Radiographic findings classification: This task involves classifying findings in X-ray and CT images, such as fractures or lung nodules. The complexity arises from the diverse range of possible findings and their presentations in images.\n5. Bone age classification: Based on hand X-rays, this task estimates the skeletal maturity of a patient, which is crucial for diagnosing growth disorders in pediatrics. The models must be precise as the implications of the results can affect treatment plans.\n6. Diabetic retinopathy classification: Models classify the severity of diabetic retinopathy by analyzing retinal photographs. The grading scale's subtlety and the disease's progressive nature make this a challenging task.\n7. Imagined motor imagery classification: Using EEG data, this task classifies the type of motor imagery a subject is thinking about, which has applications in brain-computer interfaces. The challenge is the interpretation of noisy EEG signals and their low spatial resolution.\n8. Cell type classification: From single-cell RNA sequencing data, this task involves identifying cell types based on their gene expression profiles. It requires handling high-dimensional data and distinguishing between closely related cell types.\n9. Expression prediction: Predicting the expression level of genes from various inputs such as genetic markers or environmental conditions. This task tests models' ability to handle large, sparse datasets and to model complex genomic sequences.\n10. Protein structure prediction: In this task, models predict the three-dimensional structures of proteins from their amino acid sequences. It requires significant computational power and precise modeling techniques to accurately predict structures and understand protein function.\n11. Medical visual question answering involves answering clinical questions based on medical images, requiring a deep understanding of visual content, medical knowledge, and language understanding.\nThrough these diverse tasks, each with its unique challenges, MULTIMED facilitates a thorough evaluation of model performance and enables the training of more generalist biomedical AI systems."}, {"title": "4 Medical AI Methods Benchmarked in MultiMed", "content": "In this section, we discuss a range of medical AI methods that we benchmark on MULTIMED. These methods include those trained on a single modality and task, as well as multimodal and multitask models. We briefly review these methods below."}, {"title": "4.1 Notations", "content": "Let $X = \\{X^{(1)}, X^{(2)}, ..., X^{(M)}\\}$ represent the set of input data where $X^{(m)}$ corresponds to the m-th modality. Each modality contains N samples, and each sample can be represented as $x^{(m)}_i$ where i indexes the sample. Similarly, let $Y = \\{Y_1, Y_2,...,Y_N\\}$ denote the set of labels or outputs associated with these samples, which may vary based on the specific task T being performed. Tasks are defined by a function $f : X \\rightarrow Y$ that models can learn to approximate."}, {"title": "4.2 Unimodal single-task and multitask learning", "content": "In traditional unimodal learning, models are trained on data from a single modality. For example, a model might be trained exclusively on MRI images or genomic data. This approach limits the ability of the model to leverage complementary information from other data types. Unimodal multitask"}, {"title": "4.3 Multimodal fusion methods", "content": "To overcome the limitations of unimodal approaches, multimodal techniques integrate data from multiple modalities, aiming to exploit the complementary information available. We refer the reader to [29] for a comprehensive review of various methods for multimodal fusion and representation learning, but summarize several baselines that we benchmark on MULTIMED below:\nEarly fusion: Data from different modalities are combined at the input level, allowing the model to learn directly from multimodal input data. This approach is straightforward but may not handle modality-specific features effectively, and tends to require larger multimodal models due to larger input dimensionality. Early fusion can be mathematically represented as:\n$X_{early} = \\phi(\\{x^{(1)}, x^{(2)},...,x^{(M)}\\}),$ (2)\nwhere $\\phi$ is a fusion function, such as concatenation or summation, applied across modal inputs.\nIntermediate fusion: Features from each modality are extracted separately and then combined at one or more hidden layers within the model. This allows the model to process each modality according to their unique information before integration. The function for intermediate fusion looks like:\n$h_{inter} = \\psi(\\{h^{(1)}, h^{(2)},...,h^{(M)}\\}),$ (3)\nwhere $g^{(m)}$ is a function that extracts features from modality m, $h^{(m)} = g^{(m)} (x^{(m)}; \\theta^{(m)})$ and $\\psi$ is the fusion function combining intermediate representations.\nLate fusion: Each modality is processed through separate models, and their predictions are combined at the output stage. This method is suitable when there is primarily unique information in each modality and less synergistic integration across modalities. Late fusion can be modeled as:\n$Y_{late} = \\omega(\\{y^{(1)}, y^{(2)}, ..., y^{(M)}\\}),$ (4)\nwhere $y^{(m)} = f^{(m)} (x^{(m)}; \\theta^{(m)})$, and $\\omega$ is a decision-level fusion function such as weighted averaging or voting."}, {"title": "4.4 Multimodal and multitask learning", "content": "Further building upon multimodal fusion, multimodal multitask fusion involves learning from multiple modalities and performing multiple tasks simultaneously. This approach not only leverages the complementary strengths of different modalities but also exploits the relationships across several medical tasks. For example, models might use imaging, genetic, and clinical text data to simultaneously diagnose diseases, predict prognoses, and recommend treatments. This holistic approach aims to maximize the use of available data and task-related knowledge, potentially leading to more robust and effective models. Multimodal multitask learning can be formalized as follows:\n$\\Theta^* = arg \\min_{\\Theta} \\min \\sum_{t=1}^T \\sum_{m=1}^M \\lambda_{t,m}L_t(f_t(x^{(m)}; \\Theta), Y_t),$ (5)\nwhere $\\Theta$ denotes the collective parameters of the model across all modalities and tasks, $L_t$ is the loss function associated with task t, $f_t$ is the prediction function for task t, and $\\lambda_{t,m}$ are weighting coefficients that balance the importance of each task and modality."}, {"title": "5 Experiments", "content": "In this section, we describe the experimental setup to evaluate the performance of models on MULTIMED, and the results from this comprehensive analysis."}, {"title": "5.1 Experimental setup", "content": "Evaluation metrics. To assess performance across these datasets, we employ the average accuracy score on the test set, computed over three runs with different seeds. Accuracy is particularly suitable for tasks where outcomes are categorical and labels are balanced. For gene expression prediction, we adopt the Pearson correlation score. This measure evaluates the linear correlation between the predicted and actual gene expressions, offering insight into the precision of the model's quantitative outputs. It is especially relevant in this context as gene expression data is continuous and predictions can vary in scale. For protein structure prediction, we use the TM score (Template Modeling score) to evaluate the quality of the predicted 3D structure. The TM score compares the predicted protein structures against the actual structure to assess the similarity in the spatial arrangement of the protein's backbone. A higher TM score, closer to 1, indicates a model's effectiveness in predicting complex protein folds, which is crucial for understanding protein function and interaction.\nImplementation details and computation. We employed a variety of state-of-the-art neural network architectures tailored to each data modality. For imaging data (X-ray, MRI, CT), vision transformers were primarily used [14]. For genomic and scRNA-seq data, we utilized attention-based models [19, 46] to capture complex biological interactions and dependencies. EEG data were processed using recurrent neural networks (RNNs) with LSTM units to effectively handle their time-series nature [4]. We utilized Adam optimizer with a learning rate initially set at 0.001 and employed a decay mechanism to reduce the learning rate gradually as the training progressed. The training was performed on a batch size of 64 for imaging data and up to 256 for genomic data. Experiments were conducted on a high-performance computing cluster equipped with NVIDIA Tesla A100 GPUs."}, {"title": "5.2 Experimental results", "content": "We present results across different modalities and tasks in Table 1. On all tasks, the multimodal multitask approach achieved the highest performance, demonstrating the value of integrating multiple data sources to enhance disease diagnostic accuracy. Some tasks with the largest improvement were disease classification, from 45.39% (unimodal) to 61.89%, and Medical Visual Question Answering (VQA) from 49.35% (unimodal) to 69.38%. These improvements suggest that integrating diverse data types greatly aids in complex tasks that require a holistic understanding of medical conditions and patient data. Other tasks such as brain tumor classification and diabetic retinopathy classification also showed notable improvements (54.27% to 73.52% and 57.86% to 67.86% respectively). These tasks benefit from the fusion of imaging modalities with clinical data, highlighting the method's ability to leverage detailed visual information effectively. Finally, multimodal multitask approaches are also able to fuse complex biological data, improving cell type classification and expression prediction from 55.72% and 53.25% to 71.15% and 72.35% respectively. For protein structure prediction, the improvement was from 35.15% to 53.28%, which was one of the more modest increases due to the task's complexity."}, {"title": "5.3 Experimental analysis", "content": "We now study various out-of-distribution scenarios and the model's capabilities in zero-shot and few-shot learning contexts on MULTIMED.\nOrgan out-of-distribution analysis. One critical aspect of medical model evaluation is the ability to perform well in out-of-distribution (OOD) scenarios, particularly when dealing with data from organs not seen during the training phase. In this part, we analyze the performance of models when they are tested on organ data that were excluded from the training set. The models are evaluated based on their accuracy, sensitivity, and specificity in these OOD scenarios, as reported in Figure 2 (top). The OOD performance across different organs shows relatively consistent results for disease classification (in blue), with most organs demonstrating accuracy rates above 50%. Adrenal, cerebellum, and lung organs display the best generalization performance, and intestine, liver, and muscle organs show the least generalization. For cell classification tasks (in orange), the variability is somewhat larger, indicating that certain organs like the kidney and pancreas might possess unique cellular structures that are not easily generalizable without direct training data. The expression prediction tasks (in green) show less variability than cell classification, which might suggest that gene expression patterns are more conserved across different organs than cellular phenotypes, thus showing more consistent OOD performance.\nCell type out-of-distribution analysis. Similar to the organ OOD analysis, we also explore the performance of models on cell type OOD scenarios in Figure 2 (bottom). This analysis tests the models' ability to classify or predict outcomes based on cell types that were not present in the training dataset, which is critical given the diversity and specificity of cell types involved in various diseases. The performance in disease classification tasks (in blue) across different cell types remains consistent, with most cell types showing around 60% accuracy. This indicates that the learned models can transfer pathological features associated with diseases across different cell types not seen during training. The cell type classification results (in orange) show that the models are capable of maintaining relatively stable performance across different cell types, even in OOD scenarios. Notably, certain cell types like Astrocytes and Schwann cells exhibit slightly higher accuracy in disease classification and expression prediction, which may suggest that these cell types have more generalizable features, while Lymphoid"}, {"title": "6 Conclusion", "content": "MULTIMED is a comprehensive framework designed for advancing the state-of-the-art in multimodal and multitask medical data analysis, with 2.56 million samples across ten diverse modalities and eleven challenging medical tasks from disease classification to medical visual question answering. Our experiments demonstrated the superiority of multimodal and multitask learning approaches that leverage multiple data modalities in terms of overall performance, few-shot generalization, and robustness to diverse organs and cell types. We also highlight how novel combinations of data modalities can be orchestrated to optimize performance across medical tasks, revealing insights into the synergistic effects of data integration.\nLimitations. While the MULTIMED benchmark represents a significant step forward in multimodal medical data analysis, some limitations exist and are avenues for future work. Although efforts were made to address data bias, the benchmark may still contain biases inherent in the dataset collection processes or the methods used, which can result in performance imbalances between gender or demographic groups. Addressing these limitations is essential for the next phase of development in multimodal medical machine learning. Future iterations of MULTIMED should also aim to expand dataset diversity, investigate more expressive multimodal fusion techniques, reduce computational demands, enhance model fairness, and improve model robustness and adaptability.\nBroader impact. We are aware that applying AI in real-world healthcare settings can always carry a risk. Therefore, broad evaluation frameworks like MULTIMED are necessary to ensure that models are sufficiently robust and prevent unintended consequences when deployed. Future work can also work towards adding more metrics to MULTIMED measuring real-world societal concerns such as fairness, privacy, efficiency, and accessibility to all demographic groups. Fairness can be measured using new metrics such as individual or group fairness with respect to treatment outcomes. Privacy metrics such as differential privacy can be used to test the sensitivity of a model's prediction to an individual datapoint in the training set, therefore characterizing how much the model was relying on potentially private information from that training datapoint. MULTIMED also offers opportunities to design more efficient multimodal AI models for healthcare, since any efficiency innovations can be tested at scale across multiple medical modalities and tasks. Finally, we are working with medical experts on scientific knowledge discovery from the trained models on MULTIMED, which can make these results more accessible to practitioners. It is crucial to continue evaluating these impacts to ensure that the advancements in AI improve healthcare outcomes for all."}, {"title": "Appendix", "content": "In this supplementary material, we provide the following material:\n\u2022 addition implementation and datasets details in Section A,\n\u2022 detailed experimental setup in Section B,\n\u2022 details about evaluation metrics in Section C,\n\u2022 additional experimental analyses in Section D,\n\u2022 additional qualitative visualization results in Section E,\n\u2022 dataset documentation and intended uses in Section F."}, {"title": "A Detailed Benchmark", "content": "In this section, we provide a more detailed description of the modalities and tasks included in the MULTIMED benchmark. This expanded information aims to assist researchers in understanding the scope and depth of the dataset, facilitating its effective utilization and promoting the development of advanced multimodal and multitask machine learning models."}, {"title": "A.1 Modalities", "content": "MULTIMED encompasses a wide range of modalities to ensure a comprehensive representation of medical data:\n1. Imaging Modalities [45, 49, 9, 22, 24]:\n- Optical Coherence Tomography (OCT): This dataset includes 84,495 high-resolution cross-sectional images of the retina. OCT is widely used in ophthalmology to diagnose and monitor diseases such as macular degeneration, diabetic retinopathy, and glaucoma. The high level of detail in OCT images allows for precise visualization of the retinal layers, facilitating early detection and treatment of retinal conditions.\n- X-ray: Comprising 194,922 images, this dataset covers various body parts, with a significant focus on chest X-rays used for diagnosing lung diseases such as pneumonia, tuberculosis, and COVID-19. X-rays are a fundamental diagnostic tool in medicine due to their ability to provide quick and non-invasive imaging of internal structures.\nCT (Computed Tomography): With 617,775 scans, the CT dataset offers detailed cross-sectional images of internal organs and tissues. CT imaging is crucial for diagnosing a wide range of conditions, including cancers, cardiovascular diseases, and traumatic injuries, by providing a more detailed view than standard X-rays.\nMRI (Magnetic Resonance Imaging): This dataset includes 7,023 scans, primarily used for detailed imaging of soft tissues such as the brain, musculoskeletal system, and internal organs. MRI is essential in diagnosing neurological conditions, musculoskeletal disorders, and detecting tumors, as it provides high-contrast images of soft tissues without the use of ionizing radiation.\nPathology Images: Encompassing 27,560 images, this dataset provides microscopic views of tissues, aiding in the diagnosis and research of diseases. Pathology images are critical for understanding the cellular and molecular basis of diseases, enabling pathologists to identify abnormalities and classify disease states accurately.\n2. Electrophysiological Data [6]:\nEEG (Electroencephalography): With 120,000 samples, this dataset captures the electrical activity of the brain. EEG is used in various tasks such as seizure detection, sleep studies, and brain-computer interface (BCI) applications. The dataset focuses on tasks like imagined motor imagery classification, where subjects imagine specific movements, and the EEG signals are used to interpret these mental actions, potentially aiding in the development of assistive technologies for individuals with motor impairments.\n3. Molecular Data [10, 39, 8]:\nGenomic Sequences: This dataset includes 12,560 samples, facilitating studies on genetic markers and mutations. Genomic data is crucial for understanding the genetic basis of dis-eases, identifying potential therapeutic targets, and personalizing medical treatments based on an individual's genetic profile."}, {"title": "A.2 Tasks", "content": "The tasks included in MULTIMED are designed to test the adaptability and generalization capabilities of learning models across a variety of medical challenges. Below is a detailed description of each task:\n1. Disease Classification:\nObjective: Categorize patient data into disease categories based on symptoms, lab results, and imaging data.\nChallenge: Recognizing and differentiating between a wide range of diseases, from common to rare conditions.\n2. Brain Tumor Classification:\nObjective: Identify and classify various types of brain tumors using MRI scans.\nChallenge: Distinguishing between tumor types that often have similar appearances in imaging data.\n3. Breast Cancer Classification:\nObjective: Identify and classify stages of breast cancer using mammography and histopathology images.\nChallenge: Detecting subtle variations between stages and achieving high accuracy for clinical relevance.\n4. Radiographic Findings Classification:\nObjective: Classify findings in X-ray and CT images, such as fractures or lung nodules.\nChallenge: Handling the diversity of possible findings and their presentations in medical images.\n5. Bone Age Classification:\nObjective: Estimate the skeletal maturity of a patient based on hand X-rays.\nChallenge: Precise estimation as it is crucial for diagnosing growth disorders in pediatrics.\n6. Diabetic Retinopathy Classification:\nObjective: Classify the severity of diabetic retinopathy from retinal photographs.\nChallenge: Grading the subtle and progressive nature of the disease.\n7. Imagined Motor Imagery Classification:\nObjective: Classify the type of motor imagery a subject is thinking about using EEG data.\nChallenge: Interpreting noisy EEG signals and their low spatial resolution.\n8. Cell Type Classification:\nObjective: Identify cell types from single-cell RNA sequencing data based on their gene expression profiles.\nChallenge: Handling high-dimensional data and distinguishing between closely related cell types.\n9. Expression Prediction: - Objective: Predict the expression level of genes from various inputs such as genetic markers or environmental conditions."}, {"title": "F.3 Dataset Access", "content": "The dataset is available on our website, accessible via https://multimed.github.io, where researchers can view and download the data upon agreeing to our terms of use. This website ensures easy access and use of the data in compliance with all relevant ethical standards. The metadata record is available for our Croissant metadata to be viewed and downloaded."}, {"title": "F.4 Author Statement", "content": "The creators of the MULTIMED dataset bear all responsibilities in case of violation of rights and confirm that the dataset is released under the Creative Commons Attribution 4.0 International License. This license allows users to share and adapt the material provided the original work is properly cited, and adaptations are shared under the same terms."}, {"title": "F.5 Hosting, Licensing, and Maintenance Plan", "content": "The dataset is hosted on our website, ensuring reliable and scalable access. The chosen platform provides the necessary security measures to protect the data and users' privacy. The dataset will be maintained by the authors, who will handle regular updates, respond to user inquiries, and ensure the dataset's integrity over time. Maintenance will include updating the dataset documentation, fixing reported issues, and improving the platform based on user feedback.\nThe MULTIMED dataset is a carefully collected and maintained resource aimed at advancing research in multimodal and multitask medical data analysis. By providing detailed documentation and a clear usage plan, we aim to foster an environment of innovation and ethical use of AI in healthcare."}]}