{"title": "Predictive Models in Sequential Recommendations: Bridging Performance Laws with Data Quality Insights", "authors": ["Tingjia Shen", "Hao Wang", "Chuhan Wu", "Jin Yao Chin", "Wei Guo", "Yong Liu", "Huifeng Guo", "Defu Lian", "Ruiming Tang", "Enhong Chen"], "abstract": "Sequential Recommendation (SR) plays a critical role in predicting users' sequential preferences. Despite its growing prominence in various industries, the increasing scale of SR models incurs substantial computational costs and unpredictability, challenging developers to manage resources efficiently. Under this predicament, Scaling Laws have achieved significant success by examining the loss as models scale up. However, there remains a disparity between loss and model performance, which is of greater concern in practical applications. Moreover, as data continues to expand, it incorporates repetitive and inefficient data. In response, we introduce the Performance Law for SR models, which aims to theoretically investigate and model the relationship between model performance and data quality. Specifically, we first fit the HR and NDCG metrics to transformer-based SR models. Subsequently, we propose Approximate Entropy (ApEn) to assess data quality, presenting a more nuanced approach compared to traditional data quantity metrics. Our method enables accurate predictions across various dataset scales and model sizes, demonstrating a strong correlation in large SR models and offering insights into achieving optimal performance for any given model configuration.", "sections": [{"title": "1. Introduction", "content": "Sequential Recommendation (SR), focused on suggesting the next item for a user based on their past sequential interactions to capture dynamic user preferences, has gained significant attention in commercial, internet, and diverse scenarios. However, as the volume of user data increases, more expansive recommendation models are being implemented. The high computational requirements of these recommendation models lead to considerable expenses and unpredictability during development. This places added stress on developers to allocate resources efficiently and GPU consumption. To anticipate how recommendation models will perform without executing full-scale experiments, researchers have crafted a range of scaling laws to evaluate the models' effectiveness in various scenarios.\nScaling laws were first explored in the context of Large Language Models. Specifically, since the introduction of the Chinchilla scaling law, which models the final pre-training loss L(N, D) as a function of the number of model parameters N and the number of training tokens D, models such as LLaMA2, Mistral, and Gemma have applied this principle. However, merely increasing model size does not indefinitely enhance performance. A line of research focuses on the generalization behavior of over-parameterized neural networks. Recent experiments reveal that over-trained transformers exhibit an inverted U-shaped scaling behavior, which existing empirical scaling laws cannot explain. Following the success of scaling laws in large language models, many researchers have also sought to apply these principles to recommendation models, such as HSTU and Wukong."}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Sequential Recommendation", "content": "The focus of recommendation systems has undergone significant transformations. Among these, sequential recommendation is a technique that aims to delve into and understand users' interest patterns by analyzing their historical interactions. Initially, techniques such as markov chain and matrix factorization were employed. However, with the emergence of neural networks, deep learning approaches have been developed for sequential recommendation tasks. Convolutional Neural Networks(CNN) ,Graph Neural Networks (GNN) from GCE-GNN to SR-GNN and URLLM. Multilayer Perceptron (MLP) modeled SR from various perspectives, while the Diffusion-based method and Fourier transform-based method are used to attempt noise reduction. However, the compatibility of RNN with sequences has ultimately garnered more attention. Early work like GRU4Rec and Caser were introduced to improve recommendation accuracy. Another notable technique in sequential recommendation is the attention mechanism. SASRec , for instance, utilizes self-attention to independently learn the impact of each interaction on the target behavior. On the other hand, BERT4Rec incorporates bi-directional transformer layers after conducting pre-training tasks. Since LLaMA4Rec and HSTU demonstrated improvements in recommendation performance brought by large models and large datasets, It is meaningful to study how the model performance would change as the model size scales up."}, {"title": "2.2. Scaling Law on Large Sequential Models", "content": "Scaling laws was first explored in the context of Large Language Models. Specifically, since the introduction of the Chinchilla scaling law, which models the final pre-training loss L(N, D) as a function of the number of model parameters N and the number of training tokens D, models such as LLaMA2, Mistral, and Gemma have applied this principle. Empirical evidence indicates that model performance consistently improves with increased model size and training data volume. Extensive experiments have explored neural scaling laws under various conditions, including constraints on computational budget , data limitations and regeneration , and instances of over-training. These analyses employ a decomposition of expected risk, resulting in the fit of\n$L(N, D) = \\frac{ON}{N^{\\alpha_N}} + \\frac{AD}{D^{\\alpha_D}}$, where ON and AD are parameters. However, increasing the model size does not necessarily lead to better performance. Some studies have observed a decline in performance due to overfitting . Following theoretical analysis, and empirically validated this point, underscoring the necessity for an enhanced understanding of scaling laws."}, {"title": "3. Preliminary and Theorem", "content": ""}, {"title": "3.1. Problem Definition", "content": "Sequential Recommendation (SR) focuses on predicting the next item a user is likely to interact with, given a sequence of previous interactions. This task is essential in personalized systems, as it considers the temporal dynamics and evolving preferences of users. Formally, let U denote the set of users and I the set of items. For a specific user u \u2208 U, we represent their interaction history as an ordered sequence Su = [11, 12, ..., in], where each ik \u2208 I indicates an item interacted with at time step k. The objective is to model the probability distribution over I, given Su, to predict the subsequent item in+1. This problem is challenging due to the complex, dynamic patterns in user behavior, which are influenced by various contextual factors. Effective solutions must balance capturing short-term user intent and leveraging long-term preference structures."}, {"title": "3.2. Preliminary", "content": ""}, {"title": "3.2.1. Scaling Law on Large Sequential Models", "content": "As we introduced in the Related Work Section 2.2, empirical evidence indicates that model performance consistently improves with increased model size and training data volume. These analyses employ a decomposition of expected risk, resulting in the following fit:\n$P(N, D) x \u2212[L(N, D)+\\alpha_Nln(N) + \\alpha_pln(D)]$\n$P(N, D) x -L(N, D)$\n$L(N,D) = [\\frac{N}{N^{\\alpha_N}} + \\frac{D}{D^{\\alpha_D}}]$,\nwhere $\\alpha_N$ and $\\alpha_D$ are parameters. In some works, the scaling law formula might be simplified to L'(N, D) =\n$\\frac{A}{N^{\\alpha}}$+ $\\frac{B}{D^{\\beta}}$ with parametera and \u03b2. For example, in Chinchilla , the fitted parameters are:\nE = 1.61, A = 406.4, B = 410.7 $\\alpha$ = 0.34, \u03b2 = 0.28\nHowever, directly applying the scaling laws of LLMs to the recommendation domain faces inconsistencies between language sequence modeling and recommendation sequence modeling. To address this, we continue to refine and supplement the scaling laws for recommendations to better adapt to the distribution characteristics of SR."}, {"title": "3.2.2. \u0391\u03a1\u0395N EXTENSION ON RECOMMENDATION", "content": "As mentioned above, although recommendation and text datasets contain sequential information, the structural differences and significant variations in user preferences prevent a straightforward application of token-based scaling laws for SR. Therefore, we introduce the Approximate Entropy (Pincus, 1991) (ApEn) factor to further enhance scaling laws in SR.\nSpecifically, ApEn is a statistical measure used to quantify the regularity and unpredictability of time series data, which is computed as follows: First, for a given time series {$x_i$} of length N and parameters m (embedding dimension) and r (tolerance), we construct a m-dimensional vectors\n$X_i = [X_i, X_{i+1},..., X_{i+m\u22121}]$ for i = 1,..., N \u2212 m+1.\nSubsequently, the distance between two such vectors Xi and Xj is defined as\n$d[X_i, X_j] = max |X_{i+k}-X_{j+k}|$.\nNext, for a given tolerance r, the similarity measure $C_m(r)$ is calculated as\n$C_m(r) = \\frac{|{j | d[X_i, X_j] < r}|}{N-m+1}.$\nThe average similarity $\\Phi_m(r)$ is then computed as\n$\\Phi_m(r) = \\frac{1}{N-m+1} \\sum_{i=1}^{N-m+1}ln C_m(r)$.\nThe Approximate Entropy is finally defined as\nApEn(m, r, N) = $\\Phi_m(r) \u2013 \\Phi_{m+1}(r)$.\nIn our subsequent calculation of ApEn, we set the tolerance r = 0. This decision is due to the unique nature of recommended items, where products with similar IDs can potentially convey entirely different meanings."}, {"title": "3.3. Theorem of Performance Law", "content": "Before accurately predicting model performance, we need to consider factors such as model decay as the model size increases indefinitely, the relationship between model performance and loss, and how data scale correlates with token count and dataset Approximate Entropy (ApEn). For the first aspect, we introduce a log(.) decay term when the model layer H and embedding dimension demb is large, as proven in Theorem 3.5. Regarding the second aspect, we assume a linear relationship between model performance and loss, expressed as Performance = 1 - kL, Performance = HR@10, NDCG@10, For the final point, we provide a theoretical proof in Theorem 3.3, establishing the data scale D is bounded by D = $\\frac{\\#Tokens}{ApEn}$. Building on the aforementioned theorems, we ultimately fit the model's performance into the following equation:\nPerformance = $w_1(log N + \\frac{P1}{Nw_3}) + w_2(log d_{emb} + \\frac{P2}{d_{emb}w4}) + log D' + \\frac{q}{D^{\\'w5}}$\nwhere D' = $\\frac{\\#Tokens}{ApEn}$ represents the data parameter, N is the number of model layers, and demb is the item embedding dimension. When controlling model size, we only adjust the number of layers and embedding dimension, which is why our formula includes only these two parameters. However, our theoretical framework can still be applied for analysis when other model parameters are modified. Below is the detailed proof of our performance law theory:\nLemma 3.1. In the first-order stationary Markov chain (discrete state space X) case, with r< min(|x-y|, x\u2260y, x and y state space values), a.s. for any m\nApEn(m, r) = \u2013 $\\sum_{X\u2208X} \\sum_{y\u2208X} \\pi(x)P_{xy}log(P_{xy})$.\n, where \u03c0(x) is the stationary distribution of x.\nProof. By the definition of ApEn, ApEn = -E(log(Cm+1(r)/Cm(r))).\nApEn = -E(log(Cm+1(r)/Cm(r)))\n= -Ej (log P($X_{j+m} - x_{m+1}$| \u2264 r ||\n|$X_{j+k\u22121} - x_{k}$ <r fork = 1,2, .., m)\n= -Ej (log P($x_{j+m}$ = $X_{m+1}$ || $X_{j+m\u22121}$ = $x_{m}$))\n= -Ej (log P($x_{j+m}$ = $X_{m+1}$ || $X_{j+k\u22121}$ = $x_{k}$\nfor k = 1, 2, .., m)\n= -$\\sum_{X\u2208X} \\sum_{y\u2208X} P(x_{j+m} = y,x_{j+m\u22121} = x)\n(log P($x_{j+m}$ = y, $X_{j+m\u22121}$ = x)/P($x_{j+m\u22121}$ = x))\n- $\\sum_{X\u2208X} \\sum_{y\u2208X} \\pi(x)P_{xy}log(P_{xy})$.\nLemma 3.2. If $x_i, y_i, i = 0,1,..., n$, and $\\sum_{i=1}^{q} xi$= $\\sum_{i=1}^{q} yi$ = 1, then\n$\\sum_{i=1}^{q} x_i log_r \\frac{x_i}{y_i} < - \\sum_{i=1}^{q} x_i \\frac{x_i}{y_i}$"}, {"title": "4. Methodology", "content": "Following the prior empirical study in SR, for all experiments, we adopt the decoder-only transformer models as the backbone. Specifically, for each user u, items and rating scores in the user behavior sequence Xu = {11, r1, ..., ik, rk, ..., in, rn} are firstly encoded into embeddings, forming eu = {e^1, er1, ..., eik, erk, . . ., ein, er^n}. After the embedding layer, we stack multiple Transformer decoder blocks. At each layer 1, query Q, key K, and value V are projected from the same input hidden representation matrix H\u00b9. We modified the SiLU activation module and the Rab positional encoding module within the standard transformer block to ensure the model's effectiveness. In Section 5.4.2, we will discuss the impact and improvements brought by these modules and demonstrate that our model is also applicable to standard transformers. Specifically, the modification is mainly on two core sub-layers: the spatial aggregation layer, and the pointwise transformation layer:\nFirstly, the Spatial Aggregation Layer is defined as follows:\nAttn(e{i,r}k)V = SiLU(QKT + Rab)V, k = 1, ..., n\nUpon deriving matrices query Q, key K, and value V, the spatial aggregation layer utilizes an attention mechanism to adjust V. This layer is distinguished by two key features: First, the SiLU activation replaces the standard softmax function typical in transformer models, effectively handling large and non-stationary data sets by removing the necessity for full normalization, thereby improving computational efficiency and stability. Second, a relative attention bias is applied, enriching the model with positional and temporal information, thus enhancing its ability to discern contextual interrelations and capture dependencies within the data.\nSubsequently, the Pointwise Transformation Layer is defined as follows:\ne'{i,r}k = Norm(Attn(e{i,r}k)V Gate(e{i,r}k), k = 1, ..., n\nFollowing the spatial aggregation layer, the pointwise transformation layer applies a transformation to each individual data point independently. Here, the gating weights Gate(X) are combined with the normalized values Norm(Attn(X)V(X)) via a Hadamard product, effectively gating the transformed representations and allowing"}, {"title": "5. Experimental Evaluation", "content": ""}, {"title": "5.1. Datasets", "content": "To demonstrate the performance of our proposed model, we conducted experiments on three publicly available datasets and a private dataset, whose introduction is as follows:\n\u2022 MovieLens-1M : This dataset is a standard benchmark in recommendation systems research, containing 1 million ratings from 6,000 users on 4,000 movies, featuring ratings from 1 to 5, along with demographic and movie metadata.\n\u2022 Amazon Books : Amazon Books, a subset of the Amazon review dataset, includes"}, {"title": "5.2. Recommendation Fit on Scaling Law", "content": "To validate the soundness of our Theory 1, we aim to demonstrate: (1) our model's adherence to Scaling Laws, and (2) the appropriateness of using both Approximate Entropy (ApEn) and token count to assess data scale. Specifically, we first examine the alignment of the model's loss curve with Scaling Laws. We then analyze Loss across various datasets, fitting it to traditional Scaling Laws to obtain data parameters. If the fitted data exhibits a clear linear relationship with the combination of ApEn and token count, it will sufficiently substantiate the validity of Theory 3.3."}, {"title": "5.3. Recommendation Fit on Performance Law", "content": "To validate the soundness of our Theory 2, we need to verify that: (1) Our formula demonstrates a strong fitting correlation coefficient under different model parameters. (2) Using ApEn and token count as measures of data quality, there is still a strong linear correlation with the data parameter D'."}, {"title": "5.4. Applications of Performance Law", "content": ""}, {"title": "5.4.1. APPLICATION 1: GLOBAL AND LOCAL OPTIMAL PARAMETER SEARCH", "content": "An intriguing and practical application of the performance law lies in predicting the performance gain from model expansion techniques. Due to the inclusion of a decay term in our performance law, it becomes feasible to potentially achieve a global optimum. From the aforementioned fitting, we derived two distinct performance law formulations from HR and NG, as follows:\nHR=0.50(34N\u22120.0001 + ln(N)) - 19.01(3 * d-0.0365\nemb+ ln(demb)) + 2.5D\u2032\u221212.74 + ln(D\u2032) + 628.12\nNG = 0.63(31N-0.0001 + ln(N)) \u2013 24.85(31d-0.0395\nemb+ln(demb)) + 3D\u2032\u22120.0410 + ln(D\u2032) + 743.9"}, {"title": "5.4.2. APPLICATION 2: EXPLORING SCALING LAW POTENTIAL AMONG FRAMEWORK", "content": "Another application of the Performance Law is to observe the potential performance gains when scaling up the model. We conducted experiments and fitting analyses on three different frameworks (HSTU, LLaMA2, and SASRec), evaluated at different precisions (float32 and bfloat16). We conducted experiments on the smallest dataset (ML-1m) to facilitate the models in reaching their optimal upper bounds more easily. Their optimal results were fitted with respect to the coefficient and exponent terms of N and d, respectively denoted as (w\u2081, w3) and (w2, w4). In the expressionsW1NW3andW2dw4, when the coefficient terms w\u2081 and w2are negative, larger values of w3 and w4 indicate poorerscaling-up potential of the model, and vice versa. Our results, along with their performance under optimal parameters, are presented in Table 4. In all the fittings conducted here, both w\u2081 and w\u2082 are positive values. As observed from the table, the model's performance closely aligns with the magnitude trend of w3 and w4, which further underscores the accuracy of our fitting in predicting the model's performance."}, {"title": "6. Conclusion", "content": "In conclusion, this paper addressed two critical challenges in the domain of SR: the discrepancy between model loss and actual performance, and the adverse effects of data redundancy on model efficacy. Our introduction of the Performance Law for SR models marks a significant advancement in the theoretical exploration of performance metrics in relation to SR models, emphasizing the role of data quality over sheer quantity. By fitting performance metrics such as hit rate HR and NDCG to transformer-based SR models and proposing the use of ApEn, we offer a novel approach that enhances the predictive accuracy of model outcomes. This framework not only provides a more nuanced understanding of how data quality impacts model performance but also facilitates the strategic balancing of computational resources to achieve optimal results. Our findings demonstrate that meaningful predictions regarding model performance can be achieved across varying scales of datasets and model sizes, thereby delivering valuable insights into optimizing recommendation systems in practical applications."}]}