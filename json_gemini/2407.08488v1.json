{"title": "Lynx: An Open Source Hallucination Evaluation Model", "authors": ["Selvan Sunitha Ravi", "Bartosz Mielczarek", "Anand Kannappan", "Douwe Kiela", "Rebecca Qian"], "abstract": "Retrieval Augmented Generation (RAG) techniques aim to mitigate hallucinations in Large Language Models (LLMs). However, LLMS can still produce information that is unsupported or contradictory to the retrieved contexts. We introduce LYNX, a SOTA hallucination detection LLM that is capable of advanced reasoning on challenging real-world hallucination scenarios. To evaluate LYNX, we present HaluBench, a comprehensive hallucination evaluation benchmark, consisting of 15k samples sourced from various real-world domains. Our experiment results show that LYNX outperforms GPT-40, Claude-3-Sonnet and closed and open-source LLM-as-a-judge models on HaluBench. We release LYNX, HaluBench and our evaluation code for public access.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) learn in-depth knowledge from their pre-training data (Petroni et al., 2019) making them useful for knowledge-intensive downstream tasks such as Question Answering. However, their knowledge cannot be easily expanded and they often struggle with \u201challucinations\" (Roller et al., 2020; Dziri et al., 2022; Cao et al., 2022). This has led to the adaptation of Retrieval Augmented Generation (RAG) (Lewis et al., 2020) systems, which enables flexibility and extensibility of LLMs to internal data stores. However, these systems are still prone to generating text that is inconsistent with the provided knowledge source (Mallen et al., 2022).\nIn an ideal RAG system, LLMs exhibit \u201cfaithfulness\" by producing outputs that are grounded in the retrieved contexts. Detecting whether generated answers are faithful to the provided context is therefore critical to the success of RAG systems in production. RAGAS (Es et al., 2023) use LLMs to generate statements from a question-answer pair and compute a faithfulness score based on how many statements are supported by the given context. Other methods involve using LLM-as-a-Judge (Zheng et al., 2023) or fine-tuning lightweight LLM judges (Saad-Falcon et al., 2024) to evaluate hallucinations.\nWhile LLMs as judges have shown promise in automated evaluation on certain tasks (Zheng et al., 2024; Zhu et al., 2023; Kim et al., 2023), hallucination detection presents a complex challenge as it requires language models to have ability to perform nuanced reasoning and disambiguation.\nAdditionally, closed source LLMs as judges lack transparency and accessibility. Open-source LLMs still exhibit a significant gap in performance compared to closed source alternatives (Li et al., 2023). The gap in baseline performance between closed and open-source models increases when applied to specialized domains such as finance and medicine (Islam et al., 2023; Wu et al., 2023a).\nTo address these issues, we propose LYNX (70B) that outperforms GPT-40 and closed source LLMs in hallucination detection tasks, while being fully reproducible and open source. LYNX (8B) produces high quality evaluations at a fraction of the size and cost of closed source LLMs. LYNX is the first open source hallucination detection model that outperforms GPT-40 and closed source LLMs-as-Judge.\nTo train LYNX, we finetune Llama-3-70B-Instruct on data from multiple domains, focusing on hard-to-detect hallucinations. We source examples from existing Question Answer (QA) datasets such as CovidQA (M\u00f6ller et al., 2020), PubmedQA (Jin et al., 2019), DROP (Dua et al., 2019) and FinanceBench (Islam et al., 2023), introducing perturbations to generate hallucinated answers that appear plausible but are not faithful to the context.\nEvaluating the performance of different LLMs as judges in real-world hallucination detection tasks is difficult due to the lack of a comprehensive and diverse benchmark. Halueval (Li et al., 2023) and RAGTruth (Wu et al., 2023b) provide a large collection of generated and human-annotated hallucinated samples but cover limited domains. To evaluate hallucination detection systems, we construct HaluBench, a large scale hallucination evaluation benchmark that consists of 15k hallucinated as well as faithful responses to questions across multiple real-world domains.\nOur contributions are as follows:\n\u2022 We present HaluBench, a hallucination evaluation benchmark of 15k samples that consists of Context-Question-Answer triplets annotated for whether the examples contain hallucinations. Compared to prior datasets, HaluBench is the first open-source benchmark containing hallucination tasks sourced from real-world domains that include finance and medicine.\n\u2022 We train LYNX, the first open-source LLM capable of high quality, reference-free hallucination detection in RAG settings. We show that LYNX outperforms GPT-40, Claude-3-Sonnet and other closed and open-source models on HaluBench.\n\u2022 We propose a novel method to generate hard-to-detect hallucination examples from Question Answering tasks by applying semantic perturbations to LLM responses. We find that our perturbed examples are challenging for LLM judges, as nuanced differences in semantic meaning can lead to different reasoning outcomes.\n\u2022 We conduct experiments to benchmark LYNX against closed and open-source LLMs and RAG evaluation metrics. We release all models, datasets and experiment results for public access."}, {"title": "2 Related Work", "content": "While LLMs have shown remarkable performance in knowledge-intensive tasks such as Question Answering, one of the major drawbacks is generation of inaccurate or false information (Azaria and Mitchell, 2023; Ji et al., 2023). Several techniques have been proposed to evaluate hallucinations in LLMs (Guerreiro et al., 2022; Lin and Chen, 2023; Manakul et al., 2023) including Retrieval-Augmented Generation (RAG) (Lewis et al., 2020; Shuster et al., 2021a; Yu, 2022; Biswas et al., 2022). By leveraging retrieval, RAG helps LLMs acquire domain-specific knowledge and ground their outputs in factual information (Shuster et al., 2021b). However, RAG systems can still suffer from hallucinations (Wu et al., 2023b; Li et al., 2023; Es et al., 2023; Saad-Falcon et al., 2024). Automatic evaluation is crucial for quickly deploying these systems in new environments where creating a traditional benchmark dataset from the ground up is challenging.\nTo evaluate RAG systems, LLMs have been utilized to compute metrics such as answer correctness, groundedness and the relevance of retrieved contexts (Zhao et al., 2019; Yuan et al., 2021; Liu et al., 2023; Es et al., 2023; Saad-Falcon et al., 2024). RAGAS (Es et al., 2023) relies on a set of heuristic, hand-written prompts, while using LLMs and embedding-based similarity scores. Similarly, the EXAM (Sander and Dietz, 2021) metric evaluates retrieval-augmented generation (RAG) systems by estimating how many exam questions a simulated QA system can correctly answer based on the generated responses. ARES constructs LLM judges using few-shot demonstrations and a bootstrapped training dataset (Saad-Falcon et al., 2024). We find that heuristics-based metrics such as RAGAS perform poorly on hallucination tasks compared to LLM-as-judge evaluators (Table 3). While ARES offers more flexibility than prior metrics, the construction of training datasets on the fly introduces significant overhead, making the approach less suitable for production settings.\nA related area of research is Natural Language Inference (NLI) (Chen et al., 2018), where the task of categorizing whether statements entail or contradict one another is similar to detecting LLM outputs that are inconsistent with provided contexts. Recent work has drawn parallels between the task of NLI and hallucination detection (Honovich et al., 2022). While most existing models are fine-tuned solely to output an evaluation score, LYNX is trained using both reasoning chains and evaluation scores similar to NLI tasks, thereby improving the interpretability of the evaluation score.\nAnother line of research assesses the factuality of LLM responses. Several datasets have been constructed for fact extraction and verification, including FEVER (Aly et al., 2021) and AIS (Rashkin et al., 2022), which assesses whether outputs are attributable to identifiable sources. Prior work on automated factuality evaluation includes metrics and model based approaches (Ganesan, 2018; Kry\u015bci\u0144ski et al., 2019; Sellam et al., 2020). More recently, Wei et al. (2024) proposed augmenting evaluator LLMs with Google Search for scoring long form factuality. Though factuality is often measured in search-based settings, TruthfulQA (Lin et al., 2022) measures how models respond to common falsehoods and misconceptions. While factuality is important for building trust in AI systems, our work focuses on the problem of hallucination detection as it applies to RAG settings.\nTo evaluate the performance of models on hallucination detection, Wu et al. (2023b) introduced RAGTruth, a dataset of 18k responses consisting of generations from a variety of LLMs. Similarly Li et al. (2023) introduced the HaluEval dataset, which contained synthetic responses generated from LLMs that were prompted to generate hallucinatory outputs. However, these datasets do not include domain specific tasks, which can be significantly more complex and more similar to real-world scenarios that users encounter in industry applications. While CRAG (Yang et al., 2024) consists of several industry domains, the task includes external APIs for end-to-end system testing as opposed to focusing on hallucinations in provided contexts. In our construction of HaluBench we use existing datasets as well as synthetic data perturbations to construct a comprehensive hallucination evaluation benchmark that includes specialized domain specific QA tasks from finance and medicine."}, {"title": "3 Methodology", "content": "In a RAG pipeline, we first (1) Retrieve the relevant context(s) given a query, then (2) Generate an answer to the query given the retrieved context(s) with an LLM. \u201cHallucinations\" (Jian et al., 2022; Ji et al., 2023) occur when the generated answers are not faithful to the context (intrinsic hallucinations) or don't align with factual reality (extrinsic hallucinations). In this paper, we focus solely on intrinsic hallucination evaluation since in real-world settings, user-provided documents may contain information that conflicts with external knowledge sources. The purpose of LYNX is to provide a reference-free metric for automated RAG evaluation, thus we consider factuality assessments out of scope for this work.\nIn the following sections, we describe the process for training LYNX, a SOTA hallucination detection LLM. We begin with the definition of hallucination (Section 3.1), followed by the construction process of our training and evaluation data (Section 3.2). Finally, we present experimental results on hallucination tasks sourced from real-world domains."}, {"title": "3.1 Hallucination Evaluation", "content": "For a given question x, we say that the LLM is hallucinating if the answer P(x) is not supported by the context C(x) when contextualized by the question. In practice, LLM generated answers are often inconsistent with the retrieved context (Li et al., 2023). In our definition of hallucination, we do not assess the relevance of the retrieved context C(x) to the query x. If the answer, P(x) is consistent with the irrelevant context, C(x) we will consider the answer to be faithful to the context. Similarly, if the answer, P(x) to a question, x is incorrect but it states information consistent with the context, C(x) it will be evaluated as faithful."}, {"title": "3.2 HaluBenchConstruction", "content": "We sourced examples from several existing QA datasets to build the hallucination evaluation benchmark. We constructed tuples of (question, context, answer, label), where label is a binary score that denotes whether the answer contains a hallucination. HaluBench consists of the following tasks:\n\u2022 FinanceBench (Islam et al., 2023): FinanceBench consists of 10k questions, contexts and answers over financial documents, containing both tables and bullet point lists. FinanceBench was designed to be similar to real-world financial question and answering from financial analysts. We randomly sampled 1k samples, of which 500 contain hallucinations.\n\u2022 DROP (Dua et al., 2019): DROP is an English reading comprehension benchmark that assesses reasoning ability over the content of paragraphs. The dataset was crowdsourced and adversarially created. We randomly sampled 1k samples, of which 500 contain hallucinations.\n\u2022 COVID-QA (M\u00f6ller et al., 2020): COVID-QA consists of 2k question-answer pairs annotated by volunteer biomedical experts on scientific articles related to COVID-19. We randomly sampled 1k samples, of which 500 contain hallucinations.\n\u2022 PubMedQA (Jin et al., 2019): PubMedQA is a biomedical question answering (QA) dataset collected from PubMed abstracts. The task consists of answering research questions with yes/no/maybe responses. It also contains a long answer that provides evidence from the context for the response.\n\u2022 HaluEval (Li et al., 2023): HaluEval is a hallucination evaluation dataset consisting of general user queries with ChatGPT responses and task-specific examples from three tasks, i.e., question answering, knowledge-grounded dialogue, and text summarization. We used the qa_samples subset, which contains 10k questions with knowledge from Wikipedia and question text and ground-truth answers collected from HotpotQA.\n\u2022 RAGTruth (Wu et al., 2023b): RAGTruth is a corpus containing word-level hallucination annotations on LLM generated text. We used the test split that comprised of 900 samples, of which 160 examples contain hallucinations."}, {"title": "Construction of Hallucination Examples", "content": "Four of the QA datasets we sourced from (DROP, FinanceBench, COVID-QA, PubMedQA) do not contain answers that are not faithful to the context. To construct unfaithful answers, we used the Context-Question-Answer to generate semantically perturbed versions of gold answers to questions. We define a semantic answer perturbation as a response that is minimally different to the gold answer, but contains an inconsistency with the context that results in a hallucination. We use GPT-40 to construct these perturbations. The prompt and generation settings are in Appendix A.\nLet {q, c, x, y} denote the question, context, answer and label of a given example in dataset D, where y \u2208 {0,1}. For our perturbation generator fp, let x ~ fp(q, c, x) be the semantically altered perturbation output. Our perturbed dataset is thus\nD' = {(q, c, x,1 \u2212 y)|(q, c, x, y) \u2208 D} (1)\nTo construct HaluBench, we randomly sampled 500 examples from each of the four datasets. We then additionally sampled 500 examples and constructed a perturbed set containing hallucinations by applying the perturbation generator fp. The final task consisted of a balance of positive and negative labels. We present some examples from HaluBench in Table 1. The hallucinated answers from DROP and FinanceBench demonstrate answer perturbations. We adopted the same perturbation approach to construct training and validation datasets to finetune models for faithfulness evaluation."}, {"title": "Human Annotation", "content": "To verify that LLM generated samples in HaluBench are of high quality and that our perturber fp did actually induce hallucinations, we selected a random subset of 50 examples each from DROP, FinanceBench, CovidQA and PubMedQA for human annotation. Expert annotators manually checked the original and perturbed answers as well as reasoning provided for each example. Annotators found the data to be of relatively high quality (see Table 2), with high human agreement of 0.94 across 200 samples."}, {"title": "3.3 Model Training", "content": "Training Dataset Construction The training dataset for LYNX consists of 2400 samples, along with 800 samples for validation. The dataset consists of demonstrations sourced from RAGTruth, DROP, CovidQA and PubMedQA. For each subtask, we sampled 600 examples from the train split of the source dataset, of which 300 were perturbed to construct hallucinated answers.\nChain of Thought (CoT) has been shown to improve zero-shot performance of LLMs (Wei et al., 2022). To distill the evaluation reasoning capabilities of GPT-40 to our finetuned open-source model, we used GPT-40 to generate reasoning for the label of each example in our training set. We provided this as as part of the assistant response, along with the label in the instruction tuning process.\nSelf-Instruct Tuning We trained two models with supervised fine-tuning using the Llama-3-70B-Instruct and Llama-3-8B-Instruct checkpoints on a dataset of 2400 (question, answer, context, label) examples. Examples are formatted for instruction-tuning (Wei et al., 2021) in a chat-based format, where the evaluation task is provided in the instruction to the assistant, and the gold answer is the assistant response. The model is tasked to output JSON in the following format:\n{\n\"REASONING\": <reasoning provided as bullet points>,\n\"SCORE\": <final score of PASS or FAIL>\n}\nWe trained the model for 3 epochs with a learning rate of 5.0e-7 and batch size of 256. For supervised finetuning on 70B models, we trained on 32 Nvidia H100 GPUs. We used several performance optimizations including FSDP and flash attention. Our full training setup for both LYNX (8B) and LYNX (70B) is described in detail in Appendix B."}, {"title": "4 Results", "content": "4.1 Evaluation Results\nWe evaluated LYNX on HaluBench to assess its performance on hallucination detection in real world settings.\nTo put the results in context, we compared our proposed solution (shown as LYNX in Table 2) with several baseline methods. We prompted the model to assess whether the response was faithful to the context, and provided the question, answer and context. We instructed the model to produce a binary score, where \"FAIL\" indicates that it was hallucinated and \"PASS\" indicates that the response was faithful. We also instructed the model to produce reasoning for the score. We used the same zero-shot prompt for all models and tasks, to ensure a fair comparison and generalization of our approach to new domains. We additionally show results for RAGAS by setting a faithfulness threshold of 50%, where any score less than 50% is treated as a hallucination. Results are shown in Table 3.\nOut of all closed source and open-source models evaluated, LYNX (70B) reports the highest accuracy on all evaluation tasks. LYNX (70B) outperformed GPT-40 by almost one percent accuracy on average across all tasks. For domain specific tasks, this difference is even more pronounced; LYNX (70B) is 8.3% more accurate than GPT-40 at identifying inaccurate responses in medical answers in PubMedQA. LYNX (8B) and LYNX (70B) both show an increase in accuracy on all tasks compared to the baseline Llama 3 models, with the finetuned 70B model resulting in a 7.8% increase in average accuracy. When compared to closed source LLMs, LYNX outperforms GPT-3.5-Turbo by an even wider margin, with an average increase of 27.6% across all tasks. LYNX (70B) is the best performing model overall, with 87.4% accuracy on HaluBench. GPT-3.5-Turbo showed the lowest accuracy out of all models evaluated, with 58.7% accuracy averaged across all tasks."}, {"title": "5 Conclusion", "content": "As RAG systems continue to rise in popularity, automated reference-free evaluation of RAG systems is critical for the safe deployment of RAG systems at scale. We propose an evaluation model that assesses faithfulness of model responses in reference-free settings, which has important implications in business contexts ranging from detecting erroneous responses in financial Q&A to preventing misinformation in medical AI assistants. Our results show that LYNX outperforms industry and academic alternatives on HaluBench.\nWe have introduced HaluBench, a comprehensive hallucination evaluation benchmark that contains annotations of the faithfulness of textual responses across several real-world domains. HaluBench is unique for containing balanced distributions of positive and negative examples, and for a high percentage of examples grounded in real-world contexts. HaluBench consists of challenging hallucination detection examples, and shows high agreement with human annotations.\nLastly, we are open sourcing our evaluation datasets and model outputs, along with human annotations. The LYNX model is lightweight and easy to use, and can provide developers of RAG systems with useful insights, especially in the absence of ground truth annotations."}, {"title": "6 Limitations and Future Work", "content": "Failures outside of LLM Generation In real world deployments of RAG systems, there are often failures outside of RAG systems that can result in inaccuracies in LLM outputs. A common failure in RAG systems outside of LLM generation is in the retrieval component, where the retriever does not return relevant contexts to the query. This can result in downstream hallucinations, as the context provided to the generation model does not contain sufficient information to address the input.\nOther sources of failures in RAG systems unrelated to the LLM include pre-processing and post-processing steps, database queries and inconsistencies in data sources. In particular, source documents that contain conflicting information, or misinformation, present a challenge for failure detection due to its ambiguity. The resolution of conflicting information in fact checking tasks is a continued area of research. We leave an in depth exploration of improving retrieval modules to future work.\nMultilingual Coverage The bulk of datasets used in HaluBench is in English, which presents a limitation in real-world applications that include multilingual inputs and contexts. We hope to enhance coverage and diversity in HaluBench and training data by incorporating non-English and low resource languages in future extensions.\nSummarization and Other NLP Tasks HaluBench is focused on Question Answering tasks due to the prevalence of chat-based interfaces used by knowledge workers in industry RAG applications. An area for future work is extending LYNX to additional NLP domains, including abstractive summarization tasks where LLM produced summaries may contain inconsistent information with the source document.\nTruthfulness and World Knowledge LYNX focuses on the problem of hallucination detection. The assessment of truthfulness and factuality is also an important factor of consideration, and requires the incorporation of external knowledge sources as world knowledge.\nNatural Language Inference An interesting area for future work involves applying LYNX to NLI tasks. Since the problem of hallucination detection is closely related to NLI, a strong hallucination detection model is likely capable of performing reasoning in other NLP domains. We leave research on the relationship between evaluation tasks and other NLP tasks to future work."}, {"title": "7 Acknowledgements", "content": "We would like to thank Brandon Cui, Connor Jennings from Databricks Mosaic AI, Candace Ross from Meta AI and Andriy Mulyar from Nomic AI for their feedback and contributions to the paper. We additionally thank the Mosaic Databricks AI team for their support in the development of LYNX. We also thank Nino Scherrer for his contributions to the research process. Finally, we want to thank our partners, Nvidia and Nomic AI for their support of LYNX and HaluBench."}, {"title": "A Prompts", "content": "A.1 Data Generation\nFor generating the perturbed answers, we use the following prompt with GPT-40 with temperature=0.\nQUESTION:\n{question}\nGOLD_ANSWER:\n{gold_answer}\nEVIDENCE_TEXT:\n{evidence_text}\nHow can we change the GOLD_ANSWER subtly such that it would be wrong? The perturbed answer should still give the impression of a valid answer, but inspection of the EVIDENCE_TEXT would reveal that the perturbed answer is factually wrong. Output the new answer and change made in JSON format with the key 'new_answer' and 'change_made'.\nTo generate the reasoning chains, we use the following prompts with GPT-40 with temperature=0.\nI. For perturbed samples:\nBelow is the System Prompt:\nYou are given a QUESTION, CONTEXT, CHANGE_MADE, GOLD_ANSWER and ANSWER. Explain why the ANSWER is not faithful to the CONTEXT, given the QUESTION. CHANGE_MADE specifies the change made to the GOLD_ANSWER which made the ANSWER not faithful. Do not refer explicitly to the words 'CHANGE_MADE' or 'GOLD_ANSWER' in your reasoning. Generate your reasoning in JSON format:\n{\"REASONING\": \"<your reasoning steps as bullet points>\"}\nBelow is the User Prompt:\n<QUESTION>\n{question}\n</QUESTION>\n<CONTEXT>\n{context}\n</CONTEXT>\n<CHANGE_MADE>\n{change_made}\n</CHANGE_MADE>\n<GOLD_ANSWER>\n{answer}\n</GOLD_ANSWER>\n<ANSWER>\n{new_answer}\n</ANSWER>\nII. For original samples:\nSystem Prompt:\nYou are given a QUESTION, CONTEXT, ANSWER. Explain the similarities between the CONTEXT and the ANSWER. Reason about why the ANSWER is faithful to the CONTEXT given the QUESTION. Generate your reasoning in JSON format:\n{\"REASONING\": \"<your reasoning steps as bullet points>\"}\nUser Prompt:\n<QUESTION>\n{question}\n</QUESTION>\n<CONTEXT>\n{context}\n</CONTEXT>\n<ANSWER>\n{answer}\n</ANSWER>\nA.2 Evaluation\nWe use the following prompt for instruction fine-tuning as well as for evaluation of models:\nUser Prompt:\nGiven the following QUESTION, DOCUMENT and ANSWER you must analyze the provided answer and determine whether it is faithful to the contents of the DOCUMENT. The ANSWER must not offer new information beyond the context provided in the DOCUMENT. The ANSWER also must not contradict information provided in the DOCUMENT. Output your final verdict by strictly following this format: \"PASS\" if the answer is faithful to the DOCUMENT and \"FAIL\" if the answer is not faithful to the DOCUMENT. Show your reasoning.\nQUESTION (THIS DOES NOT COUNT AS BACKGROUND INFORMATION):\n{question}\nDOCUMENT:\n{context}\nANSWER:\n{answer}\nYour output should be in JSON FORMAT with the keys \"REASONING\" and \"SCORE\":\n{{\"REASONING\": <your reasoning as bullet points>, \"SCORE\": <your final score>}}"}, {"title": "B Training and Evaluation Details", "content": "B.1 Setup\nFor LYNX, we do mixed precision training with flash attention. We use a cosine scheduler with warmup. warmup steps is set to 100. We use lionw optimizer with \u03b2\u2081 = 0.9 and B2 = 0.95 and norm gradient clipping with threshold=1.0. FSDP is used with FULL_SHARD strategy and activation_checkpointing enabled.\nFor evaluating the 70B models, we use vLLM on 8 H100s with tensor_parallel = 8. For evaluating the 8B variants, we use model and data sharding with accelerate. We use HuggingFace pipelines for the generations, with greedy decoding and max_new_tokens = 600.\nB.2 Llama-2-13B-Chat Evaluation\nWe observe that the Llama-2-13B-Chat model does not output JSON data or adhere to the response structure requested in the prompt. However, after finetuning the model, we are able to parse responses to extract REASONING and SCORE. The results are present in Table 5.\nB.3 Training with extended datasets\nAs LYNX (70B) performed worse than Llama-3-Instruct-70B on the RAGTruth test split, we extended the training data to include 2k samples from RAGTruth. We finetuned Llama-3-Instruct-70B on this extended dataset. The results are reported in Table 4. While the performance increases on the RAGTruth split, we see a slight decrease in performance on the other splits. The overall performance gain with the extended RAGTruth dataset is ~0.4%."}]}