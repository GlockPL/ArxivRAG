{"title": "Lynx: An Open Source Hallucination Evaluation Model", "authors": ["Selvan Sunitha Ravi", "Bartosz Mielczarek", "Anand Kannappan", "Douwe Kiela", "Rebecca Qian"], "abstract": "Retrieval Augmented Generation (RAG) techniques aim to mitigate hallucinations in Large Language Models (LLMs). However, LLMS can still produce information that is unsupported or contradictory to the retrieved contexts. We introduce LYNX, a SOTA hallucination detection LLM that is capable of advanced reasoning on challenging real-world hallucination scenarios. To evaluate LYNX, we present HaluBench, a comprehensive hallucination evaluation benchmark, consisting of 15k samples sourced from various real-world domains. Our experiment results show that LYNX outperforms GPT-40, Claude-3-Sonnet and closed and open-source LLM-as-a-judge models on HaluBench. We release LYNX, HaluBench and our evaluation code for public access.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) learn in-depth knowledge from their pre-training data (Petroni et al., 2019) making them useful for knowledge-intensive downstream tasks such as Question Answering. However, their knowledge cannot be easily expanded and they often struggle with \u201challucinations\u201d (Roller et al., 2020; Dziri et al., 2022; Cao et al., 2022). This has led to the adaptation of Retrieval Augmented Generation (RAG) (Lewis et al., 2020) systems, which enables flexibility and extensibility of LLMs to internal data stores. However, these systems are still prone to generating text that is inconsistent with the provided knowledge source (Mallen et al., 2022).\nIn an ideal RAG system, LLMs exhibit \u201cfaithfulness\u201d by producing outputs that are grounded in the retrieved contexts. Detecting whether generated answers are faithful to the provided context is therefore critical to the success of RAG systems in production. RAGAS (Es et al., 2023) use LLMs to generate statements from a question-answer pair and compute a faithfulness score based on how many statements are supported by the given context. Other methods involve using LLM-as-a-Judge (Zheng et al., 2023) or fine-tuning lightweight LLM judges (Saad-Falcon et al., 2024) to evaluate hallucinations.\nWhile LLMs as judges have shown promise in automated evaluation on certain tasks (Zheng et al., 2024; Zhu et al., 2023; Kim et al., 2023), hallucination detection presents a complex challenge as it requires language models to have ability to perform nuanced reasoning and disambiguation. \nAdditionally, closed source LLMs as judges lack transparency and accessibility. Open-source LLMs still exhibit a significant gap in performance compared to closed source alternatives (Li et al., 2023). The gap in baseline performance between closed and open-source models increases when applied to specialized domains such as finance and medicine (Islam et al., 2023; Wu et al., 2023a).\nTo address these issues, we propose LYNX (70B) that outperforms GPT-40 and closed source LLMs in hallucination detection tasks, while being fully reproducible and open source. LYNX (8B) produces high quality evaluations at a fraction of the size and cost of closed source LLMs. LYNX is the first open source hallucination detection model that outperforms GPT-40 and closed source LLMs-as-Judge.\nTo train LYNX, we finetune Llama-3-70B-Instruct on data from multiple domains, focusing on hard-to-detect hallucinations. We source examples from existing Question Answer (QA) datasets such as CovidQA (M\u00f6ller et al., 2020), PubmedQA (Jin et al., 2019), DROP (Dua et al., 2019) and FinanceBench (Islam et al., 2023), introducing perturbations to generate hallucinated answers that appear plausible but are not faithful to the context.\nEvaluating the performance of different LLMs as judges in real-world hallucination detection tasks is difficult due to the lack of a comprehensive and diverse benchmark. Halueval (Li et al., 2023) and RAGTruth (Wu et al., 2023b) provide a large collection of generated and human-annotated hallucinated samples but cover limited domains. To evaluate hallucination detection systems, we construct HaluBench, a large scale hallucination evaluation benchmark that consists of 15k hallucinated as well as faithful responses to questions across multiple real-world domains.\nOur contributions are as follows:\n\u2022 We present HaluBench, a hallucination evaluation benchmark of 15k samples that consists of Context-Question-Answer triplets annotated for whether the examples contain hallucinations. Compared to prior datasets, HaluBench is the first open-source benchmark containing hallucination tasks sourced from real-world domains that include finance and medicine.\n\u2022 We train LYNX, the first open-source LLM capable of high quality, reference-free hallucination detection in RAG settings. We show that LYNX outperforms GPT-40, Claude-3-Sonnet and other closed and open-source models on HaluBench.\n\u2022 We propose a novel method to generate hard-to-detect hallucination examples from Question Answering tasks by applying semantic perturbations to LLM responses. We find that our perturbed examples are challenging for LLM judges, as nuanced differences in semantic meaning can lead to different reasoning outcomes.\n\u2022 We conduct experiments to benchmark LYNX against closed and open-source LLMs and RAG evaluation metrics. We release all models, datasets and experiment results for public access."}, {"title": "2 Related Work", "content": "While LLMs have shown remarkable performance in knowledge-intensive tasks such as Question Answering, one of the major drawbacks is generation of inaccurate or false information (Azaria and Mitchell, 2023; Ji et al., 2023). Several techniques have been proposed to evaluate hallucinations in LLMs (Guerreiro et al., 2022; Lin and Chen, 2023; Manakul et al., 2023) including Retrieval-Augmented Generation (RAG) (Lewis et al., 2020; Shuster et al., 2021a; Yu, 2022; Biswas et al., 2022). By leveraging retrieval, RAG helps LLMs acquire domain-specific knowledge and ground their outputs in factual information (Shuster et al., 2021b). However, RAG systems can still suffer from hallucinations (Wu et al., 2023b; Li et al., 2023; Es et al., 2023; Saad-Falcon et al., 2024). Automatic evaluation is crucial for quickly deploying these systems in new environments where creating a traditional benchmark dataset from the ground up is challenging.\nTo evaluate RAG systems, LLMs have been utilized to compute metrics such as answer correctness, groundedness and the relevance of retrieved contexts (Zhao et al., 2019; Yuan et al., 2021; Liu et al., 2023; Es et al., 2023; Saad-Falcon et al., 2024). RAGAS (Es et al., 2023) relies on a set of heuristic, hand-written prompts, while using LLMs and embedding-based similarity scores. Similarly, the EXAM (Sander and Dietz, 2021) metric evaluates retrieval-augmented generation (RAG) systems by estimating how many exam questions a simulated QA system can correctly answer based on the generated responses. ARES constructs LLM judges using few-shot demonstrations and a bootstrapped training dataset (Saad-Falcon et al., 2024). We find that heuristics-based metrics such as RAGAS perform poorly on hallucination tasks compared to LLM-as-judge evaluators (Table 3). While ARES offers more flexibility than prior metrics, the construction of training datasets on the fly introduces significant overhead, making the approach less suitable for production settings.\nA related area of research is Natural Language"}, {"title": "3 Methodology", "content": "In a RAG pipeline, we first (1) Retrieve the relevant context(s) given a query, then (2) Generate an answer to the query given the retrieved context(s) with an LLM. \u201cHallucinations\u201d (Jian et al., 2022; Ji et al., 2023) occur when the generated answers are not faithful to the context (intrinsic hallucinations) or don't align with factual reality (extrinsic hallucinations). In this paper, we focus solely on intrinsic hallucination evaluation since in real-world settings, user-provided documents may contain information that conflicts with external knowledge sources. The purpose of LYNX is to provide a reference-free metric for automated RAG evaluation, thus we consider factuality assessments out of scope for this work.\nIn the following sections, we describe the process for training LYNX, a SOTA hallucination detection LLM. We begin with the definition of hallucination (Section 3.1), followed by the construction process of our training and evaluation data (Section 3.2). Finally, we present experimental results on hallucination tasks sourced from real-world domains."}, {"title": "3.1 Hallucination Evaluation", "content": "For a given question x, we say that the LLM is hallucinating if the answer P(x) is not supported by the context C(x) when contextualized by the question. In practice, LLM generated answers are often inconsistent with the retrieved context (Li et al., 2023). In our definition of hallucination, we do not assess the relevance of the retrieved context C(x) to the query x. If the answer, P(x) is consistent with the irrelevant context, C(x) we will consider the answer to be faithful to the context. Similarly, if the answer, P(x) to a question, x is incorrect but it states information consistent with the context, C(x) it will be evaluated as faithful."}, {"title": "3.2 HaluBenchConstruction", "content": "We sourced examples from several existing QA datasets to build the hallucination evaluation benchmark. We constructed tuples of (question, context, answer, label), where label is a binary score that denotes whether the answer contains a hallucination. HaluBench consists of the following tasks:\n\u2022 FinanceBench (Islam et al., 2023): FinanceBench consists of 10k questions, contexts and answers over financial documents, containing both tables and bullet point lists. FinanceBench was designed to be similar to real-world financial question and answering from financial analysts. We randomly sampled 1k samples, of which 500 contain hallucinations.\n\u2022 DROP (Dua et al., 2019): DROP is an English reading comprehension benchmark that assesses reasoning ability over the content of paragraphs. The dataset was crowdsourced and adversarially created. We randomly sampled 1k samples, of which 500 contain hallucinations.\n\u2022 COVID-QA (M\u00f6ller et al., 2020): COVID-QA consists of 2k question-answer pairs annotated by volunteer biomedical experts on scientific articles related to COVID-19. We randomly sampled 1k samples, of which 500 contain hallucinations.\n\u2022 PubMedQA (Jin et al., 2019): PubMedQA is a biomedical question answering (QA) dataset collected from PubMed abstracts. The task consists of answering research questions with yes/no/maybe responses. It also contains a long answer that provides evidence from the context for the response.\n\u2022 HaluEval (Li et al., 2023): HaluEval is a hallucination evaluation dataset consisting of general user queries with ChatGPT responses and task-specific examples from three tasks, i.e., question answering, knowledge-grounded dialogue, and text summarization. We used the qa_samples subset, which contains 10k questions with knowledge from Wikipedia and question text and ground-truth answers collected from HotpotQA.\n\u2022 RAGTruth (Wu et al., 2023b): RAGTruth is a corpus containing word-level hallucination annotations on LLM generated text. We used the test split that comprised of 900 samples, of which 160 examples contain hallucinations."}, {"title": "Construction of Hallucination Examples", "content": "Four of the QA datasets we sourced from (DROP, FinanceBench, COVID-QA, PubMedQA) do not contain answers that are not faithful to the context. To construct unfaithful answers, we used the Context-Question-Answer to generate semantically perturbed versions of gold answers to questions. We define a semantic answer perturbation as a response that is minimally different to the gold answer, but contains an inconsistency with the context that results in a hallucination. We use GPT-40 to construct these perturbations. The prompt and generation settings are in Appendix A.\nLet {q, c, x, y} denote the question, context, answer and label of a given example in dataset D, where y \u2208 {0,1}. For our perturbation generator fp, let x ~ fp(q, c, x) be the semantically altered perturbation output. Our perturbed dataset is thus\n$$D' = \\{(q, c, x,1 \u2212 y)|(q, c, x, y) \u2208 D\\} \\qquad (1)$$"}, {"title": "3.3 Model Training", "content": "Training Dataset Construction The training dataset for LYNX consists of 2400 samples, along with 800 samples for validation. The dataset consists of demonstrations sourced from RAGTruth, DROP, CovidQA and PubMedQA. For each sub-task, we sampled 600 examples from the train split of the source dataset, of which 300 were perturbed to construct hallucinated answers.\nChain of Thought (CoT) has been shown to improve zero-shot performance of LLMs (Wei et al., 2022). To distill the evaluation reasoning capabilities of GPT-40 to our finetuned open-source model, we used GPT-40 to generate reasoning for the label of each example in our training set. We provided this as as part of the assistant response, along with the label in the instruction tuning process. The prompt to generate reasoning traces is present in Appendix A.\nSelf-Instruct Tuning We trained two models with supervised fine-tuning using the Llama-3-70B-Instruct and Llama-3-8B-Instruct checkpoints on a dataset of 2400 (question, answer, context, label) examples. Examples are formatted for instruction-tuning (Wei et al., 2021) in a chat-based format, where the evaluation task is provided in the instruction to the assistant, and the gold answer is the assistant response. The model is tasked to output JSON in the following format:\n{\n\"REASONING\": <reasoning provided as\nbullet points>,\n\"SCORE\": <final score of PASS or FAIL>\n}\nWe trained the model for 3 epochs with a learning rate of 5.0e-7 and batch size of 256. For supervised finetuning on 70B models, we trained on 32 Nvidia H100 GPUs. We used several performance optimizations including FSDP and flash attention. Our full training setup for both LYNX (8B) and LYNX (70B) is described in detail in Appendix B."}, {"title": "4 Results", "content": "We evaluated LYNX on HaluBench to assess its performance on hallucination detection in real world settings.\nTo put the results in context, we compared our proposed solution (shown as LYNX in Table 2) with several baseline methods. We prompted the model to assess whether the response was faithful to the context, and provided the question, answer and context. We instructed the model to produce a binary score, where \"FAIL\" indicates that it was hallucinated and \"PASS\" indicates that the response was faithful. We also instructed the model to produce reasoning for the score. We used the same zero-shot prompt for all models and tasks, to ensure a fair comparison and generalization of our approach to new domains. We additionally show results for RAGAS by setting a faithfulness threshold of 50%, where any score less than 50% is treated as a hallucination.\nOut of all closed source and open-source models evaluated, LYNX (70B) reports the highest accuracy on all evaluation tasks. LYNX (70B) outperformed GPT-40 by almost one percent accuracy on average across all tasks. For domain specific tasks, this difference is even more pronounced; LYNX (70B) is 8.3% more accurate than GPT-40 at identifying inaccurate responses in medical answers in PubMedQA. LYNX (8B) and LYNX (70B) both show an increase in accuracy on all tasks compared to the baseline Llama 3 models, with the finetuned 70B model resulting in a 7.8% increase in average accuracy. When compared to closed source LLMS, LYNX outperforms GPT-3.5-Turbo by an even wider margin, with an average increase of 27.6% across all tasks. LYNX (70B) is the best performing model overall, with 87.4% accuracy on HaluBench. GPT-3.5-Turbo showed the lowest accuracy out of all models evaluated, with 58.7% accuracy averaged across all tasks."}, {"title": "5 Conclusion", "content": "As RAG systems continue to rise in popularity, automated reference-free evaluation of RAG systems is critical for the safe deployment of RAG systems at scale. We propose an evaluation model that assesses faithfulness of model responses in reference-free settings, which has important implications in business contexts ranging from detecting erroneous responses in financial Q&A to preventing misinformation in medical AI assistants. Our results show that LYNX outperforms industry and academic alternatives on HaluBench.\nWe have introduced HaluBench, a comprehensive hallucination evaluation benchmark that contains annotations of the faithfulness of textual responses across several real-world domains."}, {"title": "6 Limitations and Future Work", "content": "Failures outside of LLM Generation In real world deployments of RAG systems, there are often failures outside of RAG systems that can result in inaccuracies in LLM outputs. A common failure in RAG systems outside of LLM generation is in the retrieval component, where the retriever does not return relevant contexts to the query. This can result in downstream hallucinations, as the context provided to the generation model does not contain sufficient information to address the input.\nOther sources of failures in RAG systems unrelated to the LLM include pre-processing and post-processing steps, database queries and inconsistencies in data sources. In particular, source documents that contain conflicting information, or misinformation, present a challenge for failure detection due to its ambiguity. The resolution of conflicting information in fact checking tasks is a continued area of research. We leave an in depth exploration of improving retrieval modules to future work.\nMultilingual Coverage The bulk of datasets used in HaluBench is in English, which presents a limitation in real-world applications that include multilingual inputs and contexts. We hope to enhance coverage and diversity in HaluBench and training data by incorporating non-English and low resource languages in future extensions.\nSummarization and Other NLP Tasks HaluBench is focused on Question Answering tasks due to the prevalence of chat-based interfaces used by knowledge workers in industry RAG applications. An area for future work is extending LYNX to additional NLP domains, including abstractive summarization tasks where LLM produced summaries may contain inconsistent information with the source document.\nTruthfulness and World Knowledge LYNX focuses on the problem of hallucination detection. The assessment of truthfulness and factuality is also an important factor of consideration, and requires the incorporation of external knowledge sources as world knowledge.\nNatural Language Inference An interesting area for future work involves applying LYNX to NLI tasks. Since the problem of hallucination detection is closely related to NLI, a strong hallucination detection model is likely capable of performing reasoning in other NLP domains. We leave research on the relationship between evaluation tasks and other NLP tasks to future work."}, {"title": "7 Acknowledgements", "content": "We would like to thank Brandon Cui, Connor Jennings from Databricks Mosaic AI, Candace Ross from Meta AI and Andriy Mulyar from Nomic AI for their feedback and contributions to the paper. We additionally thank the Mosaic Databricks AI team for their support in the development of LYNX. We also thank Nino Scherrer for his contributions to the research process. Finally, we want to thank"}, {"title": "A Prompts", "content": "For generating the perturbed answers, we use the following prompt with GPT-40 with temperature=0.\nHow can we change the GOLD_ANSWER subtly\nsuch that it would be wrong? The perturbed\nanswer should still give the impression\nof a valid answer, but inspection of\nthe EVIDENCE_TEXT would reveal that the\nperturbed answer is factually wrong.\nOutput the new answer and change made\nin JSON format with the key 'new_answer'\nand 'change_made'."}, {"title": "A.2 Evaluation", "content": "We use the following prompt for instruction fine-tuning as well as for evaluation of models:"}, {"title": "B Training and Evaluation Details", "content": "For LYNX, we do mixed precision training with flash attention. We use a cosine scheduler with warmup. warmup steps is set to 100. We use lionw optimizer with \u03b2\u2081 = 0.9 and B2 = 0.95 and norm gradient clipping with threshold=1.0. FSDP is used with FULL_SHARD strategy and activation_checkpointing enabled.\nFor evaluating the 70B models, we use vLLM on 8 H100s with tensor_parallel = 8. For evaluating the 8B variants, we use model and data sharding with accelerate. We use HuggingFace pipelines for the generations, with greedy decoding and max_new_tokens = 600."}]}