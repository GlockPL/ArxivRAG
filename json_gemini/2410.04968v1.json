{"title": "Collaboration! Towards Robust Neural Methods for Routing Problems", "authors": ["Jianan Zhou", "Yaoxin Wu", "Zhiguang Cao", "Wen Song", "Jie Zhang", "Zhiqi Shen"], "abstract": "Despite enjoying desirable efficiency and reduced reliance on domain expertise, existing neural methods for vehicle routing problems (VRPs) suffer from severe robustness issues \u2013 their performance significantly deteriorates on clean instances with crafted perturbations. To enhance robustness, we propose an ensemble-based Collaborative Neural Framework (CNF) w.r.t. the defense of neural VRP methods, which is crucial yet underexplored in the literature. Given a neural VRP method, we adversarially train multiple models in a collaborative manner to synergistically promote robustness against attacks, while boosting standard generalization on clean instances. A neural router is designed to adeptly distribute training instances among models, enhancing overall load balancing and collaborative efficacy. Extensive experiments verify the effectiveness and versatility of CNF in defending against various attacks across different neural VRP methods. Notably, our approach also achieves impressive out-of-distribution generalization on benchmark instances.", "sections": [{"title": "1 Introduction", "content": "Combinatorial optimization problems (COPs) are crucial yet challenging to solve due to the NP- hardness. Neural combinatorial optimization (NCO) aims to leverage machine learning (ML) to automatically learn powerful heuristics for solving COPs, and has attracted considerable attention recently [2]. Among them, a large number of NCO works develop neural methods for vehicle routing problems (VRPs) one of the most classic COPs with broad applications in transportation [54], logistics [35], planning and scheduling [52], etc. With various training paradigms (e.g., reinforcement learning (RL)), the neural methods learn construction or improvement heuristics, which achieve com- petitive or even superior performance to the conventional algorithms. However, recent studies show that these neural methods are plagued by severe robustness issues [20], where their performance drops devastatingly on clean instances (sampled from the training distribution) with crafted perturbations.\nAlthough the robustness issue has been investigated in a couple of recent works [87, 20, 42], the defensive methods on how to help forge sufficiently robust neural VRP methods are still underexplored."}, {"title": "2 Related Work", "content": "Neural VRP Methods. Most neural VRP methods learn construction heuristics, which are mainly divided into two categories, i.e., autoregressive and non-autoregressive ones. Autoregressive methods sequentially construct the solution by adding one feasible node at each step. [71] proposes the Pointer Network (Ptr-Net) to solve TSP with supervised learning. Subsequent works train Ptr-Net with RL to solve TSP [1] and CVRP [51]. [37] introduces the attention model (AM) based on the Transformer architecture [69] to solve a wide range of COPs including TSP and CVRP. [38] further proposes the policy optimization with multiple optima (POMO), which improves upon AM by exploiting solution symmetries. Further advancements [39, 34, 3, 24, 13, 44, 27, 45, 21, 12] are often developed on top of AM and POMO. Regarding non-autoregressive methods, the solution is typically constructed in a one-shot manner without iterative forward passing through the model. [31] leverages the graph convolutional network to predict the probability of each edge appearing on the optimal tour (i.e., heat-map) using supervised learning. Recent works [17, 36, 56, 64, 49, 82, 33, 77] further improve its performance and scalability by using advanced models, training paradigms, and search strategies. We refer to [40, 28, 83, 88] for scalability studies, to [30, 5, 90, 18, 41, 16, 89, 4] for generalization studies, and to [11, 59, 84, 61] for other COP studies. On the other hand, some neural methods learn improvement heuristics to refine an initial feasible solution iteratively, until a termination condition is satisfied. In this line of research, the classic local search methods and specialized heuristic solvers for VRPs are usually exploited [8, 43, 10, 76, 47, 78, 46]. In general, the improvement heuristics can achieve better performance than the construction ones, but at the expense of much longer inference time. In this paper, we focused on autoregressive construction methods.\nRobustness of Neural VRP Methods. There is a recent research trend on the robustness of neural methods for COPs [68, 20, 42], with only a few works on VRPs [87, 20, 42]. In general, they primarily focus on attacking neural construction heuristics by introducing effective perturbation models to generate adversarial instances that are underperformed by the current model. Following the AT paradigm, [87] perturbs node coordinates of TSP instances by solving an inner maximization problem (similar to the fast gradient sign method [22]), and trains the model with a hardness-aware instance-reweighted loss function. [20] proposes an efficient and sound perturbation model, which ensures the optimal solution to the perturbed TSP instance can be directly derived. It adversarially inserts several nodes into the clean instance by maximizing the cross-entropy over the edges, so that the predicted route is maximally different from the derived optimal one. [42] leverages a no- worse optimal cost guarantee (i.e., by lowering the cost of a partial problem) to generate adversarial instances for asymmetric TSP. However, existing methods mainly follow vanilla AT [48] to deploy the defense, leaving a considerable gap to further consolidate robustness.\nRobustness in Other Domains. Deep neural networks are vulnerable to adversarial examples [22], spurring the development of numerous attack and defensive methods to mitigate the arisen security issue across various domains. 1) Vision: Early research on adversarial robustness mainly focus on the continuous image domain (e.g., at the granularity of pixels). The vanilla AT, as formulated by [48] through min-max optimization, has inspired significant advancements in the field [65, 60, 85, 7, 86]. 2) Language: This domain investigates how malicious inputs (e.g., characters and words) can deceive (large) language models into making incorrect decisions or producing unintended outcomes [14, 72, 50, 91, 74]. Challenges include the discrete nature of natural languages and the complexity of linguistic structures, necessitating sophisticated techniques for generating and defending against adversarial attacks. 3) Graph: Graph neural networks are also susceptible to adversarial perturbations in the underlying graph structures [92], prompting research to enhance their robustness [29, 15, 19, 23, 63]. Similar to the language domain, challenges stem from the discrete nature of graphs and the interconnected nature of graph data. Although various defensive methods have been proposed for these specific domains, most are not adaptable to the VRP (or COP) domain due to their needs for ground-truth labels, reliance on the imperceptible perturbation model, and unique challenges inherent in combinatorial optimization."}, {"title": "3 Preliminaries", "content": "3.1 Neural VRP Methods\nProblem Definition. Without loss of generality, we define a VRP instance x over a graph $G = {V, \\mathcal{E}}$, where $V = {v_i}_{i=1}^n$ represents the node set, and $(v_i, v_j) \\in \\mathcal{E}$ represents the edge set with $v_i \\neq v_j$."}, {"title": "3.2 Adversarial Training", "content": "Adversarial training is one of the most effective and practical techniques to equip deep learning models with adversarial robustness against crafted perturbations on the clean instance. In the supervised fashion, where the clean instance x and ground truth (GT) label y are given, AT is commonly formulated as a min-max optimization problem:\n$\\min_{\\theta} \\mathbb{E}_{(x,y)\\sim D}[l(y, f_{\\theta}(x))], \\text{ with } x^{\\prime} = \\arg \\max_{x^{\\prime} \\in \\mathcal{N}_\\epsilon [x]} [l(y, f_{\\theta}(x^{\\prime}))],$ (2)\nwhere D is the data distribution; l is the loss function; $f_{\\theta}(\\cdot)$ is the model prediction with parameters $\\theta$; $\\mathcal{N}_\\epsilon [x]$ is the neighborhood around x, with its size constrained by the attack budget $\\epsilon$. The solution to the inner maximization is typically approximated by projected gradient descent:\n$x^{(t+1)} = \\Pi_{\\mathcal{N}_\\epsilon [x]} [x^{(t)} + \\alpha \\cdot \\text{sign}(\\nabla_{x^{(t)}}l(y, f_{\\theta}(x^{(t)})))],$ (3)\nwhere $\\alpha$ is the step size; $\\Pi$ is the projection operator that projects the adversarial instance back to the neighborhood $\\mathcal{N}_\\epsilon [x]$; $x^{(t)}$ is the adversarial instance found at step t; and the sign operator is used to take the gradient direction and carefully control the attack budget. Typically, $x^{(0)}$ is initialized by the clean instance or randomly perturbed instance with small Gaussian or Uniform noises. The adversarial instance is updated iteratively towards loss maximization until a stop criterion is satisfied.\nAT for VRPs. Most ML research on adversarial robustness focuses on the continuous image domain [22, 48]. We would like to highlight two main differences in the context of discrete VRPs (or COPs). 1) Imperceptible perturbation: The adversarial instance $\\tilde{x}$ is typically generated within a small neighborhood of the clean instance x, so that the adversarial perturbation is imperceptible to human eyes. For example, the adversarial instance in image related tasks is typically bounded by $\\mathcal{N}_\\epsilon[x]: ||x - \\tilde{x}||_p \\leq \\epsilon$ under the $l_p$ norm threat model. When the attack budget $\\epsilon$ is small enough, $\\tilde{x}$ retains the GT label of x. However, it is not the case for VRPs due to the nature of discreteness. The optimal solution can be significantly changed even if only a small part of the instance is modified. Therefore, the subjective imperceptible perturbation is not a realistic goal in VRPs, and we do not exert such an explicit imperceptible constraint on the perturbation model (see Appendix A for further discussions). In this paper, we set the attack budget within a reasonable range based on the attack methods. 2) Accuracy-robustness trade-off: The standard generalization and adversarial robustness seem to be conflicting goals in image related tasks. With increasing adversarial robustness the standard generalization tends to decrease, and a number of works intend to mitigate such a trade-off in the image domain [66, 85, 73, 57, 80]. By contrast, a recent work [20] claims the existence of"}, {"title": "4 Collaborative Neural Framework", "content": "In this section, we first present the motivation and overview of the proposed framework, and then introduce the technical details. Overall, we propose a collaborative neural framework to synergistically promote adversarial robustness among multiple models, while boosting standard generalization. Since conducting AT for deep learning models from scratch is computationally expensive due to the extra inner maximization steps, we use the model pretrained on clean instances as a warm-start for subsequent AT steps. An overview of the proposed method is illustrated in Fig. 2.\nMotivation. Motivated by the empirical observations that 1) existing neural VRP methods suffer from severe adversarial robustness issue; 2) undesirable trade-off between adversarial robustness and standard generalization may exist when following the vanilla AT, we propose to adversarially train multiple models in a collaborative manner to mitigate the above-mentioned issues within a reasonable computational budget. It then raises the research question on how to effectively and efficiently train multiple models under the AT framework, involving a pair of inner maximization and outer minimization, which will be detailed in the following parts. Note that despite the accuracy-robustness trade-off being a well-known research problem in the literature of adversarial ML, most works focus on the image domain. Due to the needs for GT labels or the dependence on the imperceptible perturbation model, their methods (e.g., TRADES [85], Robust Self-Training with AT [57]) cannot be directly leveraged to solve this trade-off in VRPs. We refer to Appendix A for further discussions.\nOverview. Given a pretrained model $\\theta_p$, CNF deploys the AT on its M copies (i.e., $\\Theta^{(0)} = {\\theta_{p}}_{j=1}^M$) in a collaborative manner. Concretely, at each training step, it first solves the inner maximization optimization to synergistically generate the local ($x_i$) and global ($x_i^{\\prime}$) adversarial instances, on which the current models underperform. Then, in the outer minimization optimization, we jointly train an attention-based neural router $\\theta_r$ with all models $\\Theta$ by RL in an end-to-end manner. By adaptively distributing instances to different models for training, the neural router can reasonably exploit the overall capacity of models and thus achieve satisfactory load balancing and collaborative efficacy. During inference, we discard the neural router $\\theta_r$ and use the trained models $\\Theta$ to solve each"}, {"title": "4.1 Inner Maximization", "content": "The inner maximization aims to generate adversarial instances for the training in the outer mini- mization, which should be 1) effective in attacking the framework; 2) diverse to benefit the policy exploration for VRPs. Typically, an iterative attack method generates local adversarial instances for each model only based on its own parameter (e.g., the same $\\theta$ in Eq. (3) is repetitively used throughout the generation). Such local attack (line 6) only focuses on degrading each individual model, failing to consider the ensemble effect of multiple models. Due to the existence of multiple models in CNF, we are motivated to further develop a general form of local attack - global attack (line 7-8), where each adversarial instance can be generated using different model parameters within T generation steps. Concretely, given each clean instance x, we generate the global adversarial instance by attacking the corresponding best-performing model in each iteration of the inner maximization. In doing so, compared with the sole local attack, the generated adversarial instances are more diverse to successfully attack the models $\\Theta$ (see Appendix A for further discussions). Without loss of generality, we take the attacker from [87] as an example, which directly maximizes the variant of the reinforcement loss as follows:\n$l(x; \\theta) = \\frac{C(\\tau)}{b(x)} - \\log p_{\\theta}(\\tau|x),$ (4)\nwhere $b(\\cdot)$ is the baseline function (as shown in Eq. (1)). On top of it, we generate the global adversarial instance such that:\n$x^{(t+1)} = \\Pi [x^{(t)} + \\alpha \\cdot \\nabla_{\\tau^{(t)}}l(x^{(t)}; \\theta_{\\mathcal{O}^{(t)}})], \\theta_{\\mathcal{O}^{(t)}} = \\arg \\min_{\\theta \\in \\Theta} C(\\tau|x^{(t)}; \\theta),$ (5)\nwhere $x^{(t)}$ is the global adversarial instance and $\\theta_{\\mathcal{O}^{(t)}}$ is the best-performing model (w.r.t. $x^{(t)}$) at step t. If $x^{(t)}$ is out of the range, it would be projected back to the valid domain $\\mathcal{N}$ by $\\Pi$, such as the min-max normalization for continuous variables (e.g., node coordinates) or the rounding operation for discrete variables (e.g., node demands). We discard the sign operator in Eq. (3) to relax the imperceptible constraint. More details are presented in Appendix B.1.\nIn summary, the local attack is a special case of the global attack, where the same model is chosen as $\\theta$ in each iteration. While the local attack aims to degrade a single model $\\theta$, the global attack can be viewed as explicitly attacking the collaborative performance of all models, which takes into consideration the ensemble effect by attacking $\\theta_{\\mathcal{O}}$. In CNF, we involve adversarial instances that"}, {"title": "4.2 Outer Minimization", "content": "After the adversarial instances are generated by the inner maximization, we collect a set of instances $\\mathcal{X}$ with $|\\mathcal{X}| = N$, which includes clean instances x, local adversarial instances $\\hat{x}$ and global adversarial instances $\\tilde{x}$, to train M models. Here a key problem is that how are the instances distributed to models for their training, so as to achieve satisfactory load balancing (training efficiency) and collaborative performance (effectiveness)? To solve this, we design an attention-based neural router, and jointly train it with all models $\\Theta$ to maximize the improvement of collaborative performance.\nConcretely, we first evaluate each model on $\\mathcal{X}$ to obtain a cost matrix $\\mathcal{R} \\in \\mathbb{R}^{N \\times M}$. The attention- based neural router $\\theta_r$ takes as inputs the instances $\\mathcal{X}$ and $\\mathcal{R}$, and outputs a logit matrix $f_{\\theta_r}(\\mathcal{X}, \\mathcal{R}) = P \\in \\mathbb{R}^{N \\times M}$, where $f$ is the decision function. Then, we apply Softmax function along the first dimension of P to obtain the probability matrix $\\mathbb{P}$, where the entity $\\mathbb{P}_{ij}$ represents the probability of the $i$th instance being selected for the outer minimization of the $j$th model. For each model, the neural router distributes the instances with TopK-largest predicted probabilities as a batch for its training (line 10-13). In doing so, all models have the same amount (K) of training instances, which explicitly ensures the load balancing (see Appendix A). We also discuss other strategies of instance distributing, such as sampling, instance-based choice, etc. More details can be found in Section 5.2.\nAfter all models $\\Theta$ are trained with the distributed instances, we further evaluate each model on $\\mathcal{X}$, obtaining a new cost matrix $\\mathcal{R}^{\\prime} \\in \\mathbb{R}^{N \\times M}$. To pursue desirable collaborative performance, it is expected that the neural router $\\theta_r$ can reasonably exploit the overall capacity of models. Since the action space of $\\Theta_r$ is huge and the models are changing throughout the training, we resort to the reinforcement learning (based on trial-and-error) to optimize parameters of the neural router $\\theta$ (line 14-15). Specifically, we set (min $\\mathcal{R}$ \u2013 min $\\mathcal{R}^{\\prime}$) as the reward signal, and update $\\theta_r$ by gradient ascent to maximize the expected return with the following approximation:\n$\\nabla_{\\theta_r} \\mathcal{L}(\\theta_r|\\mathcal{X}) = \\mathbb{E}_{j \\in [1, M], i \\in \\text{TopK}(\\mathbb{P}_{\\cdot j}), \\mathbb{P}}[(\\min \\mathcal{R} \u2013 \\min \\mathcal{R}^{\\prime})_{ij} \\nabla_{\\theta_r} \\log \\mathbb{P}_{ij}],$ (6)\nwhere the min operator is applied along the last dimension of $\\mathcal{R}$ and $\\mathcal{R}^{\\prime}$, since we would like to maximize the improvement of collaborative performance after training with the selected instances. Intuitively, if an entity in (min $\\mathcal{R}$ \u2013 min $\\mathcal{R}^{\\prime}$) is positive, it means that, after training with the selected instances, the collaborative performance of all models on the corresponding instance is increased. Thus, the corresponding action taken by the neural router should be reinforced, and vice versa. For example, in Fig. 2, if the reward entity for the first instance $x_1$ is positive, the probability of this action (i.e., the red one in the first row of $\\mathbb{P}$) will be reinforced after optimization. Note that the unselected ones (e.g., black ones) will be masked out in Eq. (6). In doing so, the neural router learns to effectively distribute instances and reasonably exploit the overall model capacity, such that the collaborative performance of all models can be enhanced after optimization. Further details on the neural router structure and the in-depth analysis of the learned routing policy are presented in Appendix C."}, {"title": "5 Experiments", "content": "In this section, we empirically verify the effectiveness and versatility of CNF against attacks spe- cialized for VRPs, and conduct further analyses to provide the underlying insights. Specifically, our experiments focus on two attack methods [87, 42], since the accuracy-robustness trade-off exists when conducting vanilla AT to defend against them. We conduct the main experiments on POMO [38] with the attacker in [87], and further demonstrate the versatility of the proposed framework on MatNet [39] with the attacker in [42]. More details on the experimental setups, data generation and additional empirical results (e.g., evaluation on large-scale instances) are presented in Appendix D. All experi- ments are conducted on a machine with NVIDIA V100S-PCIE cards and Intel Xeon Gold 6226 CPU at 2.70GHz. The source code is available at https://github.com/RoyalSkye/Routing-CNF.\nBaselines. 1) Traditional methods: we solve TSP instances by Concorde and LKH3 [25], and CVRP instances by hybrid genetic search (HGS) [70] and LKH3. 2) Neural methods: we compare our method with the pretrained base model POMO (~1M parameters), and its variants training with various defensive methods, such as the vanilla adversarial training (POMO_AT), the defensive method"}, {"title": "5.1 Performance Evaluation", "content": "The results are shown in Table 1. We have conducted t-test with the threshold of 5% to verify the statistical significance. For all neural methods, we report the inference time on a single GPU. More-"}, {"title": "5.2 Ablation Study", "content": "We conduct extensive ablation studies on TSP100 to demonstrate the effectiveness and sensitivity of our method. Note that the setups are slightly different from the training ones (e.g., half training instances). The detailed results and setups are presented in Fig. 3 and Appendix D.1, respectively.\nAblation on Components. We investigate the role of each component in CNF by removing them separately. As demonstrated in Fig. 3(a), despite both components contribute to the collaborative performance, the neural router exhibits a bigger effect due to its favorable potentials to elegantly exploit training instances and model capacity, especially in the presence of multiple models.\nAblation on Hyperparameters. We investigate the effect of the number of trained models, which is a key hyperparameter of our method, on the collaborative performance. The results are shown in Fig. 3(b), where we observe that increasing the number of models can further improve the collaborative performance. However, we use M = 3 in the main experiments due to the trade-off between empirical performance and computational complexity. We refer to Appendix D.4 for more results.\nAblation on Routing Strategies. We further discuss different routing strategies, including neural and heuristic ones. Specifically, given the logit matrix P predicted by the neural router, there are various ways to distribute instances: 1) Model choice with TopK (M-TopK): each model chooses potential instances with TopK-largest logits, which is the default strategy (K = B) in CNF; 2) Model choice with sampling (M-Sample): each model chooses potential instances by sampling from the probability distribution (i.e., scaled logits); 3-4) Instance choice with TopK/sampling (I-TopK/I-Sample): in contrast to the model choice, each instance chooses potential model(s) either by TopK or sampling. The probability matrix P is obtained by taking Softmax along the first and last dimension of P for model choice and instance choice, respectively. Unlike the model choice, instance choice cannot guarantee load balancing. For example, the majority of instances may choose a dominant model (if exists), leaving the remaining models underfitting and therefore weakening the ensemble effect and collaborative performance; 5) Random: instances are randomly distributed to each model; 6) Self-training: each model is trained on adversarial instances generated by itself without instance distributing. The results in Fig. 3(c) show that M-TopK performs the best."}, {"title": "5.3 Out-of-Distribution Generalization", "content": "In contrast to other domains (e.g., vision), the set of valid problems is not just a low-dimensional manifold in a high-dimensional space, and hence the manifold hypothesis [62] does not apply to VRPs (or COPs). Therefore, it is critical for neural methods to perform well on adversarial instances when striving for a broader out-of-distriburion (OOD) generalization in VRPs. In this section, we"}, {"title": "5.4 Versatility Study", "content": "To demonstrate the versatility of CNF, we extend its application to MatNet [39] to defend against another attacker in [42]. Specifically, it constructs the adversarial instance by lowering the cost of a partial clean asymmetric TSP (ATSP) instance. When adhering to the vanilla AT, the undesirable trade-off is also observed in the empirical results of [42]. In contrast, our method enables models to achieve both high standard generalization and adversarial robustness. The detailed attack method, training setups, and empirical results are presented in Appendix B.3, D.1 and D.3, respectively."}, {"title": "6 Conclusion", "content": "This paper studies the crucial yet underexplored adversarial defense of neural VRP methods, filling the gap in the current literature on this topic. We propose an ensemble-based collaborative neural framework to concurrently enhance performance on clean and adversarial instances. Extensive experiments demonstrate the effectiveness and versatility of our method, highlighting its potential to defend against attacks while promoting various forms of generalization of neural VRP methods. Additionally, our work can be viewed as advancing the generalization of neural methods through the lens of adversarial robustness, shedding light on the possibility of building more robust and generalizable neural VRP methods in practice.\nThe limitation of this work is the increased computational complexity due to the need to synergistically train multiple models. Fortunately, based on experimental results, CNF with just three models has already achieved commendable performance. It even surpasses vanilla AT trained with nine models, demonstrating a better trade-off between empirical performance and computational complexity.\nInteresting future research directions may include: 1) designing efficient and effective attack or defense methods for other COPs; 2) pursuing better robustness with fewer computation, such as through conditional computation and parameter sharing; 3) theoretically analyzing the robustness of neural VRP methods, such as certified robustness; 4) investigating the potential of large language models to robustly approximate optimal solutions to COP instances."}, {"title": "A Frequently Asked Questions", "content": "Load Balancing. In this paper, load balancing refers to distributing each model with a similar or the same number of training instances from the instance set X, in each outer minimization step. It can avoid the appearance of dominant model or biased performance. The proposed neural router with the TopK operator explicitly ensures such load balancing since each model is assigned exactly K instances based on the probability matrix predicted by the neural router.\nWhy does CNF Work? Instead of simply training multiple models, the effectiveness of the proposed collaboration mechanism in CNF can be attributed to its diverse adversarial data generation and the reasonable exploit of overall model capacity. As shown in the ablation study (Fig. 3(a)), the diverse adversarial data generation is helpful in further improving the adversarial robustness (see results of CNF vs. W/O Global Attack). Meanwhile, the neural router has a bigger effect in mitigating the trade-off (see results of CNF vs. W/O Router). Intuitively, by distributing instances to suitable submodels for training, each submodel might be stimulated to have its own expertise. Accordingly, the overlap of their vulnerability areas may be decreased, which could promote the collaborative performance of CNF. As shown in Fig. 5, not all models perform well on each kind of instance. The expertise of $\\theta_0$ lies primarily in handling clean instances, whereas $\\theta_1$ specializes in dealing with adversarial instances. Such diversity in submodels contributes to the collaborative efficacy of $\\Theta$ and the mitigation of the undesirable trade-off between standard generalization and adversarial robustness, thereby significantly outperforming vanilla AT with multiple models.\nWhy using Best-performing Model for Global Attack? The collaborative performance of our framework depends on the best-performing model $\\theta_\\mathcal{O}$ w.r.t. each instance, since its solution will be chosen as the final solution during inference. The goal of inner maximization is to construct the adversarial instance that can successfully fool the framework. Intuitively, if we choose to attack other models (rather than $\\theta_\\mathcal{O}$), the constructed adversarial instances may not successfully fool the best-performing model $\\theta_\\mathcal{O}$, and therefore the final solution to the adversarial instance could still be good, which contradicts the goal of inner maximization. Therefore, to increase the success rate of attacking the framework and generate more diverse adversarial instances, for each instance, we choose the corresponding best-performing model $\\theta_\\mathcal{O}$ as the global model in each attack iteration.\nDivergence of Same Initial Models. We take POMO [38] as an example. During training, in each step of solution construction, the decoder of the neural solver selects the valid node to be visited by sampling from the probability distribution, rather than using the argmax operation. Even though we initialize all models using the same pretrained model, given the same attack hyperparameters (e.g., attack iterations), the adversarial instances generated by different models are generally not the same at the beginning of the training. Therefore, the same initial models can diverge to different models due to the training on different (adversarial) instances.\nLarger Model. In addition to training multiple models, increasing the number of parameters for a single model is another way of enhancing the overall model capacity. However, technically, 1) a larger model needs more GPU memory, which puts more requirements on a single GPU device. It is also more sophisticated to enable parallel training on multiple GPUs compared to the counterpart with multiple models; 2) currently, our method conducts AT upon the pretrained model, but there does not exist a larger pretrained model (e.g., larger POMO) in the literature. Despite the technical issues, we try to pretrain a larger POMO (i.e., 18 encoder layers with 3.64M parameters in total) on the uniform distributed data, and further conduct the vanilla AT. The performance is around 0.335% and 0.406% on clean and adversarial instances, respectively, which is inferior to the counterpart with multiple models (i.e., POMO_AT (3)). The superiority of multiple models may be attributed to its ensemble effect and the capacity in learning multiple diverse policies."}, {"title": "Advanced AT Methods in Other Domains", "content": "In this paper, we mainly focus on the vanilla AT [48]. More advanced AT techniques, such as TRADES [85], AT in RL [55], and ensemble-based AT [65, 32, 53, 79, 81] can be further considered. However, some of them may not be applicable to the VRP domain due to their needs for ground-truth labels or the dependence on the imperceptible perturbation model. 1) TRADES is empirically effective for trading adversarial robustness off against standard generalization on the image classification task. Its loss function is formulated as $\\mathcal{L} = \\text{CE}(f(x), y) + \\beta \\text{KL}(f(x), f(\\tilde{x}))$, where CE is cross-entropy; KL is KL-divergence; x is a clean instance; $\\tilde{x}$ is an adversarial instance; f(x) is the logit predicted by the model; y is the ground-truth label; $\\beta$ is a hyperparameter. By explicitly making the outputs of the network (logits) similar for x and $\\tilde{x}$, it can mitigate the accuracy-robustness trade-off. However, the above statement is contingent upon the imperceptible perturbation model, where the ground-truth labels of x and $\\tilde{x}$ are kept the same. As we discussed in Section 3.2, in the discrete VRPs, the perturbation model does not have such an imperceptible constraint, and the optimal solutions to x and $\\tilde{x}$ are not the same in the general case. Therefore, it does not make sense to make the outputs of the model similar for x and $\\tilde{x}$. 2) Another interesting direction is AT in RL, where the focus is the attack side rather than the defense side (e.g., most of the design in [55] focuses on the adversarial agent). Specifically, it jointly trains another agent (i.e., the attacker or adversary), whose objective is to impede the first agent, to generate hard trajectories in a two-player zero-sum way. Its goal is to learn a policy that is robust to modeling errors in simulation or mismatch between training and test scenarios. In contrast, our work focuses on the defense side and aims to mitigate the trade-off between standard generalization and adversarial robustness. Moreover, this method is specific to RL while our framework has the potential to work with the supervised learning setting. Overall, it is non-trivial to directly apply this method to address our research problem (e.g., the trade-off). But it is an interesting future research direction to design attack methods specific to RL (e.g., by training another adversarial agent or attacking each step of MDP). 3) Similar to the proposed CNF, ensemble-based AT also leverages multiple models, but with a different motivation (e.g., reducing the adversarial transferability between models to defend against black-box adversarial attacks [79]). Concretely, ADP [53] needs the ground-truth labels to calculate the ensemble diversity. DVERGE [79] depends on the misalignment of the distilled feature between the visual similarity and the classification result, and hence on the imperceptible perturbation model. Therefore, it is non-trivial to directly adapt them to the discrete VRP domain. DivTrain [32] proposes to decrease the gradient similarity loss to reduce the overall adversarial transferability between models, and TRS [81] further uses a model smoothness loss to improve the ensemble robustness. Technically, their methods are computational expensive due to the needs to keep the computational graph before taking an optimization step. We show their empirical results in Table 7. Compared with others, their empirical results are not superior in VRPs. For example, for TSP100, POMO_TRS (3), which is adapted from [81], achieves 0.098% and 0.528% on the metrics of Uniform and Fixed Adv., respectively, failing to mitigate the undesirable trade-off. We leave the discussion on defensive methods from other domains (e.g., language and graph) to Section 2."}, {"title": "Attack Budget", "content": "As discussed in Section 3.2", "perspectives": 1, "100": "for [87"}]}