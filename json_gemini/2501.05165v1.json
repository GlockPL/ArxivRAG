{"title": "Bringing Order Amidst Chaos: On the Role of Artificial Intelligence in Secure Software Engineering", "authors": ["Matteo Esposito"], "abstract": "Context: Developing secure and reliable software is an enduring challenge in software engineering (SE). The current evolving landscape of technology brings myriad opportunities and threats, creating a dynamic environment where chaos and order vie for dominance. Secure software engineering (SSE) faces the continuous challenge of addressing vulnerabilities that threaten the security of software systems and have broader socio-economic implications, as they can endanger critical national infrastructure and cause significant financial losses. Researchers and practitioners investigated methodologies such as Static Application Security Testing Tools (SASTTs) and artificial intelligence (AI) such as machine learning (ML) and large language models (LLM) to identify and mitigate these vulnerabilities, each possessing unique advantages and limitations.\nAim: In this thesis, we aim to bring order to the chaos caused by the haphazard usage of AI in SSE contexts without considering the details that specific domain holds and can impact the accuracy of AI.\nMethodology: Our Methodology features a mix of empirical strategies to evaluate effort-aware metrics, analysis of SASTTs, method-level analysis, and evidence-based strategies, such as systematic dataset review, to characterize vulnerability prediction datasets.", "sections": [{"title": "1 Introduction", "content": "In software engineering, defect prediction is essential for improving software reliability and quality, guiding efforts to address potential issues preemptively. Researchers have developed various models for defect prediction, using metrics such as product attributes, process details, historical defect data, change-inducing fixes, and even leveraging deep learning for feature engineering directly from source code. However, despite extensive work on predicting defects in commits and classes, there has been limited exploration of the potential benefits of integrating these predictive models. We investigate how defect predictions for methods and classes may be improved by incorporating just-in-time (JIT) defect prediction, which leverages commit-defectiveness data.\nTo enhance classifier accuracy in defect prediction, we focus on effort-aware metrics (EAMs), which evaluate how effectively a classifier ranks defect-prone entities. For example,  PofBx measures the percentage of bugs a developer can identify by inspecting the top x percent of the code. Our work addresses a critical gap by normalizing EAMs to better account for entities' size and importance. This leads to an optimized ranking process and a higher defect detection rate with minimal inspection efforts.\nIn today's world, software underpins everything from household devices to critical systems in transportation and healthcare. However, as systems grow in complexity, so does the security threat landscape. Vulnerabilities in software represent a costly risk to businesses and a significant threat to national infrastructure. Effective identification of these vulnerabilities relies"}, {"title": null, "content": "on Static Application Security Testing Tools (SASTTs), which analyze code for potential weaknesses without executing it. These tools are often benchmarked against the Common Weakness Enumeration (CWE) framework and the Juliet Test Suite, both of which classify and document known software vulnerabilities. Yet, SASTTs frequently generate false positives, driving researchers to explore machine learning as a complementary solution.\nVulnerability prediction has emerged as a proactive measure to prevent potential exploits, helping to secure systems by identifying software sections that may harbor vulnerabilities. Since machine learning models rely on large, validated datasets, dataset availability and quality are crucial for reproducible research in vulnerability prediction studies. Addressing this, we provide a systematic review of existing datasets alongside a tool, VALIDATE, to streamline dataset discovery and enhance research replicability in vulnerability prediction.\nMoreover, the increasing accumulation of unresolved vulnerabilities or security debt has underscored the need for prioritizing remediation efforts. Like technical debt, unresolved vulnerabilities can grow exponentially if not addressed, posing significant risks to organizations' security. By introducing a framework for vulnerability prioritization, our work aids organizations in focusing resources on high-risk vulnerabilities to reduce security debt and maintain a strong security posture."}, {"title": null, "content": "In mission-critical environments, the rapid identification of potential risks is essential. Preliminary Security Risk Analysis (PSRA) is used in sectors like healthcare, aerospace, and finance to evaluate whether a scenario might pose a security risk without identifying specific vulnerabilities. Large Language Models (LLMs) offer promising potential for PSRA by rapidly processing and synthesizing information to assist in risk assessment. In this work, we present a case study on applying LLMs for PSRA, evaluating a fine-tuned model's proficiency against seven human experts. This study demonstrates LLMs' capability to augment traditional risk analysis and shows that AI may provide valuable support for cybersecurity risk management.\nThrough these contributions, we address critical areas in defect prediction, vulnerability detection, and risk assessment, advancing both the effectiveness of predictive models and the accessibility of tools for research reproducibility. Our approach strengthens traditional software engineering practices and opens new avenues for AI-driven security solutions and hybrid classical-quantum computing in software reliability."}, {"title": "1.2 Goal and Research Questions", "content": "Developing secure and reliable software is an enduring challenge in software engineering (SE). The current evolving landscape of technology brings myriad opportunities and threats, creating a dynamic environment where chaos and order vie for dominance. Amidst the SE tasks, the role of artificial intelligence (AI) emerges as a beacon of hope, offering promising avenues for enhancing the security and resilience of software systems.\nWe aim to bring order in the chaos caused by the usage of AI in different contexts without considering the details of the specific domain in which secure software engineering exploits AI. Hence, we focus on two specific domains: defect prediction and vulnerability detection and prioritization."}, {"title": "1.3 Contributions", "content": "Our work advances defect prediction and vulnerability detection techniques through several targeted contributions. First, we conduct a systematic mapping study on effort-aware metrics (EAMs), uncovering trends and revealing issues in the inconsistent use of these metrics. To enhance accuracy and comparability, we propose a normalization approach for EAMs, which improves the ranking of classifiers and demonstrates the need for multiple EAMS to evaluate classifier performance fully. We also introduce a computational tool that supports reproducibility and generalizability in defect prediction studies by simplifying EAM computation and ensuring consistent terminology. Similarly, to advance JIT defect prediction, we evaluate CDP versus within-project MDP approaches, showing how leveraging JIT information can increase prediction accuracy in both scenarios (RQ1.1).\nFurthermore, we explore hidden risks in untouched code by analyzing the frequency and defectiveness of UM and TM. This analysis clarifies how data segmentation impacts defect prediction and offers models that improve bugginess predictions based on isolated UM and TM data (RQ1.2).\nIn vulnerability detection, we examine the relationship between CVE, CWE, and SQ rules, highlighting ambiguities that may mislead practitioners. Our correlation analysis between NVD severity and default severity shows how severity ratings influence vulnerability management practices (RQ2.1). We also provide the first comprehensive review of vulnerability prediction datasets and introduce VALIDATE, an online tool that facilitates dataset discovery based on specific features (RQ2.2)."}, {"title": "2 State of the Art", "content": "This Chapter presents the background literature and related work to our thesis. For each area of research, we compiled the state of the art and compared the previous methodologies and results with the ones obtained in our contributions to the SSE field."}, {"title": "2.1 Effort Aware Metrics", "content": "2.1.1 Accuracy metrics\nAccuracy metrics evaluate the ability of a classifier to provide correct classifications. Examples of accuracy metrics include the following:\n\u2022 True Positive (TP): The class is actually defective and is predicted to be defective.\n\u2022 False Negative (FN): The class is actually defective and is predicted to be non-defective.\n\u2022 True Negative (TN): The class is actually non-defective and is predicted to be non-defective.\n\u2022 False Positive (FP): The class is actually non-defective and is predicted to be defective.\n\u2022 Precision: TP / TP+FP\n\u2022 Recall: TP / TP+FN\n\u2022 F1-score: 2*Precision*Recall / Precision+Recall\n\u2022 AUC (Area Under the Receiving Operating Characteristic Curve) [313] is the area under the curve, of true positive rate versus false positive rate, that is defined by setting multiple thresholds. AUC has the advantage of being threshold-independent."}, {"title": null, "content": "TP*TN-FP*FN / \u221a(TP+FP)(TP+FN)(TN+FP)(TN+FN)\n2* Recall*(Recall+(1-pf)) / Recall+(1-pf)\nFP / FP+TN\n\u2022 MCC (Matthews Correlation Coefficient) is commonly used in assessing the performance of classifiers dealing with unbalanced data [249], and is defined as:\nIts interpretation is similar to correlation measures, i.e., MCC < 0.2 is considered to be low, 0.2 < MCC < 0.4\u2014fair, 0.4 < MCC < 0.6\u2014moderate, 0.6 < MCC < 0.8\u2014strong, and MCC \u2265 0.8\u2014very strong.\n\u2022 Gmeasure:  is the harmonic mean between recall and probability of false alarm (pf), which denotes the ratio of the number of non-defective modules that are wrongly classified as defective to the total number of non-defective modules as.\nA drawback of the metrics above is that they somehow assume that the costs associated with testing activities are the same for each entity, which is not reasonable in practice. For example, costs for unit testing and code reviews are roughly proportional to the size of the entity under test.\n2.1.2 Effort-aware metrics\nThe rationale behind EAM is that they focus on effort reduction gained by using classifiers [259]."}, {"title": null, "content": "In general, there are two types of EAM: normalized by size or not normalized by size. The most known not-normalized EAM is called PofB [64, 399, 417, 385] which is defined as the proportion of defective entities identified by analyzing the first x% of the code base as ranked according to their probabilities, as provided by the prediction model, to be defective. The better the ranking, the higher the PofB, the higher the support provided during testing. For instance, a method with a PofB10 of 30% means that 30% of defective entities have been found by analyzing 10% of the codebase using the ranking provided by the method.\nSince the PofBX of a perfect ranking is still costly, it is interesting to compare the ranking provided by a prediction model with a perfect ranking; this helps understanding how the prediction model performed compared to a perfect model. Therefore, Mende et al [259], as inspired by Arisholm et al. [25], proposed Popt which measures the ranking accuracy provided by a prediction model by taking into account how it is worse than a perfect ranking and how it is better than a random ranking. Popt is defined as the area Aopt between the optimal model and the prediction model. In the optimal model, all instances are ordered by decreasing fault density, and in the predicted model, all instances are ordered by decreasing predicted defectiveness. The equation of computing Popt is shown below, where a larger Popt value means a smaller difference between the optimal and predicted model: Popt =1 Aopt [428].\nPopt and PofB are two different metrics describing two different aspects of the accuracy of a model. Popt and PofB rank entities in two different ways: Popt according to bug density (i.e., bug probability divided by entity size), PofB according to bug probability. Therefore, the ranking of classifiers provided by Popt and PofB might differ. Finally, Popt is more realistic than PofB as the ranking is based on density rather than probability. However, Popt is harder to interpret than PofB as a classifier with the double of Popt does not provide the double of benefits to its user. Thus, in our thesis, we try to bring the best of PofB and Popt by proposing a new EAM metric that ranks entities similarly to both Popt and PofB."}, {"title": null, "content": "In the following, we describe additional EAMS.\n\u2022 Norm(Popt): is introduced by [130] and coincides with Popt20\n\u2022 PCI@20% and PMI@20%: have been introduced b [173] and [67] respectively and they represent the Proportion of Changes Inspected and Proportion of Modules Inspected, respectively, when 20% LOC are inspected. Note that these metrics are about the ranking of modules in general rather than about the ranking of defective modules. The idea behind these two similar metrics is that context switches shall be minimized to support effective testing. Specifically, a larger PMI@20% indicates that developers need to inspect more files under the same volume of LOC to inspect. Thus bug prediction models should strive to reduce PMI@20% while trying to increase Popt [315] simultaneously.\n\u2022 PFI@20%: has been introduced by [315] and it coincides with PMI@20 [67] when the module is a file.\n\u2022 IFA: \u201creturns the number of initial false alarms encountered before the first real defective module is found\u201d [67]. This effort-aware performance metric has been considerably influenced by previous work on automatic software fault location [205]. When IFA is high then there are many false positives before detecting the first defective module. [67]."}, {"title": null, "content": "\u2022 Peffort: has been introduced by [87] and it is similar to our proposed NPofB. Peffort uses the LOC metric as a proxy for inspection effort. Peffort evaluates a ranking of entities based on the number of predicted defects divided by size. In contrast, our NPofB evaluates a ranking of entities based on the predicted defectiveness divided by size.\n2.1.3 Evaluations\nAs EAMs drive and impact the results of prediction models evaluations, it is important to discuss studies about how to evaluate prediction models. The evaluation of prediction models performed in studies has been largely discussed.\nMany papers explicitly criticized specific empirical evaluations. For instance, [160] criticized the use of the ScottKnottESD test in [372].\n[352] found that the choice of classifier has less impact on results than the researcher group. Thus, they suggest conducting blind analysis, improve reporting protocols, and conduct more intergroup studies. [374] replied for a possible explanation for the results aside from the researcher's bias; however, after a few months, [354] concluded that the problem of the researcher's bias remains."}, {"title": null, "content": "[439] criticized [261] because their results are not satisfactory for practical use due to the small percentage of defective modules. [439] suggest using accuracy metrics, such as Recall and Precision, instead of pd or pf. [260] replied that it is often required to lower precision to achieve higher recall and that there are many domains where low precision is useful. [260], in contrast to [439], advised researchers to avoid the use of precision metric; they suggest the use of more stable metrics (i.e., recall (pd) and false alarm rates) for datasets with a large proportion of negative (i.e. not defective) instances.\n[117] reports on the importance of preserving the order of data between the training and testing set. Afterward, the same issue was deeply discussed in [132] Thus, results are unrealistic if the underlying evaluation does not preserve the order of data.\n[115] show that dormant defects impact classifiers' accuracy and hence its evaluation. Specifically, an entity, such as a class or method used in the training/testing set, can be labeled in the ground-truth as defective only after the contained defect is fixed. Since defects can sleep for months or years [66, 5] then the entity erroneously seems to be not defective until the defect it contains is fixed. Thus, [5] suggest to ignore the most recent releases to avoid that dormant"}, {"title": null, "content": "defects impact classifiers' accuracy.\n[353] commented on the low extent to which published analyses based on the NASA defect datasets are meaningful and comparable.\nVery recently [267] proposed a new approach and a new performance metric (the Ratio of Relevant Areas) for assessing a defect proneness model by considering only parts of a ROC curve. They also show the differences and how their metric is more reliable and less misleading compared to the existing ones."}, {"title": "2.2 Defect Prediction", "content": "In the context of class-level bugginess prediction, various papers were published suggesting methodologies, tools, and metrics to increase the prediction's accuracy. In the context of metrics, two main papers describe a catalog of metrics. Code metrics were first introduced by Menzies et al. [261] showing that despite the dispute of \"McCabes versus Halstead versus lines of code counts,\" the relevant thing was how the attributes were used to build predictors. In this context, the prediction was targeted to more useful prioritization of resource-bound code exploration. Process metrics, on the other hand, were first introduced by Weyuker et al. [405]. They have shown that considering change information about the files, like adding developer information as a prediction factor, such as the number of developers that made changes to each file both in the prior release and cumulatively raised the accuracy of correctly identified bugs, from a 20% to 85%.\nZimmermann et al. [450] Investigated an important paramount question: Where do bugs come from? Their work was centered around three specific releases of the Eclipse project. They showed that the combination of complexity metrics can predict bugs, suggesting that the more complex the code is, the more defects it has, so practically attributing to code complexity the root of software bugs.\nFew studies have recently focused on predicting methods' bugginess rather than classes. Giger et al. [146] and Hata et al. [158] first introduced the concept of process and code metrics, also used by Pascarella et al. [298]."}, {"title": null, "content": "McIntosh and Kamei [254] presented bug prediction models at the methods level. The models were based on change metrics and source code metrics typically used in bug prediction. Their results indicate that change metrics significantly outperform source code metrics in general., Still, their study did not consider, like we do, the differences between a touched and an untouched method.\nMany recent bug-predicting studies are focusing on predicting at the level of the commit [317, 173, 207, 190, 254, 124] or even of the line [312] into a commit.\nIn the context of predicting Just-in-time a buggy commit, Kamei and Shihab [190] showed that rapid changes in the properties of fix-inducing changes could impact the performance and interpretation of JIT models. Indeed, fluctuation in the relevance of some metrics at a specific point of the development process, combined with a shift in the development object and the involved entities, may severely drop the model's performance.\nFan et al. [124], as Kamei and Shihab [190], remarked that the labeling part indeed covers the most crucial role and that the model's performance is not influenced only by the amount of data. Inadequate labeling induces great confusion in the model, i.e., JIT models, in which performance can severely drop."}, {"title": null, "content": "Chen et al. [66] firstly introduced the concept of dormant defects (though called dormant bugs). They showed that dormant defects are fixed faster and by more experienced developers. Similarly, Rodriguez-Perez et al. [332] and Costa et al. [80] show that the time to fix a defect, i.e., the sleeping phenomenon, is on average about one year. Thus, we conclude that dataset creation will miss most defects on releases less than a year old. In a recent paper [5] we reported that many defects are dormant and many classes snore. Specifically, on average among projects, most of the defects in a project are dormant for more than 20%"}, {"title": "2.2.1 Dataset creation from mining version control systems", "content": "of the existing releases, and 2) in most projects the missing rate is more than 25% even if we remove the last 50% of releases. Concerning previous work on dormant defects, including ours, in our thesis, we quantify, for the first time, the effect of dormant defects on defect prediction accuracy and their evaluation, and we provide and evaluate a countermeasure.\nMany papers have suggested the use of feature selection to improve the performance of classifiers. For instance, Shivaji et al. [357] used a feature selection technique for bug-prediction using classifiers like Naive Bayes and Support Vector Machine (SVM), reporting that a reduction of 4.1% to 12.52% of the original feature's set yielded optimal classification result increasing both speed and accuracy.\n2.2.1 Dataset creation from mining version control systems\nRegarding the dataset creation, similar to what was done by Giger et al. [146], we used the link between Jira Tickets and Bug-fixing commit to get the original labels for the bugginess. Still, as discovered by Ahluwalia et al. [5], dormant bugs do exist, so following their same workflow, we proceeded to label as buggy the same methods in all their dormant states. A problem of paramount importance when creating defect prediction models is represented by noise in the underlying datasets. In this context, different works study the noise sources and foresee countermeasures for that. Kim et al. [201] measured the impact of noise on defect prediction models and provided guidelines for acceptable noise levels. They also propose a noise detection and elimination algorithm to address this problem. However, the noise studied and removed is supposed to be random."}, {"title": "2.2.2 Combining heterogeneous predictions", "content": "Tantithamthavorn et al. [373] found that (1) issue report mislabelling is not random; (2) precision is rarely impacted by mislabeled issue reports, suggesting that practitioners can rely on the accuracy of modules labeled as defective by models that are trained using noisy data; (3) however, models trained on noisy data typically achieve about 60% of the recall of models trained on clean data. Herzig et al. [166] reported that 39%\n2.2.2 Combining heterogeneous predictions\nWhile countless studies investigated how to predict the defectiveness of commits [165, 254, 296, 161, 208, 173, 124, 385, 333, 297, 146], or classes [188, 372, 188, 39, 375, 371, 162, 170, 418, 236, 70, 184, 93, 291, 364, 435, 213, 428, 300, 164, 316, 354, 163, 22, 32, 40, 207, 267, 268, 380, 436, 180, 63, 88] in a separate fashion, to the best of our knowledge, no study other than Pascarella et al. [296], investigated how heterogeneous predictions can benefit one another. Another family of studies that combines heterogeneous information is the ensemble model, which has been used in the context of defect prediction as a way to combine the prediction of several classifiers [210, 301, 382, 420]."}, {"title": "2.2.3 Method Defectiveness Prediction", "content": "2.2.3 Method Defectiveness Prediction\nThe first proposing of lowering the granularity of defective prediction have been Menzies et al. [261] and Tosun et al. [381]. Giger et al. [146] were the first to perform an MDP study. Specifically, Giger et al. [146] defined a set of product and process features and found that both product and process features support MDP (i.e., F-Measure=86%).\nOur paper has been highly inspired by Pascarella et al. [297]. Specifically,"}, {"title": null, "content": "Pascarella et al. [297] provide negative results regarding the performance of MDP. In other words, using the same design of Giger et al. [146], they show that MDP is as accurate as a random approach, i.e., the obtained AUC is about 0.51. We share with them several design decisions, including:\n\u2022 The use of process metrics as features for MDP. Specifically, \"The addition of alternative features based on textual, code smells, and developer-related factors improve the performance of the existing models only marginally, if at all.\" [297]\n\u2022 The use of a realistic validation procedure. However, they performed a walk-forward procedure, whereas we performed a simple split by preserving. However, both procedures are realistic since they preserve the order of data [117].\nThe differences in design include:\n\u2022 We use an advanced SZZ implementation (RA-SZZ) whereas they use Relink [415].\n\u2022 The use of a different definition of a defective entity. In our research, an entity is defective from when the defect was injected until the last release before the defect has been fixed.\n\u2022 We use a different set of classifiers.\n\u2022 We use effort-aware metrics such as PofB.\n\u2022 We selected a different set of projects from which we derived the datasets. The change was since we needed the same dataset to produce commit, method, and class data."}, {"title": null, "content": "The differences in results include that the accuracy achieved by MDP, even without leveraging JIT, is much better than a random approach. Specifically, According to Figure 3.1.19, the median AUC across classifiers and datasets is 0.81 without leveraging JIT and 0.96 when leveraging JIT. Moreover, the proportion of defective methods is lower by about an order of magnitude in our datasets than in their datasets. Those differences are due to the set of changes in the design, and we prefer not to speculate on which specific change caused the difference in results.\nTo our knowledge, no study is investigating MDP other than Pascarella et al. [297] and Giger et al. [146].\nDefect prediction can focus on finer-grained software entities other than methods, such as commits (JIT) and statements [312]. However, these types of entities (commits and statements) seem more helpful when ranked at the moment of a commit instead of during the testing phase (our target phase in our thesis)."}, {"title": "2.3 Static Analysis Security Testing Tools", "content": "In this section, we present the background and the related work using SASTTs. Undiscovered vulnerabilities can lead to costly impacts on software firms [377]. Cavusoglu et al. [54] show that a fast fix is essential to avoid loss connected to the vulnerability and its public disclosure. Louridas [240] present the considerable cost associated with inadequate software testing infrastructure; thus, a vulnerability discovery tool is needed [94]. SASTTs are a practical choice to actively discover software vulnerabilities during development [51, 351, 45, 441]. The National Vulnerabilities Database NVD [281] collects the Common Vulnerabilities and Exposures (CVE) cataloged via the CWEs [246]. SASTTs are developed to target CWEs for vulnerability discovery. While multiple SASTTS can identify the same vulnerability, only a few can detect more specific and challenging CWEs [288]. NIST provides the SARD Test Suites [44] targetting the most common Programming Languages for benchmarking SASTTS. SASTT analysis can support code review. Tufano et al. [387] very recently conducted an exhaustive examination of the state of the art in code review. The authors evaluate ChatGPT's effectiveness and found that it falls short of outperforming human expertise in code review. Therefore, human intervention remains necessary, regardless of the extent of automation implemented."}, {"title": null, "content": "The wide spreading of software vulnerability increases over time, and new vulnerability discovery is becoming a matter of every day [272]. To aid developers in reducing the vulnerability present in the latest releases of software, various SASTTS exist [28, 288, 277, 394] targeting different and specific vulnerabilities hence, showing diverse capabilities among them [59]. The most important and used SASTTS [288, 220, 59, 152, 28, 16, 277, 214, 222, 111] include the following: FindSecBugs: Security plugin for SpotBugs analyzing Java code for vulner- abilities [138]. Infer Facebook's scalable static analysis tool for Java, C, and C++ detecting errors and vulnerabilities [114]. JLint: Open-source Java code analyzer for identifying errors and security issues [27]. PMD: Java static analy- sis tool providing detailed reports on issues like unused code [308]. SonarQube: Open-source code quality management tool supporting multiple languages [363]. Snyk Code: Identifies code issues in Java and Python projects, offering detailed reports [362]. Spotbugs: Free Java bytecode analyzer for problems and vulner- abilities with bytecode analysis [366]. VCG: Security review tool for various languages, including Java, with customizable checks [393].\nYang et al. [419], discusses the challenges associated with SASTTs in software development due to difficulties handling large numbers of reported vulnerabil- ities, a high rate of false warnings, and a lack of guidance in fixing reported vulnerabilities. To address these challenges, the authors propose a set of ap- proaches called Priv. The authors show that it effectively identifies and prior- itizes vulnerability warnings, reduces the rate of false positives, and provides complete and correct fix suggestions for a significant percentage of evaluated warnings."}, {"title": "2.3.1 Evaluations on test suites", "content": "2.3.1 Evaluations on test suites\nTable 2.3.1 presents the critical aspect of related work on evaluating SASTTS on publicly available benchmarks such as JTS. Oyetoyan et al. [288] discuss the inclusion of tools to aid agile development pipeline in focusing on security as- pects; the authors choose to target the 112 CWE provided by the JTS (v. 1.2) testing a total of six different tools, focusing on the accuracy metrics. Li et al."}, {"title": "2.4 Vulnerability Prediction", "content": "This section introduces ML for Software Engineering (SE) and discusses related works in VPS, emphasising reproducibility.\nML can support SE in many tasks [434]. Zhang and Tsai [433] defined the field of SE as a \"fertile ground where many software development and mainte- nance tasks could be formulated as learning problems and approached in terms of learning algorithms\". ML can tackle problems that humans usually have a mere grasp or no knowledge at all [433] or optimise solutions to otherwise well- known problems but with inefficient solutions[346, 339]. Researchers focus on systematically reviewing the tasks of SE that benefit from the ML \"unparal- leled capabilities\" [421, 247, 434, 147]. For instance, Lyu et al. [242] discuss the development process that benefited from emerging AIOps models. Kapur and Sodhi [193] discuss the effort estimation based on metrics such as software features similarity and developer activity. Durelli et al. [100] conduct a mapping study on ML applications for software testing."}, {"title": null, "content": "The activity of mining software repositories (MSR) led to ideas and chal- lenges for empirical studies in SE [29, 396, 248]. Hassan et al. [157] point out the effectiveness of MSR in empirically validating new ideas and techniques. MSR activity supports the creation of datasets containing useful information for predictive models [390, 1, 266, 200, 239, 104]. Several studies describe tools, techniques, and advantages in automatic mining [273, 228, 409]. Bavota [36] presents issues in mining software repositories, including the lack of meaningful content in commit messages [167], misclassification of data [24, 373] and the missing link between ticket and commit [30]. Zhou et al. [447] investigate on how automatically identify security patches through commit-related data; Zou et al. [451] uses ML to protect the return pointer. Finally, Vandehei et al. [391] and Falessi et al. [115] highlight the importance, in the data preparation stage, of correctly labelling the data according to the ticketing system and the information from the version control system (VCS) platform.\nFinally, Ibrahim et al. [175] discuss the significant threat posed by software vulnerabilities in opensource projects, which can compromise system integrity, availability, and confidentiality. Their analysis of the top 100 PHP opensource projects reveals that 27% of them exhibit security vulnerabilities. Increasing cyber-warfare [15] and data breaches [197] threaten critical infrastructure and user privacy [378, 178, 223]. Furthermore, our recent study [108] reveals that vulnerability default severity might be inaccurate.\n2.4.1 Machine Learning for Vulnerability Prediction Studies"}, {"title": "2.4.1 Machine Learning for Vulnerability Prediction Studies", "content": "ML for VPS led to several studies that approached challenges and novel ideas in the field [368, 350, 224, 414, 285, 320, 368]. Croft et al. [83] conduct an SLR in the data preparation phase of a VPS study. They show that data is the crucial component of any data-driven application; nevertheless, the preparation phase of a dataset is still full of challenges. Our study relies on Croft et al. [83] study selection as all datasets require the data preparation phase. We focus on the entire dataset rather than only on the data preparation phase. Thus, our selection of studies extends from Croft et al. [83]. We conduct a new SLR adopting Croft et al. [83] search strings and inclusion and exclusion criteria. We expand the search scope by adding two specific keywords, \"dataset\" and"}, {"title": "2.4.2 Replicability and Reproducibility", "content": "\"repository\", and by broadening the time range, including studies published until January 2023. Finally, it is essential to emphasise that although Croft et al. [83", "177": "experimented on the effectiveness of different ML and statistical techniques for software vulnerability prediction. The authors use goodness-of-fit and criteria on prediction capabilities to assess the performances. Jabeen et al. [177", "445": "investigate the factors that can affect the vulnerability detection capabilities of ML models. Zheng et al. [445", "448": "highlight a \"perception gap\" between deep learning and human experts in understanding code semantics. In real-world scenarios, deep learning-based methods underperform by over 50% compared to controlled experiments, prompting a deep dive into this phenomenon and exploring current solutions to narrow the gap.\nPartenza et al. [295"}]}