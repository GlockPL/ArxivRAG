{"title": "Present and Future Generalization of Synthetic Image Detectors", "authors": ["Pablo Bernabeu-Perez", "Enrique Lopez-Cuena", "Dario Garcia-Gasulla"], "abstract": "The continued release of new and better image generation models increases the demand for synthetic image detectors. In such a dynamic field, detectors need to be able to generalize widely and be robust to uncontrolled alterations. The present work is motivated by this setting, when looking at the role of time, image transformations and data sources, for detector generalization. In these experiments, none of the evaluated detectors is found universal, but results indicate an ensemble could be. Experiments on data collected in the wild show this task to be more challenging than the one defined by large-scale datasets, pointing to a gap between experimentation and actual practice. Finally, we observe a race equilibrium effect, where better generators lead to better detectors, and vice versa. We hypothesize this pushes the field towards a perpetually close race between generators and detectors.", "sections": [{"title": "1 Introduction", "content": "The rapid advancement of generative AI has revolutionized image generation, presenting challenges related to visual information integrity, trust and rights in digital environments, and mitigation of misinformation, among others. As a result, recent legislation mandates the identification and notification of the synthetic nature of such digital content [38]. By virtue of all these needs, correctly attributing synthetic content has become a social demand, and a top scientific priority (i.e., is this image authentic or synthetic?).\nThe field of synthetic image detection is in a continuous race with the field of synthetic image generation. The detection field tries to prevent the race by developing universal detectors [8, 32], but the generalization capacity of these solutions is not entirely known. Meanwhile, new generative models join the race every month (if not every day), begging for the question, where is this going?\nThis work uncovers where current Synthetic Image Detection (SID) is, and where it seems to be headed to. To do so, it first analyzes the impact of several training conditions on model generalization, producing a recipe for building robust detectors. Those guidelines are then used to train a baseline for the study of generalization across source variations (who created the data), model variation (which model was used) and model age (when was the model released). The same experimental setup is used to conduct an updated benchmark of state-of-the-art detector models, using synthetic data produced by the latest generators.\nFindings indicate current detectors are insufficient for synthetic content detection, especially when used alone. Suggestions include the use of detector ensembles, as well as the training of generator-specific detectors. This last option appears as the most reliable solution and is shown to generalize to different data sources. Finally, the ethical considerations of this line of work are discussed, including when and how should detectors be publicly released,"}, {"title": "2 Related Work", "content": "Any work done on training and analyzing synthetic image detectors is defined by the models it is trained and evaluated on. This limits the consistency and relevance of results in a fast-moving field like the one of generative AI, where models are continuously and quickly improved and replaced. The main body of previous work on synthetic image detectors was conducted on Generative Adversarial Network (GAN) generators (now only used because of their relative speed), as well as early versions of diffusion-based models [9, 17, 27, 32, 42, 43, 47, 48]. A few recent works target data produced by some of the latest diffusion models, which are the ones currently compromising the human ability to discriminate between real and fake [7, 8, 10, 33].\nGeneralization is key in the SID field, as failing to successfully detect samples produced by alternative generative models limits the applicability of any proposed solution. To study generalization, common practice in the field is to train models using data produced by one model, while evaluating performance on data produced by other models [7\u20139, 17, 32, 33, 47, 48]. However, none of these studies considers the temporal natural of generative models nor what this may tell us about the detection performance on models to be released in the future. Remarkably, this is the case that matters the most for the practical use of synthetic image detectors.\nWhile the field is missing an exhaustive and updated study on the generalization of SID, a few sources of bias have been considered before. Particularly, the effect of image format and resolution has been studied in the past. In [9], authors highlight the impact of the resizing operation, which is common in deep learning architectures, and used to adapt any image to the desired network input resolution. This transformation could remove the high-frequency artifacts created by the generative models, hindering the detection process. GenImage [48] studies the image content generalization, that is, how a detector performs on domains outside the training data, such as art and face images. They show a great generalization capability for these two cases, with 95% and 99.9% accuracy scores respectively. Yet, the generalization of image content has not been thoroughly explored. This is particularly important given that SID models, which often rely on common training datasets like LSUN [45] and ProGAN [22], suffer from limited class representation.\nThe study presented in [17] highlights biases associated with JPEG compression and image size, demonstrating that these biases are prevalent in many training datasets. Notably, they reveal a significant disparity in image formats between authentic and synthetic"}, {"title": "3 Methods", "content": "To focus on the properties of synthetic images, we will fix the detector architecture used in all our experiments (\u00a73.1). In contrast, we consider a variety of recent synthetic image datasets, and by extension of Al image generators, as this is the real target of our work (see \u00a73.2 and \u00a73.3). Finally, we report computation details, enabling full reproducibility of our work, in \u00a73.4."}, {"title": "3.1 Architecture", "content": "There are two popular architectural choices when building a synthetic image detector: training a direct classifier or using the features extracted from a pre-trained or fine-tuned model to discriminate samples. Both CNNs [17, 33, 48] and transformers (ViT encoder) [10, 32, 43, 47] have been considered to that end, both of them performing competitively.\nIn this work we fix the architecture, to focus on the evolution, nature and practical use of generalization models. For our experimentation, we choose a ResNet [19], as this has shown competitive results [7, 17, 48], and it's a relatively lightweight architecture. In particular, we follow the staircase design proposed in [30] with a ResNet-18 backbone (12.7M parameters), which feeds features extracted at different depths into an MLP (see Figure 1).\nSID often relies on analyzing specific portions of an image rather than the entire composition. This approach is advantageous for several reasons. Firstly, not all parts of an image may be artificial, as they can be partially edited or altered through methods like inpainting. Secondly, its nature may be better identified in specific portions of the image as some regions might contain more telltale signs of manipulation than others. Finally, processing entire high-resolution images can be computationally intensive and time-consuming.\nTo address these challenges, most detectors are trained on image patches. This introduces new variables in the detection process: the number of patches and their selection strategy, as well as the criterion to convert single-patch predictions to whole image ones. To determine our patch selection strategy, we conduct early experiments and find that patches which exhibit the highest contrast in their grey-level co-occurrence matrix (GLCM) produce slightly more performant models. Due to the size of our input images, we consider a maximum of five 224 \u00d7 224 patches. The predictions of these individual patches can then be aggregated according to the use case requirements (e.g., maximizing the detection of false positives, or prioritizing false negatives instead). In particular, we select a voting mechanism and set a minimum amount of patches for an image to be considered synthetic. We will study the effect of the value of this threshold in \u00a75.2."}, {"title": "3.2 Train Datasets", "content": "To train the synthetic content detectors we consider two types of datasets. The first contains real-world images, that is photographs of different scenes taken under different light conditions. We use COCO [26] for training and validation (\u00a74). The second type of datasets considered are those composed of synthetic images. That is, AI-generated images. Five datasets are used, created using five different Al image generators: dalle3-images, diffusiondb, SDXL, mj-tti and mj-images.\nThe dalle3-images [14] dataset contains 1,647 unique, deduplicated images generated by DALLE3 (2023), encompassing both photorealistic and digital art styles. Another dataset, the diffusiondb [40], was created using models of the 1.x Stable Diffusion series, which were released in 2022. In this dataset, we filter samples making sure that \"photo\" appears in the prompt. Images from this dataset are of lower quality and visual detail than those of its successor SDXL [3], which was released in 2023. The associated dataset, SDXL [13] contains 5,435 images in the \"realistic\" subset.\nBeyond DALL-E and Stable Diffusion, the third main provider of synthetic images is Midjourney. Its early iterations, the V1 and V2 models, date from early 2022, and were used to populate the mj-tti [37] dataset, which contains 4,530 images. Collage images and mosaics made of synthetic images were removed from this dataset. Later models, the V5 and V6 models from 2023 compose our last training dataset, mj-images [15], with 1,226 images. This dataset also had to be deduplicated. All these synthetic models will be used for training and validation in \u00a74.\nIn \u00a74, during the training stages of this work, COCO and diffusiondb datasets are undersampled to include a maximum of 5,435 samples. Existing train, validation and test partitions are respected. When undefined a 60%-20%-20% random split is performed. For the SDXL dataset we include the realistic-2.2 split for training and validation, and the realistic-1 split for test."}, {"title": "3.3 Evaluation Datasets", "content": "The datasets in this section will assess the detector's generalization ability when trained on limited data and applied to diverse external sources (\u00a7 5.1). The evaluation aims to cover three scenarios: datasets from the same model used in training but generated by different users or slightly different software or model versions; datasets from entirely different models; and samples from various unknown sources and models, simulating real-world applications.\nFor samples representing the class of authentic images, three datasets are added. The Flickr30k [44] dataset, which includes 31,655 samples primarily depicting scenes with humans. A subset of the Google Landmarks v2 [41] test partition, containing 5,000 images of both natural and human-made landmarks, and finally, the authors have curated a small-scale dataset referred to as In-the-wild. The first subset of this dataset, containing authentic images from platforms such as Reddit (from communities that forbid AI content) and Flickr (uploaded before 2020), is included as an authentic evaluation dataset.\nSynthetic data for evaluation is obtained either from the Synthbuster dataset [5] or is collected/generated by the authors. Synthbuster contains images generated with nine different models (1,000 images each), and provides a wide coverage for our experimentation by including data from models with our train set (e.g., SD1. X, DALLE3) and others outside of it (e.g., DALLE2, Firefly). The images are based on the RAISE-1k dataset [11], the corresponding prompts were manually refined to enhance photorealism and remove references to living persons or specific artists.\nThree datasets are crafted for this paper. The first contains 8,192 images generated with Stable Diffusion 3 Medium (SD3), a Multi-modal Diffusion Transformer (MMDiT) text-to-image model. Input prompts are extracted from Gustavosta/Stable-Diffusion-Prompts[18]. For each image, height and width were randomly selected from a uniform distribution over the set {512, 768, 1024, 1344} pixels. The"}, {"title": "3.4 Computation", "content": "The software stack used to develop and evaluate SID models was based on PyTorch. To enable the full reproducibility of our work, we share the code needed to run the experiments , the datasets compiled and cleaned , and the model weights for our best detector (see \u00a74.3.1).\nFor the benchmarking of other detectors (\u00a7 6), we use the sidbench library [34], which supports several models while facilitating inference on custom datasets. Available checkpoints were downloaded from the AIGCDetectBenchmark [46] repository. We download the missing models from their corresponding official sources.\nWe utilize an Intel Xeon Platinum 8460Y processor and one NVIDIA Hopper H100 64GB GPU. Seventy-five training runs were conducted with this setup, totalling sixteen hours of computing time, while continuously monitoring GPU power usage. Using the European Union's latest CO2 emission ratio [2], we estimated the carbon footprint of these experiments to be 0.63 kg of CO2."}, {"title": "4 Train Experiments", "content": "In this section we explore the effect of varying the training strategy, considering single-class models, multiclass classifiers, and finally, image alterations."}, {"title": "4.1 Single Class Models", "content": "To study the relation between the images produced using the latest image generation models, let us consider the generalization capacity of single-class discriminative models trained on those images. In this section we train binary classifiers, using each synthetic dataset of Table 1 as a positive class, and the COCO dataset as a negative class. Those models are then tested on the remaining datasets, to see how they generalize. Single-class models are trained for a maximum of 20 epochs with an early-stopping mechanism monitoring validation accuracy with patience of 2. Data augmentation is limited to horizontal flips with a 50% probability, with further transformation studies left for \u00a7 4.3."}, {"title": "4.2 Multi-class Models", "content": "The performance of binary classifiers can be misleading [32], learning only one class and predicting the alternative only by omission. To produce more robust detection models, better at generalizing, we must include datasets from different sources in their training set, enriching and detailing the decision boundary available for prediction.\nWe train two models to evaluate the effect of multi-class learning on generalization capacity. First, we train a binary classifier merging all synthetic data sources from Table 1 into a single synthetic class, including 14,323 images. An analogous amount of samples is drawn from the COCO dataset to compose the authentic class. Then, we train a six-class recognition model using the original splits defined in Table 1. To enable a direct comparison, in this experiment, a prediction of a multiclass model will be considered correct for a synthetic image if any synthetic class is predicted, regardless of whether it matches the ground truth class."}, {"title": "4.3 Image Alteration Methods", "content": "Images undergo various transformations when uploaded to social media and online hosting services. These modifications, primarily aimed at reducing file sizes to optimize storage and transmission costs, can significantly alter the original image characteristics. In addition, malicious actors may intentionally edit or manipulate images to obscure artifacts and patterns that synthetic image detectors rely on for accurate predictions. If image analysis models are not robust enough to withstand these transformations, their utility in real-world scenarios becomes severely limited.\nTo address this challenge, we conducted a series of experiments focused on understanding and enhancing model robustness. Our approach involves training six-class recognition models, which performed best in the previous experiments, using different data augmentation modalities in isolation, including various forms of blur (AdvancedBlur and GaussianBlur), brightness and gamma alterations (RandomBrightnessContrast and RandomGamma) and compression of images through the JPEG algorithm, from the Albumentations library [6]. These augmentation techniques were chosen to simulate the range of transformations that images might undergo in real-world conditions.\nIn addition to the baseline with no alterations, five additional models are trained (one for each alteration), and five new test sets are created (also one per alteration). With these, cross-testing is conducted to explore generalization across alteration methods. The"}, {"title": "4.3.1 SuSy - Our robust model.", "content": "In the following section (\u00a7 5), generalization is explored by evaluating different data sources. To scale experimentation, we fix a model based on the insights from this section, which we call SuSy. In detail, we train a six-class classifier, using the original splits defined in Table 1 and incorporating the alteration methods explored in the previous section to enhance its resilience against image manipulations. Each transformation is applied with a 20% probability, besides the horizontal flip which remains at 50%, allowing the model to see unaltered images and images that have suffered one or multiple transformations. We train this model for a maximum of 20 epochs with early stopping monitoring validation loss with patience 2, as in previous experiments."}, {"title": "5 Evaluation Experiments", "content": "So far generalization has been explored from the perspective of time, task, and alterations. All of them, in a controlled setting, using a limited amount of data sources. But this is unrealistic in practice. Al image generators have become a wildly popular technology, with large communities of users re-training and sharing models. Meanwhile, new and updated models keep coming out (e.g., FLUX.1).\nIn this situation, the biggest challenge for synthetic image detectors is the adaptation to a changing and uncontrolled generative environment. To test generalization in this context, we explore the role of data source, using the datasets described in \u00a73.3. First, this section considers the performance of SuSy (described in \u00a74), using the central patch of images. Then, the same experiments are conducted by making predictions at image level, using the patching strategy defined in \u00a7 3.1.\nAdditionally, to confirm the validity and difficulty of the In-the-wild dataset, we conduct a small human experiment. We ask 10 volunteers aged 22-30 who have social media accounts and are potential targets of AI-generated deception, to discriminate between both In-the-wild versions (authentic and synthetic). To ensure unbiased results, images were presented in random order and the evaluators were not informed about the distribution of the data. Participants took an average of 15 minutes to label them, with no time constraints imposed. Results are reported in Table 6 and used as a baseline."}, {"title": "5.1 Generalization to source", "content": "Let us explore the performance of SuSy when evaluating it on the datasets described in \u00a73.3. Table 5 shows the results, sorted by category. For the authentic datasets, reported at the top part of the table, SuSy generalizes well to the Flickr30k dataset and decently to Google Landmarks v2, but suffers a large drop in performance for In-the-wild images. This could be caused by changes in the scale of images, differences in the domain of information represented in the pictures and a higher amount of image postprocessing, and could potentially be mitigated by using a richer authentic class that combines images from different sources in the training dataset.\nThe second set of data sources considered include datasets produced using models that were also used during the training of SuSy, listed in Table 1. Even though the underlying generative models are the same, differences in the generation process (e.g., prompts, generator hyperparameters, etc.) can induce significant biases. In this case, generalization holds significantly well, with all datasets reaching a recall between 73% and 89%.\nIn the third set of experiments, which involves datasets obtained from models unseen during training, performance varies significantly, with recalls ranging from as high as 96% to as low as 20%. The impact of model family on performance is also inconsistent, as SuSy exhibits decent to strong generalization for Stable Diffusion models, with SD2 achieving a recall of 68.40% and SD3 reaching 93.23%. However, it only detects 20.70% of images from DALLE-2, despite being trained on DALLE-3.\nRegardless of its blind spots, SuSy performs significantly well in the most realistic setting, the one using In-the-wild images collected from unknown sources and manually selected based on their quality, aligning with the performance on the latest and most"}, {"title": "5.2 Generalization to image", "content": "While our initial experiments focused solely on the central patch of each sample, a prediction at image level is likely to be of interest in real-world scenarios. In this section, we explore the transition from patch-level to image-level predictions, and how this affects generalization. To achieve this, we employ the patch selection method described in \u00a73.1, selecting five patches and using a fixed threshold (minimum number of synthetic patches) to classify an entire image as synthetic."}, {"title": "6 Generalization of state-of-the-art models", "content": "In this section, we study the performance of state-of-the-art models on the evaluation datasets listed in \u00a7 3.3. We analyzed ten different models available in SIDBench [34], focusing on the six that demonstrated superior performance across our tests. The results for the four underperforming models (CNNDetect [39], FreqDetect [16], Fusing [21] and UnivFD [32]) are presented in Appendix A for completeness. Table 7 showcases the performance of the best models, LGrad [36], GramNet [29], Rine [23], DIMD [23], DeFake [9] and Dire [35].\nThe majority of the models examined leverage pre-trained neural networks as feature extractors, adapting them for SID. LGrad, GramNet, and DIMD all utilize CNNs, each with a unique emphasis on different image characteristics. Rine and DeFAKE shift towards more recent architectures, employing transformer-based models. In contrast, Dire does not rely on direct feature extraction, instead using the diffusion concept of image reconstruction.\nLGrad trains a ResNet-50 classifier using image gradients from a pre-trained CNN, with images generated by ProGAN and authentic images from Celeba-HQ [20]. Similarly, GramNet employs global image texture representations extracted at different levels from a ResNet-18, trained on StyleGAN-generated images and authentic Celeba-HQ images. Rine leverages image representations extracted by intermediate blocks of CLIP, with an additional trainable module. We use the checkpoint trained with Latent Diffusion Model [9] and ProGAN images. DIMD trains a ResNet-50 avoiding downsampling step, to preserve high-frequency fingerprints. We take the checkpoint trained on Latent Diffusion images. Training authentic images are taken from MSCOCO and LSUN. In DeFAKE, text and image encoders from a Visual-Language Model model are finetuned to detect synthetic images, using Latent Diffusion data. Dire uses the error between an input image and its reconstruction by a pre-trained diffusion model for identification. A ResNet50 is trained as"}, {"title": "6.1 Benchmarking", "content": "Overall, results in Table 7 indicate the lack of universal detectors, with the performance of all methods failing at random classifier level in six or more datasets. Performance inconsistency is generalized, with all models being the best and worst detectors for at least one dataset.\nIn addition, some models fail to characterize authentic images. DeFake and Dire achieve particularly low recalls on all real-world datasets tested. This compromises their utility, as it increases the number of false positive detections (misclassifying authentic images as synthetic). Meanwhile, GramNet, Rine and DIMD demonstrate consistently low false positive rates.\nFocusing on the models with a reasonable amount of false positives (LGrad to DIMD in Table 7) still leads to contractive insights.\nAt times the underlying generative model seems relevant for detection (variants of SD models are identified at higher rates, even for models not trained on diffusion) while other cases show discrepancies between datasets generated using the same model (DALLE3, SDXL). This evidence suggests a complex relationship between training data, data sources and detection efficacy.\nTime and obsolescence are another critical factor, exemplified by LGrad's diminished effectiveness on data beyond 2022. A continuous evaluation of old detectors is thus recommended, together with the development of new detectors to keep pace with advancing generation techniques, particularly considering the significant existing blind spots in recent models (e.g., recalls over 90% are scarce).\nFinally, let us examine the behaviour on the In-the-wild, as an approximation of performance in practice. None of the tested detectors reaches a recall above 50% for both the authentic and synthetic versions of it. The current best-performing models are DIMD and Rine (with a large false negative rate), and SuSy and DeFake (with a large false positive rate)."}, {"title": "6.2 Scale Generalization", "content": "In our final generalization study, rather than cropping, we resize all images to 224 \u00d7 224 pixels regardless of the original size. This approach notably impacts the frequency components of the images, as well as the content itself, particularly for high-resolution originals. The resizing process can alter or eliminate certain frequency artifacts and defects that some SID models rely on.\nResults reveal certain patterns across detection models. Three models (GramNet, Dire and SuSy) exhibit a balanced trade-off between false positives and false negatives, approximately losing in one recall what is gained in the other. For instance, Dire shows a substantial improvement for authentic images (+25.88) but an analogous decrease for synthetic ones (-23.97). This indicates that, for these models, changing the resolution changes the decision boundary, in a way which is independent from the features characterizing the final discrimination between authentic and synthetic images. Such that, when the model becomes more/less sensitive, this affects equally both the authentic and synthetic predictions. Preliminary experimentation on Dire shows this symmetry to be consistent on other resolutions (512 \u00d7 512).\nOne model collapses, suffering catastrophic losses after the resolution change. DIMD's average recall on the synthetic class drops 62%, while only gaining 1.5% on the authentic class. This could be caused by DIMD's avoidance of the downsampling step, making it more dependent on image scale. Regardless, it points towards the importance of considering model sensitivity during the design and training of detectors. This is a valuable feature that is present in two other models, LGrad and Rine. Remarkably, their performance generalizes very well to scale changes.\nLastly, DeFake stands out by showing large improvements in both recall scores (> +15%) under scale reduction. This may be caused by the Visual Language Model architecture empowering the detector, which could benefit from having the whole image context as input, despite losing frequency artifacts. We find these results to be consistent with the findings in [34]."}, {"title": "7 Conclusions", "content": "The race between synthetic image generators and detectors is far from over. New and better generative models appear regularly, and detectors cannot generalize to all (see \u00a76). Meanwhile, the increasing realism of synthetically generated content brings along a proportional demand for detectors, as tools for protecting social trust and digital rights, while preventing disinformation.\nThe biggest challenge for detectors is generalization. Changes of generative model, data source, image post-processing and training task, affect detector performance, limiting their reliability. This work studied all these factors, starting with the underlying generative model used to produce the images. By temporally grounding our experiments, we observe how older models/datasets (less realistic and producing bigger artifacts) can be easily detected when trained for them, but are harder to generalize to when not seen during training (when compared to more recent generative models, see Table 2). This indicates the stronger biases defining older synthetic images are rarely found in newer datasets. As a result, including early generative models in the train set of new detector models should become common practice. Another consistent family of generative models that is consistently harder to generalize to are private models (e.g., DALLE3, Firefly), as shown in Tables 2, 5 and 7. This speaks of the importance of open science in the field.\nData source is another key factor in detection. Using the same generative model, individuals employing various software and hardware configurations can produce significantly different content, to the point where generalization to the same model but a different source often becomes challenging (see Tables 5 and 7). As a result, detector models can never be assumed to work in data obtained from an untested source. This is further reinforced by tests conducted on data gathered In-the-wild, showing the dangerous effect of uncontrolled data sources. To mitigate this risk, recurrent sanity checks are recommended, as well as dataset-specific fine-tunes.\nA common pitfall of detector models is the lack of a proper characterization of authentic images. This has a higher prevalence on binary classifiers (Table 3) but also happens on multi-class ones (Table 5). Since characterizing authentic images is essential for practical use, specific tests using different sources of authentic data should be prioritized. Training on authentic data from many sources should also be common practice.\nThis work shows how to train detectors that maximize generalization, by focusing on many different data sources and generative models. This alone is no guarantee, and diversity, even within the same source or model, is of essence. Our limited field study indicates that, while we are far from a universal detector, models trained for specific targets may already be as good as humans at identifying synthetic content (see Table 6).\nThe race between generators and detectors will go on, particularly since data from better generative models produces detectors that generalize better (see Table 2), while better detectors are likely to be used to improve generators. This leads to a race equilibrium paradox, ensuring that the race for synthetic content detection is always going to be a close one."}, {"title": "7.1 Ethical risks", "content": "To finalize, let us consider the ethical risks associated with this work and with the materials released. The most obvious of which is the fallibility of the detectors trained. The SuSy, as any other detector model, makes both false positive and negative predictions (see Table 5), potentially labelling authentic images as synthetic and vice versa. Digital rights could be affected and censorship could be enabled through its use. For this reason, when applied in a setting affecting the rights and privileges of humans, the model should always be overseen by a human expert, and its predictions should never be taken as conclusive evidence.\nThe datasets used in this work are biased in many ways. Some of these biases are learnt by the models and could influence its decision in undesired ways (e.g., countryside images could be more likely to be tagged as synthetic than urban landscapes). The most effective mitigation strategy in this case is to conduct a study on model performance, based on the existing populations within the application domain. Once undesirable biases are identified, these can be corrected through customized fine-tuning using synthetic data."}]}