{"title": "No Culture Left Behind: ArtELingo-28, a Benchmark of WikiArt with Captions in 28 Languages", "authors": ["Youssef Mohamed", "Runjia Li", "Ibrahim Said Ahmad", "Kilichbek Haydarov", "Philip Torr", "Kenneth Ward Church", "Mohamed Elhoseiny"], "abstract": "Research in vision and language has made considerable progress thanks to benchmarks such as COCO. COCO captions focused on unambiguous facts in English; ArtEmis introduced subjective emotions and ArtELingo introduced some multilinguality (Chinese and Arabic). However we believe there should be more multilinguality. Hence, we present ArtELingo-28, a vision-language benchmark that spans 28 languages and encompasses approximately 200,000 annotations (140 annotations per image). Traditionally, vision research focused on unambiguous class labels, whereas ArtELingo-28 emphasizes diversity of opinions over languages and cultures. The challenge is to build machine learning systems that assign emotional captions to images. Baseline results will be presented for three novel conditions: Zero-Shot, Few-Shot and One-vs-All Zero-Shot. We find that cross-lingual transfer is more successful for culturally-related languages. Data and code will be made publicly available.", "sections": [{"title": "1 Introduction", "content": "A quick review of recent surveys on multimodal AI (Cao et al., 2023; Berrios et al., 2023; Zhang et al., 2023), reveals just how much the literature is focused on English. The literature on benchmarking (Liu et al., 2023c; Li et al., 2023a) provides an astoundingly similar story. With the pervasiveness of AI technology in our societies, it is essential to make the technology accessible to a wider population. Although English is widely spoken as a first language or a second language, most of the world (75% per capita) does not speak English.\nFigure 1 shows some annotations from ArtELingo-28. For 2000 images from WikiArt, we have ~140 emotion labels per image, as well as captions from annotators with diverse backgrounds covering 28 languages. Unlike captions in traditional benchmarks such as COCO (Lin et al., 2014) and Visual Genome (Krishna et al., 2017) which"}, {"title": "2 Related Work", "content": "Multimodal Benchmarks: Benchmarks have always driven the development of many breakthroughs. Imagenet (Deng et al., 2009) being a perfect example, it led to the development of AlexNet (Krizhevsky et al., 2012) which sparked the fire of the deep learning era. Recent benchmarks are moving towards multimodality. In particular, Vision and Language understanding datasets such as COCO (Lin et al., 2014), Conceptual Captions (Sharma et al., 2018), LAION (Schuhmann et al., 2022), VQA (Antol et al., 2015), Visual Commonsense Reasoning (Zellers et al., 2019), and GQA (Hudson and Manning, 2019) have pushed the frontiers of what is possible. They allowed developing models that can perform complex tasks such as visual grounding, image captioning, text-to-image generation, guided segmentation, and more. Although these datasets are framed as benchmarks to develop vision and language, they mainly cover English language."}, {"title": "3 ArtELingo-28", "content": "Table 1 compares the difference between the three datasets; ArtEmis, ArtELingo, ArtELingo-28. Our dataset ArtELingo-28 expands horizontally by adding more 25 languages. The challenges for such an expansion are unique. We detail in this section our collection effort with a team of 220 native annotators spanning 23 countries.\nWe utilize the data collection interfaces from ArtELingo (Mohamed et al., 2022a). We ask annotators to carefully examine the image before selecting the most dominant emotion out of 8 emotions.3 In addition, the annotators have the option to select \"Something else\" if their feelings do not align with any of the 8 emotions. Finally, the annotators are asked to write an explanation of why the image made them feel the selected emotion. Similar to ArtELingo we aim to collect annotations from five different annotators for each image. In total, we cover 2000 images. We make sure to have a representative sample of images covering many genres and styles. (Please see the full list of art styles and genres in appendix C.1.) To embrace the different cultural perspectives, we collect annotations for 25 languages from geographically diverse regions. We cover languages from (we include ArtELingo data for completeness):\n\u2022 Africa: Kinyarwanda, Swahili, IsiZulu, Setswana, Yoruba, Hausa, Igbo, IsiNdebele, IsiXhosa, Emakhuwa.\n\u2022 Southeast Asia: Vietnamese, Indonesian, Thai, Burmese, Malay.\n\u2022 Sub-Indian continent: Tagalog, Tamil, Hindi, Urdu.\n\u2022 East Asia: Korean, Chinese.\n\u2022 Middle-East: Turkish, Darija, Arabic.\n\u2022 Central Asia: Uzbek, Kazakh, Kyrgyz\n\u2022 Europe and North America: English.\nQuality Control. We deliver training for our annotators. In their native language, we explain the task and the criteria of good explanations; describing image details and relating those details to the selected emotions. The training includes a clear definition of positive emotion labels since not all cultures agree on the meaning of those labels. We"}, {"title": "3.1 Quantitative Analysis", "content": "Number of Annotations. Figure 2 reports the number of annotations per language, ranging from 10,493 for Urdo to 2032 for Kazakh.\nAnnotators. Figure 3 reports the number of annotators per language. A major challenge for ArtELingo-28 was obtaining access to native speakers, especially for low-resource languages. Amazon Mechanical Turk was used to collect data, though not to find annotators. We recruited annotators for Hindi, Urdu, Burmese, Thai, Malay, Vietnamese, Indonesian, Tagalog, Tamil, and Turkish through aiXplain.5 For other langauges, we used personal networks to find annotators. Although ArtELingo had more annotations, the number of annotators was also proportionally larger making data collection much simpler. For each language, ArtELingo consumed an average of 10.5K hours/language performed by ~2500 annotators, corresponding to 4.2 hours/annotator. In ArtELingo-28, we added 25 languages with an average of 250 hours/language and 8.8 annotators corresponding to 28.15 hours/annotator, seven times as much work per annotator. These calculations do not include management and coordination efforts. In total, annotations consumed 6.25K hours, plus ~2.5K hours for management hours.\nEmotion Distribution.  Figure 4 shows KL divergence in emotion labels by language pair. That is, for a pair of languages (l_1, l_2) with emotion distributions (p, q), we calculate the emotional disagreement as  D = \\sum_{k \\in emotions} p_k * log \\frac{p_k}{q_k}. We can interpret D as disagreements. The lighter the color, the more similar the language pair. We applied hierarchical clustering to sort languages by agreement in Figure 4. There are two large clusters in the plot; the larger cluster contains mostly languages from Africa and the smaller cluster contains mostly languages from Asia."}, {"title": "4 Models", "content": "We are interested in models able to perform vision and language understanding in many languages. Unfortunately, most open-source state-of-the-art models are heavily biased toward English. Hence, we adapt SOTA general vision and language models to a multilingual setting. The drastic increase in the vocabulary size make such adaptation a challenging task. BLOOMZ tokenizer has a vocabulary size of 250680 tokens compared to only 32000 for Llama2. Hence, the embedding layer of BLOOMZ is much bigger making it harder to align visual features with the language embedding space."}, {"title": "4.1 LLM-based methods", "content": "We replace the Large Language Model (LLM) with BLOOMZ and introduce a multilingual instruction-following fine-tuning task, resulting in enhanced performance compared to baseline models. This approach is applied to models such as InstructBLIP (Dai et al., 2023), ClipCap (Mokady et al., 2021), MBlip (Geigle et al., 2023), and MiniGPT-4 (Zhu et al., 2023).\nInstructions. We utilize a two-stage training process. The first stage aims to align the visual features with the language model input. In this stage, we utilize large-scale datasets, in particular LAION (Schuhmann et al., 2021) and Conceptual Captions (Sharma et al., 2018), both have English captions only. In addition, we utilize LAION-2B-multi (Schuhmann et al., 2022) which is multilingual. We follow MiniGPT-4 (Zhu et al., 2023) and use the following instructions during the training,"}, {"title": "4.2 Non LLM-based methods", "content": "Additionally, we adapt models that are not based on LLMs by replacing the language encoder with a multilingual language encoder (XLM-R(Conneau et al., 2019)). CCLM provides pre-trained models with the XLM-R backbone, however, their model does not natively support caption generation since it is an encoder-style model. We follow the standard procedure similar to UniLM (Dong et al., 2019) and X2-VLM (Zeng et al., 2022a) to generate captions via using the [mask] token autoregressively. In particular, we start with an empty sentence and append [mask] as the first token. The model then predicts a probability distribution over the vocabulary. We sample the first token from that distribution. Next, we append the [mask] token as a second token and repeat the whole process. We continue the generation of tokens until we reach the maximum sentence length or the end of sentence [eos] predicted by the model."}, {"title": "5 Experiments", "content": "This section provides baselines for the task of Multilingual Affective Image Captioning. This task takes three inputs: image, emotion, language. The model is trained to generate an affective caption in the desired language explaining why the image evokes the desired emotion. The next three subsections discuss three experimental setups to evaluate baseline models over a variety of practical situations. In all three setups, we report standard metrics; BLEU-4 (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), ROUGE (Lin, 2004) and CIDEr (Vedantam et al., 2015). We report summary scores averaged over evaluation languages; due to page limitations, detailed results by language are reported in appendix D.3.\nFinally, we provide results for emotion label prediction in section 5.4."}, {"title": "5.1 Setup 1: Zero-Shot", "content": "This setup is intended for cases where we have just a few high-resource languages with large training sets. Specifically, we consider Arabic, Chinese, and English as high-resource languages. In the Zero-Shot setting, the system is trained on ArtELingo data (Mohamed et al., 2022a) in Arabic, Chinese and English, and tested on 25 other languages in ArtELingo-28."}, {"title": "5.2 Setup 2: Few-Shot", "content": "This setup corresponds to the scenario where we have a modest amount of data (~7K) from low-resource languages in addition large amounts of data (~900K) from high-resource languages. We start from the Zero-Shot datasets and the Zero-Shot"}, {"title": "5.3 Setup 3: One-vs-All Zero-Shot", "content": "This setup aims to study the pairwise interaction between languages. We fine-tune the Zero-Shot model on one language, and evaluate on all the other languages. We hypothesize that successful cross-lingual transfer is driven by close cultural connections. For example, let model x be fine-tuned on Hindi, while model y is fine-tuned on Hausa. We evaluate both models on Urdu. If both models perform similarly, then there is no underlying cultural relationship between Hindi or Hausa with Urdu. However, if either model performs better, then we can assume an extra cultural connection between Urdu and the best performing model's language."}, {"title": "5.4 Emotion Label Prediction", "content": "This task takes a caption as an input and predicts one of the nine emotions as an output. We finetune XLM-roBERTa (Conneau et al., 2019) in multiple settings to show the advantages of finetuning with ArtELingo-28. Our first model called base is trained on 900K annotations from ArtELingo consisting of captions in Arabic, Chinese, and English languages. ArtELingo model is further fine-tuned on 200K samples from ArtELingo different from the initial training data. This model simulates collecting more data in high resource languages hoping to positively improve multilingual performance. ArtELingo-28 model load the base model and then finetunes on our ArtELingo-28 dataset. This model corresponds to collecting native multilingual data. Finally, ArtELingo-280 finetunes XLM-roBERTa only on ArtELingo-28 to measure the usefulness of training using high resource languages before finetuning. In all of our experiments, we finetune the XLM-roBERTa large for 5 epochs."}, {"title": "6 Conclusion", "content": "In summary, ArtELingo-28 addresses a critical gap in evaluating large-scale multilingual Affective Vision and Language understanding. By adding 25 languages and 200K high-quality annotations, including low-resource languages, our dataset embraces the cultural differences.\nOur evaluation setups \u2013 Zero-Shot, Few-Shot, and One vs All Zero-Shot assess affective explanation generation across diverse linguistic contexts. The One vs All Zero-Shot setup extends evaluation to languages beyond the training dataset, revealing cultural connections through cross-lingual transfer performance.\nIn this work, we introduced a dataset, proposed a benchmark, and adapted four Vision and Language models, overcoming current limitations in multilingual AI evaluation. ArtELingo-28 sets a benchmark for bridging linguistic and cultural gaps in Affective Vision and Language understanding."}, {"title": "7 Limitations", "content": "Modeling. Our approach faces limitations in modeling, evaluation, and data. Modeling relies on the quality and availability of multilingual large language models (MLLMs), stressing the need for attention and resources beyond English models. Current evaluation metrics overlook emotional and subjective aspects, necessitating new metrics. To enhance benchmarking, broader language coverage in evaluation datasets is crucial, alongside the collection of more diverse native multilingual data. Addressing these limitations is essential for advancing Multilingual General Purpose Vision Language Models' effectiveness and applicability.\nDataset. In addition to the limitations of ArtELingo (Mohamed et al., 2022a), our dataset reflects the viewpoints of the annotators. We instructed the annotators neither to attack any given group of people based on ethnicity, religion, etc. nor use vulgar language. In addition, We asked our coordinators to reject and report any captions that used vulgar language or attacked a group of people.\nThe captions might reflect ideas that might be sensitive in the western cultures. However, they reflect the world views of people from different cultures. We should expect some things and topics to be more or less sensitive depending on culture: dress, gender, religion, drinking, sex, respect for animals, respect for the environment, etc. It is not helpful to demand that all cultures conform to a specific world view.\nOur annotators represent a group of people who have internet access and are educated. Most of them speak English in addition to their native language. We leave extending our dataset to include other groups of people to future work.\nOur goal in ArtELingo-28 is to embrace diversity and capture the diverse perceptions of the world held by different cultures. While we may disagree on topics such as religion, dress, and other cultural norms, our position as authors, coming from different cultures and covering three continents, is that it is important to consider these varying perspectives to build culture-aware models, while of-course promoting respect and eliminating, as much as we can, hateful content. We should embrace the fact that what is considered appropriate or inappropriate varies across cultures and try to understand other people. We find it not our job to draw one line for all cultures but to expose this phenomenon that we find worth studying. Our dataset aims to help people understand and respect each other's world-views, even if disagreement is inevitable on some topics, which may serve as a resource for advancing cultural and cross-cultural psychology."}, {"title": "8 Acknowledgment", "content": "The authors would like to thank aiXplain for annotators in 10 languages and providing high quality annotations. In addition, we thank all our language coordinators for their amazing effort and support throughout the project. We extend our gratitude to all the annotators for their effort in the data collection.\nThis work was supported by King Abdullah University of Science and Technology (KAUST), under Award No. URF/1/5016."}, {"title": "A Dataset Collection Interface", "content": "We use the interface in Figure 7 to collect the annotations for ArtELingo-23. We hire native speakers who\nare very proficient in English to translate the interface to the respective languages."}, {"title": "B Quality Control", "content": "Our quality control includes multiple stages. Below we outline these stages and their details:\n\u2022 Hiring Stage: we have worked with our language coordinators on multiple projects before and they\nhave a proven track record of providing high quality data through their annotators. They know the\nannotators in person as a result the hired annotators are top quality.\n\u2022 Training Stage: we provide detailed training through a conference meeting for all the annotators and\ncoordinators. We then ask the coordinators to translate the list of instructions and the key points of\nthe training into their respective language to make sure that all annotators understand the instructions\ncompletely.\n\u2022 Reviewing Stage:\nWe developed automatic scripts that perform duplicate detection, sentiment analysis verification,\nand language identification. For duplicates we used exact text matching. We utilized a sentiment\nanalysis model fine tuned on ArtELingo (English, Arabic, Chinese) to classify whether the\ncaption is positive or negative. Finally, we utilized the NLLB(Costa-juss\u00e0 et al., 2022) language\nidentification fasttext model 7 to make sure the languages match our target. Any automatically\nrejected annotation was marked for the language coordinators to review in detail. Noteworthy,\nduplicate detection was triggered 20%, 32%, 47% for Thai, Urdu, and Turkish (All languages\nwere supported by AiXplain). We discussed the issue with the annotators; All duplicates were\nre-annotated and we didn't observe this issue. The sentiment analysis classifier marked very\nfew instances <1%. Finally, the language identification also marked <1% for all languages\nexcept Malay and Indonesian since the two languages are very similar to each other. The\nlanguage identification model classified 75% of Malay instances as indonesian. However,\nmanual inspection revealed no issues.\nThe language coordinators manually review the annotations. Overall, the rejection rate in each\nlanguage was between 1 5% reflecting the high quality of annotations. The most common\nmistake encountered was selecting the \u201csomething else\" emotion label while the caption reflects\none of the 8 emotions. For this mistake, we provided extra training that focused on explaining\nthe emotional labels for the annotators. The initial training explained the emotion labels but\nsome languages (Burmese, Turkish, Swahili, Hausa, Indonesian, Korean) required extra training.\nAfter that the annotators re-selected the emotional label to align better with our definition of\nemotions.\nAnother issue is that some annotators started their sentences with \u201cThis image makes me\nfeel...\" which heavily influenced the performance of our captioning models. We asked the\nannotators to fix those annotations by paraphrasing them to more natural sentences. This was\nmainly encountered in Yoruba, but was fixed early in the annotation process.\n\u2022 Translated Validation: As a final quality check after the coordinators, we (authors) translated a\nrandom sample of 500 annotations from each language to English and performed sanity checks. We\ndidn't encounter any bad quality annotations."}]}