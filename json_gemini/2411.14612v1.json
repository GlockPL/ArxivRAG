{"title": "Exploiting Boosting in Hyperdimensional Computing for Enhanced Reliability in Healthcare", "authors": ["SungHeon Jeong", "Hamza Errahmouni Barkam", "Sanggeon Yun", "Yeseong Kim", "Shaahin Angizi", "Mohsen Imani"], "abstract": "Hyperdimensional computing (HDC) enables efficient data encoding and processing in high-dimensional spaces, benefiting machine learning and data analysis. However, under-utilization of these spaces can lead to overfitting and reduced model reliability, especially in data-limited systems-a critical issue in sectors like healthcare that demand robustness and consistent performance. We introduce BoostHD, an approach that applies boosting algorithms to partition the hyperdimensional space into subspaces, creating an ensemble of weak learners. By integrating boosting with HDC, BoostHD enhances performance and reliability beyond existing HDC methods. Our analysis highlights the importance of efficient utilization of hyperdimensional spaces for improved model performance. Experiments on healthcare datasets show that BoostHD outperforms state-of-the-art methods. On the WESAD dataset, it achieved an accuracy of 98.37% \u00b10.32%, surpassing Random Forest, XGBoost, and OnlineHD. BoostHD also demonstrated superior inference efficiency and stability, maintaining high accuracy under data imbalance and noise. In person-specific evaluations, it achieved an average accuracy of 96.19%, outperforming other models. By addressing the limitations of both boosting and HDC, BoostHD expands the applicability of HDC in critical domains where reliability and precision are paramount.", "sections": [{"title": "I. INTRODUCTION", "content": "Hyperdimensional computing (HDC) is a rapidly advancing field within artificial intelligence, offering numerous valuable qualities that apply to various areas. HDC's ability to efficiently encode and process data in high-dimensional spaces has generated substantial interest, particularly in machine learning and data analysis [1]-[3]. As we contemplate the practical use of HDC-based models in real-world applications, we encounter crucial requirements beyond mere accuracy. These requirements encompass robustness, reliability, consistency, and resource efficiency [4]. Despite these requirements, analyses of HDC in high-dimensional spaces has not yet been fully explored which can lead to overfitting by selecting very high dimension above only performance [5]. Our research shows that the partitioning of high-dimensional spaces directly correlates with the utility of the space, fulfilling those requirements under certain condition.\nMeeting the requirements of HDC is anticipated to significantly expand its applicability across various domains, particularly in fields where reliability and precision are paramount, such as healthcare [6], [7]. In addressing healthcare datasets, our evaluation focuses on critical issues such as robustness to noise, consistency in performance, and stability that are non-negotiable for the successful application of HDC in healthcare.\nEnsemble methods have been recognized for their ability to prevent overfitting and achieve enhanced performance and stability in predictive modeling [8]. Among these methods, boosting stands out for its capability to aggregate the predictions of multiple weak learners into a robust and accurate ensemble model. The strengths of boosting encompass significant accuracy improvements, resistance to overfitting, versatility across a wide range of machine learning tasks. However, boosting's ability across sensitivity to noisy data, performance and stability is dependent on the ability of weak learner. The computational demands of training boosting ensembles can also present challenges in scenarios requiring real-time responses or operating under resource constraints. Furthermore, the overall performance of the ensemble model may be compromised if the performance of the weak learners is not assured, potentially leading to biases towards challenging examples in imbalanced datasets.\nHDC offers promising attributes to effectively address the limitations associated with traditional boosting techniques, as highlighted in prior studies [9], [10]. However, a simplistic parallel ensemble of HDC models may inadvertently escalate the computational costs associated with training and may not guarantee robustness against noise for each weak learner. In our approach, we leverage the OnlineHD model [4] as a foundation, proposing a novel partitioning strategy where the model's hyperdimensional space (D) is divided among n weak learners, with each receiving a D/n dimensional segment. This segmentation approach prompts us to designate these segmented models as weak learners. Subsequently, we analyze the utilization efficiency of the hyperdimensional space by each weak learner Figure 5 and theoretically demonstrate the inherent limitations posed by high-dimensional spaces Equations 3, 2. Under conditions where the performance of weak learners is assured, BoostHD elevates the capabilities of OnlineHD, ensuring stability, and providing robustness against overfitting and noise."}, {"title": "II. RELATED WORK", "content": "To address the limitations of individual predictive models, researchers have explored various ensemble methods that seek"}, {"title": "A. Boosting methods on Machine Learning", "content": "To address the limitations of individual predictive models, researchers have explored various ensemble methods that seek to enhance performance by combining multiple models. One such successful ensemble technique is boosting, which sequentially integrates multiple simple models called weak learners and aggregates their predictions to make final decisions. These weak learners are trained to compensate for each other's prediction errors and can take various forms, including shallow decision trees or a few layers of multi-layer perceptions. One of the pioneering works in boosting is Adaptive Boosting (AdaBoost) [8]. AdaBoost begins by assigning equal weight values to data points within a training set. It then sequentially trains each weak learner, focusing on identifying misclassified data points and increasing their weights to ensure subsequent weak learners prioritize their correct classification. Each weak learner is assigned a confidence value based on its prediction performance, and the final prediction is determined through a confidence-weighted voting mechanism among the weak learners. Another notable approach is the Gradient Boosting Machine (GBM) [11], which differs from AdaBoost by seeking to minimize the residual error rather than assigning weights to data points. GBM also follows a sequential training process, with each weak learner striving to reduce the residual error left by the previous model through gradient descent. Subsequent boosting algorithms, such as Light GBM [12] and XGBoost [13], adopted this residual error minimization approach due to its demonstrated effectiveness in comparison to the data-point weighting strategy initially employed by AdaBoost."}, {"title": "B. Healthcare with wearable device", "content": "The WESAD dataset [14] stands at the forefront of wearable stress and affect detection, providing a comprehensive collection of physiological and motion data from both wrist- and chest-worn devices in a controlled lab environment with 15 subjects. Distinguishing itself through diverse sensor modalities, WESAD captures data such as blood volume pulse, electrocardiogram, electrodermal activity, electromyogram, respiration, body temperature, and three-axis acceleration. Notably, the dataset introduces a unique dimension by including three affective states-neutral, stress, and amusement, enriching its applicability in studying stress and emotions. The prevalent state-of-the-art solution involves varied convolutional neural network (CNN) architectures [15]. Amidst this landscape, Hyperdimensional Computing (HDC) emerges as a compelling alternative to CNN models. The inherent resource constraints of wearable devices, exemplified by WESAD, align seamlessly with HDC's capabilities. Offering lightweight and online learning, HDC proves to be a natural fit for the intricacies of the WESAD dataset. Unlike conventional CNNs, HDC excels in managing multimodal data, addressing the varied sensor modalities present in WESAD. The fusion of WESAD's rich sensor modalities and affective states with the computational robustness of HDC holds great promise for real-world ap-plications in wearable stress and affect detection, presenting WESAD as an ideal testing ground for harnessing the power of hyperdimensional computing."}, {"title": "C. Hyperdimensional Classification", "content": "HDC is a computational technique inspired by the neural processes in the brain, where data points are encoded into a high-dimensional space. The learning process in HDC involves extracting universal patterns that define each label in a dataset through matrix multiplication using Gaussian distribution values and trigonometric activation functions(Binding, Permutation), such as cosine and sine. The encoded hypervectors H are linearly combined to generate representations for each"}, {"title": "III. BOOSTING HYPERDIMENSIONAL COMPUTING", "content": "In this section, we introduce the BoostHD framework, integrating boosting techniques into HDC with a focus on dimensionality (D). Instead of relying solely on a single robust learner with high D, this approach breaks down the dimension into numerous sub-dimensions, each represented by a weak learner. These weak learners are trained sequentially, with each one adaptively learning from and correcting the errors of its predecessor. This methodology effectively addresses the limitations of the strong learner, thereby enhancing its performance ceiling (as outlined in Algorithm 1). A notable feature of this method is its sequential training process, while parallelization becomes feasible during the inference phase.\nPerformance, assessed based on the parameters D and the number of weak learners (NL), shows a direct relationship, ensuring stable and improved performance with substantial values for both D and NL, as depicted in Figure 3. However, it's crucial to note that elevated values of D and NL introduce increased computational costs, establishing an inherent trade-off between computational cost and performance. To maintain the effectiveness of weak learners, preserving a baseline dimensionality is imperative.Failure to do so, as exemplified in the case where , can lead to a substantial degradation in performance, as demonstrated in Figure 3(b)."}, {"title": "IV. EVALUATION", "content": "In our experimental setup, we employed a single GPU, the GeForce RTX 4070, and a 12th generation Intel(R) Core(TM) i7-12700K CPU. We conducted a total of 10 runs for each experiment, utilizing datasets that included WESAD [14], the Nurse Stress Dataset [17], and the Stress-Predict Dataset [18] to evaluate accuracy and time efficiency. For all other aspects of the experiments, we exclusively employed the WESAD [14] dataset. The WESAD [14] dataset holds a prominent position as a benchmark dataset tailored for stress detection research. It encompasses multimodal sensor data collected from wearable devices such as Empatica E4 and RespiBAN, sourced from 16 subjects. This dataset encompasses a rich array of physiological and motion data indicators, encompassing EDA, ECG, EMG, and BVP readings. The Stress-Predict Dataset [18] and Nurse Stress Dataset [17] datasets, on the other hand, are solely collected via the E4 sensor and share features like EDA, ECG, EMG, and BVP. Notably, the Nurse Stress Dataset [17] and Stress-Predict Dataset [18] datasets include 37 and 15 subjects, respectively. We performed stress level classification, reducing it to three labels (good, common, stress), and the test data was organized based on subject units, with all results grounded in the test dataset.\nDatasets undergo preprocessing using a moving average filter with a window size of 30, extracting statistical measures like minimum, maximum, mean, and standard deviation. Due to the varied ranges of these statistics, normalization is applied to ensure consistent scaling. Our experimental models include AdaBoost with learning rate (lr) set to 1.0 and 10 estimators, Random Forest with bootstrap enabled and 10 estimators, XGBoost with 10 estimators, SVM with a linear kernel, DNN consists of convolutional layers with a learning rate of 0.001, 4 linear layers [2048,1024,512,classes], ReLU activation, and dropout. Additionally, OnlineHD is configured with dimension adjustment, bootstrap enabled, lr of 0.035, and a gaussian probability distribution (0, 1). These setups ensure a comprehensive evaluation of model performance across various algorithms in the experimental framework. In the ensemble models, we employed NL = 10. The HDC model was configured with Dtotal values spanning the range 10, 20, ..., 1000, 4000, 10000. As for the BoostHD model, the weak learner (wl) dimensionality (Dwl) was set as Dtotal/LN."}, {"title": "A. Performance", "content": "In our comprehensive experimentation, we subjected the BoostHD algorithm to rigorous evaluation across three distinct datasets: WESAD [14], Nurse Stress Dataset [17], and Stress-Predict Dataset [18]. The focal points of our assessment encompassed key metrics such as accuracy, inference time, and training time. Our findings unveiled the remarkable prowess of BoostHD, consistently attaining the highest accuracy levels across all three datasets, as presented in Table I. Notably, when compared to OnlineHD on the WESAD [14] dataset, the accuracy achieved by BoostHD remained outside the range of two standard deviations. This distinction was further underscored by OnlineHD's inability to reach a 98% accuracy threshold in any of the ten independent trials, while BoostHD consistently surpassed this benchmark, highlighting its superior predictive accuracy.\nBoostHD strategically harnesses in-memory learning, epitomized by HDC, and augments it with adaptive learning mechanisms within an ensemble framework. This unique fusion results in a significant acceleration of the learning process, even when training is serialized. Empirical assessments conducted within a GPU environment unequivocally established BoostHD's superiority over DNNs, demonstrating substantially enhanced processing speed across the three benchmark datasets. Moreover, in a CPU environment, BoostHD showcased remarkable computational efficiency, achieving processing speeds 26 times faster than its DNN counterparts. Furthermore, our optimization efforts fine-tuned the BoostHD algorithm for parallel processing, yielding substantial gains in inference efficiency (as detailed in Table II). This optimization proved particularly beneficial when handling the relatively extensive input vectors in the Nurse Stress Dataset [17] and Stress-Predict Dataset [18]. The outcome was a significant reduction in the time required for inferring data from the test dataset, firmly establishing BoostHD as the model of choice for maximizing time efficiency across all models evaluated, especially in the context of the Nurse Stress Dataset [17] and Stress-Predict Dataset [18]."}, {"title": "B. Stability", "content": "BoostHD demonstrates strong and consistent performance and exhibits stable convergence even in constrained environments. As illustrated in Figure 3, the accuracy of the weak learner experiences a steep decline when the minimum dimensionality requirement is unmet. By ensuring this minimum dimensionality and progressively increasing the value of D, we observe a clear separation between the two models within a given one standard deviation (\u03c3), as shown in Figure 6(a)."}, {"title": "C. Overfitting", "content": "BoostHD presents a noteworthy advantage over traditional HDC methods in its resilience to overfitting. To empirically assess this characteristic, we intentionally induce overfitting by crafting an imbalanced dataset (D) through the generation of data subsets. The extent of overfitting, denoted as r, is quantified by creating subset data for all classes except the target class (Ctarget). This process yields the final dataset D as described in Equation 8. We employ Macro accuracy as a more suitable metric for imbalanced datasets to ensure a fair performance evaluation that the varying sample counts per class do not skew. As the value of r increases, OnlineHD experiences a noticeable decline in performance. In stark contrast, BoostHD consistently maintains its performance levels, demonstrating a robust ability to preserve stability, as depicted in Figure 7."}, {"title": "D. Robustness", "content": "We assess the robustness of models against bitflip noise to explore in wearable device originating from hardware components according to its probability of bit flip error pb. In Figure 8, our experiments spanned two distinct ranges of pb, given the substantial impact of this parameter on performance. Our experimental scope was deliberately limited to a narrow range of pt. Despite this inherent constraint and our meticulous approach of conducting 100 independent trials to ensure statistical validity, we observed occasional peaks in the graph. Nevertheless, a discernible trend as a function of pb persisted. Within the range where pb = 10-5, as illustrated in Figure 8(a), BoostHD incurred an accuracy loss of no more than 5.7%. This represents approximately one-fourth of the loss observed in OnlineHD and roughly one-seventh of that observed in DNN. To statistically validate the accuracies concerning varying pb, we employed the Median Absolute Deviation (MAD) as a measure of robustness (defined as MAD = median(|Xi-median(X)|)). In Figure 8(a), the MAD for BoostHD was quantified at 0.024, which is six times lower than that of OnlineHD (0.1454) and four times lower than that of DNN (0.083). In Figure 8(b), BoostHD exhibited a MAD of 0.005, which is three times lower than OnlineHD (0.015) and a substantial 30 times lower than DNN (0.1509). These results are compelling evidence of BoostHD's superior robustness compared to the other methods."}, {"title": "E. Bias and reliability for person-specific groups", "content": "We carefully segmented subjects based on various attributes in WESAD [14], including hand preference, gender, age, and height. This stratification resulted in subsets with specific subject characteristics, such as left-hand preference, female gender, age groups, and height categories. Our primary objective was to assess how well models performed across individuals with diverse characteristics, a crucial consideration for healthcare applications to ensure fairness and accuracy. Notably, our BoostHD model consistently outperformed other models in all categories III, except for two cases where it ranked second. This underscores the potential of combining boosting methods and HDC, especially in healthcare, where equitable performance is essential."}, {"title": "V. CONCLUSION", "content": "We introduced BoostHD, a novel method that integrates hyperdimensional computing with boosting to create a robust ensemble model. BoostHD effectively overcomes limitations of traditional HDC by efficiently utilizing high-dimensional spaces and preventing overfitting. Our experiments on healthcare datasets demonstrated that BoostHD consistently outperforms existing methods in accuracy, efficiency, stability, and robustness. These results confirm BoostHD's potential in critical domains where reliability and precision are essential."}]}