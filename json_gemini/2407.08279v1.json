{"title": "CONTINUALLY LEARN TO MAP VISUAL CONCEPTS TO LARGE\nLANGUAGE MODELS IN RESOURCE-CONSTRAINED\nENVIRONMENTS", "authors": ["Clea Rebillard", "Julio Hurtado", "Andrii Krutsylo", "Lucia Passaro", "Vincenzo Lomonaco"], "abstract": "Learning continually from a stream of non-i.i.d. data is an open challenge in deep learning, even\nmore so when working in resource-constrained environments such as embedded devices. Visual\nmodels that are continually updated through supervised learning are often prone to overfitting,\ncatastrophic forgetting, and biased representations. On the other hand, large language models\ncontain knowledge about multiple concepts and their relations, which can foster a more robust,\ninformed and coherent learning process. This work proposes Continual Visual Mapping (CVM),\nan approach that continually ground vision representations to a knowledge space extracted from a\nfixed Language model. Specifically, CVM continually trains a small and efficient visual model to\nmap its representations into a conceptual space established by a fixed Large Language Model. Due\nto their smaller nature, CVM can be used when directly adapting large visual pre-trained models\nis unfeasible due to computational or data constraints. CVM overcome state-of-the-art continual\nlearning methods on five benchmarks and offers a promising avenue for addressing generalization\ncapabilities in continual learning, even in computationally constrained devices.", "sections": [{"title": "1 Introduction", "content": "Increasing the amount of training data has repeatedly shown significant positive impacts on the performance of deep\nlearning models [1, 2]. Generally speaking, as the quality and diversity of the training set expands, the performance\nof predictive models increases. This trend is particularly pronounced with Transformer architectures [3], which has\nuncovered new models capable of achieving state-of-the-art performance in multiple tasks, such as textual [2], visual\n[4] and multiple modalities [5], provided that the necessary amount of data is available.\nThe Transformer architecture has shown remarkable performance, fostering the creation of a vast collection of large\npre-trained models. These models can be used for multiple tasks, especially for individuals or entities lacking the\nnecessary resources to train such large models. Since a considerable amount of data and computational power is\nrequired to fine-tune these models, much effort has been focused on enhancing their knowledge transferability [6, 7]."}, {"title": "2 Related Work", "content": "Continual Learning Learning continually in deep learning [15, 16, 17] mainly aimed at addressing Catastrophic\nForgetting by enabling predictive models to learn new tasks without forgetting previously learned ones. Most CL\nmethods can be categorized by different, non-exclusive computational approaches including: subdividing model\nparameters into subspaces for each new task [18, 19]; imposing constraints on the learned gradients [20, 21]; and using\nmeta-learning to learn reusable weights for all tasks [22, 23]. Out of these categories, memory-based methods such as\nExperience Replay (ER) [24, 25] provide a straightforward solution that achieves good results. Memory-based methods\naddress catastrophic forgetting by incorporating data from previous tasks into the training process for the current task\n[26, 27], and have been the state-of-the-art on most benchmarks since their ability to mitigate forgetting due to the\nconstant repetition of previous tasks.\nWhile using pre-trained models is not new in CL, the use of large pre-trained models has naturally shown better\nperformance in several downstream tasks [9, 28, 8, 10], since they can take advantage of the pre-trained representations\nthat can significantly help the mitigation of catastrophic forgetting. These models are trained on vast amounts of data,\nallowing them to obtain valuable representations for different modalities and inputs [4, 29], making them very useful as\na starting point for CL solutions. However, their massive amount of parameters and the need to access a considerable\namount of data make them impractical to train (continually) as new distribution arrives [30].\nLearning Multi-Modal Representations Training predictive models able to connect different data modalities (vision,\ntext, speech, others) have been shown to provide richer representations and more reusable representations in deep\nlearning [2]. One of the most popular multi-modal models is CLIP [5]. Here, the authors train visual and language\nmodels to generate a latent space where visual and textual representations of the same input are close to each other.\nThis fundamental insight has been further refined and expanded [31, 32, 33] achieving state-of-the-art performance in\nmultiple tasks [34].\nPioneering works [35, 36, 37] have explored training a visual model while using a static language representation (i.e.,\nbefore the advent of contextualized embeddings and large language models). However, unlike previous works, we take\nadvantage of extracting contextual information. Specifically, we build the knowledge space upon pre-trained LLM,\nexpecting our visual model to benefit from the semantic knowledge contained within the LLM. Additionally, we explore\nhow this approach can help reduce forgetting in CL scenarios by including a retention loss that mitigates the forgetting\nof the semantic knowledge of the previously learned tasks in the optimisation function. Such a solution provides a\nresource-efficient alternative to using large pre-trained models."}, {"title": "3 Strategy", "content": "In supervised CL, we consider a stream of T tasks. Each task t consists of a new data distribution Dt = (Xt, Yt),\nwhere Xt denotes the input instances and Yt denotes the instance labels. The goal is to train a classification model\nf\u0473 : X \u2192 Y using data from a sequence of T tasks: D = {D\u00b9, ..., DT }. During each task, model fe will minimize\nthe objective function L using data Dt.\n$L(D\u207a) = \\frac{1}{N_t} \\sum_{i=1}^{N_t} L_t(f_\\theta(x_i^t), y_i^t)$"}, {"title": "3.2 Continual Visual Mapping (CVM)", "content": "In a classic formulation of a classification problem, a model fe is composed of a feature extractor fe and a classifier cw,\nbeen 0 and w the trainable weights of the model fe. During training, the learning strategy simultaneously tries to learn\nrobust, semantic representations of the input (fo) and a classifier (c) that uses these features to discriminate between\nthe classes correctly, ideally generalising to samples not seen during training.\nIn CL, the constant modification of the weights 0 and w results in ongoing interference between tasks. This interference\ncan induce forgetting, which causes a decrease in the overall performance. We hypothesise that this interference is\nprimarily due to the biases and boundaries the model learns during a task, knowledge that becomes unsuitable for\nfuture distributions. These biases are reinforced by the limited information the model receives through the input, which\nforces it to construct mappings based on limited visual information. Collectively, these factors strongly restrict the\ntransferability of the learned representations to future tasks due to bias, which makes them prone to overfitting and less\nreusability.\nOur proposed method replaces the classifier cw with a set of anchor vectors representing the classes or concepts. These\nvectors are extracted from a pre-trained LLM, creating a fixed and knowledgeable latent space. The vectors establish a\nfixed latent space that the visual model can use as external knowledge. With this modification, we expect to (1) leverage\nthe knowledgeable space created by the anchor vector to guide the training of the visual model and (2) increase the\ntransferability of the visual representations.\nUsing a pre-trained LLM, we extract the current task Dt representations, creating a latent space imbued with a broad\nunderstanding of the world [40]. Given the absence of textual descriptions for images in most visual datasets, we\ngenerate a vector Ce for each class c, using the prompt \"This is an image of \" + label, where label represents the"}, {"title": "4 Experiments", "content": "To showcase the effectiveness of CVM, we have empirically assessed it in various scenarios and benchmarks traditionally\nused in CL and following standard practices. We tested our method in a Class-IL scenario using CIFAR100 [42] and\nTiny-ImageNet [43] datasets. We split these datasets into 10 experiences. We also run experiments in a Domain-IL\nscenario and higher-dimensional input spaces using the more realistic CORe50 dataset [44], designed explicitly for\nembedded continual object recognition applications. Here, the setting is composed of 10 domestic objects, and each\nnew experience imposes a new class distribution (e.g., background, illumination, occlusion, scale, among others)."}, {"title": "4.1 Implementation Details", "content": "We use a reduced version of a ResNet architecture proposed in [45] for the visual model to simulate a resource-\nconstrained environment. The only difference between the model used in CVM and other baselines is that in CVM,\nwe remove the classifier. This modification causes CVM to train fewer parameters. For the text model, we use\nSentenceBERT [46] to extract the textual representation for the classes. We ran all the experiments with three different\nseeds, each inducing a different ordering of the sequences of tasks.\nWe compare our proposal to different types of methods. First, we compare our method against some classic\nregularization-based CL methods like AGEM [47], EWC [20], and LwF [48]. We also compare against memory-based\nmethods like ER [39], DER++ [49] and iCarl [45]. iCarl is especially interesting to our case since it also removes the\nclassifier at inference time. However, the methods differ mainly in training, as iCarl uses a cross-entropy loss with\nlabels instead of the distance to knowledge anchor vectors. The way iCarl generates the vector per class is also different\nsince the vectors are generated based on samples in the memory.\nIn all experiments, we use an SGD optimizer with a learning rate of 0.1 and a batch size of 32. CIFAR100 and\nTinyImagenet train for 50 epochs per experience; for CORe50, we train for 30 epochs. We use the implementation\nprovided by Avalanche [50]. We used the hyper-parameters proposed by the authors in the corresponding paper. The\ncode of CVM will become available upon acceptance.\nWe evaluate the average accuracy (Acc) and forgetting (For) over the T tasks after the sequential learning, proposed in\n[21]. Equations 7 and 8 show the formula for the accuracy and forgetting, respectively, where Acci,j is the accuracy of\ntask i after training task j.\n$Acc = \\frac{1}{T} \\sum_{i=1}^{T} Acc_{T, i}$\n$FFor = \\frac{1}{T-1} \\sum_{i=1}^{T-1} Acc_{T, i} - Acc_{i,i}$"}, {"title": "4.2 Results", "content": "The results for Class-IL benchmarks are provided in Table 1. As previous studies have shown, regularization methods\nare suboptimal in this scenario. However, performance gains can be achieved by leveraging access to previous tasks\nstored in a buffer, exemplified by ER and DER++. Additionally, combining EWC with classical ER only seems to\nincrease performance in Tiny-ImageNet."}, {"title": "4.3 Ablation", "content": "In CL, preserving the plasticity of the model is fundamental to learning new tasks and distributions. However, the\ntrade-off between learning new classes and forgetting the previous one is the primary concern that most CL methods\nface.\nIn memory-based methods, increasing the memory size improves the representativeness of the buffer, which should\nincrease performance by decreasing forgetting. As shown in Table 2, DER++ is the method that scales the best as we\nincrease the buffer size, significantly increasing its performance; however, it still achieves worse results than CVM,\nwhich consistently outperforms previous methods. On the other hand, iCarl scales poorly.\nSomething that explains the low performance of iCarl is plasticity. As shown in Table 1, iCarl obtained a lower\nforgetting value. These results can be explained by the significant restrictions that iCarl imposes when training new\ntasks. These constraints become more evident as we increase the memory size. iCarl can maintain a lower level of\nforgetfulness but at the cost of having low plasticity and being unable to scale up when we increase memory size. In\ncontrast, CVM has no problem scaling up and maintaining a high level of plasticity.\nWe present two compelling arguments affirming the effectiveness of CVM and the importance of the proposed learning\nstrategy for CL. Firstly, our approach involves mapping visual representations to a stable and well-informed space,\nremoving the need for a classifier. Previous studies highlight the classifier as a critical contributor to model overfitting,\nparticularly in challenging CL environments [13]. This finding gains further support from the insights in Table 2,\nwhere enhancing memory representativeness enhances overall performance and mitigates the risk of overfitting to"}, {"title": "5 Generalization Capabilities", "content": "For a continual learner to be efficient, it should reuse previous knowledge when learning new tasks. It is hypothesised\nthat reusing previous knowledge could mitigate forgetting since the model would have no incentive to modify that\nknowledge. On this note, an essential feature in CL models is the generalization ability due to the transfer capability to\nfuture tasks.\nThere are two distinct approaches to assessing the transfer capability of a model. The first approach involves freezing\nthe model and training a linear classifier (linear probing) on unseen classes. This evaluation method measures the\ngenerality of the representations acquired by the model for future tasks. The second approach focuses on evaluating the\nzero-shot capabilities of the model. In the case of CVM, this involves incorporating an anchor vector of unseen classes\nand verifying the accuracy exclusively for those classes while keeping the visual model frozen. This method provides\ninsights into the model's ability to classify classes that are not part of the training set."}, {"title": "6 Limitations of Pre-trained Models", "content": "Even in scenarios where we have ample computational power, other issues can make methods based on large visual\npre-trained models unsuitable, e.g., the need for more data, the specificity or even the granularity of the problem.\nWe introduce two Continual Learning (CL) benchmarks utilizing the GTSRB [51] and Aircraft [52] datasets. For the\nGTSRB dataset, we delineate 10 tasks, while the Aircraft dataset is partitioned into 5 tasks, both adhering to a Class-IL\nscenario.\nThe primary challenge confronted by methods relying on pre-trained models arises from the granularity of the dataset.\nThis fine-grained nature necessitates additional information not present in the weights of large pre-trained models.\nConsequently, pre-trained models require assistance to learn the sequence, leading to low performance. In Table 5,\nwe present the performance of L2P [9] both with and without memory. Notably, these approaches fail to outperform\nmethods employing simpler models. Although pre-trained models may perform similarly to simpler and smaller models,\ntheir longer inference times can adversely impact overall performance."}, {"title": "7 Conclusions", "content": "We presented Continual Visual Mapping (CVM), which uses a conceptual latent space created by a frozen Large\nLanguage Model (LLM) to train a small visual model. By learning to map the visual representations to a knowledgeable\nspace, the visual model can learn new concepts based on the knowledge extracted through pre-learned textual informa-\ntion. We demonstrate the performance of CVM in class-incremental and domain-incremental learning using various\nbenchmarks, achieving state-of-the-art results within resource-constrained environments. We also reveal scenarios\nwhere simple and small models can be more efficient (and surprisingly, even more effective) than methods based on\nlarge pre-trained models. In particular, these are scenarios where the low transferability of these methods affects the\nperformance. Our learning strategy opens new research directions for CL methods to asynchronously take advantage of\nthe knowledge of large pre-trained models efficiently and effectively mapping new knowledge into existing ones."}]}