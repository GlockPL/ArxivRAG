{"title": "CONTINUALLY LEARN TO MAP VISUAL CONCEPTS TO LARGE LANGUAGE MODELS IN RESOURCE-CONSTRAINED ENVIRONMENTS", "authors": ["Clea Rebillard", "Julio Hurtado", "Andrii Krutsylo", "Lucia Passaro", "Vincenzo Lomonaco"], "abstract": "Learning continually from a stream of non-i.i.d. data is an open challenge in deep learning, even more so when working in resource-constrained environments such as embedded devices. Visual models that are continually updated through supervised learning are often prone to overfitting, catastrophic forgetting, and biased representations. On the other hand, large language models contain knowledge about multiple concepts and their relations, which can foster a more robust, informed and coherent learning process. This work proposes Continual Visual Mapping (CVM), an approach that continually ground vision representations to a knowledge space extracted from a fixed Language model. Specifically, CVM continually trains a small and efficient visual model to map its representations into a conceptual space established by a fixed Large Language Model. Due to their smaller nature, CVM can be used when directly adapting large visual pre-trained models is unfeasible due to computational or data constraints. CVM overcome state-of-the-art continual learning methods on five benchmarks and offers a promising avenue for addressing generalization capabilities in continual learning, even in computationally constrained devices.", "sections": [{"title": "1 Introduction", "content": "Increasing the amount of training data has repeatedly shown significant positive impacts on the performance of deep learning models [1, 2]. Generally speaking, as the quality and diversity of the training set expands, the performance of predictive models increases. This trend is particularly pronounced with Transformer architectures [3], which has uncovered new models capable of achieving state-of-the-art performance in multiple tasks, such as textual [2], visual [4] and multiple modalities [5], provided that the necessary amount of data is available.\nThe Transformer architecture has shown remarkable performance, fostering the creation of a vast collection of large pre-trained models. These models can be used for multiple tasks, especially for individuals or entities lacking the necessary resources to train such large models. Since a considerable amount of data and computational power is required to fine-tune these models, much effort has been focused on enhancing their knowledge transferability [6, 7]."}, {"title": "2 Related Work", "content": "Continual Learning Learning continually in deep learning [15, 16, 17] mainly aimed at addressing Catastrophic Forgetting by enabling predictive models to learn new tasks without forgetting previously learned ones. Most CL methods can be categorized by different, non-exclusive computational approaches including: subdividing model parameters into subspaces for each new task [18, 19]; imposing constraints on the learned gradients [20, 21]; and using meta-learning to learn reusable weights for all tasks [22, 23]. Out of these categories, memory-based methods such as Experience Replay (ER) [24, 25] provide a straightforward solution that achieves good results. Memory-based methods address catastrophic forgetting by incorporating data from previous tasks into the training process for the current task [26, 27], and have been the state-of-the-art on most benchmarks since their ability to mitigate forgetting due to the constant repetition of previous tasks.\nWhile using pre-trained models is not new in CL, the use of large pre-trained models has naturally shown better performance in several downstream tasks [9, 28, 8, 10], since they can take advantage of the pre-trained representations that can significantly help the mitigation of catastrophic forgetting. These models are trained on vast amounts of data, allowing them to obtain valuable representations for different modalities and inputs [4, 29], making them very useful as a starting point for CL solutions. However, their massive amount of parameters and the need to access a considerable amount of data make them impractical to train (continually) as new distribution arrives [30].\nLearning Multi-Modal Representations Training predictive models able to connect different data modalities (vision, text, speech, others) have been shown to provide richer representations and more reusable representations in deep learning [2]. One of the most popular multi-modal models is CLIP [5]. Here, the authors train visual and language models to generate a latent space where visual and textual representations of the same input are close to each other. This fundamental insight has been further refined and expanded [31, 32, 33] achieving state-of-the-art performance in multiple tasks [34].\nPioneering works [35, 36, 37] have explored training a visual model while using a static language representation (i.e., before the advent of contextualized embeddings and large language models). However, unlike previous works, we take advantage of extracting contextual information. Specifically, we build the knowledge space upon pre-trained LLM, expecting our visual model to benefit from the semantic knowledge contained within the LLM. Additionally, we explore how this approach can help reduce forgetting in CL scenarios by including a retention loss that mitigates the forgetting of the semantic knowledge of the previously learned tasks in the optimisation function. Such a solution provides a resource-efficient alternative to using large pre-trained models."}, {"title": "3 Strategy", "content": "In supervised CL, we consider a stream of T tasks. Each task t consists of a new data distribution Dt = (Xt, Yt), where Xt denotes the input instances and Yt denotes the instance labels. The goal is to train a classification model f\u0473 : X \u2192 Y using data from a sequence of T tasks: D = {D\u00b9, ..., DT }. During each task, model fe will minimize the objective function L using data Dt.\n$L(D\u207a) = \\frac{1}{N_t} \\sum_{t=1}^{N_t} Lt(f_\\theta(x_t^i), y_t^i)$"}, {"title": "3.2 Continual Visual Mapping (CVM)", "content": "In a classic formulation of a classification problem, a model fe is composed of a feature extractor fe and a classifier cw, been 0 and w the trainable weights of the model fe. During training, the learning strategy simultaneously tries to learn robust, semantic representations of the input (fo) and a classifier (c) that uses these features to discriminate between the classes correctly, ideally generalising to samples not seen during training.\nIn CL, the constant modification of the weights 0 and w results in ongoing interference between tasks. This interference can induce forgetting, which causes a decrease in the overall performance. We hypothesise that this interference is primarily due to the biases and boundaries the model learns during a task, knowledge that becomes unsuitable for future distributions. These biases are reinforced by the limited information the model receives through the input, which forces it to construct mappings based on limited visual information. Collectively, these factors strongly restrict the transferability of the learned representations to future tasks due to bias, which makes them prone to overfitting and less reusability.\nOur proposed method replaces the classifier cw with a set of anchor vectors representing the classes or concepts. These vectors are extracted from a pre-trained LLM, creating a fixed and knowledgeable latent space. The vectors establish a fixed latent space that the visual model can use as external knowledge. With this modification, we expect to (1) leverage the knowledgeable space created by the anchor vector to guide the training of the visual model and (2) increase the transferability of the visual representations.\nUsing a pre-trained LLM, we extract the current task Dt representations, creating a latent space imbued with a broad understanding of the world [40]. Given the absence of textual descriptions for images in most visual datasets, we generate a vector Ce for each class c, using the prompt \"This is an image of \" + label, where label represents the"}, {"title": "4 Experiments", "content": "To showcase the effectiveness of CVM, we have empirically assessed it in various scenarios and benchmarks traditionally used in CL and following standard practices. We tested our method in a Class-IL scenario using CIFAR100 [42] and Tiny-ImageNet [43] datasets. We split these datasets into 10 experiences. We also run experiments in a Domain-IL scenario and higher-dimensional input spaces using the more realistic CORe50 dataset [44], designed explicitly for embedded continual object recognition applications. Here, the setting is composed of 10 domestic objects, and each new experience imposes a new class distribution (e.g., background, illumination, occlusion, scale, among others)."}, {"title": "4.1 Implementation Details", "content": "We use a reduced version of a ResNet architecture proposed in [45] for the visual model to simulate a resource-constrained environment. The only difference between the model used in CVM and other baselines is that in CVM, we remove the classifier. This modification causes CVM to train fewer parameters. For the text model, we use SentenceBERT [46] to extract the textual representation for the classes. We ran all the experiments with three different seeds, each inducing a different ordering of the sequences of tasks.\nWe compare our proposal to different types of methods. First, we compare our method against some classic regularization-based CL methods like AGEM [47], EWC [20], and LwF [48]. We also compare against memory-based methods like ER [39], DER++ [49] and iCarl [45]. iCarl is especially interesting to our case since it also removes the classifier at inference time. However, the methods differ mainly in training, as iCarl uses a cross-entropy loss with labels instead of the distance to knowledge anchor vectors. The way iCarl generates the vector per class is also different since the vectors are generated based on samples in the memory.\nIn all experiments, we use an SGD optimizer with a learning rate of 0.1 and a batch size of 32. CIFAR100 and TinyImagenet train for 50 epochs per experience; for CORe50, we train for 30 epochs. We use the implementation provided by Avalanche [50]. We used the hyper-parameters proposed by the authors in the corresponding paper. The code of CVM will become available upon acceptance.\nWe evaluate the average accuracy (Acc) and forgetting (For) over the T tasks after the sequential learning, proposed in [21]. Equations 7 and 8 show the formula for the accuracy and forgetting, respectively, where Acci,j is the accuracy of task i after training task j.\n$Acc = \\frac{1}{T} \\sum_{i=1}^{T} Ac_{T,i}$\n$FOr = \\frac{1}{T-1} \\sum_{i=1}^{T-1} Ac_{T,i} - Acc_{i,i}$"}, {"title": "4.2 Results", "content": "The results for Class-IL benchmarks are provided in Table 1. As previous studies have shown, regularization methods are suboptimal in this scenario. However, performance gains can be achieved by leveraging access to previous tasks stored in a buffer, exemplified by ER and DER++. Additionally, combining EWC with classical ER only seems to increase performance in Tiny-ImageNet.\nTo boost the efficacy of memory-based approaches, replacing Cross-Entropy Loss with the mapping loss (Equation 4) proves to be a powerful learning strategy. This strategy significantly increases accuracy, particularly in ER + EWC, where the accuracy increases 6%. The model gains valuable insights by incorporating additional information from latent representations, which is particularly advantageous in CL scenarios where knowledgeable representations can enhance generalization. However, it is noteworthy that this transferability is most pronounced in simple benchmarks like CIFAR100. In contrast, on more complex benchmarks such as Tiny-ImageNet and CORe50, Lm does not contribute to performance improvement. This is primarily attributed to higher forgetting, indicating that complex benchmarks need extra encouragement to mitigate forgetting.\nTo mitigate the forgetting, we propose the novel loss La. This loss is specifically crafted to influence the distances between classes, acknowledging that certain classes are more similar than others, which is not considered in the traditional triplet loss. Combining this loss with Lm creates CVM, which aims to extract valuable knowledge from the latent space while preserving the inherent class relationships. Our experimental results, as detailed in Table 1, confirm that CVM outperforms previous methods, including iCarl, which also removes the classifier during inference. These findings highlight that relying solely on information extraction from an LLM is insufficient, emphasizing the necessity of incorporating additional mechanisms in CL scenarios.\nSimilar results can be seen in CORe50, as shown in Table 1. Most regularization techniques find it challenging to perform reasonably, and memory-based methods produce the best results. iCarl, despite being a memory-based method, also faces difficulties due to the constant shift of the class distribution. Here, introducing Lm does not aid ER nor DER++ in enhancing their outcomes. However, CVM still surpasses its predecessors by encouraging the reuse of previous distances."}, {"title": "4.3 Ablation", "content": "In CL, preserving the plasticity of the model is fundamental to learning new tasks and distributions. However, the trade-off between learning new classes and forgetting the previous one is the primary concern that most CL methods face.\nIn memory-based methods, increasing the memory size improves the representativeness of the buffer, which should increase performance by decreasing forgetting. As shown in Table 2, DER++ is the method that scales the best as we increase the buffer size, significantly increasing its performance; however, it still achieves worse results than CVM, which consistently outperforms previous methods. On the other hand, iCarl scales poorly.\nSomething that explains the low performance of iCarl is plasticity. As shown in Table 1, iCarl obtained a lower forgetting value. These results can be explained by the significant restrictions that iCarl imposes when training new tasks. These constraints become more evident as we increase the memory size. iCarl can maintain a lower level of forgetfulness but at the cost of having low plasticity and being unable to scale up when we increase memory size. In contrast, CVM has no problem scaling up and maintaining a high level of plasticity.\nWe present two compelling arguments affirming the effectiveness of CVM and the importance of the proposed learning strategy for CL. Firstly, our approach involves mapping visual representations to a stable and well-informed space, removing the need for a classifier. Previous studies highlight the classifier as a critical contributor to model overfitting, particularly in challenging CL environments [13]. This finding gains further support from the insights in Table 2, where enhancing memory representativeness enhances overall performance and mitigates the risk of overfitting to limited memory capacities. Secondly, the strategic alignment of concepts in the latent space is crucial. The semantic information within the space generated by a frozen LLM discerns the similarities among diverse classes. This concept is actively promoted by incorporating the loss described in Equation 5.\nCVM components A critical component of CVM is the \u1e9e coefficient, which, as shown in Figure 3a, the optimal value is strongly related to the memory size. As we increase the memory size, the model can repeat more concepts presented in the buffer, which decreases the need to remember the exact distance of previous tasks, meaning that CVM can work with a smaller B. On the other hand, if we decrease the memory size, the model must depend mainly on the loss function to maintain semantic knowledge learned in the past.\nThe second experiment about \u1e9e concerns the plasticity of the model. As shown in figure 3b, the accuracy of the last task will decrease as we increase the \u1e9e coefficient. This is explained by discouragement in moving the weights when training the new task, reducing the plasticity. With a high value of \u1e9e, the loss promotes fewer modifications on the model weights, reducing forgetting but decreasing the final accuracy.\nAnother critical component of CVM is the LLM from where to create the fixed latent space. Maintaining the prompt fixed, we compare SentenceBERT with the textual model of CLIP and BERT. Results are shown in Table 3. These results show that using an appropriate model to generate the latent space is essential. However, future work will explore using more detailed descriptions or concepts extracted from the image instead of directly using the class."}, {"title": "5 Generalization Capabilities", "content": "For a continual learner to be efficient, it should reuse previous knowledge when learning new tasks. It is hypothesised that reusing previous knowledge could mitigate forgetting since the model would have no incentive to modify that knowledge. On this note, an essential feature in CL models is the generalization ability due to the transfer capability to future tasks.\nThere are two distinct approaches to assessing the transfer capability of a model. The first approach involves freezing the model and training a linear classifier (linear probing) on unseen classes. This evaluation method measures the generality of the representations acquired by the model for future tasks. The second approach focuses on evaluating the zero-shot capabilities of the model. In the case of CVM, this involves incorporating an anchor vector of unseen classes and verifying the accuracy exclusively for those classes while keeping the visual model frozen. This method provides insights into the model's ability to classify classes that are not part of the training set."}, {"title": "6 Limitations of Pre-trained Models", "content": "Even in scenarios where we have ample computational power, other issues can make methods based on large visual pre-trained models unsuitable, e.g., the need for more data, the specificity or even the granularity of the problem.\nWe introduce two Continual Learning (CL) benchmarks utilizing the GTSRB [51] and Aircraft [52] datasets. For the GTSRB dataset, we delineate 10 tasks, while the Aircraft dataset is partitioned into 5 tasks, both adhering to a Class-IL scenario.\nThe primary challenge confronted by methods relying on pre-trained models arises from the granularity of the dataset. This fine-grained nature necessitates additional information not present in the weights of large pre-trained models. Consequently, pre-trained models require assistance to learn the sequence, leading to low performance. In Table 5, we present the performance of L2P [9] both with and without memory. Notably, these approaches fail to outperform methods employing simpler models. Although pre-trained models may perform similarly to simpler and smaller models, their longer inference times can adversely impact overall performance."}, {"title": "7 Conclusions", "content": "We presented Continual Visual Mapping (CVM), which uses a conceptual latent space created by a frozen Large Language Model (LLM) to train a small visual model. By learning to map the visual representations to a knowledgeable space, the visual model can learn new concepts based on the knowledge extracted through pre-learned textual informa- tion. We demonstrate the performance of CVM in class-incremental and domain-incremental learning using various benchmarks, achieving state-of-the-art results within resource-constrained environments. We also reveal scenarios where simple and small models can be more efficient (and surprisingly, even more effective) than methods based on large pre-trained models. In particular, these are scenarios where the low transferability of these methods affects the performance. Our learning strategy opens new research directions for CL methods to asynchronously take advantage of the knowledge of large pre-trained models efficiently and effectively mapping new knowledge into existing ones."}]}