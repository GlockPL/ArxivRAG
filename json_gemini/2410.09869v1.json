{"title": "Prompt Tuning for Audio Deepfake Detection: Computationally Efficient Test-time Domain Adaptation with Limited Target Dataset", "authors": ["Hideyuki Oiso", "Yuto Matsunaga", "Kazuya Kakizaki", "Taiki Miyagawa"], "abstract": "We study test-time domain adaptation for audio deepfake detection (ADD), addressing three challenges: (i) source-target domain gaps, (ii) limited target dataset size, and (iii) high computational costs. We propose an ADD method using prompt tuning in a plug-in style. It bridges domain gaps by integrating it seamlessly with state-of-the-art transformer models and/or with other fine-tuning methods, boosting their performance on target data (challenge (i)). In addition, our method can fit small target datasets because it does not require a large number of extra parameters (challenge (ii)). This feature also contributes to computational efficiency, countering the high computational costs typically associated with large-scale pre-trained models in ADD (challenge (iii)). We conclude that prompt tuning for ADD under domain gaps presents a promising avenue for enhancing accuracy with minimal target data and negligible extra computational burden.", "sections": [{"title": "1. Introduction", "content": "Audio deepfake is a collection of deep learning techniques that create artificial speech [1]. It can cause significant harm, including compromising the security of automatic speaker verification systems, contributing to spreading fake news, defaming an individual's reputation, and copyright violation. Thus, developing ADD models has attracted much attention, and the ASVspoof Challenges [2, 3, 4, 5, 6, 7] have made significant progress.\nIn addressing the major challenges in ADD, we identify three primary areas of focus: (i) domain gaps in source and target data, (ii) limitation of the target dataset size, and (iii) high computational costs.\nThe challenge (i) arises from the domain gaps between training (source) and test (target) datasets for ADD models [8, 9]. They come from discrepancies in deepfake generation methods [2], recording environments (e.g., devices and surroundings) [8], and languages [9], necessitating effective domain adaptation. Specifically, we focus on test-time domain adaptation with a labeled target dataset. In this context, we adapt a pre-trained model from the source domain to the labeled test dataset from the target domain, without accessing the source domain dataset during the adaptation phase. This approach markedly differs from prior studies in ADD under domain gaps, which focus on domain generalization, where the target dataset is unavailable."}, {"title": "2. Prompt tuning for audio deepfake detection", "content": "2.1. Problem definition: test-time domain adaptation with labeled target dataset for audio deepfake detection\nWe consider test-time domain adaptation with a labeled target dataset for ADD (binary classification). Let $X \\subset \\mathbb{R}^{\\triangle}$ be the audio waveform space, where $\\triangle > 1$ is the number of sampling points. Let $Y = \\{Real, Fake\\}$ be the binary label space. The pre-trained ADD model, $f : X \\rightarrow Y$ (Front-End and Back-End in Fig. 1), parameterized by $\\theta_f$, determines whether a given waveform $x \\in X$ is classified as Real or Fake. $\\theta_f$ is pre-trained on a source dataset $D_s = \\{(x^s,y^s)\\}$ that is sampled from a source distribution $P_s(X, Y)$. The target dataset is defined as $D_T = \\{(x^t,y^t)\\}$, sampled from a target distribution $P_T(X, Y)$. We assume that there exists a discrepancy between $P_s(X, Y)$ and $P_T(X, Y)$. We aim to adapt the pre-trained model $f$ to the target distribution $P_T(X, Y)$, using post-hoc prompt tuning on $D_T$. This could involve introducing additional trainable parameters, or a prompt, $\\theta_p$, and tuning both $\\theta_p$ and $\\theta_f$ on $D_T$ to minimize the empirical risk: $\\frac{1}{M_T}\\sum_{i=1}^{M_T}L(f(x^t_i; \\theta_f, \\theta_p), y^t_i)$, where $L : \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$ is an arbitrary loss function. In our experiment, we use the class balanced loss [25], a class-imbalance-aware loss function. This is because ADD datasets are generally imbalanced, and we want to focus on performance gains by the prompt. Finally, it is important to note that we assume the source data $D_s$ is not accessible at the adaptation stage due to privacy reasons, for example. Note also that, unlike typical test-time domain adaptation, $D_T$ is labeled; i.e., $\\{y^t_i\\}_{i=1}^{M_T}$ as well as $\\{x^t_i\\}_{i=1}^{M_T}$ are available.\n2.2. Architecture\nThe overall architecture of the ADD model used in our experiment is shown in Fig. 1. It is divided into two submodules: Front-End and Back-End, separated by dashed lines in Fig. 1. This Front-End-Back-End architecture is common in many SOTA models in ADD [1, 18, 19, 20, 21]. Front-End is a large-scale foundation model such as wav2vec 2.0 [16] and Whisper [17]. It consists of the first convolutional feature extractor and the following transformer encoders. The feature vector thus obtained is then input to Back-End. It consists of one non-linear module (denoted by Head) and the last linear layer and performs binary classification.\n2.3. Proposed prompt tuning\nThe prompt $\\theta_p$ is inserted to the intermediate feature vectors in Front-End (Fig. 1) during both the training phase in the target domain and the inference phase. Specifically, a prompt is $d \\times N_P$ dimensional trainable parameters, where $d$ is the dimension of a prompt, and $N_p$ is the prompt length (the number of $d$-dimensional prompt vectors). The initialization algorithm of prompts is the same as [22].\nWe compare three types of prompt tuning, denoted by Tuned (A), (B), and (C) in Fig. 1. During the training phase in the target domain, type (A) tunes the prompt $\\theta_p$ only. Type (B) tunes the prompt $\\theta_p$ and the last linear layer. Type (C) tunes the prompt $\\theta_p$ and all the parameters of in $f$ (i.e., Front-End and Back-End)."}, {"title": "3. Experiments", "content": "3.1. Experimental setup\nDatasets. We use ASV spoof 2019 LA [4] as the source dataset $D_s$ for pre-training ADD models. ASVspoof 2019 LA is an audio deepfake dataset from English audio samples recorded in a hemi-anechoic chamber. For the target datasets $D_T$, we use In-The-Wild [8], Hamburg Adult Bilingual LAnguage (HABLA) [9], ASVspoof 2021 LA [6], and Voice Conversion Challenge (VCC) 2020 [26]. These four target datasets have multiple domain gaps, including deepfake generation methods (G), recording environments (E), and languages (L) (see Tab. 1). Overall, we have seven different target domains denoted by T1, T2,..., and T7 in Tab. 1. We perform experiments with $|D_T| = 10, 50, 100$, and 1000 (except for VCC 2020, for which $|D_T| = 1000$ is not used); i.e., we randomly sample the target training datasets $D_T$ from the original training datasets. Each\nComputing infrastructure and runtimes. We use an NVIDIA Tesla V100 GPU throughout all experiments. The training durations for W2V and WSP models on the target datasets are approximately 500 and 100 seconds, respectively, under the conditions of a prompt length of 5, batch size of 16, dataset size of 50, and 100 epochs. For W2V, GPU memory consumption is"}, {"title": "3.3. Discussion", "content": "This paper focuses on accuracy in the target dataset rather than the source dataset. The latter is less relevant when considering domain gaps due to changes in recording environments and languages. Nevertheless, preserving source accuracy is crucial, particularly when the deepfake generation method in the target domain varies from that in the source domain; i.e., the adapted model should be robust against both the previous and new attack methods. To address this issue, we can easily integrate our plug-in-style method with existing techniques that maintain source accuracy while enhancing target accuracy [15, 32].\nOur proposed method assumes that the base architecture includes transformer encoders. However, it could also be used for convolutional neural networks (CNNs) in light of recent studies of prompt tuning for CNNs [33]. Nevertheless, most of the SOTA models in ADD are based on transformers anyway [1, 18, 19, 20, 21]; thus, our method can be applied to a wide variety of SOTA models."}, {"title": "4. Conclusion", "content": "In our study on test-time domain adaptation for ADD, we tackle three prevalent challenges: (i) domain gaps between source and target dataset, (ii) limitation of target dataset size, and (iii) high computational costs. To overcome these issues, we introduce a method for ADD utilizing prompt tuning in a plug-in style. Our method can be applied to SOTA transformer models and other fine-tuning methods, to boost accuracy on target data. Additionally, our method efficiently improves accuracy even with limited amounts of labeled target data (e.g., 10). Furthermore, the computational cost of our method is low compared to full fine-tuning even when the base pre-trained model is huge. Our experiments show that the proposed method improves detection performance in most cases for two SOTA models and seven domain gaps."}]}