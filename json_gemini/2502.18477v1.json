{"title": "Recommendations Beyond Catalogs:\nDiffusion Models for Personalized Generation", "authors": ["Gabriel Patron", "Zhiwei Xu", "Ishan Kapnadak", "Felipe Maia Polo"], "abstract": "Modern recommender systems follow the guiding principle of serving the right user, the right item at the right time. One of their main limitations is that they are typically limited to items already in the catalog. We propose REcommendations BEyond CAtalogs, REBECA, a new class of probabilistic diffusion-based recommender systems that synthesize new items tailored to individual tastes rather than retrieve items from the catalog. REBECA combines efficient training in embedding space with a novel diffusion prior that only requires users' past ratings of items. We evaluate REBECA on real-world data and propose novel personalization metrics for generative recommender systems. Extensive experiments demonstrate that REBECA produces high-quality, personalized recommendations, generating images that align with users' unique preferences. Code available at the REBECA repository.", "sections": [{"title": "1. Introduction", "content": "Despite the widespread awareness of generative AI, its integration into daily content consumption remains surprisingly limited. A recent Reuters survey (Fletcher & Nielsen, 2024) highlights this gap, showing that while many users are familiar with generative models, their actual adoption lags behind expectations. Meanwhile, recommendation algorithms continue to dominate online interactions, shaping how users engage with digital content and products.\nModern recommender systems excel at matching users to items but are inherently constrained by the content available in their catalogs. These systems implicitly assume that relevant, engaging items exist for all users at all times; this assumption, however, needs to be reconsidered with the rise of generative AI."}, {"title": "2. Related Work", "content": "Recommender systems and generative models are two rapidly evolving fields. While traditional recommender systems rely on pre-existing catalogs, generative approaches offer the promise of creating novel, user-specific content. This section reviews prior work in these areas, positioning our method as an underexplored bridge between them.\nModern recommender systems are often described as understanding users better than they understand themselves. This perception stems from their ability to gather and analyze vast amounts of online activity data, building detailed profiles of user interests and habits. Such systems have become highly effective at matching users to items within pre-existing catalogs (Castells & Jannach, 2023; Li et al., 2023). Novel-item generative recommenders can enhance the current catalog offering, not just by creating, but also by making items more suitable to an individual's preferences."}, {"title": "2.2. Generative Recommenders", "content": "In recent years, there has been substantial progress at the intersection of generative models and recommender systems. Research in generative retrieval (Zhai et al., 2024) and the use of large language models (LLMs) for preference discernment (Paischer et al., 2024) has introduced new paradigms, including the term \u201cGenerative Recommender\" (Zhai et al., 2024). While these developments are exciting, they are not the focus of this work because they address the problem of within-catalog recommendation with generative item retrieval. Instead, we reference them here to provide contrast and contextualize our approach.\nDespite the relative novelty of generative recommenders,"}, {"title": "2.3. Model Alignment", "content": "Aligning generative models' outputs with human values and preferences is a vibrant research domain whose discourse often extends beyond technical communities into the public sphere. From a technical perspective, one widely adopted approach is Reinforcement Learning with Human Feedback (RLHF) (Christiano et al., 2023; Ziegler et al., 2020), which optimizes model behavior based on reward signals derived from human preferences. However, RLHF remains computationally expensive, requires careful reward function design, and can be prone to mode collapse.\nAnother promising approach is Direct Preference Optimization (DPO) (Rafailov et al., 2023), which aligns base models with human values through contrastive preference learning. DPO assumes uniform user preferences, treating human feedback as globally consistent, which is problematic in personalized settings like recommender systems, where preferences are highly heterogeneous. Additionally, DPO still requires an implicit reward structure, meaning that careful curation of preference data is crucial to avoid biases.\nIn our case, where we seek to model individual user preferences dynamically, both RLHF and DPO present limitations. Rather than imposing a global reward structure, we instead condition generation on user embeddings, allowing the model to capture personalized preferences in a more flexible, data-driven manner."}, {"title": "2.4. Adapters, Priors and Decoders", "content": "While our approach addresses the problem setting of GeneRec (Wang et al., 2024), our proposal is close to the adapter in generative modeling. The purpose of adapters is to produce augmentation to new input conditions or fine-tuning behavior (Zhang et al., 2023; Ye et al., 2023) of large pre-trained models, and they typically require text captions during training. We argue that text captions are not only unusual in most recommendation datasets, but they also fail to distill the nuances of individual preference, better captured in the features tracked in modern recommender systems (Zhai et al., 2024). Although some adapters can be successfully calibrated with Low-Rank adaptations (Hu et al., 2022), they are fine-tuned per user, which not only leads to poor scaling properties, but also does not benefit from correlations between individuals. Instead, we propose a probabilistic adaptation framework that leverages CLIP latents as custom priors and an off-the-shelf decoder in a principled adapter formulation. In the past, priors and decoders have been utilized to train full base text-to-image models (Pradeep et al., 2023). REBECA is the first used to produce adapting behavior to additional input signals."}, {"title": "2.5. Challenges in Establishing Baselines", "content": "Generative recommenders, as a novel class of systems, currently lack established baselines or datasets combining user ratings with image captions. Traditional methods in recommender systems focus on retrieval, while generative models lack personalization mechanisms. Adapters, although promising, require paired datasets that are impractical in recommendation contexts. This lack of baselines underscores the need for innovative methodologies and benchmarks, which we aim to address in this work."}, {"title": "3. Methodology", "content": "In this section, we present REBECA's pipeline. During training, images are first mapped to a lower-dimensional embedding space using CLIP (Radford et al., 2021). Then a conditional diffusion model (Ho et al., 2020) is trained on these embeddings, conditioned on user and rating information. Specifically, at each time step t of the diffusion process, the model derives an image embedding $I_t$. This embedding, along with the time step t, the user ID U and the rating R, is then fed into $\\varepsilon_{\\theta}(U, R, t, I_t)$, a multi-layer decoder-only transformer, to predict the noise vector. The motivation is to integrate user and rating information directly into the diffusion dynamics. Classifier-free guidance (Ho & Salimans, 2022) is further incorporated into the diffusion model to improve the sampling quality of image embeddings.\nDuring inference, the trained diffusion model is provided with a user ID and a desired rating. The diffusion model subsequently generates image embeddings, which are then passed to a pretrained text-to-image model via IP-adapter (Ye et al., 2023) to produce the final images."}, {"title": "3.1. Personalized Image Embedding Generation", "content": "Our goal is to sample images I from the personalized distribution $p(I\\vert U, R)$ rather than the user-agnostic marginal distribution $p(I)$, where U represents the user and R represents the desired rating. We decompose the personalized distribution as follows:\n$p(I\\vert U, R) = p(I, I_e\\vert U, R) = p(I_e\\vert U, R)p(I\\vert I_e, U, R)$,\nwhere $I_e$ denotes the deterministic embedding of the image I. To model this decomposition, we employ two stages:\n\u2022 A conditional diffusion model $\\lsem p_{\\theta}(.\\vert U, R)$ with parameter $\\theta$ is trained to approximate $p(I_e\\vert U, R)$. Given user U and desired rating R, we sample new image embeddings from"}, {"title": "3.2. Model training and inference", "content": "Training our diffusion model to generate personalized image embeddings conditioned on user preferences follows a standard diffusion formulation, where a data distribution $I_e\\lsem p(I_e\\vert U, R)$ undergoes a forward noising process, and the model learns to predict the reverse denoising trajectory by maximizing the lower bound of a variational Markov Chain.\nFormally, given an original unperturbed embedding $I_e$, we apply a noising schedule $\\beta_t$ to produce noisy embeddings during the forward process:\n$q(I_t\\vert I_{t-1}) = \\mathcal{N}(I_t; \\sqrt{\\alpha_t}I_{t-1}, \\sigma_t^2I)$,\nwhere $\\alpha_t$ and $\\sigma_t$ depend on the choice of noise schedule.\nIn the reverse process, we use a transformer $\\varepsilon_{\\theta}(U, R, t, I_t)$ with input sequence being a concatenation of user, rating, time, and image tokens to approximate $\\varepsilon_t$, the noise injected in the forward process conditioned on user and rating information. The reverse process does not have an exact analytic formulation, however, for $\\beta_t \\to 0$, it approximates a normal distribution (Sohl-Dickstein et al., 2015). We thus approximate the reverse process by:\n$p_{\\theta}(I_{t-1} \\vert I_t, U, R) = \\mathcal{N}(I_{(t-1)}; \\mu_{\\theta}(U, R, t, I_t), \\sigma_t^2I)$,\nwhere $\\mu_{\\theta}$ is derived in relation to the noise prediction model $\\varepsilon_{\\theta}$. Finally, we optimize the model using a simplified denoising objective (Ho et al., 2020):\n$L_{simple}(\\theta) = \\mathbb{E}_{(t, \\varepsilon, U, R)} [\\Vert\\varepsilon - \\varepsilon_{\\theta}(U, R, t, I_t)\\Vert^2]$,\nwhere $t \\sim \\text{Unif}(\\{1, \\dots, T\\})$ and $\\varepsilon \\sim \\mathcal{N}(0, I)$."}, {"title": "4. Experiments", "content": "We repurpose the FLICKR-AES dataset (Ren et al., 2017) to learn a generative model for aesthetically pleasing images based on user preferences. Initially compiled to learn models for personalized aesthetics predictions, the dataset contains a curated sample of 40,988 Creative Commons-licensed photos from FLICKR, rated for aesthetic quality on a scale of 1 to 5. For reference, annotations were collected using Amazon Mechanical Turk, with each image receiving ratings from five different workers. In total, 210 workers contributed, scoring 40,988 images for a total of 193208 ratings. On average, each worker labeled 137 images.\nMore formally, we define our initial data setup as consisting of triplets of users, ratings, and images (U, R, I). CLIP embeddings are deterministic, and computing all of them requires a single pass of our image data through the appropriate CLIP ViT-H-14 encoder, and so we augment our data with them. We further map the ratings to binary rating, representing like and dislike: given a pair of user and its rated image, we let R = 1 if the rating is 4 or higher, and let R = 0 otherwise. Finally, we filter out users with less than 100 liked images, ensuring enough information on individual preference.\nAt the training stage, we fit our image embedding prior with (U, R, $I_e$). Importantly, image captions are not part of our data, and we uniquely rely on users' historic ratings."}, {"title": "4.2. Image Generation", "content": "We restate our objective as enhancing catalogs in accordance with users' preferences. This means that at the sampling stage, we generate image embeddings that are personalized based on the user's historical interactions.\nTo ensure that the sampled embeddings align with user preferences, we set R = 1 to indicate a strong preference signal. This choice enforces the model to condition its generation process on the user embedding, thereby prioritizing images that align with previously liked content.\nWe follow the standard denoising process using CFG and set the guidance scale of 10.0 (see details in Section 4.5) to balance personalization and diversity. The final generated embedding $\\hat{I_e}$ represents a user-adapted image feature that is used as input to a Stable Diffusion 1.5 model with an empty prompt (denominated $T_0$) (Rombach et al., 2022). We select this model for its maturity and known properties, alongside its well-known IP-Adapter (Ye et al., 2023)."}, {"title": "4.3. Baselines", "content": "We consider three baselines, all utilizing the same decoder as REBECA, namely Stable Diffusion 1.5 (Rombach et al., 2022). The first baseline, $T_0$, generates images from Stable Diffusion using an empty prompt, providing insight into the model's generic outputs. The second baseline, $T_1$, samples images using high-quality but simple positive and negative prompts. The third baseline, $T_2$, employs high-quality and detailed positive and negative prompts, simulating a scenario where a user can significantly influence image generation through prompt engineering. We paste the prompts we use in the following box.\n\u2022 $T_1$ prompts:\n(+) \u201chigh quality photo\u201d\n(-) \u201cbad quality photo, letters\u201d\n\u2022 $T_2$ prompts:\n(+) \u201cRealistic image, finely detailed, with balanced composition and harmonious elements. Dynamic yet subtle tones, versatile style adaptable to diverse themes and aesthetics, prioritizing clarity and authenticity.\u201d\n(-) \u201cdeformed, ugly, wrong proportion, frame, watermark, low res, bad anatomy, worst quality, low quality\u201d"}, {"title": "4.4. Metrics", "content": "Our objective is to generate images that not only align with user preferences but also maintain high quality. To evaluate our approach, we compute the Fr\u00e9chet Inception Distance (FID) (Heusel et al., 2017) and Conditional Maximum Mean"}, {"title": "DOES REBECA GENERATE PERSONALIZED IMAGES?", "content": "Dependence between generated images and user IDs\nWe start measuring and testing the degree of personalization of REBECA utilizing our trained verifier $\\hat{v}$ and a statistical hypothesis testing. The degree of personalization can be measured by randomly assigning REBECA images to users and comparing the verifier scores with the case in which images are correctly assigned to the user they were created for; the difference of verifier scores (correct assigned minus randomly assigned) denotes how happier users get when the correct images are assigned to them versus when random images are assigned. To measure if the difference is statistically significant, we employ a hypothesis test measuring the degree of dependence between users U and generated images I; in statistical terms, we test the null hypothesis $H_0: U \\perp I$ using a permutation test (Lehmann et al., 1986) and a significance level $\\alpha \\in (0,1)$. The procedure is described in concrete terms in Algorithm 1. Figure 3 depicts our results and shows that, on average, assigning the correct REBECA images vs randomly assigning them, increases the verifier score, on average, almost 6 standard deviations and leads to a p-value $p < 10^{-3}$ (much smaller than the standard threshold $\\alpha = .05$), giving strong evidence that images and users are statistically dependent and that REBECA generates images tailored to each user.\nTopic Precision For each user, we define their liked topics as the set of topics that are assigned to some image that the user liked in the FLICKR data. We use the REBECA model to generate 50 images for each user conditioning on (U, R = 1), implying that we expect the user to like the"}, {"title": "4.5. Results", "content": "GLOBAL QUALITY OF GENERATED IMAGES\nTable 1 presents a comparison of different models based on FID (Fr\u00e9chet Inception Distance) and CMMD (Conditional Maximum Mean Discrepancy), where lower values indicate better performance and a better alignment with the target distribution of images. The results demonstrate that REBECA achieves the best performance, with the lowest FID 117.77 and CMMD 0.68, indicating superior image quality and better alignment with user preferences. The $T_2$ baseline shows a clear degradation in performance, having the highest FID (145.45) and worst CMMD (1.56), indicating a significant drop in both image realism and conditional consistency despite a well-crafted prompt. These results reinforce that REBECA is the most effective approach, outperforming all baselines in global image quality."}, {"title": "5. Discussion", "content": "In this paper, we introduce REBECA, a novel probabilistic framework for personalized recommendation beyond catalogs. Unlike traditional retrieval-based recommender systems, REBECA generates new items tailored to individual preferences using a diffusion-based generative model. By leveraging implicit user preference instead of explicit text prompts, our approach seamlessly integrates generative AI with recommendation pipelines.\nOur experiments demonstrate that REBECA outperforms competing approaches that rely on textual prompts. Through topic modeling and a verifier-based evaluation, we established that our method generates images aligned with user interests, without requiring costly fine-tuning or extensive prompt engineering. Notably, we showed that introducing generic prompts can degrade personalization, highlighting the importance of direct preference modeling. Despite its success, developing personalized generative recommenders presents key challenges, particularly the lack of suitable image datasets that pair user preferences (e.g., ratings) with visual content. Future research could focus on collecting and curating benchmark datasets tailored to this task.\nThe effectiveness of REBECA in capturing user-specific content and style preferences suggests broader applicability to other domains, such as music, video, and textual content generation. Several directions remain open for future work:\n\u2022 Extending REBECA to multimodal recommendations, incorporating text and audio to further enrich user experiences."}, {"title": "A. Appendix", "content": "A.1. Training details\nOur model is a diffusion-based generative model with a transformer backbone that operates in CLIP embedding space rather than pixel space. This design significantly reduces computational overhead while maintaining high expressiveness in learned representations. Notably, training each model requires less than 10 minutes to reach early stopping on a single NVIDIA RTX 4090 GPU.\nGiven the imbalance in the number of rated images per user in the FLICKR dataset, we employ strategic sampling to mitigate this issue. Specifically, in each batch, we sample an equal number of images per user to ensure balanced training.\nFor the transformer model within the diffusion framework, we conduct a grid search over key hyperparameters:\n\u2022 Number of layers: {2, 4, 8, 16}\n\u2022 Number of heads: 2, 4, 8, 16, 32}\n\u2022 Number of user tokens: {1, 2, 4, 8}\n\u2022 Number of image tokens: {1, 2, 4, 8}\n\u2022 Learning rate: {1e-4, 1e-3}\nFollowing this, we perform an additional grid search over:\n\u2022 Scheduler steps: {1000, 2000, 4000, 6000}\n\u2022 Samples per user: {30, 50, 80, 110, 140}\nThe selected model, achieving the lowest validation loss, is an 8-layer, 16-head decoder-only transformer with four user tokens, one image token, and one rating token. The optimal learning rate is 1e-4, and the best-performing configuration samples 80 images per user.\nWe use the AdamW optimizer (Loshchilov & Hutter, 2019) with a learning rate of 1e-4 and a weight decay of le-5.\nA.2. Topic modeling\nWe ask GPT-40 mini (OpenAI, 2024) to generate captions for each image using the following prompt:"}]}