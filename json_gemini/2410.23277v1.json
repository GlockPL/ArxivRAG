{"title": "SLOWFAST-VGEN: SLOW-FAST LEARNING FOR ACTION-DRIVEN LONG VIDEO GENERATION", "authors": ["Yining Hong", "Beide Liu", "Maxine Wu", "Yuanhao Zhai", "Kai-Wei Chang", "Lingjie Li", "Kevin Lin", "Chung-Ching Lin", "Jianfeng Wang", "Zhengyuan Yang", "Yingnian Wu", "Lijuan Wang"], "abstract": "Human beings are endowed with a complementary learning system, which bridges the slow learning of general world dynamics with fast storage of episodic memory from a new experience. Previous video generation models, however, primarily focus on slow learning by pre-training on vast amounts of data, overlooking the fast learning phase crucial for episodic memory storage. This oversight leads to inconsistencies across temporally distant frames when generating longer videos, as these frames fall beyond the model's context window. To this end, we introduce SLOWFAST-VGEN, a novel dual-speed learning system for action-driven long video generation. Our approach incorporates a masked conditional video diffusion model for the slow learning of world dynamics, alongside an inference-time fast learning strategy based on a temporal LoRA module. Specifically, the fast learning process updates its temporal LoRA parameters based on local inputs and outputs, thereby efficiently storing episodic memory in its parameters. We further propose a slow-fast learning loop algorithm that seamlessly integrates the inner fast learning loop into the outer slow learning loop, enabling the recall of prior multi-episode experiences for context-aware skill learning. To facilitate the slow learning of an approximate world model, we collect a large-scale dataset of 200k videos with language action annotations, covering a wide range of scenarios. Extensive experiments show that SLOWFAST-VGEN outperforms baselines across various metrics for action-driven video generation, achieving an FVD score of 514", "sections": [{"title": "1 INTRODUCTION", "content": "Human beings are endowed with a complementary learning system (McClelland et al., 1995). The slow learning paradigm, facilitated by the neocortex, carves out a world model from encyclopedic scenarios we have encountered, enabling us to make decisions by anticipating potential outcomes of actions (Schwesinger, 1955). Complementing this, the hippocampus supports fast learning, allowing for rapid adaptation to new environments and efficient storage of episodic memory (Tulving, 1983), ensuring that long-horizon simulations remain consistent (e.g., when we travel across locations 1 to 6 and return, the scene remains unchanged.)\nRecently, riding the wave of success in video generation (Blattmann et al., 2023a), researchers have introduced action-driven video generation that could generate future frames conditioned on actions (Yang et al., 2024; Wang et al., 2024a; Hu et al., 2023; Wu et al., 2024), mimicking the slow learning paradigm for building the world model. However, these models often generate videos of constrained lengths (e.g., 16 frames for 4 secs (Xiang et al., 2024; Wu et al., 2024)), limiting their ability to foresee far into the future. Several works focus on long video generation, using the outputs of one step as inputs to the next (Harvey et al., 2022; Villegas et al., 2022; Chen et al., 2023; Guo et al., 2023a; Henschel et al., 2024). Due to the lack of fast learning, these models often do not memorize trajectories prior to the current context window, leading to inconsistencies in long-term videos.\nIn this paper, we propose SLOWFAST-VGEN, which seamlessly integrates slow and fast learning in a dual-speed learning system for action-driven long video generation. For the slow learning process, we introduce a masked conditional video diffusion model that conditions on language inputs denoting actions and the preceding video chunk, to generate the subsequent chunk. In the inference time, long video generation can be achieved in a stream-in fashion, where we consecutively input our actions for the current context window, and the succeeding chunk can be generated. To enable the model to memorize beyond the previous chunk, we further propose a fast learning strategy, which rapidly adapts to new contexts and stores episodic memory, enabling the model to maintain coherence over extended sequences. By combining slow learning and fast learning, SLOWFAST-VGEN can generate consistent, action-responsive long videos.\nA key challenge for fast learning in long video generation resides in maintaining consistency across long sequences, while avoiding computationally intensive or memory-inefficient strategies for storing lengthy video sequences. To address this, we propose a novel inference-time fast learning strategy that stores episodic memory in Low-Rank Adaptation (LoRA) parameters. Specifically, we draw inspiration from Temporary LoRA (Wang et al., 2024b), originally developed for long text generation, and extend this concept for fast learning in video generation. In its original form, TEMP-LORA embeds contextual information in a temporary LoRA module for language modeling by progressively generating new text chunks based on inputs and training on these input-output pairs. Our approach modifies the update mechanism of TEMP-LORA parameters by discarding the input-to-output format. At each context window, our TEMP-LORA for fast learning operates as follows: we generate a new video chunk conditioned on both the input chunk and the corresponding action, then concatenate the input and output into a seamless temporal continuum. This sequence undergoes a noise injection process, creating a temporally coherent learning signal. We then train the denoising UNet on this noise-augmented, action-agnostic sequence. This method emphasizes memorizing entire trajectories rather than focusing on immediate input-output streams or transitions. By leveraging the forward diffusion and reverse denoising processes over the concatenated sequence, we effectively consolidate sequential episodic memory in the TEMP-LORA parameters of the UNet, enabling it to maintain coherence across extended video sequences. Qualitative examples demonstrate that fast learning enhances the consistency, smoothness, and quality of long videos, as well as enabling the fast adaptation to test scenes.\nWhile our fast learning module excels at rapidly adapting to new contexts and storing episodic memory, there exist specific context-aware tasks that require learning from all previous long-term episodes rather than just memorizing individual ones. For instance, in the planning section"}, {"title": "2 RELATED WORKS", "content": "Text-to-Video Generation Nowadays, research on text-conditioned video generation has been evolving fast (Ho et al., 2022b;a; Singer et al., 2022; Luo et al., 2023; Esser et al., 2023; Blattmann et al., 2023b; Zhou et al., 2023; Khachatryan et al., 2023; Wang et al., 2023), enabling the generation of short videos based on text inputs. Recently, several studies have introduced world models capable of generating videos conditioned on free-text actions (Wang et al., 2024a; Xiang et al., 2024; Yang et al., 2024). While these papers claim that a general world model is pretrained, they often struggle to simulate dynamics beyond the context window, thus unable to generate consistent long videos.\nThere are also several works that formulate robot planning as a text-to-video generation problem (Du et al., 2023; Ko et al., 2023). Specifically, Du et al. (2023) trains a video diffusion model to predict future frames and gets actions with inverse dynamic policy, which is followed by this paper.\nLong Video Generation A primary challenge in long video generation lies in the limited number of frames that can be processed simultaneously due to memory constraints. Existing techniques often condition on the last chunk to generate the next chunk(Harvey et al., 2022; Villegas et al., 2022; Chen et al., 2023; Guo et al., 2023a; Henschel et al., 2024; Zeng et al., 2023; Ren et al., 2024), which hinders the model's ability to retain information beyond the most recent chunk, leading to inconsistencies when revisiting earlier scenes. Some approaches use an anchor frame (Yang et al., 2023; Henschel et al., 2024) to capture global context, but this is often insufficient for memorizing the whole trajectory. Other methods generate key frames and interpolate between them (He et al., 2023; Ge et al., 2022; Harvey et al., 2022; Yin et al., 2023), diverging from how world models simulate future states through sequential actions, limiting their suitability for real-time action streaming, such as in gaming contexts. Additionally, these methods require long training videos, which are hard to obtain due to frequent shot changes in online content. In this paper, we propose storing episodic"}, {"title": "3 SLOWFAST-VGEN", "content": "In this section, we introduce our SLOWFAST-VGEN framework. We first present the masked diffusion model for slow learning and the dataset we collected (Sec 3.1). Slow learning enables the generation of new chunks based on previous ones and input actions, while lacking memory retention beyond the most recent chunk. To address this, we introduce a fast learning strategy, which can store episodic memory in LoRA parameters (Sec 3.2). Moving forward, we propose a slow-fast learning loop algorithm (Sec 3.3) that integrates TEMP-LORA into the slow learning process for context-aware tasks that require knowledge from multiple episodes, such as long-horizon planning. We also illustrate how to apply this framework for video planning (Sec 4.2) and conduct a thorough investigation and comparison between our model and complementary learning system in cognitive science, highlighting parallels between artificial and biological learning mechanisms (Sec 3.5)."}, {"title": "3.1 SLOW LEARNING", "content": null}, {"title": "3.1.1 MASKED CONDITIONAL VIDEO DIFFUSION", "content": "We develop our slow learning model based on the pre-trained ModelScopeT2V model (Wang et al., 2023), which generates videos from text prompts using a latent video diffusion approach. It encodes training videos into a latent space $z$, gradually adding Gaussian noise to the latent via $z_t = \\sqrt{\\bar{\\alpha}_t} z_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon$. A Unet architecture, augmented with spatial-temporal blocks, is responsible for denoising. Text prompts are encoded by the CLIP encoder (Radford et al., 2021).\nTo better condition on the previous chunk, we follow Voleti et al. (2022) for masked conditional video diffusion, conditioning on past frames in the last video chunk to generate frames for the subsequent chunk, while applying masks on past frames for loss calculation. Given $f_p$ past frames and $f_g$ frames to be generated, we revise the Gaussian diffusion process to:"}, {"title": null, "content": "$z_{t, :f_p} = z_{0,:f_p}$\n$z_{t,f_p:(f_p+f_g)} = \\sqrt{\\bar{\\alpha}_t} z_{0,:f_p:(f_p+f_g)} + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon$\n$z_t = z_{t,:f_p} + z_{t,f_p:(f_p+f_g)}$\n(1)"}, {"title": null, "content": "where $z_{t,:f_p}$ is the latent of the first $f_p$ frames at diffusion step $t$, which is clean as we do not add noise to the conditional frames. $z_{t,f_p:(f_p+f_g)}$ corresponds to the latent of the ground-truth output frames, which we add noise to for conditional generation of the diffusion process. We concatenate (+) them together and send them all into the UNet. We then get the noise predictions out of the UNet, and apply masks to the losses corresponding to the first $f_p$ frames. Thus, only the last $f_g$ frames are used for calculating loss. The final loss is:\n$L(\\Phi) = E_{t,z_0~P_{data}, \\epsilon~N(0,1),c} [||\\epsilon - \\epsilon_\\Theta(z_t[f_p : (f_p + f_g)], t, c)||^2]$\n(2)\nHere, c represents the conditioning (e.g., text), and $\\Theta$ denotes the UNet parameters. Our model is able to handle videos of arbitrary lengths smaller than the context window size (i.e., arbitrary $f_p$ and $f_g$). For the first frame image input, $f_p$ equals 1."}, {"title": "3.1.2 DATASET COLLECTION", "content": "Our slow learning dataset consists of 200k data, with each data point in the format of (input video chunk, input free-text action, output video chunk). The dataset can be categorized into 4 domains :\n\u2022 Unreal. We utilize the Unreal Game Engine (Epic Games) for data collection, incorporating environments such as Google 3D Tiles, Unreal City Sample, and various assets purchased online. We introduce different agent types (e.g., human agents and droids) and use a Python script to automate action control. We record videos from both first-person and third-person perspectives, capturing keyboard and mouse inputs as actions and translating them into text (e.g., \u201cgo left\").\n\u2022 Game. We manually play Minecraft, recording keyboard and mouse inputs and capturing videos.\n\u2022 Human Activities. We include the EPIC-KITCHENS (Damen et al., 2018; 2022) dataset, which comprises extensive first-person (egocentric) vision recordings of daily activities in the kitchen.\n\u2022 Robot. We use several datasets from OpenX-Embodiment (Collaboration et al., 2024), as well as tasks from Metaworld (Yu et al., 2021) and RLBench (James et al., 2019). As most robot datasets consist of short episodes (where each episode is linked with one language instruction) rather than long videos with sequential language inputs, we set $f_p$ to 1 for these datasets.\n\u2022 Driving. We utilize the HRI Driving Dataset (HDD) (Ramanishka et al., 2018), which includes 104 hours of real human driving. We also include driving videos generated in the Unreal Engine.\""}, {"title": "3.2 FAST LEARNING", "content": "The slow learning process enables the generation of new video chunks based on action descriptions. A complete episode can thus be generated by sequentially conditioning on previous outputs. However, this method does not ensure the retention of memory beyond the most recent chunk, potentially leading to inconsistencies among temporally distant segments. In this section, we introduce the novel fast learning strategy, which can store episodic memory of all generated chunks. We begin by briefly outlining the generation process of our video diffusion model. Subsequently, we introduce a temporary LORA module, TEMP-LORA, for storing episodic memory.\nGeneration Each iteration $i$ consists of $T$ denoising steps ($t = T  ... 1$). We initialize new chunks with random noise at $z_i$. During each step, we combine the previous iteration's clean output $z_0^{i-1}$ with the current noisy latent $z_i^{t}$. This combined input is fed into a UNet to predict and remove noise, producing a less noisy version for the next step. We focus only on the newly generated frames, masking out previous-iteration outputs. This process repeats until the iteration's final step, yielding a clean latent $z_0^{i}$. The resulting $z_0^{i}$ becomes input for the next iteration. The generation process works directly with the latent representation, progressively extending the video without decoding and re-encoding from pixel space between iterations.\nTEMP-LORA Inspired by Wang et al. (2024b), we utilize a temporary LoRA module that embeds the episodic memory in its parameters. TEMP-LORA was initially designed for long text generation, progressively generating new text chunks based on inputs, and use the generated chunk as ground-truth to train the model conditioned on the input chunk. We improve TEMP-LORA for video generation to focus on memorizing entire trajectories rather than focusing on immediate input-output"}, {"title": null, "content": "streams. Specifically, after the generation process of iteration $i$, we use the concatenation of the input latent and output latent at this iteration to update the TEMP-LORA parameters.\n$z_0^i = z_0^{i-1} \\oplus z_0^{i}; z_i^{t} = \\sqrt{\\alpha_t}z_i + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon$\n$L(\\Theta_i|\\Phi) = E_{t,z_i^{t},\\epsilon~N(0,1)} [||\\epsilon - \\epsilon_{\\Phi+\\Theta_i}(z_i^{t},t)||^2]$\n(3)\nSpecifically, we concatenate the clean output from the last iteration $z_0^{i-1}$ (which is also the input of the current iteration) and the clean output from the current iteration $z_0^{i}$ to construct a temporal continuum $z_0^i$. Then, we add noise to the whole $z_0^{i}$ sequence, which results in $z_i^{t}$ at noise diffusion step $t$. Note that here we do not condition on clean $z_0^{i-1}$ anymore, and exclude text conditioning to focus on remembering full trajectories rather than conditions. Then, we update the TEMP-LORA parameters of the UNet $\\Theta_i$ using the concatenated noise-augmented, action-agnostic $z_0^i$. Through forward diffusion and reverse denoising, we effectively consolidate sequential episodic memory in the TEMP-LORA parameters."}, {"title": "3.3 SLOW-FAST LEARNING LOOP WITH TEMP-LORA", "content": "Previously, we develop TEMP-LORA for inference-time training, allowing for rapid adaptation to new contexts and the storage of per-episode memory in the LoRA parameters. However, some specific context-aware tasks require learning from all collected long-term episodes rather than just memorizing individual ones. For instance, solving a maze involves not only recalling the long-term trajectory within each maze, but also leveraging prior experiences to generalize the ability to navigate and solve different mazes effectively. Therefore, to enhance our framework's ability to solve various tasks that require learning over long-term episodes, we propose the slow-fast learning loop algorithm, which integrates the TEMP-LORA modules into a dual-speed learning process.\nWe detail our slow-fast learning loop algorithm. Our slow-fast learning loop consists of two primary loops: an inner loop for fast learning and an outer loop for slow learning. The inner loop, representing the fast learning component, utilizes TEMP-LORA for quick adaptation to each episode. It inherits the fast learning algorithm and updates the memory in the TEMP-LORA throughout the long video generation process. Crucially, the inner loop not only generates long video outputs and updates the TEMP-LORA parameters, but also prepares training data for the slow learning process. It aggregates inputs, ground-truth outputs, and corresponding TEMP-LORA parameters in individual episodes into a dataset $D_s$. The outer loop implements the slow learning process. It leverages the learned frozen TEMP-LORA parameters in different episodes to capture the long-term information of the input data. Specifically, it utilizes the data collected from multiple episodes by the inner loop, which consists of the inputs and ground-truth outputs of each iteration in the episodes, together with the memory saved in TEMP-LORA up to this iteration.\nWhile the slow-fast learning loop may be intensive for full pre-training of the approximate world model, it can be effectively used in finetuning for specific domains or tasks requiring long-horizon planning or experience consolidation. A key application is in robot planning, where slow accumulation of task-solving strategies across environments can enhance the robot's ability to generalize to complex, multi-step challenges in new settings."}, {"title": "3.4 VIDEO PLANNING", "content": "We follow Du et al. (2023), formulating task planning as a text-conditioned video generation problem using Unified Predictive Decision Process (UPDP). We define a UPDP as $G = (X,C, H, \\phi)$, where X is the space of image observations, C denotes textual task descriptions, $H \\in T$ is a finite horizon length, and $\\phi(x_0,c) : X \\times C \\rightarrow \\Delta(X^{T-1})$ is our proposed conditional video generator. synthesizes a video sequence given an initial observation $x_0$ and text description c. To transform synthesized videos into executable actions, we use a trajectory-task conditioned policy $\\pi(x=0,c): X^T \\times C \\rightarrow \\Delta(A^{T-1})$. Decision-making is reduced to learning $\\phi$, which generates future image states based on language instructions. Action execution translates the synthesized video plan $[x_1,...,x_T]$ to actions. We employ an inverse dynamics model to infer necessary actions for realizing the video plan. This policy determines appropriate actions by taking two consecutive image observations from the synthesized video. For long-horizon planning, ChatGPT decomposes tasks into subgoals, each realized as a UPDP process. After generating a video chunk for a subgoal, we use the inverse dynamics model to execute actions. We sequentially generate video chunks for subgoals, updating TEMP-LORA throughout the long-horizon video generation process."}, {"title": "3.5 RETHINKING SLOW-FAST LEARNING IN THE CONTEXT OF COMPLEMENTARY LEARNING SYSTEMS", "content": null}, {"title": "Slow-Fast Learning as in Brain Structures", "content": "In neuroscience, the neocortex is associated with slow learning, while the hippocampus facilitates fast learning and memory formation, thus forming a complementary learning system where the two learning mechanisms complement each other. While slow learning involves gradual knowledge acquisition, fast learning enables rapid formation of new memories from single experiences for quick adaptation to new situations through one-shot contextual learning. However, current pre-training paradigms (e.g., LLMs or diffusion models) primarily emulate slow learning, akin to procedural memory in cognitive science. In our setting, TEMP-LORA serves as an analogy to the hippocampus."}, {"title": "TEMP-LORA as Local Learning Rule", "content": "It's long believed that fast learning is achieved by local learning rule (Palm, 2013). Specifically, given pairs of patterns (x, y) to be stored in the matrix C, the process of storage could be formulated by the following equation:\n$c = \\frac{x y^H}{\\mu}$\n(4)\nConsider a sequential memory storage process where learning steps involve adding input-output pairs $(x^t, y^t)$ to memory, the change $\\Delta c_{ij}$ in each memory entry depends only on local input-output interactions (Palm, 2013):\n$\\Delta c(t) = x(t) \\cdot y(t)$\nThis local learning rule bears a striking resemblance to LoRA's update mechanism.\nW' W+$\\Delta$W = W$_{slow}$ + W$_{fast}$ = $\\Phi$ + $\\Theta$\n(5)\n(6)\nWhere W$_{fast}$ is achieved by the matrix change, updated based on the current-iteration input and output locally as in Equation 3 ($\\Delta$W $\\leftarrow z_{0,i-1} \\oplus z_{0,i}$)."}, {"title": "Slow-Fast Learning Loop as a Computational Analogue to Hippocampus-Neocortex Interplay", "content": "The relationship between TEMP-LORA and slow learning weights mirrors the interplay between hippocampus and neocortex in complementary learning systems. This involves rapid encoding of new experiences by the hippocampus, followed by gradual integration into neocortical networks (McClelland et al., 1995). As in Klinzing et al. (2019), memory consolidation is the process where hippocampal memories are abstracted and integrated into neocortical structures, forming general knowledge via offline phases, particularly during sleep. This bidirectional interaction allows for both quick adaptation and long-term retention (Kumaran et al., 2016). Our slow-fast learning loop emulates this process, where W' = W + $\\Delta$W = W$_{slow}$ + W$_{fast}$. Here, W$_{fast}$ (TEMP-LORA) rapidly adapts to new experiences, analogous to hippocampal encoding, while W$_{slow}$ gradually incorporates this information, mirroring neocortical consolidation."}, {"title": "4 EXPERIMENT", "content": "For our experimental evaluation, we will focus on assessing the capabilities of our proposed approach with regard to two perspectives: video generation and video planning. We will detail our baseline models, datasets, evaluation metrics, results, and qualitative examples for each component. Please refer to the supplementary material for experimental setup, implementation details, human evaluation details, computational costs, more ablative studies and more qualitative examples."}, {"title": "4.1 EVALUATION ON VIDEO GENERATION", "content": "Baseline Models and Evaluation Metrics We compare our SLOWFASTVGEN with several baselines. AVDC (Ko et al., 2023) is a video generation model that uses action descriptions for training video policies in image space. Streaming-T2V (Henschel et al., 2024) is a state-of-the-art text-to-long-video generation model featuring a conditional attention module and video enhancer. We also evaluate Runway (Runway) in an off-the-shelf manner to assess commercial text-to-video generation models. Additionally, AnimateDiff (Guo et al., 2023b) animates personalized T2I models. SEINE (Chen et al., 2023) is a short-to-long video generation method that generates transitions based on text descriptions. iVideoGPT (Wu et al., 2024) is an interactive autoregressive transformer framework.\nWe tune these models (except for Runway) on our proposed dataset for a fair comparison.\nWe first evaluate the slow learning process, which takes in a previous video together with an input action, and outputs the new video chunk. We include a series of evaluation metrics, including: Fr\u00e9chet Video Distance (FVD), Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM) and Learned Perceptual Image Patch Similarity (LPIPS). We reserved a portion of our collected dataset as the test set, ensuring no scene overlap with the training set. We evaluate the model's ability to generate consistent long videos through an iterative process. Starting with an initial image and action sequence, the model sequentially generates video chunks by conditioning on the previous chunk and the next action. To assess the consistency of the generated sequence, we use Short-term Content Consistency (SCuts) Henschel et al. (2024), which measures temporal coherence between adjacent frames. We utilize PySceneDetect (PySceneDetect) to detect scene cuts and report their number."}, {"title": "4.2 EVALUATION ON LONG-HORIZON PLANNING", "content": "In this section, we show how SLOWFAST-VGEN could benefit long-horizon planning. We carefully design two tasks which emphasize the memorization of previous trajectories, in the domains of robot manipulation and game navigation respectively. We follow the steps in Section 4.2 for task execution and employ the slow-fast learning loop for these two specific tasks separately. We report the distance to pre-defined waypoints as well as the FVD of the generated long videos.\nRobot Manipulation We build a new task from scratch in the RLBench (James et al., 2019) environment. The task focuses on moving objects and then returning them to previous locations. We record the distance to the previous locations of the cubes. Table 4.2 shows that our model and Streaming-T2V outperform models without memories. Streaming-T2V has satisfying results since the appearance preservation module takes an anchor image as input, yet is still inferior to our model. Ablative results of \"Ours wo Loop\" also demonstrate the importance of the slow-fast learning loop.\nGame Navigation We develop a task in Minecraft that requires the gamer to retrace the initial path to return to a point. We define a set of waypoints along the way, and measure the closest distance to these waypoints."}, {"title": "5 CONCLUSION AND LIMITATIONS", "content": "In this paper, we present SLOWFAST-VGEN, a dual-speed learning system for action-driven long video generation. Our approach combines slow learning for building comprehensive world knowledge with fast learning for rapid adaptation and maintaining consistency in extended videos. Through experiments, we demonstrate SLOWFAST-VGEN's capability to generate coherent, action-responsive videos and perform long-horizon planning. One limitation is that our model could not suffice as an almighty world model as many scenarios are not covered in the dataset. Our method also introduces additional computation during inference as specified in the Supplementary Material."}, {"title": "A CONTRIBUTION STATEMENT", "content": "Yining Hong was responsible for all of the code development, paper writing, and experiments. She also collected the data for Minecraft.\nBeide Liu contributed to most of the data collection with regard to Unreal data. He was in charge of setting up the Unreal Engine, purchasing assets online, writing the Python scripts for automate agent control, and recording first-person and third-person videos of Unreal data.\nMaxine Wu collected the data of Google 3D Tiles. She was also responsible for the task setup of RLBench and the data collection of RLBench. She also curated part of the driving data.\nYuanhao Zhai wrote the codes for AnimateDiff, which was one of the baseline models.\nThe other people took on the advising roles, contributing extensively to the project by offering innovative ideas, providing detailed technical recommendations, assisting with troubleshooting code issues, and conducting multiple rounds of thorough paper reviews. They provided valuable expertise on video diffusion models. Zhengyuan Yang, Yingnian Wu and Liyuan Wang were involved in brainstorming and critical review throughout the project. Specifically, Zhengyuan Yang provided much technical support. Yingnian Wu came up with the idea of TEMP-LORA for modelling episodic memory as well as the masked video diffusion model. Liyuan Wang provided valuable insights throughout the project."}, {"title": "B MORE DETAILS ABOUT THE METHOD", "content": null}, {"title": "B.1 PRELIMINARIES ON LATENT DIFFUSION MODELS", "content": "Stable Diffusion (Rombach et al., 2022), operates in the compressed latent space of an autoencoder obtained by a pre-trained VAE . Given an input $x_0$, the process begins by encoding it into a latent representation: $z_0 = E(x_0)$ where E is the VAE encoder function. Noise is then progressively added to the latent codes through a Gaussian diffusion process:\nq(z_t|z_{t-1}) = N(z_t; \\sqrt{1 - \\beta_t}z_{t-1}, \\beta_tI)\n(7)\nfor t = 1, ..., T, where T is the total number of diffusion steps and $\\beta_t$ are noise schedule parameters. This iterative process can be expressed in a simpler form:\nz_t = \\sqrt{\\bar{\\alpha}_t} z_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon\n(8)\nwhere $\\epsilon \\sim N(0,I)$, $\\bar{\\alpha}_t = \\Pi_{i=1}^{t} a_i$, and $a_i = 1 - \\beta_i$. Stable Diffusion employs an $\\epsilon$-prediction approach, training a neural network $\\epsilon_{\\theta}$ to predict the noise added to the latent representation. The loss function is defined as:\nL = E_{t,x_0~P_{data},\\epsilon~N(0,1),c} [||\\epsilon - \\epsilon_{\\theta}(z_t, t, c) ||^2]\n(9)\nHere, c represents the conditioning (e.g., text), and $\\theta$ denotes the neural network parameters, typically implemented as a U-Net (Ronneberger et al., 2015).\nDuring inference, the model iteratively denoises random Gaussian noise, guided by the learned $\\epsilon_{\\theta}$, to generate latent representations. These are then decoded to produce high-quality images consistent with the given textual descriptions.\nVideo diffusion models (Ho et al., 2022b) typically build upon LDMs by utilizing a 3D U-Net architecture, which enhances the standard 2D structure by adding temporal convolutions after each spatial convolution and temporal attention blocks following spatial attention blocks."}, {"title": "B.2 PRELIMINARIES ON LOW-RANK ADAPTATION (LORA)", "content": "LORA Hu et al. (2021) transforms the fine-tuning process for large-scale models by avoiding the need to adjust all parameters. Instead, it utilizes compact, low-rank matrices to modify only a subset of the model's weights. This approach keeps the original model parameters fixed, addressing the problem of catastrophic forgetting, where new learning can overwrite existing knowledge. LoRA"}, {"title": null, "content": "utilizes compact, low-rank matrices to modify only a subset of the model's weights, therefore avoiding the need to adjust all parameters. In LoRA, the weight matrix $W \\in R^{m \\times n}$ is updated by adding a learnable residual. The modified weight matrix W' is:\nW' = W + $\\Delta$W = W + A$B^T$\nwhere $A \\in R^{m \\times r}$ and B $\\in R^{n \\times r}$ are low-rank matrices, and r is the rank parameter that determines their size. In this paper, we denote the LoRA finetuning as the fast learning process and the pre-training as slow learning process. The equation then becomes:\nW' = W + $\\Delta$W = W$_{slow}$ + W$_{fast}$ = $\\Phi$ + $\\Theta$\n(10)\nwhere $\\Phi$ corresponds to the pre-trained slow-learning weights, and $\\Theta$ corresponds to the LORA paraemters in the fast learning phase."}, {"title": "B.3 MODELSCOPET2V DETAILS", "content": "We base our slow learning model on ModelscopeT2V (Wang et al., 2023). Here, we introduce the details of this model.\nGiven a text prompt p, the model generates a video $v_{pr}$ through a latent video diffusion model that aligns with the semantic meaning of the prompt. The architecture is composed of a visual space where the training video $v_{gt}$ and generated video $v_{pr}$ reside, while the diffusion process and denoising UNet $\\epsilon_{\\theta}$ operate in a latent space. Utilizing VQGAN, which facilitates data conversion between visual and latent spaces, the model encodes a training video $v_{gt} = [f_1,..., f_F", "E(f_F)": "."}]}