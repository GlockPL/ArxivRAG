{"title": "Spatio-Temporal Context Prompting for Zero-Shot Action Detection", "authors": ["Wei-Jhe Huang", "Min-Hung Chen", "Shang-Hong Lai"], "abstract": "Spatio-temporal action detection encompasses the tasks of localizing and classifying individual actions within a video. Recent works aim to enhance this process by incorporating interaction modeling, which captures the relationship between people and their surrounding context. However, these approaches have primarily focused on fully-supervised learning, and the current limitation lies in the lack of generalization capability to recognize unseen action categories. In this paper, we aim to adapt the pretrained image-language models to detect unseen actions. To this end, we propose a method which can effectively leverage the rich knowledge of visual-language models to perform Person-Context Interaction. Meanwhile, our Context Prompting module will utilize contextual information to prompt labels, thereby enhancing the generation of more representative text features. Moreover, to address the challenge of recognizing distinct actions by multiple people at the same timestamp, we design the Interest Token Spotting mechanism which employs pretrained visual knowledge to find each person's interest context tokens, and then these tokens will be used for prompting to generate text features tailored to each individual. To evaluate the ability to detect unseen actions, we propose a comprehensive benchmark on J-HMDB, UCF101-24, and AVA datasets. The experiments show that our method achieves superior results compared to previous approaches and can be further extended to multi-action videos, bringing it closer to real-world applications. The code and data can be found in ST-CLIP.", "sections": [{"title": "1. Introduction", "content": "The task of spatial-temporal action detection is to detect people and recognize their respective actions in both space and time, which holds broad applications in various fields, including self-driving cars, sports analysis, and surveillance. Recently, the rise of 3D CNN backbones [5,25,26] has strengthened the capabilities of representation learning in spatial-temporal context, which has greatly improved the performance of action detection. Furthermore, some recent studies have extended their focus by incorporating attention-based relation modeling [4, 19, 24]. These approaches aim to model the relationship between individuals and their surrounding environment, including other people, objects, and the contextual scene. By integrating more interaction information into the person feature, a more comprehensive representation of their actions is achieved, consequently enhancing the accuracy of action classification. However, these methods primarily center around fully supervised learning, limiting their capability to detect only the action classes included in the training phase. In real-world applications, numerous actions beyond the training classes are bound to occur. Therefore, our approach aims to push the boundaries further by detecting unseen actions in zero-shot scenarios, alleviating the considerable labor-intensive efforts associated with the annotation process.\nIn recent years, visual-language models [11,20,33] have gradually become the models of choice in zero-shot video understanding due to their strong generalization capability. Nevertheless, the predominant focus in this domain is currently on video classification [12,18,27,28,30] and temporal action detection [12, 17], where the entire video or a short clip is considered for action classification. [9] is most similar to our goal, which is to detect individual unseen actions. However, the scenario they handle is too simple. They only process videos with single actions and target specific unseen labels, limiting their ability to effectively evaluate the robustness of the method. In contrast, our goal is to detect a variety of unseen actions and extend the method to videos containing multiple actions.\nTowards the aforementioned goal, we propose a novel framework called ST-CLIP, which adapts CLIP [20] to Zero-Shot Spatio-Temporal Action Detection in both visual and textual aspects, as shown in Figure 1. In terms of vision, we propose to utilize the visual knowledge embedded in CLIP to perform Person-Context Interaction. This approach enables us to grasp the relationship between individuals and their surrounding context without the necessity for additional interaction modules, thereby preserving the generalization capabilities of CLIP and streamlining the interaction modeling process. In the textual domain, given that the class names in the dataset offer limited semantic information, the ambiguity between different labels may degrade the quality of the classification results. Our objective is to enhance the textual content through effective prompting. Inspired by [18], we design a multi-layer Context Prompting module, which incrementally utilizes visual clues from spatio-temporal context to augment text descriptions, thereby increasing the discrimination capability. Furthermore, given that real-world scenarios often involve multiple individuals concurrently performing different actions, we further introduce Interest Token Spotting, which aims to identify context tokens most relevant to each person's actions. Subsequently, these tokens are utilized in the prompting process to generate a description that aptly captures each individual's situation.\nIn order to assess the effectiveness of our method, we refer to [9] and propose a more complete benchmark on J-HMDB, UCF101-24 and AVA datasets. For the first two datasets, we conduct cross-validation with varying train/test label combinations. This approach, as opposed to [9], which exclusively experiments with a specific label split, provides a more comprehensive assessment of the method's robustness. For experiments on AVA, where a single video may involve multiple actions, we randomly select certain videos that lack common classes for training, then the subsequent evaluation will focus on assessing the performance of detecting these unseen actions. The experimental results demonstrate that, in comparison to other zero-shot video classification methods, our approach exhibits superior performance on J-HMDB and achieves competitive results on UCF101-24. Furthermore, experiments on AVA demonstrate that our method can detect various unseen actions individually within the same video, affirming its potential extension to real-world applications. To summarize, our contributions are as follows:\n\u2022 We propose a novel method ST-CLIP that fully leverages the visual-language model to capture the relationship between people and the spatial-temporal context, without training extra interaction modules.\n\u2022 We devise a multi-layer Context Prompting module that employs both low-level and high-level context information to prompt class names, enriching the semantic content. In addition, we introduce an Interest Token Spotting mechanism to identify tokens most relevant to individuals for prompting, thereby generating text features that are unique to each person.\n\u2022 We propose a complete benchmark on J-HMDB, UCF101-24 and AVA datasets to evaluate performance on Zero-Shot Spatio-Temporal Action Detection. The experiments demonstrate the strong generalization capabilities of our method, and show the ability to individually detect unseen actions within the same video."}, {"title": "2. Related Work", "content": "Spatio-Temporal Action Detection. Typical action detection methods mostly use the two-stage pipeline, which means first localizing people in a video, and then performing action classification based on the features of these people. Most of these methods utilize additional person detectors like Faster R-CNN [21] to generate actor bounding boxes, which are used to perform RoIAlign [8] on the video features generated by the 3D CNN backbone to obtain the person features. While [5] directly utilizes naive actor features to classify actions, [4, 19, 24] further exploit relation modeling to combine more information about human and environmental interactions. Besides, some methods [1, 6, 23,31] optimize the two-stage networks by a joint loss in an end-to-end framework. Recently, there are query-based detectors that simultaneously predict the actor positions and their corresponding actions in a single-stage manner. [35] utilizes a set of tubelet-queries to sample features from video representation, while [29] samples discriminative features based on the multi-scale feature map. Although the above works have made significant progress in traditional fully-supervised scenarios, our goal is to better recognize unseen actions through zero-shot learning."}, {"title": "Video Understanding with Visual-Language Models.", "content": "Recently, large-scale visual-language models such as CLIP [20], ALIGN [11], and Florence [33] have demonstrated their usability to different visual-language tasks including image captioning [16], video-text retrieval [3] and scene text detection [32]. Due to a shared feature space that effectively aligns the visual and text domains, an increasing number of methods [12, 18, 27, 28, 30] choose to perform zero-shot video classification based on these foundation models. The main focus of these works is to design temporal modeling to adapt the image encoder to the video domain and to develop ways to prompt the text. On this basis, [9] processes zero-shot spatio-temporal action detection to further subdivide the scope of action classification to the individual level. They proposed extracting people and objects in the image and using different interaction blocks to model the relationship between them. Besides, they will use each person's interaction feature to prompt labels. To evaluate their performance, they selected specific unseen actions to detect on the two datasets, J-HMDB and UCF101-24. However, this benchmark is still not close enough to real-world scenarios. First, they did not extensively test a variety of unseen labels. Second, the videos in both datasets contain only single actions. Considering this, we propose a more complete benchmark to evaluate performance on multi-action videos, aiming for more practical applications."}, {"title": "3. Proposed Method", "content": "As our method exploits the pretrained knowledge of the CLIP model, we briefly review the formulation of image and text encoders in this section. For the image encoder of ViT architecture [2], given an image $I \\in \\mathbb{R}^{H \\times W \\times 3}$ with height H and width W, it will be split into $N = \\frac{H}{P} \\times \\frac{W}{P}$ patches, where the patch size is $P \\times P$, then each patch will obtain its token through patch embedding. In this process, the Conv2D with kernel size $P \\times P$ and output channel size D will be used to generate patch tokens $x \\in \\mathbb{R}^{N \\times D}$. After that, an extra learnable token $X_{cls} \\in \\mathbb{R}^{D}$ is concatenated for classification. The input tokens for the transformer encoder layer is given by:\n$X = [X_{cls}, x_{1}, x_{2},..., x_{N}] + e$  (1)\nwhere $e$ is the positional encoding. The classification token $X_{cls}$ output from the last encoder layer is often regarded as an image feature. Similarly, the text encoder is also a transformer architecture, while it first tokenizes the text into embeddings, and then uses the EOS token of the last encoder layer output as the text feature."}, {"title": "3.2. Overall Architecture", "content": "The overview of our ST-CLIP is shown in Figure 2. As a two-stage framework, our model takes the person detected from video frames by a human detector as input and outputs the corresponding action classification results. Given an image with the person bounding boxes, we first extract these portions and obtain person-specific tokens through the image encoder. In this process, we utilize the adapter to make these person tokens more suitable for subsequent interaction modeling, which will be discussed later. Given the continuous nature of actions, in addition to considering a person's information, we sample neighboring frames to construct a spatial-temporal context, thereby capturing information across different spaces and times. To obtain these context tokens, we conduct temporal modeling on the patch tokens of different frames, enabling the tokens to aggregate information over this period of time.\nSubsequently, to fully leverage CLIP's visual knowledge, we jointly input person and context tokens into the image encoder. The Multi-Head Self Attention (MHSA) in each encoder layer is employed to guide us in achieving the following three objectives: (i) performing further spatial modeling on the input context tokens to obtain spatial-temporal tokens. (ii) modeling person-person and person-context interaction through the mutual influence between tokens. (iii) identifying the interest tokens most relevant to each person's actions through attention weight. On the textual side, we initially utilize the CLIP text encoder to generate the original text features for class names. Then, followed by each image encoder layer, the Context Prompting layer will use context tokens to prompt each label. In this process, considering that the videos in J-HMDB and UCF101-24 only contain single action, we can treat all context tokens as relevant to this action. Hence, we use all context tokens for prompting, resulting in everyone in the same frame sharing the same text features. However, in AVA, to discern different actions by multiple individuals, we utilize context tokens that each person deems important (i.e., interest tokens) to prompt their respective text features. Finally, we use the person tokens and label features output by the last image encoder layer and Context Prompting layer to calculate the cosine similarities, which are used as the action classification scores.\nIn the training process, in order to retain the generalization capability of CLIP for zero-shot tasks, we freeze the pretrained weights in the image and text encoders, and only train our additional learnable modules. Besides, we insert the LORA trainable matrices into the Feed-Forward Network (FFN) of each image encoder layer, which can further adapt CLIP model to detect actions without affecting its well-aligned visual-language features."}, {"title": "3.3. Person-Context Interaction", "content": "As mentioned earlier, to utilize spatial-temporal context and facilitate the recognition of continuous actions, we sample T neighboring frames before and after the current frame. We first conduct temporal modeling on these frames to consolidate information at different times. Subsequently, we leverage the spatial modeling capability of the image encoder to further fuse these visual contents in both space and time, which results in the generation of spatial-temporal tokens. Our temporal modeling is shown in Figure 3. First, we use CLIP's pretrained patch embedding to obtain the patch tokens of each frame $X_{t} = [X_{t,1}, X_{t,2},..., X_{t,N}] \\in \\mathbb{R}^{N \\times D}$, where $t \\in \\{1,...,T\\}$ denotes the frame index, N is the number of patch tokens, and D is the token dimension. Then we gather the tokens of each frame into $[X_{1}, X_{2}, \\ldots, X_{T}] \\in \\mathbb{R}^{T \\times N \\times D}$. After that, we utilize MHSA to model the relationship between patch tokens at the same position in different frames $Z_{i} = [X_{1,i}, X_{2,i},..., X_{T,i}] \\in \\mathbb{R}^{T \\times D}$, where $i \\in \\{1, ..., N\\}$, as follows:\n$\\begin{aligned}\nZ_i &= Z_i + e^{temp} \\\\\n\\tilde{Z_i} &= Z_i + MHSA(LN(Z_i)) \\\\\nZ_i &= AvgPool(\\tilde{Z_i}),\n\\end{aligned}$  (2)\nwhere $e^{temp}$ is the temporal encoding and LN stands for layer normalization. After temporal modeling, we can obtain context tokens $Z_{i} \\in \\mathbb{R}^{D}$ at each position $i\\in \\{1, ..., N\\}$ that have aggregated temporal information, which will be used to model the interaction with person.\nRegarding the person tokens, we initially utilize the image encoder to obtain features for each person. Considering that these person features have undergone multiple transformer encoder layers, they are relatively high-level compared to the aforementioned context tokens, which are only at the patch-embedding level. To enhance the utilization of self-attention in modeling relationships among all tokens, we employ an adapter to adjust person features to the same level as other context tokens. The adapter is a straightforward two-layer FFN commonly used in the transformer encoder layer, and we have found it to be effective in adapting these person features. Specifically, we generate person tokens in the following ways:\n$P_{i} = P_{i} + FFN(LN(P_{i}))$  (3)\nwhere $P_{i}$ is the person feature output from the image encoder and $P_{i}$ is the person token we will use for subsequent interaction modeling."}, {"title": "3.4. Interest Token Spotting", "content": "In a multi-action dataset like AVA, the same timestamp may encompass various actions performed by multiple individuals, and all the context tokens will contain information about different actions. Therefore, we introduce Interest Token Spotting, a mechanism that employs a personal perspective to extract context tokens most relevant to each individual's actions. Subsequently, we utilize these tokens for prompting to generate personalized text features. In this process, we also leverage CLIP's pretrained visual knowledge to find each person's interest tokens. To be more specific, we exploit the attention weight calculated by the MHSA in each image encoder layer as an indicator of the token importance. The Multi-Head Self Attention will first calculate an attention map $M_{i} \\in \\mathbb{R}^{C \\times C}$ based on the query and key of each head, where $i \\in \\{1, ..., num\\_heads\\}$, and C is the number of input tokens. Then, we average the attention maps of each head to obtain an importance score matrix $M \\in \\mathbb{R}^{C \\times C}$. In this matrix, each row represents the importance of every token to a certain token. For instance, $M(i, j)$ represents how important token $j$ is to token i. Based on this, we use a person's token as the row index, and select the top K highest among all C importance scores. These selected K indexes are the token positions that the person is interested in. After that, we pass each token through the MHSA and FFN of this encoder layer, and use the selected K indexes to obtain the interest tokens."}, {"title": "3.5. Context Prompting", "content": "The Context Prompting layer primarily consists of Cross-Attention (CA), where text features serve as the query, and the context tokens act as key and value. This setup facilitates the gradual absorption of visual information into the text content. In AVA, we further narrow down the scope of the extracted information for each person to their personal interest tokens. Given an image with B detected individuals, we need to classify their actions into NL possible labels. First, we assign the same set of original text features which are obtained by the CLIP text encoder to these B people. Subsequently, we employ a linear projector to project each person's interest tokens to the same dimension as the text features. Following this, both the interest tokens and the text features are sent to the Context Prompting layer for prompting. The following equations describe how it works:\n$\\begin{aligned}\nF_{T} &= F_{T} + CA(F_{T}, F_{I}), \\\\\nF_{T} &= F_{T} + FFN(F_{T}), \\\\\nF_{T} &= F_{T} + p F_{T}\n\\end{aligned}$  (4)\nwhere $F_{T} \\in \\mathbb{R}^{B \\times N_{L} \\times D}$ consists of the text features, $F_{I} \\in \\mathbb{R}^{B \\times K \\times D}$ are the interest tokens, $p \\in \\mathbb{R}^{D}$ is a learnable weight vector, and $F_{T}$ will be input to the next prompting layer. For J-HMDB and UCF101-24, we straightforwardly use all context tokens for prompting, making all B individuals in an image utilize the same text features $F_{T} \\in \\mathbb{R}^{N_{L} \\times D}$ for similarity calculation."}, {"title": "4. Datasets and Benchmarks", "content": "We establish benchmarks for zero-shot spatial-temporal action detection on three popular datasets: J-HMDB, UCF101-24, and AVA. For the first two datasets, we further extend the settings of [9] to include more diverse unseen actions. Besides, we also build benchmark on AVA which is more representative of real-world scenarios. We use frame mAP with 0.5 IoU threshold in all the benchmarks for evaluation. More details about the label splits are described in the supplementary materials."}, {"title": "4.1. ZS-JHMDB", "content": "J-HMDB dataset [10] is a subset of the HMDB51 dataset. It has 21 classes and 928 videos. The videos are trimmed and there are totally 31,838 annotated frames in these videos. To assess the generalization capability of an action detection method, the zero-shot evaluation necessitates that the model has not seen samples related to test classes during the training process, which means the training and testing labels are disjoint. In this scenario, we refer to the evaluation settings proposed by [9], which exploits random sampling to take 75% action classes for training, and the remaining 25% for testing. However, they only employ a specific label split for evaluation, which is inadequate for fully measuring the effectiveness of the method. Instead, we perform cross-validation on multiple label splits as follows: we split J-HMDB into 4 label splits, each split has 15 training classes and 6 testing classes, and the testing classes in each split are disjoint (part of split 4 will overlap with split 1). The split 1 is the same as the split used in [9]."}, {"title": "4.2. ZS-UCF", "content": "UCF101-24 dataset is a subset of UCF101 [22]. It consists of 3207 videos from 24 action classes, and each video contains a single action. In this benchmark, we employ the same setting as in ZS-JHMDB, which also divides all classes into 75% for training and 25% for evaluation. The label split 1 is also the same as used in [9]."}, {"title": "4.3. ZS-AVA", "content": "AVA [7] is a large-scale action detection dataset which contains multiple actions within a single video. It consists of 235 videos for training and 64 videos for validation. Each video lasts for 15 minutes and is annotated by sampling one keyframe per second. For AVA, since the same video contains multiple actions, and some action categories exist in multiple videos, it becomes challenging to find a sufficient amount of training data and testing data with disjoint labels. Instead, we randomly select some training videos, ensuring that they all lack samples of the same classes. These missing classes are then treated as unseen classes for evaluation. During the evaluation phase, we test all classes in the validation videos, but the focus is solely on evaluating the performance of unseen classes. Under this setting, we propose three splits. When the amounts of training and testing data for these splits are nearly the same, we select different combinations of three action types - pose action, object interaction, and person interaction as unseen classes, allowing for a more comprehensive evaluation."}, {"title": "5. Experiments", "content": ""}, {"title": "5.1. Experimental Setup", "content": "Person Detector: In the following experiments, we employ Faster R-CNN [13] with a ResNet-50-FPN [14] backbone pretrained on MSCOCO [15] for person detection. For J-HMDB and AVA, we directly inference on the test data with pretrained person detector. While for UCF101-24, since the images have lower resolution, we further use the ground-truth of training classes to finetune the person detector for 10 epochs in each label split. Besides, to remove the false positives, we keep the detected box with the highest confident score S in each frame, then we select the boxes with scores higher than S - T from the rest, where T is 0.001 for J-HMDB and 0.7 for both UCF101-24 and AVA.\nHyperparameters: For all the experiments on J-HMDB and UCF101-24, we employ ViT-B/16 as our CLIP backbone and use the same hyper-parameters as follows: Training for 3K iterations with batch size of 8. We use SGD as our optimizer and the base learning rate is set to 2.5e-4. As for AVA, we use ViT-L/14 backbone and train the model for 20K iterations with a base learning rate of 4e-4."}, {"title": "5.2. Zero-Shot Spatial-Temporal Action Detection", "content": "Table 1 shows our results on detecting unseen actions in AVA. The main distinction between our method and the video classification approaches is our capability to detect different unseen actions within the same video, which are difficult to achieve. Firstly, in the training process of those methods, providing a fixed video label is challenging due to the presence of numerous different actions. Additionally, during the inference stage, these methods tend to detect that everyone in the video has the same action. To study the feasibility of the video classification method, we further narrowed the scope of classification from the entire video to tracklets to avoid misclassifying different actions into the same category. Besides, considering the limited prior research in this area, we follow [9] to implement a naive baseline for comparison with our method. For a frame with detected individuals, the baseline utilizes the pretrained image encoder of CLIP to extract the image feature of this frame. Subsequently, it calculates the cosine similarities with the text features of each class name, which are then considered as the action classification scores for these individuals. Since the baseline regards people in the same frame as having the same actions, we also implement the person crop method. This involves cropping out parts of each person to obtain their respective image features for classification. Nevertheless, this cropping method may result in reduced performance for the baseline in split 1 and split 2, as it captures insufficient information. The results show that our ST-CLIP achieves the best average performance, whether using detected boxes or ground-truth boxes, demonstrating its effectiveness in detecting individual unseen actions.\nWe present the experimental results on ZS-JHMDB and ZS-UCF in Tabs. 2 and 3. Firstly, since each video in both datasets contains only a single action, a straightforward approach is to conduct action detection through zero-shot video classification. These methods can initially classify the entire video into an action class and then consider all detected individuals in the video as performing this action. However, since our method concentrates on detecting different actions for each person, individuals in the same video may be classified into different actions. This general setting can result in other video classification methods having an advantage over us in these two datasets. For a fair comparison with the other methods, we further adopt the assumption that a video contains only one action. In this context, we perform soft voting on each person's classification score, extending our method to suit this scenario.\nWe first present the results on ZS-JHMDB in Table 2. Regardless of whether the assumption is applied, our ST-CLIP outperforms others in most label splits, and thus achieves the best average performance, along with 3.82 mAP higher than the state-of-the-art method [28]. Additionally, by using ground-truth bounding boxes to eliminate localization errors, our average performance improves to 90.12 mAP, which is 4.1 mAP higher than [28].\nTable 3 presents the results on ZS-UCF. Firstly, without the assumption, our method exhibits a 2.31 mAP improvement over [9], which also focuses on detecting individual actions. It is worth mentioning that the localization errors have a noticeable impact on our performance in this dataset. Since there are instances where irrelevant people who are not performing actions are detected, these individuals should be considered as part of the background. However, these false positive cases will also contribute to the soft voting process, leading to our classification results being slightly inferior to other methods that solely rely on sampled frames to determine video labels. In this case, our method still outperforms [12, 27], and exhibits similar performance to [18]. With ground-truth bounding boxes, our method demonstrates the second-best average performance, surpassing [18] and achieving results comparable to [28]. The results on ZS-JHMDB and ZS-UCF show that our ST-CLIP can perform competitively with other video classification methods when processing single-action videos."}, {"title": "5.3. Ablation Study", "content": "We present an ablation study in Table 4 to investigate different design choices in our method. The experiment in Table 4b is performed on the split 1 of ZS-AVA. Tabs. 4c to 4g are conducted on the label split 1 of ZS-JHMDB, and Table 4a is performed on both.\nProposed components: We first investigate the importance of each component in Table 4a. On J-HMDB, our proposed Person-Context Interaction effectively models the relationship between individuals and their surroundings, resulting in a 2.35 mAP improvement compared to the baseline. Furthermore, the Context Prompting module leverages context information to enhance text content, leading to an additional improvement of 7.57 mAP. On AVA, the above two components also demonstrate their effectiveness. Additionally, in multi-action videos, our Interest Token Spotting can identify context tokens most relevant to individual actions for prompting, further enhancing performance.\nInterest tokens: In table 4b, we conduct experiments using different numbers of interest tokens to observe their impact on the results. Our findings indicate that in multi-action videos, introducing too many tokens can potentially sample background noise unrelated to the action, thereby impacting the effectiveness of prompting.\nComparison with iCLIP [9]: We demonstrate the advantages of our method compared to [9] in Table 4c. Without prompting, our method performs slightly better than [9], and we do not need to use additional object detectors and interaction blocks in the interaction modeling process. In addition, our prompting strategy uses multiple levels of context tokens to augment text content, resulting in an improvement of 7.57 mAP. However, the prompting method of [9] relies heavily on the results of interaction modeling, which limits their performance improvement.\nLORA in FFN: In Table 4d, we investigate the impact of LORA ranks. The results show that when we additionally train learnable matrices with rank 8, we can perform better than relying solely on CLIP's pretrained weight.\nPerson tokens: Table 4e explores different ways of generating person tokens. Initially, the simple approach of pooling over the embeddings of all patches in the person crop fails to deliver satisfactory performance. Furthermore, equipping adapter can perform better than using person features at the image encoder level. This demonstrates that our adapter can effectively adapt person tokens, making them more suitable for the subsequent person-context interaction.\nContext tokens: Table 4f shows the importance of temporal modeling. Using only the current frame to obtain context tokens results in the worst performance, indicating that aggregating temporal information is beneficial when identifying continuous actions. Additionally, equipping only a 1-layer MHSA can significantly improve performance compared to simple average pooling.\nContext prompting: The results in Table 4g show the effectiveness of our prompting strategy. The results demonstrate that our prompting method, which utilizes tokens from low-level to high-level, yields better outcomes compared to using only high-level tokens from the last layer."}, {"title": "6. Conclusion", "content": "In this paper, we explore zero-shot spatio-temporal action detection. We propose a complete benchmark on J-HMDB, UCF101-24 and AVA. Besides, we propose a method to adapt the visual-language model for this task. The Person-Context Interaction employs pretrained knowledge to model the relationship between people and their surroundings, and the Context Prompting module utilizes the visual information to augment the text content. To address multi-action videos, we further introduce the Interest Token Spotting mechanism to identify the visual tokens most relevant to each individual action. The experiments demonstrate that our method achieves competitive performance compared to other video classification methods and can also handle multi-action videos."}, {"title": "Supplementary Material", "content": "In the supplementary material, we present additional experimental results to substantiate the efficacy of our method and provide more details on the experimental settings. Initially, we elaborate our proposed benchmark for Zero-Shot Spatio-Temporal Action Detection in Sec 1. Subsequently, we showcase a visualization depicting interest tokens on ZS-AVA in Sec. 2. Then, in Sec. 3, we illustrate the distribution of text features on both ZS-JHMDB and ZS-UCF to assess the impact of prompting. We then provide the complexity analysis of our method and others in Sec. 4, and the results using ground-truth bounding boxes on each benchmark in Sec. 5. Finally, we discuss some limitations of our approach in Sec. 6, and give the implementation details of other methods in Sec. 7."}, {"title": "1. Label Splits Details", "content": "In this section, we provide details of each label split used in our benchmarks. For ZS-JHMDB and ZS-UCF, we perform cross-validation on 4 label splits to assess the efficacy of our method. Each label split of ZS-JHMDB has 15 training classes and 6 testing classes, and each split of ZS-UCF has 18 training classes and 6 testing classes. More specifically, within each label split, there are 6 classes for testing, and all the remaining classes in the dataset are designated as training classes. In each label split, we follow the official split 1 of the two datasets, J-HMDB and UCF101-24, obtaining the training videos of the training classes and the testing videos of the test classes.\nFor ZS-AVA, to ensure an adequate volume of training data, we refrain from utilizing classes that frequently appear in most training videos as unseen classes. Our objective is to diversify each split by incorporating various types of unseen classes, including pose actions, object interactions, and person interactions. The split 1 contains 5 pose actions and 13 object interactions, split 2 contains 2 pose actions, 6 object interactions and 4 person interactions, and the split 3 contains 5 object interactions and 1 person interactions."}, {"title": "2. Interest Tokens", "content": "We first explore the effects of utilizing only interest tokens to prompt labels, as opposed to incorporating all context tokens. In Figure 4, we showcase the current frame along with a bounding box, indicating our objective of detecting the person's action. Additionally, we include neighboring frames to facilitate the observation of changes in the video over this period. Since the context tokens have not undergone spatial modeling in the first image encoder layer, we choose to visualize the results obtained from this layer to improve the interpretability of the identified interest tokens. In Figure 4a, the target person is engaged in the action of brushing teeth, while another person undertaking distinct actions is highlighted within the circle. In this scenario, our Interest Token Spotting mechanism can identify the context tokens most relevant to the action of brushing teeth. This ensures that the prompting process selectively incorporates these crucial visual clues, enhancing the focus on pertinent information. Likewise, in Figure 4b, the individual highlighted as the target person is engaged in fishing, while the person within the circled area is swimming. In this instance, the identified interest tokens exclude information associated with swimming, consequently enhancing the quality of the prompting process. Furthermore, in comparison to utilizing all context tokens for prompting, the use of only interest tokens can elevate the confidence score for the \"brush teeth\" action from 0.34 to 0.45, and the score of \"fishing\" can also be improved from 0.30 to 0.34. The results demonstrate that even when dealing with an unseen action not encountered during the training process, our method excels in identifying information most relevant to this action within a multi-person environment.\nWe provide more visualization results in Figure 5. In Figure 5a and 5b, when the action performed by the target person has limited relevance to others, the identified interest tokens will exhibit reduced emphasis on areas involving other people. Conversely, in Figure 5c and 5d, where the person's actions interact with others, our method adeptly recognizes context tokens within other people's areas as interest tokens, effectively extracting relevant information from those regions. Moreover, our Interest Token Spotting possess the capability to identify crucial objects, enhancing our ability to recognize actions, such as the chair and the computer in Figure 5a, and the cellphone in Figure 5d."}, {"title": "3. Text Features Distribution", "content": "As class names inherently carry limited semantic information, relying solely on the text features of these words to calculate similarity with person tokens may introduce ambiguity, potentially impacting the accuracy of action classification. To examine the distribution of text features in the feature space, we employ Principal Component Analysis (PCA) to reduce each feature to two dimensions. Subsequently, we illustrate the text features distribution of the CLIP text encoder and various Context Prompting layer outputs in Figure 6.\nInitially, the results reveal that regardless of"}]}