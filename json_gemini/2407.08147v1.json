{"title": "Looks can be Deceptive: Distinguishing Repetition Disfluency from Reduplication", "authors": ["Arif Ahmad", "Mothika Gayathri Khyathi", "Pushpak Bhattacharyya"], "abstract": "Reduplication and repetition, though similar in form, serve distinct linguistic purposes. Reduplication is a deliberate morphological process used to express grammatical, semantic, or pragmatic nuances, while repetition is often unintentional and indicative of disfluency. This paper presents the first large-scale study of reduplication and repetition in speech using computational linguistics. We introduce IndicRedRep, a new publicly available dataset containing Hindi, Telugu, and Marathi text annotated with reduplication and repetition at the word level. We evaluate transformer-based models for multi-class reduplication and repetition token classification, utilizing the Reparandum-Interregnum-Repair structure to distinguish between the two phenomena. Our models achieve macro F1 scores of up to 85.62% in Hindi, 83.95% in Telugu, and 84.82% in Marathi for reduplication-repetition classification.", "sections": [{"title": "1 Introduction", "content": "Speech recognition technology significantly enhances accessibility for individuals with disabilities by improving their interaction with technology and overall quality of life (Green et al., 2021; MacArthur and Cavalier, 2004; Noyes and Frankish, 1992). It also plays a vital role in consumer electronics, where Automatic Speech Recognition (ASR) technologies like Siri, Google Assistant, and Amazon Alexa enrich user experiences through voice-activated controls (Buteau and Lee, 2021; Juang and Rabiner, 2005).\nResearch shows that speech disfluencies, such as repetitions, can notably increase Word Error Rates (WER) by up to 15% (Goldwater et al., 2008). Addressing these disfluencies in ASR systems can improve performance, as demonstrated by enhancements in Machine Translation (MT) systems' BLEU scores (Cho et al., 2014). This paper focuses on repetition a type of disfluency characterized by the unintended recurrence of words or phrases, which typically occurs during moments of cognitive processing, such as recalling a word or structuring a thought (Tree, 1995).\nInterestingly, repetition shares structural similarities with reduplication a deliberate linguistic process used globally to alter word meanings, indicating attributes like plurality or intensity. While both processes involve repetition, their functions and implications differ significantly, with reduplication playing a grammatical and semantic role in languages and repetition often marking interruptions in speech flow (Newman, 2000; Bauer, 2003; Xu, 2012; Kajitani, 2005).\nGiven the prevalence of both reduplication and repetition in spontaneous speech, and their significant implications for language technologies, this study introduces a novel dataset named \"IndicRedRep\". This dataset fo-"}, {"title": "2 Related Work", "content": "Reduplication and repetition are well-studied phenomena in the domains of morphology and speech disfluencies, respectively. The related work is organized into two main themes: a) computational models and frameworks developed for processing reduplication in multiword expressions, b) approaches and models employed for detecting and addressing repetition as speech disfluency. Here, we discuss the literature most relevant to our work and establish the relationship between them."}, {"title": "2.1 Reduplication as Multiword\nExpression", "content": "Multiword expressions (MWEs) are a cornerstone of linguistic studies and pose significant challenges in natural language processing (NLP) due to their complex, non-compositional nature. Recent research highlights a framework for integrating MWE processing into NLP systems to improve linguistic understanding (Baldwin and Kim, 2010; Sag et al., 2002).\nSignificant efforts have been made to computationally address reduplication across languages such as Bengali, Cantonese, Mandarin Chinese, Indonesian, Sanskrit, Hindi, and Marathi (Chakraborty and Bandyopadhyay, 2010; Lam, 2013; Chen et al., 1992; Mistica et al., 2009; Kulkarni et al., 2012; Singh et al., 2016). Notable computational models include the use of two-way finite-state transducers (2-way FSTs) and finite-state buffered machines (FSBMs), which effectively model reduplicative processes (Dolatian and Heinz, 2018; Wang, 2021). Additionally, integrating reduplicated expressions into machine translation has enhanced translation accuracy (Doren Singh and Bandyopadhyay, 2011). The creation of the RedTyp database marks a significant advancement in the cataloging of reduplicative morphemes, aiding both theoretical and computational studies (Dolatian and Heinz, 2019). While these studies offer significant theoretical insights, no previous work has released a large-scale dataset specifically for the study of reduplication and repetition."}, {"title": "2.2 Repetition as Speech Disfluency", "content": "Repetition is a well-known speech disfluency often observed in spontaneous and unscripted speech (Shriberg, 1994). It refers to the unintentional recurrence of words, phrases, or sounds, which may occur due to hesitations, corrections, or cognitive processing.\nIt is tackled using various computational techniques aimed at enhancing speech recognition and processing. These techniques include Sequence Tagging, Parsing-based, and Noisy Channel models, each leveraging different aspects of machine learning and syntactic analysis (Liu et al., 2006; Georgila et al., 2010; Ostendorf and Hahn, 2013; Zayats et al., 2016, 2014; Ferguson et al., 2015; Wang et al., 2018, 2020; Qian and Liu, 2013; Hough and Schlangen, 2015; Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014; Wu et al., 2015; Yoshikawa et al., 2016; Jamshid Lou and Johnson, 2020; Johnson and Charniak, 2004; Zwarts and Johnson, 2011; Jamshid Lou and Johnson, 2017). Inspired from these works, we move forward with sequence tagging based modeling as this approach has its merits of allowing direct and explicit tagging of disfluencies at the word level, which enables fine-grained detection and classification, critical for developing robust speech recognition systems."}, {"title": "3 Background and Definitions", "content": "In this section, we define reduplication and repetition, discussing their roles in language and speech. Understanding these definitions is essential for recognizing the differences between these two linguistic phenomena, which is a key focus of this study."}, {"title": "3.1 Reduplication.", "content": "Reduplication is a morphological process in which a part or the entirety of a word's phonological material is systematically repeated, carrying semantic or grammatical significance. This mechanism is prevalent across numerous global languages, serving diverse linguistic purposes including (plurality, distribution, intensity, aspect (continued or repeated occurrence), reciprocity and more. (Rubino, 2005; Spaelti, 1997).\nExamples of complete reduplication in Hindi sentences:"}, {"title": "3.2 Repetition", "content": "Repetition is a type of Speech Disfluency. Speech Disfluencies are geneally defined as phenomena that interrupt the flow of speech and do not add propositional content to an utterance. Honal and Schultz (2003) describes five types of disfluencies, including Flase Start, Repetition, Editing Term, Filler Word, Interjection. In this paper, we focus on the repetition type of disfluency. Repetition, refers to the unintentional recurrence of whole words, phrases, or segments during spontaneous speech. This form of disfluency often occurs when speakers are trying to recall a word, grappling with a complex thought, or deciding how to phrase something (Tree, 1995).\nExamples of word repetition disfluencies:"}, {"title": "4 IndicRedRep Dataset", "content": "This section discusses the formation of the IndicRedRep dataset, which includes data collection, annotation, and key statistics across three Indic languages: Hindi, Marathi, and Telugu, focusing on token-level labels for reduplication and repetition. Hindi resources are more plentiful, necessitating different collection strategies compared to Marathi and Telugu."}, {"title": "4.1 Data Collection", "content": "To the best of our knowledge, there currently exists no dataset explicitly annotated for both reduplication and repetition. We employed the GramVaani (GV) corpus, a spontaneous telephone speech corpus in Hindi, to establish the Hindi subset of the dataset, addressing the lack of datasets annotated for reduplication and repetition(Deekshitha et al., 2022). For Marathi and Telugu, where similar datasets are absent, we extrapolated from the Hindi data using the Gemma Instruction Tuned models(Team et al., 2024) for sentence generation and engaged native speakers for manual creation of test sets.\nIt was important to use a dataset containing spontaneous speech rather than read speech, as disfluencies are more commonly observed in spontaneous speech. However, in datasets such as the Shrutilipi corpus (Bhogale et al., 2023), Indian Language Corpora (Abraham et al., 2020), and Mozilla Common Voice (Ardila et al., 2020), which predominantly feature read speech, the majority of word duplications are the result of either reduplication or transcription errors. True instances of repetition were significantly rarer in these sources."}, {"title": "4.2 Annotation and Quality Control\nProcess", "content": "The collected data was annotated by three trained linguists, who are language majors with a focus on Hindi. They noted the poor quality and many errors in the transcripts of the GramVaani (GV) corpus. Consequently, data annotation occurred in two stages: initially correcting the speech transcripts and subsequently labeling the tokens as reduplication, repetition, or other.\nGiven the noisy annotations in the GV corpus, which often skipped repetition words, we filtered the corpus to select sentences likely containing reduplication or repetition. After filtering, these sentences were reannotated to properly account for missed instances. This resulted in a clean, ground truth dataset with real-world instances of reduplication and repetition in Hindi, which were then labeled at the token level. For Marathi and Telugu, we used the cleaned and annotated Hindi subset to bootstrap sentence generation with the Gemma Instruction Tuned models (Team et al., 2024). Native speakers of Marathi and Telugu were later engaged to manually create sentences for the test sets, using the Hindi data as a reference.\nAnnotation guidelines were developed based on existing works (Murthy et al., 2022) and are detailed in Appendix B. To ensure consistency, interannotator agreement was assessed using Fleiss' kappa, resulting in an agreement level of 83.29%, indicating substantial consistency. Quality control included independent re-annotation by multiple annotators, with discrepancies resolved during regular meetings (Sabou et al., 2014). Additionally, the Gemma prompting details used for generating Marathi and Telugu sentences are included in Appendix C."}, {"title": "4.3 Dataset Statistics", "content": "The GramVaani corpus, inherently rich in colloquial expressions and spontaneous speech patterns, provided an ideal foundation for our specific annotations."}, {"title": "4.4 Data Splits", "content": "The data was divided into training, validation, and test sets following the standard 80:10:10 ratio. The splits were stratified to ensure that the distribution of reduplication and repetition instances was similar across all subsets."}, {"title": "5 Modelling", "content": "In this section, we detail our approach to differentiate between reduplication and repetition, with a particular focus on utilizing the Reparandum-Interregnum-Repair (RiR) structure. We believe that by considering the context surrounding the repeated elements, we can disambiguate intricate cases where reduplication and repetition coexist. This is also supported by analisysis of the disfluency structure by Shriberg (1994), which we discuss in detail here."}, {"title": "5.1 RiR Structure", "content": "Shriberg (1994) describes the structure of disfluencies, to consist of four parts: Redarandum, Interruption Point, Interregnum, and Repair. Figure 1 shows an eaxmple of a disfluency following this structure in English as well as in Hindi. Interruption Point is equivalent to the \u201cmoment of interruption\" and is not explicity present in the transcript, but a part of the speech signal. Hence, we donot capture it in our modelling strategy.\nThe Reparandum-Interregnum-Repair structure serves as the foundation of our classification methodology. It captures the distinctive patterns associated with reduplication and repetition:"}, {"title": "5.2 Importance of the RiR Structure", "content": "Our motivation for incorporating the RiR structure into our classification approach stems from its ability to disentangle complex linguistic structures especially when both reduplication and repetition are present in the same sentence. We believe that the following examples illustrate the significance of considering the RiR structure:\nThe examples given below follow the standard scheme:\n[ reparandum + {interregnum} repair ]"}, {"title": "5.2.1 Examples", "content": "\u2022 Example 2:\n\u0935\u0939 [\u0928\u0940\u0932\u093e + { \u0928\u0939\u0940\u0902 } \u0928\u0940\u0932\u093e \u0928\u0940\u0932\u093e] \u092b\u0942\u0932 \u0939\u0948\u0964\nvah [neela + nahi + neela neela] phool hai.\nIt [blue + no + blue blue] flower is.\nTranslation: \"It is a blue, no blue-blue flower.\"\nThe interregnum \"\u0928\u0939\u0940\u0902\" (no) indicates repetition between first \u0928\u0940\u0932\u093e (neela) and second first \u0928\u0940\u0932\u093e (neela) with a negation marker as interregnum, making it distinct from reduplication between second \u0928\u0940\u0932\u093e (neela) and third \u0928\u0940\u0932\u093e (neela).\n\u2022 Example 3:\n\u0935\u0939 [\u0928\u0940\u0932\u093e + { } \u0928\u0940\u0932\u093e] \u092b\u0942\u0932 \u0939\u0948\u0964\nvah [neela + + neela] phool hai.\nIt [blue + {} + blue] flower is.\nTranslation: \"It is a blue, no blue-blue flower.\"\nThe empty, interregnum suggests reduplication, where the Reparandum is repeated without any additional elements."}, {"title": "5.3 Modeling Approach", "content": "To classify reduplication and repetition, we propose a model that takes into account the RiR structure. Our feature extraction process will involve capturing information from the Reparandum, Interregnum, and Repair segments. To do so, we provide the model as separate features, the words surrounding the reduplicated words. These are highlighted in green, in the above examples. This helps especially in intricate cases, where both phenomena overlap as in Example 2 in Section 5.2.1.\nBy leveraging the RiR structure and considering the surrounding words, we believe our approach will contribute to better utilization of this structural information for the classification of reduplication and repetition."}, {"title": "6 Experimental Setup", "content": "This section details the methodology adopted to distinguish between reduplication, repetition, and other phenomena in speech transcripts."}, {"title": "6.1 Data Processing", "content": "Speech transcripts were preprocessed by removing punctuation to ensure consistency in the dataset. This step also made the task more challenging and realistic."}, {"title": "6.2 Baseline Models", "content": "We evaluated performance using two established baseline models: Logistic Regression and BiLSTM-CRF, chosen for their common use in sequence labeling and classification tasks within NLP. Logistic Regression provides a basic measure of linear separability, while BiLSTM-CRF, better suited for sequential dependencies, offers improved performance in sequence-to-sequence predictions (Huang et al., 2015)."}, {"title": "6.3 Transformer-Based Models", "content": "Given the success of transformer-based models in NLP, we employed them for our task. We selected multilingual transformer architectures suitable for Hindi.\nWe used bert-base-multilingual and its variant with the Reparandum-Interregnum-Repair (RiR) structure, which are recognized for their robust language modeling capabilities. Both models were fine-tuned for our task.\nFurther, we explored the XLMR models: XLMR-base, XLMR-base with RiR, XLMR-large, and XLMR-large with RiR, to evaluate their performance and the possible advantages of the RiR structure."}, {"title": "6.4 Training and Finetuning Setup", "content": "Models were trained using a batch size of 8 for a maximum of 5 epochs. The AdamW optimizer was used with a learning rate of 1e-5. Models were fine-tuned on a dataset specific to reduplication and repetition."}, {"title": "7 Results and Analysis", "content": "In this section, we thoroughly analyse all experiments on reduplication-repetition classification. Results for all the models across the three languages, fine-tuned on the IndicRedRep dataset, are shown in Table 4. For a more detailed examination of the language-wise results, readers can refer to Appendix A. This section also discusses some qualitative examples across languages as given in Table 5, providing interesting insights of confusion cases, and our analysis of how RiR modelling helps in resolving these cases.\nTo evaluate the performance of all models used in our experiments, we use precision, recall, and F1 score; metrics that are commonly used across classification tasks in natural language processing (Manning and Schutze, 1999; Jurafsky, 2000). These metrics have also been used in previous related works focusing on disfluency detection and similar tasks (Jamshid Lou and Johnson, 2017; Passali et al., 2022)."}, {"title": "7.1 Baseline Models", "content": "Results in Table 4 show average F1 scores of 21.10 for Logistic Regression and 52.57 for BILSTM-CRF, highlighting the latter's superiority in handling complex linguistic tasks. Analysis across Hindi, Telugu, and Marathi indicated superior performance in Hindi, attributed to better data resources, whereas Telugu and Marathi posed additional challenges due to their linguistic complexities."}, {"title": "7.2 Multilingual Transformer Models", "content": "We observed that fine-tuning pre-trained models on the IndicRedRep test set, specifically for detecting and identifying reduplication and repetition, yielded significantly higher accuracy compared to baseline Logistic Regression and BiLSTM-CRF models. Incorporating the Reparandum-Interregnum-Repair (RiR) structure into these multilingual transformer models further enhanced their performance, as detailed in Table 4. Specifically, models employing the RiR structure achieved superior results over standard models trained on the same dataset.\nOur experiments highlighted a notable increase in performance metrics with the RiR structure. For example, the bert-base-multilingual model saw its average F1 score improve from 77.13% to 79.67% with RiR, and similar enhancements were noted with the XLMR models: the F1 score for the XLMR-base model rose from 81.11\nThis improvement was not uniform across all languages, reflecting the varied complexities and characteristics of Hindi, Telugu, and Marathi, which underscores the nuanced challenges of language-specific processing in NLP. Further qualitative analysis on the impact of RiR structure integration is elaborated in Section 7.3."}, {"title": "7.3 Qualtitaitve Analysis", "content": "Table 5 presents a detailed examination of specific inference cases from our model, which was applied to unseen test sentences across Hindi, Telugu, and Marathi. It highlights some consistent misclassifications that are crucial for understanding its limitations and illustrating how RiR modeling contributes to improvement.\nFor example, in the first row featuring a Hindi reduplication type error, \u0918\u0930 (ghar, 'house') is misclassified as repetition. This error may be due to the model's oversensitivity to the presence of another repetition instance in the same sentence. A similar pattern of errors is observed in the Telugu and Marathi examples within Table 5. The RiR modeling approach enhances focus on the local context of the word, resulting in correct classification when the XLMR-base + RiR model"}, {"title": "8 Conclusions and Future Work", "content": "Our study introduced and validated a model that employs the Reparandum-Interregnum-Repair (RiR) structure to enhance the classification of linguistic phenomena such as reduplication and repetition in multilingual contexts. Our experiments, as detailed in the table, demonstrated that incorporating the RiR structure consistently improves the performance across multiple languages (Hindi, Telugu, and Marathi), as evidenced by higher F1 scores when compared to baseline models, including both traditional ML-based and advanced transformer models.\nThe RiR structure's utility in distinguishing complex linguistic patterns is particularly notable. This approach provided clear benefits over traditional models like Logistic Regression and BiLSTM-CRF, and even showed marked improvement over advanced models like the multilingual BERT and XLMR in their standard configurations. The most significant improvements were observed with the XLMR-large + RiR model, highlighting the effectiveness of integrating structural linguistic insights into sophisticated neural architectures for NLP tasks.\nFuture research should expand our approach to include more languages, especially those under-represented in NLP, and explore additional linguistic structures beyond the RiR to enhance understanding of language processing. There is also potential to integrate our RiR like models to localize context in other NLP tasks like syntactic parsing or semantic analysis, which could lead to improvements in complex language understanding systems. Moreover, applying our model's insights to real-world applications such as speech recognition could significantly enhance their effectiveness and accuracy."}, {"title": "9 Limitations", "content": "Given the complexities of disambiguating reduplication and repetition in different languages, our study, while rigorous, presents limitations that are acknowledged below:"}, {"title": "A Detailed Results", "content": "In this section we expand Table 4 and give label-wise results for each language. Tables 6, 7, 8 contains results for Hindi, Marathi and Telugu respectively."}, {"title": "B Annotation Guidelines", "content": "Thank you for participating in our study, on identifying reduplication and repetition in speech. During this task, you will be presented with an interface (see Fig. 2), which shows you an audio file as well as the corresponding transcript for that audio.\nInstructions You need to identify whether a word being repeated in the text transcript is reduplication or repetition. These are defined as below.\nReduplication When we say, reduplication in this study, we mean complete reduplication. Complete reduplication, also known as full reduplication, is a linguistic process in which the entire base word is repeated to create a new word or form. In Hindi, complete reduplication is commonly used to express intensity, repetition, or to emphasize a particular action or state.\nExamples of complete reduplication in Hindi sentences:"}, {"title": "C Gemma LLM Prompting Details", "content": "This section provides the details of the prompts used to generate sentences with reduplication and repetition in Marathi and Telugu using the Gemma Instruction Tuned models. The prompts are designed to elicit specific"}, {"title": "D Qualitative analysis", "content": "The gloss for each example listed in Table 5 is given as a list in the same order in Table 9."}]}