{"title": "Mind the Confidence Gap: Overconfidence, Calibration, and Distractor Effects in Large Language Models", "authors": ["Prateek Chhikara"], "abstract": "Large Language Models (LLMs) demonstrate impressive performance across diverse tasks, yet confidence calibration remains a challenge. Miscalibration-where models are overconfident or underconfident-poses risks, particularly in high-stakes applications. This paper presents an empirical study on LLM calibration, examining how model size, distractors, and question types affect confidence alignment. We introduce an evaluation framework to measure overconfidence and investigate whether multiple-choice formats mitigate or worsen miscalibration. Our findings show that while larger models (e.g., GPT-40) are better calibrated overall, they are more prone to distraction, whereas smaller models benefit more from answer choices but struggle with uncertainty estimation. Unlike prior work, which primarily reports miscalibration trends, we provide actionable insights into failure modes and conditions that worsen overconfidence. These findings highlight the need for calibration-aware interventions and improved uncertainty estimation methods. To ensure reproducibility, we provide our code and dataset\u00b9.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have transformed natural language understanding, achieving state-of-the-art performance across diverse applications, from conversational AI (Skjuve et al., 2024; Zhang, 2024) to scientific discovery (Kumar, 2024). Their versatility has also facilitated advancements in multimodal learning, where language models integrate with vision-based systems to enhance tasks such as automated content generation and decision support (Zhang et al., 2023; Chhikara et al., 2024). However, as LLMs are increasingly deployed in high-stakes domains such as healthcare, finance, and law, critical concerns arise regarding calibration-the alignment between model confidence and actual correctness (Dhuliawala et al., 2023). When LLMs express unwarranted confidence in incorrect predictions, they risk misleading users, spreading misinformation, and reducing trust and reliability in AI-driven systems (Chen et al., 2023; Zhang et al., 2024).\nFigure 1 illustrates this issue: when asked \"Who received the IEEE Frank Rosenblatt Award in 2010?\", an LLM incorrectly responds with \u201cGeoffrey Hinton\", assigning a 93% confidence score, despite correct answer being \u201cMichio Sugeno\u201d. This overconfidence is especially problematic because users tend to equate high-confidence outputs with reliability (Liu et al., 2024). The right-hand side of Figure 1 further demonstrates the broader miscalibration problem: while a well-calibrated model should align confidence with accuracy, real-world LLMs frequently overestimate their correctness.\nAlthough calibration errors in neural networks are well-documented (Guo et al., 2017), the calibration behavior of LLMs remains poorly understood, particularly in response to fine-tuning techniques such as Reinforcement Learning from Human Feedback (RLHF) (Leng et al., 2024; Li et al., 2024). Prior work (Zhu et al., 2023; Geng et al., 2024) has noted that RLHF can amplify overconfidence, but the extent and specific conditions under which this occurs remain largely unexplored. Key open questions include: (i) Do bigger models exhibit better calibration, or does scale introduce new sources of miscalibration? and (ii) Can structured input formats, such as multiple-choice answer choices, improve confidence alignment, or do they introduce additional biases? Addressing these gaps is crucial for ensuring the reliable deployment of LLMs in decision-critical applications.\nThis paper presents an empirical study of LLM calibration, focusing on three key objectives: (i) Quantifying Overconfidence: We systematically measure miscalibration across multiple LLMs and analyze the extent of overconfidence. (ii) Evaluating the Role of Answer Choices: We assess whether providing structured answer options (e.g., multiple-choice format) improves or worsens accuracy and calibration. (iii) Identifying Failure Modes: We categorize question types where LLMs exhibit the highest miscalibration, revealing systematic weaknesses in uncertainty estimation.\nUnlike prior work (Ulmer et al., 2024; Kapoor et al., 2024; Jiang et al., 2021) that broadly characterizes miscalibration, our study pinpoints specific conditions that worsen overconfidence, offering an actionable framework for improving LLM reliability. Our findings emphasize the need for task-specific confidence adjustments and structured uncertainty modeling to enhance trust in real-world deployments. Our study reveals that miscalibration is not a uniform problem; its severity varies based on model scale and input structure, with larger models gaining significant calibration improvements when provided with distractors, whereas smaller models struggle with meaningful uncertainty estimation."}, {"title": "2 Experimental Setup", "content": "Evaluation Dataset: We use the SimpleQA dataset (Wei et al., 2024), which provides a reliable benchmark for evaluating LLM factual accuracy and calibration. Comprising short, fact-seeking queries with clearly defined correct answers, SimpleQA enables precise measurement of model confidence and alignment with factual correctness. Its high-quality annotations, verified by multiple independent AI trainers, ensure accuracy and unambiguity, making it well-suited for calibration assessment.\nThe dataset contains 4326 question-answer pairs. For each pair, we used GPT-40-mini to generate three distractors-factually incorrect yet contextually plausible answers that matched the answer type (e.g., dates for date-based questions), remained distinct, and maintained similar specificity. Additional details on the prompt are provided in Appendix A.\nSelected LLMs: We select models from OpenAI\u00b2 and GroqCloud\u00b3. From OpenAI, we include GPT-40, GPT-40-mini, and GPT-4-turbo (Hurst et al., 2024), all large-scale models with billions of parameters. From GroqCloud, we select smaller, efficiency-optimized models: LLaMA-3-8B-8192 (Dubey et al., 2024), LLaMA3.1-8B-instant, and Gemma2-9B-it (Team et al., 2024). More details about the selected models are in Appendix C.\nEvaluation Criteria Following prior work, we use GPT-40-mini as an LLM-based judge to classify responses as CORRECT, INCORRECT, or NOT_ATTEMPTED (Packer et al., 2023; Wei et al., 2024). A response is CORRECT if it fully captures the gold target's key information without contradiction, allowing minor variations in wording, order, or hedging. It is INCORRECT if it contains factual errors, contradictions, or misleading speculation, even if hedged. NOT_ATTEMPTED applies when a response lacks essential information without introducing errors, including vague or evasive answers. We experiment with using the same LLM for both prediction and judgment, finding that smaller LLM judges often misclassify responses or hesitate to assign NOT_ATTEMPTED when no valid answer is generated. Manual inspection confirms these issues, and further details are provided in the Appendix B.\nEvaluation Methods: The standard LLM approach (N) generates answers based only on the given question and a predefined prompt, without external knowledge. We introduce an alternative distractor method (D), where the model receives the question along with the correct answer and three incorrect options (shuffled). This setup enables us to evaluate whether additional context enhances calibration, reinforces overconfidence, or simply encourages random guessing from the provided options. Details on prompts are in the Appendix A."}, {"title": "3 Results Analysis and Discussion", "content": "Performance across LLMS Results indicate a clear performance gap between bigger and smaller LLMs, with bigger models (e.g., GPT-40) significantly outperforming smaller ones in open-ended question answering (Table 1). However, when provided with structured answer choices, smaller models exhibit disproportionately higher gains, while bigger models experience relative increase in distractor-induced errors ($D_{harmed}$). This suggests that bigger models rely more heavily on associative recall and self-generated confidence heuristics, which can be misleading when plausible but incorrect distractors are introduced.\nFor instance, GPT-40's accuracy jumps from 35.14% (N) to 73.42% (D), while LLaMA3-8B exhibits an even more dramatic improvement from 4.79% to 44.01%. The fact that smaller models show a greater relative benefit from answer choices suggests that their decision-making relies more on structured elimination strategies rather than independent fact retrieval. This finding highlights a key consideration for practical deployment: while bigger models can be more self-reliant, their confidence in unaided settings does not necessarily translate into robustness when faced with misleading choices."}, {"title": "Calibration and Confidence Patterns", "content": "A well-calibrated model should align its confidence scores with actual accuracy\u2014meaning that if a model expresses 80% confidence in an answer, it should be correct roughly 80% of the time. Our study reveals that bigger models, while better calibrated overall, still exhibit systematic overconfidence in open-ended tasks. The ECE for GPT-40 decreases significantly in the D setting, demonstrating that answer choices improve confidence alignment by providing external constraints. However, smaller models, while benefiting from options, still fail to estimate their uncertainty meaningfully, as evidenced by consistently high ECE values.\nFigure 2 shows that calibration behavior varies significantly across models. While GPT-40 is relatively well-calibrated at higher confidence levels (70-100%), minor overconfidence remains. In contrast, smaller models frequently overestimate their accuracy, assigning high confidence to incorrect answers, particularly in the 80\u2013100% range. Additionally, all LLMs at lower confidence levels (20-40%) show inconsistencies in uncertainty estimation, sometimes underestimating correctness but not necessarily engaging in random guessing. This distinction has significant implications in real-world applications: in AI-driven tutoring or medical diagnosis, overconfidence in incorrect answers can lead to misleading recommendations. Calibration-aware interventions\u2014such as confidence regularization or explicit uncertainty weighting-are necessary to mitigate these risks."}, {"title": "Performance Across Question Types", "content": "To analyze calibration weaknesses, we evaluate performance across different question types (Figure 3). Person-based queries are most challenging, likely due to name ambiguities and inherent variability in names, overlapping roles, and contextual dependencies that require deeper reasoning beyond surface-level pattern matching. LLMs frequently confuse historical figures with similar names, but providing structured answer choices significantly improves accuracy, suggesting that explicit disambiguation helps mitigate uncertainty in this category. In contrast, place-based queries exhibit relatively strong performance across both settings, indicating that geographic knowledge is well-represented in pre-training. However, calibration improvements vary (Table 2): the person category sees highest relative ECE drop (69%), while place category shows the lowest (50%). This suggests that structured choices help correct overconfidence in ambiguous queries but offer limited calibration gains when models already retrieve knowledge with high confidence. These results demonstrate that miscalibration depends on both task framing and knowledge representation, not just model scale. While structured reasoning improves confidence alignment in person-based queries, factual retrieval tasks like place-based questions may require alternative calibration strategies to prevent persistent overconfidence."}, {"title": "Implications for Model Deployment", "content": "Our findings indicate that calibration errors are not uniform and vary based on both model size and input structure. The susceptibility of bigger models to distractors suggests that prompt engineering strategies should include explicit calibration constraints-for example, by encouraging models to justify their confidence levels rather than relying on raw probability scores. For smaller models, answer ranking or re-scoring techniques may provide more robust confidence adjustments, ensuring that they do not simply default to high-confidence selections when uncertain. Future research should explore task-specific calibration techniques rather than applying one-size-fits-all solutions, particularly for applications where overconfidence poses significant risks (e.g., legal and medical AI systems)."}, {"title": "4 Conclusion", "content": "This study provides an investigation of calibration in LLMs, revealing key trade-offs between model size, answer format, and confidence alignment. While bigger models exhibit better calibration, they remain susceptible to distraction, whereas smaller models show dramatic performance improvements with structured answer choices but fail to meaningfully estimate uncertainty. Unlike prior work that broadly characterizes LLM miscalibration, our study pinpoints conditions under which calibration errors arise, offering a framework for mitigating overconfidence in real-world applications. Future work should explore calibration-aware fine-tuning strategies and uncertainty-aware response generation to enhance LLM reliability, particularly in high-stakes domains."}, {"title": "5 Limitations", "content": "Our study is limited by its focus on a small set of LLMs, excluding domain-specific or fine-tuned variants, which may affect generalizability. Additionally, the use of LLMs to pick from multiple-choice options introduces potential selection bias, as highlighted by recent work (Zheng et al., 2023). While we mitigate this by randomizing option order, it may not fully eliminate bias, impacting generation reliability.\nFurthermore, our reliance on GPT-40-mini as the primary evaluation judge could be questioned, as automated LLM-based scoring may introduce biases. Although we conducted a small-scale human evaluation to validate its reliability, finding substantial agreement with human annotators (Cohen's kappa = 0.82), minor inconsistencies were observed in ambiguous cases (see Appendix B for details)."}, {"title": "A Custom Prompts", "content": "We generate the answers for N setting using the following prompt. The prompt outputs the answer and confidence for the answer in a json format.\n1 LLM_RESPONSE_PROMPT = \"\"\"\n2 You are an intelligent assistant who is given a question. Your role is to provide\n3\naccurate, helpful, and well-reasoned responses based on your knowledge and\ncapabilities.\n4 Along with the question, you need to provide a confidence score for your answer. The\nconfidence score should be a number between 0 and 100, where:\n5\n0-25 indicates low confidence\n6\n26-75 indicates moderate confidence\n7\n76-100 indicates high confidence\n8\n9 Guidelines for providing answers:\n10 1. Be direct and concise in your answer while ensuring completeness. Avoid\nunnecessary words or tangents.\n11 2. If you are uncertain, provide a lower confidence score.\n12 3. Base your confidence score on:\n13\nThe reliability and recency of available information\n14\nYour knowledge of the specific domain\n15\n16 Here are some examples:\n17\n18 Example 1:\n19 Question: What is the capital of France?\n20 Answer: Paris\n21 Confidence score: 91\n22 (High confidence as this is a well-established fact)\n23\n24 Example 2:\n25 Question: Which country has the best healthcare system?\n26 Answer: It depends on the criteria used. Some rankings favor Switzerland, while\nothers favor Sweden or Singapore.\n27 Confidence score: 25\n28 (There is no definitive answer, and the confidence is low due to the lack of a clear\n29\nconsensus.)\n30 Example 3:\n31 Question: Which state is between Washington and California?\n32 Answer: Oregon\n33 Confidence score: 87\n34 (Maximum confidence as this is a clear geographic fact)\n35\n36 Example 4:\n37 Question: What was Albert Einstein's favorite food?\n38 Answer: There is no definitive record of his favorite food, but he reportedly liked\npasta.\n39 Confidence score: 25\n40 (There are anecdotal mentions, but no verified records.)\n41\n42 Example 5:\n43 Question: Is Irvine a city in California?\n44 Answer: Yes\n45 Confidence score: 81\n46 (High confidence as this is a verifiable fact)\n47\n48 Example 6:\n49 Question: What is the most popular programming language for AI development?\n50 Answer: Python\n51 Confidence score: 66\n52 (Moderate-high confidence based on current trends, but this can change over time)\n53\n54 Here is a new example. Simply reply with your answer and confidence score.\n55\n56 Question: {{question}}\n57\n58 Provide your response in the following JSON format:"}, {"title": "B Same LLM judge as Prediction LLM", "content": "We employ the same LLM as both the judge and the base model responsible for predicting answers to the questions. The performance metrics are presented in Table 3. Upon manually inspecting instances from smaller LLMs, we observe that the LLM judge occasionally misclassifies responses and refrains from assigning NOT_ATTEMPTED to certain data points. This can be seen by comparing $N_{none}$ in Table1 and 3. To address this issue and ensure consistency, we use gpt-40-mini as the LLM judge across all models. We also create reliability diagrams of pipeline where LLM-judge was different and the graphs are shown in Figure 4.\nTo validate the reliability of GPT-40-mini as an LLM judge, we conducted a small-scale human evaluation. We sampled 100 responses and had three human annotators independently classify them as CORRECT, INCORRECT, or NOT_ATTEMPTED. The inter-annotator agreement, measured using Cohen's kappa, was 0.82 for GPT-40-mini, indicating substantial agreement. This comparison allowed us to measure the extent of biases introduced by automated evaluation and confirm that LLM judges generally aligned with human judgments, though minor inconsistencies were observed in ambiguous cases."}, {"title": "C LLMs details", "content": "In our study, we utilize six distinct LLMs representing a diverse range of capabilities and architectures. The GPT models (GPT-40-mini, GPT-4-turbo, and GPT-40) are state-of-the-art LLMs provided via OpenAI's API. These models are known for their advanced reasoning, contextual understanding, and robust performance across a variety of tasks. The larger models, such as GPT-40, excel in handling complex queries and demonstrate superior accuracy, while the smaller variant, GPT-40-mini, serves as a lightweight alternative with reduced computational overhead. We use GroqCloud for the LLaMA3.1-8B-instant and LLaMA3-8B-8192 models. These models are smaller in scale but are effective in tasks requiring efficient knowledge retrieval and basic reasoning. Finally, Gemma2-9B-it, also from GroqCloud, offers a balance between size and capability, though its performance is limited by its relatively smaller model size and less extensive pretraining. Together, these models provide a comprehensive landscape for evaluating calibration, accuracy, and decision-making across diverse architectures and computational setups."}]}