{"title": "On Scaling Neurosymbolic Programming through Guided Logical Inference", "authors": ["Thomas Jean-Michel Valentin", "Luisa Sophie Werner", "Pierre Gen\u00e8ves", "Nabil Laya\u00efda"], "abstract": "Probabilistic neurosymbolic learning seeks to integrate neural networks with symbolic programming. Many state-of-the-art systems rely on a reduction to the Probabilistic Weighted Model Counting Problem (PWMC), which requires computing a Boolean formula called the logical provenance. However, PWMC is #P-hard, and the number of clauses in the logical provenance formula can grow exponentially, creating a major bottleneck that significantly limits the applicability of PNL solutions in practice. We propose a new approach centered around an exact algorithm DPNL, that enables bypassing the computation of the logical provenance. The DPNL approach relies on the principles of an oracle and a recursive DPLL-like decomposition in order to guide and speed up logical inference. Furthermore, we show that this approach can be adapted for approximate reasoning with \u0454 or (\u20ac, \u03b4) guarantees, called ApproxDPNL. Experiments show significant performance gains. DPNL enables scaling exact inference further, resulting in more accurate models. Further, ApproxDPNL shows potential for advancing the scalability of neurosymbolic programming by incorporating approximations even further, while simultaneously ensuring guarantees for the reasoning process.", "sections": [{"title": "Introduction", "content": "Neurosymbolic artificial intelligence [van Harmelen and ten Teije, 2019; Vermeulen et al., 2023] seeks to integrate neural perception with symbolic methods to produce more accurate, efficient, and reliable models.\nProbabilistic Neurosymbolic Learning is a research direction that combines neural networks with probabilistic reasoning [Manhaeve et al., 2018; De Raedt et al., 2020; Li et al., 2023]. Neural networks are employed to extract probability distributions from raw data. They are end-to-end integrated with the reasoning process of a logic program to derive solutions. In contrast to pure neural black-box models, the passage through a logic program provides a clear lineage of how a result is derived, enhancing interpretability.\nHowever, neurosymbolic models face significant scalability challenges due to the complexity of the inference in the logic program. Prominent systems for exact inference such as DeepProbLog [Manhaeve et al., 2018] rely on a reduction to the Probabilistic Weighted Model Counting problem (PWMC). This requires computing a logical provenance formula, whose size can grow exponentially depending on the task. Furthermore, solving the PWMC is #P-hard. This significantly limits the applicability of PNL approaches in practice, as they struggle to handle large problem instances effectively.\nApproximations have been developed to overcome these scalability issues [Manhaeve et al., 2021; Winters et al., 2022; van Krieken et al., 2023; Huang et al., 2021]. However, these approximations do not provide guarantees, which potentially undermines one of the primary benefit of PNL: increased confidence in the reliability of the model.\nContribution. We introduce a new approach for probabilistic neurosymbolic learning based on a new algorithm, DPNL, for exact probabilistic reasoning. One of its main advantages is that it bypasses the explicit computation of the logical provenance formula. Instead, DPNL relies on an oracle to guide and accelerate the logical inference process. This is achieved by recursively decomposing probabilistic computations in a DPLL-inspired manner. We explain how to construct valid oracles and formally prove the termination and correctness of DPNL. Additionally, it can be adapted for approximate computations with \u0454 or (\u03b5, \u03b4) guarantees. Experiments demonstrate that DPNL scales better in practice. This makes it possible to build accurate models for tasks that previous PNL methods with exact inference cannot handle. Furthermore, the experiments show that ApproxDPNL is advantageous in scaling DPNL to even larger tasks while providing reliability in the form of guarantees for the approximation."}, {"title": "Problem statement and definitions", "content": "Notations. We first introduce some notations. A function f that maps elements from a set A to a set B is denoted as $f : A \u2192 B$. The set of all such functions is written as $F(A, B)$. For any function $f : A\u2192 B$ and any $y \u2208 B$, the preimage of y, i.e., the set of all $x \u2208 A$ such that $f(x) = y$, is denoted by $f^{-1}(y)$. An estimator of a function f is denoted by $\\hat{f}$.\nWe consider the Probabilistic Learning (PL) problem and focus on solving it using Probabilistic Neurosymbolic Learning (PNL) systems, which integrate both neural and symbolic components. The formal definitions follow.\nProbabilistic Learning Problem. A PL problem is a quadruplet P = ((\u03a9, A, P), I, O, F) where:\n\u2022 (\u03a9, A, P) is a probability space with sample space \u03a9, event space A and probability function P.\n\u2022 I is the input space.\n\u2022 O is the finite output set.\n\u2022 $F : I \u2192 F(\u03a9, O)$ is a function that maps each input $i \u2208 I$ to its output random variable F(i).\nThe probability distribution of P is the function p that maps every input i to the probability distribution of F(i):\n$\u2200i \u2208 I, p(i) : o \u2208 O \u2194 P(F(i) = o)$.\nWe define solving P as estimating p.\nExample 1 (MNIST Classification). Consider the MNIST classification task. The goal is to recognise handwritten digits in images, e.g. the digit in is 8. This task can be modelled as the MNIST PL problem ((\u03a9, A, P), IMNIST, OMNIST, FMnIST), where:\n\u2022 IMNIST = R28\u00d728 represents the space of all possible images,\n\u2022 OMNIST = [0,9] is the set of possible output labels,\n\u2022 FMNIST associates with each image i the random variable FMNIST(i) for the digit in i.\nSolving the MNIST PL problem amounts to estimating the function PMNIST that maps each image i to the probability distribution of FMNIST(i), which can be done using a neural network PMNIST.\nProbabilistic Neurosymbolic Learning System. Let P = ((\u03a9, A, P), I, O, F) be a PL problem and p its corresponding probability distribution. A PNL system for P is a quadruplet ((Vk)1\u2264k\u2264m, (Xk)1<k\u2264m, (Pk)1<k<m, S) where:\n\u2022 m\u2265 1 is the order of the PNL system.\n\u2022 \u2200k \u2208 [1,m], Pk = ((\u03a9, A, P), I, Vk, Xk) is a PL problem with its probability distribution pk.\n\u2022 Vi \u2208 I, the random variables (Xk(i))1<k<m are independent. We say that the sub-problems (Pk)1<k<m are independent.\n\u2022 \u2200k \u2208 [1, m], pk is a neural network which solves Pk, i.e, it estimates pk. The set of parameters of pk is \u03b8\u03ba.\n\u2022S: V\u2081 \u00d7 ... \u00d7 Vm \u2192 O is a symbolic function that maps values obtained from the sub-problems to the output. S is known.\n\u2022 $F = S \u25e6 (X1, ..., Xm)$, that is, for all $i \u2208 I$ and $w \u2208 \u03a9$, $F(i)(w) = S(X1(i)(w), ..., Xm(i)(w))$.\nThe PNL system decomposes P into m smaller, independent sub-problems (Pk)1\u2264k\u2264m. Each sub-problem Pk is solved by estimating pk with $\\hat{p_k}$. The outputs of $\\hat{p_k}$ are then combined using the symbolic function S to estimate p thanks to the following relation, which holds because of the independence assumption of the random variables:\n$p(i)(o) = \\sum_{(x_1,...,x_m) \u2208 S^{-1}(o)} P(\\bigcap_{k=1}^m (X_k(i) = x_k))$ (1)\n$\\qquad = \\sum_{(x_1,...,x_m) \u2208 S^{-1}(o)} \\prod_{k=1}^m P(X_k(i)=x_k)$.\nThe corresponding estimator $\\hat{p}$ is defined by:\n$\\hat{p}(i)(o) = \\sum_{(x_1,...,x_m) \u2208 S^{-1}(o)} \\prod_{k=1}^m \\hat{p_k}(i) (x_k)$. (3)\nMoreover, $\\hat{p}(i)(o)$ is differentiable w.r.t. to $\u03b8 = (\u03b8_k)_{1\u2264k\u2264m}$, which allows to train the $(\\hat{p_k})_{1\u2264k\u2264m}$ directly with gradient descent and a data set for the problem P.\nMajor Challenge. The primary computational challenge lies in efficiently evaluating the sum over $S^{-1}(o)$ in equation (3). Indeed, |$S^{-1}(o)$| can grow exponentially w.r.t. m, and inverting a symbolic function is in essence a NP-hard task. Therefore, addressing the complexity of computing $S^{-1}(o)$ is a key focus in improving PNL system scalability.\nExample 2 (MNIST-SUM Task). We consider the neurosymbolic MNIST-SUM task [Manhaeve et al., 2018], which will be used as a running example. The goal is to learn to predict the sum of handwritten digits, given only the label of the sum, but not the labels of the individual digits in the images, e.g. + = 8. This involves two subtasks: (1) perception of the digit in the image and (2) reasoning on the combination of the digits (with the sum as the symbolic function S). Building on the MNIST PL problem, see Example 1, we define the MNIST-SUM PL problem as ((\u03a9, A, P), I, O, F), where\n\u2022 I = ImNIST \u00d7 Imnist are pairs of images,\n\u2022 O = [0, 18] are the output values of the sum,\n\u2022 F(11,12) = Fmnist(i1)+ FMnIST(12) is the random variable related to the sum of the digits in the two images 11, 12.\nFor this task, we also define the PNL system ((V1, V2), (X1, X2), (p1, p2), S), where\n\u2022 V\u2081 = V2 = [0,9],\n\u2022 X1(11,12) = FMNIST(i1) and X2(11,12) = FMNIST(12) for all pairs of images (i1, 12) \u2208 Imnist \u00d7 Imnist,\n\u2022 P1(11,12) = mn\u0131st(i1) and p2(i1, 12) = Pmnist(12) for all (i1, 12) \u2208 I2,\n\u2022 S(d1,d2) = d\u2081 + d2 for all digit values d\u2081 \u2208 V1, d2 \u2208 V2.\nThus, this PNL system for MNIST-SUM consists of two independent MNIST PL problems for classifying the digits in the images.\nThis task can easily be extended to MNIST-N-SUM, where N is the number of digits per summand. In this notation, the previous example is a MNIST-1-SUM. For N=2, two digits form the summands, e.g. 28+35 = 63. By extending from one-digit sums (N=1) to two-digits sums (N=2),"}, {"title": "A novel architecture for PNL systems", "content": "We introduce the new approach in two steps: first, we present ProbDPLL as an intermediate algorithm that serves as the basis for key observations. Then, building on these observations, we extend it to a more general setting and introduce the DPNL algorithm."}, {"title": "ProbDPLL", "content": "We first introduce a new algorithm: ProbDPLL for computing PWMC& when G is a Boolean formula in CNF form. ProbDPLL builds on the observation that model counting is analogous to PWMC. It extends the #DPLL model counter introduced in [Bacchus et al., 2003], which is based on the seminal DPLL algorithm [Davis et al., 1962], adapting it to compute PWMCG.\nThe ProbDPLL Algorithm. The pseudocode of ProbDPLL is given in Algorithm 1. It uses the notation G|x=b to denote the CNF formula obtained by replacing X with b in G and simplifying the formula. It explores partial valuations of the formula G and stops exploration when the partial valuation evaluates G as true (when no clauses remain) or false (when the empty clause is reached). A vizualization of an example run of ProbDPLL is given in Appendix A.2.\nCorrectness and termination. For all G provided in CNF and a probability distribution about the variables in G, ProbDPLL(G, \u03c3) always terminates and returns PWMCG. A detailed proof of the termination and correctness of the algorithm is provided in Appendix A.\nApplication to logic programs. We can apply this algorithm to probabilistic logic programs such as in DeepProbLog [Manhaeve et al., 2018] (c.f. Section 4). Since the logical provenance in DeepProbLog is expressed in DNF, we instead apply the algorithm to its negation, which is in CNF. This yields a probability p, allowing us to return 1 p as the probability of the logical provenance.\nKey observations. While improvements to ProbDPLL, such as the component caching introduced for #DPLL by [Bacchus et al., 2003], are possible, they do not directly address the issue of logical provenance size. To tackle this problem, we highlight the following key observations:\n\u2022 ProbDPLL is designed for Boolean formulas but can be adapted to general (non-Boolean) functions, such as the symbolic function S defined in Section 2, by working with independent random variables with finite domains (instead of independent binary random variables, as in PWMC). For instance, in the MNIST-SUM example 2, the input values of S are the digit values [0,9]. Therefore, it is more convenient to use random variables with"}, {"title": "The DPNL Algorithm", "content": "We now introduce the main contribution: the DPNL algorithm, which generalizes ProbDPLL to support more general symbolic functions S (not necessarily Boolean formulas like G) and independent random variables with finite domains (instead of only independent binary random variables).\nInstead of working directly with a specific representation of S, as ProbDPLL does with G, DPNL follows a different approach. It calls an oracle function that extracts only the necessary information about S to guide the inference process in a way similar to ProbDPLL. The concept of an oracle here is an abstraction that captures precisely the properties required for DPNL to be correct. The advantage of this method is that, in many cases, the oracle can be thoughtfully implemented, making DPNL more efficient than other PNL frameworks, such as DeepProbLog.\nWe now formally define the concepts of valuations and oracles used in DPNL.\nValuation. A valuation v of the variables (Xk)1<k<m is an array of size m such that for all k \u2208 [1,m], v[k] \u2208 VkU {unknown}. The following notations are used:\n\u2022 V denotes the set of all valuations.\n\u2022 A total valuation is one where : \u2200k \u2208 [1,m], v[k] \u2260 unknown.\n\u2022 A partial valuation is one that is not total.\n\u2022 v' is a sub-valuation of v if:\n$\u2200k \u2208 [1,m], v'[k] \u2260 unknown \u2192 v'[k] = v[k]$\n\u2022 sub(v) is the set of sub-valuations of v.\n\u2022 tot(v) is the set of total sub-valuations of v.\n\u2022 w(i,v) is the event corresponding to v with input i. Formally, it is the set of w \u0395 \u03a9 such that [X1(i)(w), ..., Xm(i)(w)] \u2208 tot(v).\n\u2022$U_{v[k]\u2190x}$ is the valuation v where the value of v[k] is replaced by x.\nOracle. Given the symbolic function S of a PNL system, an oracle Os for S is a function that, given a valuation v \u2208\u03bd and an output o \u2208 O, answers the question \"Is S always equal to or always different from o for all sub-valuations of v?\". Intuitively, from a given output o, the oracle is in charge of determining whether it is still relevant to explore sub-valuations or whether the exploration process can be stopped.\nFormally, a function Os : V \u00d7 O \u2192 {0, 1, unknown} is a valid oracle for S if and only if, for all v \u2208 V and \u03bf \u2208 \u039f:\n\u2022 If the valuation v is total,\n$O_S (v, o) = \\begin{cases} 1 & \\text{if } S(v[1], ..., v[m]) = o, \\\\ 0 & \\text{otherwise} \\end{cases}$\n\u2022 If $O_S (v, o) = 1$,\n$\u2200v' \u2208 tot(v), S(v'[1], ..., v'[m]) = o$\n\u2022 If $O_S (v, o) = 0$,\n$\u2200v' \u2208 tot(v), S(v'[1], ..., v'[m]) \u2260 o$\nWe say that the oracle is complete if the last two points are equivalences.\nDPNL. The pseudocode of DPNL is given in Algorithm 2. We can compute p(i)(0) by initially calling DPNL(i, o, [unknown, ..., unknown], Os). In practice, however, we want to compute p(i)(0) because we do not know P. Therefore, we replace P(Xk(i) = y) at line 7 by its neural estimation pk(i)(y).\nTermination and Correctness. For every\n\u2022 PL problem ((\u03a9, A, P), I, O, F),\n\u2022 PNL system ((V)1\u2264k\u2264m, (Xk)1<k\u2264m, (Pk)1\u2264k\u2264m, S) for the PL problem,\n\u2022 valid oracle Os for S,\n\u2022 \u03af \u2208 \u0399, \u03bf \u2208 O and v \u2208 V,\nDPNL(\u03af, \u03bf, \u03c5, \u039fs) always terminates and returns P(F(i) = o|w(i, v)). A detailed proof of the termination and correctness of the algorithm is provided in Appendix B.\nAdvantages of using Oracles\nFor any symbolic function S, if we dispose of a procedure computing S, then there exists a valid oracle for S. Indeed, the oracle that returns unknown when the valuation is partial and tests if the result of S is equal to the output when the valuation is total, is valid for S. However, with this naive oracle, the complexity of DPNL is the same as testing S over all possible inputs to compute S-1(0). It is thus important to design oracles that return 0 or 1 for partial valuations in order to prune the search space of DPNL. Again, a naive oracle could test all total sub-valuations of v to answer 0 or 1 as soon as possible, but this is not efficient. Thus, a good oracle should strike a balance between pruning capability and efficiency. In practice, S could naturally be represented by an algorithm. It is often possible to derive an efficient valid oracle"}, {"title": "Obtaining Oracles from Logic Programs", "content": "We now explain how valid oracles can be systematically derived from a broad class of logic programs. To this end, we first recall the connection between logic and probabilistic learning (PL) problems. Next, we define a class of neural probabilistic logic programs, building on ProbLog [De Raedt et al., 2007] and DeepProbLog. Finally, we provide an algorithm to compute oracles from these programs, and prove that the obtained oracles are valid. The formal definitions follow.\nLogics. A logic L consists of a set of functions F and predicates P, logical operators such as \u2227 and \u2203, their semantics, and an infinite set of variables X. Combinations of functions and variables form terms, predicates applied to terms form atoms and logical combinations of atoms form logical formulas. The purpose of a logic is to model aspects of reality, which is formalized through an interpretation I = (\u2206, [.], [.]p), where:\n\u2022 A is a set of real objects called the domain,\n\u2022 for all f \u2208 F, [f]F \u2208 F(\u2206arity(f), \u2206) is the interpretation of f,\n\u2022 for all p \u2208 P, [p]p \u2208 F(\u2206arity(p), {0, 1}) is the interpretation of p.\nThis defines a semantics [.]1 by recursively applying the semantics of logical operators and the interpretations of functions and predicates.\nExample 4 (Logic for MNIST). In MNIST, we can consider L where F = {i1 (0), 12(0), 0(0), ..., 9(0)} and P = {is(2)} where s(k) denotes a symbols of arity k. We can define I = (Images \u222a [0,9], [.]F, [.]p) where [11]F, [12]F are images, [d] = d and where, for example, [is(i1,7)] = [is]p([11]F, [7]F) = 1 iff [i1]F is an image of 7.\nIn the MNIST classification example, the task consists in estimating the probability of is (i\u2081, d) for every digit d. Therefore, we can model this probabilistic aspect in the logic by considering the predicate interpretation as probabilistic. We can then define logic PL problems by following this idea.\nLogic PL problem. Consider a logic L, a domain A and a closed (i.e without variables) logical formula & over L. We define:\n\u2022 \u0393\u00a3,\u2206 = (\u03a9\u03c2,\u25b3, Ac,\u25b3, Pc,\u2206) is the probabilistic space of L in the real context \u2206. It models the probabilistic aspect of facts by taking \u03a9\u03c2, as the set of all predicate interpretations [.]p for \u2206 that are possible. PC,\u25b3 is specific to the real context and models the likelihood of each [.]p.\n\u2022 Ic, is a subset of all possible function interpretations [.]F for \u2206. It models the possible observations about the reality and depends on the real context.\n\u2022 $B_{\\phi,\\Delta}^{L}$ is the function that maps interpretations of Ic,\u25b3 to binary random variables in \u0393\u03c2,\u2206 corresponding to \u03c6:\n$B_{\\phi,\\Delta}^{L}([.]_F)([.]_p) = [\\phi]_{({\\Delta},[.]_F,[.]_p)}$.\n\u2022 $P_{\\phi,\\Delta} = (\\Gamma_{\\mathcal{L},\\Delta},I_{\\mathcal{L},\\Delta},\\{0,1\\},B_{\\phi,\\Delta}^{L})$ is the logic PL problem of \u03c6.\nBy taking inspiration from ProbLog and DeepProbLog, we now define a class of PNL systems for logic PL problems. Consider a logical formula . The idea is to decompose P into m smaller logic PL problems $P_{\\phi_1},...,P_{\\phi_m}$ where $$\\phi_1,..., \\phi_m$$ represent simpler concepts. A symbolic function S, based on axioms about the context, combines $$\\phi_1,..., \\phi_m$$ to determine the truth value of ."}, {"title": "Logic PNL system", "content": "Consider a logic L and a domain \u0394. Suppose the existence of an axiom set A = {1, ..., an} such that all a\u2081 are true in \u0393,\u0394:\n$\\bigcap_{a \\in A}(\\mathcal{B}_{a}^{L}([.]) = 0) = 0 $\nWe write {1, ..., \u03a6\u03b9} - \u03a6 to denote that there exists a proof of I in Lassuming \u03a61, ..., \u03a6\u03b9. Now suppose there exist formulas $$\\phi_1,..., \\phi_m$$ such that:\n\u2022 $(P_{\\phi_k})_{1 \\le k \\le m}$ are independent logic PL problems,\n\u2022 for all $$\\phi'_1,..., \\phi'_m$$ such that $$\\phi'_k = \\phi_k$$ or $$\\phi'_k = \\neg\\phi_k$:\n$$\\mathcal{A} \\cup \\{\\phi'_1,..., \\phi'_m\\} \\models \\phi \\space or \\space  \\mathcal{A} \\cup \\{\\phi'_1,..., \\phi'_m\\} \\models \\neg \\phi$$\n$\\mathcal{A} \\cup \\{\\phi'_1,..., \\phi'_m\\} \\models \\bot $.\nWe say that the $$\\phi_1,..., \\phi_m$$ decide & and are coherent with respect to A.\nUnder these assumptions, we have sufficient elements to prove that the following constitutes a PNL system for $P_{\\phi}$ ((\\{0,1\\})_{1\\le k \\le m}, (B_{\\phi_k}^{L,\\Delta})_{1\\le k \\le m}, (\\hat{p}_{k})_{1\\le k \\le m}, S')$, where:\n\u2022 for all k \u2208 [1, m], pk is a neural network which takes as input ground terms of k interpreted as elements of A and returns an estimation of the probability distribution of $B_{\\phi_k}^{L,\\Delta}$.\n\u2022 for all x1, ..., xm \u2208 {0,1}, S(x1,..., xm) is equal to\n$S(x_1,...,x_m) = \\begin{cases} 1 & \\text{ if } \\mathcal{A} \\cup \\{\\phi_k | x_k=1\\} \\cup \\{\\neg \\phi_k | x_k = 0\\} \\models \\phi,  \\\\ 0 & \\text{ otherwise}. \\end{cases}$\nThe proof is given in Appendix E.1."}, {"title": "ApproxDPNL", "content": "DPNL can be readily adapted to approximate reasoning with guarantees on the approximation error. We consider (\u20ac, \u03b4)-approximation defined as follows, where e denotes the relative error bounded by a parameter e with probability 1 \u2013 \u03b4 [Chakraborty et al., 2016; Dubray et al., 2024].\n(\u20ac, \u03b4)-approximation. An estimator \u0177 is an (\u03b5, \u03b4)-approximation when $P(\\hat{y} - y| > \u03b5y) < \u03b4$.\ne-approximation. An estimator \u0177 is an e-approximation when $y/(1 + \u20ac) \u2264 \\hat{y} \u2264 y \u00b7 (1 + \u20ac)$.\nDPNL with (\u20ac, \u03b4)-approximations, named ApproxDPNL, is proposed in Algorithm 4. As proposed in [Dubray et al., 2024], ApproxDPNL can be stopped at any time T and provides lower and upper bound guarantees on the result of the exact weighted model counting. The choice of the function STOP determines the guarantee of the result, where low and up are the lower and upper bound of the approximation interval.\n\u2022 If STOP(low, up):= 1 if up \u2264 low. (1 + c)2\n0 else\nthe result of ApproxDPNL is an e-approximation of P(F(i) = ow(v)) (see [Dubray et al., 2024]).\n\u2022 If STOP(low, up):= 1 if up - low \u2264 e\n0 else\nthe result of ApproxDPNL r respects the following property P(F(i) = ow(v) \u2013 \u0454 \u2264 r < P(F(i) = o|w(v) + \u20ac.\n\u2022 If STOP returns 1 after time T has passed, the result of ApproxDPNL is a biased approximation of P(F(i) = ow(v)) that took time T to compute.\nThe heuristic H controls in which order ApproxDPNL explores sub-valuations. Intuitively, it is important to prioritise the exploration of sub-valuations that are the most probable so that the size of the approximation interval up \u2013 low narrows faster. However, it is also possible for use a random choice as H, which results in uniform sampling."}, {"title": "Experiments", "content": "We experimentally evaluate DPNL in order to answer the following research questions:\nQ#1 How does DPNL compare to PNL systems with exact reasoning in terms of efficiency and accuracy?\nQ#2 How does DPNL compare to PNL systems with approximate reasoning in the state-of-the-art in terms of efficiency and accuracy?\nQ#3 How does ApproxDPNL compare to exact and approximate reasoning methods in the state-of-the-art in terms of efficiency and accuracy?\nTask. We evaluate DPNL on the popular MNIST-N-SUM task [Manhaeve et al., 2018] as introduced in Section 2. By extending from an N digits sum to an N+1 digits sum, we increase the complexity of the reasoning task exponentially. Thus, the multi-digit MNIST-N-SUM task is well-suited to investigate the scaling properties of DPNL. We use the MNIST dataset from [Lecun et al., 1998] that contains 60K training images and 10K test images. In consequence, with respect to N, the training set contains 60K / 2N images and the test set contains 10K / 2N images so that every image is used once. Each training sample consists of the images for the two summands and the supervision label on their ground truth sum.\nExperimental Setup. We compare DPNL with the PNL systems DeepProbLog [Manhaeve et al., 2018] and Scallop [Huang et al., 2021] that use exact inference (Q#1), and with A-Nesi [van Krieken et al., 2023], DPLA* [Manhaeve et al., 2021] and Scallop (with the top-k semiring), which use approximate inference (Q#2). A-Nesi is evaluated across its different variants (predict only, explain, prune). We further test ApproxDPNL compare the results to the previously mentioned exact and approximate methods (Q#3). We choose a range of different values for reasoning timeouts T = {0.05, 0.075,0.1, 0.5} for ApproxDPNL. We evaluate the test accuracy and the runtime of the symbolic reasoning component. To ensure comparability, we maintain consistent experimental settings across all models, by reproducing baseline results on the same hardware and using the same parameters.\nFurthermore, we also want to compare exact and approximate reasoning. To address this, we use a common set of hyperparameters. For all methods, we employ the MNIST classifier architecture proposed in [Manhaeve et al., 2018]. The Adam optimizer [Kingma and Ba, 2017], a batch size of 2, a single training epoch, and a learning rate of 1e-3 are used. Each experiment is performed with multiple independent runs, and we report the average test accuracy and the average reasoning time per sample during training. Further, we enforce timeout thresholds, terminating any experiment with a run that exceeds the limit of six hours. Details on hyperparameter values, hardware and information on baseline configurations are provided in Appendix H.\nResults The results of the experiments are given in Table 1. For Q#1, we first compare DPNL with other PNL systems using exact inference. DeepProbLog achieves similar accuracy to DPNL for N=1 and N=2. In terms of reasoning time per sample, DPNL outperforms DeepProbLog, with the advantage becoming more noticeable as task complexity increases. Scallop with exact reasoning only succeeds to complete the task N = 1. For N \u2265 3, DeepProbLog fails to finish, while DPNL achieves an accuracy of 0.82 for N = 4 with an average reasoning time per sample of 1.24s. Among all the exact PNL systems considered, DPNL is the only one that scales to N = 4.\nWhen comparing exact and approximate reasoning systems for Q#2, exact reasoning positively impacts accuracy, and this effect becomes more pronounced as the task difficulty increases. For N = 1, the mean test accuracy of all systems considered is centered around the value 0.95 (except DPLA*). However, for N = 2 there is already a noticeable accuracy gap between approximate methods and exact methods such as DeepProbLog and DPNL. This gap becomes even larger for N = 3, where DPNL achieves a considerably higher accuracy of 0.87 compared to the approximate methods, which do not exceed 0.83. For N = 4, even the approximate methods such as DPLA* and Scallop (top-k) fail to complete their runs, while DPNL finishes successfully. The only baseline that finishes for N = 4 is A-Nesi. All variants of A-Nesi have noticeably lower reasoning times per sample compared to DPNL. However, DPNL has a clear advantage in terms of accuracy. The most accurate A-Nesi model is the explain variant, which achieves an accuracy of 0.65, while DPNL reaches an accuracy of 0.82.\nRegarding Q#3, ApproxDPNL controls the total runtime by setting a timeout T during the reasoning process per sample. Over all tasks N = {1,2,3,4}, ApproxDPNL achieves an accuracy competitive to DPNL with exact reasoning. For the approximate reasoning methods, although A-Nesi still has an advantage in terms of run time per sample, ApproxDPNL clearly outperforms A-Nesi in terms of test set accuracy."}, {"title": "Related Works", "content": "There is a large body of work on neuro-symbolic methods aimed at bridging the gap between learning and reasoning. Within this field, PNL systems can be categorized into two groups: those based on exact reasoning and those based on approximate reasoning.\nPNL systems with exact reasoning. DeepProbLog [Manhaeve et al., 2018], one of the first PNL systems, is often used as a benchmark for accuracy, establishing itself as a foundational framework in PNL. DeepProbLog is based on a probabilistic extension of Prolog called ProbLog [De Raedt et al., 2007] where neural networks are used to estimate probabilities of extensional predicates. ProbLog reduces the probabilistic reasoning task to the PWMC problem. However, two key scalability issues arise. First, the logical provenance is initially computed in DNF, whose size can grow exponentially depending on the task. Even when the clauses of the logical provenance are stored in a prefix tree, its size can grow exponentially with the number of neural ground atoms. Second, solving the PWMC itself is #P-hard causing blowups in the size of the compiled forms of the logical provenance. Specifically, the size of the Binary Decision Diagram can increase exponentially (as detailed in Appendix F).\nNeurASP [Yang et al., 2020] uses a similar approach but relies on Answer Set Programming. It considers the neural network output as the probability distribution over atomic facts in answer set programs.\nAs noticed in [van Krieken et al., 2023], DeepProbLog, NeurASP and other methods in this line of works [Xu et al., 2018; Ahmed et al., 2022] provide exact inference with probabilistic circuit methods [Choi et al., 2020]. However, they"}, {"title": "Conclusion", "content": "We have presented DPNL, a new approach for exact probabilistic reasoning in neurosymbolic learning. The core feature of DPNL lies in its use of oracles, which not only eliminate the need for computing logical provenance formulas, but also allow for significant acceleration of the inference process. Oracles can be automatically generated or customized for further efficiency gains, making DPNL a flexible solution. The correctness and termination of DPNL are formally proved. We furthermore introduced ApproxDPNL that provides approximate reasoning with guarantees on the approximation error in the context of DPNL. Experimental results firstly show that DPNL outperforms existing exact inference methods, handling more complex tasks with greater efficiency and producing more accurate models compared to approximate reasoning techniques. Secondly, ApproxDPNL has shown to further improve the scalability of neurosymbolic"}]}