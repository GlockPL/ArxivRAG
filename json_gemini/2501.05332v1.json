{"title": "AnCoGen: Analysis, Control and Generation of Speech with a Masked Autoencoder", "authors": ["Samir Sadok", "Simon Leglaive", "Laurent Girin", "Ga\u00ebl Richard", "Xavier Alameda-Pineda"], "abstract": "This article introduces AnCoGen, a novel method that leverages a masked autoencoder to unify the analysis, control, and generation of speech signals within a single model. AnCoGen can analyze speech by estimating key attributes, such as speaker identity, pitch, content, loudness, signal-to-noise ratio, and clarity index. In addition, it can generate speech from these attributes and allow precise control of the synthesized speech by modifying them. Extensive experiments demonstrated the effectiveness of AnCoGen across speech analysis-resynthesis, pitch estimation, pitch modification, and speech enhancement. Code and audio examples are available online\u00b9.", "sections": [{"title": "I. INTRODUCTION", "content": "Over the years, many speech processing algorithms have been developed to analyze, transform, and synthesize speech signals. This includes time stretching, pitch shifting, timbre modification, and speech enhancement in noise. Conventional parametric approaches rely on a signal model whose parameters are estimated during the analysis stage and explicitly modified in the transformation and synthesis stages. Examples of well-known parametric signal models include linear predictive coding [1], sinusoidal models [2], [3] and harmonic-plus-noise models [4]\u2013[6]. Non-parametric methods do not require the explicit estimation and manipulation of speech parameters. Historical examples of such methods for pitch- and timescale modification include PSOLA [7] and the phase vocoder [8]. More recently, vocoders such as STRAIGHT [9] and WORLD [10] were proposed for real-time manipulation of speech. They can be seen as learning-free encoding/decoding-based methods in which signal processing techniques are used to decompose (resp. synthesize) a speech signal into (resp. from) a set of acoustic features consisting of the fundamental frequency (fo), spectral envelope, and aperiodicity.\nToday, the most effective methods for manipulating speech signals are based on deep learning. Various neural encoder-decoder models have been proposed to encode a speech signal into a compact representation that can then be decoded with minimal loss of quality and intelligibility. Neural audio codecs such as SoundStream [11] and EnCodec [12] focus on extracting low-bitrate discrete units while achieving high-quality reconstruction. Encoders based on self-supervised learning (SSL), such as HuBERT [13], have also been used to extract a representation of the linguistic content of speech, which can then be combined with acoustic attributes (e.g., fo) and speaker identity to reconstruct the speech signal [14]. Existing neural encoder-decoder models achieve high-quality speech reconstruction but significantly lack controllability for speech signal manipulation and robustness to noise and reverberation.\nIn this paper, we introduce AnCoGen for analyzing, controlling, and generating speech signals. AnCoGen can decompose a speech signal into a compact but comprehensive set of speech attributes representing not only the linguistic content, prosody (pitch and loudness), and speaker identity, but also the acoustic recording conditions in terms of noise and reverberation. As in the previously proposed neural architectures for speech manipulation, the encoded attributes can be controlled before resynthesis, enabling various applications, including pitch transformation, voice conversion, noise suppression, and dereverberation. However, in the previous works, separate encoder and decoder models were designed to estimate the speech attributes and synthesize the speech signal, respectively. With AnCoGen, we propose a fundamentally different approach. We draw inspiration from the masked autoencoder (MAE) model [15] to learn a bidirectional mapping between a Mel-spectrogram and the speech attributes, hence leading to a single model for the analysis and the resynthesis, as illustrated in Fig. 1. The Mel-spectrogram and the attributes can be seen as two different representations of the uttered speech. The Mel-spectrogram is a low-level representation close to the raw signal, while the attributes are a higher-level representation more suitable for controlling and manipulating speech. Following the learning strategy of the multimodal MAE [16], training the AnCoGen model consists of reconstructing masked elements in a given representation from visible elements in this same representation and the other. In doing so, the model learns the inter- and intra-representation dependencies. Then, by masking entirely one representation (e.g., the Mel-spectrogram), it is possible to generate the other (e.g., the speech attributes), and this generative process operates in both ways, with the same MAE model (see Fig. 1). Combined with a HiFi-GAN neural vocoder [17], the proposed approach allows us to analyze, transform, and synthesize speech signals efficiently with a single model."}, {"title": "II. THE PROPOSED MODEL: ANCOGEN", "content": "We first provide a general overview of the proposed AnCoGen model, with technical details in the following subsections. The"}, {"title": "B. Token representations", "content": "The MS discrete tokens are obtained from the discrete latent representation of a pre-trained and frozen vector-quantized variational autoencoder (VQ-VAE) [21]. Following [16], [22], the VQ-VAE operates independently on each time frame and it is fully convolutional on the Mel-frequency axis, thus preserving the time-frequency structure of the original MS representation. For each time frame, the VQ-VAE encoder outputs a vector $x_t \\in X = \\{1, ...,C\\}^F$ of discrete indices (corresponding to the set of F quantized latent vectors in the VQ-VAE codebooks, each codebook being of size C). $x_t$ is a blue square in Fig. 2 and a (time) sequence of N MS discrete tokens $x = \\{x_t \\in X\\}_{t=1}^{N}$ is the vertical set of blue squares. We also experimented using tokens created from 2D (time-frequency) patches of indices, but the frame-based approach led to better performance. A sequence of MS discrete tokens can be converted back to an MS using the VQ-VAE decoder.\nThe SA discrete tokens result from the quantization of the six SAs A1-A6. We denote by $a_i = \\{a_{i,t} \\in A_i\\}_{t=1}^{M_i}$ the (time) sequence of $M_i$ discrete tokens for $A_i$, $i \\in \\{1, ..., 6\\}$, where one $A_i$ token corresponds to $D_i$ integer values between 1 and $K_i$, i.e., $A_i = \\{1, ..., K_i\\}^{D_i}$. Note that in Fig. 2, due to space limitations, we represented with red squares one single sequence $a_i$ for some index i, but we actually have 6 sequences of SAs at the input of the model. Attributes A2 (fo), A3 (loudness), A5 (SNR), and A6 (C50) are 1D real-valued sequences that are normalized, resampled (if needed to have the same length), and rounded to obtain sequences of discrete integer values. Each token $a_{i,t}$ for $i \\in \\{2,3,5,6\\}$ is then obtained by grouping $D_i$ contiguous discrete values from the corresponding sequence. Attributes A1 (HuBERT representation) and A4 (speaker identity embedding vector) are quantized using the k-means algorithm, as in [23]. The number of clusters is set to $K_1 = 100$ for A1 and to the number of speakers in the training dataset for $K_4$. HuBERT already provides a sequential representation, while for the speaker identity attribute, a sequence of discrete tokens is obtained by repeating the cluster index. In practice, we have $(D_i, K_i) = (2,100), (4, 400), (4, 100), (4,251), (4, 128), and (4, 128)$, for i = 1, 2, 3, 4, 5, 6.\nThe MS and SA discrete tokens are transformed by the embedding layers into MS and SA continuous tokens, respectively, before being fed to the encoder. This means that each entry of a discrete token $x_t$ or $a_{i,t}$ is replaced by a real-valued vector taken in a learnable codebook of fixed size (each SA token has its own codebook), and the vectors for all the entries of a given token are concatenated to form a continuous token. The vector dimensions are chosen such that the resulting MS and SA continuous tokens are of the same size, since they are fed to the same encoder model as shown in Fig. 2. At the output of the encoder, the encoded continuous tokens sequence"}, {"title": "C. Masking strategy", "content": "Training AnCoGen is achieved by randomly masking the input sequences of MS and SA tokens. The masking ratio (i.e., the proportion of tokens that are masked) for these two speech representations is drawn randomly according to a uniform distribution on the 1-simplex. This means that if $p \\times 100 \\%$ of the MS tokens are masked, then $(1-p) \\times 100\\%$ of the SA tokens are masked, where p is distributed uniformly between 0 and 1. Note that the masked tokens are chosen randomly and independently for the different SAs $A_i$, even if the masking ratio is the same. The training process involves two consecutive phases: First, the above-described \u201ccoupled\u201d masking strategy is applied, which enables to learn the inter- and intra-representation dependencies. Then an \"all or nothing\" masking strategy is applied, in which one of the two speech representations (MS or SAs) is entirely masked out and the other remains entirely visible (i.e., p = 0 or 1). This helps AnCoGen to learn to estimate the SAs from an MS (analysis) and conversely (synthesis). At inference (analysis or synthesis), of course, only the \"all or nothing\" strategy is applied."}, {"title": "D. Encoder-decoder architecture", "content": "The encoder and decoder of AnCoGen each contain 12 multi-head self-attention (MHSA) blocks from the Vision Transformer (ViT) [24]. Each block comprises an MHSA module with 4 heads and a multilayer perceptron (MLP) module, with layer normalization preceding and residual connections following every module. This block is inspired by the attention layer of the original Transformer [25]. Position embeddings are added to the continuous tokens at the input of the encoder and decoder."}, {"title": "III. EXPERIMENTS", "content": "Tasks In this work, AnCoGen is evaluated across four tasks: Analysis-resynthesis of clean speech signals, robust fo estimation, pitch shifting, and speech denoising. As illustrated in Fig. 1, analysis-resynthesis consists of using AnCoGen to map an MS to the"}, {"title": "B. Results and discussion", "content": "Analysis-resynthesis Table I presents the analysis-resynthesis results, obtained using clean speech utterances from the LibriSpeech"}, {"title": "Robust $f_0$ estimation", "content": "For the robust $f_0$ estimation experiment, we mixed the clean speech signals of the PTDB-TUG database [43] with cafeteria noise from the DEMAND dataset (PCAFETER noise type, unmatched with the training conditions) [31], varying the SNR from 0 to 10 dB. The $f_0$ ground-truth is obtained from laryngograph signals. We evaluate the performance with and without reverberation. The proposed approach is compared to the following reference methods: (i) pYin [44], an enhanced version of the Yin algorithm using Viterbi decoding; (ii) SWIPE [45], which is based on frequency-domain autocorrelation (we used the pysptk implementation); and (iii) CREPE [18], which we already used to generate training data (see Section II-A). The results are shown in Fig. 3 regarding average absolute error (AAE, lower is better) for different SNRs and with or without reverberation. AnCoGen shows very accurate $f_0$ estimation performance, matching that of CREPE on clean data, and maintaining very good accuracy in noisy and reverberant conditions, with an AAE that remains lower than 6.7 Hz in all conditions. This strongly contrasts with pYin and SWIPE, whose accuracy significantly degrades with the SNR."}, {"title": "Pitch shifting", "content": "In this experiment, clean speech signals are manipulated by shifting the original $f_0$ trajectory by \u00b110% and \u00b150% in the control stage of AnCoGen (see Fig. 1). Using again the PTDB-TUG database, we evaluate the performance in terms of AAE between the desired and predicted $f_0$ (using CREPE), and in terms of speech signal quality using again the N-MOS metric of [41]. We compare our approach to the WORLD vocoder [10] and to the time-domain PSOLA algorithm [7]. The results in Table II show that AnCoGen is competitive with these reference methods. It achieves accurate $f_0$ control with a low AAE and maintains good speech quality with a high N-MOS, even for large shifts (\u00b150%)."}, {"title": "Speech denoising", "content": "In this experiment, noisy speech signals are denoised with AnCoGen by setting the SNR attribute A5 to 40 dB in the control stage of AnCoGen. The proposed approach is compared to the deep complex convolution recurrent network (DCCRNet) of [26], the deep complex U-net (DCUNet) of [27], the dual-path transformer network (DPTNet) of [28], and the fully-convolutional time-domain audio separation network (Conv-TasNet) of [29]. The results are presented in Table III. AnCoGen consistently outperforms other methods in most signal quality-related metrics (N-MOS, SIG, BAK, OVRL) under both matched and mismatched conditions, while achieving the second-best performance in terms of WER. This demonstrates that AnCoGen effectively balances noise reduction, speech quality, and intelligibility. However, it ranks last in speaker identity preservation (as measured by COS), which is consistent with the analysis-resynthesis results discussed above. Nevertheless, some researchers argue that it may be desirable to prioritize overall quality over fidelity to the reference signal, such as in terms of speaker identity [46]. One notable strength of AnCoGen is its capacity to generalize its performance in noise suppression and speech quality when the test conditions differ from the training ones. AnCoGen maintains a very stable N-MOS and BAK performance between matched and mismatched noise conditions (only -0.06 points of N-MOS and -0.12 points of BAK), whereas the reference methods show a significant drop in performance (between -0.89 and -1.23 points of N-MOS and between -0.37 and -1.24 points of BAK). The high quality of denoised signals is ensured by the fact that AnCoGen is a generative model trained to generate clean signals. One drawback, though, is that AnCoGen sometimes outputs a denoised signal with a phonetic content that does not exactly correspond to the input noisy signal."}, {"title": "IV. CONCLUSION", "content": "In this article, we presented AnCoGen, a bidirectional model that maps an audio signal into controllable speech attributes (such as pitch, loudness, identity, noise, and clarity index) and generates the corresponding signal from those attributes. Combined with a neural vocoder, the proposed approach allows for the analysis, control, and generation of speech signals using a single model. Extensive experiments showed that AnCoGen accurately estimates pitch even under noisy conditions, enables precise pitch manipulation, and improves speech quality in background noise, even under mismatched conditions. The main limitation of AnCoGen is its ability to recreate a speaker's voice for an unseen speaker accurately. Finer quantization of speaker embeddings could improve the voice reconstruction for new speakers, such as using a hierarchical codebook [47]."}]}