{"title": "Semi-Supervised Learning from Small Annotated Data and Large Unlabeled Data for Fine-grained PICO Entity Recognition", "authors": ["Fangyi Chen", "Gongbo Zhang", "Yilu Fang", "Yifan Peng", "Chunhua Weng"], "abstract": "Objective: Extracting PICO elements-Participants, Intervention, Comparison, and Outcomes\u2014 from clinical trial literature is essential for clinical evidence retrieval, appraisal, and synthesis. Existing approaches do not distinguish the attributes of PICO entities. This study aims to develop a named entity recognition (NER) model to extract PICO entities with fine granularities.\nMaterials and Methods: Using a corpus of 2,511 abstracts with PICO mentions from 4 public datasets, we developed a semi-supervised method to facilitate the training of a NER model, FinePICO, by combining limited annotated data of PICO entities and abundant unlabeled data. For evaluation, we divided the entire dataset into two subsets: a smaller group with annotations and a larger group without annotations. We then established the theoretical lower and upper performance bounds based on the performance of supervised learning models trained solely on the small, annotated subset and on the entire set with complete annotations, respectively. Finally, we evaluated FinePICO on both the smaller annotated subset and the larger, initially unannotated subset. We measured the performance of FinePICO using precision, recall, and F1.\nResults: Our method achieved precision/recall/F1 of 0.567/0.636/0.60, respectively, using a small set of annotated samples, outperforming the baseline model (F1: 0.437) by more than 16%. The model demonstrates generalizability to a different PICO framework and to another corpus, which consistently outperforms the benchmark in diverse experimental settings (p-value <0.001).\nConclusion: This study contributes a generalizable and effective semi-supervised approach to named entity recognition leveraging large unlabeled data together with small, annotated data. It also initially supports fine-grained PICO extraction.", "sections": [{"title": "1 Introduction", "content": "Evidence-based medicine (EBM) has gained increasing popularity over the past decades and has become the guiding principle of medical practice [1-5]. Aggregating, synthesizing, and understanding the best available clinical evidence is essential to enhancing decision-making in medical practices and optimizing treatment outcomes [6]. Meta-analysis is a crucial statistical technique in evidence synthesis that helps inform the best clinical actions by gathering and combining results from different research studies [7]. However, it is a highly time-consuming and labor-intensive process, making it impractical to constantly keep pace with the rapidly rising number of published studies [6, 8, 9]. The PICO (Participants, Intervention, Comparison, and Outcomes) framework serves as the basis for formulating clinical questions and facilitates the efficient retrieval, selection, and categorization of evidence from clinical studies. To enable and streamline the workflow of automated meta-analysis, a more granular PICO characterization is needed for accurately characterizing randomized controlled trials (RCTs). For example, instead of using the category of P to characterize participants, we need more information of different participant attributes such as age, sex, race, and ethnicity.\nAutomated PICO entity extraction is a named entity recognition (NER) task, wherein each token is tagged with a pre-defined label. Early methods relied on rule-based approaches, Conditional Random Fields (CRF) models, or a combination of basic classifiers [10-12]. These approaches necessitate exhaustive feature engineering. More recently, the adoption of deep learning algorithms, such as bidirectional long short-term memory (BiLSTM) networks [13-15] and BiLSTM models augmented with a CRF module [16, 17], have demonstrated superior performance without laborious feature extraction. Later, transformer-based models (e.g., BERT and its variants) have further advanced the field [18-21].\nDespite these advancements, several widely acknowledged challenges persist. One primary challenge is the lack of large, high-quality annotated datasets since annotation is a labor-intensive and time-consuming task that often requires domain experts. Furthermore, the absence of standardized PICO annotation guidelines, which becomes impractical due to variations in study purposes and domains, has further complicated the annotation process. The largest publicly available corpus, EBM-NLP [22], was reported to exhibit significant inconsistency in annotated results [23-25]. These inconsistencies are mainly attributed to unclear definitions of text span boundaries and complex annotation guidelines, resulting in suboptimal model performances [23-25]. To address these limitations, manual corrections or heuristic rule-based approaches have been leveraged to relabel entities [23, 25, 26]. Notably, Hu et al. proposed a two-step NLP pipeline that first classifies sections of sentences and then extracts PICO from sentences in Title and Method sections using BiomedBERT trained on re-annotated abstract [23]. Although their proposed method reduced annotation time for sentences rich in PICO information and achieved high inter-annotator agreement, the overall number of annotated abstracts remained considerably limited.\nAnother issue is the lack of fine-grained annotation. Most public datasets only provide coarse-level PICO annotations [27], which do not always meet the requirements for many downstream tasks, such as meta-analysis or evidence appraisal. Although the EBM-NLP dataset was unusually annotated with fine-grained PICO entities, these annotations are unsuitable for meta-analysis because they do not capture numeric values associated with outcome measures for different study arms (e.g., intervention and control). The ability to extract numerical data is critical for conducting a statistical analysis to evaluate the efficacy of the intervention [28]. Nevertheless, limited effort has been dedicated to extracting detailed outcome information, e.g., the number of subjects experiencing specific outcome events. Mutinda et al. introduced a fully annotated dataset comprising 1,011 randomized controlled trials (RCTs) on breast cancer [29]. While their PICO annotation framework was suitable for conducting meta-analysis, it did not include annotations for key population characteristics (e.g., sex) because their selected RCTs focused mainly on the female population. Therefore, the generalizability of NER models built using this dataset was significantly compromised.\nRecognizing these challenges, we proposed FinePICO, a semi-supervised learning (SSL) algorithm to enhance the extraction of fine-grained PICO entities. SSL is a branch of machine learning model that utilizes both labeled and unlabeled data for model training [30]. Compared to fully supervised learning that demands"}, {"title": "2 Materials and Methods", "content": "a vast number of labeled samples to achieve optimal performance, SSL effectively leverages abundant unlabeled data combined with scarce labeled data to improve learning outcomes. Current PICO extraction models heavily depend on the availability and quality of annotated samples, which are challenging to obtain and inconsistent across sites, thereby limiting their robustness and generalizability [13, 17, 22, 23]. In contrast, SSL offers significant advantages in low-resource settings where labeled data is expensive and sparse. While several limitations have been acknowledged, such as higher computational costs, risk of propagating errors, and assumption about data distribution [31, 32], SSL has been widely adopted and demonstrated promising results in various applications, such as object recognition and image segmentation [33, 34], document retrieval and classification [32, 35-37], and biomedical information mining [38]. The primary focus of this study is to explore SSL in fine-grained PICO extraction, as its efficacy in this area remains uncertain.\"\nOur main objective was to demonstrate that combining limited labeled data and a substantial volume of unlabeled data can achieve performance comparable to that of models trained using fully annotated data. Our findings suggested that SSL techniques can optimize fine-grained PICO extraction by greatly expanding the training sample size while minimizing reliance on extensive manual annotation efforts."}, {"title": "2.1 Workflow Overview", "content": "FinePICO employs an iterative SSL process to adjust model weights and generate pseudo-labels for unlabeled data.  depicts the overview design of the model. Specifically, we first develop a NER model using the available annotated data via the traditional supervised learning approach. Once the initial model is trained, it is deployed to make inferences on the unlabeled data, referred to as pseudo-labels. We enrich the original labeled data with the high-confidence pseudo-labeled data for fine-tuning the model. We iteratively repeat the cycle of generating pseudo-labels and updating model weights until the model's performance converges on the validation dataset or a predefined maximum number of iterations has been reached. To ensure the quality of the pseudo-labels and minimize the risk of error propagation, we incorporate a quality enhancement module. It performs a quality check and selects the generated labels with high confidence."}, {"title": "2.1.1 Foundation Model", "content": "To leverage the power of pre-trained language models, we select a BERT-based model as our foundation model [39]. We define S as the entire collection of sentences of interest, where $S_{label}$ refers to the sentences with pre-annotated named entity tags associated with their tokens. For each sentence $s \\in S_{label}$, we have a sequence of tokens $\\{t_{1}, t_{2},..., t_{im}\\}$, where each token $t_{ij}$ is associated with a label $y_{ij}$, and m is the length of the sentence s.\nWe also define $S_{unlabel}$ as the set of sentences without annotated named entity tags. We leverage the BERT-based model that was previously trained on $S_{label}$ to make inferences on $S_{unlabel}$ and generate the set of pseudo-labels $(\\hat{y})$ for each token in the unlabeled sentence $s\\in S_{unlabel}$.\nThe training and fine-tuning process involves applying the softmax function $\\sigma(.)$ on the last layer of the neural network to compute the probability $p^k_{ij}$ for the $k^{th}$ entity class associated with the token $t_{ij}$. The predicted entity class $\\hat{y}_{ij}$ is then determined as follows:\n$P^k_{ij} = \\sigma(z^k_{ij}) = \\frac{exp(z^k_{ij})}{\\Sigma_{u=1}^C exp(z^u_{ij})}$\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(1)\n$\\hat{y}_{ij} = arg \\max (\\sigma(z)_{ij}), P_{ij} \\subseteq \\sigma(z)_{ij}$\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(2)\nwhere z is an embedding-based representation of each token, and C is the total number of entity class. $\\sigma (z)_{ij}$ represents probabilities across entity tags for token $t_{ij}$. The target function is to minimize the cross-"}, {"title": "2.1.2 Supervised Learning Loss", "content": "entropy loss function. The loss function at token $t_{ij}$ is defined below:\n$LCE_{ij} = -\\sum_{k=1}^C \\mathbb{1}(Y_{ij} = k) \\log p^k_{ij}$\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(3)\nThe binary indicator $\\mathbb{1}(*)$ \u2208 {0,1} equals to 1 if a token belongs to the $k^{th}$ class and 0 otherwise. The overall loss function comprises of two parts: the supervised loss ($L_s$) and unsupervised loss ($L_u$).\n$L_{total} = Ls + \\alpha Lu$\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(4)\nWe leveraged $S_{label}$ as the main dataset for training and developing our initial baseline models $M_0$. The training process follows well-established supervised learning methods. In this stage, we aim to develop a model that can make reasonable inferences on unseen data. The baseline models were then iteratively refined using both $S_{label}$ and $S_{unlabel}$ to minimize the learning loss. The total supervised learning loss $L_s$ at $t^{th}$ iteration is computed as follows:\n$Ls = \\frac{1}{n^l} \\sum_{i=1}^{n^l} \\frac{1}{m_i} \\sum_{q=1}^{m_i} \\sum LCE^{t}_{ij}$\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(5)\nwhere $n^l$ refers to the number of sentences with annotation and $m^l$ is the number of tokens at $i^{th}$ sentence. $LCE^{t}_{ij}$ denotates as the supervised learning loss function at token $t_{ij}."}, {"title": "2.1.3 The Quality enhancement Mechanism of Pseudo-label Generation", "content": "The baseline model $M_0$ infers labels for each token in the unlabeled sentences. We incorporated the sets of pseudo-labels $\\{\\hat{Y}_1, \\hat{Y}_2,..., \\hat{Y}_{um}\\}$ with $\\{t_{i1},t_{i2},..., t_{itim}\\}$ of the sentence $s_u \\in S_{unlabel}$ into the original training pool $S_{label}$ to further improve $M_0$. For a token $t_{i}$ in the sentence $s_u$, its pseudo-label is formally defined as:\n$\\hat{y}^{u} = arg \\max (\\sigma(z)^{u}_{ij})$\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(6)\nTo maintain the quality and consistency of the generated pseudo-labels on a diverse set of training samples, we introduced a quality enhancement module to select the high-quality labels that would be used in subsequent training iterations. Specifically, we implemented three different quality enhancement approaches within the label selection process and evaluated their relative effectiveness in enhancing the overall model performances.\nThe selective unsupervised learning loss of a token is computed as follows:\n$LE = -\\sum_{k=1}^C \\mathbb{1} (\\hat{y}_i) \\log p^k_{ij}$\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(7)\n$\\mathbb{1} (\\hat{y}_i) = \\mathbb{1} (\\hat{y} = k) \\land \\mathbb{1}( f(\\hat{y}, t) )$\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(8)\nwhere the binary indicator $\\mathbb{1} (\\hat{y})$ = 1when the two conditions are met simultaneously. The quality enhancement function f minimizes noises resulting from erroneous predictions by checking if the pseudo-label $\\hat{y}$ is accurate or has a high degree of certainty. In this study, we investigated three checking strategies.\n1) Confident-based masking. This approach leverages prior studies that revealed the benefits of masking out low-confident examples from the training set [40, 41]. It uses a predefined threshold to filter out pseudo-labels lower than this level. The threshold is empirically determined to balance between maintaining high label quality and retaining a sufficient volume of training samples."}, {"title": "2) Class adaptive threshold-based masking", "content": "A recognized limitation of confident-based masking is its potential bias toward classes with higher quality pseudo-labels [42]. To address this issue, we also implemented a class-wise threshold adjustment algorithm, where the threshold for entity class k is dynamically calculated per iteration:\n$T^k = \\frac{\\Sigma_{t=1}^{n^u} \\max_j; P(k_{tu})}{\\Sigma_{u=1}^{n^u} \\Sigma_{t=1}^{m^u} \\mathbb{1}(y_{ij} = k)}$\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(9)\nwhere $n^u$ denotes the number of unlabeled sentences and $m^u$ refers to the number of unlabeled tokens. We update the threshold for each class and filter the token and its label if the associated probability is less than the dynamic threshold $T^k$.\n3) Label Selection via Model Distillation (GPT-based Selection). We leverage GPT-40 to evaluate the pseudo-label quality. With the tokenized sentences as input, we prompt GPT-40 to confirm whether the pseudo-labels are correct. Inspired by Hu et al. [43], we curate customized prompts for different entities. Each prompt includes annotation guidelines, error-based instruction, as well as a few annotated examples (Supplementary Table 1) The labels confirmed as accurate by GPT-40 are then incorporated into the new training dataset."}, {"title": "2.2 Data Source", "content": "We tested FinePICO with different data augmentation strategies, including the use of in-domain data, cross-domain data, and both. In-domain augmentation refers to the scenario where the labeled and unlabeled data are sampled from the same domain, while cross-domain augmentation refers to the scenario where the labeled and unlabeled are sampled from different domains.\nFor this purpose, we used four public datasets in this study, including PICO-Corpus [29], EBM-NLP [22] samples (n = 1,200 abstracts), and two sets of RCT abstracts [23] focused on Alzheimer's disease (AD) and COVID-19. The number of PICO entities is summarized in Table 1.\nPICO-Corpus includes 1,011 RCTs related to breast cancer, where each abstract was manually annotated for the pre-defined PICO subcategories (e.g., total sample size, age, and outcome values) [29]. EBM-NLP corpus composes RCT abstracts in diverse domains, where the training set of the abstracts was annotated by Amazon Mechanical Turk and inter-annotator conflicts were resolved via a voting strategy. Previous studies reported a lack of consistency and agreement among the annotators [22, 24, 25], with Cohen's kappa coefficient of inter-rater reliability being 0.3 [23]. Due to these limitations, we adopted the annotation scheme in PICO-Corpus and utilized EBM-NLP mainly for training data augmentation. We randomly picked 1,200 abstracts from EBM-NLP. The two datasets of AD and COVID-19 did not provide fine-grained PICO annotation; as such, these two were reserved for testing purposes only.\nFollowing the preprocessing workflow of earlier studies [44, 45], we extracted PICO entities from each sentence in the abstract. The RCT abstracts (n = 2,511) were tokenized into sentences using a Python library NLTK [46]. We divided sentences from PICO-Corpus into training, validation, and testing sets. The train-test splitting ratio was set to 80:20, and within the training set, we reserved 10% of sentences for validation. Clinical trials in EBM-NLP with PICO annotations removed were included as the unlabeled data in the training set. The two datasets, AD and COVID-19, were reserved for testing purposes. We adopted the BIO2 tagging schema in this task [47, 48], which is widely used in NER tasks. Specifically, each token in a sequence is labeled with a combination of a prefix and the type of predefined entities. The prefix indicates the beginning (B), inside (I), or outside (O) of the entities. The common method follows a two-step process that first identifies the relevant entities and then performs relationship extraction to determine intervention and control values. In our study, we streamlined the workflow by eliminating the relationship extraction step, as our detailed annotation labels explicitly classify these values into distinct entity categories (Table 1)."}, {"title": "2.3 Foundation Model Choice & Baseline Model", "content": "We first tested several open-source models (e.g., BiomedBERT [39], BioBERT [19], SciBERT [21], ClinicalBERT [49]) used by previous studies to extract fine-grained PICO entities. These models were built using all the labeled training data and were evaluated on the test set. We followed the same hyper-parameter settings described in the prior works [23, 45], using a learning rate of 5e-5, a batch size of 8, and a total of 10 training epochs.\nThe performances of several BERT-based models (BioBERT, SciBERT, ClinicalBERT, BiomedBERT) are detailed in Supplementary Table 2. BiomedBERT achieved the highest macro-average precision of 0.662, recall of 0.716, and F1 score of 0.688 in extracting fine-grained PICO elements, outperforming the other models. Such results aligned with the findings of a previous study focusing on extracting granular PICO information from texts [44], suggesting the superior performance of BiomedBERT in identifying PICO entities. Therefore, in the remaining experiments, we used BiomedBERT as the baseline model.\nConsidering the constraints of limited available annotations, we defined an ideal scenario where the unlabeled data would be annotated by human experts. We used the model performance from this ideal scenario as the upper bound of SSL model performance in our experiments."}, {"title": "2.4 Data Augmentation with Unlabeled Data", "content": "We augmented the training data with unlabeled text corpus from three distinct domains: in-domain (similar domain with the labeled data), cross-domain (different domains from the labeled data: EBM-NLP), and all-domain (both in-domain and cross-domain unlabeled data). To evaluate the in-domain and all-domain cases, we masked out annotations with different ratios in the training data. Specifically, we randomly selected 10%, 30%, 50%, 70%, 90%, and 100% of the sentences from the training set to act as labeled data and treat the rest as unlabeled data (Supplementary Table 3). The proposed algorithm was assessed across these different masking ratios and compared with the performances of the baseline model."}, {"title": "2.5 Generalizability Test on an Enhanced PICO Scheme", "content": "To demonstrate generalizability, we evaluated FinePICO on a newly annotated dataset under a revised guideline adopted from the one used for PICO-Corpus. The first change is a new demographic entity representing the genders of participants. Gender is an important demographic characteristic that enables the exploration of varying treatment effects across different gender subgroups [50, 51]; however, it was not included in the original annotation scheme.\nTo streamline the gender entity labeling process, we constructed a gender entity tagger using the Biomed-BERT fine-tuned on carefully selected samples from EBM-NLP. The samples were selected by first extracting sentences containing tokens tagged with the \u201csex\u201d entity label, followed by manual validation, and supplemented by a keyword search approach to ensure accurate extraction of the sex entity from the text. The final data comprised 569 sentences, partitioned with 80% for training, 10% for validation, and 10% for testing.\nWe trained the model for 5 epochs with a learning rate of 5e-5, achieving a high F1 score of 0.989. The best-performing model was then utilized to recognize sex entities in the PICO-Corpus (training and validation set). Finally, two researchers (FC, YF) manually annotated the sex entity in the testing set to provide a benchmark, with Cohen's kappa score of 0.98.\nThe second change involves replacing and consolidating several categories to enhance clarity and efficiency. The revised PICO scheme is illustrated in Figure 2, and the details of the entity counts can be found in Supplementary Table 4. Specifically, we combined the \u201csubject eligibility\" and \"conditions\" into a single entity group now named \u201crecruited participant eligibility conditions.\u201d This merger reflects their interrelated"}, {"title": "2.6 Evaluation Metrics", "content": "nature and simplifies the tagging process. Additionally, we combined \u201coutcome names\u201d and \u201coutcome measures\" into one group to avoid redundancy and streamline the dataset.\nWe tested our models on two independent test sets (PICO-Corpus, AD, and COVID-19 from Hu et al. [23]). In the first test set derived from the PICO-Corpus, we evaluated our NER models at a strict entity level that requires the recognition of the complete span of each entity. Since token-level evaluation can be misleadingly high for the intended task, as missing tokens could result in significant misinterpretation, it is essential to accurately capture entire PICO entities. We computed the macro-average precision, recall, and F1 score using seqeval [52], a well-tested tool often deployed in numerous NLP studies for system evaluation [53]. The 95% confidence interval of the performance was estimated based on the bootstrapped test samples.\nAcknowledging the variance in annotated spans across different datasets, we conducted a second evaluation using partial-matching [54] on AD and COVID-19 datasets. Here, we counted a predicted named entity as a true positive if it overlaps with the human-labeled entities with at least one token. It is worth noting that AD and COVID-19 did not include fine-grained PICO annotation. Therefore, we first converted the predicted fine-grained entities into coarse-level entities and evaluated them using a partial matching strategy [54]."}, {"title": "3 Results", "content": null}, {"title": "3.1 Performance on Limited Labeled Samples", "content": "Performance on Limited Labeled Samples\nThe baseline models were established solely using labeled samples. The lower bound performance refers to the baseline model evaluated on the test set, whereas the upper bound corresponds to the model trained on the entire set of labeled training samples and evaluated on the test set."}, {"title": "3.2 Performance Comparison of Different Quality Enhancement Approaches", "content": "The performances of three quality enhancement strategies for optimizing pseudo-label selection are summarized in Table 2. All three quality enhancement methods outperformed the baseline models by over 10% in precisions, recall, and F1 scores, with their respective 95% confidence intervals (CIs) provided in Supplementary Table 5. In the original PICO scheme, GPT-based selection achieved the highest performance (average F1 of 0.6, 95% CI between 0.609 and 0.664) among the three methods. However, we did not perceive any statistical enhancement (p-value =0.171) using GPT-based selection over the confident-based masking algorithm. In the revised PICO scheme, the adaptive threshold-based method was the most effective in selecting high-quality pseudo-labels among the three quality enhancement approaches, obtaining the highest average F1 score of 0.653 (95% CI: 0.657 - 0.706) when augmented with in-domain unlabeled data. Additionally, both confident-based and adaptive threshold-based masking methods have performed statistically better than GPT-based selection (p-value < 0.05)."}, {"title": "3.3 Generalizability Assessment", "content": "To assess the generalizability of FinePICO, with the consideration of available resources, we selected confident-based masking as the primary quality enhancement approach. The best-performing models were examined on additional data augmentation cases ranging from 30% to 100% of annotated samples."}, {"title": "3.3.1 Additional Data Augmentation Scenarios", "content": "Table 3 presents the average performances of models with different data augmentation cases, with the baseline levels detailed in Supplementary Table 6. Our analysis revealed a positive linear relationship between model performance and the number of annotated samples used for training. Specifically, performance increased from an F1 score of 0.667 (cross-domain) with 30% of the annotated data to 0.695 with the entire labeled data. This suggests that while additional labeled data continues to improve the model performance, the marginal gains diminish as the proportion of annotations approaches 100%.\nAs we increased the number of annotated samples while keeping the size of unlabeled training samples constant, we consistently observed statistically significant improvements (p-value<0.001) in the model's performance compared to the benchmark. These improvements were particularly notable in the extreme case when the maximum amount of labeled data was used (Figure 4). Furthermore, the performance of the proposed algorithm consistently surpassed the baseline levels across the revised PICO scheme, showcasing the model's robustness and adaptability.\nAdditionally, we examined the performance differences among semi-supervised learning under various data augmentation approaches (in-domain, cross-domain, all-domain). In the original PICO scheme, models trained on both cross-domain and all-domain data performed statistically better than models trained using in-domain data (p-value < 0.001), whereas, in the revised scheme, we observed the opposite trend."}, {"title": "3.3.2 Evaluation on the Independent Testing Sets", "content": "We further applied the best-performing model to another independent testing corpus (AD, COVID-19) [23], and the averaged performances over 30 bootstrapped samples, along with the baseline levels, were recorded in Table 4. The proposed model demonstrated statistically significant improvement (p-value = 0.014 in the original scheme and p-value = 0.025 in the revised scheme) over the baselines evaluated under AD and"}, {"title": "3.4 Error Analysis", "content": "COVID-19 corpus.\nWe conducted an error analysis of our optimal model on 100 sentences randomly selected from the test set and identified the following error categories: 1) boundary detection error (n =14), 2) entity misclassification (n=10), and 3) failure to detect the presence of the entity (n=9). Examples of these categories are summarized in Supplementary Table 7. Boundary detection errors are the most prevalent, suggesting that the model often failed to capture the complete entity span, especially in the names of the intervention arms and measured outcome. For instance, in the sentence \u201cA key secondary endpoint was the feasibility of achieving 12 meth/week (metabolic equivalent of task hours per week)\u201d, the outcome measured was annotated as the entire phrase \u201cfeasibility of achieving 12 meth/week (metabolic equivalent of task hours per week)\u201d. However, our model failed to identify the content within the paratheses as part of the outcome name. Entity misclassification was the second most common error, occurring when the model incorrectly assigned values to different arms; for example, it misclassified intervention outcomes values as belonging to the control arm."}, {"title": "4 Discussion", "content": "In this study, we developed a semi-supervised learning approach to overcome several key challenges in fine-grained PICO entity recognition, including the limited amount of high-quality annotated data and the lack of standardized fine-grained PICO annotation guidelines. These limitations have historically hindered the adaptability and generalizability of existing PICO extraction models.\nFinePICO demonstrated substantial improvements (p-value < 0.001) compared to the baseline models across various experimental settings, including in-domain, cross-domain, and all-domain datasets. This was especially evident in scenarios where a large percentage of trained samples were unannotated. For instance, in the case where only 10% of the training sample was labeled, FinePICO demonstrated an overall improvement of over 16% in F1 score compared to the conventional supervised learning-based approach (in the original PICO scheme, our best model using a GPT-based label selector achieved an average F1 of 0.60 versus 0.437 for the baseline model, p-value < 0.001). FinePICO also consistently outperformed the benchmarks when applied to the revised PICO scheme, demonstrating its robustness and adaptability to varied annotation guidelines. This flexibility allows users to use their preferred fine-grained PICO scheme. As shown in the experiments (Figure 4), the proposed algorithm effectively enhanced the model performance by augmenting training samples without needing an additional manual labeling process, significantly surpassing the models trained exclusively on fully annotated datasets.\nPrior research [33, 55, 56"}]}