{"title": "BIG-MATH: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models", "authors": ["Alon Albalak", "Duy Phung", "Nathan Lile", "Rafael Rafailov", "Kanishk Gandhi", "Louis Castricato", "Anikait Singh", "Chase Blagden", "Violet Xiang", "Dakota Mahan", "Nick Haber"], "abstract": "Increasing interest in reasoning models has led math to become a prominent testing ground for algorithmic and methodological improvements. However, existing open math datasets either contain a small collection of high-quality, human-written problems or a large corpus of machine-generated problems of uncertain quality, forcing researchers to choose between quality and quantity. In this work, we present BIG-MATH, a dataset of over 250,000 high-quality math questions with verifiable answers, purposefully made for reinforcement learning (RL). To create BIG-MATH, we rigorously filter, clean, and curate openly available datasets, extracting questions that satisfy our three desiderata: (1) problems with uniquely verifiable solutions, (2) problems that are open-ended, (3) and problems with a closed-form solution. To ensure the quality of BIG-MATH, we manually verify each step in our filtering process and iteratively improve our filters over multiple rounds. Based on the findings from our filtering process, we introduce 47,000 new questions with verified answers, BIG-MATH-REFORMULATED: closed-ended questions (i.e. multiple choice questions) that have been reformulated as open-ended questions through a systematic reformulation algorithm. Compared to the most commonly used existing open-source datasets for math reasoning, GSM8k and MATH, BIG-MATH is an order of magnitude larger (250,000 questions vs. 8,000 questions in GSM8k and 12,000 in MATH), while our rigorous filtering ensures that we maintain the questions most suitable for RL. We also provide a rigorous analysis of the dataset, finding that BIG-MATH contains a high degree of diversity across problem domains, and incorporates a wide range of problem difficulties, enabling a wide range of downstream uses for models of varying capabilities and training requirements. In conclusion, this work presents our new dataset, BIG-MATH, the largest open dataset of math problems suitable for RL training. By bridging the gap between data quality and quantity, BIG-MATH establish a robust foundation for advancing reasoning in LLMs.", "sections": [{"title": "1. Introduction", "content": "In the past few years, mathematics has emerged as a critical testing ground for the development and evaluation of advanced reasoning techniques used in Large Language Models (LLMs) [Cobbe et al., 2021, Wei et al., 2023, Schaeffer et al., 2023, Mirzadeh et al., 2024, OpenAI et al., 2024, DeepSeek-AI et al., 2025, inter alia]. Following the release of strong reasoning models, such as OpenAI's 01 [OpenAI et al., 2024] and DeepSeek's R1 [DeepSeek-AI et al., 2025], a plethora of supervised fine-tuning (SFT) datasets have been released with the aim to distill reasoning capabilities into other models, or to bootstrap a model prior to RL [Liu et al., 2024, Li et al., 2025, AI, 2025, Labs, 2025, Muennighoff et al., 2025]. However, knowledge distillation has its limitations. For example, research has shown that while SFT distillation enables models to memorize reasoning patterns and solutions, the resultant models do not generalize well [Chu et al., 2025]. On the other hand, RL training algorithms have been shown to yield models with better generalization to novel problems by emphasizing the exploration and refinement of reasoning strategies. Furthermore, [Wang et al., 2025] find that current reasoning models (across model sizes and families) frequently achieve the correct answer with incorrect reasoning steps, suggesting that distilled reasoning models will also suffer from incorrect reasoning. Additionally, OpenAI et al. [2025] show that scaling RL training is very effective in improving reasoning and other capabilities. Therefore, while distillation through SFT datasets is clearly a helpful method for improving model performance on some evaluations, it does not address the greater needs at the forefront of reasoning capabilities which can be addressed by RL: learning how to reason [Xiang et al., 2025, Yeo et al., 2025, Kim et al., 2025].\nA significant bottleneck in RL-based reasoning research is the lack of high-quality datasets tailored for reinforcement learning. RL training methods all assume access to a dataset with verifiable answers [DeepSeek-AI et al., 2025, Team et al., 2025]; either to train a reward model [Lightman et al., 2023b, Havrilla et al., 2024b, Zhang et al., 2024, Mahan et al., 2024] or to directly evaluate the correctness of a generated solution and, as has been demonstrated through the course of deep learning's existence, scaling up data quantity and quality is a crucial step towards success [Havrilla et al., 2024a]. However, a central issue with the existing math datasets is that they are either (1) human written, but limited in quantity, or (2) machine generated and large, but of unknown quality, forcing researchers to choose datasets with either quality or quantity. For example, prior works have mostly used the GSM8k [Cobbe et al., 2021] and MATH [Hendrycks et al., 2021] datasets. While they contain human-written questions and answers, both datasets are quite limited in quantity, with 8,000 and 12,000 problems, respectively. On the other hand, large-scale datasets, such as NuminaMath [Li et al., 2024b], exhibit quality issues, including many duplicate problems and incomplete solutions, hindering their utility in RL training. Furthermore, many existing math datasets contain problems which are not well-suited for training a reasoning model with RL. For instance, they sometimes have a high proportion of multiple choice questions. While multiple choice questions may be useful for SFT, they are less effective for RL-based training. Recent works [Xiang et al., 2025], suggest that the goal of RL for reasoning is not just to train the model to answer correctly, but more importantly, to reason correctly. With this in mind, questions in multiple choice formats are problematic for RL; even though the correct answers may be difficult to deduce, the model can simply guess the correct answer without performing the correct reasoning. Problems like these highlight the biggest issues with curating a dataset for RL.\nTo address these challenges, we present BIG-MATH, a dataset of over 250,000 high-quality math problems and solutions, curated with three core desiderata:\n1.  Uniquely verifiable solutions: problems must admit a single correct answer that can be reliably verified;\n2.  Open-ended problem formulations: problems that cannot be easily solved by guessing (as might occur in multiple choice formats) and instead require nontrivial reasoning steps; and\n3.  Closed-form solutions: answers must be expressible in a closed form (e.g., a scalar or formula, not a proof), thereby enabling automated evaluation.\nWith these desiderata in mind, we design and develop a process of cleaning and curating datasets by applying a strict set of filters. To ensure the quality of our filtration process, we apply a human-in-the-loop algorithm, iteratively improving each filter through multiple rounds of manual verification until the filter achieves suitable levels of precision and recall (minimum of 90%). Then, we apply our filtration process to three openly available datasets, extracting a high-quality and large-scale subset of over 200,000 uniquely verifiable, open-ended problems with closed-form solutions. Additionally, we introduce BIG-MATH-REFORMULATED, a novel subset of 47,000 problems derived by reformulating multiple-choice questions into an open-ended format, while preserving their integrity and complexity"}, {"title": "2. BIG-MATH", "content": "In this section, we describe the technical details of our data collection, cleaning, filtering, and reformulation processes."}, {"title": "2.1. Dataset Collection", "content": "We considered a number of openly available math datasets and selected 3 well established math-ematical problem datasets that are commonly used in recent literature: HARP [Yue et al., 2024], Omni-MATH [Gao et al., 2024], and NuminaMath [Li et al., 2024b]. The goal of this work is to find a large set of high-quality math problems that satisfy our three desiderata. Human-written problems are generally associated as being high-quality, and the datasets that we select provide a large quantity of human-written problems to start with (> 75%). Additionally, we choose to incorporate synthetically generated data, but limit ourselves to a single, well established source"}, {"title": "2.2. Dataset Cleaning and Filtering", "content": "The collection of all the above datasets leads us to a combined dataset of over 640,000 problems. However, this dataset likely has many duplicated questions, undesirable content, and data that does not satisfy our desiderata. To achieve a dataset of the highest quality, and appropriate for RL training, we next clean and filter the data from each source using a combination of bespoke and common strategies [Albalak et al., 2024, Soldaini et al., 2024, Li et al., 2024a]. We ensure the quality of our filters by applying human-in-the-loop methodology, iteratively refining the filters through human verification and annotation of positive and negative examples. By the end of the iterative process our filters achieve over 90% F1 score, oftentimes reaching much higher than 90%. After filtering, the data should contain only problems that closely follow our three desired properties: open-ended, verifiable, closed-form problem-solution pairs."}, {"title": "2.2.1. Source-specific Filtering and Cleaning", "content": "The first step in our cleaning and filtering process is to observe a sampling of data from each dataset, and to design bespoke filters to be utilized on each source separately based on their unique idiosyncrasies.\nHARP For the HARP dataset [Yue et al., 2024], we find many problems that contain figures in the Asymptote vector graphics language, which we identify by string matching for \u201c[asy]\u201d. We err on the side of caution and remove 625 such problems (13% of the dataset), assuming that the model would need to see the rendered image to solve the question.\nOmni-MATH When exploring Omni-MATH [Gao et al., 2024], we found a number of problems containing author attributions (e.g. a person's name in parenthesis, or in the format \u201c[i] Name [/i]", "If the correct answer is X and your answer is Y": "n a number of problems. Additionally, we found and removed 2 problems with the following solution:", "extracted.": "likely a parsing error when extracting problem-solution pairs from online sources.\nNuminaMath Within NuminaMath [Li et al., 2024b], we find some unique characteristics of each subset. First, because some of the subsets are quite large, we deduplicate problems within each subset with a MinHashLSH filter. We use 128 hashing functions, and through a few rounds of experiments, empirically determine that a similarity threshold of 0.6 or 0.7 (depending on the subset) is the strictest threshold we can set without the filter quality degrading. Next, the NuminaMath dataset does not explicitly contain answers to each problem, so we extract answers by searching for boxed solutions (\u201c\\boxed{}", "proposed by": "year of submission, point scoring (e.g"}, {"title": "2.2.2. Source-agnostic Filtering", "content": "After running each of the described filters over the individual subsets, we perform 11 filtering operations across the full collection. These filters are specially designed to convert the raw dataset of question-answer pairs into a dataset that is suitable for training a math reasoning model with RL. Just as for the source-specific filters, we also manually verify the results of each source-agnostic filter, and iteratively improve their performance until they achieve suitable results (where appropriate). The high-level results of our filtering process are found in Table 2, with more detailed results in Appendix A. Prompts for model-based filters are found in Appendix D.1.\nDeduplication and Decontamination To prevent a model from unknowingly being trained on the same problem too frequently (and risk overfitting), we need to remove duplicates. To handle this, we run a very simple and efficient deduplication step, calculating duplicate problem by string matching (not including whitespace), removing all but one copy of the duplicated problem. Next, we also want to ensure that the final dataset has a diverse set of problems, without focusing on any specific problem types. To do so, we remove semantic duplicates (problems with similar meaning, e.g. the same problem with numbers changed) with the SemDeDup algorithm [Abbas et al., 2023]. To embed the problems, we use the model at sentence-transformers/all-MiniLM-L6-v2 and remove problems with a cosine similarity over 0.5. For semantic deduplication, we tested thresholds between 0.95 and 0.2, finding that even the strictest thresholds can cluster dissimilar problems (likely caused by short problems). We want to ensure that no duplicated data passes through, so we purposefully select a threshold where our manual verification determines that no duplicates remain. Finally, the MATH [Hendrycks et al., 2021] and Omni-MATH [Gao et al., 2024] test sets are prime candidates for evaluating a model trained on our dataset, so we need to ensure that the problems in those test sets do not exist in our training set. To decontaminate our dataset with any of the 500 MATH test set problems and the 500 Omni-MATH test set problems we find and remove all contaminated data by a string matching algorithm on the problems, the same as for deduplication. We were surprised to find minimal duplication (< 1%), removing only 4,229 problems, with the majority of duplicates being"}, {"title": "Ensuring that problems are solvable", "content": "For a number of reasons, there are problems in existing math datasets which are not solvable.\n(Language Filter) First, in this work, we focus on English-only models and require English math problems. So, we use a FastText language identifier and remove any problems where English is not the primary language (only 101 non-English problems were detected). Through our iterative improvement process, we found that it was important to remove LaTeX, along with most special characters (e.g. numbers, math symbols, \u201c()[]{}!@#$%^&\", etc.) in order to achieve a high level of accuracy with the language detection model. Additionally, problems which were very short (< 10 characters) were often classified as non-English (even if they were entirely numbers), so we simply include all problems with fewer than 10 non-LaTeX, non-special characters.\n(Hyperlink Detection) Next, we remove problems containing a hyperlink using a simple regular expression, as the existence of hyperlinks suggests that a model may not have the full resources required to solve the problem (e.g. hyperlinks that point to a website containing a theorem). While this likely removes problems that are solvable (hyperlinks may also link to the website source of the problem), we prefer to err on the side of caution and remove all problems containing hyperlinks. This over-filtering can be addressed in the future through more complex filters.\n(Model Solve Rate) Finally, while it is not feasible to manually ensure the correctness of each problem-answer pair, we develop a heuristic for correctness using language models. For each problem, we generate 64 solutions from Llama-3.1-8B (~ 30,000,000 rollouts) and 5-8 solutions from Llama-3.1-405B (~ 1,100,000 rollouts, generated on a pre-filtered subset) [Dubey et al., 2024]. If either model answers the question with the ground truth answer, then we determine that the question-answer pair may be valid. We do not apply this filter to HARP, Omni-Math, MATH, or GSM8k as these datasets include pre-parsed answers.\nOf course, this method does not guarantee that the given answer is correct, as it is possible that the answers fall under a commonly made mistake, or that the models have seen the data during pre- or post-training. Furthermore, this filter does not guarantee that removed data has an incorrect answer, as it is very likely that the models we use cannot solve the most difficult math problems. One method for improving this filter would be to use a stronger, math-specific model.\""}, {"title": "Ensuring that problems are open-ended", "content": "An important aspect of reinforcement learning is that the training signal should appropriately attribute the good actions with high rewards and poor actions with low rewards. Therefore, problems with multiple choice answers pose a problem: the model can inadvertently respond with the correct answer option (generally between a 25-50% chance of guessing correctly) without providing the correct intermediate reasoning steps, leading to a poor learning signal. For this reason, we choose to remove any problems that are multiple choice, True/False, and Yes/No. To detect and remove all three types of questions, we develop both a regular expression-based filter, and a model-based filter.\n(Regular Expression Filters) For multiple choice questions, we use a simple regular expression filter that searches for either alphabetic options (A, B, C, D) or numerical options (1, 2, 3, 4), occurring in order. To ensure that we do not incidentally remove questions referring to shapes (e.g. \"rectangle ABCD...\") or numbers (e.g. 1234), we first remove those strings from the question, prior to the regular expression search. Next, for the True/False questions, we search for either \u201ctrue\u201d or \u201cfalse\" in the answer or, when available, in the final line of the solution. Then, for Yes/No questions, we perform the same check as True/False questions, searching in the answer or solution for exact phrases."}, {"title": "Ensuring that problems are uniquely verifiable", "content": "A critical aspect of training reasoning models with reinforcement learning is the existence of verifiable answers, generally in the form of a ground truth to be compared against the model response.\n(Answer Filter) As a simple first step, we remove all examples where the final answer did not previously exist, or could not be extracted from the solution (e.g. if there was no \u201c\\boxed{}", "filter": "a dual-filter approach including a regular expression filter and a model-based filter. For the regular expressions, we iteratively improve the filter through manual search of the data and improvements to the regular expressions. The final version of our filter searches for commonly found signals: ordered roman numerals (e.g. I, II), multiple numbered parts in parentheses (e.g. (1) ... (2)), multiple numbered parts with a period (e.g. 1. 2.), as well as numbered special characters (e.g. \u2460 2). For the model-based filter, we use the same iterative process as for the multiple choice question, using a Llama-3.1-70B [Dubey et al., 2024], inspecting 100 examples of positively and negatively classified examples at each iteration, ending only once we achieve over 98% recall.\n(Proof Filter) While proofs can be verified, there can be many correct variations, and how to quickly verify proofs written in natural language is unclear at the moment (other than converting to a theorem proving language, which can incur additional parsing errors). Therefore we also elect to remove proofs from our dataset, leaving them for a future version which is more difficult. We develop our proof filter with the same dual-method approach previously discussed. The regular expression filter simply searches for either of the following phrases in the problem: \u201cprove that\u201d or \u201ca proof"}, {"title": "2.3. BIG-MATH-REFORMULATED", "content": "Based on findings from the previous sections we introduce a new subset of 47,000 questions and answers, BIG-MATH-REFORMULATED, which we describe here.\nDuring the development of our filters, we found a staggering number of multiple choice questions"}, {"title": "2.3.1. Reformulation strategy", "content": "We develop a 4-step process to reformulate multiple choice problems as open-ended problems. All prompts can be found in Appendix D.2, and we use Llama-3.1-405B [Dubey et al., 2024] to create BIG-MATH-REFORMULATED.\n(Key Information Extraction) We begin the process by identifying a few core pieces of information about each question. We determine whether the question is in the multiple choice format, extract the core mathematical concepts (e.g. geometry), and identify key problem details (e.g. distances, goal of the question). In the first step, we also ask the language model to develop a strategy to convert the question into an open-ended format, as well as strategies for rephrasing and ensuring that the integrity of the original problem is maintained. Finally, we extract a plan for what the format of the final answer should be (e.g. the answer should be expressed in cm\u00b3).\n(Reformulation) Next we reformulate the multiple choice question into an open-ended question by conditioning on the key extracted information extracted."}, {"title": "2.3.2. Reformulation Post-Processing", "content": "The outcome of our reformulation is 88,983 questions that have passed the judgement and verification steps. However, we still need to ensure that the questions in BIG-MATH satisfy the same criteria: uniquely verifiable, open-ended problems with closed-form solutions. Therefore, we next examine the solvability of each reformulated problem by evaluating Llama-3.1-8B (8 rollouts) and Llama-3.1-405B (3 rollouts) on them. We filter the 88,983 problems down to only 48,698 by keeping only problems that were solved at least once by either model, but not solved 100% by Llama-3.1-8B (to remove questions that may be too simple or obvious). In instances where neither model produces a solution that matches the reformulated answer, it is difficult to determine whether the issue lies with the model's performance, or with the reformulated answer. To mitigate such uncertainty, we exclude these problems from the dataset.\nFinally, BIG-MATH-REFORMULATED undergoes the same comprehensive filters as the rest of the datasets, as outlined in Section 2.2.2, yielding a final set of 47,010 reformulated and filtered problems, with the composition of source data found in Table 3. We find that this process has successfully reintroduced high-quality questions that were previously removed. Specifically, 72.7% of the amc_aime subset was found to be multiple choice, but the reformulation process successfully reintroduces 63.4% of the multiple choice problems."}, {"title": "3. Analysis and Discussion", "content": "In this section we discuss and analyze the BIG-MATH dataset. We consider the dataset difficulty, diversity, and outcomes of the filters that we proposed. Throughout the analysis, we include discussion points to aid the downstream uses of BIG-MATH."}, {"title": "3.1. Dataset Difficulty", "content": "We calculate the difficulty of problems in BIG-MATH based on rollouts from the Llama-3.1-8B model [Dubey et al., 2024], which provide a benchmark for understanding problem complexity. For each problem in the dataset we generate 64 rollouts and calculate the success rate per problem. The distribution of success rates, split by source and domain is found in Figures 3 and 4.\nFirst, Figure 3 shows that the majority of the easiest data (highest solve rate) comes from the Orca-Math, cnk_12, and MATH datasets, while the most difficult data is divided more evenly across the datasets. In particular, we find that nearly all of Omni-MATH and HARP are unsolveable by Llama-3.1-8B. Thus, in order to apply RL for Llama-3.1-8B on these difficult subsets, this particular model would need to be either be supervised fine-tuned on these datasets, or using an RL training algorithm that makes use of a process reward model. For example, Reinforcement Learning with Verifiable Rewards (RLVR) [Lambert et al., 2024] would be unlikely to work effectively on Omni-Math and HARP as the models responses would produce no training signal.\nNext, we group problems into difficulty quintiles, with the hardest quintile being problems that have a success rate less than 20% and the easiest quintile with a success rate over 80%. We find that, from easiest to hardest, the quintiles have 71,926 (28.64%), 30,533 (12.16%), 25,763 (10.26%), 31,249 (12.44%), and 91,647 problems (36.50% of the total problems). An obvious question now is: how should practitioners use these dataset difficulties for their own purposes? In general, those training less capable, or smaller, models may want to remove the most difficult problems as it is unlikely that model rollouts will lead to a correct answer. This leads to inefficiency in the learning process because most RL methods used for LLMs (except those with a process reward model) will have 0 signal if the model can never come to the correct answer. On the other hand, for those training a larger, or math-specific, model will find many of the easy questions redundant, and training on such data will be inefficient. Therefore, for practitioners training strong models it would be sensible to keep only the harder problems. Supposing that the hardest two quintiles of data are retained, there is still > 120,000 problems, 10 times more problems than the next closest RL-suitable dataset.\nNext, we look at the difficulty of our novel BIG-MATH-REFORMULATED subset. We see that our subset follows a similar solve rate distribution as the rest of the dataset; it has slightly more density around the low- and high-ends of the difficulty distribution. However, BIG-MATH-REFORMULATED is skewed towards more difficult problems. Specifically, we find that 34.44% of BIG-MATH-REFORMULATED is in the hardest quintile, with an additional 16.42% in the second hardest quintile, combining to greater than 50% of the new data.\nFinally, we look into the distribution of solve rates by each problem domain, shown in Figure 4. We find that the most difficult problems come from the differential equations, discrete mathematics, and abstract algebra domains, while the prealgebra domain is the easiest by a wide margin. Interestingly, the remaining domains have a very wide distribution of difficulties, suggesting that within each domain there are likely problems requiring varying levels of expertise. Surprisingly, linear algebra was found to be one of the easier domains, while geometry was one of the most difficult domains, however, this may either be an artifact of the domain classification process, or of the specific training data for Llama-3.1-8B."}, {"title": "3.2. Dataset Diversity", "content": "To better understand our dataset and the diversity of problems it provides, we next study the domain composition and distribution of our dataset.\nWe consider the distribution of problems sorted into mathematics domains according to two ontologies. The first ontology we consider is that proposed by Gao et al. [2024], and the second is the ontology defined by the American Mathematical Society in their 2020 Mathematics Subject Classification (MSC). We use the same prompt and procedure as Gao et al. [2024] to classify questions into their domains. Similar to Muennighoff et al. [2025], we classify problems into one of the 63 domains defined by the MSC using GPT-40-mini. The domains of Gao et al. [2024] contain high-level topics ranging from grade-school to college-level, while the MSC domains are more fine-grained and include more detailed domains on math-adjacent topics. For the domains defined by Gao et al. [2024], we present examples in Figure 6 and a chart with the number of problems per domain in Figure 5, as well as a detailed breakdown of domains by source in Section B in the Appendix. For the MSC domains, Figure 5 shows the distribution of all domains with 10 or more problems, and Figure 8 shows example problems for the top-5 most common domains.\nWe see in Figures 5 and 7 that the distributions for both ontologies have long tails. The largest domain from Gao et al. [2024] is the Math Word Problems, which we find to come disproportionately from Orca-Math, which contains > 66,000 such problems. Surprisingly, we find that the largest MSC domain was operations research, and we inspected the data under this category to better understand where such a large quantity came from. We found that these problems are generally an application of algebra, geometry, or statistics to real world domains and could just as easily have been classified into another category. We found that the distribution of MSC domains follows a nearly log-linear relation, with large quantities of problems in college-level topics, including ~ 10,000 problems on ordinary differential equations, field theory, and optimal control. We also found smaller quantities of applications in the sciences in the dataset, with problems in electromagnetic theory, thermodynamics, and fluid mechanics, showing that the collected dataset contains problems from a wide variety of mathematics domains."}, {"title": "3.3. Filter Analysis", "content": "In this section we provide an analysis of the filtering process we designed to curate BIG-MATH. See Table 2 for the main results and Appendix A for detailed results. Before looking closely at the filter outputs, it is important to note that our filtering process is intentionally designed to be more strict than is necessary. This conservative approach ensures that BIG-MATH contains only the types of data that we desire. However, this leaves room for improvements to our filtering process, and expansion to the BIG-MATH dataset. The filtering set-up proposed in this work serves only as a first step, allowing us to identify areas for improvement to maximize the utility of the source data and develop more precise filters.\nIn order to best identify where to spend future efforts, we start by identifying the filters which remove the most data: multiple-choice, multi-part, and proof filters."}, {"title": "3.3.1. Multiple Choice Filter", "content": "The largest portion of removed data falls under the multiple choice filter (~ 18%), with over 80% of the multiple choice questions being sourced from the cn_k12 subset. Since this filter affects such a large number of problems, we see this as promising area for improvement in the future, where the recall of filters can be improved to reintroduce some of the open-ended questions that are mistakenly classified as multiple choice. Additionally, we find large discrepancies in the classifications of the regular expression filter compared with the model-based filter, especially for the olympiad and MATH subsets, where the regular expression-based filter removes significantly more data than the model-based filter. A closer look at these data, which are classified as positive by one filter but negative by the other filter, are a prime area for further inspection. Exploration in this area can further improve the recall of the whole filtering system, and may lead to 1,000s of reintroduced problems."}, {"title": "3.3.2. Multi-Part Filter", "content": "Similarly, the multi-part filter shows potential for further refinement. Again, we see that the regular expression-based filter consistently removes a larger portion of data than the model-based filter. Specifically, we find that the regular expression-based filter removes 14,000 more questions from Orca-Math, 6,500 more from Olympiads, and 2,700 more from the MATH subset, compared with the model-based filter. Given these large differences, there is a clear opportunity to further improve this filtering step."}, {"title": "3.3.3. Proof Filter", "content": "Interestingly, the proof filter has the opposite outcome from the multiple choice and multi-part filters: the model-based filter removes 10,000 more problems than the regular expression-based filter. This is again an area for further investigation to determine how many of those problems can be reintroduced to the dataset, while still fulfilling our desiderata."}, {"title": "3.3.4. Asymptote Filter", "content": "One area that would be interesting to study further is a model's ability to use the Asymptote vector graphics language. In our work, we found 625 problems from the HARP subset using this language, and removed them all. However, this is not an insignificant portion of human-written problems, and may be beneficial to include these problems. In contrast to our work, Muennighoff et al. [2025] evaluate a model on AIME24 [of America, 2024] which contains images in the asymptote language. Presumably, training on such problems would be beneficial for this setting, however, they do not"}, {"title": "4. Future Directions", "content": "While this work primarily focuses on the creation and curation of the BIG-Mathand BIG-MATH-REFORMULATEDdatasets", "dataset": "n\u2022\tScaling - Investigate data scaling laws on RL training. In particular, there are a number of training algorithms, and new methods continually developing, and the efficiency of each is unknown [Team et al., 2025", "2025": "demonstrate that training data and inference time compute scale in a complementary way (increasing inference is most useful when also scaling training data), and Setlur et al. [2025", "2024a": "."}]}