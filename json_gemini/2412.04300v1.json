{"title": "T2I-FactualBench: Benchmarking the Factuality of Text-to-Image Models with Knowledge-Intensive Concepts", "authors": ["Ziwei Huang", "Wanggui He", "Quanyu Long", "Yandi Wang", "Haoyuan Li", "Zhelun Yu", "Fangxun Shu", "Long Chen", "Hao Jiang", "Leilei Gan"], "abstract": "Evaluating the quality of synthesized images remains a significant challenge in the development of text-to-image (T2I) generation. Most existing studies in this area primarily focus on evaluating text-image alignment, image quality, and object composition capabilities, with comparatively fewer studies addressing the evaluation of the factuality of T2I models, particularly when the concepts involved are knowledge-intensive. To mitigate this gap, we present T2I-FactualBench in this work-the largest benchmark to date in terms of the number of concepts and prompts specifically designed to evaluate the factuality of knowledge-intensive concept generation. T2I-FactualBench consists of a three-tiered knowledge-intensive text-to-image generation framework, ranging from the basic memorization of individual knowledge concepts to the more complex composition of multiple knowledge concepts. We further introduce a multi-round visual question answering (VQA)-based evaluation framework to assesses the factuality of three-tiered knowledge-intensive text-to-image generation tasks. Experiments on T2I-FactualBench indicate that current state-of-the-art (SOTA) T2I models still leaves significant room for improvement.", "sections": [{"title": "Introduction", "content": "In recent years, text-to-image (T2I) generation have made significant advancements in generating high-fidelity and diverse style images from input textual descriptions[Rombach et al., 2022, Podell et al., 2024, Li et al., 2024a, Chen et al., 2024a, Sun et al., 2024a,b, Wang et al., 2024a, He et al., 2024]. T2I models, represented by diffusion models[Rombach et al., 2022, Podell et al., 2024, Li et al., 2024a] and autoregressive models[Sun et al., 2024a, Wang et al., 2024a, Sun et al., 2024b, He et al., 2024], have been applied to a wide range of scenarios, including medical imaging analysis and autonomous driving.\n\nA significant challenge accompanying the advancement of T2I generation lies in evaluating the generated im-ages[Hartwig et al., 2024]. Most existing efforts on this challenge primarily focus on evaluating text-image align-ment[Hessel et al., 2021, Radford et al., 2021], image quality[Kirstain et al., 2023, Li et al., 2024b, Xu et al., 2024]and object composition capability[Huang et al., 2023, Park et al., 2021, Saharia et al., 2022, Li et al., 2024c, Wu et al., 2024a] inter alia, using automated metrics such as Fr\u00e9chet Inception Distance (FID)[Heusel et al., 2017], Inception Score (IS) [Salimans et al., 2016], and CLIPScore[Hessel et al., 2021]. Text-image alignment evaluation focuses on assessing if the generated image is faithful to the textual description, particularly when the description is longer and more detailed[Yu et al., 2022]. Image quality evaluation emphasizes metrics such as visual fidelity, coherence, and aesthetic appeal[Saharia et al., 2022, Wu et al., 2023]. Additionally, object composition capability evaluation focuses on the ability of T2I models to handle complex and unseen combinations of objects[Huang et al., 2023, Park et al., 2021, Saharia et al., 2022, Li et al., 2024c, Wu et al., 2024a]. Recently, several efforts have been made to evaluate the reasoning capabilities of T2I models, as exemplified by Commonsense-T2I [Fu et al., 2024] and PhyBench [Meng et al., 2024].\n\nHowever, despite the aforementioned efforts, a comprehensive benchmark for evaluating the factuality of T2I models in generating knowledge-intensive concepts and their compositions is still lacking. Knowledge-intensive concepts differ significantly from general concepts or objects because their visual features are often difficult\u2014or even unnecessary-to explicitly describe in the input textual description. This characteristic sets the evaluation of such concept generation apart from traditional text-alignment evaluations.\n\nTo our best knowledge, the most closely related studies to the evaluation of knowledge-intensive concept generation are as follows: HEIM[Lee et al., 2024] conduct a holistic evaluation of T2I models across 12 different aspects, such as alignment, quality and aesthetic, etc. Among them, the knowledge dimension in HEIM [Lee et al., 2024] evaluate whether the model have knowledge about the world or domains. However, HEIM evaluates only a limited set of real-world entities and employs superficial CLIPScore to assess the factuality of entities for the knowledge aspect. We also note the concurrent work KITTEN [Huang et al., 2024] explores the knowledge-intensive evaluation of image generation for real-world visual entities. However, KITTEN employs four fixed templates to generate input textual descriptions, restricting its ability to provide a comprehensive evaluation of concept under different scenarios.\n\nIn this work, we present T2I-FactualBench, the largest benchmark to date in terms of the number of concept and prompts designed to evaluate the factuality of T2I models when generating images that involves knowledge-intensive concepts. The construction of T2I-FactualBench begins with the collection of a set of Knowledge Concepts, which are defined as concepts with a limited number of hyponyms in the knowledge base. These concepts are specifically designed to challenge T2I models by requiring them to precisely generate inherent visual details of each concept. Building upon the collection of knowledge concepts, we next propose a three-tiered knowledge-intensive text-to-image generation task,"}, {"title": "T2I-FactualBench Construction", "content": "In this section, we detail the construction process of T2I-FactualBench, a benchmark designed to evaluate the factuality of T2I models when generating images that rely on rich world knowledge."}, {"title": "Knowledge Concept Collection", "content": "The first step in constructing T2I-FactualBench involves collecting a set of knowledge-intensive concepts aimed to challenge T2I models by requiring generating precise visual details, rather than merely depicting general con-cepts. In this paper, we define a Knowledge Concept as a concept that has limited hyponyms in the knowledge base BabelNet Navigli and Ponzetto [2012]. Our knowledge concept collection process is depicted in Figure 3.\nConcept Category. Specifically, we source to CNER [Martinelli et al., 2024] as the corpus to construct the knowledge concept set, as it offers a broad coverage of concept categories. CNER is a task designed for recognizing nominal concepts and named entities within a unified category space. It utilizes the completeness and broad semantic coverage of lexicographer files for nominal concepts in WordNet, along with the widely adopted semantic categories for named entities in OntoNotes, resulting in the establishment of 29 distinct categories. Among the 29 distinct categories, we focus on eight categories that can be grounded in real-world entities, such as animal, artifact, and food, while excluding abstract concept categories, such as language, law, and discipline."}, {"title": "Text-to-Image Generation with Knowledge-Intensive Concept", "content": "Building upon the collection of knowledge-intensive con-cepts, we propose a three-tiered text-to-image genera-tion task, ranging from the basic memorization of single knowledge concept to the more complex composition of multiple knowledge concepts, which enables a compre-hensive evaluation of the factual accuracy of T2I models.\nT1: Single Knowledge Concept Memorization. We define the first level T2I generation task as Single Knowl-edge Concept Memorization (SKCM), which aims to"}, {"title": "Single Knowledge Concept Understanding", "content": "We define the second-level T2I generation task as Single Knowledge Concept Understanding (SKCU). Building upon the generation of basic knowledge concepts in SKCM, SKCU advances the evaluation by assessing the T2I model's ability to produce accurate variations of these knowledge concepts under reasonable conditions. These conditions may include generating diverse actions for animals or varying attributes for objects. SKCU serves as a test of the model's comprehension of the intrinsic properties and behaviors knowledge associated with specific concepts, while such knowledge is difficult or unnecessary to explicitly describe in the prompt.\nSpecifically, in SKCU, we define three types of variations, denoted as $\\mathcal{T} = \\{Action, Attribute, Scene\\}$. A knowledge concept $c \\in \\mathcal{C}$ is randomly sampled, along with a variation type $t \\in \\mathcal{T}$ selected based on the category of the concept. Given $c$ and $t$, a powerful LLM, such as GPT-40, is prompted to generate 4 reasonable variant phrases $\\mathcal{P} = \\{p_1, p_2, p_3, p_4\\}$. From this set, a phrase $p_i$ is randomly chosen and combined with the concept $e$ to prompt the LLM to produce a SKCU prompt $S$, which is used as input for the T2I model to generate concept variants."}, {"title": "Multiple Knowledge Concepts Composition", "content": "We define the third level text-to-image generation task as Multiple Knowledge Concepts Composition (MKCC), which aims to evaluate the model's proficiency in simultaneously applying multiple knowledge concepts within a single image. Building upon the SKCU framework, MKCC not only assesses the model's comprehension of the intrinsic properties and behaviors of each knowledge concept, but also tests its capability to grasp the interdependencies and differentiations among multiple concepts. Specifically, we define four types of variations:\n\u2022 Basic Knowledge Concept Composition. The task evaluates the capability of a T2I model to accurately generate two distinct knowledge concepts while ensuring the factual correctness of their compositional relationship within a single image. Specifically, we introduce the first variation: Size. For this task, we randomly sample 2 concepts, $c_1$ and $c_2$, from the concept set $\\mathcal{C}$. Given the pair $c_1$, $c_2$ and the Size task, LLM is prompted to determine whether a pronounced size disparity exists between $c_1$ and $c_2$. If such a size discrepancy is identified, the LLM then proceeds to generate Size Prompt $S$.\n\u2022 Instantiated Knowledge Concept Composition. Instead of simply composing knowledge concepts as the model wishes, we propose generating images that represent the composition of instantiated knowledge concepts. The instantiation of knowledge concept, as introduced in SKCU, include performing specific actions or being in a certain state. Instantiated knowledge concept composition critically evaluates the ability of the T2I model to simultaneously represent the distinctive visual features of different knowledge concepts under various instantiation. Specifically, we define the variation: Differentiating. In this task, two concepts, $c_1$ and $c_2$, are randomly sampled. Given these concepts and the Differentiating task, LLM is prompted to generate actions or attributes for each concept that exhibit significant differences. This process is similar to the SKMU, ultimately resulting in the construction of the Differentiating Prompt $S$.\n\u2022 Instantiated Knowledge Concept Composition with Interaction. Lastly, building upon instantiated knowl-edge concept composition image generation, we incorporate specific semantic relationships between knowledge concepts to further assess the model's ability to composite multiple knowledge concepts that interact with one another. Specifically, we define the final variation: Interaction. In this variation, two foreground (animal, plant, food, person, artifact) concepts, $c_1$ and $c_2$, and an optional background (location) concept are randomly sampled. LLM is tasked with initially determining the feasibility of an interaction between the two foreground concepts, as well as assessing the suitability of these interactions occurring within the background concept. If the interaction is plausible and the background is appropriate, the LLM proceeds to generate 4 plausible interaction phrases for the pair of concepts. One of these phrases, alongside the optional background concept, is then randomly selected to construct the Interaction prompt $S$."}, {"title": "Overview", "content": "To conduct an effective and efficient evaluation of the T2I model's performance on the proposed T2I-FactualBench , we introduce a multi-round visual question answering (VQA)-based evaluation framework, supported by ad-vanced multi-modal LLMs. This framework consists of three VQA tasks: (1) Concept Factuality Evaluation, which focuses on assessing the factuality of the generated concept with respect to the reference image; (2) Task Completeness Evaluation, which evaluates the complete-ness of concept variants under different conditions; and (3) Composition Factuality Evaluation, which examines the factuality of multiple concept compositions under varying conditions. Through the three VQA tasks, this framework evaluates the factuality of the generated im-ages, spanning from single concept memorization and understanding to multiple concept composition, thereby enabling a precise and thorough evaluation. Figure 4 provides an overview of the multi-round VQA-based evalu-ation for T2I-FactualBench.\nSpecifically, for the first-level SKCM, we take a one-round VQA, i.e., Concept Factuality Evaluation. For the second-level SKCU, we employ a two-round VQA, consisting of both Concept Factuality Evaluation and Task Completeness Evaluation. Finally, for the third-level MKCC, we take all three VQA tasks to verify the factuality of the generated images.\""}, {"title": "Concept Factuality Evaluation", "content": "At the core of the T2I-FactualBench evaluation is the precise assessment of the factuality of the generated knowledge-intensive concepts. To achieve this, we employ an advanced multi-modal LLM combined with the reference image as an effective proxy for the human evaluator to assess the factuality of the generated image. The reference image is obtained in the knowledge concept collection process sourced from BabelNet.\nSpecifically, given the knowledge concept $c$, its model generated image $I$, and the reference image $I_R$, we have designed a dedicated evaluation prompt to instruct GPT-40 to assess the factuality of the knowledge concept in the generated image across four dimensions: shape, color, texture, and feature details, as outlined in DreamBench++[Peng et al., 2024]. For each dimension, the multi-modal LLM assigns a score of 1, accompanied by a rationale, if the generated image meets the defined criteria; otherwise, a score of 0 is assigned. Additionally, we employ a Likert-scale rating system ranging from 0 to 4 to evaluate the overall factuality of the knowledge concept in the generated image.\nFor the MKCC level, which includes multiple concepts in the generated image, we apply the aforementioned evaluation process to assess each knowledge concept individually."}, {"title": "Experimental Setup", "content": "Text-to-image models We comprehensively evaluate the performance of seven text-to-image models on the T2I-FactualBench, including three variants of Stable Diffusion: (1) Stable Diffusion v1.5 [Rombach et al., 2022], (2) Stable Diffusion XL [Podell et al., 2024], and (3) Stable Diffusion 3.5 [Esser et al., 2024]. Other models evaluated include (4) PixArt-alpha [Chen et al., 2024a], (5) Playground v2.5 [Li et al., 2024a], and (6) Flux.1 [Andreas Blattmann, 2024]. For API-based models, we evaluate (7) DALL-E 3 [Betker et al., 2023], noting that due to policy restrictions on generating images of individuals, our evaluation metrics are incomplete for datasets with human subjects.\nTo rigorously evaluate the efficacy of knowledge injection, we conducted additional experiments on four distinct models."}, {"title": "Evaluation Metrics", "content": "We utilized our proposed Multi-Round VQA-based Factuality Evaluation framework, which provides assessments across three distinct aspects: Concept Factuality Score, Task Completeness Score, and Composition Factuality Score.\nIn parallel, we also evaluated the models using previous metrics, including CLIP-T, CLIP-I[Radford et al., 2021], and the DINO Score [Caron et al., 2021]."}, {"title": "Quantitative Analysis", "content": "In this section, we present a comprehensive evaluation and analysis of diverse text-to-image models, alongside two distinct knowledge injection methods in Table 3. We then analyze the performance variations of these models across different knowledge concept domains. Lastly, we examine the correlation between the Multi-Round VQA metrics and previous metrics with human experts evaluations."}, {"title": "Visual knowledge injection models", "content": "We utilized the knowledge concept images obtained in Section 3.1 as reference visuals to assist the models in image generation. We select two subject-driven generation models: (8) SSR-Encoder [Zhang et al., 2023b], which is based on Stable Diffusion v1.5, and (9) MS-Diffusion [Wang et al., 2024c], built upon Stable Diffusion XL. These models are capable of referencing one or more images during the generation process to enhance the fidelity of subject representation."}, {"title": "Text knowledge injection models", "content": "We augment the prompts by appending the definitions of the knowledge concepts, acquired in Section 3.1, immediately following their first mention in the text. This textual augmentation aims to enable the models to generate more precise representations of the concepts. We select (10) Stable Diffusion 3.5* and (11) the Flux.1 dev* model due to the robust semantic comprehension capabilities afforded by their Diffusion Transformer (DiT) architecture."}, {"title": "Models Performance Across Three Levels", "content": "Table 3 demonstrates that with updates to the model backbone, Stable Diffusion 3.5 achieves higher concept factuality scores (46.2, 64.6, 68.9) when compared to less robust models such as SD v1.5 (40.5, 52.9, 37.6) and SD XL (45.8, 59.9, 51.7). This underscores its capability to generate high factuality of knowledge concepts using only the textual input from prompts. Furthermore, stronger models exhibit subtle changes in concept factuality scores when transitioning from generating single concepts (SKCU) to composite multiple concepts (MKCC) (e.g., SD 3.5: 64.6\u219268.9; Flux.1 dev: 54.9\u219256.9), and they perform better in composition evaluation (e.g., SD 3.5: 75.5; Flux.1 dev: 63.8). In contrast, weaker models experience significant declines in factuality scores (e.g., Playground: 66.1 \u2192 53.8) and achieve lower composition scores (e.g., Playground: 44.8). This evidences the superior capacity of models with stronger backbones to effectively handle complex contexts and integrate multiple concepts.\nMoreover, as task complexity increases from SKCU level simple tasks (e.g., action, attribute, scene) to MKCC level complex tasks (e.g., size differentiation, interaction), all models exhibit a decline in Task Completeness scores. These more intricate tasks necessitate not only an understanding of multiple knowledge concepts but also a robust ability of prompt following and distinctively manage these concepts during generation. This observation highlights the existing limitations of text-to-image models in their capability to process complex multi-concept tasks, underscoring a significant need for continued advancements and development in this area."}, {"title": "Visual-Knowledge Injection Boosts Concept Factuality but Undermines Other Abilities", "content": "Our investiga-tion reveals that two subject-driven generation models, SSR-Encoder and MS-Diffusion, exhibit significant im-provements in Concept Factuality when provided with images containing the knowledge concepts as refer-ence, compared to their underlying text-to-image models SDv1.5 (40.5\u219271.8) and SDXL (45.8\u219284.8). How-ever, there is a noticeable decline in their performance on the Task and composition Metrics. We hypothe-size that although this form of visual-knowledge injection enhances the fidelity of concept generation, it concur-rently undermines the model's ability to follow instructions and integrate multiple knowledge concepts faithfully."}, {"title": "Text-Knowledge Injection Brings Improvement and Challenges", "content": "We observed that the Flux.1 dev* demon-strated significant improvements across various metrics upon the inclusion of textual descriptions of knowledge concepts. Conversely, while SD 3.5* exhibited some en-hancement in concept factuality, it suffered negative im-pacts on task completion and composition metrics. This decline could potentially be attributed to the prompts becoming too long after adding the textual descriptions, thereby impairing the model's instruction-following and concepts integration capabilities. This finding indicates that although text-knowledge injection can enhance the accurate generation of concepts, it may also introduce potential challenges in instruction adherence, particularly when handling complex generation prompts."}, {"title": "Models Face Challenge in Certain Domains", "content": "We con-ducted an analysis of the concept factuality scores across 8 domains within the SKCM level for 11 different models, as depicted in Figure 5. Our findings reveal that models perform relatively well in the domains of animals, arti-facts and food, yet exhibit poorer performance of plants and locations. We hypothesize that this discrepancy is due to the limited representation of plant and location concepts in traditional training datasets. Further analysis on specific instances indicates that models particularly struggle when generating knowledge concepts requiring detailed features. Specifically, when generating concepts related to plants and detailed elements of landmark buildings such as statues or architectural decorations, models fail to accurately capture intricate characteristics and complex textures, thereby significantly compromising the authenticity of the outputs."}, {"title": "Multi-Round VQA Metrics Better Aligning with Human Preference", "content": "The basis for effectively evaluating generative models lies in assessing their alignment with human preferences. We curated a balanced selection of 150 concept validation samples, 150 composition validation samples, and 300 task validation samples. We employed Spearman and Kendall correlation analyses to quantify the alignment between human ratings and the scores generated by Concept"}, {"title": "Qualitative Analysis", "content": "Figure 6 showcases selected qualitative results from T2I-FactualBench, providing an intuitive insight into the per-formance of evaluated models. We have identified several prevalent deficiencies in current text-to-image models, categorized into four main areas: Concept Error, Task Failures, Realism Error, and Feature Mixture Error.\nFirst, models demonstrates a tendency to obscure fine details of unfamiliar knowledge concepts or to generate alternative concepts that resemble the intended ones. For instance, in generating \u201cMapo Tofu\u201d and \u201cgyoza\u201d, models frequently overlooks specific textural details of the food surfaces, resulting in outputs that appear excessively smooth. Similarly, the generation of \u201cBaird\u2019s tapir\u201d, the model instead produce images of similar animals like wild boars. These observations reflect the model\u2019s significant limitations in accurately capturing and expressing the features of unfamiliar knowledge concepts.\nSecond, while models are capable of generating concepts with faithful visual characteristics, they exhibit significant shortcomings when tasked with executing related attribute changes. For instance, despite capturing the visual features of an \u201cice skate,\u201d models fail to generate variations like \u201cbroken ice skate.\u201d A similar issue occurs with \u201cworn sackbut,\u201d indicating that while models can capture static visual concepts, they fall short in understanding deeper semantic layers within text.\nThird, integrating multiple knowledge concepts poses further challenges. Specifically, when prompts involve multiple objects, models often struggle to accurately integrate distinct features of these Knowledge Concepts, leading to a chaotic fusion that fails to represent the individual characteristics of each concept appropriately. For example, mixing traits of an Egyptian Mau cat with a Basset Hound results in unrealistic imagery. Moreover, multi-concept interactions sometimes produce implausible scenarios, such as a Keeshond appearing embedded in a car window, rather than inside the vehicle.\nAdditionally, with visual-knowledge injection, the output from methods like MS-Diffusion frequently lacks natural integration of multiple knowledge concepts, manifesting as simplistic concatenations rather than coherent synthesis.\nRegarding text-knowledge injection, while it aids in generating more accurate visual representations, it is particularly constrained with complex concepts such as Mapo Tofu. In these cases, the technique fails to significantly enhance task performance or integration capabilities, underscoring the need for further refinement in handling complex conceptual tasks."}, {"title": "Conclusion", "content": "In this work, we present T2I-FactualBench-the largest benchmark to date in terms of the number of concepts and prompts specifically designed to evaluate the factuality of knowledge-intensive concept generation. T2I-FactualBench consists of a three-tiered knowledge-intensive text-to-image generation framework, ranging from the basic memorization of individual knowledge concepts to the more complex composition of multiple knowledge concepts. We further introduce a multi-round visual question answering (VQA)-based evaluation framework to assesses the factuality of three-tiered knowledge-intensive text-to-image generation tasks. Extensive experiments are performed on a variety of text-to-image models, including two methods for knowledge injection. The results demonstrate that current text-to-image models still struggle to achieve a high degree of factuality when generating specific knowledge concepts. Moreover, there remains a significant need for improvement in both the understanding of individual knowledge concepts and the ability to compose multiple knowledge concepts within one image. While both knowledge injection methods enhance the faithfulness of the generated knowledge concepts, they offer limited improvements in the deeper semantic understanding of these concepts. This highlights the necessity to advance current models to improve the factuality of generated knowledge concepts."}]}