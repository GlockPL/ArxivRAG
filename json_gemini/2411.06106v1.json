{"title": "Personalize to generalize: Towards a universal medical multi-modality\ngeneralization through personalization", "authors": ["Zhaorui Tan", "Xi Yang", "Tan Pan", "Tianyi Liu", "Chen Jiang", "Xin Guo", "Qiufeng Wang", "Anh Nguyen", "Yuan Qi", "Kaizhu Huang", "Yuan Cheng"], "abstract": "Personalized medicine is a groundbreaking healthcare\nframework for the 21st century, tailoring medical treat-\nments to individuals based on unique clinical characteris-\ntics, including diverse medical imaging modalities. Given\nthe significant differences among these modalities due to\ndistinct underlying imaging principles, generalization in\nmulti-modal medical image tasks becomes substantially\nchallenging. Previous methods addressing multi-modal\ngeneralization rarely consider personalization, primarily\nfocusing on common anatomical information. This paper\naims to bridge multi-modal generalization with the con-\ncept of personalized medicine. Specifically, we propose a\nnovel approach to derive a tractable form of the underly-\ning personalized invariant representation $X_h$ by leverag-\ning individual-level constraints and a learnable biologi-\ncal prior. We demonstrate the feasibility and benefits of\nlearning a personalized $X_h$, showing that this representa-\ntion is highly generalizable and transferable across vari-\nous multi-modal medical tasks. Our method is rigorously\nvalidated on medical imaging modalities emphasizing both\nphysical structure and functional information, encompass-\ning a range of tasks that require generalization. Extensive\nexperimental results consistently show that our approach\nsignificantly improves performance across diverse scenar-\nios, confirming its effectiveness.", "sections": [{"title": "1. Introduction", "content": "Personalized medicine represents a transformative frame-\nwork for 21st century healthcare, tailoring medical treat-\nments to each patient's unique characteristics [4, 24, 45].\nThis approach necessitates diverse information, includ-\ning clinical data such as radiological images.\nThree-\ndimensional medical images, generated through specialized\ntechniques and radiopharmaceuticals, excel at highlighting\nspecific anatomical features. Collectively, different med-\nical image modalities provide a comprehensive view of a\npatient's structural and functional characteristics. However,\nthis distinction between medical modalities creates signif-\nicant generalization challenges in medical image analysis,\nespecially when certain modalities may be inaccessible due\nto an individual's financial constraints or physical limita-\ntions, thereby complicating the effectiveness of personal-\nized medicine.\nAs illustrated in Fig. 1, contemporary research in medi-\ncal intelligence is mainly concentrated on structural modal-\nities that depict physical anatomy. This includes Mag-\nnetic Resonance Imaging (MRI) scans [53], which use\nstrong magnetic fields and radiofrequency currents yield-\ning distinct sequences, and Computed Tomography (CT)\nscans [33, 51], which employ X-rays to measure its atten-\nuation. Other studies [49] focus on the functional modal-\nities associated with biochemistry, such as Positron Emis-\nsion Tomography (PET) scans. PET scans are expensive\nfunctional imaging scans that employ radiotracers emitting\ngamma rays to visualize and measure metabolic processes.\nThe differing imaging principles result in substantial modal-\nity gaps, presenting a critical challenge for model gener-\nalization. For clarity, we categorize these modalities for"}, {"title": "2. Related work", "content": "Medical generalization tasks currently concentrates on\nhomogeneous generalization, introducing tasks, such as\nmodality transfer and missing modality segmentation for\nstructural modalities Flair, T1, T2, and T1ce of MRI-\nin brain tumor segmentation [53], or between MRI and\nCT [51] for modality transfer. [34] propose an approach for\nheterogeneous generalization in terms of modality transfer,\nbut only tailored for transferring PET to CT. In terms of\ntasks, most current studies focus on one specific general-\nization task, either segmentation [5, 12, 43, 44, 52, 52] or\nmodality transfer [11, 13, 22, 25, 26, 30, 33, 35, 38, 47,\n51, 54]. This paper aims to develop an approach that is\nfeasible for different downstream tasks under both homo-\ngeneous and heterogeneous modality generalization. Our\napproach aims to learn the $X_h$ through pre-training; we list\nrelated medical pre-training work [8, 23, 40, 46] here. A no-\ntable work among them is [23], which extracts class-specific\nanatomical invariance. However, they only focus on a sin-\ngle modality. Such single-modality approaches may not be\nable to construct $X_h$ for improving the generalization across\nmodalities. The generalization in medical images also con-\nnects to alignment in multi-domain generalization for natu-\nral images [14, 20, 27, 28, 39]. Please refer to Appendix A\nfor a detailed literature review."}, {"title": "3. Learning personalized invariant representa-\ntion for medical generalization", "content": "Preliminaries. In this paper, we denote the encoder as\n$E$ and its corresponding decoder as $D$. For an individ-\nual human being $h \\in H$, the corresponding medical im-\nages are represented as $X_h = {X_h^i, X_h^j, ..., X_h^k}$, where\n$i, j,..., k \\in M$, and $M$ represents the set of all possible"}, {"title": "3.1. Using global prior for better personalized in-\nvariant representation", "content": "To learn a better approximation of $X_h$, we leverage a global\nbiological prior, denoted as $\\mathcal{O}$. If $\\mathcal{O}$ can be learned, repre-\nsentations from any modality can complete themselves by\nretrieving the missing knowledge from $\\mathcal{O}$, forming a bet-\nter approximation of $X_h$. Empirically, we initialize a learn-\nable tensor as $\\mathcal{O}$. As shown in Fig. 1, the representation\n$z_h$ retrieves its missing knowledge from $\\mathcal{O}$ via attention:\n$z_{ih}' := attn(query : z_h, key : \\mathcal{O}, value : \\mathcal{O})$. The original\nrepresentation and the retrieved knowledge are then fused\nthrough convolution: $X_i' := conv(z_{ih}', z_h)$. If the model is\nwell-trained under the constraints of equivariance, invari-\nance, and decomposition, the fused feature $X_i'$ becomes\n$X_h'$, a good approximation of $X_h$. The details of these con-\nstraints are discussed in Sec. 3.2."}, {"title": "3.2. Constraints of equivariance, invariance, and\ndecomposition", "content": "Contrastive learning. Before we introduce the constraints,\nwe include the contrastive loss as our baseline. During the\npre-training stage, we follow previous work [8, 40] and em-\nploy the contrastive learning loss. Specifically, the positive\npairs are constructed as augmented samples from the same\nsub-volume, while the negative pairs are the views from dif-\nferent sub-volumes. Similar to [40], the contrastive coding\nis obtained by attaching a linear layer $\\psi(\\cdot)$ to the $z_h$, its pos-\nitive pair $z_h^+$, and all samples ${z_i}_{i=0}^{B}$ where $B$ is the total\nnumber of samples. The contrastive loss is then defined as:\n$\\mathcal{L}_{contr} = -log \\frac{exp \\left(sim \\left(\\psi\\left(z_{h}\\right), \\psi\\left(z_{h}^{+}\\right)\\right) /t\\right)}{\\sum_{i=0}^{B} exp \\left(sim \\left(\\psi\\left(z_{h}\\right), \\psi\\left(z_{i}\\right)\\right) /t\\right)},$ (2)\nwhere $t$ is the measurement of the normalized temperature\nscale and $sim(\\cdot, \\cdot)$ denotes the dot product between normal-\nized embeddings as the similarity.\nAs discussed in Sec. 3.1, the $X_h$ can be obtained through\na model trained under the constraints of equivariance, in-\nvariance, and decomposition. The following part presents\ndetails of those constraints according to the $X_h$ hypothesis.\nInvariance constraint. We constrain the invariance\nfor $X_h$ where $X_h' || \\mathcal{M}, \\Phi$ through alignment. The $z_h$\nfirstly uses attention to fetch the knowledge from the prior:\n$z_h' = attn(z_h, \\mathcal{O})$ and then they are concatenated and fused\nthrough convolution $X_i' = conv(z_h, z_h')$. Despite the\ndifferent modality combinations and geometric transforma-\ntions, $X_h'$ should be invariant for the person:\n$L_{inv} = \\sum_{i} ||X_i'-X_h'||^2$, $i \\in \\mathcal{M}.$ (3)\nWhile it is well aligned, $X_i' = X_j' = ... = X_k' = X_h'$ where\n$j \\in \\mathcal{M}$. Empirically, we use $X_h' = mean(X_i',X_j',...)$"}, {"title": "3.2.1. The connection between the constraints and global\nbiological prior", "content": "It is important to note that the above constraints are closely\ninterconnected, as they align with Eq. (1). After obtain-\ning additional knowledge from $\\mathcal{O}$, the invariance constraint\nensures that the representations from each modality for a\ngiven individual are the same, such that $X_i'$ and $X_h'$ can be\nconsidered equivalent. Combined with the decomposition"}, {"title": "3.3. Application of different modalities and tasks", "content": "After pre-training with the loss function $\\mathcal{L}_{pre}$, the model\nis then utilized for downstream tasks such as segmentation\nor generation. We denote the commonly used loss func-\ntions for these tasks, such as dice loss, cross-entropy loss, or\nmean squared error (MSE) loss, as $\\mathcal{L}_{task}$, where paired data\nand labels $(X, Y) \\in (\\mathcal{X}, \\mathcal{Y})$ are provided. In addition to\n$\\mathcal{L}_{task}$, we incorporate the invariance loss, denoted as $\\mathcal{L}_{inv}$,\nas part of the fine-tuning process for downstream tasks:\n$\\mathcal{L}_{down} = \\mathcal{L}_{task} + \\mathcal{L}_{inv}.$ (8)\nEmpirically, we adopt the SwinUNETR architecture [17]\nas the backbone of the encoder $E$, and implement the pro-\nposed components. The model is trained with $\\mathcal{L}_{pre}$ dur-\ning the pre-training phase; users have the option to ei-\nther use the standard SwinUNETR by loading only our\npre-trained encoder weights or to employ our proposed\nmodel structure with all pre-trained weights for downstream"}, {"title": "4. Homogeneous generalization: MRI", "content": "This section demonstrates that our approach enhances the\nhomogeneous generalization across structural modalities in\nMRI. To validate that our method captures personalized in-\nformation, especially anatomical structure features, during\nthe pre-training stage, we first apply it to modality trans-\nfer tasks. Next, we adapt the pre-trained model for the\ndownstream missing modality segmentation task. Experi-\nmental results indicate that our approach outperforms state-\nof-the-art (SOTA) methods in both tasks, thereby support-\ning the $X_h$ Hypothesis and confirming the effectiveness of\nour method."}, {"title": "4.1. Pre-training for modality transfer", "content": "Modality transfer tasks focus on converting medical images\nfrom multiple modalities to other modalities. We test our\napproach on the structural modalities of MRI. This task\naligns seamlessly with our pre-training objective, and the\nquality of the generated modalities serves as a validation\nof the anatomical knowledge captured by our pre-trained\nmodel. Importantly, our approach can generate all modal-\nities without knowing the exact modalities where the input\nform, as the learned representation $X_h'$ encompasses com-\nprehensive information across all possible modalities.\nExperimental settings. Following previous meth-\nods [25], we utilize the multi-modal brain tumor seg-\nmentation challenge 2023 (BRATS23) dataset [1-3, 32].\nBRATS23 includes four structural MRI modalities (T1,\nT1ce, T2, and FLAIR) for each individual. Our model is"}, {"title": "4.2. Tuning for missing modality segmentation", "content": "Experimental settings. To validate the generalization abil-\nity of the pre-trained model, we fine-tune the model ob-\ntained from Sec. 4.1 on the BRATS18 [32] from the Mul-\ntimodal Brain Tumor Segmentation Challenge. Similar to"}, {"title": "5. Heterogeneous generalization: PET and CT", "content": "Given the differing imaging principles, the modality gap\nin heterogeneous generalization may be more pronounced\nthan that in homogeneous generalization, making the for-"}, {"title": "5.1. Pre-training for modality transfer", "content": "Experimental settings.\nWe utilize the AutoPET-II\ndataset [15] from the Automated Lesion Segmentation in\nPET/CT challenge for pre-training. The AutoPET-II dataset\nincludes AC-PET and CT pairs, where the PET scans\nadopt FDG tracers, and their attenuation is corrected us-\ning the corresponding CT scans. Specifically, we divide the\nAutoPET-II dataset into training and testing sets. Similar\nto our approach for heterogeneous generalization, we adopt\nthe Peak Signal-to-Noise Ratio (PSNR) and Structural Sim-\nilarity Index (SSIM) as evaluation metrics. In this section,\nwe present the results of models that employ different com-\nbinations of the constraints and the set. We use both con-\ntrastive loss and the decomposition constraint as our base-\nline. Please refer to training details in Appendix D."}, {"title": "5.2. Tuning for segmentation", "content": "Experimental settings. We utilize the AutoPET-II [15]\ndataset for segmentation, evaluating performance using the\nDICE metric. It is important to note that we employ the\nsame training and testing splits as in Sec. 5.1 to avoid data\nleakage. Specifically, we adhere to the settings from the of-\nficial challenge; DICE is calculated in the standard manner\nbut is set to zero for false negatives and true negatives. Ad-\nditionally, we introduce DICE- to include the mean across\nall samples, along with true positive rate (TPR), true nega-\ntive rate (TNR), false negative rate (FNR), and false positive\nrate (FPR) for the missing modality segmentation evalua-\ntion. Our method is compared against nnUNET [21], UN-\nETR [18], and SwinUNETR [17], which are trained directly\non the dataset without pre-training. Notably, we also com-\npare our approach with SwinUNETR using its pre-training\nstrategy [40]. Please refer to training details in Appendix D."}, {"title": "6. Special case: A complex scenario", "content": "We introduce a more complex scenario, in which the pre-\ntrained model for heterogeneous generalization settings is\ntuned downstream that span both heterogeneous and homo-\ngeneous generalization.\nExperimental settings. The pre-train model we adopted\nis from Sec. 5 that trained on AC-PET and CT. Specifically,"}, {"title": "7. Conclusion", "content": "This paper proposes a universal approach to address multi-\nmodality generalization by approximating a personalized\ninvariant representation, $X_h$, through constraints of invari-\nance, equivariance, and decomposition, guided by a learn-\nable biological prior. We demonstrate that learning $X_h$ is\nboth feasible and highly beneficial for enhancing general-"}, {"title": "A. Related work", "content": "Medical generalization tasks. Most current work focuses\non homogeneous generalization, introducing tasks such as\nmodality transfer and missing modality segmentation. The\nmost commonly employed structural modalities Flair,\nT1, T2, and T1ce of MRI\nare used for brain tumor seg-\nmentation [53], or between MRI and CT [51] for modality\ntransfer. [34] propose an approach for heterogeneous gen-\neralization in terms of modality transfer, but only tailored\nfor transferring PET to CT.\nSelf-supervised medical pre-train models for medi-\ncal generalization. Our approach aims to learn the $X_h$\nthrough pre-training. We list related medical pre-training\nwork [8, 23, 40, 46] here. A notable work among them\nis [23], which extracts class-specific anatomical invariance.\nHowever, they only focus on a single modality. Such single-\nmodality approaches may not be able to construct $X_h$ for\nimproving the generalization across modalities.\nAlignment in multi-domain generalization. The issue\nof cross-modality generalization is similar to the problem of\nmulti-domain generalization, which aims to extract domain\ninvariant representations [14, 20, 27, 28, 39]. Most of these\napproaches focus on learning invariance across different do-\nmains, which may not fit the scope of personalization.\nGeneralization for medical translation. Typical\nmodality transfer approaches are based on GAN mod-\nels [13, 22, 26, 35, 54]. In contrast to these GAN-based ap-\nproaches, some work adopts transformer models [30, 38],\nwhile others, such as [11, 25, 33, 47], explore diffusion-\nbased approaches. The methods such as MedM2G [51] fur-\nther incorporate textual information for modality transfer.\nAdditionally, UNET-like architectures, which can also be\napplied to these tasks, are highlighted in [17, 18]. Most cur-\nrent modality transfer research focuses on improving syn-\nthesis quality. Our approach, however, demonstrates that\nfull-modality transfer, when accompanied by specific con-\nstraints, not only enhances generation but also improves\ndownstream generalization.\nGeneralization for medical segmentation. There are\nthree main types of approaches to missing modality seg-\nmentation. Knowledge distillation-based approaches trans-\nfer knowledge from models with complete modality infor-\nmation (teachers) to models with missing modality informa-\ntion (students) [5, 43]. [12, 52] recover missing information\nby leveraging the multimodal latent feature space. Domain\nadaptation-based methods aim to reduce the gap between\nmodels with complete and incomplete modalities by align-\ning their domains [44]. One prominent shared latent space\nmethod, MmFormer [52], exploits intra- and inter-modality\ndependencies for feature fusion, which is closely related to\nour work. Our work reveals that our pre-train model with\nbasic segmentation tuning exceeds these approaches."}, {"title": "B. Limitations, challenges, and future work", "content": "To enhance the validation of our approach, we adhere to\ncommonly used settings during the tuning stage. Exploring\nalternative strategies, such as knowledge distillation, could\nfurther improve downstream performance. Our approach\nrequires datasets where all modalities are instance-level\nmatched, which can be a stringent condition and may be\nunattainable for certain modalities. Future research should\nexplore methods to achieve personalized invariance without\nrelying on instance-level matched datasets. Additionally,\nwe advocate for the availability of more open-source multi-\nmodal medical datasets, particularly for functional modali-\nties, as these are not widely accessible to researchers."}, {"title": "C. Social impact", "content": "This work presents an approach to tackle multi-modality\ngeneralization through personalization. We hope our work\ncan encourage the community to work towards practical,\npersonalized medical models with border generalization\nability."}, {"title": "C.1. Downstream segmentation ablation study", "content": "We demonstrate the effectiveness of our proposed com-\nponents and discuss the process of learning an anatomy-\ninvariant representation. Experimental results for down-\nstream segmentation tasks and visualizations of the pre-\ntrained models are presented in Tab. 7. All experiments are\nconducted under consistent settings to ensure a fair compar-\nison.\nUsing all constraints together with $\\mathcal{O}$ yields the best\nresults. Consistent with Sec. 3.2.1, the results indicate\nthat using different constraints alone may not guarantee im-\nprovements; however, incorporating all constraints along\nwith $\\mathcal{O}$ results in the best outcomes. This validates the plau-\nsibility of the $X_h$ Hypothesis and demonstrates that achiev-\ning good approximation of it significantly enhances gener-\nalization.\nUsing prior $\\mathcal{O}$ with decomposition constraint im-\nproves the model performance for different settings. De-\nspite different settings, additionally using $\\mathcal{O}$ with decom-\nposition improves the downstream model performance. Com-\nbined with the improvements from modality transfer results\nin Tab. 4, it suggests that $\\mathcal{O}$ helps with better obtaining\nanatomical structure.\nThe invariance and equivariance constraints can not\nbe applied to the same feature. It needs to be highlighted\nthat invariance and equivariance constraints can not be ap-\nplied to the same features as they conflict with each other.\nAs shown in task 3, without $\\mathcal{O}$, invariance and equivariance\nconstraints are applied to the latent feature simultaneously,\nleading to a significant performance drop. In comparison,\napply equivariance constraint before using $\\mathcal{O}$ and applying"}, {"title": "D. Experimental details", "content": "The model and data loaders are built by using\nMONAI https://docs.monai.io/en/stable/\nindex.html. Please refer to all the details of the\nimplementation in the code. We present some key\nimplementations below."}, {"title": "D.1. Overall training procedure", "content": "We provide a pseudo-code for our approach. The loss cal-\nculation for Pre-training procedure is simplified as Algo-\nrithm 1 and Downstream tuning as Algorithm 2. It is no-\ntable that the empirical procedure is flexible as long as the\n$\\mathcal{O}$ is properly used to construct $X_h'$ and those constraints are\napplied to $X_h'.$"}, {"title": "D.2. Homogeneous generalization:\nmodalities in MRI", "content": "structural"}, {"title": "D.2.1. Pre-training and Modality transfer.", "content": "Experimental settings. We use four A100 GPUs for train-\ning. The learning rate we used for the modality transfer is\nset to 0.0002, and the training epoch is set to 1000. Both\nthe number of input and out channels is set as 4.\nTraining details. For the model, both the input and out-\nput channels are set to 4, corresponding to the four MRI\nmodalities. All modalities are loaded and cropped to a size\nof 96 \u00d7 96 \u00d7 96 simultaneously. Following [25], we also\nnormalize each MRI modality to have zero mean and unit\nvariance. During training, the background is excluded for\nmodal generation. A single modality is repeated four times\nto create four channels during training to obtain $X'$. The\ntraining loss follows the $L_{pre}$, whose calculation details\nduring the training phase can be seen in Algorithm 1."}, {"title": "D.2.2. Missing modality segmentation.", "content": "We use four A100 GPUs for tuning. The learning rate we\nused for the modality transfer is set to 0.0002, and the train-"}, {"title": "D.4. Special case: Tuning from heterogeneous to\nhomogeneous generalization with domain gap", "content": "Training details. For the fine-tuning stage, we use the\ndecoder architecture of SwinUNETR, which is randomly\ninitialized. The training procedure is similar to the above\nmodality transfer experiments, with the primary difference\nbeing that the input and output channels are set to two. Ad-\nditionally, we reproduced the results of UNETR and Swin-\nUNETR for comparison, ensuring that the same loss func-\ntions were applied across models."}, {"title": "E. More results", "content": "E.1. Modality transfer results on BRATS22:\nTab. 8 and Tab. 9 presents the generation result with stan-\ndard derivations. The results of our method and Swin-\nUNETR are produced by ourselves, while the rest of the\nresults are gathered from [25]. Generated examples are pre-\nsented in Figs. 6 to 8."}, {"title": "E.2. Missing modality segmentation results on\nBRATS18:", "content": "We provide detailed segmentation results on BRATS18 as\nTab. 10."}]}