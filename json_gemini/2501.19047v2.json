{"title": "UNDERSTANDING MODEL CALIBRATION - A GENTLE INTRODUCTION AND VISUAL EXPLORATION OF CALIBRATION AND THE EXPECTED CALIBRATION ERROR (ECE)", "authors": ["Maja Pavlovic"], "abstract": "To be considered reliable, a model must be calibrated so that its confidence in each decision closely reflects its true outcome. In this blogpost we'll take a look at the most commonly used definition for calibration and then dive into a frequently used evaluation measure for model calibration. We'll then cover some of the drawbacks of this measure and how these surfaced the need for additional notions of calibration, which require their own new evaluation measures. This post is not intended to be an in-depth dissection of all works on calibration, nor does it focus on how to calibrate models. Instead, it is meant to provide a gentle introduction to the different notions and their evaluation measures as well as to re-highlight some issues with a measure that is still widely used to evaluate calibration.", "sections": [{"title": "WHAT IS CALIBRATION?", "content": "Calibration makes sure that a model's estimated probabilities match real-world likelihoods. For example, if a weather forecasting model predicts a 70% chance of rain on several days, then roughly 70% of those days should actually be rainy for the model to be considered well calibrated (Dawid, 1982; DeGroot & Fienberg, 1983). This makes model predictions more reliable and trustworthy, which makes calibration relevant for many applications across various domains."}, {"title": "(CONFIDENCE) CALIBRATION", "content": "A model is considered confidence-calibrated if, for all confidences c, the model is correct e proportion of the time:\nP(Y = arg max(p(X)) | max(p(X)) = c) = c \\forall c \\in [0, 1],\nwhere (X, Y) is a datapoint and \\hat{p} : X \\rightarrow \\triangle^K returns a probability vector as its output.\nThis definition of calibration, ensures that the model's model's final predictions align with their observed accuracy at that confidence level (Guo et al., 2017). The left chart below visualises the perfectly calibrated outcome (green diagonal line) for all confidences using a binned reliability diagram (Guo et al., 2017). On the right hand side it shows two examples for a specific confidence level across 10 samples."}, {"title": "EVALUATING CALIBRATION - EXPECTED CALIBRATION ERROR (ECE)", "content": "One widely used evaluation measure for confidence calibration is the Expected Calibration Error (ECE) (Naeini et al., 2015; Guo et al., 2017). ECE measures how well a model's estimated probabilities match the observed probabilities by taking a weighted average over the absolute difference between average accuracy (acc) and average confidence (conf). The measure involves splitting all n datapoints into M equally spaced bins:\nECE = \\sum_{m=1}^M \\frac{|B_m|}{n} |acc(B_m) \u2013 conf(B_m)|,\nwhere B is used for representing \"bins\" and m for the bin number, while acc and conf are:\nacc(B_m) = \\frac{1}{|B_m|} \\sum_{i \\in B_m} \\mathbb{1}(\\hat{y_i} = Y_i) & conf(B_m) = \\frac{1}{|B_m|} \\sum_{i \\in B_m} \\hat{p(x_i)}\n\\hat{y_i} is the model's predicted class (arg max) for sample i and yi is the true label for sample i. \\mathbb{1} is an indicator function, meaning when the predicted label \\hat{y_i} equals the true label y_i it evaluates to 1, otherwise 0. Let's look at an example, which will clarify acc, conf and the whole binning approach in a visual step-by-step manner."}, {"title": "ECE - VISUAL STEP BY STEP EXAMPLE", "content": "In the image below, we can see that we have 9 samples indexed by i with estimated probabilities p(x_i) (simplified as p_i) for class cat (C), dog (D) or toad (T). The final column shows the true class Y_i and the penultimate column contains the predicted class \\hat{y_i}."}, {"title": "MOST FREQUENTLY MENTIONED DRAWBACKS OF ECE", "content": ""}, {"title": "PATHOLOGIES - LOW ECE DOES NOT EQUAL HIGH ACCURACY", "content": "A model which minimises ECE, does not necessarily have a high accuracy (Kumar et al., 2018; Kull et al., 2019; Si et al., 2022). For instance, if a model always predicts the majority class with that class's average prevalence as the probability, it will have an ECE of 0. This is visualised in the image above, where we have a dataset with 10 samples, 7 of those are cat, 2 dog and only one is a toad. Now if the model always predicts cat with on average 0.7 confidence it would have an ECE of 0. There are more of such pathologies (Nixon et al., 2019). To not only rely on ECE, some researchers use additional measures such as the Brier score or LogLoss alongside ECE (Kumar et al., 2018; Kull et al., 2019)."}, {"title": "BINNING APPROACH", "content": "One of the most frequently mentioned issues with ECE is its sensitivity to the change in binning (Kumar et al., 2018; Nixon et al., 2019; Gupta et al., 2021; Zhang et al., 2020; Roelofs et al., 2022). This is sometimes referred to as the Bias-Variance trade-off (Nixon et al., 2019; Zhang et al., 2020): Fewer bins reduce variance but increase bias, while more bins lead to sparsely populated bins increasing variance. If we look back to our ECE example with 9 samples and change the bins from 5 to 10 here too, we end up with the following:"}, {"title": "ONLY MAXIMUM PROBABILITIES CONSIDERED", "content": "Another frequently mentioned drawback of ECE is that it only considers the maximum estimated probabilities (Nixon et al., 2019; Ashukha et al., 2020; Vaicenavicius et al., 2019; Widmann et al.,"}, {"title": "MULTI-CLASS CALIBRATION", "content": "A model is considered multi-class calibrated if, for any prediction vector q = (q_1, ..., q_\\kappa) \\in \\triangle^K, the class proportions among all values of X for which a model outputs the same prediction \\hat{p}(X) = q match the values in the prediction vector q.\nP(Y = k | \\hat{p}(X) = q) = q_k \\quad \\forall k \\in {1, ..., K}, \\forall q \\in \\triangle^K,\nwhere (X, Y) is a datapoint and \\hat{p} : X \\rightarrow \\triangle^K returns a probability vector as its output.\nWhat does this mean in simple terms? Instead of c we now calibrate against a vector q, with K classes. Let's look at an example below:"}, {"title": "CLASS-WISE CALIBRATION", "content": "A model is considered class-wise calibrated if, for each class k, all inputs that share an estimated probability Pk (X) align with the true frequency of class k when considered on its own:\nP(Y = k | p_k(X) = q_k) = q_k \\quad \\forall k \\in {1, ..., K},\nwhere (X, Y) is a datapoint; q \\in \\triangle^K and \\widehat{p} : X \\rightarrow \\triangle^K returns a probability vector as its output.\nClass-wise calibration is a weaker definition than multi-class calibration as it considers each class probability in isolation rather than needing the full vector to align. The image below illustrates this by zooming into a probability estimate for class 1 specifically: q1 = 0.1. Yet again, we assume we have 10 instances for which the model predicted a probability estimate of 0.1 for class 1. We then look at the true class frequency amongst all classes with q1 = 0.1. If the empirical frequency matches q\u2081 it is calibrated."}, {"title": "HUMAN UNCERTAINTY CALIBRATION", "content": "A model is considered human-uncertainty calibrated if, for each specific sample x, the predicted probability for each class k matches the 'actual' probability Pvote of that class being correct.\nPvote(Y=k | X = x) = p_k(x) \\quad \\forall k \\in {1, ..., K},\nwhere (X, Y) is a datapoint and \\widehat{p} : X \\rightarrow Kreturns a probability vector as its output."}, {"title": "FINAL THOUGHTS", "content": "We have run through the most common definition of calibration, the shortcomings of ECE and additional notions of calibration: multi-class, class-wise & human-uncertainty calibration. We also touched on some of the newly proposed evaluation measures and their shortcomings. Despite several works arguing against the use of ECE for evaluating calibration, it remains widely used. The aim of this blogpost is to draw attention to these works and their alternative approaches. Determining which notion of calibration best fits a specific context and how to evaluate it should avoid misleading results. Maybe, however, ECE is simply so easy, intuitive and just good enough for most applications that it is here to stay?"}]}