{"title": "Universal Post-Processing Networks for Joint Optimization of Modules in Task-Oriented Dialogue Systems", "authors": ["Atsumoto Ohashi", "Ryuichiro Higashinaka"], "abstract": "Post-processing networks (PPNs) are components that modify the outputs of arbitrary modules in task-oriented dialogue systems and are optimized using reinforcement learning (RL) to improve the overall task completion capability of the system. However, previous PPN-based approaches have been limited to handling only a subset of modules within a system, which poses a significant limitation in improving the system performance. In this study, we propose a joint optimization method for post-processing the outputs of all modules using universal post-processing networks (UniPPNs), which are language-model-based networks that can modify the outputs of arbitrary modules in a system as a sequence-transformation task. Moreover, our RL algorithm, which employs a module-level Markov decision process, enables fine-grained value and advantage estimation for each module, thereby stabilizing joint learning for post-processing the outputs of all modules. Through both simulation-based and human evaluation experiments using the MultiWOZ dataset, we demonstrated that UniPPN outperforms conventional PPNs in the task completion capability of task-oriented dialogue systems.", "sections": [{"title": "1 Introduction", "content": "Typical task-oriented dialogue systems process a single user input through multiple subtasks to produce a final response. These subtasks include (1) natural language understanding (NLU), which estimates the user's intent from the input; (2) dialogue state tracking (DST), which accumulates the user's requests up to the current turn as a dialogue state; (3) dialogue policy (policy), which determines the next action that the system should take as dialogue acts (DAs); and (4) natural language generation (NLG), which converts these DAs into a final system response. Recent research has moved beyond optimizing dedicated modules for each subtask individually. It has explored the use of reinforcement learning (RL) to train several modules based on actual dialogue experiences, thereby optimizing the overall task completion capability of the system (Ni et al. 2022; Kwan et al. 2023).\nRecently, a new approach using post-processing networks (PPNs) was proposed to optimize the task completion capability of dialogue systems without directly training the modules (Ohashi and Higashinaka 2022). In this approach, instead of training the modules directly, PPNs, which are components with trainable parameters that modify their outputs, are trained using RL. For example, Ohashi and Higashinaka (2022) implemented PPNs that post-process outputs from NLU, DST, and policy as multi-binary classification tasks using multi-layer perceptrons (referred to as BinPPN), and demonstrated that this optimization improved the overall task completion capability of the systems. In addition, a large language model (LLM)-based generative PPN (GenPPN) was proposed to post-process the natural language output from NLG, and its effectiveness has been demonstrated (Ohashi and Higashinaka 2023).\nHowever, conventional PPN-based methods have two major limitations. First, BinPPN and GenPPN cannot be optimized jointly because of their different model architectures and training algorithms. Although it is possible to post-process the outputs of all modules by combining disjointly trained BinPPNs and GenPPNs, using multiple networks that are not jointly optimized may fail to achieve sufficient performance improvement. The second limitation is the narrow applicability of GenPPN. GenPPN aims to generate utterances that are easily understood by users and relies on a reward function that requires feedback on whether the DAs output from the policy are correctly conveyed to the user. This approach cannot be applied to an end-to-end dialogue system, such as the LLM-powered model proposed by Hude\u010dek and Dusek (2023), which does not explicitly output DAs, imposing significant limitations on GenPPN's applicability.\nIn this study, we propose a universal post-processing network (UniPPN) and an optimization method that combines the strengths of two conventional PPNs. UniPPN can jointly optimize the post-processing of outputs from all modules in task-oriented dialogue systems (Figure 1). Our proposed method involves a single-language model-based UniPPN that post-processes the outputs from all modules as a sequence-transformation task. Additionally, we introduce a newly designed module-level Markov decision process (MDP) that extends the standard MDP paradigm and incorporates it into UniPPN's optimization algorithm. This approach enables fine-grained value and advantage estimation for each module, even with sparse feedback obtained only at the end of multi-turn dialogues, thus ensuring stable joint optimization. Because UniPPN does not require as dense feedback about DAs as GenPPN, it can be applied to a wide range of systems, including end-to-end systems that do not output DAs.\nTo verify the effectiveness of UniPPN across various dialogue systems, we conducted experiments using dialogue simulations based on the MultiWOZ dataset (Budzianowski et al. 2018). Specifically, we compared the task completion capabilities of dialogue systems equipped with conventional PPNs to those using UniPPN. The results demonstrated that UniPPN significantly outperformed conventional PPNs in terms of task completion capability. Additionally, through dialogue experiments with human users, we demonstrated that dialogue systems using UniPPN outperformed those using conventional PPNs."}, {"title": "2 Related Work", "content": "Modularity of Task-Oriented Dialogue Systems In typical pipeline task-oriented dialogue systems, dedicated modules for each subtask, such as NLU, DST, policy, and NLG, have been individually developed and optimized (Zhang et al. 2020). However, in recent years, methods addressing multiple subtasks using a single model have become common (Ni et al. 2022). For example, word-level DST (Wu et al. 2019; Zhao et al. 2022) estimates the dialogue state directly from the dialogue history without requiring user intent estimation using NLU. Similarly, a word-level policy (Lubis et al. 2020; Wang et al. 2020) generates system responses directly from the dialogue state without requiring conversion from DA to system utterances by NLG. Furthermore, end-to-end dialogue systems (Hosseini-Asl et al. 2020; He et al. 2022; Wu et al. 2023), which learn all subtasks using a single model, are becoming popular. Because these end-to-end systems maintain modularity by sequentially executing each subtask, they are often referred to as modularly end-to-end systems (Qin et al. 2023).\nOnline RL for Task Completion In response generation for task-oriented dialogue systems, it is crucial not only to maximize the probability of reference tokens in corpora but also to maximize task completion capability in actual multi-turn dialogues (Kwan et al. 2023). Some studies (Liu et al. 2018; Tseng et al. 2021) employed online RL frameworks to train dialogue systems based on experiences obtained online from interactions with users. For example, research has focused on optimizing DST (Chen et al. 2022) or policy (Li et al. 2020; Deng et al. 2024) within pipeline systems. Additionally, Zhao and Eskenazi (2016) demonstrated that jointly optimizing DST and policy with shared parameters outperforms systems in which the DST and policy are trained separately. Our proposed method also utilizes an online RL framework to optimize dialogue systems. However, contrary to previous studies that focused on learning the modules, we concentrate on learning to modify the outputs of these modules.\nPost-Processing Networks Methods that train modules via RL cannot be applied to dialogue systems with non-trainable modules, such as rule-based or API-based modules, as expected in real-world scenarios. To address this issue, Ohashi and Higashinaka (2022) proposed optimizing BinPPNs instead of the modules. BinPPNs modify the outputs of NLU, DST, and policy, and are optimized using online RL. Specifically, BinPPNs perform post-processing on the set of slot-value pairs output by each module through binary classification to determine whether to delete (0) or maintain (1) each pair. To handle post-processing for NLG, which outputs natural language rather than a set of slot-value pairs, Ohashi and Higashinaka (2023) introduced GenPPN, which uses LLMs to paraphrase the system utterance output by NLG, to improve task completion by generating system utterances that can be easily understood by users. It is optimized through RL based on feedback regarding whether DAs are correctly conveyed to users.\nOur proposed UniPPN can process arbitrary sequences as both input and output. Therefore, contrary to BinPPN, which is limited to binary decisions of deletes/maintenance, it allows for more flexible post-processing. In addition, contrary to GenPPN, which is optimized using detailed feedback on DAs, UniPPN is optimized solely using the task success/failure signal obtained at the end of the dialogue. This makes it applicable to a wide range of systems, including word-level policies and end-to-end systems."}, {"title": "3 Preliminary", "content": "The problem of learning capabilities for multi-turn task-oriented dialogue is often formulated as an MDP and optimized through RL. An MDP is defined by tuple $(S, A, P, R, \\gamma)$. Essentially, S and A represent all possible dialogue histories and system response sentences, respectively. $P(s'|s, a)$ represents the transition model, $S \\times A \\times S \\rightarrow [0, 1]$ defines the dialogue environment containing the user, and $R(s, a)$ represents the immediate reward function $S \\times A \\rightarrow R$. $\\gamma$ denotes the discount factor. At each turn t, the policy $F: S \\rightarrow A$ (i.e., the dialogue system) samples an action (i.e., the system response) $a_t \\sim F(a_t|s_t)$. Until the final state at turn T is reached, the next state $s_{t+1} \\sim P(s_{t+1}|S_t, a_t)$ and the immediate reward $r_t = R(s_t, a_t)$ are obtained. The goal of RL is to train F to maximize the value function V, which is the expected cumulative discounted reward as follows.\n$V^F(s) := E[\\sum_{t=0}^{T} \\gamma^t r_t | s_0 = s]$\n(1)\nNumerous studies targeted only a part of F, such as the policy module, rather than the entire F.\nIn complex problems, such as task-oriented dialogues, directly obtaining a policy that maximizes Eq. (1) is challenging."}, {"title": "4 Proposed Method", "content": "In this section, we explain the problem formulation of our study, the proposed UniPPN, imitation learning (IL) and RL, which together constitute our optimization procedure for UniPPN.\nProblem Formulation\nHere, we formulate the optimization problem for the dialogue system F through post-processing. We assume that F has a modularity consisting of M modules: Module1, ..., ModuleM. At each turn t, each module Modulem takes the output $out_{(t,m-1)}$ of the previous Modulem\u22121 as its input $in_{(t,m)}$, outputting its processing result $out_{(t,m)} = Module_m(in_{(t,m)})$. Some modules may use the dialogue history $s_t$ as additional input. The post-processing network PPNm for Modulem modifies $out_{(t,m)}$ and the modified $out'_{(t,m)} \\sim PPN_m(s_t, in_{(t,m)}, out_{(t,m)})$ becomes the input for Modulem+1. For the optimization of F, we train PPNm instead of Modulem.\nUniPPN\nIn our proposed method, the post-processing of the outputs from all M modules is performed by a single network, UniPPN \u03c0 (Figure 1). Specifically, UniPPN modifies the output of any Modulem: $out'_{(t,m)} \\sim UniPPN(s_t, in_{(t,m)}, out_{(t,m)}, prefix_m)$. Here, prefixm is an indicator that specifies that the module to be post-processed is Modulem. The input and output formats of UniPPN are text sequences, with post-processing executed as a sequence-transformation task. For the tokenized sequences $x = (x_1, ..., x_k)$ and $y = (y_1, ..., y_l)$, representing $(s_t, in_{(t,m)}, out_{(t,m)}, prefix_m)$ and $out'_{(t,m)}$ respectively, the following conditional probability is modeled:\n$\\pi_{\\theta}(y/x) = \\prod_{i=1}^{l} \\pi_{\\theta}(y_i | x, y_{<i})$\n(3)\nwhere $\u03c0_\u03b8$ represents a pre-trained language model parameterized by \u03b8. By treating not only the post-processing of natural language, such as the output of NLG modules but also structural data, such as the output of NLU or DST as a sequence-transformation task (Raffel et al. 2020; Liang et al. 2020), UniPPN can uniformly perform post-processing across all modules.\nImitation Learning of Post-Processing\nPre-trained language models are typically trained on web text and may not sufficiently possess the ability to modify the outputs of modules in task-oriented dialogue systems. Therefore, we conduct additional pre-training to teach the model $\u03c0_\u03b8$ the formats of input $(s_t, in_{(t,m)}, out_{(t,m)}, prefix_m)$ and output $out'_{(t,m)}$ through supervised fine-tuning. In the general RL paradigm, IL conducted before online RL uses demonstration data, which consist of the action history of experts, such as humans. However, in our problem setting, demonstration data for post-processing the outputs of each module in the dialogue system F do not exist. Therefore, we automatically generate post-processing demonstration data and use them for supervised fine-tuning.\nUsing the procedure shown in Figure 2, we create pseudo-post-processing demonstration data for each Modulem. This process involves sampling dialogues by repeating interactions between Fand the environment P to generate the input-output history $h_{(t,m)} = (s_t, in_{(t,m)}, out_{(t,m)})$ for each Modulem at each turn t, resulting in $H_m = \\{h_{(1,m)}, ..., h_{(|H_m|,m)}\\}$. We now demonstrate the modification of $out_{(t,m)}$. Here, the label $out'_{(t,m)}$, which represents the correct modification of $out_{(t,m)}$, cannot be created automatically.\nically. Under the assumption that the output $out_{(t,m)}$ of Modulem is reasonably valid, we consider $out_{(t,m)}$ to be the target output after post-processing; we use $out_{(t,m)}$, randomly sampled from another turn u (which may be from the same or a different dialogue) as the negative output that should be post-processed. This creates one demonstration instance $d_{(t,m)} = \\{(s_t, in_{(t,m)}, out_{(t,m)}, prefix_m), out'_{(t,m)}\\}$, representing the modification from $out_{(t,m)}$ to $out'_{(t,m)}$. We applied this pseudo-data creation process to all samples in Hm, resulting in the final demonstration dataset $D_m = \\{d_{(1,m)}, ..., d_{(|H_m|,m)}\\}$. In the following section, we describe the two techniques used to create Dm.\nSampling Realistic out If we sample a turn u that is completely irrelevant to the context of t, it could introduce noise, causing \u03c0 to potentially learn to ignore $out_{(t,m)}$ rather than to modify it appropriately. To ensure that the mistakes are reasonable, we sample turns with contexts similar to t. Specifically, from the entire history excluding $h_{(t,m)}$ (i.e., $H_m \\\\ \\{h_{(t,m)}\\}$), we extract the top few turns with the highest cosine similarity to the vector representation of the context st in h(t,m), and randomly sample h(u,m) from the extracted turns. We use a general-purpose embedding model, such as E5 (Wang et al. 2022) to vectorize the context.\nLearning to Copy In post-processing, it is not always necessary to modify the outputs; outputs without issues should be \"copied\" without modification. To reflect this, during the IL phase, we input the original $out_{(t,m)}$ into \u03c0to ensure that modifications are not always required. In these cases, the target output is only the special token \"copy\". Specifically, demonstrations of such cases are $d_{(t,m)} = \\{(s_t, in_{(t,m)}, out_{(t,m)}, prefix_m), copy\\}$. This approach allows the model to explicitly learn whether post-processing is necessary, while also reducing the generation costs when post-processing is unnecessary. Whether each instance becomes a copy instance is determined randomly using copy ratio $\u03b1 \u2208 [0, 1]$, which is a hyperparameter.\nWe update \u03b8 based on the maximum likelihood objective using the final dataset $D_{1:M} = [D_1; ...; D_M]$, which combines the pseudo-post-processing data for all M modules. The optimized parameters in this IL step are denoted by \u03c6."}, {"title": "Optimization with Reinforcement Learning", "content": "In the RL phase, we install the UniPPN $\u03c0_\u03c6$ obtained from the IL step into the dialogue system F. Subsequently, let F interact repeatedly with the environment P over multiple turns and update \u03c6 based on these experiences using a policy-gradient-based approach. In typical task-oriented dialogue systems using online RL, only a single policy network (e.g., a policy module) operates per turn, and it is updated according to Eq. (2). In contrast, our study involves a policy \u03c0 that acts M times per turn, and outputs the system response as action a. Although each of the M actions should have different gradients based on their individual advantages, Eq. (2) treats them as having the same contributions. This can result in coarse rewards and learning instability.\nTherefore, we extend the standard MDP described in Section 3 and introduce a module-level MDP, where the unit of time step is the \u201cpost-processing of one module by \u03c0\" rather than the \u201cone turn response by F\". Specifically, the value function to be maximized and policy gradient of \u03c0 are as follows:\n$V^{\\pi}(x) := E[\\sum_{t=0}^{TM} \\gamma^{(t+1)(m-1)}r_{(t,m)} | X_{(0,1)} = x]$\n(4)\n$\\nabla_{\\phi} J(\\phi) = E[\\sum_{t=0}^{TM} \\Psi_{(t,m)} \\nabla_{\\phi} log \\pi_{\\phi} (Y_{(t,m)} | X_{(t,m)})]$\n(5)\nHere, $r_{(t,m)}$ represents the immediate reward for post-processing the output of Modulem at turn t. As in previous studies using online RL (Hou et al. 2021), a small negative fixed value is assigned continuously until the end of the dialogue. $x_{(t,m)}$ and $y_{(t,m)}$ are the tokenized sequences of the input text ($s_t, in_{(t,m)}, out_{(t,m)}, prefix_m$) and output text of UniPPN, respectively. Eq. (5) shows that the gradients can be computed in M gradient accumulation steps. Note that, in Eq. (4), the number of calculations for the value function in the module-level MDP is T \u00d7 M, resulting in a possible exponential increase in the computational cost. However, because M in a typical task-oriented dialogue system is four at most, this is not a problem in practice.\nTo implement $\\Psi_{(t,m)}$, we adopt a generalized advantage estimation (Schulman et al. 2015). Specifically, we compute the advantage estimate $\\hat{A}_{(t,m)}$ based on the value $V(x_{(t,m)})$ of $x_{(t,m)}$ estimated using another language model $V_\u03c8$ parameterized by \u03c8 as a critic network:\n$\\hat{A}_{(t,m)} = \\delta_{(t,m)} + \\gamma\\lambda\\hat{A}'_{(t,m)}$\n(6)\n$\\delta_{(t,m)} = r_{(t,m)} + \\gamma V_\\psi(x'_{(t,m)}) - V_\\psi(x_{(t,m)})$\n(7)\n$x'_{(t,m)} = \\{\n\\begin{array}{ll}\n(t, m + 1) & \\text{if } m < M \\\\\n(t+1,1) & \\text{if } m = M\n\\end{array}$\nHere, $\\delta_{(t,m)}$ represents the TD residual, and the hyperparameter $\u03bb\u2208 [0, 1]$ controls the trade-off between utilizing actual long-term rewards and the estimated values. Because V estimates the state value for each Modulem at each turn t, fine-grained advantage estimation according to the contribution of each module is possible even in settings with sparse rewards across multi-turn dialogues. An advantage of this algorithm is that it does not require a high-cost manual reward design for each module, as required in previous studies. V is trained to minimize the mean squared error with respect to the cumulative reward and $\u03c0_\u03c6$ is optimized using a clipped surrogate objective with proximal policy optimization (PPO) (Schulman et al. 2017). For a detailed implementation of the RL algorithm, refer to Appendix A."}, {"title": "5 Experiments", "content": "In this evaluation experiment, we demonstrate that joint optimization using UniPPN is more effective than disjoint optimization combining conventional BinPPN and GenPPN for post-processing outputs from all modules to improve task-oriented dialogue systems."}, {"title": "Impact of Module-level MDP", "content": "We examined the contribution of the module-level MDP, introduced in our optimization algorithm, to UniPPN learning. We trained UniPPN for all the modules of SYSPPO with and without module-level MDP. During training without module-level MDP, all actions performed by UniPPN in the same turn received the same value estimation (called turn-level MDP). For each setup, the training was conducted using three random seeds.\nFigure 3(a) shows the learning curves for the success rate and number of turns. The introduction of the module-level MDP improved the scores. Furthermore, turn-level MDP becomes particularly unstable in the latter half of learning, and the final performance decreases. To investigate this, we plotted the advantage estimates for the post-processing of each module (Figure 3(b)). The plots reveal that advantage estimation is significantly destabilized and destroyed in the turn-level MDP. This can be considered an example of a credit assignment problem in RL. Specifically, assigning the same value estimation to all actions (i.e., the post-processing outputs of all modules) performed within the same turn fails to estimate the proper contribution of each action, resulting in learning instability. This instability is believed to significantly affect the final system performance."}, {"title": "C Human Evaluation Details", "content": "The procedure of the human evaluation experiment followed these steps. First, each worker read a user goal randomly created for each dialogue. Next, the worker conducted a dialogue with one of the three systems (SYSPPO, +BinPPN&GenPPN, +UniPPN). The maximum number of turns for the dialogue was set to 20, which is the same as that used in the automatic evaluation experiment. Each worker judges whether a goal has been achieved within the maximum number of turns. After the dialogue ended, the participants answered three subjective evaluation questionnaires. To ensure the quality of the subjects, we only recruited workers with masters qualifications from AMT. In addition, we restricted each worker from participating in the experiment only once. The reward for each participant was set to $2, considering the time required per task (around ten minutes or less) and the minimum wage in the U.S.\nTo examine the performance of UniPPN, we quantitatively analyzed the dialogue history between the systems and crowdworkers. Table 5 displays the response history for SYSPPO with UniPPN applied. Initially, for the user's input utterance, the original NLU failed to recognize that the area was \u201ccentre\u201d and only output that the user was looking for a restaurant. UniPPN corrected the output by adding \u201carea=centre.\u201d For the DST output, UniPPN judged that it was not problematic and maintained (copied) the output without modifications. Regarding policy output, UniPPN added new DAs to restaurant names and phone numbers. This is probably because, during RL with the user simulator, UniPPN learned that adding a name and other information increased the likelihood of task success. Note that in dialogues with humans, outputting excessive information not explicitly requested by the user might negatively impact dialogue satisfaction scores. From the analysis, we confirmed cases where UniPPN, developed in a simulation environment, was also effective in dialogue with humans, resulting in improved task completion performance."}, {"title": "Appendix", "content": "A Reinforcement Learning Algorithm\nIn the implementation of our proposed method, we used Proximal Policy Optimization (PPO) (Schulman et al. 2017) as the optimization algorithm for \u03c0. In addition, we employed the generalized advantage estimate (Schulman et al. 2015) to implement \u03a8(t,m). The value network Vy is trained to minimize the mean squared error with a cumulative reward:\n$L_V(\\psi) = E[\\sum_{i=0}^{T} \\sum_{j=1}^{M} (V_{\\psi}(X_{(i,j)}) - \\sum_{t=i}^{T} \\sum_{m=j}^{M} \\gamma^{(i+1)(j-1)}r_{(t,m)} + (i,j))^2]$\n(8)\nHere, (t, m) + (i, j) represents the progression of timesteps in the module-level MDP, similar to Eq. (7). V\u03c8 estimates the state value of each module m at each turn t. The final policy objective is formulated based on Eq. (5), and the clipped surrogate objective (Schulman et al. 2017):\n$L(\\phi) = - E[\\sum_{t=0}^{T} \\sum_{m=1}^{M} min(s(\\phi_{(t,m)}, 1 - \\epsilon, 1 + \\epsilon)\\hat{A}_{(t,m)}]$\n(9)\n$s(\\phi_{(t,m)}) = \\frac{\\pi_{\\phi}(Y_{(t,m)}|X_{(t,m)})}{\\pi^{\\phi}_{old}(Y_{(t,m)}|X_{(t,m)})}$ is the ratio of probabilities between the policy being updated and the policy before update in each iteration.\nFollowing previous studies (Ziegler et al. 2019; Stiennon et al. 2020), to prevent the probability distribution output by the policy network from deviating too far from the original $\u03c0^{RL}_{\u03b8}$ obtained by IL and breaking the output, we used a penalty based on Kullback-Leibler (KL) divergence as the final immediate reward $r'_{(t,m)}$:\n$r'_{(t,m)} = R_{(t, m)} - \\beta D_{KL}[\\pi^{\\phi}(Y_{(t,m)}|X_{(t,m)}) || \\pi^{RL}_{\\theta}(Y_{(t,m)}|X_{(t,m)})]$\n(10)\nwhere \u03b2 denotes a hyperparameter indicating the KL penalty coefficient. Algorithm 1 summarizes the RL algorithm.\nB Details of UniPPN Training\nWe adopted the 355M parameter version of GPT-2 as the backbone model for UniPPN. The outputs of some modules, such as NLU and DST contain redundant occurrences of domain names or other symbols (e.g., [restaurant, -, price, range, =, cheap, ; restaurant, -, food, =, italian] with 12 tokens). This redundancy may complicate LM's generation task and hinder exploration during RL. To address this, we add tokens representing domain and slot name pairs and assign one token to a pair to simplify the sequence format (e.g., [restaurant-pricerange=, cheap, restaurant-food=, italian] with four tokens). This enables the model to focus solely on learning to modify $out'_{(t,m)}$. The embedding parameters for these new vocabularies were randomly initialized. Below, we describe the learning details of IL and RL, as well as the impact of the module-level MDP.\nImitation Learning\nIn the supervised fine-tuning of $\u03c0_\u03b8$ using D1:M, we divided Dm into mini-batches of size 64 and trained them for 10 epochs using the AdamW optimizer. For the first five epochs, we trained only the embedding vectors of the newly added vocabulary for the slots with a learning rate of 5 \u00d7 10\u22123, whereas for the latter five epochs, we trained the entire $\u03c0_\u03b8$ with 5 \u00d7 10-5. This prevents random gradients caused by randomly initialized embedding parameters from destroying $\u03c0_\u03b8$ as a whole during the early stages of learning.\nReinforcement Learning\nFor $\u03c0'\u03c2$ generation parameters during RL, we set the maximum number of input and output tokens to 256 and 128, respectively, and both the top p and temperature to 1.0, to promote exploration. The total number of iterations for the entire learning process was set to 200, and the number of turns sampled in each iteration was 1,024. In each iteration, the trajectories of the post-processing outputs from all modules accumulated in the replay buffer were trained for four epochs with a mini-batch size of 128 and gradient accumulation steps M. We used the Adam Optimizer with a learning rate of 1 \u00d7 10-6. \u03b3 and \u03bb were set to 0.99 and 0.95 respectively, and the coefficient $\u03b2$ of the KL divergence penalty was set to 0.01."}]}