{"title": "DiffETM: Diffusion Process Enhanced Embedded Topic Model", "authors": ["Wei Shao", "Mingyang Liu", "Linqi Song"], "abstract": "The embedded topic model (ETM) is a widely used approach that assumes the sampled document-topic distribution conforms to the logistic normal distribution for easier optimization. However, this assumption oversimplifies the real document-topic distribution, limiting the model's performance. In response, we propose a novel method that introduces the diffusion process into the sampling process of document-topic distribution to overcome this limitation and maintain an easy optimization process. We validate our method through extensive experiments on two mainstream datasets, proving its effectiveness in improving topic modeling performance.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, the embedded topic model [1] has garnered significant attention [2]\u2013[9] due to its interpretable and flexible variational auto-encoder architecture [10], [11]. Despite its merits, the embedded topic model still faces a critical challenge: it assumes that the topic distribution of a document assumes to the logistic-normal distribution and thus employs a simple yet effective variational loss for training. While this assumption facilitates easier optimization, it also imposes a strict constraint on the learned document-topic distribution. As a result, the embedded topic model struggles to achieve higher performance in topic modeling, as it fails to capture the complexity of the real document-topic distribution (we further illustrate this limitation in Fig. 1).\nTo alleviate this problem, we rethink the architecture of the embedded topic model, which is a variational auto-encoder. Compared to the standard auto-encoder [12], it tries to model more complex document-topic distribution by introducing noise to the hidden representations sampled from a standard normal distribution. The noise is fused into these hidden representations in the form of mean and variance and thus results in a variable following a Gaussian distribution. This variable is then transformed into document-topic distributions using a softmax function. However, according to the experimental results we observed, the introduction of these noises is still insufficient to model the document topic distribution well.\nTo this end, we propose a new idea of representation enhancement: we directly sample these hidden representations from document representations. The advantage of this idea is that compared to ETM, we integrate document information into these hidden representations, improving its ability to model document-topic distribution. On the other hand, the disadvantage is that due to the change in the hidden representation's distribution, we cannot use the existing objective function for optimization, and it is difficult for us to obtain an accurate loss function.\nTo alleviate this disadvantage, we combine the sampling process with the forward process of diffusion model [13], which gradually introduces noise conforming to the normal distribution into the document representations, following the steps of the diffusion process. Finally, the resulting representation contains both document information and is close enough to the normal distribution and thus we can still utilize the objective function of the embedded topic model to optimize the new model.\nBy utilizing this diffusion-based approach, our proposed"}, {"title": "II. PRELIMINARY", "content": "Topic modeling is a task to obtain the hidden document-topic distribution via modeling the whole document set. For a set with N documents, it has V unique words. We use a bag-of-words model [16] to represent each document $X_i \\in R^V$. There is a latent topic set $Z = {Z_1, Z_2, ..., Z_K}$ consisting of K latent topics in the document set and each document $X_i$ has a distribution $\\theta_i \\in R^{1\\times K}$ over this topic set (document-topic distribution). For each topic zi, there is also a distribution $\\beta_i \\in R^{1\\times V}$ over vocabulary.\nThe topic model aims to model the document set and the modeling process is equivalent to maximizing the likelihood of documents:\n$$L = \\sum_{i=1}^Nlogp(X_i),$$\n$$p(X_i) = \\prod_{j=1}^V\\sum_{k=1}^K p(z_k|X_i)p(W_j|z_k))^{X_{ij}},$$\n$$p(X_i) = \\prod_{j=1}^V(\\theta_i \\times \\beta)^ {X_{ij}}$$\nwhere $w_j$ is the j-th word in the vocabulary, and the $X_{ij}$ means the number of occurrences of $w_j$ in $X_i$. In the embedded topic model, words and topics are projected into a vector space and form two embedding matrices: word embedding matrix $\\rho \\in R^{V\\times E}$ and topic embedding matrix $a \\in R^{K\\times E}$. The topic-word distribution $\\beta = softmax(a \\times \\rho)$."}, {"title": "III. METHODOLOGY", "content": "The architecture of our model is shown in Fig. 2, which includes three modules: diffusion module (in yellow), document-topic distribution computation module (in green), and topic-word distribution computation module (in blue).\nThe diffusion module and document-topic distribution module work together to produce the document-topic distribution $\\theta$, and the topic-word distribution module could generate the topic-word distribution $\\beta$. Then, the two distributions' product $X'$ is regarded as the reconstruction of the input document's normalized bag-of-words representation $X$. In the following sections, we will introduce each module and how to train the whole model in detail."}, {"title": "B. Diffusion Process", "content": "At first, we use a feed-forward network, consisting of three linear layers and two ReLU activation functions, to produce the enhanced representation Xo of document representation X.\n$$X_o = NN(X).$$\nThe $X_o$ is the input of the following diffusion process. The diffusion model's forward process adds noise to the inputs at different levels step by step. There are various methods to add noise. To ensure the simplicity of the topic model, we utilize the linear noise scheduler in our model. The linear noise scheduler adds the noise to the input document $X_o$ and finally produces $X_T$ as the hidden representations $e$, which is very close to the normal distribution. We could represent the process as:\n$$q(X_T|X_o) = N(X_T; \\sqrt{\\bar{a_T}}X_o, (1 - \\bar{a_T})I),$$\nwhere $\\bar{a_T} = \\prod_{t=1}^T a_t$, $a_t = 1 - \\beta_t$. In the embedded topic model, the initial value e is usually sampled from the normal distribution N(0, 1)."}, {"title": "C. Document-Topic Distribution Module", "content": "In this module, the noise is added to e in the form of mean and variance. The mean \u00b5 and variance \u03c3 are obtained from two neural network layers with the same architecture as the neural network module in the diffusion module. Then, e is multiplied by the standard deviation \u03c3, added to \u00b5, and finally passed through a softmax function to produce the document-topic distribution \u03b8. It is represented by:\n$$\\mu = NN(X; v_\\mu),$$\n$$\\sigma = NN(X; v_\\sigma),$$\n$$z = \\epsilon \\sigma + \\mu,$$\n$$\\theta = softmax(z),$$\nwhere the v\u00b5 and v are parameters of the neural networks."}, {"title": "D. Topic-Word Distribution Module", "content": "In the embedded topic model, the topic-word distribution is the product of topic embedding and word embedding:\n$$\\beta = a \\times \\rho.$$\nEach element in vector $B_{k}$ represents the probability of the corresponding word belonging to topic k (e.g., $B_{i,j} = p(w_j|z_k)$)."}, {"title": "E. Training", "content": "This model is trained via two loss terms: one is the reconstruction loss L(X, X'):\n$$L(X, X') = XlogX',$$\nAnd another term is the KL divergence between the produced z and the normal distribution N(0,1):\n$$L_{KLD} = KL(z||N(0, 1)).$$\nThe final loss function is:\n$$L = L(X, X') + \\lambda * L_{KLD},$$\nwhere is a hyperparameter used to balance the trade-off between the two terms. The whole parameters include the neural networks' parameters in diffusion and document-topic distribution module and the topic and word embedding matrices."}, {"title": "IV. EXPERIMENT", "content": "In this section, we assess the performance of our proposed method in comparison with embedded topic models (ETM [1], ERNTM [4]), classic neural topic models (NTM [17], NTMR [17]) and recent neural topic models (DeTiME [7], Meta-CETM [9]).\nWe adopt the same hyper-parameters as NTM for fair comparisons. The parameters are initialized randomly for all experiments with the same random seed. We report topic coherence, topic diversity, and topic quality (the product between topic coherence and topic diversity) and document perplexity for each model on 20NewsGroup and NewYorkTimes datasets. All the codes were implemented using PyTorch [18]."}, {"title": "A. Dataset's Statistical Information", "content": "We conduct experiments on 20NewsGroup and New York-Times datasets. The statistical information of datasets we used in this paper is shown in Table I."}, {"title": "B. Implement Details", "content": "In our settings, the word embedding size and topic embedding size are both 300 and we train these two embeddings in the training process. The diffusion step T is 100, the $B_0$ is 0 and $\u03b2_T$ is 0.02. Batch sizes on 20NewsGroup and New YorkTimes datasets are 1000 and 512, respectively. The \u03bb in the loss is 1. The learning rates on the 20Newsgroup dataset are 0.008, 0.009, and 0.01 for K=50, K=100 and K=200. The learning rates on the NewYorkTimes dataset are 0.007, 0.007 and 0.008 for NYT-3000, NYT-5000 and NYT-10000."}, {"title": "C. Main Results", "content": "Since the performance of the topic model is influenced by the number of topics and the vocabulary size, we evaluate the model under different settings to better illustrate its performance. Specifically, we test models with varying topic numbers on the 20NewsGroup dataset and with different vocabulary sizes on the NewYorkTimes dataset. The detailed results are presented in Tables II and III.\nIn detail, Table II shows the comparison results on the 20NewsGroup dataset with different topic numbers. According to this table, our model outperforms all baselines with various topic numbers. Compared to ETM, the improvements are up to 77.89%, which is achieved on the topic quality with 100 topics.\nFor the New YorkTimes dataset, we construct different vocabulary sizes by removing words whose frequency is lower than 3000, 5000, and 10000. As shown in Table III, our proposed method reaches the best topic quality and topic perplexity on all three New York Times datasets."}, {"title": "D. Ablation Study", "content": "In order to verify the validity of the diffusion process of our model, we conducted the following ablation study: we compare our model with a model that undergoes the following modifications: (-Diffusion) removing diffusion process and directly using the document representation to produce the hidden representation. The results are shown in the Table IV. We find that compared to DiffETM, the model with diffusion removed has a slight decrease in metrics such as topic diversity, and topic coherence, but still outperforms ETM. The decline in perplexity metrics is large. The variant's performance on perplexity is weaker than ETM. These results indicate that the introduction of the diffusion process plays an important role in ensuring the smooth optimization of the model."}, {"title": "E. Hyper-Parameter Analysis", "content": "We observe that DiffETM has an important hyperparameter: the diffusion step T. This parameter determines the closeness of the hidden representations to the normal distribution, affecting the optimization process of the model as well as the final perplexity. To better illustrate the effect of this hyperparameter on the model, we list the performance of the DiffETM on the 20NewsGroup dataset for different T at K = 50. The results are shown in Table V. Based on the results in the table, we find that T has a large impact on perplexity. When T is small, perplexity is large. It indicates that the model is not well optimized. When T is large, perplexity is small and maintained at a stable stage. In addition, as T changes, the other two indicators also fluctuate slightly. In practice, we need to choose an appropriate T to keep the model harmonized on the three indicators."}, {"title": "V. CONCLUSION", "content": "After considering the limitations of the ETM in modeling document-topic distribution, we introduce the diffusion process into the sampling process of this distribution in the ETM for stronger modeling ability and to maintain a smooth optimization process. Extensive experiments conducted on two mainstream datasets have demonstrated our model's effectiveness in improving topic modeling performance."}]}