{"title": "Evaluating Binary Decision Biases in Large Language Models: Implications for Fair Agent-Based Financial Simulations", "authors": ["Alicia Vidler", "Toby Walsh"], "abstract": "Large Language Models (LLMs) are increasingly being used to simulate human-like decision making in agent-based financial market models (ABMs). As models become more powerful and accessible, researchers can now incorporate individual LLM decisions into ABM environments. However, integration may introduce inherent biases that need careful evaluation. In this paper we test three state-of-the-art GPT models for bias using two model sampling approaches: one-shot and few-shot API queries. We observe significant variations in distributions of outputs between specific models, and model sub versions, with GPT-40-Mini-2024-07-18 showing notably better performance (32-43% yes responses) compared to GPT-4-0125-preview's extreme bias (98-99% yes responses). We show that sampling methods and model sub-versions significantly impact results: repeated independent API calls produce different distributions compared to batch sampling within a single call. While no current GPT model can simultaneously achieve a uniform distribution and Markovian properties in one-shot testing, few-shot sampling can approach uniform distributions under certain conditions. We explore the Temperature parameter, providing a definition and comparative results. We further compare our results to true random binary series and test specifically for the common human bias of Negative Recency - finding LLMs have a mixed ability to 'beat' humans in this one regard. These findings emphasise the critical importance of careful LLM integration into ABMs for financial markets and more broadly.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) are founded on human text, and inherit patterns from their training data. They demonstrate capabilities in understanding and generating human-like text across various domains (Chang et al. 2023) and replicating human like behaviours as agents (Park et al. 2023). Since humans demonstrate well-documented biases in their perception of randomness (Warren et al. 2018), we test to see if LLMs produce or replicate these biases when asked to make probabilistic decisions. LLMs have been found to deviate from intended trading instructions (Vidler and Walsh 2024a), leading to systematic decision making biases despite demonstrating LLMs' potential for multi-agent systems in finance. This becomes particularly relevant when using LLMs to make binary choices (e.g. \"Randomly choose yes or no\") such as in financial markets trading applications. If financial models or markets seek to replace random generating functions with LLMs - what would happen?\nIn financial markets, randomness is a widely used mechanism for safety and fairness: markets such as the London Stock Exchange open and close at a random time within a pre-set window\u00b9, and many banks are regulated under rules such as BIS Basel IV, focused on estimation of risk and capital adequacy through use of underlying probability distributions\u00b2. Throughout financial pricing and risk models, extensive reliance on distributional assumptions and sampling is common place (Hull 2022). Any integration of LLM's into ABMs, or more broadly, financial market trading or risk systems, will necessarily incorporate any LLM bias.\nIn this paper we pose a deeper question about the integration of LLMs into financial models; give that random numbers form an integral part of financial markets modelling, and decision making is crucial, can state-of-the-art LLMs reliably reproduce a simple uniform probability distribution, producing fair binary choices in response to independent agent-like API requests? Can an LLM \"toss a fair coin\" when we ask it to within a model?"}, {"title": "Our Contribution", "content": "In this paper we explore the significant variations in coin toss experimental results achieved with three widely-used LLMs: GPT-3.5 Turbo-0125, GPT-4-0125-preview, and GPT-40-mini-2024-07-18. To mirror natural language and avoid potential usage bias with words such as \"heads\" or \"tails\", we define a coin toss as asking for a \"yes\" or \"no\" binary output. We focus our results on \"yes\" and \"no\" decisions to increase applicability and broaden possible usage. We build on recent work on randomness and human bias in LLMs by (Koevering and Kleinberg 2024), which found a mixed picture of results. We extend their work to explore specific released versions of common LLM's and differences between sub versions. We extend (Renda, Hopkins, and Carbin 2023) with new terminology to better reflect the way in which a typical ABM might approach querying an LLM:"}, {"title": "Recent and Relevant Literature on LLMs", "content": "In November 2024, OpenAi, the creators of ChatGPT, announced they are working on an autonomous agent based version of their seminal LLMs, reportedly called \"Operator\" (Ghaffary and Metz 2024). Agentic AI tools are at the forefront of LLM design (Ng 2024). Pre-trained LLMs, such as GPT-3 (Bubeck et al. 2023) and GPT 4 (Luo et al. 2023), have demonstrated proficiency in tasks such as dialogue generation and natural language interactions, however a fundamentally different questions is asked of an LLM in an agentic AI tool and ABM. Unlike text and analysis needs, ABMs rely on agency, that is to say, agents are imbued, and defined by, the requirement to make a decision in some frame or context (Wooldridge 2009). Many such implementations ultimately require a binary choice from the agent: to more or not to move - \"yes\" or \"no\". While LLMs offer promising capabilities as decision making agents, they face both engineering implementation challenges (Chopra et al. 2024) and fundamental issues with decision making processes that can significantly impact overall behaviour (Vidler and Walsh 2024a)."}, {"title": "Generative Agent based models (GABMs)", "content": "Current thinking suggests that integrating LLM's into multi-agent frameworks has great potential for future AI systems (Ng 2024). Initial work by (Park et al. 2023) found that generative agents could be realistic simulacra of humans. Early attempts to introduce an LLM as an agent (Chopra et al. 2024) found granular agent decisions were unfeasible to implement from a technological standpoint. More recent work on these GABMs in financial trading decisions, using GPT-40-Mini-2024-07-18, showed realistic representations of order-to-trade ratios found in US equity markets (Vidler and Walsh 2024a)."}, {"title": "Financial market use and Markov property", "content": "When using an LLM as a decision maker within an ABM or any other model, it is foreseeable that there may be challenges in relation to the sampling of the LLM's internal distribution of tokens, in effect producing an LLM model specific probability distribution of responses. We focus also on the question of binary outcomes being Markovian, or memory-less. The Markov property is a key financial market concept introduced into modelling of markets by (Fama 1965). In ABM terms this translates into a need to have each agents decision be based only on the decision process at the time step in question. Non-Markovian properties of ABMs for financial decisions would point to information leakage, collusion or agent group dynamics by default, defeating the goal of independent agents as a starting point for modelling.\nThe potential to utilise LLMs to enrich other model methods with human characteristics (such as human biases) is beginning to be explored in text based arenas such as chat bots (Gu et al. 2024) and survey participation (Tjuatja et al. 2024), and human \"sims\" (Park et al. 2023) to name just a few. With the rapid advancement of both foundational LLM models, and hardware, it is now feasible to incorporate LLMs into ABMs directly (Vidler and Walsh 2024a) without the need for archetypes, or reduced agent fidelity, something that as little as 6 month ago was not considered computationally feasible (Chopra et al. 2024)."}, {"title": "Human Bias in Random sequence generation", "content": "Research into human capacity for generating random sequences spans decades in psychological literature (Ginsburg and Karpiuk 1994), (Towse and Neil 1998). Traditional findings identified three primary biases in human-generated random sequences: cycling (avoiding recently used numbers), seriation (stereotypical patterns like ascending or descending sequences), and repetition avoidance (Angelike and Musch 2024). Recent studies have revealed a more nuanced understanding of these limitations (Warren et al. 2018), (Wong, Merholz, and Maoz 2021). Researchers have identified key metrics for distinguishing human-generated sequences from truly random processes, notably \"algorithmic complexity\" and \"repetition median\u201d which are found to be especially effective for sequences under 20 repetitions (Angelike and Musch 2024). As specific bias, Negative Recency has been identified,where humans rely on recent responses in sequential decision-making (such as coin tosses) to influence subsequent choices ((Baena-Mirabete et al. 2023), (Koevering and Kleinberg 2024)) illustrating a \"negative recency\" bias. Human psychology research has found that this effect varies with age (Baena-Mirabete et al. 2023) due to differences in working memory capacity, necessitating age-based criteria in comparative analyses. Additionally, (Gauvrit et al. 2017) demonstrated that age is the primary factor affecting random sequence complexity, while (Biesaga, Talaga, and Nowak 2021) found that fatigue negatively impacts performance. Our research extends these insights focusing on the negative recency bias in random sequence generation (Towse and Neil 1998), (Angelike and Musch 2024) in specific LLM versions and independent sampling methods."}, {"title": "Computational Randomness", "content": "Research on LLM randomness has revealed limitations in number generation (Renda, Hopkins, and Carbin 2023; Harrison 2024; Liu 2024), probabilistic text (Tjuatja et al. 2024; Renda, Hopkins, and Carbin 2023; Peeperkorn et al. 2024), and behavioral simulations (Gu et al. 2024). Studies show LLMs struggle with sampling from specific distributions (Imani and Du 2023; Renda, Hopkins, and Carbin 2023), though findings conflict: (Tjuatja et al. 2024) found greater bias than humans, while (Harrison 2024) demonstrated superior randomness in gpt-3.5-turbo-0125. Human comparisons often rely on (Figurska, Sta\u0144czyk, and Kulesza 2008)'s limited 37-person study, leaving room for more comprehensive analysis.\nOur work addresses the understudied impact of temperature settings on LLM randomness by systematically analyzing its effects across three specific GPT model versions using a binary decision test."}, {"title": "Temperature", "content": "Temperature, while absent from (Vaswani et al. 2017), is a crucial parameter in modern LLM APIs that controls output randomness. Defaulting to 1 (max 2) across tested models, its name derives from the SoftMax function's connection to Boltzmann's thermodynamic work (Bridle 1989), making it relevant for analysing decision-making behavior. We provide a formal definition here:\nDefinition: SoftMax function $\\sigma$: $\\mathbb{R}^P \\rightarrow (0,1)^P$, where P > 1, takes a vector $z = (z_1, ..., z_k) \\in \\mathbb{R}^P$ and computes each component of vector $\\sigma(z) \\in (0, 1)^P$ with\n$\\sigma(z) = \\frac{e^{z_i}}{\\sum_{j=1}^{P}e^{z_j}}$\nWith the addition of a scaler $\\beta$ to the exponential terms in Equation (1) produces;\n$\\sigma(z) = \\frac{e^{\\beta z_i}}{\\sum_{j=1}^{P}e^{\\beta z_j}}$\nDefinition: Temperature Let T to be a value proportional to the inverse of $\\beta$ such that: $T = \\frac{1}{\\beta}$\nTemperature's impact has been studied across LLM applications including creative writing (Peeperkorn et al. 2024), question answering (Renze and Guven 2024), coding (Zhu et al. 2023), and structured reasoning (Ouyang et al. 2023; Hickman, Dunlop, and Wolf 2024). While (Wang et al. 2024) found task-dependent optimal temperatures, methodological approaches vary: (Harrison 2024) used only model defaults, whereas (Koevering and Kleinberg 2024) tested multiple settings but omitted model specifics. Our work extends this field by examining Temperature's specific impact on a binary decision distribution, independently sampled for three specific GPT sub-versions."}, {"title": "Testing methodology", "content": "We test LLM binary decision making, across three OpenAI models and refer to the following models (3):\n\u2022 M1 (gpt-40-mini-2024-07-18): Advanced small model optimised for cost-efficiency\n\u2022 M2 (gpt-4-0125-preview): Full GPT-4 Turbo with enhanced instruction following\n\u2022 M3 (gpt-3.5-turbo-0125): Latest GPT-3.5 iteration with improved format adherence\nWe extend Few-Shot Learning Terminology to LLM Querying. We borrow from (Vinyals et al. 2017) and present methods similar in the vein of (Renda, Hopkins, and Carbin 2023) and define specific query methods:\nDefinition: One-Shot Querying:\nIndependent API calls with fixed prompt, each generating one decision, representing a distinct sampling event\nDefinition: Few-Shot Querying:\nSingle API call requesting multiple samples n with fixed prompt, in the same API call\n1. One-Shot Testing: Sequential API calls with 1-second delays, alternating between prompts to collect 100 responses per prompt (i.e. 200 API requests) in an independent sampling approach. Each API call is an independent request of the internal model state.\n2. Few-Shot Testing: Single API calls requesting 100 comma-separated responses, with 1-second delays between batches. We collect 10 batches per question, per model. (i.e. 10 API requests only)\n3. Temperature Impact Analysis: Temperature settings from 0.5-2.0, collecting 100 responses per setting. We conduct these tests in a one-shot setting, to align with ideal use cases in ABMs for financial decision in line with (Vidler and Walsh 2024a)."}, {"title": "Results", "content": "Unable to reliably replicate a mean of 50%: In the first test, none of the models accurately replicated the expected distribution (p = 0.5). While GPT-40-Mini-2024-07-18's Q2 responses still deviated from the expected distribution (43% yes, 57% no), it was the only result where we cannot reject the null hypothesis of uniformity and the difference is not statistically significant at the a = 0.05 level (see Table1). The remaining models (GPT-4-0125-preview and GPT-3.5-Turbo-0125) performed particularly poorly for all questions and were statistically significant deviations away from uniformity (all p < 0.001. Question 1's response for GPT-40-Mini-2024-07-18 were also found to be non -uniform.\nModel variations are extreme: across models, with GPT-4-0125-preview producing 99% \"Yes\" responses for Q1 compared to GPT-40-Mini-2024-07-18's 32%. GPT-40-Mini showed moderate bias (32-43% yes), while GPT-4-0125-preview and GPT-3.5-Turbo-0125 exhibited extreme yes bias (98-99% and 87-98% respectively).\nImpact of basic prompt with \"random\" included: Question 1 simply prompted \"yes or no\u201d where as Question 2's \"random\" framing only improved uniformity in GPT-40-Mini-2024-07-18. The $x^2$ statistic reduced from 12.96 to 1.96 (p=0.162), making it the only case where we do not reject the null hypothesis. However, GPT-4-0125-preview and GPT-3.5-Turbo-0125 maintained strong \"Yes\" biases in Q2 with $x^2$ > 87 and p < 1e \u2013 20, regardless of the tested prompts."}, {"title": "Test for Markovian responses:", "content": "For HP2, we analyse the Markov property to test response independence, with results reported per model and question in Table 2. GPT-4-0125-preview and GPT-3.5-Turbo-0125 produce near-perfect dependence in Q1 (P(Yes) = 0.99-1.00, P(Yes\u2014Yes(t-1)) = 0.98-1.00), with deterministic behaviour especially stark in GPT-3.5-Turbo-0125 Q1 responses (86 consecutive Yes Yes transitions). While GPT-3.5-turbo-0125 and GPT-4-turbo-preview show low chi\u00b2 values (0-0.187), this likely reflects their strong response bias rather than true independence. GPT-4-turbo-0125-preview-mini exhibited more varied responses but showed significant dependencies in both questions ($X_{1}^{2}$ = 21.86, $X_{0}^{2}$ = 36.19, both p < 0.05), rejecting Ho."}, {"title": "Few-Shot Results: Improved Performance with Multi-Sample Generation", "content": "Our few-shot testing, where LLMs generated 100 responses per API call across 10 iterations, showed significantly improved uniformity. When averaged, all models and questions approximated the expected 50/50 distribution more closely than in one-shot testing. All cases could not reject the null hypothesis except GPT-40-Mini-2024-07-18's Q2 responses-notably, the only case that showed uniformity in one-shot testing. Results are detailed in Table 3.\nHowever, the results reveal concerning patterns. Lists of yes/no pairs show strong sequential dependencies, rejecting the Markov property (HP2) across all models. The conditional probability P(Yes|Yes) varies from below 10% for"}, {"title": "Inter-batch Analysis", "content": "Mean: Looking at the 10 batches of 100 responses (within 1 API), the average Yes% is 51.83% across all runs and the average x2 level is 0.37 with a maximum of any batch across all models being 1.96, well below the test statistic threshold of 3.841, meaning we cannot reject Ho.\nMarkovian Independence: Exploring independence within and across batches, we see that a mixed pattern of results with the main theme being that Question 2 produces more reliable results per batch and again the SOTA GPT-40-mini-2024-07-18 has the advantage over the other models. Across 10 batches, for 3 models and 2 questions each, we test 60 sets of 100 few-shot responses. Across these 60 sets we see Ho is not reject 35 times, and 80% of Q2 results cannot reject Ho. Results broken down by model show that GPT-4-0125-preview has the highest number of batches that can not reject Ho but in terms of robustness to prompts, GPT-40-Mini-2024-08-17 shows most consistency (across Q1 and Q2), where as other models only Q2 is not rejected. This mixed pattern of Markovian independence of responses is more promising than the one-shot version where only 1 test was found to be Markovian. However, tests across batches still produced biased distribution in 41.7% (25 or 60) batch results."}, {"title": "Limitations of practical implementation of Few-shot methods", "content": "Few-shot sampling in ABMs diverges from realistic individual decision-making by providing multiple decisions simultaneously. This approach increases computational overhead, LLM costs, and memory requirements, while requiring agents to process distribution data statistically. Given these limitations and their impact on model performance, we do not pursue further few-shot analysis in this paper.\nThe testing of Few-Shot methods across different models (GPT-3.5, GPT-4, and GPT-40-Mini) failed to reject the null hypothesis in 58.3% of cases, with a notably higher non-rejection rate for Q2 (80%) compared to Q1 (36.7%)."}, {"title": "Temperature: Impact of Temperature settings on One-shot Query results", "content": "Exploring temperature settings for One-Shot Query testing at values: T = [0.5,0.75, 1.0, 1.25, 1.5, 1.75, 2.0] we find that LLM APIs do not accept T > 2. Temperature variations tested in GPT-3.5 and GPT-4-0125-preview consistently yields skewed results, with 73% to 100% of responses being \"Yes\", failing to approximate the expected 50% mean and showing response dependence, rendering Markovian tests ineffective. No temperature variation tested was able to produce a binary distribution or Markovian response sequence in these models. In contrast, varying Temperature in GPT-40-Mini-2014-07-18 did effects the mean Yes/No levels non-linearly. Also, tests at Temperature = 1 reveal variable P(Y) from previous tests reported earlier in this paper (see Table 5). The only model that produces Markovian response is GPT-40-Mini-2014-07-18, and it did so across both questions and all temperature settings, in line with our earlier results reported here."}, {"title": "Yes/No frequency in Common Crawl open repository of web data", "content": "The best performing model, 4o-Mini-2024-07-18, with a training cut off date of 2024-07-18 prompted an investigation into potential training data biases. Using Common Crawl (Common Crawl 2024) (CC-MAIN-2024-30), we analysed \"Yes\u201d/\u201dNo\u201d (case insensitive) frequencies in web archives up to the model's training cutoff. We examined both truncated (first 1000 characters) and full page content across samples ranging from 100 to 25,000+ pages, providing insight into potential word frequency biases in LLM training data. We have no reason to suspect that the results will differ for other web archive scraping methods or sources.\nAnalysis of Common Crawl data revealed distinct patterns: truncated data showed \"Yes\" comprising 9.7% of Yes/No instances (P(Yes| Yes or No) = 33.5%), with 70% of pages containing neither term. Full-page analysis found Yes/No responses in 51% of pages, with P(Yes) = 10% and P(Yes| Yes or No) = 20.3%. Chi-square tests (p\u00a1 0.05) confirm significant deviation from uniformity across all sample sizes (detailed in Table 6). These findings highlight discrepancies between both the assumed uniform distribution in LLMs (P(Yes) = P(No) = 0.5) and the human usage actual outputs."}, {"title": "Comparing LLM Randomness to Human and True Random Sequences", "content": "Extending recent work by (Harrison 2024) and (Angelike and Musch 2024), we compare our one-shot and few-shot results against true random binary sequences from Random.org4. Following (Baena-Mirabete et al. 2023), we analyze sequences using sliding windows of size w (1 \u2264 w < 5), where w represents the sequence length influencing subsequent choices. For a binary sequence $S = (s_1, ..., s_n)$ where $s_i \\in$ Yes, No, we calculate negative recency effects. Our method is as follows:\nBaseline Switching Rate The baseline switching rate represents the overall probability of alternation in the sequence.\n$\\text{baseline rate} = \\frac{\\sum_{i=1}^{|S|-1} \\mathbb{I}(s_i \\neq s_{i-1})}{|S| - 1}$ \nwhere $\\mathbb{I}(s_i \\neq s_{i-1})$ is the indicator function that equals 1 if adjacent elements differ and 0 otherwise.\nWindow-Specific Switching Rates For each window size w, we calculate the switching rate after runs of length w as:\n$\\text{switch rate}_w = \\frac{\\sum_{i=1}^{|S|-1} \\mathbb{I}(s_i \\neq s_{i-1})\\cdot \\text{run}_w(i)}{|\\{i: \\text{run}_w(i)\\}|}$\nwhere:\n\u2022 $\\text{run}_w(i)$ equals 1 if position i follows a run of length w\n\u2022 $|\\{i: \\text{run}_w(i)\\}|$ is the number of positions that follow runs of length w\nRecency Effect The recency effect for window size w is then calculated as the difference between the window-specific switching rate and the baseline rate;\n$\\text{recency effect}_w = \\text{switch rate}_w - \\text{baseline rate}$"}, {"title": "Recency Bias Results", "content": "Analysis reveals that most model outputs exhibit recency bias compared to true random sequences, with few exceptions (highlighted in green). While GPT-40-Mini-2024-07-18 avoided the human recency effects documented by (Baena-Mirabete et al. 2023) for 1 and 3-step lookbacks. In this way, these model combinations could be considered more \"random\" than humans, however these models still failed to simultaneously achieve both uniform distribution and Markovian independence."}, {"title": "Conclusion", "content": "We test three LLM model subversions for decision making biases by examining binary decision outputs across: GPT-3.5 Turbo-0125, GPT-4-0125-preview, and GPT-40-Mini-2034,07,18. We find they cannot adequately replicate a uniform distribution in independent sampling of these models (One-Shot).\nWe also find statistically significant performance variations between models (GPT 4 to GPT 3.5) and especially between specific sub-versions (GPT 4 and 40-Mini), in addition to significant impacts of sampling methods on results. Using a simple Yes/No benchmark task, only GPT-4o-Mini-2024-07-18 (one-shot) achieves output not statistically different from uniform distribution, though responses remain non-Markovian. Temperature adjustments (0.5-2.0) failed to reliably influence these distributions, with GPT-40-Mini showing non-linear responses while other models maintained strong biases regardless of temperature setting.\nThe few-shot methodology, while producing better distributional outcomes, are less practical for ABM applications and nearly half of all tests result in non-Markovian decision sequences, indicating persistent temporal dependencies. Between model versions, we observe substantial variations in response patterns, suggesting architectural and training differences significantly impact decision making capabilities. We further compare results to true random binary series and test specifically for the common human bias of Negative Recency - finding LLMs have a mixed ability to 'beat' humans, with GPT-40-Mini-2024-07-18 notably avoiding human recency effects in one-shot testing, though still producing non-Markovian outputs.\nThese findings expose systematic biases in LLM-based decision making, sensitivity to model sub-versions and to sampling methods. These hold critical implications for ABMs, particularly in finance where Markovian properties are typically assumed.\nBuilding on these findings, future work should focus on several key areas: enhancing LLM integration in financial ABMs through improved real-time decision mechanisms; investigating the relationship between model architecture, underlying causes of bias and decision-making capabilities; evaluating performance across non-OpenAI models; exploring methods to mitigate identified decision biases; and examining implications for broader financial modelling applications. This research pathway aims to better understand and address the challenges of implementing LLMs in practical financial modelling environments, ultimately improving their reliability for real-world applications."}]}