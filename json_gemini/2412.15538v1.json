{"title": "FedRLHF: A Convergence-Guaranteed Federated Framework for Privacy-Preserving and Personalized RLHF", "authors": ["Flint Xiaofeng Fan", "Cheston Tan", "Yew-Soon Ong", "Roger Wattenhofer", "Wei-Tsang Ooi"], "abstract": "In the era of increasing privacy concerns and demand for personalized experiences, traditional Reinforcement Learning with Human Feedback (RLHF) frameworks face significant challenges due to their reliance on centralized data. We introduce Federated Reinforcement Learning with Human Feedback (FedRLHF), a novel framework that decentralizes the RLHF process. FedRLHF enables collaborative policy learning across multiple clients without necessitating the sharing of raw data or human feedback, thereby ensuring robust privacy preservation. Leveraging federated reinforcement learning, each client integrates human feedback locally into their reward functions and updates their policies through personalized RLHF processes. We establish rigorous theoretical foundations for FedRLHF, providing convergence guarantees, and deriving sample complexity bounds that scale efficiently with the number of clients. Empirical evaluations on the MovieLens and IMDb datasets demonstrate that FedRLHF not only preserves user privacy but also achieves performance on par with centralized RLHF, while enhancing personalization across diverse client environments.", "sections": [{"title": "1 INTRODUCTION", "content": "Reinforcement Learning with Human Feedback (RLHF) has emerged as a powerful paradigm for training intelligent agents that align closely with human values and preferences [3, 23]. By integrating human feedback into the reinforcement learning loop, RLHF has enabled significant advancements in natural language processing, robotics, and personalized recommendation systems [34, 47]. A prominent example is ChatGPT [22], where RLHF has been instrumental in fine-tuning large language models (LLMs) to generate more coherent, contextually appropriate, and user-aligned responses [46].\nDespite these successes, the practice of aggregating data and feedback from multiple users in a central location for processing in RLHF poses significant privacy risks, especially in domains involving sensitive information such as healthcare or finance [32]. For instance, consider a personalized healthcare assistant that uses RLHF to adapt to individual patient needs. Centralizing patient data and feedback not only raises ethical concerns but may also violate regulations like the Health Insurance Portability and Accountability Act (HIPAA) [35] in the United States, or the General Data Protection Regulation (GDPR) [7] in the European Union. This centralized approach can lead to potential breaches of personal information, exposing users to risks of identity theft or unauthorized profiling. Moreover, different organizations or individuals may be reluctant to share their feedback data due to intellectual property concerns or competitive advantages [16].\nIn addition to privacy risks, centralization creates substantial hurdles for achieving personalization in RLHF systems. Users exhibit diverse preferences and behaviors, making it challenging for a centralized policy to cater effectively to all individuals [3]. Balancing global performance with personalization becomes non-trivial, as optimizing for the average user can lead to suboptimal experiences for specific individuals [33]. Returning to our healthcare assistant example, patients may have unique health conditions and treatment preferences. A one-size-fits-all model, trained on centralized data, may fail to provide the personalized recommendations necessary for optimal care, potentially impacting patient outcomes negatively.\nTo address these limitations, we propose Federated Reinforcement Learning with Human Feedback (FedRLHF), a novel federated framework that seamlessly integrates federated reinforcement learning principles [4, 8] with RLHF. As illustrated in Figure 1, FedRLHF uniquely addresses both privacy and personalization simultaneously through a decentralized approach. Each client in FedRLHF maintains a local model exclusively trained with RLHF on its own data and human feedback, ensuring that sensitive user information remains on-device. Clients interact with their unique local environments-reflecting individual user preferences, behaviors, and contexts-and collaborate by sharing only model updates with a central server. The server aggregates these updates to refine a global policy model, which is then broadcast back to the clients.\nThe FedRLHF framework introduces unique technical challenges, notably ensuring convergence guarantees in a decentralized environment and managing the trade-off between global performance and individual personalization. In federated reinforcement learning, the variability in clients' environments and behaviors can lead to instability and divergence in the learning process. Our FedRLHF framework addresses these challenges in a principled manner. We provide a comprehensive theoretical analysis establishing convergence guarantees and deriving bounds on the sample complexity under standard assumptions. Furthermore, we introduce a quantitative measure of personalization to analyze the trade-off between individual client policies and the global policy, offering insights into balancing personalization with overall system performance.\nOur contributions are as follows:\n\u2022 We introduce FedRLHF, a framework that uniquely integrates federated reinforcement learning with human feedback, enabling privacy-preserving and personalized policy learning (Section 3).\n\u2022 We provide convergence guarantees and derive sample complexity bounds that account for the integration of human feedback, extending existing FedRL theory (Section 4).\n\u2022 We develop a quantitative measure of personalization to analyze the trade-off between global performance and individual client policies (Section 5).\n\u2022 We empirically demonstrate FedRLHF's effectiveness on the MovieLens and IMDb datasets, showcasing its ability to preserve privacy, match centralized RLHF performance, and enhance personalization (Section 6)."}, {"title": "2 BACKGROUND & RELATED WORK", "content": "Reinforcement Learning with Human Feedback (RLHF) has become instrumental in aligning machine learning models with human values and preferences [3, 23, 34, 46]. By integrating human-generated feedback into the reward structure, RLHF facilitates the training of policies that exhibit behaviors more aligned with human preferences. However, traditional RLHF frameworks predominantly rely on centralized aggregation of data and feedback, which introduces significant privacy concerns. In addition, the centralization nature of RLHF also limits personalization, as it typically involves training a single reward model for all clients. This approach fails to accommodate the heterogeneous preferences of individual clients, leading to suboptimal policy performance in diverse environments, as illustrated in Figure 1.\nWhile previous studies [25] have explored personalized reward models in RLHF through representation learning, they still depend on the centralized aggregation of user data and feedback, failing to address the privacy concerns inherent in centralized systems. Additionally, methods like Group Robust Preference Optimization (GRPO) [28] aim to reduce performance disparities across user groups but similarly rely on aggregated, centralized datasets and do not provide mechanisms for personalization at the individual level. A recent effort by Li et al. [17] introduces a framework for personalized language modeling from personalized human feedback, which learns user-specific embeddings to capture individual preferences. However, their approach still relies on centralized data collection and does not address privacy concerns. Moreover, these prior approaches do not simultaneously tackle both privacy preservation and personalization challenges in real-world scenarios where user data is distributed across multiple devices or organizations.\nFederated Reinforcement Learning (FedRL) has garnered significant attention in recent years, aiming to leverage the principles of federated learning [21] across diverse RL clients to enhance their sample efficiency without sharing raw data or trajectories of the sequential decision-making process. This approach has shown promise in diverse applications, from optimizing autonomous vehicles and enhancing edge caching in IoT networks to smart management of building facibilities [5, 10, 18, 20, 38, 44, etc.]. Recent theoretical advancements have solidified the foundations of FedRL. Notably, convergence guarantees and sample complexity bounds have been established, demonstrating speedup with increasing numbers of participating agents [8]. In addition, the application of Markovian sampling techniques has been shown to achieve linear convergence speedup in FedRL settings [14]. Furthermore, recent analysis of decentralized FedRL with Byzantine fault tolerance has proven fast convergence rates without relying on a central server, marking a significant step towards fully distributed and resilient RL systems [12, 26]. Recent works have explored the benefits of heterogeneity among Q-learning agents [9]. Woo et al. [41] prove that leveraging agent heterogeneity can lead to linear speedup and significant efficiency gains in federated Q-learning settings. In addition, Woo et al. [42] introduce a federated offline RL method that achieves linear speedup with low communication costs in heterogeneous client environments. Moreover, Wang et al. [37] leverages momentum mechanisms to achieve exact convergence and state-of-the-art sample efficiency in highly heterogeneous environments.\nNovelty of Our Approach. Despite recent advancements, existing FedRL methods primarily focus on shared reward structures and fail to incorporate the human element into the learning process. FedRLHF bridges this gap by incorporating human feedback into the federated reinforcement learning framework, allowing for a human-centric approach. Unlike standard FedRL, which relies on a unified reward function, FedRLHF allows each client to shape its reward function with client-specific human feedback, thereby enhancing personalization. This decentralized approach ensures that sensitive user feedback remains on-device, avoiding the need for centralization typical of conventional RLHF settings. Furthermore, FedRLHF extends current theoretical frameworks by providing convergence guarantees and sample complexity bounds that explicitly consider the variability introduced by human feedback-an aspect not typically present in existing FedRL literature."}, {"title": "3 PROBLEM FORMULATION", "content": "We consider a federated reinforcement learning system with K clients, where each client $k \\in 1, 2, ..., K$ interacts with its own environment, modeled as a Markov Decision Process (MDP) $M_k = (S, A, P_k, R_k, p_0(s), \\gamma)$. Here, $S$ and $A$ represent the state and action spaces, respectively. The state-transition function $P_k: S \\times A \\times S \\rightarrow [0, 1]$ defines the probability $P_k(s' | s, a)$ of transitioning from state $s$ to state $s'$ after taking action $a$. The reward function $R_k: S \\times A \\rightarrow \\mathbb{R}$ specifies the expected reward $R_k(s, a)$ for client $k$ when action $a$ is taken in state $s$. Finally, $\\gamma \\in [0, 1)$ is the discount factor balancing immediate and future rewards. The initial state distribution $p_0(s)$ specifies the probability of the MDP starting in state $s$. We assume that both $\\gamma$ and $p_0$ are fixed and known for all clients. This uniformity simplifies the theoretical analysis and ensures consistency in policy evaluation and optimization. However, extensions to client-specific discount factors and initial state distributions are straightforward within our framework.\nEach client's MDP may vary in the transition dynamics $P_k$ and reward functions $R_k$, reflecting the heterogeneity among clients due to personalized environments or preferences. Let $\\pi_\\theta: S \\rightarrow \\Delta(A)$ denote a stochastic policy parameterized by $\\theta \\in \\mathbb{R}^d$, where $\\Delta(A)$ is the set of probability distributions over the action space $A$. The policy $\\pi_\\theta(a | s)$ specifies the probability of taking action $a$ in state $s$ under the parameters $\\theta$. For each client $k$, the objective is to find the policy parameters $\\theta$ that maximize the expected cumulative discounted reward:\n$J_k(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\Big[ \\sum_{t=0}^{\\infty} \\gamma^t R_k(s_t, a_t) \\Big]$\nwhere the expectation is taken over trajectories $\\tau = (s_0, a_0, s_1, ...)$ generated by following policy $\\pi_\\theta$ in $M_k$, starting from $p_0(s)$."}, {"title": "3.1 Incorporating Human Feedback", "content": "Unlike conventional RLHF where a single reward function and policy are learned from aggregated data, FedRLHF allows for client-specific reward functions $R_k$ and locally adapted policies $\\pi_k$. In FedRLHF, human feedback is integrated locally to shape each client's reward function. Specifically, the reward function for client $k$ is augmented as:\n$R_k(s, a) = \\tilde{R}(s, a) + \\lambda H_k(s, a)$ \nwhere $\\tilde{R}(s, a)$ is the intrinsic reward provided by the environment, $H_k(s, a)$ is the client-specific human feedback function representing additional reward or penalty based on human evaluation, and $\\lambda > 0$ is a scaling factor balancing the influence of human feedback relative to the intrinsic reward."}, {"title": "3.2 Global Objective", "content": "The global objective is to find policy parameters $\\theta$ that maximize the average expected cumulative reward across all clients:\n$J(\\theta) = \\frac{1}{K} \\sum_{k=1}^{K} J_k(\\theta).$"}, {"title": "3.3 The FedRLHF Framework", "content": "We propose the Federated Reinforcement Learning with Human Feedback (FedRLHF) framework to optimize the global objective (3) across all clients in a federated manner while respecting their individual environments and preferences. Algorithm 1 presents the pseudocode for FedRLHF. The key components and operations of the framework are as follows:\nLocal RLHF (lines 6-12).: The FedRLHF framework allows clients to use different RL methods, including Q-learning [39] and policy gradient (PG) [40], to perform \u03c4 steps of local optimization. The theoretical analysis provided in this manuscript assumes PG methods for local updates, which was necessary to facilitate the convergence analysis and derive sample complexity bounds.\nReward Shaping (line 9). : Rewards are shaped as $R_k = \\tilde{R} + \\lambda H_k$, where $\\tilde{R}$ is the intrinsic reward and \u03bb controls the influence of human feedback $H_k$.\nPrivacy Preservation. : Only the model updates $\\Delta \\theta_{t+1}$ are transmitted to the server, preventing direct access to individual user data and feedback, thus providing a significant level of privacy protection. For applications requiring formal privacy guarantees or protection against advanced inference attacks, additional measures such as Differential Privacy (DP) [6] could be incorporated into the framework.\nAggregation (line 13). Multiple server aggregation methods exist, such as FedAvg (simple averaging) [21], Weighted Average based on client data sizes, and robust aggregation techniques like median-based methods [43]. The choice depends on specific requirements such as fairness, robustness to outliers, or heterogeneity in client data. In this algorithm and our theoretical analysis, we employ FedAvg for its simplicity and to facilitate clearer theoretical results. However, our framework is flexible and can accommodate other aggregation methods if needed."}, {"title": "4 CONVERGENCE RESULTS", "content": "In this section, we provide theoretical guarantees on the convergence and sample complexity of the FedRLHF framework. These results apply to implementations of the framework that adhere to the core principles and structure outlined in Algorithm 1, under the necessary assumptions stated below."}, {"title": "4.1 Assumptions", "content": "ASSUMPTION 1 (L-SMOOTH GRADIENTS). For all $\\theta, \\theta' \\in \\mathbb{R}^d$ and $k \\in [K]$, the gradients of the clients' objective functions are L-Lipschitz continuous:\n$||\\nabla J_k(\\theta) - \\nabla J_k(\\theta') || \\leq L||\\theta - \\theta' ||$.\nASSUMPTION 2 (G-BOUNDED GRADIENTS). For all $\\theta \\in \\mathbb{R}^d$ and $k \\in [K]$, the gradients are bounded:\n$||\\nabla J_k(\\theta)|| \\leq G$.\nASSUMPTION 3 (\u03c3-BOUNDED VARIANCE). For all $\\theta \\in \\mathbb{R}^d$ and $k \\in [K]$, the variance of the stochastic gradient estimator is bounded:\n$\\mathbb{E} [||\\nabla \\hat{J}_k(\\theta) - \\nabla J_k(\\theta)||^2] \\leq \\sigma^2$,\nwhere $\\nabla \\hat{J}_k(\\theta)$ is the stochastic gradient computed from a mini-batch.\nASSUMPTION 4 (BOUNDED SECOND MOMENT). For all $\\theta \\in \\mathbb{R}^d$ and $k \\in [K]$, the second moment of the stochastic gradient is bounded:\n$\\mathbb{E} [||\\nabla \\hat{J}_k(\\theta)||^2] \\leq M^2$.\nASSUMPTION 5 (POLYAK-\u0141OJASIEWICZ (PL) CONDITION). The global objective function satisfies the PL condition:\n$2\\mu (J(\\theta^*) - J(\\theta)) \\leq ||\\nabla J(\\theta)||^2, \\forall \\theta \\in \\mathbb{R}^d$,\nwhere $\\mu > 0$ is a constant and $\\theta^* = \\arg \\max_{\\theta} J(\\theta)$.\nREMARK. Assumptions 1-4 are common in the stochastic optimization literature. The PL condition (Assumption 5) is stronger, especially for reinforcement learning's typically non-convex objectives. However, it approximates scenarios where objective functions exhibit properties conducive to linear convergence. Policy gradient methods with trust region constraints, such as TRPO [30], or those using proximal objectives, like PPO [31], often result in smoother updates to the policy parameters, making the PL condition more reasonable. Recent works [2, 13, 45, etc.] have used the PL condition for non-convex convergence guarantees for RL, further justifying its use in our analysis.\nASSUMPTION 6 (BOUNDED HUMAN FEEDBACK). For all $s \\in S, a \\in A$, and $k \\in [K]$, the human feedback is bounded:\n$|H_k(s, a)| \\leq H_{max}$.\nREMARK. Assumption 6 limits the variance introduced by human feedback in the learning process. In our experiments with the MovieLens task, we implement this by bounding feedback values and options (Section 6.1.2), similar to practical systems like ChatGPT that curate feedback for consistency."}, {"title": "4.2 Convergence and Sample Complexity", "content": "We now present the main theoretical results, starting with key lemmas leading up to the convergence theorem.\nLEMMA 4.1 (BOUNDED LOCAL-GLOBAL DIFFERENCE). Under Assumptions 1, 2, and 3, for any communication round t and client k, we have:\n$\\mathbb{E} [||\\theta_k^{t, \\tau} - \\theta_t||^2] \\leq \\eta^2 \\tau^2 (G^2 + \\sigma^2)$\nwhere $\\theta_k^{t, \\tau}$ is the local model of client k, $\\theta_t$ is the global model, \u03b7 is the learning rate, \u03c4 is the number of local updates, G is the gradient bound, and o\u00b2 is the variance bound.\nREMARK. Lemma 4.1 quantifies the extent to which local models diverge from the global model after \u03c4 local updates. This deviation is influenced by the learning rate \u03b7 and the number of local updates \u03c4, both of which amplify the divergence when increased.\nLEMMA 4.2 (ONE-STEP DESCENT). Under Assumptions 1-6, for any round t, the expected improvement in the global objective satisfies:\n$\\mathbb{E}[J(\\theta_{t+1})] \\geq J(\\theta_t) + \\eta \\tau \\Big(1 - \\frac{L \\eta \\tau}{2} \\Big) ||\\nabla J(\\theta_t)||^2 - \\frac{L (\\eta \\tau)^3}{2K} (G^2 + \\sigma^2) - H_{max}$\nwhere $\\theta_{t+1}$ is the updated global model.\nREMARK. This lemma establishes that each communication round in FedRLHF yields a quantifiable improvement in the global objective $J(\\theta)$. The positive term $\\eta \\tau \\Big(1 - \\frac{L \\eta \\tau}{2} \\Big) ||\\nabla J(\\theta_t)||^2$ signifies progress towards maximizing the objective, while the negative terms $\\frac{L (\\eta \\tau)^3}{2K} (G^2 + \\sigma^2)$ and $H_{max}$ account for the inherent variance in stochastic gradients and the bounded impact of human feedback, respectively.\nTHEOREM 4.1 (CONVERGENCE OF FEDRLHF). Under Assumptions 1-6, if we choose the constant learning rate $\\eta = \\frac{1}{L \\tau}$, then the output"}, {"title": "5 PERSONALIZATION-PERFORMANCE TRADE-OFF ANALYSIS", "content": "In this section, we establish a formal relationship between personalization and global performance within the FedRLHF framework. The convergence analysis in Section 4 already hints at the personalization-performance trade-off, particularly through the influence of the human feedback weight \u03bb on the convergence. Here, we provide a quantitative measure of this trade-off by analyzing how personalization affects global performance of intrinsic rewards and sample complexity. The complete proof for theorems presented in this section is provided in Appendix C."}, {"title": "5.1 Definitions and Preliminaries", "content": "DEFINITION 5.1 (MAXIMUM REWARD). We define $R_{max}$ as the maximum absolute value of the intrinsic reward function across all clients and state-action pairs:\n$R_{max} = \\max_{k \\in \\{1,2,...,K\\}, s \\in S, a \\in A} |\\tilde{R}(s, a)|$.\nDEFINITION 5.2 (PERSONALIZATION SCORE). For a client k with policy $\\pi_k(\\cdot|s, \\theta)$ and the global policy $\\pi(\\cdot|s, \\theta)$, the personalization score is defined as:\n$P_k(\\theta) = \\mathbb{E}_{s \\sim p} [D_{KL} (\\pi_k(\\cdot|s, \\theta) || \\pi(\\cdot|s, \\theta))]$,"}, {"title": "5.2 Personalization-Performance Trade-off", "content": "We now present our main theorem on the trade-off between personalization and global performance.\nTHEOREM 5.1 (PERSONALIZATION-PERFORMANCE TRADE-OFF). Under Assumptions 1\u20136 and Definition 5.1-5.3, for any set of client policies $\\{\\pi_k(\\cdot|s, \\theta)\\}_{k=1}^{K}$ and the global policy $\\pi(\\cdot|s, \\theta)$, the global performance metric satisfies:\n$J_g (\\theta) \\geq \\frac{1}{K} \\sum_{k=1}^{K} J_k(\\pi_k) - C \\cdot \\Big[ \\frac{1}{K} \\sum_{k=1}^{K} \\sqrt{P_k(\\theta)} \\Big]^2$,\nwhere C > 0 is a constant given by:\n$C = \\frac{2 \\sqrt{2} R_{total,max}}{(1 - \\gamma)^2}$\nand $R_{total,max} = R_{max} + H_{max}$ is the maximum possible total reward.\nTheorem 5.1 establishes that the global performance $J_g (\\theta)$ is lower bounded by the average client-specific performance (intrinsic rewards) $\\sum_{k=1}^{K}J(\\pi_k)$ minus a penalty term proportional to the average of the square roots of the personalization scores $\\Big[ \\frac{1}{K} \\sum_{k=1}^{K} \\sqrt{P_k(\\theta)} \\Big]^2$. The constant C encapsulates the maximum possible total reward and the discount factor, indicating that in environments with higher rewards or longer planning horizons, the impact of personalization on global performance is more pronounced."}, {"title": "5.3 Impact of Human Feedback", "content": "We analyze how the incorporation of human feedback, governed by the weight \u03bb, influences personalization and global performance.\nTHEOREM 5.2 (IMPACT OF HUMAN FEEDBACK). Under the same assumptions and definitions in Theorem 5.1, as the human feedback weight \u03bb increases:\n(1) The average personalization score $\\frac{1}{K} \\sum_{k=1}^{K} P_k(\\theta)$ increases at a rate of $O(\\lambda^2)$.\n(2) The global performance $J_g(\\theta)$ decreases at a rate of $O(\\lambda)$.\n(3) The sample complexity N increases at a rate of $O(\\lambda)$.\nTheorem 5.2 quantitatively demonstrates that increasing the human feedback weight \u03bb intensifies personalization (as the personalization score grows at $O(\\lambda^2)$) but leads to a linear decrease in global performance and an increase in sample complexity.\n(1) Personalization Score Increases at $O(\\lambda^2)$: The personalization score $P_k (\\theta)$ for each client scales quadratically with \u03bb, indicating that the degree of personalization becomes more pronounced as \u03bb increases.\n(2) Global Performance Decreases at $O(\\lambda)$: The global performance $J_g (\\theta)$ experiences a linear decrease with respect to \u03bb. This implies that while personalization enhances client-specific performance, it concurrently introduces a controlled degradation in overall system performance.\n(3) Sample Complexity Increases at $O(\\lambda)$: The total number of samples N required to achieve a desired level of performance grows linearly with \u03bb. This reflects the increased data demands associated with higher levels of personalization to maintain convergence guarantees."}, {"title": "6 EMPIRICAL RESULTS", "content": "We evaluate FedRLHF's effectiveness in integrating human feedback within a federated reinforcement learning setting through two real-world tasks: movie rating prediction using the MovieLens dataset and sentiment-controlled review generation using the IMDb dataset. Our experiments benchmark FedRLHF against a centralized RLHF baseline, with a focus on personalization, and maintaining performance levels.\nAll experiments were conducted on an NVIDIA GeForce RTX 3090 GPU, using the Flower framework [1] to simulate a realistic federated learning environment with gRPC communication, mimicking real-world distributed systems. Detailed experimental results and analyses are provided in Appendix D."}, {"title": "6.1 Movie Rating Prediction on MovieLens", "content": "6.1.1 Task Description and Setup. In this task, we simulate a streaming service enhancing its recommendation system while preserving user privacy and catering to individual preferences. Using the ml-latest-small version of the MovieLens dataset [11] which contains 100,836 ratings from 610 users on 9,742 movies, we randomly selected K = 10 users as clients, each with unique viewing histories and preferences. The objective is to predict whether a user would assign a high rating (4 stars or above) to a given movie, effectively framing this as a binary classification task.\n6.1.2 Human Feedback Simulation. To emulate realistic user behavior and feedback mechanisms, we developed a noise-aware, rule-based feedback simulator generating two types of feedback: a. Direct Feedback: Categorizes predictions as \"too high\" (-1), \"too low\" (1), or \"about right\" (0) based on the difference between predicted and actual ratings; b. Comparative Feedback: Expresses preferences between movie pairs, mirroring real-world scenarios where users more easily compare options than provide absolute ratings. Feedback values are bounded within [-1, 1], satisfying Assumption 6. This feedback trains a local reward (preference) model for each client. Full details on the feedback simulation are provided in Appendix D.1.2.\n6.1.3 Implementation. We implemented a neural network model with embedding layers for users and movies. The model inputs included user IDs, movie IDs, and movie genre information to capture complex user-movie interactions. In the federated learning process, each client trained the model locally using intrinsic rewards and simulated human feedback, employing Q-learning as the local RLHF step."}, {"title": "6.2 Sentiment-Controlled Review Generation", "content": "6.2.1 Task Description and Setup. In this task we simulate multiple movie review platforms collaborating to fine-tune a language model for sentiment-controlled text generation without sharing data. Each client represents a distinct platform with its own collection of movie reviews, introducing natural data heterogeneity. Using the IMDb dataset [19], we partitioned 50,000 reviews among K = 5 clients, each receiving approximately 10,000 unique reviews.\n6.2.2 Implementation (details in Appendix D.2.2). We employed a GPT-2 model [27] fine-tuned using PPO [31] within the TRL library [36]. Clients conducted local RLHF training for 5 epochs per federated round, using Adam optimizer (learning rate 1\u00d710-5). Global aggregation used FedAvg [21] over 5 communication rounds.\n6.2.3 Human Feedback Simulation. We simulated human feedback using a sentiment analysis model (DistilBERT [29] fine-tuned on IMDb) implemented locally on each client. The reward function combined sentiment score and language model log probability:\n$R_k = \\lambda_k \\cdot R_{sentiment} + (1 - \\lambda_k) \\cdot R$, where $R_{sentiment}$ is the sentiment alignment reward, R is the intrinsic fluency reward and $\u03bb_k \u03b5 [0, 1]$ is a client-specific parameter controlling personalization. This formulation closely aligns with the reward shaping in Equation 2 and validates Assumption 6, allowing clients to personalize the importance of sentiment alignment."}, {"title": "7 CONCLUSION AND FUTURE WORK", "content": "In this work, we have introduced FedRLHF, a novel framework that integrates federated reinforcement learning principles with RLHF to address privacy and personalization challenges of data centralization. Our theoretical analysis provides convergence guarantees and sample complexity bounds, demonstrating stable, linear convergence. The personalization-performance trade-off analysis shows how FedRLHF balances global performance with individual client needs. Empirical evaluations on MovieLens and IMDb validate our approach, achieving results comparable to centralized RLHF while preserving privacy and enhancing personalization.\nFuture work will focus on enhancing FedRLHF's robustness through advanced aggregation techniques and strengthening privacy preservation by integration of formal privacy guarantees, such as differential privacy. Additionally, we aim to investigate the trade-off between communication efficiency and personalization, optimizing FedRLHF's performance by balancing communication overhead with personalized model adaptations in federated environments."}]}