{"title": "Unbiased Evaluation of Large Language Models from a Causal Perspective", "authors": ["Meilin Chen", "Jian Tian", "Liang Ma", "Di Xie", "Weijie Chen", "Jiang Zhu"], "abstract": "Benchmark contamination has become a significant concern in the LLM evaluation community. Previous Agents-as-an-Evaluator address this issue by involving agents in the generation of questions. Despite their success, the biases in Agents-as-an-Evaluator methods remain largely unexplored. In this paper, we present a theoretical formulation of evaluation bias, providing valuable insights into designing unbiased evaluation protocols. Furthermore, we identify two type of bias in Agents-as-an-Evaluator through carefully designed probing tasks on a minimal Agents-as-an-Evaluator setup. To address these issues, we propose the Unbiased Evaluator, an evaluation protocol that delivers a more comprehensive, unbiased, and interpretable assessment of LLMs. Extensive experiments reveal significant room for improvement in current LLMs. Additionally, we demonstrate that the Unbiased Evaluator not only offers strong evidence of benchmark contamination but also provides interpretable evaluation results.", "sections": [{"title": "1. Introduction", "content": "Recently, proprietary models such as GPT-4 (Achiam et al., 2023), Claude(Anthropic, 2024), Gemini(Team et al., 2023), and open-source ones, such as Llama(Touvron et al., 2023), Mistral(Jiang et al., 2023), Qwen(Bai et al., 2023), Yi(AI et al., 2024) have demonstrated remarkable capabilities in natural language processing tasks and beyond. As the capabilities of community models continue to grow, the importance of robust and fair model evaluation becomes increasingly critical. The community has made great efforts to evaluate model performance by expanding the comprehensiveness of benchmarks(Wang, 2018; Wang et al., 2019; Srivastava et al., 2022; Hendrycks et al., 2021; Liang et al., 2022; White et al., 2024), or by introducing more complex and challenging tasks to push the boundaries of model capabilities(Wei et al., 2024; He et al., 2024b; Lightman et al., 2023; \u0391\u0399-\u039c\u039f, 2024; 2023; He et al., 2024a).\nDespite their success, public benchmarks, most widely used to assess and compare model performance, are particularly vulnerable to contamination issues(Lovin, 2023; Bender et al., 2021; Koco\u0144 et al., 2023; Li, 2023; Zhou et al., 2023; Ni et al., 2024), which are increasingly inevitable due to the scale of training data used in modern models. Recently, agent-based evaluation have been proposed to address contamination issue(Zhu et al., 2024; Liu et al.). Among them, MPA (Zhu et al., 2024) proposes to involve probing and judging agents to automatically transform existing problems in benchmarks into new ones. CogMath(Liu et al.) decouple questions into several evaluation dimensions via an multi-agent system for evaluating LLM's mathematical abilities.\nWe termed this paradigm as Agents-as-an-Evaluator. Different from previous LLM-as-a-Judge (Zheng et al., 2023), which operates on the evaluation side by solely determining whether something falls within the scope of a given rule, Agents-as-an-Evaluator extends beyond assessment to the generation side, where LLMs actively contribute to the design of the very questions involved in the task. Considering that prior works have revealed that LLM-as-a-Judge posses certain biases(Blodgett et al., 2020; Ahn et al., 2022;"}, {"title": "2. Related Works", "content": "2.1. LLMs Evaluation\nThe rapid growth of large language models (LLMs) underscores the need for increasingly robust and fair evaluation methods. Benchmarks offer an effective alternative for model evaluation. The research community has made significant strides in expanding the comprehensiveness of benchmarks (Wang, 2018; Wang et al., 2019; Srivastava et al., 2022; Hendrycks et al., 2021; Liang et al., 2022; White et al., 2024), while also introducing more complex"}, {"title": "2.2. Benchmark Contamination", "content": "Recent research has attached great importance to contamination in LLMs. In particular, (Lee et al., 2021; Sainz et al., 2023; McIntosh et al., 2024; Riddell et al., 2024; Jiang et al., 2024) knowledged that contamination poses significant challenges to the reliability and validity of LLM evaluations. Several research studies (Ni et al., 2024) developed various methods to detect data contamination.\nSeveral works (Fan et al., 2023; Lei et al., 2023; Zhu et al., 2023; 2024; Liu et al.) have been proposed to address the contamination issue. Among these, protocols like (Fan et al., 2023; Zhu et al., 2023; Liu et al.) are specifically designed for mathematical tasks, while (Lei et al., 2023) focuses on long-context evaluation. Among the research of Agents-as-an-Evaluator(Zhu et al., 2024; Liu et al.), MPA (Zhu et al., 2024) proposes to involve paraphrasing and judging agents to automatically transform existing problems in benchmarks into new ones. CogMath(Liu et al.) decouple questions into several evaluation dimensions via an multi-agent system for evaluating LLM's mathematical abilities.\nOur proposed Unbiased Evaluator stands in contrast to these, as it is designed to be generalized for a wide range of tasks and ensures an unbiased evaluation."}, {"title": "2.3. Evaluation Bias", "content": "Recent studies have highlighted that LLM-as-a-Judge exhibit various types of biases across various tasks(Dai et al., 2024; Gallegos et al., 2024; Chen et al.; Ye et al., 2024), such as position bias, length bias, self-enhancement bias etc. These internal biases of LLMs may also affect LLM-as-a-judge, leading to unfair evaluation outcomes and subsequently impacting the development of LLMs."}, {"title": "3. Bias Analysis", "content": "3.1. Theoretical Analysis\nFormulation of Evaluation Bias. Consider in an LLM evaluation, where the true capability of a model is parameterized by $\\phi$, which is a fixed but unknown quantity we aim to estimate. Given evaluation data $X = \\{x_1, x_2,..., x_n\\}$, the estimator $\\hat{\\phi} = E(X)$ is a function of the data and provides an approximation of $\\phi$. Then, the difference between expectation of $\\hat{\\phi}$ and $\\phi$ can be defined as evaluation bias:\n$\\epsilon(\\hat{\\phi}) = E[\\hat{\\phi}] - \\phi$ (1)\nwhere $E[\\hat{\\phi}]$ is the expected value of the estimator over $X$. A bias greater than 0 indicates that the estimator overestimates $\\phi$, often due to contaminated data. A bias of 0 means the estimator is unbiased, providing a perfect estimate. A bias less than 0 suggests the estimator underestimates $\\phi$, which can occur when a model is evaluated on a limited dataset that fails to capture the task's complexity.\nProposition 3.1. (Proof in Appendix C.1) Given a new designed evaluation protocol, which transforms original benchmark $D$ into $D'$. Then, the strength of bias with the new evaluation protocol can be decomposed into three terms: original, related and independent term.\n$E[\\epsilon(\\hat{\\phi}_{D'})^2] = E[\\epsilon(\\hat{\\phi}_D)^2] + 2Cov(\\epsilon(\\hat{\\phi}_D), \\Delta) + 2E[\\epsilon(\\hat{\\phi}_D)]E[\\Delta] + E[\\Delta^2]$\nwhere, $\\Delta$ is the delta bias which arises from the introduction of new evaluation protocol.\n*   $E[\\epsilon(\\hat{\\phi}_D)^2]$ is an original term that is associated with the original bias existing in the original benchmark $D$."}, {"title": "3.2. Bias in Agents-as-an-Evaluator", "content": "In addition to our theoretical analysis, we conduct extensive experiments in this section to demonstrate that Agents-as-an-Evaluator exhibit two types of bias: data bias and model bias.\n*   Data bias arises from an imbalance in accuracy across different domains during generation. For example, in tasks involving diverse domains, such as various subjects in MMLU, LLMs tend to excel in domains where they already perform well while struggling significantly in domains where their performance is weaker.\n*   Model bias originates from inherent unfairness during evaluation. an LLM tends to generate content that aligns more with its implicit strengths, giving it an unfair advantage.\nDiscussion. Previous works(Chen et al.; Ye et al., 2024) have revealed the self-enhanced bias in LLM-as-a-Judge, i.e. LLM judges may favor the answers generated by themselves. Model bias, however, focus more on the bias of generation side in Agents-as-an-Evaluator, which still remain unexplored."}, {"title": "3.3. Probing Task", "content": "In this section, we design a probing task to detect the potential bias in Agents-as-an-Evaluator.\nMinimum Agents-as-an-Evaluator. To perform bias analysis, following (Zhu et al., 2024), we design a minimal Agents-as-an-Evaluator framework. Specifically, given an original benchmark D, an LLM is tasked with rephrasing the questions in D without altering their meaning. Formally, this process generates a new benchmark, D' = LLM(D).\nTask Design. Given an original benchmark D on with m evaluation problems $\\{x_1, x_2,..., x_m \\}$, and n cutting edged LLMs $M = \\{\\theta_1, \\theta_2,..., \\theta_n \\}$, $D'_i$ denotes the rephrased benchmark using i-th LLM $\\theta_i$, rephrased j-th evaluation problem $x'_{ij}$ in $D'_i$ is rephrased from $d_j$ with probability p, i.e.\n$D'_i = \\{x'_{i1}, x'_{i2}, ..., x'_{im}\\}$\n$x'_{ij}$ = {$\\theta_i(x_j)$ if random \u2264 p; $x_j$ if random > p};$\\forall x'_{ij} \\in D'_i$ (3)\nNote that during probing task, the ground truth label of each problem remains unchanged. With $D'_i$, we perform LLM-as-a-Judge using all models in M. Specifically, we ask each model to assess its confidence that it thinks the ground truth label is still the right answer for the rephrased question, i.e.\n$S_{ijk} = \\theta_k(x'_{ij})$ $\\forall i \\in [1, n]; \\forall j \\in [1, m]; \\forall k \\in [1, \\eta]$ (4)\nwhere $S_{ijk}$ is the confidence score assessed by $\\theta_k$ for question $x'_{ij}$. The score $S_{ijk}$ lies within the range [1, 10], where a higher score indicates that the model has greater confidence in the ground truth label being the correct answer, while a lower score suggests stronger confidence that the ground truth label is not the correct answer. Detailed prompt is presented in Appendix A.\nMetric design. To quantify bias during LLMs evaluation, we draw inspiration from group consensus in human society, where collective opinions are often regarded as more reliable and fair compared to individual perspectives. Building on this idea, we propose three metrics: RCE(Consensus-Error Rate), Roc(Over-Confidence Rate) and Ruc(Under-Confidence Rate), incorporating both self and collective judgment.\n$RCE = \\frac{1}{m} \\sum_{j=1}^m [(10 \u2013 S_{iji}) * (10 \u2013 \\hat{S}_{ij})] * \\prod_{k=1}^n sgn(S_{ijk} < t_u)$ (5)\nwhere sgn is a sign function. $t_u$ is a upper threshold of \u201cNO\u201d. In this paper, following prompt in probing task, $t_u$ is set to 5. $\\hat{S}_{ij}$ is the averaged score among the other models: $\\hat{S}_{ij} = \\frac{1}{n-1} \\sum_{k=1 k!=i}(S_{ijk})$.\n$R_{OC} = \\frac{1}{m} \\sum_{j=1}^m [S_{iji} * (10 \u2013 \\hat{S}_{ij})] * sgn [S_{iji} > t_l] * \\prod_{k=1 k!=i} sgn(S_{ijk} < t_u)$ (6)\n$R_{UC} = \\frac{1}{m} \\sum_{j=1}^m [(10 \u2013 S_{iji}) * \\hat{S}_{ij}] * sgn [S_{iji} < t_u] * \\prod_{k=1 k!=i} sgn(S_{ijk} > t_l)$ (7)\nData Bias. We treat each subject in MMLU as a domain and calculate the Pearson and Kendall correlation between domain accuracy and $R_{CE}$ across four LLMs. The results, presented in Table 1, reveal a relatively strong negative correlation with small p-values. This indicates that LLMs tend to perform better in domains with lower $R_{CE}$ values while facing greater challenges in domains where their performance is weaker.\nModel Bias. In Fig. 2, we visualize how $R_{OC}$ and $R_{UC}$ evolve as the strength parameter p in Eq. (4) increases. We observe a significant increase in $R_{OC}$ with growing strength, particularly for models like Llama3.1-70b-Instruct and Yi-34B-Chat. In contrast, $R_{UC}$ remains relatively stable. This suggests that transitioning from the original evaluation protocol (p = 0) to the Agents-as-Evaluator (p = 1) causes LLMs to generate content that aligns closer with their implicit strengths ($R_{OC}$), rather than diverging from them ($R_{UC}$), ultimately providing themselves with an unfair advantage."}, {"title": "4. Unbiased Evaluator", "content": "4.1. General formulation\nConsider evaluating an LLM $\\theta$, where the model is tasked with generating answers for a set of evaluation problems $X = \\{X_1, X_2,..., X_n\\}$. The ground truth labels $y = \\{Y_1, Y_2,..., Y_n\\}$ are then used to calculate performance metrics. For each problem $x_i \\in X$ and its corresponding label $Y_i \\in y$, the evaluation process can be framed as a causal analysis, where the input X and the output Y represent the cause and effect, respectively.\nThe evaluation process for a question x can generally be represented as a causal diagram, specifically a causal directed acyclic graph (DAG) $G = (E, V)$. In this representation, the nodes or vertices $V = \\{V_{input}, V_{latent}, R\\}$ correspond to the observed input variables $V_{input}$, unobserved latent variables $V_{latent}$, and the ground truth label R. The edges E capture the direct causal relationships between these variables.\nSpecifically, $V_{input} = \\{A, B, C, D\\}$ represents the input variables, which are composed of several atomic elements derived from the input samples. In widely used multiple-choice questions, atomic variables may include elements such as the instruction, answer labels, and other context-specific components. For instance, consider the question:"}, {"title": "4.2. Bags of Atomic Interventions", "content": "Based on the general formulation described above, we provide the details of our designed Bags of Atomic Interventions in Table 2. Specifically, we focus on two widely used tasks: Multiple Choice Questions (MCQ) and Mathematics. For Multiple Choice Questions, we have designed six atomic interventions targeting different intervention positions within the task.\n*   Distractor Hint. A hint is introduced in the form of an additional option indicating that no correct answer exists for the question.\n*   Distractor Question. To ensure the model thoroughly understands the question, a distractor question is introduced, randomly selected from the same dataset.\n*   Answer Removal. Building upon the first strategy, some answers (including correct or not) is removed and replaced with an unrelated option from other questions.\n*   Option Shuffling. The order of the options is randomly shuffled to investigate whether the model's performance is influenced by their positional arrangement.\n*   Label Replacement. The conventional option labels (A, B, C, D) are replaced with numerical labels (1, 2, 3, 4 or I, II, III, IV) to explore whether the labeling format affects the model's output tendencies.\n*   Binary Transformation. The stem of a multiple-choice question is combined with each of the four options to create four true/false questions.\nFor mathematics, we design two interventions to transform questions into True/False question and MCQ, and then combined with aforementioned MCQ interventions.\n*   Question Jitter. Slightly altering the numbers in a question allows an open-ended math problem to be reframed as a True/False question.\n*   Answer Jitter. To transform an open-ended math problem into a multiple-choice question, the numerical answer can be adjusted with minor variations."}, {"title": "5. Experiments", "content": "5.1. Experimental Setup\nEvaluated Datasets and LLMs. Following (Zhu et al., 2024), we evaluate on two widely used benchmarks for multiple-choice questions: ARC-Challenge (ARC-C) (Clark et al., 2018) and MMLU (Hendrycks et al., 2021). For mathematical problem-solving, we utilize the GSM8K dataset"}, {"title": "5.2. Main Results", "content": "Table 3 presents the evaluation results of different LLMs on the original protocol and our Unbiased Evaluator, showing all LLMs experienced performance degradation on the Unbiased Evaluator. Specifically, GPT-4-Turbo and Gemini 2.0 demonstrated the smallest relative performance drop, maintaining their position as the strongest models. In contrast, GPT-40 performed similarly to GPT-4-Turbo on the original protocol but exhibited a larger average performance decline, particularly on MMLU and GSM8K. We hypothesize that the omni design of GPT-40 may hinder its performance on NLP tasks. Compared to proprietary models, open-source ones showed a more significant average drop, with Yi1.5-34B-Chat standing out as particularly affected, suggesting a substantial potential for improvement."}, {"title": "5.3. Bias Analysis", "content": "Following Sec. 3.3, we conduct an analysis of data and model bias for our proposed method, with the results presented in Table 1 and Fig. 2. Regarding data bias, our method exhibits a smaller coefficient and a large p-value, indicating reduced data bias. In terms of model bias, compared to Agents-as-Evaluator, our Unbiased Evaluator remains relatively stable in both $R_{OC}$ and $R_{UC}$ as the strength increases, suggesting lower model bias."}, {"title": "5.4. Confusion Matrix Analysis", "content": "We present a comprehensive confusion matrix in Fig. 4, which evaluates performance across four dimensions: \"True/False\" labels from the original protocol and our Unbiased Evaluator. For instance, the category labeled \"Original True\" and \"Unbiased Evaluator False\" highlights scenarios where the model performs correctly under the original protocol but fails when assessed with the Unbiased Evaluator. A striking observation across all models is the high frequency of \"Original True\u201d and \u201cUnbiased Evaluator False\" instances. This trend suggests issues with data contamination and inherent capability limitations within the models. Notably, the \"Original True\" and \"Unbiased Evaluator False\" of Yi1.5-34B-Chat are significantly higher compared to other larger models. This indicates that it may be more susceptible to data contamination and capability constraints."}, {"title": "5.5. Effect of Interventions & Evaluation Interpretability", "content": "Figure 6 illustrates the accuracy of Qwen2.5-7B-Instruct when each single atomic interventions are applied. The results show that the model's performance varies significantly depending on the type of intervention. Notably, compared to the original protocol, the Binary Transformation intervention poses the greatest challenge, leading to a substantial decline in accuracy. This suggests that the model may not fully comprehend every option in a MCQ but instead might rely on certain heuristic shortcuts to arrive at an answer. Moreover, even with relatively simple interventions, such as Option Shuffling and Label Replacement, the model's performance exhibits a slight degradation. This provides strong evidence of potential data contamination."}, {"title": "5.6. Effect of the Scaling", "content": "As shown in Figure 5, we evaluate the Qwen1.5 and Llama2 series models with Unbiased Evaluator on MMLU and ARC-C. The results indicate that as the model parameters increase, performance gradually improves. Notably, under the original evaluation protocol, the larger Qwen1.5 models achieved over 90% performance, nearing saturation and limiting the ability to evaluate larger models, such as 32/72/110B. In contrast, Unbiased Evaluator demonstrate a consistent performance improvement from 32B to 110B."}, {"title": "5.7. Human Verification", "content": "To ensure the correctness of proposed method, we randomly selected 300 samples from the MMLU and 100 samples from the ARC-C and GSM8K, 500 questions in total. 9 human experts (with bachelor or higher degree) are divided into 3 groups, each with 3 person. They were asked to judge the following question: Whether the answers to the intervened questions are correct. As shown in Table E, the human verification demonstrates an overall accuracy rate of 99.3%, 99.9% and 99.7% for ARC-C, MMLU and GSM8K, respectively, indicating the effectiveness of our methodology."}, {"title": "6. Conclusions", "content": "This paper present a theoretical analysis of evaluation bias, offering valuable findings for designing protocols. Moreover, two types of bias in Agents-as-an-Evaluator are identified with probing task. To mitigate the bias, guided with previous findings, an new evaluation protocol, Unbiased Evaluator, is proposed to offer unbiased and interpretable assessment for benchmark contamination. We look forward that our method may bring inspirations for future design for"}, {"title": "7. Impact Statement", "content": "Bias in the evaluation of LLMs is a crucial issue for ensuring responsible Al development in society. This work conduct a detailed bias analysis on previous protocol, followed by a novel evaluation protocol designed to fairly measure the true capabilities of LLMs. By providing a more precise assessment framework, this protocol aims to enhance our understanding of these models, ultimately contributing to more transparent, fair, and reliable AI systems."}, {"title": "A. Prompt in Probing Task", "content": "I have a question with a possible answer.\n### Question:\n{question}\n### Possible Answer:\n{answer}\nYou need to rate the given possible answer on a scale of 1 to 10 based on the confidence\nof its correctness for the question, using the following rating rules:\nScore 1 to 2: You are very confident that the given possible answer is completely\nincorrect for the question.\nScore 3 to 4: You are fairly confident that the given possible answer is incorrect, but\nthere is a small chance it could be partially correct.\nScore 5 to 6: You are uncertain about the correctness of the given possible answer; it\ncould be right or wrong.\nScore 7 to 8: You are fairly confident that the given possible answer is correct, but\nthere is a small chance it could be partially incorrect.\nScore 9 to 10: You are very confident that the given possible answer is completely correct\nfor the question.\nFinally, you must rate the answer strictly on a scale of 1 to 10 with in the format of\n``<<< >>>'', for example, ``<<<5>>>.''"}, {"title": "B. Implementation Details", "content": "B.1. Hyperparameter\nTo ensure a balance of interventions in BOAT, the probability for each intervention was set to 0.5, except for Binary\nTransformation, which was assigned a probability of 0.1. Under the 5-shot evaluation setting, we introduced the same\ncombinations of atomic interventions to the few-shot samples as those in the final question. An exception was made for\nquestions affected by Answer Removal: in such cases, we randomly selected half of the few-shot samples for intervention to\nprevent the model from repeating the output corresponding to option N.\nB.2. BOAT constraints\nAs shown in 4, there are constraints among different interventions, and not all interventions can be combined arbitrarily. For\nexample, the Answer Removal can only be introduced when the Distractor Hint intervention exists. Moreover, the binary\ntransformation intervention cannot be combined with other interventions.\nPlease note that the instructions will change depending on the interventions. The detailed instructions are presented in 5."}, {"title": "C. Proof of Proposition", "content": "C.1. Proof of Proposition 3.1\nGiven the definition in 3.1, the evaluation bias on benchmark D is given by:\n$\\epsilon(\\hat{\\phi}) = E[\\hat{\\phi}] - \\phi$ (8)\nSimilarly, the evaluation bias on rephased one D' from D is defined as $\\epsilon_{D'}$, then\n$\\epsilon(\\hat{\\phi}_{D'}) = E[\\hat{\\phi}_{D'}] -\\phi$\n$= E[\\hat{\\phi}_{D'}] \u2013 (E[\\hat{\\phi}] \u2013 $\\epsilon(\\hat{\\phi})$)\n$= (E[\\hat{\\phi}_{D'}] \u2013 E[\\hat{\\phi}]) + $\\epsilon(\\hat{\\phi})$\n$= \\Delta + $\\epsilon(\\hat{\\phi})$ (9)\nwhere $\\Delta$ is the delta bias which arises from the introduction of new evaluation protocol.\nConsider that $\\epsilon(\\hat{\\phi})$ could be positive, negative, and ideally, zero. Therefore, we seek to analyze the its mean squared term:\n$E[\\epsilon(\\hat{\\phi}_{D'})^2] = E[($\\Delta$ + $\\epsilon(\\hat{\\phi})$)$^2]$\n$= E[$\\Delta^2$] + $E[\\epsilon(\\hat{\\phi})^2] + 2E[$\\Delta$\\epsilon(\\hat{\\phi})]$ (10)\nBecause,\nCov($\\epsilon(\\hat{\\phi})$, $\\Delta$) = E[($\\epsilon(\\hat{\\phi}) - E[$\\epsilon(\\hat{\\phi})]$)($\\Delta - E[$\\Delta$]$)]\n= $E[$\\epsilon(\\hat{\\phi})\\Delta]$ \u2013 $E[$\\epsilon(\\hat{\\phi})]$$E[$\\Delta]$ \u2013 $\\Delta$$E[$\\epsilon(\\hat{\\phi})]$ + $E[$\\epsilon(\\hat{\\phi})]$$E[$\\Delta]$]\n= $E[$\\epsilon(\\hat{\\phi})\\Delta]$ \u2013 $E[$\\epsilon(\\hat{\\phi})]$$E[$\\Delta] (11)\nTherefore,\n$E[$\\epsilon(\\hat{\\phi})\\Delta]$ = Cov($\\epsilon(\\hat{\\phi})$, $\\Delta$) + $E[$\\epsilon(\\hat{\\phi})]$$E[$\\Delta]$ (12)\nSubstitute the expression into Equation (10),\n$E[$\\epsilon(\\hat{\\phi}_{D'})^2]$ = $E[$\\epsilon(\\hat{\\phi})^2]$ + 2Cov($\\epsilon(\\hat{\\phi})$, $\\Delta$) + 2$E[$\\epsilon(\\hat{\\phi})]$$E[$\\Delta]$ + $E[$\\Delta^2]$ (13)\nHere,\n*   $E[\\epsilon(\\hat{\\phi})^2]$ is an original term that is associated with the original bias existing in the original benchmark D.\n*   2Cov($\\epsilon(\\hat{\\phi})$, $\\Delta$) is a related term which pertains to biases that newly introduced biases are correlated with the pre-existing biases in the original benchmark.\n*   2$E[$\\epsilon(\\hat{\\phi})]$$E[$\\Delta]$ + $E[$\\Delta^2]$ is an independent term, stemming from biases inherent to the methodology itself."}, {"title": "D. Intervened Examples", "content": "Here is a multiple choice question. If there is no correct answer in the options, please\nreply with N.\nQuestion:\nThe end result in the process of photosynthesis is the production of sugar and oxygen.\nWhich step signals the beginning of photosynthesis?\nI. Chemical energy is absorbed through the roots.\nII. Light energy is converted to chemical energy.\nIII. Chlorophyll in the leaf captures light energy.\nIV. Sunlight is converted into chlorophyll.\nAnswer:\nIII\nExample #1\nHere are two questions and only one of them corresponds to the options. Please select the\ncorrect answer.\nQuestion:\nA group of engineers wanted to know how different building designs would respond during an\nearthquake. They made several models of buildings and tested each for its ability to\nwithstand earthquake conditions. Which will most likely result from testing different\nbuilding designs?\nThe voltage is held constant in an electric circuit. What will happen to the current in\nthis circuit if the resistance is doubled?\nA. buildings will be built faster\nB. buildings will be made safer\nC. building designs will look nicer\nD. building materials will be cheaper\nAnswer:\nB\nExample #2\nHere are two questions and only one of them corresponds to the options. Please select the\ncorrect answer. If there is no correct answer in the options, please reply with N.\nQuestion:\nWhich statement best explains why a tree branch floats on water?\nWhat happens to a wooden log when it is burned?\nI. Wood is light.\nII. parasitism\nIII. Wood is magnetic.\nIV. Wood is porous.\nAnswer:\nN\nExample #3\nThe following are true/false questions. If the answer is correct, please reply with T,\notherwise reply with F.\nQuestion:\nStatement 1: A polished metal ball looks very shiny and bright on a sunny day. What makes\nthe ball look shiny? The ball makes light.\nStatement 2: A polished metal ball looks very shiny and bright on a sunny day. What makes\nthe ball look shiny? The ball reflects light.\nStatement 3: A polished metal ball looks very shiny and bright on a sunny day. What makes\nthe ball look shiny? The ball absorbs light and then releases it.\nStatement 4: A polished metal ball looks very shiny and bright on a sunny day. What makes\nthe ball look shiny? The ball absorbs light and keeps it inside.\nAnswer:\nFTFF\nExample #4"}]}