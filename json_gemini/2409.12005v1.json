{"title": "Representing Positional Information in Generative World Models for Object Manipulation", "authors": ["Stefano Ferraro", "Pietro Mazzaglia", "Tim Verbelen", "Bart Dhoedt", "Sai Rajeswar"], "abstract": "Object manipulation capabilities are essential skills that set apart embodied agents engaging with the world, especially in the realm of robotics. The ability to predict outcomes of interactions with objects is paramount in this setting. While model-based control methods have started to be employed for tackling manipulation tasks, they have faced challenges in accurately manipulating objects. As we analyze the causes of this limitation, we identify the cause of underperformance in the way current world models represent crucial positional information, especially about the target's goal specification for object positioning tasks. We introduce a general approach that empowers world model-based agents to effectively solve object-positioning tasks. We propose two declinations of this approach for generative world models: position-conditioned (PCP) and latent-conditioned (LCP) policy learning. In particular, LCP employs object-centric latent representations that explicitly capture object positional information for goal specification. This naturally leads to the emergence of multimodal capabilities, enabling the specification of goals through spatial coordinates or a visual goal. Our methods are rigorously evaluated across several manipulation environments, showing favorable performance compared to current model-based control approaches.", "sections": [{"title": "Introduction", "content": "For robotic manipulators, replicating the tasks that humans perform is extremely challenging, due to the complex interactions between the agent and the environment. In recent years, deep reinforcement learning (RL) has emerged as a promising approach for addressing these challenging scenarios (Levine et al. 2016; OpenAI et al. 2019; Kalashnikov et al. 2018; Lu et al. 2021; Lee et al. 2021). Among RL algorithms, model-based approaches aim to provide greater data efficiency compared to their model-free counterparts (Fujimoto, van Hoof, and Meger 2018; Haarnoja et al. 2018). With the advent of world models (WM) (Ha and Schmidhuber 2018), model-based agents have demonstrated impressive performance across various domains (Hafner et al. 2020; Rajeswar et al. 2023; Hafner et al. 2023; Hansen, Wang, and Su 2022; Hansen, Su, and Wang 2024; Lancaster et al. 2024), including real-world robotic applications (Wu et al. 2022; Seo et al. 2023).\nWhen considering object manipulation tasks, it seems natural to consider an object-centric approach to world modeling. Object-centric world models, like FOCUS (Ferraro et al. 2023) learn a distinct dynamical latent representation per object. This contrasts with the popular Dreamer method (Hafner et al. 2023), where a single flat representation, referring to the whole scene is extracted.\nModel-based generative agents, like Dreamer and FOCUS, learn a latent model of the environment dynamics by reconstructing the agent's observations and use it to generate latent sequences for learning a behavior policy in imagination (Hafner et al. 2020, 2021, 2023). However, these kinds of agents have shown consistent issues in succeeding in object manipulation tasks, both from proprioceptive/vector inputs (Hansen, Su, and Wang 2024) and from images (Seo et al. 2022). For instance, in the Shelf Place task from Metaworld (Yu et al. 2019), DreamerV3's success rate after 2M steps oscillates around 25% when other approaches, such as the simpler SAC baseline (Haarnoja et al. 2018), quickly reach 100% success, as reported in (Hansen, Su, and Wang 2024).\nIn this work, we aim to study this failure mode of generative model-based agents and propose a set of solutions that alleviate these issues in existing world models. To be able to perform controlled experiments, in a simple but widely used setup, we focus on the visual object positioning problem. As shown in Figure 1, in this setting, at each timestep, the agent receives a visual observation, which shows what the current scene looks like, and a goal target, representing where an object should be moved. Notably, the target could be expressed either as a vector, containing the spatial coordinates of the target location or as an image, showing the object in the desired pose and location.\nAfter analyzing the causes of failure in this setup, we"}, {"title": "Preliminaries", "content": "The agent is a robotic manipulator that, at each discrete timestep t receives an input xt from the environment. The goal of the agent is to move an object in the environment from its current position $p_{obj}^t$ to a target goal position $p_{obj}^g$. In this work, we focus on observations composed of both visual and vector entities. Thus, $x_t = (o_t, v_t)$ is composed of the visual component of and of the vector vt. The latter is a concatenation of proprioceptive information of the robotic manipulator qt, the object's position $p_{obj}^t$, and the target position $p_{obj}^g$. The target position can also be expressed through a visual observation xg, from which the agent should infer the corresponding $p_{obj}^g$ to succeed in the positioning task."}, {"title": "Generative World Models", "content": "Generative world models learn a latent representation of the agent inputs using a variational auto-encoding framework (Kingma and Welling 2022). Dreamer-like agents (Hafner et al. 2021, 2023) implement the world model as a Recurrent State-Space Model (RSSM) (Hafner et al. 2019). The encoder f() is instantiated as the concatenation of the outputs of a CNN for high-dimensional observations and an MLP for low-dimensional proprioception. Through the encoder network, the input xt is mapped to an embedding et, which then is integrated with dynamical information with respect to the previous RSSM state and the action taken at, resulting in st features.\nEncoder: $e_t = f(x_t)$\nPosterior: $P(S_{t+1}|S_t, a_t, e_{t+1})$,\nPrior: $P(S_{t+1} |S_t, a_t)$,\nDecoder: $P_o (X_t|S_t)$.\nGenerally, the system either learns to predict the expected reward given the latent features (Hafner et al. 2020), using a reward predictor $p_o(r_t| s_t)$. Alternatively, some world-model based methods adopt specialized ways to compute rewards in imagination, as the goal-conditioned objectives in LEXA (Mendonca et al. 2021).\nRewards are computed on rollouts of latent states generated by the model and are used to learn the policy and value network v in imagination (Hafner et al. 2020, 2021, 2023).\nIn our experiments, we consider a world model with a discrete latent space (Hafner et al. 2021). We also implement advancements of the world model representation introduced in DreamerV3 (Hafner et al. 2023), such as the application of the symlog transform to the inputs, KL balancing, and free bits to improve the predictions of the vector inputs and the robustness of the model."}, {"title": "Object-centric World Models", "content": "Compared to Dreamer-like flat world models, the world model of FOCUS (Ferraro et al. 2023) introduces the following object-centric components:\nObject latent extractor: $s_{obj}^t = P_o(s_{obj} | S_t, c_{obj})$,\nObject decoder: $p_o(x_{obj}^t, m_{obj}^t| s_{obj})$.\nHere, $x_{obj}^t = (x_t, p_{obj}^t, p_{obj}^g)$ represents the object-centric inputs and it is composed of segmented RGB images $x_t^o$ of and object positions $p_{obj}^t$. The variable $c_{obj}^t$ indicates which object is being considered.\nThanks to the object latent extractor unit, object-specific information is separated into distinct latent representations $s_{obj}^t$. Two decoding units are present. The introduced object-centric decoder $p_o(x_{obj}^t, m_{obj}^t| s_{obj})$ reconstructs each object's related inputs $x_{obj}^t$ and segmentation mask $m_{obj}^t$. The original Dreamer-like decoder takes care of the reconstruction of the remaining vector inputs, i.e. proprioception qt and given goal targets $p_{obj}^g$.\nWe provide additional descriptions of the world model and policy learning losses, hyperparameters, and training details in the Appendix."}, {"title": "Object Positioning Tasks", "content": "In general terms, we consider positioning tasks the ones where an entity of interest has to be moved to a specific location. Two positioning scenarios are considered in this analysis: pose reaching and object positioning. Pose-reaching tasks can be seen as simplified positioning tasks where the entity of interest is part of the robotic manipulator itself. Pose-reaching tasks are interesting because these only require the agent to have knowledge of the proprioceptive information to infer their position in space and reach a given target. When interacting with objects instead, there is the additional necessity of knowing the position of the object entity in the environment. Then, the agent needs to be able to manipulate and move the entity to the provided target location.\nFor object positioning tasks, especially when considering a real-world setup, there is a significant advantage in relying mainly on visual inputs. It is convenient because it avoids the cost and difficulty associated with tracking additional state features, such as the geometrical shape of objects in the scene or the presence of obstacles. Some synthetic benchmarks additionally make use of \"virtual\u201d visual targets for positioning tasks (Tunyasuvunakool et al. 2020; Yu et al. 2019), which strongly facilitates the learning of these tasks, leveraging rendering in simulation. However, applying such \"virtual\" targets in real-world settings is not often feasible. Non-visual target locations can be provided as spatial coordinates. Alternatively, an image showing the target location could be used to specify the target's position.\nRewards and evaluation criteria. When applying RL algorithms to a problem, a heavily engineered reward function is generally necessary to guide the agent's learning toward the solution of the task (OpenAI et al. 2019). The object positioning setup allows us to consider a natural and intuitive reward definition that scales across different agents and environments. We define the reward as the negative distance between the position of the entity of interest and the goal target position:\n$r_t = -distance(object, target) = - ||p_{obj}^t \u2013 p_{obj}^g ||_2$.\nIn the spirit of maintaining a setup that is as close as possible to a real-world one, to retrieve positional information $p_{obj}^t$ of the objects we rely on image segmentation information, rather than using the readings provided from the simulator. For each entity of interest, the related position is extracted by computing the centroid of the segmentation mask and subsequently transformed according to the camera extrinsic and intrinsic matrices to obtain the absolute position with respect to the workspace.\nFor evaluation purposes, we use the goal-normalized score function:\n$normalized score = exp\\left(-\\frac{||p_{obj}^t \u2013 p_{obj}^g||_2}{||p_{obj}^g||_2}\\right)$\nAs detailed in the Appendix, the above function allows us to rescale performance between 0 and 1, where 1 = expert performance, a common evaluation strategy in RL (Cobbe et al. 2020; Fan 2023)."}, {"title": "Analysis of the Current Limitations", "content": "To provide insights into the limitations of current world model-based agents in object-positioning tasks, we consider the performance of Dreamer and FOCUS on a pose-reaching and an object-positioning task.\nFor pose-reaching, we opted for the Reacher environment from the DMC suite (Tunyasuvunakool et al. 2020). In this task, we consider the end-effector of the manipulator as the entity to be positioned at the target location. For the more complex object positioning task, we opted for a cube-manipulation task from Robosuite (Zhu et al. 2020). The given cube has to be placed at the specified target location to succeed in the task.\nIn both environments, the target position is uniformly sampled within the workspace at every new episode. We test the environments in two different scenarios: first, with a virtual visual target that is rendered in the environment, and second, without a visual target, where the target location is provided only as a vector in the agent's inputs.\nDreamer and FOCUS agents are trained for 250k gradient steps on both environments in an offline RL fashion. Datasets of 1M steps are collected adopting the exploration strategy presented in (Ferraro et al. 2023). Both the world model and the policy are updated at every training step. In Figure 3, we present an overview of the agents' success rate and of the capabilities at reconstructing positional information.\nWe make the following observations:\n\u2022 The agents' performance is comparable, with FOCUS being slightly more successful and more accurate in predicting the target and entity position. This is likely thanks to the object-centric nature of the approach;\n\u2022 There is a significant gap in performance between the tasks with the virtual visual targets rendered in the environment and the tasks using only spatial coordinates as"}, {"title": "Position Conditioned Policy (PCP)", "content": "The first declination of our proposed solutions is the conditioning of the policy directly on the positional coordinates of the desired target. By default, the world model encodes the target's positional information in the latent states, which are then fed to the policy for behavior learning. Instead, as shown in the bottom of Figure 2, we propose to concatenate the object positional coordinates $p_{obj}^g$ to the latent states st as an input to the policy network $\\Pi_{PCP}$."}, {"title": "Latent Conditioned Policy (LCP)", "content": "Conditioning the policy on explicit features has its limitations, particularly when extending features beyond positional ones, or when working with different goal specifications, e.g. visual ones. Therefore, expressing features implicitly could represent a more robust approach. To address this, we propose a latent conditioned method for behavior learning. This approach is analogous to the one adopted in LEXA (Mendonca et al. 2021) for goal-conditioned behavior learning. However, we tailor our strategy for object manipulation by designing an object-centric approach. We refer to our novel implementation as Latent-Conditioned Policy (LCP).\nIn LEXA, policy conditioning occurs on the entire (flat) latent state, using either cosine or temporal distance methods. However, in manipulation tasks involving small objects, the cosine approach is inadequate because it prioritizes matching the robot's position over visually smaller aspects of the scene, such as an object's position, rather than on bigger visual components of the scene, e.g. the robot pose. The temporal approach was introduced to mitigate this issue. However, this approach generally requires a larger amount of data to converge, as the training signal is less informative, being based only on the temporal distance from the goal (Mendonca et al. 2021).\nWe argue that object-centric latent representations offer greater flexibility to condition the policy, thanks to the disentangled latent information. With LCP, we can condition the policy solely on the object's latent states, enabling fine-grained target conditioning focused exclusively on the entity of interest.\nWe now present the novel components with respect to the FOCUS architecture presented earlier. The overall LCP method is depicted in Figure 6.\nLatent Positional Encoder. To obtain object latent features for a given target position, we introduce the Latent Positional Encoder model. This model enables inferring an object's latent state directly from the object's positional information, namely $p(\\hat{s}_{obj}|p_{obj}^g)$.\nDuring training, the latent positional encoder is trained to minimize the negative cosine distance between the predicted and the reference object latent state:\n$L_{pos} = - \\frac{\\hat{s}_{obj} . s_{obj}}{||\\hat{s}_{obj}|| ||s_{obj}||}$.\nCompared to the original loss function of FOCUS (defined in appendix), the world model loss becomes:\n$L_{OCWM} = L_{FOCUS} + L_{pos}$\nLatent-Conditioned Policy Learning. The introduction of the latent positional encoder enables the conditioning over the target object's latent. By encoding a desired target position $p_{obj}^g$, the target object's latent state $\\hat{s}_{obj}^g$ is inferred. The latter serves as the conditioning factor for the policy network $\\Pi_{LCP}$:\n$\\Pi_{LCP}(a_t| S_t, \\hat{s}_{obj}^g)$\nTo incentivize the policy to move the entity of interest to the target location, we maximize the negative latent distance between $s_{obj}^t$ and $\\hat{s}_{obj}^g$. The distance function used is cosine similarity. $r_{LCP}$ becomes then:\nr_{LCP} = \\frac{s_{obj}^t . \\hat{s}_{obj}^g}{||s_{obj}^t|| ||\\hat{s}_{obj}^g||}.\nThus, similarly to PCP, LCP requires no reward head for object positioning tasks. The latent dynamical consistency of the RSSM allows for the policy network to be trained purely on imagination.\nVisual targets. Additionally with respect to PCP, and similarly to LEXA, LCP enables conditioning the policy on visual targets. In this case, the agent does not use the latent position encoder. Instead, given a visual observation representing the goal target position for the object, the world model can infer the corresponding world model state, using the encoder and the posterior. Then given such a state, the object extractor allows extracting the target latent state $\\hat{s}_{obj}^g$, which is used in the reward computation."}, {"title": "Conclusion", "content": "We analyzed the challenges in solving visual robotic positional tasks using generative world model-based agents. We found these systems suffer from information bottleneck issues when considering positional information for task resolution. Under-represented information is challenging for the model to encode, especially in the case of not stationary values such as a variable target.\nThe approaches we presented overcome this issue by providing the policy network with more direct access to the target information. Two declinations are proposed. The first, the Positional Conditioning Policy (PCP), allows direct conditioning on the target spatial coordinates. We showed PCP improves performance for any class of world models, including Dreamer-like \u201dflat\u201d world models and FOCUS-like object-centric world models. The second declination, Latent Conditioning Policy (LCP), is an object-centric approach that we implement on top of FOCUS. This allows the conditioning of the policy on object-centric latent targets.\nAs future work, it would be interesting to analyze the application of such approaches to non-positional features, such as a shape (e.g. object deformation) or object configuration (e.g. open/close faucet, light on/off). Moreover, it would be interesting to scale the application of LCP to other modalities. For instance, one could consider multimodal inputs including both visual and tactile observations."}, {"title": "Supplementary Material", "content": "Scaling performance using expert performance is a common evaluation strategy in RL (Cobbe et al. 2020; Fan 2023). In our problem, we define the reward as the negative distance:\n$r_t = -r(p_{obj}^t, p_{obj}^g) = - ||p_{obj}^t - p_{obj}^g ||_2$.\nFor a given goal $p_{obj}^g$, $r_t \\in ] \u2013 inf, 0]$. In order to compare different tasks, where distances may have different magnitudes, we divide the rewards rt by the typical reward range. This is given by $r_{max} - r_{min}$, where $r_{min} = r(p_{obj}^g)$, with $p_{obj}^i$ being the initial position of the object (this is normally around the origin, and $r_{max} = r(p_{obj}^g) = 0$.\nThus, we obtain:\n$s_t = r_t/(r_{max} - r_{min})$\n$= r(p_{obj}^t)/(0-r(p_{obj}^g)) =$\n$=\\frac{-||p_{obj}^t - p_{obj}^g ||_2}{(0+ ||0 \u2013 p_{obj}^g ||_2)}$\n$=-\\frac{||p_{obj}^t - p_{obj}^g ||_2}{||p_{obj}^g ||^2}$\nFinally, we apply the exp operator, to make values positive and bring them in the [0, 1] range, where 1 is the expert score:\nnormalized score = exp\\left(-\\frac{||p_{obj}^t \u2013 p_{obj}^g ||_2}{||p_{obj}^g ||^2}\\right)$\nTraining of the FOCUS architecture is guided by the following loss function:\n$L_{FOCUS} = L_{dyn} + L_{state} + L_{obj}$.\n$L_{dyn}$ refers to the dynamic component of the RSSM, and equals too:\n$L_{dyn} = D_{KL}[P_{\\phi}(S_{t+1}|S_t, a_t, e_{t+1})||P_{\\phi}(S_{t+1}|S_t,a_t)]$.\n\n\nFinally, the decoder learns to reconstruct the rest of vector state information vt by minimization of the negative log-likelihood (NLL) loss:\n$L_{state} = - log p_{\\phi} (q_t, p_{obj}^g |S_t)$"}]}