{"title": "CAN TRANSFORMERS REASON LOGICALLY? A STUDY IN SAT SOLVING", "authors": ["Leyan Pan", "Vijay Ganesh", "Jacob Abernethy", "Chris Esposo", "Wenke Lee"], "abstract": "We theoretically and empirically study the logical reasoning capabilities of LLMs\nin the context of the Boolean satisfiability (SAT) problem. First, we construct a\ndecoder-only Transformer that can solve SAT using backtracking and deduction via\nChain-of-Thought (CoT). We prove its correctness by showing trace equivalence\nto the well-known DPLL SAT-solving algorithm. Second, to support the imple-\nmentation of this abstract construction, we design a compiler PARAT that takes as\ninput a procedural specification and outputs a transformer model implementing this\nspecification. Third, rather than programming a transformer to reason, we evaluate\nempirically whether it can be trained to do so by learning directly from algorithmic\ntraces (\"reasoning paths\") of the DPLL algorithm.", "sections": [{"title": "INTRODUCTION", "content": "Transformer-based Language Models (LLMs, Vaswani et al. (2017)) have demonstrated remarkable\nsuccess in a wide range of tasks framed in natural language, especially when using prompting\ntechniques such as Chain-of-Thought (CoT, Wei et al. (2022)). On the other hand, even the most\nadvanced LLMs face challenges in reliable multi-step reasoning, frequently hallucinating towards\nnonsensical conclusions (Kambhampati et al. (2024)). Evaluating progress on logical deduction in\nlanguage models remains an ongoing challenge as researchers have continued to disagree on even a\nreasonable definition of what constitutes \u201creasoning.\"\nThis paper focuses on the question of LLM reasoning capability in what we believe is the simplest\nand most mathematically precise setting: the Boolean satisfiability problem (SAT, Cook (1971)). SAT\nproblems provide an excellent starting point for studying the reasoning ability of LLMs given that (a)\nnatural language often encodes Boolean logic, and (b) we already have many useful algorithms that\nimplement logical deduction to solve SAT problems Biere et al. (2009). Notably, notwithstanding the\nNP-completeness of SAT, humans implicitly solve simple boolean satisfaction problems in their daily\nlives; scheduling a multi-person meeting across time zones, for example.\nIn this work we aim to rigorously investigate Transformers' multi-step reasoning and backtracking\ncapability in solving formal logical reasoning problems, and we demonstrate through a theoretical\nconstruction that decoder-only Transformers can reliably decide SAT instances.\nTheorem 1.1 (Informal version of Theorem 4.5). For any $p, c \\in N+$, there exist a decoder-only\nTransformer with $O(p^2)$ parameters that can decide all 3-SAT instances of at most $p$ variables and $c$\nclauses using Chain-of-Thought reasoning.\nTo investigate the properties of our construction empirically, we design a compiler that converts\ncomputational graphs of abstract sequence operations used in our construction into Transformer\nmodel weights. We implemented the construction in PyTorch and empirically validated its correctness\non random 3-SAT instances. We also investigated its empirical properties such as the number of\ngenerated CoT tokens.\nAdditionally, we perform training experiments to demonstrate that Transformers can effectively\nlearn from deductive reasoning and the backtracking process of the DPLL algorithm encoded as"}, {"title": "2 RELATED WORK", "content": "Theoretical Expressiveness of Transformers and Chain-of-Thought (CoT): Owing to the empir-\nical success of Transformer-based models, many researchers have investigated the capabilities of\nthe Transformer architecture from a theoretical perspective. This line of research focuses on what\ntypes of computation can Transformer models simulate by providing theoretical constructions of\nTransformer models with idealized assumptions. The seminal work of Liu et al. (2023) showed that\nTransformers can simulate automata using a single pass over only a logarithmic number of layers\nw.r.t. the number of states. Yao et al. (2021) demonstrated that transformers can perform parentheses\nmatching of at most k types of parentheses and D appearance of each (Dyckk,D) with D + 1 layers.\nHowever, the computation power of one pass of the Transformer model is fundamentally limited\n(Merrill & Sabharwal (2023)), and the success of Chain-of-Thought (CoT) reasoning (Wei et al."}, {"title": "3 PRELIMINARIES", "content": "The Boolean satisfiability problem (SAT) is the problem of determining whether there exists an\nassignment A of the variables in a Boolean formula F such that F is true under A. In this paper we"}, {"title": "3.1 AUTOREGRESSIVE DECODER-ONLY TRANSFORMER ARCHITECTURE", "content": "The Transformer architecture Vaswani et al. (2017) is a foundational model in deep learning for\nsequence modeling tasks. In our work, we focus on the autoregressive decoder-only Transformer,\nwhich generates sequences by predicting the next token based on previously generated tokens. It\nis a relatively complex architecture, and here we only give a precise but quite concise description,\nand we refer the reader Vaswani et al. (2017) among many others for additional details. Given an\ninput sequence of tokens $s = (s_1, s_2, ..., s_n) \\in V^n$, where V is a vocabulary, a Transformer model\n$M : V^* \\rightarrow \\Delta(V)$ maps s to a distribution $p_{n+1} \\in \\Delta(V)$ by composing a sequence of parameterized\nintermediate operations. These begin with a token embedding layer, following by L transformer\nblocks (layers), each block consisting of H attention heads, with embedding dimension $d_{emb}$, head\ndimension $d_h$, and MLP hidden dimension $d_{mlp}$. Let us now describe each of these maps in detail.\nToken Embedding and Positional Encoding\nEach input token $s_i$ is converted into a continuous vector representation $Embed(s_i) \\in \\mathbb{R}^{d_{emb}}$ using a\nfixed embedding map Embed($\\cdot$). To incorporate positional information, a positional encoding vector\n$p_i \\in \\mathbb{R}^{d_{emb}}$ is added to each token embedding. The initial input to the first Transformer block is\n$x^{(0)} \\leftarrow (Embed(s_1) + p_1, Embed(s_2) + p_2, ..., Embed(s_n) + p_n) \\in \\mathbb{R}^{n \\times d_{emb}}$.\nTransformer Blocks. For $l = 1, ..., L$, each block l of the transformer processes an embedded\nsequence $x^{(l-1)} \\in \\mathbb{R}^{n \\times d_{emb}}$ to produce another embedded sequence $x^{(l)} \\in \\mathbb{R}^{n \\times d_{emb}}$. Each block consists\nof a multi-head self-attention (MHA) mechanism and a position-wise feed-forward network (MLP).\nWe have a set of parameter tensors that includes MLP parameters $W_1^{(l)} \\in \\mathbb{R}^{d_{emb} \\times d_{mlp}}$, $b_1^{(l)} \\in \\mathbb{R}^{d_{mlp}}$,\n$W_2^{(l)} \\in \\mathbb{R}^{d_{mlp} \\times d_{emb}}$, and $b_2^{(l)} \\in \\mathbb{R}^{d_{emb}}$, self-attention parameters $W_Q^{(l,h)}$, $W_K^{(l,h)}$, $W_V^{(l,h)} \\in \\mathbb{R}^{d_{emb} \\times d_{h}}$ for\nevery $h = 1, ..., H$, and multi-head projection matrix $W^O \\in \\mathbb{R}^{(Hd_h) \\times d_{emb}}$. We will collectively\nrefer to all such parameters at layer l as $\\Gamma^{(l)}$, whereas the self-attention parameters for attention head\nh at layer l will be referred to as $\\Gamma^{(l,h)}$. We can now process the embedded sequence $x^{(l-1)}$ to obtain\n$x^{(l)}$ in two stages:\n$h^{(l)} \\leftarrow x^{(l-1)} + MHA(x^{(l-1)}; \\Gamma^{(l)}),$\nand $x^{(l)} \\leftarrow h^{(l)} + MLP(h^{(l)}; \\Gamma^{(l)}),$\nwhere\n$MHA(x; \\Gamma^{(l)}) := Concat(Attention(x; \\Gamma^{(l,1)}), ..., Attention(x; \\Gamma^{(l,H)})) W^O$,\n$Attention(x; \\Gamma^{(l,h)}) := softmax\\left(\\frac{1}{\\sqrt{d_h}} x W_Q^{(l,h)} (x W_K^{(l,h)})^T + M\\right) x W_V^{(l,h)}$,\n$MLP(h; \\Gamma^{(l)}) := \\sigma\\left(hW_1^{(l)} + b_1^{(l)}\\right) W_2^{(l)} + b_2^{(l)}$.\nThe $n \\times n$ matrix M is used as a \u201cmask\u201d to ensure self-attention is only backward looking, so we set\n$M[i, j] = -\\infty$ for $i \\geq j$ and $M[i, j] = 0$ otherwise. Finally, we use the ReGLU($\\cdot$) : $\\mathbb{R}^{n \\times 2d_{mlp}} \\rightarrow \\mathbb{R}^{n \\times d_{mlp}}$\nactivation function $\\sigma(\\cdot)$ at each position. Given input $u \\in \\mathbb{R}^{n \\times 2d_{mlp}}$, for each position i we split $u_i$\ninto two halves $u_{i,1}, u_{i,2} \\in \\mathbb{R}^{d_{mlp}}$ and, using $\\otimes$ denotes element-wise multiplication, we define\n$\\sigma_{ReGLU}(u_i) = u_{i,1} \\otimes ReLU(u_{i,2}).$", "\title": "Output Layer"}, {"content": "After the final Transformer block, the output representations are projected onto the vocabulary space,\nand the output vector at the final position is converted to a probability distribution over the vocabulary\nvia the softmax function:"}, {"title": "4 TRANSFORMERS AND SAT: LOGICAL DEDUCTION AND BACKTRACKING", "content": "This section presents and explains our main results on Transformers' capability in deductive reasoning\nand backtracking with CoT. To rigorously state our results, we first formally define decision problems,\ndecision procedures, and what it means for a model to \"solve\" a decision problem using CoT:\nDefinition 4.1 (Decision Problem). Let V be a vocabulary, $\\Sigma \\subseteq V$ be an alphabet, $L \\subseteq \\Sigma^*$ be a set\nof valid input strings. We say that a mapping $f: L \\rightarrow \\{0, 1\\}$ is a decision problem defined on L.\nDefinition 4.2 (Decision Procedure). We say that an algorithm A is a decision procedure for the\ndecision problem f, if given any input string x from L, A outputs 1 if $f(x) = 1$, and 0 otherwise.\nDefinition 4.3 (Autoregressive Decision Procedure). For any map $M : V^* \\rightarrow \\Delta(V)$, which we\nrefer to as an auto-regressive next-token prediction model, and $\\mathcal{E} = \\{E_0, E_1\\} \\subset V$, define decision\nprocedure $A_{M,\\mathcal{E}}$ as follows: For any input $s_{1:n}$, run Algorithm 1 with stop tokens $\\mathcal{E}$. $A_{M,\\mathcal{E}}$ outputs\n0 if $s_{1:t}$ ends with $E_0$ and $A_{M,\\mathcal{E}}$ output 1 otherwise. We say M autoregressively decides decision\nproblem f if there is some $\\mathcal{E} \\subset V$ for which $A_{M,\\mathcal{E}}$ decides f.\nDefinition 4.4 (3-SATp,c). Let DIMACS(p, c) denote the set of valid DIMACS encodings of 3-SAT\ninstances with at most p variables and c clauses with a prepended [BOS] token and an appended\n[SEP] token. Define 3-SATp,c : DIMACS(p, c) \u2192 {0, 1} as the problem of deciding whether the\n3-SAT formula encoded in the input in DIMACS(p, c) encoding is satisfiable.\nWith the above definition, we're ready to present a formal statement of our theoretical construction of\na Transformer model that performs SAT Solving:\nTheorem 4.5 (Decoder-only Transformers can solve SAT). For any $p,c \\in \\mathbb{N}^{+}$, there exists a\nTransformer model $M : V^* \\rightarrow \\triangle(V)$ that autoregressively decides 3-SATp,c in no more than $p \\cdot 2^{p+1}$\nCoT iterations. M requires L = 7 layers, H = 5 heads, $d_{emb} = O(p)$, and O($p^2$) parameters.\nRemarks on Theorem 4.5\n\u2022 The upper bound on the CoT length $p \\cdot 2^{p+1}$ is a worst-case upper bound which assumes that\nthe model is unable to make any logical deductions have to try all $2^p$ assignments. However,\nthis upper bound is never reached in practice, and in Figure 3 we show that the number of\nCoT tokens is no greater than $8p \\cdot 2^{0.08p}$ for most formulas. If the number of backtracking\nsteps is bounded by T then the CoT is no longer than (2p + 1)(T + 1)"}, {"title": "5 COMPILER FOR COMPLEX TRANSFORMER ALGORITHMS", "content": "In the previous section, we presented a theoretical construction of a Transformer capable of solving\nSAT instances through backtracking and parallel deduction. However, relying solely on theorems and\nproofs can make it challenging to gain practical insights and verify correctness. To address this, we\nintroduce ParametricTransformer (PARAT), which provides a framework for converting theoretical\nconstructions of Transformers into practical models to facilitate empirical analysis and validation.\n5.1 SUPPORTED FEATURES AND OPERATIONS\nOur compiler is designed to provide an intuitive syntax resembling standard numerical array manipu-\nlation, akin to NumPy, while supporting a diverse and extensible set of abstract operations. PARAT is\ncapable of implementing\n\u2022 NumPy-like Array Syntax for indexing, arithmetic, and comparison.\n\u2022 Multi-Level Abstraction to enable low-level customization.\n\u2022 Multi-stage Evaluation Mechanisms to facilitate debugging and error localization\n\u2022 High Extensibility through structured class inheritance, promoting the addition of new\nfeatures and operations.\nEach intermediate \"variable\u201d is an instance of the SOp base class (name adapted from Lindner et al.\n(2023)), and each instance sop of SOp is assigned a dimension $d_{sop} \\in \\mathbb{N}^{+}$ and can be viewed as"}, {"title": "5.2 THE COMPILATION PROCESS", "content": "PARAT takes in a SOp that contains the computational graph of the algorithm and outputs a PyTorch\n(Paszke et al. (2017)) model. The compilation process follows stages similar to those of Tracr:\n1. Computational Graph Construction: When a user writes sop operations, each operation\nautomatically creates a dependency tree of all operations required for computing the resulting\nsop value.\n2. Reduction to Base Operations: Each sop operation is reduced to one of 5 base classes:\nSelfAttention for operation that requires information from other token positions,\nGLUMLP for non-linear local operations, Linear for linear local operations, PosEncSOp\nfor positional encodings, or TokEmbSOp for token embeddings. Sequential Linear\noperations are reduced to a single operation through matrix multiplication and dependency\nmerging."}, {"title": "5.3 ANALYSIS OF THE COMPILED SAT-SOLVING MODEL", "content": "With PARAT, we successfully compiled our theoretical construction in Theorem 4.5 using the code\nin Appendix D. For p = 20 number of variables, the resulting Transformer has 7 layers, 5 attention\nheads, 502 embedding dimensions, and 5011862 parameters. With a concrete implementation of our\ntheoretical construction in PyTorch, we empirically investigate 3 questions (1) Does the compiled\nmodel correctly decide SAT instances? (2) How many steps does the model take to solve actual\n3-SAT instances? (3) How does error induced by soft attention affect reasoning accuracy? These\nquestions reveal further insights that are not available by observing the theoretical constructions alone\nand demonstrate the additional values provided by PARAT.\nEvaluation Datasets We evaluate our models on randomly sampled DIMACS encoding of 3-SAT\nformulas. We focus on SAT formulas with exactly 3 literals in each clause, with the number of clauses\nc between 4.1p and 4.4p, where p is the number of variables.\nIt is well-known that the satisfiability of such random 3-SAT formulas highly depends on the\nclause/variable ratio, where a formula is very likely satisfiable if $c/p \\ll 4.26$ and unsatisfiable if\n$c/p > 4.26$ (Crawford & Auton (1996)). This potentially allows a model to obtain high accuracy\njust by observing the statistical properties such as the c/p ratio. To address this, we constrain this\nratio for all formulas to be near the critical ratio 4.26. Furthermore, our \u201cmarginal\" datasets contain\npairs of SAT vs UNSAT formulas that differ from each other by only a single literal. This means\nthat the SAT and UNSAT formulas in the dataset have almost no statistical difference in terms of c/p\nratio, variable distribution, etc., ruling out the possibility of obtaining SAT vs UNSAT information\nsolely via statistical properties.\nWe also use 3 different sampling methods to generate formulas of different solving difficulties to\nevaluate our model:\n\u2022 Marginal: Composed of pairs of formulas that differ by only one token.\n\u2022 Random: Formulas are not paired by differing tokens and each clause is randomly generated."}, {"title": "6 CAN TRANSFORMER LEARN SAT SOLVING FROM DATA?", "content": "Our previous sections showed that Transformer and weights exist for solving SAT instances using\nCoT with backtracking and deduction. However, it is unclear to what extent Transformers can learn\nsuch formal reasoning procedures by training on SAT formulas. Previously, Zhang et al. (2023)\nshowed that when using a single pass of a Transformer model (without CoT), Transformers fail to\ngeneralize to logical puzzles sampled from different distributions even when they have the same\nnumber of propositions.\nThis section provides proof-of-concept evidence that training on the Chain-of-Thought procedure with\ndeduction and backtracking described in Figure 1 can facilitate Out-of-Distribution generalization\nwithin the same number of variables.\nDatasets In Section 5.3 we introduced 3 different distributions over random 3-SAT formulas of\nvarying difficulties. For training data, we use the same sampling methods, but instead of having a\nseparate dataset for each variable number p, we pick 2 ranges $p \\in [6, 10]$ and $p \\in [11, 15]$, where\nfor each sample a random p value is picked uniformly random from the range. Each formula with\np variables contains 16.4p to 17.6p tokens. This results in 2 \u00d7 3 training datasets, each containing\n5 \u00d7 105 training samples\u00b9, with balanced SAT vs UNSAT samples. For each formula, we generate\nthe corresponding chain of thought in the same format as Figure 1 using a custom SAT Solver. The\nevaluation data is exactly the same as Section 5.3.\nModel and Training We use the LLaMa (Touvron et al. (2023)) architecture with 70M and 160M\nparameters for the training experiments, which uses Rotary Positional Encodings (RoPE) and SwiGLU\nas the activation function for MLP layers. Following prior works (Feng et al. (2023)), we compute"}, {"title": "6.1 INTRA-LENGTH OOD GENERALIZATION", "content": "\u00b9The number of training samples is negligible compared to the total number of possible formulas. Note that\nthe number of clauses is at least 4p, each clause contains 3 literals and each literal has at least p choices. This\nresults in $p^{12p}$ possibilities, which is > $10^{56}$ for p = 6.\n6.2 LIMITATIONS IN LENGTH GENERALIZATION\nThe second experiment evaluates the model's ability to generalize to formulas with a different number\nof variables than seen during training. We use the model trained on the Marginal dataset from section\n6.1 and evaluate datasets with 4-20 variables, generated using the three methods described, with\n2,000 samples each. For this experiment, we evaluate the binary SAT vs UNSAT prediction accuracy.\nResults In Figure 2, our results indicate that performance degrades drastically beyond the training\nregime when the number of variables increases. This shows that the model is unable to learn a\ngeneral SAT-solving algorithm that works for all inputs of arbitrary lengths, which corroborates\nour theoretical result where the size of the Transformer for SAT-solving depends on the number of\nvariables. This further demonstrates the value of having a compiled Transformer that provably works\nwell on all inputs up to p variables for any given p."}, {"title": "C PROOFS", "content": "C.1 NOTATION DETAILS\n3-SAT SAT problems where the Boolean formula is expressed in conjunctive normal form (CNF)\nwith three literals per clause will be referred to as 3-SAT. A formula in CNF is a conjunction (i.e.\n\"AND\") of clauses, a clause is a disjunction (i.e. \u201cOR\u201d) of several literals, and each literal is either a\nvariable or its negation. In the case of 3-SAT, each clause contains at most three literals. An example\n3-SAT formula with 4 variables and 6 clauses is:\n$(x_1 \\lor \\neg x_2) \\land (\\neg x_1 \\lor x_2 \\lor \\neg x_3) \\land (x_2 \\lor x_4 \\lor \\neg x_1) \\land$\n$(x_1 \\lor \\neg x_3 \\lor x_4) \\land (\\neg x_2 \\lor \\neg x_3 \\lor \\neg x_4) \\land (\\neg x_4 \\lor \\neg x_1)$\nThe 3-SAT problem refers to determining if any assignment of truth values to the variables allows the\nformula & to evaluate as true. It is well-known that 3-SAT is NP-hard and is widely believed to be\nunsolvable in polynomial time.\nDIMACS Encoding The DIMACS format is a standardized encoding scheme for representing\nBoolean formulas in conjunctive normal form (CNF) for SAT problems. Each clause in the formula\nis represented as a sequence of integers followed by a terminating \u201c0\u201d (i.e. \u201c0\u201d represents / symbols\nand parentheses). Positive integers correspond to variables, while negative integers represent the\nnegations of variables. For instance, if a clause includes the literals x1, -x2, and x3, it would be\nrepresented as \"1 -2 3 0\" in the DIMACS format.\nFor the 3-SAT example in the previous paragraph, the corresponding DIMACS representation would\nbe:\n1-2 0 -1 2 -3 0 2 4 -1 0 1 -3 4 0 -2 -3 -4 0 -4 -1 0\nC.2 USEFUL LEMMAS FOR TRANSFORMERS\nIn this section, we present adapted versions of several lemmas from Feng et al. (2023). Specifically,\nan MLP with ReGLU can exactly simulate ReLU, linear operations, and multiplication without error.\nFor Self-attention lemmas, we directly adapt from Feng et al. (2023).\nC.2.1 LEMMAS FOR MLP WITH REGLU ACTIVATION\nThis section shows several lemmas showing the capabilities of the self-attention operation and MLP\nlayers to approximate high-level vector operations. These high-level operations are later used as\nbuilding blocks for the Transformer SAT-solver. Specifically, with appropriate weight configurations,\na 2-layer MLP with ReGLU activation $f(x) = W_2[(W_1x + b_1) \\odot relu(Vx + c)]$ can approximate\nthe following vector operations for arbitrary input x:\n\u2022 Simulate a 2-layer MLP with ReLU activation: $W_2 ReLU(W_1x + b_1) + b_2$\n\u2022 Simulate any linear operation Wx\n\u2022 Simulate element-wise multiplication: $x_1 \\odot x_2$\nLemma C.1 (Simulating a 2-Layer ReLU MLP with ReGLU Activation). A 2-layer MLP with\nReGLU activation function can simulate any 2-layer MLP with ReLU activation function.\nProof. Let the ReLU MLP be defined as:\n$g(x) = W_2 ReLU(W_1x + b_1) + b_2.$\nSet the weights and biases of the REGLU MLP as follows:\n$W_1 = 0, b_1 = 1,$\n$V = W_1,b_2 = b_1,$\n$W_2 = W_2,b = b_2.$"}, {"title": "C.2.2 CAPABILITIES OF THE SELF-ATTENTION LAYER", "content": "Then, the ReGLU MLP computes:\n$f(x) = W_2 [(0x + 1) \\otimes ReLU(W_1x + b_1)] + b_2.$\nSimplifying:\n$f(x) = W_2 [1 \\odot ReLU(W_1x + b_1)] + b_2 = W_2 ReLU(W_1x + b_1) + b_2 = g(x).$\nThus, the ReGLU MLP computes the same function as the ReLU MLP.\nLemma C.2 (Simulating Linear Operations with ReGLU MLP). A 2-layer MLP with ReGLU\nactivation can compute any linear operation $f(x) = Wx + b.$\nProof. To compute a linear function using the ReGLU MLP, we can set the activation to act as a\nscalar multiplier of one. Set the weights and biases as:\n$W_1 = W,b_1 = b,$\n$V = 0,b_2 = 1,$\n$W_2 = I,b = 0.$\nHere, I is the identity matrix.\nSince $Vx + b_2 = b_2 = 1$, we have:\n$ReLU(Vx + b_2) = ReLU(1) = 1.$\nThen, the ReGLU MLP computes:\n$f(x) = I \\odot [(Wx + b) 1] = Wx + b.$\nThus, any linear operation can be represented by appropriately setting $W_1, b_1$, and $W_2$.\nLemma C.3 (Element-wise Multiplication via ReGLU MLP). A 2-layer MLP with ReGLU activation\ncan compute the element-wise multiplication of two input vectors x1 and x2, that is,\n$f(x) = x_1 \\odot x_2,$\nwhere $x = [x_1; x_2]$ denotes the concatenation of x1 and x2.\nProof. Let $x = [x_1; x_2] \\in \\mathbb{R}^{2n}$, where $x_1, x_2 \\in \\mathbb{R}^n$.\nSet the weights and biases:\n$W_1 = \\begin{bmatrix}I_n\\\\I_n\\end{bmatrix},b_1 = 0_{2n},$\n$V = \\begin{bmatrix}I_n\\\\-I_n\\end{bmatrix},b_2 = 0_{2n},$\n$W_2 = [I_n \\odot I_n],b = 0_n.$\nCompute:\n$W_1x + b_1 = \\begin{bmatrix}x_1\\\\x_2\\end{bmatrix},$\n$Vx + b_2 = \\begin{bmatrix}x_1\\\\-x_2\\end{bmatrix},$\n$ReLU(Vx + b_2) = \\begin{bmatrix}ReLU(x_1)\\\\ReLU(-x_2)\\end{bmatrix}.$\nThe element-wise product:"}, {"title": "C.3 THEORETICAL CONSTRUCTION", "content": "$\\begin{aligned}&(W_1x + b_1) \\odot ReLU(Vx+b_2) \\\\&\\qquad= \\begin{bmatrix}x_1 \\\\x_2\\end{bmatrix} \\odot \\begin{bmatrix}ReLU(x_2) \\\\ReLU(-x_2)\\end{bmatrix}\\end{aligned}$\nCompute the output:\n$\\begin{aligned}&f(x) = W_2 [(W_1x + b_1) \\odot ReLU(Vx+b_2)] + b \\\\&\\qquad= x_1 \\odot ReLU(x_2) - x_1 \\odot ReLU(-x_2) \\\\&\\qquad= x_1 \\odot (ReLU(x_2) - ReLU(-x_2)) \\\\&\\qquad= x_1 \\odot x_2.\\end{aligned}$\nThus, the ReGLU MLP computes $f(x) = x_1 \\odot x_2$ without restrictions on $x_2.$\nPreprint Note: We're in the process of reformatting the construction and proof for better organization\nNotations\n\u2022 p denotes the number of variables"}, {"title": "C.4 CORRECTNESS", "content": "\u2022 ti denotes the token at position i\n\u2022 TVars denotes the set of tokens that denote variables and their negations. i.e. '1', '2',...,\n'n', '-1', '-2',..., '-n'\n\u2022 b denotes boolean variables\nProof. We first describe the encoding format of the formulas and the solution trace format before\ngoing into the details of model construction.\nInput Format. We consider 3-CNF-SAT formulas in the DIMACS representation, with an initial\n[BOS] token and an ending [SEP] token. Each variable $x_i$ for $i \\in [n]$ has 2 associated tokens: i\nand -i (e.g., 1 and \u22121), where the positive token indicates that the i-th variable appears in the clause\nwhile the negative token indicates that the negation of the i-th variable appears in the clause. Clauses\nare separated using the 0 token. For example, the formula\n$(\\neg x_2 \\lor \\neg x_4 \\lor \\neg x_1) \\land (x_3 \\lor x_4 \\lor \\neg x_1) \\land (\\neg x_1 \\lor \\neg x_3 \\lor \\neg x_2)$\n$\\land (x_1 \\lor \\neg x_2 \\lor \\neg x_4) \\land (\\neg x_4 \\lor x_2 \\lor x_1) \\land (x_1 \\lor \\neg x_2 \\lor x_4)$\nwould be represented as:\n[BOS] -2 -4 -1 0 3 4 -1 0 -1 -3 -2 0 1 -2 -4 0 -4 2 1 0 1 -2 4 0\n[SEP]\nSolution Trace Format. The trace keeps track of the order of the assignments made and whether\neach assignment is a decision (assumption) or a unit propagation (deduction). Literals with a\npreceding D token are decision literals while other literals are from unit propagation. When the\nmodel encounters a conflict between the current assignment and the formula, it performs a backtrack\noperation denoted by [BT] and performs another attempt with the last decision literal negated. In\nparticular, compared to Figure 1, we used D to abbreviate Assume and use [BT] to abbreviate\nBacktrack\nAs an example, the solution trace for the above SAT formula would be:\n[SEP] D 2 D 1 -4 3 [BT] D 2 D -1 -4 [BT] -2 D 3 D 4 -1 SAT\nEmbedding Layer. Our token set consists of one token for each variable and its negation, the\nseparator token 0, and a special token D to denote where decisions are made. The positional encoding\noccupies a single dimension and contains the numerical value of the position of the token in the string.\n(i.e. there exists a dimension pos such that the position embedding of position i is i\u00b7epos)\nLayer 1. The first layer prepares for finding the nearest separator token and D token. Let i denote\nthe position index of tokens:\n1. Compute $i_{sep}$ where $i_{sep} = i$ if the corresponding token $t_i \\in \\{\u20180\u2019, \u2018[SEP]\u2019, \u2018[BT]\u2019\\}$ and\n$i_{sep} = 0$ otherwise\n2. Similarly, compute $i_D$ where $i_D = i$ if the corresponding token $t_i = D$ and $i_{sep} \\neq 0$ otherwise.\n3. Compute $(i - 1)^2, i^2$ for index equality comparison\nThe first 2 operations can both be computed using a single MLP layer that multiplies between i from\nthe positional encoding using Lemma C.3. Similarly, the 3rd operation is a multiplication operation\nthat can be performed with Lemma C.3."}]}