{"title": "Graph Similarity Computation via Interpretable Neural Node\nAlignment", "authors": ["Jingjing Wang", "Hongjie Zhu", "Haoran Xie", "Fu Lee Wang", "Xiaoliang Xu", "Yuxiang Wang"], "abstract": "Graph similarity computation is an essential task in many real-world graph-related applications\nsuch as retrieving the similar drugs given a query chemical compound or finding the user's\npotential friends from the social network database. Graph Edit Distance (GED) and Maximum\nCommon Subgraphs (MCS) are the two commonly used domain-agnostic metrics to evaluate\ngraph similarity in practice, by quantifying the minimum cost under optimal alignment between\nthe entities of two graphs, such as nodes, edges, and subgraphs. Unfortunately, computing the\nexact GED is known to be a NP-hard problem. To solve this limitation, neural network based\nmodels have been proposed to approximate the calculations of GED/MCS between the given\npair of graphs. However, deep learning models are well-known 'black boxes', thus the typically\ncharacteristic one-to-one node/subgraph alignment process in the classical computations of GED\nand MCS cannot be seen. Such alignment information provides interpretability for graph editing\noperations and visualizes the common subgraph between two graphs, which is extremely useful\nfor graph-based downstream tasks. Existing methods have paid attention to approximating the\nnode/subgraph alignment (soft alignment), but the one-to-one node alignment (hard alignment)\nhas not yet been solved. To fill this gap, in this paper we propose a novel interpretable\nneural node alignment model without relying on node alignment ground truth information.\nFirstly, the quadratic assignment problem in classical GED computation is relaxed to a linear\nalignment via embedding the features in the node embedding space. Secondly, a differentiable\nGumbel-Sinkhorn module is proposed to unsupervised generate the optimal one-to-one node\nalignment matrix. Experimental results in real-world graph datasets demonstrate that our method\noutperforms the state-of-the-art methods in graph similarity computation and graph retrieval\ntasks, achieving up to 16% reduction in the Mean Squared Error and up to 12% improvement in\nthe retrieval evaluation metrics, respectively. Furthermore, the experimental results demonstrate\nthat interpretability and accuracy are not always opposites, in fact, enhancing interpretability can\nbenefit accuracy. The source code is available at https://github.com/sasr11/GNA.", "sections": [{"title": "1. Introduction", "content": "Real-world data are usually nonlinear structures, such as social networks in sociology, protein-protein interactions\nin natural sciences, and user behavior histories in recommendation systems. The complex relationships between\nnodes within a graph provide rich semantic information, making graphs a hotspot for research in the field of data\nmodeling. Among the wide range of graph-based applications, graph similarity computation between pairs of graphs\nis a fundamental task, used in graph retrieval and clustering. Graph Edit Distance (GED) and Maximum Common\nSubgraphs (MCS) are the two commonly used domain-agnostic metrics to evaluate graph similarity in practice, by\nquantifying the minimum cost under optimal alignment between the entities of two graphs, such as nodes, edges, and\nsubgraphs. Specifically, GED is defined as the minimum cost required to transform one graph into another, including\ninserting, deleting and substituting. MCS measures the similarity by identifying the largest common subgraph between"}, {"title": "2. Related Work", "content": "In this section, we introduce the related works about graph similarity computation and graph/node matching."}, {"title": "2.1. Graph Similarity Computation", "content": "Traditional Search-based Similarity Computation. These traditional methods rely on graph search and structural\nmatching. Typically, the A* beam search method [2] optimizes the search space via heuristic strategies that apply a\ncost function to estimate the minimal cost from the current node to the goal one. This strategies are beneficial to reduce\nthe complexity of each search step by limiting the number of candidate pool into a controllable size. Other approaches\nsuch as Bipartite approximation [4; 13] also utilized greedy search strategies similar to A* beam, yet progressively\napproximating the potential optimal solution based on local optimal solutions. Additionally, subgraph isomorphism is\nalso a hot direction in this domain, for example Lou et al. [14] proposed to identify the similar substructures between a\ngiven pair of graphs to explore the potential graph matching. These traditional search methods are high interpretability\nwhich is valuable for downstream tasks such as in bioinformatics and cheminformatics fields. Howvever, they also\nsuffered from the exponential time complexity, limiting their applicability to large-scale graphs.\nNeural Network-based Similarity Computation. Considering that a desirable graph search method should achieve\nhigher accuracy and improve interpretability within a reasonable time. Researchers are encouraged to design models\nbased on neural network to solve the above problem. Early works about node-level embedding based methods such\nas Node2Vec [15] and DeepWalk [16] encoded the node features based on random walk, then transforming the graph\nsimilarity computation into vector comparisons. Additionally, graph-level similarity computation have been proposed,\nfor example SimGNN [17], GraphSim [18], Graph Matching Network (GMN) [19]. Although these existing GNN-\nbased methods achieve better generalization and significant time reduction in exact GED computation, they lack an\nexplanation of node/subgraph alignment."}, {"title": "2.2. Graph/Node Matching", "content": "Graph Matching. The matching method is designed to enhance the interpretability of the above GNN-based models\nincluding (sub)graph-level and node-level matching. Problems such as entity matching [20], social network alignment\n[21], and protein-protein interaction network [22] alignment can be classified as graph matching. SLOTAlign [23]\nproposed to learn the knowledge graph structure information to generate the the entities matching matrix. GOT [11]\nemployed the Gromov-Wasstein distance to obtain the optimal permutation matrix. GNN-PE [24] proposed a novel\npruning strategy based on path labeling/significant features, which efficiently and accurately matches subgraphs by\ntraversing indexes over graph partitions in parallel.\nNode Matching. The goal of node-level matching is to find the correspondence between nodes, which can be classified\ninto many-to-one (soft) and one-to-one (hard). Node matching is closely related to various graph tasks, including graph\nalignment [25], GED computation [26], and subgraph matching [27]. For example, GOTSim [10] propsed a linear\nmatching algorithm to derive one-to-one node alignment from the node similarity matrix and the addition/deletion\ncost matrix generated by the graph neural network. Additionally, GEDGNN [5] designed the Cross Matrix Module to\nobtain soft matching, accompanied by fine-grained supervision for gradient learning."}, {"title": "3. Preliminaries", "content": "Graph matching problem. In the graph matching task, finding the optimal permutation matrix is known to be a\nNP-hard problem. Given two graphs adjacency matrix $A_q$ and $A$, in generally $G_q \\subseteq G$, each non-zero element in A\ncan be found the only corresponding element in $A_q$ via the row and column permutation conversion. Take the case of"}, {"title": "4. Our Method", "content": "The proposed interpretable neural node alignment model, GNA, which comprises three components: 1) Embedding\nModule and 2) Costing and Matching Module, 3) Prediction, as illustrated in Figure 1. For a given pair of graphs, GNA\nfirst utilizes the multiple GNN layers to encode the graph structure and feature information into the node embeddings.\nThen, two bilateral branches are executed in GNA. One branch aims to estimate the pairwise node-to-node matching\ncost named \"costing module\u201d. The matching module outputs the optimal permutation matrix, as shown in Figure 1.\nWhen dealing with pairs of graphs that have different numbers of nodes, it is necessary to compute the associated costs\nof node addition or deletion under the one-to-one node matching setting. Thus, we pad the cost and matching matrices"}, {"title": "4.1. Embedding Module", "content": "Recently, graph neural networks (GNNs) have gained significant attention as a backbend for encoding node\nembeddings in various graph tasks. Following ref. [5], we adopt three-layer of the Graph Isomorphism Network (GIN\n[28]]) as the basic node encoder, which has been verified to have superior performance in different graph structural\ninformation capturing compared to conventional GNN models (e.g., GCN, GraphSAGE[29]).\nInspired by the Weisfeiler-Lehman isomorphism test, GIN achieves comparable results using a learnable multi-layer\nperception module. The k-th layer output of the GIN is formulated as follows:\n$h_v^{(k)} = MLP^{(k)}((1+ \\varphi^{(k)})h_v^{(k-1)} + \\sum_{u\\in N(v)} h_u^{(k-1)})$"}, {"title": "4.2. Costing Module", "content": "As shown in Figure 1, the graph similarity computation is composed of two components. We elaborate on the\ncosting and matching components at the node-level. Inspired by the bilateral branch network [17], we adopt two separate\nbranches to equip the model with both alignment and retrieval capabilities. From the implementation perspective, the\ndual branch is more consistent with the subsequent calculations, which will be explained in the following sections.\nAfter the GIN backend, node-level embeddings of two graphs $H_1$ and $H_2$ are generated. The cost computation\nmodule is designed to estimate the transfer cost (e.g institution, add and delete) from one node to another between the\ngraph pairs $(G_1, G_2)$. The goal of cost module is $A^{cost} \\in [0, 1]$ of size $|V_1|\\times |V_2|$ in which an element $A_{ij}$ denotes the\ncost of edit operations for matching $v_i \\in V_1$ with $v_j \\in V_2$.\n$A^{cost} = \\begin{bmatrix} c_{1,1} & ... & c_{1, V_2} \\\\ : & & : \\\\ c_{|V_1|,1} & ... & c_{|V_1|, V_2}  \\end{bmatrix}$"}, {"title": "4.3. Matching Module", "content": "Matching module outputs optimal permutation matrix $A^{matching} \\in {0,1}^{|V_2|\\times |V_2|}$ where each element $A_{ij}^{matching}$\nindicates whether the node $v_i$ in $G_1$ are matched to the node $v_j$ in $G_2$, and the $|V_2| \u2013 |V_1|$ nodes in $G_2$ will be deleted\n(or added). The optimal permutation matrix A* is a doubly stochastic matrix that satisfies the one-to-one matching in\nthe linear assignment task. As the objective function in the formula (3), we apply the Relu function to ensure $(\u2022)_+$.\nSpecifically, a Linear-ReLU \u2013 Linear network named \u201cLRL\u201d is performed on $H_1^*$ and $H_2$. LRL is utilized to generate\nthe costing matrix as the foundation for determining pair of node matches, which is similar to the costing module.\n$A = GS(LRL_{\\theta_1}(H_1^*) \\cdot LRL_{\\theta_2}(H_2))$"}, {"title": "4.4. Prediction", "content": "$A^{matching}$ is the final permutation matrix that demonstrates the value of the node matching based on the\nsemantic distance between two nodes, $A^{cost}$ is the edit cost matrix. Following existing methods [5], the element-wise\nmultiplication and summation is employed as the prediction GED of transforming one graph into another.Since both\nthe permutation matrix and the cost matrix are obtained via the inner product based on the node embedding, an NTN\nnetwork is designed to calculate a bias in the prediction based on the graph-level features which serve as supplement\ninformation. This strategy is widely used in several GNN-based networks [17; 30; 5] that rely on node-to-node cost to\npredict the graph similarity.\n$GED(G_1, G_2)_{prediction} = A_{ij}^{matching} * A_{ij}^{cost} + bias$"}, {"title": "5. Experiments", "content": "We introduce three real-world graph datasets: AIDS, Linux and IMDB to evaluate our method, the detail\ndescriptions are listed in the table 1. Here, '#' denotes the total number, such as #Graphs represents the number of\ngraphs contained in the current dataset. We construct the pair graph training dataset following [31; 17] as denoted as\n#Pairs.\nEach graph in AIDS dataset is antivirus screen chemical compounds, consisting of 42,687 chemical compound\nstructures with C, N, O, Cu, etc. atoms and covalent bonds. We selected graphs with the number of node and edges\nranging from 2 to 10 and 1 to 14, respectively. Then randomly grouped them to generate the training set of 940,000\ngraph pairs.\nLinux is a collection of 48,747 Program Dependence Graphs (PDGs) obtained from the Linux kernel functions.\nEach node in a graph represents a function statement, and each edge represents the dependency between two statements.\nWe filter out graphs with more than 10 nodes and randomly select 1,000 graphs as the initial dataset.\nIMDB is a larger dataset in which contains a maximum of 89 nodes. The IMDB dataset consists of a ego-network\nof 1,500 movie actors, with each node representing an actor and each edge a co-star relation. To avoid significant\ndifferences in the size of the graph, for IMDB large graph (number of nodes > 10), we generate 100 synthetic graphs\nfollowing the ref. [5]. For smaller graph, pairwise combinations is adopted. Finally, there are 501 \u00d7 501 + 399 \u00d7 100\ngraph pairs generated for the model training in total.\nWe split the three entire dataset with a ratio of 6:2:2 into training, validation, and testing sets."}, {"title": "5.2. Experiment Setting and Metrics", "content": "5.2.1. Hyper-parameter Settings\nWe stack 3-layer GIN as basic graph encoder following the GEDGNN[5] and the output feature size of each layer\nis 32, 64 and 128 respectively. For the cross matrix module, we set the number of layers as 16. For the matching matrix\nmodule, the temperature parameter is 0.1. The training batchsize is set to 128, all methods achieved the best result with\nless 20 epochs. We set the learning rate to 0.001 with the Adam optimizer, and the weight decay is 5 \u00d7 10\u22124.\nAll experiments are done on a Windows PC using the i7-8750H CPU. Our model is implemented in Python 3.7,\nwith machine learning models developed using PyTorch and PyTorch Geometric. For a fair comparison, all baselines\nare performed using the reported hyperparameter settings.\n5.2.2. Baselines\nWe evaluate the proposed method against 7 baselines, including end-to-end deep learning based approaches and\nnode matching approaches.\n1. End-to-end GED prediction approaches. (a) The EmbMean and EmbMax methods are the basic trainable\nmodels based on graph neural network by mean or max the node-level embeddings to predict the GED. (b) SimGNN[17]\nis the first and representative graph similarity method. (c) TaGSim [32] predicts the GED at a fine-grained level by\ndefining the different graph edit types and convert the GED into the sum cost of each editing type. (d) GPN [26] is\none of the state-of-art method in terms of graph edit path searching and graph similarity prediction, which is originally\ndesigned to supervise the A*-beam search.\n2. Node matching algorithms. (a) GOTSim[10] proposed an efficient unsupervised differentiable GED compu-\ntation method and simultaneously learning the alignments (i.e., explanations) similar to those of classical intractable\nmethods. (b) GEDGNN[5] is the the state-of-art method in node matching methods, however it is a supervised matching\nalgorithm.\n5.2.3. Metrics\nWe adopt the following commonly used metrics to valuate the performance between our model and baselines.\n1. Metrics about graph similarity estimation. The goal of the graph similarity estimation is to evaluate the\nprecision of the predicted GED. We consider the Mean Absolute Error (MAE) and Accuracy as the base metrics. For\nexample, given a pair of testing graph, MAE is defined to calculate the distance between the predicted GED value d\nand the ground-truth d* as |d \u2013 d*|. Accuracy is a commonly used in the classification tasks, in this work, we round\nthe predicted floating GED to a integer as round(d) = d* to evaluate the performance.\n2. Metrics about graph retrieval ranking. Ranking Metrics are sensitive the item rank. There are 100 candidate\ngraphs randomly selected from the item pool for each query graph G. Then, rank these candidates based on their\npredicted GED values. Therefore, we adopt Spearman's Rank Correlation Coefficient denoted as p, Kendall's Rank\nCorrelation Coefficient denoted as \u03c4, and precision (P@10, P@20) as the retrieval ranking evaluation metrics."}, {"title": "5.3. Performance Analysis", "content": "In this section, we present the results of graph similarity estimation and graph retrieval ranking. Table 2 show the\nsimilarity score estimation and ranking results for the GED problem, respectively. Our method outperforms most of the\nbenchmarks on all metrics across the three dataset both in GED prediction task and graph retrieval task. GPN, GOTSim,\nGEDGNN achieved the second best performance in some dataset, respectively, which denotes our model can learn a\ngood embedding function that generalizes to unseen test graphs. In Linux and IMDB dataset, GEDGNN outperforms in\nthe terms of \u03c4 and P@10. The reason might be that GEDGNN utilize the node matching ground-truth targets, which is\ncomputationally very expensive. This can also be obtained from GOTSim, unless the graph matching task in the IMDB"}, {"title": "5.4. Ablation Study", "content": "In this section, we evaluate the affect of the two key components in GNA: bijection in the matching module and\ndelete/add cost in the cost module. w/o Add/Delete Cost refers to removing the add and deleting cost; w/o Gumbel-\nsinkhorn refers to removing the gumbel-sinkhorn algorithm to relax the bijection limitation. The final result is shown in\nFigure 2. In other words, the alignment matrix is obtained based on the node distance. To gain a deeper understanding\nof the affectiveness, we conducted two ablation studies, one is on the default testing dataset and the other testing\ndataset we filtered out pair graphs that |V2| = |V1|. In Figure 2, we remove the Gumbel-Sinkhorn algorithm to\nrelax the bijection limitation. From the Figure 2, it is observed that the results demonstrate a significant decline in\nperformance on the graph similarity similarity task, while showing slight variations on graph retrieval ranking metrics.\nIt might be concluded that the similarity similarity task is more sensitive to the node-level alignment, while the graph\nretrieval task is vice versa. In spite of we generate two separate testing set on the similarity task, the overall trend is\nconsistent that is GNA performs the best, w/o Add/Delete Cost following, w/o Gumbel-Sinkhorn performs the worse.\nIt demonstrates that node-level interpretable especially bijection can effectively improve model's understand ability\nabout the differences between two pair graphs and improve the accuracy of GED."}, {"title": "5.5. Case study about Graph Ranking", "content": "We present the ranking results on the AIDS datasets with A*, SimGNN (representative GNN based methods),\nGoTSim (unsupervised node-level matching method) and GNA. The top row represents the ground-truth ranking results\nby A* along with the predicted normalized GEDs. The graphs within the red rectangle are isomorphic, indicating that\nthey have the same GED relative to the query graph. From Figure 3, it is clear that retrieve graphs obtained by GNA"}, {"title": "5.6. Interpretable Graph Matching", "content": "In this section, we discuss the effectiveness of GNA in the interpretable of the GED estimation and the results is\nshown in Figure 4. We conducted two case studies where one pair of graphs had the same number of nodes (the top\nrow in Figure 4) and the other pair did not (the bottom in Figure 4). The matching matrix $A_{match}$ produced by our\nmethod is visualized as a heat map. The optimal node alignment is identified by \"*\" in Figure 4. GNA approximates\nthe ground-truth node matching between a pair of graphs. Different from previous soft graph matching algorithms such\nas GMN [19], we can easily find the required operations to transform one graph into another based on the hard bijective\nmapping. This implies that GNA is highly interpretable and more similar to the classical GED models."}, {"title": "6. Conclusion", "content": "The paper proposes a novel interpretable graph similarity computation framework that provides node-level bijection\nalignment in GED approximation tasks. First, this architecture employs multi-layers of GIN to encapsulate the subgraph\nand edge information into the node features. Thanks to the embedding layer, the classical quadratic assignment problem\nis relaxed to a linear alignment. Then costing and matching modules are adopted to enhance the interpretable ability\nof the GED prediction. Experimental results verify that fine-grained alignment is beneficial to the expressive ability of\nthe model. GNA achieves superior performance both in the GED prediction task and graph retrieval task. In this work,\nGNA embeds the node features to relax the quadratic assignment problem. Additionally, edge embedding presents an\nalternative solution. In the future, we will attempt to achieve the edge alignment in the graph similarity computation.\nMore attention will be paid to find the solution about the graph similarity and entity alignment on more complex graphs,\nsuch as knowledge graphs and heterogeneous graphs."}, {"title": "CRediT authorship contribution statement", "content": "Jingjing Wang: Conceptualization, Methodology. Hongjie Zhu: Visualization, Investigation. Haoran Xie:\nValidation. Fu Lee Wang: Writing-Reviewing and Editing. Xiaoliang Xu: Supervision. Yuxiang Wang: Data\ncuration."}]}