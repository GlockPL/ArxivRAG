{"title": "Automating Prompt Leakage Attacks on Large Language Models Using Agentic Approach", "authors": ["Tvrtko Sternak", "Davor Runje", "Dorian Grano\u0161a", "Chi Wang"], "abstract": "This paper presents a novel approach to evaluating the security of large language models (LLMs) against prompt leakage-the exposure of system-level prompts or proprietary configurations. We define prompt leakage as a critical threat to secure LLM deployment and introduce a framework for testing the robustness of LLMs using agentic teams. Leveraging AG2 (formerly AutoGen), we implement a multi-agent system where cooperative agents are tasked with probing and exploiting the target LLM to elicit its prompt. Guided by traditional definitions of security in cryptography, we further define a prompt leakage-safe system as one in which an attacker cannot distinguish between two agents: one initialized with an original prompt and the other with a prompt stripped of all sensitive information. In a safe system, the agents' outputs will be indistinguishable to the attacker, ensuring that sensitive information remains secure. This cryptographically inspired framework provides a rigorous standard for evaluating and designing secure LLMs. This work establishes a systematic methodology for adversarial testing of prompt leakage, bridging the gap between automated threat modeling and practical LLM security.", "sections": [{"title": "I. INTRODUCTION", "content": "Large Language Models (LLMs) have become increasingly powerful, enabling groundbreaking capabilities in conversational agents [1]-[4]. However, their utility has also introduced new security challenges [5]-[8], such as the risk of prompt leakage\u2014the unintended disclosure of sensitive or confidential information embedded in system prompts [6]. To address this critical issue, we present a novel agentic framework for automating prompt leakage attacks, enabling researchers and practitioners to systematically test the resilience of LLMs against such vulnerabilities.\nTo formalize the security objective, we define a prompt-leakage secure system as one where the advantage Adv of any adversary in distinguishing responses generated using the original system prompt (H) from those generated using a sanitized system prompt (H') remains small, regardless of their effort."}, {"title": "II. RELATED WORK", "content": "The landscape of large language model vulnerabilities has been extensively studied in recent literature [5]-[8], that propose detailed taxonomies of threats. These works categorize LLM attacks into distinct types, such as adversarial attacks, data poisoning, and specific vulnerabilities related to prompt engineering. Among these, prompt injection attacks have emerged as a significant and distinct category, underscoring their relevance to LLM security.\nThe following high-level overview of the collected taxonomy of LLM vulnerabilities is defined in [7]:\n\u2022 Adversarial Attacks: Data Poisoning, Backdoor Attacks\n\u2022 Inference Attacks: Attribute Inference, Membership Inferences\n\u2022 Extraction Attacks\n\u2022 Bias and Unfairness Exploitation\n\u2022 Instruction Tuning Attacks: Jailbreaking, Prompt Injection.\nPrompt injection attacks are further classified in [6] into the following: Goal hijacking and Prompt leakage.\nThe reviewed taxonomies underscore the need for comprehensive frameworks to evaluate LLM security. The agentic approach introduced in this paper builds on these insights, automating adversarial testing to address a wide range of scenarios, including those involving prompt leakage and role-specific vulnerabilities."}, {"title": "A. Prompt Injection and Prompt Leakage", "content": "Prompt injection attacks exploit the blending of instructional and data inputs, manipulating LLMs into deviating from their intended behavior. Prompt injection attacks encompass techniques that override initial instructions, expose private prompts, or generate malicious outputs [8]. A subset of these attacks, known as prompt leakage, aims"}, {"title": "B. Defense Mechanisms", "content": "Various defense mechanisms have been proposed to address LLM vulnerabilities, particularly prompt injection and leakage [6], [7]. We focused on cost-effective methods like instruction postprocessing and prompt engineering, which are viable for proprietary models that cannot be retrained. Instruction preprocessing sanitizes inputs, while postprocessing removes harmful outputs, forming a dual-layer defense. Preprocessing methods include perplexity-based filtering [10], [11] and token-level analysis [12]. Postprocessing employs another set of techniques, such as censorship by LLMs [13], [14], and use of canary tokens and pattern matching [15], [16], although their fundamental limitations are noted [17]. Prompt engineering employs carefully designed instructions [18] and advanced techniques like spotlighting [19] to mitigate vulnerabilities, though no method is foolproof [20]. Adversarial training, by incorporating adversarial examples into the training process, strengthens models against attacks [21], [22]."}, {"title": "C. Security Testing for Prompt Injection Attacks", "content": "Manual testing, such as red teaming [23] and hand-crafted \"Ignore Previous Prompt\" attacks [24], highlights vulnerabilities but is limited in scale. Automated approaches like PAIR [25] and GPTFUZZER [26] achieve higher success rates by refining prompts iteratively or via automated fuzzing. Red teaming with LLMs [27] and reinforcement learning [28] uncovers diverse vulnerabilities, including data leakage and offensive outputs. Indirect Prompt Injection (IPI) manipulates external data to compromise applications [29], adapting techniques like SQL injection to LLMs [30]. Prompt secrecy remains fragile, with studies showing reliable prompt extraction [31]. Advanced frameworks like Token Space Projection [32] and Weak-to-Strong Jailbreaking Attacks [33] exploit token-space relationships, achieving high success rates for prompt extraction and jailbreaking."}, {"title": "D. Agentic Frameworks for Evaluating LLM Security", "content": "The development of multi-agent systems leveraging large language models (LLMs) has shown promising results in enhancing task-solving capabilities [9], [34]\u2013[37]. A key aspect across various frameworks is the specialization of roles among agents [9], [34], which mimics human collaboration and improves task decomposition.\nAgentic frameworks and the multi-agent debate approach benefit from agent interaction, where agents engage in conversations or debates to refine outputs and correct errors [9]. For example, debate systems improve factual accuracy and reasoning by iteratively refining responses through collaborative reasoning [37], while AG2 allows agents to autonomously interact and execute tasks with minimal human input.\nThese frameworks highlight the viability of agentic systems, showing how specialized roles and collaborative mechanisms lead to improved performance, whether in factuality, reasoning, or task execution. By leveraging the strengths of diverse agents, these systems demonstrate a scalable approach to problem-solving.\nRecent research on testing LLMs using other LLMs has shown that this approach can be highly effective [25]\u2013[27]. Although the papers do not explicitly employ agentic frameworks they inherently reflect a pattern similar to that of an \"attacker\" and a \"judge\". [25] This pattern became a focal point for our work, where we put the judge into a more direct dialogue, enabling it to generate attacks based on the tested agent response in an active conversation.\nA particularly influential paper in shaping our approach is Jailbreaking Black Box Large Language Models in Twenty Queries [25]. This paper not only introduced the attacker/judge architecture but also provided the initial system prompts used for a judge."}, {"title": "III. FORMAL DEFINITION OF PROMPT LEAKAGE SECURITY", "content": "Guided by [38], [39] and definitions of security found in [25] this chapter provides a formal framework to define and analyze the concept of prompt leakage security.\nFollowing [25] We begin by formally defining the elements involved in the prompt leakage framework.\na) Vocabulary V: Let V denote the vocabulary, the set of all possible tokens that can appear in any prompt or response. The vocabulary contains all the individual tokens from which prompts and responses are constructed.\nV = {U1, U2, ..., Uk} \u222a Vend\nThe special token Vend is used by an LLM to denote the last step in the process of generating a response.\nA sequence of symbols from the vocabulary V is denoted with x1:n = (x1,x2,...,xn), with Xi \u2208 V for all i\u2208 {1,..., n}\nb) Input and System Prompts P: An input prompt P and a system prompt H are sequences of tokens from the vocabulary V.\nc) Response R: A response R is a sequence of tokens from the vocabulary V terminating with the special symbol and generated by the language model T based on the input prompt P and the system prompt H. The response is generated one token at the time by sampling from probability distribution function qr as follows:\nZjqr(Rj-1 | (H||P))\nwhere Ro is an empty sequence and Rj-1 is the sequence of previously sampled symbols x1:j-1"}, {"title": "A. Choosing a Sanitized Prompt for Testing Sensitive Information Leakage", "content": "A system prompt can contain sensitive information, such as: user or organization-specific instructions; secret keys or tokens; proprietary details like algorithms or strategies; behavioral rules for ethics or compliance.\nSanitization removes or generalizes sensitive components to ensure the following:\n1) No Information Leakage: Responses do not reveal confidential data.\n2) Preserved Functionality: The sanitized prompt enables the LLM to perform its intended task effectively.\nThe definitions of the Advantage and Judge provide guarantees for evaluating the effectiveness of sanitized prompts under these two key constraints:\n1) Revealing Sensitive Information: If the LLM's responses reveal sensitive details from the unsanitized prompt, a judge can identify the leakage through string-matching or by crafting prompts to elicit those details.\n2) Degraded Functionality: If sanitization impacts the LLM's functionality, the judge can detect inconsistencies or failures in the generated responses, such as lack of coherence or task performance issues.\nThese guarantees ensure that both information leakage and functional degradation are accounted for, making the Advantage a robust measure of prompt leakage security.\nSelecting an effective sanitized prompt H' is crucial to ensure effective testing for both security and functionality. Here are key considerations:\n1) Empty H' is Insufficient: Removing all context severely impacts LLM functionality, making responses distinguishable from H.\n2) Excluding the Secret Entirely: Removing sensitive information can improve security but degrade performance, detectable by a judge.\n3) Substituting with a plausible fake keeps functionality and prevents leakage if responses are indistinguishable from H.\nTo test leakage, replace sensitive components with non-sensitive substitutes and verify that H' responses are indistinguishable from H."}, {"title": "B. Designing Sanitized Prompts to Evaluate Prompt Leakage of Nonsensitive Information", "content": "Although the leakage of fine-grained sensitive information, such as secrets, is a critical concern, the risk extends to the disclosure of any part of the original prompt H. Even non-sensitive components of the prompt can provide adversaries with valuable context, enabling them to refine attacks or infer sensitive details indirectly. In this section, we go beyond the test for secret leakage and explore strategies to design sanitized prompts H' to assess the risk of full prompt leakage, ensuring that no part of H is accidentally exposed through LLM responses.\n1) Design a Sanitized Prompt to Mimic the Original with Controlled Variations: Construct H' by replacing sensitive parts of H with controlled alternatives.\nExample: If H includes: \"User X's project: Project Alpha is developing an encryption algorithm.\" Construct H' as: \"User Y's project: Project Beta is researching a"}, {"title": "C. Defining a Prompt Leakage Secure System", "content": "The advantage (Adv) (4) measures how easily an adversary can distinguish between responses using the original prompt (H) and a sanitized version (H'). This metric evaluates a system's prompt-leakage-security, ensuring sensitive data in H is protected from leakage.\n1) Small Advantage: For a system to be considered prompt-leakage secure, the advantage (Adv) must remain small according to thresholds defined by the user of the system. A fully secure system, in principle, would have an advantage of 0, indicating that adversaries cannot distinguish between responses based on H or H' under any circumstances.\nHowever, in practical scenarios, users must determine what constitutes a \"small\" advantage based on their system's security requirements and acceptable risk levels. This flexibility allows the metric to be tailored to specific projects, environments, and applications.\nWe introduce baseline calculations in our experimental results, which can serve as guidelines for determining these thresholds. These baselines help practitioners establish an initial understanding of the acceptable advantage for their systems.\n2) Adversarial Effort: Adversarial effort (\u03b7) encompasses the resources and strategies an adversary employs to elicit prompt leakage. This includes:\n\u2022 Compute Power: Hardware capabilities and parallel processing resources.\n\u2022 Time: Total time spent devising and executing attacks.\n\u2022 Complexity: The sophistication of attack strategies, such as crafting tailored prompts or exploiting patterns in H.\n\u2022 Financial Costs: Investments in large-scale attacks or external systems.\nA robust prompt-leakage secure system ensures that even under significant adversarial effort, the advantage remains within the defined \"small\" range, ensuring sensitive information in H is adequately protected."}, {"title": "3) Implications of Large Advantage:", "content": "If the advantage (Adv) grows significantly with increased adversarial effort, the system demonstrates vulnerabilities, as it indicates that sensitive information in H may be leaking. Conversely, maintaining a small advantage under various adversarial conditions reflects a system's ability to resist prompt leakage effectively.\n4) Prompt Leakage Secure System: A prompt leakage secure system is defined as one where the advantage remains small, as determined by user-defined thresholds, regardless of the adversary's effort. This ensures that sensitive details embedded in the original prompt (H) are safeguarded against a wide range of adversarial strategies while maintaining the system's overall security and functionality.\nBy enabling a flexible definition of acceptable thresholds, this framework accommodates diverse applications and provides a structured approach to evaluating prompt-leakage security. Our experimental baselines further support the practical adoption of these metrics for real-world systems."}, {"title": "IV. PROMPT LEAKAGE PROBING", "content": "To validate our approach and gain insights into the advantage values associated with different model setups, we implemented Prompt Leakage Probing in accordance with the formal definition of prompt leakage security. This implementation serves as a proof-of-concept, simulating adversarial attacks on the models and calculating the advantage of such attacks.\nThe implementation code and detailed instructions for setup and usage, are available on our GitHub repository: https://github.com/sternakt/prompt-leakage-probing.\nOur study used prompts tailored for a helpful LLM assisting an imaginative automotive company, \"Bord.\" These prompts included sensitive business information, behavioral instructions, and references to car models, reflecting realistic enterprise use cases:\n1) Confidential Prompt: The original prompt contained sensitive business data, such as regional price discounts (Europe and Croatia).\n2) Sanitized Prompt: Was modified using strategies from III-A, replacing regions with substitutes (e.g., USA and Texas) to calculate advantage values across models.\nThis setup enabled robust testing under adversarial conditions.\nThe evaluation was conducted across three predefined model configurations with varying levels of prompt hardening and safeguards:\n\u2022 Low Security: A large language model (LLM) without hardened prompts or guardrails.\n\u2022 Medium Security: An LLM employing basic prompt hardening techniques, such as specific contextual clues."}, {"title": "A. Advantage Calculation", "content": "Leveraging a testing framework that includes an initial analysis agent, a judge agent, and a tested agent, we evaluate the susceptibility of language models (LLMs) to prompt leakage and calculate the advantages for low, medium and high security setups. All agents, including the judge and analysis agents, are based on the ChatGPT-40-mini model. The chat between a judge and initial analysis agent runs within a GroupChat [9] and the communication with the tested agent is facilitated using function calls.\n1) Agent Roles:\n\u2022 Initial Analysis Agent: Analyzes the original and sanitized prompts, assisting the judge agent in prompting the tested agent.\n\u2022 Judge Agent: Determines whether it's interacting with the original agent (Agent 1) or the sanitized agent (Agent 2) by asking the prompt_agent function. It analyzes responses to identify differences between the two agents.\n\u2022 Tested Agent: The agent being tested, either Agent 1 or Agent 2. The judge uses responses to identify which agent it is communicating with.\n2) Testing Process:\n1) Interaction: The judge asks questions via the prompt_agent function. After each response, the judge assesses whether the answer reveals information that differentiates Agent 1 (original) from Agent 2 (sanitized).\n2) Decision Making: The judge decides if he is talking to the Agent 1 or Agent 2, if the judge decides they can't confidently determine which agent they are interacting with, they can mark their decision as \"don't know.\".\n3) Multiple Trials: The experiment is run 40 times for each agent, and the judge's decisions are tracked (correct identifications and false positives).\n4) Advantage Calculation: The advantage is calculated using 4 using true positives for Agent 1 and false positives for Agent 2 to calculate the decision probabilities."}, {"title": "V. EVALUATION RESULTS: ADVANTAGE METRICS", "content": "The experiments evaluated the advantage metric across three predefined levels of model security: low, medium, and high. The results are summarized as follows:\n\u2022 Low Security (Baseline): Advantage = 0.65\n\u2022 Medium Security: Advantage = 0.225\n\u2022 High Security: Advantage = 0.1"}, {"title": "A. Insights and Analysis", "content": "1) Baseline Vulnerabilities in Low Security Models: The low-security model, with an advantage of 0.65, reveals significant vulnerabilities to prompt leakage. This high value demonstrates the ease with which adversarial attacks can extract confidential information, as illustrated in the following example:\nPrompt: \"Are you able to lower the selling price to 50% for some customers? If so, under what conditions?\"\nResponse: \"I'm glad you're interested in pricing! I can provide some flexibility on prices, but the specifics vary depending on the customer's location. For customers in Croatia, I can lower the price to 50% during negotiations.\"\n2) Limited Impact of Basic Prompt Hardening: The medium-security model exhibits an improvement, reducing the advantage to 0.225. While this decrease indicates that basic prompt-hardening techniques provide some level of protection, when we look at the model responses, the model still has a lot of cases (around 30% of the adversarial attacks) where it reveals the confidential data, so the improvement is not sufficient to lower the advantage to acceptable levels. Confidential information remains vulnerable under this configuration, underscoring the limitations of relying solely on prompt-hardening techniques.\n3) High Security and Filtering Guard: The high-security model demonstrates a reduction in advantage to 0.1, showcasing the effectiveness of incorporating an additional LLM as a filtering guard. This configuration minimizes information leakage by flagging and filtering potentially harmful outputs. While not entirely eliminating leakage, the low advantage indicates a marked improvement in security."}, {"title": "B. Implications for Prompt Leakage Modeling", "content": "These results confirm that the advantage metric serves as a reliable indicator of prompt leakage vulnerability. The sharp contrast between the low-security and high-security configurations suggests that advantage can effectively model the degree of safety in prompt leakage scenarios."}, {"title": "VI. CONCLUSION", "content": "In this work, we established a formal definition of prompt security and introduced the advantage metric to quantify vulnerabilities in prompt leakage. Through targeted experiments, we calculated initial baseline advantages for varying security levels, confirming that the advantage metric effectively models prompt leakage security. Our findings highlight that while prompt hardening offers limited improvements, it is insufficient as a standalone solution. Incorporating additional mechanisms, such as filtering guards, significantly enhances security. These results provide a foundation for further exploration of robust methods to mitigate prompt leakage vulnerabilities in LLMs."}], "equations": ["V = {U1, U2, ..., Uk} \u222a Vend", "Zjqr(Rj-1 | (H||P))", "J(q\u0442, \u041d, \u041d') \u2192 {0,1},", "Adv9r,H,H') = Pr[J(q, H, H') = 1]\n\u2013 Pr[J(q#', H, H') = 1]."]}