{"title": "Synthetic Knowledge Ingestion: Towards Knowledge Refinement and Injection for Enhancing Large Language Models", "authors": ["Jiaxin Zhang", "Wendi Cui", "Yiran Huang", "Kamalika Das", "Sricharan Kumar"], "abstract": "Large language models (LLMs) are proficient in capturing factual knowledge across various domains. However, refining their capabilities on previously seen knowledge or integrating new knowledge from external sources remains a significant challenge. In this work, we propose a novel synthetic knowledge ingestion method called Ski, which leverages fine-grained synthesis, interleaved generation, and assemble augmentation strategies to construct high-quality data representations from raw knowledge sources. We then integrate Ski and its variations with three knowledge injection techniques: Retrieval Augmented Generation (RAG), Supervised Fine-tuning (SFT), and Continual Pre-training (CPT) to inject and refine knowledge in language models. Extensive empirical experiments are conducted on various question-answering tasks spanning finance, biomedicine, and open-generation domains to demonstrate that Ski significantly outperforms baseline methods by facilitating effective knowledge injection. We believe that our work is an important step towards enhancing the factual accuracy of LLM outputs by refining knowledge representation and injection capabilities.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) demonstrate proficiency in capturing vast amounts of factual information across a wide range of fields, attributable to their extensive pre-training datasets (Petroni et al., 2019; Cohen et al., 2023; Hu et al., 2023). Although these models hold an impressive repository of knowledge, integrating new information via external datasets or enhancing their capacity on previously seen information still poses several challenges. One primary challenge is outdated knowledge, where the static nature of the information fails to evolve over time. Another issue is the domain knowledge deficit where language models, typically generalists, lack detailed, specialized knowledge in sectors like finance (Wu et al., 2023) and healthcare (Singhal et al., 2023). Additionally, there is the problem of catastrophic forgetting, where language models may lose previously acquired knowledge (Luo et al., 2023), which particularly affects rare facts that are minimally represented in the training data (Kandpal et al., 2023). These issues underscore the necessity for ongoing enhancements to the knowledge capabilities.\nIn response to emerging challenges, the practice of customizing Large Language Models (LLMs) to specialized fields, coupled with regular updates to their knowledge base, is becoming increasingly prevalent (Yu et al., 2022; Wang et al., 2023). Various strategies have been developed to enhance the factual accuracy and domain-specific expertise of LLMs. These initiatives are crucial for integrating precise and current knowledge, thus broadening the utility and effectiveness of LLMs within professional settings. Despite these advances, the performance of LLMs often remains suboptimal, with issues such as hallucination (Manakul et al., 2023; Zhang et al., 2023a), especially in tasks that require extensive knowledge. Consequently, assessing the LLM's capability to understand, memorize, and retrieve knowledge is substantially essential. Given some knowledge in the form of a raw text corpus, our objective is to understand two key concepts and fundamental questions: (i) Knowledge injection: What is the optimal approach to encoding targeted knowledge into a language model to enhance its capability and functionality? and (ii) Knowledge ingestion: What are the most effective strategies for formatting knowledge or databases to ensure that such representations are readily assimilable and digestible by large language models (LLMs) prior to injection?\nA prevalent technique for injecting external information into model responses is Retrieval Augmented Generation (RAG) (Lewis et al., 2020). This method relies on external knowledge sources, yet its effectiveness is contingent upon the efficacy of its retrieval system (Chen et al., 2024). If this retrieval process is flawed, it might not secure the necessary information, resulting in inappropriate responses. Additionally, a misalignment in knowledge representation could hinder RAG's comprehension of the context (Gao et al., 2023). Another approach is Supervised Fine-Tuning (SFT) (Dettmers et al., 2024), which improves a model by continuing its training on relevant tasks. Instruction tuning (Zhang et al., 2023c; Taori et al., 2023) under SFT has shown substantial enhancements in language model performance, though it doesn't necessarily enrich the model's knowledge base (Ouyang et al., 2022; Chia et al., 2023; Zhou et al., 2024). Techniques in fine-tuning also include reinforcement learning (RL) strategies such as RLHF (Touvron et al., 2023; Achiam et al., 2023) and DPO (Rafailov et al., 2024), which refine the model's alignment post-training but do not expand its knowledge capacity. Hence, developing a precise supervised mechanism for raw knowledge implementation poses a significant challenge. A third strategy is Continual Pre-Training (CPT) (Ke et al., 2023), or unsupervised fine-tuning, where the model is further trained on specific knowledge datasets tailored to certain tasks or domains (Wu et al., 2024). While CPT does not require labeled data, structuring the data in a format that closely reflects the specific goals and tasks can be particularly effective. Recent studies have garnered attention for these three knowledge injection strategies (Ovadia et al., 2023; Balaguer et al., 2024; Mecklenburg et al., 2024), yet there remains a gap in knowledge ingestion. Given unprocessed and unstructured knowledge, two key questions arise: (i) What data representations are most effective for each injection strategy? (ii) How can we systematically construct diverse and high-quality representations to facilitate effective knowledge injection?\nInspired by this gap, we introduce Synthetic knowledge ingestion (Ski), an innovative synthetic data generation method that automates and enhances knowledge ingestion, as illustrated in Fig.1. Ski leverages three key innovations to generate high-quality and diverse data representation from a raw knowledge base. First, Fine-grained Synthesis creates hypothetical questions based on n-gram knowledge contexts, ensuring a detailed match in relevance, minimizing the semantic gap between questions and answers, and increasing representation diversity. Second, Interleaved Generation simultaneously generates both questions and answers based on specific knowledge. This synthetic question-answering (QA) format naturally mirrors the process of information-seeking, providing direct contextual alignment and relevance between the questions and their respective answers. Third, Assemble Augmentation combines fine-grained synthesis across different n-gram spans and their QA pair iterations, balancing repetition with diverse elements. By integrating these components, the Ski approach significantly enhances the refinement of knowledge from its raw state, thus facilitating effective knowledge ingestion into LLMs.\nSki offers a generic solution for generating synthetic data from raw knowledge, which can be integrated with all three knowledge injection strategies: RAG, SFT, and CPT. With this advancement, Ski not only incorporates new information using external datasets but also refines LLMs' capability on previously seen knowledge and information. We provide a comprehensive evaluation of Ski on two open-source LLMs, Llama2-7B (Touvron et al., 2023) and Mistral-7B (Jiang et al., 2023), over cross-domain QA tasks, including those in the finance, biomedical, open-generation and multi-hop domain. Through performing extensive experiments, our approach Ski achieves substantial improvements upon the baselines by a large margin. In summary, our contributions are threefold:"}, {"title": "Problem Formulation", "content": "Knowledge Ingestion. It generally refers to the methodology of acquiring, integrating, and transforming information from a variety of knowledge sources. It encompasses gathering, absorbing, and morphing knowledge to build a database that facilitates future use. Mathematically, let's M represent a language model, Q denote a set of factual questions, and assume that we only have access to relevant raw knowledge base $K_Q$. We aim to enhance the representation of the knowledge base, represented as $K_Q^*$\n\n$K^*_Q = T(K_Q),  T(\u00b7) : text \\rightarrow text$ \n\nwhere $T$ signifies a transformation process that converts raw knowledge into refined knowledge $K_Q \\rightarrow K^*_Q$. Although advanced knowledge representations such as knowledge graphs show promise, their discussion falls beyond the scope of this study.\nKnowledge Injection. It involves actively encoding or integrating specific knowledge into a pre-trained language model to enhance its performance by incorporating new information from external datasets or refining the model's capabilities on previously seen information. Specifically, when the refined knowledge applied improves the understanding of the pre-trained model M regarding questions Q through a transformation F,\n\n$M^* := F(M,K_Q)  s.t. S_{M^*,q} > S_{M,q}$ \n\nwhere S is a score metric, e.g., an accuracy that measures the performance of the pre-trained model in answering the questions Q, and $M^*$ is the updated language model. We explore three options for F: RAG, SFT, and CPT, and provide a systematic evaluation of how our proposed synthetic knowledge ingestion method, Ski, can enhance the capabilities of LLMs across various QA tasks."}, {"title": "Synthetic Knowledge Ingestion", "content": "The human learning process often involves asking questions and inquiring \u201cwhy\u201d, which can enhance comprehension. Drawing inspiration from this, Ski also leverages the power of questions. By utilizing three key innovations, Ski transforms raw knowledge (such as documents and articles) into question-augmented representations. The following section provides a detailed description of these three key innovations, as illustrated in Figure 2.\nFine-grained Synthesis\nConsidering the scenario where we possess a paragraph of text as a knowledge base, crafting effective questions to transform and augment this knowledge base poses a challenging task. Questions that are too broad may struggle to encompass all pertinent knowledge points, whereas excessively detailed questions risk losing sight of the overall content. Moreover, ensuring that these questions are both non-repetitive and diverse adds extra challenges.\nTo tackle these challenges, we introduce a fine-grained synthesis approach, inspired by n-gram language models, allowing for a balanced incorporation of both detailed and overarching content. Assume the knowledge base $K_Q$ paragraph consists of m sentences, $K_Q = \\{k_1,k_2, ..., k_m\\}$ where $k_i$ means the i-th sentence in $K_Q$, we generate hypothetical questions by querying an LLM model, given a specific set of sentences conditioning on the whole knowledge paragraph $K_Q$:\n\n$L_{Pfs}(\\{k_j, k_{j+1}, ..., k_{j+n-1}\\} ; K_Q)$ \n\nwhere L is a pioneer LLM model with meta-prompt $P_{fs}$, $q_j^n$ is the j-th (1 \u2264 j \u2264 m \u2212 n + 1) generated hypothetical questions given the specific sentence set $\\{k_j, k_{j+1}, ..., k_{j+n-1}\\}$ and the raw knowledge context $K_Q$. We denote $Q^n$ as the set of n-gram hypothetical questions: $Q^n = \\{q^n_1, q^n_2, q^n_3, ..., q^n_{m-n}\\}$.\nWe define a set of n-gram knowledge context $C^n$\n\n$C^n =\\{\\{k_1,..., k_n \\}, ..., \\{k_{m-n+1},k_m\\}\\}$ \n\nand therefore have $C^1 = \\{k_1,k_2, ...,k_m\\}$, $C^2 = \\{\\{k_1,k_2\\}, \\{k_2, k_3\\},..., \\{k_{m-1}, k_m\\}\\}$, etc. The question-context pair $\\{Q^n, C^n\\}$ can be written by:\n\n$\\{Q^n, C^n\\} =\\{\\{q^n_1, \\{k_1, ..., k_n \\} \\}, ..., \\{q^n_{m-n}, \\{k_{m-n+1},k_m\\} \\}\\}$ \n\nSpecifically, we have two variants of Ski leveraging n-gram synthesis principle:\n\u2022 Ski-Q-n: synthetic hypothetical questions $Q^n$ by n-gram given knowledge contexts $C^n$\n\u2022 Ski-QC-n: synthetic hypothetical questions with knowledge context pair $\\{Q^n, C^n\\}$ by n-gram\nwhere n typically ranges from 1 to 3. Specifically, Ski-Q-1 refers synthetic 1-gram questions over m sentences, $Q^1 = \\{q^1_1, q^1_2, ..., q^1_m\\}$, and Ski-QC-1 means synthetic 1-gram question-context (QC) pair $\\{Q^1, C^1\\} = \\{\\{q^1_1, k_1\\},..., \\{q^1_m, k_m\\}\\}$ that includes m pairs in this set.\nInterleaved Generation\nHypothetical question and context pairs have the potential to transform knowledge articles into questions (Ski-Q-n) and question-context (QC) pairs (Ski-QC-n), which can revolutionize representations of knowledge articles. However, SFT requires succinct QA pairs, while the context portion of the QC pair consists of sentences, which are too lengthy to be formulated as answers.\nTo deal with this issue, we introduce an interleaved generation strategy that simultaneously generates QA pairs based on a specific knowledge context. This synthetic QA format naturally emulates the information-seeking process, providing direct contextual alignment and relevance between the questions and their corresponding answers. This strategy can be built upon hypothetical questions yet delivers QA pairs tailored to the specific knowledge context $K_Q$:\n\n$\\{q^n_j,a^n_j\\}L_{Pig}(\\{k_j, ..., k_{j+n-1}\\} ; K_Q)$ \n\nwhere $P_{ig}$ is the meta-prompt (see details in Appendix) different from the prompt $P_{fs}$ used above. We denote $A^n$ as the set of answers corresponding to the n-gram hypothetical questions: $A^n = \\{a^n_1, a^n_2, a^n_3, ..., a^n_{m-n}\\}$. The question-answering pairs $\\{Q^n, A^n\\}$ can be written by:\n\n$\\{Q^n, A^n\\} = \\{\\{q^n_1, a^n_1\\},..., \\{q^n_{m-n},a^n_{m-n}\\}\\}$ \n\nThe synthetic questions $Q^n$ and corresponding answers $A^n$ align well with the given knowledge context $C^n$ due to interleaved mechanism. We thus denote the following Ski variants from the perspective of the QA pair:\n\u2022 Ski-QA-n: synthetic QA pair $\\{Q^n, A^n\\}$ by n-gram given specific knowledge contexts $C^n$\n\u2022 Ski-QCA-n: synthetic QA pair combined with the knowledge context, $\\{Q^n, C^n, A^n\\}$"}, {"title": "Assemble Augmentation", "content": "To enhance the effectiveness of knowledge ingestion, we propose a strategic assembly approach that harnesses the benefits of fine-grained and interleaved generation. This strategy is built upon two main pillars: (1) article/document augmentation for optimized retrieval, and (2) data augmentation for both supervised and unsupervised fine-tuning. Specifically, we first combine all question-context pairs from the same document into a singular article, thereby improving the retrieval quality\n\n$[QC]^n =\\{\\{q^n_1, \\{k_1, ..., k_n\\} \\} \\Join \\{q^n_2, \\{k_2, ..., k_{n+1}\\}\\}\\, ...\\, \\Join \\{q^n_{m-n}, \\{k_{m-n+1},k_m\\}\\}\\}$ \n\nwhere denotes the concatenation operator, which combines all QC pair strings into a single article. The notation $[QC]^n$ represents the augmented article composed of all QC pairs using n-gram techniques. As the same information can exist in different formats under different n-gram strategies, combining pairs from different n-gram allows repetition and diversity, enhancing the depth and breadth of knowledge integration.\nAssemble augmentation also offers scalability suitable for synthesizing expansive sets of question-answer pairs utilizing various n-gram generation techniques. For fine-tuning schemes, this strategy can be employed to generate a diverse ensemble of n-gram QA pairs for data augmentation purposes. Specifically, the QA assembly aggregates all n-gram QA pairs as follows:\n\n$[QA]^n = \\{\\{Q^1, A^1\\} \\cup ... \\cup \\{Q^n, A^n\\}\\}$ \n\nwhere $[QA]^n$ is a combination of QA pairs from 1-gram to n-gram. Similarly, we have the ensemble of QCA pairs from 1-gram to n-gram:\n\n$[QCA]^n = \\{\\{Q^1, C^1, A^1\\} \\cup ... \\cup \\{Q^n, C^n, A^n\\}\\}$ \n\nwhere $cup$ denotes the union of sets, which refers to containing all the elements in a large set. We thus summarize three Ski variants from the perspective of assemble that typically performs article augmentation (for retrieval) or data augmentation:\n\u2022 Ski-QC-ASM: assemble of all question-context pairs into one augmented set $[QC]^n$\n\u2022 Ski-QA-ASM: assemble of all question-answer pairs from 1-gram to n-gram $[QA]^n$\n\u2022 Ski-QCA-ASM: assemble of all question-context-answer pairs from 1-gram to n-gram $[QCA]^n$"}, {"title": "Knowledge Injection for Enhancing Language Models", "content": "Retrieval Augmented Generation (RAG)\nWe employ three variations of the Ski approach, namely Ski-Q-n, Ski-QC-n, and Ski-QC-ASM, when utilizing the RAG pipeline for knowledge injection. At the retrieval stage, Ski-Q-n matches the query's embedding with embeddings of all questions generated in an n-gram pattern from the knowledge documents. This can be interpreted as an inverse HyDE implementation, where HyDE (Gao et al., 2022a) transfers a query to a potential answer and performs an answer-to-knowledge search. For Ski-Q-n we transform knowledge into questions and execute a question-to-question (Q-Q) search. In contrast, Ski-QC-n compares the query's embedding with embeddings of all question-context (QC) pairs, facilitating a question-to-(question+context) (Q-QC) search. Additionally, Ski-QC-n introduces another variation, Ski-QC-ASM, where QC pairs from the same document are amalgamated into a single article, and the query's embedding is compared with embeddings of such articles. After identifying the most relevant items, Ski consolidates only the most pertinent snippets or sentences from the knowledge documents related to the retrieved items into an LLM for generating the final answer. This process minimizes noise from sentences in the same documents but not directly pertinent to user queries.\nSupervised Fine-tuning (SFT)\nWe utilize three variants of the Ski method, namely question-answer (QA) pairs, question-context (QC) pairs, and question-context-answer (QCA) pairs, to incorporate knowledge via supervised fine-tuning. In the QCA approach, we concatenate the question (Q) and context (C) input and use the answer as the output, following the setting in Viswanathan et al. (2023). To conform to the n-gram principle, we create separate training datasets for every n-gram (n=1,2,3) for SFT, resulting in a decrease in dataset size as n increases. We focus on evaluating three fine-grained generations (n=1), i.e., Ski-QA-1, Ski-QC-1, and Ski-QCA-1.\nContinual Pre-training (CPT)\nContinual pre-training can leverage the dataset used for SFT, but it typically requires a larger volume of data to enhance the capabilities of LLMs (Lin et al., 2024; Ke et al., 2023). To address this, we amalgamate all the generated pairs from SFT to create a comprehensive augmented training set, Ski-QC-ASM, Ski-QA-ASM, Ski-QCA-ASM. This strategy not only amplifies repetition but also preserves diversity (Ovadia et al., 2023)."}, {"title": "Experiments", "content": "Experiment Setup\nDatasets. We evaluate our approach on multiple cross-domain question-answering tasks, including those in the finance domain (e.g., FiQA (Maia et al., 2018)), biomedical domain (e.g., BioASQ (Tsatsaronis et al., 2015)), open-generation domain (e.g., NQ (Kwiatkowski et al., 2019) and multi-hop domain (e.g., HotpotQA (Yang et al., 2018)). Please find more relevant details about data usage for RAG, SFT, and CPT in the Appendix A.3-A.4.\nEvaluation Models and Metrics. We utilize GPT-3.5-turbo from OpenAI as our target agent model to generate synthetic questions, answers, and their corresponding pairs. To evaluate our approach, we employ two open-source LLMs, Llama2-7B (Touvron et al., 2023) and Mistral-7B (Jiang et al., 2023), which serve as the base models for RAG, SFT and CPT, implemented using the LLaMA-Factory pipeline. Our evaluation encompasses a comparison between two retrieval methods: the Dense-Retrieval approach, Contriever (Izacard et al., 2021), and the Sparse-Retrieval method, BM25. We assess the performance of three generators, GPT-3.5-turbo, Llama2-7B, and Mistral-7B using the F1 score as a measure of accuracy, following the RAGGED (Hsia et al., 2024) pipeline across RAG, SFT, and CPT scenarios.\nBaseline Methods. For retrieval and RAG tasks, we utilize raw knowledge \u2013 specifically, the original article as a standard baseline. Our approach draws inspiration from Hypothetical Document Embeddings, known as HyDE (Gao et al., 2022b). We introduce an adaptation termed \"inverse\" HyDE (iHyDE) (Gao et al., 2023) established by Ski-Q-1. In SFT evaluations, the base model without fine-tuning serves as the standard baseline. We also employ a vanilla QA baseline, which generates a set of question-answer pairs by querying LLMs using extracted documents/articles, as outlined in (Mecklenburg et al., 2024; Balaguer et al., 2024). For the CPT pipeline, our standard baselines encompass the base model and raw context, which is"}, {"title": "Main Results", "content": "RAG Results. We evaluate the retrieval performance of Ski using two different retrievers, as illustrated in Table 1. For Contriever, Ski-QC-1 outperformed both the inverse HyDE (Ski-Q-1) and the raw article baselines, particularly achieving a significant improvement (+15%) on the FiQA task. For BM25, Ski-QC-ASM surpassed all the baselines and Ski variations across three datasets. Further evaluations were conducted on our method within RAG systems, detailed in Table 2. Inverse HyDE fell short of the raw article baseline, indicating that the question-to-question search approach might not be as effective as anticipated. Ski-QC-ASM emerged as the superior method, showing consistent improvements compared to other baselines across all tasks. The results of BM25 are aligned with the retrieval-only task, confirming the effectiveness of assemble augmentation.\nSFT Results. Beyond the RAG evaluation, we assess the enhancement of pre-trained models using SFT in our study with Ski. The synthetic QA pairs, Ski-QA-1, exhibit superior performance across three datasets. Specifically, Mistral-7B achieves an average performance gain of 5.13% over the base model and 4.07% improvement compared to the vanilla QA baseline, while Llama2-7B markedly improves by 19.3% and 11.1% respectively. Generally, the data representation provided by Ski-QA-1 is the more effective method. We also observe that Llama2-7B demonstrates a stronger capability in refining knowledge compared to Mistral-7B, aligning with findings reported by Ovadia et al. (2023).\nCPT Results. Llama2-7B demonstrates a superior capability for knowledge injection compared to Mistral-7B in the SFT evaluation. Building on this observation, we further explore the impact of Ski through the CPT approach with Llama2-7B. While QA pairs excel in SFT, Ski-QC-ASM and Ski-QCA-ASM show significantly enhanced performance in the CPT evaluation. Specifically, these methods lead to performance gains of +21.2% in BioASQ and +11.5% in NQ task relative to the base model. Ski-C-ASM presents a slight improvement over other QC and QCA variations in HotpotQA. This highlights the benefit of assembly augmentation where longer contexts, which include more comprehensive information than short answers, aid in deepening the pretrained models' understanding.\nHolistic Comparison of RAG, SFT, and CPT.\nFig.5 presents a comprehensive comparison of the Ski method across various knowledge injection pipelines. While RAG demonstrates marked improvement over the raw model and baselines, the gains from CPT are relatively modest. Despite this, Llama2-7B consistently shows significant improvements over Mistral-7B across RAG, SFT, and CPT. GPT-3.5-turbo is utilized solely as the generator for RAG, where it exhibits relatively low-performance gains due to its already high base performance. Our Ski method appears promising in enhancing the RAG performance of open-source pre-trained models, suggesting that Llama2-7B could be an effective candidate for knowledge injection, potentially achieving competitive gains similar to RAG when applied through SFT and CPT."}, {"title": "Related Work", "content": "Incorporating new information into LLMs or refining their capabilities with previously seen information has posed a substantial challenge (Chen et al., 2022; Martino et al., 2023; Zhang et al., 2023d; Ye et al., 2023; Zhang et al., 2023b), the lack of effective approaches and evaluations is a notable concern. Recently, Xu et al. (2023) explored continued pre-training with a knowledge infill objective to enhance fact retention, while Tian et al. (2023) applied constrained optimization through direct preference optimization (Rafailov et al., 2024) to reinforce factuality during post-training. Additionally, Ovadia et al. (2023) aimed to compare retrieval, fine-tuning, and their combinations but the optimal way to inject knowledge into LLMs remains an open and sometimes contentious debate.\nHowever, a critical knowledge ingestion gap remains: what are the data representations that work best for each injection pipeline with unprocessed and unstructured knowledge? (Baek et al., 2024) To answer this, Balaguer et al. (2024) extracts information from raw PDF documents, leading to questions and answers for supervised fine-tuning pipelines. Mecklenburg et al. (2024) presents two data generation processes, token-based and fact-based, to inject knowledge via SFT. In contrast, our study aims to develop an innovative and generic synthetic knowledge ingestion approach, Ski, which achieves significant improvement for each knowledge injection pipeline."}, {"title": "Conclusions", "content": "We propose a novel method for refining knowledge representation and injection to enhance LLMs. Through extensive empirical analysis, our work has revealed several important findings. Firstly, synthetic QA generations are highly effective in digesting knowledge from its raw format, resulting in consistent enhancement for each injection pipeline. Secondly, fine-grained assemble augmentation has proven to be proficient in constructing high-quality, diverse datasets for knowledge injection. Lastly, RAG shows the potential to enhance knowledge, whereas the advancement of SFT and CPT largely depends on selecting appropriate base models and leveraging their existing knowledge capabilities."}, {"title": "Ethics Statement", "content": "This paper studies knowledge ingestion and representation in LLMs, which has significant broader impacts on the field of natural language processing and helps to address ethical considerations regarding factuality and reliability. The research outcome may contribute to the development of more accurate and factual LLMs by mitigating the risks of misinformation and biased outputs and promoting accountability and trust in AI systems."}, {"title": "Limitations", "content": "Our current experiments focus on tasks in a question-answering setting which requires a knowledge base. Further research is needed to expand to more complex tasks such as conversational or dialogue-based prompting. Also, the quality of the results introduced by Ski depends on the quality of the knowledge base. For OOD questions or edge cases where the knowledge base is corrupted or provides conflicting information, Ski is not able to solve that. Ski does require LLM calls to generate different representations. Such jobs can be conducted offline and only once. Thus the cost will be relatively reduced as it serves more queries. In addition, although advanced knowledge representations such as knowledge graphs show promise, their discussion falls beyond the scope of the current study but will appear in our future work."}]}