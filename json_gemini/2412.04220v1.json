{"title": "Customize Segment Anything Model for Multi-Modal Semantic Segmentation with Mixture of LoRA Experts", "authors": ["Chenyang Zhu", "Bin Xiao", "Lin Shi", "Shoukun Xu", "Xu Zheng"], "abstract": "The recent Segment Anything Model (SAM) represents a significant breakthrough in scaling segmentation models, delivering strong performance across various downstream applications in the RGB modality. However, directly applying SAM to emerging visual modalities, such as depth and event data results in suboptimal performance in multi-modal segmentation tasks. In this paper, we make the first attempt to adapt SAM for multi-modal semantic segmentation by proposing a Mixture of Low-Rank Adaptation Experts (MoE-LoRA) tailored for different input visual modalities. By training only the MoE-LoRA layers while keeping SAM's weights frozen, SAM's strong generalization and segmentation capabilities can be preserved for downstream tasks. Specifically, to address cross-modal inconsistencies, we propose a novel MoE routing strategy that adaptively generates weighted features across modalities, enhancing multi-modal feature integration. Additionally, we incorporate multi-scale feature extraction and fusion by adapting SAM's segmentation head and introducing an auxiliary segmentation head to combine multi-scale features for improved segmentation performance effectively. Extensive experiments were conducted on three multi-modal benchmarks: DELIVER, MUSES, and MCubeS. The results consistently demonstrate that the proposed method significantly outperforms state-of-the-art approaches across diverse scenarios. Notably, under the particularly challenging condition of missing modalities, our approach exhibits a substantial performance gain, achieving an improvement of 32.15% compared to existing methods.", "sections": [{"title": "I. INTRODUCTION", "content": "Accurate segmentation of diverse objects is pivotal for various scene understanding applications, including robotic perception, autonomous driving, and AR/VR [1], [2]. The Segment Anything Model (SAM) [3] represents a groundbreaking advancement in instance segmentation, particularly for RGB images. Trained on an extensive dataset of 11 million high-resolution images and over 1 billion annotated segmentation masks, SAM achieves exceptional zero-shot segmentation performance, enabling its application across diverse domains such as medical imaging, remote sensing, and more [4]-[7].\nWhile SAM has revolutionized single-modality segmentation tasks, particularly for RGB images, its application to multi-modal segmentation presents unique challenges. Emerging domains often require integrating diverse modalities such as depth and event data, which capture complementary scene information but exhibit distinct characteristics from RGB data. Furthermore, the recently proposed SAM2 model [8] incorporates temporal dimensions for video segmentation, addressing additional complexities such as motion, deformation, occlusion, and lighting variations. These advancements extend SAM's applicability to dynamic and multi-modal environments, but integrating cross-modal information while preserving SAM's generalization capabilities remains under-explored.\nDespite its success in single-modality segmentation, extending SAM to multi-modal semantic segmentation poses significant challenges. Each modality, such as LiDAR, radar, and event cameras, exhibits distinct spatial, temporal, and noise characteristics, complicating their seamless integration into SAM's architecture [9]. SAM's pre-trained features, optimized for RGB images, often result in suboptimal performance when directly applied to heterogeneous multi-modal data. Real-world scenarios further complicate this integration, as missing or unreliable modalities can degrade performance, and SAM lacks mechanisms to adaptively handle incomplete inputs [10]\u2013[12]. Additionally, effective multi-modal fusion requires advanced techniques to align, weigh, and integrate inputs while preserving the complementary strengths of each modality. Achieving robust fusion requires addressing several challenges, including mitigating modality-specific noise, harmonizing discrepancies in spatial and temporal resolutions, and balancing the contributions of each input modality [13].\nIn this work, we present a novel framework that extends SAM2's functionality to support multi-modal semantic segmentation. As shown in Figure 1(a), our approach incorporates Low-Rank Adaptation (LoRA) modules designed for each modality, facilitating efficient modality-specific fine-tuning while preserving the generalization capabilities of SAM2's pre-trained image encoder. To address the inherent challenges of multi-modal fusion, we develop a Mixture of LoRA Experts (MLE) routing mechanism that adaptively generates weighted feature representations, ensuring effective integration across modalities and mitigating inconsistencies caused by noise or missing inputs. Meanwhile, we enhance the SAM2 segmentation pipeline by incorporating multi-scale feature extraction and fusion mechanisms. Specifically, we augment the original segmentation head with an auxiliary head designed to exploit complementary information across multiple scales, leading to improved segmentation accuracy.\nExtensive experiments conducted on benchmark datasets, including DELIVER [13], MUSES [10], and MCubeS [14], demonstrate the superior performance of our framework in multi-modal semantic segmentation tasks. As illustrated in Figure 1(b) and (c), our approach achieves a significant improvement of +4.9% on the DELIVER dataset with four modalities and +28.14% on the MUSES dataset with three modalities, compared to state-of-the-art methods. Detailed ablation studies confirm the individual contributions of each module to the overall performance. Furthermore, additional experiments under challenging conditions, such as noisy or missing modalities, highlight the robustness and adaptability of the proposed model, emphasizing its practical utility in real-world scenarios. Notably, as shown in Figure 1(d) and (e), our model achieves a performance gain of 14.13% on the DELIVER dataset and 32.15% on the MUSES dataset in these adverse settings, further establishing its efficacy and reliability.\nOur contributions are outlined as follows: (I) We improve the SAM2 framework by integrating a MoE mechanism with LORA modules for multi-modal semantic segmentation tasks. This design enables efficient modality-specific adaptation by training distinct LoRA modules for each modality and leveraging a dynamic routing mechanism to integrate features across modalities effectively. (II) We redesign the SAM2 segmentation pipeline by incorporating a modified segmentation head tailored for multi-modal input and introducing an auxiliary segmentation head. This configuration facilitates the effective fusion of multi-scale features, significantly improving segmentation accuracy. (III) Our method achieves state-of-the-art performance on three widely-used multi-modal benchmarks, ranging from synthetic to real-world scenarios, surpassing existing methods in terms of segmentation accuracy and generalization across diverse modalities. (IV) Extensive experimental evaluation demonstrates the robustness of the proposed framework under challenging conditions, including missing modalities and high levels of noise. The results highlight its adaptability and reliability for real-world applications."}, {"title": "II. RELATED WORK", "content": "Multi-modal semantic segmentation seeks to leverage complementary information from multiple sensing modalities, such as RGB, depth, and thermal data, to assign semantic labels to each pixel, thereby improving the accuracy and robustness of scene understanding [15]. This task is predominantly addressed using encoder-decoder architectures, where the encoder extracts hierarchical features, and the decoder reconstructs pixel-level predictions [16]\u2013[18].\nThe evolution of encoders has been significantly influenced by Fully Convolutional Networks (FCNs), which enable end-to-end learning for pixel-level predictions [19], [20]. Notable advancements in FCNs include the introduction of dilated convolutions to expand the receptive field [21], [22] and pyramid pooling modules to incorporate multi-scale contextual information [23]. DeepLab further refined these methods by combining atrous convolutions with fully connected conditional random fields to enhance segmentation boundaries and accuracy [24]. However, FCNs face challenges in capturing long-range dependencies, which are essential for understanding complex scenes. Transformer-based encoders address this limitation by employing self-attention mechanisms to model global context effectively [25]\u2013[31]. Moreover, transformer-based decoders integrate robust multi-level context mining and process diverse multi-scale features extracted by the encoder, enabling precise and efficient segmentation, particularly in complex or high-resolution images [32]\u2013[35].\nCombining information from different modalities enhances scene understanding in multi-modal segmentation, especially in challenging environments where a single modality may be insufficient. Early fusion strategies combine data from all modalities at the input level, allowing the encoder to learn joint representations but risking redundancy or noise in the fused input [36]\u2013[38]. In contrast, late fusion methods process"}, {"title": "III. METHODOLOGY", "content": "Segment Anything Model. The SAM2 architecture is a transformer-based framework [61] developed for instance segmentation, integrating three key components: a hierarchical backbone, a Feature Pyramid Network (FPN)-based neck, and a mask decoder. The hierarchical backbone adopts the Hiera architecture [62] as a multi-scale feature extractor, embedding input images into high-dimensional feature spaces via a patch embedding mechanism. This backbone processes features hierarchically, doubling their dimensionality and reducing spatial resolution at each stage. These transformations leverage a combination of window-based multi-head self-attention and pooling operations, enabling the model to capture spatial and semantic relationships across varying scales. The FPN-based neck refines and consolidates these features by aligning feature dimensions from different stages, producing a unified multi-scale representation. Through its lateral connections and top-down pathways, the FPN merges fine-grained details from shallow layers with high-level semantic information from deeper layers. A sine-based positional encoding is incorporated to encode spatial relationships, enhancing the fused features for precise mask generation. The mask decoder employs transformer-based cross-attention with learnable mask tokens that iteratively interact with the fused features and positional encodings. These tokens are refined across multiple layers of cross-attention and feedforward operations. An upscaling module ensures that the final segmentation masks are high-quality and fine-grained. Moreover, the decoder's ability to output multiple masks allows it to disambiguate overlapping regions and effectively handle complex scenes.\nBuilding on the SAM2 framework, we propose a customized SAM2 architecture, namely MLE-SAM framework, designed explicitly for multi-modal semantic segmentation task, as illustrated in Figure 2. This customization begins by freezing the pre-trained image encoder and fine-tuning it with LoRA layers, efficiently adapting the model to new visual modalities while preserving its intensive pre-trained knowledge. The image encoder processes input visual modalities $X$ to generate Semantic Feature Map (SFM) $Y_m$, which are further transformed by the mask decoder's convolutional module into two additional feature pyramids: a Fine-grained Feature Pyramid (FFP) $Y^0_m$ and an Intermediate-resolution Feature Pyramid (IFP) $Y^1_m$. These feature pyramids and the SFM enhance the model's spatial and semantic representation capabilities.\nTo achieve an integrated feature representation, we propose a framework that combines the SFM, FFP, and IFP by averaging these representations across modalities to derive the integrated feature $Y_i$, where $i \\in \\{0,1,n\\}$. To further refine this integration, a selective top-k mechanism is employed, generating weighted feature maps $\\hat{Y}$ that prioritize salient information for each index i. These refined features, $Y_i$ and $\\hat{Y}_i$, are subsequently fused into a unified feature representation $Y_i$, forming the input for downstream semantic segmentation.\nThe unified feature $\\tilde{Y}$ is processed using a dual-pathway mask prediction strategy to enhance segmentation accuracy. In the first pathway, the fused features are fed into the SAM2 mask decoder, which utilizes a frozen transformer block to extract mask tokens from the SFM. These tokens interact with the fine-grained and intermediate-resolution pyramids to construct a high-resolution feature representation. This representation is further refined by a hypernetwork to produce precise segmentation masks, denoted as $\\tilde{S_0}$.\nIn the second pathway, the fused features are processed by an auxiliary segmentation head comprising three Multi-Layer Perceptrons (MLPs) and a series of upscaling layers. The outputs of this pathway are concatenated, passed through dropout layers to prevent overfitting, and fused linearly to predict an alternative set of high-resolution masks, $S_1$. The final segmentation output is derived by combining the predictions from both pathways, leveraging their complementary strengths. This dual-pathway design effectively addresses the challenges posed by multi-modal data distributions and diverse feature scales, ensuring robust and accurate semantic segmentation across multiple modalities.", "subsections": [{"title": "C. Hierarchical Multi-Modal Feature Extraction with LoRA", "content": "Give the input set for M modalities $X = \\{X_m \\in \\mathbb{R}^{H\\times W\\times C} | m \\in [1, M]\\}$, where H, W, and C represent the height, width, and number of channels of each modality, respectively. The index m denotes a specific modality, such as RGB, depth, LiDAR, or event camera. Each modality is processed independently through the hierarchical backbone network of Hiera to extract multi-scale features.\nInitially, a patch embedding operation transforms each input $X_m$ into an embedded feature map $P(X_m) \\in \\mathbb{R}^{H_0\\times W_0\\times d}$ as shown in Eq. (1), where $W_e \\in \\mathbb{R}^{C\\times d}$ is a weight matrix, $b_e \\in \\mathbb{R}^{d}$ is a bias vector, d is the dimensionality of the feature embedding, and $H_0 = H/s_0, W_0 = W/s_0$ denote the down-sampled height and width after applying a down-sampling factor $s_0$.\n$$P(X_m) = X_mW_e + b_e$$ (1)\nThe backbone of SAM2 progressively reduces spatial resolution while increasing feature dimensionality over n stages, producing multi-scale feature maps as defined in Eq. (2), where $H_i = H/s_i, W_i = W/s_i$, and $s_i = 2^{i+2}$ defines the down-sampling factor at stage i. The number of channels at stage i is denoted by $C_i$.\n$$\\{X_m^i \\in \\mathbb{R}^{C_i\\times H_i\\times W_i} | i \\in [0, n], m \\in [1, M]\\}$$\n(2)\nEach stage employs window-based multi-head self-attention to extract features, as shown in Eq. (3), where Q, K, and V are"}, {"title": "Attention", "content": "the query, key, and value matrices, $d_k$ is the dimensionality of the key matrix, and softmax applies along the last dimension.\n$$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$\n(3)\nTo enhance efficiency and modality-specific adaptation, we introduce a LoRA layer to update the query and value projections, as shown in Eq. (4), where $W_q, W_v \\in \\mathbb{R}^{d\\times r}$ and $W^\\top_q, W^\\top_v \\in \\mathbb{R}^{r\\times d}$ are low-rank matrices with $r < d$ as the rank parameter. These updates yield augmented projections, as defined in Eq. (5). LoRA parameters are modality-specific and trained independently while freezing the backbone parameters, ensuring efficient cross-modal adaptation.\n$$\\triangle Q_m = W_q^\\top W^q_m, \\qquad \\triangle V_m = W_v^\\top W^v_m$$\n(4)\n$$Q'_m = Q_m + \\triangle Q_m, \\qquad V'_m = V_m + \\triangle V_m$$\n(5)\nHierarchical features are refined using an FPN, which integrates lateral and top-down pathways to enhance diverse multi-scale features. At each stage i, the input feature map $X^i_m$ undergoes a precise lateral convolution operation, yielding a refined modality-specific feature map $Z^i_m \\in \\mathbb{R}^{d\\times H_i\\times W_i}$. This operation reduces the channel dimensionality to d while preserving the essential spatial dimensions $H_i$ and $W_i$, ensuring robust consistency in spatial resolution and compatibility for subsequent fusion operations within the FPN.\nLet L denote the set of layers where top-down fusion is applied. For each layer $i \\in L$, top-down fusion combines feature representations from deeper layers with those at the current stage, producing the fused feature map $Y^i_m$. This fusion process is mathematically defined in Eq. (6).\n$$Y^i_m = \\begin{cases}\nZ^i_m + Upsample(Y^{i+1}_m) &, i \\in L \\\\\nZ^i_m &, i \\notin L.\n\\end{cases}$$\n(6)\nHere, $Y^i_m \\in \\mathbb{R}^{d\\times H_i\\times W_i}$ represents the fused feature map at stage i, integrating modality-specific features $Z^i_m$ with the upsampled features from the subsequent layer $Y^{i+1}_m$. The Upsample operation adjusts the spatial resolution of $Y^{i+1}_m$ to match that of $Z^i_m$, ensuring accurate integration. The hierarchical refinement that underlies the multi-scale feature representation of the FPN is central to this fusion process."}, {"title": "D. Dynamic Multi-Modal Feature Fusion with MoE and Routing Mechanisms", "content": "The FPN is employed to generate three distinct feature maps for each modality, designed to capture semantic and spatial information at multiple different resolutions: the SFM ($Y^n_m \\in \\mathbb{R}^{d\\times H_n\\times W_n}$), the FFP ($Y^0_m \\in \\mathbb{R}^{d\\times H_0\\times W_0}$), and the IFP ($Y^1_m \\in \\mathbb{R}^{d\\times H_1\\times W_1}$). To improve the overall representational capacity of the finer-resolution feature maps ($Y^0_m$ and $Y^1_m$), 1x1 convolutional layers are applied to reduce their channel dimensions while preserving spatial resolution. Following these operations, the dimensions are transformed such that $Y^0_m \\in \\mathbb{R}^{d/8\\times H_0\\times W_0}$ and $Y^1_m \\in \\mathbb{R}^{d/4\\times H_1\\times W_1}$, ensuring a compact and efficient representation suitable for subsequent fusion and effective analysis.\nTo aggregate features across modalities, the integrated feature map $\\bar{Y_i}$ for $i \\in \\{0, 1, n\\}$ is computed by averaging the features across all modalities, as shown in Eq. (7).\n$$\\bar{Y_i} = \\frac{1}{M}\\sum_{m=1}^M Y^i_m, \\quad i \\in \\{0, 1, n\\}$$\n(7)\nwhere $Y^i_m$ denotes the feature map for modality m at pyramid level i. This operation ensures uniform aggregation, capturing a holistic representation of multi-modal features. However, the equal-weight assumption in $\\bar{Y_i}$ may be suboptimal when certain modalities are more informative than others. To address this limitation, a MoE mechanism is introduced to assign dynamic weights to features based on their relevance, enabling the model to prioritize significant features while attenuating irrelevant information.\nFor the cross-modal routing procedure, spatially averaged embeddings $f^i_m$ are computed for each modality and feature level as compact representations of spatial information. These embeddings, defined in Eq. (8), are derived by averaging spatial features over height $H_i$ and width $W_i$. Here $Y^i_m (h, w)$ represents the feature map of modality m at spatial location (h, w) for level i.\n$$f^i_m = \\frac{1}{H_i W_i}\\sum_{h=1}^{H_i}\\sum_{w=1}^{W_i} Y^i_m (h, w), \\quad i \\in \\{0, 1, n\\}$$\n(8)\nRouting weights $w^i_m$, which quantify the importance of each modality for feature integration, are calculated using a linear transformation followed by an activation function $\\sigma$, as described in Eq. (9), where $W_i \\in \\mathbb{R}^{D\\times d}$ is the weight matrix, $b_i \\in \\mathbb{R}^{D}$ is the bias term, and $\\sigma$ represents a softmax function to ensure proper normalization of the routing weights.\n$$w^i_m = \\sigma (W_i.f^i_m + b_i), \\quad i \\in \\{0, 1, n\\}$$\n(9)\nThe routing mechanism dynamically selects features from the most relevant modalities based on their routing weights. For each feature level i, the top-k modalities with the highest routing weights $w^i_m$ are identified. This ensures that only the most significant modalities contribute to the final feature representation. The fused feature map $\\hat{Y_i}$ is then computed as Eq. (10), where Top-k selects the weights corresponding to the top-k modalities, $\\odot$ denotes element-wise multiplication, and $Y^i_m$ represents the feature map of modality m at level i.\n$$\\hat{Y_i} = \\sum_{m=1}^M Top-k(w^i_1, ..., w^i_M) \\odot Y^i_m, \\quad i \\in \\{0, 1, n\\}$$\n(10)\nThis fusion strategy enables the model to effectively adjust the contribution of each modality, integrating both global information and modality-specific nuances into a cohesive feature representation. By prioritizing the most relevant modalities for each feature level, the approach enhances the model's capacity to handle multi-modal data and capture complementary information across modalities.\nBy combining $\\bar{Y}$ and $\\hat{Y}$ to the unified feature map $\\tilde{Y}$, the proposed framework effectively balances uniform aggregation"}, {"title": "E. Adapted Mask Decoder with Auxiliary Segmentation Head", "content": "Next, we employ a dual-pathway mask prediction strategy on the unified feature map $\\tilde{Y}$ to generate high-resolution segmentation masks.\nIn the first pathway shown in Figure 3, we extend SAM2's mask decoder to produce high-resolution multimasks. This involves generating high-resolution segmentation logits, denoted as $S_0 \\in \\mathbb{R}^{C\\times H_0\\times W_0}$, through a structured multi-scale fusion process. Here, C represents the number of segmentation categories. The backbone features $Y_n \\in \\mathbb{R}^{d\\times H_n\\times W_n}$, which encapsulate global semantic context, are processed via a transformer-based decoder $f_{dec}$, producing low-resolution logits. These logits are iteratively refined by incorporating spatially detailed features from intermediate-resolution feature maps $\\hat{Y_1} \\in \\mathbb{R}^{d/4\\times H_1\\times W_1}$ and fine-grained feature maps $\\hat{Y_0} \\in \\mathbb{R}^{d/8\\times H_0\\times W_0}$. This hierarchical refinement process is mathematically described as Eq (11), where $f_{dec}$ denotes the transformer-based decoding operation applied to $Y_n$, Upsample performs bilinear upsampling to match spatial resolutions, and Conv is a 1\\times1 convolution for channel alignment.\n$$\\begin{cases}\nS_{low} = f_{dec}(Y_n) \\\\\nS_{inter} = Upsample(S_{low}) + Conv(\\hat{Y_1}) \\\\\n\\tilde{S_0} = Upsample(S_{inter}) + Conv(\\hat{Y_0})\n\\end{cases}$$\n(11)\nAs shown in Figure 4, the second pathway utilizes a feature fusion mechanism to integrate multi-scale features into a unified high-resolution embedding. Specifically, backbone features $Y_n \\in \\mathbb{R}^{d\\times H_n\\times W_n}$, $Y_1 \\in \\mathbb{R}^{d/4\\times H_1\\times W_1}$, and $Y_0 \\in \\mathbb{R}^{d/8\\times H_0\\times W_0}$ are first transformed via MLPs and upsampled to a common target resolution $H_t \\times W_t$ using bilinear interpolation. This results in upsampled feature maps $Y_{up}^n \\in \\mathbb{R}^{d/8\\times H_t\\times W_t}$, $Y_{up}^1 \\in \\mathbb{R}^{d/8\\times H_t\\times W_t}$, and $Y_{up}^0 \\in \\mathbb{R}^{d/8\\times H_t\\times W_t}$, respectively. These upsampled features are then concatenated along the channel dimension and passed through a linear fusion layer $f_{fuse}$, followed by a prediction layer $f_{pred}$, to produce the high-resolution segmentation logits $\\tilde{S_1}$ as described in Eq. (12). $f_{fuse}$ effectively integrates features from multiple scales, while $f_{pred}$ generates the segmentation logits. This dual-pathway approach captures both global and local contextual information, thereby enhancing segmentation accuracy and robustness.\n$$\\tilde{S_1} = f_{pred} (f_{fuse} (Concat (Y_{up}^n, Y_{up}^1, Y_{up}^0)))$$\n(12)\nThe training process minimizes a loss function that integrates the Online Hard Example Mining Cross-Entropy (OhemCrossEntropy) loss [63], which focuses on hard-to-predict pixels to improve model robustness and efficiency. The ground truth segmentation labels $L \\in \\mathbb{R}^{H_t\\times W_t}$ are defined such that $L(i, j) \\in \\{0, 1, ..., C - 1, 255\\}$, where 255 indicates the ignore label. The OhemCrossEntropy loss for a single prediction map S is given by Eq. (13).\n$$L_{ohem} (S, L) = \\frac{1}{N_{min}}\\sum_{i\\in H}L_{CE}(S(i), L(i))$$\n(13)\nwhere $L_{CE}$ is the pixel-wise cross-entropy loss, and H represents the set of hardest pixels, selected based on prediction difficulty. The normalization factor $n_{min} = max(|H|, N_{threshold})$ ensures that a sufficient number of complex examples are included, where $N_{threshold} = N_{total}/16$, and $N_{total}$ is the total number of valid pixels in the image.\nThe overall loss function incorporates the OhemCrossEntropy loss applied to both $S_0$ and $S_1$, as defined in Eq. (14).\n$$L = w_0 \\cdot L_{ohem} (S_0, L) + w_1 \\cdot L_{ohem} (S_1, L)$$\n(14)\nwhere $w_0, w_1 \\in \\mathbb{R}^+$ are scalar weights that control the relative importance of each loss term."}]}, {"title": "IV. EXPERIMENTS", "content": "To comprehensively evaluate the performance of the proposed MLE-SAM model in multi-modal semantic segmentation, three distinct datasets were selected, each targeting specific challenges in autonomous driving and material segmentation tasks. These datasets provide complementary benchmarks to address real-world complexities such as adverse weather conditions, sensor failures, and multi-modal fusion in diverse scenarios.\nThe DELIVER dataset [13] is a large-scale multi-modal benchmark designed explicitly for semantic segmentation in autonomous driving scenarios. Developed using the CARLA simulator, it incorporates data from four modalities:RGB (R), Depth (D), LiDAR (L) and Event (E), enabling advanced"}, {"title": "V. CONCLUSION AND FUTURE WORK", "content": "This paper presented MLE-SAM, a novel adaptation of the SAM2 architecture tailored for multi-modal semantic segmentation. MLE-SAM incorporates LoRA-based adaptation, a selective feature weighting mechanism, and a dual-pathway mask prediction strategy. By effectively fusing dense and sparse modalities, MLE-SAM harnesses their complementary strengths to achieve precise segmentation while maintaining robustness across diverse conditions and datasets.\nExtensive experiments demonstrate that MLE-SAM consistently outperforms state-of-the-art models in terms of mIoU across various datasets and modality combinations. Notably, the model exhibits resilience in challenging scenarios, including noisy inputs and missing modalities, underscoring the advantages of its multi-modal fusion approach. Dense modalities contribute detailed spatial information crucial for high-resolution segmentation, while sparse modalities enhance robustness in adverse or resource-constrained environments.\nFuture research can prioritize refining the multi-modal integration through advanced pretraining techniques, noise-tolerant module designs, and adaptive attention mechanisms for sparse feature enhancement. Developing dynamic fusion strategies to balance dense and sparse modalities seamlessly can improve MLE-SAM's adaptability and effectiveness in real-world applications."}]}