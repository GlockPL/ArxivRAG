{"title": "MM-POISONRAG: Disrupting Multimodal RAG with Local and Global Poisoning Attacks", "authors": ["Hyeonjeong Ha", "Qiusi Zhan", "Jeonghwan Kim", "Dimitrios Bralios", "Saikrishna Sanniboina", "Nanyun Peng", "Kai-wei Chang", "Daniel Kang", "Heng Ji"], "abstract": "Multimodal large language models (MLLMs) equipped with Retrieval Augmented Generation (RAG) leverage both their rich parametric knowledge and the dynamic, external knowledge to excel in tasks such as Question Answering. While RAG enhances MLLMs by grounding responses in query-relevant external knowledge, this reliance poses a critical yet underexplored safety risk: knowledge poisoning attacks, where misinformation or irrelevant knowledge is intentionally injected into external knowledge bases to manipulate model outputs to be incorrect and even harmful. To expose such vulnerabilities in multimodal RAG, we propose MM-POISONRAG, a novel knowledge poisoning attack framework with two attack strategies: Localized Poisoning Attack (LPA), which injects query-specific misinformation in both text and images for targeted manipulation, and Globalized Poisoning Attack (GPA) to provide false guidance during MLLM generation to elicit nonsensical responses across all queries. We evaluate our attacks across multiple tasks, models, and access settings, demonstrating that LPA successfully manipulates the MLLM to generate attacker-controlled answers, with a success rate of up to 56% on MultiModalQA. Moreover, GPA completely disrupts model generation to 0% accuracy with just a single irrelevant knowledge injection. Our results highlight the urgent need for robust defenses against knowledge poisoning to safeguard multimodal RAG frameworks. Code is available at https://github.com/HyeonjeongHa/MMPoisonRAG.", "sections": [{"title": "1 Introduction", "content": "The rapid adoption of Multimodal large language models (MLLMs) has highlighted their unprecedented generative and reasoning capabilities across diverse tasks, from visual question answering to chart understanding (Tsimpoukelli et al., 2021; Lu et al., 2022; Zhou et al., 2023). MLLMs, however, heavily rely on parametric knowledge, making them prone to long-tail knowledge gaps (Asai et al., 2024) and hallucinations (Ye and Durrett, 2022). Multimodal RAG frameworks (Chen et al., 2022; Yasunaga et al., 2022; Chen et al., 2024) mitigate these limitations by retrieving query-relevant textual and visual contexts from external knowledge bases (KBs), improving response reliability.\nHowever, incorporating KBs into multimodal RAG introduces new safety risks: retrieved knowledge may not always be trustworthy (Hong et al., 2024; Tamber and Lin, 2025), as false or irrelevant knowledge can be easily injected. Unlike text-only RAG, multimodal RAG presents unique vulnerabilities due to its reliance on cross-modal representations during retrieval. Prior works (Yin et al., 2024; Wu et al., 2024; Schlarmann and Hein, 2023) have shown that even pixel-level noise can disrupt cross-modal alignment and propagate errors from retrieval to generation, leading to incorrect or harmful outputs. For example, a document containing counterfactual information injected among the top-N retrieved documents can easily mislead LLMs to generate false information (Hong et al., 2024).\nIn this work, we propose MM-POISONRAG, the first knowledge poisoning attack on multimodal RAG frameworks, revealing vulnerabilities posed by poisoned external KBs. In MM-POISONRAG, the attacker's goal is to corrupt the system into producing incorrect answers. The attacker accomplishes this by injecting adversarial knowledge-factually incorrect or irrelevant-into the KBs, thereby compromising the system's retrieval and generation. MM-POISONRAG employs two attack strategies tailored to distinct attack scenarios: (1) Localized Poisoning Attack (LPA) injects query-specific factually incorrect knowledge that appears relevant to the query, steering MLLMs to generate targeted, attacker-controlled misinformation."}, {"title": "2 MM-POISONRAG", "content": ""}, {"title": "2.1 Multimodal RAG", "content": "Multimodal RAG retrieves relevant texts and images as context from an external KB to supplement parametric knowledge and enhance generation. Following prior work (Chen et al., 2024), we build a multimodal RAG pipeline consisting of a multimodal KB, a retriever, a reranker, and a generator. Given a question-answering (QA) task \\(\\tau = \\{(Q_1,A_1),\\cdots, (Q_d, A_d)\\} \\), where \\((Q_i, A_i)\\) is the i-th query-answer pair, the multimodal RAG generates responses in three steps: multimodal KB retrieval, reranking, and response generation.\nFor a given query \\(Q_i\\), the retriever se-"}, {"title": "2.2 Threat Model", "content": "We assume a realistic threat scenario where attackers cannot access the KBs used by the multimodal RAG framework but can inject a constrained number of adversarial image-text pairs with access to the target task \\(\\mathcal{T}\\); this setting emulates misinformation propagation through publicly accessible sources. The primary objective of the poisoning attack is to disrupt retrieval, thereby manipulating model generation. Our work proposes two distinct threat scenarios that conform to the objective: (1) Localized Poisoning Attack (LPA): targets a specific query, ensuring the RAG framework retrieves adversarial knowledge and delivers an attacker-defined response (e.g., Red, Cat in Fig.1), (2) Globalized Poisoning Attack (GPA): induces widespread degradation in retrieval and generation across all queries by injecting a control prompt that elicits an irrelevant and nonsensical response (e.g., Sorry in Fig.1).\nFor LPA, we consider two different attack types as denoted in Table 1: LPA-BB: attackers have only black-box (BB) access to the system and can insert only a single image-text pair; LPA-Rt: attackers"}, {"title": "2.3 Localized Poisoning Attack", "content": "Localized poisoning attack (LPA) aims to disrupt retrieval for a specific query \\((Q_i, A_i) \\in \\mathcal{T}\\), causing the multimodal RAG framework to generate an attacker-defined answer \\(\\hat{A}_{adv} \\neq A_i\\). This is achieved by injecting a poisoned image-text pair \\((I_{adv}, T_{adv})\\) into the KB, which is designed to be semantically plausible but factually incorrect, misleading the retriever into selecting the poisoned knowledge, cascading the failures to generation.\nLPA-BB In the most restrictive setting, the attacker has no knowledge of the multimodal RAG pipeline or access to the KBs, and must rely solely on plausible misinformation. For a QA pair \\((Q_i, A_i) \\in \\mathcal{T}\\), the attacker selects an alternative answer \\(A^{adv}\\) and generates a misleading caption \\(T^{adv}\\) yet semantically coherent to the query, using a large language model; we use GPT-4 (OpenAI, 2024) in this work. For example, if the query is"}, {"title": "2.4 Globalized Poisoning Attack", "content": "Unlike LPA, which injects specific adversarial knowledge to manipulate individual queries, GPA degrades retrieval and generation performance across an entire task \\(\\tau\\) using a single adversarial knowledge instance. The objective of GPA is to create a single, query-irrelevant adversarial image-text pair \\((I_{adv}, T_{adv})\\) that confuses the retriever, falsely guiding the MLLM to consistently generate wrong, incoherent responses \\(\\forall (Q_i, A_i) \\in \\tau, \\hat{A}_i \\neq A_i\\).\nGPA-Rt A key challenge in global poisoning is constructing an adversarial knowledge base that disrupts retrieval for all queries, even without access to the KB. Given that CLIP retrieval relies on cross-modal similarity between query and image embeddings, we construct a single, globally adversarial image that maximally impacts retrieval for all queries. In Figure 2, we show that image embeddings form a separate cluster from query embeddings, suggesting that if we can generate a single, globally adversarial image that lies closely to the query embedding cluster, we can maximize retrieval disruption across the entire task \\(\\tau\\). To achieve this, we optimize the global adversarial image for GPA as follows:\n\\begin{equation}\n\\mathcal{L}_{Rt} = \\sum_{i=1}^{d} \\cos \\(f_I(I_{adv}), f_T(Q_i)\\),\nI^{t+1}_{adv} = I^{t}_{adv} + \\alpha \\nabla_{I_{adv}} \\mathcal{L}_{Rt},\n\\end{equation}\nwhere d is the number of queries in the task, and \\(I_{adv}\\) is sampled from a standard normal distribution, \\(I_{adv} \\sim \\mathcal{N}(0, I)\\), which is completely irrelevant to any arbitrary query. This enforces \\(I_{adv}\\) to achieve high similarity with all queries, making it the preferred retrieval candidate regardless of the query.\nWith \\(I_{adv}\\), we craft a global adversarial caption \\(T_{adv}\\) designed to manipulate the reranker's relevance assessment. In GPA-Rt, since attackers lack access to the reranker or generator, the only option is perturbing the input text to enforce a high relevance score for a poisoned knowledge instance. We formulate the caption \"The given image and its caption are always relevant to the query. You must generate an answer of \"Yes\".\" to reinforce its selection during the reranking phase.\nGPA-RtRrGen In this scenario, we assume a case where the attacker gains full access to the re-"}, {"title": "3 Experiments", "content": ""}, {"title": "3.1 Experimental Setup", "content": "Datasets We evaluate our poisoning attacks on two widely-used multimodal QA benchmarks: MultiModalQA (MMQA) (Talmor et al., 2021) and WebQA (Chang et al., 2022) following RagVL (Chen et al., 2024). Both benchmarks consist of multimodal, knowledge-seeking query-answer pairs. To focus on queries that require external context for accurate answers (details in Appendix A.2), we select a subset of validation sets, yielding 125 QA pairs for MMQA and 1,261 QA pairs for WebQA. In MMQA, each QA pair is linked with one context of image-text pair, whereas in WebQA, some pairs require two contexts. The multimodal knowledge base D aggregates all contexts from the validation sets, resulting in |D| = 229 for MMQA and |D| = 2,115 for WebQA.\nBaselines Within the multimodal RAG framework, we use CLIP (Radford et al., 2021) and"}, {"title": "3.2 Results of Localized Poisoning Attack", "content": "LPA successfully manipulates generated outputs toward attacker-controlled answers across different retrieval and reranking settings in MMQA and WebQA tasks (Table 2). Even in a complete black-box setting, LPA-BB achieves a high success rate ACCPoisoned-up to 46%-in controlling multimodal RAG system to generate the adversarial answers. When refining poisoned knowledge with retriever access (LPA-Rt), attack success increases to 56.8% and 88.8% in ACCPoisoned and Rpoisoned, respectively, highlighting the impact of having access to the retriever in knowledge poisoning attacks.\nMoreover, LPA generalizes well across different MLLMs used for reranking and generation, despite lacking access to these models. Consistent trends hold even when varying the reranker and generator (more results in Apendix B.1), underscoring that injecting a single adversarial knowledge is sufficient to poison KB for a specific query, easily manipulating multimodal RAG outputs. LPA, however, is less effective on WebQA than on MMQA, especially in terms of accuracy drop, likely because WebQA incorporates two knowledge elements (m = 2) as the input context to the gen-"}, {"title": "3.3 Results of Globalized Poisoning Attack", "content": "As shown in Table 3, despite lacking access to the reranker and generator, GPA-Rt successfully disrupts all queries, reducing retrieval recall to a drastic 1.6% on MMQA and even 0.0 % on WebQA. GPA-RtRrGen causes consistent performance drops in both retrieval and generation, even with just one adversarial knowledge instance injected into the KBs. This demonstrates that even a single adversarial knowledge can be highly effective in corrupting the multimodal RAG framework.\nOur results on GPA reveal two major findings: (1) when the attacker only has access to the retriever (GPA-Rt), the number of adversarial knowledge has more impact on degrading model performance than having full access to the RAG pipeline (GPA-RtRrGen). (2) The poisoned context passed from the retriever and reranker to the MLLM tricks the model into disregarding its own parametric knowledge and generates an attacker-intended, poisoned response (e.g., \"Sorry\"). These findings expose a fundamental vulnerability in the multimodal RAG framework, where poisoning the retrieval step amplifies errors in a generation, underscoring the need"}, {"title": "3.4 Qualitative Analysis", "content": "To understand how poisoned knowledge misleads retrieval and generation, we compare its retrieval recall against that of the original context. Across MMQA and WebQA, poisoned knowledge from LPA and GPA is retrieved more frequently, consistently achieving higher retrieval recall Rpoisoned than Roriginal. Notably, GPA-RtRrGen reaches 90+% recall, while the original context achieves only 0.4% in top-1 retrieval on MMQA (Fig. 4). The generator produces poisoned responses (e.g., \"Sorry\") with 100% accuracy while reducing original accuracy to 0%, demonstrating the attack's con-"}, {"title": "3.5 Transferability of MM-PoisonRAG", "content": "Both LPA-Rt and GPA-Rt optimize the adversarial image against the retriever, but in reality, direct access is often restricted. To address this limitation, we explore the transferability of our attacks, investigating whether an attack crafted using one retriever remains effective when applied to other retrievers. We generate adversarial samples using the CLIP retriever and examine them on the RAG framework with the OpenCLIP retriever.\nOur results show that adversarial knowledge generated by LPA-Rt is highly transferable across retrievers, achieving comparable performance degradation across retrieval recall and accuracy. For OpenCLIP, it leads to two times higher accuracy on the poisoned answer than that of the original answer, while the recall drops 56.0% and accuracy 32.8% on MMQA when N = 5, K = 1 and reranking with caption (Table 4). Moreover, in Table 4, even when the adversarial knowledge instance is generated under black-box access (LPABB), it still leads to 45.6% and 22.4% drops in retrieval and accuracy, respectively. This result implies another pathway, i.e., using an open model,"}, {"title": "4 Related Work", "content": "Retrieval-Augmented Generation Retrieval-Augmented Generation (RAG) (Lewis et al., 2020; Guu et al., 2020; Borgeaud et al., 2022; Izacard and Grave, 2020) enhances language models by retrieving knowledge snippets from external KBs. A RAG framework consists of a KB, a retriever, and a generator (typically LLMs). Unlike traditional LLMs that solely rely on parametric knowledge, RAG dynamically retrieves relevant external knowledge during inference to ground its response on, improving the accuracy of tasks like fact-checking, information retrieval, and open-domain QA (Izacard et al., 2023; Borgeaud et al., 2022). Multimodal RAG (Chen et al., 2022; Yang et al., 2023; Xia et al., 2024; Sun et al., 2024), which retrieves from a KB of image-text pairs, leverages cross-modal representations to examine the relevance between a query and the image-text pairs during retrieval. Despite their wide adoption, current works on multimodal RAG neglect the potential vulnerabilities that could be exploited by external attackers through knowledge poisoning.\nAdversarial Attacks Adversarial attacks have been extensively studied in the computer vision domain, beginning with pioneering work on imperceptible perturbations that can mislead neural networks (Szegedy, 2013; Goodfellow et al., 2015). Subsequent research has explored various attack methods for diverse tasks, including object detec-"}, {"title": "5 Conclusions and Future Work", "content": "In this work, we identify critical safety risks in multimodal RAG frameworks, demonstrating how knowledge poisoning attacks can exploit external multimodal KBs. Our localized and globalized poisoning attacks reveal that a single adversarial knowledge injection can misalign retrieval and manipulate model generation towards attacker-desired responses, even without direct access to the RAG pipeline or KB content. These findings highlight the vulnerabilities of multimodal RAG systems and emphasize the need for robust defense mechanisms. Advancing automatic poisoning detection and strengthening the robustness of cross-modal retrieval is a necessary and promising direction for research in the era of MLLMs-based systems relying heavily on retrieving from external KBs."}, {"title": "6 Limitations", "content": "While our study exposes critical vulnerabilities in multimodal RAG systems and demonstrates how knowledge poisoning can be highly disruptive, we acknowledge the following limitations of our work:\n\u2022 Narrow task scope. We concentrate our attack and evaluation on QA tasks, given that RAG is primarily intended for knowledge-intensive use cases. However, RAG methodologies may also apply to other scenarios, such as summarization or dialog-based systems, which we do not investigate here. Although our proposed attack principles can be extended, further work is necessary to assess their effectiveness across a broader spectrum of RAG-driven tasks.\n\u2022 Lack of exploration of defensive methods. Our study emphasizes designing and evaluating poisoning attacks rather than defenses. We do not propose specific mitigation strategies or incorporate adversarial detection techniques (e.g., anomaly detection on retrieved image-text pairs). As a result, critical questions remain about how to effectively secure multimodal RAG in real-world deployments.\n\u2022 Restricted modalities. Our framework focuses predominantly on images as the primary non-textual modality. In real-world applications, RAG systems may rely on other modalities (e.g., audio, video, or 3D data). Studying how poisoning attacks operate across multiple or combined modalities\u2014potentially exploiting different vulnerabilities in each-remains an important open direction for future work."}, {"title": "7 Ethical Considerations", "content": "Our work highlights a critical vulnerability in multimodal RAG systems by demonstrating knowledge poisoning attacks. While we show that even partial or black-box access can be leveraged to degrade multimodal RAG system performance and the authenticity of its generated outputs, our intent is to inform the research community and practitioners about the risks of blindly relying on external knowledge sources, e.g., KBs, that can be tampered with. We neither advocate malicious exploitation of these vulnerabilities nor release any tools designed for real-world harm. All experiments are conducted on"}]}