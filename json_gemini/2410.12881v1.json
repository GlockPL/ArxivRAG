{"title": "MIND: MATH INFORMED SYNTHETIC DIALOGUES FOR PRETRAINING LLMS", "authors": ["Syeda Nahida Akter", "Shrimai Prabhumoye", "John Kamalu", "Sanjeev Satheesh", "Eric Nyberg", "Mostofa Patwary", "Mohammad Shoeybi", "Bryan Catanzaro"], "abstract": "The utility of synthetic data to enhance pretraining data quality and hence to improve downstream task accuracy has been widely explored in recent large language models (LLMs). Yet, these approaches fall inadequate in complex, multi-hop and mathematical reasoning tasks as the synthetic data typically fails to add complementary knowledge to the existing raw corpus. In this work, we propose a novel large-scale and diverse Math Informed synthetic Dialogue (MIND) generation method that improves the mathematical reasoning ability of LLMS. Specifically, using MIND, we generate synthetic conversations based on OpenWebMath (OWM), resulting in a new math corpus, MIND-OWM. Our experiments with different conversational settings reveal that incorporating knowledge gaps between dialog participants is essential for generating high-quality math data. We further identify an effective way to format and integrate synthetic and raw data during pre-training to maximize the gain in mathematical reasoning, emphasizing the need to restructure raw data rather than use it as-is. Compared to pretraining just on raw data, a model pretrained on MIND-OWM shows significant boost in mathematical reasoning (GSM8K: +13.42%, MATH: +2.30%), including superior performance in specialized knowledge (MMLU: +4.55%, MMLU-STEM: +4.28%) and general purpose reasoning tasks (GENERAL REASONING: +2.51%).", "sections": [{"title": "INTRODUCTION", "content": "The ability to reason is a fundamental element of human cognition, encompassing our ability to think logically, draw conclusions, and make decisions based on available information (Gendron et al., 2024). Large Language Models (LLMs) have demonstrated remarkable performance across wide range of general reasoning and specialized knowledge tasks. In particular, the improvement of LLMS in solving complex mathematical reasoning tasks (Hendrycks et al., 2021b; Cobbe et al., 2021a) has been significant in recent years (Gemini, 2024; Anthropic, 2024b; OpenAI, 2024).\nStrong mathematical reasoning ability heavily relies on the abundance of high-quality, composite, and structured pretraining corpora. An effective mathematical corpus should not only contain relevant content but also be formatted to guide models break down complex problems into smaller sub-problems and solve each part step-by-step-enhancing the model's ability to process and reason about complex problems (Wei et al., 2022). Prior studies show that structured and well-formatted corpora play a crucial role in enhancing multi-hop and logical reasoning abilities (Cobbe et al., 2021a; Li et al., 2023; Gunasekar et al., 2023), underscoring the importance of well-organized mathematical datasets in pretraining LLMS.\nCurating complex, high-quality structured mathematical data is costly and resource-intensive, largely due to the uneven distribution of high-quality sources. Most advanced models (OpenAI, 2024; Gemini, 2024) are not publicly accessible, and it is unclear how their approach is enhancing math reasoning. To mitigate this challenge, synthetic data generation has emerged as a scalable, and cost-effective alternative for creating a more balanced and diverse training corpus for pretraining LLMS (Maini et al., 2024; Eldan & Li, 2023; Gunasekar et al., 2023; Shah et al., 2024). However,\nwhile these techniques have shown promise in improving general reasoning tasks, their data often lack the step-by-step problem solving structure crucial for multi-hop reasoning and complex mathematical tasks (Maini et al., 2024), making them sub-optimal for such reasoning.\nTo address these challenges, we propose MIND, a novel approach to generate Math Informed synthetic Dialogue data at scale. In MIND, we provide a pretrained LLM with a web document and explicitly prompt it in a zero-shot manner to generate a conversation that-(a) decomposes the original context step-by-step into multi-turn conversations and (b) explores each step in depth within a single turn. As illustrated in Figure 2,\nMIND generates conversation from a raw text by prompting an open-source LLM on seven diverse conversational styles. The generated conversa-tions are refined using heuristic filters and then can be used to pretrain a language model.\nMIND demonstrates that transforming raw web text into structured conversations using an off-the-shelf open-source LLM significantly enhances the mathematical and logical reasoning abilities of LLMs compared to unstructured raw or rephrased web text. Additionally, MIND provides the flexibility to preserve the diversity of the web corpora and leverage knowledge imbalances between participants for further expansion of the corpora as they either educate each other or collaboratively bridge their shared knowledge gaps through explanation and analysis in a conversation. Moreover, MIND enables the continuous generation of synthetic data from a single document by employing infinite conversational styles, further enriching the diversity. Unlike static text rephrasing (Maini et al., 2024), conversations encourage dynamic reasoning, where participants build on each other's ideas, ask questions, and offer clarifications. This quality makes conversations particularly effective for complex reasoning tasks, as they not only preserve the original information but also expand it with new layers of understanding and explanation.\nIn summary, the key contributions of this work are as follows:\n\u2022 We propose a novel approach, MIND, to generate structured conversational synthetic data for math reasoning. Leveraging MIND, we produce 64B tokens of synthetic data using 14B tokens from OpenWebMath corpus.\n\u2022 We conduct comprehensive experiments with various conversational styles, altering participant roles to assess their impact on conversation quality and reasoning tasks. Our findings emphasize the importance of the knowledge imbalance between participants in producing high-quality mathematical data.\n\u2022 We scale our approach to higher number of tokens and to two math specific datasets, demonstrating its efficacy in large and high-quality raw corpus.\n\u2022 We demonstrate an effective way for integrating synthetic and raw data during pretraining to enhance mathematical reasoning ability of LLMS, emphasizing the importance of carefully reformatting raw data to optimize reasoning processes instead of using it in its original form.\nIn this paper, we evaluate MIND across three dimensions: (1) the effectiveness of each conversational style in mathematical reasoning, (2) whether the impact of conversation persist as data scales, and (3) whether MIND remains beneficial when the raw text originates from high-quality sources. Continuously pretraining a 7B LLM on synthetic conversations (MIND-OWM-4B), generated from a subset of OpenWebMath (OwM-4B), results in 6.29% average improvement across three mathematical reasoning benchmarks, 4.30% on specialized knowledge tasks (MMLU), and a 2.20% boost across 10 zero-shot tasks, compared to the model trained with raw owm-4B. Additionally, our experiment with entire OpenWebMath (OwM-14B) and its corresponding synthetic conversations shows a consistent trend, indicating that the benefits of conversational data continue to hold as the data scales. In fact, with all conversations generated from owM-4B, we can outperform model trained with owM-14B, a 3.6\u00d7 larger data-2.94% average improvement across GSM8K and MATH\ntasks, 1.56% across all benchmarks (Figure 1). This underlines the value of synthetic conversations, particularly when high-quality in-domain data is limited. Moreover, our analysis with other datasets reveals that conversational data further amplifies reasoning capabilities in models even when the raw data originates from high-quality sources. We hope that MIND will pave a way to improve complex reasoning ability of smaller models with limited training data and accelerate further innovation towards building strong reasoning ability with structured high-quality data."}, {"title": "MIND: MATH INFORMED SYNTHETIC DIALOGUE GENERATION", "content": "To generate high-quality data at scale, current synthetic data generation approach explores rephrasing texts using LLMS in varied syntax while preserving the core content (Maini et al., 2024). However, their proposed approach limits up-sampling high-quality data in a way that does not go beyond grammatical styles or surface form transformations\u2014leading little to no improvement when it comes to performance across complex and logical reasoning tasks. We hypothesize that simple rephrasing does not leverage the full potential of the synthetic data to improve the mathematical and complex multi-hop reasoning ability of LLM. Therefore, we propose, MIND, a conversational synthetic data generation approach that adds semantic variations and structured complexity to the raw text which is required to improve complex reasoning ability of the LLMs. In addition, multi-turn conversations can break down the original context step-by-step while each step addresses a sub-context at a time by often injecting complimentary reasoning or explanations. This resonates with how human solves a complex problem using consecutive chain-of-thought reasoning.\nAs depicted in Figure 2, given a raw dataset $R = {r_1,...r_n}$, we define a set of conversational prompts $P = {p_1, ...p_7}$ and utilize a pretrained LLM, denoted as M, for synthetic data generation. We combine raw data $r_j$ with a prompt $p_i$ and pass it to M to produce synthetic conversation $s_{i,j}$.\n$s_{i,j} = M(p_i || r_j)$\nHere, $s_{i,j}$ represents the synthetic data generated by applying prompt $p_i$ to the raw example $r_j$. For a specific prompt, the total synthetic data generated can be represented as\n$S = {s_{i,j} | j \\in [1, N]}$ for a fixed $i \\in [1, 7]$\nWe further apply heuristic filtering (H) to remove bad generations:\n$S' = H(S)$\nFinally, we have a high-quality synthetic dialogue corpus $S'$ which is specifically designed to improve mathematical and logical reasoning ability. To summarize MIND:\n$R \\rightarrow MIND \\rightarrow S'$"}, {"title": "EXPERIMENTAL SETUP", "content": "Conversation Generator Configuration. To generate conversation, we consider zero-shot prompting M, where we only pass a basic prompt (Appendix A.1) and the raw text. We sample conversations with temperature=1.0 and top_p=0.9 where the total number of input-output tokes is limited to 4096. We use the TensorRT-LLM toolkit to deploy large scale generation\u00b9.\nPretrained Model Architecture. We train a standard decoder-only Transformer (Vaswani et al., 2017) architecture of 7B parameters (C). The framework uses causal attention masks and Rotary"}, {"title": "TRAINING DETAILS", "content": "Pretraining Data. Our pretraining data blend comprises of publicly available datasets from 13 snapshots of CommonCrawl (73.37%) (Gao et al., 2020), books/patents (9%), papers (9%), code (5.12%), stack-exchange (2.66%), and Wikipedia (0.8%). Our code data consists of 42 programming languages while the other datasets come from various sources including web documents, news articles, scientific papers, and books.\nGeneral Pretraining. To prepare a base model, we pretrain the 7B parameter model on our pre-training data blend till 700B tokens using 512 H100 80GB SXM5 GPUs. During training, we use the AdamW optimizer (Loshchilov & Hutter, 2019) with $\u03b2\u2081 = 0.9$, $\u03b22 = 0.95$ and weight decay Of 0.1. We use a 2-way tensor and pipeline parallelism to train the model. We set the maximum value of learning rate to 3e-4, minimum learning rate to 3e-6, and use a batch size of 6M tokens with a 4096 context length.\nContinued Pretraining. After pretraining the base model (C) on 700B tokens, we proceed with continuous pretraining using an additional 50B tokens to obtain E. To reduce the shift between pre-training and continuous pretraining token distributions (Guo et al., 2024) we create a new data blend (D) for this phase. To ensure the model is exposed to more math tokens, blend D consists of 2:1 ratio of OpenWebMath (33B tokens)\u2014either raw (R) or synthetic (S')\u2014 and 13 snapshots of CommonCrawl (17B tokens) (Rpt) to maintain consistency with the pretraining blend. To ensure fair comparison, we always keep this token distribution constant in every experiment i.e., every model will see a the same amount of tokens from a data source regardless of its size. Unlike the pretraining blend, we use a high quality version of CommonCrawl data (Rpt) filtered by the FineWebEdu (Penedo et al., 2024) classifier to achieve reasonable performance in generative tasks. This Rpt remains constant across all our continued pretraining experiments, while we vary the OpenWeb-Math with R or S' or combining both to assess their relative significance. We maintain the same training configuration as before and continue pretraining until reaching 50B tokens, using the same pretraining loss objective. In this paper, we use two versions of OpenWebMath:\n\u2022 OWM-4B: To quickly evaluate the effectiveness of all seven prompts, we take a smaller subset of OpenWebMath containing 4B tokens. Synthetic data generated from this subset is labeled as MIND-OWM-4B throughout the paper.\n\u2022 OWM-14B: This version contains the full 14.7B tokens of OpenWebMath and the synthetic data of this is called MIND-OwM-14B."}, {"title": "EVALUATION METRICS", "content": "To evaluate the zero-shot and few-shot learning capabilities of our models, we conduct a thorough benchmark assessment using a series of datasets using LM Eval Harness (Gao et al., 2024).\nGeneral Purpose Reasoning Tasks. This category comprises datasets testing broader cognitive skills and language comprehension. We consider nine standard commonsense and logical reasoning tasks: ARC easy (ARC-E) & challenge (ARC-C) (Clark et al., 2018), PIQA (Bisk et al., 2020),\nSIQA (Sap et al., 2019), HellaSwag (Zellers et al., 2019), WinoGrande (Sakaguchi et al., 2021),\nOpenBookQA (Mihaylov et al., 2018), TruthfulQA (Lin et al., 2022), and CommonsenseQA (Tal-mor et al., 2019). We further test the understanding and reasoning skills of the pretrained models on a reading comprehension task (RACE (Lai et al., 2017)). During evaluation, we take the average results across ten general reasoning tasks under the metric \u2018GENERAL REASONING'\nMath and Specialized Knowledge Tasks. We consider three diverse math benchmarks to comprehensively evaluate the mathematical reasoning ability of the pretrained models using few-shot chain-of-thought prompting (Wei et al., 2022). These benchmarks encompass mathematical challenges from elementary to college level complexity demanding qualitative reasoning (GSM8K (Cobbe et al.,"}, {"title": "EXPERIMENTS AND RESULTS", "content": "By leveraging MIND with seven conversational prompts and the raw owM-4B, we generate a new corpus of 43 billion tokens (All Conversations). Additionally, employing the entire Owm-14B dataset and TWO STUDENTS conversation style, MIND produces an additional 21 billion tokens-resulting in a total of 64 billion tokens. This underscores MIND's potential to generate vast amount of high-quality data from relatively limited source material\u00b2.\nPerformance across Individual Prompt Style. We observe the effect of each conversation style by generating synthetic data with seven prompts for a smaller subset of OpenWebMath, denoted as OwM-4B. To establish a baseline, we continue pretraining C using the continuous pretraining blend, $D = {R URpt}$, where R = owm-4B. In subsequent experiments, we replace R with S' where\nS' = MIND-OwM-4B, corresponding to a particular conversation style, and repeat the training. To assess the utility of combining multiple conversations, we create a new dataset by selecting the longest conversation for each context from the seven generated conversations, labeling it as the LONGEST CONVERSATION dataset.\nAs shown in Table 1, models trained on synthetic conversational data of individual styles consistently outperform the baseline across all reasoning tasks. Specifically, models trained on synthetic data exhibit significant improvements in mathematical reasoning compared to the baseline, achieving absolute gains ranging from 4.78% to 12.82% on GSM8K, 0.54% to 1.28% on MATH, and 0.79% to 4.28% on MMLU-STEM. In specialized knowledge tasks such as MMLU, synthetic data leads to improvements ranging from 1.08% to 4.55%. Furthermore, synthetic data yields an overall enhancement in general reasoning ability, with up to a 2% absolute average improvement across the ten reasoning tasks. The LONGEST CONVERSATION dataset delivers the highest gains across all tasks, demonstrating the potential of incorporating multiple perspectives into the training corpus.\nAnalysis with Complete OpenWebMath. Building on the findings from owm-4B experiments, we establish that all seven conversational styles contribute to significant improvements compared to the raw data. This insight prompted us to explore the effect of increased data in reasoning by scaling our synthetic conversation generation for the complete owm-14B corpus. To generate data, we follow the similar recipe as before and apply only one conversation style to minimize the generation cost. Among the top three highest-performing prompts across all tasks, we randomly choose TWO STUDENTS prompt style to generate conversations (MIND-OWM-14B). We then continuously train\nC on owM-14B and MIND-OWM-14B alternatively to assess the impact at a larger data scale. In this phase, we include another experiment by continuously training C on 50B additional tokens using\n$D = {Rpt}$ to observe how much gain we can attain across all tasks from math-centric pretraining."}, {"title": "ABLATIONS", "content": "Does the Prompt Style matter? To evaluate the impact of prompt styles, we conduct experiment by training with conversations from seven different styles. From Table 1, we observe improvement across all tasks using six conversational styles. However, our experiment with TWO PROFESSORS conversations yield relatively equivalent or worse performance compared to the raw data (Table 3).\nThis outcome can be attributed to the nature of the TWO PROFESSORS conversation style. Upon reviewing the generated conversations, we hypothesize that the relatively lower performance is due to the zero-knowledge gap between participants. In this setup, both participants assume that the other already has sufficient knowledge as they are the domain experts, leading to surface-level engagement and less detailed discussions.\nTo further investigate, we measure the BLEU and ROUGE scores between a raw text and the corresponding conversation, as shown in Figure 3, and find that the TWO PROFESSORS style exhibits the highest similarity to raw text. This implies that TWO PROFESSORS dialogues do not fully exploit the potential of the generation model to introduce new reasoning or breakdowns of complex problems, aligning with our qualitative observation that the professors are not engaging in deeper analysis of concepts.\nThis contrasts with other conversational styles where there is either a clear knowledge gap between participants (LAYMAN KNOWALL, TEACHER STUDENT, INTERVIEW), forcing one to explain concepts in more depth, or both par-\nticipants, being non-experts are actively analyzing and solving the problem (PROBLEM SOLVING, DEBATE, TWO STUDENTS) which results in expanded dialogues with complementary explanations and reasoning. In the latter case, the lack of expertise creates an implicit knowledge gap-instead\nof one participant being more knowledgeable, both non-experts collaborate to bridge their shared knowledge gap. As depicted in Figure 3, the LAYMAN KNOWALL style, which features the greatest knowledge imbalance between participants, has the lowest BLEU and ROUGE scores. This supports our hypothesis that a larger information gap encourages the knowledgeable participant to explain concepts thoroughly, leading to more explicit and detailed conversations.\nRelating these insights to our findings in Table 1, we see that incorporating explicit knowledge gaps in dialogues is beneficial for MMLU and general reasoning tasks. Conversely, collaborative problem solving, to close the implicit knowledge gap, is crucial for improving performance on math tasks. This highlights a key characteristic of high-quality math data-merely breaking down the problem is insufficient for effective math reasoning. Instead, dynamic knowledge exchange and analysis within the dialogues are essential to achieve maximum improvement in math reasoning.\nDoes Conversation benefit other datasets? Our experiments so far have utilized OpenWebMath as the seed corpus for generating synthetic conversations and training LLM. OpenWebMath is predominantly a mathematical corpus collected from mathematical web pages that can contain noisy web contexts. Generating synthetic conversations for such noisy contexts upsamples high-quality data and hence we observe a huge gain in performance with high-quality conversations. However, it is yet to be explored whether MIND will work on high-quality datasets such as books or papers. To investigate this, we consider a new seed corpus, MATHPILE (Wang et al., 2023), that consists of 9.3B tokens extracted from high-quality data sources such as ArXiv papers, textbooks, StackExchange, Wikipedia, ProofWiki, and CommonCrawl pages.\nBy employing M, we generate conversations from raw text with the TWO STUDENTS prompt. Later, we replicate the experiments by replacing OWM with MATHPILE and MIND-MATHPILE accordingly. Table 4 shows that MIND-MATHPILE outperforms the raw counterpart in all three math benchmarks along with specialized knowledge tasks, achieving comparable scores in general reasoning task. In addition, majority of MATHPILE data is from ArXiV papers and recent work has found this source ineffective in improving mathematical reasoning (Shao et al., 2024). We observe the similar trend with GSM8K benchmark where training with non-math focused pretraining corpora yields better score than with MATHPILE corpus. However, our synthetic conversation on MATHPILE rather amplifies the quality of the corpus resulting in 3.95% absolute improvement on GSM8K in comparison with raw data. This highlights the superior structured complexity of conversations, which proves particularly effective for multi-hop and mathematical reasoning tasks, over high-quality data from Arxiv papers.\nRephrase vs Conversation. To assess the significance of MIND over other synthetic data generation approach for math reasoning, we investigate rephrasing the raw web text with LLM, introduced by Maini et al. (2024). They generate four styles of synthetic data (easy, medium, hard, and Q/A) among which medium style rephrasing produces improvements across many tasks. To maintain consistency among generation quality and training setup, we choose the OwM-4B data and generate rephrases with M using the highest performing prompt from the paper. We continuously train C with $D = {X URpt}$. where X \u2208 {Rephrase-OwM-4B}.\nAs illustrated in Table 5, model pretrained on conversations consistently outperforms those trained on rephrased or raw text across all tasks. Interestingly, rephrased data underperforms raw data in benchmarks such as GSM8K. This disparity is closely related to the limitations of the rephrasing process. Rephrase adds linguistic variations to the older data, preserving the syntactic meaning of the document, but can not generate semantic/pragmatic variations. Moreover, rephrases are limited to the information in the raw text and unable to inject new knowledge into the data. As evidenced in our experiments, while rephrasing offers some benefits, it falls short in addressing the deeper, more\ncomplex reasoning challenges that conversational data can resolve. The structured and interactive nature of conversations facilitates a more nuanced understanding of the problem space, making it an effective approach for improving mathematical reasoning of LLMs.\nIs replacing with Synthetic Data the best option? Our findings in Table 2 and Table 1 indicate that completely replacing OpenWebMath with synthetic data provides the best performance across benchmarks. However, Maini et al. (2024) emphasizes the importance of combining real data and synthetic rephrases to achieve consistent improvements across a broader range of tasks a similar trend we observe in our experiment with rephrased data, as shown in Table 6. To investigate this further, we conduct experiments with four different data combinations using OWM-4B while the Rpt remains constant:\n\u2022 OWM-4B + MIND-OWM-4B [1:1]. We combine R and S' in a 1:1 ratio, ensuring an equal number of tokens to be seen during pretraining from both sources. For the synthetic data, we utilize the LONGEST CONVERSATION, as this shows the most improvement across tasks (Table 1).\n\u2022 OWM-4B + MIND-OWM-4B [Concat]. We concatenate each raw context with all seven synthetic conversations sequentially.\n\u2022 MIND-OWM-4B [Longest Conversation]. From the seven conversations generated for each context, we select the longest conversation in token count.\n\u2022 MIND-OWM-4B [All Conversations]. This data incorporates all conversation across all styles."}, {"title": "CONCLUSION", "content": "In this paper, we focus on improving the mathematical reasoning abilities of open-source LLMS. We propose a simple approach to generate complex and structured data at scale, called MIND, that produces a new conversational synthetic math corpus, MIND-OWM, using an off-the-shelf open-source LLM. Models trained on MIND-OWM, a corpus generated through our approach, consistently outperform those trained on raw data, achieving up to a 6.29% improvement across mathematical\nreasoning benchmarks and outperforming models trained on 3.6\u00d7 larger datasets. Importantly, these gains persist across general-purpose reasoning tasks and when scaling up the data, highlighting the versatility of synthetic conversations. This work demonstrates the potential of structured conversational data to enhance reasoning, especially in cases where domain-specific high-quality data is limited, paving the way for more effective and resource-efficient pretraining of LLMS."}, {"title": "PROMPTS AND DATASETS", "content": "TWO PROFESSORS\nConvert the context above as a multi-turn discussions between two professors. Make sure that their discussions strictly adhere to the context above and remains faithful to information in the context. Please DONOT add any new information/reference other than the context.\nTEACHER STUDENT\nConvert the context above as a multi-turn discussions between a teacher and a student. The student has questions about the context and the teacher solves each of them step-by-step. Make sure that their discussions strictly adhere to the context above and remains faithful to information in the context. Please DONOT add any new information/reference other than the context.\nTWO STUDENTS\nConvert the context above as a multi-turn discussions between two students who are working on their assignment related to the given context. Make sure that their discussions strictly adhere to the context above and remains faithful to information in the context. Please DONOT add any new information/reference other than the context.\nINTERVIEW\nConduct an interview-style conversation where one participant acts as the interviewer, asking questions exclusively related to the content provided, while the other participant serves as the subject matter expert, providing detailed responses based on the content. Make sure that their discussions strictly adhere to the context above and remains faithful to information in the context. Please DONOT add any new information/reference other than the context.\nPROBLEM SOLVING\nConvert the context above as a multi-turn problem-solving conversation where participants analyze challenges or scenarios presented in the content and brainstorm solutions within the context of the provided material, avoiding speculation or unrelated discussions. Make sure that their conversation strictly adhere to the context above and remains faithful to information in the context. Please DONOT add any new information/reference other than the context.\nLAYMAN KNOW-ALL\nImagine you are presenting the content above step-by-step to a layman. While you are presenting, the layman has a lot of followup questions regarding your presentation. You answer the questions step-by-step with chain-of-thoughts. Design this interaction between you and the layman as a multi-turn conversational manner. Make sure that the interaction strictly adhere to the context above and remains faithful to information in the context. Please DONOT add any new information/reference other than the context.\nDEBATE\nConvert the context above as a multi-turn debate-style conversation where the participants present arguments and counterarguments based solely on the content provided, without introducing external information or personal opinions. Each participant defends others arguments step-by-step with chain-of-thoughts. Make sure that the conversation strictly adhere to the context above and remains faithful to information in the context. Please DONOT add any new information/reference other than the context.\nWe evaluate the LLM trained on raw and synthetic data using ten diverse general reasoning tasks, three mathematical tasks and one specialized knowledge tasks.\nAll the benchmarks under this category are evaluated in zero-shot manner.\nThis dataset is proposed by the AI2 Reasoning Challenge (ARC). There are two sets of this data: (1) ARC-E and (2) ARC-C, containing science exam questions from grades 3 to 9. The ARC Challenge set includes more difficult questions compared to ARC-E that necessitate higher-order reasoning.\nThis dataset has been collected from English reading comprehension exams designed for middle and high school Chinese students.\nPhysical Interaction Question Answering evaluates physical common-sense reasoning ability of the language model.\n[Wino.]This benchmark is structured as a fill-in-the-blank task with binary options, requiring the LLM to select the correct option for a given sentence,\nprimarily focusing on commonsense reasoning and pronoun disambiguation tasks."}, {"title": "CONVERSATION QUALITY ASSESSMENT", "content": "While the conversations generated by the LLM typically appear coherent, there are instances where the conversation fails to preserve the context or lacks grounding to the source material. In some cases, conversations may even be incomplete. Detecting poor-quality generation becomes challenging at scale. To address this, we explore two quality-filtering approaches:\nHeuristic Filtering. We employ a simple heuristic based on token length. Given that the input context is limited to a maximum of 500 tokens and split into subcontexts of 500 tokens each to maximize information retention, we discard any generated conversations that fall below 50 tokens. This ensures that minimal information loss is detected early.\nLLM-based Scoring. For a more comprehensive assessment, we use an LLM to score the quality of the generated conversations. We introduce four key metrics for evaluation:\n\u2022 Correctness: Verifies that all information, such as numbers and parameters, is accurately reflected in the conversation.\n\u2022 Faithfulness: Ensures the conversation remains grounded in the context provided.\n\u2022 Information Preservation: Checks whether all relevant facts and knowledge from the original context are retained in the conversation.\n\u2022 New Knowledge: Evaluates whether the conversation introduces additional explanations, reasoning, or definitions not present in the raw input.\nGiven a raw context and its corresponding conversation, we ask M to rate the conversation on a scale of 1 to 5 in each of four metrics, with 1 representing poor quality and 5 representing the best possible conversation. To determine the overall quality, we compute the average score across the metrics and choose conversations with average scores more than or equal to 3. Additionally, we utilize the prompt from the FineWebEdu (Penedo et al., 2024) annotation framework to further check the correlation between two scoring approaches. In Figure 5, we plot the scores for 140K conversations using FineWebEdu metrics and our metrics. It is clearly visible from the figure is that LLM tends to\nrate its own generation higher almost all the time resulting in a skewed distribution of rating. Around 96% of conversations are labelled as high quality. However, compared to FineWebEdu, our metric results in less skewed distribution-making our approach more suitable for evaluating synthetic data derived from a seed corpus."}, {"title": "COMPARE WITH DEEPSEE\u039a\u039c\u0391\u03a4\u0397", "content": "To asses the quality of our data, we run pre-training experiments to compare MIND-OWM with the recently released DEEPSEEKMATH (Shao et al., 2024). The DEEPSEEKMATH approach is iterative. They construct a dataset for binary classification consisting of 500K positive data points randomly sampled from OpenWebMath (the seed corpus) and 500K negative data points randomly sampled from CommonCrawl. They train a fastText (Joulin, 2016) classifier on these data which they then use to extract samples from CommonCrawl as math content. All CommonCrawl domains for which over 10% of the existing web pages have been extracted are at this point understood to be math-related. URLs which are associated with these domains but which have yet to be collected are manually labeled as math content. The web pages hosted at these addresses are added to the seed corpus and the classifier is retrained. DEEPSEEKMATH performs 4 rounds in total resulting in the DEEPSEEKMATH Corpus, consisting of some 120B math tokens. They continuously train a partially converged 7B DEEPSEEKCODER-V1.5 model on a 500B token blend to attain the DEEPSEEKMATH model and achieve substantial improvement on several math tasks. In contrast, MIND proposes a simple alternative for generating high-quality math data that boosts the mathematical reasoning ability of LLM given access to a small seed corpus.\nAs the DEEPSEEKMATH dataset is not public, we replicate our previous blend, $D = {X URpt}$, where X = {MIND-OWM-4B (conversations of all styles except the TWO STUDENTS one) U MIND-OWM-14B (TWO STUDENTS conversations)}. We maintain a 2:1 ratio of X and Rpt in the training blend. Similar to the approach of DEEPSEEKMATH, we take a converged DEEPSEEKCODER-V1.5 model as C the unconverged model weights are unpublished as far as we are aware and convert the model weights to a format compatible with Megatron-LM, which serves as our training framework, before continuously training for 500B tokens. We use a cosine learning rate schedule with a 19B token linear ramp-up, a maximum learning rate of 3e-4, and a minimum learning rate of 3e-6, and we anneal the learning rate over 500B tokens. We use Adam with parameters $\u03b2\u2081 = 0.9$ and $B2 = 0.95$, a weight decay of 0.1, a gradient clipping threshold of 1.0, a sequence length of 4096, and a global batch size of 2304 sequences.\nFrom Table 13, we can see that a model trained on conversations which MIND generated given a small seed corpus can attain math accuracies comparable to the DEEPSEEKMATH model with access to 120B unique math tokens in its continuous training blend. In fact, we outperform DEEPSEEK-MATH in MMLU and general reasoning tasks, reaching higher average accuracy across all tasks. This underscores the quality of MIND generated conversations and signifies the efficacy of MIND in improving mathematical reasoning ability of LLM when the underlying raw data is limited."}, {"title": "CONVERSATIONS ON CODE TASKS", "content": "Unlike raw data", "benchmarks": "HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021), HumanEval+, and MBPP+ (Liu et al., 2024). These benchmarks are specifically designed to assess the model's ability to generate functional code in response to given prompts.\nOur results, as presented in Table 14, demonstrate that conversational synthetic data does not enhance coding performance. This is largely due to the way conversations tend to fragment code, wrapping it in natural language and thereby obscuring the intended sequence and logic inherent in programming tasks. Consequently, while conversations may be effective in contexts that benefit from collaborative reasoning, they are not suited for preserving the integrity of code, leading to diminished performance in coding benchmarks.\nInterestingly, we also observe that rephrasing, which resembles raw data more closely in structure, further degrades coding accuracy. Our qualitative analysis of the rephrased documents reveals that the conversation generator (LLAMA"}]}