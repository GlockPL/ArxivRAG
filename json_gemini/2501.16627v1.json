{"title": "ENGAGING WITH AI: HOW INTERFACE DESIGN SHAPES\nHUMAN-AI COLLABORATION IN HIGH-STAKES\nDECISION-MAKING", "authors": ["Zichen Chen", "Yunhao Luo", "Misha Sra"], "abstract": "As reliance on AI systems for decision-making grows, it becomes critical to ensure that human\nusers can appropriately balance trust in AI suggestions with their own judgment, especially in high-\nstakes domains like healthcare. However, human + AI teams have been shown to perform worse\nthan AI alone, with evidence indicating automation bias as the reason for poorer performance, par-\nticularly because humans tend to follow AI's recommendations even when they are incorrect. In\nmany existing human + AI systems, decision-making support is typically provided in the form of\ntext explanations (XAI) to help users understand the AI's reasoning. Since human decision-making\noften relies on System 1 thinking (fast, intuitive, heuristics driven, prone to cognitive biases), users\nmay ignore or insufficiently engage with the explanations, leading to poor decision-making. Pre-\nvious research suggests that there is need for new approaches that encourage users to engage with\nthe explanations and one proposed method is the use of cognitive forcing functions (CFFs). In this\nwork, we examine how various decision-support mechanisms impact user engagement, trust, and\nhuman-AI collaborative task performance in a diabetes management decision-making scenario. In a\ncontrolled experiment with 108 participants, we evaluated the effects of six distinct decision-support\nmechanisms split into two categories of explanations (text, visual) and four CFFs: (1) text expla-\nnations (baseline) (2) visual explanations, (3) AI confidence levels (CLs), (4) human feedback, (5)\nAI-driven questions, and (6) performance visualization. Our findings reveal that mechanisms such as\nAI CLs, text explanations, and performance visualizations significantly enhanced human-AI collab-\norative task performance, as measured by decision accuracy, and improved trust when AI reasoning\nclues were provided. While mechanisms like human feedback and AI-driven questions encouraged\ndeeper reflection, they may have resulted in decreased task performance, likely due to increased\ncognitive effort and heightened scrutiny, which negatively impacted trust. Simple mechanisms like\nvisual explanations did not significantly improve trust, highlighting the need for a contextual and\nbalanced approach between CFF and XAI design with interactivity, decision frequency, and task\ncomplexity.", "sections": [{"title": "1 Introduction", "content": "Chronic diseases like diabetes require constant informed decision making to maintain optimal health outcomes [1, 2,\n3, 4, 5]. This ongoing responsibility traditionally burdens both patients and healthcare providers, requiring significant\neffort and attention. The emergence of artificial intelligence (AI) systems offers promising new tools to support these\nfrequent and critical decisions [6, 7, 8, 9]. In particular, generative AI shows potential to help people with diabetes in\nmeal planning to better control blood sugar levels [10, 11, 12, 13, 14]. The effectiveness of human-AI collaboration in\nthis context critically depends on users finding the right balance between trusting AI suggestions and exercising their\nown judgment.\nPrevious research has shown that trust plays a critical role in human-AI interactions, particularly in healthcare set-\ntings [15, 16, 17]. Users may over-rely or under-trust an AI system. Over-trust can lead to unquestioned acceptance\nof AI suggestions, even wrong ones, potentially resulting in incorrect or harmful recommendations [18, 19]. Con-\nversely, under-trust may cause users to disregard valuable AI-generated advice that could potentially improve health\noutcomes [20]. An appropriate balance between trust and skepticism is crucial for the safe and effective use of\nAI [21, 22, 23], especially in healthcare where decisions can have immediate and significant consequences [24, 25].\nRecent research has explored various strategies for trust calibration in AI systems [26, 27, 28, 29, 19]. However,\nthe challenges of overtrust and undertrust persist, preventing the ideal scenario in which human-AI collaboration\noutperforms humans or AI systems alone [27, 30]. Even with the growing emphasis on explainable AI (X\u0391\u0399) [31, 32],\nproviding explanations alone has not consistently mitigated trust issues [33]. Studies suggest that overreliance persists\nwhen users are faced with complex tasks or explanations [34, 35, 36]. This phenomenon can be understood through the\nlens of the dual process theory, which distinguishes between System 1 (fast, intuitive) and System 2 (slow, deliberative)\nthinking [37, 38, 39, 40, 30]. Users often weigh the cognitive cost (System 2) of evaluating AI suggestions against\nperceived benefits, potentially defaulting to uncritical acceptance (System 1) if the effort seems too high [41, 42, 39].\nConversely, under-trust can lead to the dismissal of valuable AI support, limiting the potential benefits of human-AI\ncollaboration. Users may rely excessively on their intuitions (System 1) or engage in overly critical analysis (System\n2) that can both discount AI contributions. Strategies to promote more analytical thinking and mitigate cognitive biases\ninherent in System 1 thinking have recently been explored [27]. Among these, cognitive forcing functions (CFFs) are\nstrategically designed interventions that can interrupt automatic, intuitive reasoning at the precise moment of decision\nmaking, causing individuals to shift toward a more deliberate, analytical thought process or System 2 thinking [43].\nTo encourage a shift from System 1 to System 2 thinking, with the goal to help overcome issues of over-trust or under-\ntrust by enhancing engaged task performance, we designed six decision-support mechanisms including a baseline text\nexplanation method. The six design mechanisms are grouped into two main areas: XAI (Explainable AI) and CFFs\n(Cognitive Framework Features). XAI focuses on enhancing the transparency of AI systems and is further divided into\nthree categories based on their presentation modality: (1) Textual XAI, (2) Visual XAI, and (3) Multimodal XAI. CFFS\nare designed to influence user decision-making by reducing cognitive biases and encouraging deliberate, thoughtful\nprocesses. These features are classified into three areas based on the dimension of cognitive effort:\n\u2022 Attention-Directing CFFs: Guide user focus and prompt critical thinking (e.g., AI-driven questions).\n\u2022 Complexity-Reduction CFFs: Reduce cognitive load by making AI outputs easier to interpret (e.g., AI con-\nfidence levels (CLs) that transparently communicate the model's uncertainty or performance visualizations\nthat show AI task performance over time).\n\u2022 Decision-Constraint CFFs: Allow users to intervene in decision-making processes (e.g., human feedback\nmechanisms that enable validation of AI outputs).\nTo evaluate the effectiveness of our designed mechanisms, we conducted a human subject study involving 108 par-\nticipants, with 18 participants assigned to each of the six conditions, in a diabetes meal planning scenario. Each\nparticipant was went through 3 phases in order for each of the 20 questions they answered: decision-making without\nAI assistance (P1), decision-making with AI assistance but without any explanation (P2), and decision-making with\nAI assistance accompanied by one of the six decision-support mechanisms (P3). Our evaluation integrates both quan-\ntitative and qualitative methods. We collected both objective and subjective data on decision accuracy, satisfaction,\nsystem complexity, reliability, trust, and perceived accuracy using pre- and post-questionnaires. In addition, we exam-\nine user responses across dimensions such as confidence and understanding, consistency and cognitive load, ease and\nusefulness, trust and comfort, and trust dynamics across the designed mechanisms. Statistical analyzes are performed\nto assess the impact of each decision support mechanism while controlling for individual differences, including AI\nfamiliarity and educational background.\nThe contribution of our work is threefold. First, we provide empirical evidence on the effectiveness of different\ndecision support mechanisms in influencing user trust, engagement, and decision-making. Second, we contribute to\nunderstanding of trust dynamics in human-AI interaction by exploring how different explanation types and CFFs influ-\nence trust calibration. And lastly, we present a methodological approach for evaluating decision support mechanisms\nthrough controlled experiments, isolating the effects of individual explanation and CFF types. Our approach can be\nadapted to other domains beyond healthcare."}, {"title": "2 Results", "content": "Our study explores three key hypotheses regarding these mechanisms:\n1.  Hypothesis 1 (H1): People tend to trust AI's answers, even when explanations are minimal, and this can lead\nto overreliance.\n2.  Hypothesis 2 (H2): Access to explanation mechanisms makes users more critical of AI suggestions, encour-\naging deeper evaluation of AI outputs.\n3.  Hypothesis 3 (H3): Explanation mechanisms with a balanced Explanation Information Load (EIL) that\nprovide interpretable model reasoning support for effective human-AI collaboration by enhancing decision-\nmaking and facilitating trust calibration without overwhelming users."}, {"title": "2.1 Users Over-Rely on AI Suggestions, Even When Incorrect (H1)", "content": "To test the first hypothesis (H1), which posits that users tend to trust AI's suggestions, we conduct a statistical analysis\nexploring how users behave when the AI suggestions are both correct and incorrect. Specifically, we investigate\nwhether users adjust their decisions based on the accuracy of Al's output. We chose the McNemar test for analysis,\nsince it allows us to examine changes in user behavior before and after AI's suggestion, particularly focusing on\npaired nominal data. We show how user decisions shift in Table. 1: (a) when AI predictions are correct, (b) when AI\npredictions are incorrect. The test evaluates whether there is a significant change in decision patterns between these\ntwo phases.\nWhen AI Suggestions are Incorrect. The highly significant p-value (p < 0.001) indicates that a notable proportion\nof users follow AI suggestions even when they are incorrect, demonstrating a bias toward trusting AI-generated advice.\nThis highlights an over-reliance on AI, even in cases where the suggestions are faulty, something that prior work has\nalso demonstrated [44, 45, 46].\nWhen AI Suggestions are Correct. The statistical analysis revealed a highly significant result (p < 0.001), showing\nthat users are strongly inclined to accept AI's correct answers. This finding supports the hypothesis that users exhibit\na high level of trust in AI, particularly when its suggestions are accurate.\nBehavioral Pattern Analysis. These results provide evidence supporting our hypothesis. The results suggest\nthat people tend to follow AI suggestions with limited critical evaluation, often accepting them without thoroughly\nprocessing the outcomes. Users show a tendency to rely on AI suggestions, even when the AI is wrong. Such\nover-reliance can be explained by cognitive bias, where users may perceive the AI system as an authoritative figure,\nespecially in tasks requiring complex decision-making.\nWhen AI is correct, users demonstrate a greater degree of trust, suggesting that accurate AI assistance can reinforce\nuser confidence in the system. However, when AI is incorrect, the lack of significant correction by the users indicates"}, {"title": "2.2 AI Explanation Mechanisms Can Help People Engage (H2)", "content": "We hypothesize that integrating decision support mechanisms during the decision-making process would lead to an\nincrease in user engagement (H2). We compare engagement changes between phase 1 (P1: decision-making w/o AI\nassistance) to phase 2 (P2: decision-making w/ AI assistance but w/o any explanation) and between phase 1 (P1:\ndecision-making w/o AI assistance) to phase 3 (P3: decision-making w/ AI assistance accompanied by a decision-\nsupport mechanism) using the Wilcoxon signed-rank test [47], a non-parametric test suitable for analyzing paired\ndata and comparing these engagement rates. We also compare the pre- and post-questionnaire results to measure the\nengagement level. The questionnaires include questions related to satisfaction, system complexity, reliability, trust,\nand system accuracy. More details on the questionnaires and measurement can be found in Method 6.7.\nCondition 1: Textual Explanations. Textual explanations (C1), as our baseline condition, includes detailed textual\nexplanations of the AI's decision-making process. There is a significant increase in engagement (p = 0.049), indi-\ncating that textual explanations help users to understand AI decisions through a logical structure. By comparing the\npre- and post-questionnaire results, we find a significant improvement in system reliability (p = 0.048), suggesting\nthat such explanations can improve user's trust in the AI system. However, it is important to note that the system\ncomplexity also increased (p = 0.008), which may have led to increased cognitive overload.\nCondition 2: Visual Explanations. Visual explanations (C2) include a basic visual explanation of the AI's attention\nwithout any additional feedback mechanism. The results do not show any significant engagement improvement (p =\n0.776), suggesting that visual explanations alone may not be sufficient to encourage deeper engagement. Users likely\naccepted the AI's output without reflecting on its reasoning, which points to potential over-reliance.\nCondition 3: AI Confidence Levels. AI CL (C3) allows users to view the AI's and their own confidence levels (CLs).\nThe results show a significant increase in engagement (p = 0.001), indicating that confidence levels may help users\nto calibrate their trust in the AI system. CL is a form of transparency that provides users with direct insight into the\nAl's certainty, which encourages users to critically compare their judgment with the AI's. Users are not only passive\nrecipients of AI suggestions but active participants in the decision-making loop, which may explain the increased\nengagement. However, we observe that there is no significant improvement in system reliability and trust, suggesting\nthat confidence levels alone may not be sufficient to build trust.\nCondition 4: Human Feedback. Similar to AI CLs (C3), human feedback (C4) allows users to input their CLs.\nIt shows a significant increase in engagement (p = 0.006), suggesting that allowing users to input their own think-\ning encourages introspection and deep cognitive involvement in decision-making. However, same to AI CLs (C3),\nthere is no significant improvement in system reliability and trust. This indicates that while user input can increase\nengagement, it may not necessarily modulate trust in the AI system.\nCondition 5: AI-Driven Questions. AI-driven questions (C5) include AI-generated questions to prompt users to\nreflect on AI's suggestions. The results show a borderline significance (p = 0.058), indicating that while AI-generated\nreflective questions encourage some degree of engagement, they are not as effective as other conditions. The questions\nrequire users to think deeper about the AI's decision, which forces user's to shift from System 1 to System 2 thinking.\nThe borderline result suggests that AI-driven questions may hold promise, if optimized.\nCondition 6: Performance Visualization. Performance visualization (C6) includes a visualization of the AI and\nuser's task performance over time. It shows a significant increase in engagement (p = 0.009), suggesting that the\nvisualized comparison of human and AI performance contributed to a reflective decision-making process. Perhaps it"}, {"title": "2.3 Explanation Information Load and Interpretable Reasoning Enhance Human-AI\nCollaboration (H3)", "content": "We calculated an Explanation Information Load (EIL) across the six conditions. The details of EIL can be found in\nMethod 6.8. The resulting values represent the information load on the users, that needed to be cognitively processed,\nfor each condition.\nBased on our hypothesis (H2), we expect to see a balanced cognitive load for better human-AI collaboration (H3).\nAcross the six conditions, we observe a variety of outcomes, ranging from significant improvements in user accuracy\nto negligible or even borderline impacts.\nIn visual explanations (C2) and AI CLs (C3), where AI explanation mechanisms are designed to provide transparency\nand interpretability, we observe a significant increase in user accuracy (p = 0.018 and p = 0.004). The moderate EIL\nin visual explanations (C2) (EIL = 1.749) provides users with comprehensive but understandable insights into AI\nreasoning, encouraging critical engagement without overwhelming them. The low EIL in AI CLs (C3) (EIL = 0.602)\nallows users to judge the certainty of AI suggestions at a glance, serving as an efficient cognitive aid. These mecha-\nnisms provide users with interpretable model reasoning that conveys the AI's rationale and confidence, respectively.\nThis combination of a manageable EIL and interpretable reasoning support users in making informed decisions without\ninformation overload.\nIn contrast, mechanisms that lack clear interpretive reasoning exhibit less impact on performance. Performance vi-\nsualization (C6), with an EIL of 0.60, demonstrates borderline significance (p = 0.092), suggesting that while users\nare encouraged to engage with historical performance data, the lack of explicit interpretive guidance left them to\ndraw their own conclusions from visual information. This aligns with cognitive psychology research, which indicates\nthat high information density, without sufficient interpretive support, can lead to cognitive strain, ultimately limiting\nperformance and reducing trust calibration [48].\nFurthermore, in visual explanations (C2) (EIL = 1.520), human feedback (C4) (EIL = 1.204), and AI-driven\nquestions (C5) (EIL = 1.965), we found no significant improvement in user performance (p = 0.673, p = 0.674, and\np = 0.302). These conditions either lack a direct interpretive model component or introduce high cognitive demands\nwithout adequate support. For example, textual explanations (C1) present intuitive but limited information, which\nmay not provide users with the depth needed to make well-calibrated decisions. Human feedback (C4) encourages\nself-assessment, but without specific reasoning cues from the AI, it does not significantly enhance decision accuracy.\nAI-driven questions (C5), with the highest EIL, likely lead to information overload, as users are required to reflect\ndeeply on their decisions.\nThese results suggest that EIL and the presence of interpretable model reasoning are important for effective human-\nAI collaboration. Mechanisms that balance a manageable EIL with clear reasoning cues can improve collaboration\nperformance. In contrast, mechanisms that lack interpretive support or impose high cognitive demands may hinder\nperformance, highlighting the importance of designing AI explanation mechanisms that offer clear interpretive insights\naligned with the user's cognitive capacity."}, {"title": "2.4 Textual Explanations (C1): The Impact of Basic AI Support on Decision-Making", "content": "In textual explanations (C1), participants interact with the AI system that provides support but without any additional\nexplanatory mechanisms, feedback loops, or engagement-enhancing features. We evaluate the impact of this condi-\ntion's effectiveness with respect to user satisfaction, system complexity, reliability, trust, and perceived accuracy. We\nalso apply these metrics to other conditions. Detailed statistical results for all conditions can be found in Table 3.\nSatisfaction. While there is a modest increase in satisfaction metrics, the improvement is not statistically significant\n(p = 0.122). The results show that while textual explanations may provide some additional clarity, they do not have"}, {"title": "2.5 Visual Explanations (C2): The Impact of Text Explanations on Decision-Making", "content": "In visual explanations (C2), participants interact with an AI system that enhances transparency by highlighting key\nreasoning areas within the visual input and labeling them, directly mapping the AI's decision-making process and the\nvisual evidence. Text explanations are widely regarded as a means of improving AI interpretability [49, 50], while\nvisual explanations offer an intuitive way for users to verify and contextualize the AI's reasoning [51].\nSatisfaction. We observe a statistically significant increase in satisfaction (p = 0.050), before and after interacting\nwith visual explanations (C2), suggesting that participants feel more content with the system after using it. The\nresults highlight the potential for even simple mechanisms to improve the user experience in decision-making tasks.\nThough the improvement in satisfaction is modest, it shows that users are receptive to explanation mechanisms, as\nthese mechanisms can help users navigate complex decision-making tasks more effectively. It is also worth noting\nthat this improvement may plateau without further engagement mechanisms, as evidenced by the results for trust and\nengagement discussed below.\nSystem Complexity. System complexity remains consistently low, with no significant changes before and after\ninteracting with visual explanations (C2). The result implies that the visual explanation mechanism does not introduce\nadditional complexity or cognitive burden on users. Participants generally find the system easy to use. However, while\nsimplicity is desirable, a lack of change may also indicate users are not engaging deeply with the AI system.\nReliability. We observe a statistically significant increase in perceived reliability (p = 0.005), demonstrating that\nusers feel the AI system is more reliable after they are shown the visual explanations (C2). The result suggests\nthat even without detailed textual explanations, the AI system's performance in supporting decisions can enhance its\nperception of reliability. The results suggest that users may find the system to be reliable when the AI system provides\nreasonably accurate suggestions, even without a clear understanding of its decision-making process. However, this\ngain in reliability does not translate into a corresponding increase in trust, as shown in the next subsection, pointing\ntoward potential limitations in how trust is developed in AI systems without transparency or interpretability.\nTrust. While reliability improves significantly, there is no evidence of a corresponding increase in trust (p = 0.421).\nThe results suggest that users may perceive the AI system as reliable based on its performance, but they do not trust it\nto guide their decisions under the current conditions. Trust in AI systems depends not only on how reliable the system\nappears but also on how transparent and interpretable the system is, and how much agency the user has. Without\nexplanations or feedback, users may begin to feel uncertain about how to critically assess its outputs, leading to a trust\ndecline.\nPerceived Accuracy. Perceived accuracy shows a small, non-significant increase in visual explanations (C2). While\nparticipants improve their views on the accuracy of the AI's suggestions, the improvement does not show statistical\nsignificance. The results suggest that while the AI is generally perceived as accurate, the lack of system transparency\nmay limit user ability to engage critically with the AI's outputs with impact on perception of credibility and trust."}, {"title": "2.6 AI CLS (C3): The Impact of CLs on Decision-Making", "content": "In AI CLs (C3), participants receive AI's CLs for each suggestion, providing a quantifiable metric of the AI's certainty.\nCL is a form of transparency that provides users with a clear sense of the system's certainty in its outputs. We also\nprovide a CL to estimate user performance to help participants compare their behavior with the AI's. Different from\ntextual explanations, which provide detailed logical reasoning, CLs offer explicit and quantitative measures of AI\nconfidence.\nSatisfaction. While there is a small increase in satisfaction, the change is not statistically significant. The results\nsuggest that although CLs may provide additional clarity to users, they do not greatly enhance the overall satisfaction\nwith the system. The relatively stable satisfaction levels across pre- and post-evaluations imply that CLs provide\nsupplementary rather than transformative information to the user and, therefore, have minimal impact on satisfaction.\nSystem Complexity. There is no significant change in system complexity after users interact with AI CLs (C3).\nThe results suggest that while the the confidence levels introduce additional information, it is not overwhelming.\nThe relatively high initial complexity ratings indicate that participants already perceive the AI system as moderately\ncomplex, and the addition of CLs do not dramatically change this perception. The results suggest that CLs may not\nrequire deep cognitive effort to interpret.\nReliability. The results show a clear increase in reliability, though this change does not reach statistical significance.\nIt suggests that CLs help users feel more confident in the AI system's reliability. While not statistically significant, the\nimprovement in reliability points to the potential of CLs as a tool for trust calibration, enabling users to compare their\nperformance with the AI's and adjust their trust levels accordingly.\nTrust. Although trust in the AI slightly decreased after the introduction of CLs, this change is not statistically\nsignificant (p = 0.206). The results indicate a subtle interaction between the reliability and trust of the system.\nWhile CLs may improve user perception of the AI's reliability, it is possible that users become more critical of the AI\nsuggestions when they can see that the system is not always highly confident or more accurate than a human.\nPerceived Accuracy. There is a significant increase in perceived accuracy after users interact with AI CLS (C3)\n(p = 0.039). The result highlights the value of transparency in improving user perception of AI performance. By\ncomparing the CLs of their performance with Al's, it helps users make more informed decisions and adjust their\nreliance on the system. The significant increase also suggests that CLs play an important role in calibrating trust,\nenabling users to align their expectations with the AI's internal assessments of its own performance."}, {"title": "2.7 Human Feedback (C4): The Impact of Human Feedback on Decision-Making", "content": "In human feedback (C4), participants need to input both their own CL and their estimation of the AI's confidence in\nits suggestion. Human feedback (C4) is designed to investigate how user engagement with the AI system may change\nwhen they are enforced to reflect on both their own ability and the AI's.\nSatisfaction. Satisfaction increased slightly after interacting with human feedback (C4), though the change is not\nstatistically significant. The relatively stable satisfaction levels across pre- and post-evaluations imply that human\nfeedback may not be as transformative as other explanation mechanisms in the decision-making process for the current\ntask. The lack of significance indicates that the added cognitive task of self-reflection does not substantially enhance\noverall satisfaction, potentially because it may introduce additional cognitive effort.\nSystem Complexity. There is a slight and non-significant increase in perceived complexity (p = 0.310), reflecting\nthe additional complexity of asking users to input CLs. The results show that while the added interaction may increase\nthe cognitive processing load, users generally find the system manageable.\nReliability. Reliability ratings improve slightly, but this change is not statistically significant (p = 0.318). The\nresults suggest that while human feedback may help users calibrate their trust in the AI system, it does not significantly\nimpact their perception of the system's reliability. It shows that while reflection on their own confidence and the AI's\nconfidence may lead users to feel slightly more confident in the system's reliability, the task of inputting CLs does not\nproduce a major shift in how users perceive the AI's reliability. One potential reason is that users may have struggled\nto accurately judge the AI's confidence or to compare it meaningfully with their own.\nTrust. Trust in the AI system slightly decreased after participants have to reflect on their own CLs and those of the AI.\nWhile this change is not statistically significant (p = 0.417), the slight decline suggests that the process of reflecting on\nconfidence might lead some users to become more critical of the AI's suggestions and question the system's accuracy\nwhen they think their confidence is not aligned. This finding highlights the potential challenge of trust calibration\nwhen users are given more control over the decision-making process without clear guidance on how to interpret CLs.\nPerceived Accuracy. Perceived accuracy remains largely unchanged, with a very slight and non-significant increase\n(p = 0.592). The results suggest that allowing users to input CLs does not substantially affect their perception of how\naccurate the Al's decisions are. It is possible that users find it challenging to interpret their CLs and align with those\nof the AI, limiting the impact of this interaction on perception of the AI's accuracy."}, {"title": "2.8 AI-Driven Questions (C5): The Impact of AI-Generated Questions on\nDecision-Making", "content": "In AI-driven questions (C5), participants are presented with AI-generated questions designed to encourage critical\nthinking and deeper engagement. The questions are intended to prompt users to actively evaluate the AI's suggestions,\nrather than passively accepting them.\nSatisfaction. We observe that satisfaction slightly decreases, although this change is not statistically significant\n(p = 0.773). The results suggest that while the reflective questions may help some users engage more deeply, they\ndo not universally enhance user satisfaction with the system. It is possible that questions may introduce additional\ncognitive processing load, leading to this slight decrease.\nSystem Complexity. Interestingly, there is a statistically significant decrease in system complexity after participants\ninteract with AI-driven questions (C5) (p = 0.008). The counterintuitive result suggests that the AI's reflective\nquestions may help users focus their decision-making process, thereby simplifying the task. By guiding users toward\nkey considerations, the AI-generated questions might reduce cognitive overload, offering a structured framework for\nevaluating the AI's recommendations. The results also highlight the potential of guided reflection as a tool to make\ncomplex decision-making processes feel more manageable.\nReliability. We find a slight increase in reliability, though this change is not statistically significant (p = 0.210). The\nresults show that while the reflective questions may encourage users to think more critically about the AI's suggestions,\nthey do not significantly impact user perception of the system's reliability. When users are prompted to engage more\ndeeply with AI decisions, they may become more aware of potential errors, which can undermine their confidence\nin the system's reliability. From the results, we find that the reflective questions encourage deep reflection but may\nsimultaneously reveal errors in the AI's reasoning, thus impacting how users perceive the system's reliability.\nTrust. We observe a statistically significant decrease in trust after participants interact with AI-driven questions\n(C5) (p = 0.032). The result suggests that, while reflective questions are designed to improve engagement, they\nmay also lead users to question the AI's recommendations more critically, potentially leading to a loss of trust when\nthe Al's logic does not align with the user's expectations. This finding highlights a critical challenge in human-AI\ncollaboration: while encouraging users to engage critically with AI outputs can promote better decision-making, it can\nalso expose the limitations of the AI, leading to reduced trust.\nPerceived Accuracy. Perceived accuracy decreases slightly, but this change is not statistically significant (p =\n0.543). The results suggest generated questions do not substantially impact user perception of the AI's accuracy. The\nvariability in post-accuracy ratings (Std = 1.77) indicates a wide range of user experiences, with some participants\npossibly found the reflective questions helpful, while others may have struggled with interpreting the AI's logic,\nleading to doubts about its accuracy."}, {"title": "2.9 Performance Visualization (C6): The Impact of Performance Visualization on\nDecision-Making", "content": "In performance visualization (C6), participants are presented with performance visualizations that compare the AI's\nhistorical performance on the ongoing task against their own decisions. Performance visualization (C6) is intended to\nprovide users with a clear, empirical basis for evaluating the Al's recommendations over time, thereby enabling users\nto calibrate their trust more effectively.\nSatisfaction. The participant satisfaction with the AI system remains largely unchanged after the introduction of\nperformance visualization. The results show that while the visualization may provide useful insights into past perfor-\nmance, it does not have a strong impact on the overall user experience. It indicates that users view the visualizations\nas supplementary rather than central to their decision-making process.\nSystem Complexity. There is a slight decrease in system complexity in performance visualization (C6), though\nthis change is not statistically significant (p = 0.218). The visualizations may provide users with a clearer under-\nstanding of the AI's decision-making performance over time, helping them evaluate the AI's consistency and accuracy\nwithout overwhelming them with additional information load. However, the non-significant results imply that the\nvisualizations do not substantially change user perception of system complexity.\nReliability. Interestingly, participant perception of the AI system's reliability remains unchanged. The result suggests\nthat while the visualization provided a historical perspective on the Al's accuracy, it was not compelling enough to\nsignificantly shift user views on how reliable the system is."}, {"title": "2.10 Cross-Condition Analysis: Confidence, Cognitive Load, Usefulness, Trust and Trust\nDynamics", "content": "We analyze user decision-making patterns across all the conditions to understand how different mechanisms influence\nkey factors of human-AI collaboration. We evaluate the dimensions of confidence and understanding, consistency and\ncognitive load, ease and usefulness, trust and comfort, and trust dynamics. Detailed statistical results for all conditions\ncan be found in Figure 3.\nWe observe that there is a significant difference in confidence and understanding across conditions: textual explana-\ntions (C1) (p = 0.007), human feedback (C4) (p = 0.002), AI-driven questions (C5) (p = 0.003), and performance\nvisualization (C6) (p = 0.008). Interestingly, textual explanations (C1) demonstrates the highest confidence mean\n(mean = 6.000, std = 0.553), reflecting the fact that basic mechanisms, while lacking transparency, can still evoke\nstrong confidence, perhaps due to simplicity. In contrast, conditions 4 and 5, which require users to engage deeply,\nhave significantly lower ratings. It suggests that the increased cognitive load leads users to question AI suggestions\nmore critically.\nThe results for consistency and cognitive load show no significant differences across conditions (p = 0.263). Mean\nscores are relatively similar across all conditions, with AI CLs (C3) having a slightly higher consistency scores\n(mean = 4.694, std = 1.056) compared to AI-driven questions (C5) (mean = 3.944, std = 1.212). We observe that\nCLs may help users perceive the AI system as more consistent, because CLs provide users with transparency about\nthe AI's certainty in its decisions. On the other hand, mechanisms that require deeper cognitive engagement (e.g.,"}, {"title": "2.11 Impact of Diabetes Management Familiarity on Decision Accuracy", "content": "We divided participants into two groups based on their self-rated familiarity with diabetes management: unfamiliar\n(\u2264 4) and familiar (> 4). For each condition, we conducted a two-sample t-test to compare decision accuracy between\nthese two groups. The results revealed no statistically significant differences (all p > 0.05) in average accuracy across\nconditions. For example, in textual explanations (C1), unfamiliar users (n = 8) had an average accuracy of 69.4%\nwhile familiar users (n = 10) achieved 72.5% (p = 0.398). Similarly, in AI CLs (C3) unfamiliar users (n = 11)\nachieved 71.8% accuracy compared to 77.9% among familiar users"}]}