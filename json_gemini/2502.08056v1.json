{"title": "Cognify: Supercharging Gen-Al Workflows With Hierarchical Autotuning", "authors": ["Zijian He", "Reyna Abhyankar", "Vikranth Srivatsa", "Yiying Zhang"], "abstract": "Today's gen-AI workflows that involve multiple ML model calls, tool/API calls, data retrieval, or generic code execution are often tuned manually in an ad-hoc way that is both time-consuming and error-prone. In this paper, we propose a systematic approach for automatically tuning gen-Al workflows. Our key insight is that gen-Al workflows can benefit from structure, operator, and prompt changes, but unique properties of gen-AI workflows require new optimization techniques. We propose AdaSeek, an adaptive hierarchical search algorithm for autotuning gen-Al workflows. AdaSeek organizes workflow tuning methods into different layers based on the user-specified total search budget and distributes the budget across different layers based on the complexity of each layer. During its hierarchical search, AdaSeek redistributes the search budget from less useful to more promising tuning configurations based on workflow-level evaluation results. We implement AdaSeek in a workflow autotuning framework called Cognify and evaluate Cognify using six types of workflows such as RAG-based QA and text-to-SQL transformation. Overall, Cognify improves these workflows' generation quality by up to 2.8\u00d7, reduces execution monetary cost by up to 10x, and reduces end-to-end latency by 2.7x.", "sections": [{"title": "1 Introduction", "content": "Today's production use of generative AI (gen AI), such as code assistants [8], AI customer service and sales [36], and AI-assisted search [15], often involves multiple steps of gen-AI model calling, tool/API calling, data retrieval, or generic code execution. In this paper, we call such gen-AI software that goes beyond a single model call gen-AI workflows 1. Gen-AI workflows allow for more customization, integration, and capabilities, thus being the de facto gen-AI product solutions for many business uses.\nDespite gen-Al workflows' appeal, their full capabilities are yet to be unlocked due to today's gen-AI workflow practice. After developing a workflow's basic logic, engineers manually explore various variations to its structure (e.g., by adding a step), test different models, and tune various prompts. This manual, ad-hoc workflow tuning practice is time-consuming and ineffective, as humans cannot efficiently navigate through a vast search space of tuning options. As a result, gen-AI workflows today still suffer from subpar generation quality and other metrics like end-to-end execution latency.\nThese problems are exacerbated by the rapidly increasing amounts of gen-Al tuning options, such as new models and new prompt engineering techniques [46, 47]. As more gen-AI workflows are introduced, automated gen-AI workflow optimization is a must. Unfortunately, existing automated gen-AI optimizers either focus on single model calls [30, 37] or cater to only a selected set of workflow tuning methods and evaluation metrics [7, 21, 48, 50].\nWe believe workflow auto-tuning should be conducted under a systematic and extensible framework. Our insights are that within a workflow, we can view each computation step (a model call, a tool call, a code block, a data retrieval) as a node and messages directed from one computation step to another as directed edges. The \"weight\" of an edge in a gen-Al workflow is the alteration to the original message sent via the edge (e.g., adding few-shot examples to a prompt message). We categorize workflow tuning methods into three types as shown in Figure 1: (1) architecture changes that alter the original workflow by adding/removing/moving steps or edges, such as task decomposition and ensembling, (2) step changes that alter the operations performed at a step, such as calling different models or rewriting code pieces, and (3) weight changes that alter the value of edges, such as augmentation to prompts.\nAlthough traditional ML models also have similar concepts that manifest as AutoML (or Neural-Architecture Search, NAS) and weight training, classical methods like Reinforcement-Learning-and Bayesian-Optimization-based NAS [13, 17, 19, 34, 52] and SGD-based weight training [25] do not work for gen-Al workflows for several reasons. First, unlike ML model weights, \u201cweights\u201d in workflows like added prompting messages are not differentiable and thus cannot be trained using SGD. Second, workflows (tens of steps and edges) are significantly smaller than ML models, potentially allowing search-based approaches to be applied for all types of workflow tuning changes. Third, users often have a limited budget (e.g., 50 evaluation iterations) for their workflow optimization, not allowing enough search points for RL to learn from or for traditional BO to converge, especially when considering more types of cogs (i.e., search dimensions)."}, {"title": "2 Formalizing Gen-AI Workflow Autotuning", "content": "This section presents how we formalize the problem of gen-AI workflow autotuning with a general, extendable framework and how it resonates but differs from classical AutoML [17] and weight training [35].\nWe formalize the definition of gen-AI workflows as generic program flows that contain at least one generative Al model call. A step, si, in a workflow represents a unit whose internal states are not autotuned. For example, a model call is a step as its internal weights are not autotuned at the workflow level. A code block can be a step if it is always tuned as one piece. All steps in a workflow forms the node set, S. We define an edge, eij, for every pair of steps si and sj where si sends its output to sj. As we allow arbitrary gen-AI workflow program structures, edges in a workflow can represent control paths and form cycles. We define the edge set, E. We define the weight of an edge, wij, as the auxilary values or operations applied to the output eij from si to sj. For example, wij can be few-shot examples added to the output message oij to form a prompt to the next LLM step, sj. A gen-Al workflow, W, is W = (S, E).\nAfter a workflow architecture is defined, its structure, step operators, and edge weights can all be altered to achieve better results. We view all these processes as autotuning a gen-AI workflow and call each type of alteration a cog. Each cog can have many different values and forms one dimension in the entire autotuning search space. We categorize cogs into three types. The first type, architecture cogs, Ca, alters the structure of a workflow by adding or removing steps and edges to get a new workflow graph Ws = (Ss, Es). For example, Cognify includes two architecture cogs, task decomposition and task ensembling (Section 4.1), CTD and CTE. Each cog can be applied to different locations of a workflow, i.e., having different values along their dimensions. The second type, step cogs, Cs, alters a step's operation without changing its input and output, i.e., si \u2192 s. For example, the model cog cMD Cognify supports uses different models at different model-call steps. The third method, weight cogs, Cw, alters the values of edges, eij. For language model steps, their inputs are prompts, and weights are various prompt engineering methods added to the original prompt message.\nThe effectiveness of gen-AI workflow autotuning is measured by user-defined metrics, M = (M1, M2, ..). For example, a quality evaluator can be the percentage of workflow generations exactly matching the ground truth. It can also be a numerical score given by an LLM as a judge. Other metrics include end-to-end request execution latency and total workflow execution monetary cost. A"}, {"title": "3 The AdaSeek Search Algorithm", "content": "With our insights in Section 2, we believe that search methods based on Bayesian Optimizer (BO) can work for all types of cogs in gen-AI workflow autotuning because of BO's efficiency in searching discrete search space. A key challenge in designing a BO-based search is the limited search budgets that need to be used to search a high-dimensional cog space. For example, for 4 cogs each with 4 options and a workflow of 3 LLM steps, the search space is 412. Suppose each search uses GPT-40 and has 1000 output tokens, the entire space needs around $168K to go through. A user search budget of $100 can cover only 0.06% of the search space. A traditional BO approach cannot find good results with such small budgets.\nTo confront this challenge, we propose AdaSeek, an adaptive hierarchical search algorithm that efficiently assigns search budget across cogs based on budget size and observed workflow evaluation results, as defined in Algorithms 1 and 2 and described below."}, {"title": "3.1 Hierarchical Layer and Budget Partition", "content": "A non-hierarchical search has all cog options in a single-layer search space for an optimizer like BO to search, an approach taken by prior workflow optimizers [33, 51]. With small budgets, a single-layer hierarchy allows BO-like search to spend the budget on dimensions that could potentially generate some improvements. However, a major issue with a single-layer search space is that a search algorithm like BO can be stuck at a local optimum even when budgets increase. To mitigate this issue, our idea is to perform a hierarchical search that works by choosing configurations in the outermost layer first, then under each chosen configuration, choosing the next layer's configurations until the innermost layer. With such a hierarchy, a search algorithm could force each layer to sample some values. Given enough budget, each dimension will receive some sampling points, allowing better coverage in the entire search space. However, with high dimensionality (i.e., many types of cogs) and insufficient budget, a hierarchical search may not be able to perform enough local search to find any good optimizations.\nTo support different user-specified budgets and to get the best of both approaches, we propose an adaptive hierarchical search approach, as shown in Algorithm 1. AdaSeek starts the search by combining all cogs into one layer (L = 1, line 9 in Algorithm 1) and estimating the expected search budget of this single layer to be the total number of cogs to the power of \u03b1 (lines 16-19, by default \u03b1 = 1.1). This budget is then passed to the LayerSearch function (Algorithm 2) to perform the actual cog search. When the user-defined budget is no larger than this estimated budget, we expect the single-layer, non-hierarchical search to work better than hierarchical search.\nIf the user-defined budget is larger, AdaSeek continues the search with two layers (L = 2), combining step and weight cogs into the inner layer and architecture cogs as the outer layer (lines 11-14). AdaSeek estimates the total search budget for this round as the product of the number of cogs in each of the two layers to the power of \u03b1 (lines 16-20). It then distributes the estimated search budget between the two layers proportionally to each layer's complexity (lines 22-24) and calls the upper layer's LayerSearch function. Afterward, if there is still budget left, AdaSeek performs a last round of search using three layers and the remaining budget in a similar way as described above but with three separate layers (architecture as the outermost, step as the middle, and weight cogs as the innermost layer). Two or three layers work better for larger user-defined budgets, as they allow for a larger coverage of the high-dimensional search space.\nFinally, AdaSeek combines all the search results to select the best configurations based on user-defined metrics (line 34)."}, {"title": "3.2 Recursive Layer-Wise Search Algorithm", "content": "We now introduce how AdaSeek performs the actual search in a recursive manner until the inner-most layer is searched, as presented in Algorithm 2 LayerSearch. Our overall goal is to ensure strong cog option coverage within each layer while quickly directing budgets to more promising cog options based on evaluation results. Specifically, every layer's search is under a chosen set of cog configurations from its upper layers (Cchosen) and is given a budget b. In the inner-most layer (lines 7-20), AdaSeek samples b configurations and evaluates the workflow for each of them together with the configurations from all upper layers (Cchosen). The evaluation results are added to the feedback set F as the return of this layer.\nFor a non-inner-most layer, AdaSeek samples a chunk (W) of points at a time using the TPE BO algorithm [3] until all this layer's pre-assigned budget is exhausted (lines 27-30). Within a chunk, AdaSeek uses a successive-halving-like approach to iteratively direct the search budget to more promising configurations within the chunk (the dynamically changing set, \u0398). In each iteration, AdaSeek calls the next-level search function for each sampled configuration in \u0398 with a budget of rs and adds the evaluation observations from lower layers to the feedback set F for later TPE sampling to use (lines 35-37). In the first iteration (s = 0), rs is set to R / n\u00ba = R (line 34). After the inner layers use this budget to search, AdaSeek filters out configurations with lower performance and only keeps the top \\u300a\\u300a /\u03b7 \\u300b\\u300b configurations as the new \u0398 to explore in the next iteration (line 42). In each next iteration, AdaSeek increases rs by \u03b7 times (line 34), essentially giving more search budget to the better configurations from the previous iteration.\nThe successive halving method effectively distributes the search budget to more promising configurations, while the chunk-based sampling approach allows for evaluation feedback to accumulate quickly so that later rounds of TPE can get more feedback (compared to no chunking and sampling all b configurations at the same time). To further improve the search efficiency, we adopt an early stop approach where we stop a chunk or a layer's search when we find its latest few searches do not improve workflow results by more than a threshold, indicating convergence (lines 14,38,45)."}, {"title": "4 Cognify Design", "content": "We build Cognify, an extensible gen-AI workflow autotuning platform based on the AdaSeek algorithm. The input to Cognify is the user-written gen-AI workflow (we currently support LangChain [24], DSPy [21], and our own programming model), a user-provided workflow training set, a user-chosen evaluator, and a user-specified total search budget. Cognify currently supports three autotuning objectives: generation quality (defined by the user evaluator), total workflow execution cost, and total workflow execution latency. Users can choose one or more of these objectives and set thresholds for them or the remaining metrics (e.g., optimize cost and latency while ensuring quality to be at least 5% better than the original workflow). Cognify uses the AdaSeek algorithm to search through the cog space. When given multiple optimization objectives, Cognify maintains a sorted optimization queue for each objective and performs its pruning and final result selection from all the sorted queues (possibly with different weighted numbers). To speed up the search process, we employ parallel execution, where a user-configurable number of optimizers, each taking a chunk of search load, work together in parallel. Cognify returns multiple autotuned workflow versions based on user-specified objectives. Cognify also allows users to continue the auto-tuning from a previous optimization result with more budgets so that users can gradually increase their search budget without prior knowledge of what budget is sufficient. Appendix A.4 shows an example of Cognify-tuned workflow outputs. Cognify currently supports six cogs in three categories, as discussed below."}, {"title": "4.1 Architecture Cogs", "content": "Cognify currently supports two architecture cogs: task decomposition and task ensemble. Task decomposition [22] breaks a workflow step into multiple sub-steps and can potentially improve generation quality and lower execution costs, as decomposed tasks are easier to solve even with a small (cheaper) model. There are numerous ways to perform task decomposition in a workflow. To reduce the search space, we propose several ways to narrow down task decomposition options. Even though we present these techniques in the setting of task decomposition, they generalize to many other structure-changing tuning techniques.\nIntuitively, complex tasks are the hardest to solve and worth decomposition the most. We use a combination of LLM-as-a-judge [49] and static graph (program) analysis to identify complex steps. We instruct an LLM to give a rating of the complexity of each step in a workflow. We then analyze the relationship between steps in a workflow and find the number of out-edges of each step (i.e., the number of subsequent steps getting this step's output). More out-edges imply that a step is likely performing more tasks at the same time and is thus more complex. We multiply the LLM-produced rating and the number of out-edges for each step and pick the modules with scores above a learnable threshold as the target for task decomposition. We then instruct an LLM to propose a decomposition (i.e., generate the submodules and their prompts) for each candidate step.\nThe second structure-changing cog that Cognify supports is task ensembling. This cog spawns multiple parallel steps (or samplers) for a single step in the original workflow, as well as an aggregator step that returns the best output (or combination of outputs). By introducing parallel steps, Cognify can optimize these independently with step and weight cogs. This provides the aggregator with a diverse set of outputs to choose from."}, {"title": "4.2 Step Cogs", "content": "We currently support two step-changing cogs: model selection for language-model (LM) steps and code rewriting for code steps. For model selection, to reduce its search space, we identify \"important\" LM steps-steps that most critically impact the final workflow output to reduce the set AdaSeek performs TPE sampling on. Our approach is to test each step in isolation by freezing other steps with the cheapest model and trying different models on the step under testing. We then calculate the difference between the model yielding the best and worst workflow results as the importance of the step under testing. After testing all the steps, we choose the steps with the highest K% importance as the ones for TPE to sample from.\nThe second step cog Cognify supports is code rewriting, where it automatically changes code steps to use better implementation. To rewrite a code step, Cognify finds the k worst- and best-performing training data points and feeds their corresponding input and output pairs of this code step to an LLM. We let the LLM propose n new candidate code pieces for the step at a time. In subsequent trials, the optimizer dynamically updates the candidate set using feedback from the evaluator."}, {"title": "4.3 Weight Cogs", "content": "Cognify currently supports two weight-changing cogs: reasoning and few-shot examples. First, Cognify supports adding reasoning capability to the user's original prompt, with two options: zero-shot Chain-of-Thought [40] (i.e., \"think step-by-step...\") and dynamic planning [16] (i.e., \u201cbreak down the task into simpler sub-tasks...\u201d). These prompts are appended to the user's prompt. In the case where the original module relies on structured output, we support a reason-then-format option that injects reasoning text into the prompt while maintaining the original output schema.\nSecond, Cognify supports dynamically adding few-shot examples to a prompt. At the end of each iteration, we choose the top-k-performing examples for an LM step in the training data and use their corresponding input-output pairs of the LM step as the few-shot examples to be appended to the original prompt to the LM step for later iterations' TPE sampling. As such, the set of few-shot examples is constantly evolving during the optimization process based on the workflow's evaluation results."}, {"title": "5 Evaluation Results", "content": "We evaluate Cognify across six types of tasks: question-answering (HotPotQA [44]), text-to-SQL (BIRD [14]), data visualization (Mat-PlotAgent benchmark [45]), financial analysis (FinRobot [43]), code generation (HumanEval [6]), and BigBench [2]. The quality evaluators used in these tasks, respectively, are F1 score (text similarity to the ground truth answer), pass rate (exactly correct SQL results out of all test data), LLM-as-a-Judge, a combination of F1-score and LLM-as-a-judge, pass rate (exactly correct code results out of all test data), and exact match rate (for word sorting and object counting). Out of these workflows: FinRobot employs a leader-agent that routes requests to other different workers; text-to-SQL and data visualization both loop over model calls for iterative refinement; question-answering involves multi-step retrieval-augmented generation (RAG) [26]."}, {"title": "5.1 Overall Workflow Optimization Results", "content": "We first present the overall results across all workloads, highlighting the first four in Figure 2. Overall, Cognify improves generation quality (by up to 2.8\u00d7), reduces execution cost (by up to 10x), and execution latency (by up to 2.7\u00d7) compared to the original workflows across all the workloads by pushing their Pareto frontiers. Cognify also improves quality, cost, and latency over DSPy and Trace across workloads.\nComparing across workloads, Cognify has the largest improvements on HotpotQA (2.8\u00d7 over the original workflow using 40-mini) and data visualization (38% higher than the original workflow using only 40-mini or only Llama 3.1-8B). Cognify achieves its quality improvement for HotpotQA due to its good selection of few-shot examples at various edges. Cognify achieves its benefit for data visualization from inserting chain-of-thought reasoning at the beginning of the workflow and planning steps during the initial code generation phase. Cognify has the largest cost saving on text-to-SQL (10x cheaper) by introducing reasoning; the original workflow included a significant number of retries that Cognify avoids with improved generation in earlier steps of the workflow. Cognify has the largest latency cut for FinRobot (2.5\u00d7 faster) because the model selection cog chooses the faster Llama-8B model for certain steps.\nFor all workloads, DSPy performs better than Trace, although still worse than Cognify. This is because DSPy generates few-shot examples during its optimization process, whereas Trace primarily relies on rewriting user-annotate code blocks. While code rewriting may be effective to generate is especially ineffective for the Text-2-SQL and FinRobot workloads, as its optimized workflow yields 0 quality, i.e., no generated SQL queries / analyses are correct. This is because Trace has a strong tendency to overfit to specific training examples or generate erroneous code rewrites. DSPy is also not as effective on the FinRobot workload, as their optimized workflows have worse results than the original workflow running GPT-40-mini. DSPy is unable to combine complex prompt optizations (e.g.reasoning) with structured output generation, which is required by the FinRobot task.\nWe use three additional benchmarks, code generation from HumanEval, word sorting from BigBench, and object counting from BigBench, to demonstrate Cognify's ability to focus on a single objective, as DSPy and Trace are both designed for quality improvement. The evaluator in these tasks is pass rate (either the workflow output is correct or incorrect) or exact match. This forces a stricter quality expectation. Cognify improves the accuracy of code gen over the original workflow by 30% and over Trace and DSPy by 4%. On BBH-object counting, Cognify demonstrates an impressive 95% accuracy, which is 9% higher than Trace and 2.2\u00d7 higher than DSPy. On BBH-word sorting, Cognify is near-perfect, achieving 99% accuracy, which is 11% higher than Trace. Both DSPy (on word sorting) and the original workflow (on word sorting and objecting counting) are unable to generate the answer that matches exactly with the expected output. Cognify's code rewriter is primarily responsible for the improvement in quality on these workflows."}, {"title": "5.2 Detailed Evaluation Results", "content": "We now explain Cognify's benefits and sensitivity with more detailed experiments."}, {"title": "5.2.1 Search Effectiveness.", "content": "Comparison to grid search. To evaluate the effectiveness of Cognify's search, we perform an exhaustive grid search of 4096 configurations for the HotpotQA question-answering workflow. As indicated by the heatmap, Cognify quickly moves towards the highest quality and lowest cost in approximately 20 iterations. With just 1/32 of the grid search budget, Cognify finds 5 new points on the Pareto frontier and misses only one point found by grid search.\nLayering and search budgets. We validate our hypothesis that an increasing budget should correspond to an increase in the number of layers used by Cognify. When the budget is small (i.e., 16 iterations), a single layer will yield the best results due to faster convergence to a local optimum. As budget increases, layering allows a more diverse exploration of the search space. At 64 iterations, a 2-layer approach performs the best, and at 128 iterations (our maximum budget used for experiments), 3 layers is superior. This pattern holds across quality, cost, and latency independently.\nSensitivity to training input size. We evaluate the effectiveness of AdaSeek with varying training input sizes on the FinRobot workload With just 6 or more examples, AdaSeek is able to find higher quality optimized workflows. In less than 50 examples, AdaSeek generates a 13% improvement in quality and a 19% improvement in less than 100 examples. The only instance in which the search degrades in quality relative to the original workflow is when the training dataset contains only 3 examples, which is an unlikely scenario for workflow developers."}, {"title": "5.2.2 Ablation Study.", "content": "To understand where Cognify's benefits come from, we evaluate the effect of different techniques by adding one at a time on the Text-to-SQL workload. The initial version is a non-hierarchical search approach that places all cogs in a single layer. We then incorporate our adaptive hierarchical layer approach and budget distribution, which results in improved quality configurations after 24 search iterations. Afterward, we add the per-layer chunk-based successive-halving approach, yielding additional quality improvements as seen by the green line. Finally, we enable early exiting, achieving the best quality as shown in the red line."}, {"title": "5.2.3 Search Time and Cost.", "content": "We compare the optimization cost and time of Cognify and DSPy. Cognify completes its optimization in 1.7\u00d7 to 2.5\u00d7 less time than DSPy due to Cognify's efficient use of the overall training budget and its parallel search mechanism. While DSPy's optimizer can be marginally cheaper in $ cost for smaller workflows, its cost bloats for more complex workflows. For example, the cost of optimizing text-to-SQL with DSPy is $24, which is 2.4\u00d7 more expensive than Cognify."}, {"title": "6 Related Works", "content": "This section discusses works related to us."}, {"title": "Gen-AI workflow developing frameworks.", "content": "Recent years have seen a surge of programming frameworks that facilitate the development of gen-Al workflows, such as LangChain [24], LlamaIndex [27], OpenAI Swarm [4], CrewAI [10], and Dify [12], Vellum [38], and Coze [9]. These programming frameworks allow programmers to more easily develop and test their workflows, but does not offer workflow autotuning capabilities. Cognify's design is compatible with these frameworks, as it can be used as a tool after developers write their programs with these frameworks. For example, Cognify currently supports out-of-the-box LangChain and DSPy programs."}, {"title": "Gen-Al workflow autotuning systems.", "content": "While this paper provides the first comprehensive formalization and solution for gen-Al workflow autotuning, there are a few other works targeting the optimization of gen-AI workflows, primarily LLM-based workflows. As seen, Cognify is the first autotuning system that incorporates workflow structure change, allows for multiple optimization objectives, and is fully extensible.\nExisting gen-AI workflow optimizers can be categorized into two groups based on their optimization approaches. The first group relies on an LLM to propose workflow changes and guide workflow autotuning. For example, OPRO [42] and Agent Symbolic Learning (Symbolic) [50] use LLMs to directly refine prompts of language model calls in a workflow. TextGrad [48] lets an LLM evaluate the result of a workflow with an LLM-generated \"loss\u201d and asks an LLM to improve prompts at different LM call sites based on the loss (\"backpropagating\" the textual feedback). Trace [7] extends this concept of LLM-based backpropagation to let LLMs rewrite user-annotated code blocks. Different from these works, Cognify takes a data-driven approach; its workflow optimization is based on the sampled evaluation of workflow end results instead of asking the LLM for feedback. While an LLM can be useful in proposing improvements to the workflow, it is less stable as a feedback mechanism, as shown by our superior results than Trace."}, {"title": "The second group searches over optimization options guided by workflow evaluation results.", "content": "DSPy [20, 21, 33] is a gen-Al workflow programming and optimization framework that applies various prompt tuning techniques like adding few-shot examples and CoT prompts for improving workflow generation quality. It supports several variations of OPRO as the search optimizer [33]. Unlike Cognify, DSPy does not adapt their search according to total budgets and only focuses on prompt tuning for higher quality. GPTSwarm [51] optimizes DAG workflows by iteratively updating nodes and edges using the REINFORCE algorithm [41]. Cognify supports generic graphs, including ones that contain cycles, and supports step changes. Furthermore, Cognify adapts to limited budgets, whereas GPTSwarm requires orders of magnitude more optimization iterations due to its use of reinforcement learning."}, {"title": "Single model call optimizers.", "content": "There are several optimizers for a single call to gen-AI models. For example, RouteLLM [30] and TensorOpera-Router [37] train a model to route LLM requests to a more cost-effective model. FrugalGPT [5] sequentially retries a request with more expensive models until a particular score threshold is met. Differently, Cognify targets the optimization of an entire workflow, where optimizing steps in isolation does not efficiently or effectively work at the workflow level."}, {"title": "7 Conclusion", "content": "This paper formalizes the problem of gen-AI workflow autotuning and outlines the key differences between it and traditional ML autotuning/training. Based on our insights, we proposed an adaptive, hierarchical BO-based optimization algorithm, AdaSeek and demonstrated its robustness. We presented Cognify, a multi-objective gen-AI workflow autotuning platform we built on top of AdaSeek. Our evaluation results show that Cognify significantly pushes the Pareto frontier on generation quality, cost, and latency for workflows across a wide range of domains. We also demonstrated the robustness of the AdaSeek algorithm, allowing Cognify to serve as a generic, extensible, open-source gen-AI workflow autotuning platform future researchers and practitioners can leverage."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Bayesian Optimization", "content": "Assuming our goal is to minimize the objective function f(x). At each iteration i, Bayesian optimization (BO) constructs a surrogate model S := p(y | x, D) to approximate the objective function f, where the set D = {(xn, yn)}n=1 contains previously observed data points. BO also leverages an acquisition function to balance exploration and exploitation, based on predictions from S. The optimization process is be summarized in Alg 3. A widely used acquisition function is Expected Improvement (EI) [18]:\nEly\u2217 (x) := \u222b\u2212\u221ey\u2217 (y\u2217 \u2212 y)p(y | x, D)dy\nEl intuitively indicates the expectation that the objective function f(x), approximated by S, will be below a certain threshold y\u2217."}, {"title": "A.1.1 Optimizing El with Tree-structured Parzen Estimator (TPE).", "content": "In this work, we choose TPE as our surrogate model for its natural support of mixed discrete and continuous search space and efficiency in model building. TPE [?] employs kernel density estimators (KDEs) as the probabilistic model. Instead of modeling p(y | x, D) directly, it models p(x | y, D) by:\np(x | y, D) := {l(x) := p(x | D<\u03b3)) if y \u2264 y\u2217g(x) := p(x | D>\u03b3)) if y > y\u2217\nwhere the entire observation set D is separated into two groups and y\u03b3 is the top \u03b3 quantile evaluation results in D. The control parameter \u03b3 is calculated based on |D| in each iteration [39]. The next configuration x\u2217 to evaluate is proposed with:\nx\u2217 := argmaxx\u2208X l(x)/g(x)"}, {"title": "A.2 Budget Constraints", "content": "Here we show that each layer I will follow the assigned budget B\u1d62 in 2. Starting from the outer-most layer. The number of proposed configurations at this layer is C\u2081 = KW \u2264 B\u2081. The total number of budget consumed at the next layer is:\nC\u2082 = \u2211\u1d62\u2211\u2096 K\u1d62 S\u1d62\n\u2264 \u2211\u1d62\u2211\u2096 (B\u2081\u1d62 / KW\u1d62) S\u1d62\n\u2264 B\u2081 \u2211\u1d62 (K\u1d62 / KW\u1d62) = B\u2081 B\u2082 / RW\n\u2264 B\u2081 B\u2082\nwhich aligns with the constraint of B\u2081 B\u2082. The calculation can be easily extended to three-layer situation, where multiple instances of the search at the middle layer will be executed each with an uneven budget. The total number of evaluation required is:\nC\u2083[i] C\u2082[i] \u2264 B\u2083\nC\u2083 = \u2211\u1d62 C\u2083[1] \u2264 B\u2083 \u2211\u1d62 C\u2082[1]\u2264 B\u2083 \u2211\u2096 K\u1d62 S\u1d62 \u2264 B\u2083 B\u2082\nwhere c\u2081[i] represents the budget assigned to ith optimize instance at search layer 1. Sum of each c\u2081 [i] at layer i stands for the total number of configurations proposed at that layer, which will be the product of all preceding budgets."}, {"title": "A.3 Programming Model", "content": "Cognify natively supports programs written in LangChain/LangGraph and DSPy. In addition, Cognify comes with its own programming model: cognify. Model and cognify. StructuredModel. These two constructs serve as drop-in replacements for all model calls in a user's program. There are four necessary components to construct a cognify. Model:\n(1) System prompt: this defines the role of the model\n(2) Inputs: placeholder values that are filled in at runtime based on the end-user request\n(3) Output format: either a label assigned to the output value or a schema\n(4) LM backend: the model that will be called (in the absence of model selection)"}, {"title": "A.4 Example Workflow", "content": "The following is the workflow used for the data visualization.\nAn example optimization for this workflow is as follows:\n\u2022 Query expansion: Few-shot example (shown below)\n\u2022 Initial code generation: Reasoning (with planning option, i.e., \"break down the task into simpler sub-tasks...\")\n\u2022 Debugger: Reasoning (with chain-of-thought option, i.e., \"think step-by-step...\")\nThe example used for query expansion was selected due to strong performance during the optimization process:"}]}