{"title": "Stochastic Communication Avoidance for Recommendation Systems", "authors": ["Lutfi Eren Erdogan", "Vijay Anand Raghava Kanakagiri", "Kurt Keutzer", "Zhen Dong"], "abstract": "One of the major bottlenecks for efficient deployment of neural network based recommendation systems is the memory footprint of their embedding tables. Although many neural network based recommendation systems could benefit from the faster on-chip memory access and increased computational power of hardware accelerators, the large embedding tables in these models often cannot fit on the constrained memory of accelerators. Despite the pervasiveness of these models, prior methods in memory optimization and parallelism fail to address the memory and communication costs of large embedding tables on accelerators. As a result, the majority of models are trained on CPUs, while current implementations of accelerators are hindered by issues such as bottlenecks in inter-device communication and main memory lookups. In this paper, we propose a theoretical framework that analyses the communication costs of arbitrary distributed systems that use lookup tables. We use this framework to propose algorithms that maximize throughput subject to memory, computation, and communication constraints. Furthermore, we demonstrate that our method achieves strong theoretical performance across dataset distributions and memory constraints, applicable to a wide range of use cases from mobile federated learning to warehouse-scale computation. We implement our framework and algorithms in PyTorch and achieve up to 6\u00d7 increases in training throughput on GPU systems over baselines, on the Criteo Terabytes dataset.", "sections": [{"title": "I. INTRODUCTION", "content": "A significant portion of machine learning research has advanced due to better memory and computational speeds of accelerators, alongside faster interconnects and more efficient parallelization in large systems. However, accelerators often have limited memory compared to CPUs, rendering many memory-intensive algorithms infeasible for deployment. One approach to mitigate this issue is to increase memory, but this cannot keep up with the rapid growth of machine learning models. An alternative is to develop new parallelization strategies that balance memory usage and communication, as explored in strategies like those in [1]\u2013[3]. Another optimization strategy involves quantization [4]\u2013[6], which aims to minimize the memory footprint and computational requirements without significantly impacting accuracy. Embedding tables, which map sparse categorical features to dense vectors [7], [8], are often prime targets for quantization due to their large sizes. By quantizing the embeddings to lower bit-width representations in DLRMs [9], such as 4-bit [10], [11], or performing tensor train decomposition [12], memory usage can be significantly reduced, making it more feasible to train and inference. These methods primarily focus on reducing the embedding size [13], while another solution is to modify the training process to save memory. Examples of this approach include reversible networks [14], which change the model's structure, and techniques like Checkmate [15], which alter the model's execution pattern by adding additional operations to backpropagation to decrease the number of intermediate values that need to be stored in memory.\nRecent algorithms have significantly increased the scale of NLP and CV models by reducing the memory demands per GPU [16]\u2013[18], allowing the use of accelerators for extremely large models. However, these methods struggle with models that have large embedding tables, which are not easily managed by pipeline parallelism and remain large in parameter count. Data parallelism also falls short as it is better suited for compute-heavy tasks rather than memory-intensive embedding operations. Furthermore, techniques like recomputation or checkpointing do not suit embeddings well, as their high memory cost does not justify the modest savings from managing intermediate activations.\nThis forces the use of model parallelism, but oftentimes the number of accelerators required to fit the large embedding tables is too great to make model parallelism a financially viable solution.\nThese embeddings can often be terabytes large, but as observed in practice, they are not accessed uniformly at random. In real datasets, the access pattern of these embeddings varies, generally with a small portion of embeddings being accessed far more frequently than others [19]. Existing methods [20] have explored the usage of distributed communication to decrease the communication cost, but the theoretical bounds of communication efficiency has not been analyzed in previous work. In addition, there has been no existing work exploring how various methods of distributing embeddings across GPUs and CPUs impact the system performance.\nTo summarize, our contributions are as follows:\n1) We develop a simple framework to calculate the ex-pected communication cost under a training regime. And we expand this framework to address considerations such as determining the optimal levels of communication and caching, as well as methods for adjusting them accordingly.\n2) We use the above framework to obtain communication strategies that minimize expected communication costs without caching. We demonstrate that these methods"}, {"title": "II. METHODOLOGY", "content": "As shown in Fig. 1, the existing training paradigm for the above model is as follows\n1) A batch of training examples is retrieved\n2) For every training example, the embeddings are retrieved from embedding tables in the main memory\n3) The batch of embeddings is sent to the GPU\nIn the following chapters, we will analyze the communica-tion cost of the embedding layers of the recommendation sys-tems and propose algorithms that maximize throughput subject to memory, computation, and communication constraints.\nA. Coalescing on device\nTo start, we wish to estimate how much overlap exists per batch of the dataset. To do so, we want to find the expected number of unique elements in the list and the expected total communication given a batch size b. If we transmit the batch of b training examples, the communication cost per feature is $b \\times d$, where d is the number of lookups per sample. We call this \"communicating the batch\", as we send over the entire batch.\nAs depicted in Fig. 2 we can alternatively coalesce the embeddings into a list of unique embeddings in the batch as well as their indices. For embedding e, let P(e) be the probability that e refers to a categorical feature in a training example chosen uniformly at random from the dataset. The likelihood that e is transmitted within our batch is computed by taking the complement of the probability that no embedding for that feature in our batch is e:\n$1- (1 \u2013 P(e))^b$                                                                                                                                                (1)\nUsing (1), if E is the set of possible embeddings for this fea-ture, we can then find our expected embedding communication cost as:\n$\\sum_{e \\in E} 1 - (1 \u2013 P(e))^b$                                                                                                                                                 (2)\nIn addition to transmitting the unique embeddings as de-scribed in (2), we also have to transmit the indices of each embedding. This means that we have a constant b cost. As a result, our net cost is\n$b + \\sum_{e \\in E} 1 - (1 \u2013 P(e))^b$                                                                                                                                                 (3)\nWe call this method \"coalescing\" because coalesces, or combines distinct, embeddings for each feature.\nB. Caching on device\nWe can also use the GPU as a cache for our embeddings as described in Fig. 3, we examine several ways of determining what embeddings to store on GPU."}, {"title": "", "content": "The first method: minimize communication bandwidth per epoch. For this method, we assume that the batch size is sufficiently large on GPU to saturate communication during training, and as a result, the time it takes to execute computa-tions using larger batch sizes is proportional to the batch size itself. As a result, the expected time spent on computation per epoch is constant, and the primary source of change in the latency is how much time is spent on communication per epoch.\nWe shall let the number of samples in the dataset be Q, batch size b, and the lookups per sample be d. Our expected communication cost is equal to the expected number of batches times the expected communication per batch. Without caching or overlap, this is equal to:\n$\\frac{Q}{b} \\times b \\times d = Q \\times d$                                                                                                                                                 (4)\nIf we exploit the overlap between lookups, the communica-tion cost changes to\n$\\frac{Q}{b} + \\sum_{e \\in E} (1-(1-P(e))^{b}) \\times d$                                                                                                                                                 (5)"}, {"title": "", "content": "The first term in (5) represents the cost of sending indices, while the second term is the cost of sending the embeddings. Generally speaking, the communication cost without caching will decrease as batch size increases, but the memory re-quirements will also increase. The tradeoff between these two values depends on the distribution of embeddings.\nIf we utilize the remaining memory to cache embeddings, we can remove the communication cost of embeddings that are cached on device. Let the set of embeddings cached on the device be C\n$\\frac{Q}{b} \\sum_{e \\in E/C} (1-(1-P(e))^{b}) \\times d$                                                                                                                                                 (6)\nHowever, due to the fact that we have limited memory, there is a direct trade-off between the batch size and the number of cached embeddings.\nAs a result, if we model the memory usage of the cached embeddings, we can theoretically calculate the largest potential batch size on device. While in practice compilers do not allo-cate memory with this theoretical efficiency, it helps illustrate the mathematics of the tradeoffs. Define the total number of parameters that can fit on the device to be M, the parameters used for running the model per sample to be a, i.e. space taken up by both model's weights, biases and also space required for intermediate activations of a single sample. Then, given that |C| embeddings are cached, and each take up d parameters on device, as before, the maximum batch size possible is\n$b = \\frac{M-|C|d}{a}$                                                                                                                                                 (7)\nThis introduces a tradeoff between the amount of communi-cation saved by overlap and the amount of communication saved by caching, because the higher our batch size is the more overlap will occur but the less memory will be available for caching.\nIn order to understand the efficacy of this tradeoff, we need to examine the relative change in communication from caching one additional embedding, e'.\nLet C be the current set of cached embeddings and b be the maximum batch size with C cached.\nLet C' = $C\\cup e'$, b' be be the maximum batch size with C' cached,\nIf we store e', our expected decrease in communication cost is $1 \u2013 (1 \u2013 P(e'))^b$, which is the likelihood e' is in a batch. However, using (7), our expected batch size decreases by $b - b' = \\frac{d}{a}$, or the size of one embedding divided by the size of activation. This both decreases communication and decreases potential overlap, using (6) and (7), we can mathematically say that\n\u2206communication/epoch = commn \u2013 commn                                                                                                                                                (8)\nwhere,\ncommn = $\\sum_{e \\in E/C} (1 \u2013 (1 \u2013 P(e'))^{b'}) \\times \\frac{Q}{b'}$                                                                                                                                                 (9)\n$\\frac{commn}{commn} = \\frac{\\sum_{e \\in E/C} (1 \u2013 (1 \u2013 P(e'))^{b'}) \\times \\frac{Q}{b'}}{\\sum_{e \\in E/C} (1 \u2013 (1 \u2013 P(e'))) \\times \\frac{Q}{b}}$                                                                                                                                                 (10)\nFor the communication to decrease, we need (8) to be negative. We can separate out the term for e' to get that\n$(1-(1-P(e'))^{b'}) \\geq t_1$                                                                                                                                                 (11)\nwhere\n$t_1 = \\frac{\\sum_{e \\in E/C'} (b(1 \u2013 (1 \u2013 P(e))') \u2013 b'(1 \u2013 (1 \u2013 P(e))))}{b'}$                                                                                                                                                 (12)\nWhen M >> a > d, or when available memory is significantly larger than the memory needed for running model on a one sample, and the memory needed for running a model on a single sample is larger than the memory needed for an embedding, caching generally improves performance in use cases where the dataset is randomized, such as offline training. Because this equation minimizes communication bandwidth, it only serves as an accurate model of communication when communication is dominated by bandwidth.\nIn practice, this means the caching generally decreases the cost of communication relative to relying on coalescing for large batch size training.\nIn addition to using this to understand the tradeoff between the batch and the number of cached embeddings, it is possible to find the theoretically optimal number of embeddings to min-imize communication bandwidth in O(log(|E|)) steps using binary search. However, the embedding memory and activation memory need to be measured empirically, as existing machine-learning platforms often allocate more memory than necessary to improve performance.\nFurthermore, (13) is also a good proxy for the expected number of accesses to main memory for embeddings. This is because cache performance for these embeddings are poor, and as a result most embeddings communicated over the channel are looked up in main memory.\n$d(b-b') \\geq \\frac{M-|C|}{ \\sum_{e \\in E/C'} (1-P(e'))}$                                                                                                                                                 (13)\nIf the batch size is too small to saturate computation after communication, the size of the cached set can be determined by measuring the runtime of the model on various splits of the data. This is much more time-intensive, but can potentially lead to more accurate results.\nWhen analyzed with the different dataset distributions, our method displays strong theoretical performance. We use three distributions, Zipf distribution (P(x) ~ 1/x = $e^{-log(x)}$), exponential distribution (P(x) ~ 1/x = $e^{x}$), and half normal distribution (P(x) ~ 1/x = $e^{-x^2}$), to scale to 5\u00d7 number of embeddings and 5\u00d7 batch size. Half-normal distribution is of particular importance since the Criteo Terabyte Dataset is most similar to that distribution. With our method, the total commu-nication cost increases by < 1.5\u00d7 for the exponential and the normal distributions and by < 2\u00d7 for Zipf distribution while"}, {"title": "", "content": "with prior methods, the total communication cost increases by 5x. That is a 3\u00d7 increase in theoretical performance."}, {"title": "III. EXPERIMENT SETTINGS", "content": "We evaluate our method using DLRM [9] on the Criteo Terabyte Dataset. Since our method and our theoretical frame-work is applicable to any arbitrary distributed system that uses lookup tables, we expect the same performance gains if we have used TBSM [21] on the Alibaba UBA dataset.\nBoth models, for their respective datasets, are usually trained on CPUs. This is due to the limited memory availability for GPU training.\nWe compare our method against baseline Deep Learning Recommendation Model (DLRM) implementations that use only CPUs or a combination of CPUs and GPUs without caching. The CPU-only setup processes the entire computation graph, including the DNN's large tensor operations, on the CPU-tasks for which the CPU is not optimized. The CPU-GPU setup assigns the memory-intensive embedding layer to the CPU and the compute-intensive DNN layers to the GPU, resulting in CPU-GPU communication overhead during the backward pass and when handling intermediate results, which extends the training time.\nThe effectiveness of caching hot embeddings in mini-batches hinges on using only the cached (hot) embeddings and avoiding the cold embeddings stored on the CPU, thus eliminating data shuffling between the CPU and GPUs during the embedding layer. While it's unrealistic to expect all mini-batches to require only hot embeddings, it is feasible to ensure that most do. This is achieved by classifying training samples into \"hot\" (those that only need hot embeddings) and \"normal\" (those that require both hot and cold embeddings). We can then create mini-batches exclusively composed of hot samples, and others of normal samples. Given that hot samples generally predominate, this method maximizes the use of cached embed-dings. This classification involves constructing a ranked skew table for all embeddings, selecting the top few embeddings for caching, and then categorizing training samples based on their dependency on these cached embeddings.\nFor all of the experiments, we benchmark the iteration time and the total training time for one epoch."}, {"title": "IV. EXPERIMENTS", "content": "We first evaluate our results on the Criteo Terabyte dataset. For this method, we trained a DLRM model with the default parameters on two NVIDIA TITAN RTX with 24GB of memory. For our first experiment, we wanted to devise a simple experiment to verify the benefit of overlapping and caching on the iteration time and the total training time for one epoch. We set the batch size to 2048 for all runs, and we cache 256MB of the hot embeddings with our method.\nFrom Table I, we can see that models with caching and coalescing achieve large performance improvements relative to baselines. We even argue that without caching, the CPU-GPU implementation is worse than the CPU-only implementation. This means that the state-of-the-art implementations cannot utilize the hardware accelerators, leading to scalability problems; while, caching enables us to integrate fast accelerators into deep recommendation systems training by avoiding CPU-GPU communication."}, {"title": "V. CONCLUSION", "content": "In this work, we develop a versatile framework that mod-els the communication costs of various distributed systems. Leveraging this framework, we minimize the expected costs by applying effective communication strategies for large-scale recommendation system training. Specifically, our method utilizes overlapping embeddings to transmit only the unique embeddings along with their indices for each feature. Ad-ditionally, we leverage the GPU as a cache for frequently accessed embeddings. We explore several methods to deter-mine which embeddings to store on the GPU and derive a theoretical solution addressing the tradeoff between batch size and the size of the cached embeddings, as well as theory-inspired experimental solutions. As a result, our method generalizes well across different datasets and systems with varying memory constraints. Furthermore, our PyTorch im-plementation demonstrates up to 6\u00d7 improvements in training and convergence times on large GPU systems using both the Criteo Terabyte and Alibaba User Behavior datasets."}]}