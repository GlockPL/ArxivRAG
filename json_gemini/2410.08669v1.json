{"title": "SMARTPRETRAIN: MODEL-AGNOSTIC AND DATASET- AGNOSTIC REPRESENTATION LEARNING FOR MOTION PREDICTION", "authors": ["Yang Zhou", "Hao Shao", "Letian Wang", "Steven L. Waslander", "Hongsheng Li", "Yu Lin"], "abstract": "Predicting the future motion of surrounding agents is essential for autonomous vehicles (AVs) to operate safely in dynamic, human-robot-mixed environments. However, the scarcity of large-scale driving datasets has hindered the development of robust and generalizable motion prediction models, limiting their ability to capture complex interactions and road geometries. Inspired by recent advances in natural language processing (NLP) and computer vision (CV), self-supervised learning (SSL) has gained significant attention in the motion prediction community for learning rich and transferable scene representations. Nonetheless, existing pre-training methods for motion prediction have largely focused on specific model architectures and single dataset, limiting their scalability and generalizability. To address these challenges, we propose SmartPretrain, a general and scalable SSL framework for motion prediction that is both model-agnostic and dataset-agnostic. Our approach integrates contrastive and reconstructive SSL, leveraging the strengths of both generative and discriminative paradigms to effectively represent spatiotemporal evolution and interactions without imposing architectural constraints. Additionally, SmartPretrain employs a dataset-agnostic scenario sampling strategy that integrates multiple datasets, enhancing data volume, diversity, and robustness. Extensive experiments on multiple datasets demonstrate that SmartPretrain consistently improves the performance of state-of-the-art prediction models across datasets, data splits and main metrics. For instance, SmartPretrain significantly reduces the MissRate of Forecast-MAE by 10.6%. These results highlight SmartPretrain's effectiveness as a unified, scalable solution for motion prediction, breaking free from the limitations of the small-data regime. Codes are available at our webpage.", "sections": [{"title": "1 INTRODUCTION", "content": "Motion prediction, predicting the future states of space-sharing agents nearby (e.g., vehicle, cyclist, pedestrian) is crucial for autonomous driving systems (Hu et al., 2023; Shao et al., 2023b;a; 2024) to safely and efficiently operate in the dynamic and human-robot-mixed environment. Context information, including surrounding agents' states and high-definition maps (HD maps), provides critical geometric and semantic information for motion behavior, as agents' behaviors are highly dependent on interactions with surrounding agents and the map topology. For example, agents' interactive cues, such as yielding, would influence other agents' decision-making, and vehicles usually move in drivable areas and follow the direction of lanes. Thus, designing (Chai et al., 2019a; Gao et al., 2020; Liang et al., 2020; Wang et al., 2022) and learning (Salzmann et al., 2020; Ngiam et al., 2021; Varadarajan et al., 2022; Zhao et al., 2021) scene representation that captures rich motion and context information has long been a core challenge for motion prediction."}, {"title": "2 RELATED WORK", "content": ""}, {"title": "2.1 TRAJECTORY PREDICTION", "content": "Traditional methods for motion prediction primarily use Kalman filtering (Kalman & Others, 1960) with physics and maneuver priors from HD-maps to predict future motion states (Houenou et al., 2013; Xie et al., 2017; Shao et al., 2023a), or sampling-or-optimization-based planning algorithms with manually specified or learned reward functions to generate future trajectories (Wang et al., 2021; 2023a; Schwarting et al., 2019; Li et al., 2022). With the rapid development of deep learning, recent works utilize data-driven approaches for motion prediction. Generally, these methods fit into three different architectures involving rasterized images and CNNs (Cui et al., 2019; Chai et al., 2019b), vectorized representations and GNNs (Zhou et al., 2022; 2023; Park et al., 2023; Tang et al., 2024) and transformers (Wang et al., 2023b; Nayakanti et al., 2023; Shi et al., 2024). To represent scene information, raster-based methods utilize CNNs to rasterize scene context into a bird-eye-view image, while the other two architectures represent each entity of the scene as a vector following Gao et al. (2020). For the multimodal trajectory outputs, in addition to the standard one-stage prediction pipelines which output trajectories directly, there are now two-stage prediction-"}, {"title": "2.2 SELF-SUPERVISED LEARNING IN TRAJECTORY PREDICTION", "content": "Self-supervised learning (SSL) methods have been applied widely in both visual understanding (Chen et al., 2020; Grill et al., 2020; Caron et al., 2021; He et al., 2022; Benaim et al., 2020; Bertasius et al., 2021; Feichtenhofer et al., 2022) and natural language processing (Devlin et al., 2019; Brown, 2020; Touvron et al., 2023). SSL aims to learn informative and general representations via carefully designed pretext tasks, which can be fine-tuned for downstream tasks with supervision. Recently, the trajectory prediction community has made significant strides in incorporating SSL techniques. These methods utilize pretext tasks to pre-train models, allowing them to learn valuable representations that can be fine-tuned for enhanced trajectory prediction performance. Existing pre-training pipelines for trajectory prediction can be divided into three different categories, involving augmented or synthetic data (Yang et al., 2023; Li et al., 2024), contrastive learning (Xu et al., 2022; Bhattacharyya et al., 2022; Azevedo et al., 2022; Pourkeshavarz et al., 2023) and generative masked representation learning (Chen et al., 2023; Cheng et al., 2023; Lan et al., 2024).\nMethods with synthetic data generate trajectory and map data with prior knowledge and manually designed rules. For example, Li et al. (2024) generates driving scenarios with a map augmentation module and a model-based planning model. Contrastive learning methods aim to align and differentiate embeddings by comparing them with positive and negative examples, capturing high-level semantic relationships. For example, PreTraM (Xu et al., 2022), a raster-based approach, uses contrastive learning to model the relationships between trajectories and maps, as well as between the maps themselves. On the other hand, generative masked representation methods leverage the transformer (Vaswani et al., 2023) to learn token relationships by reconstructing randomly masked HD map context and trajectories, inspired by the broad success of Masked Autoencoders (MAE) (He et al., 2022). For example, Forecast-MAE (Cheng et al., 2023) treats agents' histories and lane segments as individual tokens, applies the random mask at the token level, and feeds the masked tokens into the transformer backbone for reconstruction.\nExisting SSL pipelines have two major limitations: 1) they pre-train on a single trajectory prediction dataset due to the varied formats in different datasets and 2) they rely on specific model architectures and map embeddings, limiting their adaptability to general models. Additionally, these pipelines struggle to extend to advanced GNN-based approaches, which integrate all inputs into a unified graph, preventing the use of explicit map embeddings for pretext tasks. To overcome these challenges, we propose SmartPretrain, a model-agnostic and dataset-agnostic solution. It can flexibly apply to various models and datasets, regardless of model architecture or dataset format."}, {"title": "3 METHODOLOGY", "content": ""}, {"title": "3.1 PROBLEM FORMULATION - MOTION PREDICTION MEETS SELF-SUPERVISED LEARNING", "content": "Typical motion prediction task can be defined as follows: taking the observed states of the target agent $s_h = [s_{-T_h+1}, s_{-T_h+2}, ..., s_0] \\in \\mathbb{R}^{T_h \\times 2}$ over the last $T_h$ steps, we aim to predict its future states $s_f = [s_1, s_2, ..., s_{T_f}] \\in \\mathbb{R}^{T_f \\times 2}$ for $T_f$ future steps as well as the associated probabilities $p$. Naturally, the target agent will interact with its context $c$, including observed states of surrounding agents, and the HD map. The typical motion prediction task is formulated as $(s_f, p) = f(s_h, c)$, where $f$ denotes the prediction model. Generally, motion prediction models are structured in an encoder-decoder architecture with two stages:\n\u2022 Encoding $z = f_{enc}(s_h, c)$: an encoder $f_{enc}$ embeds and fuses $s_h$ and $c$ to capture the evolution and interaction in the traffic scene, and generate trajectory embedding $z$;\n\u2022 Decoding $(s_f, p) = f_{dec}(z)$: a decoder $f_{dec}$ decipher $z$ to generates multiple possible future trajectories $s_f$ and corresponding probabilities $p$.\nSSL takes a pivot from the typical motion prediction learning by introducing a self-supervised pre-training phase. The objective is to pre-train the encoder $f_{enc}$ with pretext tasks, to learn more"}, {"title": "3.2 SMARTPRETRAIN SSL FRAMEWORK", "content": "We propose SmartPretrain, a model-agnostic and dataset-agnostic pre-training framework for motion prediction, that can be flexibly applied to a range of models regardless of their architectures, and leverage various datasets despite their differences in data formats. As illustrated in Fig. 2, Smart- Pretrain is composed of two parts: 1) a dataset-agnostic scenario sampling strategy for constructing representative and diverse pre-training data, and 2) a model-agnostic SSL strategy, consisting of two trajectory-focused pretext tasks, trajectory contrastive learning (TCL) and trajectory reconstruction learning (TRL), which together shape the pre-training to improve performance on downstream tasks."}, {"title": "3.2.1 DATASET-AGNOSTIC SAMPLING", "content": "In the dataset-agnostic temporal sampling process, we aim to create positive and negative pairs for contrastive learning and construct trajectories for reconstructive learning, while ensuring data consistency across various datasets.\nData Sampling for Contrastive/Reconstructive SSL. Starting with formulating positive and negative samples for contrastive learning, one intuitive design would be contrasting different agents' trajectories within the same scenario. However, this approach may lead to suboptimal performance due to 1) an overemphasis on learning spatial context among agents without sufficient temporal modeling and 2) limited variability in positive samples, as they contain the same features. To this end, we propose a temporal sampling strategy to create sample pairs that capture both the spatial context and the temporal evolution of the agents. Specifically, we mix multiple datasets to form a comprehensive data bank, and randomly sample a scenario with a time horizon $T = T_h + T_f$. Next, we temporally sample two sub-scenarios, which have the same temporal horizon $T_h$, but start at different time $t$ and $t'$ respectively. To prevent information leakage during sub-scenario sampling, we ensure that the two sub-scenarios of a single scenario do not overlap, which could compromise the training of the pretext tasks. The two sub-scenarios are then later used to construct a positive trajectory pair from the same agent across different time, and negative pairs from different agents or different time. Note that, beyond contrastive learning, the sub-scenario starting at $t$ will also be used for reconstructive learning, as its temporal horizon is designed to align well with the input horizon $T_h$ of the target downstream dataset.\nMaintaining Dataset-Agnosticism. To leverage multiple datasets with varied configurations and achieve data scaling, we introduce three key designs:"}, {"title": "3.2.2 MODEL-AGNOSTIC CONTRASTIVE AND RECONSTRUCTIVE SSL", "content": "We propose a model-agnostic SSL strategy that consists of two pretext tasks: 1) a trajectory contrastive learning task (TCL), that enriches learned trajectory embeddings by contrasting them across agents and time windows; 2) a trajectory reconstruction learning task (TRL), that aligns more closely with the primary goal of motion prediction, better shaping the direction of pre-training. Note that both pretext tasks are designed to ensure model-agnosticism via their trajectory-focus: they only contrast and reconstruct agents' trajectory embeddings, rather than any other embeddings such as map or customized embeddings. Thus they remove restrictions on the model architectures and map representations, and can be applied to a much wider variety of motion prediction models.\nEmbedding Generation. With 2 sampled sub-scenarios, we then generate embeddings for all the trajectories they contain. Specifically, we follow a self-training strategy from existing SSL literature (He et al., 2020; Caron et al., 2021) to bootstrap performance and avoid model collapse. Specifically, the two sub-scenarios are fed into two identical architecture branches, an online branch and a momentum branch. Within each branch, the input sub-scenario is first passed through the motion prediction model to generate all trajectories' embeddings, which are then fed to a projector for further encoding modification. During pre-training, the online branch is continuously updated, while the momentum branch is occasionally updated using an exponential moving average mechanism. The embeddings from the online branch are also additionally passed through a contrastive predictor to generate the final embeddings for contrastive learning.\nTrajectory Contrastive Learning (TCL). TCL is designed to learn rich trajectory representations by contrasting trajectory embeddings across spatiotemporal dimensions. Specifically, considering that the sampled scenarios in a mini-batch consist of $N$ agents, we now have two sets of embeddings ${Z_{i,t}}_{i=1}^N$ and ${Z'_{j,t'}}_{j=1}^N$, generated from the online branch and momentum branch respectively. We define the contrastive loss to pull closer positive samples, and repel away negative samples:\n$\\begin{aligned}L_c = -\\frac{1}{N} \\sum_{i=1}^N log \\frac{exp (r(z_{i,t}, z'_{i,t}) / \\tau)}{\\sum_{j=1, j\\neq i}^N exp (r(z_{i,t}, z_{j,t}) / \\tau) + \\sum_{j=1}^N exp (r(z_{i,t}, z'_{i,j}) / \\tau)},\\end{aligned}$  (1)\nwhere $r$ denotes the cosine similarity measure, and $\\tau$ denotes the temperature hyper-parameter. Specifically, the numerator term represents the similarity of positive sample pairs $(z_{i,t}, z'_{i,t})$, namely trajectory embeddings from the same agent in different timelines. The denominator term consists of two types of negative pairs: 1) intra-repelling pairs $(z_{i,t}, z_{j,t})$: the trajectory embeddings from other agents of the same sub-scenario; 2) inter-repelling $(z_{i,t}, z'_{j,t})$: trajectory embeddings from different sub-scenarios. Through this objective, the similarities of the same agent's embeddings are maximized, and those of the other pairs are minimized, thereby refining the model's ability to capture meaningful contextual relationships and temporal dynamics.\nTrajectory Reconstruction Learning (TRL). While contrastive learning is a discriminative task that helps features distinguish motion and contextual differences among trajectories, the ultimate goal of motion prediction is a regression task. Therefore, the features learned only through contrastive learning may not necessarily align with or benefit the needs of motion prediction. To this"}, {"title": "3.2.3 TRAINING DETAILS", "content": "Pre-training Stage. The overall pre-training scheme integrates trajectory contrastive learning and reconstruction. The combined loss function is formulated as follows:\n$\\mathcal{L} = L_c + \\lambda L_r,$(2)\nwhere $\\lambda$ is a hyper-parameter balancing the contribution of both tasks.\nFinetuning Stage. After pre-training on a specific model, we initialize the model's encoder $f_{enc}$ with pre-trained weights, and fine-tune the whole model on the downstream trajectory prediction task, using the model's original prediction objective and training schedules."}, {"title": "4 EXPERIMENTS", "content": ""}, {"title": "4.1 EXPERIMENTAL SETUP", "content": "Datasets. We train and evaluate our method on three large-scale motion forecasting datasets: Argoverse (Chang et al., 2019), Argoverse 2 (Wilson et al., 2023) and Waymo Open Motion Dataset (WOMD) (Sun et al., 2020). Argoverse contains 333k scenarios collected from interactive and dense traffic. Each scenario provides the HD map and 2 seconds of historic trajectory data, to predict the trajectory for the next 3 seconds, sampled at 10Hz. The training, validation, and test set of Argoverse is set to 205k, 39k and 78k scenarios, respectively. Argoverse 2 upgrades the previous dataset to include 250K sequences with higher prediction complexity. It extends the historic and prediction horizon to 5 seconds and 6 seconds respectively, sampled at 10Hz. The data is split into 200k, 25k, and 25k for training, validation, and test, respectively. For WOMD, the dataset provides 1 second of historic trajectory data and aims to predict the trajectory for 8 seconds into the future, sampled at 10Hz as well. It contains 487k training scenes, 44k validation scenes and 44k testing scenes. Notably, our pre-training pipeline only utilizes the training splits of these datasets.\nBaselines. As mentioned previously, our pre-training pipeline can be seamlessly integrated into most existing trajectory prediction methods. In our experiments, we consider four popular and advanced methods as the prediction backbone to evaluate how our SmartPretrain further improves performance: HiVT (Zhou et al., 2022), HPNet (Tang et al., 2024), Forecast-MAE (Cheng et al., 2023) and QCNet (Zhou et al., 2023). We use their official open-sourced code for implementation.\nMetrics. Following the official dataset settings (Chang et al., 2019), we evaluate our model using the standard metrics for motion prediction, including minimum Average Displacement Error (minADE), minimum Final Displacement Error (minFDE), and Miss Rate (MR). While the prediction model forecasts up to 6 trajectories for each agent, these metrics evaluate the trajectory with the minimum endpoint error, as a signal of best possible performance from the multi-modal predictions.\nImplementation Details. We implement the projector and contrastive predictor as a 2-layer MLP with batch normalization, and the trajectory decoder also as a 2-layer MLP but with layer normalization to better fit its sequence nature. We use AdamW to optimize the online branch, which consists of the encoder in the motion prediction model, a projector, a contrastive predictor, and a trajectory decoder. In the momentum branch, the weights of the motion prediction encoder and the projector are initialized to be identical to that of the online branch, and updated via an exponential moving average (EMA) strategy. We use a momentum value of 0.996 and increase this value to 1.0 with a"}, {"title": "4.2 QUANTITATIVE RESULTS", "content": "Performance of Applying SmartPretrain to Multiple Models. As shown in Table 1, we first report the performance when we apply SmartPretrain to multiple state-of-the-art prediction models, on the validation and test set of Argoverse and Argoverse 2. We consider two pre-training settings: pre-training only on the single downstream dataset, and pre-training on all three datasets. Specifically, we pre-train HiVT and QCNet with both two settings, and only pre-train HPNet and Forecast-MAE with the single downstream dataset due to compute constraints. SmartPretrain can consistently improve all considered models, on downstream datasets, data splits and main metrics. For instance, SmartPretrain can significantly reduce the minFDE, minADE, MR of QCNet on validation set by 4.9%, 3.3%, 7.6% respectively. Besides, Pre-training with all datasets also shows consistently higher improvement compared to pre-training with only one dataset.\nPerformance Comparison with other Pre-training Method. We also compare SmarPretrain with other pre-training methods. For a fair and meaningful comparison, we look for methods that have open-source code. However, to the best of our knowledge, only Forecast-MAE was open-sourced at the time of this paper's submission. Specifically, Forecast-MAE proposes a motion prediction backbone and a pre-training strategy. We then apply SmartPretrain to Forecast-MAE's backbone, and compare it with Forecast-MAE's pre-training strategy, on the Argoverse 2 dataset. As in Table 2, our pre-training method shows a larger improvement than Forecast-MAE's pre-training method by a substantial margin (e.g. improvement of 4.5% vs 1.9% on minFDE).\nThese results demonstrate that our pre-training pipeline: 1) can be flexibly applied to a wide range of motion prediction models; 2) consistently improves performance through pre-training and data scaling; and 3) delivers stronger performance enhancements compared to existing pre-training methods."}, {"title": "4.3 ABLATION STUDIES", "content": "We conduct comprehensive ablation studies to analyze the impact of different components, including the data scale of pre-training, the two proposed pretext tasks, and associated hyperparameters and configurations. For efficient evaluation, we use HiVT as the prediction model and Argoverse for both pre-training and fine-tuning, reporting performance on the Argoverse validation set."}, {"title": "5 CONCLUSION", "content": "In this paper, we introduce SmartPretrain, a novel, model-agnostic, and dataset-agnostic self- supervised learning (SSL) framework designed to enhance motion prediction in autonomous driving. Through a combination of contrastive and reconstructive SSL techniques, SmartPretrain consistently improves the performance of state-of-the-art models across multiple datasets. Our extensive experiments demonstrate significant improvements when applying SmartPretrain to various prediction models like HiVT, HPNet, Forecast-MAE, and QCNet. Additionally, the flexibility of our framework allows effective pre-training across multiple datasets, leveraging data diversity to improve accuracy and generalization. SmartPretrain outperforms existing methods, confirming the effectiveness of our approach. These results highlight SmartPretrain's scalability, versatility, and potential to advance motion prediction in driving environments."}]}