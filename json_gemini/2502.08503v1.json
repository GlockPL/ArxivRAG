{"title": "Revisiting 3D LLM Benchmarks: Are We Really Testing 3D Capabilities?", "authors": ["Jiahe Jin", "Yanheng He", "Mingyan Yang"], "abstract": "In this work, we identify the \"2D-Cheating\" problem in 3D LLM evaluation, where these tasks might be easily solved by VLMs with rendered images of point clouds, exposing ineffective evaluation of 3D LLMs' unique 3D capabilities. We test VLM performance across multiple 3D LLM benchmarks and, using this as a reference, propose principles for better assessing genuine 3D understanding. We also advocate explicitly separating 3D abilities from 1D or 2D aspects when evaluating 3D LLMs. Code and data are available at https://github.com/LLM-class-group/Revisiting-3D-LLM-Benchmarks.", "sections": [{"title": "1 Introduction", "content": "Recent advances in Large Language Models (OpenAI, 2022, 2023) have led to the development of Vision-Language Models (VLMs) (Liu et al., 2023; OpenAI, 2024). To overcome their lack of grounding in the real 3D physical world (Hong et al., 2023), researchers developed 3D LLMs (Qi et al., 2024; Xu et al., 2023) for 3D processing.\nGiven the extreme scarcity of 3D training data (Hong et al., 2023; Zhu et al., 2024), many approaches leverage existing LLMs and VLMs to generate annotations for 3D data (Hong et al., 2023; Qi et al., 2023; Xu et al., 2023). This reliance on 2D and language (1D) priors raises a fundamental question: What capabilities do 3D LLMs possess that truly differentiate them from 2D VLMs?\nTo explore this, we revisit some benchmarks used for 3D LLM evaluation. Current 3D LLMS are primarily evaluated on Q&A or captioning tasks (Ma et al., 2024), rather than specific downstream tasks like object detection (Caesar et al., 2020; Wang et al., 2021), as LLMs provide a general-purpose language interface (Hao et al., 2022). As shown in Figure 1, we find some tasks"}, {"title": "2 Method", "content": "As shown in Figure 2, we propose VLM3D, a simple yet general pipeline that adapts VLMs to 3D tasks. Specifically, it first renders point clouds into images and augments queries with few-shot, then feeds them into a VLM."}, {"title": "2.1 VLM3D", "content": ""}, {"title": "2.2 Viewpoint Selection", "content": "Viewpoint of image rendering significantly impacts VLM's input information about the 3D asset. We hypothesize that a key limitation of 2D models in 3D understanding stems from the viewpoint-specific nature of images, whereas point clouds inherently provide a holistic 3D representation. The challenges posed by viewpoint dependency include:\n(1) blind spots in areas outside the selected viewpoint; (2) occlusion and overlap of objects; and (3) single-surface capture that lacks multifaceted geometry. Based on this, we set different viewpoint rendering configurations for object and scene point cloud benchmarks."}, {"title": "2.2.1 Single View", "content": "In this setting, we follow the common practice (Ma et al., 2022) to render images from BEV."}, {"title": "2.2.2 Multi View", "content": "In this setting, we render images from four fixed viewpoints (East, South, West, and North) and combine them into a multi-view image."}, {"title": "2.2.3 Oracle View", "content": "In this setting, we conducted experiments on the validation set to explore the upper limits of VLM3D's capabilities. We first rendered images from all five viewpoints shown in Figure 3, and then used the Best-of-N (BON) method to select the optimal view for each question.\nSince LLMs are probabilistic, simply taking the highest evaluation score risks inflating the likelihood of randomly guessing the correct answer. To"}, {"title": "2.3 Other Rendering Factors", "content": "While other factors like rendering style may affect image quality, viewpoint selection remains the trickiest due to the difficulty of a unified optimal setup. For each benchmark, we applied a consistent set of other rendering configurations across all point clouds to ensure high-quality results."}, {"title": "3 Experiments", "content": "We conducted experiments on the benchmarks in Figure 4, which covered major benchmarks used for evaluating 3D LLMs."}, {"title": "3.1 Object Point Cloud Benchmark", "content": "For object point cloud benchmarks, we use 3D MM-Vet (Qi et al., 2024) for Q&A and ObjaverseXL-LVIS Caption (Deitke et al., 2022) for captioning, with results presented in Table 1."}, {"title": "3.2 Scene Point Cloud Benchmark", "content": "We conducted experiments on two widely used scene point cloud benchmarks, ScanQA (Azuma et al., 2021) and SQA3D (Ma et al., 2022). ScanQA evaluates general Q&A, while SQA3D further assesses situation understanding. We chose the 3D Baseline as the best-performing model at the time of the benchmark's release."}, {"title": "3.2.1 Single View Evaluation", "content": "We first conducted the single view evaluation mentioned in Section 2.2.1, with results in Table 2."}, {"title": "3.2.2 Multi View Evaluation", "content": "We conducted multi-view evaluation based on the settings in Section 2.2.2, with results in Table 3."}, {"title": "3.2.3 Oracle View Evaluation", "content": "We performed oracle view evaluation using the Best-of-N method in Section 2.2.3. For each question, we rendered images from k viewpoints and sampled n responses per viewpoint. As shown in Figure 5, the result decreases as n increases and converges around n = 20, indicating that randomness from selecting the maximum score is effectively mitigated. The scores are detailed in Table 4.\nBesides the Best-of-N, we explored another method called Human-Intuition-Selection (HIS). We calculate the centroids of all relevant objects of each question and use a heuristic algorithm to select the best viewpoint that captures the centroids effectively. Details and results are in Appendix B.1."}, {"title": "4 Analysis", "content": "Results reveal that VLM outperforms 3D LLMs using only single view images on these object benchmarks. This suggests that these tasks can be easily solved without specialized 3D representations, indicating a lack of effective 3D capability assessment. The limitation may arise from (1) the inherent"}, {"title": "4.1 Scene Point Cloud", "content": ""}, {"title": "4.1.1 Poor Performance with Single View", "content": "In these scene benchmarks, even the most advanced VLMs consistently underperform top 3D models, revealing their limitations in handling complex 3D scenes.\nBesides, many benchmarks rely on text similarity metrics for evaluation, which is overly rigid given the flexible nature of natural language. See case study in Appendix C.1."}, {"title": "4.1.2 Challenges of Multi View Understanding", "content": "As shown in Table 3 & 4, providing multi-view images only brings a slight improvement. In contrast, oracle view leads to a significant boost. Notably, when setting k = 4, all candidate viewpoints of Best-of-N are included in the multi-view inputs, theoretically providing sufficient information. However, results indicate that VLMs struggle to form a unified understanding of a scene from multi-view."}, {"title": "4.1.3 Enhancement with Oracle View", "content": "As shown in Table 4, oracle view performance significantly surpasses the 3D baseline, confirming that providing good viewpoints greatly enhances VLM's potential to solve these tasks through 2D-Cheating. However, dynamically providing the oracle viewpoint for each question considerably simplifies the task by identifying and presenting the key information from the complete scene.\nBesides, the performance of BoN still lags behind the 3D SOTA. This suggests that, even provided with a favorable view, VLMs still struggle to match the best performance of 3D LLMs in some tasks."}, {"title": "4.1.4 Failure of HIS Method", "content": "Table 5 shows that HIS performs significantly worse than Best-of-N. Through case studies, we identify three key factors: (1) Selecting the best viewpoint for detail items in complex scenes with intuition is challenging, often leading to occlusions. (2) Limited ground truth reveals a clear preference, as shown in Appendix C.2. While multiple viewpoints all provide sufficient information, BoN selects the one that aligns most closely with the ground truth. (3) Many questions rely on common sense, not the specific details of the point cloud, as shown in Appendix C.3. Specific information in HIS viewpoints might even lead to a worse score compared to answering leveraging world knowledge with an unrelated viewpoint in BON."}, {"title": "5 Principles For Effective 3D Evaluation", "content": "Based on the analysis, we propose the following principles for effective 3D capabilities evaluation of 3D LLMs.\u00b9 We applied these principles to redesign a task, as shown in Figure 6.\n\u2022 Point Cloud Selection A complex point cloud should be selected, such as a point cloud of scenes or objects with more intricate structures.\nTask Focus Tasks should go beyond general surface-level information, diving into the intricate details of 3D assets. This includes the structural specifics of individual objects and the finer items within complex scenes. Furthermore, questions should prioritize aspects that cannot be easily answered with images from any viewpoints.\nContext Specific Inquiry Avoid overly general questions, ask the unique aspects of the current 3D asset, and try to \"violate common sense\" to test genuine understanding of 3D input.\nThe analyses leading to each principle are marked with corresponding colored circular markers in the preceding text."}, {"title": "6 Conclusion", "content": "This paper identifies the 2D-Cheating problem in current 3D LLM evaluation, compares VLM performance with 3D LLMs across various benchmarks, and provides principles for assessing real 3D capabilities in 3D LLMs. However, this does not imply that 3D LLMs can ignore the ability to solve tasks vulnerable to 2D-Cheating. Therefore, we emphasize the need to deliberately decouple the evaluation of 3D capabilities from 1D and 2D capabilities to ensure that core 3D evaluations are not confused or overlooked."}, {"title": "7 Limitation", "content": "Our evaluation may not fully capture the capabilities of VLMs. First, the 3D LLMs were trained on the benchmark's training set or similar tasks, whereas we only adapted VLMs to these benchmarks through few-shot prompting. Additionally, rendering point clouds into images inherently loses information compared to using real photographs. These factors could limit the assessment of VLMS' real capability and may have weakened the observed 2D-Cheating issue.\nFurthermore, our experiments were conducted solely on the Qwen2-VL and GPT-40, which may not fully represent the performance of all available VLMs. The number of benchmarks we tested was limited, meaning the results might not generalize across a broader set of tasks. Ideally, a new benchmark that aligns with these principles would be developed, but due to constraints in space, resources, and other practical considerations, we leave this for future work."}]}