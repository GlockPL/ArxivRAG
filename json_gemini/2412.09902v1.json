{"title": "One Node One Model: Featuring the Missing-Half for Graph Clustering", "authors": ["Xuanting Xie", "Bingheng Li", "Erlin Pan", "Zhaochen Guo", "Zhao Kang", "Wenyu Chen"], "abstract": "Most existing graph clustering methods primarily focus on exploiting topological structure, often neglecting the \"missing-half\" node feature information, especially how these features can enhance clustering performance. This issue is further compounded by the challenges associated with high-dimensional features. Feature selection in graph clustering is particularly difficult because it requires simultaneously discovering clusters and identifying the relevant features for these clusters. To address this gap, we introduce a novel paradigm called \"one node one model\", which builds an exclusive model for each node and defines the node label as a combination of predictions for node groups. Specifically, the proposed \"Feature Personalized Graph Clustering (FPGC)\" method identifies cluster-relevant features for each node using a squeeze-and-excitation block, integrating these features into each model to form the final representations. Additionally, the concept of feature cross is developed as a data augmentation technique to learn low-order feature interactions. Extensive experimental results demonstrate that FPGC outperforms state-of-the-art clustering methods. Moreover, the plug-and-play nature of our method provides a versatile solution to enhance GNN-based models from a feature perspective.", "sections": [{"title": "Introduction", "content": "As a fundamental task in graph data mining, attributed graph clustering aims to partition nodes into different clusters without labeled data. It is receiving increasing research attention due to the success of Graph Neural Networks (GNNs) (Welling and Kipf 2017; Li, Pan, and Kang 2024; Qian, Li, and Kang 2024). Typically, GNNs use many graph convolutional layers to learn node representations by aggregating neighbor node features into the node. In recent years, many graph clustering techniques have achieved promising performance (Pan and Kang 2023; Liu et al. 2023; Kang et al. 2024; Shen, Wang, and Kang 2024).\nMost of these methods apply a self-supervised learning framework to fully explore the graph structure (Zhu et al. 2024; Yang et al. 2024), often neglecting the \"missing-half\" of node feature information, i.e., how node features can enhance clustering quality. Consequently, their performance heavily depends on the quality of the input graph. If the original graph is of poor quality, such as having many nodes from different classes connected together and missing important links, the representations learned through multilayer aggregation become nondiscrimination. This issue is known as representation collapse (Chen, Wang, and Li 2024).\nFor the attributed graph, the node feature and the topological structure should play an equal role in the unsupervised graph embedding and downstream clustering. We argue that a node's personalized features should be crucial for identifying its cluster label. For example, the Cora dataset contains 2,708 papers, each described by a 1,433-dimensional binary feature indicating the presence of keywords. Thus, these personalized features are representative of different clusters.\nFurthermore, most GNN-based methods capture only higher-order feature interactions and tend to overlook low-order interactions (Kim, Choi, and Kim 2024). The graph filtering process transforms input features into higher-level hidden representations. Recent studies on GNNs focus mainly on designing node aggregation mechanisms that emphasize various connection properties, such as local similarity (Welling and Kipf 2017), structural similarity (Donnat et al. 2018), and multi-hop connectivity (Li et al. 2022). However, the cross feature information is always ignored, despite its importance for model expressiveness (Feng et al. 2021)."}, {"title": "Related work", "content": "Graph clustering can be roughly divided into three categories (Xie et al. 2024; Li et al. 2024). (1) Shallow methods. FGC (Kang et al. 2022) incorporates higher-order information into graph structure learning for clustering. MCGC (Pan and Kang 2021) uses simplified graph filtering rather than GNNs to obtain a smooth representation. (2) Contrastive methods. SCGC (Liu et al. 2023) simplifies network architecture and data augmentation. CCGC (Yang et al. 2023a) leverages high-confidence clustering information to improve sample quality. CONVERT (Yang et al. 2023b) uses a perturb-recover network and semantic loss for reliable data augmentation. (3) Autoencoder methods. SDCN (Bo et al. 2020) transfers learned representations from auto-encoders to GNN layers, employing data structure information. AGE (Cui et al. 2020) uses Laplacian smoothing filters and adaptive learning to improve node embeddings. DGCN (Pan and Kang 2023) uses an adaptive filter to capture important frequency information and reconstructs heterophilic and homophilic graphs to handle real-world graphs with different levels of homophily. DMGNC (Yang et al. 2024) uses a masked autoencoder for node feature reconstruction. However, their training is highly dependent on the input graph. Consequently, if the original graph is poor quality, the learned representation through multilayer aggregation becomes indiscriminate.\nTo improve the discriminability of representation, some recent methods learn node-specific information by considering the distinctness of each node (Zhu et al. 2024; Dai et al. 2021). NDLS (Zhang et al. 2021) assigns different filtering orders to each node based on its influence score. DyFSS (Zhu et al. 2024) learns multiple self-supervised learning task weights derived from a gating network considering the difference in the node neighbors. In an orthogonal way, in this paper, we improve the representation learning from a feature perspective. We perform an in-depth analysis of how the \"one node one model\" paradigm benefits clustering."}, {"title": "Notation", "content": "Define the graph data as $G = \\{V, E, X\\}$, where V represents a set of N nodes and $e_{ij} \\in E$ denotes the edge between node i and node j. $X = \\{X_1, ..., X_N\\}^T \\in \\mathbb{R}^{N\\times d}$ is the feature matrix with d dimensions, $X_i$ and $X_{\\cdot j}$ indicate the i-th row and the j-th column and of X, respectively. Adjacency matrix $A \\in \\mathbb{R}^{N\\times N}$ represents the graph structure. D represents the degree matrix. The normalized adjacency matrix is $A = D^{-\\frac{1}{2}}(\\tilde{A}+I)D^{-\\frac{1}{2}}$, and the corresponding graph Laplacian is $L = I - A$."}, {"title": "Methodology", "content": "The Pipeline of FPGC\nThe FPGC framework is shown in Fig. 2. Our design comprises two critical components. The first component concretizes \"one node one model\" with GNNs, with a squeeze-and-excitation block selecting cluster-relevant features. The second is the contrastive learning framework with a new data augmentation technique based on feature cross.\nSqueeze-and-Excitation Block\nCluster-relevant features characterize the clusters. We design a squeeze-and-excitation block to select them.\nSqueeze: The squeeze operation compresses the node features from $X \\in \\mathbb{R}^{N\\times d}$ to $q \\in \\mathbb{R}^{1\\times d}$. Specifically, it computes the summation of the features as follows:\n$q = F_{sq} (X) = \\frac{1}{N}\\sum_{i=1}^{N} X_i,$\\nwhere q serves as a channel-wise statistic that captures global feature information.\nExcitation: The excitation operation follows the squeeze step and aims to capture dependencies among all channels. It is a simple gating mechanism that uses the sigmoid function $\\sigma(\\cdot)$ to activate the squeezed feature map, enabling the learning of nonlinear connections between channels. Firstly, the dimension is reduced using a multi-layer perceptron (MLP) $W_1$. Next, a ReLU function $\\delta(\\cdot)$ and another MLP $W_2$ are used to increase the dimensionality, returning it to the original dimension d. Finally, the sigmoid function is applied. The process is summarized as follows:\n$\\bar{q} = F_{ex} (q) = \\sigma (W_2\\delta (W_1q)).$\\nSelection: The outcome of the excitation operation is considered to be the significance of each feature. Then, we define the function $F_{top}$ to select top n important features according to $\\bar{q}$. This operation is defined as follows:\n$X^n = F_{top} (\\bar{q}X),$\\nwhere $X^n \\in \\mathbb{R}^{N\\times n}$ is the node feature matrix containing the top n significant features.\nOne Node One Model\nIdeally, an exclusive model should be constructed for each individual node, which is the core of our method:\n$Y_i = f^{(i)} (X_i),$\\nwhere Y and f are the predicted label and the model. In this paradigm, personalization is fully maintained in the models. Unfortunately, the enormous number of nodes makes this scheme unfeasible. Specifically, the information about the mathematics group may be related to his professional research, while the information about sports clubs may be associated with his hobbies. As a result, we can decompose the output for a specific node as a combination of predictions for clusters:\n$Y_i = \\sum_{j=1}^{M} w_j f^{(i)} (X_i),$\\nwhere j denotes the model index and there are M models. For an unsupervised task, learning $w_j$ directly is difficult. Instead, we generate $w_j$ from personalized features: $w_j ="}, {"title": "Theoretical Analysis", "content": "Most theoretical analysis in the GNN area focuses on graph structures (Mao et al. 2024). In this section, we establish a theoretical analysis from the feature perspective. Without loss of generality, we consider the case with two clusters, $C_1$ and $C_2$. Assume the original node features follow the Gaussian distribution: $X_i \\sim \\mathcal{N} (\\mu_1, I)$ for $i \\in c_1$ and $X_i \\sim \\mathcal{N} (\\mu_2, I)$ for $i \\in C_2 (\\mu_1 \\neq \\mu_2 \\geq 0)$. We define the filtered feature as $X' = D^{-1}AX$. We define $f_i$ and $f_j$ as models for $X_i$ and $X_j$, respectively, focusing on the personalized features in sets $T_i$ and $T_j$, while other irrelevant features are ignored. We then present the following theorem:\nTheorem 1. Assuming the distribution of filtered features $\\tilde{X}$ shares the same variance $I$ and the cluster has a balance distribution $P (Y = c_1) = P (Y = c_2)$. The upper bound of $|P (Y_i = c_1 | f_i(X_i)) - P (Y_i = c_1 | f_j(X_j))|$ is decreasing with respect to $E_{u \\in T_i \\cap T_j}; X_{iu}X_{ju}$.\nNote that the assumptions are not strictly necessary but are used to simplify the proof in the Appendix. Theorem 1 indicates that if two nodes share more cluster-relevant features, they are more likely to be classified into one cluster, and vice versa."}, {"title": "Contrastive Clustering", "content": "Feature Cross Augmentation Although graph filtering can smooth the features between node neighborhoods, it only captures high-order feature interactions and suffers from overlooking low-order feature interaction (Feng et al. 2021). Feature cross can provide valuable information for downstream tasks.\nFeature cross can provide valuable information for downstream tasks. For example, features like $ \\{title \\& approach \\& experiment\\}$ can provide additional information about the paper's category. Features that collectively represent title can be viewed as a field, like title = $\\{0,1,0\\}$. To this end, we randomly divide the filtered features into m fields:\n$X = \\{X_{field_1}, X_{field_2}, ..., X_{field_m}\\}.$\nThen, every node has m fields, i.e., $X_i = \\{X_{i field_1}, X_{i field_2},..., X_{i field_m}\\}$. We use $\\tilde{X}$ instead of X because $\\tilde{X}$ has more information. The feature cross is defined as:\n$R_i = \\{ x | x = X_{i field_z} \\times X_{i field_j} | 1 \\leq z < j \\leq m\\},$\nWe input the original features X as a view for contrastive learning. It's well known that matrix factorization techniques can capture the low-order interaction (Guo et al. 2017). Thus, we propose to perform data augmentation as follows:\n$\\tilde{X}_{aug} = X + RW,$\nwhere W is an MLP to adjust the dimension. Unlike previous methods, Eq.(12) has two advantages. First, instead of using typical graph enhancements such as feature masking or edge perturbation, our method generates augmented views from filtered features. This maintains the semantics of the augmented view. Second, other learnable augmentations design complex losses to remain close to the initial features. Eq.(12) is based on the feature cross, which can provide rich semantic information. The complexity of the feature cross is O(Nm\u00b2), which is linear to N. The results can be stored and accessed after a one-time computation.\nSimilarly to Eq.(9), the second view is finally formulated as:\n$Y' = g (\\tilde{X}^n) \\tilde{X}_{aug}W.$\nNote that the same node in two different views shares the same model."}, {"title": "Experiments", "content": "Datasets\nWe select seven graph clustering benchmark datasets, which are: Cora (Yang, Cohen, and Salakhudinov 2016), CiteSeer (Yang, Cohen, and Salakhudinov 2016), Pubmed (Yang, Cohen, and Salakhudinov 2016), Amazon Photo (AMAP) (Liu et al. 2022), USA Air-Traffic (UAT) (Mrabah et al. 2022), Europe Air-Traffic (EAT) (Mrabah et al. 2022), and Brazil Air Traffic (BAT) (Mrabah et al. 2022). We also add two large-scale graph datasets, the image relationship network Flickr (Zeng et al. 2020) and the social network Twitch-Gamers (Rozemberczki and Sarkar 2021). To see the relevance between graph structure and downstream task, we also report the Aggregation Class Distance (ACD) (Shen, He, and Kang 2024). We can see that these datasets are inherently low-quality.\nComparison Methods\nTo demonstrate the superiority of FPGC, we compare it to several recent baselines. These methods can be roughly divided into three kinds: 1) traditional GNN-based methods: SSGC (Zhu and Koniusz 2021). 2) shallow methods: MCGC (Pan and Kang 2021), FGC (Kang et al. 2022), and CGC (Xie et al. 2023). 3) contrastive learning-based methods: MVGRL (Hassani and Khasahmadi 2020), SDCN (Bo et al. 2020), DFCN (Tu et al. 2021), SCGC (Liu et al. 2023), CCGC (Yang et al. 2023a), and CONVERT (Yang et al. 2023b). 4) Advanced autoencoder-based methods: AGE (Cui et al. 2020), DGCN (Pan and Kang 2023), DMGNC (Yang et al. 2024), and DyFSS (Zhu et al. 2024).\nExperimental Setting\nTo ensure fairness, all experimental settings follow the DGCN (Pan and Kang 2023), which performs a grid search to find the best results. Our network is trained with the Adam optimizer for 400 epochs until convergence. The learning rate is set to 1e-2 on BAT/EAT/UAT/Twitch-Gamers, 1e-3 on Cora/Citeseer/Pumbed/Flickr, and 1e-4 on AMAP. The graph aggregating layers k is searched in {2,3,4,5}. The number of fields m is set according to the density of features, i.e., denser features should have smaller values to cross fewer times, the number of important features n is set according to the number of features, i.e., more features should have larger values, the trade-off parameter $\u03bb$ is tuned in {0.001, 0.1, 1, 100}. Thus, {k, m, n, $\u03bb$} are set to {3, 60, 100, 1} on Cora, {4, 50, 50, 0.1} on Citeseer, {5, 10, 10, 0.001} on Pubmed, {5, 20, 10, 100} on UAT, {2,10,10,0.001} on AMAP, {4, 10, 10,0.001} on EAT, {5, 20, 20, 1} on BAT, {4, 10, 100, 1} on Flickr and {2, 2, 4, 1} on Twitch-Gamers. We evaluate clustering performance with two widely used metrics: ACC and NMI.\nResults Analysis\nThe results are illustrated. We find that FPGC achieves dominant performance in all cases. For example, on the Cora dataset, FPGC surpasses the runner-up by 4.04% and 2.65% in terms of ACC and NMI. Traditional GNN-based methods have poor performance compared to other methods, which dig for more structure information or use contrastive learning to implicitly capture the supervision information. Note that SCGC, CCGC, and CONVERT are the most recent contrastive methods that design new augmentations. Our method constantly exceeds advanced GNN-based methods with adaptive filters, which indicates the significance of fully exploring feature information.\nDMGNC and DyFSS are the latest methods that explicitly consider feature information. Though applying a node-wise feature fusion strategy, DyFSS also performs poorly. For example, our method's ACC and NMI are 7.00% and 4.06% higher on Cora, and 15.54% and 13.01% higher on EAT, respectively. This is because they perform general embedding without exploiting the relationship between features and clusters."}, {"title": "Loss Function", "content": "For two views Y and Y', we treat the same node in different views as positive samples and all other nodes as negative samples. The pairwise loss is defined as follows:\nl (Y_i, Y') = - log \\frac{e^{sim(Y_i,Y'_i)}}{\\sum_{i=1}^{N} e^{sim(Y_i,Y'_i)} + \\sum_{i=1}^{N} e^{sim(Y_i,Y'_i)}},\\\\L_{con} = \\frac{1}{2N}\\sum_{i=1}^{2N} [l (Y_i, Y') + l (Y', Y_i)],\\nwhere sim is the cosine similarity. Besides, the reconstruction loss can be calculated as follows:\nL_{re} = \\frac{1}{N^2} ||Y'Y - A||_F.\\nFinally, the total loss is formulated as:\nL = L_{re} + \\lambda L_{con},\\nwhere $\u03bb$ > 0 is a trade-off parameter. The clustering results are obtained by performing K-means on (Y + Y')."}, {"title": "Robustness Analysis", "content": "To demonstrate the significance of the feature, we examine the situation in which the graph structure is of poor quality. Specifically, we add edges to the graphs and remove the same number of original edges from them at random (Fu, Zhao, and Bian 2022). We define $r = \\frac{\\text{#random edges}}{\\text{#all edges}}$ as the random edge rate. With r = {0.2, 0.4, 0.6, 0.8}, we report the ACC of CGC, CCGC, DyFSS, and FPGC in Cora and Citeseer. From Fig. 6, it can be seen that the PFGC achieves the best performance in all cases. Especially when the perturbation rate is extremely high, our method shows a more stable tendency than other clustering methods. The performance of other methods changes dramatically, indicating that they rely highly on the graph structure. Thus, our method is robust to the graph structure noise."}, {"title": "Ablation Study", "content": "To assess the impact of feature cross, we test the clustering performance after removing it, marking it as \"FPGC w/o aug\". It is clear that feature cross does improve the clustering performance. In addition, we replace feature cross with classical graph augmentation strategies, including drop edges (Liu et al. 2024), add edges (Xia et al. 2022), graph diffusion (Hassani and Khasahmadi 2020), and mask feature (Yu et al. 2022). The best performance among these four methods is marked as \"FPGC w aug\". Our method can still beat them, thus feature cross can provide more rich information.\nTo see the influence of feature selection, we test the performance without g(Xn) and mark it as \u201cFPGC w/o g()\u201d. We can see the performance degradation in most cases. Thus learning a model for each node can successfully keep each node's personality.\nTo verify that the proposed squeeze-and-excitation block can select essential features, we categorize them into three intervals: features with the top 33.3% (highest) $\\bar{q}$, features from 33.3% to 66.7%, and the remaining 66.7% to 100%. We use these indexes to select features for $X^n$ and test their performance, respectively. From Fig. 7, we can see that when selecting the most important features (top 33.3%), we achieve the best results in all cases. This indicates that features with high value in $\\bar{q}$ are vital. Therefore, our proposed squeeze-and-excitation block can successfully assign high weights to essential features, which is beneficial for downstream task."}, {"title": "Conclusion", "content": "In this paper, we introduce the \"one node one model\" paradigm, which addresses the limitations of existing graph clustering methods by emphasizing missing-half feature information. By incorporating a squeeze-and-excitation block to select cluster-relevant features, our method effectively enhances the discriminative power of the learned representations. The proposed feature cross data augmentation further enriches the information available for contrastive learning. Our theoretical analysis and experimental results demonstrate that our method significantly improves clustering performance."}, {"title": "Algorithm", "content": "The detailed learning process of FPGC is illustrated in Algorithm 1."}]}