{"title": "HOPE: A Novel Positional Encoding Without Long-Term Decay for Enhanced Context Awareness and Extrapolation", "authors": ["Yuhan Chen", "Ang Lv", "Jian Luan", "Bin Wang", "Wei Liu"], "abstract": "Many positional encodings (PEs) are designed to exhibit long-term decay, based on an entrenched and long-standing inductive opinion: tokens farther away from the current position carry less relevant information. We argue that long-term decay is outdated in the era of LLMs, as LLMs are now applied to tasks demanding precise retrieval of in-context information from arbitrary positions. Firstly, we present empirical analyses on various PEs, demonstrating that models inherently learn attention with only a local-decay pattern while forming a U-shape pattern globally, contradicting the principle of long-term decay. Furthermore, we conduct a detailed analysis of rotary position encoding (ROPE, a prevalent relative positional encoding in LLMs), and found that the U-shape attention is caused by some learned components, which are also the key factor limiting RoPE's expressiveness and extrapolation. Inspired by these insights, we propose High-frequency rotary Position Encoding (HOPE). HoPE replaces the specific components in RoPE with position-independent ones, retaining only high-frequency signals, which also breaks the principle of long-term decay in theory. HOPE achieves two major advantages: (1) Without constraints imposed by long-term decay, contradictory factors that limit spontaneous attention optimization and model extrapolation performance are removed. (2) Components representing positions and semantics are are optimized. These enhances model's context awareness and extrapolation, as validated by extensive experiments.", "sections": [{"title": "1 Introduction", "content": "Positional encoding (PE) plays a crucial role in Transformers (Vaswani et al., 2017) to capture the order of input sequence, as the attention mechanism is permutation invariant. The original PE proposed by Vaswani et al. (2017) struggles to generalize beyond the training sequence length. To address this limitation, relative positional encoding (RPE) methods have been introduced, including ROPE (Su et al., 2021), ALiBi (Press et al., 2021), and KERPLE (Chi et al., 2022a). These RPES share a long-standing and entrenched design (Su et al., 2021): the long-term decay, i.e., tokens with a long relative distance should receive less attention.\nHowever, in the era of LLMs, a question arises: is it still necessary to retain this design? As LLMs are increasingly being applied to long-text and even ultra-long-text scenarios, the importance of distant information appears to depend more on semantics rather than positions.\nIn this paper, we demonstrate that the answer to the above question is no. Through empirical analyses on various PEs, we found that the attention patterns learned by models tend to contradict the principle of long-term decay. Specifically, models only retain a local-decay pattern, while learning a U-shape attention distribution globally. We further delve into a analysis of ROPE (Rotary Position Encoding by Su et al., 2021, a widely-used RPE), which claims to ensure the long-term decay by combining various frequency components (See Section 4.2.1 for details) while empirically learns the U-shape pattern. We decomposed these components and obtain the following observations:\n(1) In RoPE, some components of specific frequency (which we named \u201cactivated\u201d components in this paper) play a key role in shaping the final U-shape attention pattern and they exhibit fluctuations similar to the overall attention pattern. We observed that these components have a predominant impact on attention in the early stages of training. As the training steps increase, however, the model attempts to offset their effects, increasing the weight of other components. We assume this reflects a form of shortcut learning behavior (Geirhos et al., 2020; Robinson et al., 2021; Du et al., 2022), which hinders the optimization. We further found that the frequency of activated components can be pre-calculated before training, based on the relationship between the RoPE rotary angle and the training context length.\n(2) We explored the attention patterns in extrapolation tasks and found that \"activated\" components are a key factor limiting RoPE's extrapolation abilities. These components cause out-of-distribution (OOD) attention logits in the first layer during extrapolation, leading to disarray in subsequent layers' attention.\n(3) The top low-frequency components (whose frequency lower than the \u201cactivated\" components) tend to stabilize as constant patterns, with a small magnitude. This indicates that these components are not being effectively utilized for representing positional information and learn more about semantics information.\nBased on the findings above, we summarize three key insights: (1) Global long-term decay is not necessary for the model and may even hinder optimal learning. (2) To enhance the model's context awareness and extrapolation, the frequencies of ROPE's learned components should be constrained. (3) There is redundancy in RoPE, representation subspaces occupied by certain components could be better utilized.\nIn this paper, we propose a novel positional encoding method called High-frequency rotary Position Encoding (HoPE). HoPE follows above insights and is quite intuitive to implement: we replace the \"to-be-activated\" and top low-frequency components in the original RoPE with position-independent ones, while retaining the high-frequency components. As a result, contradictory factors for attention optimization are eliminated, extrapolation limitations are reduced, and position information is still well-represented by high-frequency signals.\nWe conducted extensive evaluations on the model's extrapolation capabilities and context awareness, including language modeling perplexity, copying tasks, and few-shot learning tasks, both within and beyond the context length. HoPE demonstrates the superior performance compared to other PES.\nTo sum up, we make three major contributions:\n(1) We show that long-term decay in PEs is unnecessary in the era of large models, as supported by empirical analysis of various PEs.\n(2) We explore the relationship between the overall attention pattern and the decomposed components of ROPE, and propose a new explanation for ROPE's limited performance and poor extrapolation.\n(3) Based on the above insights, we design HoPE, a novel relative positional encoding. Experiments empirically validate the effectiveness of HoPE."}, {"title": "2 Related Work", "content": "Positional encoding is a fundamental component of Transformer models (Vaswani et al., 2017), addressing the lack of sequential information inherent in self-attention mechanisms. The existing positional embeddings can be broadly categorized into absolute (APE) and relative (RPE) various."}, {"title": "2.1 Absolute positional encoding (APE)", "content": "Absolute positional encoding (Vaswani et al., 2017; Wang et al., 2021; Kiyono et al., 2021) focuses solely on individual position information and is typically applied in the first layer of the model. It is implemented by assigning a (learnable or fixed sinusoidal) real-valued encoding $pe_i$ to each position $i$ and is integrated into the representation of input sequences through simple vector addition."}, {"title": "2.2 Relative positional encoding (RPE)", "content": "Although absolute positional encoding (APE) is simple and intuitive, it struggles to generalize effectively for long sequences. As a result, recent research has focused primarily on optimizing relative positional encoding (RPE) (Shaw et al., 2018; Raffel et al., 2019b; Lv et al., 2023). Currently, the popular RPE methods can be divided into two main types (Zheng et al., 2024): rotary position encoding and addition position encoding.\nRotary position encoding (RoPE) Su et al. (2021) proposed Rotary Position Encoding (RoPE), which encodes positional information by rotating the query and key vectors. The detailed implementation can be found in Section 4.2.1. This encoding method cleverly computes the inner product of relative positions by encoding absolute positions without altering the attention computation process, making it more compatible with various efficient inference methods. However, the original ROPE encoding exhibits poor extrapolation capability for longer sequences (Press et al., 2021; Kazemnejad"}, {"title": "3 General Experiments Setups", "content": "We train the model using the Llama architecture (Touvron et al., 2023a,b) with the task of next-token prediction. The training dataset contains 200 billion tokens sourced from RedPajama (Computer, 2023). All experiments use Llama tokenizer with a vocabulary of 32,000 tokens and a maximum of 50,000 update steps. Unless otherwise specified, the training length is 512, the model size is 125M and the tests use the last checkpoint. Detailed configurations and other hyperparameters are provided in the Appendix A."}, {"title": "4 Discussion on Long-term Decay in Attention Pattern", "content": "In this section, we first present the position-related attention patterns (within the training length) learned by three PEs. We observed that, although the long-term decay of PEs is intuitive, this decay is not global in the empirical attention patterns. Instead, the attention patterns tend to resemble a U-shape curve.\nSecondly, we delve into a detailed analysis of the relationship between this U-shape pattern and the"}, {"title": "4.1 Attention Patterns based on various PES", "content": "Set up To observe the position-related attention patterns, we generate 5,000 data samples, each assigning a random token from the vocabulary to all positions in the input sequence (except for the initial [bos]). We then compute the (pre-softmax) attention logit for each position and averaged the results across all heads, noting that most heads exhibited similar patterns. We focused on three PEs including learnable APE, ROPE, and KERPLE. We don't take ALiBi into account, as its bias matrix B is unlearnable and forces the attention pattern to be global long-term decay.\nResult Results shown in Figure 1. One important observation from the figure is that the attention patterns do not exhibit global long-term decay. Instead, the attention patterns tend to form a U-shape curve, which ensures the decay of adjacent tokens while increasing the importance of the initial tokens.\nMoreover, we noted that while ROPE claims it employs multiple components with different frequencies to ensure long-term decay attention, it empirically learns the U-shape pattern. We wonder which components truly matter in this process and delve into a detailed analysis."}, {"title": "4.2 Effects of Different Components of RoPE in Attention Pattern", "content": null}, {"title": "4.2.1 Preliminaries", "content": "Rotary Positional Encoding (RoPE, Su et al., 2021) In each Transformer layer, RoPE applies a d-dimensional rotation matrix (denoted as $R_{\\theta,m}$) to the query or key vector at position m in the sequence for positional encoding. The specific inner product process can be illustrated as follows:\n$q_m = R_{\\theta,m}W_q x_m = R_{\\theta,m}q$,\n$k_n = R_{\\theta,n}W_k x_n = R_{\\theta,n}k$,\n$q_m k_n = (R_{\\theta,m}q)^T (R_{\\theta,n}k) = q^T R_{\\theta,m-n} k$\nwhere x is the d-dimensional input of the current Transformer layer, and the matrix $R_{\\theta,m}$ is a block diagonal matrix consisting of d/2 blocks, each of which size 2 \u00d7 2 and assigned a specific angle \u03b8. This is defined as:\n$R_{\\theta i, m} = \\begin{bmatrix} cos(m \\theta_i) & - sin(m \\theta_i) \\\\ sin(m \\theta_i) & cos(m \\theta_i) \\end{bmatrix}$\n$R_{\\theta,m} = Diag(R_{\\theta 0,m}, ..., R_{\\theta d/2-1,m})$\nwhere $\u03b8_i = b^{- \\frac{i}{d}}$ , and b is referred to as the base of the rotary angle.\nAccording to the formula above, we can see that the dot product in attention can be broken down into an inner product process of d/2 components, each with a distinct angle \u03b8i, followed by a summation. This can be expressed by the following formula, which allows us to explore the individual effect of each positional component."}, {"title": null, "content": "$q_m^T k_n = q^T R_{\\theta,m-n} k = \\sum_{i=0}^{d/2-1} q_i^T R_{\\theta i,m-n} k_i$\n$= \\sum_{i=0}^{d/2-1} ((q_{i,0}k_{i,0} + q_{i,1}k_{i,1}) \\cdot cos((m \u2013 n)\\theta_i))$\n$+ (q_{i,0}k_{i,1} - q_{i,1}k_{i,0}) \\cdot sin((m \u2013 n)\\theta_i))$\nwhere i represents the component ID, ranging from 0 to d/2 - 1. As shown in the above formula, each component follows the cosine property, with a frequency \u03b8i. And the overall effect of all components theoretically follows the principle of long-term decay(See Section 3.4.3 in Su et al., 2021).\nVariance Accounted For (VAF, Yoon et al., 2021; Qiu et al., 2021) VAF is primarily used to measure the explanatory power of components for the total variability. It serves as an crucial criterion for identifying effective principal components. A larger value indicates that the component holds greater importance. The formula is as follows:\n$VAF_{\\hat y,y}(\\%) = [1 - \\frac{\\sum_{i=1}^n (y_i - \\hat y_i)}{\\sum_{i=1}^n y}] \\times 100$\nwhere \u0177 is a component of y."}, {"title": "4.2.2 Experiments", "content": "Set up We adopt a setup similar to that in Section 4.1. Additionally, We calculate the inner product of each component assigned with a unique frequency \u03b8. We then calculate the Variance Ac- counted For (VAF) every 2,000 training steps to"}, {"title": "5 A Novel PE Enhances Model's Context Awareness and Exploration", "content": "Inspired by all experimental observations above, we proposed High-frequency rotary Position Encoding (HoPE). With slight modification in ROPE, HOPE greatly improves the model's context awareness and extrapolation. We first detail our approach and then validate its effectiveness on perplexity, copy task, and few-shot tasks. The results demonstrate that HoPE exhibits superior performance compared to other PEs."}, {"title": "5.1 Method", "content": "We propose our method based on the following considerations: (1) Global decay is unnecessary, thus some components in position encoding could be removed. (2) Components with U-shape fluctuations within the training length lead to shortcut learning and poor extrapolation. (3) Components with lower frequencies tend to learn semantics but are not be well learned. Since both types of components belong to the low-frequency and are mostly controlled by the latter part of the $R_{\\theta,m}$ matrix in the original RoPE, we implement our approach by replacing these components with position-independent ones while retaining the high-frequency components. We call our method High-frequency rotary Position Encoding (HoPE).\nWe first identify the \"to-be-activated\" components and top low-frequency components in original ROPE. As mention in Section 4.2.2, the frequencies ($\u0398_{al}$) and the minimum ID (a) of these components could be calculated based on the training context length L. The process is as follows:\n$\u0398_{al} = {\u03b8| \u03b8 < \\frac{2\\pi}{L} }, \u03b8 \u2208 \u0398$\n$a = argmax(\u0398_{al})$\nNext, we divide the query (or key) into two parts based on the ID a, applying positional encoding only to the first part. For the $R_{\\theta n,m}$ matrix applied in positional encoding, we obtain it by setting"}, {"title": null, "content": "$\u0398_h = \u0398 - \u0398_{al}$. The entire process is shown in the following formula.\n$q_{m,h} = q_m[: 2a]$, $q_l = q_m[2a :]$\n$k_{m,h} = k_m[: 2a]$, $k_l = k_m[2a :]$\n$q'_{m,h} = R_{\\theta n,m}q_{m,h}$, $k'_{n,h} = R_{\\theta n,n}k_{n,h}$,\n$R_{\\theta n,m} = Diag(R_{\\theta 0,m}, \u2026\u2026, R_{\\theta a\u22121,m})$\n$q_m k_n = q'_{m,h} R_{\\theta n,m-n}k'_{n,h} + q_l^T k_l$"}, {"title": "5.2 Effect Verification of HOPE", "content": null}, {"title": "5.2.1 Evaluation", "content": "Traditional methods for testing extrapolation usually use perplexity (PPL) as a metric. However, previous studies (Press et al., 2021) indicate that perplexity (PPL) does not effectively reflect a model's ability to fully leverage the context. In fact, a model can achieve lower PPL by primarily focusing on nearby tokens within the training length. Therefore, to more comprehensively assess the model's extrapolation, along with its contextual awareness and instruction-following potential, we additionally design two simple tasks: copying and few-shot learning.\nPerplexity Perplexity (PPL) is the most commonly used metric for evaluating a model's extrapolation capability. We conduct our evaluation on a subset of the C4 dataset (Raffel et al., 2019a) with 1,000 samples by comparing the zero-shot perplexity of the last 256 tokens across different input lengths.\nCopy Task The copy capability is one of the most fundamental abilities of language models and is closely related to token order. Many previous works (Liu et al., 2023; Golovneva et al., 2024; Lv et al., 2024) on model structure optimization have designed similar tasks to evaluate the models' effectiveness. Based on these studies, We designed our copy task. Specifically, we constructed a test set containing 500 samples, with each sample consisting of multiple sequences. Each sequence has an average length of 12 tokens, with a unique 8-gram prefix and 4-gram suffix. During testing, we concatenate a specific number of sequences with the prefix of a certain i-th sequence (queried sequence) to serve as the model's input. The model's objective is to output the suffix of the queried prefix, with the middle sequence selected as the queried one."}, {"title": "5.2.2 Result", "content": "The results for perplexity (PPL), copy task, and few-shot task are presented in Figure 4, Table 1 and Table 2, respectively. From these results, we can draw the following conclusions:\n(1) From all perspectives in the figure and tables above, it can be confirmed that our approach significantly enhances the context awareness and extrapolation of the original RoPE. As shown in Figure 4, our method noticeably smooths the increase in PPL observed in RoPE, achieving low PPL even with training lengths 4 times longer or more. It records a PPL of 8.5241 at 512, and 13.0257 at 4,096. Table 1 and 2 further demonstrate that that our approach not only improves extrapolation but also enhances context awareness within the training length. Specifically, compared to RoPE,"}, {"title": "6 Ablation Study of HoPE", "content": "In this section, we conducted an ablation study of our method and validated its effectiveness through measurements on the copy task.\nSet up We mainly performed three ablation settings:\n\u2022 AB1: replacing only the \"activated\" components with positional-independent components.\n\u2022 AB2: replacing only the top low-frequency components with positional-independent components.\n\u2022 AB3: removing both components without adding positional-independent ones. We do this by replacing these components with high-frequency components.\nResult As shown in Table 3, removing the \u201cactivated\" components (AB1) results in a significant improvement in both context awareness and extrapolation, with an average increase of 14.5 points compared to RoPE. This outcome suggests that these \"activated\" components indeed contribute to the model's short-cut learning, hindering optimal learning.\nRemoving the top low-frequency components (AB2) helps improve the model's context awareness but contributes less to extrapolation. This confirms that the \"activated\" components are the key factor behind poor extrapolation performance.\nAdditionally, we can observe that AB3 shows a significant decline in both context awareness and extrapolation, highlighting the importance of the position-independent components. This indicates that the model indeed requires certain components to learn semantic information. The slight improvement (an average increase of 0.67 points) from AB2 further suggests that the original low-frequency components in RoPE effectively fulfill this role, while they have not been fully learned.\nBased on the results above, we have demonstrated the rationale behind our HoPE's design and identified the source of its performance improvements."}, {"title": "7 Effect Scaling on Model Size and Training Length of HoPE", "content": "In this section, we explore the scaling effects of our method with respect to model size and training length."}, {"title": "7.1 Scaling on Model Size", "content": "Set up We increased the scale of model from 125M to 300M and compared the performance of all position encodings in terms of perplexity (PPL) and the copy task. The details of model configurations can be found in Appendix A.\nResult Results are shown in Figure 5 and Table 4. As the model size grows, all methods show improvements in their metrics. Our HoPE's PPL at a length of 512 decreases from 8.5241 to 7.4629, and at 4192, it drops from 13.0257 to 10.6933. In terms of copy ability, HoPE's average score increases from 60.23 to 66.00. Additionally, our HOPE consistently demonstrates superior performance, highlighting its robust improvements. Our HoPE yields an average increase of 27.77"}, {"title": "7.2 Scaling on Training Length", "content": "Set up We varied the training lengths from 512 to 2048 in a doubling increase. We use PPL and few-shot tasks for evaluation.\nResult The results in Table 5 and Table 6 demonstrate that our approach consistently shows strong performance as the training length increases. The model maintains low PPL even beyond four times the training length, indicating robust extrapolation. Moreover, as the training length extends, the model demonstrates notable improvements in context awareness. As shown in the Table 5 and Table 6, when the training length increases from 512 to 2048, PPL for the test length of 512 decreases from 8.5241 to 7.8704, while performance in the few-shot task improves from 96 to 100."}, {"title": "8 Conclusion", "content": "In this paper, we explore the empirical attention patterns of various positional encodings and observe that position-related attention tend to form a U-shape pattern, benefiting more from local decay rather than global. Our further analysis of ROPE reveals a strong correlation between the U-shape pattern and its learned components. We identify that certain \"activated\" components and top low-frequency components in ROPE hinder the model's optimal learning process, limiting its context awareness and extrapolation. Consequently, we propose our method, HoPE, which breaks the principle of long-term decay in theory, allowing for optimal utilization of components for positional encoding. Extensive experiments demonstrate its effectiveness in enhancing both context awareness and extrapolation. We hope our method will inspire the community to examine many of the \"taken for granted\" design choices in transformers, with the aim of advancing the development of more powerful large language models (LLMs)."}]}