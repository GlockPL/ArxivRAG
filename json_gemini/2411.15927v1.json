{"title": "Generative Context Distillation", "authors": ["Haebin Shin", "Lei Ji", "Yeyun Gong", "Sungdong Kim", "Eunbi Choi", "Minjoon Seo"], "abstract": "Prompts used in recent large language model based applications are often fixed and lengthy, leading to significant computational overhead. To address this challenge, we propose Generative Context Distillation (GCD), a lightweight prompt internalization method that employs a joint training approach. This method not only replicates the behavior of models with prompt inputs but also generates the content of the prompt along with reasons for why the model's behavior should change accordingly. We demonstrate that our approach effectively internalizes complex prompts across various agent-based application scenarios. For effective training without interactions with the dedicated environments, we introduce a data synthesis technique that autonomously collects conversational datasets by swapping the roles of the agent and environment. This method is especially useful in scenarios where only a predefined prompt is available without a corresponding training dataset. By internalizing complex prompts, Generative Context Distillation enables high-performance and efficient inference without the need for explicit prompts.", "sections": [{"title": "Introduction", "content": "In the real world, for inference in large language model (LLM) based applications (e.g. ChatGPT), fixed and complex prompts are often used repeatedly. Although advanced prompts can improve performance, their dependence on lengthy prompts raises concerns regarding computational efficiency for service providers and cost-effectiveness for users. Longer prompts can considerably increase computational overhead, especially during multi-turn inference, making it a potential bottleneck in practical applications. Therefore, finding effective strategies to optimize these prompts while maintaining performance is crucial for the practical deployment of LLM-based applications.\nTo address this issue, existing methods can be categorized into two approaches. One practical approach only for efficient prompt computation is compressing prompts into external embeddings (Mu et al., 2023; Chevalier et al., 2023; Ge et al., 2024) or compressed text (Jiang et al., 2023b; Li et al., 2023; Jiang et al., 2024; Pan et al., 2024). However, these methods still require additional tokens, limiting their ability to fully internalize prompts and effectively modify the model's behavior as intended. Alternatively, fine-tuning (Kim and Rush, 2016; Zou et al., 2024) or distilling, a.k.a context distillation (Askell et al., 2021; Snell et al., 2022; Choi et al., 2023; Li et al., 2024), have been explored as canonical approaches for internalizing prompts. These methods adjust the language model's behavior to follow the prompt's intentions without requiring a prompt during inference.\nHowever, these internalization methods are limited in that the model cannot reference the prompt's content during training. Instead, they rely on indirect training based on the model's output (Kim and Rush, 2016) or distribution (Askell et al., 2021; Snell et al., 2022; Choi et al., 2023; Li et al., 2024) of the original model when the prompt is provided. This leads to significant performance degradation in information-rich tasks or specific requirements, such as agent's action requirements or schema grounding (Choi et al., 2023).\nTo overcome the limitations, we propose Generative Context Distillation (GCD), a method that is trained to generate the target prompt, rather than merely using it as input. GCD employs joint loss training, combining two approaches: 1) mimicking the teacher's output to guide behavior, similar to context distillation approaches (Askell et al., 2021; Snell et al., 2022; Choi et al., 2023; Li et al., 2024), and 2) generating the content of the prompt while inferring why the output should change based on that prompt. GCD employs a lightweight internalization process for each prompt, requiring only"}, {"title": "Related Works", "content": "Context Distillation is a methodology designed to internalize context tokens within a language model, enabling the model to perform various tasks without requiring explicit contextual information. For instance, Askell et al. (2021) distills a persona as context to facilitate helpful, honest, and harmless alignment, while Snell et al. (2022) incorporates both prompts and scratch pads into the context distillation process to enhance performance on more complex tasks. Li et al. (2024) applies context distillation to internalize demonstrations for in-context learning scenarios. While these methods focus on distilling generalized and coarse-grained contexts, Choi et al. (2023); Zou et al. (2024) introduce a fine-grained context distillation approach, targeting more specific and predetermined prompts. Choi et al. (2023) focuses on short chat histories or prompts, and Zou et al. (2024) relies on a tailored training dataset to retrieve similar examples based on the given user input. Following these previous works, our goal is to enable the model to internalize the specific predetermined prompts. However, existing methods still limit the model's ability to directly learn the content of the prompt, as they rely on training a student model using outputs from a teacher model. We address this limitation by generating prompts that allow the model to learn the content directly, enabling it to handle more realistic and information-rich prompts.\nPrompt Compression is one of the practical approach to reducing the computational overhead caused by lengthy prompts. In case of the API-based large language models (LLMs) services, systematic caching solutions allow frequently used prompts to be stored between API calls. For users of API-based LLMs, text-based prompt compression methods are proposed, where key segments of long prompts are selected on a token-by-token basis to generate a compressed version of the original prompt (Jiang et al., 2023b; Li et al., 2023; Jiang et al., 2024; Pan et al., 2024). Although this approach reduces prompt length, it often still results in relatively long token sequences, as essential tokens must be retained. On the other hand, embedding-based prompt compression methods generate cached token embeddings for prompts, which can be utilized as a more efficient representation in LLMs (Mu et al., 2023; Chevalier et al., 2023; Ge et al., 2024). These methods offer the advantage of using fewer token embeddings compared to text-based compression but often require modifications to the model architecture, making it challenging to leverage the compressed vectors across different models."}, {"title": "Problem Definition", "content": "Following Choi et al. (2023), we assume a scenario where an application-specific prompt p is predetermined. Our goal is to guide the model to behave as if the prompt is given, even in its absence. Similar to the previous context distillation approaches (Askell et al., 2021; Snell et al., 2022; Choi et al., 2023; Li et al., 2024), we define the teacher model T and the student model S based on whether the prompt p is provided to the same model \\theta.\nThe teacher model T is defined as a function that takes the prompt p and the input \\textbf{x}_i at turn i, generating the teacher's output: \\textbf{y}_i^T = T(\\textbf{x}_i, p) = f_T(\\textbf{x}_i, p; \\theta). The student model S is a function that takes the input \\textbf{x}_i at turn i, generating the student's output: \\textbf{y}_i^S = S(\\textbf{x}_i) = f_S(\\textbf{x}_i; \\theta). We approximate the student model's behavior to match that of the teacher model, conditioned on the prompt p over multiple turns, as shown in Equation 1,\nP(\\textbf{y}_i^S|\\textbf{x}_i) \\approx P(\\textbf{y}_i^T|\\textbf{x}_i, p).   (1)"}, {"title": "Generative Context Distillation", "content": "In this section, we demonstrate our novel method, Generative Context Distillation (GCD), to internalize the given prompt. Basically, GCD involves training the student model to mimic the behavior of the teacher model given the prompt p. We finetune the student model on the multi-turn outputs generated by the teacher model (\\textbf{y}_i, i \\in {0,1, ..., N}), inspired by sequence-level knowledge distillation approaches (Kim and Rush, 2016; Touvron et al., 2023b) which follow the hard label distribution from the teacher model.\nL_{SFT} = -\\sum_{i=0}^N log P(\\textbf{y}_i^T| \\textbf{x}_{<i}, \\textbf{y}_{<i}, \\textbf{x}_i)\nHowever, the model is still unable to learn the content of the prompt directly; it only learns indirectly through the teacher's output. We introduce an additional loss function, Prompt Generation loss (PG), where the loss is calculated directly on the prompt. This loss involves training the student model to understand why its behavior should align with the teacher's behavior based on the prompt content. As illustrated in Figure 1, for a given input \\textbf{x}, the model is trained to generate a prompt p along with a reason r for why the output should be changed. In this process, the student's output \\textbf{y}_i^S is considered the initial state (\u201cAS-IS\u201d), while the teacher's output \\textbf{y}_i^T represents the desired state (\u201cTO-BE\u201d). This process is formalized in Equation 3:\nL_{PG} = -log P(p, r|\\textbf{y}_i^S, \\textbf{y}_i^T, \\textbf{x}).   (3)\nWe utilize a hyperparameter \\lambda to combine the losses into a joint loss function, resulting in the final joint loss function:\nL = (1 - \\lambda) L_{PG} + \\lambda L_{SFT}   (4)\nOur goal is to enable lightweight training and inference for each prompt, while also adapting to changes in the prompt effectively. To achieve this, we employ QLoRA (Dettmers et al., 2023) to learn prompt-specific adaptors, allowing us to tailor our approach to each individual prompt."}, {"title": "Components for Prompt Internalization", "content": "Following Choi et al. (2023), we assume a realistic scenario where the predetermined prompt has never been encountered before and is not included in any training dataset for prompt internalization. Consequently, we first generate the components as a pseudo training dataset from the prompt p.\nPseudo User Input. Similar to typical query generators (Lewis et al., 2021; Choi et al., 2023; Oh"}, {"title": "Experiments Setup", "content": "We evaluate our method on the three agent benchmark tasks: OS-Interaction (Liu et al., 2023), Web Browsing (Deng et al., 2023), Web Shopping (Yao et al., 2022). As mentioned in Section 5, we assume our scenario where an application-specific prompt p is predetermined before the model is deployed. For that, we utilize the AgentBench (Liu et al., 2023) settings, which organize a prompt with multi-turn evaluation samples for each task. Each task's prompt consists of general task description, agent's action space description, and shot-examples for agent behavior. Detailed information for each task's prompt is described in Appendix B.\nOS Interaction (Liu et al., 2023). This task involves interacting with an Ubuntu Docker container using bash commands and committing answers, with a 474 token prompt and 144 evaluation samples. The agent's performance is measured by the Success Rate.\nWeb Browsing (Deng et al., 2023). This task formulates element selection as a multi-choice QA problem. We follow the AgentBench (Liu et al., 2023) setting, where the agent operates within an HTML action space (e.g., click, type, select options) using a 1,424 token prompt and 100 evaluation samples. The agent is evaluated using a Success Rate metric, which is based on two criteria: the correctness of the selected element and whether the predicted operation matches the ground truth value for each step.\nWeb Shopping (Yao et al., 2022). In this task, the agent navigates scraped amazon.com pages to"}, {"title": "Baselines", "content": "We explore various baselines to internalize the prompt, ranging from distillation approaches (Snell et al., 2022; Choi et al., 2023) to compression approaches (Pan et al., 2024; Ge et al., 2024).\nFull Fine-tuning. Since previous typical context distilled language models are based on much smaller model (Choi et al., 2023) or are not publicly available (Snell et al., 2022), we adopt recent knowledge distillation approaches (Lin et al., 2020; Gu et al., 2024; Ko et al., 2024) for autoregressive large language models in our context distillation setup. (1) The basic distillation approach (Hinton et al., 2015) employs the Kullback-Leibler Divergence loss (KLD) between the logit distributions of the student and teacher model. (2) Sequence-level Knowledge Distillation (SeqKD) (Kim and Rush, 2016) enforces the student model to generate the teacher model's outputs on a fixed dataset. (3) As a strong baseline, we also employ a joint loss (SeqKD+KLD), inspired by recent knowledge distillation works (Song et al., 2020; Gu et al., 2024), which incorporates the language modeling loss during distillation. This approach can be interpreted as a hybrid distillation loss that combines the benefits of both soft labels and hard labels from the teacher model.\nPrompt Prepending. One straightforward approach to consider is prepending the prompt during finetuning. If the prompt is consistently prepended during training, we expect the model to indirectly contextualize the prompt. However, since the model needs to predict without the prompt during inference, there is a potential mismatch between the training and inference distributions. To address this concern, we compare two baseline approaches: (1) always prepending the prompt during training (100% probability), and (2) stochastically prepending the prompt during training (50% probability). This baseline utilizes QLoRA (Dettmers et al., 2023) with the same settings as our method. This approach can be viewed as an extension of the method that relies exclusively on SFT loss for training.\nText-based Prompt Compression. LLMLingua-2 (Pan et al., 2024) is a prompt-agnostic method for generating compressed texts. LLMLingua-2 explicitly compresses tokens using a smaller model, such as XLM-ROBERTa-large (Conneau et al., 2020). By performing with the compressed prompt, this baseline is expected to achieve efficient inference while maintaining comparable performance.\nEmbedding-based Prompt Compression. We utilize ICAE (Ge et al., 2024) to compress the prompt into cached prompt embeddings. Following Ge et al. (2024), we compress the prompt into 128 tokens and prepend compressed tokens to the user input at each inference time. Since the official checkpoint of ICAE exhibits significantly low performance on AgentBench (Liu et al., 2023), we finetune the baseline specifically for AgentBench (Liu et al., 2023). Additional details regarding this baseline are described in Appendix \u0421.3.\nUpper Bound. We utilize the teacher model as an upper bound that inputs the full prompt, consistent with previous studies (Snell et al., 2022; Choi et al., 2023). In our preliminary experiments (see Appendix C.2), we observe that the fine-tuned model performed worse than the original model in AgentBench (Liu et al., 2023). Consequently, we consider the original model with the full prompt as the teacher model following the setup of Liu et al. (2023). Since ICAE (Ge et al., 2024) is based on the Mistral-7B (Jiang et al., 2023a) model, we establish a separate upper bound specifically for ICAE (Ge et al., 2024) to ensure a fair comparison. For a detailed explanation of the upper bound, please refer to the Appendix C.2."}, {"title": "Implementation Details", "content": "To internalize the agent-based prompt into the language model, we utilize the LLaMA-3-8B-Instruct (Dubey et al., 2024) as the target model. Following our scenario in Section 5, we fine-tune the model using the 1,000 pairs of pseudo dataset as our train dataset, and this is applied equally to all baselines. We utilize QLoRA (Dettmers et al., 2023) with rank r = 16 only requires 0.5% of parameters. Additional training details are described in Appendix C.4."}, {"title": "Results", "content": "Compression baselines. LLMLingua-2 (Pan et al., 2024) exhibits a significant performance drop as the compression rate increases. Although the semantics of the prompt can be inferred, the loss of format crucial for agent tasks leads to substantial performance drops. In the Web Shopping scenario, LLMLingua-2 (Pan et al., 2024) achieves the best performance at a compression rate of 90%. However, compression rates exceeding 30% led to a failure in all tasks. In the case of ICAE (Ge et al., 2024), despite fine-tuning, it struggles to handle the agent application prompts. Many embedding-based compression approaches (Mu et al., 2023; Chevalier et al., 2023), including ICAE (Ge et al., 2024), are primarily optimized for general texts such as articles, and thus exhibit limitations in compressing task-specific information, as required by AgentBench (Liu et al., 2023).\nDistillation/Finetune Baselines. When trained with only the basic distillation loss (KLD), the model fails in AgentBench (Liu et al., 2023), suggesting that 1,000 training examples may not be enough to train the entire model. While SeqKD helps promising results in the Web Browsing task, its performance is still lacking in other tasks. Among prompt prepending baselines, stochastically prepending prompts demonstrates some performance gains but ultimately fails to overcome the distribution mismatch at inference time, resulting in poorer performance compared to our approach.\nGenerative Context Distillation. GCD achieves superior performance in the OS interaction task, reaching 100% of the upper bound. It also consistently demonstrates high performance of over 82% across Web Browsing and Web Shopping tasks, both of which have longer prompts that exceed 1,000 tokens in length. The incorporation of PG loss into SFT loss shows a notable performance improvement of approximately 25% in web shopping tasks. Furthermore, when compared to Prompt Prepending which is based on SFT loss and QLoRA (Dettmers et al., 2023) adaptor, our approach exhibits higher performance by leveraging PG loss, rather than relying solely on prepending prompts to the input."}, {"title": "Analysis", "content": "To understand the factors behind the performance improvement of GCD, we perform three types of ablation studies. First, we compare the impact of PG loss and SFT loss on task performance. Second, we investigate the influence of the reason and prompt in PG loss by removing each component during generation. Finally, instead of omitting components, we explore the effect of reordering the input and output elements in PG loss while keeping the overall information level constant."}, {"title": "Impact of the Loss", "content": "As shown in Table 2, the removal of SFT loss causes the model to suffer significantly in task performance. This suggests that task performance is fundamentally dependent on SFT loss. However, incorporating PG loss consistently improves performance across all tasks. Our findings indicate that while SFT loss focuses on task behavior, PG loss plays a complementary role in enhancing task performance by concentrating on prompt internalization."}, {"title": "Impact of the Reason/Prompt", "content": "In Table 2, the prompt has a greater impact on performance than the reason across all tasks. This is interpreted as being due to the fact that the prompt is longer in length than the reason and generally contains more information, thereby exerting a more significant influence on the Prompt Generation loss."}, {"title": "Impact of Input/Output Ordering", "content": "Despite ensuring consistency in the overall amount of information in Table 3, we observe a decline in overall performance when certain components are removed during the generation process similar to Table 2. Except for the OS task which has relatively short prompt length than Web Browsing or Web Shopping, P(r|p, x, y) consistently outperforms P(p|x, y, r). This result is attributed to the fact that providing the model with a prompt, input, and output, followed by generating a reason, aligns more closely with the typical dialogue flow of an LLM, compared to inferring the prompt from an input, output, and reason. Since the prompt generally contains more information than the reason, inferring a prompt based on the reason is likely to be a more challenging task for the model.\nSimilarly, when comparing w/o reason in Table 2 (equivalent to P(p|x,y)) and P(p|x,y,r), we see that while incorporating the reason as input slightly improved scores in the OS task, the performance drops in the web browsing and web shopping tasks. The prompt in the OS task is relatively shorter, making it easier to infer prompts based on the reason. However, as the length of the prompt increases, it becomes more challenging for the model, leading to a negative impact on training. When comparing with w/o prompt in Table 2 (equivalent to P(r|x, y)) and P(r|p, x, y), we observe that adding prompt information leads to an improvement in all tasks."}, {"title": "Efficiency", "content": "To compare the efficiency between Generative Context Distillation and other baselines that require fixed prompts, we sample an example with more than 5 turns from a Web Shopping task containing over 1200 tokens. We then compare the MACs (Multiply-Accumulate Operations), FLOPs (Floating Point Operations), and latency to LLaMA-3-8B-Instruct (Dubey et al., 2024) based baselines. All metrics were measured by DeepSpeed Profile. As shown in Figure 3, the overhead for baselines that require prompts accumulates and increases with each turn. While LLMLingua-2 (Pan et al., 2024) exhibits slightly better performance due to the reduced number of prompt tokens, it still incurs a larger overhead compared to ours. Furthermore, when considering compression overhead, the LLMLingua-2 (Pan et al., 2024) which is based on XLM-ROBERTa-large (Conneau et al., 2020),"}, {"title": "Conclusion", "content": "In this paper, we propose a novel prompt internalization method, Generative Context Distillation (GCD), which generates both the contents of the prompt and the reasoning behind changes to the model's output while mimicking the behavior of the teacher model. To address the challenging scenario where only predetermined prompts are available without additional training data, we introduce Self Role-Playing Conversation, a method that generates a pseudo-conversational training dataset from the given prompt. Our approach demonstrates that even in scenarios with lengthy agent prompts, GCD maintains high performance without relying on the prompt itself. Moreover, GCD improves efficiency by fully internalizing the prompt without requiring any additional tokens."}, {"title": "Limitations", "content": "We assume the use of long and fixed prompts from realistic application scenarios. However, collecting official prompts from real-world applications (such as ChatGPT) poses significant challenges, leading us to rely on prompts from academic agent-based applications. This introduces a limitation, as only a representative prompt for each task are considered, and our approach handles a relatively small set of prompts. In future work, we plan to explore context internalization in various domains, including long chat histories, in-context learning, and retrieval-augmented generation (RAG), as well as in multimodal applications involving video and image data."}]}