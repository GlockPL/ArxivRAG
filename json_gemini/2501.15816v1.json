{"title": "AdaF2M\u00b2: Comprehensive Learning and Responsive Leveraging Features in Recommendation System", "authors": ["Yongchun Zhu", "Jingwu Chen", "Ling Chen", "Yitan Li", "Feng Zhang", "Xiao Yang", "Zuotao Liu"], "abstract": "Feature modeling, which involves feature representation learning and leveraging, plays an essential role in industrial recommendation systems. However, the data distribution in real-world applications usually follows a highly skewed long-tail pattern due to the popularity bias, which easily leads to over-reliance on ID-based features, such as user/item IDs and ID sequences of interactions. Such over-reliance makes it hard for models to learn features comprehensively, especially for those non-ID meta features, e.g., user/item characteristics. Further, it limits the feature leveraging ability in models, getting less generalized and more susceptible to data noise. Previous studies on feature modeling focus on feature extraction and interaction, hardly noticing the problems brought about by the long-tail data distribution. To achieve better feature representation learning and leveraging on real-world data, we propose a model-agnostic framework AdaF2M\u00b2, short for Adaptive Feature Modeling with Feature Mask. The feature-mask mechanism helps comprehensive feature learning via multi-forward training with augmented samples, while the adapter applies adaptive weights on features responsive to different user/item states. By arming base models with AdaF2M\u00b2, we conduct online A/B tests on multiple recommendation scenarios, obtaining +1.37% and +1.89% cumulative improvements on user active days and app duration respectively. Besides, the extended offline experiments on different models show improvements as well. AdaF2M\u00b2 has been widely deployed on both retrieval and ranking tasks in multiple applications of Douyin Group, indicating its superior effectiveness and universality.", "sections": [{"title": "1 INTRODUCTION", "content": "Video platforms like Douyin and TikTok are extremely hot nowadays, with billions of users. The progress of personalized recommendation systems has made a significant contribution to the success of those platforms, which helps expose attractive items to users according to their interests. While tracing the source of personalization, features play an essential role by providing the original information of users, items, and interactions between them. Actually, feature engineering has been very important in industrial recommendation systems from past to present.\nConventional matrix factorization (MF) methods [20, 31, 32] work by decomposing the user-item interaction matrix into the product of two lower dimensional matrices, where each row represents a user/item ID embedding. After MF, Factorization Machines (FMs) [30] leverage more features in addition to ID and conduct pair-wise interactions via the inner product of two embeddings. With more computing firepower and the growing scale of data, recommendation models in industry have migrated from FM-based models to Deep Neural Networks (DNN) [1, 9, 16, 21, 50, 53, 56, 58], which significantly improve model performance. DNNs have the ability to model more complex feature interactions [8, 40, 44], thus being able to handle more types of feature inputs, e.g., image [34, 45] and text [6]. Reviewing the development history of recommendation models, the utilization of features is getting better, and more complex features are becoming acceptable, varying from ID to multimodal features.\nIn practice, recommendation models usually take lots of features as inputs, such as IDs, sequences, and characteristics. With diverse features engaged in, feature modeling is crucial for final performance. Well-learned feature representations can help models generalize better, handle noise and uncertainty in the data, and uncover hidden patterns. However, the data distribution in industrial recommendation systems usually follows a highly skewed long-tail pattern due to the popularity bias, e.g., 5% of users/items account for over 80% of samples. Head users probably dominate the model learning with a large proportion of samples. Since the user/item ID is the most fine-grained and the most informative feature for users with a large amount of behavioral data, the model also tends to attribute more knowledge to IDs. This easily leads to over-reliance on ID-based features, such as user/item IDs and ID sequences of interactions. Such over-reliance will seriously affect the comprehensive learning of features, making the model less generalized and more vulnerable to noise interference. Taking data noise as an example, user behavior is often interfered with unobserved factors. For instance, a user might skip a favorite video just because they received a phone call at that time, but such noisy pattern can be wrongly fitted by the user ID.\nFurther, the above over-reliance also limits the learning of non-ID meta features, causing poor performance in the user/item cold-start stage. For example, recommending a new video with few samples highly relies on its attribute features such as theme and category. The weakening of feature representation learning will make the feature leveraging less responsive to different user/item states.\nIn this paper, to tackle problems brought about by the long-tail distribution, we propose a simple but powerful framework Adaptive Feature Modeling with Feature Mask (AdaF2M\u00b2), which consists of a feature-mask mechanism and a state-aware adapter. We generate multiple augmented samples with the feature-mask mechanism and learn features comprehensively via multi-forward task-oriented training on these samples. To adaptively model users/items of different states, we propose the state-aware adapter, which takes empirical state signals as input, to apply adaptive weights on features responsive to different user/item states. Note that it is convenient to deploy AdaF2M\u00b2 with different base recommendation models.\nThe main contributions of our work are summarized into four folds:\n\u2022 To achieve feature representation comprehensive learning and responsive leveraging on real-world data, we propose a simple but powerful framework AdaF2M\u00b2.\n\u2022 We conduct multiple rounds of online experiments on multiple recommendation scenarios, obtaining +1.37% and +1.89% cumulative improvements on user active days and app duration respectively.\n\u2022 We conduct offline experiments on both public datasets and industrial datasets with billions of samples to demonstrate the effectiveness of AdaF2M\u00b2. In addition, the offline results testify that it is convenient to apply the proposed framework upon different base models.\n\u2022 AdaF2M\u00b2 has been widely deployed on both retrieval and ranking tasks in multiple applications of Douyin Group, indicating its superior effectiveness and universality."}, {"title": "2 RELATED WORK", "content": "In this section, we will introduce the related work of recommendation systems from two aspects: Feature Representation Learning and Feature Representation Leveraging.\nFeature Representation Learning. Researchers have investigated feature representation learning for a long time. Feature interaction aims at capturing associations among different features and producing more diverse combinations of features to improve feature representations. The early works focus on how to produce cross-feature interaction. Factorization machines (FM) [30] have been shown to be an effective method for feature interaction, and many methods [13, 16, 18, 21, 35] capture high-order interaction based on FM. In addition, some methods [43, 44] exploit Multi-layer Perceptron to produce high-order interactions. However, these methods generally enumerate all the cross features under a predefined maximum order, which is very time-consuming, so some researchers pay attention to adaptively feature interactions. With the advantage of attention mechanisms, multi-head attention is utilized for adaptively feature interactions [17, 33, 54]. In addition, AutoML is exploited to seek useful high-order feature interactions without manual feature selection [22]. Due to the high computational costs caused by the exponential growth of high-order feature combinations, some researchers pay attention to efficient feature interaction [10, 40, 51], which aims at reducing the computational costs. Besides, many methods focus on improving representations of some specific features, e.g., item ID [27, 28, 60], user ID [2, 3, 26, 59], short-term sequential features [37, 39, 46, 48, 55, 56, 58], long-term sequential features [5, 29], multi-modal features [34, 45, 52].\nIn addition, some researchers make efforts to enhance representations with novel training frameworks. DropoutNet [41] applies dropout to ID embeddings, which improves the recommendation performance for cold-start users and items. Yao et al. [49] exploits self-supervised learning to improve item representation learning as well as serve as additional regularization to improve generalization. However, these methods ignore the importance of effective feature representation leveraging for users/items of different states and lack additional supervised signals.\nFeature Representation Leveraging. The useless features may introduce noise and complicate the training process [36, 38]. Feature representation leveraging aims at identifying useful feature interactions through model training. The attention mechanism is convenient to model importance of features, and many methods [17, 33, 47, 54] utilize it for the feature selection. AFN [8] takes learnable parameters for feature selection. AutoFIS [23] exploited a two-stage framework, which identifies the feature importance and retrains the model without redundant feature interactions. However, these methods detect beneficial features by learning from the features themselves, which could be dominated by the head users and items, and the ID-based features are usually assigned with high weights. In this paper, we propose a state-aware adapter taking empirical state signals as input responsive to different user/item states."}, {"title": "3 PROPOSED FRAMEWORK", "content": "In this section, we introduce our proposed framework named Adaptive Feature Modeling with Feature Mask (AdaF2M\u00b2), which consists of a feature-mask mechanism and a state-aware adapter. In Section 3.1, we specify the common setup of a recommendation task in industrial recommendation systems. In Section 3.2, we explain the feature-mask mechanism and how it helps comprehensive feature learning. In Section 3.3, we present a state-aware adapter that is able to apply adaptive weights on features responsive to different user/item states. In Section 3.4, we demonstrate the overall framework and how to deploy it in ranking and retrieval models."}, {"title": "3.1 Recommendation Task Setup", "content": "First, we consider the common setup for a binary classification task, such as CTR predicting in recommendation systems. Each sample consists of the input raw features $x = [x_1, , x_n]$ and a label $y \\in \\{0, 1\\}$, where $n$ indicates the number of raw features. In binary classification, a deep recommendation model approximates the probability $\u0177 = Pr(y = 1|x)$ for the sample with input x. Generally, a deep recommendation model consists of a feature embedding layer, a feature interaction layer, and a deep network. The feature embedding layer aims to transform raw features $[x_1,..., x_n]$ into low-dimensional representations, named feature embeddings, denoted as $[v_1,\uff65\uff65\uff65, v_n]$. Then, the feature interaction layer takes the embeddings as input to generate high-order cross-feature representations, and most existing methods focus on this layer, e.g., FM [30], DCN [44]. Finally, the cross-feature representations are fed into a deep network for the prediction. In this paper, we focus on improving the representation of the embedding layer, and the interaction layer and the deep network are denoted as g(.). The prediction of a recommendation model is formulated as:\n$\u0177 = g([v_1,..., v_n])$.\n(1)\nThe cross-entropy loss is often used as the optimization target for binary classification:\n$L = -y log \u0177 - (1 \u2013 y) log(1 \u2013 \u0177)$.\n(2)\nWe further elaborate on the input features. Industrial recommendation systems rely on a large number of features, which can be categorized into three groups: user features, item features, and context features. Here, we present some features widely used in the industry:\n\u2022 User features: user ID, age, gender, city, operating system, App version, activity level, ID sequence of 'Like, Finish, Comment, Share, Follow, Click' behavior and so on.\n\u2022 Item features: item ID, author ID, description, tag, genre, theme and so on.\n\u2022 Context features: time, scenario and so on.\nThere are both ID-based personalized features and non-ID meta features. However, most existing methods [5, 27-29, 39, 56, 60] work on the ID-based personalized features, ignoring the non-ID meta features. This paper focuses on comprehensive learning and responsive leveraging all features."}, {"title": "3.2 Feature Mask", "content": "We propose a feature-mask mechanism to enhance representations via multi-forward task-oriented training with augmented samples, which enables more comprehensive feature learning. The key idea is to get rid of the over-reliance on important features by randomly masking part of all features. We create multiple augmented samples by feature-mask and train on these samples with a task-oriented loss. The augmented samples can force the model to make predictions based on diverse combinations of features, which enables all features to be learned well.\nFirst, for each instance, we sample k times randomly from a range [\u1e9e, y] and obtain k values, where \u1e9e and y represent a range of sampling probability (setting as 0.1 and 0.5 in this paper). Each value p indicates the probability of replacing a feature embedding v with a default mask embedding [MASK]. Note that the default mask embedding of each feature is different. Given the masking probability p, we randomly mask some feature embeddings of a sample with corresponding default mask embeddings to generate an augmented sample. The feature embeddings of an augmented sample are formulated as:\n$[0_1,\uff65\uff65\uff65, [MASK]_i,\u00b7\u00b7\u00b7, [MASK]_j,\u2026\u2026\u2026, v_n]$,\n(3)\nwhere $[MASK]_i$ indicates the default embedding of the i-th feature. With k times random masking, k augmented samples are generated from a given original sample.\nThe second step is learning with the augmented samples. Inspired by recent natural language processing (NLP) [11, 25] and computer vision (CV) [7, 15] techniques, most existing recommendation methods [49, 57] also use self-supervised learning for the augmented samples. The main idea of these methods is that let augmented data from the same sample be discriminated against others, which can improve the discriminability of the representations of users/items. However, the recommendation tasks are different from NLP and CV in two ways: (1) Most tasks of CV and NLP lack sufficient labeled samples, but there are a large amount of unlabeled samples in the real world. However, in recommendation systems, there are billions of labeled samples every day (the feedbacks of users are utilized as labels), and the number of labeled samples is more than the unlabeled samples (the items with no interaction). (2) The prerequisite for many tasks of CV and NLP is understanding the content (text and image), so they need to differentiate between different samples/contents. However, the goal of recommendation models is to make accurate predictions, rather than distinguish different users/items.\nAlong this line, we propose multi-forward training with a task-oriented optimization procedure to learn the augmented samples. In detail, the concatenated embeddings of an augmented sample as Equation (3) are fed into the deep network g(.) to generate the predicted result \u0177, formulated as:\n$\u0177_{mask} = g([v_1,\uff65\uff65\uff65, [MASK]_i, \u00b7\u00b7\u00b7, v_n])$.\n(4)\nThen, we feed k augmented samples into deep networks to obtain k prediction \u0177mask. A task-oriented optimization procedure (directly computing the target loss as Equation (2)) is adopted, which applies additional supervision over the prediction of the augmented samples, denoted as:\n$L_{aux} = \\sum_{(x,y) \\in D} \\sum_{i=1}^{k} -y log(\u0177_{mask}^i) \u2013 (1 \u2013 y) log(1 \u2013 \u0177_{mask}^i)$,\n(5)\nwhere D indicates the training dataset and $\u0177_{mask}^i$ indicates the prediction of the i-th random augmented sample. Then, we apply stochastic gradient descent to update all parameters of the deep model and all embeddings.\nThe feature-mask mechanism which generates multiple augmented samples can help comprehensive feature learning via multi-forward training with task-oriented optimization. It has two main advantages: (1) In real-world recommendation systems, it is common that some features of a sample are missing or mislabeled. The augmented samples are used to simulate the noisy condition of the real-world recommendation systems, which can improve the robustness and generalizability of the model. (2) With the feature-mask mechanism, any features are possible to be masked, including ID-based personalized features. Feeding inputs without personalized features to the deep network can force the model to pay attention to the remained non-ID meta features, e.g., age and gender. Since we adopt task-oriented optimization, the model directly makes predictions based on these unmasked features, which can learn them better. The underlying idea is similar to counterfactual learning, and the counterfactual assumption is that if a high-active user only has the basic features that are assigned in his new-user stage, the model would have consistent prediction for the same user in different stages."}, {"title": "3.3 Adaptive Feature Modeling", "content": "In this section, we propose adaptive feature modeling with a state-aware adapter, which can assign adaptive feature weights for users and items with different states. The deep model takes all feature embeddings $[v_1, , v_n]$ as input, and the adaptive feature modeling aims to assign adaptive feature weights, and the input with weights can be formulated as:\n$[a(v_1),..., a(v_n)]$,\nwhere $a(v_i) = w_i v_i$,\n(6)\nwhere $w_i$ indicates the adaptive weight for the i-th feature.\nHow to generate adaptive weights is the most important key of adaptive feature modeling. The existing methods [8, 17, 33, 47, 54] mainly generate the weights from the feature themselves, which could be formulated as:\n$[W_1,..., W_n] = h([v_1,\uff65\uff65\uff65, v_n])$,\n(7)\nwhere the h() represents the weight generator. However, the learning process is dominated by the head users/items, and the generator would assign bigger weights for the important features of the head users and items. Generally, ID-based personalized features, e.g., ID and sequential features, are important in these methods.\nTo tackle the problem of over-reliance on ID-based features in existing methods, we propose a state-aware adapter, which can assign adaptive weights to features according to the states of users/items. To enable the adapter to perceive the different states, we propose four kinds of empirical state signals:\n\u2022 Active days: The number of days users have opened App in the last 7/30 days is used as active-day features, which can distinguish low-, middle-, and high-active users, denoted as ractive.\n\u2022 ID embedding: With ID embeddings which contain sufficient personalized information as input, the adapter can keep the ability to make personalized predictions for head users and items. We concatenate all ID embedding (user, item, and artist ID) into a vector, denoted as rID.\n\u2022 Norm of ID embedding: Generally, the norm of ID embedding can indicate the quality of ID embedding, e.g., the norm of old users' ID embeddings is bigger than the new users'. Thus, the norm of ID embedding can be exploited to distinguish new and old users, hot and long-tail items. We concatenate the norm of various ID embeddings and use different non-linear functions (log, sqrt, square) to enhance the representations of the norm, denoted as rnorm.\n\u2022 Interaction count: The number of interactions is a strong signal to distinguish the states of users and items. For example, the low-active users (long-tail items) have less interactions than high-active users (hot items), so a small count indicates low-active users (long-tail items). Thus, we utilize the numbers of 'impression', 'add comment', 'click like button' of users and items, indicated as rcount.\nThus, the state-aware adapter takes the concatenated state embeddings [ractive, rID, rnorm, rcount] as input to assign adaptive weight for users/items with different states, formulated as:\n$[W_1,\uff65\uff65\uff65, W_n] = \u03c3(h([ractive, rID, rnorm, rcount]))$,\n(8)\nwhere \u03c3(\u00b7) indicates the Sigmoid function. Online experiments show that the performance with the Sigmoid function is better than the Softmax function. The main reason would be that the output of the Softmax function can be dominated by the main features of head users/items. With the guide of the strong empirical signals, the adapter has better ability to generate suitable distributions of weights for different states of users and items.\nIn addition, the adaptive mechanism in existing methods introduces another issue: it can seriously influence the learning process of feature embeddings. In detail, the prediction with adaptive weights can be formulated as:\n$\u0177 = g([a(v_1),\u2026\u2026, a(v_n)])$.\n(9)"}, {"title": "3.4 Overall Framework", "content": "The overall framework of AdaF2M\u00b2 is shown in Figure 1, which consists a state-aware adapter and a feature-mask mechanism. The deployment of the AdaF2M\u00b2 framework consists of two stages: the training stage and the serving stage.\nTraining stage: The final probability prediction with the weights generated by the state-aware adapter is formulated as:\n$\u0177_{adapt} = g([a(v_1),\u00b7\u00b7\u00b7, a(v_n)])$,\n(11)\nwhere the g() indicates the prediction function. We train the overall framework, including embeddings, the deep network, the adapter, with the cross-entropy loss:\n$L_{main} = \\sum_{(x,y) \\in D} -y log(\u0177_{adapt}) \u2013 (1 \u2013 y) log(1 \u2013 \u0177_{adapt})$,\n(12)\nwhere D indicates the training dataset. For comprehensive feature learning, we utilize the auxiliary loss as shown in Equation (5). Note that the adaptive weights are not one of the input of the Equation (4). The overall loss function can be formulated as:\n$L = L_{main} + \u03b1 L_{aux}$,\n(13)\nwhere the \u03b1 denotes a hyper-parameter, which is set as 0.2 in this paper. With the training procedure, AdaF2M\u00b2 can comprehensively learn features and apply adaptive feature weights responsively to different user/item states.\nServing stage: The serving stage aims at delivering attractive items for the right users, and the model needs to predict the probability that the user might click/like/share each item. $\u0177_{adapt}$ is directly used as the final prediction in the serving stage. Note that the additional forwards with randomly masked features as input are only utilized in the training stage. Thus, the feature-mask mechanism has no impact on the latency of the serving stage.\nDeployment: The proposed AdaF2M\u00b2 as shown in Figure 1 can be applied upon various existing recommendation models [8, 9, 16, 44], and the overall training and serving procedure is summarized in Algorithm 1. For models of ranking tasks, AdaF2M\u00b2 can be directly deployed as Figure 1. For two-tower models of retrieval tasks, the user tower and the item tower have different corresponding adapters, and the adapter of the user/item tower only takes the user's/item's state embeddings as input."}, {"title": "4 EXPERIMENTS", "content": "In this section, we conduct extensive offline and online experiments with the aim of answering the following evaluation questions:\nEQ1 Can this AdaF2M\u00b2 framework bring improvement to the performance of different online recommendation tasks?\nEQ2 How does the AdaF2M\u00b2 framework perform in public and industrial datasets upon various deep recommendation models?\nEQ3 What are the effects of the feature-mask mechanism and the state-aware adapter in our proposed AdaF2M\u00b2?"}, {"title": "4.1 Experimental Settings", "content": "Datasets. We evaluate AdaF2M\u00b2 with baselines on both public and large-scale industrial recommendation datasets.\nDouyinMusic: Douyin provides a music recommendation service, with over 10 million daily active users. We collect from the impression logs and get two datasets with different sizes, which helps verify the impact of the volume of the dataset. Besides, there is more than a month gap between the two sampled datasets to test the effectiveness of the proposed framework in different periods. The small dataset contains more than 4 billion samples, denoted as DouyinMusic-4B. The large dataset contains more than 20 billion samples, denoted as DouyinMusic-20B. Each sample of the industrial datasets contains more than one hundred features, including both non-ID meta features (gender, age, genre, mood, scene, and so on) and ID-based personalized features (user ID, item ID, artist ID, interacted ID sequence), which can represent the real-world scenarios. We use 'Finish' as the label.\nBaselines. We categorize our baselines into two groups according to their approaches. The first group includes the popular retrieval methods, including:\n\u2022 Factorization Machines(FM) [30]: FM can capture high-order interaction information. In the industry, three-field FM is widely adopted in retrieval tasks. In detail, user features, item features, and context features are pooled into one embedding respectively. Then, FM is utilized on the three embeddings.\n\u2022 YouTube DNN [9]: It is a classical two-tower model, which encodes user features and item features with a deep network respectively. Recently, based on YouTube DNN, many two-tower models [14, 24] are proposed.\nThe second group includes state-of-the-art methods for ranking tasks in recommendation systems, including:\n\u2022 Adaptive Factorization Network(AFN) [8]: It is a recent FM-based [30] method which learns arbitrary-order cross features adaptively from data. The core of AFN is a logarithmic transformation layer that converts the power of each feature in a feature combination into the learnable coefficient.\n\u2022 Deep & Cross Network V2(DCN-V2) [44]: In light of the pros/cons of DCN and existing feature interaction learning approaches, DCN-V2 is an improved framework to make DCN [43] more practical in large-scale industrial settings, which is more expressive yet remains cost efficient at feature interaction learning, especially when coupled with a mixture of low-rank architecture.\n\u2022 EulerNet [40]: It is a recent state-of-the-art method, in which the feature interactions are learned in a complex vector space by conducting space mapping according to Euler's formula. Furthermore, EulerNet incorporates the implicit and explicit feature interactions into a unified architecture.\nThe proposed AdaF2M\u00b2 is a model-agnostic framework that can be applied upon various models, and we apply AdaF2M\u00b2 on the above five base models to demonstrate its effectiveness and universality."}, {"title": "4.2 Online A/B Testing (EQ1)", "content": "To verify the real benefits AdaF2M\u00b2 brings to our system, we conducted online A/B testing experiments for more than two weeks for the ranking, retrieval, and item cold-start tasks in Douyin Music App respectively. Indeed, the proposed AdaF2M\u00b2 has been deployed to the online ranking, retrieval, and item cold-start tasks in multiple applications of Douyin Group.\nOnline Metrics. We evaluate model performance based on two main metrics, Active Days and Duration, which are widely adopted in practical recommendation systems. The total days that users in the experimental bucket open the application are denoted as Active Days. The total amount of time spent by users in the experimental bucket on staying in the application is denoted as Duration. We also take additional metrics, which evaluate user engagement, including Like/Dislike (clicking the like/dislike button on the screen), Finish (hearing the end of a song), and Comment (leaving comments on a song), which are usually used as constraint metrics. We calculate all online metrics per user.\nRanking Tasks. We apply the proposed AdaF2M\u00b2 on a DCN-V2-based multi-task model which is deployed in the online ranking tasks and conduct an online A/B testing to demonstrate the effectiveness of AdaF2M\u00b2 to improve ranking models. The online A/B results of new users (the registration time is less than X days from now, and X is a predefined threshold), old users (the registration time is greater than X days), and whole users are shown in Table 1. In addition, the old users are further divided into low-, middle-, and high-active users according to the active days in the recent 30 days, and the results are shown in Table 2. For the main metrics Active Days and Duration, the proposed AdaF2M\u00b2 achieves a large improvement of +0.212% and +0.442% for all users with statistical significance, which is remarkable given the fact that the average Active Days and Duration improvement from production algorithms is around 0.05% and 0.1%, respectively. In addition, the results demonstrate that AdaF2M\u00b2 could improve the recommendation performance for users of different states.\nRetrieval Tasks. We deploy AdaF2M\u00b2 on two online retrieval models, including an FM-based model and a two-tower model that is similar to YouTube DNN [9]. The online A/B results of the retrieval tasks are shown in Table 3. From the results, we find the proposed AdaF2M\u00b2 framework achieves a significant improvement on Active Days and Duration, against the two base retrieval models. Especially, an upgrade of a retrieval model can increase about 0.07% Active Days, which is a very significant improvement. In addition, with different base models, AdaF2M\u00b2 is effective in improving the recommendation performance, which demonstrates that AdaF2M\u00b2 has satisfying universality.\nItem Cold-start Tasks. Practical recommendation systems create a strong feedback loop [4] resulting in \"rich gets richer\" effect [42]. Cold-start items face a significant barrier to being picked up by the systems and shown to the right users due to lack of initial exposure and interaction [42], while the hot items can be naturally recommended well. Thus, to demonstrate the effectiveness of AdaF2M\u00b2 for items, we apply AdaF2M\u00b2 on a two-tower cold-start model and conduct online A/B testing. We divide items into three groups according to the number of impressions, denoted as 0-128, 128-512, and 512-1024, and the results are shown in Table 4. From the results, we find that AdaF2M\u00b2 can improve the number of impressions, items, Like, and Finish for cold-start items, which demonstrates AdaF2M\u00b2 is able to identify more quality cold-start items that can attract user interactions."}, {"title": "4.3 Offline Experiments (EQ2)", "content": "In this section, we conduct extended offline experiments with one public dataset and two industrial datasets. Specifically, we apply the proposed AdaF2M\u00b2 framework upon five base models to testify the effectiveness and universality.\nExperimental Details. For all methods, the initial learning rate for the Adam [19] optimizer are tuned by grid searches within"}, {"title": "4.4 Analysis (EQ3)", "content": "In this section, we analyze the effects of the adapter and the feature-mask mechanism in our proposed AdaF2M\u00b2. First, we conduct an ablation study to verify the effectiveness of each module. Second, we analyze the important features of users/items of different states.\nAblation Study. To test the effectiveness of each module in AdaF2M\u00b2, we present an ablation study. We introduce three kinds of models, w/ Feature Mask, w/ Adapter, and w/ AdaF2M\u00b2, which add the feature-mask mechanism, the adapter, and the overall AdaF2M\u00b2 based on a two-tower model, respectively. The online and offline results are shown in Table 7. The results show that the base model with AdaF2M\u00b2 achieves better performance than the model only with Feature Mask or Adapter, which demonstrates each module is effective. In addition, we find that the model with the adapter can improve the offline performance, but there is no improvement in the online results. The main reason could be the gap between the offline and online metrics. The offline metrics (AUC and UAUC) are dominated by the performance of the head users, while the online metrics average scores with the same weights for all users.\nIn addition, to demonstrate the proposed AdaF2M\u00b2 is better than existing mask methods with self-supervised learning (SSL), we conduct another online A/B testing that replaces the AdaF2M\u00b2 with MaskNet [49]. The results show that AdaF2M\u00b2 is better than MaskNet with the self-supervised loss in recommendation systems.\nThe Effectiveness of Adapter. To demonstrate the effectiveness of the adapter, we visualize the importance of several typical features (the limitation of pages) in Figure 2. We find that {genre_interest, city, gender, age} are important for new users, while {like sequence, finish sequence, search sequence, user ID} are important for old users. Besides, the predictive features for cold items and hot items are {multi-modal features, genres, mood, theme} and {item ID, artist ID, album ID, similar items' ID} respectively. The results of the analysis are consistent with practical experience, which demonstrates that AdaF2M\u00b2 could capture different distributions of feature importance for different user/item states."}, {"title": "5 CONCLUSION", "content": "In this paper, for better feature representation learning and leveraging in recommendation systems, we propose Adaptive Feature Modeling with Feature Mask (AdaF2M\u00b2), a novel framework consisting of feature-mask mechanism and a state-aware adapter. AdaF2M\u00b2 can comprehensively learn features and adaptively leverage features responsive to different user/item states. We demonstrated the superior performance of the proposed AdaF2M\u00b2 in offline experiments. In addition, we conducted online A/B testing in ranking, retrieval, and item cold-start tasks on multiple recommendation scenarios, obtaining +1.37% and +1.89% cumulative improvements on user active days and app duration respectively, which demonstrates the effectiveness and universality of AdaF2M\u00b2 in online systems. Moreover, AdaF2M\u00b2 has been deployed on both ranking and retrieval tasks in multiple applications of Douyin Group."}]}