{"title": "Test-Time Augmentation Meets Variational Bayes", "authors": ["Masanari Kimura", "Howard Bondell"], "abstract": "Data augmentation is known to contribute significantly to the robustness of machine learning models. In most instances, data augmentation is utilized during the training phase. Test-Time Augmentation (TTA) is a technique that instead leverages these data augmentations during the testing phase to achieve robust predictions. More precisely, TTA averages the predictions of multiple data augmentations of an instance to produce a final prediction. Although the effectiveness of TTA has been empirically reported, it can be expected that the predictive performance achieved will depend on the set of data augmentation methods used during testing. In particular, the data augmentation methods applied should make different contributions to performance. That is, it is anticipated that there may be differing degrees of contribution in the set of data augmentation methods used for TTA, and these could have a negative impact on prediction performance. In this study, we consider a weighted version of the TTA based on the contribution of each data augmentation. Some variants of TTA can be regarded as considering the problem of determining the appropriate weighting. We demonstrate that the determination of the coefficients of this weighted TTA can be formalized in a variational Bayesian framework. We also show that optimizing the weights to maximize the marginal log-likelihood suppresses candidates of unwanted data augmentations at the test phase.", "sections": [{"title": "1 Introduction", "content": "Machine learning has been used successfully in many fields, including computer vision (Guo et al., 2016; Voulodimos et al., 2018; Mahadevkar et al., 2022), natural language processing (Powers and Turk, 2012; Alshemali and Kalita, 2020; Goldberg, 2022), and signal processing (Hu and Hwang, 2002; Fran\u00e7a et al., 2021). It is widely accepted that the performance of such machine learning algorithms depends mainly on the training data. That is, if a large amount of high-quality training data is obtained, it is expected that a good model can be obtained. Indeed, in the field of natural language processing, the scaling law is known that as the number of model parameters, the size of the dataset, and the computational resources used for training increase, the loss decreases according to a power law (Kaplan et al., 2020). However, in many real-world problem settings, there is no guarantee that the quality and quantity of training data are sufficient. For example, it is often impossible to collect enough training data for tasks with high data observation costs. As another example, crowdsourcing for annotation to create teacher labels may result in noisy training data due to variations in the quality of the workers. These examples suggest that the quantity and quality of the training dataset may be insufficient in real-world problem settings.\nTherefore, in practical applications, it is necessary to apply some regularization techniques in the training procedure. One of the most commonly used approaches is data augmentation. Data augmentation is a framework in which some transformation of a given training instance is included in the new training data. The usefulness of data augmentation has been reported for various tasks such as classification (Wong et al., 2016; Perez and Wang, 2017; Miko\u0142ajczyk and Grochowski, 2018), segmentation (Zhao et al., 2019; Sandfort et al., 2019; Ghiasi et al., 2021), image generation (Dong et al., 2017; Tran et al., 2021) and anomaly detection (Lim et al., 2018; Kimura and Yanagihara, 2019; Castellini et al., 2023). Examples of simple data augmentation are injecting Gaussian noise into training instances or affine transformations. More recent studies have also shown that the synthesis of multiple training instances is effective (Zhang et al., 2017a; Yun et al., 2019; Chen et al., 2022; Liu et al., 2022). These techniques are useful as a practical solution when inadequate training data are given.\nThese data augmentation techniques are typically applied during the training phase. Indeed, many neural network training strategies based on stochastic gradient descent add augmented instances into the training mini-batches. However, it has recently been reported that using these techniques in the testing phase can contribute to improving the predictive performance of the model. This framework is called Test-Time-Augmentation (TTA), and its usefulness in various tasks has been shown (Wang et al., 2019b; Moshkov et al., 2020; Kimura, 2021b; Kandel and Castelli, 2021; Cohen et al., 2024). The central idea of TTA is to make predictions with a trained model on a set of input instances at test time to which multiple data augmentations have been applied and to make the aggregation of these predictions as the final prediction. This aggregation is often performed on the outputs of the model in the case of regression problems, or on the softmax outputs of the model in the case of classification problems. In addition, various variants of TTA have been proposed due to the simplicity of the idea and its extensibility (Lyzhov et al., 2020; Mocerino et al., 2021; Tomar et al., 2022, 2023; Xiong et al., 2023). There are several studies investigating when TTA is effective (Kimura, 2021b; Zhang et al., 2022; Conde et al., 2023). In particular, this study focuses on the effectiveness of TTA in cases where the training data is noisy. In such a setup, a given training instance has multiple labels that are not consistent. Considering again the example of crowdsourcing annotations to data, such noisy cases occur frequently due to differences in the skill level of the annotators and the ambiguity of the target instance. Such noisy training data is often due to annotation quality or instance ambiguity. If the training instances and labels are not determined one-to-one, it is anticipated that the predictions of the model are negatively affected. We can conjecture that TTA can be regarded as correcting these predictions at test time by performing aggregation of multiple outputs.\nIn this study, we consider formalizing the TTA procedure in noisy training environments using the variational Bayesian framework (Fox and Roberts, 2012; Corduneanu and Bishop, 2001; Ghahramani and Beal, 2000). We assume that the instances transformed by each data augmentation can be viewed as a perturbation of the test instance drawn from some probability distribution. The TTA procedure can then be regarded as sampling from a mixture model of these distributions (see Figure 1). By linking TTA and variational inference"}, {"title": "2 Background and Preliminary", "content": "In this work, we consider the supervised learning problem. Let $\\mathcal{X} \\subset \\mathbb{R}^d$ be a d-dimensional input space, and let $y$ be an output space. The goal of supervised learning is to obtain a good model $f_\\theta : \\mathcal{X} \\rightarrow \\mathcal{Y}$ parameterized by $\\theta \\in \\Theta$, where $\\Theta \\subset \\mathbb{R}^r$ is an r-dimensional parameter space. In the training phase, a training sample $\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^N$ is used to optimize the parameter as\n$\\theta = \\underset{\\Theta \\in \\Theta}{\\arg \\min} \\frac{1}{N} \\sum_{i=1}^N l(f_\\theta(x_i), y_i),$\\nwhere $l : \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow [0, \\infty)$ is a loss function. Since $\\hat{\\theta}$ is a minimizer of the empirical risk, it is a consistent estimator for the expected risk under the i.i.d. assumption and is expected to induce correct predictions for unknown data. This fundamental principle is called Empirical Risk Minimization (ERM) (Vapnik, 1999, 2013; Hastie et al., 2009; James et al., 2013). However, in real-world situations, it is not necessarily true that the model $f_\\theta$ obtained in this way leads to ideal predictions. For example, it is known that under a distribution shift, where the probability distributions followed by the training and test data differ, the estimators obtained by ERM do not satisfy consistency (Shimodaira, 2000; Sugiyama et al., 2007; Quinonero-Candela et al., 2008; Moreno-Torres et al., 2012). Another common case is inconsistent labeling of training data due to variations in annotation quality, sensor failure, and other factors (Patrini et al., 2017; Fr\u00e9nay and Verleysen, 2013).\nIn this study, we focus specifically on inconsistent labeling settings. Figure 2 shows examples of several instances in CIFAR10-N (Wei et al., 2021), one of the datasets for such problem setting. To address these issues, we need to consider some modifications or regularizations of the estimator. One powerful tool for such regularization is data augmentation. In the following, we summarize the data augmentation used in the training phase and the technique called Test-Time Augmentation, which utilizes them in the testing phase."}, {"title": "2.1 Data Augmentation", "content": "Data augmentation is a technique in which instances in the training sample $\\mathcal{D}$ are transformed in some way to generate new training instances (Van Dyk and Meng, 2001; Shorten and Khoshgoftaar, 2019; Perez and Wang, 2017). Numerous studies have reported the usefulness of data augmentation, and various data augmentation methods have been developed. The simplest one is the strategy of generating a new instance $x' = x + \\epsilon$ by adding Gaussian noise $\\epsilon$ to the training instance $x$. Another common method is to apply a random affine transformation to instance x using some $A \\in \\mathbb{R}^{d \\times d}$ and $b \\in \\mathbb{R}^d$ as $x' = Ax + b$. More recent studies have shown the effectiveness of a method called mixup (Zhang et al., 2017a), in which the convex combination $\\tilde{x}_{ij} = (1 - \\lambda) x_i + \\lambda x_j$ of two instances $x_i, x_j \\in \\mathcal{D}$ with $\\lambda \\in [0, 1]$ is used as the new training instance (Liang et al., 2018; Kimura, 2021a; Carratino et al., 2022). Numerous variants have also been proposed due to the simplicity of the idea (Verma et al., 2019; Chou et al., 2020; Guo, 2020; Xu et al., 2020; Kim et al., 2020b). Furthermore, the framework of differentiable automatic data augmentation that performs end-to-end selection and tuning of these data augmentation methods is also attracting attention (Li et al., 2020a,b; Hataya et al., 2020).\nThere are also many studies on when data augmentation is effective. Several studies have reported that data augmentation contributes to improved robustness against out-of-distribution data and adversarial attacks (Rebuffi et al., 2021; Hendrycks et al., 2021; Yao et al., 2022; Volpi et al., 2018). The behavior of data augmentation in a setting with label noise is also studied (Nishi et al., 2021; Jiang et al., 2020)."}, {"title": "2.2 Test-Time Augmentation", "content": "In general, it is assumed that data augmentation techniques are applied during the training phase. In recent years, however, there has been a growing interest in utilizing these data augmentation techniques in the testing phase. This framework is called Test-Time Augmentation (TTA) (Wang et al., 2019a,b; Kimura, 2021b). Let $\\Gamma = \\{\\varphi_k\\}_{k=1}^K$ be a set of data augmentation functions $\\varphi_k : \\mathcal{X} \\rightarrow \\mathcal{X}$. TTA considers the following prediction using $\\Gamma$.\n$\\hat{y} = \\frac{1}{K} \\sum_{k=1}^K f_{\\hat{\\theta}}(\\varphi_k(x)) = \\frac{1}{K} \\sum_{k=1}^K f_{\\hat{\\theta}}(\\varphi_k(x)),$\\nwhere $x \\in \\mathcal{X}$ is an arbitrary input vector given in the test phase, and $\\hat{\\theta}$ is the parameter obtained by Eq. 1. In recent years, several variants of TTA have been proposed. Kim et al. (2020a) propose to learn a meta-model that predicts expected loss by applying $\\varphi_k \\in \\Gamma$ and selecting the best candidate. They report that better performance is achieved by applying the top-K transformations chosen by their method than by randomly applying top-K data augmentations. Shanmugam et al. (2021) also propose to learn a weighting matrix of aggregated predictions that minimizes validation loss. Similar ideas have been proposed by Son and Kang (2023) and Xiong et al. (2023), named Selective Test-Time Augmentation. These variants can be regarded as considering the choice of the coefficients $w_k(x)$ of the following weighted TTA.\n$\\hat{y} = \\frac{1}{K} \\sum_{k=1}^K w_k(x) \\cdot f_{\\hat{\\theta}}(\\varphi_k(x)),$\nwhere $\\sum_{k=1}^K w_k(x) = 1$. Indeed, the Kim et al. (2020a) method of selecting the top-K transformations such that the expected loss is minimized is equivalent to setting $w_k(x) = 0$ on unnecessary candidates. That is, selecting useful candidates for the testing phase from a set of data augmentations is an essential problem.\nTest-Time Augmentation under Noisy Environments If the training sample of sufficient size is perfectly clean and from the identical distribution as the test data, then the prediction by the minimizer of the ERM obtained in Eq. 1 should be optimal. However, because of the noisy nature of the real-world problem setting, TTA is expected to be effective. In particular, we conjecture that TTA is effective in cases where the predictions for the input vector are not uniquely determined in the training data. In this study, we consider the noisy training sample where each $x \\in \\mathcal{D}$ has a set of noisy labels $\\mathcal{S}_x$. We assume that $\\mathcal{S}_x$ includes several inconsistent class labels in the classification problem and $\\mathcal{S}_x$ includes several perturbed responses in the regression problem.\nWhy is Determination of TTA Weight Coefficients Difficult? To achieve good prediction, the coefficients $w_k$ of the weighted TTA need to be determined appropriately. In particular, finding out which data augmentation strategies $\\varphi_k$ are unnecessary in TTA is an important issue. Intuitively, the simplest idea seems to be to eliminate $\\varphi_k$ such that the predictive performance of the composite function $f_{\\hat{\\theta}} \\circ \\varphi_k$ is poor. However, simple numerical experiments can provide a counterexample to this idea (see Table 1). The dataset used in this experiment is the MNIST database (Modified National Institute of Standards and Technology database) (Deng, 2012) and the base model is a simple three-layer convolutional neural network for [A] ERM (Empirical Risk Minimization). Here, the MNIST database is a large collection of handwritten digits, and it has a training set of 60, 000 examples and a test set of 10,000 examples. In this experiment, we only use 50 instances for training of the base model. Here, the training instances are randomly selected for each trial. The experimental results are the means and standard deviations of ten trials with different random seeds. We consider a combination of four data augmentation methods ([B] Rotate 20\u00b0, [C] Rotate -20\u00b0, [D] Gaussian noise, and [E] Mixup (Zhang et al., 2017a)) to evaluate the performance of TTA. In this table, K-TTA denotes a TTA based on K data augmentation strategies. The experiments show that the performance of TTA based on a single data augmentation is relatively good with Gaussian Noise and Mixup, while the two Rotate strategies are poor. However, considering these combinations, it can be seen that the best performance is achieved when Rotate is included (4-TTA (A + B + C + D)), which exceeds the performance of those that exclude Rotate (3-TTA (A + D + E)). This simple experiment shows that the single worst-performing data augmentation method is not always unnecessary in TTA. This result motivates the development of a method for determining the coefficients of the weighted TTA by an appropriate procedure."}, {"title": "3 Test-Time Augmentation as Bayesian Mixture Model", "content": "In this section, we consider the reformulation of the TTA procedure as a Bayesian mixture model. The key idea is to regard the final output obtained by the TTA procedure as being acquired by sampling from a mixture of distributions made by several data augmentation functions. The following discussion is divided according to whether the prediction problem is a continuous or categorical case. Here, we first consider the continuous case (Section. 3.1) as the formulation is most straightforward and can then be extended to discuss the categorical case (Section. 3.2)."}, {"title": "3.1 Continuous Case", "content": "We consider $Y = f_\\theta(X) + \\epsilon$, where $\\epsilon \\sim N(0, \\sigma_\\epsilon)$. We also assume that the transformation $\\xi_{k,x} := \\varphi_k(x)$ of an instance $x$ by a data augmentation $\\varphi_k \\in \\Gamma$ follows a Gaussian distribution as $\\Xi_{k,x} \\sim N(x, \\Sigma_k)$, where $\\Sigma_k \\in \\mathbb{R}^{d \\times d}$. That is, the mapping by a given data augmentation is assumed to be normally distributed around the original instance. The Taylor expansion of $f_{\\theta}(\\xi_{k,x})$ around $x$ yields\n$\\mu_k(x; \\theta) := \\mathbb{E}_{\\xi_{k,x}} [f_{\\theta}(\\xi_{k,x})] \\approx f_{\\theta}(x),$\n$\\Sigma_k(x; \\theta) := V[f_{\\theta}(\\Xi_{k,x})] \\approx \\frac{1}{N} \\nabla f_{\\theta}(\\xi_{k,x})^T Cov(\\xi_{k,x}) \\nabla f_{\\theta}(\\xi_{k,x}) + \\sigma_\\epsilon,$\\n$\\approx \\frac{1}{N} \\nabla f_{\\theta}(\\varphi_k(x)) \\Sigma_k \\nabla f_{\\theta}(\\varphi_k(x)) + \\sigma_\\epsilon.$\nThat is, we can say that\n$\\sqrt{N} \\Big( \\frac{1}{N} \\sum_{j=1}^N f_{\\theta}(\\varphi_k(x)) - f_{\\theta}(x) \\Big) \\xrightarrow{d} N \\Big( 0, \\big[ \\frac{\\partial}{\\partial x} f_{\\theta}(x) \\big]^T \\Sigma_k \\big[ \\frac{\\partial}{\\partial x} f_{\\theta}(x) \\big] + \\sigma_\\epsilon^2 \\Big),$\nwhere the symbol $\\xrightarrow{d}$ stands for the convergence in distribution (or convergence in law). Then, we have $f_{\\theta}(\\varphi_k(x)) \\sim N(\\mu_k(x; \\theta), \\Sigma_k(x; \\theta))$ and\n$P(y \\mid x, w, \\Sigma_k) = \\sum_{k=1}^K w_k \\cdot N(y \\mid \\mu_k(x; \\theta), \\Sigma_k(x; \\theta)),$\n$P(\\mathcal{S}_x \\mid x, w, \\Sigma_k) = \\prod_{j=1}^{S_x} \\sum_{k=1}^K w_k \\cdot N(y_j \\mid \\mu_k(x; \\theta), \\Sigma_k(x; \\theta)),$\nwhere $w = \\{w_k\\}_{k=1}^K$. Now consider a binary random variable $z = \\{z_{jk}\\}$ and set $z_{jk} = 1$ only when a label $y_j$ is generated from the k-th distribution. Using this variable, we can write as\n$P(\\mathcal{S}_x \\mid x, z, \\Sigma_k) = \\prod_{j=1}^{S_x} \\prod_{k=1}^K N(y_j \\mid \\mu_k(x; \\theta), \\Sigma_k(x; \\theta))^{z_{jk}},$\nand,\n$P(z \\mid w) = \\prod_{k=1}^K \\prod_{j=1}^{S_x} w_k^{z_{jk}}.$\nBy marginalizing Eq. 9 with the prior distribution of Eq. 10, the marginal distribution of the observed data of Eq. 8 can be recovered.\nWe consider the following conjugate priors over $\\mu_{\\kappa,\\theta} = \\{\\mu_k(x; \\theta)\\}$ and $\\Sigma_{\\kappa,\\theta} = \\{\\Sigma_k(x; \\theta)\\}$.\n$P(\\mu_{\\kappa,\\theta}) = \\prod_{k=1}^K N(\\mu_k(x; \\theta) \\mid 0, \\beta I),$\n$P(\\Sigma_{\\kappa,\\theta}) = \\prod_{k=1}^K \\mathcal{W}(\\Sigma_k(x; \\theta) \\mid \\nu, V),$\nwhere $\\beta$ is a fixed and large value that corresponds to a broad prior distribution over $\\mu_{\\kappa,\\theta}$, $I$ is the identity matrix, and $\\mathcal{W}$ is the Wishart distribution with $\\nu$ degrees of freedom and a scale matrix $V$ having the following density function\n$p(\\Sigma) = \\frac{|\\Sigma|^{(\\nu-c-1)/2} \\exp \\{-\\frac{1}{2} tr(\\mathcal{V}^{-1} \\Sigma)\\} }{2^{c\\nu/2} \\pi^{c(c-1)/4} |V|^{\\nu/2} \\prod_{i=1}^c \\Gamma(\\frac{\\nu+1-i}{2})},$\nHere, $\\Gamma(z) = \\int_0^\\infty t^{z-1} \\exp(-t) dt$ is the Gamma function. The joint distribution of all of the random variables conditioned on the weighting coefficients is given by\n$P(\\mathcal{S}_x, \\mu_{\\kappa,\\theta}, \\Sigma_{\\kappa,\\theta}, z \\mid w) = P(\\mathcal{S}_x \\mid \\mu_{\\kappa,\\theta}, \\Sigma_{\\kappa,\\theta}, z) P(z \\mid w) P(\\mu_{\\kappa,\\theta}) P(\\Sigma_{\\kappa,\\theta}).$\nEvaluating $P(\\mathcal{S}_x \\mid w)$ requires marginalization of Eq. 14 with respect to $z, \\mu_{\\kappa,\\theta}$, and $\\Sigma_{\\kappa,\\theta}$, which is analytically intractable. Therefore, we utilize the variational method to obtain a tractable lower bound for $P(\\mathcal{S}_x \\mid w)$. Denote $\\eta = \\{z, \\mu_{\\kappa,\\theta}, \\Sigma_{\\kappa,\\theta}\\}$. Then the marginal likelihood we need to evaluate is given by\n$P(\\mathcal{S}_x \\mid w) = \\int P(\\mathcal{S}_x, \\eta \\mid w) d\\eta.$\nIn the variational inference framework, we introduce a distribution $Q(\\eta)$ that provides an approximation of the true posterior distribution. Consider the following transformation of the log marginal likelihood using the distribution $Q(\\eta)$.\n$\\ln P(\\mathcal{S}_x \\mid w) = \\ln \\int \\frac{P(\\mathcal{S}_x, \\eta \\mid w)}{Q(\\eta)} Q(\\eta) d\\eta \\ge \\int Q(\\eta) \\ln \\frac{P(\\mathcal{S}_x, \\eta \\mid w)}{Q(\\eta)} d\\eta = \\mathcal{L}(Q).$\nHere we denote the lower bound by $\\mathcal{L}(Q)$. It is known that a judicious choice of distribution $Q$ makes the quantity $\\mathcal{L}(Q)$ tractable to compute, even if the original log-likelihood function is not. Here, the difference between the true log marginal likelihood $\\ln P(\\mathcal{S}_x \\mid w)$ and the bound $\\mathcal{L}(Q)$ is given by the Kullback-Leibler (KL)-divergence\n$D_{KL}[Q||P] = \\int Q(\\eta) \\ln \\frac{Q(\\eta)}{P(\\eta \\mid \\mathcal{S}_x, w)} d\\eta.$\nThe goal of the variational inference framework is to choose an appropriate form of $Q$ that is simple enough to easily evaluate the lower bound $\\mathcal{L}(Q)$, but flexible enough to make the lower bound reasonably tight. Since the true log-likelihood is independent of $Q$, we see that this procedure is equivalent to minimizing the KL-divergence. Although it is known that $D_{KL}[Q||P] = 0$ when $P = Q$, for efficient approximation we consider restricting the class of $Q$.\nWe consider a family of constrained variational distributions, assuming that $Q(\\eta)$ is factorized over a subset $\\{\\eta_i\\}$ of the variables in $\\eta$ as\n$Q(\\eta) = \\prod_{i=1}^q Q_i(\\eta_i).$\nHere, the KL-divergence can be minimized over all possible factorial distributions, and the solutions are given as\n$Q_i(\\eta_i) = \\frac{\\exp\\{ \\langle \\ln P(\\mathcal{S}_x, \\eta) \\rangle_{k \\neq i} \\}}{\\int \\exp\\{ \\langle \\ln P(\\mathcal{S}_x, \\eta) \\rangle_{k \\neq i} \\} d\\eta_i}.$\nwhere $\\langle \\cdot \\rangle_{k \\neq i}$ is an expectation with respect to $Q_i(\\eta_i)$ for all $k \\neq i$. Then, the variational posterior distributions we are interested in are\n$Q(\\eta) = Q(z, \\mu_{\\kappa,\\theta}, \\Sigma_{\\kappa,\\theta}) = Q_z(z) Q_{\\mu_{\\kappa,\\theta}}(\\mu_{\\kappa,\\theta}) Q_{\\Sigma_{\\kappa,\\theta}}(\\Sigma_{\\kappa,\\theta}),$\nand the solutions for the factors of the variational posterior are\n$Q_z(z) = \\prod_{i=1}^{NS_x} \\prod_{j=1}^{S_x} \\prod_{k=1}^K P_{ijk}^{z_{jk}},$\n$Q_{\\mu_{\\kappa,\\theta}}(\\mu_{\\kappa,\\theta}) = \\prod_{k=1}^K N(\\mu_k(x; \\theta) \\mid m_k(\\mu_{\\kappa,\\theta}), G_k(\\mu_{\\kappa,\\theta})),$\n$Q_{\\Sigma_{\\kappa,\\theta}}(\\Sigma_{\\kappa,\\theta}) = \\prod_{k=1}^K \\mathcal{W}(\\Sigma_k(x; \\theta) \\mid \\nu_k(\\Sigma_{\\kappa,\\theta}), V_k(\\Sigma_{\\kappa,\\theta})),$\nwhere\n$p_{jk} = \\frac{\\tilde{p}_{jk}}{\\sum_{k=1}^K \\tilde{p}_{jk}},$\n$\\tilde{p}_{jk} = \\exp \\Big{ \\mathbb{E}[\\ln N(y_j \\mid \\mu_k(x; \\theta))] + \\ln w_k - \\frac{1}{2} tr \\Big( \\mathbb{E}[\\Sigma_k(x; \\theta)] (y_j - \\mathbb{E}[\\mu_k(x; \\theta)])^2 \\Big) \\Big\\},$\n$m_k(\\mu_{\\kappa,\\theta}) = G_k^{-1}(\\mu_{\\kappa,\\theta}) \\mathbb{E}[\\Sigma_k(x; \\theta)] \\sum_{j=1}^{S_x} y_j \\mathbb{E}[z_{jk}],$\n$G_k(\\mu_{\\kappa,\\theta}) = \\beta I + \\mathbb{E}[\\Sigma_k(x; \\theta)] \\sum_{j=1}^{S_x} \\mathbb{E}[z_{jk}],$\n$\\nu_k(\\Sigma_{\\kappa,\\theta}) = \\nu + \\sum_{j=1}^{S_x} \\mathbb{E}[z_{jk}],$\n$V_k(\\Sigma_{\\kappa,\\theta}) = V + \\sum_{j=1}^{S_x} y_j^2 \\mathbb{E}[z_{jk}] - \\mathbb{E}[\\mu_k(x; \\theta)] \\sum_{j=1}^{S_x} y_j \\mathbb{E}[z_{jk}] \\Big( \\sum_{j=1}^{S_x} y_j \\mathbb{E}[z_{jk}] \\Big) \\mathbb{E}[\\mu_k(x; \\theta)] + \\mathbb{E}[\\mu_k(x; \\theta)^2] \\sum_{j=1}^{S_x} \\mathbb{E}[z_{jk}].$\nWe can evaluate the variational lower bound in Eq. 16 as\n$\\mathcal{L}(Q) = \\int Q(\\eta) \\ln \\frac{P(\\mathcal{S}_x, \\eta \\mid w)}{Q(\\eta)} d\\eta = \\int Q_z(z) Q_{\\mu_{\\kappa,\\theta}}(\\mu_{\\kappa,\\theta}) Q_{\\Sigma_{\\kappa,\\theta}}(\\Sigma_{\\kappa,\\theta}) \\times \\ln \\frac{P(\\mathcal{S}_x \\mid z, \\mu_{\\kappa,\\theta}, \\Sigma_{\\kappa,\\theta}) P(z) P(\\mu_{\\kappa,\\theta}) P(\\Sigma_{\\kappa,\\theta})}{Q_z(z) Q_{\\mu_{\\kappa,\\theta}}(\\mu_{\\kappa,\\theta}) Q_{\\Sigma_{\\kappa,\\theta}}(\\Sigma_{\\kappa,\\theta})} dz d\\mu_{\\kappa,\\theta} d\\Sigma_{\\kappa,\\theta} \\approx J_{\\mathcal{S}_x \\mid \\eta} + J_z + J_{\\mu_{\\kappa,\\theta}} + J_{\\Sigma_{\\kappa,\\theta}} - \\Pi_z - \\Pi_{\\mu_{\\kappa,\\theta}} - \\Pi_{\\Sigma(\\kappa,\\theta)},$\nwhere\n$J_{\\mathcal{S}_x \\mid \\eta} := \\mathbb{E}_Q [\\ln P(\\mathcal{S}_x \\mid \\eta)] = \\sum_{i=1}^{N} \\sum_{k=1}^K \\sum_{j=1}^{S_x} p_{jk} \\mathbb{E}[\\ln N(x; \\theta)] - \\frac{1}{2} \\ln (2\\pi) - \\frac{1}{2} tr \\Big( \\mathbb{E}[\\Sigma_k(x; \\theta)] (y_j - \\mathbb{E}[\\mu_k(x; \\theta)])^2 \\Big),$\n$J_z := \\mathbb{E}_{Q_z} [\\ln P(z)] = \\sum_{i=1}^{N} \\sum_{k=1}^K \\sum_{j=1}^{S_x} p_{jk} \\ln w_k,$\n$J_{\\mu_{\\kappa,\\theta}} := \\mathbb{E}_{Q_{\\mu_{\\kappa,\\theta}}} [\\ln P(\\mu_{\\kappa,\\theta})] = K \\cdot \\ln \\Big( \\frac{\\beta}{2\\pi} \\Big) - \\frac{\\beta}{2} \\sum_{k=1}^K (G_k(\\mu_{\\kappa,\\theta})^{-1} + m_k(\\mu_{\\kappa,\\theta}) m_k(\\mu_{\\kappa,\\theta})^T) \\mathbb{E}_{(\\kappa,\\theta)},$\n$J_{\\Sigma_{\\kappa,\\theta}} := \\mathbb{E}_{Q_{\\Sigma_{\\kappa,\\theta}}} [\\ln P(\\Sigma_{\\kappa,\\theta})] = K \\Big(\\frac{\\nu c}{2} \\ln 2 + \\frac{c(c-1)}{4} \\ln \\pi + \\frac{1}{2} \\sum_{i=1}^c \\ln \\Gamma \\Big( \\frac{\\nu + 1 - i}{2} \\Big) + \\ln |V| \\Big) - \\frac{K | \\mathcal{V} |}{2} + \\sum_{k=1}^K \\mathbb{E}[\\ln \\Sigma_k(x; \\theta)] - tr V \\sum_{k=1}^K \\frac{1}{2} \\Big( \\mathbb{V}_k(\\Sigma_{\\kappa,\\theta}) V_k^{-1}(\\Sigma_{\\kappa,\\theta}) \\Big),$\n$\\Pi_z := \\mathbb{E}_{Q_z} [\\ln Q_z(z)] = \\sum_{i=1}^{N} \\sum_{k=1}^K \\sum_{j=1}^{S_x} p_{jk} \\ln p_{jk},$\n$\\Pi_{\\mu_{\\kappa,\\theta}} := \\mathbb{E}_{Q_{\\mu_{\\kappa,\\theta}}} [\\ln Q_{\\mu_{\\kappa,\\theta}}(\\mu_{\\kappa,\\theta})] = \\frac{K}{2} \\Big(\\frac{c}{c} + \\ln (2\\pi) + \\frac{1}{2} tr (G_k(\\mu_{\\kappa,\\theta}) \\Big),$\n$\\Pi_{\\Sigma_{\\kappa,\\theta}} := \\mathbb{E}_{Q_{\\Sigma_{\\kappa,\\theta}}} [\\ln Q_{\\Sigma_{\\kappa,\\theta}}(\\Sigma_{\\kappa,\\theta})] = \\sum_{k=1}^K \\Big(\\frac{\\mu c}{2} \\ln 2 + \\frac{c(c-1)}{4} \\ln \\pi \\Big) + \\frac{\\nu_k(\\Sigma_{\\kappa,\\theta})}{2} + \\sum_{i=1}^c \\ln \\Gamma \\Big( \\frac{\\nu_k(\\Sigma_{\\kappa,\\theta}) + 1 - i}{2} \\Big) - \\frac{\\nu_k(\\Sigma_{\\kappa,\\theta})}{2} + \\ln |V_k(\\Sigma_{\\kappa,\\theta})| \\Big).$\nOur aim is to maximize this variational lower bound with respect to w to obtain an optimal choice of weighting coefficients. To do so, we utilize an EM procedure with i) M-step: maximization of $\\mathcal{L}(Q)$ with respect to w, and ii) E-step: update $Q_z, Q_{\\mu_{\\kappa,\\theta}}, Q_{\\Sigma_{\\kappa,\\theta}}$. We call our framework Variational Bayesian Test-Time Augmentation (VB-TTA)."}, {}]}