{"title": "Harnessing Generative Pre-Trained Transformer for Datacenter Packet Trace Generation", "authors": ["Chen Griner"], "abstract": "Today, the rapid growth of applications reliant on datacenters calls for new advancements to meet the increasing traffic and computational demands. Traffic traces from datacenters are essential for further development and optimization of future datacenters. However, traces are rarely released to the public. Researchers often use simplified mathematical models that lack the depth needed to recreate intricate traffic patterns and, thus, miss optimization opportunities found in realistic traffic. In this preliminary work, we introduce DTG-GPT, a packet-level Datacenter Traffic Generator (DTG), based on the generative pre-trained transformer (GPT) architecture used by many state-of-the-art large language models. We train our model on a small set of available traffic traces from different domains and offer a simple methodology to evaluate the fidelity of the generated traces to their original counterparts. We show that DTG-GPT can synthesize novel traces that mimic the spatiotemporal patterns found in real traffic traces. We further demonstrate that DTG-GPT can generate traces for networks of different scales while maintaining fidelity. Our findings indicate the potential that, in the future, similar models to DTG-GPT will allow datacenter operators to release traffic information to the research community via trained GPT models.", "sections": [{"title": "I. INTRODUCTION", "content": "Datacenter networks (DCNs) constitute a critical component of the infrastructure supporting the contemporary digital world. Large-scale web applications, cloud computing platforms, and, in particular, the rapidly growing deployment of generative Al models all require increasing efficiency and computing power. Thus, in the coming years, an explosive increase in the demand for datacenters is expected [1], [2]. Novel technologies and developments will likely be needed for the next evolution in DCNs. While there have been many recent developments from researchers in datacenter networking [3], an important challenge to progress is the lack of realistic datacenter network traces that would be used in the development of these technologies [4], [5], [6], [7]. Traditional DCNs are oblivious to traffic and have a static topology; their design often assumes a uniform distribution of traffic [4], [8]. Some novel technologies for DCNs take a different approach [5], [9], [10]. These networks, called demand-aware networks, can change their topology and adjust to a changing traffic pattern, thus increasing efficiency. The lack of realistic traffic traces hinders progress for novel demand-aware networks, since network designers remain oblivious to the actual traffic patterns in modern datacenters. However, these tracks are usually available only to network operators and not to researchers [5]. The lack of traces, except for a few examples, is often caused by privacy concerns. Indeed, even simple anonymization of the trace may not be enough to release the trace [11]. Lastly, even if traces were available, replaying them in a simulation would likely only benefit simulated networks of an identical topology [5].\nThe networking community has devised many synthetic traffic generators to address these issues and others. However, it is not clear to what extent they mimic realistic traffic patterns, nor is there a good framework to decide which traces are a good emulation of realistic network traffic [5]. Indeed, it is likely that different types of DCNs running different tasks would each have different traffic patterns [8].\nRecently, several works have appeared that use different generative AI methods to generate traces. For example, [12], [13], [14], [15] use GANs (based on convolutional recurrent networks, convolutional long short-term memory) to generate packet traces. A few papers have emerged that use a generative pre-trained transformer (GPT) based architecture for traffic generation. In [16], the authors utilize the GPT architecture to generated traces of DNS and cryptocurrency traffic. In [17], the authors attempt to reconstruct simple packet traces using a pre-trained GPT-3 model, which they fine-tune to generate packet traces containing ICMP and DNS packets. In [6], the authors use a GPT model to generate traces that mimic control-plane traffic of 4G and 5G cellular networks. However, we are unaware of any work dedicated to generating and evaluating realistic DCN traces using the GPT architecture. In this work, we present DTG-GPT, a novel DCN trace generation model based on the GPT architecture used by large language models such as OpenAI Chat-GPT and others [18]. In this initial work, we will focus on a small but crucial part of the DCN packet trace: the source and destination IP columns. In this manner, we follow an example set in previous works looking at DCN traces for DCN development [8], [19], [20].\nGenerating complex, varied, and realistic DCN traces is not a straightforward task. Many variables exist, such as the network size, the datacenter's task, the protocols used, etc. However, as a first step, we would like to mimic existing traces. That is, given a network trace (which stands for a traffic pattern), we would like our model, DTG-GPT, to generate more traces, which are similar to the original trace in a fundamental way but not identical, essentially making more of the same trace. To judge the fidelity of the novel traffic traces generated with DTG-GPT, we would like to compare them to the original. However, comparing two traces is not very straightforward; there could be many ways to measure and compare traffic [5]. We begin with a simple approach: we look"}, {"title": "II. PRELIMINARIES", "content": "This section briefly discusses our traces and network model and provides background on trace complexity, which we use in the technical sections.\nNetwork Typically, a network is defined via its topology as a set of vertices and nodes that form a graph. However, in the context of this paper, we are agnostic to the network's structure in terms of its particular set of edges. We are only required to know its set of nodes. The scale of a network is the number of nodes that comprise the network, denoted by n. The ID of each node is in the range [0, n \u2013 1]. We will also assume that any pair of nodes in the network may communicate.\nA Network Trace A trace o is a sequence of requests \u03c3\u03b5 = {01, 02,..., 5|6|}, of length |0|, where each request represent a single packet of traffic. In this work, each request in the trace contains the source and destination node IDs \u03c3i = (si, di), Si,\nA. Traffic & Trace models"}, {"title": "B. Trace Complexity", "content": "Part of the toolbox we will use to judge the fidelity of our generated traces is trace complexity [8]. This approach can identify and quantify the types of structure featured by packet traces in communication networks, particularly data centers. Trace complexity, approximates information theory's measure of entropy rate [22] for traces\u00b9. By estimating the entropy rate, it can quantify the amount of temporal and non-temporal structure in traffic traces.\nBriefly, trace complexity gives an information-theoretic measure on the structure, or conversely, randomness, found in a traffic trace. Intuitively, a packet trace with low entropy has low complexity: it contains little information, and the sequence behavior is more predictable [24]; hence, it has high structure. Conversely, a more random trace will have high complexity and low structure. Importantly, it is possible to measure different aspects of a trace's complexity by randomizing different aspects of the trace while keeping the rest intact.\nFor the purpose of this paper, we explore two types of trace complexity: temporal complexity and non-temporal com-plexity. Concisely, temporal complexity measures the ran-domness represented by the ordering of the packets (e.g., bursts, repetitions, cycles of similar elements, etc.). Non-temporal complexity estimates the randomness represented by the frequency requests or nodes.\nFor a full discussion of trace complexity, we refer the reader to the original paper [8]\nWe believe that trace complexity offers a simple tool to estimate the fidelity of our generated trace. That is, if the generated traces are similar to their original counterparts, they should have similar values in terms of both temporal and non-temporal complexities. In particular, temporal complexity measures an aspect of a sequence that is otherwise difficult to quantify. Furthermore, since trace complexity is a normalized measure, it can compare traces of different lengths and node counts."}, {"title": "III. DTG-GPT ARCHITECTURE", "content": "In this section, we will discuss the architecture of our model. Briefly, our model is based on the GPT architecture, with the main changes being for the input embeddings. Here, the input to the\n\u00b9Since the term entropy is defined for random variables, as opposed to a sequence of individual communication requests in a packet trace, the term complexity is used similarly to earlier work on random sequences, such as in this work by Lempel and Ziv [23], where the complexity of a random sequence is linked to its similarity to a random sequence."}, {"title": "IV. TRAINING AND GENERATION SETUP", "content": "In this section, we discuss the different aspects of the training and the trace generation setup for our model. We begin by reviewing the dataset.\nWe use two different trace sets. The first is from a Meta (at the time, Facebook) datacenter, and the second is an HPC cluster. We will denote these two subsets as Facebook and HPC.\nThe HPC trace set is composed out of Four traces of exascale applications i.e., computing applications that require at least 10\u00b9\u2078 double precision operations per second) in high-performance computing (HPC) clusters [21]: CNS, MultiGrid, MOCFE and NeckBone. These represent several different computational kernels of different applications and show the communication pattern between 1024 CPUs. The Facebook dataset consists of three traces collected from three datacenter clusters, each corresponding to a distinct application type: Hadoop (HAD), a Hadoop cluster; web (WEB), comprising servers that handle web traffic; and databases (DB), which include MySQL servers responsible for storing user data and processing SQL queries. For the purpose of this research, all traces were clipped to be at most 20M entries long.\nThe Facebook traces were also filtered. The original Face-book traces set were longer and more verbose, up to 300k packets and several different fields, as described in previous works [29], [8]. Only the source and destination rack columns were extracted and tested for this work. Furthermore, the original trace had an uneven set of source and destination rack ID entries (as the trace was captured only on a single part of the DC). The traces were filtered so that only nodes in the source column could appear in the destination column\nA. Data"}, {"title": "B. Fine-Tuning", "content": "In this section, we briefly discuss our approach to fine-tuning.\nThe training process of LLMs using the GPT architecture typically involves at least two phases. In the first phase, called pre-training, the model is exposed to vast amounts of textual data. This is usually the most cost-intensive part of training. In the second phase, often called the fine-tuning phase, the model can be adjusted or trained on a smaller, task-specific data set to increase its performance for a specific task [27], [25], [28]. We can draw an analogy between our model, DTG-GPT, and typical LLMs, where traces represent textual data. Since this paper deals with a novel application of the GPT architecture, in this analogy, the main focus of our effort will be the pre-training phase.\nWe take a simple approach for fine-tuning and test the effect of different temperatures on the model's performance.\nIn the context of large language models, the temperature parameter, t, sometimes also called the creativity parameter [31], [32], controls the randomness of the model's output by scaling the probabilities of the next predicted token when generating traces (or text).\nMore formally, we follow the definition given in [32], [33]. For a given model output logits vector z \u2208 Rn, we apply the softmax function (Equation 2) with the temperature parameter t.\n$SOFTMAX(z) = \\frac{exp(\\frac{z_i}{t})}{\\sum_{j=1}^{n} exp(\\frac{z_j}{t})}$    (2)\nWhere n is the vocabulary size of our model (Equivalent to the number of nodes in the network for which the trace is generated), and SOFTMAX(z)\u2081 is the probability for token i. The probability distribution for all tokens will increase skewness for lower temperatures t < 1 or be less skewed for higher temperatures t > 1. Setting t = 1 gives the model's default behavior; we will denote this as the default temperature. Typically, the temperature is a real number in the range t \u2208 [0, 2].\nIt is difficult to predict exactly the effects of different tem-peratures on the model's output, and the optimal temperature depends on the desired output characteristics. In general, we expect that a lower temperature leads to a model that generates fewer novel sequences, while a higher temperature model generates more. We, therefore, see this as part of the model's fine-tuning process. In this paper, we will explore different temperatures when generating novel traces and see the effect of different temperatures on the generated traces for different models."}, {"title": "C. Tokenization", "content": "Tokenization is essential to LLMs. LLMs use tokenization to break down natural human language text into 'tokens', which can then be fed into the model. The tokenization scheme the model utilizes can be on the character, the sub-word or word level, or a different combination of these word structures. In each scheme, A token is assigned a number, and the entire text is transformed into a list of tokens. Tokenization is used to increase the performance of LLMs by reducing the computa-tional load on the model and improving accuracy by indirectly increasing the context length. The number of possible tokens found in the input sequence is a hyperparameter of the model, and increasing the number of possible tokens will increase the model's complexity by increasing the vocabulary size; thus, this number must be chosen carefully. As we have mentioned, in our framework, network traces are formed from a list of the source and destination nodes of the network. Typically, these would be IP addresses. For this preliminary work, we used a straightforward tokenization scheme for our traces. Each ID (source or destination IP, etc.) in the original trace is assigned a number. For the Facebook rack traces, each rack ID in the original trace was reassigned a number to ensure all tokens are consecutive (i.e., from 0 to the n - 1 where n is the number of source nodes in Table I) The HPC traces were used almost unchanged (from the source in [30]), with the original ID values reduced by one, that is, from the range [1,1024] they were moved to the range [0,1023]. We considered a naive approach where all requests (i.e., source and destination ID) are bundled into a single token. However, this would square the vocabulary size (from n to n\u00b2) and only double the 'context length.' Furthermore, not all pairs would even be used, and thus, this is very wasteful. It is possible to consider different tokenization schemes for network traces, such as bundling longer sequences of IDs in the trace into a single token if they are frequent (much like a several-character word is bundled into a single token), perhaps using byte-pair encoding for tokenization. However, while natural language text has many properties common across all sources, this is likely not true for traffic traces, particularly those of different source networks. Thus, a useful token for one trace might be completely irrelevant for another. Finally, we note that a deeper discussion of tokenization is outside the scope of this paper and will require further research into datacenter traffic traces."}, {"title": "D. Training Setup", "content": "This preliminary work only evaluates a single model with a single set of hyperparameters used for training. We use a similar set of hyperparameters as the smallest GPT-2 model [25]. We used an embedding dimension of dmodel = 768. The number of linear layers L and the number of self-attention heads H were set to L, H = 12. Furthermore, the vocabulary size was set as the most significant number of IDs found in the traces, 1024. Our context length was set to 512, and the batch size was set to 216. Regarding the meta-embedding parameters, we set the number of segments to s = 24, and the largest traces ID is tr = 7. The ID of each trace"}, {"title": "E. Trace Generation", "content": "Let us briefly discuss the parameters we used for trace generation. As our result trace set, for each trace from our data set (in Table I), we (at least) partially recreate all traces using the following set of parameters.\nWe generated Each HPC trace to its original length, or at most 8M requests. While for Facebook, we generated traces that are 2M requests long. The actual generated trace lengths are listed in Table I. Each generated trace has five versions, one for each temperature value from the set {0.9, 0.95, 1, 1.1, 1.2}.\nTo produce the meta-data embedding, we use the correspond-ing token ID of each trace, and for the segment tokens, we assume that the length of each segment is the original segment length of the trace. We note that the Facebook traces are from a network with a smaller scale; that is, it has a smaller set of IDs. We, therefore, mask the output probability distribution to ensure that no values above the largest ID are generated for those traces. Throughout this paper, we note that, unless otherwise specified, every generated trace was created with the default temperature (i.e., t = 1). When presenting results and comparing traces, all original traces were truncated to be of the same length as their generated counterpart presented in the result."}, {"title": "V. EVALUATION", "content": "In this section, we evaluate the fidelity of the traces gener-ated from our DTG-GPT model to the original set of traces described in Section IV-A. On every measurement we take on a trace throughout this section, our goal will be to find if traces generated by DTG-GPT recreate the patterns found in the original traces. We are not seeking an exact match for every measure. Our goal is to generate traces that are similar but different.\nWe will judge the fidelity of generated traces using four measures. We first look into the traffic matrices (as shortly presented in Section I). We take an information-theoretic approach and analyze the trace complexity of our traces, as discussed in Section II-B. We then explore the burstiness of the generated trace. Finally, we look into the novelty of the generated traces by looking into the ratio of similar n-grams of each trace. In the appendix, in Section A, we present a partial result; we test our model's ability to generate a trace representing the traffic for a network with a smaller set of nodes than found in the trace, which it is trying to mimic."}, {"title": "A. Traffic Matrices", "content": "Traffic traces can have temporal or non-temporal (spatial) aspects [8]. In this section, we examine the spatial aspect by looking at traffic matrices generated from our generated model.\nFigure 3 (a)- (g) presents the traffic matrices for our dataset, that is, the original traces, while 3 (h)- (n) represents the gen-erated counterparts. Each matrix represents an accumulation of all requests in a trace and reveals their spatial structure, as more frequent requests appear as brighter colors. Furthermore, all cells in the matrix were clipped to be at a size of at most 100; this allows for better contrast between cells. In Figure 3 (a)- (g), we can observe that each of the traffic matrices, and in particular, the HPC matrices, present a clear and unique pattern. Looking at Figure 3 (h)-(n), we observe that, indeed, our model recreates the general pattern in the traffic matrix. Looking closely, the major difference in any matrix is the frequency of some areas or cells; this is apparent in 3 (a) & (h) for the CNS trace and the Facebook matrices. More generally, the model seems to more easily recreate traces from HPC. In particular, the DB traces seem to miss some perpendicular lines. We conjecture that the Facebook set is more random and holds less structure and, therefore, might require a larger model. In the scope of this paper, we examine the potential of setting a different temperature for the model to improve the result.\nIn Figure 4, we see three different traffic matrices for the DB trace and two generated counterparts of two different temperatures. In Figure 4 (b), the generation temperature was set to 0.9; in Figure 4 (c), the generation temperature was set to 1.2. Figure 4 (a) shows the original traffic matrix for reference. The pattern of the original trace is not very specific. It can be broadly described with an active diagonal and a few active hot spots, and the rest of the traffic is more uniformly distributed except for a few nodes. The low-temperature matrix in 4 (b) shows a good similarity regarding the most active features, such as the diagonal and the hot spots. However, the rest of the traffic seems to be less uniform. For 4 (c), it also shows a good reconstruction of the most active features, but the rest of the matrix seems more uniform and closer to the original. We can tentatively conclude that, in this case, a trace generated using a higher temperature setting better recreated the original.\nWe note that recreating a traffic matrix is simple on its own. We merely need to sample each trace from the requested distribution. We offer this as a basic validation of our models' performance. It allows us to be sufficiently sure that the spatial structure of the distribution created by the model is of a similar nature to the original trace it mimics. In conclusion, we see that our model can recreate the general pattern found in the original traffic trace. This is also consistent with a model generating a trace using different temperatures from the tested model; fine-tuning the model with different temperatures may help create a more accurate trace. However, recreating a trace requires far more than generating a similar traffic matrix. In the following chapters, we will focus more on the temporal aspect of the trace and see if our model can generate a similar trace in the temporal sense; we will show how the nature of the GPT architecture can generate a trace of a similar temporal pattern."}, {"title": "B. Trace Complexity", "content": "In this section, we explore trace complexity, as described in Section II-B. We implemented the process found in the original paper [8] and calculated complexity values for all traces. We believe that if the traces generated from our DTG-GPT model are indeed faithful to the original traces, they will have a similar complexity profile, i.e., similar values of both temporal and non-temporal complexity. In this section, we will discuss this hypothesis. In Figure 5 (a) and (b), we see the complexity maps for the HPC and Facebook trace sets, respectively. Each trace is paired with a generated trace from our model, marked by a transparent circle of the same color. We can observe that our original trace set is diverse in terms of its complexity values, particularly on the temporal scale, with traces having temporal complexity values ranging from roughly 0.2 to near 1. Regarding the generated traces, in 5 (a), we observe that each circle for each generated trace overlaps with its original trace circle; this means that both traces have a similar complexity profile. We note that the generated Neckbone trace is nearly identical. Looking at each complexity dimension (the x or y axis) separately, we see that the generated trace follows the non-temporal structure of their original counterparts closely, as the non-temporal complexity is nearly identical on all traces, with the CNS trace being somewhat of an outlier; this is concordant with our previous result looking at the traffic matrices. We see that the generated traces also contain a fairly similar quantity of temporal structure, which is the main difference between the traces. In 5 (b), we see that only the generated WEB trace overlaps with its original counterpart, while the others are further apart, with the Hadoop trace being the most separated from its generated counterpart. We conjecture that these traces are more difficult to mimic as they are more random, as evident by their larger overall complexity.\nLet us explore if setting a different temperature during trace generation could help us achieve a more accurate complexity profile.\nIn Figure 6, we see the complexity map for the two most 'misaligned' traces from each trace set in Figure 5; these are the HPC CNS trace and the Facebook Hadoop trace. The map in Figure 6 also contains the results of five generated traces with different temperatures from the set [0.9, 0.95, 1, 1.1, 1.2], each generated trace is marked by the temperature used to generate it, while the original traces are denoted by 'Orig.' Increasing the temperature increases the randomness of the model, thus we expect that increasing temperature will in-crease the trace complexity of the generated traces and vise versa. Indeed, this is evident in the map as each generated trace moves towards maximal complexity at the upper right corner (at point (1,1)) with increasing temperatures. Regard-ing the generated traces, for the CNS trace, we can see that a trace generated at a temperature of 0.95 better matches the complexity profile of the original CNS trace than a trace generated at a temperature of 1.0. For the Hadoop trace, we notice that greater temperature improves the complexity profile to make it closer to the original Hadoop trace. The complexity map allows us to visualize the complexity profile of different traces on the same graph. However, to judge the similarity of generated traces to the original in a more precise manner, we take the Euclidean distance between the original and generated complexity profiles. That is, we measure the distance between the Cartesian coordinates of both traces. Figure 7 shows the distance between each of our seven traces and their generated counterparts for a set of five temperatures. We can see that different traces have different temperatures,"}, {"title": "C. Traffic Bursts", "content": "While trace complexity looks at the fidelity of traces from the point of view of information theory, in this section, we take a more classical network approach and explore traffic bursts.\nA very commonly researched topic for network planners is the bursty behavior of the traffic. Generally, a burst can be defined as a sequence of requests arriving during a short time period, above the typical traffic behavior [34]. Traces commonly have an explicit temporal measure (i.e., a timestamp in seconds); however, our traces are more simplistic. Therefore, in our context, we measure our bursts over a window W of w requests; that is, a window W is part of o starting at some index i and ending at i+w, W = \u03c3[\u03af, i+w]. Non-bursty traces will have all possible request types (i.e., source-destination pairs (s, d)) uniformly distributed through the window, while bursty traces will have only a few request types during the window. For each window, the burst Bw (s, d) of a request (s, d) is the number of times (s, d) appeared in the window sequence W. Let us denote the mean burst size of the window W as MB(W). This can be calculated by counting the unique requests in W and dividing by the window size w. More formally,\n$MB(W) = \\sum_{\\forall(s,d)\\in W} \\frac{1}{w}$     (3)\nAnother interesting measure we would like to test is the ratio between the largest burst in the window W to the mean burst size of the window MB(W), that is, the peak-to-average ratio (PAR). More formally,\n$PAR(W) = \\frac{max(Bw (s, d))}{MB(W)}$     (4)\nWhere max(Bw(s,d)) is the largest burst in W. The mean burst size serves as an indicator of the uniformity of traffic over time. For each window, a low value means that many pairs communicate during each window, while a high value means only a few communicate. The PAR of bursts provides insight into the magnitude of bursts within the traffic, highlighting the gap between peak and average traffic levels [35]. A high PAR would show that the traffic is indeed bursty and that, at least a certain pair is very active in a specific window. In this evaluation, we compare the mean burst size and the PAR of burst and their change over time and examine whether traces generated from the DTG-GPT show similar bursty behavior to their original counterparts. For our DTG-GPT model to be useful, we believe that generated traces should follow a similar but not identical pattern of bursts as the original trace.\nWe study three representative examples. presents the mean burst size and PAR results for three traces, MultiGrid, Nekbone, and DB, and their generated counterparts on a single window size, w = 20k.\nFigure 8 shows the mean burst size for every window in the trace; that is, for every index x in the X axis, we calculated the mean burst size of the window starting at index x in"}, {"title": "D. Novelty of Generated Traces", "content": "As mentioned, our goal when generating new traces is to generate traces that mimic the original traces on different statistical measures and patterns to a close degree. Conversely, we would not like the model to memorize long sequences from the original trace and place them in the generated trace. Applications trained on generated traces would benefit more from a varied trace than one that copies the original. However, since we deal with traces from different domains and traffic patterns, some traces may be less varied. Thus, much like with natural language generation [36], not all memorization is strictly bad. For example, a trace with only a few sequences forces the model to copy existing sequences to some degree. Ultimately, the degree of novelty of generated traces will depend on the original source trace and the user's needs."}, {"title": "VI. CONCLUSIONS AND FUTURE WORK", "content": "In this paper, we proposed DTG-GPT, a GPT-based ma-chine learning model for synthetic datacenter traffic gen-eration. We have also proposed several measures to judge the fidelity of the generated trace. Our evaluation shows DTG-GPT can generate novel traces from different domains, lengths, and network sizes while maintaining fidelity. We have shown that DTG-GPT can generate traces which maintain a nontrivial temporal pattern similar to the original traces. We've also briefly explored the effect of temperatures on the generated traces. However, our work only partially solves the problem of synthetic datacenter traffic generation. The lack of real traces from datacenter operators continues to be a major hurdle, as no significant progress can be made without more data.Evidently, by Section V-D, nearly all sequences produced by our model are novel. We, therefore, hope that in the future, DCN operators will be able to release their network traces indirectly as a set of weights to a model, as these will hopefully not infringe on privacy laws [11]. Our work takes some preliminary steps to evaluate generated traces. We acknowledge that further work will be required to improve the evaluation of new synthetic traces' fidelity by introducing other and more rigorous measurements, possibly by examining the development of the demand graph over time and testing the model's adherence to the rules of different communication protocols. This, in turn, will allow the development of better models.\nFurther work can be done to improve and expand on DTG-GPT. Expanding the token set of the model to include other fields in a network trace. Other address fields (Mac address, ports, etc.) could be tokenized similarly, as the range of these addresses is limited. Increasing the number of tokens in the field vector (as discussed in Section III) should make this addition natural. We believe a GPT-based architecture could be similarly capable of recreating more complex traces containing those fields.\nThe time field may be harder to mimic. however, we would consider employing one-hot encoding as was done in this paper by Kong et al. [6].\nTo conclude, we believe that in the future, GPT-based large-scale network traffic models may be able to create most types of network traces with a large degree of detail.\nAn important step towards this goal would be training on a very large-scale dataset, which would be possible. This will enable researchers and network architects to design and develop future datacenter networks."}, {"title": "APPENDIX", "content": "An interesting use case for our approach is that it also lends itself to generating traces with networks of smaller scale, i.e., having fewer nodes. Throughout the rest of the paper, all generated traces have been generated to have exactly the same number of source and destination nodes as their original counterparts (as mentioned in Table I ). However, generating a trace with a smaller set of nodes is simple through output masking of the logits. A full review of the results for smaller-scale traces generated from our DTG-GPT is beyond the scope of this paper. However, we provide a small test case that may hint towards further capabilities and have recreated the MOCFE traces with 512 nodes instead of the original 1024."}]}