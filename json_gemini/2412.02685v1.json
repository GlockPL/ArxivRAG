{"title": "T-REG: Preference Optimization with Token-Level Reward Regularization", "authors": ["Wenxuan Zhou", "Shujian Zhang", "Lingxiao Zhao", "Tao Meng"], "abstract": "Reinforcement learning from human feedback (RLHF) has been crucial in aligning large language models (LLMs) with human values. Traditionally, RLHF involves generating responses to a query and using a reward model to assign a reward to the entire response. However, this approach faces challenges due to its reliance on a single, sparse reward, which makes it challenging for the model to identify which parts of the sequence contribute most significantly to the final reward. Recent methods have attempted to address this limitation by introducing token-level rewards. However, these methods often rely on either a trained credit assignment model or AI annotators, raising concerns about the quality and reliability of the rewards. In this paper, we propose token-level reward regularization (T-REG), a novel approach that leverages both sequence-level and token-level rewards for preference optimization. Harnessing the self-refinement capabilities of LLMs, our method uses contrastive prompting to enable LLMs to self-generate token-level rewards. These self-generated rewards then act as reward regularization, guiding the model to more effectively distribute sequence-level rewards across tokens. This facilitates better token-level credit assignment and enhances alignment performance. Experiments on the instruction following benchmarks, including Alpaca Eval 2 and Arena-Hard, show that our method consistently outperforms baseline methods by up to 3.8% and 4.4%, respectively.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in large language models (LLMs; Tunstall et al. 2023; Chung et al. 2024; Team et al. 2024) have centered on aligning model outputs with human intentions and preferences. Reinforcement learning from human feedback (RLHF; Christiano et al. 2017; Glaese et al. 2022) has become a dominant approach for achieving this alignment by incorporating human feedback into the training process. The RLHF process typically consists of two phases. In the first phase, responses are generated either with the model being optimized (on-policy RL; Schulman et al. 2017; Guo et al. 2024) or with different models (off-policy RL; Rafailov et al. 2023; Ethayarajh et al. 2024). In the second phase, a reward is obtained based on feedback from either humans (Schulman et al., 2017) or AI annotators (Lee et al., 2023), and this reward is then used to optimize the policy model through algorithms such as PPO (Schulman et al., 2017) and DPO (Rafailov et al., 2023). During this process, the quality of the reward signal is critical to the success of RLHF, where poor or misaligned rewards can lead to issues such as reward hacking (Gao et al., 2023) and model collapse (Wang et al., 2024a; Chowdhury et al., 2024).\nCurrent RLHF methods typically use sequence-level rewards, where reward signals are provided at the end of an entire sequence. Recent studies have shown that incorporating finer-grained reward signals (Wu et al., 2024; Lightman et al., 2023), ultimately at the token level (Zhong et al., 2024), can significantly enhance the performance and convergence of the policy model. However, annotating token-level rewards is challenging for humans, especially when sequences may span thousands of tokens. As a result, current token-level RLHF methods often rely on token-level rewards labeled by AI annotators (Guo et al., 2023; Yoon et al., 2024) or utilize credit assignment models that learn to redistribute sequence-level rewards to individual tokens (Yang et al., 2024b; Zhong et al., 2024). These token-level rewards are then used to train policy models through algorithms such as PPO. However, the quality of these automatically generated rewards remains a concern. AI-generated annotations and credit assignment models are prone to errors, raising questions about the reliability of the rewards and, consequently, the overall quality of the trained policy models.\nIn this paper, we investigate the integration of auto-labeled token-level rewards into preference optimization. We introduce T-REG, a novel token-level preference optimization algorithm that leverages both sequence-level and token-level rewards. Our approach is motivated by the observation that DPO inherently learns token-level rewards. Instead of directly optimizing the policy model with auto-labeled token-level rewards, T-REG retains sequence-level preference optimization while using these token-level rewards as weak supervision. This weak supervision regularizes the token-level reward signals implicitly learned during preference optimization. This design enables T-REG to optimize the overall sequence-level reward while also achieving effective token-level credit assignment. Inspired by the self-refinement capabilities of LLMs (Madaan et al., 2024), we derive token-level rewards through contrastive prompting, where revision-based prompts rewrite the output in opposing directions. By comparing the token probabilities generated from these prompts, we compute the token-level rewards. This approach assigns token-level rewards effectively without requiring additional training or token-level annotations.\nWe evaluate T-REG on two instruction following benchmarks, including Alpaca Eval 2 (Li et al., 2023) and Arena-Hard (Li et al., 2024). T-REG demonstrates consistent improvements across both Llama-3-Instruct-8B and Gemma-2-9B-it models. On Alpaca Eval 2, T-REG increases the length-controlled win rate by up to 24.8% over the SFT checkpoint, surpassing DPO by up to 3.8%. On the more challenging Arena-Hard benchmark, T-REG improves the win rate by up to 20.0%, outperforming DPO by as much as 4.4%. Moreover, we find that T-REG can be integrated into different preference optimizations (e.g., SimPO) and shows consistent improvements. Additionally, we analyze the token-level rewards learned by T-REG, demonstrating that it effectively achieves token-level credit assignment.\nOur contributions are summarized as follows:\n\u2022 We propose T-REG, a token-level preference optimization method that utilizes both sequence-level and token-level rewards to enhance alignment with human preferences.\n\u2022 T-REG utilizes token-level rewards derived through contrastive prompting to guide the token-level rewards learned during preference optimization, enabling effective token-level credit assignment without the need for external token-level reward annotations.\n\u2022 T-REG achieves consistent improvements on instruction-following benchmarks including Alpaca Eval 2 and Arena-Hard, outperforming DPO by up to 3.8% and 4.4%, respectively, and showing compatibility with other preference optimization algorithms."}, {"title": "2 Related Work", "content": "Reinforcement learning from human feedback.\nReinforcement Learning from Human Feedback (RLHF) has become a cornerstone in aligning Large Language Models (LLMs) with human intentions, allowing for fine-tuning of models to produce more useful and aligned responses (Ziegler et al., 2019). The early implementations of RLHF typically used the proximal policy optimization algorithm (PPO; Schulman et al. 2017), which operates by optimizing policies through episodic rewards, calculated using a reward model trained on human preferences (Bai et al., 2022; Ouyang et al., 2022). However, applying deep RL techniques such as PPO to LLMs has proven challenging due to instability and inefficiency in training (Engstrom et al., 2019). To address these challenges, alternative methods have emerged that employ supervised fine-tuning (SFT) to simulate the RL process, using high-quality responses to create more stable training signals (Lu et al., 2022; Zhao et al., 2023; Yang et al., 2024b). In contrast to classical RLHF methods, a new line of research has proposed direct alignment algorithms (Zhao et al., 2023; Rafailov et al., 2023; Meng et al., 2024; Azar et al., 2024; Zhou et al., 2024a; Wang et al., 2024b), which optimize a supervised target directly based on preference data, without the need for a separate reward model. Despite their advantages, these methods rely solely on sequence-level rewards. In this work, we extend this paradigm by prompting LLMs to generate token-level rewards using contrastive prompting, integrating these rewards as regularization during preference optimization.\nToken-level RLHF. While sequence-level rewards provide effective supervision for RLHF, the extended length of LLM generations means that different parts of a sequence contribute unequally to the final rewards, making sequence-level rewards less data-efficient and effective (Chan et al., 2024; Zhong et al., 2024). To address these limitations, recent efforts have shifted from sequence-level to token-level rewards, with a focus on how to effectively derive these finer-grained signals. Existing methods have adopted various approaches to token-level reward derivation. For instance, Token-Level Direct Preference Optimization (TDPO; Zeng et al. 2024) uses Markov decision processes (MDPs) to derive token-level rewards. Other methods, such as token-level continuous reward (TLCR; Yoon et al. 2024), FIGA (Guo et al., 2023), and RLMEC (Chen et al., 2024), rely on off-the-shelf LLMs to auto-label token-level rewards. Additionally, since DPO has been shown to implicitly learn token-level rewards (Rafailov et al., 2023), some approaches (Zhong et al., 2024; Yang et al., 2024a) leverage DPO for this purpose. Once token-level rewards are derived, existing methods typically optimize the policy using algorithms such as PPO (Zhong et al., 2024; Yoon et al., 2024) or token selection strategies (Yang et al., 2024a). However, these approaches often depend heavily on auto-labeled token-level rewards, which introduces potential challenges due to the inherent noise of auto-labeled token-level rewards. In our approach, we use token-level rewards derived from LLMS as guidance in preference optimization to enhance token-level credit assignment. Therefore, we mitigate the risks of solely relying on auto-labeled token-level rewards, achieving a more robust and effective optimization process."}, {"title": "3 Method", "content": "In this section, we present in detail our proposed method, T-REG (Algorithm 1). We first provide the theoretical background of RLHF in Section 3.1. Next, we introduce the token-level credit assignment problem and explain how to guide preference optimization using a token-level regularization in Section 3.2. Finally, we present our contrastive prompting method for generating token-level rewards in Section 3.3.\n3.1 Preliminaries\nRLHF. Reinforcement learning from human feedback (RLHF; Schulman et al. 2017) is a widely used technique for aligning LLMs with human preferences. Using human or AI annotations, RLHF enables LLMs to generate outputs that align better with human expectations. Consider a preference dataset $D = \\{(x^{(i)}, y_w^{(i)}, y_l^{(i)})\\}_{i=1}^N$, where each sample consists of a prompt $x^{(i)}$ and two outputs, $y_w^{(i)}$ (preferred) and $y_l^{(i)}$ (dispreferred), as judged by annotators, the core of RLHF lies in a reward model, $r^*(x, y)$, which assigns a score to each candidate output y based on how well it aligns with the prompt x. A commonly adopted framework for modeling preference distributions is the Bradley-Terry (BT; Bradley and Terry 1952) model, which defines the probability that yw is preferred over y\u0131 as:\n$p(y_w > y_l | x) = \\frac{exp(r^*(x, y_w))}{exp(r^*(x, y_w)) + exp(r^*(x, y_l))}$.\nThe RLHF process typically consists of two phases: learning the reward model from preference data, followed by using reinforcement learning to optimize a policy model based on this reward. In the first phase, the reward model is trained using maximum likelihood estimation, producing an estimated reward function f(x, y). The reward model can be structured to return feedback either at the end of a sequence (Liu et al., 2023; Dong et al., 2023; Xiong et al., 2024), where the evaluation is based on the entire output, or at each step of the sequence (Uesato et al., 2022; Lightman et al., 2023), where feedback is provided based on intermediate reasoning steps. Once the reward model is trained, it is used to finetune the policy model by optimizing the following objective:\n$\\max_{\\pi_{\\theta}} E_{x \\sim D, y \\sim \\pi_{\\theta}(:\\2)} [f(x, y) - \\beta log \\frac{\\pi_{\\theta}(y|x)}{\\pi_{ref}(y|x)}]$,\nwhere \u03b2 is a KL penalty coefficient to regularize the deviation between the policy model $ \\pi_{\\theta}$ and the reference model $\\pi_{ref}$.\nDPO implicitly learns token-level rewards. In RLHF, rewards are typically assigned at the end of sequences, which can extend to thousands of tokens. However, not all tokens contribute equally to the final reward (Chan et al., 2024; Rafailov et al., 2024; Yang et al., 2024a). To address this, it is crucial for a preference optimization algorithm to effectively distribute sequence-level rewards across individual tokens. Previous work (Zhong et al., 2024; Rafailov et al., 2024) has shown that Direct Preference Optimization (DPO; Rafailov et al. 2023) implicitly learns token-level rewards under sequence-level reward supervision. Specifically, with the token-level reward defined as $r_{token}(y_t|x, y_{<t}) = \\beta log \\frac{\\pi^*(y_t|x, y_{<t})}{\\pi_{ref}(y_t|x, y_{<t})}$, the sequence-level reward is computed as\u00b2:\n$r_{DPO}(x, y) = V^*(x) + \\sum_{t=1}^{T} \\beta log \\frac{\\pi^*(y_t|x, y_{<t})}{\\pi_{ref}(y_t|x, y_{<t})}$,\nwhere T is the number of tokens in the sequence, $ \\pi^*$ is the optimal policy, $\\pi_{ref}$ is the reference policy, and $V^*$ is the value function of $ \\pi^*$. For a pair of outputs (yw, y\u0131), the probability that yw is preferred over y\u0131, modeled using the BT model, is given by:\n$p(y_w > y_l|x) = \\sigma\\left(\\sum_{t=1}^{T_w} \\beta log \\frac{\\pi^*(y_{wt}|x, y_{w<t})}{\\pi_{ref}(y_{wt}|x, y_{w<t})} - \\sum_{t=1}^{T_l} \\beta log \\frac{\\pi^*(y_{lt}|x, y_{l<t})}{\\pi_{ref}(y_{lt}|x, y_{l<t})}\\right)$,\nwhere $V^*(x)$ cancels out because yw and y\u0131 correspond to the same prompt x. By applying maximum likelihood estimation to the preference dataset, the policy model $ \\pi$ can be optimized by the following loss function:\n$L_{DPO} = -E_{(x, y_w, y_l) \\sim D} [log \\sigma(r(y_w > y_l | x))]$,\nwhich resembles the original DPO loss (Rafailov et al., 2023). Therefore, performing RLHF with DPO is redistributing the sequence-level reward into the token level as $\\beta log \\frac{\\pi(y_t|x, y_{<t})}{\\pi_{ref}(y_t|x, y_{<t})}$, where $ \\pi$ is the policy trained by DPO. This redistribution ensures token-level credit assignment while optimizing the sequence-level objective.\n3.2 Regularized Token-level Preference Optimization\nToken-level reward as regularization. In preference optimization, our objective is to ensure consistency between the model's pairwise rankings and human preference while also achieving effective token-level credit assignment to enhance the model's generalization capabilities. Since DPO models are trained with sequence-level rewards as supervision, they can capture pairwise rankings at the sequence level (Lambert et al., 2024). However, since token-level rewards are implicitly derived through the redistribution of sequence-level rewards in DPO, they often lack direct guidance. Recent studies have demonstrated that LLMs can serve as dense token-level reward functions without fine-tuning, by employing techniques such as contrastive decoding (Li et al., 2022) or contrastive prompting (Kim et al., 2024; Zhao et al., 2024b). These methods infer token-level rewards from the difference in token probabilities between a strong model and a weak model, expressed as $\\log \\frac{\\pi_{strong}(y_t | x, y_{<t})}{\\pi_{weak}(y_t | x, y_{<t})}$ (see details in Section 3.3). However, these methods do not guarantee that the accumulated sequence-level rewards align with the ground-truth sequence-level rewards provided by the preference data. In this paper, we propose integrating token-level and sequence-level rewards to leverage the strengths of both. Our approach incorporates token-level rewards as regularization in preference optimization to improve token-level credit assignment, thereby improving generalization of the policy model.\nSpecifically, we introduce a regularization term to ensure that the token-level rewards learned by DPO align with dense token-level rewards derived from LLMs. Let $r_{token}$ denote the token-level reward learned by policy $ \\pi$, and $\\hat{r}_{token}$ denote the given dense token-level reward, we can compute their similarity at yt by:\n$sim(y_t | x, y_{<t}) = r_{token}(y_t | x, y_{<t}) \\hat{r}_{token}(y_t | x, y_{<t})$.\nOur goal is to maximize the alignment between the token-level rewards across the entire output. The regularization term, therefore, is formulated as:\n$L_{reg} = \\sum_{t=1}^{T} sim(y_t | x, y_{<t}) = \\sum_{t=1}^{T} r_{token}(y_t | x, y_{<t}) \\hat{r}_{token}(y_t | x, y_{<t}) = \\sum_{t=1}^{T} \\beta r_{token}(y_t | x, y_{<t}) log \\frac{\\pi(y_t | x, y_{<t})}{\\pi_{ref}(y_t | x, y_{<t})}$.\nHere, the term $\\pi_{ref}(y_t | x, y_{<t})$ is a constant reference probability, which does not affect the gradient and can thus be omitted. This leads to the following modified regularization term:\n$L_{reg} = -\\sum_{t=1}^{T} \\beta \\hat{r}_{token}(y_t | x, y_{<t}) log \\pi(y_t|x, y_{<t})$. (1)\nThis term acts as a weighted language modeling loss, increasing the likelihood of tokens with positive token-level rewards while decreasing the likelihood of tokens with negative token-level rewards, thereby improving token-level credit assignment.\nRegularized token-level preference optimization. During preference optimization, we then optimize both the preference optimization loss (e.g., LDPO) and the regularization loss (Lreg). The preference optimization loss aims to increase the margin between the probabilities of preferred and dispreferred sequences, while the regularization loss maximizes the probabilities of high-reward tokens. Since these two objectives optimize in different directions, balancing them is critical for effective optimization. To achieve this balance, we use sequence-level gradient norms (Chen et al., 2018). Recall that the gradient of the DPO loss is:\n$\\nabla L_{DPO} = -\\beta \\sigma (r_{DPO}(x, y_l) - r_{DPO}(x, y_w)) (\\nabla log \\pi (y_w|x) - \\nabla log \\pi (y_l|x))$.\nwhere for each sequence, the gradient norm with respect to each token is proportional to $\\sigma (r_{DPO}(x, y_l) - r_{DPO}(x, y_w))$. To ensure that the regularization loss does not dominate the preference optimization loss for each sequence, we introduce a sequence weight to modulate the regularization loss. The final regularized token-level preference optimization objective is:\n$L_{DPO-REG} = L_{DPO} + \\alpha E_{(x, y_w, y_l) \\sim D} [W(x, y_w, y_l) (L_{REG}(x, y_w) + L_{REG}(x, y_l))]$, (2)\nwhere $W(x, y_w, y_l) = \\sigma (r_{DPO}(x, y_l) - r_{DPO}(x, y_w))$ is the sequence weight and is detached from back propagation, $ \\alpha$ is a hyperparameter that controls the strength of regularization. As shown in our ablation study (see Section 4.2), sequence weighting achieves the best performance."}, {"title": "3.3 Self-generated Token-level Rewards", "content": "In this section, we focus on deriving the dense token-level rewards by LLMs. Existing work (Zhou et al., 2024b; Zhao et al., 2024a) often approximates this reward by contrasting a strong model against a weak model, where:\n$r(y_t|x, y_{<t}) = log \\frac{\\pi_{strong} (y_t|x, y_{<t})}{\\pi_{weak}(y_t|x, y_{<t})}$.\nThe strong model can be a model from the same family but at a larger scale (Li et al., 2022), a model that is more aligned with human expectations (Zhou et al., 2024b; Zhong et al., 2024; Huang et al., 2024), or a model with better prompts (Kim et al., 2024; Zhao et al., 2024b). In this paper, we focus on using contrastive prompting to derive token-level reward, as this approach only requires a single model and does not require additional fine-tuning. Specifically, we utilize two contrastive, revision-based prompts, $x_{better}$ and $x_{worse}$ designed to refine the current output y in positive or negative directions to evaluate token quality. Building on prior work in self-refinement (Madaan et al., 2024), we leverage the demonstrated ability of LLMs to adjust outputs in diverse directions. The revision prompt, adapted from Wang et al. (2024e) and illustrated in Fig. 2, refines an output based on four aspects: helpfulness, correctness, coherence, and verbosity. Given a token yt and a causal language model $ \\pi_{eval}$ for generating the rewards, we calculate the token-level reward by:\n$r(x, y_{<t}, y_t) = \\sigma \\left(log \\frac{\\pi_{eval}(y_t|x_{better}, y_{<t})}{\\pi_{eval}(y_t|x_{worse}, y_{<t})} - 0.5\\right)$, (3)\nwhere $ \\sigma$ clips the reward value, and -0.5 clips recenters the original reward into range of [-0.5, 0.5]. This normalization helps mitigate extreme token-level reward values, thereby stabilizing the preference optimization process. Thanks to the autoregressive nature of causal language models, the rewards for all tokens in an output can be derived with only two forward passes. Due to tokenization issues, $\\pi_{eval}$ should ideally share the same vocabulary as the reference model $ \\pi_{ref}$. In this work, we specifically focus on the self-generated token-level rewards, where we use the reference model $\\pi_{ref}$ to generate the token-level rewards."}, {"title": "4 Experiments", "content": "In this section, we outline our experimental settings in Section 4.1, present the main results and ablations in Section 4.2, and provide qualitative case studies in Section 4.3.\n4.1 Experimental Settings\nModel configurations. Our methods are implemented using the official repo of Zephyr4. Preference optimization is performed based on two large language models: Llama-3-8B-Instruct (Dubey et al., 2024) and Gemma-2-9B-it (Team et al., 2024). We use the hyperparameters used by Meng et al. (2024), who conducted an extensive hyperparameter search. For our newly introduced hyperparameter, a, we search in the range {0.1, 0.25, 0.5}.\nTraining data. We perform RLHF in an on-policy setting, where outputs are sampled from the policy being optimized. However, generating outputs during training is computationally expensive. To address this, we adopt an approximate method similar to that used by Meng et al. (2024), where outputs are sampled using the reference policy before preference optimization. Specifically, the reference policy generates five outputs for each prompt, which are then evaluated using an external reward model. The best and worst outputs are selected to form a preference pair. For our experiments, we use the preference data generated by Meng et al. (2024), derived from prompts in Ultrafeedback (Cui et al., 2023) and scored using the ArmoRM reward model (Wang et al., 2024d,c).\nEvaluation. We evaluate the models on two benchmarks: Alpaca Eval 2 (Li et al., 2023) and Arena-Hard (Li et al., 2024). Alpaca Eval 2 is an automated benchmark designed to assess the alignment of LLMs with human preferences across 805 representative instructions. For each instruction, the evaluated model's response is compared head-to-head with the response generated by gpt-4-turbo using an automatic evaluator (with gpt-4-turbo as the default evaluator). The win rate reflects the probability that the evaluator prefers the responses of the evaluated model over those of gpt-4-turbo. Additionally, Alpaca Eval 2 introduces a length-controlled win rate (Dubois et al., 2024) to mitigate length bias in gpt-4-turbo. We use the generation configurations recommended by Zheng et al. (2024) to generate the outputs during evaluation."}, {"title": "4.2 Main Results and Ablation", "content": "T-REG consistently outperforms the baselines and the compared methods. The results on Alpaca Eval 2 and Arena-Hard are presented in Table 1. We observe that T-REG, when applied to both DPO and SimPO, consistently outperforms these methods on both benchmarks. Specifically, on Alpaca Eval 2, T-REG increases the length-controlled win rate by up to 24.8% over the SFT checkpoint, surpassing DPO by up to 3.8%. On the more challenging Arena-Hard benchmark, T-REG improves the win rate by up to 20.0%, outperforming DPO by as much as 4.4%. Similar improvements are observed with SimPO, indicating that although T-REG is primarily derived from DPO, it generalizes well to other preference optimization methods. Among other token-level preference optimization methods, RTO, which conducts PPO on token-level rewards derived from DPO, is the best performing one and achieves results comparable to or better than DPO, especially on the challenging Arena-Hard benchmark. However, RTO's performance gains are smaller or even negative on Alpaca Eval 2, which focuses on general questions, while T-REG consistently yields positive improvements across both benchmarks. These results highlight the effectiveness of T-REG in enhancing preference optimization.\nSelective regularization on high-reward tokens yields better results. As shown in Eq. 1, T-REG can be interpreted as performing weighted SFT selectively focusing on high-reward tokens. Previous methods (Dubey et al., 2024; Xu et al., 2024) apply an SFT loss over the entire preferred output yw, which has been shown to help prevent degeneration:\n$L_{DPO-SFT} = L_{DPO} + \\alpha E_{(x, y_w, y_l) \\sim D} [log \\pi (y_w|x)]$.\nHowever, as shown in Table 2, incorporating this loss results in significant performance degradation on both tasks, even with a small a value. This is likely because optimizing over the entire yw includes tokens of lower quality. In contrast, T-REG focuses exclusively on high-reward tokens achieves consistent improvements over DPO.\nSequence weighting enhances performance. To evaluate the impact of sequence-level weighting on performance, we trained DPO-REG using static weighting by removing the weighting term from the loss. However, as shown in Table 2, this approach did not result in consistent improvements over DPO, showing the effectiveness of sequence-level weighting.\nSelf-generated reward outperforms DPO reward. In line with prior approaches like RTO and SePO, we also experimented with using the token-level rewards derived from DPO for regularization. As shown in Table 2, this approach achieved similar performance to self-generated rewards on Alpaca Eval 2 but performed much worse on Arena-Hard, underperforming self-generated rewards by 3.4%. These results indicate that self-generated token-level rewards are comparable to or better than those derived from DPO.\nBetter preference data yields stronger results. To enhance our model further, we leverage hybrid preference data as proposed by Zhou et al. (2024a). This dataset is constructed by sampling five out-"}, {"title": "4.3 Case Study", "content": "In this section, we examine the quality of token-level rewards learned by T-REG. Since no existing evaluation datasets specifically assess token-level rewards, we perform a qualitative analysis instead. Figure 3 presents three example tokens from Chatbot Arena (Chiang et al., 2024). We calculate the token-level rewards with $\\log \\frac{\\pi_{\\theta}(y_t | x, y_{<t})}{\\pi_{ref}(y_t|x, y_{<t})}$, where $ \\pi_{\\theta}$ and $\\pi_{ref}$ are the policy and reference models, respectively. The positive and negative rewards are visualized in red and blue, respectively. Our analysis demonstrates that integrating T-REG into DPO enhances the precision of token-level reward assignments.\n\u2022 Prompt 1: The prompt specifies that only the initial letter of the word \"Test\" should be capitalized, but the response is entirely in uppercase. Here, T-REG assigns a negative reward to capture the mismatch, whereas DPO incorrectly assigns a positive reward.\n\u2022 Prompt 2: The correct response is one dog, yet DPO erroneously assigns a positive reward to the incorrect answer. In contrast, T-REG accurately reflects the error by assigning a negative reward.\n\u2022 Prompt 3: The correct answer is 4, and the response matches. While DPO fails to assign a positive reward to this response, T-REG correctly assigns a positive reward.\nThese examples underscore the effectiveness of T-REG in achieving precise token-level reward assignments, demonstrating its superiority in addressing discrepancies that DPO overlooks."}, {"title": "5 Conclusion", "content": "In this study, we addressed the challenge of token-level credit assignment in preference optimization by introducing T-REG, a novel method that leverages self-generated token-level rewards derived through opposite prompting as regularization. This approach allows for more effective and fine-grained credit assignment, seamlessly integrating with sequence-level preference optimization to enhance alignment with human preferences. Experiments on instruction-following benchmarks including Alpaca Eval 2 and Arena-Hard, as well as our qualitative case study, demonstrate that T-REG not only enhances alignment performance but also achieves more precise token-level credit assignment."}, {"title": "Limitations", "content": "Lack of quantitative results on token-level credit assignment. In this paper, we only provide qualitative results on the token-level rewards learned by T-REG. Therefore, there lacks a systematic and more rigorous study on the accuracy of token-level rewards. While we believe this analysis will provide valuable insights, to the best of our knowledge, no benchmarks currently exist for evaluating token-level rewards. Future work should focus on constructing dedicated evaluation datasets to facilitate quantitative assessment of token-level reward accuracy.\nRewards in other levels. Our approach focuses on utilizing rewards at the token and sequence levels. However, intermediate levels, such as step-level and span-level rewards, also provide useful information for alignment tasks and have been widely applied, especially in math and coding problems. The current method does not account for these intermediate reward levels. Future research could explore methods that incorporate multiple levels of rewards, potentially enhancing the flexibility and effectiveness of preference optimization."}]}