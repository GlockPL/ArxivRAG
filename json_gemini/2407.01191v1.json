{"title": "MARS: Multimodal Active Robotic Sensing for Articulated Characterization", "authors": ["Hongliang Zeng", "Ping Zhang", "Chengjiong Wu", "Jiahua Wang", "Tingyu Ye", "Fang Li"], "abstract": "Precise perception of articulated objects is vital\nfor empowering service robots. Recent studies\nmainly focus on point cloud, a single-modal ap-\nproach, often neglecting vital texture and lighting\ndetails and assuming ideal conditions like optimal\nviewpoints, unrepresentative of real-world scenar-\nios. To address these limitations, we introduce\nMARS, a novel framework for articulated object\ncharacterization. It features a multi-modal fusion\nmodule utilizing multi-scale RGB features to en-\nhance point cloud features, coupled with reinforce-\nment learning-based active sensing for autonomous\noptimization of observation viewpoints. In exper-\niments conducted with various articulated object\ninstances from the PartNet-Mobility dataset, our\nmethod outperformed current state-of-the-art meth-\nods in joint parameter estimation accuracy. Addi-\ntionally, through active sensing, MARS further re-\nduces errors, demonstrating enhanced efficiency in\nhandling suboptimal viewpoints. Furthermore, our\nmethod effectively generalizes to real-world articu-\nlated objects, enhancing robot interactions. Code is\navailable at https://github.com/robhlzeng/MARS.", "sections": [{"title": "Introduction", "content": "In an era increasingly marked by the integration of robotic as-\nsistance into everyday scenarios, conducting research on the\nprecise perception of articulated objects, such as kitchen uten-\nsils and personal devices, is of paramount importance. These\nobjects often have intricate joints and multiple moving parts,\npresenting unique and complex challenges in robotic percep-\ntion and manipulation.\nBy precisely perceiving the joint parameters of articu-\nlated parts, robots enhance their effectiveness in manipu-\nlation planning for these objects, considering parameters\nsuch as joint position, orientation, and part state. We re-\nviewed previous research [Yi et al., 2018; Yan et al., 2020;\nJain et al., 2021; Jiang et al., 2022; Chu et al., 2023] and\nidentified limitations in existing methods. Firstly, Many stud-\nies perceive joint characteristics using a single point cloud\nmodality, neglecting valuable information of color and tex-\nture data. Secondly, different joints, such as revolute and\nprismatic, necessitate separate perception networks, which\ncan limit practical applications. Lastly, current work often\nassumes the availability of an ideal observation viewpoint,\ndisregarding scenarios where the target part is obstructed or\ninvisible. However, robots frequently face suboptimal view-\ning angles, hindering complete object observation.\nTo address these challenges, MARS implements a mul-\ntimodal fusion strategy with a novel multi-layer dueling\nmodule, efficiently extracting and combining feature maps\nfrom diverse convolutional layers into a more effective,\ninformation-dense image feature representation. Subse-\nquently, MARS integrates multi-scale RGB features and\npoint cloud features via a transformer encoder-based fusion\nmodule. Moreover, MARS standardizes the description of\njoint parameters, allowing for the perception of various joint\ntypes through a single network. Addressing the issues posed\nby suboptimal viewing angles, MARS features a Reinforce-\nment Learning (RL) driven active sensing strategy. This ap-\nproach authorizes dynamic camera position adjustment, en-\nsuring acquisition of the most informative viewpoint.\nUltimately, we introduce a comprehensive task flow for\nrobotic perception and manipulation of articulated objects"}, {"title": "2 Related Works", "content": "Articulated Object Characterization. To enhance robotic\ncapabilities in perceiving and manipulating articulated ob-\njects, a wealth of simulators [Todorov et al., 2012; Xiang et\nal., 2020; Szot et al., 2021] and open datasets [Chang et al.,\n2015; Mo et al., 2019; Wang et al., 2019; Geng et al., 2023]\nhave emerged as essential resources. These tools have fa-\ncilitated advancements in 3D reconstruction [Li et al., 2020;\nBozic et al., 2021; Yang et al., 2021], joint parameter estima-\ntion [Jain et al., 2021; Shi et al., 2021; Zeng et al., 2021;\nJiang et al., 2022; Chu et al., 2023], and the prediction\nof interactive positions and trajectories [Mo et al., 2021;\nWu et al., 2021; Wang et al., 2022] for articulated objects.\nIn joint parameter estimation, prior work [Yi et al., 2018;\nAbbatematteo et al., 2019; Shi et al., 2021; Jain et al., 2021;\nSiarohin et al., 2021; Jiang et al., 2022] has leveraged multi-\nview observations from ongoing monitoring as visual inputs,\ncapitalizing on the rich visual cues provided by the chang-\ning joint states. However, this method complicates data col-\nlection, limiting its practicality in real-world robotics. Fur-\nthermore, many studies [Yan et al., 2020; Jiang et al., 2022;\nChu et al., 2023] rely exclusively on point cloud data, ne-\nglecting the valuable features available in RGB imagery that\ncan enhance joint parameter estimation. Our methodology\nfocuses on employing a single RGB image along with point\ncloud data to achieve accurate perception of joint parameters.\nMultimodal Feature Fusion. In the realm of multimodal\nfeature fusion, especially in combining RGB and point cloud\ndata, current research predominantly focuses on 3D object de-\ntection [Ku et al., 2018; Sindagi et al., 2019; Zhu et al., 2021;\nPiergiovanni et al., 2021; Wu et al., 2022; Zhang et al., 2022].\nHowever, due to differences in input data, these fusion meth-\nods often struggle to be directly applicable and effective in\njoint analysis. A notable example similar to our approach\nis EPNet [Huang et al., 2020], which enhances point cloud\nfeatures by integrating image features into the point cloud\ndomain, utilizing both local and global contexts of images.\nIn contrast, our method employs a competitive mechanism\nto dynamically prioritize the significance of image features\nacross various scales.\nActive Sensing. In practical applications, the challenging\ntask of obtaining optimal imaging angles for robots [Ammi-\nrato et al., 2017; Zhao et al., 2022] underscores the need for\nactive sensing. This technique entails dynamically adjusting\nsensor positions or the manipulation environment to enhance\ndata acquisition, proving essential for certain tasks [Han et\nal., 2019; Mattamala et al., 2021; Safronov et al., 2021],\nespecially in scenarios where initial views are insufficient\ndue to partial occlusions or limited visibility of articulated\nobjects. In current studies, researchers commonly assume\nthat high-quality images and point clouds can be obtained\nfrom an ideal viewpoint [Jain et al., 2021; Yang et al., 2021;\nJiang et al., 2022; Chu et al., 2023], but such assumptions\nfrequently fail to align with the reality. Our approach suc-\ncessfully adapts to complex real-world application scenarios\nby incorporating active sensing techniques."}, {"title": "3 Method", "content": "We introduce MARS, a framework designed for estimating\njoint parameters in articulated objects. As depicted in Fig. 2,\nMARS consists of two primary components: Multimodal\nFeature Fusion Perception (MFFP) and Active Sensing (AS).\nThe MFFP component of MARS utilizes ResNet18 [He et\nal., 2016] and PointNet++ [Qi et al., 2017] as backbone net-\nworks to efficiently extract features from the input RGB im-\nage and point cloud data. The Multi-Layer Dueling Module\n(MLDM) is designed to strategically extract and weigh im-\nage features across different scales. Following this, image\nand point cloud features are merged at the feature level via\na specialized fusion module. A decoding module then pro-\ncesses these integrated features to produce outputs for joint\nparameters and a perception score, reflecting the efficacy of\nthe current viewpoint.\nThe AS module operates based on the perception score and\na set threshold. If the score is below this threshold, it trig-\ngers a viewpoint change. A new observation position is de-\ntermined from the action space, followed by a repeated per-\nception process. This approach enables real-time adjustments\nfor optimal data acquisition and improved joint parameter es-\ntimation in complex or obstructed scenarios."}, {"title": "3.1 MLDM", "content": "Employing ResNet, we capturing a range of articulated ob-\nject RGB features from local to global, as depicted in Fig. 3a.\nFor each feature map $f_i \\in \\mathbb{R}^{C_i \\times H_i \\times W_i}$ with $C_i$ channels\nand spatial dimensions $H_i \\times W_i$, and for point cloud features\n$f_i \\in \\mathbb{R}^{N_i \\times K}$ comprising $N_i$ points each of $K$ dimensions,\nwe perform point-wise convolution and subsequent pooling\noperations to aggregate the features (as shown in Fig. 3b).\nThe aggregated image feature $\\overline{f}_i \\in \\mathbb{R}^{K}$ is computed as fol-\nlows:\n$\\overline{f}_i = g \\left(\\delta \\left(\\mathcal{B} \\left(\\operatorname{PWConv}\\left(f_i\\right)\\right)\\right)\\right),\\qquad(1)$\nwhere $g$ is the global average pooling, $\\delta$ is the ReLU func-\ntion [Nair and Hinton, 2010], and $\\mathcal{B}$ represents BN [Ioffe and\nSzegedy, 2015], with the kernel size for the PWConv being\n$K \\times 1 \\times 1$. For point cloud feature aggregation, the max\npooling operation $m$ is utilized:\n$f_p = m \\left(f_i\\right).\\qquad(2)$\nWe combine the aggregated features $\\overline{f}_i$ and $f_p$, and use a\nMulti-Layer Perceptron (MLP) to determine the weights for\nthe final RGB image feature $f_r \\in \\mathbb{R}^{K}$. The computation is\ngiven by:\n$f_r = \\sum_{i=1}^{n} \\left(\\sigma \\left(\\theta_w \\left(\\overline{f}_i+f_p\\right)\\right)\\right),\\qquad(3)$"}, {"title": "3.2 Feature Fusion Block", "content": "This component leverages transformer encoders [Vaswani et\nal., 2017], intentionally excluding positional embedding due\nto the intrinsic spatial information of the RGB and point cloud\nfeatures. The input tokens $t^i=f_j \\oplus f_r \\in \\mathbb{R}^{N_i \\times 2 K}$ for the\nfusion module are formed by concatenating image feature $f_r$\nto each point feature. A CLS token $t_{\\text {CLS }}$ is also incorporated to\nencapsulate global features, which is pivotal for the model's\ngeneralization. The Feature Fusion Block outputs a global\nfeature $f_{\\text {CLS }} \\in \\mathbb{R}^{2 K}$ and local features $f^i \\in \\mathbb{R}^{N_i \\times 2 K}$ for\neach point, expressed as:\n$\\left\\{f_{\\mathrm{CLS}}, f^i\\right\\}=\\mathcal{F}_L\\left(\\mathcal{F}_{L-1}\\left(\\ldots \\mathcal{F}_1\\left(t_{\\mathrm{CLS}}, t^i\\right) \\ldots\\right)\\right),\\qquad(4)$\nwhere $\\mathcal{F}_i$ is the $i$-th layer of the fusion block."}, {"title": "3.3 Articulation Decoders", "content": "We utilize the global feature $f_{\\text {CLS }}$ to determine the movability\nof the chosen rigid part, represented as a binary variable $\\tau \\in$\n{0,1}, where $\\tau=0$ signifies an immovable part and $\\tau=1$ a\nmovable part. An MLP head dedicated to this task decodes\nthis parameter:\n$\\tau=\\theta_{\\text {mov }}\\left(f_{\\text {CLS }}\\right)\\qquad(5)$\nJoint Parameters. Upon determining the movability of the\ntarget part, we assume a fully closed position as the initial\nstate to estimate the joint parameters. These include the joint\ntype $p \\in \\{0,1\\}$, where $p=0$ indicates a revolute joint and\n$p=1$ a prismatic joint, the joint position $h \\in \\mathbb{R}^3$, orientation\n$u \\in \\mathbb{R}^3$, and the current state $v \\in \\mathbb{R}$. Unlike previous models\nthat omit position predictions for prismatic joints [Jiang et\nal., 2022; Chu et al., 2023], our framework accommodates\nboth revolute and prismatic joints, identifying the centroid of\nthe movable part as the position for the prismatic joint. The\njoint type $p$ is deduced from $f_{\\text {CLS }}$ using an MLP head. For the\njoint parameters, a shared voting module leverages each point\n$p_i \\in \\mathbb{R}^{N_i \\times 3}$ and its corresponding feature $f_i \\in \\mathbb{R}^{N_i \\times 2 K}$\nfor point-wise voting to infer $h_i, u_i$, and $v_i$. The final joint\nparameters are then the mean of these votes:\n$\\begin{aligned}&p=\\theta_{\\text {type }}\\left(f_{\\text {CLS }}\\right), \\\\\n&\\left\\{h_i, u_i, v_i\\right\\}=\\Theta_{\\text {para }}\\left(p^i, f^i\\right), \\\\\n&\\{h, u, v\\}=\\left\\{\\frac{1}{N} \\sum_{i=1}^N h_i, \\frac{1}{N} \\sum_{i=1}^N u_i, \\frac{1}{N} \\sum_{i=1}^N v_i\\right\\}.\\end{aligned}\\qquad(6)$\nPerception Score. To assess the quality of perception re-\nsults, we use an additional MLP head with $f_{\\text {CLS }}$ as the input.\nThis module predicts the likelihood of successful perception\n$\\gamma \\in(0,1)$ in the following manner:\n$\\gamma=\\sigma\\left(\\Theta_{\\text {score }}\\left(f_{\\text {CLS }}\\right)\\right),\\qquad(7)$\nwhere $\\sigma$ is the sigmoid activation function, ensuring $\\gamma$ falls\nbetween 0 and 1. A success threshold of 0.5 is set for binary\ndecision-making within the module.\nLoss Functions. To supervise the predictions of movability\nand joint type, binary cross-entropy loss functions $\\mathcal{L}_{m o v}$ and\n$\\mathcal{L}_{t y p e}$ are utilized. For joint state prediction, an L1 norm loss\nfunction $\\mathcal{L}_{\\text {state }}$ is employed. As the perception score predic-\ntion has been transformed into a binary classification task for\nassessing viewpoint optimality, it is supervised using binary\ncross-entropy loss $\\mathcal{L}_{\\text {score }}$. To penalize the discrepancy in ori-\nentation between the estimated joint $\\hat{\\mathbf{u}}$ and the ground truth $\\mathbf{u}$,\nwhich is a unit vector, the loss $\\mathcal{L}_{\\text {ori }}$ is defined as:\n$\\mathcal{L}_{o r i}=\\frac{1}{N} \\sum_{i=1}^N \\arccos \\left(\\mathbf{u}_i \\hat{\\mathbf{u}}_i\\right),\\qquad(8)$\nThe loss $\\mathcal{L}_{p o s}$ penalizes the distance between the estimated\nprojection point $\\hat{h}$ and the actual joint axis $\\mathbf{u}$:\n$\\mathcal{L}_{p o s}=\\frac{1}{N} \\sum_{i=1}^N\\left|\\mathbf{h}_i-\\hat{h}\\right|_2,\\qquad(9)$\nTraining Steps. In the initial training phase, we pre-train\nthe model parameters using the movability loss $\\mathcal{L}_{m o v}$ along\nwith the corresponding decoding head. This step is crucial\nfor ensuring efficient learning of movability prediction. For\njoint parameter prediction, the model is trained to minimize\nthe differences between the predicted and the actual ground\ntruth values. The cumulative loss for this training phase is\ngiven by:\n$\\mathcal{L}_{p a r a}=\\mathcal{L}_{t y p e}+\\mathcal{L}_{o r i}+\\mathcal{L}_{p o s}+\\mathcal{L}_{s t a t e} \\qquad(10)$\nIn the final training stage, parameters except for the percep-\ntion scoring decoder are fixed, with the training focused on\noptimizing the scoring decoder using the loss $\\mathcal{L}_{s c o r e}$."}, {"title": "3.4 The RL Policy For Active Sensing", "content": "Training of a conditional RL policy, using a DQN ap-\nproach [Mnih et al., 2015], follows the completion of percep-\ntion module training. This policy, aimed at active viewpoint\noptimization, is trained within a simulated environment. Suc-\ncess in a training iteration occurs when the perception score\nexceeds 0.5; over 5 action steps indicate failure. Details of\nthe RL policy are provided in the following section.\nState Space. In the state space, a pre-trained perception\nnetwork processes an RGB image and point cloud from the\ncurrent viewpoint, yielding the global feature $f_{\\text {CLS }}$. Addi-\ntionally, the position of camera is represented as a one-hot\nencoded vector $x \\in \\mathbb{R}^{16}$, indicating one of 16 distinct posi-\ntions. The state input for the RL algorithm is formed by the\ncombination of $f_{\\text {CLS }}$ and this one-hot encoded position $x$.\nAction Space. We established a discrete action space $A \\in$\n$\\{0,1\\}^{16}$ with 16 viewpoints around the object. This action\nspace aligns with the coordinate framework of the object, as\ndepicted in Fig. 4a. To better simulate a real environment,\nsome locations will be randomly set as unreachable.\nReward Design. The per-step rewards $r_{\\text {step }}$ are determined\nby two primary criteria: perception score and point cloud\nquantity variation. To balance these aspects, the step reward\nis defined as:\n$r_{\\text {step }}=\\lambda_s r_{\\text {score }}+\\lambda_n r_{\\text {num }},\\qquad(11)$\nwhere $\\lambda_s$ and $\\lambda_n$ are weight coefficients that adjust the rel-\native importance of $r_{\\text {score }}$ and $r_{\\text {num }}$ in the reward. Here,\n$r_{\\text {score }}^{\\prime}=s-s^{\\prime}$ represents the change in perception scores be-\nfore and after an action, and the point cloud variation reward\nis given by:\n$r_{\\text {num }}=\\frac{n-n^{\\prime}}{n^{\\prime}},\\qquad(12)$\nwhere $n$ denotes the number of point clouds. Additionally, at\nthe end of a round, a positive reward of +10 is assigned for\nsuccessful task completion. Conversely, a negative reward of\n-10 is incurred if the task fails due to exceeding the action\nstep limit."}, {"title": "3.5 Command-Based Point Cloud Manipulation", "content": "We use perceived joint parameters for point cloud manipula-\ntions based on operational commands $C$. The system mod-\nifies the joint state in response to $C$, involving adjustments\nsuch as angle and position changes. Specifically, $C$ specifies\nthe target state for the joint, and we calculate the difference\nbetween this target and the current perceived joint state as the\noperational compensation $\\Delta v=C-v$. Based on $\\Delta v$, joint\norientation $u$, and joint position $h$, we compute a rotation-\ntranslation matrix $M$ to represent the current operation. The\nmatrix is calculated as follows:\n$M=\\left[\\begin{array}{cc}R(\\Delta v, u) & T(\\Delta v, h) \\\\0 & 1\\end{array}\\right],\\qquad(13)$"}, {"title": "4 Experimental Evaluation", "content": "We evaluated perception capabilities of MARS for articulated\nobjects. Quantitative assessment across various object cate-\ngories confirmed accuracy of the MFFP module in estimating\njoint parameters. Integration of active sensing for viewpoint\noptimization significantly enhanced algorithm performance.\nVisual demonstrations of command-based point cloud manip-\nulation and qualitative showcasing of method effectiveness on\nreal-world objects were also conducted."}, {"title": "4.1 Experimental Setup", "content": "Datasets. For evaluation, we utilized the SAPIEN simula-\ntor [Xiang et al., 2020] and PartNet-Mobility dataset [Mo et\nal., 2019], selecting 14 common articulated objects (10 with\nrevolute and 4 with prismatic joints). In the simulator, these\nobjects, with randomized joint states and camera positions,\ngenerated various viewpoint samples (see Fig. 4b). Post mov-\nability prediction module training, immovable parts data was\nremoved, resulting in 10K training, 1K testing, and 1K valida-\ntion samples for each category for perception network train-\ning. Image data was captured at 600 \u00d7 600 resolution using\nan RGB-D camera."}, {"title": "Baselines", "content": "We benchmarked our method against six ap-\nproaches. RPM-Net [Yan et al., 2020] employs recurrent\nneural networks for predicting object motion from point\nclouds. ANCSH [Li et al., 2020] focuses on joint param-\neter estimation in canonical object space. Ditto [Jiang et\nal., 2022] utilizes multi-view for articulated object under-\nstanding. Cart [Chu et al., 2023], the current state-of-the-art\n(SOTA), specializes in joint parameter estimation. To facil-\nitate prediction of the current joint state by Cart, a 'Closed'\ncommand was issued in each evaluation. EPNet [Huang et\nal., 2020], a multimodal fusion approach for 3D object detec-\ntion, was adapted by replacing our MLDM module with its\nLiDAR-guided Image Fusion (LI-Fusion) module, enabling\nEPNet* to estimate joint parameters. Additionally, we in-\ncluded an ablated version of MARS lacking MLDM for com-\nparison. Lastly, we proportionally increased poor viewpoints\ndata for all methods to ensure a fair comparison.\nEvaluation Metrics. Our investigation primarily focuses\non the estimation errors of articulated object joint parameters.\nSpecifically, we measure errors in estimating joint orienta-\ntion, joint position, and the current joint state of the selected\npart relative to its fully closed initial state.\nRL Environment. To train and validate active sensing\nstrategies, a reinforcement learning environment was con-\nstructed. In each training round, an object is randomly im-\nported into the simulation environment, and a camera posi-"}, {"title": "4.2 Main Results", "content": "Comparison of joint parameters estimation. Table 1\nshows the joint parameter estimation results from our quan-\ntitative evaluation, where our method surpasses the SOTA in\nmost categories and closely matches it in the rest. This su-\nperior performance can be attributed to our robust feature\nrepresentation, a result of the multimodal fusion approach\nthat effectively utilizes the rich information in RGB images\nto enhance point cloud features. Additionally, in compar-\nison to EPNet*, our MLDM exhibits enhanced multi-scale\nfeature extraction capabilities, significantly improving articu-\nlated object perception. It's important to note that joint state\nestimation is the most challenging, with the largest errors.\nOur method leads in performance, yet encounters a 3.74\u00b0 er-\nror in revolute joints and 0.07m in prismatic joints. The large\nerror mainly arises from poor observation perspectives in the\nacquired data, as depicted in Figure 4b. Practically, robots\noften encounter such angles, highlighting the importance of\nactive sensing to adjust the viewpoint.\nAblation Studies. Table 1 demonstrates evaluation of the\nMLDM module impact on performance. Across all cate-\ngories, the complete version outperformed the ablated version\nin joint parameter estimation. Notably, even the ablated ver-\nsion marginally surpassed other approaches, highlighting the\neffectiveness of multimodal fusion with RGB data combined\nwith limited point cloud input.\nHarmonized Training for Mixed Joint Types. To address\nthe need for separate models for revolute and prismatic joints\nin current approaches, joint parameter representation was"}, {"title": "4.3 Enhancing Results through Active Sensing", "content": "We compared the performance of our method with and with-\nout the active sensing module. To expedite testing, we pre-\ncollected samples from each articulated object instance at 16"}, {"title": "4.4 Real-world Experiments", "content": "To validate the generalizability of our method in real-world\nsettings, we selected two articulated objects: a door and a\ntable with drawers. As depicted in Fig. 7, we used an In-\ntel RealSense RGB-D camera on a mobile robot to capture\nRGB and point cloud data of these objects. Segmentation\nof the rigid parts was achieved using 3D U-Net [Choy et\nal., 2019]. The perception scoring module assessed the in-\nput viewpoint quality. If the score fell below the threshold,\nactive sensing guided the robot to a new position ID for ad-\nditional data acquisition. Once the perception score exceeded\nthe threshold, the robot formulated its action plan based on\nthe perception results and specified commands. Interaction\npositions between the robot and the object were manually as-\nsigned, leading to the successful manipulation of the target\nobject."}, {"title": "5 Conclusion", "content": "In this paper, we introduced MARS, a multimodal frame-\nwork specifically designed for accurately sensing joint pa-\nrameters of articulated objects. Central to this framework is\nthe MLDM, an innovative approach for adaptive multiscale\nfeature fusion that significantly enhances image feature rep-\nresentation. MARS utilizes a transformer encoder, devoid\nof positional embedding, to effectively integrate RGB fea-\ntures with point cloud data.The significant advancement in\nthis work is the reinforcement learning-based active percep-\ntion strategy, empowering robots to autonomously seek new\nperspectives and substantially improve practical applicability\nin response to inadequate perception. Future research aims to\nenhance MARS by seeking more powerful point cloud repre-\nsentation capabilities and improving algorithmic generaliza-\ntion to cover a broader range of articulated objects."}]}