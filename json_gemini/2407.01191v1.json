{"title": "MARS: Multimodal Active Robotic Sensing for Articulated Characterization", "authors": ["Hongliang Zeng", "Ping Zhang", "Chengjiong Wu", "Jiahua Wang", "Tingyu Ye", "Fang Li"], "abstract": "Precise perception of articulated objects is vital for empowering service robots. Recent studies mainly focus on point cloud, a single-modal approach, often neglecting vital texture and lighting details and assuming ideal conditions like optimal viewpoints, unrepresentative of real-world scenarios. To address these limitations, we introduce MARS, a novel framework for articulated object characterization. It features a multi-modal fusion module utilizing multi-scale RGB features to enhance point cloud features, coupled with reinforcement learning-based active sensing for autonomous optimization of observation viewpoints. In experiments conducted with various articulated object instances from the PartNet-Mobility dataset, our method outperformed current state-of-the-art methods in joint parameter estimation accuracy. Additionally, through active sensing, MARS further reduces errors, demonstrating enhanced efficiency in handling suboptimal viewpoints. Furthermore, our method effectively generalizes to real-world articulated objects, enhancing robot interactions.", "sections": [{"title": "Introduction", "content": "In an era increasingly marked by the integration of robotic assistance into everyday scenarios, conducting research on the precise perception of articulated objects, such as kitchen utensils and personal devices, is of paramount importance. These objects often have intricate joints and multiple moving parts, presenting unique and complex challenges in robotic perception and manipulation.\nBy precisely perceiving the joint parameters of articulated parts, robots enhance their effectiveness in manipulation planning for these objects, considering parameters such as joint position, orientation, and part state. We reviewed previous research [Yi et al., 2018; Yan et al., 2020; Jain et al., 2021; Jiang et al., 2022; Chu et al., 2023] and identified limitations in existing methods. Firstly, Many studies perceive joint characteristics using a single point cloud modality, neglecting valuable information of color and texture data. Secondly, different joints, such as revolute and prismatic, necessitate separate perception networks, which can limit practical applications. Lastly, current work often assumes the availability of an ideal observation viewpoint, disregarding scenarios where the target part is obstructed or invisible. However, robots frequently face suboptimal viewing angles, hindering complete object observation.\nTo address these challenges, MARS implements a multimodal fusion strategy with a novel multi-layer dueling module, efficiently extracting and combining feature maps from diverse convolutional layers into a more effective, information-dense image feature representation. Subsequently, MARS integrates multi-scale RGB features and point cloud features via a transformer encoder-based fusion module. Moreover, MARS standardizes the description of joint parameters, allowing for the perception of various joint types through a single network. Addressing the issues posed by suboptimal viewing angles, MARS features a Reinforcement Learning (RL) driven active sensing strategy. This approach authorizes dynamic camera position adjustment, ensuring acquisition of the most informative viewpoint.\nUltimately, we introduce a comprehensive task flow for robotic perception and manipulation of articulated objects, as shown in Fig. 1. The process starts with the robot selecting"}, {"title": "Related Works", "content": "Articulated Object Characterization. To enhance robotic capabilities in perceiving and manipulating articulated objects, a wealth of simulators [Todorov et al., 2012; Xiang et al., 2020; Szot et al., 2021] and open datasets [Chang et al., 2015; Mo et al., 2019; Wang et al., 2019; Geng et al., 2023] have emerged as essential resources. These tools have facilitated advancements in 3D reconstruction [Li et al., 2020; Bozic et al., 2021; Yang et al., 2021], joint parameter estimation [Jain et al., 2021; Shi et al., 2021; Zeng et al., 2021; Jiang et al., 2022; Chu et al., 2023], and the prediction of interactive positions and trajectories [Mo et al., 2021; Wu et al., 2021; Wang et al., 2022] for articulated objects.\nIn joint parameter estimation, prior work [Yi et al., 2018; Abbatematteo et al., 2019; Shi et al., 2021; Jain et al., 2021; Siarohin et al., 2021; Jiang et al., 2022] has leveraged multi-view observations from ongoing monitoring as visual inputs, capitalizing on the rich visual cues provided by the changing joint states. However, this method complicates data collection, limiting its practicality in real-world robotics. Furthermore, many studies [Yan et al., 2020; Jiang et al., 2022; Chu et al., 2023] rely exclusively on point cloud data, neglecting the valuable features available in RGB imagery that can enhance joint parameter estimation. Our methodology focuses on employing a single RGB image along with point cloud data to achieve accurate perception of joint parameters.\nMultimodal Feature Fusion. In the realm of multimodal feature fusion, especially in combining RGB and point cloud data, current research predominantly focuses on 3D object detection [Ku et al., 2018; Sindagi et al., 2019; Zhu et al., 2021; Piergiovanni et al., 2021; Wu et al., 2022; Zhang et al., 2022]. However, due to differences in input data, these fusion methods often struggle to be directly applicable and effective in joint analysis. A notable example similar to our approach is EPNet [Huang et al., 2020], which enhances point cloud features by integrating image features into the point cloud domain, utilizing both local and global contexts of images. In contrast, our method employs a competitive mechanism to dynamically prioritize the significance of image features across various scales.\nActive Sensing. In practical applications, the challenging task of obtaining optimal imaging angles for robots [Ammirato et al., 2017; Zhao et al., 2022] underscores the need for active sensing. This technique entails dynamically adjusting sensor positions or the manipulation environment to enhance data acquisition, proving essential for certain tasks [Han et al., 2019; Mattamala et al., 2021; Safronov et al., 2021], especially in scenarios where initial views are insufficient due to partial occlusions or limited visibility of articulated objects. In current studies, researchers commonly assume that high-quality images and point clouds can be obtained from an ideal viewpoint [Jain et al., 2021; Yang et al., 2021; Jiang et al., 2022; Chu et al., 2023], but such assumptions frequently fail to align with the reality. Our approach successfully adapts to complex real-world application scenarios by incorporating active sensing techniques."}, {"title": "Method", "content": "We introduce MARS, a framework designed for estimating joint parameters in articulated objects. As depicted in Fig. 2, MARS consists of two primary components: Multimodal Feature Fusion Perception (MFFP) and Active Sensing (AS).\nThe MFFP component of MARS utilizes ResNet18 [He et al., 2016] and PointNet++ [Qi et al., 2017] as backbone networks to efficiently extract features from the input RGB image and point cloud data. The Multi-Layer Dueling Module (MLDM) is designed to strategically extract and weigh image features across different scales. Following this, image and point cloud features are merged at the feature level via a specialized fusion module. A decoding module then processes these integrated features to produce outputs for joint parameters and a perception score, reflecting the efficacy of the current viewpoint.\nThe AS module operates based on the perception score and a set threshold. If the score is below this threshold, it triggers a viewpoint change. A new observation position is determined from the action space, followed by a repeated perception process. This approach enables real-time adjustments for optimal data acquisition and improved joint parameter estimation in complex or obstructed scenarios."}, {"title": "MLDM", "content": "Employing ResNet, we capturing a range of articulated object RGB features from local to global, as depicted in Fig. 3a. For each feature map $f_i \\in \\mathbb{R}^{C_i \\times H_i \\times W_i}$ with $C_i$ channels and spatial dimensions $H_i \\times W_i$, and for point cloud features"}, {"title": "Feature Fusion Block", "content": "This component leverages transformer encoders [Vaswani et al., 2017], intentionally excluding positional embedding due to the intrinsic spatial information of the RGB and point cloud features. The input tokens $t_i = f^p_i \\oplus f_r \\in \\mathbb{R}^{N_i \\times 2K}$ for the fusion module are formed by concatenating image feature $f_r$ to each point feature. A CLS token $t_{CLS}$ is also incorporated to encapsulate global features, which is pivotal for the model\u2019s generalization. The Feature Fusion Block outputs a global feature $f_{CLS} \\in \\mathbb{R}^{2K}$ and local features $f^i \\in \\mathbb{R}^{N_i \\times 2K}$ for each point, expressed as:\n$\\left\\{f_{CLS}, f^{i}\\right\\} = F_L(F_{L-1}(...F_1(t_{CLS}, t^0)...)),$ (4)\nwhere $F_l$ is the $l$-th layer of the fusion block."}, {"title": "Articulation Decoders", "content": "We utilize the global feature $f_{CLS}$ to determine the movability of the chosen rigid part, represented as a binary variable $\\tau \\in {0, 1}$, where $\\tau = 0$ signifies an immovable part and $\\tau = 1$ a movable part. An MLP head dedicated to this task decodes this parameter:\n$\\tau = \\theta_{mov} (f_{CLS})$ (5)\nJoint Parameters. Upon determining the movability of the target part, we assume a fully closed position as the initial state to estimate the joint parameters. These include the joint type $\\rho \\in {0, 1}$, where $\\rho = 0$ indicates a revolute joint and $\\rho = 1$ a prismatic joint, the joint position $h \\in \\mathbb{R}^3$, orientation $u \\in \\mathbb{R}^3$, and the current state $v \\in \\mathbb{R}$. Unlike previous models that omit position predictions for prismatic joints [Jiang et al., 2022; Chu et al., 2023], our framework accommodates both revolute and prismatic joints, identifying the centroid of the movable part as the position for the prismatic joint. The"}, {"title": "The RL Policy For Active Sensing", "content": "Training of a conditional RL policy, using a DQN approach [Mnih et al., 2015], follows the completion of perception module training. This policy, aimed at active viewpoint optimization, is trained within a simulated environment. Success in a training iteration occurs when the perception score exceeds 0.5; over 5 action steps indicate failure. Details of the RL policy are provided in the following section."}, {"title": "Command-Based Point Cloud Manipulation", "content": "We use perceived joint parameters for point cloud manipulations based on operational commands $C$. The system modifies the joint state in response to $C$, involving adjustments such as angle and position changes. Specifically, $C$ specifies the target state for the joint, and we calculate the difference between this target and the current perceived joint state as the operational compensation $\\Delta v = C - v$. Based on $\\Delta v$, joint orientation $u$, and joint position $h$, we compute a rotation-translation matrix $M$ to represent the current operation. The matrix is calculated as follows:\n$M =\\begin{bmatrix}R(\\Delta v, u) & T(\\Delta v, h)\n\\\\0 & 1\\end{bmatrix}$ (13)"}, {"title": "Experimental Evaluation", "content": "We evaluated perception capabilities of MARS for articulated objects. Quantitative assessment across various object categories confirmed accuracy of the MFFP module in estimating joint parameters. Integration of active sensing for viewpoint optimization significantly enhanced algorithm performance. Visual demonstrations of command-based point cloud manipulation and qualitative showcasing of method effectiveness on real-world objects were also conducted."}, {"title": "Experimental Setup", "content": "Datasets. For evaluation, we utilized the SAPIEN simulator [Xiang et al., 2020] and PartNet-Mobility dataset [Mo et al., 2019], selecting 14 common articulated objects (10 with revolute and 4 with prismatic joints). In the simulator, these objects, with randomized joint states and camera positions, generated various viewpoint samples (see Fig. 4b). Post movability prediction module training, immovable parts data was removed, resulting in 10K training, 1K testing, and 1K validation samples for each category for perception network training. Image data was captured at 600 \u00d7 600 resolution using an RGB-D camera.\nBaselines. We benchmarked our method against six approaches. RPM-Net [Yan et al., 2020] employs recurrent neural networks for predicting object motion from point clouds. ANCSH [Li et al., 2020] focuses on joint parameter estimation in canonical object space. Ditto [Jiang et al., 2022] utilizes multi-view for articulated object understanding. Cart [Chu et al., 2023], the current state-of-the-art (SOTA), specializes in joint parameter estimation. To facilitate prediction of the current joint state by Cart, a 'Closed' command was issued in each evaluation. EPNet [Huang et al., 2020], a multimodal fusion approach for 3D object detection, was adapted by replacing our MLDM module with its LiDAR-guided Image Fusion (LI-Fusion) module, enabling EPNet* to estimate joint parameters. Additionally, we included an ablated version of MARS lacking MLDM for comparison. Lastly, we proportionally increased poor viewpoints data for all methods to ensure a fair comparison.\nEvaluation Metrics. Our investigation primarily focuses on the estimation errors of articulated object joint parameters. Specifically, we measure errors in estimating joint orientation, joint position, and the current joint state of the selected part relative to its fully closed initial state.\nRL Environment. To train and validate active sensing strategies, a reinforcement learning environment was constructed. In each training round, an object is randomly imported into the simulation environment, and a camera posi-"}, {"title": "Main Results", "content": "Comparison of joint parameters estimation. Table 1 shows the joint parameter estimation results from our quantitative evaluation, where our method surpasses the SOTA in most categories and closely matches it in the rest. This superior performance can be attributed to our robust feature representation, a result of the multimodal fusion approach that effectively utilizes the rich information in RGB images to enhance point cloud features. Additionally, in comparison to EPNet*, our MLDM exhibits enhanced multi-scale feature extraction capabilities, significantly improving articulated object perception. It's important to note that joint state estimation is the most challenging, with the largest errors. Our method leads in performance, yet encounters a 3.74\u00b0 error in revolute joints and 0.07m in prismatic joints. The large error mainly arises from poor observation perspectives in the acquired data, as depicted in Figure 4b. Practically, robots often encounter such angles, highlighting the importance of active sensing to adjust the viewpoint.\nAblation Studies. Table 1 demonstrates evaluation of the MLDM module impact on performance. Across all categories, the complete version outperformed the ablated version in joint parameter estimation. Notably, even the ablated version marginally surpassed other approaches, highlighting the effectiveness of multimodal fusion with RGB data combined with limited point cloud input.\nHarmonized Training for Mixed Joint Types. To address the need for separate models for revolute and prismatic joints in current approaches, joint parameter representation was"}, {"title": "Enhancing Results through Active Sensing", "content": "We compared the performance of our method with and without the active sensing module. To expedite testing, we pre-collected samples from each articulated object instance at 16"}, {"title": "Real-world Experiments", "content": "To validate the generalizability of our method in real-world settings, we selected two articulated objects: a door and a table with drawers. As depicted in Fig. 7, we used an Intel RealSense RGB-D camera on a mobile robot to capture RGB and point cloud data of these objects. Segmentation of the rigid parts was achieved using 3D U-Net [Choy et al., 2019]. The perception scoring module assessed the input viewpoint quality. If the score fell below the threshold, active sensing guided the robot to a new position ID for additional data acquisition. Once the perception score exceeded the threshold, the robot formulated its action plan based on the perception results and specified commands. Interaction positions between the robot and the object were manually assigned, leading to the successful manipulation of the target object."}, {"title": "Conclusion", "content": "In this paper, we introduced MARS, a multimodal framework specifically designed for accurately sensing joint parameters of articulated objects. Central to this framework is the MLDM, an innovative approach for adaptive multiscale feature fusion that significantly enhances image feature representation. MARS utilizes a transformer encoder, devoid of positional embedding, to effectively integrate RGB features with point cloud data.The significant advancement in this work is the reinforcement learning-based active perception strategy, empowering robots to autonomously seek new perspectives and substantially improve practical applicability in response to inadequate perception. Future research aims to enhance MARS by seeking more powerful point cloud representation capabilities and improving algorithmic generalization to cover a broader range of articulated objects."}]}