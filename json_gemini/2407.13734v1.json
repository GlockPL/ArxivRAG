[{"title": "Understanding Reinforcement Learning-Based Fine-Tuning of Diffusion Models: A Tutorial and Review", "authors": ["Masatoshi Uehara", "Yulai Zhao", "Tommaso Biancalani", "Sergey Levine"], "abstract": "This tutorial provides a comprehensive survey of methods for fine-tuning diffusion models to optimize downstream reward functions. While diffusion models are widely known to provide excellent generative modeling capability, practical applications in domains such as biology require generating samples that maximize some desired metric (e.g., translation efficiency in RNA, docking score in molecules, stability in protein). In these cases, the diffusion model can be optimized not only to generate realistic samples but also to maximize the measure of interest explicitly. Such methods are based on concepts from reinforcement learning (RL). We explain the application of various RL algorithms, including PPO, differentiable optimization, reward-weighted MLE, value-weighted sampling, and path consistency learning, tailored specifically for fine-tuning diffusion models. We aim to explore fundamental aspects such as the strengths and limitations of different RL-based fine-tuning algorithms across various scenarios, the benefits of RL-based fine-tuning compared to non-RL-based approaches, and the formal objectives of RL-based fine-tuning (target distributions). Additionally, we aim to examine their connections with related topics such as classifier guidance, Gflownets, flow-based diffusion models, path integral control theory, and sampling from unnormalized distributions such as MCMC. The code of this tutorial is available at https://github.com/masa-ue/RLfinetuning_Diffusion_Bioseq.", "sections": [{"title": "Introduction", "content": "Diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2020) are widely recognized as powerful tools for generative modeling. They are able to accurately model complex distributions by closely emulating the characteristics of the training data. There are many applications of diffusion models in various fields, including computer vision (Podell et al., 2023), natural language processing (Austin et al., 2021), biology (Avdeyev et al., 2023; Stark et al., 2024; Li et al.,\n2023), chemistry (Jo et al., 2022; Xu et al., 2022; Hoogeboom et al., 2022), and biology (Avdeyev et al., 2023; Stark et al., 2024; Campbell et al., 2024).\nWhile diffusion models exhibit significant power in capturing the training data distribution, there's often a need to customize these models for particular downstream reward functions. For instance, in computer vision, Stable Diffusion (Rombach et al., 2022) serves as a strong backbone pre-trained model. However, we may want to fine-tune it further by optimizing downstream reward functions such as aesthetic scores or human-alignment scores (Black et al., 2023; Fan et al., 2023). Similarly, in fields such as biology and chemistry, various sophisticated diffusion models have been developed for DNA, RNA, protein sequences, and molecules, effectively modeling biological and chemical spaces. Nonetheless, biologists and chemists typically aim to optimize specific downstream objectives such as cell-specific expression in DNA sequences (Gosai et al., 2023; Lal et al., 2024; Sarkar et al., 2024), translational efficiency/stability of RNA sequences (Castillo-Hair and Seelig, 2021; Agarwal and Kelley, 2022), stability/bioactivity of protein sequence (Frey et al., 2023; Widatalla et al., 2024) or QED/SA scores of molecules (Zhou et al., 2019).\nTo achieve this goal, numerous algorithms have been proposed for fine-tuning diffusion models via reinforcement learning (RL) (e.g., Black et al. (2023); Fan et al. (2023); Clark et al. (2023);\nPrabhudesai et al. (2023); Uehara et al. (2024)), aiming to optimize downstream reward functions. RL is a machine learning paradigm where agents learn to make sequential decisions to maximize reward signals (Sutton and Barto, 2018; Agarwal et al., 2019). In our context, RL naturally emerges as a suitable approach due to the sequential structure inherent in diffusion models, where each time step involves a \u201cdecision\u201d corresponding to how the sample is denoised at that step. This tutorial aims to review recent works for readers interested in understanding the fundamentals of RL-based fine-tuning from a holistic perspective, including the advantages of RL-based fine-tuning over non-RL approaches, the pros and cons of different RL-based fine-tuning algorithms, the formalized goal of RL-based fine-tuning, and its connections with related topics such as classifier guidance."}, {"title": "", "content": "The content of this tutorial is primarily divided into three parts. In addition, as an implementation example, we also release the code that employs RL-based fine-tuning for guided biological sequences (DNA/RNA) generation at https://github.com/masa-ue/RLfinetuning_Diffusion_Bioseq.\n1.  We aim to provide a comprehensive overview of current algorithms. Notably, given the sequential nature of diffusion models, we can naturally frame fine-tuning as a reinforcement learning (RL) problem within Markov Decision Processes (MDPs), as detailed in Section 3 and 4. Therefore, we can employ any off-the-shelf RL algorithms such as PPO (Schulman et al., 2017), differentiable optimization (direct reward backpropagation), weighted MLE (Peters et al., 2010; Peng et al., 2019), value-weighted sampling (close to classifier guidance in Dhariwal and Nichol (2021)), and path consistency learning (Nachum et al., 2017). We discuss these algorithms in detail in Section 4.2 and 6. Instead of merely outlining each algorithm, we aim to present both their advantages and disadvantages so readers can select the most suitable algorithms for their specific purposes.\n2.  We categorize various fine-tuning scenarios based on how reward feedback is acquired in Section 7. This distinction is pivotal for practical algorithm design. For example, if we can access accurate reward functions, computational efficiency would become our primary focus. However, in cases where reward functions are unknown, it is essential to learn them from data with reward feedback, leading us to take feedback efficiency and distributional shift into consideration as well. Specifically, when reward functions need to be learned from static offline data without any online interactions, we must address the issue of overoptimization, where fine-tuned models are misled by out-of-distribution samples, and generate samples with low genuine rewards. This is crucial because, in an offline scenario, the coverage of offline data distribution with feedback is limited; hence, the out-of-distribution region could be extensive (Uehara et al., 2024).\n3.  We provide a detailed discussion on the relationship between RL-based fine-tuning methods and closely related methods in the literature, such as classifier guidance (Dhariwal and Nichol, 2021) in Section 8, flow-based diffusion models (Liu et al., 2022; Lipman et al., 2023; Tong et al., 2023) in Section 9, sampling from unnormalized distributions (Zhang and Chen, 2021) in Section 10, Gflownets (Bengio et al., 2023) in Section 6.3, and path integral control theory (Theodorou et al., 2010; Williams et al., 2017; Kazim et al., 2024) in Section 6.2.3. We summarize the key messages as follows.\n    \u2022 Section 6.3: The losses used in Gflownets are fundamentally equivalent to those derived from a specific RL algorithm called path consistency learning.\n    \u2022 Section 8: Classifier guidance employed in conditional generation is regarded as a specific RL-based fine-tuning method, which we call value-weighted sampling. As formalized in Zhao et al. (2024), this observation indicates that any off-the-shelf RL-based fine-tuning algorithms (e.g., PPO and differentiable optimization) can be applied to conditional generation.\n    \u2022 Section 10: Sampling from unnormalized distributions, often referred to as Gibbs distributions, is an important and challenging problem in diverse domains. While MCMC methods are traditionally used for this task, recognizing its similarity to the objectives of RL-based fine-tuning suggests that off-the-shelf RL algorithms can also effectively address the challenge of sampling from unnormalized distributions."}, {"title": "Preliminaries", "content": "In this section, we outline the fundamentals of diffusion models and elucidate the objective of fine-tuning them."}, {"title": "Diffusion Models", "content": "We present an overview of denoising diffusion probabilistic models (DDPM) (Ho et al., 2020). For more details, refer to Yang et al. (2023); Cao et al. (2024); Chen et al. (2024); Tang and Zhao (2024).\nIn diffusion models, the objective is to develop a deep generative model that accurately captures the true data distribution. Specifically, denoting the data distribution by $p_{\\text{pre}} \\in \\Delta(\\mathcal{X})$ where $\\mathcal{X}$ is an input space, a DDPM aims to approximate $P_{\\text{pre}}$ using a parametric model structured as\n$p(x_{0}; \\theta) = \\int p(x_{0:T}; \\theta) \\mathrm{d} x_{1:T}, \\text{ where } p(x_{0:T}; \\theta) = p_{T+1}(x_{T}; \\theta) \\prod_{t=T}^{1} p_{t}(x_{t-1}|x_{t}; \\theta)$\nWhen $\\mathcal{X}$ is an Euclidean space (in $\\mathbb{R}^{d}$), the forward process is modeled as the following dynamics:\n$p_{T+1}(x_{T}) = \\mathcal{N}(0, I), p_{t}(x_{t-1}|x_{t}; \\theta) = \\mathcal{N}(\\mu(x_{t}, t; \\theta), \\sigma^{2}(t) \\times I)$,\nwhere $\\mathcal{N}(\\cdot, \\cdot)$ denotes a normal distribution, $I$ is an identity matrix and $\\mu : \\mathbb{R}^{d} \\times [0, T] \\rightarrow \\mathbb{R}^{d}$. In DDPMs, we aim to obtain a set of policies (i.e., denoising process) $\\{p_{t}\\}_{t=T+1}^{1}, p_{t} : \\mathcal{X} \\rightarrow \\triangle(\\mathcal{X})$ such that $p(x_{0}; \\theta) \\approx P_{\\text{pre}}(x_{0})$. Indeed, by optimizing the variational bound on the negative log-likelihood, we can derive such a set of policies. For more details, refer to Section 1.1.1.\nHereafter, we consider a situation where we have a pre-trained diffusion model that is already trained on a large dataset, such that the model can accurately capture the underlying data distribution. We refer to the pre-trained policies as $\\{p^{\\text{pre}}(\\cdot|\\cdot)\\}_{t=T+1}^{1}$, and to the marginal distribution at $t=0$ induced by the pre-trained diffusion model as $p^{\\text{pre}}$. In other words,\n$p^{\\text{pre}} (x_{0}) = \\int \\left[\\prod_{t=T}^{1} p_{t}^{\\text{pre}} (x_{t-1}|x_{t})\\right] p_{T+1}^{\\text{pre}} (x_{T}) dx_{1:T}.$\nRemark 1 (Non-Euclidean space). For simplicity, we typically assume that the domain space is Euclidean. However, we can easily extend most of the discussion to a more general space, such as a Riemannian manifold (De Bortoli et al., 2022) or discrete space (Austin et al., 2021; Campbell et al., 2022; Benton et al., 2024; Lou et al., 2023)."}, {"title": "Score-Based Diffusion Models (Optional)", "content": "We briefly discuss how to train diffusion models in a continuous-time framework (Song et al., 2021). In our tutorial, this section is mainly used to discuss several algorithms (e.g., value-weighted sampling in Section 6.2) and their relationship with flow-based diffusion models (Section 9) later. Therefore, readers may skip it based on their individual needs.\nThe training process of diffusion models can be summarized as follows. Our objective is to train a sequential mapping from a known noise distribution to a data distribution, formalized through a stochastic differential equation (SDE). Firstly, we define a forward (fixed) SDE that maps from the data distribution to the noise distribution. Then, a time-reversal SDE is expressed as an SDE, which includes the (unknown) score function. Now, by learning this unknown score function from the training data, the time-reversal SDE can be utilized as a generative model. Here are the details.\nForward and time-reversal SDE. Firstly, we introduce a forward (a.k.a., reference) Stochastic differential equations (SDE) from 0 to T. A common choice is a variance-preserving (VP) process:\n$t \\in [0,T]; dx_{t} = -0.5 x_{t} dt + dw_{t}, x_{0} \\sim p_{\\text{pre}}(x).$\nwhere $dw_{t}$ represents standard Brownian motion. Here are two crucial observations:\n    \u2022 As T approaches $\\infty$, the limiting distribution is $\\mathcal{N}(0, I)$.\n    \u2022 The time-reversal SDE (Anderson, 1982), which preserves the marginal distribution, is expressed as follows:\n$t \\in [0, T]; dz_{t} = [0.5 z_{t} + \\nabla \\log q_{T-t}(z_{t})] dt + dw_{t}.$\nHere, $q_{t} \\in \\triangle(\\mathbb{R}^{d})$ denotes the marginal distribution at time $t$ induced by the reference SDE (1). Notably, the marginal distribution of $z_{T-t}$ is the same as the one of $x_{t}$ induced by the reference SDE.\nThese observations suggest that with sufficiently large $T$, starting from $\\mathcal{N}(0, I)$ and following the time-reversal SDE (2), we are able to sample from the data distribution (i.e., $p_{\\text{pre}}$) at the terminal time point $T$."}, {"title": "Fine-Tuning Diffusion Models with RL", "content": "Importantly, our focus on RL-based fine-tuning distinguishes itself from the standard fine-tuning methods. Standard fine-tuning typically involves scenarios where we have pre-trained models (e.g., diffusion models) and new training data $\\{x^{(i)}, y^{(i)}\\}$. In such cases, the common approach for fine-tuning is to retrain diffusion models with the new training data using the same loss function employed during pre-training. In sharp contrast, RL-based fine-tuning directly employs the downstream reward functions as the primary optimization objectives, making the loss functions different from those used in pre-training.\nHereafter, we start with a concise overview of RL-based fine-tuning. Then, before delving into specifics, we discuss simpler non-RL alternatives to provide motivation for adopting RL-based fine-tuning."}, {"title": "Brief Overview: Fine-tuning with RL", "content": "In this article, we explore the fine-tuning of pre-trained diffusion models to optimize downstream reward functions $r : \\mathbb{R}^{d} \\rightarrow \\mathbb{R}$. In domains such as images, these backbone diffusion models to be fine-tuned include Stable Diffusion (Rombach et al., 2022), while the reward functions are aesthetic scores and alignment scores (Clark et al., 2023; Black et al., 2023; Fan et al., 2023). More examples are detailed in the introduction. These rewards are often unknown, necessitating learning from data with feedback: $\\{x^{(i)}, r(x^{(i)})\\}$. We will explore this aspect further in Section 7. Until then, we assume $r$ is known.\nNow, readers may wonder about the objectives we aim to achieve during the fine-tuning process. A natural approach is to define the optimization problem:\n$\\underset{\\Phi \\in \\Delta(\\mathcal{X})}{\\text{argmax}} E_{x \\sim q}[r(x)]$\nwhere $q$ is initialized with a pre-trained diffusion model $p_{\\text{pre}} \\in \\Delta(\\mathcal{X})$. In this tutorial, we will detail the procedure of solving (8) with RL in the upcoming sections. In essence, we leverage the fact that diffusion models are formulated as a sequential decision-making problem, where each decision corresponds to how samples are denoised.\nAlthough the above objective function (8) is reasonable, the resulting distribution might deviate too much from the pre-trained diffusion model. To circumvent this issue, a natural way is to add penalization against pre-trained diffusion models. Then, the target distribution is defined as:\n$\\underset{\\Phi \\in \\Delta(\\mathcal{X})}{\\text{argmax}} E_{x \\sim q}[r(x)] - \\alpha KL(q||p_{\\text{pre}}).$\nNotably, (9) reduces to the following distribution:\n$p_{r}( \\cdot ) := \\frac{\\exp(r(\\cdot)/\\alpha) p_{\\text{pre}}( \\cdot )}{\\int \\exp(r(x)/\\alpha) p_{\\text{pre}}(x) dx}.$\nHere, the first term in (9) corresponds to the mean reward, which we want to optimize in the fine-tuning process. The second term in (10) serves as a penalty term, indicating the deviation of $q$ from the pre-trained model. The parameter $\\alpha$ controls the strength of this regularization term. The proper choice of $\\alpha$ depends on the task we are interested in."}, {"title": "Motivation for Using RL over Non-RL Alternatives", "content": "To achieve our goal of maximizing downstream reward functions with diffusion models, readers may question whether alternative approaches can be employed apart from RL. Here, we investigate these potential alternatives and explain why RL approaches may offer advantages over them.\nRejection sampling. One approach involves generating multiple samples from pre-trained diffu-sion models and selecting only those with high rewards. This method, called rejection sampling, operates without needing fine-tuning. However, rejection sampling is effective primarily when the pre-trained model already has a high probability of producing high-reward samples. It resembles sampling from a prior distribution to obtain posterior samples (in this case, high-reward points). This approach works efficiently when the posterior closely matches the prior but can become highly inefficient otherwise. In contrast, by explicitly updating weight in diffusion models, RL-based fine-tuning allows us to obtain these high-reward samples, which are seldom generated by pre-trained models.\nConditional diffusion models (classifier-free guidance). In conditional generative models, the general goal is to sample from $p(x|c)$, where $x$ is the output and $c$ denotes the conditioning variable. For example, in text-to-language diffusion models, $c$ is a text, and $x$ is the generated image. Similarly, in the context of protein engineering for addressing inverse folding problems, models often define $c$ as the protein backbone structure and $x$ as the corresponding amino acid sequence. Here, using the training data $\\{c^{(i)}, x^{(i)}\\}$, the model is trained by using the loss function:\n$\\underset{\\theta}{\\text{argmin}} \\sum_{i=1}^{n} E_{t \\sim \\text{Uni}([0,T]), \\epsilon \\sim \\mathcal{N}(0,I), x_{t}^{(i)} \\sim \\mu_{t}^{C^{(i)}} + \\epsilon \\sigma_{t}}[ ||\\epsilon - \\epsilon_{\\theta}(x_{t}, c^{(i)}, t)||^{2}],$\nwhere the denoising function $\\epsilon_{\\theta}$ additionally receives the conditioning information $c^{(i)}$ as input. In practice, a variety of improvements such as classifier-free guidance (Ho and Salimans, 2022) can further improve the model's ability to learn the conditional distribution $p(x|c)$.\nThese conditional generative models can be used to optimize down-stream rewards by condi-tioning on the reward values, then sampling $x$ conditioned on high reward values (Krishnamoorthy et al., 2023; Yuan et al., 2023). While this method is, in principle, capable of generating plausible $x$ values across a range of reward levels within the training data distribution, it is not the most effective optimization strategy. This is primarily because high-reward inputs frequently reside in the tails of the training distribution or even beyond it. Consequently, this method may not effectively generate high-reward samples that lie outside the training data distribution. In contrast, RL-based fine-tuning has the capability to generate samples with higher rewards beyond the training data. This is achieved by explicitly maximizing reward models learned from the training data and leveraging their extrapolative capabilities of reward models, as theoretically formalized and empirically observed in Uehara et al. (2024).\nReward-weighted training. Another alternative approach is to use a reward-weighted version of the standard training loss for diffusion models. Suppose that we have data $\\{x^{(i)}, r(x^{(i)})\\}$. Then, after learning a reward $\\hat{r} : \\mathcal{X} \\rightarrow \\mathbb{R}$ with regression from the data, to achieve our goal, it looks"}, {"title": "", "content": "natural to use a reward-weighted version of the training loss for diffusion models (5), i.e.,\n$\\underset{\\theta}{\\text{argmin}} \\sum_{i=1}^{n} E_{t \\sim \\text{Uni}([0,T]), x_{t}^{(i)} \\sim \\mu_{t} + \\epsilon \\sigma_{t}, \\epsilon \\sim \\mathcal{N}(0,I)}[\\hat{r}(x^{(i)}) ||\\epsilon - \\epsilon_{\\theta}(x_{t}, t)||^{2}].$\nThere are two potential drawbacks to this approach. First, in practice, it may struggle to generate samples with higher rewards beyond the training data. As we will explain later, many RL algorithms are more directly focused on optimizing reward functions, which are expected to excel in obtaining samples with high rewards not observed in the original data, as empirically observed in Black et al. (2023). Second, when fine-tuning a conditional diffusion model $p(x|c)$, the alternative approach here requires a pair of $\\{c^{(i)}, x^{(i)}\\}$ during fine-tuning to ensure the validity of the loss function. When we only have data $\\{x^{(i)}, r(x^{(i)})\\}$ but not $\\{c^{(i)}, x^{(i)}, \\hat{r}(x^{(i)})\\}$, this implies that we might need to solve an inverse problem from $x$ to $c$, which can often be challenging. In contrast, in these scenarios, RL algorithms, which we will introduce later, can operate without needing such pairs $\\{c^{(i)}, x^{(i)}\\}$, as long as we have learned reward functions $r$.\nFinally, it should be noted that reward-weighted training technically falls under the broader category of RL methods. It shares a close connection with \u201creward-weighted MLE\u201d introduced in Section 6.1, as discussed later. Employing this reward-weighted MLE helps address the second concern of \u201creward-weighted training\u201d mentioned earlier."}, {"title": "Brief Overview of Entropy-Regularized MDPs", "content": "In this tutorial, we explain how fine-tuning diffusion models can be naturally formulated as an RL problem in entropy-regularized MDPs. This perspective is natural because RL involves sequential decision-making, and a diffusion model is formulated as a sequential problem where each denoising step is a decision-making process. To connect diffusion models with RL, we begin with a concise overview of RL in standard entropy-regularized MDPs (Haarnoja et al., 2017; Neu et al., 2017;\nGeist et al., 2019; Schulman et al., 2017)."}, {"title": "MDPS", "content": "An MDP is defined as follows: $\\{\\mathcal{S}, \\mathcal{A}, \\{P_{\\text{tra}}\\}_{\\tau=0}^{T}, \\{r_{\\tau}\\}_{\\tau=0}^{T}, p_{0}\\}$ where $\\mathcal{S}$ is the state space, $\\mathcal{A}$ is the action space, $P_{\\text{tra}}$ is a transition dynamic mapping: $\\mathcal{S} \\times \\mathcal{A} \\rightarrow \\triangle(\\mathcal{S})$, $r_{\\tau} : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$ denotes reward received at $\\tau$ and $p_{0}$ is an initial distribution over $\\mathcal{S}$. A policy $\\pi_{\\tau} : \\mathcal{S} \\rightarrow \\triangle(\\mathcal{A})$ is a map from any state $s \\in \\mathcal{S}$ to the distribution over actions. The standard goal in RL is to solve\n$\\underset{\\{\\pi_{\\tau}\\}}{\\text{argmax}} E_{\\{\\pi_{\\tau}\\}} \\left[ \\sum_{\\tau=0}^{T} r_{\\tau}(s_{\\tau}, a_{\\tau}) \\right]$\nwhere $E_{\\{\\pi_{\\tau}\\}}[\\cdot]$ is the expectation induced both policy $\\pi$ and the transition dynamics as follows:\n$s_{0} \\sim p_{0}, a_{0} \\sim \\pi_{0}(s_{0}), s_{1} \\sim P_{\\text{tra}}(\\cdot|s_{0}, a_{0}), \\dots$ As we will soon detail in the next section (Section 3), diffusion models can naturally be framed as MDPs as each policy corresponds to a denoising process in diffusion models."}, {"title": "Key Concepts: Soft Q-functions, Soft Bellman Equations.", "content": "The crucial question in RL is how to devise algorithms that effectively solve the optimization problem (12). These algorithms are later used as fine-tuning algorithms of diffusion models. To see these algorithms, we rely on several critical concepts in entropy-regularized MDPs. Specifically, soft-optimal policies (i.e., solutions to (12)) can be expressed analytically as a blend of soft Q-functions and reference policies. Furthermore, these soft Q-functions are defined as solutions to equations known as soft Bellman equations. We elaborate on these foundational concepts below.\nSoft Q-functions and soft optimal policies. Soft optimal policies are expressed as a blend of soft Q-functions and reference policies. To see it, we define the soft Q-function as follows:\n$q_{t}(s_{t}, a_{t}) = E_{\\{\\pi_{\\tau}'\\}} \\left[ \\sum_{k=t}^{T} r_{k}(s_{k}, a_{k}) - \\alpha KL(\\pi_{k+1}(\\cdot|s_{k+1})||\\pi_{k+1}'(\\cdot|s_{k+1}))|s_{t}, a_{t} \\right]$\nThen, by comparing (13) and (12), we clearly have\n$\\pi_{t}^{*} = \\underset{\\pi \\in [\\mathcal{X} \\rightarrow \\triangle(\\mathcal{A})]}{\\text{argmax}} E_{a_{t} \\sim \\pi(s_{t})} [q_{t}(s_{t}, a_{t}) - \\alpha KL(\\pi(\\cdot|s_{t})||\\pi_{t}'(\\cdot | s_{t})|s_{t}].$\nHence, by calculating the above explicitly, a soft optimal policy in (12) is described as follows:\n$\\pi_{t}^{*}( \\cdot |s) \\propto \\frac{\\exp(q_{t}(s, \\cdot)/\\alpha)\\pi_{t}'(\\cdot|s)}{\\int \\exp(q_{t}(s,a)/\\alpha)\\pi_{t}'(a|s) da}$"}, {"title": "Fine-Tuning Diffusion Models with RL in Entropy Regularized MDPs", "content": "In this section, as done in Fan et al. (2023); Black et al. (2023); Uehara et al. (2024), we illustrate how fine-tuning can be formulated as an RL problem in soft-entropy regularized MDPs, where each"}, {"title": "", "content": "denoising step of diffusion models corresponds to a policy in RL. Finally, we outline a specific RL problem of interest in our context.\nTo cast fine-tuning diffusion models as an RL problem, we start with defining the following MDP:\n    \u2022 The state space $\\mathcal{S}$ and action space $\\mathcal{A}$ correspond to the input space $\\mathcal{X}$.\n    \u2022 The transition dynamics at time $t$ (i.e., $P_{t}$) is an identity map $\\delta(s_{t+1} = a_{t})$.\n    \u2022 The reward at time $t \\in [0, \\dots, T]$ (i.e., $r_{t}$) is provided only at $T$ as $r$ (down-stream reward function); but 0 at other time steps.\n    \u2022 The policy at time $t$ (i.e, $\\pi_{t}$) corresponds to $p_{T+1-t} : \\mathcal{X} \\rightarrow \\triangle(\\mathcal{X})$.\n    \u2022 The initial distribution at time 0 corresponds to $p_{T+1} \\in \\Delta(\\mathcal{X})$. With slight abuse of notation, we often denote it by $p_{T+1}(\\cdot|\\cdot)$, while this is just $p_{T+1}(\\cdot)$.\n    \u2022 The reference policy at $t$ (i.e., $\\pi_{t}'$) corresponds to a denoising process in the pre-trained model $p_{T+1-t}^{\\text{mpre}}$.\nWe list several things to note.\n    \u2022 We reverse the time-evolving process to adhere to the standard notation in diffusion models, i.e., from $t = T$ to $t = 0$. Hence, $s_{t}$ in standard MDPs corresponds to $x_{T+1-t}$ in diffusion models.\n    \u2022 In our context, unlike standard RL scenarios, the transition dynamics are known.\nKey RL Problem. Now, by reformulating the original objective of standard RL into our contexts, the objective function in (12) reduces to the following:\n$\\{p_{t}\\}_{t} = \\underset{\\{p_{t} \\in [\\mathbb{R}^{d} \\rightarrow \\triangle(\\mathbb{R}^{d})]\\}_{t=T+1}}{\\text{argmax}} E_{\\{p_{t}\\}}[r(x_{0})] - \\alpha \\sum_{t=T+1}^{1}E_{\\{p_{t}\\}}[KL(p_{t}(\\cdot|x_{t})||p_{t}^{\\text{pre}} (\\cdot|x_{t}))]$\nwhere the expectation $E_{\\{p_{t}\\}}[\\cdot]$ is taken with respect to $\\prod_{t=T+1}^{1} p_{t}(x_{t-1}|x_{t})$, i.e., $x_{T} \\sim p_{T+1}(\\cdot), x_{T-1} \\sim p_{T-1}(\\cdot | x_{T-1}), x_{T-2} \\sim p_{T-2}(\\cdot | x_{T-2}), \\dots$. In this article, we set this as an objective function in fine-tuning diffusion models. This objective is natural as it seeks to optimize sequential denoising processes to maximize downstream rewards while maintaining proximity to pre-trained models. Subsequently, we investigate several algorithms to solve (18). Before discussing these algorithms, we summarize several key theoretical properties that will aid their derivation."}, {"title": "Theory of RL-Based Fine-Tuning", "content": "So far, we have introduced a certain RL problem (i.e., (18)) as a fine-tuning diffusion model. In this section, we explain that solving this RL problem allows us to achieve the target distribution discussed in Section 1.2.1. Additionally, we present several important theoretical properties, such as the analytical form of marginal distributions and posterior distributions induced by fine-tuned models. This formulation is also instrumental in introducing several algorithms (reward-weighted MLE, value-weighted sampling, and path consistency learning in Section 6), and establishing connections with related areas (classifier guidance in Section 8, and flow-based diffusion models in Section 9). We start with several key concepts."}, {"title": "Key Concepts: Soft Value functions and Soft Bellman Equations.", "content": "Now, reflecting on how soft optimal policies are expressed using soft value functions in Section 2 in the context of standard RL problems, we derive several important concepts applicable to fine-tuning diffusion models. These concepts are later useful in constructing algorithms to solve our RL problem (18).\nFirstly, as we see in (15), soft-optimal policies are characterized as:\n$p_{t}^{*}(x_{t-1}|x_{t}) = \\frac{\\exp(v_{t-1}(.)/\\alpha) p^{\\text{pre}}(.| x_{t})}{\\int \\exp(v_{t-1}(x_{t-1})/\\alpha) p^{\\text{pre}}(x_{t-1} | x_{t}) dx_{t-1}}$\nwhere soft-value functions are defined as\n$v_{t}(x_{t}) = E_{\\{p^{*}\\}}[r(x_{0}) - \\alpha \\sum_{k=t}^{1} KL(p_{k}( \\cdot | x_{k})|| p_{k}^{\\text{pre}} ( \\cdot | x_{k}))|x_{t}],$\n$q_{t}(x_{t}, x_{t-1}) = E_{\\{p^{*}\\}}[r(x_{0}) - \\alpha \\sum_{k=t+1}^{1} KL(p_{k}( \\cdot | x_{k})|| p_{k}^{\\text{pre}} ( \\cdot | x_{k}))|x_{t}, x_{t-1}] = v_{t-1}(x_{t-1}).$\nSecondly, as we see in (16), the soft-value functions are also recursively defined by the soft Bellman equations:\n$\\exp(\\frac{v_{t}(x_{t})}{\\alpha}) = \\int \\exp(\\frac{v_{t-1}(x_{t-1})}{\\alpha}) p^{\\text{pre}} (x_{t-1} | x_{t}) dx_{t-1} (t = T + 1, \\dots, 1),$\n$v_{0}(x_{0}) = r(x_{0}).$\nNow substituting the above in (19), we obtain\n$p^{*}(x_{t}) = \\frac{\\exp(v_{t-1}(.)/\\alpha) p^{\\text{pre}}(x_{t})}{\\exp(v_{t}(x_{t})/\\alpha)}$\nAs mentioned earlier, these soft value functions and their recursive form will later serve as the basis for constructing several concrete fine-tuning algorithms (such as reward-weighted MLE and value-weighted sampling in Section 6)."}, {"title": "Induced Distributions after Fine-Tuning", "content": "Now, with the above preparation, we can show that the induced distribution derived by this soft optimal policy is actually equal to our target distribution.\nTheorem 1 (Theorem 1 in Uehara et al. (2024)). Let $p^{*}( \\cdot )$ be an induced distribution at time 0 from optimal policies $\\{p_{t}^{*}\\}_{t=T+1}^{1}$, i.e., $p^{*}(x_{0}) = \\int \\left[\\prod_{t=T+1}^{1} p_{t}^{*}(x_{t-1}|x_{t})\\right] dx_{1:T}$. The distribution $p^{*}$ is equal to the target distribution (10), i.e.,\n$p^{*}(x) = p_{r}(x)$.\nThis theorem states that after solving (18) and obtaining soft optimal policies, we can sample from the target distribution by sequentially running policies from $p_{T+1}^{*}$ to $p_{1}^{*}$. Thus, (18) serves as a natural objective for fine-tuning. This fact is also useful in deriving a connection with classifier guidance in Section 8.\nMarginal distributions. We can derive the marginal distribution as follows.\nTheorem 2 (Theorem 2 in Uehara et al. (2024) ). Let $p^{*}(x_{t})$ be the marginal distributions at $t$ induced by soft-optimal policies $\\{p_{t}^{*}\\}_{t=T+1}^{1}$, i.e., $p^{*}(x_{t}) = \\int \\left[\\prod_{k=T+1}^{t} p_{k}^{*}(x_{k-1}|x_{k})\\right] dx_{t+1:T}$. Then,\n$p^{*}(x_{t}) = \\frac{\\exp(v_{t}(x_{t})/\\alpha) p^{\\text{pre}}(x_{t})}{C},$\nwhere $v_{t}(\\cdot)$ is the soft-value function.\nInterestingly, the normalizing constant is independent of $t$ in the above theorem.\nPosterior distributions. We can derive the posterior distribution as follows.\nTheorem 3 (Theorem 3 in Uehara et al. (2024)). Denote the posterior distribution of $x_{t}$ given $x_{t-1}$ for the distribution induced by soft-optimal policies $\\{p_{t}^{*}\\}_{t=T+1}^{1}$ by $\\{p^{*}\\}_{t}( \\cdot | \\cdot )$. We define the analogous objective for a pre-trained policy and denote it by $p^{\\text{pre}}( \\cdot | \\cdot )$. Then, we get\n$\\{p^{*}\\}_{t}(x_{t}|x_{t-1}) = p^{\\text{pre}}(x_{t}|x_{t-1}).$\nThis theorem indicates that after solving (18), the posterior distribution induced by pre-trained models remains preserved. This property plays an important role in constructing PCL (path consistency learning) in Section 6.3.\nRemark 4 (Continuous-time formulation). For simplicity, our explanation is generally based on the discrete-time formulation. However, as training of diffusion models could be formulated in the continuous-time formulation (Song et al., 2021), we can still extend most of our discussion of fine-tuning in our tutorial in the continuous-time formulation. For example, the above Theorems are extended to the continuous time formulation in Uehara et al. (2024)."}, {"title": "RL-Based Fine-Tuning Algorithms 1: Non-Distribution-Constrained Approaches", "content": "So far, we have explained how to frame fine-tuning diffusion models as the RL problem in entropy-regularized MDPs. Moving forward, we summarize actual algorithms that can solve the RL problem of interest described by Equation (18). In this section, we introduce two algorithms: PPO and direct reward backpropagation.\nThese algorithms are originally designed to optimize reward functions directly, meaning they operate effectively even without entropy regularization (i.e., $\\alpha = 0$). Consequently, they are well-suited for generating samples with high rewards that may not be in the original training dataset. More distribution-constrained algorithms that align closely with pre-trained models will be discussed in the subsequent section (Section 4.2). Therefore, we classify algorithms in this section (i.e., PPO and direct reward backpropagation) as non-distribution-constrained approaches. The whole summary is described in Table 1."}, {"title": "Soft Proximal Policy Optimization (PPO)", "content": "In order to solve Equation (18), Fan et al. (2023); Clark et al. (2023) propose using PPO (Schulman et al., 2017). PPO has been widely used in RL, as well as, in the literature in fine-tuning LLMs, due to its stability and simplicity. In the standard context of RL, this is especially preferred over Q-learning when the action space is high-dimensional.\nThe PPO algorithm is described in Algorithm 1. This is an iterative procedure of updating a parameter $\\theta$. Each iteration comprises two steps: firstly, samples are generated by executing"}, {"title": "Direct Reward Backpropagation", "content": "Another standard approach is a differentiable optimization (Clark et al., 2023; Prabhudesai et al., 2023; Uehara et al., 2024), where gradients are directly propagated from reward functions to update policies.\nThe entire algorithm is detailed in Algorithm 2. This reward backpropagation entails an iterative process of updating a parameter $\\theta$. Each iteration comprises two steps; firstly, samples are generated by executing current policies to approximate the expectation in the loss function, which is directly derived from (18); second, the current parameter $\\theta$ is updated by computing the gradient of the loss"}, {"title": "RL-Based Fine-Tuning Algorithms 2: Distribution-Constrained Approaches", "content": "In this section, following Section 4.2, we present three additional algorithms (reward-weighted MLE, value-weighted sampling, and path consistency learning) aimed at solving the RL problem"}, {"title": "Reward-Weighted MLE", "content": "Here, we elucidate an approach based on reward-weighted MLE (Peters et al., 2010), a technique commonly employed in offline RL (Peng et al., 2019). While Fan et al. (2023, Algorithm 2) and Zhang and Xu (2023) propose variations of reward-weighted MLE for diffusion models, the specific formulation of reward-weighted MLE discussed here does not seem to have been explicitly detailed previously. Therefore, unlike the previous section, we start by outlining the detailed rationale for this approach. Subsequently, we provide a comprehensive explanation of the algorithm. Finally, we delve into its connection with the original training loss of diffusion models.\nMotivation. First, from Theorem 2, recall the form of the optimal policy $p_{t}^{*}(x_{t-1}|x_{t})$:\n$p_{t}^{*}(x_{t-1}|x_{t}) := \\frac{\\exp(v_{t-1}(x_{t-1})/\\alpha) p^{\\text{pre}}(x_{t-1}|x_{t})}{\\exp(v_{t}(x_{t})/\\alpha)}$\nNow, we have:\n$p_{t}^{*} = \\underset{p_{t}: \\mathcal{X} \\rightarrow \\triangle(\\mathcal{X})}{\\text{argmin}} E_{x_{t} \\sim u_{t}} [KL(p_{t}^{*}( \\cdot |x_{t})||p_{t}( \\cdot |x_{t}))]$\nwhere $u_{t} \\in \\triangle(\\mathcal{X})$ is a roll-in distribution encompassing the entire space $\\mathcal{X}$ This can be reformulated as value-weighted MLE as follows.\nLemma 1 (Value-weighted MLE). When $\\Pi_{t} = [\\mathcal{X} \\rightarrow \\triangle(\\mathcal{X})]$, the policy $p_{t}^{*}$ is equal to\n$p_{t}^{*}(\\cdot) = \\underset{p_{t} \\in \\Pi_{t}}{\\text{argmax}} E_{x_{t-1} \\sim p_{t}^{*}(\\,x_{t}), x_{t} \\sim u_{t}} \\left[\\exp(\\frac{v_{t-1}(x_{t-1})}{\\alpha})\\right] \\text{log} p_{t}(x_{t-1}|x_{t})$\nThis lemma illustrates that if $v_{t-1}$ is known, $p_{t}$ can be estimated using weighted maximum likelihood estimation (MLE). While this formulation is commonly used in standard RL (Peng et al., 2019), in our context of fine-tuning diffusion models, learning a value function is often challenging. Interestingly, this reward-weighted MLE can be performed without directly estimating the soft value function after proper reformulation. To demonstrate this, let's utilize the following lemma:"}, {"title": "Relation with Loss Functions for the Original Training Objective", "content": "In this subsection, we explore the connection with the original loss function of pre-trained diffusion models. To see that, as we see in Section 1.1.1, recall\n$x_{t-1}^{(i,t)} = \\mu^{(i,t)} + [0.5 x_{t} - 1/\\sigma_{t}^{2} \\epsilon_{\\theta}(x_{t}, T - t)] (\\delta_{t}) + \\epsilon_{i,t}, \\epsilon^{(i,t)} \\sim \\mathcal{N}(0, 1).$\nThen, the loss function (24) in reward-weighted MLE reduces to\n$\\theta_{t} \\propto \\sum_{t=T+1}^{1} \\sum_{i=1}^{m} \\text{exp} \\left[\\frac{r(x_{0}^{(i,t)})}{\\alpha}\\right] \\cdot |\\epsilon_{\\theta}(\\epsilon^{(i,t)}, T - t; \\theta) - \\epsilon_{\\theta}(\\epsilon^{(i,t)}, T - t; \\theta_{0})|^{2}|_{\\theta = \\theta_{s}}.$\nThis objective function closely resembles the reward-weighted version of the loss function (5) used for training pre-trained diffusion models."}, {"title": "Value-Weighted Sampling", "content": "Thus far, we have discussed methods for fine-tuning pre-trained diffusion models. Now, let's delve into an alternative approach during inference that aims to sample from the target distribution $p_{r}$ without explicitly fine-tuning the diffusion models. In essence, this approach involves incorporating gradients of value functions during inference alongside the denoising process in pre-trained diffusion models. Hence, we refer to this approach as value-weighted sampling. While it seems that this method has not been explicitly formalized in previous literature, the value-weighted sampling closely connects with classifier guidance, as discussed in Section 8.1.\nBefore delving into the algorithmic details, we outline the motivation. Subsequently, we present the concrete algorithm and discuss its advantages\u2014specifically, its capability to operate without the necessity of fine-tuning diffusion models\u2014and its disadvantage, which involves the need to obtain differentiable value functions."}, {"title": "Soft Q-learning", "content": "We have elucidated that leveraging Lemma 2, we can estimate soft value functions $v_{t}(\\cdot)$ based on Equation (28) in a Monte Carlo way. Subsequently, these soft value functions are used in value-weighted sampling to sample from the target distribution $p_{r}$. Alternatively, there is another method that involves using soft Bellman equations to estimate soft value functions $v_{t}(\\cdot)$. This technique is commonly called soft Q-learning in the context of standard RL.\nFirst, recalling the soft Bellman equations in (16), we have\n$\\exp(\\frac{v_{t}(x_{t})}{\\alpha}) = \\int \\exp(\\frac{v_{t-1}(x_{t-1})}{\\alpha}) p^{\\text{pre}}(x_{t-1}| x_{t}) dx_{t-1}.$\nTaking the logarithm, we obtain\n$v_{t}(x_{t}) = \\alpha \\text{ log} \\int \\text{exp}\\left[\\frac{v_{t-1}(x_{t-1})}{\\alpha}\\right] p^{\\text{pre}}(x_{t-1}| x_{t}) dx_{t-1}.$\nHence,\n$v_{t} = \\underset{h: \\mathcal{X} \\rightarrow \\mathbb{R}}{\\text{argmin}} E_{x_{t} \\sim u_{t}} \\left[h(x_{t}) - \\alpha \\text{ log} \\int \\text{exp}\\left[\\frac{v_{t-1}(x_{t-1})}{\\alpha}\\right] p^{\\text{pre}}(x_{t-1}| x_{t}) dx_{t-1}\\right]^{2}$\nwhere $u_{t} \\in \\triangle(\\mathcal{X})$ is any roll-in distribution that covers the entire space $\\mathcal{X}$. Using this relation and replacing the expectation $E_{x_{t} \\sim u_{t}}$ with empirical approximation, we are able to estimate soft value"}, {"title": "Approximation using Tweedie's formula", "content": "So far, we have explained two approaches: a Monte Carlo approach and a value iteration approach to estimate soft value functions. However, learning value functions in (28) can still be often challenging in practice. Therefore, we can employ approximation strategies inspired by recent literature on classifier guidance (e.g., reconstruction guidance (Ho et al., 2022), manifold constrained gradients (Chung et al., 2022), universal guidance (Bansal et al., 2023), and diffusion posterior sampling (Chung et al., 2022)).\nSpecifically, we adopt the following approximation:\n$\\begin{aligned}\nv_{t}(x_{t}) &\\approx \\alpha \\text{ log} E_{\\{p^{\\text{pre}}\\}}\\left[\\text{exp}\\left(\\frac{r(x_{0})}{\\alpha}\\right)\\right] \\\\\n&\\approx \\alpha \\text{ log}\\left(\\text{exp}\\left(\\frac{r(x_{0}^{(x_{t})})}{\\alpha}\\right) \\overline{v}_{t}(x_{t})\\right), \\quad x_{0}(x_{t}) = E_{\\{p^{\\text{pre}}\\}}[x_{0} | x_{t}], \\\\n&= r(x_{0}(x_{t})).\n\\end{aligned}$$\nHere, we replace the integration in (30) with a Dirac delta distribution with the posterior mean. Importantly, we can calculate $x_{0}(x_{t}) = E_{\\{p^{\\text{pre}}\\}}[x_{0} | x_{t}]$ using the pre-trained (score-based) diffusion model based on Tweedie's formula:\n$E_{\\{p^{\\text{pre}}\\}}[x_{0} | x_{t}] \\approx \\frac{x_{t} + {\\sigma_{t}}^{2}\\nabla\\text{log } q_{t}(x_{t})}{\\mu_{t}}$\nRecall that the notation $\\mu_{t}, {\\sigma_{t}}^{2}, q_{t}$ are defined in (3). Finally, by recalling $\\nabla\\text{ log } q_{t} \\approx s_{\\theta^{pre}} (x_{t}, t)$ in score-based diffusion models, we can approximate $\\nabla v_{t}(x)$ with\n$\\frac{x_{t} + {\\sigma_{t}}^{2}s_{\\theta^{pre}} (x_{t}, t)}{\\mu_{t}}$\nand plug it into Algorithm 4 (i.e., value-weighted sampling).\nAs mentioned earlier, this approach has been practically widely used in the context of classifier guidance. Despite its simplicity, the approximation error can be significant, as it can not be diminished even with a large sample size because the discrepancy from (30) to (31) is not merely a statistical error that diminishes with increasing sample size."}, {"title": "Zeroth-Order Guidance using Path Integral Control", "content": "In the previous subsection (Section 6.2.2), we discussed an approximation technique to bypass the necessity of learning explicit value functions. Another approach to bypass this requirement is to use control-based techniques for obtaining soft-optimal policies, a.k.a., path integral control (Theodorou et al., 2010; Williams et al., 2017; Kazim et al., 2024).\nRecall that we have $p(x_{t}, t; \\theta) = x_{t} + (\\delta t) g(x_{t}, t)$ and {\\sigma_{t}}^{2} = \\overline{g}(t)(\\delta t), the motivation behind this approach is as follows. Initially, we have:\n$\\nabla_{x_{t}}\\text{ exp}(v(x_{t})/\\alpha)$\n$= E_{\\{p^{\\text{pre}}\\}}\\left[\\text{exp}(v_{t-1}(x_{t-1})/\\alpha) | x_{t}\\right]$\n$= \\frac{\\int \\text{exp}(v_{t-1}(x_{t-1})/\\alpha) \\text{exp} \\left(-\\frac{(x_{t-1} - x_{t} - (\\delta t) g(x_{t}, t))^{2}}{0.5 \\overline{g}(t)(\\delta t)}\\right) dx_{t-1}}{E_{\\{p^{\\text{pre}}\\}}\\left[\\text{exp}(v_{t-1}(x_{t-1}/\\alpha))|x_{t}\\right]}$\n$\\approx \\frac{\\nabla_{x_{t}} \\left[E_{\\{p^{\\text{pre}}\\}}\\left[\\text{exp}(v_{t-1}(x_{t-1}/\\alpha))\\{x_{t-1}-x_{t}-((\\delta t) g(x_{t}, t))\\}\\right]\\right]}{\\overline{g}(t)(\\delta t)}$\n$\\approx \\frac{1}{\\overline{g}(t)(\\delta t)} E_{\\{p^{\\text{pre}}\\}}\\left[E_{\\{p^{\\text{pre}}\\}}\\left[\\text{exp}(r(x_{0})/\\alpha) | x_{t-1}\\right] \\{x_{t} - ((\\delta t) g(x_{t}, t))\\}|x_{t}\\right]$\n$= \\frac{1}{\\overline{g}(t)(\\delta t)} E_{\\{p^{\\text{pre}}\\}}\\left[E_{\\{p^{\\text{pre}}\\}}\\left[\\text{exp}(r(x_{0})/\\alpha) | x_{t}\\right]\\right].$\nNow, we obtain:\n$\\frac{{\\sigma_{t}}^{2}\\nabla_{x_{t}} v(x_{t})}{\\alpha} \\approx \\frac{1}{\\alpha} \\frac{E_{\\{p^{\\text{pre}}\\}}\\left[\\text{exp}(r(x_{0})/\\alpha) \\overline{\\epsilon}_{t}|x_{t}\\right]}{E_{\\{p^{\\text{pre}}\\}}\\left[\\text{exp}(r(x_{0})/\\alpha)|x_{t}\\right]}$\nImportantly, this approach does not require any differentiation. Therefore, by running policies from pre-trained models and simply using Monte Carlo approximation in both the denominator and numerator, we can estimate the above quantity (the right-hand side of (32)) without making any models (classifiers), unlike classifier guidance. Note while this approach is widely used in the control community, it may not be feasible for diffusion models due to the high dimensionality of the input space."}, {"title": "Path Consistency Learning (Losses Often Used in Gflownets)", "content": "Now, we explain how to apply path consistency learning (PCL) (Nachum et al., 2017) to fine-tune diffusion models. In the Gflownets literature (Bengio et al., 2023), it seems that this variant is utilized as either a detailed balance or a trajectory balance loss, as discussed in Mohammadpour et al. (2023); Tiapkin et al. (2023); Deleu et al. (2024). However, to the best of our knowledge, the precise formulation of path consistency learning in the context of fine-tuning diffusion models has not been established. Therefore, we start by elucidating the rationale of PCL. Subsequently, we provide a comprehensive explanation of the PCL. Finally, we discuss its connection with the literature on Gflownets.\nMotivation. Here, we present the fundamental principles of the PCL. To start with, we prove the following lemma, which characterizes soft-value functions and soft-optimal policies recursively."}, {"title": "Fine-Tuning Settings Taxonomy", "content": "So far, we implicitly assume we have access to reward functions. However, these functions are often unknown and need to be learned from data. We classify several settings in terms of whether reward functions are available or, if not, how they could be learned. This section is summarized in Figure 3."}, {"title": "Fine-Tuning with Known, Differentiable Reward Functions", "content": "When accurate differentiable reward functions are available (e.g., standard in computer vision tasks), our primary focus is typically on computational and memory efficiency. In general, we advocate for employing reward backpropagation (c.f. Algorithm 2) for its computational efficiency. Alternatively, if memory efficiency is a significant concern, we suggest using PPO due to its stability."}, {"title": "Fine-Tuning with Black-Box Reward Feedback", "content": "Here, we explore scenarios where we have access to accurate but non-differentiable (black-box) reward feedback, often found in scientific simulations. The emphasis still remains primarily on computational and memory efficiency. In such scenarios, due to the non-differentiability of reward feedback, we recommend using PPO or reward-weighted MLE for the following reasons.\nIn the preceding section, we advocated for the use of reward backpropagation due to its com-putational advantages. However, when dealing with non-differentiable feedback, the advantage of reward backpropagation may diminish. This is because learning from such feedback requires learning differentiable reward functions to make the algorithm work as we discuss in Section 5.2. However, obtaining a differentiable reward function can be challenging in many domains. For example, in molecular property prediction, molecular fingerprints are informative features, and accurate reward functions can be derived by training simple neural networks on these features"}, {"title": "", "content": "(Pattanaik and Coley, 2020). Yet, these mappings, grounded in prior scientific understanding, are non-differentiable, thereby restricting their applicability.\nIn contrast, PPO and reward-weighted MLE allow for policy updates without the explicit requirement to learn a differentiable reward function, offering a significant advantage over reward backpropagation."}, {"title": "Fine-Tuning with Unknown Rewards Functions", "content": "When reward functions (or computational proxies in Section 7.2) are unavailable, we must learn from data with reward feedback. In such cases, compared to the previous scenarios, two important considerations arise:\n    \u2022 Not only computational or memory efficiency but also feedback efficiency (i.e., sample efficiency regarding reward feedback) is crucial.\n    \u2022 Given that learned reward feedback may not generalize well outside the training data distri-butions, it is essential to constrain fine-tuned models to prevent significant divergence from diffusion models. In these situations, not only soft PPO and direct backpropagation but also methods discussed in Section 6, such as reward-weighted MLE, can be particularly effective.\nFurther, the scenario involving unknown reward functions can be broadly categorized into two cases, which we will discuss in more detail.\nOffline scenario (Figure 4). In many scenarios, even in cases where reward functions are unknown, we often have access to offline data with reward feedback $\\{x^{(i)}, r(x^{(i)})\\}$. One straightforward approach is to perform regression with neural networks and build a learned regressor $\\hat{r}$. However, this learned reward func-tion $\\hat{r}$ might not generalize well beyond the distribution of the offline data. In regions outside this distribution, $\\hat{r}$ could assign a high value even when the actual reward $r$ is low due to high uncertainty. Consequently, we could be easily misled by out-of-distribution samples if we solely optimize $\\hat{r}$.\nOne approach to alleviate this issue is to adopt a pessimistic (i.e., conservative) strategy, as proposed in Uehara et al. (2024). This technique has been widely applied in the field of offline RL (Levine, 2018). The main idea is to penalize $\\hat{r}$ outside the distributions us-ing techniques such as adding an explicit penalty term (Yu et al., 2020; Chang et al., 2021), bootstrapping, or more sophisticated methods (Kumar et al., 2020;\nXie et al., 2021; Rigter et al., 2022; Uehara and Sun, 2021). By doing so, we can avoid being fooled by out-to-distribution regions while still benefiting from the extrapolation capabilities of reward models."}, {"title": "", "content": "Online scenario (Figure 5). Consider situations where reward functions are unknown, but we can gather data online. These scenarios are often referred to as a lab-in-the-loop setting.\nIn such scenarios, Uehara et al. (2024) proposes an iterative procedure comprising three steps: (1) collecting feedback data by exploring new areas (i.e., obtaining x from current diffusion models and corresponding r(x)), (2) learning the reward function from the collected feedback data, and (3) fine-tuning current diffusion models by maximizing the learned reward function enhanced by an optimistic bonus term, using algorithms discussed in Section 4.2. In contrast to standard online PPO, where feedback data is directly used in the fine-tuning process, actual reward feedback is only used in step (2) and not in step (3). An additional key aspect is the use of an optimistic reward function that encourages exploration beyond the distributions of the current diffusion model. This deliberate separation between reward learning and fine-tuning steps, coupled with the use of optimism, significantly enhances feedback efficiency."}, {"title": "Connection with Classifier Guidance", "content": "Classifier guidance (Dhariwal and Nichol, 2021) is commonly used for conditional generation in diffusion models. Interestingly, RL-based methods share a close connection with classifier guidance. More specifically, in this section, following Zhao et al. (2024), we elucidate how RL-based methods can be applied to conditional generation. Furthermore, from this unified viewpoint, we highlight that classifier guidance is seen as a value-weighted sampling in Section 4.2."}, {"title": "Classfier Guidance", "content": "In this section, we first introduce classifier guidance. Classifier guidance utilizes an unconditional pre-trained model for conditional generation. More specifically, we consider a scenario where we have a pre-trained diffusion model, which enables us to sample from $p^{\\text{pre}}(x)$, and data with feedback $\\{x,y\\}$, where y represents the new label we want to condition on. Our objective here is to sample from $p(\\cdot |y), y \\in \\mathcal{Y}$:\n$p(\\cdot | y) \\propto p_{y}(y|\\cdot)p^{\\text{pre}}(\\cdot).$\nwhere $p_{y} : \\mathcal{X} \\rightarrow \\triangle(Y)$ denotes the conditional distribution of y given x.\nMotivation. In classifier guidance, we consider the following policy:\n$\\begin{aligned}\np_{\\text{ui}}(x_{t}, y) &= \\mathcal{N} \\left(\\mu(x_{t}, t; \\theta^{\\text{pre}}) + {\\sigma}^{2} (\\delta_{t}) \\nabla_{x_{t}} \\text{log } q_{y}(x_{t}, t), {\\sigma}^{2}(\\delta_{t})\\right), \\\\\nq_{y}(x, t) &:= E_{\\{p^{\\text{pre}}\\}}, Y \\sim p_{y}(\\cdot|x_{0})}[\\mathbb{I}(Y=y) | x_{t} = x].\n\\end{aligned}$$\nUnder the continuous-time formulation of diffusion models, applying Doob's h-transform (Rogers and Williams, 2000), it is shown that we can sample from p(x|y) by sequentially running $\\{p_{\\text{ui}}( \\cdot | x_{t}, Y)\\}_{t=T+1}$."}, {"title": "RL-Based Fine-Tuning for Conditional Generation", "content": "In this section, following Zhao et al. (2024), we explain an RL-based fine-tuning approach for conditional generation. The core idea is that setting r(x, y) = log p(y|x) allows us to sample from the target conditional distribution (37), drawing on the insights discussed in Section 4.2.\nMore precisely, introducing a distribution over y as q(\u00b7), we formulate the following RL problem:\n$\\{P_{t}\\}_{t} = \\underset{\\{p_{t} \\in [(\\mathcal{Y}, \\mathcal{X})\\rightarrow \\triangle(\\mathcal{X})]\\}_{t=T+1}}{\\text{argmax}} E_{\\{p_{t}(\\cdot|\\cdot,y)\\}, y \\sim q}[\\text{log}p(y|x_{0}) - \\sum_{t=T+1}^{1}KL(p_{t}(\\cdot|x_{t}, y)||p^{\\text{pre}}(\\cdot|x_{t}))].$\nCompared to the objective function (18) in Section 4.2, the main differences are: (1) policy space is expanded to accommodate additional control (i.e., y): $p_{t}(\\cdot|\\cdot, \\cdot) \\subset [\\mathbb{R}^{d} \\times \\mathcal{Y} \\rightarrow \\triangle(\\mathbb{R}^{d})]$, (2) the fine-tuning objectives are set as log-likelihoods: log p(y|x). Then, following Theorem 1, we can establish the following result.\nCorollary 2. The distribution induced by $\\{p_{t}\\}_{t=T+1}^{1}$ (i.e., $\\int \\left[\\prod_{t=T+1}^{1} p_{t}^{*}(x_{t-1} | x_{t}, y)\\right] dx_{1:T}$ is the target conditional distribution (37).\nThe above corollary suggests that for conditional generation, we can utilize various off-the-shelf algorithms discussed in Section 4.2 and 6, such as PPO, reward backpropagation, and reward-weighted MLE, as well as value-weighted sampling."}, {"title": "Connection with Flow-Based Diffusion Models", "content": "We elucidate the close relationship between bridge matching and fine-tuning, as summarized in Table 2. Bridge (flow) matching (Liu et al., 2022; Tong et al., 2023; Lipman et al., 2023; Liu et al., 2022; Shi et al., 2024; Albergo and Vanden-Eijnden, 2022), has recently gained popularity as a distinct type of diffusion model from score-matching-based ones. These works suggest that training and inference speed will be accelerated due to the ability to define a more direct trajectory from noise to data distribution than score-matching-based diffusion models.\nThe connection between bridge matching and fine-tuning becomes evident in the continuous-time formulation. To begin, we first provide a brief overview of the continuous-time formulation of score-matching-based diffusion models."}, {"title": "Continuous Time Formulation of Score-Based diffusion models", "content": "We provide a brief overview of the continuous-time formulation of diffusion models (Song et al., 2021), further expanding Section 1.1.1. In this formulation, we initially define the forward reference SDE, spanning from time 0 to T (e.g., variance exploding (VE) process or variance preserving (VP) process), by setting an initial distribution at 0 as the data distribution. Additionally, the reference SDE is tailored to converge to an easy distribution at time T, such as the normal distribution. Subsequently, we consider the time-reversal SDE (Anderson, 1982), progressing from T to 0. If we could learn this time-reversal SDE, it would enable us to sample from the data distribution by starting with the easy initial distribution (at T) and following the time-reversal SDE from T to 0 at inference time.\nTraining. Now, the remaining question is how to learn this time-reversal SDE. Here, by repre-senting the induced distribution over trajectories from T to 0 with this time-reversal SDE as $Q^{\\text{time}}$, our goal is to train a new SDE, parameterized by @ in the neural network, such that the induced distribution $P^{\\theta}$ over trajectories associated with @ aligns with $Q^{\\text{time}}$. Specifically, the objective is to minimize the following losses:\n$\\underset{\\theta}{\\text{argmin}} KL(Q^{\\text{time}} ||\\mathbb{P}^{\\theta}).$\nWith some algebra, leveraging the observation that the time-reversal SDE comprises the original forward SDE and the scoring term of the marginal distribution (Anderson, 1982) as in (2), Song et al. (2021) demonstrates that this KL loss (39) is approximated as\n$\\hat{\\theta}_{pre} = \\underset{\\theta}{\\text{argmin}} E_{t \\sim \\text{Uni}([0,T]),x_{t} \\sim q_{t|0}(x_{t}|x_{0}), x_{0} \\sim p_{pre}}[\\{0\\}^{2} |\\nabla_{x_{t}} \\text{log } q_{t|0}(x_{t} | x_{0}) - s_{\\theta}(x_{t},t)|^{2}],$"}, {"title": "Flow-Based (Bridge-Based) Diffusion Models", "content": "We adopt the explanation outlined in Liu et al. (2022). In this framework", "motion": "n$dz_{t"}, "dw_{t}, \\quad t \\in [0, T"], "x_{0": "T"}, {"T}(x_{T},...,x_{0})": "p^{\\text{pref}}(x_{T},...,x_{1}|x_{0})p^{\\text{pre}}(x_{0}).$\nThe distribution $Q^{\\text{doob}}$ above is a limiting distribution obtained by making the discretization step small."}]