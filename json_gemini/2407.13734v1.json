{"title": "Understanding Reinforcement Learning-Based Fine-Tuning of Diffusion Models: A Tutorial and Review", "authors": ["Masatoshi Uehara", "Yulai Zhao", "Tommaso Biancalani", "Sergey Levine"], "abstract": "This tutorial provides a comprehensive survey of methods for fine-tuning diffusion models to optimize downstream reward functions. While diffusion models are widely known to provide excellent generative modeling capability, practical applications in domains such as biology require generating samples that maximize some desired metric (e.g., translation efficiency in RNA, docking score in molecules, stability in protein). In these cases, the diffusion model can be optimized not only to generate realistic samples but also to maximize the measure of interest explicitly. Such methods are based on concepts from reinforcement learning (RL). We explain the application of various RL algorithms, including PPO, differentiable optimization, reward-weighted MLE, value-weighted sampling, and path consistency learning, tailored specifically for fine-tuning diffusion models. We aim to explore fundamental aspects such as the strengths and limitations of different RL-based fine-tuning algorithms across various scenarios, the benefits of RL-based fine-tuning compared to non-RL-based approaches, and the formal objectives of RL-based fine-tuning (target distributions). Additionally, we aim to examine their connections with related topics such as classifier guidance, Gflownets, flow-based diffusion models, path integral control theory, and sampling from unnormalized distributions such as MCMC. The code of this tutorial is available at https://github.com/masa-ue/RLfinetuning_Diffusion_Bioseq.", "sections": [{"title": "Introduction", "content": "Diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2020) are widely recognized as powerful tools for generative modeling. They are able to accurately model complex distributions by closely emulating the characteristics of the training data. There are many applications of diffusion models in various fields, including computer vision (Podell et al., 2023), natural language processing (Austin et al., 2021), biology (Avdeyev et al., 2023; Stark et al., 2024; Li et al., 2023), chemistry (Jo et al., 2022; Xu et al., 2022; Hoogeboom et al., 2022), and biology (Avdeyev et al., 2023; Stark et al., 2024; Campbell et al., 2024).\nWhile diffusion models exhibit significant power in capturing the training data distribution, there's often a need to customize these models for particular downstream reward functions. For instance, in computer vision, Stable Diffusion (Rombach et al., 2022) serves as a strong backbone pre-trained model. However, we may want to fine-tune it further by optimizing downstream reward functions such as aesthetic scores or human-alignment scores (Black et al., 2023; Fan et al., 2023). Similarly, in fields such as biology and chemistry, various sophisticated diffusion models have been developed for DNA, RNA, protein sequences, and molecules, effectively modeling biological and chemical spaces. Nonetheless, biologists and chemists typically aim to optimize specific downstream objectives such as cell-specific expression in DNA sequences (Gosai et al., 2023; Lal et al., 2024; Sarkar et al., 2024), translational efficiency/stability of RNA sequences (Castillo-Hair and Seelig, 2021; Agarwal and Kelley, 2022), stability/bioactivity of protein sequence (Frey et al., 2023; Widatalla et al., 2024) or QED/SA scores of molecules (Zhou et al., 2019).\nTo achieve this goal, numerous algorithms have been proposed for fine-tuning diffusion models via reinforcement learning (RL) (e.g., Black et al. (2023); Fan et al. (2023); Clark et al. (2023); Prabhudesai et al. (2023); Uehara et al. (2024)), aiming to optimize downstream reward functions. RL is a machine learning paradigm where agents learn to make sequential decisions to maximize reward signals (Sutton and Barto, 2018; Agarwal et al., 2019). In our context, RL naturally emerges as a suitable approach due to the sequential structure inherent in diffusion models, where each time step involves a \u201cdecision\u201d corresponding to how the sample is denoised at that step. This tutorial aims to review recent works for readers interested in understanding the fundamentals of RL-based fine-tuning from a holistic perspective, including the advantages of RL-based fine-tuning over non-RL approaches, the pros and cons of different RL-based fine-tuning algorithms, the formalized goal of RL-based fine-tuning, and its connections with related topics such as classifier guidance."}, {"title": "", "content": "The content of this tutorial is primarily divided into three parts. In addition, as an implementation example, we also release the code that employs RL-based fine-tuning for guided biological sequences (DNA/RNA) generation at https://github.com/masa-ue/RLfinetuning_Diffusion_Bioseq.\nWe aim to provide a comprehensive overview of current algorithms. Notably, given the sequential nature of diffusion models, we can naturally frame fine-tuning as a reinforcement learning (RL) problem within Markov Decision Processes (MDPs), as detailed in Section 3 and 4. Therefore, we can employ any off-the-shelf RL algorithms such as PPO (Schulman et al., 2017), differentiable optimization (direct reward backpropagation), weighted MLE (Peters et al., 2010; Peng et al., 2019), value-weighted sampling (close to classifier guidance in Dhariwal and Nichol (2021)), and path consistency learning (Nachum et al., 2017). We discuss these algorithms in detail in Section 4.2 and 6. Instead of merely outlining each algorithm, we aim to present both their advantages and disadvantages so readers can select the most suitable algorithms for their specific purposes.\nWe categorize various fine-tuning scenarios based on how reward feedback is acquired in Section 7. This distinction is pivotal for practical algorithm design. For example, if we can access accurate reward functions, computational efficiency would become our primary focus. However, in cases where reward functions are unknown, it is essential to learn them from data with reward feedback, leading us to take feedback efficiency and distributional shift into consideration as well. Specifically, when reward functions need to be learned from static offline data without any online interactions, we must address the issue of overoptimization, where fine-tuned models are misled by out-of-distribution samples, and generate samples with low genuine rewards. This is crucial because, in an offline scenario, the coverage of offline data distribution with feedback is limited; hence, the out-of-distribution region could be extensive (Uehara et al., 2024).\nWe provide a detailed discussion on the relationship between RL-based fine-tuning methods and closely related methods in the literature, such as classifier guidance (Dhariwal and Nichol, 2021) in Section 8, flow-based diffusion models (Liu et al., 2022; Lipman et al., 2023; Tong et al., 2023) in Section 9, sampling from unnormalized distributions (Zhang and Chen, 2021) in Section 10, Gflownets (Bengio et al., 2023) in Section 6.3, and path integral control theory (Theodorou et al., 2010; Williams et al., 2017; Kazim et al., 2024) in Section 6.2.3. We summarize the key messages as follows.\nSection 6.3: The losses used in Gflownets are fundamentally equivalent to those derived from a specific RL algorithm called path consistency learning.\nSection 8: Classifier guidance employed in conditional generation is regarded as a specific RL-based fine-tuning method, which we call value-weighted sampling. As formalized in Zhao et al. (2024), this observation indicates that any off-the-shelf RL-based fine-tuning algorithms (e.g., PPO and differentiable optimization) can be applied to conditional generation.\nSection 10: Sampling from unnormalized distributions, often referred to as Gibbs distributions, is an important and challenging problem in diverse domains. While MCMC methods are traditionally used for this task, recognizing its similarity to the objectives of RL-based fine-tuning suggests that off-the-shelf RL algorithms can also effectively address the challenge of sampling from unnormalized distributions."}, {"title": "Preliminaries", "content": "In this section, we outline the fundamentals of diffusion models and elucidate the objective of fine-tuning them."}, {"title": "Diffusion Models", "content": "We present an overview of denoising diffusion probabilistic models (DDPM) (Ho et al., 2020). For more details, refer to Yang et al. (2023); Cao et al. (2024); Chen et al. (2024); Tang and Zhao (2024).\nIn diffusion models, the objective is to develop a deep generative model that accurately captures the true data distribution. Specifically, denoting the data distribution by $p_{pre} \\in \\Delta(\\mathcal{X})$ where $\\mathcal{X}$ is an input space, a DDPM aims to approximate $p_{pre}$ using a parametric model structured as\n$p(x_{0};\\theta) = \\int p(x_{0:T}; \\theta)dx_{1:T}$, where $p(x_{0:T}; \\theta) = p_{T+1}(x_{T};\\theta)\\prod_{t=T}^{1}p_{t}(x_{t-1}|x_{t};\\theta)$\nWhen $\\mathcal{X}$ is an Euclidean space (in $\\mathbb{R}^{d}$), the forward process is modeled as the following dynamics:\n$p_{T+1}(x_{T}) = \\mathcal{N}(0, I)$, $p_{t}(x_{t-1}|x_{t}; \\theta) = \\mathcal{N}(\\rho(x_{t}, t; \\theta), \\sigma^{2}(t) \\times I)$,\nwhere $\\mathcal{N}(*,)$ denotes a normal distribution, $I$ is an identity matrix and $\\rho : \\mathbb{R}^{d} \\times [0, T] \\rightarrow \\mathbb{R}^{d}$. In DDPMs, we aim to obtain a set of policies (i.e., denoising process) $\\{p_{t}\\}_{t=T+1}^{1}$, $p_{t}: \\mathcal{X} \\rightarrow \\triangle(\\mathcal{X})$ such that $p(x_{0}; \\theta) \\approx p_{pre}(x_{0})$. Indeed, by optimizing the variational bound on the negative log-likelihood, we can derive such a set of policies. For more details, refer to Section 1.1.1.\nHereafter, we consider a situation where we have a pre-trained diffusion model that is already trained on a large dataset, such that the model can accurately capture the underlying data distribution. We refer to the pre-trained policies as $\\{p_{t}^{pre}(\\cdot|\\cdot)\\}_{t=T+1}^{1}$, and to the marginal distribution at $t=0$ induced by the pre-trained diffusion model as $p_{pre}^{pre}$. In other words,\n$p_{pre}^{pre}(x_{0}) = \\int p_{T+1}^{pre}(x_{T})\\prod_{t=T}^{1}p_{t}^{pre}(x_{t-1}|x_{t})dx_{1:T}$"}, {"title": "Score-Based Diffusion Models (Optional)", "content": "We briefly discuss how to train diffusion models in a continuous-time framework (Song et al., 2021). In our tutorial, this section is mainly used to discuss several algorithms (e.g., value-weighted sampling in Section 6.2) and their relationship with flow-based diffusion models (Section 9) later. Therefore, readers may skip it based on their individual needs.\nThe training process of diffusion models can be summarized as follows. Our objective is to train a sequential mapping from a known noise distribution to a data distribution, formalized through a stochastic differential equation (SDE). Firstly, we define a forward (fixed) SDE that maps from the data distribution to the noise distribution. Then, a time-reversal SDE is expressed as an SDE, which includes the (unknown) score function. Now, by learning this unknown score function from the training data, the time-reversal SDE can be utilized as a generative model. Here are the details.\nForward and time-reversal SDE. Firstly, we introduce a forward (a.k.a., reference) Stochastic differential equations (SDE) from 0 to T. A common choice is a variance-preserving (VP) process:\n$t \\in [0, T]; dx_{t} = -0.5x_{t}dt + dw_{t}, x_{0} \\sim p^{pre}(x)$,\nwhere $dw_{t}$ represents standard Brownian motion. Here are two crucial observations:\nAs $T$ approaches $\\infty$, the limiting distribution is $\\mathcal{N}(0, I)$.\nThe time-reversal SDE (Anderson, 1982), which preserves the marginal distribution, is expressed as follows:\n$t \\in [0, T]; dz_{t} = [0.5z_{t} + \\nabla \\log q_{T-t}(z_{t})]dt + dw_{t}$.\nHere, $q_{t} \\in \\triangle(\\mathbb{R}^{d})$ denotes the marginal distribution at time $t$ induced by the reference SDE (1). Notably, the marginal distribution of $z_{T-t}$ is the same as the one of $x_{t}$ induced by the reference SDE.\nThese observations suggest that with sufficiently large $T$, starting from $\\mathcal{N}(0, I)$ and following the time-reversal SDE (2), we are able to sample from the data distribution (i.e., $p^{pre}$) at the terminal time point $T$."}, {"title": "", "content": "Training score functions from the training data. Now, the remaining question is how to estimate the marginal score function $\\nabla \\log q_{t}(\\cdot)$ in (2). This can be computed using the following principles:\nWe have $\\nabla \\log q_{t}(x_{t}) = E_{x_{0}\\sim p_{pre}}[\\nabla_{x_{t}} \\log q_{t|0}(x_{t} | x_{0})]$, where $q_{t|0}$ represents the conditional distribution of $x_{t}$ given $x_{0}$ induced by the reference SDE.\nThe conditional score function $\\nabla_{x_{t}} \\log q_{t|0}(x_{t} | x_{0})$ can be analytically derived as\n$\\frac{\\mu_{t}x_{0} - x_{t}}{\\{\\sigma_{t}\\}^{2}}, \\mu_{t} = exp(-0.5t)x_{0}, \\{\\sigma_{t}\\}^{2} = 1 - exp(-0.5t)$.\nSubsequently, by defining a parameterized model $s_{\\theta} : \\mathbb{R}^{d} \\times [0, T] \\rightarrow \\mathbb{R}^{d}$ and utilizing the following weighted regression, the marginal score is estimated:\n$\\hat{\\theta}_{pre} = argmin_{\\theta} E_{t \\sim Uni([0,T]),x_{t} \\sim q_{t|0}(x_{t}|x_{0}),x_{0} \\sim p_{pre}}[\\lambda(t) ||\\nabla_{x_{t}} \\log q_{t|0}(x_{t} | x_{0}) - s_{\\theta}(x_{t}, t)||^{2}$  where $\\lambda: [0, T] \\rightarrow \\mathbb{R}$ is a weighted function.\nNote we typically use another parametrization well. From (3), we can easily see that this score is estimated by introducing networks $e_{\\theta}: \\mathbb{R}^{d} \\times [0, T] \\rightarrow \\mathbb{R}^{d}$, which aims to estimate the noise $\\frac{x_{t}-\\mu_{t}x_{0}}{\\sigma_{t}}$:\n$\\hat{\\theta}_{pre} = argmin_{\\theta} E_{t \\sim Uni([0,T]),x_{t} \\sim \\mu_{t}x_{0} + \\epsilon\\sigma_{t},\\epsilon \\sim \\mathcal{N}(0,1),x_{0} \\sim p_{pre}}[\\lambda(t)/{\\{\\sigma_{t}\\}^{2}\\}||\nFinally, by denoting the training data as $\\{x_{0}^{(i)}\\}_{i=1}^{n}$ and setting $\\lambda(t) = \\{\\sigma_{t}\\}^{2}$, the actual loss function from the training data is\n$\\underset{\\theta}{argmin} \\sum_{i=1}^{n} E_{t \\sim Uni([0,T]), \\epsilon \\sim \\mathcal{N}(0,1), x_{t}^{(i)} \\sim \\mu_{t}^{(i)}x_{0}^{(i)} + \\epsilon\\sigma_{t}^{(i)}} [||\nInference time. Once this $\\hat{\\theta}_{pre}$ (or $\\hat{s}_{pre}$) is learned, we insert it into the time-reversal SDE (2) and use it as a generative model. Ultimately, with standard discretization, we obtain:\n$\\rho(x_{t}, t; \\hat{\\theta}_{pre}) = x_{t} + [0.5x_{t} + s_{\\hat{\\theta}_{pre}}(x_{t}, T - t)](\\delta t), \\{\\sigma_{t}\\}^{2} = (\\delta t)$.\nEquivalently, this is\n$\\rho(x_{t}, t; \\hat{\\theta}_{pre}) = x_{t} + [0.5x_{t} - 1/\\sigma_{t}^{2} \\times \\epsilon_{\\hat{\\theta}_{pre}}(x_{t}, T - t)](\\delta t)$,\nwhere $(\\delta t)$ denotes the discretization step.\nEquivalence to DDPM. The objective function derived here is equivalent to the one formulated based on variational inference in the discretized formulation, which is commonly referred to as DDPMs (Ho et al., 2020). In DDPMs (Ho et al., 2020), we often see the following form:\n$p(x_{t}, t; \\theta) = \\frac{1}{\\sqrt{\\alpha_{t}}}x_{t} - \\frac{1}{\\sqrt{1 - \\alpha_{t}}} \\epsilon_{\\theta}(x_{t}, T-t), \\alpha_{t} = \\prod_{k=1}^{t} \\alpha_{k}$\nWhen $\\alpha_{t} = 1-(\\delta t)$, the above is equivalent to (7) when $\\epsilon_{\\theta}$ goes to 0 by noting $1/\\sqrt{\\alpha_{t}} \\approx 1+0.5(\\delta t)$."}, {"title": "Fine-Tuning Diffusion Models with RL", "content": "Importantly, our focus on RL-based fine-tuning distinguishes itself from the standard fine-tuning methods. Standard fine-tuning typically involves scenarios where we have pre-trained models (e.g., diffusion models) and new training data $\\{x^{(i)}, y^{(i)}\\}$. In such cases, the common approach for fine-tuning is to retrain diffusion models with the new training data using the same loss function employed during pre-training. In sharp contrast, RL-based fine-tuning directly employs the downstream reward functions as the primary optimization objectives, making the loss functions different from those used in pre-training.\nHereafter, we start with a concise overview of RL-based fine-tuning. Then, before delving into specifics, we discuss simpler non-RL alternatives to provide motivation for adopting RL-based fine-tuning."}, {"title": "Brief Overview: Fine-tuning with RL", "content": "In this article, we explore the fine-tuning of pre-trained diffusion models to optimize downstream reward functions $r : \\mathbb{R}^{d} \\rightarrow \\mathbb{R}$. In domains such as images, these backbone diffusion models to be fine-tuned include Stable Diffusion (Rombach et al., 2022), while the reward functions are aesthetic scores and alignment scores (Clark et al., 2023; Black et al., 2023; Fan et al., 2023). More examples are detailed in the introduction. These rewards are often unknown, necessitating learning from data with feedback: $\\{x^{(i)}, r(x^{(i)})\\}$. We will explore this aspect further in Section 7. Until then, we assume $r$ is known.\nNow, readers may wonder about the objectives we aim to achieve during the fine-tuning process. A natural approach is to define the optimization problem:\n$\\underset{\\Phi \\in \\triangle(\\mathcal{X})}{argmax} E_{x \\sim q}[r(x)]$\nwhere $q$ is initialized with a pre-trained diffusion model $p^{pre} \\in \\triangle(\\mathcal{X})$. In this tutorial, we will detail the procedure of solving (8) with RL in the upcoming sections. In essence, we leverage the fact that diffusion models are formulated as a sequential decision-making problem, where each decision corresponds to how samples are denoised.\nAlthough the above objective function (8) is reasonable, the resulting distribution might deviate too much from the pre-trained diffusion model. To circumvent this issue, a natural way is to add penalization against pre-trained diffusion models. Then, the target distribution is defined as:\n$\\underset{\\Phi \\in \\triangle(\\mathcal{X})}{argmax} E_{x \\sim q}[r(x)] - \\alpha KL(q||p^{pre})$.\nNotably, (9) reduces to the following distribution:\n$p_{r}( \\cdot ) := \\frac{exp(r(\\cdot )/\\alpha)p^{pre}(\\cdot )}{\\int exp(r(x)/\\alpha)p^{pre}(x)dx}$\nHere, the first term in (9) corresponds to the mean reward, which we want to optimize in the fine-tuning process. The second term in (10) serves as a penalty term, indicating the deviation of $q$ from the pre-trained model. The parameter $\\alpha$ controls the strength of this regularization term. The proper choice of $\\alpha$ depends on the task we are interested in."}, {"title": "Motivation for Using RL over Non-RL Alternatives", "content": "To achieve our goal of maximizing downstream reward functions with diffusion models, readers may question whether alternative approaches can be employed apart from RL. Here, we investigate these potential alternatives and explain why RL approaches may offer advantages over them.\nRejection sampling. One approach involves generating multiple samples from pre-trained diffusion models and selecting only those with high rewards. This method, called rejection sampling, operates without needing fine-tuning. However, rejection sampling is effective primarily when the pre-trained model already has a high probability of producing high-reward samples. It resembles sampling from a prior distribution to obtain posterior samples (in this case, high-reward points). This approach works efficiently when the posterior closely matches the prior but can become highly inefficient otherwise. In contrast, by explicitly updating weight in diffusion models, RL-based fine-tuning allows us to obtain these high-reward samples, which are seldom generated by pre-trained models.\nConditional diffusion models (classifier-free guidance). In conditional generative models, the general goal is to sample from $p(x|c)$, where $x$ is the output and $c$ denotes the conditioning variable. For example, in text-to-language diffusion models, $c$ is a text, and $x$ is the generated image. Similarly, in the context of protein engineering for addressing inverse folding problems, models often define $c$ as the protein backbone structure and $x$ as the corresponding amino acid sequence. Here, using the training data $\\{c^{(i)}, x^{(i)}\\}$, the model is trained by using the loss function:\n$\\underset{\\theta}{argmin} E_{t \\sim Uni([0,T]), \\epsilon \\sim \\mathcal{N}(0,1), x_{t}^{(i)} \\sim \\mu_{t}^{(i)}x_{0}^{(i)} + \\epsilon\\sigma_{t}^{(i)}} [||$,\nwhere the denoising function $\\epsilon_{\\theta}$ additionally receives the conditioning information $c^{(i)}$ as input. In practice, a variety of improvements such as classifier-free guidance (Ho and Salimans, 2022) can further improve the model's ability to learn the conditional distribution $p(x|c)$.\nThese conditional generative models can be used to optimize down-stream rewards by conditioning on the reward values, then sampling $x$ conditioned on high reward values (Krishnamoorthy et al., 2023; Yuan et al., 2023). While this method is, in principle, capable of generating plausible $x$ values across a range of reward levels within the training data distribution, it is not the most effective optimization strategy. This is primarily because high-reward inputs frequently reside in the tails of the training distribution or even beyond it. Consequently, this method may not effectively generate high-reward samples that lie outside the training data distribution. In contrast, RL-based fine-tuning has the capability to generate samples with higher rewards beyond the training data. This is achieved by explicitly maximizing reward models learned from the training data and leveraging their extrapolative capabilities of reward models, as theoretically formalized and empirically observed in Uehara et al. (2024).\nReward-weighted training. Another alternative approach is to use a reward-weighted version of the standard training loss for diffusion models. Suppose that we have data $\\{x^{(i)}, r(x^{(i)})\\}$. Then, after learning a reward $\\hat{r} : \\mathcal{X} \\rightarrow \\mathbb{R}$ with regression from the data, to achieve our goal, it looks"}, {"title": "", "content": "natural to use a reward-weighted version of the training loss for diffusion models (5), i.e.,\n$\\underset{\\theta}{argmin} \\sum_{i=1}^{n} E_{t \\sim Uni([0,T]), x_{t}^{(i)} \\sim \\mu_{t}^{(i)}x_{0}^{(i)} + \\epsilon\\sigma_{t}^{(i)}, \\epsilon \\sim \\mathcal{N}(0,1)} [\\hat{r}(x^{(i)}) ||$.\nThere are two potential drawbacks to this approach. First, in practice, it may struggle to generate samples with higher rewards beyond the training data. As we will explain later, many RL algorithms are more directly focused on optimizing reward functions, which are expected to excel in obtaining samples with high rewards not observed in the original data, as empirically observed in Black et al. (2023). Second, when fine-tuning a conditional diffusion model $p(x|c)$, the alternative approach here requires a pair of $\\{c^{(i)}, x^{(i)}\\}$ during fine-tuning to ensure the validity of the loss function. When we only have data $\\{x^{(i)}, r(x^{(i)})\\}$ but not $\\{c^{(i)}, x^{(i)}, \\hat{r}(x^{(i)})\\}$, this implies that we might need to solve an inverse problem from $x$ to $c$, which can often be challenging. In contrast, in these scenarios, RL algorithms, which we will introduce later, can operate without needing such pairs $\\{c^{(i)}, x^{(i)}\\}$, as long as we have learned reward functions $\\hat{r}$.\nFinally, it should be noted that reward-weighted training technically falls under the broader category of RL methods. It shares a close connection with \u201creward-weighted MLE\u201d introduced in Section 6.1, as discussed later. Employing this reward-weighted MLE helps address the second concern of \u201creward-weighted training\u201d mentioned earlier."}, {"title": "Brief Overview of Entropy-Regularized MDPs", "content": "In this tutorial, we explain how fine-tuning diffusion models can be naturally formulated as an RL problem in entropy-regularized MDPs. This perspective is natural because RL involves sequential decision-making, and a diffusion model is formulated as a sequential problem where each denoising step is a decision-making process. To connect diffusion models with RL, we begin with a concise overview of RL in standard entropy-regularized MDPs (Haarnoja et al., 2017; Neu et al., 2017; Geist et al., 2019; Schulman et al., 2017)."}, {"title": "MDPS", "content": "An MDP is defined as follows: $\\{\\mathcal{S}, \\mathcal{A}, \\{P_{tra}^{t}\\}_{t=0}^{T}, \\{r_{t}\\}_{t=0}^{T}, p_{0}\\}$ where $\\mathcal{S}$ is the state space, $\\mathcal{A}$ is the action space, $P_{tra}^{t}$ is a transition dynamic mapping: $\\mathcal{S} \\times \\mathcal{A} \\rightarrow \\triangle(\\mathcal{S})$, $r_{t} : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$ denotes reward received at $t$ and $p_{0}$ is an initial distribution over $\\mathcal{S}$. A policy $\\pi_{t} : \\mathcal{S} \\rightarrow \\triangle(\\mathcal{A})$ is a map from any state $s \\in \\mathcal{S}$ to the distribution over actions. The standard goal in RL is to solve\n$\\underset{\\{\\pi_{t}\\}}{argmax} E_{\\{\\pi_{t}\\}}[\\sum_{t=0}^{T}r_{t}(S_{t}, a_{t})]$\nwhere $E_{\\{\\pi_{t}\\}}[\\cdot]$ is the expectation induced both policy $\\pi$ and the transition dynamics as follows: $S_{0} \\sim p_{0}, a_{0} \\sim \\pi_{0}(S_{0}), S_{1} \\sim P_{tra}(\\cdot|s_{0}, a_{0}),\\dots.$ As we will soon detail in the next section (Section 3), diffusion models can naturally be framed as MDPs as each policy corresponds to a denoising process in diffusion models."}, {"title": "Key Concepts: Soft Q-functions, Soft Bellman Equations.", "content": "The crucial question in RL is how to devise algorithms that effectively solve the optimization problem (11). These algorithms are later used as fine-tuning algorithms of diffusion models. To see these algorithms, we rely on several critical concepts in entropy-regularized MDPs. Specifically, soft-optimal policies (i.e., solutions to (12)) can be expressed analytically as a blend of soft Q-functions and reference policies. Furthermore, these soft Q-functions are defined as solutions to equations known as soft Bellman equations. We elaborate on these foundational concepts below.\nSoft Q-functions and soft optimal policies. Soft optimal policies are expressed as a blend of soft Q-functions and reference policies. To see it, we define the soft Q-function as follows:\n$q_{t}(s_{t}, a_{t}) = E_{\\{\\pi_{t}\\}}[\\sum_{k=t}^{T}r_{k}(S_{k}, a_{k}) - \\alpha KL(\\pi_{k+1}(\\cdot|S_{k+1})||\\pi'_{k+1}(\\cdot|S_{k+1}))|s_{t}, a_{t}]$\nThen, by comparing (13) and (12), we clearly have\n$\\pi_{t}^{*}= argmax_{\\pi \\in [\\mathcal{X} \\rightarrow \\triangle(\\mathcal{X})]} E_{a_{t} \\sim \\pi(s_{t})}[q_{t}(s_{t}, a_{t}) - \\alpha KL(\\pi(\\cdot|s_{t})||\\pi'_{t}(\\cdot | s_{t})|s_{t}]$.\nHence, by calculating the above explicitly, a soft optimal policy in (12) is described as follows:\n$\\pi_{t}^{*}(\\cdot|s) \\propto \\frac{exp(q_{t}(s,\\cdot)/\\alpha)\\pi'_{t}(\\cdot|s)}{\\int exp(q_{t}(s,a)/\\alpha)\\pi'_{t}(a|s)da}$"}, {"title": "Soft Bellman equations.", "content": "We have already defined soft Q-functions in (13). However, this form includes the soft optimal policies. Actually, without using soft optimal policies, the soft Q-function satisfies the following recursive equation (a.k.a. soft Bellman equation):\n$q_{t}(s_{t}, a_{t}) = E_{\\{\\pi_{t}\\}}[r_{t}(s_{t}, a_{t}) + \\alpha \\log[\\int exp(q_{t+1}(s_{t+1}, a)/\\alpha)\\pi'_{t}(a|s_{t+1})da] | s_{t}, a_{t} ]$\nThis is proven by noting we recursively have\n$q_{t}(s_{t}, a_{t}) = E_{\\{\\pi_{t}\\}}[r_{t}(s_{t}, a_{t}) + q_{t+1}(s_{t+1}, a_{t+1}) - \\alpha KL(\\pi_{t+1}(\\cdot|S_{t+1}), \\pi'_{t+1}(\\cdot|S_{t+1}))|s_{t}, a_{t}]$\nBy substituting (15) into the above, we obtain the soft Bellman equation (16)."}, {"title": "Soft value functions.", "content": "So far, we have defined the soft Q-functions, which depend on both states and actions. We can now introduce a related concept that depends solely on states, termed the soft value function. The soft value function is defined as follows:\n$v_{t}(s_{t}) = E_{\\{\\pi_{t}\\}}[\\sum_{k=t}^{T}r_{k}(S_{k}, a_{k}) - \\alpha KL(\\pi_{k}^{*}(\\cdot|S_{k})||\\pi'_{k}(\\cdot|S_{k}))|s_{t}]$\nThen, the soft optimal policy in (14) is also written as\n$\\pi_{t}^{*}(\\cdot|s) \\propto \\frac{exp(q_{t}(s,\\cdot)/\\alpha)\\pi'_{t}(\\cdot|s)}{exp(v_{t}(s)/\\alpha)}$\nbecause we have\n$\\frac{q_{t}(s,a)}{\\alpha} - \\frac{v_{t}(s)}{\\alpha} = \\log[\\int exp(\\frac{q_{t}(s,a)}{\\alpha})\\pi'_{t}(a | s)da]$.\nThen, substituting the above in the soft Bellman equation (16), it is written as\n$q_{t}(s_{t}, a_{t}) = E_{\\{\\pi_{t}\\}}[r(s_{t}, a_{t}) + v_{t+1}(S_{t+1})|s_{t}, a_{t}]."}, {"title": "Algorithms in entropy-regularized MDPs.", "content": "As outlined in Levine (2018), to solve (12), various well-known algorithms exist in the literature on RL. The abovementioned concepts are useful in constructing these algorithms. These include policy gradients, which gradually optimize a policy using a policy neural network; soft Q-learning algorithms, which utilize the soft-Bellman equation and approximate the soft-value function with a value neural network; and soft actor-critic algorithms that leverage both policy and value neural networks. We will explore how these"}]}