{"title": "ARCEAK: An Automated Rule Checking Framework Enhanced with Architectural Knowledge", "authors": ["Junyong Chen", "Ling-I Wu", "Minyu Chen", "Xiaoying Qian", "Haoze Zhu", "Qiongfang Zhang", "Guoqiang Li"], "abstract": "Automated Rule Checking (ARC) plays a crucial role in advancing the construction industry by addressing the laborious, inconsistent, and error-prone nature of traditional model review conducted by industry professionals. Manual assessment against intricate sets of rules often leads to significant project delays and expenses. In response to these challenges, ARC offers a promising solution to improve efficiency and compliance in design within the construction sector. However, the main challenge of ARC lies in translating regulatory text into a format suitable for computer processing. Current methods for rule interpretation require extensive manual labor, thereby limiting their practicality. To address this issue, our study introduces a novel approach that decomposes ARC into two distinct tasks: rule information extraction and verification code generation. Leveraging generative pre-trained transformers, our method aims to streamline the interpretation of regulatory texts and simplify the process of generating model compliance checking code. Through empirical evaluation and case studies, we showcase the effectiveness and potential of our approach in automating code compliance checking, enhancing the efficiency and reliability of construction projects.", "sections": [{"title": "I. INTRODUCTION", "content": "The Architecture, Engineering and Construction (AEC) industry has undergone significant digital transformation in recent years, transitioning from traditional 2D line drawings to data-centric construction processes. Ensuring compliance with established rules and regulations is crucial for delivering high-quality building design models. To replace or augment manual rule checking, the concept of Automated Rule Checking (ARC) has been introduced as a potential solution. ARC refers to a technology-driven approach automated building rule compliance checking by converting rules into machine-readable formats [1]. Traditional ARC approaches often rely on hard-coded or manual rule interpretation methods [2], [3], while modern researches benefit from advancements in natural language processing [4]\u2013[6]. Recent ARC approaches leverage fine-tuned language models such as BERT [7] to convert natural language building rules into structured formats [8], [9], including knowledge graphs. Despite these advancements, the substantial demand for domain-specific labeled data for fine-tuning continues to hinder the large-scale deployment of language models in the ARC domain.\nVery recently, large language models (LLMs) such as GPT-3.5-Turbo have shown remarkable performance in various natural language processing (NLP) downstream tasks, including sentiment analysis [10], natural language inference [11] and misinformation detection [12]. Researchers [13], [14] have even assigned different characters to these models to specialize in various tasks and collaborate on developing software engineering systems. One of the most notable capabilities of LLMS is their zero-shot generalization across diverse tasks [15]. With well-designed prompts, these models can perform complex tasks at a human-level ability, addressing issues related to the lack of training data for fine-tuning. As for the ARC task, LLMs have already demonstrates its effectiveness in similar tasks such as natural language understanding [16], [17] and code generation [18], [19]. Thus, we also expect LLMs to address several challenges such as building rule interpretation and checking code generation.\nTo harness the powerful in-context learning capabilities of LLMs for the ARC task, we propose ARCEAK, an Automated Rule Checking framework Enhanced with Architectural Knowledge. Our framework consists of two main stages. The first stage is LLM-based Rule Information Extraction, which focuses on extracting verification-related information from building rules. This stage is further divided into two steps: entity discovery (ED) and event extraction (EE). ED involves recognizing construction domain-specific entities, while EE identifies assignments related to these entities using construction domain knowledge augmented prompt engineering. The second one is LLM-based verification code generation, aiming to generate fine-grained, executable verification code by combining the extracted entities, events, and rule entry content with code generation prompts. This stage is also divided into two steps: code framework generation and rule checking code completion. Code framework generation involves creating the skeleton of the verification code, and rule checking code completion fills in the specific execution code. We conducted comprehensive experiments to evaluate the performance of our proposed framework. The results show"}, {"title": "II. BACKGROUND", "content": "In this section, we first introduce the background knowledge related to LLM and prompt engineering. Then, we explore key concepts in information extraction. Finally, we discuss code generation techniques, including both specialized and general LLMs for code generation."}, {"title": "A. Large Language Model", "content": "Large Language Models (LLMs) are generative models based on the pre-trained Transformer architecture [20]. Leveraging extensive multimodal data and employing pre-training and fine-tuning techniques, LLMs has significantly advanced the field of NLP by enhancing capabilities in multilingual translation [21]\u2013[23], summarization generation [24], [25], and code simulation [26], [27]. LLM training generally involves three key stages: unsupervised learning on vast amounts of unlabeled text data without direct human annotations, supervised fine-tuning using labeled data tailored to specific tasks or domains, and reinforcement learning based on feedback from human evaluators or annotators.\nPrompt Engineering offers a powerful method to extend capabilities of LLMs without the need for extensive model retraining or modification of parameters and has emerged as a transformative technique in the realm of LLMs [28], [29]. This approach harnesses prompts to tailor model behavior to specific tasks or domains, thereby enhancing model efficacy and versatility. Prompt engineering can be categorized into two main types: zero-shot prompting [30] and few-shot prompting [15]. The key distinction between these two lies in whether examples related to the task are provided to the language model. Zero-shot prompting involves giving the LLM an instruction without any examples, while few-shot prompting includes providing a few relevant examples to guide the LLM's response. Zero-shot prompting and few-shot prompting can also be combined with in-context learning methods to further enhance the performance of LLMs in solving complex problems or unseen domain-specific tasks. One of the most widely-used in-context learning methods is Chain-of-Thought Prompting [29], which decomposes problems into intermediate steps and solves each one before arriving at the final answer."}, {"title": "B. Information Extraction", "content": "Information Extraction (IE) is a fundamental domain in NLP to convert plain text into structured knowledge format [31], [32].The IE tasks cover:\n\u2022 Entity Discovery (ED) [33] encompasses both entity recognition [34] and entity typing [35]. The former is concerned with identifying spans of entities (e.g., 'Steve'), and the latter focuses on assigning types to these identified entities (e.g., 'PERSON').\n\u2022 Event Extraction (EE) [36] generally involves two stages. The first stage, event detection [37], focuses on identifying trigger words that signify the occurrence of specific events. The second stage, event argument extraction [38], seeks to extract the arguments associated with these events from given text. The set of target arguments varies depending on the event's definition.\nConsequently, recent generative IE methods that leverage LLMs to generate structural information have gained more attention than merely extracting it from plain text. Generative IE methods have demonstrated greater flexibility compared to traditional IE approaches. However, due to the limited presence of domain-specific data in LLM training, prompt engineering can be employed to enable LLMs to learn input-output mappings for specific downstream tasks without the need for fine-tuning. Inspired by previous works [39], [40], our work builds upon a generative IE method to extract information from building rules."}, {"title": "C. Code Generation", "content": "Code Generation involves creating programs that adhere to the constraints set by the underlying task [41]. These constraints typically come in diverse forms, such as input/output pairs, examples, problem descriptions, partial programs, and assertions. The remarkable success of transformers in natural language modeling has sparked significant interest among researchers in leveraging transformer models for code generation.\nOn one hand, there has been a proliferation of specialized LLMs tailored for code generation. Notably, OpenAI has introduced Codex [42], a GPT-3 model fine-tuned on publicly"}, {"title": "III. METHODOLOGY", "content": "In this section, we introduce ARCEAK, a novel ARC framework, which enhances verification code generation through the integration of architectural knowledge. We begin with an overview of ARCEAK, followed by a detailed explanation of its components in the subsequent subsections."}, {"title": "A. Overview", "content": "The implementation of ARC can be delineated into two primary stages: rule information extraction and verification code generation. The former is dedicated to analyzing the textual rules within the architectural domain and extracting pertinent information essential for verification purposes. The latter stage focuses on the generation of execution code aimed at assessing whether a given architectural blueprint complies with the building rules and requirements.\nTo mitigate or eliminate the necessity for manual annotation and model training during the rule information extraction and verification code generation stages, we introduce the ARCEAK framework, leveraging LLMs. The overall architecture of ARCEAK is illustrated in Fig. 1. Initially, we undertake a preprocessing phase for building rules, employing a prompt designed for rule splitting and error correction. Subsequently, we proceed to extract pertinent information, such as entities and events, from the refined rule content. Finally, our attention shifts to generating high-level code informed by knowledge-enhanced building rules. The subsequent sub-sections offer a comprehensive elucidation of the distinct phases within ARCEAK."}, {"title": "B. Preprocess", "content": "Since the national code files are consistently published in PDF format, which is not suitable for information extraction, we need to preprocess the original file and convert it into a structured format."}, {"title": "C. Rule Information Extraction", "content": "Information Extraction (IE) refers to the process of automatically extracting structured information from unstructured or semi-structured data, such as text. Extracting information from building rules, which encompasses implicit structural entities and intricate constraint conditions, poses several challenges, including complex context understanding and recognition of architectural terminology.\nTo overcome these issues and extract rule information more accurately and comprehensively, we design an information extraction mechanism based on knowledge-enriched zero-shot prompting with Classification Annotation (CA), which requires minimal manual involvement, to extract entities and events from preprocessed rule entries. IE stage mainly contains two tasks: entity discovery and event extraction.\nEntity Discovery(ED) encompasses both entity recognition and entity classification. To enhance LLMs' ability to accurately discover domain-specific entities in rule entries, we recommend an adaptive prompt optimization approach in this section. Initially, we allocate instructions for recognizing entities within the given rule entry and concurrently classifying the identified entities into possible types. To address the specificity of entity types, we elaborate on the target entity classification categories to LLMs through entity type classification annotations within the context part of the prompt for the ED task. The CA for ED task may encompass any entity types as long as there are few examples of these types and they perform well with clear and unambiguous heuristic type classification. Here, we employ the four entity types and corresponding examples shown in Table I.\nEvent Extraction(EE) involves identifying and classifying events described in text. An event, serving as a fine-grained semantic unit to describe the state of entities and their actions, is typically defined as a textual span comprising a predicate and its arguments. Complex conditions pose one of the primary challenges in interpreting building rules. Rule entries in building rules can be abstracted into two categories: attribute assignment with conditions and attribute assignment without conditions.\nIn our study, we define an attribute assignment as a type of event, where entities identified in the ED phase serve as \"trigger words\", with the goal of extracting arguments related to \"assignment\" events. A complete argument entry consists of the following components:\n\u2022 Entity of Attribute: The entity to which the attribute belongs.\n\u2022 Attribute Name: The name of the extracted attribute.\n\u2022 Conditions or Constraints: The constraints on attribute assignment.\n\u2022 Comparator: The comparator used for the attribute value.\n\u2022 Attribute Value: The value assigned to the attribute.\nSince general LLMs lack domain-specific knowledge of construction, the arguments describing the core information of 'assignment' events present considerable complexity, making it difficult for LLMs to identify and extract them directly without additional explanation. To enhance the LLMs' ability to recognize assignments in the construction domain, we compiled common assignment expressions for six types of assignments."}, {"title": "D. Verification Code Generation", "content": "Code generation refers to the process of automatically generating source code by a computer program. Manual code writing and verification is expensive, and generating code from structured rules is effective for explicit rules but often incomplete due to the presence of implicit rules. To address this, we propose a knowledge-augmented code generation workflow that leverages the code generation and completion capabilities of LLMs. Given the complexity of verification code generation, we follow the approach of Plan-and-Solve Prompting [48], dividing the entire process into two primary steps: code framework generation and rule-checking code completion.\nCode Framework Generation aims to construct a fundamental code skeleton, encompassing vital sections for variable initialization, function definitions, and control structures necessary for conditional evaluations. To enable LLMs to grasp the domain knowledge of building rules and generate precise code representing the details of rule entries, as well as to control the granularity of the function code and align the code structure more closely with the specific requirements of the building rules, we integrate entities and arguments related to the \"assignment\" event extracted during the IE stage with code framework generation prompt instructions. To address semantic dependencies between rules, we use a parser to determine whether the current rule entry depends on other rule entries; if such dependencies exist, both the current rule entry and its dependencies are provided to the LLM. Furthermore, to enhance the coverage rate, we aim to guide LLMs to annotate the generated verification code with the target rule entry index, ensuring that each rule component presented in the rule entries is comprehensively checked and represented in the code framework.\nRule-Checking Code Completion begins with integrating detailed logic and specific code snippets into the basic code skeleton formed during code framework generation. This crucial phase involves populating the established framework with specific details and logic tailored to the building rules. Placeholders previously defined in the skeleton are filled with actual code sections generated to check compliance with the rules. Furthermore, to pinpoint the specific rule that an architectural blueprint violates, we instruct LLMs to add detailed assert statements, including the specific rule entry according to the indexed comments in the code. \nSince new unimplemented or undefined functions may be added during the code completion process, iterations are necessary. To determine if the code completion is complete, we design a parser to extract all variables and functions used in the generated verification code, then verify whether each variable and function is defined and implemented. Additionally, because it is uncertain whether the code generated by LLMs is runnable and may contain compilation errors, we employ a code self-refinement process to enhance the runnability of the generated verification code. In this process, we provide the LLMs with the completed verification code along with reported errors, allowing them to refine the code they generated.\nAfter the knowledge-augmented verification code generation stage, we facilitate a smoother transition from natural language descriptions of building rules to executable verification code. This code can be used to verify whether architectural blueprints comply with regulations by invoking the API of the selected models."}, {"title": "IV. EVALUATION", "content": "To comprehensively evaluate the performance of the proposed framework ARCEAK, we conduct a large-scale study to seek to address the following research questions (RQ):\n\u2022 RQ1: How effective is ARCEAK in extracting information from a building rule? As described above, we employ a knowledge-enriched zero-shot prompting strategy to enhance LLM performance in the IE stage. This RQ aims to verify whether the incorporation of CA in ARCEAK improves the LLM's ability to perform IE more effectively.\n\u2022 RQ2: How comprehensive and accurate is the code generated by ARCEAK in enforcing building rules? We employed a two-step code generation process, enhanced by knowledge extracted during the IE stage. This RQ seeks to determine whether the knowledge augmentation and two-step generation approach in ARCEAK positively impact domain-specific code generation.\n\u2022 RQ3: How does the IE stage in ARCEAK enhance the accuracy and efficiency of verification code generation stage? ARC is a domain-specific and complex task, and merely providing evaluation metrics may not be sufficient to fully convey the effectiveness of ARCEAK. It is valuable to analyze concrete cases to better understand the impact of ARCEAK's IE stage on generating building compliance checking code.\nRQ1:How effective is ARCEAK in extracting information from a building rule?\nSetup. In our study, the Chinese building code GB50116-2013 (Code for Design of Automatic Fire Alarm System) is selected to validate the IE stage in ARCEAK. For the IE stage of ARCEAK, we implement our method using GPT-3.5\u00b9, and compare it to Chain-of-Thought(CoT) Prompting [29] without CA, which we call naive CoT. This comparison is conducted across both ED and EE tasks. In the naive baseline, the model is given the natural language instruction and is asked to directly discover the entity and extract events related to the discovered entity.\nMetrics. To compare the performance of the ED phase of IE stage, we employed the following three distinct metrics. Prior to introducing these metrics, it is essential to revisit three fundamental concepts: True Positives(TP), False Positives(FP) and False Negatives(FN). TP denotes correct predictions made by the classifier for different entity types. Conversely, FP and FN denote instances from different type of entities that have been misclassified.\n\u2022 Precision: Precision is the ratio of the number of entities extracted from rule with correct types to the number of entities extracted from rule, which is calculated with, \n$Precision = \\frac{TP}{TP+FP}$ (1)\n\u2022 Recall: Recall is the ratio of the number of entities extracted from rule with correct types to the number of entities which should be extracted from rule(ground truth)d, which is calculated with,\n$Recall = \\frac{TP}{TP+FN}$ (2)\n\u2022 F1: F1 score is a weighted average of the framework precision and recall, which ranges from 0 to 1. A higher F1 score indicates better comprehensive performance of the framework. F1 score is defined as the harmonic mean of the precision and recall, which can be calculated with,\n$F1 = 2.\\frac{Precision \\cdot Recall}{Precision + Recall}$ (3)\nTo comprehensively evaluate the performance of the EE phase of IE stage, we employed four distinct metrics,\n\u2022 Tri-R: Tri-R is the ratio of the number of intersection of events extracted from rule and events which should be extracted from rule with the same \"trigger word\" entity to the number of events which should be extracted from rule(ground truth), which represents the ratio of how many real events are extracted.\n\u2022 Arg-Pa: Arg-Pa is the ratio of the number of intersection of events extracted from rule and ground truth of event with the same entity and attribute to the number of events extracted from rule, which represents the ratio of how many extracted events extract the true attribute.\n\u2022 Arg-PA: Arg-PA represents the ratio of extracted events that contain all the required components.\n\u2022 Arg-Po: Arg-Po represents the ratio of extracted events that contain all the required components and are returned in the correct order as specified by the prompt.\nResult and Analysis. The primary goal of RQ1 is to assess the effectiveness of CA in ARCEAK for extracting information with minimal manual involvement. During the ED phase of the IE stage, the LLM extracts and classifies potential architectural entities. In the EE phase, the LLM detects \"assignment\u201d events based on \"trigger word\" entities and extracts their potential arguments. These experiments use the entire set of rule entries from GB50116-2013. To ensure accurate evaluation, a two-layer assessment structure is implemented. The first layer involves five junior evaluators cross-evaluating the IE results, with each result reviewed by two evaluators. In the second layer, a domain expert equipped with empirical knowledge further assesses the results.\nTable V shows ARCEAK's performance in the ED phase of the IE stage. ARCEAK extract 90% more entities than naive CoT prompting, while achieving approximately 64% higher F1 score. The data indicates that CA significantly enhances the LLM's ability to extract and classify architectural entities. The poor performance of the naive CoT approach, lacking CA, highlights the limitations of LLMs in extracting domain-specific information. In contrast, the improved performance with CA underscores its effectiveness in the ED phase.\nTable VI shows the comprehensive performance of ARCEAK in the EE phase of the IE stage, which demonstrates that prompting with CA can improve the event detection and argument extraction capabilities of the LLM. In the EE phase, CA resulted in 1.8% and 1.3% lower performance in Arg-Pa and Arg-PA, respectively. However, it achieved a 2.2% higher performance in Arg-Po. This indicates that while the LLM prompted by naive CoT tends to retain more information in the extracted events without fully understanding the relevance of the retained information, CA in the EE phase enhances the precision of extracting specific types of arguments.\nAnswer to RQ1: ARCEAK significantly improves the precision, recall, and F1 score in the ED phase. In the EE phase, ARCEAK enhances the LLM's ability to detect events and extract arguments with higher precision for specific argument types. Overall, ARCEAK's CA method effectively enhances the LLM's performance in IE stage.\nRQ2: How comprehensive and accurate is the code generated by ARCEAK in enforcing building rules?"}, {"title": "RQ3: How does the IE stage in ARCEAK enhance the accuracy and efficiency of verification code generation stage?", "content": "Setup. To answer the RQ3, we implement our method with GPT-3.5 and GPT-4, and compare to code generation without the assistance of knowledge augmentation, which serves as a baseline to assess the effect of the IE stage.\nCase Study. The RQ3 is proposed for analyzing the impact of knowledge augmentation on verification code generation and how extracted information from rule entries contributes to improved code accuracy and efficiency. The following case studies focus on specific instances where knowledge augmentation has markedly influenced the output of the code generation process. By examining these instances, we aim to illustrate concrete examples of both success and challenges in integrating extracted information, providing a deeper understanding of the mechanisms and factors that influence the outcomes.\n\u2022 Enhancement on reliability:\nIn this case study, we explore how the knowledge augmentation phase improves the compatibility between the LLM and a selected model. Without knowledge augmentation, LLMs might generate variables or implement data retrieval logic independently, leading to redundancy and inefficiency. However, with knowledge augmentation, the LLMs are better informed about the existing components and functions within the selected model, enabling them to utilize the model's APIs(in this case study, the model represents Revit) effectively.\n\u2022 Improvement on the control of granularity:\nThis case study investigates how knowledge augmentation influences the granularity of functions generated by the LLMs. Granularity control is crucial for maintaining a balance between high-level abstractions and detailed implementations in code generation. Knowledge augmentation equips the LLMs with detailed information about the required level of abstraction, enabling them to generate functions with appropriate granularity."}, {"title": "A. Case Study: Correct and Incorrect Results", "content": "We analyze several cases demonstrating ARCEAK's ability to guide LLMs in generating compliance checking code that better aligns with practical needs. However, there are still instances where our approach does not fully prevent the generation of erroneous results. Below, we provide a brief summary of these situations for further clarification.\n1) Generating API or parameter: As shown in Fig.4, the code below generates the correct API for retrieving Room information: \"FilteredElementCollector(doc).OfCategory(BuiltInCategory.OST_Rooms)\".\n2) Generating verification code for complex rule entry: For verification code generation, positive pass rate examples are primarily concentrated on relatively simple rule entries. However, for rule entries involving complex operations (e.g., calculating the distance from a detector to the centerline of a wall), the pass rate begins to decline. GPT-3.5 often attempts to generate methods that are not implemented (without the (unimplemented) tag) to assume data, while GPT-4 tries to generate correct code but may sometimes miss special cases (e.g., transforming walls to avoid calculating the distance to the side of the wall, or excluding the floor where the detector is located when checking for obstructions within a certain range around the detector). For rule entries with complex logic (e.g., rule entry 6.2.2 in GB50116-2013, which contains over 1,000 words and includes multiple nested value intervals), even GPT-4 may overlook part of the decision logic."}, {"title": "B. Threats To Validity", "content": "In this section, We identify two main threats to the validity of our study:\n1) Limited selection of models.: In this paper, we selected two LLMs for our experiments. However, it is important to acknowledge that other LLMs are available, including general models like Llama3 [46] and specialized models like Code-Gen [49]. In future work, we plan to conduct experiments with a broader range of LLMs to more comprehensively explore the applicability of our framework.\n2) Limited dataset.: In this paper, we used twenty room-level BIM models to verify the correctness of the code generated by ARCEAK, specifically testing the generation of C# code from text. It remains uncertain whether our experimental results and findings can be generalized to other languages (e.g., generating Python code for checking in Dynamo). In the future, we plan to build a larger building model dataset and make it publicly available to further support ARC research."}, {"title": "VI. RELATED WORK", "content": "Automated Rule Checking (ARC) is a technology-driven approach that automates the compliance verification process by converting building rules into computer-recognizable formats, such as decision tables [50] or query code [51]. As engineering projects become increasingly complex, manual compliance checking has grown both tedious and costly, while also raising the risk of human errors. In response, ARC offers a solution that can significantly reduce both time and expenses, all while improving the quality and accuracy of reviews. The rise of building modeling technologies has further bolstered this automation by making data more machine-readable, facilitating smoother integration into the compliance checking process. The ARC process consists of three key stages: 1) rule interpretation, which converts natural language rules into machine-readable formats, 2) building model preparation, which organizes the necessary information for rule checking, and 3) rule execution, where the prepared model is checked against the machine-readable rules [9]. Of these stages, rule interpretation and rule execution are particularly crucial and complex, warranting further research [52].\nCurrent research on ARC for conditional rules still involves considerable manual effort, such as entity labeling or sentence reconstruction [53], [54]. Fang et al. [55] proposes a knowledge graph that fuses computer vision with ontologies to dynamically recognize construction hazards while adhering to evolving safety standards. Zhou et al. [56] proposes a smart method for diagnosing wind turbine faults using ontology-based FMECA knowledge and a JESS rule engine to speed up maintenance decision-making. Zhou et al. [8] utilized the pretrained language model BERT for automated semantic annotation to capture the semantic information of sentences and then generated code from labeled sentences. Zhen et al. [9] established an ontology to represent domain knowledge and then generate SPARQL-based queries based on a pattern matching algorithm. Even though ready- to-use NLP tools for knowledge extraction exist, there is still room for refinement in structured text processing tools to more closely cater to the unique characteristics of specific fields.\nIn addition to the challenge of standardizing architectural design rules, another critical factor affecting the efficiency of ARC is the availability and accessibility of data. Zhang et al. [57] proposes a method that enhances automated compliance checking in building designs by merging compliance information into the IFC schema using machine learning and natural language processing. Although the IFC standard format is often favored for ARC [58], Malsane et al. [59] points out that Building Information Modeling (BIM) models typically lack the necessary detail level for such checks. Given the interconnected nature of BIM design and code checking, expecting BIM modelers to include all necessary review details is impractical. Many commercial ACC systems still require manual input of certain data. Although machine learning has proven to be useful in semantically enriching and reconciling IFC data exchange issues [60], converting the extensive information present in BIM models into a machine learning-friendly format continues to be an overwhelming task [61].\nDespite significant advancements in ARC within the AEC industry, extracting requirements and compliance information from detailed text documents remains a challenge. Innovations in NLP and knowledge graphs are improving the efficiency and accuracy of these systems, but the full automation of BIM reviews continues to be an ongoing effort. This paper builds upon existing research and, leveraging LLMs, seeks to transform natural language building rules into executable verification code with minimal human intervention. This marks a step towards the intelligent evolution of compliance checking in construction."}, {"title": "VII. CONCLUSION", "content": "In this work, we focus on the field of Automated Rule Checking (ARC) with the aim of reducing the manual effort required to convert natural language building rules into verification code for selected models. To achieve this, we propose ARCEAK, a novel LLM-based Automated Rule Checking framework Enhanced with Architectural Knowledge. ARCEAK achieves an almost fully automated process for converting natural language building rules into executable verification code. By consulting construction domain experts, we developed a robust construction domain-specific entity and event schema and established appropriate verification and evaluation metrics. The evaluation results of ARCEAK demonstrate outstanding performance in rule information extraction and an acceptable compile pass rate in verification code generation.\nIn the future, we plan to continue improving our framework. This includes, but is not limited to, expanding our methodology to cover a wider range of building rules and providing LLMs with related API lists during the Rule Checking Code Completion stage to enhance compatibility between the LLMs and selected models, which is crucial for improving the logic pass rate. Additionally, we aim to evaluate our framework on real architectural blueprints."}, {"title": "V. DISCUSSION", "content": "In this section, we summarize the correct and incorrect cases encountered during the experiments and evaluation process, and analyze the potential causes behind them. We then discuss several threats that may impact the effectiveness of our work."}]}