{"title": "How Diffusion Models Learn to Factorize and Compose", "authors": ["Qiyao Liang", "Ziming Liu", "Mitchell Ostrow", "Ila Fiete"], "abstract": "Diffusion models are capable of generating photo-realistic images that combine elements which likely do not appear together in the training set, demonstrating the ability to compositionally generalize. Nonetheless, the precise mechanism of compositionality and how it is acquired through training remains elusive. Inspired by cognitive neuroscientific approaches, we consider a highly reduced setting to examine whether and when diffusion models learn semantically meaningful and factorized representations of composable features. We performed extensive controlled experiments on conditional Denoising Diffusion Probabilistic Models (DDPMs) trained to generate various forms of 2D Gaussian data. We found that the models learn factorized but not fully continuous manifold representations for encoding continuous features of variation underlying the data. With such representations, models demonstrate superior feature compositionality but limited ability to interpolate over unseen values of a given feature. Our experimental results further demonstrate that diffusion models can attain compositionality with few compositional examples, suggesting a more efficient way to train DDPMs. Finally, we connect manifold formation in diffusion models to percolation theory in physics, offering insight into the sudden onset of factorized representation learning. Our thorough toy experiments thus contribute a deeper understanding of how diffusion models capture compositional structure in data.", "sections": [{"title": "1 Introduction", "content": "Large-scale text-to-image generative models can produce photo-realistic synthetic images that com-\nbine elements in a novel fashion (compositional generalization). Nonetheless, the ability of models to\ndo so, as well as their failure modes, are not well-studied systematically in large diffusion models\ndue to the size of the model and the complex and high-dimensional nature of their training datasets.\nFactorization and compositional generalization have been theoretically and empirically investigated\nin many deep generative models before Zhao et al. [2018], Higgins et al. [2017], Burgess et al. [2018],\nMontero et al. [2021], Xu et al. [2022], Okawa et al. [2023], Wiedemer et al. [2023], Bowers et al.\n[2016], Chaabouni et al. [2020]. However, these studies have not reached a unanimous conclusion\non whether factorized representations learned in the intermediate layers of the model promote com-\npositional generalization in the model performance. Specifically, several studies Xu et al. [2022],\nMontero et al. [2021], Chaabouni et al. [2020] have found little correlation between factorization and\ncompositionality, contrary to others that suggest that factorization promotes compositionality Bengio\net al. [2013], Higgins et al. [2017], Burgess et al. [2018], Duan et al. [2020], Bowers et al. [2016].\nAs a result, there is no consensus on the exact mechanism of compositionality and how models\ngain the ability to compositionally generalize. However, these previous studies involve data with a\ncomplex mix of discrete and continuously varying features, making it difficult to explicitly analyze\nthe model's learned representations beyond the disentanglement score, and thereby hindering a deeper\nunderstanding of factorization and compositionality."}, {"title": "2 Methods", "content": "Dataset. We generate N \u00d7 N pixel grayscale images. By\ndefault, we set N = 32 unless otherwise specified. Each image\ncontains one 2D Gaussian bump (\u201cblob\"), the multiplication of\na vertical and a horizontal 1D Gaussian stripe, or one 2D Gaus-\nsian addition, the sum of a vertical and a horizontal 1D Gaussian\nstripe (\"sum of stripes (SOS's)\"), at various x, y locations. The\nbrightness v(x,y) of the pixel at position (x, y) for a 2D Gaus-\nsian blob centered at ($\\mu_x$, $\\mu_y$) with standard deviation ($\\sigma_x$,$\\sigma_y$)\nis given as $v_{(x,y)} = 255 \\times (1 - e^{-(x-\\mu_x)^2/4\\sigma_x^2-(y-\\mu_y)^2/4\\sigma_y^2})$\nand as $v_{(x,y)}^{SOS} = 255 \\times [1-(e^{-(x-\\mu_x)^2/4\\sigma_x^2} + e^{-(y-\\mu_y)^2/4\\sigma_y^2})]$ for a 2D Gaussian SOS with the normalized range of v(x,y) to\nbe [0, 255]. Each image is generated with a ground truth label\nof ($\\mu_x$, $\\mu_y$), which continuously vary within [0, N]\u00b2 unless oth-\nerwise specified. In our convention of notation, we label the\ntop left corner of the image as (1, 1) while the bottom right corner of the image as (N, N). Sample\n32 \u00d7 32 images of 2D Gaussian bump and SOS centered at $\\mu_x$ = $\\mu_y$ = 16 with $\\sigma_x$ = $\\sigma_y$ = 1 are\nshown in Fig. 1.\nA single dataset of these images consist of the enumeration of all possible Gaussians tiling the whole\nN \u00d7 N canvas at increment $d_x$ in the x-direction and $d_y$ in the y-direction. A larger $d_x$ or $d_y$ means\na sparser tiling of the image space and fewer data while a smaller $d_x$ or $d_y$ result in more data with\ndenser tiling of the total image space. Moreover, each 2D Gaussian have independent spread in\nthe x and y direction given by $\\sigma_x$ and $\\sigma_y$, with a larger spread leading to more spatial overlap of\nneighboring Gaussians and a smaller spread less overlap. By parameterically tuning the increments\n$d_x$ and $d_y$ and the spread $\\sigma_x$ and $\\sigma_y$, we can generate datasets of various sparsities and overlaps. We\nprovide a more detailed analysis of the various attributes of the data based on these parameters in\nAppendix A.5. For the majority of the experimental results we show in Sec. 3, we have chosen to fix\n$\\sigma$ := $\\sigma_x$ = $\\sigma_y$ = 1.0 and $d$ := $d_x$ = $d_y$ = 0.1 unless otherwise specified. A default 32 \u00d7 32 dataset\nof $d$ = 0.1 contains 102400 images.\nModels & Evaluations. We train a conditional DDPM [Ho et al., 2020, Chen et al., 2021, Saharia\net al., 2023] with a standard UNet architecture as shown in Appendix Fig. 7. For each image in the\ntraining dataset, we provide an explicit ground truth label ($\\mu_x$, $\\mu_y$) as an input to the network. For\nreference, we investigate the internal representation learned by the model using the output of layer\n4 as labeled in Fig. 7. Since each dataset has inherently two latent dimensions, x and y, we use\ndimension reduction tools such as PCA or UMAP McInnes and Healy [2018] to reduce the internal\""}, {"title": "3 Results", "content": "3.  1 Models learn factorized but not fully continuous representations\nIn this section, we aim to understand the factorization of the model's learned representation via\nexplicit inspection of its topology and geometry. Given a conditional DDPM trained on the 2D\nGaussian bump dataset described above, we first investigate whether the model learns a coupled or\nfactorized representation. Na\u00efvely, a 2D Gaussian dataset with two independent features of variation,\nx and y, has a 2D plane-like latent representation. Unfortunately, simply inspecting the learned\nrepresentation would not allow us to easily differentiate between a coupled versus a factorized\nrepresentation since the Cartesian product of two lines $L^1 \\times L^1 \\subset R \\times R = R^2$ is topologically\nand geometrically equivalent to a plane P2 C R2. To overcome this issue, we perform a simple\nmodification to the Gaussian dataset. We impose periodic boundary conditions in the image space to\nconnect the left-right and top-bottom boundaries of the image such that the latent representation of\nthe dataset forms a torus. Mathematically, a torus is defined by the Cartesian product of two circles\n$S^1 \\times S^1$. A Clifford Torus is an example of a factorized representation of the torus, with each circle\nindependently embedded in R2, resulting in a 4-dimensional object. However, the most efficient\nrepresentation in terms of extrinsic dimensionality is the regular torus T2, which is embedded in\nR3, albeit unfactorized. Various 2D projections of the 3D torus and the Clifford (4D) torus are\nshown in Fig. 2(a). Due to geometric differences between the Clifford and the 3D torus, we can now\ndistinguish whether the model learns a factorized representation. If the model were to represent x and\ny independently with two ring manifolds, we expect to see a Clifford torus in the neural activations,\nrather than a different geometry such as the 3D torus.\nTo apply the geometry tests for differentiating between the 3D and the Clifford torus, we first need\nto confirm that the model indeed learns a torus representation of the dataset by computing the\ntopological features of the learned representation via persistent homology. In Fig. 2(b), we compare\nthe persistence diagrams of a standard torus (left) with the final learned representation of the model\n(right). Both diagrams exhibit the similar topological features, rank-1 Ho group, rank-2 H\u2081 group\n(with overlapping orange points on top in both diagrams), and rank-1 H2 group, signaling that the\nmodel indeed learns a torus representation.\nWe then investigate when this torus representation emerges during training. In Fig. 2(c), we show the\nmodel's performance as measured by the accuracy of its generated images (top) and the corresponding\neffective dimension (bottom) of the learned representations across training, as measured by the partic-\nipation ratio ( given by $(\\Sigma_i \\lambda_i)^2/\\Sigma_i \\lambda_i^2$, where $\\lambda_i$ is the eigenvalue of the i-th principal component\nGao et al. [2017]). Intuitively, the participation ratio counts the dominant eigen-components, provid-\ning a measure of the effective dimension of the geometric object of interest. A detailed inspection of\nthe effective dimension of the learned representations over the training duration informs us of whether\nand how the model arrives at a factorized, 4D representation. We see in Fig. 2(c) that as training\nprogresses, the model's internal representation first undergoes a dimension increase then a decrease,\neventually converging to around 7 dimensions after 200 epochs. While the dimensionality converged\nto a higher dimension than 4, the top 4 eigenvectors became notably more prominent as the training\nconverges, as indicated in the eigenspectra in Fig. 2(d). This signals that a 4D, rather than 3D, torus\nrepresentation is eventually learned. Various PCA projections of the learned representations along the\ntraining process are shown in Fig. 2(g)-(i), where the projections in the last epoch resembles those of\nthe Clifford torus shown in (a).\nTo confirm whether the model learns a Clifford torus, we employ the orthogonality and parallelism\ntests of tori Cueva et al. [2021b] (details found in Appendix B). In an ideal Clifford torus, the rings\nalong the poloidal direction should be parallel with each other, and similarly for those along the\ntoroidal direction. Moreover, the rings along the poloidal direction should be orthogonal with respect\nto the rings in the toroidal direction. The orthogonality and parallelism tests measure the orthogonality\nand parallelism of subspaces spanned by various rings on the torus. If the model learns a Clifford"}, {"title": "3.2 Models can compose but not interpolate", "content": "In this section, we examine the model's ability to compositionally generalize. Specifically, we train\nthe model on incomplete datasets of 2D Gaussian SOSs, in which we leave out all Gaussian SOSs\ncentered in the red-shaded test regions (Fig. 4(f), sampling distributions shown in Fig. 4(a), (b)).\nWe then assess the performance of the models in generating 2D Gaussian SOSs centered within and\noutside of the test regions. Here we choose the width of the cuts to be around 6 pixels wide, which\nroughly corresponds to the width of the Gaussian stripes of $\\sigma$ = 1.0. We design the lesions such that\nwe can probe the model's ability at compositionally generalizing out of the training distribution as\nwell as its ability to spatially interpolate in a single variable alone. There are 4 possible outcomes\nbased on the model's ability to compose and interpolate, and we give predictions for the model's\ngeneralization performance in each case: 1) the model cannot compose or interpolate: here, we would\nsee low performance across the test regions; 2) the model interpolates but cannot compose: here, we\nwould see high performance across the test regions; 3) the model composes but cannot interpolate:\nhere, we would see high performance in one dimension but not the other in the non-intersecting part\nof the test regions, low performance in the intersection; 4) the model can compose and interpolate:\nhere, we would see higher performance across the test regions. We note that case (2) and (4) are\nindistinguishable via the behavior of the model.\nIn our first experiment, we simply train our model on a 2D Gaussian SOS dataset where all data\ncentered in the test regions are left out, i.e. excluding all $\\mu_x$, $\\mu_y$ \u2208 [13, 19] as shown in Fig. 4(a). We\ncall this model the 2D model since the model is trained on only 2D Gaussian SOS data. We then\nexamine the terminal accuracy of the 2D model in generating Gaussian SOSs at the correct $\\mu_x$ and\n$\\mu_y$ in various parts of the test regions, which we section into a horizontal part, a vertical part, both\nexcluding the area of their intersection, and the intersection itself (shown schematically in Fig. 4(f)).\nWe note that the 2D model achieves high accuracy in generating $\\mu_y$ while suffers low accuracy in\ngenerating $\\mu_x$ in the vertical section. Similarly, the 2D model achieves high accuracy in generating $\\mu_x$\nwhile suffers low accuracy in generating $\\mu_y$ in the horizontal section. The model suffers low accuracy\nin generating both $\\mu_x$ and $\\mu_y$ in the intersection region. These observations have two implications: i)\nthe model is factorized and compositional since it is able to generate the correct $\\mu_x$ or $\\mu_y$ irrespective\nof the other; ii) the model has limited ability to spatially interpolate, which suggests that it does not\nlearn a fully continuous manifold in its activation space (see Fig. 10(c)). These observations meet\nour expectation for outcome case (3), when the model can compose but not interpolate and resonates\nour conclusion from Sec. 3.1 that the model has learned to factorize x and y, but has not learned a\nconsistent representation across all x's (and likewise y's). In Appendix C.3 Fig. 11, we quantitatively\nassess model's ability to interpolate as a function of the held-out range width in the data manifold and\nfound that model's ability to interpolate gradually decrease as a function of the held-out region width."}, {"title": "3.3 Connection between manifold learning and percolation theory", "content": "We have established that the model learns factorized (but not fully continuous) and compositional rep-\nresentations. Next, we identify a potential connection between manifold learning in diffusion models\nand percolation theory in physics, which provides a normative explanation as to how factorization (or\ncompositional generalization) emerges. To start, we note that our Gaussian bump datasets can be\napproximated as a simple Poisson Boolean (Gilbert's disk) model Gilbert [1961], Hanisch [1980],\na well-studied system in continuum percolation theory. Fig. 6(a) shows a schematic illustration of\nour dataset of Gaussian bumps approximated as disks of various widths on a 2D 32 \u00d7 32 lattice of\ngrid spacings $d_x$ and $d_y$. In percolation theory, the quantity of interest is the critical fraction of nodes\nthat need to have non-zero overlaps in order for the entire system to be interconnected with high\nprobability. For most systems (finite- and infinite-sized), there exist a phase transition that occurs at\nthe critical fraction that can be either analytically derived or numerically estimated, with the transition\nbecoming sharper as the system size scales. In our context, since the Gaussian bump images are\npixelated, overlaps between neighboring Gaussian bumps are not smooth. In Fig. 6(b), we plot the\nnormalized L2-norm of the neighboring distance-d-separated Gaussian bumps with various spread\n\u03c3's. We note that since the L2-norm measure of overlap is normalized to 1 between a given Gaussian\nbump and itself, the overlap between a given Gaussian bump and a slightly offset Gaussian bump\ntemporarily goes beyond 1 due to the discrete nature of the data.\nWe hypothesize that below a threshold amount of training data, the diffusion model cannot construct\na faithful representation of the training dataset. From the percolation perspective, if there does not\nexist a large enough interconnected component within the dataset, the model will fail to learn the\nrelative spatial location of the data points, making it hard to learn a faithful 2D representation. To test\nthis hypothesis, we first simulated the mass ratio of largest interconnected components as a function\nof the unit area data density \u5165 with our 2D Gaussian bump datasets. For simulation, we use a dataset\nof 1024 data points, corresponding to d := $d_x$ = $d_y$ = 1.0 on a 32 \u00d7 32 lattice. We then randomly\nsample X \u00d7 1024 points on the lattice to compute the size of the largest interconnected cluster of\nGaussian bumps. Here, we define a hyperparameter of threshold overlap beyond which we consider\ntwo Gaussian bumps as overlapping. In Fig. 6(c), we show the simulation results. averaged over 5\nruns, with the chosen threshold overlap of 0.005 for datasets with various \u03c3's. Additional simulation\nresults with various chosen overlap thresholds can be found in Appendix D.\nNext, we quantify the connection between percolation theory and manifold formation in diffusion\nmodels. To account for the stochasticity in sampling the datasets and avoid significant overhead in"}, {"title": "4 Discussion", "content": "In the previous section, we have shown that diffusion models are capable of learning factorized and\nsemi-continuous representations. This allows for compositional generalization across factors that\nhave appeared in the training dataset, even given only a small number of compositional examples.\nWhile we studied a toy system, our results imply that the diffusion model architecture has some\ninductive biases favoring factorization and compositionality, as seen in astonishing compositional\ntext-to-image generation examples such as an \u201castronaut riding a horse on the moon\". Our results\ndemonstrate further that if the training data include isolated factors of independent variation and\nsome compositional examples, diffusion models are capable of attaining high performance in OOD\ncompositional generalization. While it is true that natural images are much more complex, and\nthere can be numerous forms of compositionality within a single image, the datasets and modes of\ncomposition we studied were not trivial. Nonetheless, further investigation is necessary to understand\nthe compositionality and factorization of models when multiple forms of compositionality are at play.\nFurthermore, we showed a connection between the models' performance, as related to its ability to\nlearn a faithful representation of the dataset, and percolation theory in physics. Percolation theory\nprovides a plausible mechanistic interpretation of the observed sudden emergence in the model's\ncapability beyond a threshold number of data points. Further work is needed to characterize the\nprecise connection between percolation theory and manifold formation, in both toy settings and\nrealistic settings. In realistic datasets, mutual information or cosine similarity between data points\ncan serve as abstract forms of overlap. Moreover, the idea of percolation can be further extended\nto study alternative observations of phase transitions in deep learning, such as percolation of the\nchain-of-knowledge in large language models.\""}, {"title": "5 Conclusion", "content": "We have shown that diffusion models are capable of learning factorized representations that can\ncompositionally generalize OOD, given data containing the full range of each independent factor\nof variation and a small amount of compositional examples. Our study suggests that diffusion\nmodels have the inductive bias for factorization and compositionality, which are believed to be key\ningredients for scalability. We identified that the diffusion models fail to generalize out-of-distribution\nwhen 1) there are unseen values of a given factor of variation for the composition, 2) there are no\ncompositional examples in the training dataset, 3) there is an insufficient quantity of data, and 4)"}, {"title": "A.1 Architecture", "content": "We train a conditional denoising diffusion probabilistic model (DDPM) Ho et al. [2020] with a\nstandard UNet architecture of 3 downsampling and upsampling blocks, interlaced self-attention\nlayers, and skip connections as shown in Fig. 7. Each down/up-sampling blocks consist of max\npooling/upsampling layers followed by two double convolutional layers made up by convolutional\nlayers, group normalization, and GELU activation functions.\nThe conditional information is passed in at each down/up-sampling block as shown in the schematic\ndrawing. In our experiments, we choose to preserve the continuity of the Gaussian position labels\npassed into the model via explicit positional encoding rather than using a separate trainable embedding\nMLP at each block. Each embedding vector is made by concatenating equal-length vectors of the\npositional encodings of the timestep, the x-position, and the y-position.\nIn our experiments, we visualize the outputs of layer 4 as the internal representation of the diffusion\nmodel. We have chosen not to use the output of the bottleneck layer for our study of the learned\nlatent manifold, as we have observed that the bottleneck layers have diminishing signals in most of\nour experiments. This is likely due to the presence of the skip connections. This choice does not\naffect the validity of our main results, as we are focused on the factorization of the model's learned\nrepresentation."}, {"title": "A.2 Dimension Reduction", "content": "We primarily use the dimension reduction technique Uniform Manifold Approximation and Projection\nfor Dimension Reduction (UMAP) McInnes and Healy [2018] to study and visualize the learned\nrepresentation of the model. Specifically, we collect image samples and their corresponding internal\nrepresentations (outputs of layer 4 from the architecture described in Sec. A.1). We then transform\nthe high-dimensional internal representations into a 3D embedding as a sample of the learned"}, {"title": "A.3 Evaluation", "content": "We assess the performance of the model using two primary criteria: 1) the quality of the denoised\nimages and 2) the quality of the learned representation.\nAt a given time during or after training, we generate 1024 denoised images and their corresponding\ninternal representations of sampled labels based on 32 \u00d7 32 grid points. We predict the label\ncorresponding to each generated image based on the x- and y-positions of the generated Gaussian\nbump/SOS in the image. We then compute the accuracy of predicted labels from the ground-truth\nlabels averaged over 1024 samples as\n$Accuracy = \\frac{1}{1024} \\sum_{i=1}^{1024} 1(|\\hat{\\mu}_x^i - \\mu_x^i| < 1) \\cdot 1(|\\hat{\\mu}_y^i - \\mu_y^i| < 1),$ (1)\nwhere 1(\u00b7) is an indicator function that returns 1 if the expression within holds true, 0 otherwise.\nSimilarly, we can modify this expression to only assess the accuracy of generated x-positions or\ny-positions separately. Here we estimate the center of the Gaussian bump/SOS \u00fb and \u00fb by finding\nthe location of the darkest pixel in the image. In the cases where there are no Gaussian bumps/SOS's\nor more than one bump/SOS, the algorithm defaults back to finding the centroid of the image. We\nthen construct the learned representation of the model based on the neural activations of layer 4\ncorresponding to the 1024 sampled images collected at the terminal diffusion generation timestep.\nWe note that we have investigated neural activations collected at alternative diffusion generation\ntimesteps and found that they do not noticeably differ from timestep to timestep."}, {"title": "A.4 Training Loss", "content": "Diffusion models iteratively denoise a Gaussian noisy image $x_T$ into a noisefree image $x_0$ over\ndiffusion timesteps t \u2208 {0,1,...,T} given the forward distribution $q(x_t|x_{t-1})$ by learning the\nreverse distribution $p_\\theta(x_{t-1}|x_t)$. Given a conditional cue c, a conditional diffusion model [Chen\net al., 2021, Saharia et al., 2023] reconstructs an image from a source distribution $q(x_0|c)$. Specifically,\nwe train our neural network (UNet) to predict the denoising direction $e_\\theta(x_t, t, c)$ at a given timestep"}, {"title": "A.5 Datasets", "content": "The datasets we used for training the models generating the results in Sec. 3 have various increments d\nand \u03c3. Here we briefly comment on the interplay between increments and sigmas, and how they affect\ndataset densities and overlaps. The ultimate goal of our task of interest is to learn a continuous 2D\nmanifold of all possible locations of the Gaussian bumps/SOS's. Intuitively, the spatial information\nnecessary for an organized, continuous, and semantically meaningful representation to emerge is\nencoded in the overlap of the neighboring Gaussian bumps/SOS's, which is tuned via the parameters\nd and o. As we increase d, the size of the dataset gets scaled quadratically, resulting in denser tilings\nof the Gaussian bumps/SOS's.\nAs we scale up o, the dataset size remains fixed while the overlaps with neighbors are significantly\nincreased. In Fig. 6(b), we plot the normalized L2-norm of the product image of neighboring Gaussian\nbumps as a function of increments for various spreads. Specifically, given two inverted grayscale\nGaussian bump images, a and b, the normalized L2-norm of their product is given by the formula\n$||\\sqrt{a*b}||_2/||a||_2$, where * is element-wise multiplication and ||\u00b7||2 is the L2-norm. This quantity\nshould give a rough measure of the image overlap with the exception at increment around 0.5 due to\nthe discrete nature of our data. Moreover, we note that the cusps in the curves occur for the same\nreason. As we can see, the number of neighbors that a given Gaussian bump has non-trivial overlaps\nwith grows roughly linearly to sub-linearly with the spread. Since the model constructs non-parallel\nrepresentation encodings of different values of x's and y's, more neighbor information results in\nmore overlaps between different values of x's and y's and hence better representation learned, as\nshown in the percolation results in Fig. 6 in Sec. 3.3. Overlaps in Gaussian SOS data can be similarly\nanalyzed, albeit the differences in overlaps of neighboring Gaussian SOS's of various spread o's is\nnot as pronounced due to the large coverage area of each Gaussian stripe."}, {"title": "A.6 Training Details", "content": "We train the models on a quad-core Nvidia A100 GPU, and an average training session lasts around\n6 hours (including intermediate sampling time). Each model is given an inversely proportional\namount of training time as a function of the size of the dataset that it is trained on such that there is\nsufficient amount of training for the models to converge. For each model, we sample 1024 samples\ncorresponding to the 32\u00d732 grid points of $\u00b5_x$ and $\u00b5_y$ pairs tiling the entire image space. The model\nis trained with the AdamW optimizer with built-in weight decay, and we employ a learning rate\nscheduler from Pytorch during training. We did not perform any hyperparameter tuning, although we\nwould expect our observations to hold regardless."}, {"title": "B Orthogonality and Parallel Test", "content": "B.1 Motivation for the Analysis\nTo better identify whether the diffusion models learn to factorize its representations, we choose to\nstudy periodic representations of Gaussian bumps, which we expect to form toroidal manifolds in the\nlatent space as opposed to planar manifolds. This is because toroidal manifolds have multiple standard\nrealizations which vary based on their degree of entanglement in the two rings. In particular, the\nstandard torus embeds the rings in three dimensions that couple the two periodic variables, whereas\nthe Clifford Torus embeds each ring in its own orthogonal subspace. This difference in geometry\nmotivates the two analyses that we implemented to identify which geometry the diffusion model\nlearned."}, {"title": "B.2 Computation of the Tests", "content": "We implement the orthogonality and parallelism tests as in Cueva et al. [2021a]. Briefly, the\northogonality test asks whether the rings that code for each variable lie in orthogonal subspaces-if so,\nthis indicates that the torus has a Clifford-like geometry. The parallelism test asks whether rings in\none variable are aligned in the other variable. If so, this also indicates that the torus has a Clifford-like\ngeometry.\nThe analysis first requires identifying each ring-surprisingly, we found that when the rings formed,\nthe top four Principal Components comprised them. In particular, the first ring were consistently\nspanned by PCs 1 and 3, and the second was spanned by PCs 2 and 4 (Fig. 2(f) and (g)). Thus, we\nconfirmed that we could use PCA for our analysis.\nFor each analysis, we identify the ring subspaces by fixing one variable of the input, and sweeping\nthe other variable. We do so for each index, arriving at 32 different subsets of the data for variable x\nand 32 different points for variable y. Next, we compute the top two Principal Components for each\nsubset of the data, thereby identifying the ring.\nFor the orthogonality test, we randomly draw pairs of rings that are fixed in x and y and compute the\npairwise angles between each rings' Principal Components via cosine similarity. For the parallelism\ntest, we compute the randomly drawn pairs of rings that are fixed in the same variable. Next, we create\na projection matrix P from the subspaces defined by one of the rings, and calculate the reconstruction\nerror of the second ring's projection onto that subspace:\nr = |h \u2013 PPTh|3. (3)\nIn each test, we aggregate histograms of the respective metric. To identify the similarity between\nour data and either toroidal embedding, we compute the same histograms for both the Clifford and\nstandard torus (based on a simulation with similar spacing in x and y samples), and compute the\nWasserstein distance between the data and these simulations. The Wasserstein distances between\nthe orthogonality and parallelism test statistics of the model's learned representation and those of an\nideal Clifford torus and 3D torus are shown in Fig. 8 as a function of the training epochs. We note\nthat the model's learned representations at smaller training epochs are not necessarily a torus, hence\nthe metric comparison at those points has less significance."}, {"title": "C Composition and Interpolation", "content": "C.1 Gaussian Stripe/SOS Dataset Generation\nTo generate 1D Gaussian stripe dataset shown in Fig. 4(b) while maintaining the structure of the 2D\nconditional embeddings, we embed the 32 \u00d7 32 data latent space into an extended 44 \u00d7 44 latent space\nwhile maintaining the image pixel size to be 32 \u00d7 32. Functionally, this allows us to mix into the 2D\nGaussian SOS datasets 1D (and 0D) Gaussian data, as defined by the portion of the Gaussian that is\nactually visible to the model in the 32 \u00d7 32 pixels. Examples of 0D, 1D, and 2D Gaussian SOS data\nare shown in Fig. 9(a)-(c), respectively. Fig. 9(e)-(g) then show the actual data latent representations\nof the 2D, 1D, and 2D+1D datasets that we use to train 2D, 1D, and 2D+1D models described in\nSec. 3.2. Here, the green points represent each individual 2D Gaussian SOS data, the blue points\nrepresent the 1D Gaussian SOS data, and the yellow points the 0D Gaussian SOS data. In all three\ndatasets, we leave out the all Gaussian SOS data centered in the red shaded regions. We note that\ndue to the nature of our data generation scheme, the 2D data only exist within the image space (and\nsome spillover) and the 1D (and 0D) data only exist in the extended data latent space. Because of the\nnonzero width of the Gaussian stripes, a portion of the data along the rim of the 32 \u00d7 32 image space\nis actually 2D with a portion of the overlaps between the horizontal and the vertical stripes visible\nwithin the image frame. In training the 1D model shown in Fig. 4, we have removed the rim where\nany overlap between the horizontal and vertical Gaussian stripes is partially visible within the image,\nas shown in Fig. 9(f). The model trained on such a 1D dataset without any compositional example is,\nas discussed in Sec. 3.2, not able to handle the conjunction of the 2 Gaussian stripes, as shown in\nFig. 10(a)-(c)."}, {"title": "C.2 Generalization with Few Compositional Examples", "content": "In Sec. 3.2, we have seen that the models are sufficiently data efficient in learning the compositionality\ngiven a few 2D examples. Here we compare the performance of models trained on a pure 1D Gaussian\nstripe dataset versus a 1D Gaussian stripe dataset + a small amount 2D compositional examples along\nthe image rim, where the conjunction of the stripes are only partially visible within the data images.\nHere we name the pure 1D Gaussian stripe dataset as 1D stripes without 2D rim and the alternative\nas 1D stripes with 2D rim. The data latent representations of both datasets are shown in Fig. 10(a),\n(d) and the overall, in-distribution, and out-of-distribution sampled image accuracy of models trained\non these datasets are shown in (b"}]}