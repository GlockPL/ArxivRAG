{"title": "Large Language-Geometry Model: When LLM meets Equivariance", "authors": ["Zongzhao Li", "Jiacheng Cen", "Bing Su", "Wenbing Huang", "Tingyang Xu", "Yu Rong", "Deli Zhao"], "abstract": "Accurately predicting 3D structures and dynamics of physical systems is crucial in scientific applications. Existing approaches that rely on geometric Graph Neural Networks (GNNs) effectively enforce E(3)-equivariance, but they often fall in leveraging extensive broader information. While direct application of Large Language Models (LLMs) can incorporate external knowledge, they lack the capability for spatial reasoning with guaranteed equivariance. In this paper, we propose EquiLLM, a novel framework for representing 3D physical systems that seamlessly integrates E(3)-equivariance with LLM capabilities. Specifically, EquiLLM comprises four key components: geometry-aware prompting, an equivariant encoder, an LLM, and an equivariant adaptor. Essentially, the LLM guided by the instructive prompt serves as a sophisticated invariant feature processor, while 3D directional information is exclusively handled by the equivariant encoder and adaptor modules. Experimental results demonstrate that EquiLLM delivers significant improvements over previous methods across molecular dynamics simulation, human motion simulation, and antibody design, highlighting its promising generalizability.", "sections": [{"title": "1 Introduction", "content": "Accurately predicting 3D structures/dynamics of physical systems remains a fundamental challenge in physics and biology. Typical tasks such as molecular dynamics simulation (Hollingsworth & Dror, 2018) and antibody design (Tiller & Tessier, 2015) require not only a deep understanding of complex spatial geometry but also the preservation of E(3)-equivariance \u2014 ensuring predictions transform correspondingly with input rotations, reflections and translations (Batzner et al., 2022; Huang et al., 2022). From the machine learning perspective, E(3)-equivariant models are more powerful than their non-equivariant counterparts, as they are inherently generalizable across arbitrary coordinate systems when modeling physical systems. To achieve equivariance, current approaches primarily rely on geometric Graph Neural Networks (GNNs) (Wu et al., 2024; Kong et al., 2022). Despite their fruitful progress, these models often lack the ability to leverage external domain knowledge and broader contextual information, such as task-specific instructions and expert-curated guidance, hindering further performance enhancement.\nRecently, Large Language Models (LLMs) have demonstrated remarkable success across a wide range of applications, owing to their large-scale pretraining on extensive datasets and their substantial model size. It is well known that LLMs can not only understand and generate text but also excel at integrating and leveraging scientific knowledge (Liu et al., 2025; Jablonka et al., 2024; Wang et al., 2023). For instance, LLMs can comprehend fundamental chemical concepts and molecular structural characteristics (Guo et al., 2023). More significantly, LLMs' flexibility in prompt engineering enables the development of tailored instructions that better leverage their capabilities, producing outputs more precisely suited to the task.\nA natural idea is to directly employ LLMs for modeling 3D physical systems. However, this approach fails to yield satisfactory results in practice. A key limitation is that LLMs are trained to process ordered and discrete text tokens, restricting their ability to directly comprehend unordered and continuous data in 3D space. One possible solution is to adapt existing multimodal LLM architectures, such as LLaVA (Liu et al., 2024b), by treating 3D structures as a separate modality and simply replacing the image encoder with a geometric GNN. However, this naive adaptation fails to satisfy the E(3)-equivariance requirement. Since geometric GNNs produce both invariant features and equivariant coordinates, passing these outputs through an LLM inevitably compromises equivariance. Therefore, it is non-trivial to integrate the strengths of both LLMs and geometric GNNs while maintaining essential geometric properties.\nTo this end, this paper introduces EquiLLM, a novel framework for representing 3D physical systems that seamlessly"}, {"title": "2 Related Work", "content": "Geometric GNN. Due to its thorough consideration of physical symmetries in 3D space, Geometric GNN (Satorras et al., 2021; Fuchs et al., 2020) has been widely applied in various scientific tasks, such as dynamics simulation (Xu et al., 2024) in physics and antibody design (Lin et al., 2024) in biology. ESTAG (Wu et al., 2024) enhances the model's simulation capability for object dynamics trajectories in both spatial and temporal dimensions through interleaved Equivariant Spatial Module and Equivariant Temporal Module. SEGNO (Liu et al., 2024c) incorporates the second-order graph neural ODE with equivariant property to reduce the roll-out error of long-term physical simulation. MEAN (Kong et al., 2022) not only leverages an Internal context encoder to model spatial correlations within antibody chains but also introduces an External attentive encoder with an attention mechanism to better capture inter-chain relationships. GeoAB (Lin et al., 2024) extends the equivariant model GMN (Huang et al., 2022) to handle binary, ternary, and quaternary interactions and applies it in atom-level updates. Despite significant progress these methods have made, they fail to explore how to harness the capabilities of LLMs to augment model performance.\nLLM + GNN. Large Language Models (LLMs) with rich knowledge are being widely transferred and applied across multiple domains to enhance model capabilities (Singhal et al., 2023; 2025). Numerous excellent works have emerged in combining GNNs with LLMs for scientific applications. ChemLLMBench (Guo et al., 2023) tests LLM's understanding, reasoning, and explaining capabilities on various chemical tasks using in-context learning. Prot2Text (Abdine et al., 2024) integrates protein sequence, structure, and textual annotations into an encoder-decoder framework composed of GNN and LLM to predict protein functions. MoleculeSTM (Liu et al., 2023a) uses a contrastive learning paradigm to align molecular graphs and textual descriptions in the semantic space, thereby learning better feature representations. MolCA (Liu et al., 2023b) employs Q-Former (Li et al., 2023) as a cross-modal projector to align the feature spaces of graph encoder and language encoder, enhancing performance in molecule captioning tasks. Although the aforementioned methods promote interactions between GNNs and LLMs through various paradigms and yield promising results, they have yet to explore tasks involving 3D structural data, such as 3D structure generation and dynamic trajectory simulation in 3D space. The EquiLLM framework we propose in this paper is the first to integrate LLMs with Geometric GNNs that embed spatial symmetry constraints and has been effectively validated across datasets from both physical and biological domains."}, {"title": "3 Method", "content": "In this section, we first introduce the preliminaries related to geometric modeling in \u00a7 3.1. Next, in \u00a7 3.2, we present the proposed framework EquiLLM. Finally, in \u00a7 3.3, we describe how the EquiLLM framework is applied to two representative tasks (e.g. dynamic simulation and antibody design). The overview of our EquiLLM is illustrated in Fig. 1."}, {"title": "3.1 Preliminaries, Notations and Definitions", "content": "Physical systems (such as molecules) can be naturally modeled with geometric graphs. We represent each static system as a geometric graph $G = (V, E)$, where each node $v_i$ in V is associated with an invariant feature $h_i \\in \\mathbb{R}$ (e.g. atom type) and an 3D equivariant vector $Z_i \\in \\mathbb{R}^3$ (e.g. atom coordinates); each edge (e.g. chemical bonds) denotes the connectivity between nodes. Apart from modeling static systems, we explore dynamic systems, focusing on constructing geometric graphs across different time steps. The details will be thoroughly discussed in \u00a7 3.3.1. In the following sections, we use the matrices $X \\in \\mathbb{R}^{N\\times 3}$ and $H \\in \\mathbb{R}^{N\\times C}$ to denote the sets of node coordinates and invariant features of the geometric graph G.\nTask Formulation. Here, we provide a general form of our task and will elaborate specific applications including dynamic simulation and antibody design in \u00a7 3.3. Given the input geometric graph $G^{in}$, our goal is to find a function $\\Phi$ to predict the output $G^{out}$. This process can be formally delineated as:\n$G^{out} = \\Phi(G^{in}).$ (1)\nMeanwhile, since we introduce LLMs into our framework, we will further construct task-specific prompts to guide the extraction of relevant domain knowledge from LLMs, recasting our task as:\n$G^{out} = \\Phi(G^{in}, P),$ (2)\nwhere P denotes the prompt.\nEquivariance. It is crucial to emphasize that in the tasks above, the function $\\Phi$ must satisfy E(3) symmetries of physical laws. Specifically, if arbitrary translations, reflections, or rotations are applied to the input coordinate matrix $X^{in}$, the output coordinate matrix $X^{out}$ should undergo the corresponding transformation."}, {"title": "3.2 Large Language Geometric Model", "content": "In this section, we provide a meticulous description of our model EquiLLM, which consists of three main components: Equivariant Encoder, LLM, and Equivariant Adapter. Unlike existing works (Gruver et al., 2024) that applys an LLM to predict the 3D coordinates directly, EquiLLM leverages an LLM to acquire broader scientific domain knowledge while employing geometric GNNs for precise modeling of 3D structures. These two components are seamlessly integrated through an equivariant adapter, achieving superior predictive performance without compromising E(3)-equivariance.\nEquivariant Encoder. The Equivariant Encoder is a domain-specific equivariant model, which can be any suitable equivariant model from the relevant field. The model takes the graph $G^{in} = (V^{in}, \\mathcal{E}^{in})$ as input, performing initial encoding and embedding of geometric information, and outputs a processed geometric graph $G' = (V', \\mathcal{E}')$, This process can be formally defined as:\n$G' = \\Phi_e(G^{in}),$ (3)\nwhere $\\Phi_e$ can be any equivariant model, used to jointly model the geometric relationships between $X^{in}$ and $H^{in}$ features across different nodes, resulting in processed features $X'$ and $H'$.\nSince LLMs are not naturally equivariant, directly feeding $X'$ into an LLM would likely undermine the intrinsic equivariance of the overall architecture. Thus, in contrast to existing works, we convey the invariant features $H'$ to the LLM, but pass the equivariant matrix $X'$ to the subsequent"}, {"title": "Equivariant Adapter", "content": "via a skip connection. Before feeding $H'$ to the LLM, we first conduct a projector on $H'$ to align its dimension with the input space of the LLM. This process can be formally characterized as:\n$H^{proj} = \\Phi_{proj}(H'),$ (4)\nwhere $\\Phi_{proj}$ is implemented as a linear layer in EquiLLM.\nGeometric-aware Prompt. One may directly input the aligned features $H^{proj}$ into the LLM to make the final predictions. However, this approach overlooks the pivotal role of the prompt, as it does not utilize the linguistic form of the prompt to effectively harness the LLM's comprehension and articulation of the specific task at hand. Therefore, in the EquiLLM framework, we carefully design task-specific prompts for different tasks to unleash domain-specific knowledge.\nThe prompt content for all tasks can be broadly divided into three key components: (1) task description, (2) object feature description, and (3) object statistical information.\nTask description. The task description consists of two parts: <Task> and <Requirement>.  appears at the beginning of the prompt, providing a succinct description of the task to help the LLM quickly identify the task's objective.  is located in the main body of the prompt and elaborates on the input-output requirements and constraints of the task, ensuring a comprehensive understanding of the task by the LLM.\nObject feature description. The feature description of the input object begins with  and primarily outlines the composition information as well as the structural characteristics of the input object.\nObject statistical information. This components starts with , encapsulating detailed metrics pertaining to the distribution of the object's coordinates in 3D space, including the maximum, minimum, and mean values. It is crucial to note that, unlike conventional tasks, directly incorporating absolute coordinate values into the prompt is not recommended in 3D spatial modeling tasks. This is due to the fact that transformations such as translation, reflection, or rotation applied to the input object will invariably alter the corresponding coordinate distribution, thereby violating the principle that the prompting process must remain E(3)-invariant. Consequently, we represent the coordinate distribution of the input object indirectly by computing statistical metrics related to distances.\nLarge Language Model (LLM). After designing the prompt, we employ the tokenizer and embedding layer of the LLM to obtain the corresponding word embedding features, denoted as P. Subsequently, depending on the specific task, we concatenate P with the invariant features $H^{proj}$ in an appropriate way. The concatenation strategies for different"}, {"title": "Equivariant Adapter", "content": "Upon obtaining the output from the LLM, we extract the part corresponding to the invariant features, denoted as $H^{llm}$. While directly utilizing it for final predictions may be viable for invariant tasks (e.g. predicting the energy of a molecular system), it is inadequate for equivariant tasks, where the core objective is to predict the 3D coordinates of objects. To address this challenge, we propose the Equivariant Adapter, which leverages one-layer EGNN to process $H^{llm}$ while minimizing the introduction of excessive additional parameters. Specifically, we first employ a projection layer to re-project $H^{llm}$ back into the space corresponding to the invariant features $H'$ and add it with $H'$, yielding the refined feature representation $H''$. Then, both the equivariant coordinate features $X'$ from Equivariant Encoder and the refined invariant features $H''$ are transmitted to the EGNN, yielding the output $X^{out}$ and $H^{out}$. The whole process is formally expressed as:\n$m_{ij} = \\Phi_m(h_i, h_j, ||x_i - x_j||),$ (6)\n$h^{out}_i = h'_i + \\Phi_h \\left( h_i, \\sum_{j \\in \\mathcal{N}(i)} m_{ij} \\right),$\n$x^{out}_i = x'_i + \\sum_{j \\in \\mathcal{N}(i)} \\Phi_x(m_{ij}) (x_j - x_i),$\nwhere $\\Phi_m$, $\\Phi_h$, and $\\Phi_x$ denote Multi-Layer Perceptrons (MLPs), and $\\mathcal{N}(i)$ refers to the set of neighboring nodes associated with the i-th node. Specifically, $m_{ij}$ represents an E(3)-invariant message transmitted from node j to node i, which is utilized to aggregate and refine the feature vector $h'_i$ via the function $\\Phi_h$. Regarding the update of $x'_i$, the function $\\Phi_x$ is employed to compute a scalar $\\Phi_x(m_{ij})$, which is subsequently multiplied by the difference $x_j - x_i$ to retain directional information, while incorporating residual connections to ensure translation equivariance.\nOur EquiLLM framework guarantees that the overall architecture preserves the critical property of E(3)-equivariance in 3D space while also avoiding the introduction of lengthy text context due to direct 3D coordinate input, which could severely affect the efficiency of training and inference. Moreover, compared with domain-specific Equivariant Encoder, EquiLLM introduces only two projection layers and"}, {"title": "4 Experiments", "content": "We validate the effectiveness of the proposed EquiLLM framework on two tasks from different domains: the dynamic simulation in physics (\u00a7 4.1) and the antibody design in biology (\u00a7 4.2). Furthermore, in \u00a7 4.3, we conduct ablation studies, explore potential design variations of the EquiLLM framework, and discuss why the current framework is superior."}, {"title": "4.1 Dynamic Simulation", "content": "In the dynamic simulation task, to demonstrate the broad applicability of our model across varying scales, we conduct experiments on two distinct datasets: the molecular-level MD17 (Chmiela et al., 2017) dataset and the macro-level Human Motion Capture (De la Torre et al., 2009) dataset. In order to expedite the dynamics simulations, we implement a sampling strategy based on previous research (Huang et al.,"}, {"title": "4.1.1 MOLECULAR DYNAMICS", "content": "Implementation details. MD17 consists of time-evolving paths produced through molecular dynamics simulation for eight different small compounds (such as aspirin, benzene, and others). To ensure a fair comparison, all hyperparameters (e.g. learning rate, number of training epochs) are kept consistent across our model and all other baselines. We utilize the ESTAG (Wu et al., 2024) as the Equivariant Encoder and GPT-2 (Radford et al., 2019) \u00b9 as the language model within our EquiLLM framework.\nResults. Table 1 presents the performance of all models on MD17 dataset under the setting of predicting 10 frames from an input of 10 frames. From the table, the following conclusions can be drawn: 1. The proposed EquiLLM framework achieves state-of-the-art (SOTA) performance on"}, {"title": "4.1.2 HUMAN MOTION SIMULATION", "content": "Implementation details. The Human Motion Capture dataset contains human motion trajectory data across multiple scenes. We focus primarily on two sub-datasets: Subject #35 (Walk) and Subject #102 (Basketball). To ensure a fair comparison, all hyperparameters (e.g. learning rate, number of training epochs) are kept consistent across our model and all other baselines. We utilize the ESTAG as the Equivariant Encoder and GPT-2 as the language model within our EquiLLM framework.\nResults. Table 2 presents a performance comparison of all models on the Walk and Basketball datasets under settings requiring the prediction of 10, 15, and 20 frames, respectively. From the table, it is evident that EquiLLM framework achieves SOTA performance across all six settings, with a performance improvement ranging from 5.63% to 36.11%. This demonstrates that EquiLLM effectively handles predictions over varying prediction lengths, exhibiting excellent robustness and generalization ability"}, {"title": "4.2 Antibody Design", "content": "Following previous study MEAN (Kong et al., 2022), we selected complete antibody-antigen complexes from the SAbDab (Dunbar et al., 2014) dataset to construct the training and validation sets. First, we performed clustering based on CDRs, grouping complexes with CDR sequence identity above 40% into the same cluster. Then, the training and validation sets were partitioned in the same manner as in MEAN. For test set, we selected 60 diverse complexes from the RAbD (Adolf-Bryfogle et al., 2018) dataset to evaluate"}, {"title": "4.3 Ablation studies", "content": "In this section, we delve into the design of the EquiLLM framework, analyzing the impact of different architectural designs and prompt configurations on model performance. The experimental results are shown in Table 4, where EE represents the Equivariant Encoder.\nArchitecture Design. 1. The results in the second row indicate that processing raw features through the LLM before feeding them into the Equivariant Encoder, while omitting the Equivariant Adapter for a simpler architecture, yields a performance improvement. This finding validates that the LLM has a fundamental capability to process and integrate structured information effectively. 2. However, to fully exploit the potential of the LLM model, it is necessary to leverage prompts to capitalize on its strengths in text understanding. Building upon the second-row model, we perform experiments by adding prompts, as the results shown in the third row. The results indicate a significant performance drop. We speculate that this is due to the large semantic space difference between the unprocessed raw feature H and the text features, which hampers the model's prediction capabilities. This suggests that LLM requires an appropriate interface to harness its advantages. This led to the design of the current EquiLLM framework.\nPrompt Design. 3. From Table 4, it is evident that either completely removing the prompt or reducing its content (such as the Object Feature or Statistics) leads to a decline in performance. This observation reinforces our design philosophy: LLMs necessitate comprehensive information, including molecular descriptions and statistical constraints, to fully utilize their knowledge integration and constraint reasoning capabilities. This, in turn, facilitates more accurate predictive guidance."}, {"title": "5 Conclusion", "content": "We present EquiLLM, a framework that synergizes the strengths of LLMs and geometric GNNs to address the dual challenges of E(3)-equivariance and knowledge integration in 3D physical system modeling. By introducing geometry-aware prompting and a modular architecture that isolates invariant and equivariant processing, EquiLLM circumvents the inherent limitations of LLMs in spatial reasoning while enabling the infusion of domain-specific knowledge through flexible prompting strategies. The separation of roles-LLMs as invariant feature processors and geometric GNNs as directional information handlers provides a principled approach to preserving symmetry constraints. In future work, we plan to explore optimal prompting strategies for better leveraging domain knowledge and extending this framework to broader scientific tasks. We hope the EquiLLM framework will serve as a valuable reference for applying LLMs in scientific domains."}]}