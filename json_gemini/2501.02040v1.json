{"title": "A Separable Self-attention Inspired by the State Space Model for Computer Vision", "authors": ["Juntao Zhang", "Shaogeng Liu", "Kun Bian", "You Zhou", "Pei Zhang", "Jianning Liu", "Jun Zhou", "Bingyan Liu"], "abstract": "Mamba is an efficient State Space Model (SSM) with linear computational complexity. Although SSMs are not suitable for handling non-causal data, Vision Mamba (ViM) methods still demonstrate good performance in tasks such as image classification and object detection. Recent studies have shown that there is a rich theoretical connection between state space models and attention variants. We propose a novel separable self-attention method, for the first time introducing some excellent design concepts of Mamba into separable self-attention. To ensure a fair comparison with ViMs, we introduce VMINet, a simple yet powerful prototype architecture, constructed solely by stacking our novel attention modules with the most basic down-sampling layers. Notably, VMINet differs significantly from the conventional Transformer architecture. Our experiments demonstrate that VMINet has achieved competitive results on image classification and high-resolution dense prediction tasks. Code is available at: https://github.com/yws-wxs/VMINet", "sections": [{"title": "I. INTRODUCTION", "content": "Modern State Space Models (SSMs) excel at capturing long-range dependencies and reap the benefits of parallel training. The Vision Mamba (ViM) methods [1]\u2013[4], which are inspired by recently proposed SSMs [5], [6], utilize the Selective Scan Space State Sequential Model (S6) to compress previously scanned information into hidden states, effectively reducing quadratic complexity to linear. Many studies integrate the original SSM framework from Mamba into their foundational models to balance performance and computational efficiency. However, Mamba is not the first model to achieve global modeling with linear complexity. Linear attention [7] replaces the non-linear Softmax function with linear normalization and adds a kernel function to both query and key, allowing for the reordering of computation based on the associative property of matrix multiplication, thereby reducing the computational complexity to linear. Separable self-attention [8] is also an early work that replaces the computationally expensive operations (e.g., batch-wise matrix multiplication) in Multi-headed Self-Attention (MHA) with element-wise operations (e.g., summation and multiplication). However, because of the limited expressive capabilities of separable self-attention and its variants, they are typically suitable for lightweight vision Transformers that have been carefully designed.\nPrevious studies on ViM have identified a fundamental contradiction between the non-causal characteristics of 2D spatial patterns in images and the causal processing framework of SSMs. Flattening spatial data into 1D tokens destroys the local 2D dependencies in the image, thereby impairing the model's capacity to accurately interpret spatial relationships. Vim [1] addresses this issue by scanning in bidirectional horizontal directions, while VMamba [2] adds vertical scanning, enabling each element in the feature map to integrate information from other locations in different directions. Subsequent works, such as LocalMamba [3] and EfficientVMamba [4], have designed a series of novel scanning strategies. These efforts aim to expand the receptive field of the SSM from the previous token to others, which may result in a multiple-fold increase in the computational cost of the scanning process. Macroscopically, we attribute the success of ViMs to the combination of global information modeling and the establishment of local dependencies, unified by a well-designed architecture.\nIn this paper, we first establish design principles by analyzing the strengths and weaknesses of separable self-attention, classical softmax self-attention, and SSMs. We then confine the receptive field of separable self-attention to the previous token. Furthermore, we introduce the recursive form of our proposed separable self-attention, thereby expressing both SSMs and our method within a unified framework. We refer to this method as the Vision Mamba Inspired Separable Self-Attention (VMI-SA). Finally, we restore the receptive field of our VMI-SA to maintain the advantages of separable self-attention in parallel computing. We construct a demonstrative network, VMINet, by stacking VMI-SA with down-sampling layers. Clearly, the structure of VMINet has not been carefully designed, and it does not adhere to the conventional architectural design principles of the Transformer. For a fair comparison, we keep the number of VMI-SAs consistent with the number of Mamba blocks in Vim [1], and the parameters are roughly equivalent. Experiments demonstrate that our VMINet consistently outperforms Vim and is also competitive with other state-of-the-art models."}, {"title": "II. PRELIMINARIES", "content": "This section provides a review of the attention mechanism and SSM. To facilitate understanding of their relationship, we standardize the notation of variables.\nSoftmax Self-Attention. In a broad sense, attention refers to a computational process that assigns scores to each pair of positions within a sequence, allowing each element to \"attend\" to other elements. The most widely used and significant variant of attention is the softmax self-attention, which can be defined as:\n$Y = softmax(QK^T) \\cdot V$ (1)\nwhere $Q, K, V \\in R^{(L,D)}$ respectively represent L tokens with D dimensions, each generated by a linear transformation from the input $X \\in R^{(L,C)}$. The attention scores between each pair of tokens in Q and K are computed using the dot product operation. Subsequently, interactions are normalized using softmax. Finally, the weighted interactions are multiplied by V using the dot product operation to produce the final weighted output. The pairwise comparison mechanism, realized by computing $QK^T$, results in a quadratic growth in the attention's training cost.\nSeparable Self-Attention. The structure of separable self-attention is inspired by Softmax Self-Attention. Similar to softmax self-attention, the input $X \\in R^{(L,C)}$ is processed using three branches: $Q \\in R^{(L,1)}$, $K \\in R^{(L,D)}$ and $V \\in R^{(L,D)}$. Notably, Q maps each token in X to a scalar, distinguishing it from the other branches. First, context scores are generated through Softmax(Q). Then, based on broadcasting mechanism, the context scores are then element-wise multiplied with K and the resulting vector is summed over the token dimension to obtain the context vector. Finally, the context vector is multiplied by V using broadcasted element-wise multiplication to spread the contextual information and produce the final output. It can be summarized as:\n$Y = (\\sum_{i=1}^L softmax(Q)_i K_i ) \\cdot V$ (2)\nHere, $ \\odot$ denotes element-wise multiplication, $K_i$ represents the feature vector of the i-th token in K. The process follows the broadcasting mechanism throughout.\nSelective State Space Model. Selective State Space Sequence Model (S6) is a recent sequence model for deep learning, which is widely related to RNNs, CNNs, and classical SSMs. Their inspiration stems from a specific continuous system that, through an implicit latent state $h \\in R^{(D,L)}$, maps a one-dimensional sequence $x \\in R^L$ to another one-dimensional sequence $y \\in R^L$ [9]. The mapping process could be denoted as:\n$h_i = A_i h_{i-1} + B_i x_i$\n$y_i = C_i h_i$ (3)\nwhere $i \\in [1, L]$, $A \\in R^{(D,D)}$, $B \\in R^{(D,1)}$ and $C \\in R^{(D,1)}$. Here, A, B and C vary with the input x."}, {"title": "III. METHODOLOGY", "content": "In this section, we first analyze the impact of the key differences in design between separable self-attention and softmax self-attention. Then, while retaining the advantages of the self-attention design, we optimize the separable self-attention according to the design method of SSM. Our goal is to clearly demonstrate the design process of Vision Mamba Inspired Separable Self-Attention (VMI-SA), to show the innovations and how performance can be enhanced by integrating the strengths of both Mamba and separable self-attention. Finally, we introduce the overall architecture of the proof-of-concept network VMINet.\nA. Element-wise Multiplication Instead of Matrix Multiplication\nIn both traditional machine learning and deep learning, handling features in high-dimensional space is crucial. We employ a straightforward derivation to establish that both element-wise multiplication and matrix multiplication can map the features from their original dimensions to a higher-dimensional space, which is crucial for feature representation.\nWe adopt the definition method from Section 2, let $X \\in R^{(L,C)}$, $W_1 \\in R^{(C,D)}$, $W_2 \\in R^{(C,D)}$, $Q = XW^1$, $K = XW^2$, $E = Q \\odot K$. For any element $E_{m,n}$ in E (where m \u2208 [1, L], and n \u2208 [1, D]):\n$E_{m,n} = Q_{m,n} \\times K_{m,n}$\n$= (\\sum_{i=1}^C X_{m,i} W_{i,n}^1) \\times (\\sum_{j=1}^C X_{m,j} W_{j,n}^2)$\n$= \\sum_{i=1}^C \\sum_{j=1}^C W_{i,n}^1 W_{j,n}^2 X_{m,i} X_{m,j}$\n$= a_{(1,1)} X_{m,1} X_{m,1} + \\cdots + a_{(C,C)} X_{m,C} X_{m,C}$ (4)\nwhere a is a coefficient for each item:\n$a_{(i,j)} = \\begin{cases} W_{i,n}^1 W_{j,n}^2 & \\text{if } i == j, \\\\ W_{i,n}^1 W_{j,n}^2 + W_{j,n}^1 W_{i,n}^2 & \\text{if } i != j \\end{cases}$ (5)\nEach term in Eq. (4) exhibits a nonlinear relationship with the input. It can be approximated as that the element-wise multiplication operation projects the feature vector in the C-dimensional space into a higher-dimensional space of $C^2$ dimensions through a nonlinear transformation and processes it.\nNow let's discuss the case of matrix multiplication. Let $E' = Q \\cdot K^T$, where any element $E'_{m,n}$:\n$E'_{m,n} = \\sum_{t=1}^D Q_{m,t} \\times K_{t,n}$\n$= \\sum_{t=1}^D (\\sum_{i=1}^C X_{m,i} W_{i,t}^1) \\times (\\sum_{j=1}^C X_{t,j} W_{j,n}^2)$ (6)\nTypically, we consider D to be a constant and $D << L$. Comparing Eq. (6) with Eq. (4), it is evident that from the perspective of dimensionality amplification, the element-wise multiplication with a linear cost is more efficient than the matrix multiplication with a quadratic cost in terms of computational efficiency.\nB. Context Vector Instead of Attention Matrix\nThe context vector is analogous to the attention matrix $softmax(QK^T)$ in Eq. (2) in a sense that it also encodes the information from all tokens in the input X [8], but is cheap to compute. Comparing Eq. (4) and Eq. (6), it can be observed that $E_{m,n}$ is merely the encoding of the m-th token, while $E'_{m,n}$ represents the encoding concerning both the m-th and n-th tokens. The softmax and summation operations also provide a global receptive field for separable self-attention, but the performance difference between separable self-attention and softmax self-attention indicates that establishing correlations between tokens is essential. We speculate that this is also the reason why networks adopting separable self-attention or its variants, such as MobileViT [8] and SwiftFormer [10], need to alternately stack the attention modules with local feature encoding modules and feedforward neural network modules. In fact, this perspective is also supported by evidence in ViMs. The SSM restricts the receptive field to the previous token, yet it is still applicable for visual tasks. In addition, it is easy to observe from Eq. (2) and Eq. (4) that, due to the sharing of parameters across different tokens, the global context information formed by the simple summation operation results in the weights of each token being identical, making the computation process of Eq. (2) lack \"attention\". Therefore, in Eq. (2), the context vector is element-wise multiplied with V, which, aside from mapping features to a higher dimension, does not have much clear significance.\nAdditionally, we can analyze the performance differences between softmax self-attention and separable self-attention from the perspective of the rank of the attention matrix. The higher the rank of the attention matrix, the more attention information it contains, and the richer the feature diversity. The attention matrix $softmax(QK^T)$ in Eq. (1) is usually full rank [11], that is $rank(softmax(QK^T)) = L$. The attention information in the context vector comes from $softmax(Q) \\odot V$ in Eq. (2), and its rank:\n$rank(softmax(Q) \\odot V) \\le rank(V) \\le min\\{L, D\\}$ (7)\nTherefore, the attention information in $softmax(Q) \\odot V$ is not only less abundant but also severely homogenized.\nC. Vision Mamba Inspired Separable Self-Attention\nSummarizing the analysis, the previous discussion provides the following four insights for the design of new separable self-attention:\nIntroduce local correlations.\nContinue to use element-wise multiplication for context encoding while reducing the computational branches.\nEnhancing the rank of attention matrices or equivalent counterparts.\nUtilize learnable weights to adjust the intensity of each token's contribution to the context information.\n1) Macro Design: Our objective is to implement the aforementioned four design philosophies using the simplest and most direct approach, thereby improving the original separable self-attention mechanism without introducing superfluous functional blocks. Due to the generation of contextual information through element-wise multiplication, there is no need to flatten 2D image data into a one-dimensional sequence. Compared to some common Transformers and ViMs, processing features in 2D space can maintain the spatial correlation of features, avoiding the additional inductive bias introduced by Patch Embedding. Secondly, it can reduce the reshaping operations, which is beneficial for improving the inference speed. As previously mentioned, element-wise multiplication can encode the features for individual tokens in pairs, but it cannot establish correlations between tokens. Therefore, the simplest and most effective improvement is to use a depthwise convolution (DW-Conv) layer to establish local spatial correlations before the element-wise multiplication.\nNext, we consider how to enhance the rank of the attention matrix (or equivalent counterparts). Clearly, for any matrix $A \\in R^{(L,D)}$ with all elements being non-zero, assuming L > D, setting the elements of the upper triangular (or lower triangular) part of A to zero can maximize the rank of the matrix, that is:\n$M = \\begin{bmatrix} 1 & 1 & 1 & \\cdots & 1 \\\\ & 1 & 1 & \\cdots & 1 \\\\ & & \\ddots & & \\vdots \\\\ & & & & 1 \\\\ & & & & 1 \\end{bmatrix}$ (8)\n$rank(M) = min\\{L, D\\} = D$,\nwhere $M \\in R^{(L,D)}$. The primary network structure of VMI-SA is shown in Fig. 1.\n2) Recurrent Form: Selective State Space Models preserve and compress global information through hidden states. Han et al. [12] have highlighted that the forget gate plays a significant role in selective State Space Models. Restricting the receptive field of linear attention to the previous token and introducing a forget gate mechanism can also improve performance, but recurrent computation severely affects the model's throughput. It can be observed that in the shallow layers of the network, each token mainly focuses on itself and the two preceding tokens; as the network depth increases, the attention range of each token gradually enlarges. The work of Han et al. indicates that for attention mechanisms with linear computational complexity, the combination of local and global information contributes to forming more effective attention, although their contributions vary at different stages.\nLike Eq. (3), we first restrict the receptive field to the previous token and preserve past information through a hidden state. The recursive form of the VMI-SA is as follows:\n$h_i = h_{i-1} + \\alpha_i (Q_i \\odot V_i)$\n$Y_i = M_i h_i + \\beta_i (Q_i \\odot V_i)$ (9)\nwhere $X \\in R^{(H,W,C)}$, $W_1 \\in R^{(C,D)}$, $W_2 \\in R^{(C,D)}$,$Q = DW-Conv(X)W^1$, $V = DW-Conv(X)W^2$, $L = H * W$, $i \\in [1,L]$, $M \\in R^{(L,D)}$ is a triangular matrix with all non-zero elements equal to 1, $\\alpha_i$ and $\\beta_i$ are a series of trainable parameters that control the importance of each token in contextual information, as well as the proportion of local information to global information in attention. Like Mamba, we also do not use softmax.\n3) Matrix Form: Just as Eq. (3) does, Eq. (9) also leads to the model being sensitive to input sequences, which is not conducive to the model processing non-causal data (such as images). Therefore, we remove the restriction on the receptive field, allowing all tokens to receive global information, and Eq. (9) is transformed into:\n$Y = \\sum_{i=1}^L Expand(M \\odot Q_i \\odot V_i) + \\beta_i \\odot Q \\odot V$ (10)\nwhere Expand() denotes the operation of expanding a vector of shape (1, D) into a matrix of shape (L, D).\nD. VMINet\nAs shown in Fig. 2, VMINet adopts a common 4-stage hierarchical architecture, utilizing convolutional layers for downsampling, and employing VMI-SA blocks for feature extraction. To ensure a fair comparison with the Vim [1], which uses a pure Mamba encoder, we set the number of VMI-SA blocks to be the same as the number of Mamba blocks with a comparable parameter count."}, {"title": "IV. EXPERIMENTS", "content": "This section presents our experimental results, starting with the ImageNet classification task and then transferring the trained model to various downstream tasks, including object detection and instance segmentation."}, {"title": "A. Image Classification", "content": "Settings. We train the models on ImageNet-1K and evaluate the performance on ImageNet-1K validation set. For fair comparisons, our training settings mainly follow Vim. Specifically, we apply random cropping, random horizontal flipping, label-smoothing regularization, mixup, and random erasing as data augmentations. When training on 224 \u00d7 224 input images, we employ AdamW with a momentum of 0.9 and a weight decay of 0.025 to optimize models. During testing, we apply a center crop on the validation set to crop out 224 \u00d7 224 images. We train the VMINet models for 300 epochs using a cosine schedule. Unlike Vim, our experiments are performed on 3 A6000 GPUs. Therefore, we adjusted the total batch size and the initial learning rate to 384 and 5 \u00d7 10-4 respectively.\nResults. We selected advanced CNNs, ViTs, and ViMs with comparable parameters and computational costs in recent years to compare with our method, and the results are shown in Table II. The various variants of VMINet are identical in every aspect except for the difference in embedding width. The experimental results demonstrate that VMINet overwhelmingly outperforms Vim [1], which utilizes a pure Mamba encoder. Compared to PlainMamba [18], which also employs depthwise convolutions, our VMINet exhibits significant advantages in terms of performance, efficiency, and model complexity. This suggests that VMI-SA is more suitable for visual tasks than Mamba. Furthermore, although VMINet is a demonstrative network architecture that has not been meticulously designed, it still achieves competitive results across various scales, particularly in lightweight scenarios. This suggests that the analysis presented in the previous sections may serve as a guiding principle, potentially reducing the unnecessary attempts researchers might make when designing attention mechanisms or general visual backbone networks."}, {"title": "B. Ablation Study", "content": "Setting aside the design philosophy, due to structural similarities, a reasonable suspicion is that the superior performance of VMINet may primarily be attributed to the introduction of depthwise separable convolutions. As shown in Fig. 3, for VMINet-S, after removing attention-related operations such as element-wise matrix multiplication and context vector generation, VMI-SA degenerates into a block similar to a ConvNeXt block [23]. Although this slightly reduces the number of parameters and computational complexity, the accuracy decreases from 80.2% to 77.6%."}, {"title": "C. Object Detection and Instance Segmentation", "content": "Settings. We use Mask-RCNN as the detector to evaluate the performance of the proposed VMINet for object detection and instance segmentation on the MSCOCO 2017 dataset. Following ViTDet [24], we only used the last feature map from the backbone and generated multi-scale feature maps through a set of convolutions or deconvolutions to adapt to the detector. The remaining settings were consistent with Swin [22]. Specifically, we employ the AdamW optimizer and fine-tune the pre-trained classification models (on ImageNet-1K) for both 12 epochs (1\u00d7 schedule). The learning rate is initialized at 1 \u00d7 10\u22124 and is reduced by a factor of 10\u00d7 at the 9th and 11th epochs.\nResults. We summarize the comparison results of VMINet with other backbones in Table III. It can be seen that our VMINet consistently outperforms Vim. Similar to the results on classification tasks, VMINet achieves a good balance between the number of parameters and computational cost, achieving comparable results with advanced CNNs and ViTs."}, {"title": "V. CONCLUSION", "content": "This paper presents a separable self-attention inspired by the visual Mamba (VMI-SA), which has a linear complexity. Through analysis and derivation, we believe that the element-wise multiplication operation used by separable self-attention can map the original features to a high-dimensional space for processing, which is more efficient than the matrix multiplication operation used by the classical softmax self-attention. Inspired by the Mamba design philosophy, we first establish local relevance through depthwise convolution, then limit the receptive field to the previous token, and then integrate local and global information according to the recursive state-space model, finally restoring the global receptive field and presenting the matrix form of VMI-SA. We believe that our work can provide a new perspective for the design of future attention mechanisms, that is, by changing the expression and constraints under a unified theoretical framework, to integrate the advantages of different methods. Currently, the research on VMI-SA is still in its infancy, and we believe that with reasonable network structure design, VMI-SA can further improve performance. In addition, the recursive form of VMI-SA is suitable for processing causal data and may be able to compete with other advanced methods in other fields."}]}