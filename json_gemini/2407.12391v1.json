{"title": "LLM Inference Serving: Survey of Recent Advances and Opportunities", "authors": ["Baolin Li", "Yankai Jiang", "Vijay Gadepally", "Devesh Tiwari"], "abstract": "This survey offers a comprehensive overview of recent advancements in Large Language Model (LLM) serving systems, focusing on research since the year 2023. We specifically examine system-level enhancements that improve performance and efficiency without altering the core LLM decoding mechanisms. By selecting and reviewing high-quality papers from prestigious ML and system venues, we highlight key innovations and practical considerations for deploying and scaling LLMs in real-world production environments. This survey serves as a valuable resource for LLM practitioners seeking to stay abreast of the latest developments in this rapidly evolving field.", "sections": [{"title": "I. INTRODUCTION", "content": "Large language models (LLMs) have rapidly gained immense popularity since the release of ChatGPT. However, deploying and scaling these powerful AI models in production environments has presented significant challenges. The substantial computational and memory demands of LLMs often necessitate the use of high-performance GPU servers, yet even these resources can be strained by the sheer size of the models and the lengthy text sequences they process.\nThe growing demand for LLM-powered applications has fueled a surge of research into LLM serving systems. In this paper, we present a comprehensive survey of these systems, focusing on advancements since 2023. While previous LLM system research existed, the landscape has dramatically shifted within the last year. Nearly every major system conference now features dedicated sessions on LLMs, with a particular emphasis on serving systems due to their widespread deployment and the importance of low-latency performance for user experience.\nThe sheer volume of research published in such a short time-frame makes it difficult for LLM practitioners to stay abreast of developments and identify the most promising approaches for real-world deployment. This survey aims to provide a clear overview of the current state of the art, highlighting key areas of innovation and practical considerations for production environments.\nIn this survey, we have meticulously selected all the high-quality research papers focused exclusively on LLM serving systems, published between January 2023 and June 2024. Our selection criteria prioritized publications from prestigious machine learning (ML) and system venues (e.g., ASPLOS, MLSys, OSDI), as well as impactful arXiv submissions from established industry and academic research groups. Notably, we exclude studies that modify LLM decoding algorithms (e.g., multiple decoding head [1], lookahead decoding [2], key token selection [3]) and solely focus on system-level enhancements that maintain the integrity of standard LLM decoding processes.\nWhile a few prior LLM inference system surveys exist [4], [5], [6], these generally cover a broader scope and do not specifically emphasize system research. Additionally, many of the papers discussed in those surveys involve decoding algorithm modifications that can affect model accuracy. Our survey, in contrast, explicitly focuses on system-level solutions that do not alter the core LLM decoding mechanisms. Moreover, our survey encompasses a significant body of research published after the release of these earlier surveys, thus providing a more comprehensive and up-to-date overview of the field.\nWe have organized the recent advances in LLM serving systems into four distinct categories, each with its own set of challenges and opportunities, which we will delve into in the following sections.\nKV cache and memory management. Efficient memory management is crucial to handle the dynamic growth of KV caches, which store previous key-value pairs to accelerate LLM inference. Recent research explores non-contiguous memory allocation, distributed management, and intelligent caching strategies to optimize memory utilization. Compression techniques are also being investigated to reduce the over-all memory footprint, ultimately enhancing LLM performance and scalability by allowing for longer context lengths and lower memory overhead.\nLLM computation optimization. Efforts to optimize LLM computation focus on request batching to maximize resource utilization. Additionally, disaggregating the inference process into prefill and decode phases enables independent optimization and hardware specialization. Model parallelism, employing various techniques, facilitates efficient execution across multiple GPUs. These strategies collectively enhance LLM execution efficiency and hardware utilization.\nCloud LLM deployment. Cloud platforms provide a scalable and cost-effective foundation for LLM inference. However, challenges remain in optimizing costs and resource utilization. Research is addressing this through techniques such as spot instance management, serverless optimizations, intelligent resource allocation, and power management. Additionally, strategies like cloud task co-location and token delivery optimization enhance user experience and overall cloud efficiency.\nEmerging research fields. Emerging areas in LLM serving include retrieval-augmented generation (RAG) and mixture-of-experts (MoE) inference. RAG faces challenges related to"}, {"title": "II. BACKGROUND", "content": "Mainstream LLMs are built on multiple transformer blocks [7]. Each identical transformer primarily consists of self-attention-based Multi-head Attention (MHA) operations and Feed-Forward Networks (FFN). Initially, the transformer applies three weight matrices (WQ,WK,WV) to the input X (encoded representation of input text sequence) to compute queries Q, keys K, and values V. Then, the Self-attention is calculated as:\nQ = XW\u00ae; K = XWK; V = XWV\nAttention(Q, K, V) = softmax(\\frac{QKT}{\\sqrt{dk}})V\nThis is the calculation of one attention head (Hi), and multiple heads are concatenated and linearly projected into the final attention result:\nH\u2081 = Attention(XW,XW,XW)\nMulti-Head Attention = Concat(H1, H2, ..., Hh)W\u00b0\nMHA makes transformers focus on different parts of the sequence in different representational spaces. Next, following the MHA block, the normalized output is fed into a position-wise FFN, which consists of two linear transformations with a ReLU activation.\nFFN(x) = max(0,xW1+b1)W2 + b2\nThe FFN can be applied separately to each position, further refining the information captured by the MHA block. The output will have the same dimension as the input X. Fig. 1 provides a visualization of the LLM architecture.\nLLM inference generates output tokens autoregressively [8] based on the initial input sequences P, referred to as Prompts. This process is divided into two major phases: the prefill phase and the decoding phase. The prefill phase is essential for setting up the model to generate text efficiently, while the"}, {"title": "III. MEMORY MANAGEMENT AND CACHING", "content": "In this section, we explore memory management techniques to mitigate memory footprint and access overhead during LLM inference. While model parameters remain constant and intermediate activations are relatively small, the KV cache used to store attention information \u2013 grows substantially with the number of generated tokens. Therefore, recent research has focused on efficient KV cache management to enable larger batch sizes and longer context processing.\nPagedAttention [10] identifies that the KV cache dynamically grows and shrinks over time as the model generates new tokens, but the request generation lifetime and length are not known a priori. Thus, it proposes to manage the KV cache as non-contiguous memory blocks. Compared to"}, {"title": "IV. COMPUTATION TASK SCHEDULING", "content": "Besides memory and KV cache management, the computation of LLM also presents significant system challenges. Due to the sequential dependency between tokens during the autoregressive generation, LLM can only generate one token at a time for each request. Thus, LLM inference workloads are less resource-efficient than training workloads on GPU hardware that is designed for massively parallel execution. Following this incentive, we investigate system solutions that optimize the scheduling of computation tasks during the inference process."}, {"title": "V. LLMS IN THE CLOUD", "content": "LLM deployments are computationally intensive and often require significant infrastructure to run effectively. Cloud platforms offer a scalable and cost-effective solution for deploying LLMs, eliminating the need for expensive hardware investments. The flexibility of cloud deployment allows or-ganizations to easily adjust resources as needed, ensuring optimal performance and minimizing downtime. However, the significant costs associated with cloud computing resources and the challenge of ensuring their efficient utilization can be major obstacles for LLM service providers."}, {"title": "VI. EMERGING RESEARCH FIELDS", "content": "Retrieval-Augmented Generation (RAG) [44] is a technique that enhances LLMs by incorporating external information sources. It addresses the limitations of LLMs in retaining factual knowledge and their tendency to generate inaccurate or fabricated information (hallucinations). RAG operates in two stages: retrieval and generation. During retrieval, the system identifies the most relevant contexts from an external knowledge base or corpus based on the given query. Once the relevant contexts are retrieved, they are integrated into the LLM's generation process in different processes including concatenation (where the retrieved contexts are simply appended to the query) and cross-attention (where the LLM attends to the retrieved contexts during generation)."}, {"title": "VII. CONCLUSION", "content": "This survey has presented a comprehensive overview of recent advancements in LLM serving systems, emphasizing the importance of system-level solutions for enhancing performance and efficiency. We have highlighted key innovations for deploying and scaling LLMs, paving the way for the future development of LLM serving systems."}]}