{"title": "LLM Inference Serving: Survey of Recent Advances and Opportunities", "authors": ["Baolin Li", "Yankai Jiang", "Vijay Gadepally", "Devesh Tiwari"], "abstract": "This survey offers a comprehensive overview of recent advancements in Large Language Model (LLM) serving systems, focusing on research since the year 2023. We specifically examine system-level enhancements that improve performance and efficiency without altering the core LLM decoding mechanisms. By selecting and reviewing high-quality papers from prestigious ML and system venues, we highlight key innovations and practical considerations for deploying and scaling LLMs in real-world production environments. This survey serves as a valuable resource for LLM practitioners seeking to stay abreast of the latest developments in this rapidly evolving field.", "sections": [{"title": "I. INTRODUCTION", "content": "Large language models (LLMs) have rapidly gained immense popularity since the release of ChatGPT. However, deploying and scaling these powerful AI models in production environments has presented significant challenges. The substantial computational and memory demands of LLMs often necessitate the use of high-performance GPU servers, yet even these resources can be strained by the sheer size of the models and the lengthy text sequences they process.\nThe growing demand for LLM-powered applications has fueled a surge of research into LLM serving systems. In this paper, we present a comprehensive survey of these systems, focusing on advancements since 2023. While previous LLM system research existed, the landscape has dramatically shifted within the last year. Nearly every major system conference now features dedicated sessions on LLMs, with a particular emphasis on serving systems due to their widespread deployment and the importance of low-latency performance for user experience.\nThe sheer volume of research published in such a short time-frame makes it difficult for LLM practitioners to stay abreast of developments and identify the most promising approaches for real-world deployment. This survey aims to provide a clear overview of the current state of the art, highlighting key areas of innovation and practical considerations for production environments.\nIn this survey, we have meticulously selected all the high-quality research papers focused exclusively on LLM serving systems, published between January 2023 and June 2024. Our selection criteria prioritized publications from prestigious machine learning (ML) and system venues (e.g., ASPLOS, MLSys, OSDI), as well as impactful arXiv submissions from established industry and academic research groups. Notably, we exclude studies that modify LLM decoding algorithms (e.g., multiple decoding head [1], lookahead decoding [2], key token selection [3]) and solely focus on system-level enhancements that maintain the integrity of standard LLM decoding processes.\nWhile a few prior LLM inference system surveys exist [4], [5], [6], these generally cover a broader scope and do not specifically emphasize system research. Additionally, many of the papers discussed in those surveys involve decoding algorithm modifications that can affect model accuracy. Our survey, in contrast, explicitly focuses on system-level solutions that do not alter the core LLM decoding mechanisms. Moreover, our survey encompasses a significant body of research published after the release of these earlier surveys, thus providing a more comprehensive and up-to-date overview of the field.\nWe have organized the recent advances in LLM serving systems into four distinct categories, each with its own set of challenges and opportunities, which we will delve into in the following sections.\nKV cache and memory management. Efficient memory management is crucial to handle the dynamic growth of KV caches, which store previous key-value pairs to accelerate LLM inference. Recent research explores non-contiguous memory allocation, distributed management, and intelligent caching strategies to optimize memory utilization. Compression techniques are also being investigated to reduce the overall memory footprint, ultimately enhancing LLM performance and scalability by allowing for longer context lengths and lower memory overhead.\nLLM computation optimization. Efforts to optimize LLM computation focus on request batching to maximize resource utilization. Additionally, disaggregating the inference process into prefill and decode phases enables independent optimization and hardware specialization. Model parallelism, employing various techniques, facilitates efficient execution across multiple GPUs. These strategies collectively enhance LLM execution efficiency and hardware utilization.\nCloud LLM deployment. Cloud platforms provide a scalable and cost-effective foundation for LLM inference. However, challenges remain in optimizing costs and resource utilization. Research is addressing this through techniques such as spot instance management, serverless optimizations, intelligent resource allocation, and power management. Additionally, strategies like cloud task co-location and token delivery optimization enhance user experience and overall cloud efficiency.\nEmerging research fields. Emerging areas in LLM serving include retrieval-augmented generation (RAG) and mixture-of-experts (MoE) inference. RAG faces challenges related to"}, {"title": "II. BACKGROUND", "content": "Mainstream LLMs are built on multiple transformer blocks [7]. Each identical transformer primarily consists of self-attention-based Multi-head Attention (MHA) operations and Feed-Forward Networks (FFN). Initially, the transformer applies three weight matrices ($W^Q, W^K, W^V$) to the input X (encoded representation of input text sequence) to compute queries Q, keys K, and values V. Then, the Self-attention is calculated as:\n$Q = XW^Q; K = XW^K; V = XW^V$\n$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$\nThis is the calculation of one attention head ($H_i$), and multiple heads are concatenated and linearly projected into the final attention result:\n$H_i = Attention(XW^Q_i, XW^K_i, XW^V_i)$\n$Multi\\text{-}Head\\ Attention = Concat(H_1, H_2, ..., H_h)W^O$\nMHA makes transformers focus on different parts of the sequence in different representational spaces. Next, following the MHA block, the normalized output is fed into a position-wise FFN, which consists of two linear transformations with a ReLU activation.\n$FFN(x) = max(0, xW_1 + b_1)W_2 + b_2$\nThe FFN can be applied separately to each position, further refining the information captured by the MHA block. The output will have the same dimension as the input X. Fig. 1 provides a visualization of the LLM architecture."}, {"title": "B. Overview of LLM Inference", "content": "LLM inference generates output tokens autoregressively [8] based on the initial input sequences P, referred to as Prompts. This process is divided into two major phases: the prefill phase and the decoding phase. The prefill phase is essential for setting up the model to generate text efficiently, while the decoding phase handles the generation of subsequent tokens.\nLet the input prompt $P = [P_1, P_2, ..., P_n]$, during the prefill phase, a new token is generated, denoted as $P_{n+1}$, and the new K and V are cached as $[(k_1, v_1), (k_2, v_2), ..., (k_n, v_n)]$.\nThe decoding phase is where the model generates new tokens autoregressively. The LLM predicts the next token, appends the newly generated token $P_{n+1}$ to the original prompt P, and updates the KV cache. Note that the KV cache grows linearly with the number of tokens generated."}, {"title": "III. MEMORY MANAGEMENT AND CACHING", "content": "In this section, we explore memory management techniques to mitigate memory footprint and access overhead during LLM inference. While model parameters remain constant and intermediate activations are relatively small, the KV cache used to store attention information \u2013 grows substantially with the number of generated tokens. Therefore, recent research has focused on efficient KV cache management to enable larger batch sizes and longer context processing."}, {"title": "A. Efficient Management of KV Cache", "content": "PagedAttention [10] identifies that the KV cache dynamically grows and shrinks over time as the model generates new tokens, but the request generation lifetime and length are not known a priori. Thus, it proposes to manage the KV cache as non-contiguous memory blocks. Compared to contiguous KV cache, non-contiguous KV cache management significantly reduces the memory waste on pre-allocation and fragmentation. Due to its efficient memory management using pages, PagedAttention has become an industry norm in LLM serving frameworks, supported by TGI [11], vLLM [10] and TensorRT-LLM [12].\nDespite its success, researchers still identify its weakness as PagedAttention requires rewriting attention kernels to accommodate the non-contiguous memory blocks, its memory manager adds software complexity and redundancy, and introduces performance overhead. Recently, vAttention [13] was proposed to retain the KV cache in contiguous virtual memory. It leverages pre-existing low-level system calls for demand paging, which is a standard operating system feature to reduce the software complexity. vAttention overlaps memory allocation with computation, pre-allocates memory ahead of time, and defers memory reclamation to hide the latency of memory allocation and improve the overall performance of the system.\nBesides system memory management, other efforts have addressed application-specific KV cache efficiency. Prompt Cache [14] designs specific prompt schema for users to submit their requests, so that attention states from these pre-defined modules (e.g., system prompt) can be reused across multiple prompts. AttentionStore [15] identifies that human interactions with applications such as ChatGPT are mostly multi-turn conversations. However, LLM engines would discard the KV cache when the user session becomes inactive to free up HBM space for other active sessions and re-compute the whole KV cache again when the session becomes active, leading to extra pre-filling costs. AttentionStore utilizes slower-mediums (e.g., CPU memory and disk), overlaps KV cache loading with computation, and designs intelligent pre-fetching and eviction policies."}, {"title": "B. Support for Long-Context Applications", "content": "Serving long-context LLM applications is particularly challenging as the size of the KV cache scales with the number of tokens. The limited memory limits LLM's ability to handle long sequences, demanding more memory-efficient solutions. Ring attention [16] is a novel distributed approach that leverages blockwise computation of attention and feedforward of long sequences across multiple devices. It efficiently overlaps KV cache communication with computation and extends the context length by the device count times. Infinite-LLM [17] is another distributed solution, it breaks down KV cache into smaller manageable units called rBlocks across GPUs/CPUs, and efficiently manages them with dynamic memory sharing and coordination. MemServe [18] unifies handling of inter-request and intra-request optimizations for LLM serving by introducing MemPool, a distributed memory pool to manage KV cache across all cluster memory and employs a global scheduler to maximize KV cache reuse.\nWhen the context grows larger than the GPU memory limit, most systems offload the KV cache to the CPU. InfiniGen [19] is a solution that speculates the important KV cache entries by rehearsing the attention computation of the current layer in the preceding layer and prefetches only the essential entries to the GPU, thereby reducing the data transfer overhead. LoongServe [20] introduces a new parallelism paradigm called Elastic Sequence Parallelism (ESP) to dynamically adapt to resource usage variance between requests and phases (pre-filling and decoding) of a request. It reduces KV cache migration overhead and KV cache fragmentation when serving long sequences."}, {"title": "C. Compression of KV Cache", "content": "Due to the large memory footprint of LLM serving, some systems have resorted to compressing the KV cache. On top of memory aggregation and communication scheduling, FlexGen [21] uses fine-grained groupwise quantization to compress the weights and KV cache to 4 bits. KIVI [22] analyzes the element distribution of the LLM KV cache and applies asymmetric quantization of the Key and Value cache. KIVI quantizes the key cache per-channel (grouping elements along the channel dimension) and the value cache per-token to achieve minimum quantization error. Gear [23] achieves near-lossless high-ratio KV cache compression by quantizing the majority of entries of similar magnitudes and employs a low-rank matrix to approximate the quantization error. MiniCache [24] observes that the KV cache states exhibit high similarity between adjacent layers in the middle-to-deep portion of LLMs. Based on this insight, MiniCache leverages this high similarity to merge them into a shared representation to reduce redundancy, while also identifying and retaining distinct states that are crucial for maintaining the model's performance, preventing information loss during compression."}, {"title": "IV. COMPUTATION TASK SCHEDULING", "content": "Besides memory and KV cache management, the computation of LLM also presents significant system challenges. Due to the sequential dependency between tokens during the autoregressive generation, LLM can only generate one token at a time for each request. Thus, LLM inference workloads are less resource-efficient than training workloads on GPU hardware that is designed for massively parallel execution. Following this incentive, we investigate system solutions that optimize the scheduling of computation tasks during the inference process."}, {"title": "A. Request Batching", "content": "When a single request cannot efficiently utilize the GPU, it is intuitive to batch multiple inference requests together to boost the occupancy of GPU cores. However, as responses to different prompts can have significantly variable lengths, when batched together, the shorter responses are forced to wait for the longer ones to complete, resulting in computational waste. Response Length Perception and Sequence Scheduling [25] instructs the LLM to predict the response length before starting to generate the actual response, and batches queries with similar predicted response lengths to reduce computational waste. A similar approach, S\u00b3 [26], finetunes a Distillbert model for sequence length prediction. Upon mispredictions, it preempts sequences that exceed their allocated memory and retrain the predictor to learn from its mistakes.\nGeneration length prediction based batching is less practical due to the strong reliance on the predictor. Orca [27] proposes continuous batching at the token level rather than the request level. It continuously schedules new requests into the batch as soon as a request in the current batch completes. Continuous batching now has become an industry standard in LLM serving frameworks, incorporated into the software of TGI, vLLM, and TensorRT-LLM. Based on continuous batching, DeepSpeed-FastGen [28] proposes a dynamic SplitFuse mechanism that decomposes long prompts into smaller chunks scheduled across multiple iterations and composes short prompts together to maintain the inference running at high throughput region (bounded by GPU compute not memory bandwidth). A similar idea was explored in Sarathi-Serve [29], which splits prefill requests into smaller chunks and schedules them alongside ongoing decode requests without causing stalls (stall-free batching). This allows new requests to join a running batch without pausing ongoing decodes, leading to minimal pipeline bubbles."}, {"title": "B. Disaggregated Inference", "content": "LLM inference goes through a prefill stage to process the prompt, populate the KV cache, and start the decoding stage to generate tokens (Sec. II). Existing LLM serving systems colocate the two phases and batch the computation of prefill and decoding across all users and requests. However, these two phases display distinct characteristics and can interfere with each other when requests at the prefill stage are batched with requests at the decoding stage. TetriInfer [30] separates prefill and decode instances, allowing each phase to run independently and preventing interference between batch-like prefill jobs and latency-critical decode tasks. It employs a two-level scheduling algorithm that incorporates predicted resource usage to avoid scheduling hotspots during the decode phase, ensuring efficient resource allocation and minimizing contention.\nSplitwise [31] extensively characterizes the differences in the execution and utilization patterns of the prefill and decoding stage on different generations of GPUs (heterogeneous hardware). Splitwise proposes to split these two phases into separate machines, allowing for specialized hardware for each phase to achieve better utilization, reduce hardware ownership costs, and save energy. DistServe [32] designs a placement algorithm to schedule the prefill and decoding stage computation tasks. In clusters with high-speed cross-node networks, DistServe optimizes parallelism configurations for prefill and decoding instances independently to achieve the best per-GPU goodput; In clusters with limited cross-node bandwidth, it ensures that prefill and decoding instances of the same stage are co-located within a single node and optimizes parallelism configurations within the node."}, {"title": "C. Model Parallelism", "content": "LLMs can have hundreds of billions of parameters, requiring model parallel execution on multiple GPUs. Pope et al. [9] develop an analytical model for inference efficiency, enabling the selection of optimal multi-dimensional partitioning techniques tailored for TPU v4 slices based on specific application needs. HeteGen [33] introduces a framework for heterogeneous parallel computing using CPUs and GPUs. It employs a heterogeneous parallel computing algorithm to distribute computation within its hybrid heterogeneous parallelism framework and enables asynchronous overlap to mitigate I/O bottlenecks between the CPU and GPU.\nExeGPT [34] can find an optimal schedule control variable of the batch size and tensor parallelism degree that maximizes inference throughput while adhering to a given latency limit. It leverages the distribution of input and output sequence lengths to allocate resources efficiently and determine the best parallelism configuration. Helix [35] is designed to partition an LLM across heterogeneous GPUs and different types of network connections. It formulates its model partition scenario as a max-flow problem of a directed, weighted graph whose nodes represent GPU instances and edges capture both GPU and network heterogeneity through their capacities in the max-flow problem."}, {"title": "V. LLMS IN THE CLOUD", "content": "LLM deployments are computationally intensive and often require significant infrastructure to run effectively. Cloud platforms offer a scalable and cost-effective solution for deploying LLMs, eliminating the need for expensive hardware investments. The flexibility of cloud deployment allows organizations to easily adjust resources as needed, ensuring optimal performance and minimizing downtime. However, the significant costs associated with cloud computing resources and the challenge of ensuring their efficient utilization can be major obstacles for LLM service providers."}, {"title": "A. Cloud Deployment Cost", "content": "Modern clouds offer a variety of spot instances (e.g., AWS EC2 Spot Instance, Azure Spot Virtual Machines, Google Cloud Spot VMs). These instances run on spare capacity and are offered at highly discounted prices, but may be preempted at any time when other instances need the capacity. Spot-Serve [36] addresses the challenges of using these instances for LLM serving, such as how to quickly adapt to changes in available instances and how to minimize the cost of migrating instances when interruptions occur. It also introduces a stateful inference recovery mechanism for inference engines to commit their progress at the token level and efficiently resume interrupted requests.\nServerless is a recently emerged cloud computing paradigm, where inference service users can submit their model to the cloud and the cloud provider takes care of all infrastructure provision and scaling with varying inference request load, and saves unused hardware costs for customers. A major challenge in serverless is mitigating cold start, where a service instance would be shut down after not being accessed for some time, and once a new request arrives, it would experience a latency spike associated with re-initializing the service instance. ServerlessLLM [37] addresses these latency issues by utilizing the underutilized storage and memory resources available on GPU servers. It introduces a new checkpoint format and loading system to speed up LLM model loading, a live migration mechanism to avoid interrupting ongoing inferences, and a locality-aware server allocation strategy to minimize LLM inference cold start latency.\nCloud providers often offer a wide range of heterogeneous instance selections labeled at different prices. M\u00e9lange [38] is a cloud resource allocation framework that considers three key LLM service characteristics: request size, request rate, and service-level objective. It automatically navigates through the GPU option space to determine the most cost-efficient heterogeneous GPU allocation for a given LLM service. With the resources allocated and model hosted on the GPUs, Llumnix [39] is a dynamic scheduling system for LLM serving that addresses the challenges of heterogeneous and unpredictable requests by rescheduling them across multiple model instances at runtime - similar to how OS context switches across cores. Llumnix introduces an efficient live migration mechanism for requests and their in-memory states, minimizing downtime during rescheduling, and employs a dynamic scheduling policy that unifies various rescheduling scenarios, such as load balancing, de-fragmentation, prioritization, and auto-scaling. This efficiency has resulted in significant cost savings while achieving similar tail latency."}, {"title": "B. Cloud Efficiency", "content": "A key bottleneck resource in cloud datacenters is power, which LLMs are quickly saturating due to their growing computation demand. POLCA [40] characterizes the power consumption patterns of LLMs in the cloud and finds that while training LLMs demands a lot of power and can strain the data center's power infrastructure, inference tasks offer more flexibility for power management due to their less predictable power demands. POLCA devises a framework to manage power in LLM inference clusters by dynamically applying techniques such as GPU frequency locking and power capping. PerLLM [41] takes the LLM inference to an edge-cloud collaboration scenario, where it leverages the strengths of edge computing (low latency, reduced energy costs) and cloud computing (high processing power) to handle LLM inference tasks efficiently. PerLLM employs a Constraint Satisfaction Upper Confidence Bound (CS-UCB) algorithm to optimize service scheduling and resource allocation while adhering to constraints like processing time, bandwidth, and computing power - achieving energy LLM efficiency.\nWorkloads often get co-located in the cloud environment. FlexLLM [42] is a system designed to efficiently service LLM inference and parameter-efficient fine-tuning (PEFT) requests in the same iteration. LLM inference, which involves generating text token by token, is primarily limited by memory bandwidth due to the need to access all model parameters for each token generation. In contrast, PEFT, which processes all tokens of a request simultaneously, is mainly constrained by compute resources, such as the tensor cores on GPUs. FlexLLM introduces a token-level fine-tuning mechanism that breaks down the fine-tuning process into smaller, more manageable token-level computations to minimize memory usage and inference latency, making co-serving feasible.\nAs LLM inference follows token-by-token generation, users also read the response word-by-word. Andes [43] defines a user experience metric of Quality of Experience (QoE) for text streaming services. It is formulated by comparing the actual token delivery timeline (TDT) of a request with its expected TDT. The expected TDT is determined by the expected time to first token (TTFT) and the expected token delivery speed (TDS), which can vary depending on factors like the user's typical reading speed. The intuition is generating text too fast (than user reading speed) does not yield QoE benefits, wasting cloud resources. Andes addresses this by strategically allocating GPU resources among multiple requests to optimize QoE. It employs a dynamic priority-based preemptive scheduler that operates at the token level, prioritizing urgent requests and preempting those that have been sufficiently served. Andes improves average QoE and can handle higher request rates while maintaining similar token generation throughput."}, {"title": "VI. EMERGING RESEARCH FIELDS", "content": "Retrieval-Augmented Generation (RAG) [44] is a technique that enhances LLMs by incorporating external information sources. It addresses the limitations of LLMs in retaining factual knowledge and their tendency to generate inaccurate or fabricated information (hallucinations). RAG operates in two stages: retrieval and generation. During retrieval, the system identifies the most relevant contexts from an external knowledge base or corpus based on the given query. Once the relevant contexts are retrieved, they are integrated into the LLM's generation process in different processes including concatenation (where the retrieved contexts are simply appended to the query) and cross-attention (where the LLM attends to the retrieved contexts during generation).\nSparse RAG [45] observes that RAG can be computationally expensive due to the increased input length from retrieved documents. It first encodes retrieved documents in parallel to eliminate latency caused by long-range attention, then selectively decodes the output by attending only to highly relevant caches chosen via prompting the LLM with special control tokens. RAGCache [46] caches intermediate states of external knowledge with a knowledge tree to organize and store intermediate states. The cached knowledge can be shared across multiple queries to reduce the redundant computation. Another knowledge caching technique is CacheBlend [47], which selectively recomputes a small portion of the KV cache based on the preceding text in the input."}, {"title": "B. Mixture-of-Experts Inference", "content": "The mixture of Experts (MoE) is used in LLMs to improve efficiency and performance. It divides the model into specialized sub-networks, called \u201cexperts\u201d, each focusing on a specific task. A \u201cgating\" network then directs input to the most suitable expert. In the inference process of an MoE transformer, the input is first passed through a gating network. This network determines which expert, or a combination of experts, is best suited to process the specific input. MoE's sparsely activated subset of experts avoids the large computational need to process the entire model for every inference.\nMoE Communication. Lina [48] is a system designed to address the all-to-all communication bottleneck in distributed MoE. The all-to-all communication occurs when distributed MoE sends tokens to their selected experts for processing and then sends the results back to the original devices. During inference, Lina dynamically schedules resources based on expert popularity, balancing the transfer size and bandwidth of all-to-all communication across devices. ExFlow [49] is an optimization technique to accelerate the inference of distributed MoE. It leverages the inter-layer expert affinity, which is the correlation between expert selection across different MoE layers. By placing experts on corresponding GPUs based on their affinity, ExFlow reduces cross-GPU routing latency and improves inference throughput.\nExpert offloading. SiDA-MoE [50] (Sparsity-inspired Data-Aware) leverages both main memory and GPU memory by exploiting the inherent sparsity of expert activation in MoE models. SiDA-MoE includes two parallel threads: an inference thread and a hash-building thread. The hash-building thread predicts which experts will be activated for each token at each layer, storing these predictions in a hash table. The inference thread then uses this information to dynamically load activated experts onto the GPU and offload inactive experts to main memory, maximizing GPU memory utilization. MoE-Infinity [51] takes a different approach toward expert offloading. The system leverages the observation that MoE models exhibit sparse activation and temporal locality during inference, meaning only a few experts are repeatedly activated for processing a specific sequence. MoE-Infinity traces expert activation at the sequence level, enabling it to predict which experts will be needed and prefetch them accordingly."}, {"title": "MoE Efficiency", "content": "Fiddler [52] is a system designed to efficiently run these models on a limited number of GPUs, even when the model's size would typically exceed the GPU's memory capacity. Fiddler strategically distributes the model's components. Non-expert layers, which are used frequently, are kept on the GPU. A subset of expert layers, chosen based on how often they're used, are also placed on the GPU. The rest remain in the CPU's memory. Huang et al. [53] introduce three optimization techniques to address the MoE inference inefficiencies. (i) Dynamic gating allows the number of tokens processed by each expert to vary, which avoids the over-provisioning of resources in static gating and reduces computational waste, communication overhead, and memory consumption. (ii) Expert buffering leverages the observation that expert activation is often sparse and exhibits temporal locality. By caching frequently used (hot) experts in GPU memory and buffering less active experts in CPU memory, expert buffering reduces the static memory allocation on GPU. (iii) Imbalanced token assignments to experts can lead to bottlenecks and performance degradation. Expert load balancing ensures a more even distribution of workload across devices."}, {"title": "C. Miscellaneous Fields", "content": "Ethics and environmental sustainability. Sheng et. al [54] ensure fairness in serving LLMs by introducing a Virtual Token Counter (VTC). VTC defines LLM serving fairness based on a cost function that accounts for the number of input and output tokens processed. It achieves fairness by tracking the services received by each client and prioritizing those with the least service, while also considering the varying costs of processing input and output tokens. Sprout [55] addresses the environmental sustainability of LLMs and designs a framework to reduce the carbon footprint of LLM inference services. Sprout introduces \u201cgeneration directives\" to guide the autoregressive generation process, balancing the need for sustainability with the demand for high-quality generation.\nInference pipeline optimization. FlashDecoding++ [56] conducts inference engine performance optimization, addressing several issues in softmax synchronization, GPU kernel, and dataflow. For example, the decoding phase performs linear GEMM operations with flat shapes where the batch size dimension involved in the multiplication is much smaller than the others. FlashDecoding++ accelerates flat GEMM with double buffering that overlaps computation and data transfer and hides the memory latency in loading input matrices. Parrot [57] is designed to optimize the performance of LLM-based applications that involve multiple LLM requests with complex workflows. Parrot performs data flow analysis and uncovers correlations across multiple LLM requests, and introduces a series of optimizations to improve performance. FlashAttention-3 [58] is a method to speed up attention for large language models and long-context applications. It introduces techniques like warp specialization and asynchronous block-wise operations to optimize GPU utilization. FlashAttention-3 achieves significant speedup on Hopper GPUs compared to its predecessor and reduces numerical errors in FP8 computations.\nFrugal inference. FrugalGPT [59] proposes several solutions to reduce the inference cost, such as prompt caching and LLM cascading which uses a sequence of LLMs, starting with cheaper ones and moving to more expensive ones only if necessary. SpecInfer [60] applies speculative decoding using smaller, speculative models to predict the LLM's output, reducing the computational resources. These predictions are organized into a tree structure, and their accuracy is verified in parallel against the LLM. RouteLLM [61] dynamically selects between a stronger and a weaker LLM during inference to optimize the balance between cost and response quality."}, {"title": "VII. CONCLUSION", "content": "This survey has presented a comprehensive overview of recent advancements in LLM serving systems, emphasizing the importance of system-level solutions for enhancing performance and efficiency. We have highlighted key innovations for deploying and scaling LLMs, paving the way for the future development of LLM serving systems."}]}