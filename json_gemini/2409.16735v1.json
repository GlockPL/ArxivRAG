{"title": "GB-RVFL: Fusion of Randomized Neural Network and Granular Ball Computing", "authors": ["M. Sajid", "A. Quadir", "M. Tanveer"], "abstract": "The random vector functional link (RVFL) network is a prominent classification model with strong generalization ability. However, RVFL treats all samples uniformly, ignoring whether they are pure or noisy, and its scalability is limited due to the need for inverting the entire training matrix. To address these issues, we propose granular ball RVFL (GB-RVFL) model, which uses granular balls (GBs) as inputs instead of training samples. This approach enhances scalability by requiring only the inverse of the GB center matrix and improves robustness against noise and outliers through the coarse granularity of GBs. Furthermore, RVFL overlooks the dataset's geometric structure. To address this, we propose graph embedding GB-RVFL (GE-GB-RVFL) model, which fuses granular computing and graph embedding (GE) to preserve the topological structure of GBs. The proposed GB-RVFL and GE-GB-RVFL models are evaluated on KEEL, UCI, NDC and biomedical datasets, demonstrating superior performance compared to baseline models.", "sections": [{"title": "1. Introduction", "content": "The randomization-based neural networks (RNNs) [1] have been effectively used for a wide range of classification and regression tasks due to their universal approximation capabilities [2, 3]. Generally, the backpropagation (BP)-based algorithm is extensively employed to train feedforward neural networks (NNs). However, BP-based algorithms come with many underlying issues such as potential slowness, susceptibility to local optima [4], and the critical influence of factors such as learning rate and initialization point. RNNs emerged as a solution to the drawbacks of BP-based NNs mentioned earlier. In RNNs, some parameters are initialized and remain fixed throughout the training process [5], while the parameters of the output layer are determined through a closed-form solution.\nThe random vector functional link (RVFL) network [6, 7] is a shallow feed-forward RNN characterized by randomly initialized hidden layer parameters that remain untouched throughout the training process. RVFL stands out among other RNNs due to its direct connections between input and output layers. These direct links act as a form of implicit regularization [8] within RVFL, leading to improved learning capabilities. By employing methods like the Pseudo-inverse or least-squares technique, RVFL delivers a closed-form solution for optimizing output parameters, resulting in efficient learning with fewer adjustable parameters. In addition to that, RVFL demonstrates universal approximation capability [2].\nHowever, in the closed-form solution, RVFL involves matrix inverse computation of the whole training matrix (see subsection 2.3), which may be intractable in large-scale problems. Additionally, in the conventional RVFL, each sample receives an equal weighting during the creation of the optimal classifier, leaving it susceptible to noise despite its robust generalization in clean datasets. To address this issue, fuzzy theory has proven effective in reducing the negative impact of noise or outliers on the performance of machine learning models [9, 10]. Intuitionistic fuzzy (IF) is an extended version of fuzzy concepts, which uses membership and nonmembership functions to give an IF score to each sample. In [11, 12], intuitionistic fuzzy RVFL (IFRVFL) and graph embedding IFRVFL for class imbalance learning (GE-IFRVFL-CIL) were proposed with the aim to address the challenges posed by noise and outliers in datasets. However, these models have two associated challenges. Firstly, they require the computation of membership and non-membership values in the kernel space, which increases the computational complexity of IFRVFL and GE-IFRVFL-CIL. Secondly, IFRVFL and GE-IFRVFL-CIL also require the computation of inverses for the whole training sample matrix while calculating the output layer parameters, which makes it unsuitable for large-scale datasets.\nMoreover, the traditional RVFL overlooks the geometric aspects of the data when determining the final output parameters [7]. Several enhanced variants of the RVFL have emerged as solutions to this issue [12, 13]. However, the developed models also need to use whole training datasets in the calculation of the inverse matrix in the closed-form solution.\nHuman cognition prioritizes a \"large scope first\" principle, with the visual system emphasizing global topological features, processing information from larger to smaller scales. Inspired by this, Xia et al. [14] developed a classifier using GBs, leveraging GBs to categorize datasets based on different granular sizes [15]. Larger granularity sizes align with scalable and efficient approaches, resembling human cognitive processes [16]. However, shifting toward larger granularity may sacrifice detail and accuracy, whereas finer granularity enhances detail focus but may compromise scalability. Achieving a balanced granular size is crucial. Scholars have extensively researched breaking down information into different granularities for various tasks [17, 18], enhancing the effectiveness of multi-granularity learning in addressing real-world challenges [19, 20].\nTo address noise and outliers present in the dataset, an approach integrating the concept of GBs into support vector machine (SVM), namely, granular ball SVM (GBSVM) [21], was proposed. GBSVM utilizes GBs derived from the dataset as inputs instead of the conventional use of individual data points. GBSVM has demonstrated proficiency in handling noise and outliers, showcasing better scalability when compared to the standard SVM approach.\nInspired by the effectiveness and scalability of a granular approach in handling noise and outliers, in this paper, we fuse it with the RVFL and propose the granular ball RVFL (GB-RVFL). The proposed GB-RVFL leverages GBs as inputs and need to inverse the matrix of centers of the GBs rather than the matrix of the whole training dataset, resulting in improved scalability and a heightened ability to withstand noise and outliers. Additionally, to maintain the intrinsic geometric structure within the dataset, we integrate graph embedding (GE) [22] into GB-RVFL, resulting in the proposed graph embedded GB-RVFL (GE-GB-RVFL). GE-GB-RVFL offers several advantages: (i) by operating on the inverse of the matrix of GB centers, it enhances scalability and is well-suited for large-scale data compared to RVFL. (ii) Leveraging granularity concepts, GE-GB-RVFL effectively mitigates the adverse effects of noise and outliers. (iii) The GE framework preserves the intrinsic topological arrangement of datasets, providing GE-GB-RVFL with the advantage of utilizing the inherent dataset structure, thereby increasing its efficacy. To the best of our knowledge, this marks the inaugural instance where the RVFL model incorporates GB as an input rather than individual point samples.\nThe paper's key highlights are as follows:\n1. We propose the GB-RVFL model, which uses GBs as input rather than individual input samples for classifier construction. The utilization of GBs enhances the scalability of the proposed GB-RVFL by requiring only the inverse of the GB centers matrix rather than the matrix of entire samples. Additionally, this design improves the proposed GB-RVFL model's robustness against noise and outliers using the coarse granularity of GBs.\n2. Further, we propose the GE-GB-RVFL model, aiming to preserve the dataset's intrinsic geometric structure while retaining the GB-RVFL model's core properties. This model incorporates subspace learning (SL) criteria for output weight computation within the GE framework (integrating intrinsic and penalty SL). The incorporation of a graph regularization term in conjunction with GE serves the purpose of preserving the structural details of the graph in the projection space.\n3. The performance evaluation of the proposed GB-RVFL and GE-GB-RVFL models involves testing on benchmark KEEL and UCI datasets. These datasets are sourced from various domains and exhibiting different sizes, and are tested with and without label noise, comparing the performance of our proposed models against existing ones. Additionally, experiments on NDC datasets to demonstrate the effectiveness of the proposed model for large data.\n4. To demonstrate the practical applicability of the proposed GB-RVFL and GE-GB-RVFL models, we apply them to real-world biomedical datasets, specifically the BreakHis dataset for breast cancer classification and the ADNI datasets for the classification of Alzheimer's disease.\n5. Finally, we demonstrate the enhanced feature interpretability of the proposed GB-RVFL and GE-GB-RVFL models.\nThe succeeding sections of this paper are structured as follows: Section 2 introduces GB Computing, RVFL, and GE. Section 3 details the mathematical framework of the proposed GB-RVFL and GE-GB-RVFL models. Experimental results and analyses of proposed and existing models are discussed in Section 4. In Section 5, we show the enhancement in the feature interpretability of the proposed models. In Section 6, we engage in discussions grounded in empirical evaluations. Conclusion and some potential future research directions are outlined in Section 7."}, {"title": "2. Related Works", "content": "In this section, we first define some notations and then discuss granular computation, the mathematical framework of RVFL, and graph embedding (GE).\n2.1. Notations\nLet $M$ be the total number of training samples and the training set is $T = \\{(v_i, z_i)|v_i \\in \\mathbb{R}^{1\\times P}, z_i \\in \\mathbb{R}^{1\\times C}, i = 1,2,\\ldots, M\\}$. Let $V = (v'_1, v_2, \\ldots, v'_M) \\in \\mathbb{R}^{M\\times P}$ and $Z = (z'_1, z_2, ..., z'_M)' \\in \\mathbb{R}^{M\\times C}$ be the collection of all input and target vectors, respectively, where $(\\cdot)'$ is the transpose operator. $g$ denotes the number of hidden layer nodes. Let $k$ number of GBs generated on $T$ be $\\{GB_1, GB_2, ..., GB_k\\}$ and $o_j$ be the center of the granular ball $GB_j$ for $j = 1,2,...,k$. $\\otimes$ and $\\oplus$ denote the Kronecker product and the Concatenation operator, respectively and are defined below. For $C\\in \\mathbb{R}^{r\\times s}, D\\in \\mathbb{R}^{t\\times u}$ and $E \\in \\mathbb{R}^{t\\times v}$, then\n$C \\otimes D = \\begin{bmatrix}  c_{11}D & \\cdots & c_{1s}D\\\\  \\vdots & \\vdots & \\vdots\\\\  c_{r1}D & \\cdots & c_{rs}D  \\end{bmatrix} \\in \\mathbb{R}^{rt\\times su}$ and $D \\oplus E = [D\\; E] \\in \\mathbb{R}^{t\\times (u+v)}.$"}, {"title": "2.2. Granular Ball Computation [14]", "content": "In 1996, Lin and Zadeh introduced the concept of \"granular computing\" with the goal of minimizing the quantity of training data points. This approach aims to capture the essence of data simplification while preserving representativeness during the learning process. Consider a granular ball (GB) encompassing $q$ data points, i.e., $\\{v_1, v_2, ..., v_q\\}$, where each $v_j$ belongs to the vector space $\\mathbb{R}^{1\\times P}$. The center $o$ of a GB is defined as the center of gravity calculated from all sample points within the ball. Mathematically, they can be calculated as: $o = \\frac{1}{q}\\sum_{j=1}^q v_j$.\nThe class/label assigned to a GB is based on the labels of the data points with the highest occurrence of data samples within the ball. To quantitatively evaluate the degree of division within a GB, the notion of \"threshold purity\" is introduced. This threshold purity signifies the proportion of the predominant samples sharing the same label within the GB. The optimization problem governing the generation of GBs on set $T$ is expressed as follows:\n$\\begin{aligned} & \\min \\quad \\gamma_1 \\times \\frac{M}{\\sum_{j=1}^k |GB_j|} + \\gamma_2 \\times k, \\\\ & s.t. \\quad purity(GB_j) \\ge p, \\quad j = 1, 2, ..., k,  \\end{aligned}$   (1)\nwhere $\\gamma_1$ and $\\gamma_2$ are weight coefficients. $p$ is the threshold purity and $| \\cdot |$ represents the cardinality of a GB. The entire dataset is treated as a unified GB at the outset. When the purity of the GB falls below the given threshold, it must be divided several times until all sub-GBs reach or exceed the threshold purity. As the GBs' purity improves, so does their alignment with the original dataset's data distribution."}, {"title": "2.3. Random Vector Functional Link (RVFL) Network [6]", "content": "The RVFL, proposed by [6], comprises input and output layers along with a single hidden layer. Throughout the training phase, the biases and weights of the hidden layer are initialized at random from uniform distributions within the domains [0,1] and [-1,1], respectively, and remain fixed. The hidden layer and the input layer (facilitated by direct links) are connected to the output layer through the output layer weights. The calculation of output layer weights employs analytical methods such as the least square technique or Pseudo inverse.\nConsider $G$ as the hidden layer matrix, obtained by randomly projecting the input matrix and then applying the non-linear activation function $\\gamma$, which is defined as:\n$G = \\gamma(VX + \\mathbb{1}' \\otimes \\zeta) \\in \\mathbb{R}^{M\\times g},$   (2)\nwhere $X \\in \\mathbb{R}^{P\\times g}$ is the randomly initialized weights matrix, $\\mathbb{1}$ is a vector of ones, and $\\zeta \\in \\mathbb{R}^{1\\times g}$ is the bias vector. Therefore, $G$ is given as:\n$G = \\begin{bmatrix}  \\gamma(v_1x_1 + \\zeta^{(1)}) & \\cdots & \\gamma(v_1x_g + \\zeta^{(g)})\\\\  \\vdots & \\vdots & \\vdots\\\\  \\gamma(v_Mx_1 + \\zeta^{(1)}) & \\cdots & \\gamma(v_Mx_g + \\zeta^{(g)})  \\end{bmatrix}$\nwhere $\\zeta^{(j)}$ represents the $j^{th}$ hidden node's bias term, $x_k \\in \\mathbb{R}^{P\\times 1}$ denotes the $k^{th}$ column vector of the weights connecting the $k^{th}$ node of the hidden layer to all the input nodes and $v_i \\in \\mathbb{R}^{1\\times P}$ is the $i^{th}$ row (original input) of the inputs matrix $V$. The output layer's weights are calculated as follows:\n$[V \\oplus G]Q = Z,$   (3)\nwhere $Q \\in \\mathbb{R}^{(P+g)\\times C}$ is the unknown weights matrix that connects the output layer with the input layer and hidden layers. $Z$ is the anticipated output. $Q$ can be calculated as:\n$Q = [V \\oplus G]^{-1}Z,$   (4)\nor, $Q = [V \\oplus G]^{\\dagger}Z,$   (5)\nwhere $[\\,]^{-1}$ and $[\\,]^{\\dagger}$ denote the inverse and pseudo-inverse, respectively. If $[V \\oplus G]$ is non-sigular, we use (4) to calculate $Q$, otherwise (5).\nThe inverse calculation (in (4) and (5)) can be a significant challenge, particularly in scenarios involving large matrices. This task is computationally demanding and becomes a bottleneck for RVFL-based approaches when dealing with high-order matrices. Essentially, the complexity, memory usage, performance impact, and scalability concerns associated with finding the inverse of such matrices can hinder the performance of RVFL-based methods in real-world scenarios characterized by high-dimensional data and large sample sizes."}, {"title": "2.4. Graph Embedding (GE) [23]", "content": "The GE process [23] is designed to retain crucial graph structural details within the projection space. In the GE framework, considering an input dataset denoted as $V$, two components are defined: the intrinsic graph $\\mathcal{U}^{int} = \\{V, \\Theta^{int}\\}$ and the penalty graph $\\mathcal{U}^{pen} = \\{V, \\Theta^{pen}\\}$. The similarity weight matrix $\\Theta^{int} \\in \\mathbb{R}^{M\\times M}$ incorporates the graph weights corresponding to the intrinsic connections among vertices in $V$. Additionally, each element of the $\\Theta^{pen} \\in \\mathbb{R}^{M\\times M}$ represents the penalty matrix of $V$, which accounts for specific relationships among the graph's vertices. The optimization problem for GE is formulated as follows:\n$\\begin{aligned}  \\hat{y}_0 &= \\underset{y_0}{\\text{argmin}} \\frac{\\sum_{k \\neq l} ||y_0'v_k - y_0'v_l||^2,}{\\text{Tr}(y_0'V'SVy_0)=\\alpha} \\\\ &= \\underset{y_0}{\\text{argmin}} \\frac{\\text{Tr}(y_0'V'LVy_0)}{\\text{Tr}(y_0'V'SVy_0)=\\alpha}  \\end{aligned}$   (6)\nHere, $y_0$ represents the projection matrix, and the trace operator is denoted as $\\text{Tr}(\\cdot)$.\nFor $\\mathcal{U}^{int}$, the Laplacian matrix is represented by $L = F-\\odot^{int} \\in \\mathbb{R}^{M\\times M}$, where $F$ is the diagonal matrix, whose diagonal elements are defined as $F_{kk} = \\sum_l \\odot^{int}_{kl}$. $S = L^{pen} = F^{pen} - \\Theta^{pen}$ is the Laplacian matrix of $\\mathcal{U}^{pen}$, and $\\alpha$ is a scalar term. The Eq. (6) reduces to a generalized eigenvalue problem [24] as follows:\n$\\mathcal{U}^{ints} = A\\mathcal{U}^{pens},$   (7)\nhere $\\mathcal{U}^{int} = V'LV$ and $\\mathcal{U}^{pen} = V'SV$. This indicates that the eigenvectors of the matrix $\\mathcal{U} = \\mathcal{U}^{pen}\\mathcal{U}^{int}$ will be used to generate the transformation matrix. The matrix $\\mathcal{U}$ encompasses the inherent relationships among data samples through the intrinsic and penalty graph connections."}, {"title": "3. Proposed Work", "content": "In this section, we first give the formulation of the proposed GB-RVFL, and then we discuss detailed mathematical formulation along with the solution of the proposed GE-GB-RVFL model. Let $\\{GB_1, GB_2,..., GB_k\\}$ be the set of GBs for the training dataset $T$. Let $o_j$ and $w_j$ be the center and label of the granular ball $GB_j$, respectively. Let $O = [o'_1, o'_2, \\ldots, o'_k]' \\in \\mathbb{R}^{k\\times P}$ and $W = [w'_1, w'_2,...,w'_k]' \\in \\mathbb{R}^{k\\times C}$ be the matrix of centers and classes of GBs, respectively.\n3.1. Granular Ball Random Vector Functional Link (GB-RVFL) Network\nThrough the fusion of RVFL and GB computing, we propose GB-RVFL with the aim of achieving greater scalability and robustness compared to the RVFL model. We first highlight the rationale behind the scalability and robustness of our model, and then we delve into the mathematical framework that fuses GB and RVFL to form GB-RVFL.\n\\begin{itemize}\n    \\item  Robustness: Our proposed GB-RVFL model fuses the concept of GBs with RVFL. During the training, the proposed GB-RVFL captures information from either the entire sample space or from subsets of the sample space (in the form of GBs). These GBs, derived from the training dataset, are inherently coarse and represent a small fraction of the total data points. By leveraging the coarse nature of GBs, specifically focusing on their centers, we effectively harness the bulk of the information situated around these centers. This strategy renders our proposed GB-RVFL model less susceptible to noise and outliers, which are typically situated farther away from the central data distribution or clusters.\n    \\item  Scalability: By training the proposed GB-RVFL on GBs instead of the entire training dataset, and considering that the number of GBs is significantly smaller than the total training data points, we enhance the scalability of our model. For a deeper understanding and justification, please refer to Section 3.3 for the complexity analysis of our proposed model and Section 4.6 for the experiments on scalability investigation.\n\\end{itemize}\nLet the hidden layer matrix is denoted by $\\mathcal{G}$, acquired through the random projection of the matrix of centers $O$ of GBs followed by applying the activation function $\\gamma$, defined as:\n$\\mathcal{G} = \\gamma(OX + \\mathbb{1}' \\otimes \\zeta) \\in \\mathbb{R}^{k\\times g},$   (8)\nwhere $X \\in \\mathbb{R}^{P\\times g}$ is the randomly initialized weights matrix and $\\zeta\\in \\mathbb{R}^{1\\times g}$ is the bias vector. The output layer's weights are calculated as follows:\n$[\\mathcal{G} \\oplus O]\\Omega = \\mathcal{W}.$   (9)\nHere, $\\Omega$ in $\\mathbb{R}^{(P+g)\\times C}$ represents the weights matrix that links the input layer and hidden layer (GBs) to the output layer (GBs). $\\mathcal{W}$ is the anticipated output and $D = [\\mathcal{G}\\oplus O] \\in \\mathbb{R}^{k\\times (P+g)}$.\nThe proposed optimization problem for the proposed GB-RVFL model is formulated as follows:\n$\\underset{\\Omega}{min} = \\underset{\\Omega}{argmin} \\frac{C}{2} ||\\xi||_2^2 + \\frac{1}{2} ||\\Omega||_F^2,$ s.t. $D\\Omega - \\mathcal{W} = \\xi,$   (10)\nwhere $\\xi$ refers to the error matrix and $C$ is the regularization parameter. Problem (10) is the convex quadratic programming problem (QPP) and hence possesses a unique solution. The Lagrangian of (10) is written as:\n$\\mathcal{L}(\\Omega, \\Phi, \\lambda) = \\frac{C}{2} ||\\Omega||_F^2 + \\frac{1}{2} ||\\Phi||_F^2 - \\lambda'(D\\Omega - \\mathcal{W} - \\phi),$   (11)\nwhere $\\lambda$ is the Lagrangian multiplier. Differentiating $\\mathcal{L}$ partially w.r.t. each parameters, i.e., $\\Omega, \\phi$ and $\\lambda$; and equating them to zero, we obtain\n$\\begin{aligned}  \\frac{\\partial \\mathcal{L}}{\\partial \\Omega} &= 0 \\Rightarrow \\Omega - D'\\lambda = 0 \\Rightarrow \\Omega = D'\\lambda,  \\\\  \\frac{\\partial \\mathcal{L}}{\\partial \\phi} &= 0 \\Rightarrow C\\phi + \\lambda = 0 \\Rightarrow \\lambda = -C\\phi,  \\\\  \\frac{\\partial \\mathcal{L}}{\\partial \\lambda} &= 0 \\Rightarrow D\\Omega - \\mathcal{W} - \\phi = 0 \\Rightarrow \\phi = D\\Omega - \\mathcal{W}.  \\end{aligned}$  (12) (13) (14)\nSubstituting Eq. (14) in (13), we get\n$\\lambda = -C(D\\Omega - \\mathcal{W}).$   (15)\nOn substituting the value of $\\lambda$ obtained in Eq. (12), we obtain\n$\\begin{aligned}  \\Omega &= D' (-C(D\\Omega - \\mathcal{W})), \\\\  \\Rightarrow \\Omega &= -CD'D\\Omega + CD'\\mathcal{W}, \\\\  \\Rightarrow (I + CD'D)\\Omega &= CD'\\mathcal{W}, \\\\  \\Rightarrow \\Omega &= (D'D + \\frac{1}{C}I)^{-1} D'\\mathcal{W},  \\end{aligned}$  (16) (17) (18) (19)\nwhere $I$ is the identity matrix of the appropriate dimension. Substituting the values of (14) and (12) in (13), we get\n$\\begin{aligned}  \\lambda &= -C(DD'\\lambda - \\mathcal{W}), \\\\  \\Rightarrow \\lambda+CDD'\\lambda &= C\\mathcal{W}, \\\\  \\Rightarrow C(\\frac{1}{C}I+DD')\\lambda &= C\\mathcal{W}, \\\\  \\Rightarrow \\lambda&= (\\frac{1}{C}I + DD')^{-1} \\mathcal{W}.  \\end{aligned}$   (20) (21) (22) (23)\nPutting the obtained value of $\\lambda$ from Eq. (23) in (12), we get\n$\\Omega = DD' (\\frac{1}{C}I + DD')^{-1} \\mathcal{W}.$   (24)\nWe get two distinct formulas, (19) and (24), that can be utilized to determine $\\Omega$. It is worth noting that both formulas involve the calculation of the matrix inverse. If the number of features $(P + g)$ in $D$ is less than or equal to the number of GBs $(k)$, we employ the formula (19) to compute $\\Omega$. Otherwise, we opt for the formula (24) to calculate $\\Omega$. As a result, we possess the advantage of calculating the matrix inverse either in the feature or sample space, contingent upon the specific scenario. Therefore, the optimal solution of (10) is given as:\n$\\Omega = \\begin{cases}  (D'D + \\frac{I}{C})^{-1} D'\\mathcal{W}, & \\text{if } (P + g) \\le k, \\\\  D'(I + DD')^{-1} \\mathcal{W}, & \\text{if } k < (P + g).  \\end{cases}$   (25)"}, {"title": "3.2. Graph Embedded Granular Ball Random Vector Functional Link (GE-GB-RVFL) Network", "content": "This subsection delves into the proposed GE-GB-RVFL model, commencing with the establishment of its foundational mathematical framework. The GE-GB-RVFL model inherits all the properties of the proposed GB-RVFL model (i.e., robustness and scalability) along with preserving the inherent geometrical structure of GBs.\n\\begin{itemize}\n    \\item  Preservation of the original geometrical structure of granular balls: The preservation of the original geometrical structure of GBs is achieved through the optimization process (see Eq. 26) for determining the GE-GB-RVFL network's output weights. This process incorporates subspace learning (SL) criteria, utilizing intrinsic and penalty SL within the GE and granular computation frameworks. The GE framework adeptly manages the geometric relationships among the centers of GBs. In contrast, the standard RVFL model lacks this capability, leading to a failure in preserving the original geometrical structure of datasets and, thus, losing valuable information during training.\n\\end{itemize}\nThe proposed optimization problem of the GE-GB-RVFL is articulated as follows:\n$\\begin{aligned}  \\underset{\\Omega}{min} \\frac{1}{2} ||\\Omega||_F^2 + \\frac{C}{2} ||\\xi||_2^2 + \\frac{\\alpha}{2} \\text{Tr}(\\Omega'U\\Omega) \\quad  \\text{s.t.} \\quad D\\Omega - \\mathcal{W} = \\xi.  \\end{aligned}$   (26)\nHere, $\\Omega$ denotes the weights matrix that establishes connections between the input layer and hidden layer (GBs) and the output layer (GBs), $\\xi$ is the error matrix, and $C$ is the regularization parameter. Eq. (26) assigns distinct significance to each center of the GBs by incorporating weights and considering the geometric relationships among GBs by including the GE term. Here, $U$ represents the GE matrix, and $\\alpha$ denotes the graph regularization parameter. In this study, both the intrinsic and penalty graphs are defined over the concatenated matrix $D$. Following subsection 2.4, the intrinsic graph is denoted as $\\mathcal{U}^{int} = \\{D, \\Theta^{int}\\}$, and the penalty graph is represented as $\\mathcal{U}^{pen} = \\{D, \\Theta^{pen}\\}$. Consequently, $\\mathcal{U}^{int} = D'LD$, and $\\mathcal{U}^{pen} = D'SD$. Drawing from the literature, the weights for intrinsic and penalty graphs in the context of linear discriminant analysis (LDA) [25] are given as follows:\n$\\Theta^{int}_{ij} = \\begin{cases}  \\frac{1}{\\mathcal{G}_{w_i}}, \\text{ if } w_i = w_j, \\\\  0, \\text{ otherwise}.  \\end{cases}$   (27)\n$\\Theta^{pen}_{ij} = \\begin{cases}  \\frac{1}{\\mathcal{G}_{w_i}}, \\text{ if } w_i = w_j, \\\\  \\frac{1}{1-\\frac{1}{k}}, \\text{ otherwise}.  \\end{cases}$   (28)\nHere, $\\mathcal{G}_{w_i}$ denotes the number of GBs (centers) with class label $w_i$.\nThe Lagrangian of (26) is written as:\n$\\mathcal{L}_{GE} = \\frac{1}{2} ||\\xi||_F^2 + \\frac{C}{2} ||D\\Omega - \\mathcal{W}||^2_F + \\frac{\\alpha}{2} \\text{Tr}(\\Omega'U\\Omega).$   (29)\nApplying the Karush-Kuhn-Tucker (K.K.T.) condition in (29), we get following:\n$\\frac{\\partial \\mathcal{L}_{GE}}{\\partial \\Omega} = CD'(D\\Omega - \\mathcal{W}) + \\Omega + \\alpha U\\Omega = 0.$   (30)\nUpon computation, the output layer parameters are obtained as follows:\n$\\Omega = (D'D + \\frac{1}{C}I + \\frac{\\alpha}{C}U)^{-1} D'\\mathcal{W}.$   (31)\nThe matrix $U$ can be defined in two cases:\n\\begin{itemize}\n    \\item  Case 1: When $U$ represents intrinsic training data relationships $(\\mathcal{U}^{pen} = I)$, it indicates that no penalties are imposed on relationships among the nodes (GB centers). This approach solely relies on intrinsic relationships to extract graphical information from the GBs.\n    \\item  Case 2: When $U$ encompasses both intrinsic and penalty training data relationships $(\\mathcal{U}^{pen} \\neq I)$, specific penalties are applied to certain relationships among the nodes (GB centers). Here, the emphasis is on utilizing both intrinsic and penalty-specific relationships to extract graphical information from the GBs.\n\\end{itemize}\nRemarks\n\\begin{enumerate}\n    \\item  In our proposed GE-GB-RVFL model, the embedding space of the Graph $U$ is in the GB-RVFL space $\\mathbb{R}^{(P+g)}$, rather than the input space $\\mathbb{R}^P$, provides a more accurate representation of both linear and nonlinear relationships among the nodes/centers of GBs.\n    \\item  Incorporating graph embedding (GE) increases the complexity of the proposed GE-GB-RVFL model. Therefore, future research could explore alternative methods, such as sparse GE techniques, to preserve the geometrical structure of datasets while reducing computational overhead.\n    \\item  In our experiments, we used LDA for the graph embedding, as it is particularly effective for maximizing class separability in supervised learning tasks. However, other techniques such as, local Fisher discriminant analysis (LFDA) or marginal Fisher analysis (MFA) [22], can also be explored, depending on the task requirements.\n\\end{enumerate}"}, {"title": "3.3. Time and Space Complexity Analysis of the Proposed Models", "content": "Here", "GB-RVFL": "The complexity of the proposed GB-RVFL model primarily depends on (a) the requirement of matrix inversion in (25)", "26": "time complexity in computing inverse in RVFL is $O(M^3)$ if $M \\le (P + g)$ or $O((P + g)^3)$ if $(P + g) < M$. Therefore", "27": "where $M$ represents the number of samples in the training dataset $T$", "GE-GB-RVFL": "Additionally", "12": "the time complexity for this computation is $O((P+g)^3+(P+g)^2k)$. Thus"}, {"GB-RVFL": "The space complexity of the GB-RVFL model can be understood by considering the storage requirements for its components. First, the model requires space to store the centers of the GBs, which involves $k$ GBs, each with $P$ features, resulting in a complexity of $O(kP)$. Additionally, the hidden layer weights matrix, involving $g$ hidden nodes and $P$ input features, contributes $O(Pg)$. Furthermore, the inversion of the GB center matrix, which is of size $k \\times g$, requires $O(kg)$. Therefore, the overall space complexity of the"}]}