{"title": "Guardians of the Machine Translation Meta-Evaluation: Sentinel Metrics Fall In!", "authors": ["Stefano Perrella", "Lorenzo Proietti", "Alessandro Scir\u00e8", "Edoardo Barba", "Roberto Navigli"], "abstract": "Annually, at the Conference of Machine Translation (WMT), the Metrics Shared Task organizers conduct the meta-evaluation of Machine Translation (MT) metrics, ranking them according to their correlation with human judgments. Their results guide researchers toward enhancing the next generation of metrics and MT systems. With the recent introduction of neural metrics, the field has witnessed notable advancements. Nevertheless, the inherent opacity of these metrics has posed substantial challenges to the meta-evaluation process. This work highlights two issues with the meta-evaluation framework currently employed in WMT, and assesses their impact on the metrics rankings. To do this, we introduce the concept of sentinel metrics, which are designed explicitly to scrutinize the meta-evaluation process's accuracy, robustness, and fairness. By employing sentinel metrics, we aim to validate our findings, and shed light on and monitor the potential biases or inconsistencies in the rankings. We discover that the present meta-evaluation framework favors two categories of metrics: i) those explicitly trained to mimic human quality assessments, and ii) continuous metrics. Finally, we raise concerns regarding the evaluation capabilities of state-of-the-art metrics, emphasizing that they might be basing their assessments on spurious correlations found in their training data.", "sections": [{"title": "1 Introduction", "content": "Over the past few years, the Machine Translation (MT) field has witnessed significant advancements, largely driven by the advent of neural architectures, with the Transformer (Vaswani et al., 2017) being the most notable. Modern MT systems deliver mostly fluent and accurate translations, posing a challenge for their quality evaluation even when conducted by human annotators, especially those who lack professional training (Freitag et al., 2021a). Under these circumstances, shallow overlap-based metrics are gradually being replaced by neural-based metrics, which demonstrate a better correlation with human judgments (Freitag et al., 2022). However, a significant limitation is that most neural-based metrics are black-box systems trained to predict human judgments in the form of scalar scores, and typically do not provide justifications for their assessments. Besides rendering them challenging to interpret, such opacity also complicates their meta-evaluation. In this respect, we found that certain strategies for the assessment of MT metrics' capabilities \u2013 which have recently been employed in the context of the Metrics Shared Task at the Conference on Machine Translation (WMT) \u2013 favor specific metric categories and potentially encourage undesirable metrics behavior. To demonstrate these problems, we introduce the concept of sentinel metrics, i.e., a suite of metrics serving as a probe to identify pitfalls in the meta-evaluation process. Sentinel metrics are either trained with incomplete information which makes them inherently unable to evaluate the quality of machine-translated text properly \u2013 or consist of variations of existing metrics \u2013 which have been devised to expose specific issues in the meta-evaluation."}, {"title": "2 The Meta-evaluation of MT Metrics", "content": "Yearly, the WMT Metrics Shared Task organizes a competition among metrics, including participants' submissions and baselines, to identify the metric that most closely aligns with human judgments. Historically, the organizers have employed correlation with human judgment as a meta-evaluation strategy. Recently, significant efforts have been made to refine the meta-evaluation process, encompassing the adoption of new measures, such as those proposed by Kocmi et al. (2021) and Deutsch et al. (2023), and the introduction of the challenge sets sub-task (Freitag et al., 2021b, 2022), among other initiatives. In this section, we provide an overview of WMT's official meta-evaluation setting.\nFirst, multiple MT systems are employed to translate source segments found in one or more test datasets. Consequently, test datasets contain several translations of the same source segment. Second, a manual evaluation campaign is carried out to assess the quality of all translations. Finally, metrics' capabilities are assessed based on their alignment with human judgments, which are in the form of scalar scores. Such alignment is typically estimated using correlation and accuracy measures. Specifically, metrics are evaluated at two granularity levels:\n\u2022 at the segment level, metrics assign a score to every translation, and they are ranked according to their ability to discern between higher- and lower-quality translations;\n\u2022 at the system level, metrics assign a score to each MT system, and they are ranked according to their ability to discern between superior and inferior systems."}, {"title": "3 To Group or Not to Group?", "content": "At early editions of the WMT Metrics Shared Task (Mach\u00e1\u010dek and Bojar, 2013, 2014; Stanojevi\u0107 et al., 2015; Bojar et al., 2016), human assessments were collected in the form of Relative Rankings (RR). Specifically, the annotators were tasked to rank up to 5 translations of the same source sentence, produced by different MT systems. From each ranking, up to 10 pairwise comparisons were extracted. Despite metrics assessments being scalar scores \u2013 which theoretically enabled the comparison of all pairs of translated segments \u2013 correlation was measured only on those pairs of translations for which RR annotations were available. Therefore, only translations of the same source sentence were compared. Later on, at subsequent editions of WMT, new techniques for human evaluation were adopted: first, Direct Assessments (Graham et al., 2013, DA) \u2013 where annotators rate individual translations on a scale from 0 to 100 - then, Multidimensional Quality Metrics (Lommel et al., 2014, MQM) \u2013 where annotators tag the spans of a translation that contain errors, specifying their category and severity. With both the new annotation schemas, each translated segment was assigned a scalar quality score independently of the other translations, which made it possible to compare all translations, not only those of the same source sentence. This new possibility raised doubts regarding the best way to compute the correlation between metrics and human assessments. Indeed, it could be computed using all translations at once \u2013 *No Grouping* \u2013 or by first grouping translations based on either their source segment \u2013 *Segment Grouping* or the system that produced them \u2013 *System Grouping* \u2013 and then returning the average correlation of these groups.\nAt the WMT21 Metrics Shared Task, Freitag et al. (2021b) chose the *No Grouping* strategy, arguing that the other options would provide only a partial view of the overall picture. At WMT22, all three grouping strategies were used (Freitag et al., 2022), and later at WMT23, Freitag et al. (2023) chose *No Grouping* again. Although *No Grouping* is the only strategy that assesses the MT metrics' ability to discern between higher- and lower-quality translations in absolute terms, irrespective of the source segment or MT system, we show that both *No Grouping* and *System Grouping* may introduce unfairness and favor trained metrics over the rest."}, {"title": "3.1 The Relation Between Spurious Correlations and Grouping Strategies", "content": "Most neural-based metrics are trained with a regression objective to approximate human judgments. They are expected to infer by pattern-matching the relation between human judgments and various phenomena, such as omissions, additions, or other translation errors. However, this mechanism might inadvertently lead to the detection of patterns that are not in a causal relation with the concept of translation quality, but are instead spurious correlations, e.g., the length of a translation, or the number of named entities in it, among others. Arguably, the meta-evaluation should not reward metrics for basing their assessments on spurious correlations between the features of the source, translation, or reference, and the human judgments. However, our intuition is that *No Grouping* and *System Grouping* strategies might be doing so by allowing the comparison of translations from different sources. To simplify, consider a metric that unfairly penalizes a translation solely because it contains many named entities. Using *No Grouping* or *System Grouping*, such a metric might have a non-negative correlation with human judgments if, on average, translating sentences containing many named entities is more challenging than translating other sentences, because MT systems would be making more mistakes in translating them. Therefore, exploiting such a pattern might be beneficial even though it is not causally related to the quality of a translation. In contrast, when using *Segment Grouping*, such a pattern would be ineffective, as different translations of the same source sentence should contain the same amount of named entities. More generally, we would expect *Segment Grouping* to lessen the impact of most spurious correlations derived from features shared by a source sentence and its translations.\nTo assess the extent of this issue, we incorporate three sentinel metrics into the current meta-evaluation framework and re-compute the metrics' rankings using all grouping strategies. Crucially, we find that the impact of spurious correlations when *No Grouping* and *System Grouping* strategies are employed is substantial \u2013 favoring trained metrics over the rest \u2013 and is significantly reduced with *Segment Grouping*."}, {"title": "3.2 The Sentinel Metrics", "content": "This section describes the three sentinel metrics employed to measure the impact of grouping strategies on the meta-evaluation process:\n1.  `SENTINELCAND`, which assesses the quality of a translation without taking its source or reference as input.\n2.  `SENTINELSRC`, which predicts the quality of a translation solely based on its source.\n3.  `SENTINELREF`, which predicts the quality of a translation solely based on its reference.\nHaving no information regarding the translation to evaluate, SENTINELSRC and SENTINELREF can only learn spurious correlations between the features of the source and reference sentences, respectively, and the human judgments. SENTINELCAND, instead, is a metric with partial information. Indeed, it is possible to evaluate a translation's fluency and grammatical correctness without comparing it with its source or reference sentences, but not its adequacy. Nonetheless, we expect SENTINELcand to base its assessments on spurious correlations also."}, {"title": "3.3 Experimental Setup", "content": "Sentinel metrics employ XLM-ROBERTa large (Conneau et al., 2020) as their backbone model, with a multi-layer fully-connected neural network on top of the [CLS] token, which is used to output predictions in the form of scalar scores. We train sentinel metrics to minimize the Mean Squared Error (MSE) between their predicted scores and human judgments. Our dataset comprises a selection of data from WMT spanning 2017 to 2022, incorporating Direct Assessments (DA) and Multidimensional Quality Metrics (MQM) scores. Following Rei et al. (2022a), we train sentinel metrics for a single epoch using DA from 2017 to 2020 and fine-tune them for a further epoch using MQM data. Additional details regarding the training process are reported in Appendix B."}, {"title": "3.4 Results", "content": "In Table 2, we report the ranking derived from the segment-level Pearson correlation of the primary submissions to the Metrics Shared Task of WMT23, with the inclusion of sentinel metrics, in the language direction ZH \u2192 EN, and with all three grouping strategies. We report in Appendix C the rankings alongside the correlation values for all the official translation directions of the Metrics Shared Task, i.e., ZH \u2192 EN, EN \u2192 DE and HE \u2192 EN. As can be seen, SENTINELSRC ranks fourth and third when the grouping strategies are *No Grouping* and *System Grouping*, respectively, surpassing strong baselines like COMET or BLEURT-20, and even state-of-the-art metrics like GEMBA-MQM. The only metrics that are not surpassed are large regression-based systems such as XCOMET-Ensemble (Guerreiro et al., 2023) and MetricX-23 (Juraska et al., 2023), which might have learned the same spurious correlations leveraged by the sentinel metrics, in addition to non-spurious patterns (cf. Section 3.4.1). Conversely, when grouping by segment, SENTINELSRC and SENTINELREF are correctly positioned at the bottom of the ranking, and SENTINELCAND ranks 11th, compared to 3rd and 2nd with *No Grouping* and *System Grouping*, respectively. A notable difference between the grouping strategies is the positioning of GEMBA-MQM, which is ranked 7th and 9th with *No Grouping* and *System Grouping*, respectively, and becomes first with *Segment Grouping*. We hypothesize that this is due to GEMBA-MQM being based on GPT-4, which has not been explicitly fine-tuned on human assessments and is less likely to leverage spurious correlations such as those described in Section 3.1. Interestingly, with grouping strategies other than *Segment Grouping*, GEMBA-MQM is surpassed by all the sentinel metrics.\nSENTINELCAND is the only sentinel metric that does not rank at the very bottom with *Segment Grouping*, outperforming prismSrc (Thompson and Post, 2020) and embed_llama (Dreano et al., 2023), and positioning itself within the same cluster of statistical significance as BLEU. This suggests that focusing solely on the candidate translation \u2013 specifically, its fluency and grammatical correctness \u2013 may be sufficient to exceed the performance of some less effective metrics, at least in terms of Pearson correlation with human judgments. Furthermore, we highlight that our results may provide an answer to the open question left at WMT23 regarding the inconsistency of segment-level and system-level correlations for prismSrc. Freitag et al. (2023) noticed that, despite displaying a moderate correlation at the segment level, prismSrc was showing negative correlation values at the system level. As can be seen from Table 2, prismSrc ranks 15th out of 24 with *No Grouping* but 13th out of 14 with *Segment Grouping* (i.e., it is in the second to last significance cluster, close to the sentinel metrics). This result is consistent with prismSrc's negative correlation at the system level."}, {"title": "3.4.1 Are MT metrics learning from spurious correlations?", "content": "We hypothesize that some of the trained metrics may be basing their assessments on the same spurious correlations as those leveraged by the sentinel metrics. To delve deeper into this, we measure their segment-level Pearson correlation with the sentinel metrics using *No Grouping*. Surprisingly, XCOMET-Ensemble, XCOMET-QE-Ensemble, MetricX-23, and MetricX-23-QE, which are the only metrics that surpass the sentinels display a high correlation with all three sentinel metrics. Interestingly, their correlation with SENTINELSRC is 0.750, 0.736, 0.690, and 0.712, respectively, while their correlation with human judgment is 0.650, 0.647, 0.625, and 0.647, respectively. We recognize that these metrics share many similarities with our sentinels, as both are neural transformer-based systems and both were trained with the same regression-based objective, using largely the same data. This similarity likely contributes to the high correlation values observed. However, with access limited to only the source segment, SENTINELSRC relies exclusively on spurious correlations to conduct the evaluation. For this reason, we argue that these results raise concerns about the reliability of state-of-the-art MT metrics, which may be learning to exploit spurious correlations to minimize the Mean Squared Error with human judgments during training. To further support our hypothesis, we plot in Figure 1 the relation between the assessments of XCOMET-Ensemble and translation length, which serves as a simple spurious correlate of translation quality. We also plot the distribution of MQM human judgments over translation length. As we can see from the figure, XCOMET-Ensemble scores decrease at increasing candidate lengths, with the metric almost never assigning scores higher than 0.9 to translations longer than 400 characters. However, the distribution of human judgments shows that human annotators rated many of those translations as perfect or near-perfect, indicating that XCOMET-Ensemble might be biased to assign lower scores to longer translations, irrespective of their quality. Furthermore, the least-squares regression lines show that, on average, and as expected, longer translations contain more errors than shorter ones, and therefore are assigned lower scores by human annotators. This suggests that detecting biases of this type might be particularly complex without datasets crafted specifically for it.\nWe leave the investigation of these phenomena to future work and, for further details, we direct readers to Appendix D, where we report the pairwise correlation between most of the considered metrics and sentinel metrics, and to appendix E, where we report the relation between such metrics' assessments and translation length."}, {"title": "4 The Evaluation of Ties", "content": "In this Section, we focus on the third statistic among those described in Section 2, i.e., the segment-level pairwise ranking accuracy with tie calibration, dubbed acceq by Deutsch et al. (2023). Prior to WMT23, the organizers of the Metrics Shared Task used to employ the Kendall \u03c4 coefficient which is a statistic used to estimate the rank-based agreement between two sets of measurements (Kendall, 1945) \u2013 to measure the correlation between metrics and human judgments at the segment level. Deutsch et al. (2023) pointed out that the Kendall \u03c4 coefficient does not account for metrics correctly predicting ties, and introduced acceq to address this issue. Unfortunately, our analysis indicates that acceq inadvertently compromises evaluation fairness in order to accommodate ties, ultimately biasing the results in favor of continuous metrics over discrete ones."}, {"title": "4.1 The Kendall \u03c4", "content": "In this section, we define the Kendall \u03c4 coefficient as employed by the organizers of the Metrics Shared Task of WMT21 and WMT22. Let m, h be the vectors of metric and human assessments, respectively. Concordant pairs are the pairs of metric assessments that have been ranked in the same order by humans; discordant pairs are those ranked in a different order. We define C and D as the number of concordant and discordant pairs, respectively. We also define Th as the number of pairs only tied in the gold scores, Tm as the number of pairs only tied in the metric scores, and Thm as the number of pairs tied both in gold and metric scores, i.e., the number of correctly predicted ties. The Kendall \u03c4 correlation coefficient is defined as follows (Kendall, 1945):\n\u03c4 = \\frac{C-D}{\\sqrt{(C + D + T_h)(C + D + T_m)}}"}, {"title": "4.2 The acceq", "content": "As noted by Deutsch et al. (2023), Kendall \u03c4 penalizes the prediction of ties, but never rewards them, as Tm and Th are in the denominator, and Thm is not used. This issue was not prominent in the earliest editions of the Metrics Shared Task, where ties in human scores were disregarded, and older metrics rarely produced ties. Currently, instead, it is essential to consider the prediction of ties, especially since human MQM annotations contain a lot of them, and some recently-proposed metrics are designed to output evaluation assessments that resemble MQM (Perrella et al., 2022; Kocmi and Federmann, 2023). For this reason, Deutsch et al. (2023) proposed a measure that mimics the \u03c4 coefficient in the way it is computed, but also accounts for correctly predicting ties:\nacceq = \\frac{C+T_{hm}}{C+D+T_h+T_m +T_{hm}}"}, {"title": "4.3 Tie Calibration", "content": "The tie calibration algorithm determines, for each metric, a threshold \u03b5 such that, given two metric assessments m\u2081 and m\u2082, they are tied if |m\u2081 \u2212 m\u2082| < \u03b5. Deutsch et al. (2023) propose selecting the \u03b5 that maximizes acceq on the same test set used for the metrics meta-evaluation, enabling metrics to output the number of tied scores that best fits the distribution of human ties in the considered test set. This distribution is not stable across test sets (Table 11), and Deutsch et al. (2023) show that \u03b5 values are not stable either. Nonetheless, they argue that this would not impact the fairness of the evaluation. Unfortunately, our analysis shows that this is not the case. Specifically, despite all metrics' \u03b5 values being selected on the same test data, we demonstrate that continuous metrics are more flexible to best fit the underlying distribution of human ties, compared to discrete ones, leading to unfairly higher acceq values."}, {"title": "4.4 Two New Sentinel Metrics", "content": "To demonstrate the impact of this phenomenon, we introduce two additional sentinel metrics, i.e., SENTINELGEMBA and SENTINELMATESE GEMBA-MQM (Kocmi and Federmann, 2023) and MaTESe (Perrella et al., 2022) are MT metrics that output discrete scores in the form of MQM quality assessments and participated in WMT23. SENTINELGEMBA and SENTINELMATESE are perturbed versions of GEMBA-MQM and MaTESe, respectively, obtained by adding Gaussian noise N(0,0.0001) to their predictions. By making their output continuous in the neighborhood of discrete values, we partially fill their gap with continuous metrics, while preventing any two different discrete assessments from inverting their ordering. That is, if two GEMBA-MQM's assessments m1, m2 are such that m1 > m2, this relation is preserved by SENTINELGEMBA. In general, we expect a fair meta-evaluation to rank these sentinels on par or below their discrete counterparts. Furthermore, we wish to remark that this solution is sub-optimal compared to metrics that are continuous by design. Indeed, due to the addition of Gaussian noise, the ordering of all SENTINELGEMBA and SENTINELMATEse's assessments in the neighborhood of discrete values is randomized.\nTo demonstrate that SENTINELGEMBA and SENTINELMATESE can better fit the distribution of human ties compared to their discrete counterparts, we modify such a distribution in the test data. Specifically, we repeatedly sub-sample the test data, such that for each pair of tied human assessments we remove that pair from the test data with a certain probability pt, and do the same for non-tied pairs, which are removed with probability pn. We extract 13 samples by assigning various values to pt and pn and report the chosen values in Table 12 in Appendix G. As a consequence, each pair (pt, pn) represents a different sub-sample of test data, with a different percentage of tied human pairs. Then, for each metric, we select the best \u03b5 and compute acceq on each of these samples."}, {"title": "4.5 Results", "content": "In Figure 2 (left), we present the acceq results for a subset of continuous metrics, together with GEMBA-MQM, MaTESe, SENTINELGEMBA, and SENTINELMATESE. We discuss our results on the WMT23 ZH \u2192 EN test set, and report results concerning the other language directions, i.e., EN \u2192 DE and HE \u2192 EN, in Appendix G. At first glance, it is evident that discrete metrics exhibit a distinct acceq pattern compared to continuous and sentinel metrics. Notably, at lower percentages of tied human pairs, SENTINELGEMBA and SENTINELMATESE significantly outperform GEMBA-MQM and MaTESe. This discrepancy arises because the tie calibration algorithm selects very small \u03b5 values, close to 0 for every metric, allowing the number of ties predicted by continuous metrics to potentially drop to 0. Conversely, metrics that yield discrete scores inherently produce a certain number of ties, placing them at a disadvantage, and thus ranking conceptually identical metrics like SENTINELGEMBA and GEMBA-MQM at significantly different positions. Interestingly, in the hypothetical scenario in which there are no tied human pairs in the dataset, SENTINELGemba would rank second (despite several of its assessments having a random ordering), whereas GEMBA-MQM would be second to last. At increasing percentages of gold ties, instead, the acceq values obtained by SENTINELGEMBA and SENTINELMATESE converge to those of their discrete counterparts. However, this is a limitation of these sentinels' design and does not imply that the evaluation is fair at higher percentages of human ties.\nTo better investigate the source of unfairness, in Figure 2 (right) we show how the optimal \u03b5 changes at varying percentages of human ties. As can be seen, continuous metrics' \u03b5 is dynamically adjusted with heightened sensitivity, contrary to what happens for discrete metrics. Specifically, their \u03b5 is exactly 0 until the percentage of human ties over all pairs is 39%. Additionally, for MaTESe, it remains constant between 44% and 56%, and between 61% and 68%, and the same happens for GEMBA-MQM between 47% and 51% and between 56% and 68%. In contrast, the values change for all the other metrics in the same intervals, enabling them to better fit the distribution of gold ties found in the test set."}, {"title": "4.5.1 Can we use a held-out set for tie calibration?", "content": "We have demonstrated that conducting the tie calibration on the same test set used for the evaluation favors continuous metrics over discrete ones. Nonetheless, this does not necessarily mean using a held-out dataset would ensure a fair meta-evaluation. Indeed, our experiments show that unfairness stems from the different levels of adaptability between continuous and discrete metrics to the distribution of human ties found in the dataset used for tie calibration. Therefore, we expect that using a held-out dataset would still advantage continuous metrics if the distribution of human ties in the held-out resembled that of the test set, and disadvantage them if such a distribution differed from that of the test set. In both cases, continuous metrics' increased adaptability compared to discrete metrics would impair the fairness of the evaluation. To investigate this further, we compute a 80-20 split of the test set to obtain an evaluation set for tie calibration. Then, we repeatedly sub-sample such an evaluation set to modify its distribution of human ties and compute acceq on the new test set. The results are shown in Figure 3. We observe that the ranking is unstable at varying percentages of human ties, putting continuous metrics at a disadvantage if the proportion of ties in the evaluation set deviates significantly from that in the test set."}, {"title": "5 Conclusion", "content": "In this work, we identified two issues with the current meta-evaluation of Machine Translation, as conducted at the Metrics Shared Task of the Conference on Machine Translation. We proposed a suite of sentinel metrics designed to highlight these issues and demonstrate their impact on the metrics rankings, revealing that certain metric categories are unfairly advantaged. Indeed, the None Grouping and System Grouping strategies favor trained metrics over overlap- and LLM-based ones and the algorithm of tie calibration favors continuous metrics over discrete ones, or vice versa, depending on the percentage of tied assessments in the dataset used for it. Specifically, continuous metrics are favored if the tie calibration is conducted on the same test set used for the evaluation. Finally, we observed a notably high correlation between sentinel metrics and state-of-the-art metrics, raising concerns about their reliability and suggesting that their assessments might be based on spurious correlations present in the training data."}, {"title": "Limitations", "content": "Our analysis recommends grouping translations by their source segment before computing segment-level correlations with human judgments, showing that the rankings derived from the No Grouping and System Grouping strategies favor certain metric categories and potentially reward metrics for leveraging spurious correlations. However, we recognize that the Segment Grouping strategy does not evaluate the ability of metrics to distinguish between higher- and lower-quality translations in absolute terms, that is, independently of their source sentence. We believe this aspect should play a role in the meta-evaluation process, and leave to future work the development of fairer methods to fill this gap. Furthermore, we acknowledge that, due to Segment Grouping, each correlation measure is computed on a limited number of data points, i.e., as many as the MT systems that translated each source segment. In this respect, we argue that it would be necessary to investigate the metrics' ranking stability with varying numbers of MT systems, similar to the work of Riley et al. (2024), where they explored MT systems' ranking stability in designing human evaluation studies.\nFinally, we acknowledge that we did not provide a clear recommendation regarding a fair option for conducting the tie calibration algorithm. We demonstrated that continuous metrics are favored if selecting the optimal \u03b5 on the same test set used for the meta-evaluation and that using a held-out dataset would not be fair either. Nonetheless, using a held-out set would at least prevent the distribution of human ties used for tie calibration from being identical to that of the test set, and therefore it should be preferred. In general, we believe that a promising approach might involve studying the meaning of the score deltas of continuous metrics (akin to the work of Kocmi et al. (2024) regarding system-level assessments) and treating as tied all assessments within pre-defined score ranges derived from such deltas. This approach would also enhance the interpretability of MT metrics' assessments."}]}