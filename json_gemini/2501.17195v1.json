{"title": "Atla Selene Mini: A General Purpose Evaluation Model", "authors": ["Andrei Alexandru", "Antonia Calvi", "Henry Broomfield", "Jackson Golden", "Kyle Dai", "Mathias Leys", "Maurice Burger", "Max Bartolo", "Roman Engeler", "Sashank Pisupati", "Toby Drane", "Young Sun Park"], "abstract": "We introduce Atla Selene Mini, a state-of-the-art small language model-as-a-judge (SLMJ). Selene Mini is a general-purpose evaluator that outperforms the best SLMJs and GPT-40-mini on overall performance across 11 out-of-distribution benchmarks, spanning absolute scoring, classification, and pairwise preference tasks. It is the highest-scoring 8B generative model on RewardBench, surpassing strong baselines like GPT-40 and specialized judges. To achieve this, we develop a principled data curation strategy that augments public datasets with synthetically generated critiques and ensures high quality through filtering and dataset ablations. We train our model on a combined direct preference optimization (DPO) and supervised fine-tuning (SFT) loss, and produce a highly promptable evaluator that excels in real-world scenarios. Selene Mini shows dramatically improved zero-shot agreement with human expert evaluations on financial and medical industry datasets. It is also robust to variations in prompt format. Preliminary results indicate that Selene Mini is the top-ranking evaluator in a live, community-driven Judge Arena\u00b9. We release the model weights on HuggingFace (https://hf.co/AtlaAI/Selene-1-Mini-Llama-3.1-8B) and Ollama\u00b2 to encourage widespread community adoption.", "sections": [{"title": "1 Introduction", "content": "Automated evaluation of large language models (LLMs) is an increasingly pertinent task as LLMs demonstrate their value across a growing array of real-world use cases. Reliable evaluation is critical to ensure that LLMs are aligned with human objectives, i.e. that these models do what they are intended to do. Human evaluation is time-consuming and expensive, and scales poorly with volume and complexity \u2013 hence the need for scalable, automated techniques. As generative models have become more capable, the field has addressed this need by using LLMs themselves to evaluate other LLMs' responses, producing judgments and natural language critiques without humans in the loop [1, 2, 3] - an approach also known as \u201cLLM-as-a-judge\u201d (LLMJ).\nLLMJ typically leverages off-the-shelf models, prompting them to act as evaluators, making it simple to use and easy to get started with. However, this approach poses a number of challenges. Prompted evaluations are often poorly correlated with human judgments, and addressing this requires extra time"}, {"title": "2 Methods", "content": "Selene Mini is optimized for fast inference, high performance, and promptability. It is a general- purpose evaluator, and is trained to respond with both critiques and judgments in order to deliver actionable insights. To achieve this, we fine-tuned a Llama 3.1 8B Instruct model on a curated mixture of 16 publicly available datasets, totaling 577k data points. We developed a curation pipeline (Figure 2) to augment these datasets by synthetically generating \"chosen\" and \"rejected\" chain-of- thought critiques and filtering them for quality. We fine-tuned our model using a variant of DPO that includes an additional negative log-likelihood loss over chosen responses [13]. Conceptually, the DPO component increases the margin between chosen and rejected responses, making the former more likely and the latter less likely. We also minimized a negative log-likelihood loss on the chosen responses, which has the effect of further driving their likelihood up. We followed [10] and constructed training pairs in two formats: 70% with chain-of-thought critiques and judgments, and"}, {"title": "2.1 Datasets", "content": "We took inspiration from the datasets used to train Foundational Large Autorater Models (FLAMe,[7]), which spanned a mix of pairwise, absolute scoring, and classification tasks. Each data point in these three task types was structured slightly differently:\n1. Pairwise datasets typically consist of ${x_i, y_i^c, y_i^r}_{i=1}^{N_p}$ tuples, where $x_i$ is the prompt, and $y_i^c, y_i^r$ are \"preferred\" and \"non-preferred\" LLM responses. The meaning here is that human annotators judged the preferred response to be better than the non-preferred one: $y_i^c > y_i^r$. We modified the standard setup by randomizing the positions of the two responses, and including them alongside the original prompt, in a new prompt provided to the judge, denoted $x'_i$. Now, we describe the form of the LLMJ's responses. Each of the LLMJ's responses consists of a chain-of-thought critique, $q_i$, and a judgment, $j_i$. $q_i$ and $j_i$ correspond to the chosen LLMJ response, and $q_i^r$ and $j_i^r$ correspond to the rejected LLMJ response. As a result, the pairwise data that we trained on had the format ${x'_i, (q_i^c, j_i^c), (q_i^r, j_i^r)}_{i=1}^{N_p}$\nIn this case, an LLMJ's judgment is a choice among two responses, e.g. saying \"I prefer response A over B.\" Some pairwise datasets allow for ties, such that the judgment could be \"A and B are equally good (or bad).\"\n2. Absolute score datasets also have a prompt, but only one response from the LLM being eval- uated: ${x_i, y_i}_{i=1}^{N_a}$. We made a similar change as above: the original prompt and response were compressed into the prompt to the judge, and we generated chosen and rejected critiques and judgments. The final absolute score training dataset was ${x'_i, (q_i^c, j_i^c), (q_i^r)}_{i=1}^{N_a}$\nThe judgment in this case contains a score on a numeric scale such as 1-5 or 1\u20137.\n3. Classification datasets are structured as ${x_i, y_i}_{i=1}^{N_c}$. We repeated the process above to generate critiques and judgments. In this case, the judgments are class labels e.g. \"Yes\" or \"No\", which gave the final classification training dataset ${x'_i, (q_i^c, j_i^c), (q_i^r)}_{i=1}^{N_c}$\nA visualization of the entire mix of training datasets is provided in Appendix A.\nWe only included datasets published after 2023. This is because older synthetically generated datasets tend to use less capable models, so they are generally of lower quality. We excluded the test split for datasets with pre-existing splits, and filtered out data points with duplicate/null values or non-Latin/non-Greek characters. These datasets were used to fill in a variety of prompt templates containing information and rules about the Judge's task (see Appendix B for an example)."}, {"title": "2.2 Synthetic augmentation", "content": "To construct pairs of contrasting evaluations, we generated rejected judgments that differed from the chosen ground-truth judgments in the data. For each judgment, we synthetically generated chosen and rejected chain-of-thought critiques by prompting a generation model to argue for the respective judgments. For pairwise (A/B) or classification (Yes/No) task types, the rejected judgment is the opposite of the chosen one. For absolute scoring tasks (on a scale from 1-5), we randomly sampled a rejected judgment 2 points away from the ground truth judgment, i.e. randomly choosing between 4 and 5 if the ground truth was 2. Where a pairwise dataset also included \"Tie\" as an option, the rejected judgment was set to a random selection between \"A\" or \"B\". We then generated critiques by prompting the model to produce actionable, concise, and clear critiques that argued for these judgments."}, {"title": "2.3 Filtering for quality", "content": "We used filtering strategies on both raw and synthetic data to ensure high quality. For raw data, we used ArmoRM [14], an off-the-shelf reward model, to score and filter four of our largest datasets that we hypothesized to contain high-variance in data quality. While filtering may have benefited other datasets too, we prioritized these four due to their size and potential for containing high-quality subsets. For the selected datasets, we removed data points below a dataset-dependent threshold, with both the threshold choice and the decision to include the filtered dataset determined through single dataset ablation runs. Appendix C shows how the impacts of reward model filtering varied between datasets.\nFollowing the generation of synthetic critiques, we occasionally observed generations where the critique and assigned judgment were misaligned. While this issue was more prevalent for rejected evaluations (23.7%), it showed up in 0.8% of chosen evaluations too. To address this, we implemented a prompted critique consistency checker and used it to filter out inconsistent chosen evaluations. The final trained model displayed negligible inconsistencies (\u22480.1% across 3k benchmark evaluations) between its critiques and judgments."}, {"title": "2.4 Training", "content": "We fine-tuned a Llama 3.1 8B Instruct model using the variant of DPO introduced in [13], and refer readers to that paper for the full derivation. The distinction between this loss and the \"vanilla\" DPO loss is that it incorporates a negative log-likelihood term:\n$L_{DPO+NLL} = L_{DPO} ((q_i^c, j_i^c), (q_i^r, j_i^r) | x'_i) + \\alpha L_{NLL} (q_i^c, j_i^c | x'_i)$ (1)\nHere, $q_i$ and $j_i$ correspond to the chain-of-thought critique and judgment for data point $i$, while $x'_i$ is the prompt to the judge. The superscript refers to the chosen (c) or rejected (r) responses. Note how NLL is only applied on the chosen responses, as we did not want to increase the likelihood of poor-quality responses. $\\alpha$ is a hyperparameter that traded off the pairwise DPO loss against the ground-truth NLL loss.\nWe performed hyperparameter tuning on the following parameters: learning rate $\\eta \\in \\{5.5 \\times 10^{-8}, 1\\times10^{-7}, 7\\times10^{-7} \\}$, RPO $\\alpha \\in \\{0.5, 1\\}$ and weight decay $\\in \\{0.01, 0.1\\}$. The final values were a learning rate of $1 \\times 10^{-7}$, $\\alpha$ = 1, and weight decay of 0.1. Training was conducted with a batch size of 32 for one epoch on 8 NVIDIA H100 80GB GPUs, taking 16 hours."}, {"title": "3 Results", "content": "We assess the performance of Selene Mini on 11 out-of-distribution benchmarks [4, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24], spanning three different types of evaluation tasks: absolute scoring, classification, and pairwise preference. Following [10], we report Pearson correlations with ground-truth scores as performance metrics on the former and accuracy on the latter two, treating parsing failures as incorrect by default. We compare Selene Mini against the following state-of-the-art SLMJs of comparable"}, {"title": "3.2 Real-world evaluation", "content": "While the performance of our SLMJ across a wide range of benchmarks offers an indication of its strong general-purpose evaluation capabilities, such benchmarks are often not entirely representative of realistic evaluation use cases. In real-world scenarios, promptability \u2013 the ability of a model to effectively follow any set of prompt instructions and still deliver accurate and robust evaluations \u2013 is of key importance. This is especially challenging given that prompts in the real world are rarely as structured or consistent as those in benchmark datasets, and may involve domain-specific instructions. Importantly, we want to ensure that our training has not simply improved performance over the base model on a narrow set of prompts.\nTo measure our model's promptability, we challenge it with three real-world scenarios: first, by prompting it to evaluate two domain-specific expert-annotated industry datasets; second by testing its robustness to subtle variations in output formatting, and finally by pitting it head-to-head against other evaluator models in a live, community-driven \"Judge Arena\" [12]."}, {"title": "3.2.1 Performance on industry datasets", "content": "To simulate a real-world use case of Selene Mini, we measure prompted zero-shot performance on two industry datasets annotated by experts in the finance and medical domains. We measure performance using accuracy of judgments compared to expert labels.\nThe first of these is CRAFT-MD [25], a dataset developed for evaluating clinical LLMs. Unlike many other medical datasets, CRAFT-MD emphasizes the evaluation of natural dialogues rather than"}, {"title": "3.2.2 Robustness to prompt formatting", "content": "A common vulnerability in evaluator models is their sensitivity to complexity and prompt formats that do not significantly change the intention or semantics of the evaluation task. Taking inspiration from [6, 27, 28], we assess the performance of our model on RewardBench using six different prompt formats: original, markdown, JSON, PrePair [29], and a version with simplified instructions. See Appendix E for details.\nAs shown in Figure 3b, our trained model is robust to various prompt templates: we consistently maintain our performance improvement over the base model with minimal variability between prompt templates. This highlights that Selene Mini does not degrade in performance when prompts vary in ways irrelevant to evaluation."}, {"title": "3.3 Performance in a community arena", "content": "Crowd-sourced, randomized battles have proven an effective technique to benchmark LLMs on human preference in the real world [30]. We developed a community platform called Judge Arena [12], that lets anyone easily compare and vote on judge models in head-to-head battles. Votes are automatically compiled and converted into ELO scores, producing rankings on the Judge Arena leaderboard. Figure 3c shows a snapshot of the Judge Arena leaderboard as of January 22nd 2025, comparing an early snapshot of Selene Mini (Atla-8B-preview) with 25 other judge models. Preliminary results indicate that Selene Mini is the top-ranking judge model, outperforming state-of-the-art evaluators including Claude 3.5 Sonnet, Prometheus 7B v2, and Llama 3.1 405B Instruct."}, {"title": "4 Discussion", "content": "In this work, we introduce Atla Selene Mini, demonstrating that effective general-purpose evaluation can be achieved in smaller model architectures through principled data curation and a hybrid training objective (DPO + SFT). The model's strong performance across benchmarks, particularly on absolute scoring tasks \u2013 which represent the most common and useful form of evaluation in practice \u2013 suggests that careful attention to training data quality can be as impactful as increased model size for evaluation capabilities. The model's success on real-world industry datasets, like CRAFT-MD and FinanceBench, indicates that our approach generalizes beyond academic benchmarks to practical applications. This is crucial for deployment in production environments where domain expertise is required but specialized evaluators may not be available. Finally, the model's ability to maintain consistent performance across different prompt formats points to robust learned evaluation capabilities rather than mere pattern matching.\nLooking ahead, we anticipate two emerging frontiers that will shape the future of AI evaluation. First is the rise of agent-based systems that combine language models with external tools and APIs, creating more powerful and versatile AI systems. Second is the increasing use of inference-time compute [31, 32] \u2013 systems that perform additional reasoning steps during inference to generate higher-quality outputs. These developments will require new evaluation frameworks and capabilities. Future research could explore how evaluator models can assess not just language outputs, but entire chains of reasoning, tool usage, and multi-step processes.\nIn conclusion, Atla Selene Mini represents a significant step forward in making reliable, general- purpose LLM evaluation more accessible to the broader community. Its combination of strong performance, domain generalization, and practical usability in an open-weights model provides a valuable tool for researchers and practitioners working to improve language model capabilities and safety."}, {"title": "5 Acknowledgments", "content": "We thank Cl\u00e9mentine Fourrier and the HuggingFace team for their help in setting up Judge Arena. We are grateful to Juan Felipe Cer\u00f3n Uribe, Seungone Kim, Shreya Shankar, Eugene Yan, Yifan Mai, Austin Xu, Peifeng Wang and the team at SalesForce for helpful discussions around evaluations. We thank Zongheng Yang, Romil Bhardwaj and the Skypilot team for their assistance with our training infrastructure."}, {"title": "C Impact of reward model filtering", "content": "We investigated the use of reward models to filter and subsample large datasets, in order to retain high-quality data points. To quantify the impact of filtering, we conducted ablation studies comparing random subsamples of these datasets to subsamples filtered using reward models. We ensured that the subsampled dataset size remained constant - for instance, comparing 20k points selected randomly to 20k points selected using a reward model. These were evaluated using accuracy on held-out pairwise preference datasets, and using Pearson correlation on held-out absolute scoring datasets, as illustrated in Figure 6. The results demonstrated that reward model filtering was highly effective in improving the quality of certain datasets, while its impact was less pronounced for others."}, {"title": "DDetailed performance breakdown across model sizes", "content": ""}, {"title": "E Prompt templates for robustness experiments", "content": ""}]}