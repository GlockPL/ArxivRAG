{"title": "Toward Efficient Generalization in 3D Human Pose Estimation via a Canonical Domain Approach", "authors": ["HOOSANG LEE", "Jeha Ryu"], "abstract": "Recent advancements in deep learning methods have significantly improved the performance of 3D Human Pose Estimation (HPE). However, performance degradation caused by domain gaps between source and target domains remains a major challenge to generalization, necessitating extensive data augmentation and/or fine-tuning for each specific target domain. To address this issue more efficiently, we propose a novel canonical domain approach that maps both the source and target domains into a unified canonical domain, alleviating the need for additional fine-tuning in the target domain. To construct the canonical domain, we introduce a canonicalization process to generate a novel canonical 2D-3D pose mapping that ensures 2D-3D pose consistency and simplifies 2D-3D pose patterns, enabling more efficient training of lifting networks. The canonicalization of both domains is achieved through the following steps: (1) in the source domain, the lifting network is trained within the canonical domain; (2) in the target domain, input 2D poses are canonicalized prior to inference by leveraging the properties of perspective projection and known camera intrinsics. Consequently, the trained network can be directly applied to the target domain without requiring additional fine-tuning. Experiments conducted with various lifting networks and publicly available datasets (e.g., Human3.6M, Fit3D, MPI-INF-3DHP) demonstrate that the proposed method substantially improves generalization capability across datasets while using the same data volume.", "sections": [{"title": "I. INTRODUCTION", "content": "Human pose estimation (HPE) is a computer vision task that identifies and localizes human joints (i.e., keypoints) in images or videos. HPE is an essential area of research and application due to its importance across several domains, including Human-Computer Interaction (HCI), Human-Robot Interaction (HRI), Rehabilitation, Healthcare, Surveillance and Security, Entertainment and Media, Sports Analytics, and Robotics. Recently, deep learning (DL)-based HPE approaches have been actively investigated to improve pose accuracy as well as generalization capability, to handle complex poses and occlusions, or to achieve real-time performance.\n The lack of generalization capability remains one of the major challenges in data-driven approaches involving DL-based HPE. This limitation arises mainly from the domain gap between the source and target data domains, on which the model is trained and tested, respectively. As a result, HPE accuracy deteriorates in cross-dataset evaluations, where the source and target domains originate from different environments [1]\u2013[9]. To overcome the domain gap problem, research has predominantly focused on two main approaches. The first approach is Domain Generalization (DG) [1]\u2013[6], which seeks to enhance data diversity through data augmentation techniques, thereby expanding the source domain to cover the target domain. However, addressing diverse environments without prior information of the target domain necessitates generating a substantial volume of diverse data, leading to data inefficiency and demands for larger model capacity. The second approach is Domain Adaptation (DA) [7]-[9], which leverages available target domain data (i.e., input 2D poses) to align the source domain with the target domain. However, this method requires test-time adaptation, which involves significant time for data augmentation and fine-tuning for each new target domain. Consequently, DG and DA methods often require significant time and effort.\n To address the domain gap problem more efficiently, we"}, {"title": "II. RELATED WORK", "content": "A. DEEP LEARNING-BASED HUMAN POSE ESTIMATION\n2D HPE vs 3D HPE DL-based HPE can be broadly classified into two streams based on complexity: 2D HPE [13]\u2013[19], which involves detecting 2D poses in images, and 3D HPE [20]-[42], which estimates 3D poses, offering a more detailed and comprehensive view of human posture.\nFrame Input vs Video Input DL-based HPE is also categorized by input type: frame input or video input. Recent studies [28], [29], [35]\u2013[38] have utilized video input to exploit temporal relationships between frames for addressing the depth ambiguity problem inherent in frame-based inputs.\nDirect Method vs 2D-to-3D Lifting 3D HPE is broadly classified into Direct method and 2D-to-3D Lifting. Direct method [20]-[26] estimates 3D poses directly from images in an end-to-end manner, whereas 2D-to-3D lifting [27]\u2013[42] estimates 3D poses using a lifting network that takes extracted 2D poses from images as input.\n Starting from [27], 2D-to-3D lifting has shown significantly better performance than traditional rule-based methods and currently achieves the best 3D HPE performance by utilizing various deep learning architectures, such as MLPs [27], Temporal Convolutional Networks (TCNs) [28], Recurrent Neural Networks (RNNs) [29], [30], Graph Convolutional Networks (GCNs) [31]-[34], Transformers [35]-[38], and Diffusion Models [39]-[42].\nIn this paper, we focus on the 2D-to-3D lifting approach under single-view, video input, and single-person settings to address the 3D HPE problem.\nB. GENERALIZATION CAPABILITY IN 3D HPE\nDomain Generalization has primarily focused on creating diverse pose data to improve generalization capabilities without relying on information from the target domain. However, it is essential not only to generate diverse poses but also to ensure their plausiblity. PoseAug [2] introduced a novel data augmentation framework based on Generative Adversarial Network (GAN) to produce both diverse and plausible pose data. In this framework, a discriminator attempts to differentiate between real and augmented (fake) poses, while an augmentor tries to generate realistic poses to deceive the discriminator. Subsequent studies [3], [5], [6], [8] have also adopted this GAN-based framework, proposing novel methods to further enhance the diversity and plausibility of augmented poses. However, endlessly expanding the source domain distribution without accounting for the target domain is inefficient. Instead, our proposed approach confines both source and target domains to a data-intensive canonical domain, effectively reducing the domain gap and improving data efficiency.\nDomain Adaptation [7]-[9], on the other hand, aims to enhance the target domain performance using available target 2D poses. This approach generates target domain-like data to finetune the lifting network at each new test environment. Among them, PoseDA [8] introduced a Global Position Alignment (GPA) method to reduce the domain gap caused"}, {"title": "III. METHOD", "content": "A. PRELIMINARY BACKGROUND\n1) Domain Gap in HPE\nIn [8], the domain gap in HPE is roughly categorized in terms of two aspects: global position and local pose. Fig. 1 (a) illustrates both types of domain gap. The global position gap arises from variations in intrinsic and extrinsic camera parameters across datasets. These differences lead to variations in the input 2D pose distribution, particularly in terms of position and scale on the image plane. Here, 'position' refers to the x, y coordinates of the root joint, while 'scale' refers to the size of the 2D pose. Meanwhile, the local pose varies depending on the diversity of actions, even when position and scale remain the same. Both types of domain gaps have a significant impact on the model's generalization capability.\nNotably, the 2D-to-3D lifting problem involves determining a mapping function from an input 2D pose to an output 3D pose. This task inherently belongs to the domain shift problem, where dataset shifts are influenced by both input and output distributions.\n2) 2D-to-3D Lifting Problem\nIn 2D-to-3D lifting problem, a lifting network takes a 2D (skeleton) pose $p \\in \\mathbb{R}^{J \\times 2}$ as input, and outputs a 3D (skeleton) pose $P \\in \\mathbb{R}^{J \\times 3}$, where $J$ denotes the number of joints in the skeleton. Each joint $j$ of a 2D pose has two components, $p_{i} = [u, v]^T$, while each joint $j$ of a 3D pose has three components, $P_{i} = [X, Y, Z]^T$. For simplicity of notation, the superscripts $j$ are omitted in the following equations. We denote the joint $j = r$ as the root joint, which in our case is the pelvis joint. In the case of video input, the dimensions of both 2D and 3D skeletons are expanded with the frame dimension"}, {"title": null, "content": "$T$ (i.e., $p \\in \\mathbb{R}^{T \\times J \\times 2}$ and $P \\in \\mathbb{R}^{T \\times J \\times 3}$), facilitating the temporal information of successive poses. Various architectures can be chosen for the lifting network, as mentioned in Section II-A.\nGround truth 3D poses are typically captured using motion capture systems (e.g., VICON) and represented in a global frame $G$. A 3D pose in the global frame, $P_G$, can be transformed into a 3D pose in the camera frame $C$, $P_c$, using the camera extrinsic parameters: the rotation matrix $R$ and the translation vector $t$.\n$$P_c = R \\times P_G + t = \\begin{bmatrix} X_c\\\\Y_c\\\\Z_c \\end{bmatrix}$$\n(1)\nThe corresponding 2D pose $p$ can be derived by projecting $P_c$ onto the image plane using the camera intrinsic matrix $K$:\n$$s \\cdot p = K \\times P_c = Z_c \\begin{bmatrix} \\frac{f_x X_c}{Z_c} + C_x \\\\ \\frac{f_y Y_c}{Z_c} + C_y \\\\ 1 \\end{bmatrix} = Z_c \\nu$$\n... p = \\begin{bmatrix} \\frac{f_x X_c}{Z_c} + C_x \\\\ \\frac{f_y Y_c}{Z_c} + C_y \\end{bmatrix} (in homogeneous form), where\n$$K = \\begin{bmatrix} f_x & 0 & C_x\\\\ 0 & f_y & C_y \\\\ 0 & 0 & 1 \\end{bmatrix}$$, $s$: scale factor.\n(2)\n$f_x, f_y, C_x, C_y$ denote the camera intrinsic parameters, representing the focal lengths in pixels and the image coordinates of the principal axis.\nWe denote $P_c$ and $p$ as the original 3D and 2D pose, respectively, to distinguish them from the proposed canonical 3D and 2D pose.\n3) Cenventional 2D-3D Mapping and its Canonicalization Strategies\nThe conventional 2D-3D mapping (Fig. 3(b)) used in the 2D-to-3D lifting literature utilizes 2D-3D pose pairs $\\{P_c, p\\}$, where $P'_c$ represents the root-relative version of the original 3D pose $P_c$. In this conventional mapping, the following canonicalization concepts are employed to enhance lifting performance. We also incorporate these strategies alongside the proposed canonicalization method.\nRoot-relative 3D Pose represents the local position of each joint relative to the root joint, thereby constraining the 3D pose distribution. Thus, it can be considered a form of canonicalization. A root-relative 3D pose in camera frame, $P'_c$, can be obtained from the $P_c$ as\n$$P'_c = P_c - P'_c = \\begin{bmatrix} X_c - X'_c \\\\ Y_c - Y'_c \\\\ Z_c - Z'_c \\end{bmatrix}$$\n(3)\nwhere $P'_C$ denotes the root joint of $P_c$.\nGenerally, the root-relative version of 3D pose is widely used as ground truth during training. This is because the error metric, Mean Per Joint Position Error (MPJPE), calculates"}, {"title": null, "content": "the root-relative position error between the ground truth and predictions after aligning the root joint, rather than measuring the absolute joint position error. As a result, it focuses solely on comparing the local posture, disregarding the individual's absolute position in 3D space.\nScreen Normalization \u00b9 is conventionally applied to the 2D pose input to standardize the range of the image plane into the range between -1 and 1, while maintaining the ratio of the 2D pose within the image space. This can also be regarded as a form of canonicalization. The screen normalized 2D pose $p_{norm}$ can be derived as\n$$p_{norm} = (u_{norm}, v_{norm}) = \\frac{(u,v)}{W, H} \\times 2 - \\frac{(W, H)}{W}$$\n(4)\nwhere $W$ and $H$ represent the width and height of the image, respectively. Note that this process does not change the position, scale, or local posture of the 2D pose in the image plane. For both conventional and canonical mappings, this process is applied to the input 2D pose before it is fed into the network.\nB. PROPOSED METHOD\nThe diverse factors contributing to the domain gap in HPE create an extensive true data distribution that a lifting network must learn to achieve strong generalization capability. Covering this distribution requires a substantial amount of data, as well as significant time and effort.\nTo address the domain gap more efficiently, we propose a novel canonical domain approach that transforms both the source and target domains into a unified canonical domain. The canonicalization of both domains is achieved through the following steps: (1) for the source domain, training a lifting network in the canonical domain, and (2) for the target domain, canonicalizing the 2D pose input before inferring the 3D pose using the trained lifting network.\nBefore defining the canonical domain, we first identify the issue of 2D-3D pose inconsistency arising from conventional 2D-3D mapping in section III-B1. Next, section III-B2 introduces a canonicalization process that generates consistent and constrained canonical 2D-3D pose mapping, which constructs the canonical domain. Subsequently, section III-B3 presents a mathematical analysis that explains how the proposed canonical domain ensures 2D-3D pose consistency and facilitates efficient learning. Finally, section III-B4 presents a 2D canonicalization process that canonicalizes the target 2D poses without ground truth 3D pose, allowing the trained network on the canonical domain to be fully utilized during inference.\n1) 2D-3D Pose Inconsistency in Conventional 2D-3D Mapping\nWhile canonicalization strategies in conventional mapping help reduce data distribution size and improve performance to some extent, an additional issue of 2D-3D pose inconsistency\n1 We follow the terminology from [8]."}, {"title": null, "content": "arises, where the 3D pose and its corresponding 2D pose exhibit different posture shapes in the x-y plane. We refer to this phenomenon as 2D-3D pose inconsistency. In Fig. 2 (a), two distinct positions of the same 3D pose (Position 1 and Position 2) relative to the camera frame are shown. While the root-relative 3D poses remain unchanged in both cases, their 2D projections onto the image plane differ (Fig. 2 (b)). In Position 1 (blue), the 2D pose retains the original posture shape of the 3D pose in the x-y plane, thereby maintaining 2D-3D pose consistency. In contrast, the 2D pose in Position 2 (red) exhibits a different posture shape (e.g., uncrossed legs), resulting in a lack of 2D-3D pose consistency.\nThis inconsistency, which is also difficult for humans to interpret, results from the relative rotation introduced by perspective projection. In perspective projection, all points on an object are projected along lines converging at the camera's origin. In Fig. 2 (c), object 1 is centered with the camera principal axis, resulting in its front part (highlighted in blue on object 1) being projected onto the image plane. In contrast, object 2, positioned farther from the principal axis, undergoes a relative rotation with respect to a virtual principal axis passing through its center. Instead of the front part, the left side of object 2 (highlighted in red on object 2) is predominantly projected onto the image plane, creating the appearance of rotation.\nTo quantify this inconsistency in 2D pose, consider two 3D poses with the same root-relative 3D pose, $P'_c$, but different positions: one with the pelvis located at Position 1, $[0, 0, Z]$ (centered on the principal axis), and the other at Position 2, $[X, Y, Z]$ (which has offset from the principal axis in the x and y directions). The 3D poses and their corresponding 2D poses at Position 1 and Position 2 are:\n$$P_{C_{pos1}} = P'_c + [0, 0, Z] = [X'_c, Y'_c, Z]$$\n$$P_{C_{pos2}} = P'_c + [X, Y, Z] = [X'_c + X, Y'_c + Y, Z]$$\n$$p_{pos1} = \\begin{bmatrix} \\frac{X'_c + c_x}{Z} \\\\ \\frac{Y'_c + c_y}{Z} \\end{bmatrix}$$\n$$p_{pos2} = \\begin{bmatrix} \\frac{(X'_c + X) + c_x}{Z} \\\\ \\frac{(Y'_c + Y) + c_y}{Z} \\end{bmatrix}$$\nThe 2D pose at Position 2, $p_{pos2}$, can be expressed in terms of the 2D pose at Position 1, $p_{C_{pos1}}$:\n$$p_{pos2} = \\begin{bmatrix} \\frac{(X'_c + X) + c_x}{Z} \\\\ \\frac{(Y'_c + Y) + c_y}{Z} \\end{bmatrix} = \\begin{bmatrix} \\frac{(X'_c + c_x) + X}{Z} \\\\ \\frac{(Y'_c + c_y) + Y}{Z} \\end{bmatrix} = p_{pos1} + \\begin{bmatrix} \\frac{X}{Z} \\\\ \\frac{Y}{Z} \\end{bmatrix}$$\nThe residual term $\\begin{bmatrix} \\frac{X}{Z} \\\\ \\frac{Y}{Z} \\end{bmatrix}$ between $p_{pos1}$ and $p_{pos2}$ quantifies the effect of relative rotation in the image plane, which contributes to the 2D-3D pose inconsistency. Consequently, this inconsistency arises from the offset to the principal axis in the x and y directions.\nAs a result, the model must estimate the same root-relative 3D pose from many different 2D poses while accounting for the residual term, leading to the many-to-one mapping problem. This increases the complexity of the 2D-to-3D lifting problem, further compounded by the inherent challenges of depth ambiguity, thereby necessitating a larger dataset for an effective solution.\n2) Canonical 2D-3D Pose Mapping\nSimilar to constraining 3D poses using the root-relative version, 2D canonicalization can further improve data-efficient learning by constraining the 2D pose distribution. A simple approach to achieve this is translating the original 2D pose to the image center. However, this approach does not consider the issue of 2D-3D pose inconsistency."}, {"title": null, "content": "To address this limitation, we propose a novel canonical 2D-3D pose mapping that constrains the 2D-3D data distribution while maintaining 2D-3D pose consistency.\nCanonical 3D Pose Consider an original 3D pose $P_c$ in the camera frame $C$ (blue pose in Fig. 3 (a)) with its root (pelvis) joint $P'_c = [X'_c, Y'_c, Z']^T$. A canonical 3D pose $P_{C_{canon}}$ (red pose in Fig. 3 (a)) can be obtained by centering $P_c$ onto the camera's principal axis. Specifically, as illustrated in Fig. 3 (a), the original 3D pose $P_c$ is rotated around an axis defined by the cross product of the principal axis, $v_{principal} = v_z = [0, 0, 1]^T$, and the pelvis vector, $v_{pelvis} = P'_c = [X'_c, Y'_c, Z']^T$. This transformation is achieved using a canonical rotation, $R_{canon}$, which aligns $v_{pelvis}$ to $v_{principal}$:\n$$P_{C_{canon}} = R_{canon} \\times P_c = \\begin{bmatrix} X_{C_{canon}} \\\\ Y_{C_{canon}} \\\\ Z_{C_{canon}} \\end{bmatrix}$$\n(5)\n$R_{canon}$ can be derived using Rodrigues' rotation formula [49] which defines the rotation matrix $R$ that aligns vector $a$ to vector $b$:\n$$R = I + sin \\theta K + (1 - cos \\theta)K^2,$$\nwhere $v = a \\times b = \\begin{bmatrix} v_x \\\\ v_y \\\\ v_z \\end{bmatrix}^T$\n$$K = \\begin{bmatrix} 0 & -v_z & v_y \\\\ v_z & 0 & -v_x \\\\ -v_y & v_x & 0 \\end{bmatrix}$$\n$$sin \\theta = ||a \\times b||$$\n$$cos \\theta = a \\cdot b$$\n(6)\nIn our case, $a$ and $b$ correspond to $v_{pelvis}$ and $v_{principal}$, respectively. This canonicalization positions the pelvis joint of canonical 3D pose at $[0, 0, ||P'_c||]$ along the principal axis, while preserving the original distance from the camera origin to the pelvis joint and local posture of original 3D pose.\nNote that we define canonicalization as a rotation process to enable 2D canonicalization in the target domain, where the"}, {"title": null, "content": "original 3D pose is not available, by leveraging the properties of perspective projection, as explained in section III-B4.\nCanonical 2D Pose Then, by projecting $P_{C_{canon}}$ onto the image plane using the camera intrinsic parameters, the canonical 2D pose $p_{canon}$ can be derived:\n$$p_{canon} = \\begin{bmatrix} u_{canon} \\\\ v_{canon} \\end{bmatrix} = Z_{C_{canon}} \\begin{bmatrix} \\frac{f_x X_{C_{canon}} + C_x}{Z_{C_{canon}}} \\\\ \\frac{f_y Y_{C_{canon}} + C_y}{Z_{C_{canon}}} \\end{bmatrix}$$\n(7)\nAs illustrated in Fig. 3 (c), the resulting canonical 2D pose is naturally located on the optical center $(c_x, c_y)$ and ensures 2D-3D pose consistency. Although the canonicalized 2D pose is derived from the 3D pose centered along the camera's principal axis, its root joint is not perfectly centered on the image plane due to offsets between the principal point $(c_x, c_y)$ and image center $(W/2, H/2)$. These offsets depend on the camera's intrinsic parameters and therefore vary between datasets. To eliminate these offsets and further constrain the data distribution, an additional centering step is applied to the canonical 2D pose after canonicalization.\n$$p_{canon} = \\begin{bmatrix} u_{canon} \\\\ v_{canon} \\end{bmatrix} = \\begin{bmatrix} \\frac{f_x X_{C_{canon}}}{Z_{C_{canon}}} + W/2 \\\\ \\frac{f_y Y_{C_{canon}}}{Z_{C_{canon}}} + H/2 \\end{bmatrix}$$\n(8)\nThis translation preserves the root-relative posture of the 2D pose, thereby maintaining 2D-3D pose consistency. After applying screen normalization as described in eq. (4), the constant term $(\\frac{W}{2}, \\frac{H}{2})$ is removed, positioning the root joint of the 2D pose at $(0, 0)$ on the image plane.\nNote that, we do not canonicalize the depth (z-direction) and instead allow it to be inherently learned from the data. As a result, the 2D canonical pose reflects varying scales. This is because the domain gap in the scale of the 2D pose is affected by multiple factors, such as focal length, depth, and variations in human size, which complicate the 2D canonicalization process."}, {"title": null, "content": "Canonical 2D-3D Mapping Finally, we define the canonical 2D-3D pose pairs, $\\{P_{C_{canon}}, p_{canon}\\}$, where $P_{C_{canon}}$ denotes the root-relative 3D canonical pose:\n$$P_{C_{canon}} = P_{C_{canon}} - P'_{C_{canon}} = \\begin{bmatrix} X_{C_{canon}} \\\\ Y_{C_{canon}} \\\\ Z_{C_{canon}} - ||P'_c|| \\end{bmatrix} = \\begin{bmatrix} X_{C_{canon}} \\\\ Y_{C_{canon}} \\\\ Z_{C_{canon}} \\end{bmatrix}$$\n(9)\nand $P'_{C_{canon}} = [0, 0, ||P'_c||]$ denotes the root joint position of the canonical 3D pose.\nFig. 3 (b) and (c) show the conventional and proposed canonical 2D-3D mapping. The proposed canonical 2D-3D mapping guarantees 2D-3D pose consistency, whereas the conventional 2D-3D mapping fails to achieve this. In addition, as shown in Fig. 4, the canonical pose mapping, compared to conventional mapping, exhibits a similar distribution for root-relative 3D poses while demonstrating a more constrained distribution for 2D poses. This reduction in the 2D pose distribution decreases the complexity of the 2D-3D pose mapping that the model must learn.\nThis canonical 2D-3D mapping that ensures 2D-3D consistency constructs the canonical domain and serves as the source domain dataset for training a lifting network.\n3) Effect of Canonical 2D-3D Pose Mapping Compared to Conventional Mapping\nIn this section, we compare the proposed 2D-3D pose mapping with the conventional mapping to demonstrate how the proposed method ensures 2D-3D pose consistency and simplifies the learning process.\nFirst, in the context of conventional mapping, the original 2D pose (eq. (2)) can be rewritten by representing the $X_c$ and"}, {"title": null, "content": "$Y_c$ in terms of their root-relative versions, $X'_c$ and $Y'_c$, as:\n$$p = \\begin{bmatrix} \\frac{X_c + c_x}{Z_c} \\\\ \\frac{Y_c + c_y}{Z_c} \\end{bmatrix} = \\begin{bmatrix} \\frac{(X'_c + X'_c) + c_x}{Z_c} \\\\ \\frac{(Y'_c + Y'_c) + c_y}{Z_c} \\end{bmatrix} = \\begin{bmatrix} \\frac{X'_c + (X'_c + c_x)}{Z_c} \\\\ \\frac{Y'_c + (Y'_c + c_y)}{Z_c} \\end{bmatrix}$$\n(10)\nwhere $X_c = X_c - X'_c$ and $Y_c = Y_c - Y'_c$ from eq. (3).\nFrom eqs. (3) and (10), we can derive the conventional 2D-3D mapping:\n$$Conventional: \\begin{bmatrix} \\frac{X_c + c_x}{Z_c} \\\\ \\frac{Y_c + c_y}{Z_c} \\end{bmatrix} = \\begin{bmatrix} \\frac{X_c}{Z_c} + \\frac{c_x}{Z_c} \\\\ \\frac{Y_c}{Z_c} + \\frac{c_y}{Z_c} \\end{bmatrix}$$\n(11)\nSecond, in the context of canonical mapping, we can rewrite the canonical 2D pose (eq. (8)) by representing the $X_{C_{canon}}$ and $Y_{C_{canon}}$ in terms of their root-relative versions, $X_{C_{canon}}$ and $Y_{C_{canon}}$, as:\n$$p_{canon} = \\begin{bmatrix} \\frac{f_x X_{C_{canon}}}{Z_{C_{canon}}} + \\frac{W}{2} \\\\ \\frac{f_y Y_{C_{canon}}}{Z_{C_{canon}}} + \\frac{H}{2} \\end{bmatrix} = \\begin{bmatrix} \\frac{f_x (X_{C_{canon}})}{Z_{C_{canon}}} + \\frac{W}{2} \\\\ \\frac{f_y (Y_{C_{canon}})}{Z_{C_{canon}}} + \\frac{H}{2} \\end{bmatrix}$$\n(12)\nwhere $X_{C_{canon}} = X_{C_{canon}}$ and $Y_{C_{canon}} = Y_{C_{canon}}$ from eq. (9).\nFrom eqs. (9) and (12), we can derive the canonical 2D-3D mapping:\n$$Canonical: \\begin{bmatrix} \\frac{f_x X_{C_{canon}}}{Z_{C_{canon}}} + \\frac{W}{2} \\\\ \\frac{f_y Y_{C_{canon}}}{Z_{C_{canon}}} + \\frac{H}{2} \\end{bmatrix} = \\begin{bmatrix} \\frac{f_x X_{C_{canon}}}{Z_{C_{canon}}} + \\frac{W}{2} \\\\ \\frac{f_y Y_{C_{canon}}}{Z_{C_{canon}}} + \\frac{H}{2} \\end{bmatrix}$$\n(13)\nBy ignoring the known constant terms $(c_x, c_y)$ and $(\\frac{W}{2}, \\frac{H}{2})$ from eqs. (11) and (13), we derive the final"}, {"title": null, "content": "version of both mappings:\n$$Conventional: \\begin{bmatrix} \\frac{X_c}{Z_c} \\\\ \\frac{Y_c}{Z_c} \\end{bmatrix} \\mapsto \\begin{bmatrix} X \\\\ Y \\end{bmatrix}$$\n$$Canonical: \\begin{bmatrix} \\frac{X_{C_{canon}}}{Z_{C_{canon}}} \\\\ \\frac{Y_{C_{canon}}}{Z_{C_{canon}}} \\end{bmatrix} \\mapsto \\begin{bmatrix} X \\\\ Y \\end{bmatrix}$$\nIn the x and y dimensions, the conventional mapping is expressed in the form $AX + B \\rightarrow X$. Consequently, the conventional mapping requires the model to estimate the offset term $B$, which originates from the offset relative to the principal axis $(X'_c, Y'_c)$, as well as the scale term $A$. This offset induces relative rotation, contributing to the 2D-3D pose inconsistency discussed in section III-B1. In contrast, the proposed canonical mapping is expressed in the form $AX \\rightarrow X$, where only the scale term $A$ needs to be estimated. The absence of the offset term $B$ reflects 2D-3D pose consistency and simplifies the task for the network. This distinction reduces the complexity of the model learning process when using the canonical mapping, as compared to the conventional mapping.\n4) 2D Canonicalization at Test Time\nOnce a lifting network is trained on the canonical domain, it can be directly applied at test time without further fine-tuning,"}, {"title": null, "content": "provided that the target domain 2D input is appropriately canonicalized. However, since ground truth 3D poses are not available at test time, canonicalizing the input 2D pose from the 3D pose, as described in section III-B2, is not applicable.\nThe proposed canonicalization process is designed to address this limitation by leveraging several properties of perspective projection: (1) under perspective projection, each joint has a unique homogeneous coordinate $[\\, , 1]$, which is obtained by dividing each joint coordinate by its corresponding depth component. This homogeneous coordinate can also be interpreted as the 2D pose projected onto the normalized image plane at a depth of 1 (referred to as the normalized 2D pose); (2) the rotated version of the original 3D joint and its corresponding normalized 2D joint on the normalized image plane also share the same homogeneous coordinates (indicated by the black line on the normalized image plane in Fig. 5); and (3) the canonical rotation $R_{canon}$ can be derived from the pelvis vector and the principal axis vector, both of which retain their direction invariance under perspective projection.\nFrom the first and second properties, given a 3D pose in the camera frame and a rotation matrix $R$, the same 2D pose can be derived from either the rotated 3D pose or the rotated normalized 2D pose by projection with the same intrinsic matrix. This observation is directly applicable to the proposed rotation-based canonicalization process. Therefore, the same canonical 2D pose can be obtained from either the"}, {"title": null, "content": "3D pose or its corresponding normalized 2D pose. From the final property", "8": [9], "Phase)": 1}]}