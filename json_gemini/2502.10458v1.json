{"title": "I Think, Therefore I Diffuse:\nEnabling Multimodal In-Context Reasoning in Diffusion Models", "authors": ["Zhenxing Mi", "Kuan-Chieh Wang", "Guocheng Qian", "Hanrong Ye", "Runtao Liu", "Sergey Tulyakov", "Kfir Aberman", "Dan Xu"], "abstract": "This paper presents ThinkDiff, a novel alignment\nparadigm that empowers text-to-image diffusion\nmodels with multimodal in-context understand-\ning and reasoning capabilities by integrating the\nstrengths of vision-language models (VLMs). Ex-\nisting multimodal diffusion finetuning methods\nlargely focus on pixel-level reconstruction rather\nthan in-context reasoning, and are constrained\nby the complexity and limited availability of\nreasoning-based datasets. ThinkDiff addresses\nthese challenges by leveraging vision-language\ntraining as a proxy task, aligning VLMs with the\ndecoder of an encoder-decoder large language\nmodel (LLM) instead of a diffusion decoder. This\nproxy task builds on the observation that the LLM\ndecoder shares the same input feature space with\ndiffusion decoders that use the corresponding\nLLM encoder for prompt embedding. As a re-\nsult, aligning VLMs with diffusion decoders can\nbe simplified through alignment with the LLM de-\ncoder. Without complex training and datasets,\nThinkDiff effectively unleashes understanding,\nreasoning, and composing capabilities in diffusion\nmodels. Experiments demonstrate that ThinkDiff\nsignificantly improves accuracy from 19.2% to\n46.3% on the challenging CoBSAT benchmark\nfor multimodal in-context reasoning generation,\nwith only 5 hours of training on 4 A100 GPUs.\nAdditionally, ThinkDiff demonstrates exceptional\nperformance in composing multiple images and\ntexts into logically coherent images. Project page:\nhttps://mizhenxing.github.io/ThinkDiff.", "sections": [{"title": "1. Introduction", "content": "Can diffusion models take \u201cIQ tests\"? Figure 1a presents\nan example of a visual analogy IQ test. The model is pro-\nvided with images of a flying monkey and a flying cat, along\nwith text prompts of monkey, cat, and zebra, and asked to\ngenerate the next image. A reasonable output image should\nbe an image of a flying zebra, requiring the model's ability\nto reason and recognize implicit patterns in context, such as\nthe shared attribute of the flying action in this example.\nThe concept of enabling diffusion models to think and then\ngenerate is compelling yet underexplored. Current text-to-\nimage diffusion models (AI, 2024c; Forest, 2024a) excel at\ngenerating high-quality images by strictly following explicit\nprompts, while typically lacking multimodal in-context rea-\nsoning. Unlocking reasoning capabilities in them can enable\nthem to handle more sophisticated tasks, such as interpreting\ncomplex instructions, solving visual analogy problems that\nrequire inferring implicit logic relationships, and composing\nmultiple images and text in a logically consistent manner."}, {"title": "2. Related Work", "content": "Diffusion models have become powerful tools for text-to-image generation (Ho et al., 2020; Rombach et al., 2022; For-est, 2024a). Early models, e.g. Stable Diffusion (Rombach\net al., 2022), use CLIP (Radford et al., 2021) for prompt\nembedding, while recent works integrate large language\nmodels (LLMs) (Saharia et al., 2022; Chen et al., 2024;\nAI, 2024c) for complex prompts. Methods such as Control-Net (Zhang et al., 2023), T2I-Adapter (Mou et al., 2024),\nand IP-Adapter (Ye et al., 2023) introduce structural and\nimage-level controls by reconstruction-based fine-tuning.\nPersonalized generation has been enhanced by methods like\nDreamBooth (Ruiz et al., 2023), and other methods (Gal\net al., 2023; Wang et al., 2024a; Li et al., 2024; Wang et al.,\n2024c; Qian et al., 2024; Wang et al., 2024d), some of which\nuse interleaved image-text inputs (Pan et al., 2023; Berman\n& Peysakhovich, 2024). However, these methods focus\non reconstruction fidelity rather than in-context reasoning.\nIn contrast, our method equips diffusion models with the\nmultimodal in-context reasoning capabilities of VLMs."}, {"title": "2.2. Unified understanding and generation", "content": "Recent work on large language models (LLMs) and diffu-sion transformers (Peebles & Xie, 2023; Forest, 2024a) has\ninspired unified models for multimodal understanding and\ngeneration. These models either finetune LLMs to gener-ate image tokens, which are then decoded into images via\ndiffusion decoders (Ge et al., 2024; Pan et al., 2023; Sun\net al., 2023; Koh et al., 2024; Wu et al., 2023; Ye et al.,\n2024), or integrate text, image, and noise tokens within a"}, {"title": "2.3. Vision-language training", "content": "Vision-language training has proven effective in develop-ing powerful multimodal models. CLIP-like models (Rad-ford et al., 2021; Fang et al., 2023) use contrastive learning\nto align image and text embeddings. Recent large vision-language models (LVLMs)(Li et al., 2023; Liu et al., 2023;\nZhu et al., 2023; AI, 2024a; Wang et al., 2024b) align\nCLIP visual features with advanced large language mod-els (LLMs)(Brown et al., 2020; Achiam et al., 2024; AI,2024a; Yang et al., 2024a) by fine-grained text prediction.\nThis vision-language training enables robust multimodal fea-ture alignment, developing multimodal understanding and\nreasoning by leveraging powerful LLMs. Inspired by these\nadvancements, our method employs vision-language train-ing as a proxy task to bridge VLMs with diffusion models,\ninheriting their advanced multimodal reasoning capabilities."}, {"title": "3. Method", "content": "ThinkDiff employs VLMs to enable diffusion decoders toperform multimodal in-context reasoning. This is achievedby an aligner network that bridges a VLM and a diffusiondecoder. As described in Section 1, ThinkDiff simplifies thealignment process by introducing a proxy task that alignsthe VLM with an LLM decoder using text supervision. Thistask is based on the shared input feature space between\nthe LLM decoder and diffusion decoder. Figure 2b and Fig-ure 4 illustrate the overall network structure and two modelvariants, respectively. The multimodal input comprises aset of images {I\u2081} and text tokens {T;}. The aligner net-work processes its input token features {x;} into its outputtoken features {x}. In training, ThinkDiff generates texttokens {y}, supervised by ground truth text tokens {y}. Ininference, it generates an image I'.\nModule Overview. ThinkDiff comprises three submodules:\na source VLM (MvLM), an aligner network (MAN), anda decoder. The decoder is a LLM decoder (MLLMD) intraining and a diffusion decoder (MDiffD) in inference.\nSource VLM. The source VLM generates multimodal to-"}, {"title": "3.2. Aligner network", "content": "The aligner network MAN is a lightweight module compris-ing two linear layers (LLinear), a GELU activation (LGELU)and an RMSNorm layer (Zhang & Sennrich, 2019) (LNorm).\nGiven the VLM's output {x}, the output {x} of Man is:\n{x} = LNorm(LLinear(LGELU(LLinear({x})))) (1)\nIn training, only MAN is updated. Despite its simplicity,\nMAN can effectively aligns feature spaces of the powerful\nVLM and the LLM decoder in the training.\nStable training. Our experiments revealed that without acarefully initialized RMSNorm layer, ThinkDiff encountersconvergence issues due to a scale mismatch between theVLM output space and the LLM decoder input space. To ad-dress this, we incorporate an RMSNorm (Zhang & Sennrich,2019) layer into MAN, initialized with parameters from theLLM encoder's final RMSNorm layer. Since the LLM en-coder output space aligns naturally with the LLM decoderinput space, this initialization ensures consistent scale align-ment at the start of training, significantly improving trainingstability and convergence."}, {"title": "3.3. ThinkDiff-LVLM", "content": "ThinkDiff-LVLM incorporates a decoder-only large vision-language model (LVLM) that excels at advanced in-contextreasoning tasks", "is": "nI' = MDiffD(Man(MLvLMG({Ii"}, {"as": "n{y"}, "MLLMD(MAN(fmask(MLvLG(I,T)))), (2)\nwhere fmask is the random masking and MLVLMG is theLVLM's generation process. The cross-entropy loss is:\nLLVLM = - \\frac{1}{N} \\sum_{i=1}^{N} log p(y = y\u00b2).\nWhy use LVLM's generated tokens. Some diffusion models (Liu et al., 2024; Xie et al., 2024) incorporate decoder-only LLMs for prompt encoding but actually treat them asencoders by using the deep features of input tokens. Incontrast, ThinkDiff-LVLM uses the deep features of thegenerated tokens from the LVLM decoder as input to thealigner. This design is motivated by the insight that, inautoregressive models, reasoning is embedded in the generation process. Tokens are generated sequentially, conditionedon both the input context and the prior generated tokens. As"]}