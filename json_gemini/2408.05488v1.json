{"title": "Structure and Reduction of MCTS for Explainable-AI", "authors": ["Ronit Bustina", "Claudia V. Goldman"], "abstract": "Complex sequential decision-making planning problems, covering infinite states' space have been shown to be solvable by AlphaZero type of algorithms. Such an approach that trains a neural model while simulating projection of futures with a Monte Carlo Tree Search algorithm were shown to be applicable to real life planning problems. As such, engineers and users interacting with the resulting policy of behavior might benefit from obtaining automated explanations about these planners' decisions offline or online. This paper focuses on the information within the Monte Carlo Tree Search data structure. Given its construction, this information contains much of the reasoning of the sequential decision-making algorithm and is essential for its explainability. We show novel methods using information theoretic tools for the simplification and reduction of the Monte Carlo Tree Search and the extraction of information. Such information can be directly used for the construction of human understandable explanations. We show that basic explainability quantities can be calculated with limited additional computational cost, as an integrated part of the Monte Carlo Tree Search construction process. We focus on the theoretical and algorithmic aspects and provide examples of how the methods presented here can be used in the construction of human understandable explanations.", "sections": [{"title": "1 Introduction", "content": "Complex real life planning problems, such as autonomous vehicles' behavior, can be framed as sequential decision-making problems. Given a general route for the vehicle, the vehicle needs to determine the specific steering angle and accelerations that result in what we would consider to be good driving. Due to the complexity of the problem, in which there are many relevant entities with probabilistic behaviors, the preferred solution falls within the reinforcement learning (rl) paradigm [23]. Moreover, due to the very large (practically infinite) state and action spaces in real life problems, a promising approach heavily researched is based on the AlphaZero algorithm [19, 20] which is a model-based algorithm allowing us to perform an evaluation of the specific state encountered in real-time and use offline learned insights only as prior information [9]. The AlphaZero algorithm builds on the Monte Carlo Tree Search (MCTS) approach [4, 24], which gradually builds a tree from the current state (which is the root of the tree), to depict the possible futures (the evaluation) resulting from different sequences of actions.\nThe motivation for this work is the need to explain the decisions of the planning system to an engineer or user that is experiencing that planner's resulting behavior. As such, this work falls in under explainable AI (\u03a7\u0391\u0399) [6, 1, 17, 16, 10]. Explanations designed specifically for automotive applications, meaning during automated functions of the vehicle, were also considered in [26], [3] and [7]. We are considering XAI for sequential decision making, which has also received some attention. A wider review of this problem that specifically focused on contrastive explanations was done in [11]. Explanations as model reconciliation, meaning the explanations come in light of the differences between the model of the AI system and the user model were examined in [5, 21, 22]. Specifically, in [21] the suggested approach is to combine the extraction of explanations with the planning problem. The explanations considered in [13] are policy summarization. The goal of policy summarization is to convey the strengths and weaknesses of the agent by demonstrating their behavior in a subset of informative states. Another work that follows a similar line, but focuses on the explanability of RL agents is [18]. Another approach, considered in [12], focuses on producing more explicable planning results. They do so by learning an explicability distance function between the AI plans and human's input. This learned explicability distance function is then used as a heuristic to guide the search towards explicable plans. In [14] the authors considered a setting similar to ours, with an RL agent that uses search trees to reach its decision. The problem they are considering is the ability of humans (AI experts) to identify reasoning flaws. They too are faced with the complexity of the search tree, and suggest the construction of a user interface to assist in the identification process. The work we suggest here could be a valuable tool to support such efforts. Finally, [2], considers the same problem of explainability of MCTS, and gives some high-level directions towards its solution.\nThe XAI for sequential decision making in automated driving is a very complex problem. To simplify the problem we suggest that it can be studied by answering three main questions: When, What and How to explain. The when refers to the decision of whether or not to provide an explanation at any point in time during the ride. The what refers to the actual content of the explanation, and the how refers to the modality in which this explanation will be conveyed. These questions are not independent\u00b9 and the split comes only as a method to simplify the overall problem. Our focus in this work is on the extraction of information from the planner that can assist in determining the above mentioned aspects of the explanations. We shortly provide concrete examples of usage of the methods suggested here relating to both the when and what questions."}, {"title": "2 Entropy of MCTS as a Measure of Structure", "content": "The MCTS algorithm gradually builds a tree, starting from the current root state, by performing the following four steps over and over again until some time/computation budget is concluded [4]:\n1. Selection: starting from the root node, the selection process goes down the tree until it reaches a leaf node. The specific selection used is the Upper Confidence Bound applied to Trees (UCT) which balances exploitation of actions that have shown to be favorable, and exploration of actions that have not been sufficiently explored.\n2. Expansion: when reaching a leaf node, it can be a terminal node, in which case it cannot be expanded. If the node is not a terminal node, the tree is expanded to the set of actions that can be considered from this node.\n3. Simulation/Neural Network (NN): the properties of the expanded node can be evaluated using a Monte-Carlo simulation, where multiple roll-outs reaching a terminal state are performed to assess the value of the expanded node, and a prior distribution over its actions. In the AlphaZero family of algorithms the off-line learning of the value and prior per state (node) replace this.\n4. Back-propagation: after expanding and extracting the value and prior distribution of the node, this new information is back-propagated all the way up the tree to the root node. The update is done for the accumulated value at each node and the number of visits at each node.\nThus, at every step the tree is expanded by a single node (excluding cases of terminal nodes in which there is no expansion). When the time/computation budget is over, the choice of action at the root node is usually determined as the most visited child of the root. At each node in the tree the vector corresponding to the number of visits of its children nodes can be considered a probability vector reflecting the choice of the algorithm to expand the tree in one direction over the other. This aspect, together with the gradual expansion are the main points used next."}, {"title": "3 Reduction of MCTS using Subtree Removal", "content": "So far we tackled the extraction of information spread within the tree to numerical values that capture the structure, and hence the reasoning of the AlphaZero algorithm. We have shown that this can be done efficiently. Another usage of these numerical entropy values is in tackling the aspect of size.\nReduction of a tree can be done in several ways. One can unify two nodes into one, or summarize two subtrees into a single subtree. In this work we specifically consider the reduction of a tree using only subtree removals. This means, that the tree can be reduced by picking the set of nodes that will be removed, where by removing a node we remove the entire subtree spanned from it.\nAny form of removal usually has some guiding criteria defining when to perform a removal. The specific choice of the criterion depends on the intended usage of the reduced tree. Our goal is to obtain a smaller, and more concise MCTS. We want to reduce the size of the MCTS, while still holding to the information it conveys. Such information contains, for example, the regions that the algorithm expanded more, the main children that were expanded at each level, etc. Thus, we propose a criterion that examines this exact trade-off. On the one hand we wish to reduce the size of the MCTS, meaning the number of nodes, while on the other hand we have the entropy of the MCTS which captures the interest. The trade-off between entropy and size should leave us with a MCTS that is smaller and focused on complex areas that are of interest. In mathematical terms this can be written as:\n$\\max {H (T^0) \u2013 \\beta \\log (|A|) \\text{numberOfNodes} }$.   (12)\nwhere $\\beta$ is the trade-off factor, $\\text{numberOfNodes}$ can be approximated by the value of the $\\text{numberOfVisits}$ in MCTS (as most visits result with an expansion of the tree, meaning they increase the tree by one node) and $A$ is our action space.\nIn order to make the two terms comparable we consider both terms as descriptive information. The first term, the entropy, provides us with the number of bits required to describe the tree given the probability vectors that were extracted from the number of visits. For the second term, we consider a worst case tree with $\\text{numberOfNodes}$ nodes, in the sense of description. An upper bound on the longest description is attained by a uniformly distributed tree over all possible actions, $A^l$, at each node. The range of relevant values to consider for $\\beta$ are $[0, \\beta_{ub}]$ where:\n$\\beta_{ub} = \\frac{H (T^0)}{\\log (|A|) \\text{numberOfNodes} }$.   (13)\nExamining equation (12) we have a clear understanding of how the right element behaves when we remove a subtree, as it is monotonically decreasing in $\\text{numberOfNodes}$. $H (T^0)$, on the other hand, is a bit more complex. This is the entropy of the entire tree. When we remove a subtree somewhere within the tree, the entropy of the entire tree changes. However, it does not necessarily increase or decrease. The next theorem shows exactly how the entropy of a node changes, given that one of its immediate children has been removed.\nTheorem 1. Assume a node at depth i defined by $T_i = t$, with N number of visits accumulated at its children. Its entropy is given by equation (8). When we remove child k the entropy of this node is as follows:\n$\\hat{H}(T^i \\vert T_i = t) = \\hat{H}(T^i \\vert T_i = t) + \\frac{1}{1 - Pr(X_i = x_k \\vert T_i = t)}(Pr(X_i = x_k \\vert T_i = t)H(T \\vert T_i = t) - H_0(Pr(X_i = x_k \\vert T_i = t)) -H (T^{i+1} \\vert T_i = t, X_i = x_k))$,   (14)\nwhere $H_0()$ is the binary entropy.\nTheorem 1 gives us the new entropy of the node whose child has been removed in terms of the original entropy of that node (before the removal) - $\\hat{H}(T^i \\vert T_i = t)$. We can see that not in all cases will the entropy decrease when a child node is removed. Whether a removal will decrease or increase the entropy, depends on the expression in the last two rows of (14). These rows examine the difference between a fraction of the original entropy (according to the probability of the removed child) and the contribution of the removed node given in two terms: the binary entropy of the probability of the removed child and the entropy contribution of the removed child. This is to be expected, and gives us an exact expression to examine whether a specific removal will increase or decrease the entropy of its parent node.\nTheorem 1 considers the effect of a subtree removal on the entropy of the immediate parent node, however, we are more interested in how a removal effects the entropy of the entire tree. Recall that a removal of a subtree changes not only the probability vector of the parent node (and hence its local entropy contribution), but also the probability vectors of all of its ancestors.\nThe next theorem considers the more general effect by considering the change to the entropy of a parent node given that the entropy of one of its children has changed. This result can be used over and over to propagate the change in entropy up the tree.\nTheorem 2. Assume a parent node at depth i defined by $T_i = t$, with $N_p$ number of visits accumulated at its children (counts). Further assume that the lth child of this node had a change in its subtree that resulted with a decrease in the accumulated number of visits of its children from $N$ to $N(1-p)$ and as a result of this change the entropy of this child changed from $H_e = H (T^{i+1} \\vert T_i = t, X_i = x_e)$ to $\\hat{H_e} = H (T^{i+1} \\vert T_i = t, X_i = x_e)$. The entropy of the parent node is as follows:\n$\\hat{H}(T \\vert T_i = t) = \\hat{H}(T \\vert T_i = t) + \\frac{1}{1 - p} (p_eH_e  - H_b(p) +(Pr(X_i = x_e \\vert T_i = t) \u2013 \\rho)\\cdot (H (T^{i+1} \\vert T_i = t, X_i = x_e) - H (T^{i+1} \\vert T_i = t, X_i = x_e)))$,   (15)\nwhere\n$\\rho = \\rho (Pr(X_i = x_e \\vert T_i = t)= \\frac{1}{Np}$.   (16)"}, {"title": "4 Implementation & Empirical Evaluations", "content": "There is a gap between our theoretical ambition to achieve a reduction according to the entropy vs. size trade-off and its possible feasible implementations. The two central issues are, first, that there are $2^{\\text{numberOfNode}}$ possible subsets of reductions to consider. Second, the benefit of a specific subtree reduction might change once another subtree reduction is performed. Putting these two together makes the complexity of finding the optimal solution unaffordable. Thus, we chose greedy methods to perform reduction that are motivated by the trade-off given in equation (12). The first is a local approach that goes down the tree and for each node finds the optimal reduction set, meaning the set of children of that node that if removed would produce the highest trade-off, and performs their reduction. The second base-line approach is a two-stage approach. It goes down the tree and finds the best possible reduction set from each node. This reduction is not performed but rather placed in a priority list - pList. This priority list is kept sorted from best reduction set to worst. Note that the trade-off values determining the performance of a specific reduction are measured assuming that this it the only reduction performed on the entire tree. The second stage is to go over pList. We consider three variants of the second stage. The first variant performs all reductions in pList. The second variant performed the reductions in pList until we reach a reduction that no longer improves the trade-off of the entire tree in its new form (after reductions performed thus far). The last variant, denoted as two-stage V2, examines each element in pList whether the specific reduction set improves the trade-off of the entire tree in its new form. The difference between the second and last variants is that the last variant continues to go over pList even if a bad element, which did not improve the trade-off has been encountered. The one-stage approach gives more preferences to reductions up the tree, since those are encountered first and performed. It has the advantage that future potential reductions are evaluated on the already reduced tree. Alternatively, the two-stage approach evaluates each node as if no reductions have been performed, and by that allows all potential reductions to compete fairly. This emphasizes the difference between the one-stage approach and the first variant, which although performing all reductions in the priority list, these can be very different reductions than those performed by the one-stage approach. The complexity of both base-line algorithms is $O(N^2A \\log N)$, where N is the number of nodes in the tree, and $2 A^l$ is included to show the dependency on the cardinality of A."}, {"title": "5 Summary", "content": "In this work we tackle two aspects of the MCTS that are fundamental in our ability to support explainability of sequential decision-making algorithms that construct MCTS, such as AlphaZero. The first aspect is the spread of information throughout the MCTS, and specifically information related to the reasoning process of the algorithm. We show that this information, which influences the resulting structure of the MCTS, can be showcased as numerical entropy values that can be calculated efficiently during the expansion of the MCTS. The second aspect is the size of the resulting MCTS. We propose to use the entropy values as guiding values in the reduction of the tree in a trade-off criterion, thus preserve interesting attributes of the MCTS while attempting to reduce its size. We examine greedy algorithms motivated by the suggest criterion and evaluate their performance on MCTSs produced in driving simulations.\nThese methods come as a tool to assist in the extraction of meaningful understandable explanations of the sequential decision-making algorithm. We have only began to apply them in such constructions, and see numerous different paths for the future."}, {"title": "Appendix", "content": "A Basic Implementation of Entropy Calculation\nA basic implementation of the calculation of the entropy at each node assumes a given tree. The approach recursively goes down the tree and is given in Algorithm 3. As mentioned in the paper, the benefit of following the suggested formalization is that it produces the entropy not only of the entire MCTS but also at every node for the subtree spanned from it.\nB Proof of Lemma 3\nProof. We first want to show that $\\hat{p}$ is a valid probability vector by showing that the sum of its components is one:\n$\\sum \\frac{p_i}{\\frac{i}{1-p}}+ \\frac{1- \\sum_i{p_i}}{1-p} = \\frac{\\sum{p_i}-p+1-\\sum_i {p_i}}{1-p} = \\frac{1-p}{1-p}=1$.   (23)\nWe are comparing two entropies. The original entropy is of $p$, and is as follows:\n$H(p) = - \\sum p_i log_i - (1-\\sum i) log (1-\\sum i)$\nand we want to connect it to the entropy of $\\hat{p}$, meaning after the change to the lth component. We can do so through the following observations:\n$\\hat{p} =(1-\\hat{p})(\\frac{p_1}{1-\\hat{p}},...,\\frac{p_{e-1}}{1-\\hat{p}},\\frac{\\hat{p}}{1-\\hat{p}},\\frac{p_{e+1}}{1-\\hat{p}},..., \\frac{1- \\sum{P_i}}{1-\\hat{p}}) + \\hat{p}. (0,...,0,1,0,...0)$   (24)\nThus, we can think of two random variables. The first is a binary random variable, denoted by $B$, with probability $\\hat{p}$ to be one and 1$\\hat{p}$ to be zero. The second random variable $X$ is defined over an alphabet of size $|A|$, and depended on $B$. When B is zero, X is distributed according to\n$(\\frac{p_1}{1-\\hat{p}},...,\\frac{p_{e-1}}{1-\\hat{p}},\\frac{\\hat{p}}{1-\\hat{p}},\\frac{p_{e+1}}{1-\\hat{p}},..., \\frac{1- \\sum{P_i}}{1-\\hat{p}} )$.   (25)\nWhen B is one X is deterministic and its value is set to the lth element in the alphabet. Thus, the entropy can be written as follows:\n$H(\\hat{p}) = H(X, B) = H(B) + H(X\\vert B)$   \n$= H_b(\\hat{p}) +\nPr (B = 0) H(X\\vert B = 0) + Pr (B = 1) H(X\\vert B = 1)$   \n$= H_b(\\hat{p}) + (1 \u2013 \\hat{p})H(X\\vert B = 0)$   \n$= H_b(\\hat{p}) + (1 \u2013 \\hat{p})H(\\hat{p}).$   (26)\nThis concludes the proof."}]}