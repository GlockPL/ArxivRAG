{"title": "Dreaming to Assist: Learning to Align with Human Objectives for Shared Control in High-Speed Racing", "authors": ["Jonathan DeCastro", "Andrew Silva", "Deepak Gopinath", "Emily Sumner", "Thomas M. Balch", "Laporsha Dees", "Guy Rosman"], "abstract": "Tight coordination is required for effective human-robot teams in domains involving fast dynamics and tactical decisions, such as multi-car racing. In such settings, robot teammates must react to cues of a human teammate's tactical objective to assist in a way that is consistent with the objective (e.g., navigating left or right around an obstacle). To address this challenge, we present DREAM2ASSIST, a framework that combines a rich world model able to infer human objectives and value functions, and an assistive agent that provides appropriate expert assistance to a given human teammate. Our approach builds on a recurrent state space model to explicitly infer human intents, enabling the assistive agent to select actions that align with the human and enabling a fluid teaming interaction. We demonstrate our approach in a high-speed racing domain with a population of synthetic human drivers pursuing mutually exclusive objectives, such as \u201cstay- behind\" and \"overtake\". We show that the combined human-robot team, when blending its actions with those of the human, outperforms the synthetic humans alone as well as several baseline assistance strategies, and that intent-conditioning enables adherence to human preferences during task execution, leading to improved performance while satisfying the human's objective.", "sections": [{"title": "1 Introduction", "content": "In high-stakes situations where members of a team must coordinate their physical actions in the world for the team to succeed, early coordination on tactical objectives is crucial. In a rapidly-evolving task, such as in high-speed competitive sports, agents must find a way to attain such coordination without explicit communication. A robotic assistive agent equipped with an ability to reason using theory of mind [1] has been shown to be critical to successful collaboration without the burden and latency of explicit communication. In such settings, agents must maintain a rich-enough model of the world to cover a common set of concepts that each agent needs to plan. This includes dynamics of the physical world, the objectives of the team, and the current intent of the other members of the team. Such considerations are prevalent in sports [2, 3], manufacturing [4], healthcare [5], and traffic modeling settings [6], among others.\nHigh-speed performance driving presents a domain where accurate and expressive models are required to have effective human-robot teams. The dynamics in racing evolve quickly, preventing team members from communicating their goals or objectives before taking an action [7]. Because of this constraint, existing approaches in shared control or advanced driver assistance often split authority on predefined boundaries (e.g. steering vs. throttle and brake) [8, 9]. The driving domain also requires us to tackle the multimodal nature of the human decision-making problem (e.g. rules or maneuvers), where discrete decisions are required to be made given knowledge of the situation [10, 11, 12, 13]."}, {"title": "1.1 Related Works", "content": "Significant work has taken place in sharing control [8] and human-robot interaction [17, 18], exploring topics from ergonomics and physiology to language-based strategic teaming. Prior research has presented learning-based approaches to human-robot interactions that target joint-representation learning [19, 20, 21, 22, 23, 24, 25, 26]. These works focus on a range of topics, including data- efficient representation learning [27], intent or plan modeling [28, 29, 30, 31], hypothesis-space specification and explanation [32, 24, 33], or other cognitive and social motivations [34, 35, 36]. We go beyond prior work by inferring the human's discrete intent and modifying our robotic assistant's objective to encourage support of the inferred intent.\nIn the context of driving, shared control has been explored for planning approaches [37, 38, 39, 40, 41, 42, 43, 44, 45] and learning-based approaches [46, 47, 48, 49, 50]. Prior work has also considered incorporating game-theory into shared control [50, 51], as well as explicit human-centric design considerations [52, 53]. We refer readers to [54, 55] for comprehensive reviews. Beyond shared control, significant literature explored intent prediction in driving, see [56, 57], and references therein.\nRecent work has proposed a shared-control model using model-predictive control that considers predicted trajectory information [39], therefore implicitly capturing driver intent. Additional recent work has augmented a model-predictive controller with the ability to explicitly infer driver intent [9], enabling the controller to share steering and control actions with a human. In our approach, we capture discrete driver intent in a way that is conducive to semantically-meaningful, multi-modal continuous behaviors (e.g., going left or right around an obstacle). Further, by framing our task as a multi-agent reinforcement-learning problem, our proposed solution extends to multiple agents much more naturally than prior work. Finally, we frame our decision making approach within a recurrent state-space model [58] which is extended to infer the objective of the human, building on recent work that has explored hierarchical or hybrid state abstractions [59, 60]."}, {"title": "2 Background and Problem Statement", "content": "We target the problem of shared control in the highly-dynamic setting of high-performance racing against other racing opponents. We aim to build an assistive agent that is capable of reasoning over well-defined task objectives, as well as more general, harder-to-define, and harder-to-observe human objectives. The assistive agent is given a map of the track, states of the ego vehicle and opponent vehicle, the ego driver's steering and throttle controls. The agent's task is to assist the ego driver through modifications to the steering and throttle of the ego car, as depicted in Fig. 1 (lower right).\nTo achieve optimal performance, the agent must provide continuous control adjustments as the driver progresses along the track, helping the driver to stay on course, maintain proper speeds, and avoid collisions. Further, the agent must accurately infer the intentions of the driver (such as \"stay to the opponent's left\") and provide control augmentations that help the driver to accomplish their immediate task objective more optimally. Note that this problem is different from a conventional autonomous driving problem, as the agent's actions are conditioned explicitly on the human driver's control input, and the control signal that actuates the car is a linear blend of the agent and the driver's control signals. This problem also differs from conventional human-robot teaming, in that the agent must infer a human's intent (i.e., the human objective) early and as consistently as possible, as the efficacy on its assistance (in terms of safety and performance) depends strongly to how early, accurately, and reliably the human objective is captured."}, {"title": "3 Approach", "content": "Our approach learns a common latent representation in the structure of a recurrent state space model (RSSM) [61, 62] to build a hybrid discrete / continuous state of the human partner's behavior, their rewards, and intent for achieving certain goals. RSSMs have been shown to be suitable for many domains (e.g. locomotion, Atari, Minecraft), and we are the first to apply this to modeling human behavior for assistance. To accommodate both a human and assistive agent operating in a collaborative setting, we train the assistive agent alongside human agents, with both agents sharing actions taken in an environment. Each agent receives a reward for each action taken, and each are trained to (1) build an accurate RSSM and (2) learn how to act in order to maximize its own expected returns [63]. One challenge in collaboration is that part of the assistive agent's world model includes aspects of the future human's plan such as preferences, desires, and goals of the human (these are"}, {"title": "3.1 Building a World Model over the Human and Physical Environment", "content": "In order to allow the system to maintain an ability to reason over the preferences of the human independent of that of the joint human-robot system, our approach considers both the human driver and the assistive agent planner as separate models. The training process is outlined in Fig. 2.\nIn the cooperative setting, the model of the environment follows a structure in which there are certain task-specific rewards which are available to both human and assistive agents, with the key distinctions being human-objective (specific to the human), and intervention penalties (specific to the assistant).\nTask-Specific Rewards We assume standard task-specific rewards for high-performance driving from prior work [64, 15], including an out-of-bounds penalty, passing reward, and collision penalty.\n$r_{task} = r_{collision} + r_{bounds} + r_{finish}$      (1)\nwhere $r_{collision}$ is a negative reward for collisions, $r_{bounds}$ is a negative reward for driving too far off the track, and $r_{finish}$ is a positive reward for reaching the finish line."}, {"title": "Intent-Aware World Model", "content": "The objective of the world model is to provide a representation that the agent can use to interact with the driver and the world, and we posit that this representation can support the agent to reason jointly about both the task and human objectives. We build off of the recurrent state-space model (RSSM) of DreamerV2/V3 [65, 61], according to the architecture in Figure 2. For agent \u043a \u2208 {H, A} (respectively, the Human and Assistive agent), the RSSM model, parameterized by , and denoted W, includes:\nEncoder for discrete representation $z_t^k \\sim q(z_t | h_t, x_t)$\nSequence model for recurrent state $h_t^k = f(h_{t-1}^k, z_{t-1}^k, a_{t-1}^k)$\nDynamics predictor: $\\hat{z}_t^k \\sim p(z_t | h_t)$     (2)\nwhere $x_t$ denotes the input observation. The output heads are similar to the DreamerV2/V3 architec- ture, with the addition of an intent predictor for the assistant agent (in red), and are all bottlenecked on the hidden states $s_t^k = {h_t^k, z_t^k}$,\nDecoder: $\\hat{x}_t \\sim p(x_t | s_t^k)$ Continue predictor: $\\hat{h}_{t+1}^k \\sim p(h_{t+1} | s_t^k)$\nReward: $\\hat{r}_t \\sim p(r_t | s_t^k)$ Intent predictor: $\\hat{y}_t \\sim p(\\hat{y}_t | s_t^k)$        (3)"}, {"title": "3.2 Alignment with the Human", "content": "To train the intent predictor, we assume at least partial access to the intent labels for a given episode, which takes the form of an integer-valued target function. We normalize the output by predicting the symlog (sign(y) ln(|y| + 1)) of the output, and use a discrete distribution to predict y using the two-hot encoding of [61]. The parameters  of the world model Wr are trained to minimize the loss\n$\\mathcal{L}(\\Phi) = E_{q} [\\beta_{pred}\\mathcal{L}_{pred} + \\beta_{KLL}\\mathcal{L}_{KL}]$ \nThe prediction loss $\\mathcal{L}_{pred}$ minimizes the likelihood under the predictor distributions in (3), $\\mathcal{L}_{KL}$ minimizes the KL divergence between the prior $p_{\\phi}$ and the approximate posterior $q_{\\phi}$, and $\\beta_{pred}$ and $\\beta_{KL}$ are scalar weighting values. Given an intent label $y_t$, the portion of loss $\\mathcal{L}_{pred}$ for the intent predictor is taken as the negative log-likelihood of the label under p.\nThe world model training is alternated with training for the behavior model governing actions $a_t$, with the behavior model learned via an actor-critic policy training over the estimates $s_t$, $\\hat{y}_t$,\nActor: $a_t^* \\sim \\pi_{\\phi}(a_t | s_t)$ Critic: $v_{\\psi}(s_t) \\approx E_{p,\\pi} [\\sum_{t} \\gamma^t r_t]$ (4)\nThe critic is trained to minimize the temporal-difference loss on the value function v. The actor attempts to maximize the critic-predicted value. For further details, we refer the reader to [65].\nTo learn alignment with human drivers, we expose the assistive agent to different humans at training time. Training with human partners in a fictitious co-play setting [27] allows the assistant to become robust to different possible human behaviors, but does not teach the assistant to distinguish between the discrete modes of human behavior, which is necessary for assistance in our driving task.\nSimilar to recent work describing human preferences for a task using reward shaping [66, 67], we generate a population of humans, but different from these works, we group labeled sub-populations according to certain inherent objectives or preferences. We train several sub-populations using a separate human reward $r_t^H = r_{task} + r_y^H$, which is composed of the base task rewards in (1), with the addition of several distinct human objectives, spanning different multimodal behaviors of the human. These are then formed into a collection of humans $\\{\\pi_{1}^{H}, \\pi_{2}^{H}, ..., \\pi_{N}^{H}\\} \\in \\{\\Pi_{y_{1}},...,\\Pi_{y_{N}}\\}$ for N different objectives. Examples of different settings for $r_y^H$ are given in Appendix C.\nIf trained to optimality, such rewards induce optimal behaviors for each human objective $\\{\\pi_{1}^{*}, \\pi_{2}^{*}, ..., \\pi_{N}^{*}\\}$, which we can consider as the collection of expert policies of each enumerated objective. We extend this notion to that of intent at runtime. At any given moment, the human may adopt an intent y to abide by policy $\\pi_y^{H}$. To simplify training, we consider drivers and rollouts with fixed intents, y, though our problem setup generalizes to variable-intent scenarios (e.g., a human switching from \u201cfollowing\u201d to \u201cpassing\u201d during a race).\nH\nHumans are error-prone and pursue their objectives irrationally [68]. To capture this, we partially train human partners to pair with the assistive agent in a process known as fictitious co-play (FCP) [27], yielding a population of humans $\\{\\pi_{k}\\}k \\cup \\{\\pi_{y}^{*H}\\}$ for each intent sub-population I, where k represents the policy checkpoint for some amount of completed training. Using the FCP framework, each checkpoint k \u2208 [1, K] is an agent trained to k% completion and the final checkpoint k = K is an optimal policy. Each (sub)optimal human is endowed with a (sub)optimal RSSM world model. While we apply FCP in this work, we note that our approach is agnostic to how human behaviors are generated and future work may consider other approaches to synthesizing human policies [69, 70]."}, {"title": "3.3 Assistive Agent Objectives", "content": "For continuous actions $a_t^H, a_t^A \\in R^m$ for the human and assistant, respectively, we encourage the assistive agent to minimize its action intervention by adopting a reward of the form\n$r_{interv} = - ||a_t^A||_2^2$\nwhich penalizes the magnitude of the intervention according to an $L_2$-norm. Aggregated over time, the intervention cost forms a mixed-norm $L_1 - L_2$ [71], encouraging time sparsity of interventions."}, {"title": "4 Experiments and Results", "content": "We examine the performance of our approach, dubbed DREAM2ASSIST, on different racing tasks with a fictitious human driver. We examine two racing settings, each derived from portions of a two-mile race track, and implemented in the CARLA Simulator [72] using a rear-wheel race vehicle physics model. For each task, opponents are randomly instantiated as replays of real human trajectories from the track, meaning that they do not react to the ego vehicle. In each setting, we run two sets of experiments-one with pass vs. stay fictitious human partners (i.e., the human is trying to overtake or stay-behind their opponent), and one with left vs. right partners (i.e., the human wants to stay on the left or right side of their opponent). We also further examine out-of-distribution intent inference (with intent changing over time), and further ablations in Appendix F and G.\nIn each experiment, we train a population of humans following each objective (e.g., \"left\" or \"right\"), and then train a DREAM2ASSIST agent over the combined population. We then evaluate the degree to which the assistive agent can improve fictitious human performance on the track, where performance is measured by total progress, average speed, and no collisions. To measure the contributions of each assistive agent for the fictitious human population, we sample checkpoints at every 20% performance increment for agents up to at least 75% of maximum, or from the bottom five performers if none are under this threshold. We report the mean change in performance (track progress and collisions) when an assistive agent is deployed, as well as the return under each human objective in Table 1. Means and standard deviations are computed over all five sub-optimal fictitious drivers.\nThe two settings we consider include Straightaway Driving and Hairpin Driving. The straightaway is a flat 370-meter portion of a track with a concrete barrier on the right-hand side. Because the"}, {"title": "Human Decision Characteristics", "content": "Based on observed behaviors from a human study (see Ap- pendix B), we propose two different sets of human characteristics (ground truth intent): pass vs. stay and left vs. right. We train fictitious human agents to satisfy each of these objectives. For the pass vs. stay behaviors, the fictitious human agents are trained to either overtake or stay-behind the opponent vehicle. For the left vs. right behaviors, the fictitious humans are trained with a preference to stay on either the left or right side of the opponent for as much of the race as possible. Attempting to provide the wrong type of assistance with such distinct behaviors (e.g., offering \"right\" assistance to a \"left\" human) will result in fighting between the assistant and human, and will likely lead to collisions or spin-outs. We provide further details on the rewards for each agent in Appendix C."}, {"title": "Action and Observation Spaces", "content": "The environment itself consists of the rewards in Sec. 3.1 and an observation and action space for each agent. The human and assistive agent's actions both adopt steering and acceleration values in the range of [-1,1]. The assistant and human actions are summed together and clipped on [-1,1] before being passed to the vehicle. The observation space contains the ego vehicle state (position, velocity, tire slip angles, yaw rate, heading), current distance traveled, and an array of forward-looking track edge points. Finally, the human and assistive agents are each able to observe the other's actions."}, {"title": "Baselines", "content": "The claim of our work is that an assistive agent will better support a human partner if the assistant can infer the human's objective and help to satisfy that objective. To test this claim, we compare DREAM2ASSIST against a baseline version of DREAMER, which makes no intent inference and is not rewarded according to a human objective. We also compare to a non-RSSM baseline, GAIL [73], obtained with the BeTAIL [74] framework. BeTAIL uses a behavior transformer as the human policy, coupled with an assistive agent trained via adversarial imitation learning to correct for distribution shift. Our DREAMER-AIL baseline challenges whether our RSSM approach is necessary at all, or whether a purely data-driven approach using behavior cloning and inverse reinforcement learning could satisfy the multimodal behavioral assistance task that we consider. For this baseline, a fictitious human using a DREAMER policy is paired with the AIL assistive agent from BeTAIL."}, {"title": "4.1 Straightaway Results", "content": "We observe a consistent trend for all straightaway results \u2013 DREAMER offers very low-magnitude intervention, leading to higher performance with expert drivers but poor performance with novice drivers (as the assistant is not helping). We show track progress and human-objective return for assistants deployed to the pass vs. stay problem in Fig. E.1 (bottom). We report additional results in Appendix E. Conversely, DREAM2ASSIST offers higher-magnitude intervention, leading to higher performance with novice drivers, but lower performance gains with expert drivers (as the assistant is not needed). As shown in Table 1, DREAM2ASSIST generally offers the highest performance over other methods. DREAMER-AIL suffers from mode collapse, trying to turn all drivers into either a \"stay\" driver or getting caught between \"left\" and \"right\" and therefore not moving. This behavior means that the DREAMER-AIL agent consistently underperforms a fictitious human with no assistance at all, as the AIL assistance keeps the driver far behind the opponent."}, {"title": "4.2 Hairpin Results", "content": "The hairpin domain is more challenging and yields fictitious humans that are not always consistently able to solve the task, thereby leaving greater scope for assistance from our trained agents. In our pass vs. stay experiment, DREAM2ASSIST significantly improves the drivers' abilities to solve the task while still satisfying the human objectives and not leading to an increase in collisions. Similarly, in the left vs. right experiment, DREAM2ASSIST leads to significant increases in track progress, reduction in collisions, and improvements in human-objective alignment. Baseline approaches fail to disentangle the \"left\" and \"right\" modes of driving. The DREAMER baseline instead opts to push the driver to stay behind the opponent vehicle in an effort to reduce collisions, thereby making it farther down the track but failing to overtake. The DREAMER-AIL baseline drives aggressively off the track, leading to a significant drop in track progress, collisions, and human-objective alignment. An illustrative example of DREAM2ASSIST is in Fig. 3, and videos are at https://youtu.be/PVugoxqX5Co."}, {"title": "5 Conclusions and Limitations", "content": "We introduce an assistive paradigm, DREAM2ASSIST, that learns to interact with humans to help them perform more optimally while supporting their personal objectives for the task. We evaluate DREAM2ASSIST in a dynamic and challenging task of high-speed racing, and we show that our approach is able to disentangle and accommodate distinct human objectives more effectively than baseline methods. We show that DREAM2ASSIST results in higher human-robot team performance than baseline methods, suggesting that explicit intent-conditioning and reward-inference can provide crucial performance gains in settings with multimodal, mutually-exclusive, human objectives.\nWhile DREAM2ASSIST represents a state-of-the-art improvement in human-robot teaming, there are limitations in our work that we hope to address in future work. First, our approach has been tested with fictitious humans, but we have not yet evaluated generalization to real human-robot teams. In future work, we intend to deploy DREAM2ASSIST in a human-subjects experiment to test how effectively our framework generalizes to real, sub-optimal human drivers. DREAM2ASSIST also relies on privileged access to the inferred reward values from the RSSM of an optimal policy; future work may consider how to estimate optimal policy rewards without such a model. Finally, future work may consider how to provide assistance via multiple modalities (e.g., providing sparse language guidance on when to overtake vs. dense control-level assistance during the overtake)."}, {"title": "A MBRL Preliminaries", "content": "We frame the model-based reinforcement learning (MBRL) problem as a two-player (human ego driver, assistive agent ego driver) partially-observable Markov decision process (POMDP) defined by the tuple $M = (X^{\\Re}, A^{k},T^{k},R^{k}, \\gamma)_{k=H,A}$, where, for agent \u043a \u2208 {H, A} (for the human, and AI agent, respectively), $X^{\\Re}$ denotes the imagined states of the world, $A^{k}$ denotes the agent's (continuous or discrete) actions, $T^{k} : X^{\\Re} \\times A^{k} \\rightarrow [0, 1]$ is the transition probability, $R^{k} : X^{\\Re} \\times A^{H} \\times A^{k} \\rightarrow \\Re$ is a reward function, and y \u2208 [0, 1] is a discount factor. We aim to train both agents such that they maximize their expected returns $R = E \\sum_{t=1}^{T} \\gamma^{t}r_{t}^{k}$.\nCrucially, in the semi-cooperative shared control setting, each reward $r_t^k$ is factored into sub- components, with both sharing the same task (driving) rewards, but where $r_t^H$ contains an additional term for a human's objective, and $r_t^A$ contains additional terms to weaken its contribution in relation to the human's and enforces alignment to the human."}, {"title": "B Human Subject Data Collection", "content": "We briefly discuss a study conducted for gathering human subject behavior data in the racing domain we use in the paper. The purpose of the study was to gather qualitative and statistical data on individuals' behavior and objectives in a racing context, and to use that to inform what criteria are important for building models of human objectives. We recruited 48 participants to drive a simulator with the hairpin and straightaway segments of the two-mile track, the same domains for the computational results in this paper. The scenarios were chosen so as to present overtake opportunities in portions of the track of varying levels of difficulty, while keeping the overall task short enough to ensure there is a rich interaction between the ego and opponent. Participants completed a series of warm-up trials in each domain, with three trials devoted to the straightaway segment and eight trials in the hairpin segment, each featuring different opponents of varying difficulty (fixed trajectories) to race against. Again, these were the same trajectories used in our domains.\nAt the conclusion of each trial, participants answered the question: \u201cDid you attempt to pass the other vehicle?\" on an iPad. We also gathered, from trajectory data, whether or not the participant actually completed an overtake without collisions or spin-outs. These results are reported in Table B.1. We conclude that even in a simulated setting, there were a lower number of actual overtakes that occurred than were attempted. This suggests that there is room to assist those wishing to overtake, but unable to do so."}, {"title": "C Human Objectives", "content": "In this section, we discuss the reward terms used to generate the explicit decision-making tendencies of the fictitious human drivers, via $r_t^H = r_{task} + r_y^H$. Each use the task-specific reward terms outlined"}]}