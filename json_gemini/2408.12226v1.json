{"title": "Eval Yaks: Instruction Tuning Datasets and\nLORA Fine-tuned Models for Automated Scoring\nof CEFR B2 Speaking Assessment Transcripts", "authors": ["Nicy Scaria", "Silvester John Joseph Kennedy", "Thomas Latinovich", "Deepak Subramani"], "abstract": "Relying on human experts to evaluate CEFR speaking assessments in an e-\nlearning environment creates scalability challenges, as it limits how quickly and\nwidely assessments can be conducted. We aim to automate the evaluation of\nCEFR B2 English speaking assessments in e-learning environments from con-\nversation transcripts. First, we evaluate the capability of leading open source\nand commercial Large Language Models (LLMs) to score a candidate's perfor-\nmance across various criteria in the CEFR B2 speaking exam in both global and\nIndia-specific contexts. Next, we create a new expert-validated, CEFR-aligned\nsynthetic conversational dataset with transcripts that are rated at different assess-\nment scores. In addition, new instruction-tuned datasets are developed from the\nEnglish Vocabulary Profile (up to CEFR B2 level) and the CEFR-SP Wiki-\nAuto datasets. Finally, using these new datasets, we perform parameter efficient\ninstruction tuning of Mistral Instruct 7B v0.2 to develop a family of models called\nEval Yaks. Four models in this family are for assessing the four sections of the\nCEFR B2 speaking exam, one for identifying the CEFR level of vocabulary and\ngenerating level-specific vocabulary, and another for detecting the CEFR level\nof text and generating level-specific text. Eval Yaks achieved an average accept-\nable accuracy of 96%, a degree of variation of 0.35 levels, and performed 3 times\nbetter than the next best model. This demonstrates that a 7B parameter LLM\ninstruction tuned with high-quality CEFR-aligned assessment data can effectively\nevaluate and score CEFR B2 English speaking assessments, offering a promising\nsolution for scalable, automated language proficiency evaluation.", "sections": [{"title": "1 Introduction", "content": "The study of English is classified into Academic and Functional (General) English,\neach fulfilling different use cases. Academic English, prevalent in professional and\neducational spheres such as universities, prioritizes a formal tone, organized writing,\nand precise vocabulary for tasks such as essays, reports, and scholarly communication.\nIn contrast, Functional English targets everyday communication skills in speaking,\nlistening, reading, and writing, aiming for practical application in social and personal\ninteractions with a more informal, conversational approach [1].\nThe Common European Framework of Reference for Languages (CEFR) evalu-\nates English proficiency on a six-level scale from A1 (Beginner) to C2 (Advanced).\nA CEFR B2 qualification indicates that the learner has the ability to independently\nuse English to live, work, or study [2]. The CEFR uses \u2018can do\u2019descriptors to tai-\nlor teaching and assessment, aligning curriculum and educational objectives. These\ndescriptors help educators set communicative goals and adapt courses to specific learn-\ning needs through consultations with experts and stakeholders [3]. Learners often have\na 'spiky profile', excelling in some language skills but struggling in others, reflect-\ning their Target Language Use (TLU). For example, the IELTS Life Skills Test [4]\nassesses only speaking and listening skills for UK visa applicants. The CEFR frame-\nwork requires adaptation to fit specific contexts and is not a universal solution. Its 'can\ndo' statements define TLUs such as Personal, Public, Occupational, and Educational,\nfacilitating customized teaching and assessment [5]. E-learning can create a compre-\nhensive curriculum customized to individual preferences and abilities, effectively using\nthe CEFR framework to meet specific needs of the learner and contextual demands.\nVarious organizations, including Cambridge English and the British Council, offer\nEnglish language programs and assessments. Cambridge English conducts in-person\nexams in three categories: Schools, General and Higher Education, and Business. The\nGeneral and Higher Education exams cater to career and academic requirements at\nfive levels: A2 Key, B1 Preliminary, B2 First, C1 Advanced, and C2 Proficiency. The\nB2 First exam is crucial for showcasing communication skills in English-speaking envi-\nronments. On the other hand, the British Council's EnglishScore [6] provides a more\nstraightforward, mobile-based online assessment for general English users.\nOur objective is to develop a range of models to automate the evaluation and\nscoring process with the ability to handle the complexities of advanced language exam-\ninations such as the B2 First. Using both international and India-specific data, we\nstrive to improve the accuracy and relevance of assessments, particularly for Indian\nstudents, in both global and local contexts.\nE-learning is uniquely positioned to address the preferences and capabilities of\nlearners, particularly as the complexity of subjects increases [7]. It is especially ben-\neficial for subjects that require in-depth knowledge and detailed responses. In this\ncontext, depending on human experts to evaluate every assessment can be extremely"}, {"title": "2 Cambridge's B2 First Speaking Assessment\nFramework", "content": "We created Eval Yaks utilizing the Cambridge framework for the B2 First Exam. This\nspeaking assessment is a component of the comprehensive exam that evaluates all\naspects of language skills, including listening, reading, writing, and speaking. The\nprimary objective of this exam is to assess the English language proficiency of the\ncandidate at the CEFR B2 level."}, {"title": "2.1 Exam format", "content": "The speaking test involves two candidates and two examiners. The first examiner acts\nas both an interlocutor and an assessor, facilitating the dialogue with the candidates.\nThe second examiner serves as an assessor, monitoring the interaction.\nThe speaking test is divided into four sections [22]. In the first section, the inter-\nlocutor individually assesses each candidate's social and interactional language skills.\nIn the second section, each candidate speaks at length by comparing two photographs,\nfollowed by a brief reply from the second candidate. This section evaluates spoken\nproduction, assessing the candidate's ability to organize and express their thoughts\ncoherently. In the third section, the candidates converse based on written prompts and\ninstructions, followed by a collaborative decision-making task. This section assesses\ntheir pragmatic and strategic competencies as they exchange ideas, justify opinions,\nand use negotiation strategies. In the final section, the candidates engage in a discus-\nsion about the topics presented in the third section. This part assesses their capability\nto delve into the subjects and substantiate their viewpoints."}, {"title": "2.2 Candidate Assessment", "content": "The assessor and the interlocutor assess the individual performances of the candi-\ndates based on the performance descriptors of the assessment scales [24]. In the actual\nassessment, the assessor does not participate in the interaction and scores the criteria\nbased on the descriptors given in Table 1. There are four criteria for assessment, viz.,\n'grammar and vocabulary', 'discourse management', 'pronunciation', and 'interactive\ncommunication'. The grammar part of the 'grammar and vocabulary' criteria assesses\nthe candidate's ability to apply grammatical rules and construct sentences utilizing"}, {"title": "3 EvalYaks Development Approach", "content": "Our methodology for developing Eval Yaks, an automated system to assess English\nspeaking proficiency of human test takers at the CEFR B2 level, consisted of four\nkey stages. First, we generated simulated candidate conversation records, hereinafter\nreferred to simply as conversation records that replicate realistic conversations corre-\nsponding to different parts of the assessment using the GPT-4 Turbo model (Jan'24).\nThis data set contains conversations that mimicked actual assessment interactions\nbetween a candidate and an interlocutor or between a candidate, an interlocutor, and\na partner, as required by different parts of the assessment, along with scores for differ-\nent assessment criteria. The data were then carefully validated and aligned by experts.\nWe considered three assessment criteria: 'grammar and vocabulary', 'discourse man-\nagement', and \u2018interactive communication' (detailed in the rubric given in Table 1).\nDetailed information on this process can be found in Section 4.1. Next, we examine\nthe effectiveness of off-the-shelf cutting-edge language models in evaluating these con-\nversation records by leveraging their built-in abilities. For this assessment, we used\ntwo types of prompts: one that included contextual data and another that did not.\nThis stage informed us that instruction fine-tuning of models is necessary to achieve\nacceptable automated evaluation systems. The third stage involved the preparation of\nspecialized instruction datasets to fine-tune a base language model to evaluate con-\nversation records. For this, we created many more conversational records for various\nscenarios with the corresponding scores to train the model. Each conversation record\nwas rigorously validated by experts. We used these conversation records and resources\nsuch as the English Vocabulary Profile (up to CEFR B2 level; [25]) and the CEFR-\nSP WikiAuto dataset [26] by adding specific instructions, making them suitable for\ninstruction-tuning. See Section 4.2 for more details. Finally, these validated data sets\nwere utilized to create a suite of six models, one model each for detecting and generat-\ning CEFR B2 level vocabulary (EvalYaks Vocab), and CEFR B2 sentence structures\n(Eval Yaks CEFR), and four models (EvalYaks Parts 1-4) for automated evaluation of\nthe performance of a candidate in the different parts of the CEFR B2 First English\nspeaking assessment. This structured multistage approach ensures that our system\nis effective and reliable. Although we use the Cambridge CEFR B2 First exam as a\nreference, this approach can be extended to other English Qualifications."}, {"title": "4 Dataset Generation and Evaluation Metrics", "content": "Educational datasets frequently contain sensitive personal information, and for partic-\nular uses such as the CEFR B2 First English speaking evaluation, publicly available\ndata are absent in the Indian context. Fortunately, synthetic data can be created\nusing contemporary LLMs. We used OpenAI's GPT 4 Turbo (Jan '24) [19] to gener-\nate the synthetic dataset. This data set contains conversation records of the CEFR\nB2 English speaking assessment and the corresponding evaluations. The generation\nprocess is described below."}, {"title": "4.1 CEFR B2 English speaking assessment conversation\nrecords generation and validation", "content": "We required a data set of the CEFR B2 English speaking assessment for Indian test\ntakers. We developed prompts to generate the conversation records and employed the\nservices of subject matter experts to validate and align the scores in the conversation\nrecords."}, {"title": "4.1.1 Prompt for synthetic data generation", "content": "A prompt acts as a directive for a language model to generate text that is not only\npertinent, but also varied, realistic, and aligned with the target application [27]. We\nneeded to create conversations that would replicate various sections of the CEFR B2\nFirst English speaking exam in an Indian and global context, along with scores for the\nthree assessment criteria. This required the creation of a comprehensive and detailed\nprompt that included a wide range of information. The prompts for the four sections\nof the assessment were different since each part emphasizes different speaking skills,\nand the interactions differ significantly between the four parts.\nEach prompt began with an overview of the particular part of the assessment. Fol-\nlowing this, the model was instructed to produce conversations that corresponded to\nspecific scores for each assessment criterion. For example, the output included assign-\ning a score of 1 to both 'grammar and vocabulary' and 'discourse management'. These\ndirections used the Chain-of-Thought (CoT) prompting technique [28], which guided\nthe model through the process of generating conversations corresponding to specific\nscores. After these directions, the output of the model consisting of conversations and\nscores was requested in JSON format. JSON's lightweight nature and ease of parsing\nmake it particularly advantageous for production servers of LLMs, enhancing data\ninterchange efficiency and reducing latency in API communications.\n tailored according to the unique objectives of each part of the CEFR\nB2 English speaking assessment.\nIn addition, the prompt also contained context data that incorporate vocabulary\nup to the CEFR B2 level, assessment scales detailing the scoring for each criterion,\nand expectations were provided. Creating an automated evaluation system customized\nfor the Indian context necessitates the model's ability to handle a wide range of\nIndia-specific information. Thus, India-specific context details such as Indian names,\nlocations, festivals, professions, and common hobbies throughout the country were\nincluded. This information was used to craft conversations that blended Indian and\nglobal contexts. To ensure realistic data generation, a few-shot example strategy [29]\nwas incorporated into the prompt utilizing example conversations from Cambridge's\nB2 First exam.\nused to create the simulated CEFR B2 English speaking assessment conversation\nrecords."}, {"title": "4.1.2 CEFR B2 English speaking assessment conversation records\nfor parts 1-4", "content": "In parts 1, 2, and 4 of the CEFR B2 English speaking assessment, candidates have\nsequential dialogues with the interlocutor. Parts 1 and 2 collect only scores for 'gram-\nmar and vocabulary' and 'discourse management'\nPart 4 provides an opportunity for the candi-\ndates to further explore the topics of Part 3 with the interlocutor. For parts 3 and 4,"}, {"title": "4.1.3 Data validation and alignment", "content": "Given the intrinsic likelihood of errors and inconsistencies in text produced by large\nlanguage models [30], it is crucial to have subject matter experts validate the CEFR\nB2 English speaking assessment conversation and their corresponding scores in the\nconversation records. We carried out an alignment exercise to compare synthetically\ngenerated data with CEFR levels, enlisting human experts. Three English as a foreign\nlanguage teachers, each with over four years of experience, assessed the quality of the\ngenerated data.\nEach conversation record was subjected to separate evaluations by two out of\nthree experts, allowing a dual perspective on its compliance with predetermined scores\nand assessment goals. The evaluations adhered to the criteria specified in Table 1,\nwhich outlines the expected outcomes for each score within those criteria. Each expert\nverified how well the conversation corresponded to the scores given for each criterion.\nWhen the conversation and the scores for each criterion aligned, the experts marked\nconversation record as correctly matched. In cases where discrepancies were identified,\nexperts recorded the specific criteria where mismatches occurred and detailed the\nnature of these misalignments. Additionally, they suggested necessary modifications"}, {"title": "4.2 Instruction dataset creation for fine-tuning EvalYaks", "content": "EvalYaks was trained utilizing the aligned conversation records to instruction tune\na base language model. In instruction tuning, input-output pairs are enhanced with\nexplicit instructions [31-33]. For our training process, we manually prepared three\nsets of instructions with varied information [34, 35] and also incorporated paraphrased\nversions of these instructions to generate additional training samples. The content of\neach instruction includes: (i) The LLM's designated role as an evaluator of the CEFR\nB2 English speaking assessment and the detailed evaluation steps; (ii) The LLM's\ndesignated role as an evaluator of the CEFR B2 English speaking assessment, the\ndetailed evaluation steps, and performance descriptors; and (iii) Evaluation steps and\nan example of output format.\nWe merged these instructions with the conversation records, incorporating special\ntokens to decipher various elements of the instruction, input, and output, to create\ninstruction data points. point templates employed for training"}, {"title": "4.3 Performance metrics", "content": "To assess the performance of EvalYaks suite of models and other standard LLMs to\nperform automated CEFR B2 English speaking assessment evaluation, we classified"}, {"title": "5 Training EvalYaks", "content": "Each of the six models that are part of the EvalYaks suite was trained separately with\nmistral-7b-instruct-v0.2 as the base model on which Low Rank Adaptation (LoRA)\n[36] was used for parameter and memory efficient instruction fine-tuning. By intro-\nducing two smaller matrices for weight updates via low-rank decomposition, LORA\nenables adaptations to new data without altering the original weight matrix, which\nremains unchanged. These LoRA adapters can be merged with the base model during\ninference. During training, individual adapters were developed for each dataset, such\nas one adapter specifically for automated evaluation and scoring of CEFR B2 speak-\ning assessment Part 1, another for part 2, and so on. We experimented with multiple\ndataset combinations, including aggregating parts 1 and 2, parts 3 and 4, all four parts\ntogether, and all parts in conjunction with the English Vocabulary Profile and CEFR-\nSP WikiAuto data sets. Upon evaluation, it was observed that the models equipped\nwith dedicated adapters for each individual data set exhibited superior performance.\nWe experimented with combinations of hyper-parameters that control the rank of\nthe update matrices r and the LoRA scaling factor a, such as (r, a) = (64, 16), (256,\n128), and (256, 512). The best adapters were obtained with r = 256 and a = 128 with\na lora dropout of 0.1. In our instruction tuning process, we trained for 5 epochs using\na cosine learning rate schedule starting at 2e-4, employing the AdamW optimizer [37]\nwith a weight decay of 0.001. The adapters were trained using bfloat16 precision on\n1 or 2 NVIDIA A100 GPUs (each with 80 GB of VRAM), 3 or 4 RTX A6000 Ada\nGPUs (each with 48 GB of VRAM), or 3 or 4 RTX A6000 GPUs (each with 48 GB\nof VRAM). This training was conducted on servers available through RunPod and on\nlocal servers.\nThus, 'Eval Yaks' refers to a family of six distinct models, each resulting from the\nintegration of an individual adapter with the base model with the best performing\nhyperparameters."}, {"title": "6 Results and Discussion", "content": "We investigated the performance of Eval Yaks and standard LLMs without LORA in\nthe automated assessment of conversation records of the CEFR B2 English speaking\nassessment. The procedure began with the evaluation of LLMs using prompts both\nwithout and with performance descriptors. We conducted an extensive analysis of the\nEval Yaks Vocab and CEFR models, as well as EvalYaks Part 1-4 models.\nThe assessment was conducted by providing the LLMs with two different sets of\nprompts, which included comprehensive instructions and conversations from the test\ndata. The first prompt specified the LLM's role as a CEFR B2 English speaking\nassessment evaluator, detailed the evaluation steps, and included the conversation to\nbe assessed. The second prompt included the performance descriptors from Table 1\naluation"}, {"title": "6.1 Performance Analysis of standard LLMs without LoRA", "content": "We first investigated the intrinsic abilities of leading (as of March 2024) LLMs to\nperform the automated CEFR B2 English speaking evaluation task. We utilized 11\ndifferent LLMs (both proprietary and open-source of different sizes) that are at the\ntop of the LMSYS leaderboard [21].\nThese include Gemma 7B [38], Mistral Instruct\nv0.2 [39], Llama2 7B Chat [40], Vicuna 33B [41], Mixtral Instruct v0.1 [42], Llama2\n70B Chat [40], Qwen 72B Chat [43], GPT 3.5 (Jan '24), Claude Haiku (Mar '24) [44],\nGemini Pro 1.0 [45], and Mistral Medium. These models vary in complexity, ranging\nfrom 7 billion parameters to some claiming to have hundreds of billion parameters."}, {"title": "6.2 Performance analysis of EvalYaks Vocab and CEFR models", "content": "We explored the ability of a relatively small, instruction-tuned language model to\ngrasp the inherent structure of sentences and link vocabulary to CEFR proficiency"}, {"title": "6.3 Performance analysis of EvalYaks Part 1-4 models", "content": "We performed an in-depth analysis to understand whether instruction tuning with\nhigh-quality CEFR-aligned conversational records could improve the performance of\na 7B parameter model compared to other leading LLMs without fine-tuning.\nthe prompts. The EvalYaks models maintain a notably high"}, {"title": "6.3.1 Analysis of EvalYaks Part 1-4 models without performance\ndescriptors", "content": "Figure 5 illustrates the models' performance in terms of the distribution of accurate,\npartly accurate, acceptable, and inaccurate responses in different parts of the evalua-\ntion without performance descriptors. The figure clearly shows that Eval Yaks models\noutperform others in all sections. Eval Yaks part 1-4 models exhibit the highest pro-\nportion of accurate responses, with percentages of 56%, 76%, 48%, and 56% for parts 1\nto 4. In each part of the assessment, the percentage of accurate responses exceeds that\nof partly accurate and acceptable responses, except in part 3, where the percentages\nof accurate and acceptable responses are identical.\nThe analysis revealed that while standard LLMs achieved a reasonable level of\nacceptable responses, their accurate and partly accurate responses were often lower.\nThis consistent performance underscores the effectiveness of targeted instruction tun-\ning in enhancing the accuracy and reliability of automated evaluators for speaking\nassessments."}, {"title": "6.3.2 Analysis of EvalYaks Part 1-4 models with performance\ndescriptors", "content": "Figure 6 demonstrates the model's performance in the distribution of accurate,\npartly accurate, acceptable, and inaccurate responses in various parts of the CEFR\nB2 English speaking exam when using performance descriptors. Notably, EvalYaks\nachieves the highest accuracy rate in all parts of the assessment, with percentages\nfrom part three, Eval Yaks models also show a higher proportion of partly accurate\nresponses. Although Eval Yaks maintain a consistent average acceptable accuracy, their\nperformance is reduced compared to evaluations without performance descriptors."}, {"title": "6.3.3 Degree of variation in EvalYaks part 1-4 models", "content": "The second performance metric used to analyze the models is the degree of variation\n(DOV) in the scores assigned by the model for the input conversation in different parts\nof the assessment. The equations governing DOV in scores are given in Equations 3\nand 4. The DOV obtained for different parts of the assessment when prompted without\nand with the performance descriptors are given in Table 3. In the context of DOV,\na lower value indicates better performance, meaning the model's outputs are closer\nto the expected standards or answers.models perform the best\nin part 2 and perform the their worse in part 3 regardless\nof the prompt type. Interestingly, the performance of the model in part 2 slightly\ndeteriorates when prompts included descriptors, giving a higher overall DOV of 0.36\ncompared to 0.34 in the evaluation using the prompts without descriptors."}, {"title": "7 Conclusion", "content": "In this study, we created EvalYaks, a collection of six models aimed at the automated\nassessment of CEFR B2 English speaking assessment parts 1 through 4, as well as\nCEFR Vocabulary and sentence structures. These models were trained by instruction\nfine-tuning the base Mistral Instruct V0.2 with a high-quality, well-aligned candidate\nconversation records dataset that we developed. The performance of the Automated\nEvaluators was evaluated based on the average acceptable accuracy and Degree of\nvariation (DOV) in the scores.\nEval Yaks surpassed its primary competitor, Gemini Pro 1.0, by threefold in terms\nof accurate score predictions and doubled its effectiveness when performance descrip-\ntors were included. The base model of Eval Yaks, Mistral Instruct V0.2, ranked near\nthe lowest among 11 models in the CEFR B2 English assessment. Eval Yaks consis-\ntently demonstrated strong performance in this assessment, registering a DOV of 0.34\nThis\nwas roughly one-third of the variation seen in Gemini Pro 1.0, which had a DOV of\nrange in performance."}, {"title": "Appendix A Data generation prompts", "content": "The prompt used with GPT 4 Turbo (Jan'24) to generate synthetic conversations and\nassessments is given below.\nIn part one, candidates engage in a one-on-one conversation with the \u201cExaminer\u201d\ncovering personal details, routines, preferences, etc. This segment aims to evaluate\ntheir spontaneous communication skills in everyday situations. \u201cCandidate\" should\nrelax as the conversation starts, focusing on listening attentively and providing detailed\nanswers, avoiding mere yes or no responses. While they should elaborate with reasons\nand examples, overly lengthy responses are not necessary at this stage."}, {"title": "Data generation instructions:", "content": "\u2022 I want to create a conversational dataset for several combinations of scores in\nGRAMMAR AND VOCABULARY and DISCOURSE MANAGEMENT with the\nprovided context. I need the output in the JSON format as given in the 'Data\nFormat'.\n\u2022 I first want to start with BAND 1 in GRAMMAR AND VOCABULARY & BAND\n1 in DISCOURSE MANAGEMENT.\n\u2022 Here the conversation is between the \"Examiner\" and the \"Candidate\". The con-\nversation lasts for 4 exchanges (4 questions from Examiner and 4 responses from\nthe Candidate). And this must be categorised as \u201cINPUT\u201d in the JSON.\n\u2022 Refer to the 'Typical Questions' and create similar questions for the conversation\n\u2022 Refer to 'Indian Context' to create to generate Indian context specific questions and\nresponses from time to time.\n\u2022 Use the A1, A2, B1 & B2 CEFR vocabulary given in the context."}, {"title": "Appendix B Instruction datapoint template", "content": "The three sets of instruction datapoint for part one of the CEFR B2 English speaking\nassessment are given below."}, {"title": "B.1 Instruction datapoint without performance descriptors", "content": "Role:\n\u2022 You are an assessor of the CEFR B2 English speaking assessment. You are an expert\nin this field with several years of experience.\n\u2022 You will be given a conversation between an Examiner and a Candidate, and your\ntask is to give scores for two metrics for the responses given by the \u201cCandidate\u201d in\nthe conversation."}, {"title": "Evaluation Steps:", "content": "\u2022 Read the conversation between the Examiner and the Candidate carefully.\n\u2022 Assign a score for GRAMMAR AND VOCABULARY and DISCOURSE MAN-\nAGEMENT on a scale of 1 to 5, where 1 is the lowest and 5 is the highest.\n\u2022 Please disregard the response provided by \u201cExaminer\u201d in your evaluation.\n\u2022 Present the assessment criteria and scores in JSON format and name it OUTPUT\nand the OUTPUT will have two key-value pairs: GRAMMAR AND VOCABU-\nLARY, and DISCOURSE MANAGEMENT"}, {"title": "B.2 Instruction datapoint with performance descriptors", "content": "Role:\n\u2022 You are an assessor of the CEFR B2 English speaking assessment. You are an expert\nin this field with several years of experience.\n\u2022 You will be given a conversation between an Examiner and a Candidate, and your\ntask is to give scores for two metrics for the responses given by the \u201cCandidate\u201d in\nthe conversation."}, {"title": "Evaluation Steps:", "content": "\u2022 Read the conversation between the Examiner and the Candidate carefully.\n\u2022 Assign a score for the assessment criteria based on the 'Performance Descriptors'\ngiven on a scale of 1 to 5, where 1 is the lowest and 5 is the highest.\n\u2022 Please disregard the response provided by \u201cExaminer\u201d in your evaluation.\n\u2022 Present the assessment criteria and scores in JSON format and name it OUTPUT\nand the OUTPUT will have two key-value pairs: GRAMMAR AND VOCABU-\nLARY, and DISCOURSE MANAGEMENT"}, {"title": "Performance Descriptors", "content": "\u2022 GRAMMAR AND VOCABULARY: {performance descriptor grammar and vocab-\nulary}\n\u2022 DISCOURSE MANAGEMENT: {performance descriptor for discourse manage-\nment}"}, {"title": "B.3 Instruction datapoint with evaluations steps and output\nformat", "content": "Evaluation Steps:\n\u2022 Read the conversation attentively, focusing on the interaction.\n\u2022 Rate the Candidate's use of GRAMMAR AND VOCABULARY on a scale from 1\nto 5.\n\u2022 Rate the Candidate's ability in DISCOURSE MANAGEMENT on the same scale.\n\u2022 Only assess the Candidate's responses, ignoring the Examiner's input.\n\u2022 Present your evaluation scores in a JSON format named OUTPUT, with two\nkey-value pairs: GRAMMAR AND VOCABULARY and DISCOURSE MANAGE-\nMENT."}, {"title": "Appendix C Evaluation prompts", "content": "The two sets of prompts that were used to evaluate the performance of leading LLMs\nwithout LORA and 'EvalYaks' for part 1 are given below."}, {"title": "C.1 Prompt without performance descriptors", "content": "Role:\n\u2022 You are an assessor of the CEFR B2 English speaking assessment. You are an expert\nin this field with several years of experience.\n\u2022 You will be given a conversation between an Examiner and a Candidate, and your\ntask is to give scores for two metrics for the responses given by the \u201cCandidate\u201d in\nthe conversation."}, {"title": "Evaluation Steps:", "content": "\u2022 Read the conversation between the Examiner and the Candidate carefully.\n\u2022 Assign a score for GRAMMAR AND VOCABULARY and DISCOURSE MAN-\nAGEMENT on a scale of 1 to 5, where 1 is the lowest and 5 is the highest.\n\u2022 Please disregard the response provided by \"Examiner\u201d in your evaluation.\n\u2022 Present the assessment criteria and scores in JSON format and name it OUTPUT\nand the OUTPUT will have two key-value pairs: GRAMMAR AND VOCABU-\nLARY, and DISCOURSE MANAGEMENT"}, {"title": "C.2 Prompt with performance descriptors", "content": "Role:\n\u2022 You are an assessor of the CEFR B2 English speaking assessment. You are an expert\nin this field with several years of experience.\n\u2022 You will be given a conversation between an Examiner and a Candidate, and your\ntask is to give scores for two metrics for the responses given by the \u201cCandidate\u201d in\nthe conversation."}, {"title": "Evaluation Steps:", "content": "\u2022 Read the conversation between the Examiner and the Candidate carefully.\n\u2022 Assign a score for the assessment criteria based on the 'Performance Descriptors'\ngiven on a scale of 1 to 5, where 1 is the lowest and 5 is the highest.\n\u2022 Please disregard the response provided by \u201cExaminer\u201d in your evaluation.\n\u2022 Present the assessment criteria and scores in JSON format and name it OUTPUT\nand the OUTPUT will have two key-value pairs: GRAMMAR AND VOCABULARY\nand DISCOURSE MANAGEMENT"}]}