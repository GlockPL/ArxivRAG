{"title": "Eval Yaks: Instruction Tuning Datasets and LoRA Fine-tuned Models for Automated Scoring of CEFR B2 Speaking Assessment Transcripts", "authors": ["Nicy Scaria", "Silvester John Joseph Kennedy", "Thomas Latinovich", "Deepak Subramani"], "abstract": "Relying on human experts to evaluate CEFR speaking assessments in an e-learning environment creates scalability challenges, as it limits how quickly and widely assessments can be conducted. We aim to automate the evaluation of CEFR B2 English speaking assessments in e-learning environments from conversation transcripts. First, we evaluate the capability of leading open source and commercial Large Language Models (LLMs) to score a candidate's performance across various criteria in the CEFR B2 speaking exam in both global and India-specific contexts. Next, we create a new expert-validated, CEFR-aligned synthetic conversational dataset with transcripts that are rated at different assessment scores. In addition, new instruction-tuned datasets are developed from the English Vocabulary Profile (up to CEFR B2 level) and the CEFR-SP Wiki-Auto datasets. Finally, using these new datasets, we perform parameter efficient instruction tuning of Mistral Instruct 7B v0.2 to develop a family of models called Eval Yaks. Four models in this family are for assessing the four sections of the CEFR B2 speaking exam, one for identifying the CEFR level of vocabulary and generating level-specific vocabulary, and another for detecting the CEFR level of text and generating level-specific text. Eval Yaks achieved an average acceptable accuracy of 96%, a degree of variation of 0.35 levels, and performed 3 times better than the next best model. This demonstrates that a 7B parameter LLM instruction tuned with high-quality CEFR-aligned assessment data can effectively evaluate and score CEFR B2 English speaking assessments, offering a promising solution for scalable, automated language proficiency evaluation.", "sections": [{"title": "1 Introduction", "content": "The study of English is classified into Academic and Functional (General) English, each fulfilling different use cases. Academic English, prevalent in professional and educational spheres such as universities, prioritizes a formal tone, organized writing, and precise vocabulary for tasks such as essays, reports, and scholarly communication. In contrast, Functional English targets everyday communication skills in speaking, listening, reading, and writing, aiming for practical application in social and personal interactions with a more informal, conversational approach [1].\nThe Common European Framework of Reference for Languages (CEFR) evalu-ates English proficiency on a six-level scale from A1 (Beginner) to C2 (Advanced). A CEFR B2 qualification indicates that the learner has the ability to independently use English to live, work, or study [2]. The CEFR uses \u2018can do\u2019descriptors to tai-lor teaching and assessment, aligning curriculum and educational objectives. These descriptors help educators set communicative goals and adapt courses to specific learn-ing needs through consultations with experts and stakeholders [3]. Learners often have a 'spiky profile', excelling in some language skills but struggling in others, reflect-ing their Target Language Use (TLU). For example, the IELTS Life Skills Test [4] assesses only speaking and listening skills for UK visa applicants. The CEFR frame-work requires adaptation to fit specific contexts and is not a universal solution. Its 'can do' statements define TLUs such as Personal, Public, Occupational, and Educational, facilitating customized teaching and assessment [5]. E-learning can create a compre-hensive curriculum customized to individual preferences and abilities, effectively using the CEFR framework to meet specific needs of the learner and contextual demands.\nVarious organizations, including Cambridge English and the British Council, offer English language programs and assessments. Cambridge English conducts in-person exams in three categories: Schools, General and Higher Education, and Business. The General and Higher Education exams cater to career and academic requirements at five levels: A2 Key, B1 Preliminary, B2 First, C1 Advanced, and C2 Proficiency. The B2 First exam is crucial for showcasing communication skills in English-speaking envi-ronments. On the other hand, the British Council's EnglishScore [6] provides a more straightforward, mobile-based online assessment for general English users.\nOur objective is to develop a range of models to automate the evaluation and scoring process with the ability to handle the complexities of advanced language exam-inations such as the B2 First. Using both international and India-specific data, we strive to improve the accuracy and relevance of assessments, particularly for Indian students, in both global and local contexts.\nE-learning is uniquely positioned to address the preferences and capabilities of learners, particularly as the complexity of subjects increases [7]. It is especially ben-eficial for subjects that require in-depth knowledge and detailed responses. In this context, depending on human experts to evaluate every assessment can be extremely"}, {"title": "", "content": "costly and does not scale efficiently [8]. The adoption of technologies that can emulate the expertise of human assessors and automate the evaluation process offers a feasible solution [9]. Implementing technologies that replicate human expertise and automate evaluations could provide a solution, facilitating effective and unbiased teaching and assessment of complex topics, thereby making quality education more scalable and accessible.\nArtificial intelligence (AI), including Generative AI (GenAI) technology, has made significant advancements in various domains, including education, healthcare, and sci-entific research. GenAI models [10] can produce diverse content such as text, images, and videos using AI techniques. The integration of AI in education is increasing, albeit at a slower pace compared to other industries, yet its potential influence on educa-tion is substantial [11]. These technologies enable personalized educational materials, assessments, and tutoring [12]. Research [13] has indicated that incorporating AI into educational settings has the potential to enhance students' autonomy in managing their own learning processes. Conversational AI, which conducts human-like conver-sations via text or audio based on LLMs, is widely accepted among students for task-oriented dialogues [12]. Tools like Google Assistant have improved EFL students' communication skills and attitudes towards intelligent assistants in learning [14]. Chat-bots providing feedback have effectively boosted vocabulary learning among Korean EFL primary students [15].\nFor a long time, researchers have been developing automated language assessments to efficiently and accurately evaluate the abilities of English learners. SpeechRaterSM Version 1.0 (v1.0) [16] is an automated system created to score the spontaneous speech of English learners, and it is operationally used in the Test of English as a Foreign LanguageTM (TOEFL\u00ae) Practice Online assessment. Students find automated scoring of speaking performance and feedback to be beneficial [17]. Recent advancements include the implementation of transformer-based models like BERT [18] for evalu-ating CEFR levels of sentences, and GPT variants [19] for assessing essays written by L2 English learners [20]. These tools aim to provide unbiased, globally applicable evaluations and focus on aligning content with recognized proficiency levels.\nThe primary aim of this paper is to create an automated system to evaluate a can-didate's performance in the CEFR B2 English speaking test, removing the need for a human assessor and ensuring relevance in both global and Indian contexts. Secondary aims involve developing an automated system capable of identifying vocabulary and proficiency levels. Furthermore, this model should generate CEFR B2 vocabulary and sentences. We introduced EvalYaks, a set of six unique models, with four models dedi-cated to the primary aim and two to the secondary. Eval Yaks is trained by instruction tuning Mistral Instruct 7B v0.2 using the Low Rank Adaptation (LoRA), a parameter efficient fine tuning (PEFT) method.\nDue to the scarcity of CEFR B2 English speaking assessment data specific to India, we generated a data set of simulated candidate responses and their evaluation scores using GPT-4. The data set was then refined with expert human feedback.\nAs a baseline, we first investigated the ability of leading LLMs, which are highly ranked on the LMSYS leaderboard [21], to directly evaluate candidate responses by"}, {"title": "", "content": "leveraging their intrinsic knowledge and prompt engineering. In addition, we exam-ined the ability of LLMs to comprehend sentence structures and link vocabulary with CEFR proficiency levels. Our EvalYaks suite of instruction fine-tuned (LoRA) mod-els demonstrated superior performance in these tasks compared to standard LLMs. In our experiments, for each scenario, we used two sets of evaluation instructions for the LLM: one with contextual information and one without. Standard LLMs are not capable of satisfactory evaluation. Our findings indicate that a 7B parameter LLM, when instruction-tuned with specific, high-quality CEFR-aligned assessment data, can be employed for automated evaluation and scoring of CEFR B2 English speaking assessments."}, {"title": "2 Cambridge's B2 First Speaking Assessment Framework", "content": "We created Eval Yaks utilizing the Cambridge framework for the B2 First Exam. This speaking assessment is a component of the comprehensive exam that evaluates all aspects of language skills, including listening, reading, writing, and speaking. The primary objective of this exam is to assess the English language proficiency of the candidate at the CEFR B2 level."}, {"title": "2.1 Exam format", "content": "The speaking test involves two candidates and two examiners. The first examiner acts as both an interlocutor and an assessor, facilitating the dialogue with the candidates. The second examiner serves as an assessor, monitoring the interaction.\nThe speaking test is divided into four sections [22]. In the first section, the inter-locutor individually assesses each candidate's social and interactional language skills. In the second section, each candidate speaks at length by comparing two photographs, followed by a brief reply from the second candidate. This section evaluates spoken production, assessing the candidate's ability to organize and express their thoughts coherently. In the third section, the candidates converse based on written prompts and instructions, followed by a collaborative decision-making task. This section assesses their pragmatic and strategic competencies as they exchange ideas, justify opinions, and use negotiation strategies. In the final section, the candidates engage in a discus-sion about the topics presented in the third section. This part assesses their capability to delve into the subjects and substantiate their viewpoints."}, {"title": "2.2 Candidate Assessment", "content": "The assessor and the interlocutor assess the individual performances of the candi-dates based on the performance descriptors of the assessment scales [24]. In the actual assessment, the assessor does not participate in the interaction and scores the criteria based on the descriptors given in Table 1. There are four criteria for assessment, viz., 'grammar and vocabulary', 'discourse management', 'pronunciation', and 'interactive communication'. The grammar part of the 'grammar and vocabulary' criteria assesses the candidate's ability to apply grammatical rules and construct sentences utilizing"}, {"title": "3 EvalYaks Development Approach", "content": "Our methodology for developing Eval Yaks, an automated system to assess English speaking proficiency of human test takers at the CEFR B2 level, consisted of four key stages. First, we generated simulated candidate conversation records, hereinafter referred to simply as conversation records that replicate realistic conversations corre-sponding to different parts of the assessment using the GPT-4 Turbo model (Jan'24). This data set contains conversations that mimicked actual assessment interactions between a candidate and an interlocutor or between a candidate, an interlocutor, and a partner, as required by different parts of the assessment, along with scores for differ-ent assessment criteria. The data were then carefully validated and aligned by experts. We considered three assessment criteria: 'grammar and vocabulary', 'discourse man-agement', and \u2018interactive communication' (detailed in the rubric given in Table 1). Detailed information on this process can be found in Section 4.1. Next, we examine the effectiveness of off-the-shelf cutting-edge language models in evaluating these con-versation records by leveraging their built-in abilities. For this assessment, we used two types of prompts: one that included contextual data and another that did not. This stage informed us that instruction fine-tuning of models is necessary to achieve acceptable automated evaluation systems. The third stage involved the preparation of specialized instruction datasets to fine-tune a base language model to evaluate con-versation records. For this, we created many more conversational records for various scenarios with the corresponding scores to train the model. Each conversation record was rigorously validated by experts. We used these conversation records and resources such as the English Vocabulary Profile (up to CEFR B2 level; [25]) and the CEFR-SP WikiAuto dataset [26] by adding specific instructions, making them suitable for instruction-tuning. See Section 4.2 for more details. Finally, these validated data sets were utilized to create a suite of six models, one model each for detecting and generat-ing CEFR B2 level vocabulary (EvalYaks Vocab), and CEFR B2 sentence structures (Eval Yaks CEFR), and four models (EvalYaks Parts 1-4) for automated evaluation of the performance of a candidate in the different parts of the CEFR B2 First English speaking assessment. This structured multistage approach ensures that our system is effective and reliable. Although we use the Cambridge CEFR B2 First exam as a reference, this approach can be extended to other English Qualifications."}, {"title": "4 Dataset Generation and Evaluation Metrics", "content": "Educational datasets frequently contain sensitive personal information, and for partic-ular uses such as the CEFR B2 First English speaking evaluation, publicly available data are absent in the Indian context. Fortunately, synthetic data can be created using contemporary LLMs. We used OpenAI's GPT 4 Turbo (Jan '24) [19] to gener-ate the synthetic dataset. This data set contains conversation records of the CEFR B2 English speaking assessment and the corresponding evaluations. The generation process is described below."}, {"title": "4.1 CEFR B2 English speaking assessment conversation records generation and validation", "content": "We required a data set of the CEFR B2 English speaking assessment for Indian test takers. We developed prompts to generate the conversation records and employed the services of subject matter experts to validate and align the scores in the conversation records."}, {"title": "4.1.1 Prompt for synthetic data generation", "content": "A prompt acts as a directive for a language model to generate text that is not only pertinent, but also varied, realistic, and aligned with the target application [27]. We needed to create conversations that would replicate various sections of the CEFR B2 First English speaking exam in an Indian and global context, along with scores for the three assessment criteria. This required the creation of a comprehensive and detailed prompt that included a wide range of information. The prompts for the four sections of the assessment were different since each part emphasizes different speaking skills, and the interactions differ significantly between the four parts.\nEach prompt began with an overview of the particular part of the assessment. Fol-lowing this, the model was instructed to produce conversations that corresponded to specific scores for each assessment criterion. For example, the output included assign-ing a score of 1 to both 'grammar and vocabulary' and 'discourse management'. These directions used the Chain-of-Thought (CoT) prompting technique [28], which guided the model through the process of generating conversations corresponding to specific scores. After these directions, the output of the model consisting of conversations and scores was requested in JSON format. JSON's lightweight nature and ease of parsing make it particularly advantageous for production servers of LLMs, enhancing data interchange efficiency and reducing latency in API communications. Figure 1 presents an example data set for each section of the assessment. The formats for the input and output data were tailored according to the unique objectives of each part of the CEFR B2 English speaking assessment.\nIn addition, the prompt also contained context data that incorporate vocabulary up to the CEFR B2 level, assessment scales detailing the scoring for each criterion, and expectations were provided. Creating an automated evaluation system customized for the Indian context necessitates the model's ability to handle a wide range of India-specific information. Thus, India-specific context details such as Indian names, locations, festivals, professions, and common hobbies throughout the country were included. This information was used to craft conversations that blended Indian and global contexts. To ensure realistic data generation, a few-shot example strategy [29] was incorporated into the prompt utilizing example conversations from Cambridge's B2 First exam. Furthermore, the prompt contained examples of typical questions [23] asked by the interlocutor during the speaking portion of the exam. A has the prompts used to create the simulated CEFR B2 English speaking assessment conversation records."}, {"title": "4.1.2 CEFR B2 English speaking assessment conversation records for parts 1-4", "content": "In parts 1, 2, and 4 of the CEFR B2 English speaking assessment, candidates have sequential dialogues with the interlocutor. Parts 1 and 2 collect only scores for 'grammar and vocabulary' and 'discourse management', as indicated in the example data shown in Figure 1. In part 3, candidates participate in a discussion among themselves and then interact with the interlocutor. Part 4 provides an opportunity for the candi-dates to further explore the topics of Part 3 with the interlocutor. For parts 3 and 4,"}, {"title": "", "content": "the data include scores for 'grammar and vocabulary', 'discourse management', and 'interactive communication'.\nIn the data generation process, various conversations were formulated that aligned with different combinations of scores for 'grammar and vocabulary', and 'discourse management' for parts 1 and 2. Similarly, for parts 3 and 4, distinct conversations were generated that captured a variety of score combinations for 'grammar and vocabulary', 'discourse management', and 'interactive communication'. The prompt instructions delineated the specific combinations to be generated with each API request to GPT-4 Turbo. The combinations were calibrated such that the score differences between the evaluation criteria within any part did not exceed two points, as differences in excess of two points are extremely rare in actual assessments. It is quite improbable that a candidate would score considerably lower in 'grammar and vocabulary' (e.g., a score of one) while securing much higher scores in 'discourse management' and 'interactive communication' (e.g., scores of four or five).\nWe developed 25 user profiles to participate in the four-part CEFR B2 English speaking assessment for data generation. The profiles were grouped into five profi-ciency levels, each containing five profiles, with an average skill level ranging from 1 to 5. For parts 1 and 2, the groupings were determined by 'grammar and vocabulary' and 'discourse management'. For parts 3 and 4, 'interactive communication' was also considered. The automated evaluation of the conversation records generated from the user profiles performed poorly with standard LLM when the ground truth scores were in the lower range (Section 6). Consequently, we generated more low-scoring conversa-tion records for training EvalYaks. In total, we produced 3,060 data points (671, 667, 927, and 795 for parts 1-4, respectively). The data set for part 3 included a broader range of scenarios due to the addition of another speaker to the conversation, leading to more data points. Likewise, part 4 extended the variety of scenarios beyond those in parts 1 and 2."}, {"title": "4.1.3 Data validation and alignment", "content": "Given the intrinsic likelihood of errors and inconsistencies in text produced by large language models [30], it is crucial to have subject matter experts validate the CEFR B2 English speaking assessment conversation and their corresponding scores in the conversation records. We carried out an alignment exercise to compare synthetically generated data with CEFR levels, enlisting human experts. Three English as a foreign language teachers, each with over four years of experience, assessed the quality of the generated data.\nEach conversation record was subjected to separate evaluations by two out of three experts, allowing a dual perspective on its compliance with predetermined scores and assessment goals. The evaluations adhered to the criteria specified in Table 1, which outlines the expected outcomes for each score within those criteria. Each expert verified how well the conversation corresponded to the scores given for each criterion. When the conversation and the scores for each criterion aligned, the experts marked conversation record as correctly matched. In cases where discrepancies were identified, experts recorded the specific criteria where mismatches occurred and detailed the nature of these misalignments. Additionally, they suggested necessary modifications"}, {"title": "4.2 Instruction dataset creation for fine-tuning EvalYaks", "content": "EvalYaks was trained utilizing the aligned conversation records to instruction tune a base language model. In instruction tuning, input-output pairs are enhanced with explicit instructions [31\u201333]. For our training process, we manually prepared three sets of instructions with varied information [34, 35] and also incorporated paraphrased versions of these instructions to generate additional training samples. The content of each instruction includes: (i) The LLM's designated role as an evaluator of the CEFR B2 English speaking assessment and the detailed evaluation steps; (ii) The LLM's designated role as an evaluator of the CEFR B2 English speaking assessment, the detailed evaluation steps, and performance descriptors; and (iii) Evaluation steps and an example of output format.\nWe merged these instructions with the conversation records, incorporating special tokens to decipher various elements of the instruction, input, and output, to create instruction data points. We generate 1151, 1266, 2843, and 2085 instruction data points for parts 1 through 4 to train Eval Yaks. The three groups of instruction data point templates employed for training are detailed in B.\nFor instruction tuning EvalYaks Vocab and CEFR models, we created instruction data for the English Vocabulary Profile (up to CEFR B2) and CEFR-SP WikiAuto datasets. The English Vocabulary Profile includes words, phrases, idioms, and collo-cations with different annotations of the appropriate CEFR levels from Cambridge. We used two sets of instructions (and paraphrased versions): one to identify and the other to generate words for specific CEFR levels, resulting in 3072 instruction data points from 5107 unique words. CEFR-SP includes 17k English sentences annotated with CEFR levels. From this, we used 7453 data points from the WikiAuto dataset, employing two sets of instructions (similar to the English Vocabulary Profile): identi-fying and generating sentences for specific CEFR levels, resulting in 19,142 instruction data points."}, {"title": "4.3 Performance metrics", "content": "To assess the performance of EvalYaks suite of models and other standard LLMs to perform automated CEFR B2 English speaking assessment evaluation, we classified"}, {"title": "", "content": "the results into four distinct levels: accurate, partly accurate, acceptable, and inac-curate. A response from the model is considered accurate when the scores across all assessment criteria are perfectly aligned with the reference scores. A response is clas-sified as partly accurate if it matches the reference score on at least one assessment criterion. The classification of a response as acceptable involves a more nuanced cri-terion: It is labeled acceptable if the scores deviate by a margin of one, in the same direction from the reference scores, across any or all of the assessment criteria. For example, if the reference scores are (3,3), then the response scores of (2,2), (4,4), (2,3), (3,2), (3,4) and (4,3) are considered acceptable. However, combinations such as (2,4) or (4,2), despite both scores being within the acceptable range, are not considered acceptable due to the inconsistent deviation between the criteria. If any of the above conditions are not satisfied, then the response is considered to be inaccurate, including invalid responses.\nThe acceptable accuracy of the model is calculated using the following formula with $N_{accu}$, $N_{part}$, $N_{acce}$ and $N$ being the number of accurate, partly accurate, acceptable, and total responses generated.\n$$acceptable\\_accuracy = \\frac{N_{accu} + N_{part} + N_{acce}}{N}$$\nAn average acceptable accuracy is defined taking into account all four parts of the CEFR B2 English speaking assessment as\n$$average\\_acceptable\\_accuracy = \\frac{\\sum_{i=1}^{4} acceptable\\_accuracy_i}{4}$$\nwhere the acceptable accuracy for each part is indicated as $acceptable\\_accuracy_i$, with $i$ = 1, 2, 3, 4 referring to the different parts of the assessment.\nIn addition to acceptable accuracy, we also calculated the degree of variation (DOV) in the responses. DOV refers to the average variation of the assessment scores relative to its reference value. DOV in the response for parts one and two is defined as\n$$DOV_1 = DOV_2 = \\frac{\\sum_{i=1}^{N} |GV_{ri} - GV_{ai}| + |DM_{ri} \u2013 DM_{ai}|}{2N}$$\nand for parts three and four is defined as\n$$DOV_3 = DOV_4 = \\frac{\\sum_{i=1}^{N} | GV_{ri} \u2013 GV_{ai} | + | DM_{ri} - DM_{ai} | + | IC_{ri} - IC_{ai} |}{3N}$$\nwhere GV stands for 'grammar and vocabulary', DM for 'discourse management', and IC for 'interactive communication'. The subscripts 'r' and 'a' are used to differentiate between the references and the actual scores. For DOV, smaller is better."}, {"title": "5 Training EvalYaks", "content": "Each of the six models that are part of the EvalYaks suite was trained separately with mistral-7b-instruct-v0.2 as the base model on which Low Rank Adaptation (LoRA) [36] was used for parameter and memory efficient instruction fine-tuning. By intro-ducing two smaller matrices for weight updates via low-rank decomposition, LORA enables adaptations to new data without altering the original weight matrix, which remains unchanged. These LoRA adapters can be merged with the base model during inference. During training, individual adapters were developed for each dataset, such as one adapter specifically for automated evaluation and scoring of CEFR B2 speak-ing assessment Part 1, another for part 2, and so on. We experimented with multiple dataset combinations, including aggregating parts 1 and 2, parts 3 and 4, all four parts together, and all parts in conjunction with the English Vocabulary Profile and CEFR-SP WikiAuto data sets. Upon evaluation, it was observed that the models equipped with dedicated adapters for each individual data set exhibited superior performance.\nWe experimented with combinations of hyper-parameters that control the rank of the update matrices r and the LoRA scaling factor a, such as (r, a) = (64, 16), (256, 128), and (256, 512). The best adapters were obtained with r = 256 and a = 128 with a lora dropout of 0.1. In our instruction tuning process, we trained for 5 epochs using a cosine learning rate schedule starting at 2e-4, employing the AdamW optimizer [37] with a weight decay of 0.001. The adapters were trained using bfloat16 precision on 1 or 2 NVIDIA A100 GPUs (each with 80 GB of VRAM), 3 or 4 RTX A6000 Ada GPUs (each with 48 GB of VRAM), or 3 or 4 RTX A6000 GPUs (each with 48 GB of VRAM). This training was conducted on servers available through RunPod and on local servers.\nThus, 'Eval Yaks' refers to a family of six distinct models, each resulting from the integration of an individual adapter with the best performing hyperparameters."}, {"title": "6 Results and Discussion", "content": "We investigated the performance of Eval Yaks and standard LLMs without LORA in the automated assessment of conversation records of the CEFR B2 English speaking assessment. The procedure began with the evaluation of LLMs using prompts both without and with performance descriptors. We conducted an extensive analysis of the Eval Yaks Vocab and CEFR models, as well as EvalYaks Part 1-4 models.\nThe assessment was conducted by providing the LLMs with two different sets of prompts, which included comprehensive instructions and conversations from the test data. The first prompt specified the LLM's role as a CEFR B2 English speaking assessment evaluator, detailed the evaluation steps, and included the conversation to be assessed. The second prompt included the performance descriptors from Table 1 along with the content from the first prompt. These descriptors served as a reference for the model to rate the assessment criteria. Both sets of prompts used in the evaluation are presented in C."}, {"title": "6.1 Performance Analysis of standard LLMs without LoRA", "content": "We first investigated the intrinsic abilities of leading (as of March 2024) LLMs to perform the automated CEFR B2 English speaking evaluation task. We utilized 11 different LLMs (both proprietary and open-source of different sizes) that are at the top of the LMSYS leaderboard [21]. These include Gemma 7B [38], Mistral Instruct v0.2 [39], Llama2 7B Chat [40], Vicuna 33B [41], Mixtral Instruct v0.1 [42], Llama2 70B Chat [40], Qwen 72B Chat [43], GPT 3.5 (Jan '24), Claude Haiku (Mar '24) [44], Gemini Pro 1.0 [45], and Mistral Medium. These models vary in complexity, ranging from 7 billion parameters to some claiming to have hundreds of billion parameters."}, {"title": "6.2 Performance analysis of EvalYaks Vocab and CEFR models", "content": "We explored the ability of a relatively small, instruction-tuned language model to grasp the inherent structure of sentences and link vocabulary to CEFR proficiency"}, {"title": "6.3 Performance analysis of EvalYaks Part 1-4 models", "content": "We performed an in-depth analysis to understand whether instruction tuning with high-quality CEFR-aligned conversational records could improve the performance of a 7B parameter model compared to other leading LLMs without fine-tuning."}, {"title": "6.3.1 Analysis of EvalYaks Part 1-4 models without performance descriptors", "content": "Figure 5 illustrates the models' performance in terms of the distribution of accurate, partly accurate, acceptable, and inaccurate responses in different parts of the evalua-tion without performance descriptors. The figure clearly shows that Eval Yaks models outperform others in all sections. Eval Yaks part 1-4 models exhibit the highest pro-portion of accurate responses, with percentages of 56%, 76%, 48%, and 56% for parts 1 to 4. In each part of the assessment, the percentage of accurate responses exceeds that of partly accurate and acceptable responses, except in part 3, where the percentages of accurate and acceptable responses are identical.\nThe analysis revealed that while standard LLMs achieved a reasonable level of acceptable responses, their accurate and partly accurate responses were often lower. This consistent performance underscores the effectiveness of targeted instruction tun-ing in enhancing the accuracy and reliability of automated evaluators for speaking assessments."}, {"title": "6.3.2 Analysis of EvalYaks Part 1-4 models with performance descriptors", "content": "Figure 6 demonstrates the model's performance in the distribution of accurate, partly accurate, acceptable, and inaccurate responses in various parts of the CEFR B2 English speaking exam when using performance descriptors. Notably, EvalYaks achieves the highest accuracy rate in all parts of the assessment, with percentages of 40%, 68%, 48%, and 56% for parts one, two, three, and four, respectively. Apart from part three, EvalYaks models also show a higher proportion of partly accurate responses. Although Eval Yaks maintain a consistent average acceptable accuracy, their performance is reduced compared to evaluations without performance descriptors."}, {"title": "6.3.3 Degree of variation in EvalYaks part 1-4 models", "content": "The second performance metric used to analyze the models is the degree of variation (DOV) in the scores assigned by the model for the input conversation in different parts of the assessment. The equations governing DOV in scores are given in Equations 3 and 4. The DOV obtained for different parts of the assessment when prompted without and with the performance descriptors are given in Table 3. In the context of DOV, a lower value indicates better performance, meaning the model's outputs are closer to the expected standards or answers. The EvalYaks models perform much better than the other models with a DOV of 0.34, 0.20, 0.52, 0.31, and 0.34 in parts 1, 2, 3, and 4 and overall, respectively when prompted without performance descriptors. During evaluations that incorporate performance descriptors within the prompts, the Eval Yaks models demonstrate consistently low DOV values, recording scores of 0.34, 0.26, 0.52, 0.31, and 0.36 for parts 1, 2, 3, and 4, and overall, respectively. The EvalYaks models perform the best in part 2 and perform the their worse in part 3 regardless of the prompt type. Interestingly, the performance of the model in part 2 slightly deteriorates when prompts included descriptors, giving a higher overall DOV of 0.36 compared to 0.34 in the evaluation using the prompts without descriptors."}, {"title": "7 Conclusion", "content": "In this study, we created EvalYaks, a collection of six models aimed at the automated assessment of CEFR B2 English speaking assessment parts 1 through 4, as well as CEFR Vocabulary and sentence structures. These models were trained by instruction fine-tuning the base Mistral Instruct V0.2 with a high-quality, well-aligned candidate conversation records dataset that we developed. The performance of the Automated Evaluators was evaluated based on the average acceptable accuracy and Degree of variation (DOV) in the scores.\nEval Yaks surpassed its primary competitor, Gemini Pro 1.0, by threefold in terms of accurate score predictions and doubled its effectiveness when performance descrip-tors were included. The base model of Eval Yaks, Mistral Instruct V0.2, ranked near the lowest among 11 models in the CEFR B2 English assessment. Eval Yaks consis-tently demonstrated strong performance in this assessment, registering a DOV of 0.34 and 0.36 for scenarios without and with performance descriptors, respectively. This was roughly one-third of the variation seen in Gemini Pro 1.0, which had a DOV of 0.66 more than EvalYaks. The subsequent five top models demonstrated DOVs rang-ing from 0.19 to 0.31 more in contrast to Gemini Pro 1.0, indicating three to four times greater effectiveness of EvalYaks among the other leading models. Overall, EvalYaks showed consistent excellence across all evaluated metrics.\nOur research confirms that large language models can effectively understand sen-tence structures and match vocabulary to CEFR levels. The alignment training for Eval Yaks models that focused on Cambridge B2 First is applicable to the other Cam-bridge English Qualifications. This adaptability suggests that similar models could be successfully used for automated CEFR speaking assessments, offering scalable and resource-efficient solutions.\nAlthough LoRA adapters have shown potential, they may not suffice for a robust, production-level system on their own. To boost reliability, we plan to incorporate EvalYaks with language agents [46], Direct Preference Optimization (DPO), and reflec-tion [47\u201349] into the workflow. The reflective agents will enable self-improvement and evaluation of the scores during the iterations preceding the final outcomes. When Eval Yaks produce 'partly accurate', 'acceptable', and 'inaccurate' responses, employ-ing preference sampling with human experts can fine-tune these scores. We also plan to add descriptive feedback to offer deeper insights into the performance, highlight-ing their strengths and areas for improvement. This enhancement not only displays detailed scores but also elucidates the reasoning behind them, fostering learning through targeted feedback. Furthermore, adding memory to the EvalYaks can connect assessment results with curated learning content, enriching the e-learning experience through a synergistic relationship between assessment and educational content. As Eval Yaks are language models, we have used text inputs for scoring. However, recog-nizing the crucial role of pronunciation in speaking assessments, a multi-modal system that can evaluate both textual and auditory elements of speech is a future area of study."}]}