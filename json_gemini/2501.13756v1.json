{"title": "Solving the long-tailed distribution problem by exploiting the synergies and balance of different techniques", "authors": ["Ziheng Wang", "Toni Lassila", "Sharib Ali"], "abstract": "In real-world data, long-tailed data distribution is common, making it challenging for models trained on empirical risk minimization to learn and classify tail classes effectively. While many studies have sought to improve long-tail recognition by altering the data distribution in the feature space and adjusting model decision boundaries, research on the synergy and corrective approach among various methods is limited. Our study delves into three long-tail recognition techniques: Supervised Contrastive Learning (SCL), Rare-Class Sample Generator (RSG), and Label-Distribution-Aware Margin Loss (LDAM). SCL enhances intra-class clusters based on feature similarity and promotes clear inter-class separability, but tends to favor dominant classes only. When RSG is integrated into the model, we observed that the intra-class features further cluster towards the class center, which demonstrates a synergistic effect together with SCL's principle of enhancing intra-class clustering. RSG generates new tail features and compensates for the tail feature space squeezed by SCL. Similarly, LDAM is known to introduce a larger margin specifically for tail classes; we demonstrate that LDAM further bolsters the model's performance on tail classes when combined with the more explicit decision boundaries achieved by SCL and RSG. Furthermore, SCL can compensate for the dominant class accuracy sacrificed by RSG and LDAM. Our research emphasizes the synergy and balance among the three techniques, with each amplifying the strengths of the others and mitigating their shortcomings. Our experiment on long-tailed distribution datasets, using an end-to-end architecture, yields competitive results by enhancing tail class accuracy without compromising dominant class performance, achieving a balanced improvement across all classes.", "sections": [{"title": "1. Introduction", "content": "Computer vision has achieved significant successes and has been widely used in real-world applications due to advance- ments in deep convolutional neural networks (CNNs) and the availability of large and high-quality datasets. However, real-world data, in general, follows a long-tailed distribution. As a result, CNN models trained on balanced datasets struggle to perform well on skewed datasets, especially for classes with few samples (tail classes). These tail classes are of imminent importance as they usually represent critical problems that could widely benefit society, such as severe diseases, endangered species, fraud activities, etc. Therefore, training unbiased models for long-tail recognition (LTR) is of great practical significance.\nTraditional re-sampling and re-weighting methods fail to address LTR well [2, 4, 14, 35\u201337, 44]. Thus, some logit adjustment approaches [3, 28, 32] that directly adjust decision boundaries for a balanced feature distribution have been proposed and widely researched. Recent studies have improved classification accuracy using ensemble models [27, 42, 44], but several studies have often introduced greater complexity. For example, decoupled learning [3, 17, 24, 44] adopts a two-stage approach, focusing on representation learning and classifier training separately to optimize each without conflicts.\nSupervised Contrastive Learning (SCL), as detailed in [17], offers robust capabilities for representation learning on balanced datasets. Originally designed for two-stage training, SCL's potential in LTR is further unlocked in an end-to-end hybrid network, Hybrid-SC in [35]. The approach combines SCL for feature learning with Cross-Entropy (CE) for classification, outperforming the conventional two-stage SCL method only. The Hybrid-SC highlights the effectiveness of SCL in long-tail recognition, and the same authors also designed a prototypical supervised contrastive learning strategy (Hybrid-PSC) to save memory space. The Balanced Contrastive Learning (BCL) [45] extends SCL by integrating Class-averaging and Class-complement strategies to ensure equitable learning across both head and tail classes. The BCL leverages data augmentation to enhance representation learning. Additionally, it employs a logit adjustment classifier [28] to bias the model towards tail classes further. The state-of-the-art (SOTA) results demonstrate the"}, {"title": "2. Related work", "content": "Long-tailed Recognition. Re-balancing encompasses two main techniques: data re-sampling and loss re-weighting. When under-sampling head classes, valuable information can be lost, leading to a reduction in the model's generalization capability and an obstruction in feature learning [1, 5, 35, 44]. Over-sampling tail classes can result in model over-fitting [1, 5, 35, 44] because the features of the tail class are not truly enriched. Loss re-weighting seeks to adjust the loss values of different classes during training via the inverse frequency of class samples [25], the number of effective samples [6], etc. However, the re-weighting method has been proved to complicate optimization [12, 13], especially in large-scale datasets. Research [15, 44] has revealed that re-balancing strategies are beneficial for classification learning but counterproductive for representation learning. Therefore, in [3] the first stage is dedicated to representation learning, and the second stage employs re-weighting strategies to fine-tune the classifier. The two-stage methodology has been successful in achieving superior accuracy [2, 8] but its inference speed is lower than the end-to-end architecture [19]. Gradients of decoupling network cannot backpropagate from the top layers to the bottom layers of CNNs [34], and it are sensitive to variations in hyperparameters and optimizers [40]. Ensemble learning [27, 42, 44], which leverages the expertise of multiple models to learn varied knowledge, enhances accuracy. However, it's necessary to trade off the increased complexity and computational resources [20, 21].\nAdjusting Feature Distribution and Decision Boundaries Due to limited sample sizes of tail class, its feature space is constricted to a narrow region [38]. Several studies [22, 34] have focused on expanding the feature space of these tail classes. By generating new tail samples, [34] not only expands tail class feature space but also ensures that class features gravitate closer to the class center. Supervised Contrastive Learning [4, 17, 24, 35, 45] drives that samples of the same class are closely clustered while those from different classes are spread out, thereby achieving clear boundaries between classes in the feature space. Besides, logits adjustment [3, 28] introduces the margin to adjust the decision boundary of the model. It provides the tail class with a larger margin to offset the bias caused by imbalanced datasets."}, {"title": "3. Adjusting the Feature Distribution and Decision Boundary for Long-tailed Learning", "content": "3.1. Preliminaries\nThe essence of deep learning is to find the optimal mapping function from input X to output Y through a fitting process. The function typically consists of two parts: a nonlinear encoder $f: X \\rightarrow Z \\in R^h$ and a linear classifier $W: Z \\rightarrow Y$, resulting in $f(x) = W\\cdotf(x)+b$. High-quality features Z provide the classifier with valuable information for achieving high accuracy. The performance of the classifier also directly impacts the model's overall effectiveness.\nThus, for long-tail recognition tasks, our goal is to balance the strengths and weaknesses of a high-quality encoder and an effective classifier to achieve superior results. Next, we recall the key concepts utilized in our research to facilitate subsequent understanding and application."}, {"title": "3.2. Analysis and Methods", "content": "SCL and RSG The $L_{SCL}$ encourages samples of the same class to cluster and keeps samples of different classes away from each other in the feature space, as depicted in Figure 1b. The second term of $L_{CESC}$ acts similarly with $L_{SCL}$.\nThe first term of $L_{CESC}$ promotes features clustering towards class centers, contributing to tighter class cohesion as"}, {"title": "4. Experiment", "content": "In our experiments, we observed that the model exhibits insensitivity to the weights \u03b7 for $L_{CESC}$ and \u00b5 for $L_{MV}$. The [34] had initially set these weights at \u03b7 = 0.1 and \u03bc = 0.01.\nHowever, following our scaling these two weights, which aimed to investigate the impact of these parameters, we found that adjustments in \u03b7 and \u03bc did not significantly influence the model's results. Therefore, to ensure a balanced contribution of each loss function to the total loss and maintain the loss functions within the same order of magnitude, we set \u03b7 = 0.00001 and \u03bc = 0.000001 for all our experiments. The hyperparameter 7 for the $L_{SCL}$ loss is set to 0.1, aligning with the configurations in [45]. For the $L_{LDAM}$ loss, the maximum margin (max_m) and scaling factor (s) are set to 0.5 and 30, respectively, following the parameter settings suggested in [3, 34, 43]. All our experiments were conducted on a NVIDIA GeForce RTX 4090. The source code will be made available upon acceptance."}, {"title": "4.1. Datasets", "content": "In our experiments, we simulate long-tailed distributions by down-sampling the original datasets exponentially, guided by imbalance factors \u03b2 defined by $\u03b2 = N_{max}/N_{min}$, where $N_{max}$ and $N_{min}$ represent the largest and smallest class sizes, respectively."}, {"title": "4.2. Implementation details", "content": "For both CIFAR-10-LT and CIFAR-100-LT datasets, we used the ResNet-32 [10] as the backbone. We trained the model for 200 epochs with a batch size of 32. The epoch threshold $T_{th}$ of RSG is 100. We utilized the SGD optimizer with a momentum of 0.9 and a weight decay of 5e-4. Our learning rate decay strategy is referenced from [45]. The learning rate was initially set to warm up to 0.1 within the first 5 epochs. After the warm-up period, a cosine annealing schedule was used to gradually reduce the learning rate. The learning rate also underwent a stepwise decay at epochs 160 and 180, with each step reducing the learning rate by a factor of 0.1. We do not use class re-weighting strategies, as using inverse class frequency to weight $L_{LDAM}$ impaired the model's performance. Through genetic algorithms [9, 29], we have determined relatively good weights \u03b1 and \u03bb for $L_{SCL}$ and $L_{LDAM}$, respectively, with specific values presented in Table 3."}, {"title": "4.3. Ablation study", "content": "In our experiments, we distinguish between two configurations: SCL-LDAM, where SCL is used for feature learning and LDAM for classification without RSG integration, and RSG-SCL-LDAM, where we integrate RSG into the model with SCL for feature learning and LDAM for classification. Previous findings [34, 43] indicate that RSG aids in improving tail class accuracy when combined with CE and LDAM, often at the cost of head class accuracy. However, our analysis in Section 3.2 posits that RSG can collaborate with SCL and LDAM, offsetting their respective limitations. This ablation study aims to confirm if RSG can enhance tail class precision without detriment to head class accuracy, thus proving its collaborative efficacy with SCL and LDAM.\nBesides, on the mini-ImageNet dataset, we conducted tests ablationing SCL from RSG-SCL-LDAM, the RSG-LDAM, to evaluate the impact of SCL on the model. We also experimented with replacing LDAM with Cross-Entropy (CE) loss, the RSG-SCL-CE, to observe the individual contribution of LDAM."}, {"title": "4.4. Main results", "content": "Experimental results on long-tailed CIFAR Based on the experimental results in Table 1, the RSG-SCL-LDAM outperforms most others. Compared to Hybrid-SC, which uses CE for classification and SCL for representation learning, SCL-LDAM employs the LDAM classifier, which is more sensitive to tail classes. However, it falls short of Hybrid-SC in scenarios with imbalance factors of 50 and 100 on CIFAR-10-LT, which can be attributed to not employing the most ideal loss weighting between SCL and LDAM. Although most of our results did not exceed BCL utilizing similar framework with ours, it's noteworthy that it utilizes data augmentation techniques that expand the dataset with additional images. Specifically, the three different views generated from each image through data augmentation contribute high accuracy to BCL also tripled the computational cost, as shown in Table 4. The SCL shows strong performance in classes with abundant samples, and it is consistently allocated a higher weight in our method, indicated in Table 3. Thus, SCL contributes to our method achieving the highest accuracy on datasets with an imbalance factor of 10."}, {"title": "5. Conclusion", "content": "In this work, we explored the synergy and compensatory effects of Supervised Contrastive Learning, Rare-Class Sample Generator, and Label-Distribution-Aware Margin Loss on the long-tail recognition challenge. Our in-depth analysis demonstrates that balancing these three techniques leverages their respective strengths to offset their limitations, enhancing tail class accuracy while maintaining dominant class performance, thereby achieving a balanced improvement across all classes. Extensive experiments validate this collaborative approach's effectiveness in addressing the long-tail learning challenge.\nLimitation Although the weighted linear combination of losses offers a straightforward and effective approach to long-tail recognition, considerable time and computational resources were used in searching for optimal loss weight hyperparameters using genetic algorithms. Suboptimal weight selection can limit the synergy between loss functions, leading to less-than-ideal results. Future work should aim to discover a robust and superior weighting method for combined loss functions."}]}