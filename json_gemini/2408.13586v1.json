{"title": "Balancing Diversity and Risk in LLM Sampling: How to Select Your Method and Parameter for Open-Ended Text Generation", "authors": ["Yuxuan Zhou", "Margret Keuper", "Mario Fritz"], "abstract": "Sampling-based decoding strategies have been widely adopted for Large Language Models (LLMs) in numerous applications, which target a balance between diversity and quality via temperature tuning and tail truncation (e.g., top-k and top-p sampling). Considering the high dynamic range of the candidate next-tokens given different prefixes, recent studies propose to adaptively truncate the tail of LLM's predicted distribution. Although improved results haven been reported with these methods on open-ended text generation tasks, the results are highly dependent on the curated truncation parameters and exemplar text. In this paper, we propose a systematic way to estimate the intrinsic capacity of a truncation sampling method by considering the trade-off between diversity and risk at each decoding step, based on our collected prefix tree which preserves the context of a full sentence. Our work provides a comprehensive comparison between existing truncation sampling methods, as well as their recommended parameters as a guideline for users.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) (Achiam et al., 2023; Touvron et al., 2023; Jiang et al., 2023; Team et al., 2023) have demonstrated incredible performance in a variety of applications, and the reliability of decoding strategies has become a critical concern, especially where diverse and coherent samples are desired. Previous works have revealed that likelihood-maximization such as beam search (Fan et al., 2018; Holtzman et al., 2020; Welleck et al., 2020; Meister et al., 2022) produces degenerated text which contains repetitive loops and incoherent context, particularly in the open-ended tasks. Therefore, sampling-based decoding strategies, e.g., Top-p (Holtzman et al., 2020) and Top-k sampling (Radford et al., 2018; Fan et al., 2018), have been widely adopted. The balance between diversity and quality of the generated text could be adjusted by tuning the temperature and truncation position to some extend, but requires non-trivial trial and error.\nRecent studies (Basu et al., 2021; Zhu et al., 2024; Hewitt et al., 2022; Meister et al., 2023) proposed adaptive tail truncation mechanisms based on different criteria or assumptions, which maintain an allowed set of tokens with a flexible size according to the given prefix. To validate the effectiveness of a sampling method, they are often compared through extrinsic evaluation based on open-ended text generation applications. For example, story generation (Fan et al., 2018) and document continuation (Merity et al., 2017). Various metrics (Welleck et al., 2020; Meister et al., 2023; Pillutla et al., 2021; Gao et al., 2021) have been adopted to consider different aspects of the generated text.\nWe reveal that there exist two underlying issues in the current evaluation setup, which might hinder the assessment of a sampling method's practical significance in real-world applications:\n\u2022 The improvement of one method against another may be simply due to a better tuned parameter for the targeted exemplar text: the performance of sampling methods is sensitive to their parameters, and parameter sweep is often operated on a extremely sparse grid due to the high computation cost. This is especially problematic considering the non-linear dependency between performance and parameters.\n\u2022 Users are agnostic to the optimal parameters in real-world applications: Practically speaking, users often pick parameters based on their own need for the compromise between diversity and quality, after few number of tryouts. There exits no universal optimal hyerparamters in different scenarios and users are agnostic to the optimal hyerparameters for their own tasks.\nThe above issues exactly indicate the need for an evaluation that allows for estimating the theoretical capacity of a sampling method, independent of hyperparamter tuning. Moreover, the second issue additionally highlights the need for identifying the sweet spots of existing sampling methods, which could serve as a general guideline for parameter selection when applying a method.\nBased on the above analysis, we propose a systematic way to assess the inherent adaptability of a sampling method in different contexts. First, we rearrange Wikipedia-English \u00b9 data in the form of a word-level prefix tree structure, or the so-called Trie (Fredkin, 1960; Ghasemi et al., 2019). As shown in fig. 3, all possible words that appear after a given prefix in the dataset are collected together as the child nodes, and their preceding word is regarded as the parent node. Starting from \"Begin of Sequence\" and collecting the child nodes recursively, we are able to transform the full dataset into a single prefix tree. It is noteworthy that a n-gram Trie (Jurafsky, 2000) tends to produce overestimated data support size given a prefix (Bengio et al., 2000), due to the lost of contextual information outside the truncation window, as shown in fig. 1. In a similar spirit to (Ding et al., 2024), we intentionally construct the prefix tree with only sentence-starting n-grams to preserve the context of a full sentence. thus we refer to the collected data as Context-Preserving Trie (CP-Trie).\nSince the child nodes of the ancestral prefix define the data support, the recall of a sampling method w.r.t. data support is able to be computed. Though the training datasets of modern LLMs are significantly larger than Wikipedia dataset, such a data support serve as a reasonable lower-bound, especially when only sentence-level context are"}, {"title": "Related Work", "content": "In this section, we summarize the recently proposed sampling-based decoding strategies and the widely adopted benchmarks as well as metrics for open-ended text generation."}, {"title": "Sampling-based Decoding Methods", "content": "Vanilla sampling suffers from the risk of obtaining incoherent tokens, thus truncation of the tail distribution has been heavily discussed to alleviate such an issue, e.g., Top-k ((Radford et al., 2018; Fan et al., 2018)) and Top-p sampling ((Holtzman et al."}, {"title": "Revisiting Truncation Sampling", "content": "We reveal three major issues in the evaluation of truncation sampling. We first summarize the problem of directly using probability as quality metric, then show the choice of truncation parameter has a significant impact on the evaluation. Finally, we reveal that a minor difference of Recall and Risk values may result in significant changes in diversity and quality of the generated text.\nUnreliable Probability The probabilities of both the predicted and empirical distribution are not reliable for reflecting the quality of a text.\n\u2022 Higher likelihood doesn't necessarily imply higher quality of the generated text (Fan et al., 2018; Holtzman et al., 2020; Nandwani et al., 2023; Wang and Zhou, 2024).\n\u2022 Word frequencies are average statistics across various topics, and assuming the optimal probabilities or the optimal ranking of each reasonable next token is ill-posed.\n\u2022 Empirical distribution suffers from the sparsity issue (Shareghi et al., 2019; Li et al., 2016; Jurafsky, 2000) of the N-gram models."}, {"title": "Remaining Issues", "content": "We reveal three major issues in the evaluation of truncation sampling. We first summarize the problem of directly using probability as quality metric, then show the choice of truncation parameter has a significant impact on the evaluation. Finally, we reveal that a minor difference of Recall and Risk values may result in significant changes in diversity and quality of the generated text.\nUnreliable Probability The probabilities of both the predicted and empirical distribution are not reliable for reflecting the quality of a text.\n\u2022 Higher likelihood doesn't necessarily imply higher quality of the generated text (Fan et al., 2018; Holtzman et al., 2020; Nandwani et al., 2023; Wang and Zhou, 2024).\n\u2022 Word frequencies are average statistics across various topics, and assuming the optimal probabilities or the optimal ranking of each reasonable next token is ill-posed.\n\u2022 Empirical distribution suffers from the sparsity issue (Shareghi et al., 2019; Li et al., 2016; Jurafsky, 2000) of the N-gram models."}, {"title": "Probability-Independent Metrics", "content": "To quantify the diversity and quality of a sampling method based on CP-Trie, we define the Recall and Risk of a sampling method regarding a given prefix below:"}, {"title": "Data Collection", "content": "As mentioned above, the construction of the CP-Trie is a re-collection of an existing dataset. We apply the described procedure to the English subset of Wikipedia dataset and name the resulting dataset EnWiki CP-Trie. Although the core idea is straight-forward to understand, we elaborate the main design choices in the following:\nBasic Unit There are many possible units for splitting the datasets into individual fragments, such as article, paragraph, sentence and n-grams. Constructing a tree based on articles or paragraphs may require a larger amount of data than the training data of LLMs to guarantee an adequate number of branches (because LLMs lean to interpolate), whereas the construction based on n-grams suffers from poor contextual information and are heavily biased towards common tuplets of n tokens regardless of the context. Therefore, we adopt sentence as the basic unit for our dataset, which guarantees a coherent context at sentence-level and requires a much smaller amount of data than the training data."}, {"title": "Evaluation Setup", "content": "Baselines Our evaluation includes Top-k sampling (Radford et al., 2018; Fan et al., 2018), Top-p sampling (Holtzman et al., 2020), \u03b7-sampling (Hewitt et al., 2022), Adaptive sampling (Zhu et al., 2024) and Mirostat (Basu et al., 2021) into comparison.\nEvaluation Data To guarantee a tight lower bound of the ideal data support given different prefixes, we first sort the sub-nodes according to their total number of leaves at each depth, then we select the top 10 sub-trees with different sentence starting tokens for evaluation. Moreover, we keep the top 2 child nodes at each depth till depth 6, since the empirical data support becomes less adequate at large depth. This results in an evaluation set of 593 prefixes with varying lengths in total.\nEvaluation Metrics As discussed in section 4.2, we measure the improvement in diversity via the increase of average recall at a given risk level, and the reduction of the total risk in the auto-regressive process via the decrease of standard deviation at a given risk level, also referred to as stability.\nLLMS To ensure that the obtained conclusion generalizes to different models, we adopt Llama-2 (Touvron et al., 2023), Llama-3 (Dubey et al., 2024), Mistral (Jiang et al., 2023, 2024) families of different sizes and GPT-2-XL (Radford et al., 2019) for comparison."}, {"title": "Comparison at Different Risk Levels", "content": "In this section, we conduct a comprehensive study of different truncation sampling methods at different risk levels. As discussed in section 4.2, this allows for a fair comparison which is independent of parameter tuning. Moreover, we provide the corresponding parameters for each truncation sampling method at different risk levels, which could serve as user reference for the parameter selection of the compared methods.\nAs can be seen in table 1, different truncation sampling methods are compared at the average risk level of 1, 5, and 15 respectively. As discussed in section 4.1, our defined risk and recall metrics explicitly exclude the source of risk induced by a LLM's capacity by design, thus similar parameter values correspond to the same risk level for most sampling methods across various model types and sizes. This exactly showcases the advantage of our evaluation being parameter-independent and sustainable to the rapid update of LLMs. Among the evaluated methods, Eta-sampling (Hewitt et al., 2022) is the most sensitive to the changes of model type and size especially at risk levels of 1 and 5, which might hinder its practical significance at a low risk level.\nRegarding diversity, i.e., the average recall at the same average risk level, Adaptive sampling (Zhu et al., 2024) and Mirostat (Basu et al., 2021) are the best and second performers, which consistently outperform the Top-k baseline by a considerable margin. Top-p mostly exhibits inferior recall comparing to the Top-k baseline, so does Eta-sampling at the risk level of 1. As for the stability represented by standard error of risks, Top-k sampling reaches the best scores in most cases. In comparison, Adaptive sampling and Mirostat deliver comparable standard error of risks to Top-k sampling, whereas Top-p sampling and Eta-sampling are again inferior. Considering both diversity and stability, Adaptive sampling and Mirostat are the"}, {"title": "Validation on TruthfulQA Benchmark", "content": "Although our evaluation protocol is grounded by the thorough design process with reasonable simplifications, we would like to verify its effectiveness in the real-world scenario using the TruthfulQA Benchmark (Lin et al., 2021). We evaluate the performance of gpt2-xl model with each truncation sampling method at the average risk levels of 1, 5 and 15 respectively. The evaluation results are shown in section 5.3. For all the methods other than greedy decoding, we run 3 times at each average risk level and report the mean and standard deviation (parenthetical value).\nIt can be observed that greedy decoding falls far behind sampling-based decoding strategies, which conforms to the issue of likelihood-oriented decoding discussed in section 1, as well as the findings in recent studies (Cobbe et al., 2021; Wang et al., 2023; Wang and Zhou, 2024; Shi et al., 2024a). The examples in fig. 7 also explain the unsatisfactory performance of greedy decoding, i.e., the decoding paths of the corrected answers might be excluded after ignoring the non-peak likelihoods.\nSimilarly, all the truncation sampling methods at the low risk level achieves lower accuracy comparing to Naive sampling, due to the over-truncation of the decoding paths. At the average risk level of 5, all the truncation sampling methods slightly improve their own accuracy. Top-k sampling, Adaptive sampling and Mirostat also reach comparable or slightly higher accuracy in comparison to Naive sampling. However, further increased average risk level (means improved average recall and thus diversity) doesn't benefit the performance on TruthfulQA, which is plausible. Moreover, there exists a even stronger correlation between Risk SE (Standard Error of Risks) and TruthfulQA accuracy, validating the importance of stability when evaluating an adaptive decoding method. The strong correlation between TruthfulQA accuracy and our proposed average recall as well as standard error of risks at different risk levels validate the soundness and effectiveness of our evaluation method."}, {"title": "Conclusion", "content": "In this work, we propose a evaluation protocol to assess the intrinsic capacity of truncation sampling methods for open-ended text generation. Our evaluation enjoys the merit of being independent on parameter tuning for the curated tasks. Its effectiveness is further validated by the results on the open-ended text generation setup of TruthfulQA Benchmark. The evaluation results also serve as user reference for creative tasks."}, {"title": "Limitations", "content": "In this work, we focus on the truncation sampling methods specially designed for the open-ended text generation scenario. There exist many related decoding strategies, which aim at improving different aspects of LLMs. For example, a line of decoding strategies are proposed to alleviate Hallucination or improve the reasoning ability, e.g., Dola (Chuang et al., 2023), Context-aware decoding (Shi et al., 2024b), Contrastive decoding (O'Brien and Lewis, 2023) and etc. However, they are beyond the scope of this study and thus not included in the discussion. Although our study is only based on text data in English for clarity, the conclusion should be transferable to other languages as well."}, {"title": "Broader Impact", "content": "Our study on the intrinsic capacity of sampling methods and their appropriate parameters for open-text generation may further promote the application of LLMs in creative industries. There exists a potential risk that our provided findings might be abused for generating harmful or fake information. However, our study itself is neutral and the mentioned risk is a general issue that LLMs face. We call for the attention on AI-Safety in the community."}, {"title": "Definition 3.1.", "content": "\\begin{equation*} P_{trunc}(x_t | x_{<t}) = \\begin{cases}  P_0(x_t | x_{<t}) / Z_{x_{<t}} & x \\in A_{x_{<t}} \\\\  0.0 & o.w., \\end{cases} \\end{equation*}"}, {"title": "Definition 3.2.", "content": "\\begin{equation*} \\begin{aligned} & \\min |A_{x_{<t}, \\theta}| \\\\ & s.t. \\quad \\frac{A_{x_{<t}, \\theta} \\cap D_{x_{<t}}}{|D_{x_{<t}}|} \\geq 100\\%. \\end{aligned} \\end{equation*}"}, {"title": "Definition 4.1.", "content": "\\begin{equation*} Recalle = Minimum \\left(\\frac{|A_{x_{<t}, \\theta}|}{|D_{x_{<t}}|}, 1\\right) \\end{equation*}"}, {"title": "Definition 4.1.", "content": "\\begin{equation*} Riske = Maximum\\left(\\frac{|A_{x_{<t}, \\theta}|}{|D_{x_{<t}}|}-1, 0\\right) \\end{equation*}"}, {"title": "Definition 4.2.", "content": "\\begin{equation*} Recall_{Risk = 0.1} = Recalle \\quad s.t. \\quad Risk_{\\theta} = 0.1 \\end{equation*}"}, {"title": "Conjecture 4.3.", "content": "\\begin{equation*} P_O(\\exists x_t \\notin A^*{x_t}) = 1 - \\prod_{t=1}^T P_O(x_t \\in A^*{x_t} | x_{<t}). \\end{equation*}"}]}