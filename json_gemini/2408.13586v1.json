{"title": "Balancing Diversity and Risk in LLM Sampling: How to Select Your Method and Parameter for Open-Ended Text Generation", "authors": ["Yuxuan Zhou", "Margret Keuper", "Mario Fritz"], "abstract": "Sampling-based decoding strategies have been widely adopted for Large Language Models (LLMs) in numerous applications, which target a balance between diversity and quality via temperature tuning and tail truncation (e.g., top-k and top-p sampling). Considering the high dynamic range of the candidate next-tokens given different prefixes, recent studies propose to adaptively truncate the tail of LLM's predicted distribution. Although improved results haven been reported with these methods on open-ended text generation tasks, the results are highly dependent on the curated truncation parameters and exemplar text. In this paper, we propose a systematic way to estimate the intrinsic capacity of a truncation sampling method by considering the trade-off between diversity and risk at each decoding step, based on our collected prefix tree which preserves the context of a full sentence. Our work provides a comprehensive comparison between existing truncation sampling methods, as well as their recommended parameters as a guideline for users.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) (Achiam et al., 2023; Touvron et al., 2023; Jiang et al., 2023; Team et al., 2023) have demonstrated incredible performance in a variety of applications, and the reliability of decoding strategies has become a critical concern, especially where diverse and coherent samples are desired. Previous works have revealed that likelihood-maximization such as beam search (Fan et al., 2018; Holtzman et al., 2020; Welleck et al., 2020; Meister et al., 2022) produces degenerated text which contains repetitive loops and incoherent context, particularly in the open-ended tasks. Therefore, sampling-based decoding strategies, e.g., Top-p (Holtzman et al., 2020) and Top-k sampling (Radford et al., 2018; Fan et al., 2018), have been widely adopted. The balance between diversity and quality of the generated text could be adjusted by tuning the temperature and truncation position to some extend, but requires non-trivial trial and error.\nRecent studies (Basu et al., 2021; Zhu et al., 2024; Hewitt et al., 2022; Meister et al., 2023) proposed adaptive tail truncation mechanisms based on different criteria or assumptions, which maintain an allowed set of tokens with a flexible size according to the given prefix. To validate the effectiveness of a sampling method, they are often compared through extrinsic evaluation based on open-ended text generation applications. For example, story generation (Fan et al., 2018) and document continuation (Merity et al., 2017). Various metrics (Welleck et al., 2020; Meister et al., 2023; Pillutla et al., 2021; Gao et al., 2021) have been adopted to consider different aspects of the generated text.\nWe reveal that there exist two underlying issues in the current evaluation setup, which might hinder the assessment of a sampling method's practical significance in real-world applications:\n\u2022 The improvement of one method against another may be simply due to a better tuned parameter for the targeted exemplar text: the performance of sampling methods is sensitive to their parameters, and parameter sweep is often operated on a extremely sparse grid due to the high computation cost. This is especially problematic considering the non-linear dependency between performance and parameters.\n\u2022 Users are agnostic to the optimal parameters in real-world applications: Practically speaking, users often pick parameters based on their own need for the compromise between diversity and quality, after few number of tryouts. There exits no universal optimal hyperparamters in different scenarios and users are agnostic to the optimal hyerparameters for their own tasks.\nThe above issues exactly indicate the need for an evaluation that allows for estimating the theoretical capacity of a sampling method, independent of hyperparamter tuning. Moreover, the second issue additionally highlights the need for identifying the sweet spots of existing sampling methods, which could serve as a general guideline for parameter selection when applying a method.\nBased on the above analysis, we propose a systematic way to assess the inherent adaptability of a sampling method in different contexts. First, we rearrange Wikipedia-English \u00b9 data in the form of a word-level prefix tree structure, or the so-called Trie (Fredkin, 1960; Ghasemi et al., 2019). As shown in fig. 3, all possible words that appear after a given prefix in the dataset are collected together as the child nodes, and their preceding word is regarded as the parent node. Starting from \"Begin of Sequence\" and collecting the child nodes recursively, we are able to transform the full dataset into a single prefix tree. It is noteworthy that a n-gram Trie (Jurafsky, 2000) tends to produce overestimated data support size given a prefix (Bengio et al., 2000), due to the lost of contextual information outside the truncation window, as shown in fig. 1. In a similar spirit to (Ding et al., 2024), we intentionally construct the prefix tree with only sentence-starting n-grams to preserve the context of a full sentence. thus we refer to the collected data as Context-Preserving Trie (CP-Trie).\nSince the child nodes of the ancestral prefix define the data support, the recall of a sampling method w.r.t. data support is able to be computed. Though the training datasets of modern LLMs are significantly larger than Wikipedia dataset, such a data support serve as a reasonable lower-bound, especially when only sentence-level context are\",\n      \""}, {"title": "2 Related Work", "content": "In this section, we summarize the recently proposed sampling-based decoding strategies and the widely adopted benchmarks as well as metrics for open-ended text generation."}, {"title": "2.1 Sampling-based Decoding Methods", "content": "Vanilla sampling suffers from the risk of obtaining incoherent tokens, thus truncation of the tail distribution has been heavily discussed to alleviate such an issue, e.g., Top-k ((Radford et al., 2018; Fan et al., 2018)) and Top-p sampling ((Holtzman et al."}, {"title": "2.2 Evaluation of Sampling-based Decoding", "content": "Benchmarks The commonly adopted benchmarks include story generation with Writing-Prompts dataset (Fan et al., 2018), document continuation with WikiText-103 dataset (Merity et al., 2017) and abstractive summarization on the CNN/DAILYMAIL dataset (Nallapati et al., 2016). These benchmarks suffer from the problem of limited exemplar text, which fails to capture the diverse nature of human language.\nStatistical metrics are mostly based on n-gram statistics and focus on a single aspect, such as Repetition (Welleck et al., 2020), Diversity (Meister et al., 2023), Semantic coherence (Gao et al., 2021), Zipf's coefficient (Holtzman et al., 2020) (simple Unigram rank-frequency statistics) and Self-BLEU (Zhu et al., 2018).\nExemplar-based metrics dominate the evaluation of sampling-based decoding methods. As observed by Fan et al. (2018); Holtzman et al. (2020), lower perplexity of the generated text doesn't necessarily indicate better quality. And Holtzman et al. (2020) suggested that the perplexity of the generated text is supposed to be close to that of the human text. MAUVE (Pillutla et al., 2021) takes the trade-off between precision and recall into account, by comparing the learnt distribution from a text generation model to the distribution of human-written text using divergence frontiers. A recent study (Shi et al., 2024a) provides a comprehensive evaluation on a large collection of tasks, which are mostly based on exemplar-based metrics."}, {"title": "3 Revisiting Truncation Sampling", "content": ""}, {"title": "3.1 Problem Formulation", "content": "Definition 3.1.\n$P_{trunc}(x_t|x_{<t}) = \\begin{cases} P_0(x_t|x_{<t})/Z_{x_{<t}} & x_t \\in A_{x_{<t}} \\\\ 0 & o.w., \\end{cases}$   (1)\nwhere $A_{x_{<t}} \\in V$ denotes the allowed set comprising candidate next-tokens for a given prefix, and $Z_{x_{<t}} = \\Sigma_{x \\in A_{x_{<t}}} P_0(x_t|x_{<t})$ is the renormalization term.\nGiven the Context-Preserving Trie of a reference dataset, we could compute the estimate of the optimal allowed set as follows :\nDefinition 3.2. Let $A_{x_{<t},0}$ be the allowed set after truncation given the prefix $x_{<t}$. The optimal allowed set $A^*_{\\ x_{<t}}$ corresponds to the allowed set with the minimum size, while covering the full data support. It is the solution to the following objective function:\n$\\begin{aligned} \\min & \\quad |A_{x_{<t},\\theta}| \\\\ s.t. & \\quad  \\frac{A_{x_{<t},\\theta} \\cap D_{x_{<t}} }{D_{x_{<t}}} > 100\\%. \\end{aligned}$  (2)"}, {"title": "3.2 Remaining Issues", "content": "We reveal three major issues in the evaluation of truncation sampling. We first summarize the problem of directly using probability as quality metric, then show the choice of truncation parameter has a significant impact on the evaluation. Finally, we reveal that a minor difference of Recall and Risk values may result in significant changes in diversity and quality of the generated text.\nUnreliable Probability The probabilities of both the predicted and empirical distribution are not reliable for reflecting the quality of a text.\n\u2022 Higher likelihood doesn't necessarily imply higher quality of the generated text (Fan et al., 2018; Holtzman et al., 2020; Nandwani et al., 2023; Wang and Zhou, 2024).\n\u2022 Word frequencies are average statistics across various topics, and assuming the optimal probabilities or the optimal ranking of each reasonable next token is ill-posed.\n\u2022 Empirical distribution suffers from the sparsity issue (Shareghi et al., 2019; Li et al., 2016; Jurafsky, 2000) of the N-gram models."}, {"title": "Impact of Parameter Selection", "content": "We highlight the complexity and biases in parameter selection: Top-k and Top-p have constant upper bounds, i.e., the vocabulary size |V| and 1, respectively. In contrast, the upper bounds of \u03b7-sampling and adaptive sampling are dependent on LLM's predicted distribution, because they truncate the tail distribution based on the likelihood of tokens and the slope of Min-Max scaled entropy, respectively. The importance of identifying the effective ranges of such parameters is also reflected in the authors' choice of numeral digit for their proposed parameters. For example, AConf is set to 0.0005 in Zhu et al. (2024) and \u03b5 is chosen from 0.0001, 0.0009 and etc in Hewitt et al. (2022). In comparison, the adopted p values for top-p sampling are merely two digits after zero, such as 0.95. Our analysis also shows the significance of identifying the sweet spots of different sampling methods.\nThe Butterfly Effect Although the top few samples possess the most probability mass, we reveal that a minor change in the size of the allowed set at each decoding step could lead to major differences in the quality of the generated text, due to LLMs' auto-regressive nature. For example, the overall probability of at least one bad token to appear at the tth position in a sequence of length T increases rapidly as T increases (due to the product operator):\n$P_0(\\exists x_t \\notin A^*_{\\ x_{<t}}) = 1 - \\prod_{t=1}^T[ P_0(x_t \\in A^*_{\\ x_{<t}}|X_{<t}).$ (3)\nSimilarly, if only 1% probability mass is assigned to additional tokens at each step, for a sequence of length T, there will be $1 - 0.99^T$ chances of obtaining extra diverse samples."}, {"title": "4 Method", "content": "In this section, we derive our method for evaluating different sampling-based decoding strategies. To circumvent the reliability issue of the probabilities we merely check whether the predicted next-token is in or out of the data support."}, {"title": "4.1 Probability-Independent Metrics", "content": "To quantify the diversity and quality of a sampling method based on CP-Trie, we define the Recall and Risk of a sampling method regarding a given prefix below:"}, {"title": "Definition 4.1.", "content": "$Recall_{x_{<t}} =  Minimum \\Big(\\frac{|A_{x_{<t}, \\theta} \\cap A^*_{x_{<t}}|}{|A^*_{x_{<t}}|} , 1\\Big)$ (4)\n$Risk_{x_{<t}} = Maximum \\Big(\\frac{|A_{x_{<t}, \\theta} \\setminus A^*_{x_{<t}}|}{|A^*_{x_{<t}}|} ,0 \\Big)$ (5)\n$A_{x_{<t}, \\theta}$ is dependent on the parameter selection for truncation, e.g., k value in top-k sampling. When the allowed set is smaller than the optimal one after truncation, Recall is smaller than one and Risk is regarded as zero. With further increased size of the allowed set, Recall reaches one but Risk emerges. Since the sizes of reasonable sets vary drastically for different prefixes, it is not possible to always retain the optimal allowed set with a predefined parameter. In this case, we reveal that the adaptability w.r.t. the varying size of data support of a sampling method indeed determines its effectiveness in real-world application.\nNote that we ignore the risk of obtaining bad samples within the optimal allowed set, because such type of risk is unsolvable by truncation and is rather determined by the inherent capacity of the trained LLMs. However, such risk is less severe comparing to that introduced by inappropriate truncation, since LLMs exhibit a significant capability in predicting the next token (Touvron et al., 2023; Achiam et al., 2023; Jiang et al., 2023; Team et al., 2023) and most bad samples reside in the tail distribution.\nMore importantly, our evaluation doesn't rely on the empirical probability, which is biased and inaccurate due to limited dataset size or context window size. However, the tokens which appear in the dataset could be confidently regarded as reasonable, regardless of their actual probabilities. In addition, considering that temperature could change the flatness of distribution arbitrarily, we use ratio of token counts instead of probability mass to make the evaluation independent on temperature tuning and exemplar text. For a more detailed discussion with supporting examples, please refer to appendix A.2."}, {"title": "4.2 Parameter-Independent Evaluation", "content": "To eliminate the huge impact of parameter tuning on fair evaluation, we define the final diversity metric Recall at a given Risk level as follows:"}, {"title": "Definition 4.2.", "content": "$Recall_{Risk=0.1} = Recall_{x_{<t}} \\quad s.t. \\quad Risk_{x_{<t}} = 0.1$ (6)\nAnalogously, a family of critical values such as $Recall_{Risk=0.5}$ can be easily defined.\nFrom the definition, it can be seen that such a metric is no longer dependent on the selection of \u03b8 parameter, thus it reflects the genuine capacity of a sampling method regardless of parameter tuning. This allows for a fair comparison between different sampling methods, especially considering their drastically different effective ranges, as mentioned in section 1 and section 3.2."}, {"title": "4.3 The Priority of Low Variance in Risk", "content": "Despite that more diverse text is desired for many generation tasks, the minimum requirement of the text to be coherent and reasonable is mostly prioritized in real-world applications. Besides the average recall at a given risk level, it is noteworthy that a stable adaptive truncation mechanism is also preferred, i.e., the variance of risks should be kept as low as possible at the given average risk level.\nConjecture 4.3. At a given average risk level, the total amount of risk when generating a sequence of length T is reduced with decreased variance of the risks at each decoding step.\nThe above conjecture can be mainly understood by the auto-regressive generation process of LLMs, as shown in eq. (3). The in-distribution probability dependent on the risk at each decoding step and is minimized when the product of the probabilities is maximized at a given average risk level. We could infer that the sum of the in-distribution probabilities is approximated determined at a given average risk level. For simplification, we assume their sum is unchanged. According to AM-GM inequality, the maximum of the product is achieved when each individual component is equal to each other. Roughly speaking, the less variance of the probability masses at each step, the larger their product and thus the smaller the total risk is."}, {"title": "5 Experiment", "content": "In this section, we conduct evaluation of existing sampling-based decoding approaches on our recollected EnWiki CP-Trie dataset. We aim to estimate the inherent adaptability of exiting sampling-based methods and the results could be used as references for the application of LLMs in open-ended tasks."}, {"title": "5.1 Data Collection", "content": "As mentioned above, the construction of the CP-Trie is a re-collection of an existing dataset. We apply the described procedure to the English subset of Wikipedia dataset and name the resulting dataset EnWiki CP-Trie. Although the core idea is straight-forward to understand, we elaborate the main design choices in the following:\nBasic Unit There are many possible units for splitting the datasets into individual fragments, such as article, paragraph, sentence and n-grams. Constructing a tree based on articles or paragraphs may require a larger amount of data than the training data of LLMs to guarantee an adequate number of branches (because LLMs lean to interpolate), whereas the construction based on n-grams suffers from poor contextual information and are heavily biased towards common tuplets of n tokens regardless of the context. Therefore, we adopt sentence as the basic unit for our dataset, which guarantees a coherent context at sentence-level and requires a much smaller amount of data than the training data."}, {"title": "Filtering", "content": "To avoid invalid words or rare proper names which are unreasonable for the model to predict, we exclude the sentences containing such words by checking their presence in the WORD LIST dataset, which is available on the website \u00b2. It contains a total amount of 354986 words and explicitly excludes proper names and compound words. Section titles are also excluded, because they are often incomplete sentences with poor contextual information.\nStatistics Wikipedia-English dataset has a total number of 6, 458, 670 articles, which results in EnWiki CP-Trie with 31, 557, 359 number of leaves after conversion, as shown in fig. 4.\nStorage The extracted prefix tree is implemented as a nested dictionary and saved in a single JSON file. Since each lookup at any depth has constant complexity, the retrieval from our dataset is highly efficient. Moreover, the dictionary is easily extendable if extra data are needed for a more accurate estimation of the full data support."}, {"title": "5.2 Evaluation Setup", "content": "Baselines Our evaluation includes Top-k sampling (Radford et al., 2018; Fan et al., 2018), Top-p sampling (Holtzman et al., 2020), \u03b7-sampling (Hewitt et al., 2022), Adaptive sampling (Zhu et al., 2024) and Mirostat (Basu et al., 2021) into comparison.\nEvaluation Data To guarantee a tight lower bound of the ideal data support given different prefixes, we first sort the sub-nodes according to their total number of leaves at each depth, then we select the top 10 sub-trees with different sentence starting tokens for evaluation. Moreover, we keep the top 2 child nodes at each depth till depth 6, since the empirical data support becomes less adequate at large depth. This results in an evaluation set of 593 prefixes with varying lengths in total.\nEvaluation Metrics As discussed in section 4.2, we measure the improvement in diversity via the increase of average recall at a given risk level, and the reduction of the total risk in the auto-regressive process via the decrease of standard deviation at a given risk level, also referred to as stability.\nLLMS To ensure that the obtained conclusion generalizes to different models, we adopt Llama-2 (Touvron et al., 2023), Llama-3 (Dubey et al., 2024), Mistral (Jiang et al., 2023, 2024) families of different sizes and GPT-2-XL (Radford et al., 2019) for comparison."}, {"title": "Implementation", "content": "Our implementation mainly relies on Pytorch (Paszke et al., 2017), HuggingFace (Wolf et al., 2020) and OpenAI API \u00b3 library. We implement a truncation sampling method ourselves if the official implementation is not available. For all truncation methods, the minimum size of the allowed set is set to 1 to prevent breaking the sampling process."}, {"title": "5.3 Comparison at Different Risk Levels", "content": "In this section, we conduct a comprehensive study of different truncation sampling methods at different risk levels. As discussed in section 4.2, this allows for a fair comparison which is independent of parameter tuning. Moreover, we provide the corresponding parameters for each truncation sampling method at different risk levels, which could serve as user reference for the parameter selection of the compared methods.\nAs can be seen in table 1, different truncation sampling methods are compared at the average risk level of 1, 5, and 15 respectively. As discussed in section 4.1, our defined risk and recall metrics explicitly exclude the source of risk induced by a LLM's capacity by design, thus similar parameter values correspond to the same risk level for most sampling methods across various model types and sizes. This exactly showcases the advantage of our evaluation being parameter-independent and sustainable to the rapid update of LLMs. Among the evaluated methods, Eta-sampling (Hewitt et al., 2022) is the most sensitive to the changes of model type and size especially at risk levels of 1 and 5, which might hinder its practical significance at a low risk level.\nRegarding diversity, i.e., the average recall at the same average risk level, Adaptive sampling (Zhu et al., 2024) and Mirostat (Basu et al., 2021) are the best and second performers, which consistently outperform the Top-k baseline by a considerable margin. Top-p mostly exhibits inferior recall comparing to the Top-k baseline, so does Eta-sampling at the risk level of 1. As for the stability represented by standard error of risks, Top-k sampling reaches the best scores in most cases. In comparison, Adaptive sampling and Mirostat deliver comparable standard error of risks to Top-k sampling, whereas Top-p sampling and Eta-sampling are again inferior. Considering both diversity and stability, Adaptive sampling and Mirostat are the top 2 adaptive methods to be recommended, and Top-p sampling shall be the last to be considered.\nWe also show in fig. 5 that larger models of the same family have higher average recall at the same risk level comparing to the smaller ones. This conforms to the fact that larger models better captures the human text distribution. Please note that our metrics doesn't allow a direct comparison between different model families, mainly due to their different vocabulary sizes and tokenizers, e.g., Llama-3 has a 128,256 vocabulary size, while Llama-2 has only 32, 000 vocabulary size. Moreover, our metrics also explicitly exclude the source of risk within the optimal allowed set, which is heavily dependent on a LLM's capacity."}, {"title": "5.4 Validation on TruthfulQA Benchmark", "content": "Although our evaluation protocol is grounded by the thorough design process with reasonable simplifications, we would like to verify its effectiveness in the real-world scenario using the TruthfulQA Benchmark (Lin et al., 2021). We evaluate the performance of gpt2-xl model with each truncation sampling method at the average risk levels of 1, 5 and 15 respectively. The evaluation results are shown in section 5.3. For all the methods other than greedy decoding, we run 3 times at each average risk level and report the mean and standard deviation (parenthetical value).\nIt can be observed that greedy decoding falls far behind sampling-based decoding strategies, which conforms to the issue of likelihood-oriented decoding discussed in section 1, as well as the findings in recent studies (Cobbe et al., 2021; Wang et al., 2023; Wang and Zhou, 2024; Shi et al., 2024a). The examples in fig. 7 also explain the unsatisfactory performance of greedy decoding, i.e., the decoding paths of the corrected answers might be excluded after ignoring the non-peak likelihoods. Similarly, all the truncation sampling methods at the low risk level achieves lower accuracy comparing to Naive sampling, due to the over-truncation of the decoding paths. At the average risk level of 5, all the truncation sampling methods slightly improve their own accuracy. Top-k sampling, Adaptive sampling and Mirostat also reach comparable or slightly higher accuracy in comparison to Naive sampling. However, further increased average risk level (means improved average recall and thus diversity) doesn't benefit the performance on TruthfulQA, which is plausible. Moreover, there exists a even stronger correlation between Risk SE (Standard Error of Risks) and TruthfulQA accuracy, validating the importance of stability when evaluating an adaptive decoding method. The strong correlation between TruthfulQA accuracy and our proposed average recall as well as standard error of risks at different risk levels validate the soundness and effectiveness of our evaluation method."}, {"title": "6 Conclusion", "content": "In this work, we propose a evaluation protocol to assess the intrinsic capacity of truncation sampling methods for open-ended text generation. Our evaluation enjoys the merit of being independent on parameter tuning for the curated tasks. Its effectiveness is further validated by the results on the open-ended text generation setup of TruthfulQA Benchmark. The evaluation results also serve as user reference for creative tasks."}, {"title": "7 Limitations", "content": "In this work, we focus on the truncation sampling methods specially designed for the open-ended text generation scenario. There exist many related decoding strategies, which aim at improving different aspects of LLMs. For example, a line of decoding strategies are proposed to alleviate Hallucination or improve the reasoning ability, e.g., Dola (Chuang et al., 2023), Context-aware decoding (Shi et al., 2024b), Contrastive decoding (O'Brien and Lewis, 2023) and etc. However, they are beyond the scope of this study and thus not included in the discussion. Although our study is only based on text data in English for clarity, the conclusion should be transferable to other languages as well."}, {"title": "8 Broader Impact", "content": "Our study on the intrinsic capacity of sampling methods and their appropriate parameters for open-text generation may further promote the application of LLMs in creative industries. There exists a potential risk that our provided findings might be abused for generating harmful or fake information. However, our study itself is neutral and the mentioned risk is a general issue that LLMs face. We call for the attention on AI-Safety in the community."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Complete Record of the Experiment Runs", "content": "The scores of the individual runs on TruthfulQA benchmark are recorded in appendix A.2, and the means and standard errors of recalls and risks at all average risk levels are listed in table 4. Note that due to a fixed amount of computation budget, we search the corresponding parameter value for each truncation sampling method till the average risk is close enough to the predefined value, thus resulting in the variations of the average risks. However, such variations are negligible given the minor differences."}, {"title": "A.2 The Advantage of Probability-Independent Metrics", "content": "In this section, we explain the practical advantages of our proposed probability-independent recall and risk metrics. As can be seen in fig. 9, the empirical distribution aligns with the by gpt2-xl predicted distribution given the same prefix in general: most of the tokens which posses high likelihood in the prediction also has a high probability based on the word frequencies of our collected CP-Trie data. However, there exists two differences:\n\u2022 Some tokens with high likelihood according to gpt2-xl have much lower probability according to the empirical distribution. The ranking of each tokens w.r.t. probability also differ in the two distributions.\n\u2022 A few tokens which should be reasonable candidates (by manual check) have 0 probability according to the empirical distribution.\nFor the first issue, as discussed in section 3.2, there exists no ideal probabilities for each token, and the discrepancy is not solvable by simply increasing the size of the data. For example, the \"perfect\" probabilities of the candidate tokens \"with\" and \"at\" are undefined and could even be regarded as equivalently important for open-ended text generation.\nThe second difference highlights the reliability of LLMs, i.e., the tokens which are assigned high likelihoods are in most cases reasonable. Note that we ignore the risk within the estimated optimal allowed set by design: All the tokens are counted as reasonable till the last token which has non-zero empirical probability, when they are arranged in a descending order according to the predicted probabilities. Thus these tokens with zero probabilities in the empirical distribution will not affect our evaluation of risk, making our method robust to noises and insufficient data support."}]}