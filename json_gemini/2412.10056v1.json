{"title": "GAOKAO-Eval: Does High Scores Truly Reflect Strong Capabilities in LLMs?", "authors": ["Zhikai Lei", "Tianyi Liang", "Hanglei Hu", "Jin Zhang", "Yunhua Zhou", "Yunfan Shao", "Linyang Li", "Chenchui Li", "Changbo Wang", "Hang Yan", "Qipeng Guo"], "abstract": "Large Language Models (LLMs) are commonly evaluated using human-crafted benchmarks, under the premise that higher scores implicitly reflect stronger human-like performance. However, there is growing concern that LLMs may \"game\" these benchmarks due to data leakage, achieving high scores while struggling with tasks simple for humans. To substantively address the problem, we create GAOKAO-Eval, a comprehensive benchmark based on China's National College Entrance Examination (Gaokao), and conduct \u201cclosed-book\u201d evaluations for representative models released prior to Gaokao. Contrary to prevailing consensus, even after addressing data leakage and comprehensiveness, GAOKAO-Eval reveals that high scores still fail to truly reflect human-aligned capabilities. To better understand this mismatch, We introduce the Rasch model from cognitive psychology to analyze LLM scoring patterns and identify two key discrepancies: 1) anomalous consistent performance across various question difficulties, and 2) high variance in performance on questions of similar difficulty. In addition, We identified inconsistent grading of LLM-generated answers among teachers and recurring mistake patterns. we find that the phenomenons are well-grounded in the motivations behind OpenAI 01, and ol's reasoning-as-difficulties can mitigate the mismatch. These results show that GAOKAO-Eval can reveal limitations in LLM capabilities not captured by current benchmarks and highlight the need for more LLM-aligned difficulty analysis.", "sections": [{"title": "INTRODUCTION", "content": "Human-aligned capabilities, typically designed based on difficulty levels aligned with human performance, have been widely used to evaluate LLMs and steer further research initiatives (Lu et al., 2022a; Shi et al., 2024; Hendrycks et al., 2021b; Zhang et al., 2023). Implicit in this approach is the assumption that high scores on these benchmarks indicate human-aligned capabilities. However, there is a growing concern within the community that LLMs may be \"gaming\" these benchmarks\u2014achieving high scores while demonstrating instability and unreliability when confronted with tasks that are simple for humans (Zhou et al., 2024). As shown in Figure 1, while LLMs may excel at complex questions, they often struggle with simpler ones. This inconsistency further indicates that an LLMs' high score of 90% does not necessarily reflect its ability to handle tasks that are considerably easier for humans, who typically score only 60%. Such findings raise a critical question: Do high scores truly reflect human-aligned capabilities in LLMs?\nThe community initially attributed the observed mismatch between benchmark performance and LLM capabilities to data leakage (Ni et al., 2024; Zhou et al., 2023) or the insufficient cov-"}, {"title": "GAOKAO-EVAL", "content": "GAOKAO-Eval leverages China's National College Entrance Examination as a comprehensive benchmark for evaluating LLMs. This evaluation framework is designed to provide a thorough assessment of LLM capabilities across various subjects and question types, while ensuring the integrity and relevance of the evaluation process. The benchmark is characterized by four key features: (1) full-paper examination covering all question types and subjects, including multimodal questions; (2)"}, {"title": "COMPREHENSIVE GAOKAO-EVAL DESIGN", "content": "GAOKAO-Eval is meticulously designed to provide a thorough evaluation by encompassing multiple subjects, diverse question types, and various paper formats. This comprehensive approach ensures that LLMs are assessed across a wide range of cognitive tasks and knowledge domains.\nSubject and Question Type Coverage. Figure 3a presents the distribution of question types across various subjects in GAOKAO-Eval. It encompasses not only multiple-choice questions but also more complex types such as short answers, essays, and subject-specific formats, providing a thorough assessment of AI models' capabilities across different cognitive tasks and knowledge domains."}, {"title": "LLM-Aligned Capabilities Improvement via GAOKAO Question", "content": "Training on a specialized Gaokao dataset contributes to a broader enhancement of its capabilities, specifically reflected in its performance on other sophisticated knowledge benchmarks. In this motivation, we propose a specific model named WQX based on InternLM2-20b-base (Cai et al., 2024). The training detail about WQX is introduced in Appendix Section A. We evaluate the performance of WQX on several latest sophisticated knowledge benchmarks Math, MMLU, CMMLU, C-Eval, GaokaoBench. With an exceptional 84.94% accuracy on the GaokaoBench, the model demonstrates its superior capability in handling complex examination-style questions, a testament to the effectiveness of our targeted training and data augmentation strategies. The improvements observed in Math, MMLU, CMMLU, and C-Eval benchmarks further affirm the WQX model's comprehensive natural language understanding and its adeptness at navigating a wide array of knowledge-intensive tasks (see Figure 3b).The model's ability to better navigate these diverse and complex tasks suggests that the Gaokao dataset covers a broad spectrum of knowledge areas and cognitive skills."}, {"title": "EVALUATION METHODOLOGY AND SECURITY MEASURES", "content": "GAOKAO-Eval employs a rigorous evaluation methodology with strict security measures to ensure accurate, meaningful results while maintaining the integrity of the assessment process.\nNon-leaky Data and Temporal Isolation. To address limitations in previous benchmarks like GAOKAO-Bench (Zhang et al., 2023) and GAOKAO-MM (Zong & Qiu, 2024), which potentially allowed data leakage, GAOKAO-Eval uses genuinely unseen data. It evaluates only open-source models released before June 6, 2024, ensuring temporal isolation and a closed-book environment. This approach provides a more objective assessment of LLMs' capabilities, avoiding the \u201copen-book test\" scenario present in earlier evaluations."}, {"title": "DATA PROCESSING", "content": "At the moment when the Gaokao concluded, we utilized various online channels to collect examination papers from various subjects. These papers were then standardized into a consistent format containing both text and images through manual processing. Specific processing techniques were applied according to the needs of each subject. For subjects such as Chinese, Mathematics, and English, which are commonly used across multiple provinces and tend to have fewer image-based questions, we opted to exclude any images. Consequently, we used solely textual questions as the data for evaluation. For subjects other than the aforementioned three, we treated each major question as a separate input. Sub-questions within each major question were formatted using (1), (2), and so on. Additionally, images within these questions were incorporated by employing the token to indicate their placement within the text. In scientific subjects, where formulas are prevalent, all mathematical expressions were converted into LaTeX format and encapsulated using the $ symbol, which ensured consistent and accurate representation of complex equations. To prevent any external instructions from influencing the model's performance, no extra prompts were included beyond the necessary formatting and data processing steps outlined above."}, {"title": "MODELS", "content": "The models' detailed information is listed in Appendix B.5. The models include Qwen2-72B (Yang et al., 2024) and Owen1-VL (Bai et al., 2023), Yi-34B (AI et al., 2024), GLM-4 (GLM et al., 2024), WQX, 1 Mixtral (Mistral AI, 2024), and GPT-40 (OpenAI et al., 2024). Given the prevalence of graphical elements in high school examination questions, LLMs tend to respond only to text-based questions (with few exceptions), whereas multimodal LLMs address all types of questions."}, {"title": "RESULTS AND KEY FINDINGS", "content": "The following sections break down our key findings, focusing on the performance discrepancies between human-aligned capabilities and LLM-generated results. We systematically explore performance, difficulty ratings, unique error patterns exhibited by LLMs, and the comprehensiveness of our GAOKAO-Eval benchmark."}, {"title": "DISCREPANCY BETWEEN HIGH SCORES AND HUMAN-ALIGNED CAPABILITIES", "content": "We first obtained the score ratings and difficulties, then compared these to the theoretical human performance curves. The results showed significant inconsistencies, as LLMs' difficulty alignment did not match human patterns."}, {"title": "Overall Performance Analysis", "content": "The Figure 4 below present the performance scores of various models on the New Curriculum Standard Paper and the National Type A Paper, ranked by Science Total Score."}, {"title": "Difficulty of Questions", "content": "Human evaluators design tests to follow a normal distribution in difficulty. To assess LLMs' alignment with this principle, we designed a hybrid approach combining manual annotations with an Elo rating system, which incorporates both human expertise and LLM-based judgments. This system adjusts LLM scores based on pairwise comparisons, allowing us to evaluate"}, {"title": "Comparison with human performance using Rasch model", "content": "This subsection explores the performance of LLMs in comparison to human performance using the Rasch model, a common method in education and psychometrics for evaluating the relationship between test item difficulty and the probability of a correct response Boone & Noltemeyer (2017); Khine (2020).\nThe Rasch model (Rasch, 1993), benchmarks its measurements against objective standards to ensure reliability and objectivity, which is particularly useful in assessing whether LLMs can replicate the expected human-aligned response patterns across different difficulty levels. According to the principles of the Rasch model, the probability of a specific individual responding correctly to a specific item can be represented by a function of the individual's ability and the item's difficulty:\n$P(X = 1|\\theta,b) = \\frac{e^{\\theta-b}}{1+e^{\\theta-b}}$ (1)\nwhere $P(X = 1|\\theta,b)$ represents the probability that an examinee with ability level $\\theta$ will answer an item correctly. $\\theta$ represents the examinee's ability level, and b represents the difficulty of the item. In this study, we directly use this equation as the basis for evaluation.\nAs shown in Figure 6, the relationship between question difficulty and scoring rate predicted by the Rasch model demonstrates a poor fit to the real data, as evidenced by the low R-squared value of -0.23. This low R-squared value suggests a significant mismatch between the LLMs' capabilities and the expected human-aligned ability. LLMs struggles to consistently align its performance with the varying difficulty of the questions."}, {"title": "LLMs' UNIQUE ERROR PATTERNS", "content": "This subsection explores the unique scoring patterns exhibited by LLMs when evaluated across various question difficulties. Understanding these patterns is essential for developing robust evaluation metrics that align with human judgment and accurately reflect LLM capabilities."}, {"title": "Analysis of Difficulty Ratings", "content": "Next, we delve into the characteristics of these difficulty ratings to understand their implications. Figure 7a illustrates the relationship between difficulty and question types, showing that certain question types consistently rank as more difficult across models. Figure 7b explores how difficulty correlates with the order of questions within a test. These analyses indicate that our difficulty ratings are well-aligned with human perception and accurately reflect the human-aligned capabilities of LLMs, further validating the reliability of GAOKAO-Eval's evaluation framework. These findings collectively suggest that our use of the Elo rating system, combined with human judgment, provides a robust and human-aligned approach to assessing LLM performance across varying question difficulties."}, {"title": "Semi Difficulty-Invariant Scoring Distribution", "content": "We define the phenomenon of Semi difficulty-invariant scoring distribution as the lack of significant correlation between question difficulty and the scoring rate of language models. Mathematically, we compute the Pearson correlation coefficient between question difficulties bs and the observed scoring rates $P_{ij}(s)$ for subject i and model j:\n$\\rho^{(ij)}_{b, P_{ij}} = \\frac{Cov(b, P_{ij})}{\\sigma_b\\sigma_{P_{ij}}}$ (2)\nwhere $Cov(b, P_{ij})$ is the covariance between question difficulty and scoring rate, and $\\sigma_b$ and $\\sigma_{P_{ij}}$ are the standard deviations of question difficulty and scoring rate, respectively. A small absolute value of $\\rho^{(ij)}_{b,P_{ij}}$ indicates that the scoring rate is approximately invariant with respect to question difficulty.\nFigure 8 presents three correlation heatmaps illustrating the relationship between question difficulty and scoring rate across different subjects and language models. The correlations are calculated"}, {"title": "INCONSISTENCIES IN LLM GRADING", "content": "We observed that human examiners encountered too high Inconsistent Score Rate (ISR) in over 32% of cases due to the unique scoring patterns of LLMs. A high ISR indicates that human graders are more likely to disagree when assessing LLM-generated answers compared to typical human responses, which exacerbates the phenomenon of high variance in performance on similarly difficult questions. As shown in Figure 9, the ISR varies across subjects. Notably, the humanities subjects, such as Politics and History, exhibit higher inconsistency rates compared to science subjects like Physics and Math. This suggests that LLMs may face more challenges in consistently interpreting and responding to questions in humanities, potentially due to the abstract and context-dependent nature of these subjects. For instance, the ISR for Politics reaches up to 41.18% in some models, indicating that nearly half of the responses deviate significantly from the average, highlighting the difficulties LLMs have with the subjective and often nuanced content typical of humanities. In contrast, the ISR for Physics is much lower, often below 20%, reflecting more consistent performance in subjects that rely on more concrete and structured knowledge.\n$ISR_{ij} = \\frac{|s \\in S_{ij} : |s - \\mu_{ij} | > \\sigma_{ij}|}{|S_{ij}|}$ (4)\nwhere $S_{ij}$ represents the set of all scores for subject i and model j, $\\mu_{ij}$ is the mean score, and $\\sigma_{ij}$ is the standard deviation of scores for the same subject-model pair.\nJust as Gaokao scores for human candidates have inherent variability, LLM evaluation cannot achieve absolute consistency. Therefore, the scores in GAOKAO-Eval should be interpreted with caution, especially when comparing across different subjects or models. To address potential bias, each question was reviewed by at least three experienced teachers, with the average score taken as the final grade. Significant discrepancies were re-evaluated and adjusted to minimize bias. However, LLM responses tend to be more misleading for human graders. As with the RM pattern observed by Qiao et al. (2024), models often produce correct final answers despite flawed intermediate steps, which makes grading more challenging for evaluators who rely on process-based scoring. This can result in greater grading discrepancies, as reflected in the ISR data (Figure 9), particularly in subjects"}, {"title": "DISCUSSION", "content": "Our findings through GAOKAO-Eval provide insights into the capabilities and limitations of current LLMs. The observed discrepancy between high benchmark scores and human-aligned capabilities in LLMs aligns with recent studies questioning the reliability of existing evaluation methods (Zellers et al., 2019; Sakaguchi et al., 2020; Lin et al., 2022). Our results extend these concerns to a comprehensive, annually updated benchmark based on real-world educational assessments.\nScore and human-aligned capability mismatch in previous benchmarks. Previous benchmarks may have missed the issue of mismatched scores and human-like capabilities due to their focus on multiple-choice questions with non-continuous score distributions. Additionally, many models likely encountered similar training data (non-leaky), making it difficult to obtain a consistent, continuous scoring range. As a result, earlier benchmarks struggled to highlight this misalignment.\nTowards LLM-Aligned Difficulties. The semi-invariant scoring behavior of LLMs suggests a critical gap in how these models process information compared to humans. Rather than solving problems through human-like reasoning, LLMs tend to rely on pattern recognition across vast datasets (Chen et al., 2024b), leading to inconsistencies in performance on similarly difficult questions. Figure 11 shows results from external experiment with the latest o1 model. Using ol's reasoning tokens as a proxy for LLM-aligned difficulties improved the fit with the Rasch model. The coefficient of determination (R2) increased from -0.22 to 0.1019. Before we try to improve LLM performance"}, {"title": "CONCLUSION", "content": "GAOKAO-Eval provides a comprehensive and annually updated benchmark for evaluating LLM capabilities. Our study reveals that high scores on existing benchmarks do not necessarily reflect"}, {"title": "DETAIL OF WQX TRAINING", "content": "In this section, we delve into the details surrounding the WQX model, which was specifically developed to validate the comprehensiveness of the Gaokao evaluation system. To achieve this, we embarked on an extensive data preparation process, continuing the pre-training on the foundation provided by the InternLM2-20b-base model. The subsequent paragraphs will outline the datasets utilized by the WQX model, our data preparation methodologies, and the intricate details of the training process."}, {"title": "GAOKAO TRAINING DATASET", "content": "To construct a comprehensive Gaokao dataset, we embarked on an extensive data collection process, gathering over 25 million examination questions from online resources and educational books. After a meticulous deduplication process, we narrowed down the dataset to 17.5 million unique questions. In addition to these questions, we incorporated more than 1GB of textual material from academic books into our training data to further enrich the dataset's diversity and depth. To enhance the quality and relevance of our dataset for training LLMs specifically for Gaokao preparation, we employed several innovative techniques for data augmentation and retrieval.\nNumerical Reasoning Enhancement Given that solution processes for numerical calculation problems often contain omissions, we employed the InternLM2-20b model to supplement and elaborate on these mathematical problem-solving steps. To further bolster the model's numerical computation capabilities, we developed a custom calculation tool similar to SymPy, which provides the correct and detailed step-by-step solutions. This tool was used to synthesize a large volume of computational data, thereby enhancing the model's arithmetic proficiency.\nChain-of-Thought Augmentation For questions lacking detailed explanations, we implemented a multi-step approach using the InternLM2-20b model. In the case of multiple-choice questions, we introduced variations in the model's response order to ensure robustness. For questions that the model consistently answered correctly, we prompted it to generate comprehensive explanations, thus enriching the dataset with detailed problem-solving rationales.\nEnhanced Retrieval and Filtering from Common Crawl We employed the Query of CC technique (Fei et al., 2024) to retrieve Gaokao-relevant data from Common Crawl, using examination questions as queries. To ensure data quality, we implemented a novel filtering process: retrieved data was used as context in a Retrieval-Augmented Generation (RAG) framework. The model reattempted questions using this context, and we retained only the retrieved documents that enabled"}, {"title": "GAOKAO TRAINING PROCESS", "content": "WQX model, built upon InternLM2-20b-base, was trained on the curated Gaokao dataset for two complete epochs. We employed the InternEvo framework (Team, 2023), maintaining a context length of 4096 tokens. For content exceeding this length, we split and truncated it into multiple data points. The training configuration utilized mixed-precision computation with bfloat16 and FlashAttention2 for optimal efficiency (Dao, 2024). We used the AdamW optimizer (\u03b2\u2081 = 0.9, \u03b22 = 0.95, weight decay = 0.1) with a cosine decay learning rate schedule, peaking at 3 \u00d7 10-5 after 2000 warm-up steps and decreasing to 3 \u00d7 10-6. Each training batch consisted of approximately 0.5M tokens, balancing computational efficiency with effective learning."}, {"title": "SUPPLEMENTARY EXPLANATION OF GAOKAO-EVAL", "content": ""}, {"title": "MORE DETAIL ABOUT GAOKAO", "content": "Paper Type Diversity The comprehensive nature of GAOKAO-Eval is further enhanced by its coverage of multiple paper types, reflecting the diverse educational landscape across China. As detailed in Table 2, the benchmark includes various Gaokao models, such as the \"3+1+2\" New Pattern, the \"3+3\" Pattern, and the Traditional Pattern. With the reform of the Gaokao (National College Entrance Examination) in 2024, there are six types of Gaokao papers nationwide. The Beijing, Shanghai, and Tianjin papers, along with the National paper A, cover all subjects. Provinces using the New Curriculum Standard I and II papers use corresponding Chinese, Mathematics, and English papers, while most provinces independently set their own exams for other subjects. In GAOKAO-Eval, we tested all publicly available papers from the New Curriculum and National paper A. Regarding the Gaokao models, the current system is primarily divided into three major categories:\n\u2022 The \"3+1+2\" New Pattern widely adopted by 23 provinces, is structured around the core subjects of Chinese, Mathematics, and English. Students are required to choose either Physics or History as their primary subject and select two additional subjects from the remaining four (Political Science, Geography, Chemistry, Biology).\n\u2022 The \"3+3\" Pattern currently used by 6 provinces, allows students, after completing the core subjects (Chinese, Mathematics, and English), to freely choose three elective subjects from six options (Political Science, Geography, Chemistry, Biology, and in Zhejiang, an additional subject of Technology).\n\u2022 The Traditional Pattern The remaining 5 provinces still adhere to the traditional subject division system of the National Paper A, maintaining the conventional academic assessment pathway.\nDistribution of Questions in GAOKAO As detailed in Table 3, the distribution of question types across various subjects showcases the comprehensive scope of the GAOKAO-Eval benchmark."}, {"title": "EXAMPLES OF QUESTIONS AND EXPLANATIONS", "content": "In this section, we present some examples of questions along with their answers and explanations. As shown in Figure 12, there are questions about the spatial distribution characteristics of traditional dwellings, their designs, and the roles of public spaces in Shuangfeng Village. Additionally, Figure 13 demonstrates the predictions made by GPT-40 on these questions."}, {"title": "OVERALL PERFORMANCE ANALYSIS", "content": "The tables below present the performance scores of various models on the New Curriculum Standard Paper and the National Paper A, ranked by Science Total Score."}]}