{"title": "Patch is Enough: Naturalistic Adversarial Patch against Vision-Language Pre-training Models", "authors": ["Dehong Kong", "Siyuan Liang", "Xiaopeng Zhu", "Yuansheng Zhong", "Wenqi Ren"], "abstract": "Visual language pre-training (VLP) models have demonstrated significant success across various domains, yet they remain vulnerable to adversarial attacks. Addressing these adversarial vulnerabilities is crucial for enhancing security in multimodal learning. Traditionally, adversarial methods targeting VLP models involve simultaneously perturbing images and text. However, this approach faces notable challenges: first, adversarial perturbations often fail to translate effectively into real-world scenarios; second, direct modifications to the text are conspicuously visible. To overcome these limitations, we propose a novel strategy that exclusively employs image patches for attacks, thus preserving the integrity of the original text. Our method leverages prior knowledge from diffusion models to enhance the authenticity and naturalness of the perturbations. Moreover, to optimize patch placement and improve the efficacy of our attacks, we utilize the cross-attention mechanism, which encapsulates intermodal interactions by generating attention maps to guide strategic patch placements. Comprehensive experiments conducted in a white-box setting for image-to-text scenarios reveal that our proposed method significantly outperforms existing techniques, achieving a 100% attack success rate. Additionally, it demonstrates commendable performance in transfer tasks involving text-to-image configurations.", "sections": [{"title": "1 Introduction", "content": "The visual-language pre-training (VLP) models in the multimodal domain have garnered considerable attention due to their robust performance across a range of visual-language tasks. Currently, VLP models are primarily applied in three downstream tasks: 1) Visual-Language Retrieval [1]: This task involves matching visual data with corresponding textual data. It consists of two sub-tasks: image-to-text retrieval (TR), which retrieves textual descriptions for given images, and text-to-image retrieval (IR), which finds matching images for specific texts. 2) visual entailment (VE) [2]: This task uses images and text as premises and hypotheses to predict whether their relationship is entailment, neutral, or contradiction. 3) visual grounding (VG) [3]: This task aims to localize object regions in images corresponding to specific textual descriptions. As deep networks are susceptible to error patterns [4-11], i.e., adversarial perturbations [12-27], the security of VLP models has also come under scrutiny. Recent studies indicate that VLP models remain vulnerable to adversarial examples [28]. Research into adversarial attacks on VLP models can further enhance their robustness and security [29-34].\nWhen dealing with multimodal models, attackers can individually target different modalities to reduce the accuracy of downstream tasks. Co-Attack pioneered collaborative attacks by innovatively considering the attack relationships between modalities. Recent research has started to focus on the adversarial transferability of VLP models. However, these attacks are limited in adversarial perturbations and cannot be applied in the physical domain. Typically, attackers use adversarial patch training methods to achieve physical domain attacks. Additionally, they all attack both images and text simultaneously, where text perturbations are easily"}, {"title": "2 Releated Work", "content": ""}, {"title": "2.1 Adversarial Patch", "content": "Adversarial patch attacks can be mainly divided into iterative-based and generative-based methods.\nIterative-based methods. Brown et al. [41] presents a method to create universal, robust, targeted adversarial image patches in the real world. DPatch [42] generates a black-box adversarial patch attack for mainstream object detectors by randomly sampling adversarial patch locations and simultaneously attacking the regression module and classification module of the detection head. Based on DPatch, Lee et al. [43] use the PGD [?] optimization method as a prototype to generate a more aggressive attack method by randomly sampling patch angle and scale changes. Pavlitskaya et al. [44] also reveal that the adversarial patch scale is proportional to the attack success rate. Thys et al. [45] introduce an adversarial patch attack designed to attack person detection in the physical domain. Saha et al. [46] analyze the attack principle of adversarial patches that do not overlap with the target and propose to use contextual reasoning to fool the detector. To reduce patch visibility and enhance the attacking ability of the adversarial patch, a large number of works have made a"}, {"title": "2.2 VLP Model", "content": "Visual language pre-training (VLP) models leverage deep learning techniques to pre-train models on large-scale data, integrating visual and language modalities. As research has progressed, several representative models have emerged.\nEarly VLP models explored integrating visual and language information into a unified framework to enhance performance across multimodal tasks. With the rise of pre-training methods, a series of new models have been developed. For instance, CLIP [36], developed by OpenAI, achieves strong correlations between images and text through contrastive learning, demonstrating excellent performance across various visual language tasks. Another notable model is BLIP [61], which introduces logical reasoning tasks to enhance performance in visual and textual reasoning tasks. Recent advancements include the ALBEF [35] model, which employs"}, {"title": "3 Preliminaries", "content": ""}, {"title": "3.1 Threat Model", "content": "The attacker aims to find a patch $P$, which usually follows a square-sized setting where $P\\in R^{SXSX3}$ and s accounts for the patch size, into the visual inputs of the VLP models, leading to incorrect outputs in downstream tasks that rely on these pre-training models. Given a benign image-text pair $d = \\{d_v,d_t\\}$, a VLP model can encode this input into a fused embedding $e$ and $P$ is designed to mislead the surrogate model $F$ into producing an incorrect embedding:\n$F((1-m) \\cdot d_v + m \\odot P, d_t) \\neq e$,\nwhere m denotes a constructed binary mask that is 1 at the placement position of the adversarial patch and 0 at the remaining positions, $\\odot$ denotes the Hadamard product (element product)."}, {"title": "3.2 Diffusion Models", "content": "We adopt a pre-trained diffusion into our framework. To better understand our work, it is useful to give an overview of Diffusion Models. Denoising Diffusion Probabilistic Models (DDPM) [66] is a class of generative models that has gained significant attention in recent years for its ability to produce high-quality samples. DDPM consists of two main processes: the forward diffusion process and the denoising process.\nThe diffusion process is a Markov chain that gradually transforms data points (such as images) into noise. The diffusion process can be represented as:\n$x_t = \\sqrt{A_t}x_{t-1} + \\sqrt{1 - A_t}\\epsilon_t, t = 1, 2, ..., T$\nwhere $x_t$ is the image at step t, $a_t$ is the diffusion coefficient (which typically decreases with increasing t), $\\epsilon_t$ is noise drawn from a standard normal distribution, and T is the number of diffusion steps.\nThe denoising process is the reverse process of the diffusion process, aiming to recover the original data from the noise. In the Diffusion Model, the denoising process is usually implemented by a conditional neural network (such as U-Net) that predicts the original image based on the current noisy image. The denoising process can be represented as:\n$X_{t-1} = \\frac{1}{\\sqrt{a_t}}(x_t - \\frac{1-\\alpha_t}{\\sqrt{1 - \\bar{\\alpha_t}}}\\epsilon_{\\theta}(x, t))$\nwhere $\\epsilon$ is the noise predicted by the neural network, and $\\bar{\\alpha}_t = \\prod_{i=1}^{t} a_i$."}, {"title": "4 The Proposed Method", "content": ""}, {"title": "4.1 Motivation", "content": "Our method is proposed based on the following observations. First, the prevailing approach in the multimodal field to launching adversarial attacks on VLP models involves attacking both images and text simultaneously. Co-Attack has demonstrated that it is indeed possible to find such a collaborative attack method that achieves a synergistic effect greater than the sum of its parts. However, attacking an additional modality also increases the likelihood of the attack being detected, while single-modality attacks often fail to achieve the same effectiveness as multimodal attacks, a contradiction that has prompted us to investigate image-only attacks on VLP models. Secondly, perturbation attacks, as a form of digital domain attack, cannot be applied to the physical domain, which poses another limitation. Combining these two points, we have explored transferring textual information to images to conduct adversarial patch attacks on images. However, this also raises another issue: adversarial patch attacks tend not to be as inconspicuous as perturbation attacks. Therefore, inspired by some diffusion work, we are studying diffusion-based methods for generating adversarial patches."}, {"title": "4.2 Patch Generation", "content": "To generate adversarial patches, we first have the init patch $P_{init}$ which is a real image and the pre-trained"}, {"title": "Algorithm 1 Patch Generation", "content": "Require: Interaction N, Time step t, Step size s, Adversarial\nperturbation dp, Learning rate lr\nEnsure: Pfinal\n1: for n = 1 to N do\n2: $x = \\sqrt{a_t}(P_{init} + d_p) + \\sqrt{1 - a_t}z; z \\sim N(0, I)$\n3: repeat\n4: $x_t = X$\n5: $x_{t-s} = \\frac{1}{\\sqrt{a_{t-s}}}(-\\frac{1 - a_{t-s}}{\\sqrt{1 - \\bar{a}_{t-s}}}\\epsilon_{\\theta}(x,t)) + \\sqrt{1 - a_{t-s}} \\sqrt{\\frac{a_t}{a_{t-s}}}x_t$\n6: t=t-s\n7: until t <s\n8: $d_p = d_p - lr * \\nabla_dLp$\n9: end for\n10: Pfinal= $X_t$\nDiffusion Model (PDM). We set an image $d_p$ (perturbation), which is the same size as $P_{init}$, as the training parameter. The generation process of the patch can be formulated as follows and diffusion process is shown in Alg. 1:\n$P_{final} = PDM(P_{init} + d_p)$"}, {"title": "4.3 Diffusion Guidance", "content": "Currently, the majority of adversarial patch methods directly optimize the adversarial patch itself, but this approach can cause significant changes to the original image to achieve good attack effects, which poses a great challenge to the naturalness of the adversarial patch. In contrast, since there are no hidden layers in the network, the model parameters can be set to a tensor $d_p$ with the same size as $P_{init}$ and a value of zero. Compared to directly optimizing the patch, adding adversarial perturbations has many advantages. Firstly, the perturbation can be seen as noise in the original image, which better matches the denoising process of diffusion model, and makes it easier to find constrained optimal solutions. Secondly, this method involves fewer changes to the original image and it can preserve the"}, {"title": "4.4 Patch Location", "content": "The vast majority of VLP models utilize attention mechanisms to capture the consistency features between image and text. Previous work [67,68] has highlighted that modality consistency features significantly influence the decision-making of multimodal models and are crucial for the success of downstream tasks. Therefore, we believe that in VLP models, the output of the commonly used cross-attention modules designed for cross-modal interaction reflects the text's attention to the image. Some works on region-specific attacks have already demonstrated the importance of attacking specific areas. For adversarial patch attacks, the placement location can affect the success rate and the training process. Placing adversarial patches on vulnerable parts of the image can achieve more with less, meaning attacks can be carried out without significant perturbations. This also helps in maintaining the naturalness of the adversarial patches. Therefore, we use cross-attention to guide the placement of adversarial patches. The attention map M is calculated as follows:\n$M = softmax(\\frac{QK^T}{\\sqrt{s}})V$,\nwhere Q,K,V denote the feature matrix of different modalities, and $\\sqrt{s}$ denotes the scaling factor for stabilizing the model. Because the generated attention map"}, {"title": "4.5 Loss Function", "content": "Our patch optimization is implemented through the computation of two losses:\n$L_p = L_{score} + \\lambda L_{tv}$.\nIn the third part of the pipeline, the obtained $P_{final}$ is applied to the clean image $d_v$ guided by the attention map to produce the attacked image $d_v'$:\n$d_v' = (1 - m) \\cdot d_v + m \\odot P_{final}$.\nThe image-text pair $d = \\{d_v', d_t\\}$ is input into the VLP model targeted for attack, and the scores for the downstream task are calculated. For a dataset of 1000 images and 5000 texts, each image will receive scores corresponding to 5000 texts. We extract the top k highest scores and divide these scores into two sets, $S_1$ and $S_2$, representing scores of texts that belong or do not belong to the image, respectively. $L_{score}$ is calculated as follows:\n$L_{score} = max(S_1) - min(S_2)$.\nTotal variation loss is effective in removing noise while preserving edge information, resulting in smoother and clearer images. Compared to other smoothing techniques, total variation loss better preserves the edges and"}, {"title": "5 Experiment", "content": ""}, {"title": "5.1 Implementation details", "content": ""}, {"title": "5.1.1 Datasets and VLP Model", "content": "Flickr30K [39] consists of 31,783 images, each with five corresponding captions. Similarly, MSCOCO [40] comprises 123,287 images, and each image is annotated with around five captions. We adopt the Karpathy split [69] for experimental evaluation. We evaluate two popular VLP models, the fused VLP and aligned VLP models. For the fused VLP, we consider ALBEF [35]. ALBEF contains a 12-layer visual transformer ViT-B/16 [70] and two 6-layer transformers for the image encoder and both the text encoder and the multimodal encoder, respectively. TCL uses the same model architecture as ALBEF but with different pre-trained objectives. For the aligned VLP model, we choose to evaluate CLIP [36]. CLIP has two different image encoder choices, namely, CLIPVIT and CLIPCNN, that use ViTB/16 and ResNet-101 [71] as the base architectures for the image encoder, respectively."}, {"title": "5.1.2 Adversarial Attack Settings and Metrics", "content": "To better compare our method with the SoTA method, we mainly use the parameter settings of SGA. We employ PGD with perturbation bound $ \\epsilon$ = 2/255, step size $a$ = 0.5/255, and iteration steps T = 10. In our experiment, the diffusion model we adopt is the unconditional diffusion model pre-trained on ImageNet [72] though we use DDIM to respace the original timesteps for faster inference. In the image-text retrieval task, each image has the top k text scores, where k is set to 15 in the white-box setting. We chose 15% of the original image as the patch size. In the ablation study, we will explore the impact of different values of k and patch sizes on the attack. We employ the attack success rate (ASR) as the main metric for evaluating the attacking capability of the generated adversarial examples in VLP downstream tasks. This metric reflects the proportion of adversarial examples that successfully influence the decisions of models. The higher the ASR, the better the attacking ability. Specifically, we offer ASR values for R@1, R@5, and R@10 in all tables for the tasks of image-to-text (TR) and text-to-image retrieval (IR), where R@N represents the top N most relevant text/image based on the image/text."}, {"title": "5.2 Comparisons of SoTA Method", "content": "To rigorously evaluate the superiority of our proposed method within the white-box setting, we conducted comprehensive comparisons with several baseline approaches. These included the image-only PGD attack [?], the text-only BERT-Attack, the combined separate unimodal attack (Sep-Attack), the Collaborative Attack"}, {"title": "5.2.1 Discussion of Naturalness", "content": "Previous work has scarcely discussed the naturalness of adversarial patches and lacks related definitions and evaluation methods. We consider that natural adversarial patches should be inconspicuous within adversarial examples. Our approach enables the selection of the most suitable adversarial patches for specific images. Fig. 4 compares natural adversarial patches with unnatural ones. We chose a rose as the adversarial patch and placed it on the right shoulder of the girl, making it easily mistaken for a part of the clothing decoration. It is noteworthy that through extensive experiments, we found that high-attention areas are often not the most prominent parts, such as the face, which greatly aids in enhancing naturalness."}, {"title": "5.3 Ablation Study", "content": "In this section, we further investigate the critical factors that influence our proposed method."}, {"title": "5.3.1 \u0422\u043e\u0440 \u041a", "content": "The choice of k is important for the training process of generating adversarial patches. It is evident that as"}, {"title": "5.3.2 Patch Location", "content": "We conducted ablation experiments on the patch location. Fig. 6 and Fig. 7 illustrate the changes in adversarial patches and the number of attack iterations under different localization strategies."}, {"title": "5.3.3 Patch Size", "content": "We define patch size as the ratio of the length (or width) of the patch to the length (or width) of the image. We set K to 10 to compare the attack success rates of different patch sizes under a white-box setting. To prevent the adversarial patches from degrading into noisy images during training, we set the maximum number of attack iterations to 300. Fig. 8 shows the changes in attack success rates and adversarial patches as the patch size varies from 0.2 to 0.05. It is evident that larger adversarial patches achieve more effective attacks and result in more natural-looking adversarial patches."}, {"title": "6 Conclusion", "content": "This paper is the first to consider using adversarial patch attacks exclusively on VLP models. By employing a dual-guided approach with diffusion and attention mechanisms, we control the optimization direction and determine the placement of the patches. We propose a framework for generating natural patches that attack image-text retrieval tasks of VLP models while keeping the text unchanged. Our experiments demonstrate the superiority and feasibility of the method.\nLimitation. While our method exhibits excellent performance in white-box settings and transfer tasks, experiments reveal a lack of model transferability. We believe this is due to the insufficient utilization of the consistency features between images and text during the attack. The natural adversarial patch attacks makes it more challenging to leverage text attention compared to digital domain perturbation attacks. Additionally, the robustness of physical attacks requires further improvement."}]}