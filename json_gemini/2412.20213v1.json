{"title": "Decoding Emotion: Speech Perception Patterns in Individuals with Self-reported Depression", "authors": ["Guneesh Vats", "Priyanka Srivastava", "Chiranjeevi Yarra"], "abstract": "The current study examines the relationship between self- reported depression and the perception of affective speech within the Indian population. PANAS and PHQ-9 were used to assess current mood and depression, respectively. Participants' emotional reactivity was recorded on a valence and arousal scale against the affective speech audio presented in a sequence. No significant differences between the depression and no-depression groups were observed for any of the emotional stimuli, except the audio file depicting neutral emotion. Significantly higher PANAS scores by the depression than the no-depression group indicate the impact of pre-disposed mood on the current mood status. Contrary to previous findings, this study did not observe reduced positive emotional reactivity by the depression group. However, the results demonstrated consistency in emotional reactivity for speech stimuli depicting sadness and anger across all measures of emotion perception.", "sections": [{"title": "1. Introduction", "content": "Speech perception plays a fundamental role in human communication and facilitates social interaction by analyzing the expression of complex thoughts and emotions. How we perceive speech depends not only on linguistic and acoustic features (e.g., words, and fundamental frequency), its content, the affective nature (e.g., anger or happy) or the paralinguistic cues (e.g., pause) but is also affected by the listeners' mood. While extensive research has investigated the role of mood in processing emotional stimuli using visual stimuli either static or dynamic, words or music, only a few studies have examined its role in speech affective perception. Noteworthy, the representation of Asian population, more specifically, Indian population has been dismal among these studies resulting in poor understanding of role of mood in speech perception from the Indian perspectives. We aim to address this gap by investigating the impact of predisposed mood, like depressive state, on affective speech perception by analyzing emotional reaction to the affective speech stimuli among the Indian population.\nMood becomes a relevant component for emotional reaction when it is characterized by persistent sadness and/ or loss of interest and pleasure in regular everyday activities, indicative of a clinical condition called depression or unipolar depression. Depression has been associated with impaired emotional regulation alongside aberration in somatic, cognitive, and psychomotor responses. In such cases, measure of emotional reactivity to positive and negative affective stimuli serves as a hallmark for examining affective processing related to an individual's depressive state. Emotional reactivity is defined as a change in individuals' affective perception or experience in correspondence with the emotionally evocative stimuli. Despite common assumptions about reduced positive emotional reactivity, and increased negative emotional reactivity during depressive episodes, cognitive models, and empirical studies present contradictory views.\nRecent studies, using prosody, showed attenuated positive emotional reactivity (PER) to speech depicting happy emotion by the individuals experiencing depressive episode, whereas speech depicting angry and neutral or sad emotion did not elicit a significantly different emotional reaction between depression and no-depression groups. Similar results were observed in a study using fMRI technique with no difference in amygdala activity was observed between depressive and no-depressive state against angry or sad speech.\nWhile depression has been consistently associated with reduced positive emotional reactivity favoring positive attenuation model of depression, the association with the negative emotional reactivity have shown mixed results. Studies examining the emotional reaction against the negative stimuli have observed either no difference in emotional reactivity between depressive and no-depressive states for negative (e.g., angry or sad) stimuli or reduced negative emotional reactivity to sad stimuli or increased emotional reactivity to sad stimuli and blunted response for threatening stimuli by the individuals with depression, demanding complex comprehensive explanation.\nThe Beck's cognitive model of depression, commonly known as the Cognitive Triad, argues that an individual internal representation determines how individuals perceive themselves and view the world around them and their future perspectives. During depressive episodes, an individual is assumed to favor processing negative compared to positive stimuli, suggesting mood-congruent information processing. In contrast, the emotional context insensitivity (ECI) model argues that individuals with depressive episodes experience resistance to emotional alteration and fail to accommodate sudden environmental changes. The two competing models offer alternative plausible explanation for the conflicting findings for emotional reactivity for negative stimuli. In a nutshell, positive emotional reactivity offers a reliable measure for assessing emotional reactivity against positive stimuli, however, negative emotional reactivity demands further examination by controlling the methodological variability, like culture, participants' profile or stimuli type.\nIn our study, we examine the impact of depression on emotional reactivity to affective speech in which participants are asked to make judgement about the spekears' emotion while listening the speech audio recordings. We focus on college going young adults, chosen deliberately to examine the role of self-reported depression in emotion perception for a given speech audio recording. The choice of participants' profile for this study was determined by the reports stating the increasing prevalence of depression in college going young adults, especially after the post-pandemic episodes, and highlighted the debilitating impact of depression on students' academic, social and interpersonal lives. The current study will help us evaluate whether self-reported depressive symptoms exhibit similar trends in emotion perception of a particular speech."}, {"title": "2. Methodology", "content": "Total ninety seven (M = 63, F = 33, and O = 1) university stu- dents, aged between 18-to-25 years volunteered for the study. All the participants reported normal hearing and high familiarity with English language. Their proficiency in listening American accented English was assessed by language fluency test, in which participants were made to listen to a paragraph with four questions. Participants were given chance to repeat the paragraph, until it is clear to them.\nAt the time participation, minimal educational degree attained by the participants was at least higher secondary or 11th grade, with sixty one participants (62.8%) reported pursing graduation degree, five (5.1%) reported either pursuing or completed PhD degree and twenty three (23.7%) indicated having completed Higher Secondary education, and 8 (8.2%) participants reported having obtained a Post-Graduation degree.\nWe used speech audio stimuli to elicit emotion. The speech stimuli were sourced from the IEMOCAP database, an open-source database created at the SAIL lab at USC, which contained dyadic sessions with actors performing scripted scenarios to elicit emotional expressions. This database is extensively annotated with categorical labels, such as anger, happiness, sadness, and neutrality, and dimensional annotations such as valence, activation, and dominance.\nWe used both the categorical and dimensional approach to create four sets of stimuli, namely, happy, sad, angry, and neutral. The audio files were selected based on three criteria, i. the four categories, ii. the emotion rating using Likert scale 1-to-5, where 1= unpleasant/low arousal and 5=pleasant/high arousal, and iii. the similar duration of audio files, that ranged between 3-7 seconds. A total of 100 such audio files, 25 for each emotion category (sad, happy, angry, and neutral), were selected from the database.\nEach participant performed a total set of three main tasks, comprising a patient healthy questionnaire (PHQ-9), positive and negative scale (PANAS-10), and affective speech perception task, alongside demographic survey, which included information related to their personal and familial health history, gender identity, and hearing ability. We also performed the language fluency test for the English language to reduce the confound caused by language. The description of the affective speech perception task, PHQ-9, PANAS and the language fluency test as follow:\n(1) Emotion Perception Task: In this task each participants were presented with a speech audio file, depicting either happy, sad, angry, or neutral emotion. Participants were instructed to listen to the audio file, and judge the speakers' emotion on valence and arousal scale, using 5 point Likert SAM scale, where 1 = extremely unpleasant or low arousal, and 5 = extremely pleasant/ high arousal. The audio file was presented until both responses were made, and participants were allowed to repeat the audio if required There were 30 such files were presented. The ratings for both valence and arousal were logged separately for each file. We also recorded total time taken, as"}, {"title": "2.1. Partcipants", "content": "Total ninety seven (M = 63, F = 33, and O = 1) university students, aged between 18-to-25 years volunteered for the study. All the participants reported normal hearing and high familiarity with English language. Their proficiency in listening American accented English was assessed by language fluency test, in which participants were made to listen to a paragraph with four questions. Participants were given chance to repeat the paragraph, until it is clear to them.\nAt the time participation, minimal educational degree attained by the participants was at least higher secondary or 11th grade, with sixty one participants (62.8%) reported pursing graduation degree, five (5.1%) reported either pursuing or completed PhD degree and twenty three (23.7%) indicated having completed Higher Secondary education, and 8 (8.2%) participants reported having obtained a Post-Graduation degree."}, {"title": "2.2. Stimuli", "content": "We used speech audio stimuli to elicit emotion. The speech stimuli were sourced from the IEMOCAP database [15], an open-source database created at the SAIL lab at USC, which contained dyadic sessions with actors performing scripted scenarios to elicit emotional expressions. This database is extensively annotated with categorical labels, such as anger, happiness, sadness, and neutrality, and dimensional annotations such as valence, activation, and dominance.\nWe used both the categorical and dimensional approach to create four sets of stimuli, namely, happy, sad, angry, and neutral. The audio files were selected based on three criteria, i. the four categories, ii. the emotion rating using Likert scale 1-to-5, where 1= unpleasant/low arousal and 5=pleasant/high arousal, and iii. the similar duration of audio files, that ranged between 3-7 seconds. A total of 100 such audio files, 25 for each emotion category (sad, happy, angry, and neutral), were selected from the database [15].\nThe descriptive Likert scale rating analysis of audio IEMOCAP databsed [15] revealed distinct characteristics for each emotion category. Speech audio, labelled as sad emotion showed low valence (M=2.16, SD=2.16) and low arousal rating (M=1.92, SD= 0.81), whereas audio labelled as happy showed high valence (M=4.04, SD=0.35) and high arousal (M= 3.12, SD = 0.12). Audio labelled as angry emotions showed low valence and high arousal ratings, (M= 1.48, SD = 0.91; and M=4.04, SD = 0.84, respectively), while neutral emotions comprised medium valence and arousal, (M = 3.12, SD= 0.33; and M=2.36, SD= 0.57, respectively).\nTo mitigate any potential biases introduced by the specific content of audio stimuli, we employed a pseudo-randomized design by creating four distinct stimulus sets labeled A, B, C, and D. Each set consisted of 30 audio files (25 unique and 5 common audio files across sets). We included almost an equal number of files from each emotion in each set ensuring a balanced representation of all emotions. For instance, in set A, there are 6 files each from Sad, Angry and Happy emotions, and 7 files from Neutral) and in each set one of the four emotions has 7 files whereas the rest has 6. This approach improved the internal validity of the study by minimizing the influence of any idiosyncrasies associated with a particular set of stimuli, thus allowing for more robust generalizations across a diverse range of emotional expressions. We ensured that in each set no 2 files labeled with the same emotion are consecutively present.\nParticipants were randomly assigned to one of four sets (A, B, C, D). Set A was allotted to 23 participants, Set B to 26, Set C to 24, and Set D to 24 participants. To ensure intra-rater reliability, we included duplicates of the first 5 files at the end of each set. This resulted in each set containing a total of 30 files for participants to perceive and rate."}, {"title": "2.3. Tasks", "content": "Each participant performed a total set of three main tasks, comprising a patient healthy questionnaire (PHQ-9), positive and negative scale (PANAS-10), and affective speech perception task, alongside demographic survey, which included information related to their personal and familial health history, gender identity, and hearing ability. We also performed the language fluency test for the English language to reduce the confound caused by language. The description of the affective speech perception task, PHQ-9, PANAS and the language fluency test as follow:\n(1) Emotion Perception Task: In this task each participants were presented with a speech audio file, depicting either happy, sad, angry, or neutral emotion. Participants were instructed to listen to the audio file, and judge the speakers' emotion on valence and arousal scale, using 5 point Likert SAM scale, where 1 = extremely unpleasant or low arousal, and 5 = extremely pleasant/ high arousal. The audio file was presented until both responses were made, and participants were allowed to repeat the audio if required (Figure 2, refer to the interface box). There were 30 such files were presented. The ratings for both valence and arousal were logged separately for each file. We also recorded total time taken, as\na response time, to analyze the latency in emotional perceptual processing across states of depression. Response time was measured from the onset of audio stimuli to participants' click to move to the next stimuli. The timing reset for each subsequent stimulus.\n(2) Positive and Negative Scale (PANAS) We used Positive and Negative Affect Scale (PANAS-10), originally developed by Watson and Clark [16], to assess current mood state. PANAS 10 comprises of 10 specific affects: upset, hostile, alert, ashamed, inspired, nervous, determined, attentive, afraid, and active. Of which, five affects are positive and rest five are of negative emotion. Participants were instructed to carefully read the items and report their current feeling on a five point scale, where 1 referred to very slightly or not at all and 5 referred to extremely. The score is calculated by summing up the total positive and negative items separately. For positive items, high score represents high levels of positive effect, and for the negative items, low score represents low levels of negative effect. In other words, the high positive score and low negative score in PANAS signifies healthy scores.\n(3) Patient Health Questionnaire (PHQ-9) The PHQ-9 is developed by Kroenke and colleagues, [17] to assess the individual experience of depression, over the preceding two weeks, including the date of assessment. It comprises statement related to cognitive, emotional, and somatic experiences, such as Over the last 2 weeks, how often have you been bothered by the following problems? For example, Little interest or pleasure in doing things. Participants were instructed to read the statements carefully and choose the options that describe their feeling most appropriately on a four point Likert scale, where 0 referred to not at all, and 3 referred to nearly every day. The total score, ranging from 0 to 27, is obtained by summing the ratings across all nine items. The cut-off scores for categorizing depression severity were set at 5, 10, 15, and 20, representing mild, moderate, moderately severe, and severe depression, respectively (Figure 1). For statistical analysis, we used two groups, no-depression comprising minimal and mild category, and depression comprising moderate and above categories (Figure 2 and 3).\n(4) Language Fluency Test A comprehension test was administered to ensure participants' ability to understand American accent audio files used in the perception task. It included listening to two audio files followed by four simple multiple-choice questions assessing comprehension. The cumulative score determined participants' proficiency, with an average total fluency score of 5.50/7 (SD = 0.7) across all participants, establishing eligibility for the subsequent speech perception task."}, {"title": "2.4. Procedure", "content": "The study involved participants completing a series of tasks involving surveys and emotion perception task after listening to speech audio recordings. The study was conducted using the software Labvanced [18], a JavaScript web application designed for professional behavioral research. Data collection was performed in a controlled environment to ensure consistency across participant experiences. Participants were seated in a noise-proof room equipped with a comfortable chair and a computer workstation. To minimize external distractions, each participant used a high-quality headset to listen to the audio stimuli. The room was soundproofed to minimize interference from external noise. An instructor was present in an adjacent room to assist participants as needed and to monitor the overall progress of the study. The instructor was available to provide clarifications and address any concerns that participants might have encountered during the tasks.\nEach session began with a welcome note and a brief introduction of the study. Upon agreeing to participate, participants were asked to read the consent form, which informed them about their rights to privacy and withdrawal of their participation without any future consequences. To ensure compliance with ethical standards, participants digitally acknowledged their consent before commencing the study. Subsequently, they received detailed instructions from the instructor and on-screen prompts outlining the study's procedures. Participants were briefed on the tasks without explicitly disclosing the research objectives, thus mitigating potential demand characteristics (Figure 2)."}, {"title": "3. Statistical Analyses", "content": "The current study examined the role of self-reported depression in emotional reaction to the affective speech audio information. We performed the power analysis using G*Power version 3.1 [19] to determine the minimum number of participants required to test the study hypothesis. We obtained sample size, N= 36, with 95% power, a moderate effect size (.25) in an ANOVA repeated measure, within-between interaction. Given that the calculated power analysis did not account of post-hoc tests that are be required for the multiple comparisons, we revised the sample size calculation by multiplying it with 4.1 (Brooks and Johanson, 2011), and obtained n=144 with a revised minimal sample size required to test the hypothesis.\nWe are presenting a preliminary data of n=97 participants, with unequal sample size data, of which n=71 reported minimal and mild self-reported depressive states (< 9 scores on PHQ-9, Figure 1), equivalent to healthy adults, i.e., no-depression group (ND), and n=26 reported moderate or severe experience of depressive states (>/= 9 on PHQ-9 survey, Figure 1) considered unhealthy, i.e., depression (D) group. The Shapiro-Wilk test showed that the current data violated the normality assumption, (p<.05, and lead us to use non-parametric comparative analysis, using Mann-Whitney test, with a Bonferroni correction (p < 0.012) to avoid error caused by multiple comparisons. We performed three separate analyses for valence, arousal and reaction time across no-depression (ND) and (D) groups."}, {"title": "4. Results and Discussion", "content": "For valence rating, we observed a significant effect of self-reported depression on perception of affective speech depicting neutral emotion with a medium effect size (U = 570.5, p = .002, r = -0.382). We observed a significnatly lower rating (Med = 2.5) by individuals with depressive than no-depressive state (3.0). However, the comparison between ND and D with other stimuli depicting happy, sad, or angry did not yield any significant difference (Figure 3).\nFurther, ND and D did not show any significant difference in arousal ratings (Table 1) and response time while processing audio files depicting happy, sad, angry, and neutral emotion.\nWe further analyzed the effect of predisposed mood on current mood, positive and negative scale (PANAS), and observed a significantly higher PANAS-negative score (Mdn = 8.5) by individuals with self-reported depression than no-depression (Mdn = 6.0), (U = 1382.5, p < 0.001, r = .498) (Figure 4).\nContrary to previous findings, we did not observe a significant difference in positive speech perception between individuals with and without depression [1, 4, 6, 7, 8]. However, our results demonstrated consistent emotional reactivity for speech stimuli depicting sadness and anger [1, 7]. This result was consistent across three critical measures of emotion perception, namely valence, arousal, and reaction time, except the valence rating for neutral stimuli.\nThe absence of a difference in positive affective rating challenges the previous finding with reduced positive emotional reactivity to positive stimuli. However, the failure to reject the null hypothesis across all emotional categories raises the possibility that the Americanized automated speech audio files may have influenced the results, emphasizing the need for culturally appropriate stimuli. While participants reported high familiarity with the audio files, the Americanized accented speech might have demanded a focus shift to the content and overshadowed the emotional aspect of speech perception. Future studies should consider employing Indian language or Indianized English speech audio files to evaluate emotional reactivity within distinct cultural contexts comprehensively."}, {"title": "5. Conclusion", "content": "The current study examines the relationship between depression and the perception of affective speech stimuli by asking participants to judge the speaker's emotion on valence and arousal. We observed no significant effects of depression on emotional perception except the alteration of valence responses to neutral stimuli. Notably, our findings contradict previous research by revealing no reduced positive emotional reactivity in individuals with depression. Despite acknowledging limitations, such as reliance on self-reported depression without clinical assessment and the use of Americanized English speech audio files, our study contributes to the emotional reactivity literature within the sub-clinical population. Most importantly, no effect of depression on negative emotional reactivity aligns with both behavioural and neural correlates data of patients diagnosed with affective disorders or depression. The findings suggest future research addressing methodological considerations and cultural nuances in emotional reactivity assessment."}]}