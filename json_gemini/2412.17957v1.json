{"title": "ArchComplete: Autoregressive 3D Architectural Design Generation with Hierarchical Diffusion-Based Upsampling", "authors": ["S. Rasoulzadeh", "M. Bank Stigsen", "M. Wimmer", "I. Kovacic", "K. Schinegger", "S. Rutzinger"], "abstract": "ArchComplete is a two-stage dense voxel-based 3D generative pipeline developed to tackle the high complexity in architectural geometries and topologies, assisting with ideation and geometric detailisation in the early design process. In stage 1, a 3D Voxel VQGAN model is devised, whose composition is then modelled with an autoregressive transformer for generating coarse models. Subsequently, in stage 2, Hierarchical Voxel Upsampling Networks consisting of a set of 3D conditional denoising diffusion probabilistic models are defined to augment the coarse shapes with fine geometric details. The first stage is trained on a dataset of house models with fully modelled exteriors and interiors with a novel 2.5D perceptual loss to capture input complexities across multiple abstraction levels, while the second stage trains on randomly cropped local volumetric patches, requiring significantly less compute and memory. For inference, the pipeline first autoregressively generates house models at a resolution of 643 and then progressively refines them to resolution of 2563 with voxel sizes as small as 18cm. ArchComplete supports a range of interaction modes solving a variety of tasks, including interpolation, variation generation, unconditional synthesis, and two conditional synthesis tasks: shape completion and plan-drawing completion, as well as geometric detailisation. The results demonstrate notable improvements against state-of-the-art on established metrics.", "sections": [{"title": "1. Introduction", "content": "Fields such as architecture, urban planning, or entertainment rely on high-quality three-dimensional models with rich geometric details and topology, which often require a significant amount of time, compute and memory to create. Volumetric design (also known as massing or schematic design) is usually the first and most important step in defining a design's appearance. It begins by constructing rough 3D shapes within a defined design space before gradually refining them to include all the details needed for exterior and interior design elements [CCL*21]. However, creating a good volumetric design requires a substantial amount of time and effort. Alternatively, a designer with a rough idea of the desired shape may quickly construct a coarse shape, to which a 3D generative model then adds realistic details. Such a generative pipeline capable of assisting the designer from the inception of an idea to its geometry detailisation has the potential to greatly simplify the design workflow.\nIn recent years, there has been a surge of new and exciting work on generative models for 3D shapes. These efforts are based on various frameworks and 3D geometric representations, achieving promising results in terms of quality and diversity [HLHF22, MCST22, CLT*23, SAA*24]. However, several challenges remain when adapting these methods to domain-specific design applications such as architecture, necessitating a tailored 3D generative pipeline addressing these challenges. Focusing on the architecture domain, first, there is a lack of detailed datasets of 3D models that encompass both exterior and interior spaces. Second, detailed 3D model of architectural buildings/houses are often more complex, or at least significantly different from the shapes in common 3D benchmark datasets like ShapeNet [CFG*15] due to their structural and stylistic intricacies. Third, much of the literature on 3D generative models focuses on specific generation tasks, which cannot be easily extended to other downstream tasks (e.g., user editing), resulting in narrow application scopes. Additionally, compared to state-of-the-art 2D generative models for images, the additional third spatial dimension results in a dramatic increase in network parameters and memory-intensive feature maps in the 3D counterparts.\nA 3D generative pipeline has to fulfil several criteria to accommodate the designers' needs: (i) The design produced by the pipeline must maintain a 1:1 scale relative to the training data, ensuring consistency between the spatial characteristics of the models (such as openings and room heights in the architectural domain); (ii) The generative pipeline should be capable of synthesizing realistic, high-fidelity models while aiding designers in quickly exploring the design space with flexible and interactive use and refinement: For example, a designer may want to blend input models from different distinct styles to create an intermediate model (aka shape interpolation), generate versions with varying details, or may provide partially modelled input asking for completion suggestions, i.e., shape completion. (iii) Or an artist may provide coarse shapes, thereby guiding the pipeline to create large varieties of fine and detailed geometries through a process referred to as geometry detailisation.\nTo address the identified challenges, we propose ArchComplete, a voxel-based 3D generative pipeline operating in two stages: a Base Network for coarse shape ideation and Hierarchical Upsampling Networks for augmenting the shapes with fine geometric details (see Figure 1). To overcome the lack of architectural datasets, the first dataset of 3D house models featuring fully modelled exteriors and interiors is introduced. Inspired by recent advancements in generative models for images [ERO21] and natural language [LSC24], we adopt a sequence-based approach to synthesise voxelised models as a sequence of local volumetric patches. A 3D Vector-Quantised Generative Adversarial Network 3D Voxel VQ-GAN is designed to encode voxelised models into latent quantised embeddings by learning a vocabulary of patches. To better capture geometric and topological features at the scale of local patches, we introduce a novel 2.5D perceptual loss and employ 3D Patch-GAN as the discriminator. We model our 3D Voxel VQGAN'S composition with a transformer that learns statistical correlations between patches enabling autoregressive generation of local patch sequences forming a 3D voxelised model. The transformer's integration enables a range of interaction modes, addressing the required flexibility in downstream tasks. In the second stage, we aim to push the limits of purely 3D generative priors for high-resolution shape generation in dense voxel-based methods while alleviating high computational demands. To this end, we define a hierarchy of 3D conditional Denoising Diffusion Probabilistic Models (3D c-DDPMS) [HJA20] that train on local chunks instead of the entire 3D voxelised models and upsample the coarse outputs from the first stage into finer grids. This hierarchical upsampling strategy mitigates ambiguity in voxel upsampling with large rates by introducing intermediate-level supervision while making distribution at each level easier to model since the coarse levels model rough patches and finer levels focus on local details.\nWe demonstrate the versatility of our pipeline in the 3D architectural modelling domain through several example applications. These include leveraging Genetic Algorithm (GA) operators to generate endless interpolations and variations of synthesised models. Additionally, we explore unconditional synthesis and two conditional synthesis tasks, shape completion and plan-drawing completion, highlighting their potential in architectural modelling. Finally, quantitative and qualitative evidence is provided showing that ArchComplete outperforms prior methods in both unconditional and conditional synthesis tasks based on established metrics."}, {"title": "2. Related Work", "content": "Unlike 2D images, it is less clear how to represent 3D data effectively. Various representations with pros and cons have been explored, particularly when considering 3D generative models. For instance, quite a large number of 3D generative models have been developed for point clouds [ADMG18, LH21], dense voxel grids [LYF17] and more recently, sparse voxel grids [SSN*22, RHZ*24], meshes [SAA*24], and signed distance functions (SDFs) [MCST22, HLHF22, CLT*23], etc. In this work, given our target task, i.e. volumetric design, we opted to use a simple and plain explicit dense voxel grid representation, as it directly corresponds to the representation of volume in 3D space. It shares a similar form with 2D pixels, facilitating the adoption of various"}, {"title": "3D Generative Models", "content": "Numerous 3D generative models build on various frameworks, including generative Variational Auto Encoders (VAEs) [DMVPSC19, GJvK20, SHR23], Generative Adversarial Networks (GANs) [LYF17, WSH*18, WZ22], Graph Neural Networks (GNNs) [ZKF23, BDEW], and Auto Regressive (AR) models [MCST22, SAA*24]. In [DMVPSC19], within the architectural domain, the VAE introduced in [Kin13] is employed for the generation, manipulation, and form-finding of building typologies represented as voxelised wireframes. Another work with architectural design use cases is [SHR23], which employs a Conditional Variational Auto Encoder (cVAE) trained on voxelised data, coupled with corresponding operative verbs (a taxonomy of simple geometrical operations) in a prototype of a generative volumetric design tool.\nOur pipeline's first network is built upon [VDOV*17] and inspired by a later proposed method VQGAN [ERO21]. [VDOV*17] first proposed a method to learn quantised and compact latent representations for 2D images using the Vector-Quantized Variational AutoEncoder (VQVAE), and later introduced a hierarchical version [RVdOV19]. Followingly, VQGAN [ERO21] learns autoregressive generation over the discrete VQVAE representations by integrating a mask generative Transformer through discrete semantic codebooks. Our work utilises VQGAN's network design as its backbone and extends it to the domain of 3D voxelised shapes in architectural design. It also integrates a new loss term affecting the cohesiveness and integrity of the generated 3D house models."}, {"title": "3D Diffusion Models", "content": "Diffusion Probabilistic Models (DPMs) [HJA20], also known as Diffusion Models, have currently arisen as a powerful family of generative models. In the fields of computer graphics and vision, several recent studies have adopted diffusion models for generative 3D modelling [HLHF22,VWG*22,LDZL23, S\u00d6LH23,SCP*23,RHZ*24,WLY*24]. Existing approaches mostly train a VQ-VAE on a 3D representation like voxel grids, SDFs, and Triplanes, and then employ a diffusion model in the learned latent space. NWD [HLHF22] encodes 3D shapes by building a compact wavelet representation with a pair of coarse and detail coefficient volumes through TSDF decomposition. It then formulates two networks upon on DPMs to generate shapes in the form of coarse and detail coefficient volumes for generating shapes and reconstructing fine details, respectively. LION [VWG*22] uses a VAE framework with hierarchical DDMs in latent space that combines a global shape latent representation with a point-structured latent space, and integrates it with Shape as Priors (SAP) [PJL*21] for mesh generation. Diffusion-SDF [LDZL23] presents a two-stage pipeline comprising a patch-wise autoencoder and a voxelised diffusion model to generate voxelised SDFs conditioned on texts. On the other hand, NFD [SCP*23] is another diffusion-based 3D generative model that, instead of voxel grids, converts the occupancy field of an object in a set of axis-aligned triplane feature representations."}, {"title": "3. Dataset", "content": "The current state of available 3D datasets such as BuildingNet [SNL*21], Houses3K [PCN*20] or 3DBAG [BAG24] presents limitations for architectural design applications. They are either inconsistent in labelling or lack a consistent degree of detail with modelled interiors for generating detailed building design [WDL*23]. Comprehensive 3D datasets, especially those depicting both exterior and interior details of buildings, are therefore particularly valuable as they provide a complete representation of the spatial relations architects and designers envision.\nTo this end, we adopt our own collected dataset consisting of 1500 3D house models, including both the original and augmented samples that depict prominent architectural houses with fully modelled interiors and material labels, as showcased in Figure 2. The dataset aims to address the aforementioned significant challenge in the literature: the lack of high-quality publicly available 3D architectural datasets that fully capture spatial relations. The dataset was curated specifically to overcome these limitations by offering detailed 3D architectural models that capture the nuanced relationships between spatial configurations, structural integrity, and materiality-critical elements in the architectural design process. Additionally, a key aspect of this dataset is that it consists of architectural precedents-pre-existing buildings/houses or projects that serve as inspiration for new designs. These precedents provide both explicit and implicit knowledge on how past design challenges related to spatial organisation and aesthetics [Cro82], making the dataset a valuable resource for 3D generative models for learning purposes. The original samples in the dataset were collected through structured research-led teachings at University of Innsbruck, where architecture students were tasked with modelling existing houses as part of the curriculum in both past and ongoing courses.\nFor detailed information on the augmentation techniques used to expand the dataset, please refer to the Supplementary Material."}, {"title": "4. Method", "content": "The framework of our voxel-based 3D pipeline generates designs for both early-stage design ideation and subsequent geometric detailisation of the designs. It operates in two main stages: A base network for coarse model generation and hierarchical upsampling, augmenting the design with fine geometric details. The following major procedures define the pipeline:\n(i) Data Preparation, to construct the ground-truth data for the pipeline's two stages, and defining the input/output of each stage using a collected dataset of 3D house models.\n(ii) A Base Network, generating coarse voxelised models by leveraging a patch-wise quantisation step along with a Transformer-"}, {"title": "4.1. Data Preparation", "content": "Prior to voxelising our data samples, as part of pre-processing, we filter out openings in the house dataset geometries, i.e., parts labelled as doors and windows, to focus solely on the mass-void relationship (closed and open spaces). Additionally, we remove parts labelled as ground to attend exclusively to the structural components of the models. We then concentrate solely on the dense binary voxelisation of data samples, where each voxel contains a value of either 0 or 1, representing an empty (void) or occupied voxel (mass), respectively, while leaving the integration of materials and semantics for future work.\nIn order to create ground-truth for our base 3D Voxel VQGAN network, we take the models in their original 1:1 scale, limit ourselves to the design space of 48m\u00b3, and voxelise only the regions of the models falling within this space at a resolution of 643, resulting in a voxel size as small as 75cm. In total, we end up with 1500 voxelised models ready for training our Base Network.\nTo build the ground-truth voxel hierarchy of local chunks for training our Hierarchical Upsampling Networks, we sample 100 approximately equidistant points using Poisson disk sampling [Yuk15] on the surface of each model from dataset. Then, similar to the data preparation for training for the 3D Voxel VQGAN, we crop chunks at a size of 6m x 6m x 6m, centred at the sampled points, and voxelise them into three different resolutions: 83, 163 and 323, as illustrated in Figure 4. This results in two pairs of 12500 pair of coarse and fine chunks to train each hierarchy level of our 3D upsampling c-DDPMs. The intuition behind this is that since the dataset models exhibit interior and exterior structures, each chunk is geometrically self-contained. Training the upsampling networks on local chunks alleviates the need for high computational resources due to their much lower resolutions."}, {"title": "4.2. Base Network: 3D Voxel VQGAN", "content": ""}, {"title": "4.2.1. Learning Quantised Voxel Grid Embeddings", "content": "We extend the original 2D VQGAN [ERO21] to the 3D Voxel VQ-GAN by making three key modifications: (i) we utilise 3D volumetric residual blocks as the backbone of its encoder E and decoder G, extending them to 3D; (ii) we replace the original discriminator with a 3D PatchGAN [IZZE17], enforcing fidelity at the scale of local volumetric patches; and (iii) we incorporate a 2.5D perceptual loss term, pushing the network to learn both local fine-grained details and global arrangement of voxels within a grid.\nGiven an input model, represented via a binary voxel grid $v \\in {0,1}^{R \\times R \\times R}$, we have\n$z=VQ(E(v)), and \\hat{v} = G(z),$\t\t(1)\nwhere $z \\in R^{r \\times r \\times r \\times D}$ is the latent feature map of E with dimension r < R and VQ is the quantisation step which maps each of the $r^3$ D-dimensional vectors of the latent feature map to their nearest vector in the discrete codebook $C = {z_k \\in R^{D}}_{k=1}^{K}$. In other words, the original input voxel grid is encoded into D-dimensional embeddings each determining a $r^3$ local patch in the original voxel grid.\nThe original vanilla VQGAN is trained by optimising a vector quantised codebook during autoencoding with the discriminator optimised to differentiate between real and reconstructed inputs. Denoting the reconstructed voxel grids with $\\hat{v}$, the overall loss of vanilla VQGAN can be written as:\n$\\begin{aligned} \\mathcal{L}_{V Q G A N}=\\left|\\mid v-\\hat{v} \\mid\\right| \\mid_2^2 &+\\left|\\mid s g[E(v)]-z_q \\mid\\right| \\mid_2^2 + \\left|\\mid s g[z_q]-E(v) \\mid\\right| \\mid_2^2 \\\\ &+[\\log D(v)+\\log (1-D(\\hat{v}))], \\end{aligned}$\t(2)\nwhere sg[.] denotes the stop-gradient operation and $\\mathcal{L}_R$, $\\mathcal{L}_C$, and $\\mathcal{L}_D$ are the reconstruction, the so-called \"commitment\", and the discriminator losses, respectively. However, when extending to 3D"}, {"title": "4.2.2. Learning the Composition of Voxel Grids with Transformers", "content": "With E and G available, we can now represent the voxel grids in terms of the sequence of codebook-index tokens of their encodings. The quantised encoding of a voxel grid v given by $z=V Q(E(v)) \\in R^{r \\times r \\times r \\times D}$, is equivalent to sequence $s \\in {0,\\dots ,K}^{r \\times r \\times r}$ of indices from the codebook obtained by replacing each code by its index in the codebook C:\n$s_{u v w}=k$ such that $(z)_{u v w}=z_k$.\t(5)\nBy mapping indices of a sequence s back to their corresponding codebook entries, $z = (z_{s_{u v w}})$ is readily recovered and decoded to a voxel grid $v = G(z)$.\nThus, after choosing some ordering of the indices in s of samples in the training dataset, voxel grid generation can be formulated as an autoregressive next-index prediction task. More precisely, given indices $s_{<i}$, a transformer can be learned to predict the distribution of possible following indices, i.e. $p(s_i|s_{<i})$ to compute the likelihood of the full representation $p(s) = \\prod_i p(s_i|s_{<i})$. Essentially, this allows us to directly maximise the log-likelihood of the data representations:\n$\\mathcal{L}_{3 \\text{DVoxelVQGAN\\_TRANS }}=E_{v \\sim p(v)}[-\\log p(s)].$\t\t(6)\nTo this end, we prefix the sequences with a start of sentence (SOS) token and employ a decoder-only transformer architecture, specifically a miniature version of original GPT model [Rad18]. Our customised architecture of this model begins with an embedding layer for encoding (indices) tokens and their corresponding positions and then the features pass through a stack of multi-headed self-attention layers, followed by a small-sized MLP trained to predict the codebook index of the next embedding in the sequence."}, {"title": "4.2.3. Regularising Voxel Grids", "content": "The raw voxel grid outputs of the 3D Voxel VQGAN may contain artefacts, such as voxels either sticking out from the model's surface or floating in space. To clean up the voxel grid, we devise an algorithm that considers the connectivity between mass (1) and"}, {"title": "4.3. Hierarchical Voxel Upsampling Networks: 3D c-DDPMS", "content": "Our method for refining dense voxel grids from a prior step into higher-resolution grids trains a hierarchy of 3D conditional Denoising Probabilistic Diffusion Models (3D c-DDPMs) on a hierarchy of coarse-to-fine chunks extracted from the raw dataset samples. The hierarchy comprises L levels of coarse-to-fine chunks $\\mathcal{C}=\\{C_1,\\dots,C_L\\}$, where each chunk in the finer level $C_{l+1}$, is strictly contained within its corresponding chunk in the coarser level $C_l$ for $l=1,\\dots L-1$, and the finest chunks, $C_L$, contain the maximum amount of detail.\nThe typical denoising probabilistic diffusion is a purely generative model that takes no conditioning input and consists of a forward and a reverse process. During the forward phase, a sample is corrupted by adding noise in a step-wise manner, where the noise-adding process forms a Markov chain. Contrarily, the reverse phase recovers the corrupted data at each step with a denoising model.\nForward Process: Given a sample $x_0 \\sim q(x_0)$, the forward Markov process $q(x_{1:T} \\mid x_0)=\\prod_{t=1}^T q(x_t \\mid x_{t-1})$ corrupts the sample into a sequence of increasingly noisy samples: $x_1,\\dots,x_T$, where t refers to the diffusion step. Generally, the noise follows a Gaussian distribution: $q(x_t|x_{t-1}) = N(x_t|x_{t-1};\\sqrt{1 - \\beta_t}x_{t-1}, \\beta_tI)$ where $\\beta_t$ is the scale of the added noise at step t.\nReverse Process: The reverse Markov process attempts to recover the last-step sample with a parametrised denoising model $p_\\theta(x_{t-1}|x_t)$. When the noise follows a Gaussian distribution, the parameterised distribution becomes\n$p_\\theta(x_{t-1}|x_t) = N(x_{t-1}|\\mu_\\theta(x_t, t), \\sigma_\\theta(x,t))$\t\t(7)\nwhere $\\mu_\\theta(x,t)$ and $\\sigma_\\theta(x,t)$ are modelled with neural networks.\nThe vanilla DDPM functions as an unconditional generative model. However, in our case, coarse chunks $C^l$ can provide additional context or guidance during the reverse denoising process. To this end the underlying U-Net architecture [RFB15] is modified slightly by utilising 3D residual blocks and changing the network architecture to accept three inputs: the coarse chunks $C^l$, fine chunks $C^{l+1}$, and time step T. This is achieved by adding an additional layer, which concatenates the subdivided coarse and the fine voxel grid in the input layer. In our 3D conditional DDPM setup, the forward process remains almost the same, within which Gaussian noise is progressively added to the fine chunks $C^{l+1}$, at times $t \\in \\{1,\\dots,T\\}:$\n$q(C_{t}^{l+1}|C_{t-1}^{l+1}):=N(C_{t}^{l+1};\\sqrt{1-\\beta_t}C_{t-1}^{l+1},\\beta_tI)$.\t\t\t(8)\nThe purpose of subdividing the coarse chunks is to match the resolution of the fine ones, thereby guiding the denoising process"}, {"title": "4.4. Implementation Details", "content": "We implement our networks using TensorFlow and run all experiments on a GPU cluster with two Nvidia A100 GPUs. In learning the VQGAN's codebook vectors, we set the resolution of the latent feature map r = 8, with the codebook constituting K = 512, D = 128-dimensional vectors. Our 3D PatchGAN discriminator has a receptive field of RD = 8. As for the loss weight factors, we set $\\alpha = 100$, $\\beta = 10$, according to our ablation study (refer to supplementary Material for further details). We set $\\gamma$ and $\\delta$ to 0.25 and 0.1, respectively, following the vanilla 2D VQGAN [ERO21] implementation. The base network is trained utilising the Adam optimiser [Kin14] with the learning rate of $10^{-4}$, while simultaneously the discriminator is trained with the learning rate of $10^{-6}$ for 128 epochs with an effective batch size of 4. For our transformer, we use a miniature GPT model equipped with a context window of up to 512 embeddings. We use AdamW optimiser [Los17] with cosine annealing strategy [LH16] with minimum and maximum learning rates of $10^{-5}$ and $2.5 \\times 10^{-4}$, for 128 epochs with a batch size of 32.\nFor the hierarchical upsampling networks (3D c-DDPMs), we follow the same setup for training the vanilla diffusion model. We use Adam optimiser with the learning rate of $10^{-4}$, for 256 epochs with a batch size of 32. For inference, we use DDIM [SME20] as our sampler with the number of timesteps T = 100.\nFor details of the networks' architectures, please refer to the Supplementary Material."}, {"title": "5. Results and Validation", "content": "In this section, we first present several example applications of our pipeline (Section 5.1) and subsequently evaluate various aspects of the pipeline in comparison with state-of-the-art methods. (Section 5.2)."}, {"title": "5.1. Example Applications", "content": "We conduct comprehensive experiments to showcase different applications of ArchComplete. We draw inspiration from Genetic Algorithm (GA) operators and apply them to codebook tokens to generate interpolations (blendings) and variations of input house models to facilitate ideation based on existing 3D models. After that, using the autoregressive model trained to predict the learned codebook indices, we demonstrate both unconditional and two conditional 3D generation tasks. For the conditional case, we utilise an existing 3D voxel creation interface to demonstrate the pipeline's application to plan-drawing completion, a newly formulated architecture-specific use-case of ArchComplete."}, {"title": "5.1.1. GA-Inspired Interpolation & Variation", "content": "The quantisation process transforms the input model into a discrete set of index tokens corresponding to specific codebook vectors. Consequently, the challenge of shape interpolation or shape variation can be reframed as a well-defined problem in this discrete space, which can be effectively addressed using two genetic algorithm (GA) operators: crossover and mutation.\nBlending. Consider two sets of codebook token sequences z1 and z2 corresponding to encoding of two different input models. Using a random uniform crossover, we create an interpolated token sequence Zint by randomly selecting each token from z1 or z2 randomly with equal probability, i.e. 0.5. This essentially entails mixing the tokens corresponding to embeddings of local patches in the two models. When decoded using G, Zint produces an interpolated shape inheriting characteristics from both original input models.\nVariation. Similarly, variations of each input model can be generated by applying mutation operators to its token sequence. The mutation alters certain tokens, resulting in distinct model variants upon decoding. To this end, we use swap mutation, which randomly selects and swaps two tokens iteratively. Given a sequence of codebook index z, we repeat this process 128 times-one-fourth of the sequence length-as we found this to introduce diversity without heavily distorting the input."}, {"title": "5.1.2. Unconditional Synthesis", "content": "Given a transformer trained to autoregressively predict token sequences, generating novel, unseen outputs without any given prior context-aka unconditional synthesis becomes a straightforward task. To this end, as a convention in the ordering of generation, we start with the SOS token and iteratively sample new tokens, each conditioned on the previously generated tokens. This process continues until the sequence reaches its maximum length, which is equal to the size of the output feature map from the encoder, i.e. $r \\times r \\times r$ which is equal to $512 = 8 \\times 8 \\times 8$ (see Section 4.2.1 and 4.2.2). Afterwards, by retrieving the corresponding codebook vectors and performing quantisation, we use the decoder to generate the novel house models (see Figure 7). The unconditionally synthesised outcomes illustrate the model's capacity to suggest architectural diverse and coherent houses which differ from the training data."}, {"title": "5.1.3. Conditional Synthesis", "content": "Thanks to the autoregressive structure of our transformer, conditioning can be achieved by simply providing a partial sequence"}, {"title": "5.1.4. Multi-Resolution Geometry Detailisation", "content": "To further support the designer with subsequent steps in volumetric design workflows, as outlined in Section 1, we utilise the hierarchy of 3D c-DDPMs to incrementally upsample the initial resolution of the output voxel grids of 643 to first 1283 and then 2563, to augment the generated shapes with fine structures and geometric details. Since ArchComplete is scale consistent, this increase in resolution reflects a change of the true voxel size of 75cm to 18,75cm, indicating the degree of detail to which the upsampling process predicts new spatial elements and features."}, {"title": "5.2. Evaluation and Comparison", "content": "We thoroughly evaluate our network design and choice of parameters, and compare our results quantitatively and qualitatively in both unconditional and conditional synthesis tasks to two of the state-of-the-art methods.\nWe compare ArchComplete to the 3D generative models SDFusion [CLT*23] and NWD [HLHF22], due to slight similarities in their geometric representations and network designs. Both methods also use dense voxel grids to represent the data as Truncated Signed Distance Fields (TSDFs), where individual voxels, instead of binary occupancy values, store scalar distances to the nearest surface. Like our base network's quantisation step, SDFusion leverages a 3D-variant of VQVAE to compress the 3D shape into a"}, {"title": "5.2.1. Unconditional: Shape Novelty Analysis", "content": "Evaluating the unconditional synthesis of 3D shapes presents challenges due to the absence of direct ground truth correspondence. To address this, we adopt a two-fold evaluation approach: first, we assess the novelty of our generated shapes, and second, we measure our method's performance in terms of shape quality to the other two existing works.\nFirst, we evaluate whether our method can generate novel shapes in unconditional synthesis tasks to ensure that the model is not merely retrieving existing shapes from the training dataset. Following the methodology in previous works [EMS*23, SAA*24], we generate 256 shapes using our base network. For each generated shape, we identify the top five nearest neighbours from the training set based on Chamfer Distance (CD) and plot the distribution of generated house models and their closeness to training distribution. As depicted in Figure 11, the CD distribution reveals that our method not only covers shapes in the training set, indicated by low CD values, but also successfully generates novel and realistic-looking house models (see Figures 8 and 12), as indicated by high CD values.\nNext, to assess the model's performance in comparison to state-of-the-art methods, we generate an equal number of shapes using two other methods from the literature and assess them under"}, {"title": "5.2.2. Conditional: Shape Fidelity and Diversity", "content": "We also quantitatively and qualitatively assess and measure the performance of our approach on the task of shape completion both in terms of fidelity and diversity. To this end, given partial input shapes, we generate k complete shapes. To measure the completions' fidelity given a partial input, we compute the average of Unidirectional Hausdorff Distance (UHD) to the k generated shapes. Correspondingly, given the k generated results for each shape, we compute the average CD to other k - 1 shapes; The sum of the average distance among k generation indicates the completion diversity and is denoted as Total Mutual Difference (TMD). We use k = 10 in our evaluations and report these metrics on our dataset, as shown in Table 2."}, {"title": "5.2.3. Runtime and GPU Memory", "content": "Generally, one critical limitation when upsampling dense voxel grids to higher resolutions, e.g., 2563(512\u00b3), is the long runtime and the (severe) usage or overflow of GPU memory. Our proposed hierarchical upsampling networks process the input voxel grids locally, i.e., chunk-by-chunk, hence drastically reducing runtime and memory requirements. This is achieved by unfolding the coarse 643 voxel grids to 83 and 163 chunks, upsampling them to 163 and 323, that when folded back from 1283 and 2563 fine grids, respectively.\nTo validate our method's inference time and GPU footprint, we adopt the generator from the upsampling network architectures used in DECOR-GAN [CKF*21] as a baseline. In their original implementation, the generator is designed to upsample X\u00b3 coarse voxel grids to (4X)3 while a style latent code is fed into the generator to condition the refinement to condition the upsampling on specific geometric styles. To ensure a fair comparison, we remove the final upsampling layer and adjust the input and output resolutions to (X)\u00b3 and (2X)3, respectively, and drop the conditioning layers. The results are presented in Table 3, which shows that our model requires much less GPU memory and runs faster as the output resolution increases. Another key advantage is a direct benefit from additional supervision at intermediate-level resolution, which leads to higher-quality generation of finer scale geometric details."}, {"title": "6. Conclusion", "content": "We presented ArchComplete", "follows": "We learn a vocabulary of local volumetric patch embeddings over a distribution of 3D voxelised house models through our tailored 3D Voxel VQGAN \u2014 with a new 2.5D perceptual loss term designed to capture the inherent complexity of input models over which a transformer is trained to synthesise novel 3D house models in an autoregressive fashion. Moreover", "case": "synthesising 3D house models from 2D plan drawings as a form of shape completion. Furthermore", "1": 1}]}