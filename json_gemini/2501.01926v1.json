{"title": "Mitigating Hallucination for Large Vision Language Model by Inter-Modality Correlation Calibration Decoding", "authors": ["Jiaming Li", "Jiacheng Zhang", "Zequn Jie", "Lin Ma", "Guanbin Li"], "abstract": "Large vision-language models (LVLMs) have shown remarkable capabilities in visual-language understanding for downstream multi-modal tasks. Despite their success, LVLMs still suffer from generating hallucinations in complex generation tasks, leading to inconsistencies between visual inputs and generated content. To address this issue, some approaches have introduced inference-time interventions, such as contrastive decoding and attention rectification, to reduce overreliance on language priors. However, these approaches overlook hallucinations stemming from spurious inter-modality correlations. In this paper, we propose an Inter-Modality Correlation Calibration Decoding (IMCCD) method to mitigate hallucinations in LVLMs in a training-free manner. In this method, we design a Cross-Modal Value-Enhanced Decoding(CMVED) module to alleviate hallucination by a novel contrastive decoding mechanism. During the estimation of distorted distribution, CMVED masks the value vectors associated with significant cross-modal attention weights, which address both uni-modality overreliance and misleading inter-modality correlations. Additionally, a Content-Driven Attention Refinement(CDAR) module refines cross-modal attention weights, guiding LVLMs to focus on important visual content. Experimental results on diverse hallucination benchmarks validate the superiority of our method over existing state-of-the-art techniques in reducing hallucinations in LVLM text generation. Our code will be available at https://github.com/lijm48/IMCCD.", "sections": [{"title": "1. Introduction", "content": "With advances in computational power and data availability, large language models [1, 2, 9, 30, 31] (LLMs) have achieved significant progress in language understanding, generation, and reasoning. Large vision-language models [3\u20136, 24, 25] (LVLMs) further extend large language models (LLMs) to vision-language tasks, demonstrating impressive performance across a range of applications, including image captioning and visual question answering. Despite these advancements, LVLMs suffer from the issue of hallucinations while generating the response, in which LVLMs generate textual content that is semantically coherent but inconsistent with ground-truth objects in the given image, hindering their reliable application.\nNumerous research efforts have been dedicated to tackling the hallucination of LVLMs. Recently, contrastive decoding [21, 27] has emerged and become one of the mainstream hallucination mitigation methods. It reduces the occurrence of the hallucination by penalizing the hallucinated outputs via distribution contrasting. Specifically, con-"}, {"title": "2. Related works", "content": "Large Vision-Language models. Large vision-language models (LVLMs) equip large language models (LLMs) with the capability to perceive and understand both textual input and visual input data. The most common practice to achieve LVLMs is to integrate pre-trained LLMs with additional visual encoders and cross-modal interfaces for cross-modal fusion. For example, the LVLMs [3\u20136] represented by InternVL [6] introduce linear projections to map the image features from the vision encoder to the token space of LLMs. While LLaVA series [24, 25] develop a vision-language model by connecting a vision encoder and an LLM with a projection layer and fine-tuning their model based on their generated instructional vision-language data. Instead, another line of work [19, 36, 37, 45] such as BLIP-2 [19] adopts the design of query transformer as the interface between vision encoders and LLMs. Dai et al. [10] further proposes InstructBLIP, which enhances visual comprehension through vision-language instruction tuning and introduces instruction-aware visual feature extraction to query transformers to enable context-relevant processing of visual content based on the given instructions. Despite these advancements in LVLMs, these models continue to struggle with severe hallucination issues, where generated content misaligns with the visual inputs. Our work aims to mitigate the hallucination of current LVLMs and facilitates the application of LVLMs in various domains.\nHallucination in LVLMs. The hallucination problem was first discovered in the field of LLMs. It refers to the misalignment between the generated content from LLMS and real-world facts (namely the factuality hallucination) or user instruction (namely the faithfulness hallucination). Built upon LLMs, LVLMs also suffer from the hallucination that manifests as a misalignment between generated text and the visual input. Various approaches have been proposed to tackle this issue, including the works from the perspective of constructing additional robust training instruction [15, 23, 39, 42], reinforcement learning with human/AI feedback [12, 16, 20, 29, 40, 41], or model structure enhancement [43], etc. Despite achieving impressive results, these methods always require extensive data collection or additional fine-tuning of LVLMs, making them both computationally and labor-intensive. An-"}, {"title": "3. Methodology", "content": "Modern LVLMs generally consist of a visual encoder, a cross-modal interface, and a language decoder. The visual encoder is typically adapted from a pre-trained vision model, while the language decoder is derived from a pre-trained LLM. During inference, LVLMs process both visual and textual inputs to iteratively generate the next token in the response sequence. Specifically, the visual encoder first encodes the input visual content into visual features. The cross-modal interface then maps these visual features to the input space of the language decoder to generate the image tokens $X = \\{x_i\\}_{i=0}^{n}$. Here $x_i$ is the image token corresponding to the i-th patch of the image and n is the number of patches. Besides, the text input is mapped to the text tokens $T = \\{t_i\\}_{i=0}^{m}$, where $t_i$ is i-th text token and m is the number of text tokens. The image tokens and text tokens are then concentrated to generate the input tokens $[T_{0:m_b}, X, T_{m_b+1:m}]$ for the language decoder. $T_{0:m_b} = \\{t_i\\}_{i=0}^{m_b}$ is the first $m_b$ tokens (system prompt) in T and $T_{m_b+1:m} = \\{t_i\\}_{i=m_b+1}^{m}$ is the remaining part of T.\nLanguage Decoder Forwarding. The multi-head self-attention layer is commonly used in the language decoder. During the forward of the language decoder, the input tokens are initially transformed into input embeddings by the embedding layer, which then serves as hidden states for the first self-attention layer. For each input sample, every head within a self-attention layer maps the hidden states to queries $Q \\in \\mathbb{R}^{(n+m)\\times d}$, keys $K\\in \\mathbb{R}^{(n+m)\\times d}$, and values $V\\in \\mathbb{R}^{(n+m)\\times d}$ by linear transformation. Here (n + m) is the sequent length of input tokens and d denotes the hidden dimensions. Then, the attention matrix $A\\in \\mathbb{R}^{(n+m)\\times(n+m)}$ is estimated based on Q and K by,\nNext Token Prediction. The language decoder parameterized by \u03b8 in LVLMs generates the t-th output token based on the text tokens T, image tokens X, and the previously generated tokens $Y_{<t}$ in an auto-regressive manner. This process can be formulated as follows,\nContrastive Decoding for LVLMs. Contrastive Decoding [18] is an effective training-free intervention strategy to suppress hallucination. The core idea is to take the extra manually crafted distorted textual or visual content as input and then contrast the output distribution derived from the original input against the distorted counterpart, which alleviates the overreliance on individual modal priors and subsequently reduces the occurrence of the hallucination. Formally, contrastive decoding can be formulated as:\n3.2. IMCCD for LVLMs Hallucination Mitigation\nIn this section, we introduce the Inter-Modality Correlation Calibration Decoding (IMCCD) framework to mitigate the hallucination in LVLMs. IMCCD comprises two key designs, namely Cross-modal Value-enhanced Decoding (CMVED) and Content-Driven Attention Refinement (CDAR), alleviating both the spurious inter-modality correlations and uni-modality over-reliance. The overview of our method is illustrated in Figure. 2."}, {"title": "3.1. Preliminaries", "content": "Modulation.To this end, we propose cross-modal value-enhanced decoding (CMVED), which performs inter-modality correlation correction via the distortion on the level of cross-modal attention.\nOur key insight is to selectively suppress the value vectors corresponding to high values in attention logits and exacerbate the hallucination from the inter-modality interaction, thus subsequently alleviating it with contrastive decoding. Concretely, from attention logits $A^l \\in \\mathbb{R}^{(n+m)\\times(n+m)}$, we isolate the cross-modal segment, which is denoted as $A_{cross} = A^l[m_b + n : m + n, m_b : m_b + n]$. $A_{cross}$ specifically represents the attention weights between image tokens and subsequent text tokens, capturing the inter-modality correlations within the self-attention layer. Based on such cross-modal attention weight, we aim to estimate the distribution that favors the hallucination, which afterward is used to mitigate the hallucination from spurious inter-modality correlations. To accomplish this, CMVED first generates a mask that selectively identifies the prominent attention weights based on their magnitude,"}, {"title": "3.2.1 Cross-Modal Value-Enhanced Decoding", "content": "Modulation. The overview of$A' = \\frac{Q K^T}{\\sqrt{d}}$,\n$A = softmax(A')$,  (1)where $A'$ is the attention logits before softmax. The atten-tion matrix A estimates the relevance of each token, whichis used to reweight the values V from each token to obtainthe attention output $O \\in \\mathbb{R}^{(n+m)\\times d}$,O = AV,(2)The attention output O is subsequently fed into the fullyconnected feed-forward network (FFN), whose output thenserves as the input hidden states for the subsequent multi-head self-attention layer.Yt \u223c P\u03b8(Yt |T, Attn(O|T, X, Y<t)),\nlogito(Yt | Attn(O|T, X, Y<t)),(3)where $p_\\theta()$ and $logit()$ denote the output probabilityscores and logits of language decoder, respectively. Attn isthe self-attention process by Eq.1 and 2. The output tokenYt is subsequently concentrated with previously generatedtokens to serve as input for the next step in the token generation sequence.   (7)where $\u03bc(\u00b7)$ represents the mean operation and $I$ isthe indicator function. The binary mask $M_{cross} \\in\\mathbb{R}^{(m-m_b)\\times(n)}$ adaptively indicates the selected signif-icant attention weights for the cross-modal segment. Wesubsequently pad the $M_{cross}$ to a global mask $M\\in[\\mathbb{R}^{(n+m)\\times(n+m)}$ which applies to all attention weights in $A$with zero padding. Specifically, $M[m_b + n : m + n, m_b :m_b + n]$ is set to the $M_{cross}$, ensuring that the cross-modalpart is accurately masked in the global context.\nWith this global mask, CMVED then distorts the cross-modal segment of self-attention. While recent approachestypically adapt token-wise pruning image tokens, CMVEDmasks the value vector $V$ to $\\mu(V)$ by dim-wise mean operation to exacerbate the hallucination. With this process, thecontent of the value vector is teased apart. CMVED thenestimates the attention output by performing the weightedsum between the origin value vector $V$ and distorted valuevector $\u03bc(V)$ guided by both the mask M and attentionweights A. Specifically, CMVED modifies the processEq. 2 by the following process,"}, {"title": "3.2.2 Content-Driven Attention Refinement", "content": "Although CMVED reduces hallucinations arising from the cross-modal interaction, MLLMs still face challenges with visual content retention. Specifically, with the position embeddings, the text tokens will be biased to pay more attention to the latter part of the image visual tokens, while overlooking the initial visual tokens, leading to more hallucinations. This is achieved by refining the atten-tion weights with content-driven attention weights, which are computed by normalizing the position indices of all image tokens to a uniform value.\nConcretely, in the(8)With the above equation, CMVED distorts the value vec-tors corresponding to the significant cross-modal attentionweights while keeping the value vectors of the remainingattention part unchanged, which suppresses important inter-modality correlations from cross-modal attention. Similarto Eq. 6, the final contrastive decoding process is performedby the following equations,"}]}