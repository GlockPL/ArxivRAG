{"title": "Studying the Impact of TensorFlow and PyTorch Bindings on Machine Learning Software Quality", "authors": ["HAO LI", "GOPI KRISHNAN RAJBAHADUR", "COR-PAUL BEZEMER"], "abstract": "Bindings for machine learning frameworks (such as TensorFlow and PyTorch) allow developers to integrate a framework's functionality using a programming language different from the framework's default language (usually Python). In this paper, we study the impact of using TensorFlow and PyTorch bindings in C#, Rust, Python and JavaScript on the software quality in terms of correctness (training and test accuracy) and time cost (training and inference time) when training and performing inference on five widely used deep learning models. Our experiments show that a model can be trained in one binding and used for inference in another binding for the same framework without losing accuracy. Our study is the first to show that using a non-default binding can help improve machine learning software quality from the time cost perspective compared to the default Python binding while still achieving the same level of correctness.", "sections": [{"title": "1 INTRODUCTION", "content": "The rapidly improving capabilities of Deep Learning (DL) and Machine Learning (ML) frameworks have been the main drivers that allow new intelligent software applications, such as self-driving cars [27, 61] and robotic surgeons [18, 77, 82]. These intelligent software systems all contain components that integrate one or more complex DL and/or ML algorithms. Fortunately, over the past decade, the need for coding these ML and DL algorithms from scratch has been largely eliminated by the availability of several mature ML frameworks and tools such as TensorFlow [1] and PyTorch [63]. These frameworks provide developers with a high-level interface to integrate ML functionality into their projects. Using such ML frameworks has several advantages including"}, {"title": "2 BACKGROUND", "content": "Machine learning frameworks are software libraries that provide ML techniques to developers for the development and deployment of ML systems. Most popular ML frameworks are supported by large companies such as Google and Facebook [4]. As shown in Figure 1, an ML framework provides interfaces to define the structure of a model, train the defined model using a selected optimizer, and save the trained model for later use. In addition, developers can deploy the trained models to the production environment by loading a saved (or pre-trained) model and performing inference. ML frameworks can load a pre-trained model using (1) the model parameters (e.g., weights and hyperparameters) or (2) serialization. If only the model parameters are saved, developers first have to define the model structure before they can load the stored parameters into the defined model. When loading a serialized model, the ML framework can recreate the model from the saved file automatically since it contains both the structure and the weights of the pre-trained model.\nModern ML frameworks, such as TensorFlow and PyTorch, have been built upon a foundation that leverages parallel processing devices like GPUs. GPUs have proven to be highly efficient for tasks that demand parallel computation, especially in the realm of ML. Their architecture is inherently designed to handle multiple tasks simultaneously, allowing for massive parallelism. However, one significant characteristic of GPU computations that needs emphasis is their asynchronous nature. When a task is dispatched to a GPU, it does not always execute immediately. Instead, it often gets scheduled in a queue. Consequently, a CPU might continue with its tasks believing that a GPU job is complete when, in fact, it has not even started. This asynchronous behaviour allows GPUs to optimize task execution but also necessitates careful synchronization when precise timing or task ordering is crucial."}, {"title": "2.2 Bindings for the ML frameworks", "content": "Python is the most popular programming language for ML applications [4, 60], but developers in other languages also have the need for using ML algorithms. Developers might choose an existing ML framework in their preferred language or they have to create a new one from scratch (which requires a large amount of work and is error-prone). Another alternative is to use a binding in their preferred language, which provides interfaces to the functionality of an existing ML framework in the language of the binding [47].\nAs shown in Figure 1, bindings access the functionality of the ML framework through foreign function interfaces (FFIs) without recoding the library. FFIs bridge the gap between programming languages, allowing developers to reuse code from other languages. For example, TensorFlow's Rust binding uses the FFI provided by the Rust language to access TensorFlow functionality. Since the GPU support is provided by the underlying C/C++ computational core of ML frameworks, bindings typically leverage FFIs to access these functionalities. For example, the Python bindings for TensorFlow and PyTorch make use of SWIG (Simplified Wrapper and Interface Generator) and Pybind11 to generate FFIs for its Python binding to tap into the C++ backend which includes the ability to access the GPU. However, the efficiency in leveraging GPU resources may vary among different bindings."}, {"title": "3 STUDY DESIGN", "content": "In this section, we first describe our experimental environment and the studied datasets, models, ML frameworks, and bindings. Then, we discuss how we evaluate the correctness and time cost in the model training and model inference experiments. Finally, we introduce the experimental setup of our study. Figure 2 gives an overview of our study design."}, {"title": "3.1 Environment setting", "content": "We set up our experimental environment on a dedicated laboratory server provided by ISAIC, where we can control the execution of other running tasks. The server runs Ubuntu Linux 20.04 with Linux kernel 5.11.0. We used the CUDA 11.1.74 and cuDNN 8.1.0 GPU-related libraries. The hardware specifications of the server are as follows:\n\u2022 GPU: 2x NVIDIA TU102 [TITAN RTX] (24 GB)\n\u2022 CPU: 3.30 GHz Intel(R) Core(TM) i9-9820X\n\u2022 RAM: 100 GB"}, {"title": "3.2 Studied datasets and models", "content": "Table 1 presents the datasets and models used in this study, specifically pairing each model with the dataset used in the experiments. The datasets we studied are MNIST [45], CIFAR-10 [43], IMDb review [55], and SQUAD [67]. These datasets are widely used as benchmarks in ML research [26, 33, 42, 49, 50, 60, 84, 88]. The models we studied are LeNet [44], VGG [79], LSTM [30], GRU [11], and BERT (the base model) [14] as all of them are typically paired with these datasets in various research domains [2, 12, 21, 26, 31, 33, 75, 83, 84, 90, 91].\nMNIST and CIFAR-10 are datasets for image classification tasks. MNIST contains 70,000 grayscale images of handwritten digits, serving as a benchmark for evaluating classification models like LeNet-1 and LeNet-5. We used the CIFAR-10 dataset, which contains 60,000 colour images of 10 different objects, to train the VGG-16 model. The primary metric for these classification tasks is accuracy, reflecting the proportion of correctly identified images out of the total dataset.\nThe IMDb review dataset is utilized for sentiment analysis (text classification). The dataset contains 25,000 positive and 25,000 negative text reviews of movies. We used it to train the LSTM and GRU models to analyze the sequential nature of text data. Both LSTM and GRU models utilize a recurrent neural network (RNN) structure for handling sequential data, and we integrated a word embedding [13] on the IMDb dataset in our experiments. The performance is measured by accuracy which indicates the model's ability to correctly classify reviews.\nSQUAD is a dataset for the extractive question-answering task. SQuAD contains around 100,000 question-answer pairs, where the questions are posed by crowdworkers on a set of Wikipedia articles and the answer to every question is a text span from the corresponding reading passage. We used SQUAD to train the BERT-base model, leveraging the model's capability in language understanding. The task is to identify the exact text span (i.e., start and end positions) within the given passage that answers a question. The evaluation metric for SQuAD is the exact match score [67], which calculates the percentage of questions for which the model's answer exactly matches the annotated answer."}, {"title": "3.3 Studied ML frameworks", "content": "We study the latest stable versions (at the time of starting our study) of TensorFlow [1] (2.5.0) and PyTorch (1.9.0), since they are two of the most popular ML frameworks. TensorFlow and PyTorch"}, {"title": "3.4 Studied Bindings", "content": "The studied TensorFlow and PyTorch bindings are shown in Table 2. These bindings are all based on the same version of the studied ML frameworks (i.e., TensorFlow 2.5.0 and PyTorch 1.9.0). Notably, TensorFlow and PyTorch both utilize the Python bindings by default. The reason behind selecting bindings in these four software package ecosystems is twofold: (1) Generally, PyPI (Python), npm (JavaScript), and NuGet (C#) are the three most popular software package ecosystems for cross-ecosystem ML bindings [47] and (2) specifically, the Cargo ecosystem (Rust) is popular (according to the number of stars on GitHub) for both TensorFlow and PyTorch. As shown in Table 2, the number of GitHub stars serves as a proxy for the popularity of a project in the software engineering domain [5, 19, 28, 86, 87], with TensorFlow's JavaScript binding being particularly notable. Although the number of stars for C# and JavaScript bindings for PyTorch may appear low, we included these to ensure a fair comparison with TensorFlow bindings in respective ecosystems."}, {"title": "3.5 Correctness evaluation", "content": "During the training process, the correctness is measured in each epoch using the training accuracy which is calculated by $Acctrain = \\frac{Ncorrect}{Ntrain}$, where $Ncorrect$ is the number of correct predictions and $Ntrain$ is the number of data samples in the training set. For the final trained models, we use the test accuracy $Acctest = \\frac{Ncorrect}{Ntest}$ as the evaluation metric for comparison, which is the accuracy on the test set.\nWhen we finish training a model, we use the test accuracy $Acctest$ of this pre-trained model as a reference. Then, we perform inference with a studied binding for the pre-trained model on the test set to obtain the cross-binding test accuracy $Acccross\\_test = \\frac{Ncorrect}{Ntest}$ using that binding. The difference between $Acctest$ and $Acccross\\_test$ is that the inference correctness is measured in the studied binding. For BERT on SQUAD, we use the exact match score [67] instead of accuracy as the metric to evaluate the correctness."}, {"title": "3.6 Time cost evaluation", "content": "The training time cost measures the time spent training a model in seconds. Developers commonly train DL models on GPU rather than CPU since the training can be time-consuming and GPU can considerably shorten the training time [7, 46]. Hence, all model training experiments of bindings for ML frameworks are conducted on GPU and we measure the training time cost on GPU only.\nThe inference time cost measures the time spent for performing inference with a pre-trained model on the test set in seconds. Since developers can deploy pre-trained models to a production environment which supports the CPU or GPU, the inference time cost of a binding is measured on both CPU and GPU."}, {"title": "3.7 Experimental setup", "content": "In this section, we detail our experimental setup with a running example of how we computed the correctness and time cost of LeNet-1 when trained and inferenced using the studied bindings for the studied ML frameworks.\nWe conduct model training experiments for each supported model-dataset pair (as shown in Table 1). For a given model-dataset pair, each binding that supports the model's interface and training features (as shown in Table 3) trains the model from scratch on that dataset. For example, LeNet-1 and MNIST form one model-dataset pair and each supported binding trains LeNet-1 on MNIST independently. We repeat this process for each model-dataset pair in each binding that supports the model. For consistency, we ensure the following across all bindings for a given model-dataset pair:\n\u2022 Model structure. We use interfaces that provide the same functionality in bindings to build up each layer of the studied models. However, not all bindings support model training, as indicated in Table 3. As a result, we do not conduct training experiments with TensorFlow's Rust binding, PyTorch's JavaScript binding, and RNNs in TensorFlow's C# binding.\n\u2022 Training set and test set. We use the provided split of the training set and test set from stud-ied datasets. Before conducting experiments, we perform comprehensive data preprocessing, ensuring that all bindings can work with the same processed data across all experiments."}, {"title": "3.8 Supported features in studied bindings", "content": "Table 3 outlines the supported features by each studied binding:\n\u2022 Training support: A lack of training support in certain bindings means developers might have to use another programming language. This can be inconvenient and result in additional overhead, especially if developers are unfamiliar with the alternative language.\n\u2022 Model interface support: When certain model types are not supported in a binding, developers might still need to switch to another language to train their models.\n\u2022 Model loading approaches: Loading models via serialization provides flexibility as developers don't need to define the model structure. In contrast, loading models via parameters requires the model's structure to be pre-defined. This can lead to challenges, especially when developers try to use pre-trained models.\nFor our training experiments in Section 3.7, certain bindings are exempt due to their limitations: TensorFlow's Rust and PyTorch's JavaScript bindings (which don't support training), TensorFlow's C# binding for RNNs, and all bindings for BERT. We acknowledged the recent inclusion of support for RNNs in TensorFlow's C# binding (aligned with TensorFlow v2.10). However, to maintain consistency in our experimental framework, we focused on TensorFlow version 2.5.0 which is the most commonly supported version of TensorFlow by the studied bindings.\nFor the inference experiments, all bindings are utilized in our work, with the exception of RNNs in TensorFlow's C# and BERT in C# bindings for both ML frameworks. The reason is that the C# bindings can only load models using parameters and lacks support for RNN and BERT interfaces."}, {"title": "4 CORRECTNESS EVALUATION", "content": "Developers can use a binding for an ML framework in their preferred programming language to train a DL model. We want to observe if the DL models trained using a binding for a given ML framework have the same training accuracy as the DL models trained using the ML framework's default Python binding (RQ1). These results can help developers understand if using a binding will achieve the same model accuracy during training and provide the same model performance for the final trained models.\nIn addition, it is important to ascertain if performing inference for these trained models using different bindings for a given framework will impact the accuracy. Pre-trained models have been widely used by the ML community [29, 85] and bindings can help developers to run inference with pre-trained models in different programming languages. Importantly, in high-stakes domains such as medical diagnosis and autonomous driving, accuracy is particularly important when decisions are made by ML systems [62]. Even a slight drop in accuracy can trigger erroneous decisions with serious implications. Hence, it is vital that bindings have the capability to achieve the same accuracy for pre-trained models as with the binding they were trained with. In RQ2, we investigate the cross-binding test accuracy of pre-trained models using the bindings for TensorFlow and PyTorch to understand whether the pre-trained models perform as we would expect them to.\nTogether, the bindings' impact on training correctness and inference correctness will enable us to understand the impact on the correctness of the ML software quality."}, {"title": "RQ1: How do the studied bindings impact the training accuracy and test accuracy of the studied DL models?", "content": "We employ both dynamic time warping (DTW) [72] for analyzing training accuracy curves and the Mann-Whitney U test [56] for comparing the performance metrics of the final trained models. We chose DTW due to its ability to analyze time-series data, which allows us to investigate whether different bindings follow the same trajectory during training. DTW calculates the distance between the training accuracy curves of the bindings (e.g., between TensorFlow's Python and C# binding) for training the same model. DTW is widely used as a distance measurement for time series data since it can manage time distortion by aligning two time series before computing the distance, which is more accurate than the Euclidean distance [15]. We normalize the calculated DTW distances between 0 to 1 to interpret the results. A normalized DTW distance of 0 means that the difference between the two curves is negligible."}, {"title": "Summary of RQ1", "content": "TensorFlow and PyTorch bindings can have different training accuracy curves for training the same DL models even when using the same configuration. In addition, the test accuracy of the final trained models can be slightly different. Hence, developers should not assume that all bindings offer the same level of correctness and should verify the model's correctness when utilizing a binding for training."}, {"title": "RQ2: How do the studied bindings impact the cross-binding test accuracy of pre-trained models?", "content": "We conducted inference experiments with all bindings using pre-trained models produced by the default Python bindings for TensorFlow and PyTorch (see Figure 4). We loaded the pre-trained models using the supported loading approach(es) and recorded the cross-binding test accuracy on both CPU and GPU for each binding. If the cross-binding test accuracy of a pre-trained model in a binding shows a 0% difference compared to the test accuracy when the model was initially"}, {"title": "Findings", "content": "The test accuracy of pre-trained models can be reproduced across bindings in different languages for the same ML framework. Figure 5 shows that only PyTorch's C# binding failed to reproduce the test accuracy in the saved VGG-16, LSTM, and GRU models. We noticed that the differences in the test accuracy in these three models are all within 1% and the root cause of the reproduction failure is a bug that results in \u201ceval() and train() methods not being properly propagated to all submodules\u201d."}, {"title": "Summary of RQ2", "content": "TensorFlow and PyTorch bindings can perform inference using pre-trained models and reproduce the same test accuracy as when the models were originally trained. This correctness property holds true whether model inference is performed on CPU or GPU. As a result, developers can leverage the capabilities of pre-trained models while still being able to use the model in their preferred language."}, {"title": "5 TIME COST EVALUATION", "content": "In RQ1 and RQ2, we studied the impact of bindings for ML frameworks on correctness, however, the impact of bindings on time cost remains unknown. Given the time-consuming nature of model training and model inference for ML frameworks, it is important to investigate how a binding may impact the time cost. Studies show that runtime efficiency and energy consumption can vary across programming languages [59, 64, 66]. Consequently, these differences may have an impact on the time cost of training and inference when using different bindings.\nThus, in RQ3, we study the time cost of training DL models with bindings in order to offer developers more information about the overhead or advantage in terms of time cost when training with a binding. In RQ4, we study the inference time of pre-trained models in bindings. The time of utilizing bindings in model inference can be a crucial consideration for developers since model inference typically takes place (as a part of the product) in the production environment, which may have limited resources. The findings can help developers decide whether or not to utilize a binding for model inference in their project."}, {"title": "RQ3: How do the studied bindings impact the training time of the studied DL models?", "content": "To study the difference in training time across bindings, we performed the Mann-Whitney U test [56] using the Bonferroni correction [74] to adjust the significance level for multiple comparisons. Specifically, for an initial significance level of $\\alpha$ = 0.05, we adjusted the significance level to $\\frac{\\alpha}{n}$ (where n is the number of comparisons made) to determine whether the distributions of the training times of the default Python bindings and the non-Python bindings, which trained the same model for the same framework, are significantly different. For example, the LeNet-1 model in TensorFlow bindings, we performed Bonferroni-corrected Mann-Whitney U test between the Python and C# bindings and Python and JavaScript bindings with an adjusted significance level of $\\frac{\\alpha}{n}$ = 0.025. We also computed Cliff's delta d [53] effect size to quantify the difference based on Equation 1 in Section 4."}, {"title": "Findings", "content": "Training times can differ greatly across bindings for the same ML framework. Figure 6 shows the training time distributions on GPU for the studied models across the studied bindings. The Bonferroni-corrected Mann-Whitney U test shows that the training time distributions of the same model are all significantly different between the default Python bindings and the other bindings for the same framework and the effect sizes are all large. In addition, the difference in training time of bindings for the same ML framework can be very large when training certain models. For example, the median training time of TensorFlow's JavaScript binding for the VGG-16 model is 15 times larger than its Python binding (32,783 vs. 1,991 seconds).\nPyTorch's default Python binding has the slowest training time for the studied models. Figure 6 shows that PyTorch's Python binding is more than two times slower than the other two bindings for training LeNet models. However, we note that the training time difference between PyTorch's Python binding and other bindings for the VGG-16, LSTM, and GRU models is relatively small (less than 15%). In contrast, TensorFlow's default Python binding has the fastest training time in the studied models.\nBatch data loading time affects the training cost of PyTorch's Python binding. As shown in Table 6, PyTorch's Python binding has a long batch data loading time, which is notably slower (between 4 to 14 times) than the Rust binding for all studied models. Specifically, For LeNet models, the Python binding's batch data loading times account for roughly 30% of the training cost, whereas the Rust binding's batch data loading for the same models consumes less than 10% of the training cost. Furthermore, the Python binding consistently underperforms the Rust binding during both forward and backward propagation phases in the studied models."}, {"title": "Summary of RQ3", "content": "Training times for training the same DL models differ significantly between the default Python bindings and the non-Python bindings for the same ML framework. Surprisingly, non-Python bindings for PyTorch are even faster in training the studied models than the default Python binding. Hence, choosing the right binding can help developers to lower the training time cost for certain models."}, {"title": "RQ4: How do the studied bindings impact the inference time of pre-trained models?", "content": "We followed the same process as shown in Figure 4 and investigated the inference time of each model on both CPU and GPU. We performed the Bonferroni-corrected Mann-Whitney U test on the recorded inference time distributions between the default Python bindings and the non-Python bindings, grouped by the same framework, model, and processing unit (CPU or GPU). We also computed Cliff's Delta effect size as described in RQ3."}, {"title": "Findings", "content": "The inference time of the same pre-trained model differs greatly between the default Python bindings and the other bindings for the same ML framework. Figure 7 shows the distributions of the inference time of the pre-trained models in the studied bindings. The results of the Bonferroni-corrected Mann-Whitney U test and Cliff's Delta d show that the Python and non-Python bindings for the same ML framework have significantly different inference times for the same model on the same processing unit (i.e., CPU and GPU) and the effect size is large,"}, {"title": "Summary of RQ4", "content": "TensorFlow and PyTorch bindings have various inference times for the same pre-trained models on CPU and GPU. Remarkably, the inference time of certain models in bindings on the CPU can be faster than other bindings for the same framework on GPU. Therefore, developers can experiment and choose the fastest binding for their usage scenario."}, {"title": "6 IMPLICATIONS", "content": "Developers are not limited to writing their projects in Python when using an ML framework. Although Python dominates the development in ML [4, 60], developers can also use bindings in other programming languages. Our results in Section 4 shows that non-default bindings for TensorFlow and PyTorch can have the same inference accuracy of a pre-trained model as the default Python binding and sometimes even faster performance. We recommend developers use the binding in their preferred programming language for either model training or inference if supported by the binding. Hence, developers can save time and effort when adopting ML techniques in their projects without having to settle for non-mature ML frameworks that might be available in the language that their current software is programmed in. For instance, in Integration Scenario 1 of Section 1, Anna can use the JavaScript binding to perform inference with pre-trained models provided by the ML team.\nDevelopers can use a binding for an ML framework which has a shorter training time for a certain model and perform inference on the trained model in another binding that has a shorter inference time based on task and requirements. Bindings for an ML framework have various training times and inference times for ML models (Section 5). Hence, developers can choose different bindings which are faster for a certain model in training and inference respectively since the accuracy of pre-trained models can be reproduced across bindings for the same framework (Section 4). We suggest that developers refer to an existing benchmark like ours or conduct experiments themselves based on our replication package [48]. For example, when using TensorFlow for LeNet models as described in Integration Scenario 3 of Section 1, Anna can train the models using the default Python binding for TensorFlow and then run inference for the trained model in the Rust binding with the assistance of a hired expert to save time and computational resources, as this factor is critical in their project requirements.\nDevelopers should perform a sanity check before using a model that was trained by a binding other than the default Python binding. Bindings corresponding to different languages can have different training accuracy curves while training the same model, and the final trained model can behave differently (as discussed in Section 4). Since the Python bindings are the default binding for most ML frameworks, these Python bindings have a larger user base and better support than other bindings. We suggest that developers perform a sanity check on the trained model if they are using a binding other than the default Python binding before deploying the models to the production environment.\nIn resource-limited scenarios (e.g., CPU only), developers may prefer or need to use a non-default binding for model inference. Traditionally, model inference is done using a GPU due to the superior inference time of GPUs [7, 46]. However, GPUs are expensive and not available in all scenarios. We found that the bindings for ML frameworks can be fast for running inference on CPU for some pre-trained models (Section 5). Developers can use such bindings if the production environment does not contain a GPU or the computational resource is limited. For example, in Integration Scenario 3 of Section 1, if Anna is using PyTorch for LeNet models and"}, {"title": "6.2 Implications for binding owners", "content": "Binding owners should include performance benchmarks for their binding. We found that bindings can have very different training and inference times for ML models (Section 5), yet this information is not well documented. To address this, we suggest that binding owners introduce performance benchmarks of training and running inference for some frequently used ML models (e.g., VGG models) and record the results in their documentation. This way, developers be aware of the trade-off between choosing a familiar language and the potential impact on time cost for various DL models. For example, the performance benchmarks can help Anna in Integration Scenario 2 of Section 1 to make informed decisions when choosing a familiar language for training while considering the potential impact on time cost."}, {"title": "6.3 Implications for researchers", "content": "Our findings provide a starting point, but further research is needed to fully understand how binding choices influence performance in large-scale models. While full-parameter fine-tuning can be computationally expensive, parameter-efficient techniques like Low-Rank Adaptation (LoRA) [32] offer a cost-effective alternative. However, LoRA's experimental status in HuggingFace and its lack of binding support highlight a direction for further research. We suggest future research adopt our methodology (see our replication package [48]), starting with representative data subsets and smaller model variants (e.g., the 7 billion parameter variant of Llama 2 [81]). This approach could provide valuable early insights into potential performance variations before committing to full-scale experiments.\nOur findings demonstrate that pre-trained models can be used across different bindings for the same ML framework with the same level of accuracy (as shown in Section 4). However, some models may not be supported or may have a slower inference time when utilizing certain bindings (as discussed in Section 5). While developers and binding owners focus on the implementation of bindings, we suggest researchers explore ways to contribute at a higher level: by devising algorithms, methodologies, or protocols to increase the interoperability and compatibility of pre-trained models across different bindings, benefiting a diverse developer base."}, {"title": "7 RELATED WORK", "content": "Researchers have studied the correctness of ML frameworks. However, no one has studied how bindings for those frameworks impact the correctness of the ML software that is created with them. The study by Guo et al. [26] is the closest related to our work. However, even though they included several bindings in their study, their work differs from ours as they focus on the impact on ML software quality of using different ML frameworks and executing ML models on different computing devices (such as PC and various types of mobile devices). In contrast, we run our experiments on the same device but we study the impact of various bindings on ML software quality. Hence, we can reason about the impact of using a binding, while in Guo et al.'s study, the different devices make this impossible.\nSeveral others have focused on comparing the accuracy of the same model across ML frameworks. Chirodea et al. [10] compared a CNN model that was built with TensorFlow and PyTorch and found that these two frameworks have similar training curves but the final trained model has a lower accuracy in PyTorch. Gevorkyan et al. [22] gave an overview of five ML frameworks and compared the accuracy of training a neural network for the MNIST dataset. They reported that the final trained model has a lower accuracy in TensorFlow than in other frameworks. Moreover, Elshawi et al. [17] conducted training experiments for six ML frameworks by using the default configuration and reported that certain frameworks have better performance than the other frameworks on the same model (e.g., Chainer on the LSTM model)."}, {"title": "7.2 Impact of ML frameworks on ML software time cost", "content": "Many studies have compared the time cost across ML frameworks. In a comparison of the training and inference time for a CNN architecture using PyTorch and TensorFlow, Chirodea et al. [10] found that PyTorch is faster in both model training and inference than TensorFlow. However, Gevorkyan et al. [22] showed that PyTorch has the worst training time for neural networks among five studied ML frameworks. In our work, we compared the training and inference time across bindings for the same ML frameworks.\nSeveral studies have focused on the time cost of ML frameworks on different hardware devices. Buber and Diri [7] compared the running time of DL models on CPU and GPU and found that GPU is faster. Jain et al. [37] focused on the performance of training DNN models on CPU with TensorFlow and PyTorch. They show that multi-processing provides better training performance when using a single-node CPU. For mobile and embedded devices, Luo et al. [54] introduced a benchmark suite to evaluate the inference time cost based on six different neural networks."}, {"title": "7.3 Impact of ML frameworks on ML software reproducibility", "content": "Reproducibility has become a challenge in ML research [25, 57, 80]. Liu et al. [51] surveyed 141 published ML papers and conducted experiments for four ML models. The results showed that most studies do not provide a replication package and the models are highly sensitive to the size of test data. In addition, Isdahl and Gundersen [35] introduced a framework to evaluate the support of reproducing experiments in ML platforms and found that the platforms which have the most users have a relatively lower score in reproducibility. In this paper, we studied the reproducibility of pre-trained models across different bindings for the same ML framework.\nTo improve the reproducibility of ML models, many researchers have conducted studies to understand and resolve non-deterministic factors in ML software. Pham et al. [65] studied nondeterminism-introducing-factors in ML frameworks (e.g., weight initialization and parallel processes) and found that these factors can cause a 10% accuracy difference in ML models. To improve the reproducibility"}, {"title": "7.4 Empirical Studies of ML Frameworks", "content": "Many empirical studies of ML frameworks exist that study software quality aspects such as software bugs [9, 38, 39], technical debt [52, 73], and programming issues [34, 36, 92]. However, no prior work has investigated the impact of bindings for ML frameworks on the ML software quality.\nMany studies have focused on the bugs of ML frameworks. Jia et al. [38, 39] investigated Tensor-Flow's GitHub repository and identified six symptoms and eleven root causes of bugs in TensorFlow. In addition, they found that most bugs are related to interfaces and algorithms. Chen et al. [9] studied bugs from four ML frameworks and investigated the testing techniques in these frameworks. They showed that the most common root cause of the bugs is the incorrect implementation of algorithms, and the current testing techniques have a low percentage of test coverage.\nML software has ML-specific technical debts such as unstable data dependence, hidden feedback loop, and model configuration debts [73]. This technical debt can hurt the maintainability of ML systems and introduce extra costs. Liu et al. [52] analyzed self-admitted technical debt in 7 DL frameworks and concluded that technical debt is common in DL frameworks, although application developers are often unaware of its presence.\nResearchers have also aimed to understand the ML frameworks from a developer perspective to study the programming issues when using an ML framework. They typically researched the questions and answers (Q&As) of developers about ML frameworks on Stack Overflow (SO). Zhang et al. [92] investigated Q&As which are related to TensorFlow, PyTorch and Deeplearning4j on SO and reported that model migration is one of the most frequently asked questions. Humbatova et al. [34] studied Q&As of these three ML frameworks on SO as well and included interviews with developers and researchers to build a taxonomy of faults in ML systems. Islam et al. [36] mined Q&As about ten ML frameworks on SO and reported that developers need both static and dynamic analysis tools to help fix errors."}, {"title": "7.5 FFIs and Bindings in Software Engineering", "content": "FFIs and language bindings are instrumental in software engineering"}, {"title": "7.5 FFIs and Bindings in Software Engineering", "content": "FFIs and language bindings are instrumental in software engineering, serving as bridges that enable different programming languages to collaborate seamlessly. These bridges often enable developers to develop applications in their language of choice while simultaneously using mature libraries that are developed in another language. The existing body of work predominantly proposes approaches to design and improve such bindings and FFIs within one specific language. For instance, Yallop et al. [89] conducted experiments to create bindings for using the ctypes library in OCaml. Their study differentiated the performance of dynamic and static bindings, revealing that static bindings could be between 10 to 65 times faster than their dynamic counterparts. This finding aligns with our investigation into the time costs associated with diverse ML software bindings.\nResearchers also proposed several approaches to FFIs. For instance, Bruni et al. [6] introduced an FFI approach called NativeBoost. This approach requires minimal virtual machine modifications and generates native code directly at the language level. They compared the time cost of different FFIs and the results show that NativeBoost is competitive. Ekblad et al. [16] presented an FFI tailored for web-targeting Haskell dialects, emphasizing simplicity and automated marshalling. The authors compare their FFI with the vanilla FFI, which is based on C calling conventions, and"}, {"title": "8 THREATS TO VALIDITY", "content": ""}, {"title": "8.1 Construct validity", "content": "We use the accuracy metric to assess the correctness of TensorFlow and PyTorch bindings on model training and inference since it is a widely used metric among researchers and developers [10, 17, 22, 26, 54]. However, other metrics may also be used to assess correctness and use of other metrics could potentially change our results. For evaluating the time cost of bindings on model training, we ran training experiments on the GPU since training DL models on CPU is time-consuming and developers usually train DL models on GPU. The results might be different from those obtained by measuring the time cost on CPU."}, {"title": "8.2 Internal validity", "content": "When implementing the studied models in TensorFlow and PyTorch bindings, we used the same/similar interfaces to ensure that the structures of these models are consistent across bindings. However, bindings might have different implementations for these interfaces (or have hidden bugs) that result in different structures in the built models. We saved the built models in bindings (via parameters or serialization) and loaded them back into the default Python bindings for TensorFlow and PyTorch to examine whether the structures were the same. The verification results confirm that the produced models in bindings have the same structures.\nTensorFlow's JavaScript binding does not support training and inference for GRU with \u201creset_after=True\u201d. Hence, we set \u201creset_after=False\" in the training experiment of TensorFlow's JavaScript binding for GRU and performed inference with a GRU model that was trained with \"reset_after=False\u201d in the default Python binding. This setup differs from other bindings, although it has no effect on the model's structure. We compared the results from the JavaScript binding to the results in the Python binding using \"reset_after=False\", and our findings still hold. Future studies should investigate how one can automatically confirm that the configurations of the bindings are exactly the same."}, {"title": "8.3 External validity", "content": "We focused on TensorFlow and PyTorch bindings in our work and the results of our study might not apply directly to other ML frameworks. One reason could be that other ML frameworks could have a different implementation and do not provide GPU support. Furthermore, the findings of our investigation may not be able to generalize to other models and datasets. Future studies should leverage our methodology to analyze bindings for other ML frameworks using different models and datasets.\nOur analysis focused on small to medium-sized models that are widely adopted in real-world applications. However, the implications for large-scale models, particularly frontier ML models with billions or trillions of parameters, require further investigation. Future research should build on our work to examine how the observed differences might persist or change at this extreme scale."}, {"title": "9 CONCLUSION", "content": "In this paper, we investigate the impact on ML software quality (correctness and time cost) of using bindings for ML frameworks for DL model training and inference. We conducted model training and model inference experiments on three CNN-based models and two RNN-based models in TensorFlow and PyTorch bindings written in four different programming languages. The most important findings of our study are:\n\u2022 When training models, bindings for ML frameworks can have various training accuracy curves and slightly different test accuracy values for the trained models.\n\u2022 Bindings have different training times for the same model, and the default Python bindings for ML frameworks may not have the fastest training time.\n\u2022 Bindings for ML frameworks have the capabilities to reproduce the accuracy of pre-trained models for inference.\n\u2022 Bindings for ML frameworks have different inference times for the same pre-trained model and certain models in bindings on the CPU can outperform other bindings on the GPU.\nOur findings show that developers can utilize a binding to speed up the training time for an ML model. For pre-trained models, developers can perform inference in their favoured programming language without sacrificing accuracy, or they can choose a binding that has better inference time."}, {"title": "DISCLAIMER", "content": "Any opinions, findings, and conclusions, or recommendations expressed in this material are those of the author(s) and do not reflect the views of Huawei."}]}