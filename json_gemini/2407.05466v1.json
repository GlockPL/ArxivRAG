{"title": "Studying the Impact of TensorFlow and PyTorch Bindings on Machine Learning Software Quality", "authors": ["Hao Li", "Gopi Krishnan Rajbahadur", "Cor-Paul Bezemer"], "abstract": "Bindings for machine learning frameworks (such as TensorFlow and PyTorch) allow developers to integrate a framework's functionality using a programming language different from the framework's default language (usually Python). In this paper, we study the impact of using TensorFlow and PyTorch bindings in C#, Rust, Python and JavaScript on the software quality in terms of correctness (training and test accuracy) and time cost (training and inference time) when training and performing inference on five widely used deep learning models. Our experiments show that a model can be trained in one binding and used for inference in another binding for the same framework without losing accuracy. Our study is the first to show that using a non-default binding can help improve machine learning software quality from the time cost perspective compared to the default Python binding while still achieving the same level of correctness.", "sections": [{"title": "1 INTRODUCTION", "content": "The rapidly improving capabilities of Deep Learning (DL) and Machine Learning (ML) frameworks have been the main drivers that allow new intelligent software applications, such as self-driving cars and robotic surgeons. These intelligent software systems all contain components that integrate one or more complex DL and/or ML algorithms. Fortunately, over the past decade, the need for coding these ML and DL algorithms from scratch has been largely eliminated by the availability of several mature ML frameworks and tools such as TensorFlow  and PyTorch. These frameworks provide developers with a high-level interface to integrate ML functionality into their projects. Using such ML frameworks has several advantages including"}, {"title": "2 BACKGROUND", "content": "The remainder of this paper is outlined as follows. Sections 2 provides background information. Section 3 describes the design of our study. Sections 4 and 5 present the results. Section 6 discusses the implications of our findings. Section 7 gives an overview of related work. Section 8 outlines threats to the validity of our study and Section 9 concludes the paper."}, {"title": "2.1 ML Frameworks", "content": "Machine learning frameworks are software libraries that provide ML techniques to developers for the development and deployment of ML systems. Most popular ML frameworks are supported by large companies such as Google and Facebook. As shown in Figure 1, an ML framework provides interfaces to define the structure of a model, train the defined model using a selected optimizer, and save the trained model for later use. In addition, developers can deploy the trained models to the production environment by loading a saved (or pre-trained) model and performing inference. ML frameworks can load a pre-trained model using (1) the model parameters (e.g., weights and hyperparameters) or (2) serialization. If only the model parameters are saved, developers first have to define the model structure before they can load the stored parameters into the defined model. When loading a serialized model, the ML framework can recreate the model from the saved file automatically since it contains both the structure and the weights of the pre-trained model.\nModern ML frameworks, such as TensorFlow and PyTorch, have been built upon a foundation that leverages parallel processing devices like GPUs. GPUs have proven to be highly efficient for tasks that demand parallel computation, especially in the realm of ML. Their architecture is inherently designed to handle multiple tasks simultaneously, allowing for massive parallelism. However, one significant characteristic of GPU computations that needs emphasis is their asynchronous nature. When a task is dispatched to a GPU, it does not always execute immediately. Instead, it often gets scheduled in a queue. Consequently, a CPU might continue with its tasks believing that a GPU job is complete when, in fact, it has not even started. This asynchronous behaviour allows GPUs to optimize task execution but also necessitates careful synchronization when precise timing or task ordering is crucial."}, {"title": "2.2 Bindings for the ML frameworks", "content": "Python is the most popular programming language for ML applications , but developers in other languages also have the need for using ML algorithms. Developers might choose an existing ML framework in their preferred language or they have to create a new one from scratch (which requires a large amount of work and is error-prone). Another alternative is to use a binding in their preferred language, which provides interfaces to the functionality of an existing ML framework in the language of the binding .\nAs shown in Figure 1, bindings access the functionality of the ML framework through foreign function interfaces (FFIs) without recoding the library. FFIs bridge the gap between programming languages, allowing developers to reuse code from other languages. For example, TensorFlow's Rust binding uses the FFI provided by the Rust language to access TensorFlow functionality. Since the GPU support is provided by the underlying C/C++ computational core of ML frameworks, bindings typically leverage FFIs to access these functionalities. For example, the Python bindings for TensorFlow and PyTorch make use of SWIG (Simplified Wrapper and Interface Generator) and Pybind11 to generate FFIs for its Python binding to tap into the C++ backend which includes the ability to access the GPU. However, the efficiency in leveraging GPU resources may vary among different bindings."}, {"title": "3 STUDY DESIGN", "content": "In this section, we first describe our experimental environment and the studied datasets, models, ML frameworks, and bindings. Then, we discuss how we evaluate the correctness and time cost in the model training and model inference experiments. Finally, we introduce the experimental setup of our study. Figure 2 gives an overview of our study design."}, {"title": "3.1 Environment setting", "content": "We set up our experimental environment on a dedicated laboratory server provided by ISAIC, where we can control the execution of other running tasks. The server runs Ubuntu Linux 20.04 with Linux kernel 5.11.0. We used the CUDA 11.1.74 and cuDNN 8.1.0 GPU-related libraries. The hardware specifications of the server are as follows:\n\u2022 GPU: 2x NVIDIA TU102 [TITAN RTX] (24 GB)\n\u2022 CPU: 3.30 GHz Intel(R) Core(TM) i9-9820X\n\u2022 RAM: 100 GB"}, {"title": "3.5 Correctness evaluation", "content": "Training correctness. During the training process, the correctness is measured in each epoch using the training accuracy which is calculated by \\(Acc_{train} = N_{correct}/N_{train}\\), where \\(N_{correct}\\) is the number of correct predictions and \\(N_{train}\\) is the number of data samples in the training set. For the final trained models, we use the test accuracy \\(Acc_{test} = N_{correct}/N_{test}\\) as the evaluation metric for comparison, which is the accuracy on the test set.\nInference correctness. When we finish training a model, we use the test accuracy \\(Acc_{test}\\) of this pre-trained model as a reference. Then, we perform inference with a studied binding for the pre-trained model on the test set to obtain the cross-binding test accuracy \\(Acc_{cross\\_test} = N_{correct}/N_{test}\\) using that binding. The difference between \\(Acc_{test}\\) and \\(Acc_{cross\\_test}\\) is that the inference correctness is measured in the studied binding. For BERT on SQUAD, we use the exact match score instead of accuracy as the metric to evaluate the correctness."}, {"title": "3.6 Time cost evaluation", "content": "Training time cost. The training time cost measures the time spent training a model in seconds. Developers commonly train DL models on GPU rather than CPU since the training can be time-consuming and GPU can considerably shorten the training time . Hence, all model training experiments of bindings for ML frameworks are conducted on GPU and we measure the training time cost on GPU only.\nInference time cost. The inference time cost measures the time spent for performing inference with a pre-trained model on the test set in seconds. Since developers can deploy pre-trained models to a production environment which supports the CPU or GPU, the inference time cost of a binding is measured on both CPU and GPU."}, {"title": "3.7 Experimental setup", "content": "In this section, we detail our experimental setup with a running example of how we computed the correctness and time cost of LeNet-1 when trained and inferenced using the studied bindings for the studied ML frameworks.\nStep 1 - Train the studied models using the studied bindings: We conduct model training experiments for each supported model-dataset pair (as shown in Table 1). For a given model-dataset pair, each binding that supports the model's interface and training features (as shown in Table 3) trains the model from scratch on that dataset. For example, LeNet-1 and MNIST form one model-dataset pair and each supported binding trains LeNet-1 on MNIST independently. We repeat this process for each model-dataset pair in each binding that supports the model. For consistency, we ensure the following across all bindings for a given model-dataset pair:\n\u2022 Model structure. We use interfaces that provide the same functionality in bindings to build up each layer of the studied models. However, not all bindings support model training, as indicated in Table 3. As a result, we do not conduct training experiments with TensorFlow's Rust binding, PyTorch's JavaScript binding, and RNNs in TensorFlow's C# binding.\n\u2022 Training set and test set. We use the provided split of the training set and test set from studied datasets. Before conducting experiments, we perform comprehensive data preprocessing, ensuring that all bindings can work with the same processed data across all experiments."}, {"title": "RQ1: How do the studied bindings impact the training accuracy and test accuracy of the studied DL models?", "content": "Approach. We employ both dynamic time warping (DTW) for analyzing training accuracy curves and the Mann-Whitney U test for comparing the performance metrics of the final trained models. We chose DTW due to its ability to analyze time-series data, which allows us to investigate whether different bindings follow the same trajectory during training. DTW calculates the distance between the training accuracy curves of the bindings (e.g., between TensorFlow's Python and C# binding) for training the same model. DTW is widely used as a distance measurement for time series data since it can manage time distortion by aligning two time series before computing the distance, which is more accurate than the Euclidean distance . We normalize the calculated DTW distances between 0 to 1 to interpret the results. A normalized DTW distance of 0 means that the difference between the two curves is negligible."}, {"title": "RQ2: How do the studied bindings impact the cross-binding test accuracy of pre-trained models?", "content": "Approach. We conducted inference experiments with all bindings using pre-trained models produced by the default Python bindings for TensorFlow and PyTorch (see Figure 4). We loaded the pre-trained models using the supported loading approach(es) and recorded the cross-binding test accuracy on both CPU and GPU for each binding. If the cross-binding test accuracy of a pre-trained model in a binding shows a 0% difference compared to the test accuracy when the model was initially"}, {"title": "RQ3: How do the studied bindings impact the training time of the studied DL models?", "content": "Approach. To study the difference in training time across bindings, we performed the Mann-Whitney U test using the Bonferroni correction to adjust the significance level for multiple comparisons. Specifically, for an initial significance level of \\(\\alpha = 0.05\\), we adjusted the significance level to \\(\\frac{\\alpha}{n}\\) (where n is the number of comparisons made) to determine whether the distributions of the training times of the default Python bindings and the non-Python bindings, which trained the same model for the same framework, are significantly different. For example, the LeNet-1 model in TensorFlow bindings, we performed Bonferroni-corrected Mann-Whitney U test between the Python and C# bindings and Python and JavaScript bindings with an adjusted significance level of \\(\\frac{\\alpha}{n} = 0.025\\). We also computed Cliff's delta d effect size to quantify the difference based on Equation 1 in Section 4."}, {"title": "RQ4: How do the studied bindings impact the inference time of pre-trained models?", "content": "Approach. We followed the same process as shown in Figure 4 and investigated the inference time of each model on both CPU and GPU. We performed the Bonferroni-corrected Mann-Whitney U test on the recorded inference time distributions between the default Python bindings and the non-Python bindings, grouped by the same framework, model, and processing unit (CPU or GPU). We also computed Cliff's Delta effect size as described in RQ3."}, {"title": "6 IMPLICATIONS", "content": ""}, {"title": "6.1 Implications for developers", "content": "Developers are not limited to writing their projects in Python when using an ML framework. Although Python dominates the development in ML , developers can also use bindings in other programming languages. Our results in Section 4 shows that non-default bindings for TensorFlow and PyTorch can have the same inference accuracy of a pre-trained model as the default Python binding and sometimes even faster performance. We recommend developers use the binding in their preferred programming language for either model training or inference if supported by the binding. Hence, developers can save time and effort when adopting ML techniques in their projects without having to settle for non-mature ML frameworks that might be available in the language that their current software is programmed in. For instance, in Integration Scenario 1 of Section 1, Anna can use the JavaScript binding to perform inference with pre-trained models provided by the ML team.\nDevelopers can use a binding for an ML framework which has a shorter training time for a certain model and perform inference on the trained model in another binding that has a shorter inference time based on task and requirements. Bindings for an ML framework have various training times and inference times for ML models (Section 5). Hence, developers can choose different bindings which are faster for a certain model in training and inference respectively since the accuracy of pre-trained models can be reproduced across bindings for the same framework (Section 4). We suggest that developers refer to an existing benchmark like ours or conduct experiments themselves based on our replication package . For example, when using TensorFlow for LeNet models as described in Integration Scenario 3 of Section 1, Anna can train the models using the default Python binding for TensorFlow and then run inference for the trained model in the Rust binding with the assistance of a hired expert to save time and computational resources, as this factor is critical in their project requirements.\nDevelopers should perform a sanity check before using a model that was trained by a binding other than the default Python binding. Bindings corresponding to different languages can have different training accuracy curves while training the same model, and the final trained model can behave differently (as discussed in Section 4). Since the Python bindings are the default binding for most ML frameworks, these Python bindings have a larger user base and better support than other bindings. We suggest that developers perform a sanity check on the trained model if they are using a binding other than the default Python binding before deploying the models to the production environment.\nIn resource-limited scenarios (e.g., CPU only), developers may prefer or need to use a non-default binding for model inference. Traditionally, model inference is done using a GPU due to the superior inference time of GPUs . However, GPUs are expensive and not available in all scenarios. We found that the bindings for ML frameworks can be fast for running inference on CPU for some pre-trained models (Section 5). Developers can use such bindings if the production environment does not contain a GPU or the computational resource is limited. For example, in Integration Scenario 3 of Section 1, if Anna is using PyTorch for LeNet models and"}, {"title": "6.2 Implications for binding owners", "content": "Binding owners should include performance benchmarks for their binding. We found that bindings can have very different training and inference times for ML models (Section 5), yet this information is not well documented. To address this, we suggest that binding owners introduce performance benchmarks of training and running inference for some frequently used ML models (e.g., VGG models) and record the results in their documentation. This way, developers be aware of the trade-off between choosing a familiar language and the potential impact on time cost for various DL models. For example, the performance benchmarks can help Anna in Integration Scenario 2 of Section 1 to make informed decisions when choosing a familiar language for training while considering the potential impact on time cost."}, {"title": "6.3 Implications for researchers", "content": "Researchers should investigate the impact of ML framework bindings on large-scale models and datasets. Our findings provide a starting point, but further research is needed to fully understand how binding choices influence performance in large-scale models. While full-parameter fine-tuning can be computationally expensive, parameter-efficient techniques like Low-Rank Adaptation (LoRA) offer a cost-effective alternative. However, LoRA's experimental status in HuggingFace and its lack of binding support highlight a direction for further research. We suggest future research adopt our methodology (see our replication package ), starting with representative data subsets and smaller model variants (e.g., the 7 billion parameter variant of Llama 2 ). This approach could provide valuable early insights into potential performance variations before committing to full-scale experiments.\nResearchers should investigate methods to enhance the interoperability and compati-bility of pre-trained models across different bindings for ML frameworks. Our findings demonstrate that pre-trained models can be used across different bindings for the same ML framework with the same level of accuracy (as shown in Section 4). However, some models may not be supported or may have a slower inference time when utilizing certain bindings (as discussed in Section 5). While developers and binding owners focus on the implementation of bindings, we suggest researchers explore ways to contribute at a higher level: by devising algorithms, methodologies, or protocols to increase the interoperability and compatibility of pre-trained models across different bindings, benefiting a diverse developer base.\nResearchers should study the patterns and origins of bugs in bindings for ML frameworks. We found that bugs in bindings for ML frameworks have an impact on the model inference correctness (Section 4). While the immediate resolution of bugs in bindings is an engineering concern, a deeper analysis of these issues can provide invaluable insights into software design and testing paradigms for bindings. Although researchers have previously studied bugs in ML frameworks , there has been no research specifically on bugs in the bindings for ML frameworks or other libraries. We encourage researchers to systematically analyze the bugs in bindings and provide guidelines for maintainers to avoid introducing such bugs."}, {"title": "7 RELATED WORK", "content": ""}, {"title": "7.1 Impact of ML frameworks on ML software correctness", "content": "Researchers have studied the correctness of ML frameworks. However, no one has studied how bindings for those frameworks impact the correctness of the ML software that is created with them. The study by Guo et al. is the closest related to our work. However, even though they included several bindings in their study, their work differs from ours as they focus on the impact on ML software quality of using different ML frameworks and executing ML models on different computing devices (such as PC and various types of mobile devices). In contrast, we run our experiments on the same device but we study the impact of various bindings on ML software quality. Hence, we can reason about the impact of using a binding, while in Guo et al.'s study, the different devices make this impossible.\nSeveral others have focused on comparing the accuracy of the same model across ML frameworks. Chirodea et al. compared a CNN model that was built with TensorFlow and PyTorch and found that these two frameworks have similar training curves but the final trained model has a lower accuracy in PyTorch. Gevorkyan et al. gave an overview of five ML frameworks and compared the accuracy of training a neural network for the MNIST dataset. They reported that the final trained model has a lower accuracy in TensorFlow than in other frameworks. Moreover, Elshawi et al. conducted training experiments for six ML frameworks by using the default configuration and reported that certain frameworks have better performance than the other frameworks on the same model (e.g., Chainer on the LSTM model)."}, {"title": "7.2 Impact of ML frameworks on ML software time cost", "content": "Many studies have compared the time cost across ML frameworks. In a comparison of the training and inference time for a CNN architecture using PyTorch and TensorFlow, Chirodea et al. found that PyTorch is faster in both model training and inference than TensorFlow. However, Gevorkyan et al. showed that PyTorch has the worst training time for neural networks among five studied ML frameworks. In our work, we compared the training and inference time across bindings for the same ML frameworks.\nSeveral studies have focused on the time cost of ML frameworks on different hardware devices. Buber and Diri compared the running time of DL models on CPU and GPU and found that GPU is faster. Jain et al. focused on the performance of training DNN models on CPU with TensorFlow and PyTorch. They show that multi-processing provides better training performance when using a single-node CPU. For mobile and embedded devices, Luo et al. introduced a benchmark suite to evaluate the inference time cost based on six different neural networks."}, {"title": "7.3 Impact of ML frameworks on ML software reproducibility", "content": "Reproducibility has become a challenge in ML research . Liu et al. surveyed 141 published ML papers and conducted experiments for four ML models. The results showed that most studies do not provide a replication package and the models are highly sensitive to the size of test data. In addition, Isdahl and Gundersen introduced a framework to evaluate the support of reproducing experiments in ML platforms and found that the platforms which have the most users have a relatively lower score in reproducibility. In this paper, we studied the reproducibility of pre-trained models across different bindings for the same ML framework.\nTo improve the reproducibility of ML models, many researchers have conducted studies to under-stand and resolve non-deterministic factors in ML software. Pham et al. studied nondeterminism-introducing-factors in ML frameworks (e.g., weight initialization and parallel processes) and found that these factors can cause a 10% accuracy difference in ML models. To improve the reproducibility"}, {"title": "7.4 Empirical Studies of ML Frameworks", "content": "Many empirical studies of ML frameworks exist that study software quality aspects such as software bugs , technical debt , and programming issues . However, no prior work has investigated the impact of bindings for ML frameworks on the ML software quality.\nMany studies have focused on the bugs of ML frameworks. Jia et al. investigated TensorFlow's GitHub repository and identified six symptoms and eleven root causes of bugs in TensorFlow. In addition, they found that most bugs are related to interfaces and algorithms. Chen et al. studied bugs from four ML frameworks and investigated the testing techniques in these frameworks. They showed that the most common root cause of the bugs is the incorrect implementation of algorithms, and the current testing techniques have a low percentage of test coverage.\nML software has ML-specific technical debts such as unstable data dependence, hidden feedback loop, and model configuration debts . This technical debt can hurt the maintainability of ML systems and introduce extra costs. Liu et al. analyzed self-admitted technical debt in 7 DL frameworks and concluded that technical debt is common in DL frameworks, although application developers are often unaware of its presence.\nResearchers have also aimed to understand the ML frameworks from a developer perspec-tive to study the programming issues when using an ML framework. They typically researched the questions and answers (Q&As) of developers about ML frameworks on Stack Overflow (SO). Zhang et al. investigated Q&As which are related to TensorFlow, PyTorch and Deeplearning4j on SO and reported that model migration is one of the most frequently asked questions. Humbat-ova et al. studied Q&As of these three ML frameworks on SO as well and included interviews with developers and researchers to build a taxonomy of faults in ML systems. Islam et al. mined Q&As about ten ML frameworks on SO and reported that developers need both static and dynamic analysis tools to help fix errors."}, {"title": "7.5 FFIs and Bindings in Software Engineering", "content": "FFIs and language bindings are instrumental in software engineering, serving as bridges that enable different programming languages to collaborate seamlessly. These bridges often enable developers to develop applications in their language of choice while simultaneously using mature libraries that are developed in another language. The existing body of work predominantly proposes approaches to design and improve such bindings and FFIs within one specific language. For instance, Yallop et al. conducted experiments to create bindings for using the ctypes library in OCaml. Their study differentiated the performance of dynamic and static bindings, revealing that static bindings could be between 10 to 65 times faster than their dynamic counterparts. This finding aligns with our investigation into the time costs associated with diverse ML software bindings.\nResearchers also proposed several approaches to FFIs. For instance, Bruni et al. introduced an FFI approach called NativeBoost. This approach requires minimal virtual machine modifications and generates native code directly at the language level. They compared the time cost of different FFIs and the results show that NativeBoost is competitive. Ekblad et al. presented an FFI tailored for web-targeting Haskell dialects, emphasizing simplicity and automated marshalling. The authors compare their FFI with the vanilla FFI, which is based on C calling conventions, and"}, {"title": "8 THREATS TO VALIDITY", "content": ""}, {"title": "8.1 Construct validity", "content": "We use the accuracy metric to assess the correctness of TensorFlow and PyTorch bindings on model training and inference since it is a widely used metric among researchers and developers [10, 17, 22, 26, 54]. However, other metrics may also be used to assess correctness and use of other metrics could potentially change our results. For evaluating the time cost of bindings on model training, we ran training experiments on the GPU since training DL models on CPU is time-consuming and developers usually train DL models on GPU. The results might be different from those obtained by measuring the time cost on CPU."}, {"title": "8.2 Internal validity", "content": "When implementing the studied models in TensorFlow and PyTorch bindings, we used the same/sim-ilar interfaces to ensure that the structures of these models are consistent across bindings. However, bindings might have different implementations for these interfaces (or have hidden bugs) that result in different structures in the built models. We saved the built models in bindings (via parameters or serialization) and loaded them back into the default Python bindings for TensorFlow and PyTorch to examine whether the structures were the same. The verification results confirm that the produced models in bindings have the same structures.\nTensorFlow's JavaScript binding does not support training and inference for GRU with \u201cre-set_after=True\u201d. Hence, we set \u201creset_after=False\" in the training experiment of TensorFlow's JavaScript binding for GRU and performed inference with a GRU model that was trained with \"reset_after=False\u201d in the default Python binding. This setup differs from other bindings, although it has no effect on the model's structure. We compared the results from the JavaScript binding to the results in the Python binding using \"reset_after=False\u201d, and our findings still hold. Future studies should investigate how one can automatically confirm that the configurations of the bindings are exactly the same."}, {"title": "8.3 External validity", "content": "We focused on TensorFlow and PyTorch bindings in our work and the results of our study might not apply directly to other ML frameworks. One reason could be that other ML frameworks could have a different implementation and do not provide GPU support. Furthermore, the findings of our investigation may not be able to generalize to other models and datasets. Future studies should leverage our methodology to analyze bindings for other ML frameworks using different models and datasets.\nOur analysis focused on small to medium-sized models that are widely adopted in real-world applications. However, the implications for large-scale models, particularly frontier ML models with billions or trillions of parameters, require further investigation. Future research should build on our work to examine how the observed differences might persist or change at this extreme scale."}, {"title": "9 CONCLUSION", "content": "In this paper, we investigate the impact on ML software quality (correctness and time cost) of using bindings for ML frameworks for DL model training and inference. We conducted model training and model inference experiments on three CNN-based models and two RNN-based models in TensorFlow and PyTorch bindings written in four different programming languages. The most important findings of our study are:\n\u2022 When training models, bindings for ML frameworks can have various training accuracy curves and slightly different test accuracy values for the trained models.\n\u2022 Bindings have different training times for the same model, and the default Python bindings for ML frameworks may not have the fastest training time.\n\u2022 Bindings for ML frameworks have the capabilities to reproduce the accuracy of pre-trained models for inference.\n\u2022 Bindings for ML frameworks have different inference times for the same pre-trained model and certain models in bindings on the CPU can outperform other bindings on the GPU.\nOur findings show that developers can utilize a binding to speed up the training time for an ML model. For pre-trained models, developers can perform inference in their favoured programming language without sacrificing accuracy, or they can choose a binding that has better inference time."}, {"title": "DISCLAIMER", "content": "Any opinions, findings, and conclusions, or recommendations expressed in this material are those of the author(s) and do not reflect the views of Huawei."}]}