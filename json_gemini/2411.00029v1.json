{"title": "Preserving Pre-trained Representation Space: On Effectiveness of Prefix-tuning for Large Multi-modal Models", "authors": ["Donghoon Kim", "Gusang Lee", "Kyuhong Shim", "Byonghyo Shim"], "abstract": "Recently, we have observed that Large Multi-modal Models (LMMs) are revolutionizing the way machines interact with the world, unlocking new possibilities across various multi-modal applications. To adapt LMMs for downstream tasks, parameter-efficient fine-tuning (PEFT) which only trains additional prefix tokens or modules, has gained popularity. Nevertheless, there has been little analysis of how PEFT works in LMMs. In this paper, we delve into the strengths and weaknesses of each tuning strategy, shifting the focus from the efficiency typically associated with these approaches. We first discover that model parameter tuning methods such as LoRA and Adapters distort the feature representation space learned during pre-training and limit the full utilization of pre-trained knowledge. We also demonstrate that prefix-tuning excels at preserving the representation space, despite its lower performance on downstream tasks. These findings suggest a simple two-step PEFT strategy called Prefix-Tuned PEFT (PT-PEFT), which successively performs prefix-tuning and then PEFT (i.e., Adapter, LoRA), combines the benefits of both. Experimental results show that PT-PEFT not only improves performance in image captioning and visual question answering compared to vanilla PEFT methods but also helps preserve the representation space of the four pre-trained models.", "sections": [{"title": "1 Introduction", "content": "Understanding the visual scene and expressing it with a natural language are two distinct tasks yet the human brain can comprehensively handle both without difficulty. Large multi-modal models (LMMs) mimic such capability by training a deep neural network (DNN) such that it learns semantically meaningful connections between vision and language from a large number of image-text pairs (Li et al., 2020b; Zhang et al., 2021b; Wang et al., 2022b; Radford et al., 2021). Recently, LMMs have been widely used due to their broad range of applications, including chatbot, robot control, and video generation (Ouyang et al., 2022; Brohan et al., 2023; Ramesh et al., 2022).\nIn the pre-training, LMMs are trained to predict the masked words or next words from the image-text pair (Li et al., 2023; Alayrac et al., 2022; Wang et al., 2022a). In the second step called fine-tuning, the pre-trained LMMs are tailored to the specific downstream task. It has been shown that fine-tuning provides superior performance in various downstream tasks such as image captioning (IC), visual question answering (VQA), and image-text retrieval (Li et al., 2023; Wang et al., 2022a,b; Zhang et al., 2021b). However, fine-tuned models often suffer from the loss of generalization capability obtained from the pre-training (Sun et al., 2015; Brown et al., 2020a). Since the task-specific dataset is far smaller than the pre-training unlabeled dataset, the pre-trained model can be easily overfitted to the small-sized downstream task dataset, leading to degraded performance (Kumar et al., 2022). Various approaches have been suggested over the years to address the problem. In prompt-based approaches, manually designed prompts or trainable continuous embedding vectors are integrated into the input data to adapt the model for downstream tasks (Li and Liang, 2021; Liu et al., 2021; Tam et al., 2022; Lester et al., 2021). In knowledge distillation-based fine-tuning approaches, the model minimizes the distance between the distribution of the pre-trained and fine-tuned models (Xu et al., 2020; Sanh et al., 2019; Boschini et al., 2022). The common wisdom behind these approaches is to minimize the modification of the pre-trained model parameters while maintaining performance on downstream tasks.\nOne drawback of the full model fine-tuning is the huge computational burden caused by the model parameters update. In an effort to reduce the huge training cost, various parameter-efficient fine-tuning (PEFT) techniques have been proposed (Li and Liang, 2021; Houlsby et al., 2019; Hu et al., 2022; He et al., 2021). In these approaches, only a small set of additional modules (e.g., prefix, Adapter, LoRA) is trained instead of relying on full fine-tuning. These approaches are especially beneficial for training the large pre-trained model like GPT (Brown et al., 2020b), T5 (Raffel et al., 2020), and Llama (Touvron et al., 2023).\nTraining efficiency is a well-known advantage of prefix-tuning. Unlike other PEFT methods, prefix-tuning does not modify the model's parameters, leaving the representation space unchanged. To investigate the changes in the representation space, we analyze the feature representation matrices using singular value decomposition (SVD). Notably, we observe that the representation space of a fine-tuned model (in IC and VQA) utilizes only a limited set of effective basis vectors (60% of those in the pre-trained model) to express the output. Clearly, this limits the model's ability to fully enjoy the benefits obtained from pre-training (see Figure 4). In contrast, we discover that all the basis vectors are utilized in the prefix-tuned model, implying that the prefix-tuning effectively preserves the inherited representation space from the pre-training.\nWhile the prefix-tuning is effective in preserving pre-trained knowledge, the efficacy of this approach is somewhat questionable since the reported evaluation results are not conclusive. Some studies claim that the prefix-tuning performs comparable to the model parameter-tuning (e.g., full fine-tuning, LoRA, Adapter), while others argue that the prefix-tuning struggles in the training of relatively small-sized language models (Liu et al., 2021; Tam et al., 2022).\nAn aim of this paper is to propose a simple yet effective tuning strategy to combine the merits of two seemingly distinct approaches. The proposed method, henceforth referred to as Prefix-Tuned PEFT (PT-PEFT), performs the prefix-tuning and the model parameter-tuning sequentially. The key feature of PT-PEFT is to preserve the pre-trained feature space through the prefix-tuning and then refine the model parameters using the PEFT method. Intuitively, this approach resembles a language model learning a new task using prompt sentences such as \"I will provide example sentences describing the given pictures in the news article style. So, please generate the caption for the given images with such style.\" By providing a context suitable for the new task, the model's adaptability is enhanced, allowing for faster convergence and minimal changes to the weights of the pre-trained model."}, {"title": "2 Representation Space Collapse Causes the Loss of Generalization Capabilities", "content": "In our experiments, we show that applying the prefix-tuning before LoRA, Adapter, and even full fine-tuning consistently improves the task performance for all datasets and various pre-trained LMMs including BLIP (Li et al., 2022), BLIP-2 (Li et al., 2023), OFA (Wang et al., 2022a) and VINVL (Zhang et al., 2021b). We also compare the simultaneous tuning of prefix and model parameters and show that the proposed sequential strategy is indeed important for maximizing performance and preserving the representation space.\nOur contributions are as follows:\nWe show the correlation between the representation space and performance through rank-based analysis. We qualitatively and quantitatively illustrate the adverse effects of representation space collapse in task performance.\nWe reveal that the prefix-tuning differs significantly from model parameter tuning techniques such as LoRA, Adapter, and full fine-tuning in the sense that it preserves the integrity of the pre-trained knowledge.\nWe propose PT-PEFT, a method that sequentially performs the prefix-tuning followed by conventional fine-tuning technique, to maximize the utilization of pre-trained knowledge in LMMs. Our experimental results demonstrate that PT-PEFT outperforms the conventional fine-tuning methods in image captioning and VQA tasks."}, {"title": "2.1 Zero-shot Sometimes Performs Better than Fine-tune", "content": "In general, model parameter tuning performs better than the prefix-tuning. However, the full fine-tuned model generates even worse answers than the zero-shot generation for some cases. Figure 3 presents a qualitative comparison between zero-shot inference, full fine-tuning, and prefix-tuning on IC and VQA tasks. In IC tasks, we find that prefix-tuning is better than full fine-tuning in capturing detailed descriptions of objects. Although the IC output from the fine-tuning is technically sound, captions generated through the prefix-tuning are rich in context and more natural. Similarly, for VQA tasks, we observe that Top-5 answers from the prefix-tuning are more relevant to the given questions, whereas the answers from the fine-tuning are often irrelevant or less likely to be correct.\nThese results stem from the problem that the downstream dataset often lacks the object and attribute diversity compared to the dataset used for the pre-training. Consequently, models may lose the learned word and image representations for objects and attributes during the fine-tuning. This issue, known as catastrophic forgetting, undermines the model's ability to retain valuable pre-trained knowledge (Rebuffi et al., 2017; Kalajdzievski, 2024)."}, {"title": "2.2 Relationship Between Semantic Richness and Representation Space", "content": "In the vector space, catastrophic forgetting appears as the rank reduction of the representation matrix, so-called the representation collapse. The information contained within the representation matrix is closely associated with its rank (Zhang et al., 2021a; Bansal et al., 2018; Swaminathan et al., 2020). For instance, low-rank compression methods intentionally pursue a reduction in the rank of the feature matrix to extract essential information exclusively (Sainath et al., 2013; Swaminathan et al., 2020). Just as other information is expunged by low-rank compression, the representation collapse by catastrophic forgetting makes the representation matrix lose semantically rich details in objects and their attributes, potentially degrading the generalization ability for downstream tasks."}, {"title": "2.3 Empirical Analysis on Representation Space Collapse", "content": "Representation Space Analysis via SVD To quantitatively measure the representation collapse in different model adaptation methods, we apply SVD on the representation matrices. SVD allows us to quantitatively analyze the average number of basis singular vectors used to represent a single text or image. In our SVD analysis, we utilize the activation matrix of the model's last layer. Specifically, LMM processes the text input $X_{txt} = [w_{sos}, w_1, ..., w_N, w_{eos}]$, yielding a sequence of output embedding vectors $F_{txt} = LMM(X_{txt})$:\n$F_{txt} := [f_{txt}^1, f_{txt}^2, ..., f_{txt}^{N-1}, f_{txt}^N]$\t\t(1)\nUsing $F_{txt}$, we perform SVD and obtain the singular values (i.e., the diagonal elements of $\\Sigma$):\n$F_{txt} = U\\Sigma V^T$.  (2)\nWe sort the singular values $s = [\\sigma_1,..., \\sigma_M]$ in descending order and normalize such that sum of all singular values equals one:\n$\\hat{s} = \\frac{1}{\\sum_{i=1}^M \\sigma_i} [\\sigma_1, ..., \\sigma_M]$.\t\t(3)\nAfter computing singular values on a per-image or per-sentence basis, we average them across the $K$ samples in the dataset:\n$s_{avg} = \\frac{1}{K} \\sum_{i=1}^K s_k$.\t\t(4)\nFinally, we compute the cumulative sum of the elements in $\\hat{s}_{avg}$:\n$\\gamma = [\\sum_{j=1}^i \\sigma_{avg, 1},..., \\sum_{j=1}^i \\sigma_{avg,j}]\\qquad(5)$"}, {"title": "3 Prefix-Tuned Parameter-Efficient Fine Tuning (PT-PEFT)", "content": "Prefix Implementation Prefix embedding vectors are first processed through the prefix encoder, following standard practices in prefix-tuning (Li and Liang, 2021) (see Appendix for details). The processed prefixes are then concatenated with text and/or image tokens to form the input to the LMMs. Figure 5 illustrates various LMM architectures that can take prefixes as inputs. The green boxes in the Figure represent learnable prefix embeddings (tokens) used during the prefix-tuning stage.\nTwo-stage Optimization We employ a two-stage approach: prefix-tuning followed by fine-tuning. In the prefix-tuning stage, we only train the prefix embeddings and prefix encoder, keeping the other parameters of LMMs frozen. In the fine-tuning stage, we adjust the parameters, including prefixes, to further adapt the model and enhance downstream performance. Here, the parameters to be adjusted depend on whether it is PEFT or full fine-tuning."}, {"title": "4 Experiments", "content": "To demonstrate the generalization capability of our method, we evaluate various pre-trained LMMs with different architectures and sizes. Specifically, we conduct experiments on VINVL-BASE/LARGE (Zhang et al., 2021b), OFA-BASE (Wang et al., 2022a), BLIP (Li et al., 2022), an BLIP-2 (Li et al., 2023) models.\nDataset We evaluate image captioning (IC) task performance on MS-COCO (Lin et al., 2014) and Flickr30k (Plummer et al., 2015) datasets. For the visual question-answering (VQA) task, we use the VQAv2 (Antol et al., 2015) dataset.\nFine-tuning Methods We take pre-trained LMMs and compare different fine-tuning methods. These include Prefix-tuning (Prefix), LoRA, Parallel-Adapter (P-Adapter), and Sequential-Adapter (S-Adapter) (Hu et al., 2023), and also the full fine-tuning (Full-FT). Adapters usually include multi-layer modules, therefore they generally equip more trainable parameters than LoRA. Prefix-tuning uses the smallest number of trainable parameters among all. For fair comparison across PEFT methods, we matched the number of trainable parameters. Note that our PT-PEFT can be applied to all methods, with prefix-tuning used before other fine-tuning methods as our key innovation."}, {"title": "4.1 Setup", "content": "To demonstrate the generalization capability of our method, we evaluate various pre-trained LMMs with different architectures and sizes. Specifically, we conduct experiments on VINVL-BASE/LARGE (Zhang et al., 2021b), OFA-BASE (Wang et al., 2022a), BLIP (Li et al., 2022), an BLIP-2 (Li et al., 2023) models.\nWe evaluate image captioning (IC) task performance on MS-COCO (Lin et al., 2014) and Flickr30k (Plummer et al., 2015) datasets. For the visual question-answering (VQA) task, we use the VQAv2 (Antol et al., 2015) dataset.\nWe take pre-trained LMMs and compare different fine-tuning methods. These include Prefix-tuning (Prefix), LoRA, Parallel-Adapter (P-Adapter), and Sequential-Adapter (S-Adapter) (Hu et al., 2023), and also the full fine-tuning (Full-FT). Adapters usually include multi-layer modules, therefore they generally equip more trainable parameters than LoRA. Prefix-tuning uses the smallest number of trainable parameters among all. For fair comparison across PEFT methods, we matched the number of trainable parameters. Note that our PT-PEFT can be applied to all methods, with prefix-tuning used before other fine-tuning methods as our key innovation."}, {"title": "4.2 Downstream Task Performance", "content": "Table 2 shows the performance of various task adaptation methods, applied to OFA-BASE and BLIP-2 models. Our proposed PT-PEFT consistently outperforms standard PEFT methods across all 8 metrics. PT-PEFT even surpasses full fine-tuning, with a 0.2p/0.1p in BLEU-4 metric for Flickr30k/COCO, along with a 0.2p improvement in SPICE score. Additionally, the results show that applying PEFT before prefix-tuning (i.e., reversing the order) is considerably less effective than PT-PEFT, though it still performs better than not using prefix-tuning at all."}, {"title": "5 Analysis & Discussion", "content": "Table 2 shows the performance of various task adaptation methods, applied to OFA-BASE and BLIP-2 models. Our proposed PT-PEFT consistently outperforms standard PEFT methods across all 8 metrics. PT-PEFT even surpasses full fine-tuning, with a 0.2p/0.1p in BLEU-4 metric for Flickr30k/COCO, along with a 0.2p improvement in SPICE score. Additionally, the results show that applying PEFT before prefix-tuning (i.e., reversing the order) is considerably less effective than PT-PEFT, though it still performs better than not using prefix-tuning at all. \nTable 3 and 4 compare prefix-tuning, full fine-tuning, and the sequential combination of both (ours). To ensure the reliability of our results, we conducted three separate runs with different random seeds and reported the mean and standard deviation obtained from these runs. Notably, the standard deviation of the scores is significantly smaller than the improvements over the baseline models. Compared to the full fine-tuning, our prefix-tuned full fine-tuning achieves approximately an 11% increase in the BLEU-4, a 16% increase in SPICE, and a noteworthy 21% improvement in CIDEr. These results highlight the effectiveness of our method, demonstrating that prefix-tuning can help preserve pre-trained knowledge and improve performance in both PEFT and full fine-tuning scenarios."}, {"title": "5.1 Preserving Representation Space", "content": "visualizes the accumulated singular values, as described in Section 2.3. The saturation curves for the pre-trained, prefix-tuned, and PT-PEFT models are almost identical, implying that the effective rank is preserved after training. In contrast, LoRA, Adapter, and full fine-tuning methods show more concave curves, indicating a narrower representation space."}, {"title": "5.2 Ablation Study", "content": "Instead of sequentially applying prefix-tuning and then fine-tuning, one may consider using both methods together in parallel. We call this variant parallel-tuning and compare its performance to our sequential training. Table 5 (a) and (b) present the downstream task performance of parallel tuning and ours, respectively. The result shows that parallel-tuning performs worse than PT-PEFT in all cases.\nTo further investigate how parallel-tuning affects the effectiveness of the prefix, we distort the trained prefixes and observe the performance change. Table 5(b) shows that for the parallel-tuned model, even without prefixes, VQA accuracy is almost pre-served, meaning that the prefix does not contribute to performance. This finding is further emphasized when replacing the trained prefix with random noise; accuracy only slightly decreases, implying that the prefixes are not very powerful. In contrast, when using prefix tuning first (Table 5(a)), removing prefixes severely hurts the accuracy, showing that they actively contribute to the performance."}, {"title": "5.3 Intuitive Explanation of PT-PEFT", "content": "Based on the analysis, we conclude that prefix-tuning and other fine-tuning methods contribute to the adaptation in different ways. By sequentially performing prefix-tuning and parameter fine-tuning, the model first encodes the representation space as prefix tokens that align with the pre-trained space. This is because the original model parameters remain unchanged during prefix-tuning, so the learned knowledge is not damaged. Once such context is established, the subsequent fine-tuning process can effectively avoid the representation collapse, as the prefixes provide a foundation for a rich representation space."}, {"title": "5.4 Prior Works in Language Domain", "content": "In this subsection, we highlight how our work differs from recent studies that combine two fine-tuning techniques in the language domain. The original LORA paper reported that combining LORA with Prefix-tuning could improve performance (Appendix E of the paper (Hu et al., 2022)). However, their combination used a \"parallel-tuning\" approach, in contrast to our \"sequential-tuning\" approach. In addition, they utilized a much larger number of trainable parameters, making it an unfair comparison between LoRA alone and LoRA with Prefix-tuning.\nAround the same time as our work, Pro-Mot (Wang et al., 2024) also suggested using prefix-tuning before model parameter tuning in a sequential manner. They also reported significant performance improvements, which is consistent with our findings. However, our work is very distinct in two key perspectives.\nFirst, our experiments focus on LMMs, demonstrating the effectiveness of PT-PEFT across various vision-language tasks and Transformer-based model architectures. Second, our analyses show that the primary reason for performance gain comes from the preservation of learned knowledge during pre-training, as revealed by our systematic investigation of the effective rank of embeddings. This sets our work apart and highlights the uniqueness of our PT-PEFT."}, {"title": "6 Conclusion", "content": "In this paper, we discovered that fine-tuning methods including LoRA, Adapter, and full fine-tuning could cause the loss of learned knowledge from the pre-training stage. We quantified this loss in representation space using a novel rank-based analysis and identified that prefix-tuning does not cause this critical loss. Based on these findings, we proposed a two-step strategy, PT-PEFT, which first performs prefix-tuning and then applies other fine-tuning methods. Our experiments showed that PT-PEFT not only preserves the representation space preservation but also improves downstream task performance."}, {"title": "7 Limitations", "content": "The proposed PT-PEFT can take advantage of both prefix-tuning and fine-tuning. However, there are two practical limitations. Firstly, it leads to an increased computational cost during inference due to the longer input sequence. Managing this increased computational cost in prefix-tuning may become challenging, especially when the portion of prefixes in the total number of input tokens is large. It's worth noting that the performance gains tend to plateau at around 16 prefixes, which doesn't significantly exacerbate the computational cost (see Appendix C, prefix length ablation study). Secondly, we manually determine the best-performing hyper-parameters, such as prefix length, learning rates, and training iterations. We did our best to find the best set for a fair comparison; however, we are aware that such a manual hyperparameter tuning process can be cumbersome, especially when applying our technique to new tasks, datasets, or models."}, {"title": "8 Ethical Statement", "content": "In our paper, we analyze various fine-tuning strategies to identify methods for preserving pre-trained knowledge during the fine-tuning process. Rather than having potential risks, we believe that our research can serve as a solution to address ethical issues related to data corruption and safety control in current Al systems. For instance, even if the model is fine-tuned with data corrupted by hacking, our technique can offer robustness to such data corruption by preserving the model's representation space. Our work can be also beneficial for not forgetting the safety guardrails learned during pre-training or instruction tuning. We'd like to note that this representation-preserving have not been studied much in VL models, regardless of the increasing interest on VL applications."}, {"title": "A Related Work", "content": "The Transformer and its variants (e.g., BERT, GPT) are widely adopted as VL model architectures due to their powerful attention mechanisms capturing correlations between image and text (Vaswani et al., 2017). Examples include VINVL using a Transformer encoder, OFA employing a Transformer encoder-decoder pair, and BLIP-2 utilizing a Transformer decoder. We evaluate PT-PEFT on these models to demonstrate its robustness and applicability.\nVL models often undergo unsupervised pre-training on large datasets, employing objectives like masked language modeling, image-text matching, and causal language modeling (Li et al., 2023; Alayrac et al., 2022; Wang et al., 2022a; Yuan et al., 2021; Zhang et al., 2021b). This pre-training helps the model understand the relationships between image and text. Tasks include predicting masked words, scoring image-text matching, and predicting the next words from given image-text pairs.\nAssessing the semantic richness of features is crucial for effective vision-language (VL) learning. This refers to how well a feature encapsulates fine-grained, dense information from the input. Evaluation includes linear probing in computer vision. Numerous studies indicate a strong correlation between rank and information content in representations (Bansal et al., 2018; Zhang et al., 2021a). For instance, low-rank compression methods intentionally reduce rank to distill essential information, such as object class (Sainath et al., 2013; Swaminathan et al., 2020).\nTo enhance pre-trained model performance for downstream tasks, various transfer learning techniques address domain adaptation challenges. A parameter-efficient fine-tuning approach often inserts additional modules into pre-trained model layers and optimizes only these modules (Houlsby et al., 2019; Hu et al., 2022). Such PEFT methods are beneficial for greatly reducing the training cost by minimizing the number of trainable parameters."}, {"title": "B Experiments Setup", "content": "To assess the effectiveness of PT-PEFT, we have employed a diverse set of pre-trained models featuring different architectures and sizes. Specifically, we have tested models such as VINVL base, VINVL large (Zhang et al., 2021b), OFA (base) (Wang et al., 2022a), BLIP (Li et al., 2022)(only for VQA) and BLIP-2 (ViT-g and OPT-2.7B) (Li et al., 2023) as our baseline model due to its good performance on VL sequence generation and classification among many VL model variants (Tan and Bansal, 2019; Lu et al., 2019; Li et al., 2020a; Zhou et al., 2020; Li et al., 2020b; Alayrac et al., 2022), as described in Table 7 (Zhang et al., 2021b; Wang et al., 2022a; Li et al., 2023).\nFigure 6 illustrates the prefix encoder (see Section 3). In contrast to previous re-parameterizations (Li and Liang, 2021), our approach incorporates prefix type embedding to establish a symmetrical setting with token type embedding, as used in previous VL models (Zhang et al., 2021b; Li et al., 2020b). After training, the output of the prefix encoder can be saved as the new prefix, so there is no computational overhead in using this block. In other words, the block is only realized during the training phase."}, {"title": "B.1 Model", "content": "To assess the effectiveness of PT-PEFT, we have employed a diverse set of pre-trained models featuring different architectures and sizes. Specifically, we have tested models such as VINVL base, VINVL large (Zhang et al., 2021b), OFA (base) (Wang et al., 2022a), BLIP (Li et al., 2022)(only for VQA) and BLIP-2 (ViT-g and OPT-2.7B) (Li et al., 2023) as our baseline model due to its good performance on VL sequence generation and classification among many VL model variants (Tan and Bansal, 2019; Lu et al., 2019; Li et al., 2020a; Zhou et al., 2020; Li et al., 2020b; Alayrac et al., 2022), as described in Table 7 (Zhang et al., 2021b; Wang et al., 2022a; Li et al., 2023).\nFigure 6 illustrates the prefix encoder (see Section 3). In contrast to previous re-parameterizations (Li and Liang, 2021), our approach incorporates prefix type embedding to establish a symmetrical setting with token type embedding, as used in previous VL models (Zhang et al., 2021b; Li et al., 2020b). After training, the output of the prefix encoder can be saved as the new prefix, so there is no computational overhead in using this block. In other words, the block is only realized during the training phase."}, {"title": "B.2 Downstream task", "content": "Visual Question Answering Visual Question Answering task requires the model to select or generate the correct answer from the given question-image pair. For VINVL (Zhang et al., 2021b; Li et al., 2020b), we train the model to classify the answer given question and image pair sequence from answer sets (i.e., 3129 for VQAv2, 1852 for GQA). For OFA (Wang et al., 2022a) and BLIP-2 (Li et al., 2023), we train the model to generate the answer given question and image pair."}, {"title": "B.3 Dataset", "content": "For IC experiments, we evaluate the performance of our proposed fine-tuning techniques on MS COCO (Lin et al., 2014) and Flickr30k (Plummer et al., 2015) datasets. We follow the Karpathy split (Karpathy and Fei-Fei, 2015) for a fair comparison. Karpathy split of COCO and Flickr30k datasets contain 83k/5k/5k and 29.8k/1k/1k images for train/val/test split."}, {"title": "Image Captioning"}, {"title": "Visual Question Answering", "content": "For VQA experiments, the model is evaluated on the VQAv2 dataset (Antol et al., 2015). VQAv2 dataset contains 83k/41k/81k images and 444k/214k/448k question sets for train/val/test split, respectively."}, {"title": "B.4 Experiment Details", "content": "For training, we employ a set of hyper-parameters as detailed in Table 13. The table shows the best configurations for prefix-tuning and fine-tuning; these settings are also used for each stage of PT-PEFT. To update the network parameters, we utilize the AdamW optimizer (Loshchilov and Hutter, 2017) with betas set to (0.9, 0.99). For the learning rate scheduling, We combine linear warm-up followed by linear decay, gradually increasing the learning rate from 0 to the maximum LR during warm-up epochs and linearly decaying it to 0 for the remaining training epochs. We employ the CIDEr, SPICE, and BLEU-4 metrics (Vedantam et al., 2015; Anderson et al., 2016; Papineni et al., 2002) to evaluate the quality of generated captions. The evaluation is performed"}, {"title": "Image Captioning", "content": "Image captioning task requires the model to generate a natural language description for the given input image. Image captioning fine-tuning typically follows a 2-stage process, which consists of cross-entropy (CE) training and self-critical sequence training (SCST) (Rennie et al., 2017).\nDuring CE training, the model uses CE loss to predict the correct words given image. Then, the model is further trained by optimizing the CIDEr score with SCST which utilizes the score as the reward for REINFORCE algorithm (Rennie et al., 2017). For inference, we utilize a beam size of 5 for beam search."}, {"title": "B.5 Implementation Details", "content": "In prefix-tuning, the VL model is kept frozen, and only the prefix-encoder block (see Figure 6) and prefix vectors are trained. Our implementation of the prefix-tuning closely follows the original prefix-tuning approach (Li and Liang, 2021), where an MLP is employed as the prefix encoder for stable optimization. The number of prefix vectors is empirically chosen for the best performance based on the experiment in Figure 8 as described in Table 8.\nWe implement the low-rank adapter following (Hu et al., 2022). We update all query, key, and value projection matrices in the self-attention module by setting the rank r = 16, scaling factor a = 32, and dropout probability of 0.05 throughout all experiments (see Table 8).\nFor image captioning, we freeze the word embedding layer and the head throughout the training process, including both the prefix-tuning stage and the subsequent fine-tuning stage. In the prefix-tuning stage, we only train the prefix encoder and prefix embedding using CE training. Subsequently, we fine-tune the model using a combination of CE training and SCST (for VINVL COCO-IC only). For visual question answering, we follow a similar procedure. We first train the prefix encoder and prefix embedding (and the CLS head for VINVL) and then proceed with fine-tuning the model.\nPT-LoRA is the parameter-efficient version of prefix-tuning which performs the LoRA instead of the full fine-tuning in the second stage. To ensure a similar number of training parameters (i.e., 0.3 %) with prefix-tuning and LoRA tuning, we train only selected blocks (e.g., only Q-former is trained for the BLIP-2) for the LoRA tuning stage in PT-LORA. Other than that, all the training settings are the same as the PT-PEFT."}, {"title": "PT-PEFT", "content": "For image captioning, we freeze the word embedding layer and the head throughout the training process, including both the prefix-tuning stage and the subsequent fine-tuning stage. In the prefix-tuning stage, we only train the prefix encoder and prefix embedding using CE training. Subsequently, we fine-tune the model using a combination of CE training and SCST (for VINVL COCO-IC only). For visual question answering, we follow a similar procedure. We first train the prefix encoder and prefix embedding (and the CLS head for VINVL) and then proceed with fine-tuning the model."}, {"title": "C Additional Experiments", "content": "Longer prefixes (i.e., many prefix tokens) involve more trainable parameters, thus assumed to enhance the performance for prefix-tuning (Li and Liang, 2021). Figure 8 shows that performance indeed improves as the number of prefix tokens increases, but saturates after a certain point. Note that previous works on prefix-tuning often used much longer prefix lengths than our PT-PEFT, but since"}]}