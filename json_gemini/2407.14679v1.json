{"title": "Compact Language Models via Pruning and Knowledge Distillation", "authors": ["Saurav Muralidharan", "Sharath Turuvekere Sreenivas", "Raviraj Joshi", "Marcin Chochowski", "Mostofa Patwary", "Mohammad Shoeybi", "Bryan Catanzaro", "Jan Kautz", "Pavlo Molchanov"], "abstract": "Large language models (LLMs) targeting different deployment scales and sizes are currently produced by training each variant from scratch; this is extremely compute-intensive. In this paper, we investigate if pruning an existing LLM and then re-training it with a fraction (<3%) of the original training data can be a suitable alternative to repeated, full retraining. To this end, we develop a set of practical and effective compression best practices for LLMs that combine depth, width, attention and MLP pruning with knowledge distillation-based retraining; we arrive at these best practices through a detailed empirical exploration of pruning strategies for each axis, methods to combine axes, distillation strategies, and search techniques for arriving at optimal compressed architectures. We use this guide to compress the Nemotron-4 family of LLMs by a factor of 2-4\u00d7, and compare their performance to similarly-sized models on a variety of language modeling tasks. Deriving 8B and 4B models from an already pretrained 15B model using our approach requires up to 40\u00d7 fewer training tokens per model compared to training from scratch; this results in compute cost savings of 1.8\u00d7 for training the full model family (15B, 8B, and 4B). MINITRON models exhibit up to a 16% improvement in MMLU scores compared to training from scratch, perform comparably to other community models such as Mistral 7B, Gemma 7B and Llama-3 8B, and outperform state-of-the-art compression techniques from the literature. We have open-sourced MINITRON model weights on Huggingface 2, with corresponding supplementary material including example code available on GitHub 3.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) now dominate real-world natural language processing and have demonstrated excellent proficiency in understanding difficult contexts [7, 40, 50, 47, 46]. To aid users targeting different deployment sizes and scales, model providers often train an entire family of models from scratch, each with a different size (number of parameters). For instance, the LLaMa-2 model family [47] includes three different variants with 7, 13, and 70 billion parameters, while the Pythia family [6] offers"}, {"title": "2 Pruning Methodology", "content": "As shown in Figure 2, we start the pruning process by first computing the importance of each layer, neuron, head, and embedding dimension and then sorting these importance scores to compute a corresponding importance ranking. In this section, we detail how rankings are computed for each axis and then subsequently used to obtain a pruned model."}, {"title": "2.1 Background and Notation", "content": "We begin with some formal definitions. Multi-Layer Perceptron (MLP) layers have two linear layers with a non-linear activation in between:\nMLP(X) = \\delta(X \\cdot W_1 W_1^T) W_2\nwhere X denotes the input, and $W_1$ and $W_2$ are the two associated weight matrices in the MLP layer. $W_1, W_2 \\in \\mathbb{R}^{d_{hidden} \\times d_{model}}$ where $d_{model}$ and $d_{hidden}$ are the embedding and MLP hidden dimensions, respectively. $\\delta(\\cdot)$ refers to the non-linear activation function.\nWe define the Multi-Head Attention (MHA) operation for an input X as follows:\nMHA(X) = Concat(head_1, ...head_L) \\cdot W^O,\nhead_i = Attn(XW^{Q,i},XW^{K,i}, XW^{V,i}),\nhere, $W^{Q,i}, W^{K,i},W^{V,i} \\in \\mathbb{R}^{d_{head} \\times d_{model}}$ and $W^O \\in \\mathbb{R}^{Ld_{head} \\times d_{model}}$ where $d_{head}$ is the size of a single attention head, and L is the total number of heads.\nFinally, the Layer Normalization operation (LayerNorm) [5] on an input X is defined as follows,\nLN(X) = \\frac{X - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\cdot \\gamma + \\beta\nwhere $\\mu$ and $\\sigma^2$ represent the mean and variance across the embedding dimensions, $\\epsilon$ is a small value for numerical stability, and $\\gamma$ and $\\beta$ are learnable parameters."}, {"title": "2.2 Importance Analysis", "content": "Estimating the importance or sensitivity of individual neural network components such as neurons, attention heads, and layers is a well-studied area [9, 13, 41]. In the context of LLMs, recent work has highlighted the ineffectiveness of traditional metrics such as weight magnitude for estimating importance [33]; instead, recent work on structured pruning of LLMs has focused on metrics such as gradient/Taylor [33], cosine similarity [34], and perplexity on a calibration dataset [26].\nOwing to their enormous size, computing gradient information on modern LLMs is prohibitively memory and compute-intensive, and one of our primary goals is to avoid this expensive step when"}, {"title": "2.3 Obtaining a Pruned Model", "content": "Figure 2 provides an overview of how pruned models are obtained. For a given architecture configuration, we first rank the elements of each axis according to the computed importance and perform trimming (reshaping) of the corresponding weight matrices directly. For neuron and head pruning, we trim MLP and MHA layer weights, respectively. In the case of embedding channels, we trim the embedding dimension of the weight matrices in MLP, MHA, and LayerNorm layers.\nWhen pruning attention heads, we add the residual info from the pruned heads back into the remaining heads, with the aim of preserving relevant knowledge from the pruned heads. This idea is an MHA analog of Layer Collapse [53] for depth pruning and provides a boost to model accuracy in our experiments. Formally, given L original attention heads $head_1, head_2, ..., head_L$, being pruned to K heads, each new head will have the form (for the ith head): $head_i + (head_i \u2013 head_{2K-i+1})$ for $i \\in [K - (L - K), K]$. In case of grouped query attention [3], we apply this strategy only to the query heads."}, {"title": "3 Retraining", "content": "We use the term retraining to refer to the accuracy recovery process following pruning. In this paper, we explore two retraining strategies: (1) conventional training, leveraging ground truth labels, and (2) knowledge distillation using supervision from the unpruned model (teacher).\nRetraining with Knowledge Distillation: Knowledge Distillation (KD) involves transfer of knowledge from a larger or more complex model called the teacher to a smaller/simpler model called the student [20]. The knowledge transfer is achieved by having the student model mimic the output and/or the intermediate states of the teacher model. In our case, the the uncompressed and pruned models correspond to the teacher and student, respectively.\nThe output probability distribution of an LLM for a given token $x_i$ is computed as:\np(x_i, t) = \\frac{exp(\\frac{xi}{\\tau})}{\\sum_{j=1}^{|V|}exp(\\frac{xj}{\\tau})}\nwhere $\\tau$ is the softmax temperature and |V| is the vocabulary size. Logit-based KD loss across the sequence of all output tokens is represented as\nL_{logits} = \\frac{1}{S} \\sum_{k=1}^{S}Loss(p_t(x_i, t), p_s(x_i, t))\nhere, $p_t(x_i, t)$ and $p_s(x_i, t)$ represent the teacher and student probability distributions on the $k^{th}$ token, respectively, and I represents the sequence length.\nFor distillation, we explore various loss functions, and several combinations of intermediate states and mappings across the Transformer model as the loss components, along with their respective trade-offs. This is illustrated in Figure 4. The intermediate state-based KD loss across a sequence of Transformer-specific hidden states is represented as\nL_{is} = \\frac{1}{|H|} \\sum_{k \\in H} \\frac{1}{S} \\sum_{i=1}^{I}Loss(h_t^k, h_s^k)\nwhere $h_t^i$ and $h_s^i$ represent the $k^{th}$ teacher and student hidden state for the $i^{th}$ token, respectively, and I represents the sequence length; H is the set of chosen intermediate states. The mismatch"}, {"title": "4 Experiments and Results", "content": "We evaluate our pruning strategy on the Nemotron-4 family of models [42]; specifically, we compress the Nemotron-4 15B model with 15.6 billion parameters down to two target parameter ranges: (1) 8 billion, and (2) 4 billion. We use the NVIDIA Megatron-LM framework [45] to implement our pruning and distillation algorithms for compression and retraining.\nData and Training Hyperparameters: we use the Nemotron-4 curated 8 trillion token (8T) base pretraining dataset and the continued training dataset (CT) [42]. We use the 8T training blend for all our ablations and use a combination of both data blends to retrain our final models. Unless otherwise specified, we use 1.8 billion tokens (400 steps) for lightweight retraining. The calibration dataset D used for importance estimation consists of 1024 samples drawn randomly from the full dataset. We use the same optimizer settings and data split as [42] with cosine LR decay schedule from 2-4 to 4.5-7.\nDownstream Tasks: following Touvron et al. [47], we evaluate our models of similar size on a series of downstream tasks, including MMLU [19], HumanEval [8] for Python code generation, several question-answering datasets for common-sense reasoning: Arc-C [10], HellaSwag [54], TruthfulQA [29] and WinoGrande [43] and XL-Sum English [17] for summarization. We report the 5-shot performance on MMLU, 5-shot on Winogrande, 25-shot on ARC-Challenge, 10-shot on HellaSwag, 0-shot on 20% of XL-Sum and average pass@1 scores for HumanEval and MBPP. For pass@1 scores we use a temperature of 0.2 and nucleus sampling [22] with top-p = 0.95. For instruction-tuned models, we use MT-Bench [55], Instruction-Following Eval (IFEval) [57], ChatRAG-Bench [30], and Berkeley Function Calling Leaderboard (BFCL) [52]."}, {"title": "4.1 Main Pruning Results", "content": "We start by introducing the following list of structured compression best practices:\n1. To train a family of LLMs, train the largest one and prune+distill iteratively to smaller LLMs.\n2. Use (batch=L2, seq=mean) importance estimation for width axes and PPL/BI for depth.\n3. Use single-shot importance estimation; iterative provides no benefit.\n4. Prefer width pruning over depth for the model scales we consider (< 15B).\n5. Retrain exclusively with distillation loss using KLD instead of conventional training.\n6. Use (logit+intermediate state+embedding) distillation when depth is reduced significantly.\n7. Use logit-only distillation when depth isn't reduced significantly.\n8. Prune a model closest to the target size.\n9. Perform lightweight retraining to stabilize the rankings of searched pruned candidates.\n10. If the largest model is trained using a multi-phase training strategy, it is best to prune and retrain the model obtained from the final stage of training.\nWe arrive at this list through a detailed set of ablations and experiments, and each point is backed by empirical evidence, as we demonstrate in the rest of this section and the Appendix. We use this list to obtain our MINITRON pruned and retrained models, whose performance is shown in Tables 3 and 4. Here, we compare the performance of our pruned models to multiple baselines: (1) the original Nemotron-4 15B model, (2) the previous generation Nemotron-3 8B model, and (3) a set of similarly-sized community models, all trained from scratch with trillions of tokens. Evaluation is performed on the downstream tasks described earlier in this Section. In both tables, we list the number of full and non-embedding parameters, along with the number of training tokens used to arrive at the model.\nWe further compare the MINITRON models to state-of-the-art depth and width-pruned baselines in Table 5; namely, LLM-Pruner [33], SliceGPT [4], LaCo [53], ShortGPT [34], and Sheared LLaMa [51]. Table 2 lists the architecture details of the Nemotron and MINITRON models shown in Tables 3 and 4. In the following subsections, we will go into more detail on how we arrived at the MINITRON pruned models.\nFrom Table 3, we notice that MINITRON 8B compares favorably to the latest community models of the same size. Specifically, we outperform Nemotron-3 8B and LLaMa-2 7B, and perform on par with Mistral 7B, Gemma 7B and LLaMa-3 8B, all while using significantly fewer training tokens. MINITRON 8B also significantly outperforms multiple depth-pruned models of larger size (~ 10B parameters) (Table 5). From Table 4, we notice that our smaller model, MINITRON 4B, retains model capabilities better compared to small specialized models that score highly only on some tasks, outperforms the Gemma2 model and is significantly superior to multiple depth and/or width pruned models shown in Table 5."}, {"title": "4.2 Obtaining the Best Pruned Model", "content": "Best Aggregation Metric (Best Practice #2): we start by exploring the best aggregation metric for use with our activation-based pruning criteria (see Section 2.2 for more details). Table 15 (Appendix) shows how zero-shot LM loss and Wikitext2 perplexity [35] vary w.r.t different intra-batch and sequence aggregation functions. Here, the Nemotron-4 15B model is pruned to the Nemotron-3 8B architecture with no retraining. We notice that there is significant variation in zero-shot performance based on the aggregation metric, indicating the importance of selecting the right one. Both (batch=L2, seq=mean) and (mean, mean) perform well; in the remainder of the paper, we use (12, mean) primarily due to its slightly better performance on the 8T dataset. To further evaluate if these relative rankings hold after retraining, we perform a related experiment: we prune the same 15B model to 8B using: (1) the best ((L2, mean) metric, and (2) a poorly performing (L2, L2) metric, and perform retraining on both for 400 steps (~1.8B tokens) The results of this"}, {"title": "4.3 Retraining and Search", "content": "Distillation vs. Conventional Training (Best Practice #5): in this experiment, we train a 4B parameter model and compare: (1) train with random initialization (4B-Random-Init); prune 15B to 4B, then (2) retrain with conventional training (4B-Pruned), and (3) retrain with distillation using the 15B model as the teacher (4B-Pruned-Distill). Since distillation adds training overheads (additional"}, {"title": "5 Related Work", "content": "Structured LLM Pruning: there have been a number of recent structured pruning papers specifically targeting LLMs; we can broadly classify these works into two main categories: (1) ones that prune only depth (layers), (2) ones that prune width (attention heads, MLP intermediate dimension, etc.) and/or depth. Recent work in the first category (depth pruning) includes ShortGPT [34], LaCo [53], and Shortened LLaMa [26]; for pruning layers in MINITRON models, we reuse and extend the metrics proposed in some of these works (eg: block importance from ShortGPT [34]). A number of recent papers have also proposed new saliency metrics and pruning strategies targeting width dimensions: namely, embedding channels, attention heads, and MLP intermediate channels [11, 4, 51, 33]. Most work in this category uses learnable masks, combined with an Augmented Lagrangian loss formulation to arrive at optimal width masks [4, 51, 33]. At LLM scale, this strategy has multiple disadvantages: (1) it requires compute and memory-intensive gradient computations, and (2) it requires a considerable amount of data and fine-tuning to arrive at reasonable masks. The notable exception in this line of work is Dery et al. [11], which recognizes these limitations and proposes saliency metrics that can be computed with only forward passes. To the best of our knowledge, we provide the first pruning strategy that (1) simultaneously targets both width and depth dimensions, (2) works at LLM scale (i.e., uses only forward passes for computing importance and uses a small fraction of pretraining data), and (3) achieves state-of-the-art compression and accuracy.\nPost-pruning Accuracy Recovery: recent work has leveraged either a teacher model which is larger/better [2, 27] or teacher-generated synthetic data [1, 16, 36, 37] to improve the accuracy of an existing trained smaller base model in the Supervised Fine Tuning (SFT)/instruction following setting. Compared to recent width and depth pruning work [26, 34, 51], to the best of our knowledge, we are the first to employ distillation from an uncompressed teacher to improve the retraining of structurally-pruned student models."}, {"title": "6 Acknowledgments", "content": "We would like to thank Ameya Sunil Mahabaleshwarkar, Hayley Ross, Brandon Rowlett, Oluwatobi Olabiyi, Ao Tang, and Yoshi Suhara for help with producing the instruction-tuned versions of MINITRON; additionally, James Shen for TRT-LLM support, and Sanjeev Satheesh, Oleksii Kuchaiev, Shengyang Sun, Jiaqi Zeng, Zhilin Wang, Yi Dong, Zihan Liu, Rajarshi Roy, Wei Ping, and Makesh Narsimhan Sreedhar for help with datasets. We'd also like to gratefully acknowledge the insightful discussion and feedback from Chenhan Yu and Daniel Korzekwa."}, {"title": "7 Conclusions", "content": "This paper has presented a thorough empirical exploration of structured pruning and retraining in LLMs, offering unique insights into pruning order, effects of combining pruning axes, and retraining techniques for minimal data use. We have developed a set of compression and retraining best practices, backed by extensive empirical evidence, which we employ to prune the Nemotron-4 15B model by a factor of 2-4x. Our compressed MINITRON models are significantly cheaper to obtain compared to training each model from scratch (requiring up to 40\u00d7 fewer training tokens), while still performing favorably to a number of similarly-sized community models; MINITRON models also outperform multiple state-of-the-art depth and width pruned models from the literature."}, {"title": "A Appendix", "content": "A.1 Width Pruning\nBest Aggregation Metric for Width Pruning: Results post-pruning (zero-shot) are shown in Table 15 and after lightweight retraining in Figure 5.\nTable 15: Zero-shot performance of activation-based importance with different batch and sequence aggregation metrics. LM loss is reported on the validation set of the 8T and WikiText2 datasets.\nFigure 5: Retraining of searched candidates with 1.8B training tokens.\nFigure 6: Comparing depth and width pruning."}, {"title": "A.2 Retraining with Distillation", "content": "Choice of loss function: In our experiments with the previous generation of Nemotron models in Table 17, we see that KLD consistently outperforms R-KLD, cosine and MSE. WSL-KD [56] also performs inferior to KLD. Hence, we do not repeat all these studies with the experiment setup in section 4, rather only a subset as shown in Table 18."}, {"title": "A.3 Choice of Losses", "content": "1. Using loss $L_o$ based on the output activations of encoder block provides a boost.\n2. The final 1-2 layers in a Transformer for LLM are highly specialized [12] and mapping hidden states across (last-2):(last-2) layers for both the student and teacher achieves the best result [31].\n3. Using word embeddings based loss($L_{emb}$) improves accuracy.\n4. Computing loss $L_{att}$(attention relation loss [48]) based on query, key and value states does not show any improvement.\n5. Adding loss $L_i$ based on the input to MLP makes no difference.\n6. We weren't able to experiment with attention scores due to Flash Attention abstractions.\n7. Mapping multiple transformer layers either results in no improvement or accuracy degradation [31].\n8. Cosine similarity loss performs the best.\nResults are shown in Table 19."}, {"title": "A.4 One-shot pruning and distillation vs Iterative pruning and distillation", "content": "One-shot vs iterative within one dimension: In order to understand the best prune-retrain strategy considering a single dimension that can be pruned across the model (depth), we experiment with two different approaches for depth pruning and retraining in order to arrive at the MINITRON 8B-Depth-pruned model mentioned above."}, {"title": "A.5 Search", "content": "All the feasible 8B candidates produced by search are shown in Table 20."}, {"title": "A.6 Compute Resources", "content": "All experiments were performed on 16\u00d7 NVIDIA DGX A100 nodes(8\u00d7 A100 80GB) for short turnaround times."}]}