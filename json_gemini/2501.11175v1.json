{"title": "ProKeR: A Kernel Perspective on Few-Shot Adaptation of Large Vision-Language Models", "authors": ["Yassir Bendou", "Amine Ouasfi", "Vincent Gripon", "Adnane Boukhayma"], "abstract": "The growing popularity of Contrastive Language-Image Pretraining (CLIP) has led to its widespread application in various visual downstream tasks. To enhance CLIP's effectiveness and versatility, efficient few-shot adaptation techniques have been widely adopted. Among these approaches, training-free methods, particularly caching methods exemplified by Tip-Adapter, have gained attention for their lightweight adaptation without the need for additional fine-tuning. In this paper, we revisit Tip-Adapter from a kernel perspective, showing that caching methods function as local adapters and are connected to a well-established kernel literature. Drawing on this insight, we offer a theoretical understanding of how these methods operate and suggest multiple avenues for enhancing the Tip-Adapter baseline. Notably, our analysis shows the importance of incorporating global information in local adapters. Therefore, we subsequently propose a global method that learns a proximal regularizer in a reproducing kernel Hilbert space (RKHS) using CLIP as a base learner. Our method, which we call ProKeR (Proximal Kernel ridge Regression), has a closed form solution and achieves state-of-the-art performances across 11 datasets in the standard few-shot adaptation benchmark.", "sections": [{"title": "1. Introduction", "content": "Large scale vision-language models (VLMs) trained with contrastive learning have gained an increasing attraction in recent years. These models have shown outstanding performances across a wide range of tasks such as classification, segmentation and video understanding. CLIP is one of the most established VLMs offering remarkable zero-shot capabilities in various downstream tasks. It operates using only the class label within a textual prompt such as \"A photo of a {CLASS}\" where {CLASS} is the groundtruth text label for each class. However, it is well-known that the zero-shot performance of CLIP is limited in scenarios with large domain shift from the pre-training distribution. To further improve CLIP's generalization, multiple follow-up works proposed to include few-shot data which has shown substantial performance gains compared to zero-shot CLIP.\nFew-shot adaptation of CLIP can be categorized into two types of methods based on whether they require fine-tuning on the few-shot samples or not. Among fine-tuning based methods, prompt learning consists of learning continuous tokens instead of hand-crafted templates in CLIP as proposed by CoOp and CoCoOp. In addition, adapter-based fine-tuning methods operate in the feature space to train their classifiers. Despite their promising performance on downstream tasks, fine-tuning methods require additional training costs to learn the new set of parameters. Furthermore, Tip-Adapter proposed a training-free few-shot adaptation alternative. Using a caching mechanism, it captures knowledge from the few-shot samples without additional fine-tuning and ensembles it with zero-shot CLIP.\nThis cache model has shown a significant improvement over the zero-shot performance, which has led to follow-up work such as APE and Tip-X.\nIn order to understand caching effectiveness and limitations, we undertake a theoretical analysis to explore the nature of Tip-Adapter. We begin by demonstrating that the Tip-Adapter adaptation term is a modified version of the well-known Nadaraya-Watson (NW) estimator, a local nonparametric regression method that allows Tip-Adapter to effectively capture various distributions. However, the NW estimator is also known to be strongly biased (e.g. Fig. 1). To mitigate this bias, we leverage existing tools from the literature of kernel methods. An effective extension to NW estimator is locally linear regression. Initially, by fitting a local linear regression around each test point using a closed-form solution, we significantly improve the performance of Tip-Adapter. However, the parameters are estimated locally, which is prone to overfitting in high-dimensional problems.\nOur analysis shows that caching methods can be understood as local nonparametric regression methods regularised through CLIP pointwise zero-shot predictions. However, this regularization only acts locally and does not provide any global information about the few-shot task. Conversely, recent training-based methods rely on global regularizers to incorporate global information . Hence, we ask ourselves the question: how can we leverage global regularizers for few-shot adapters while conserving the benefits of training-free methods ? In order to design such global regularizer, we devise two important design choices:\n\u2022 Firstly, we restrain the hypothesis space of the learned function to be a reproducing kernel Hilbert space (RKHS).\n\u2022 Second, using the RKHS norm, we introduce a proximal regularization term to ensure that the obtained solution is close to the base predictor i.e. fclip. Thanks to the properties of the RKHS, minimizing the difference between two functions using the RKHS norm ensures that they are close pointwise. Our method ProKeR provides a more effective way to preserve prior knowledge from the zero-shot predictor and maintains the expressive capacity of the learned functions. Through extensive experiments, we show the effectiveness of our method ProKeR which achieves consistent gains over state-of-the-art training-free methods on standard few-shot classification benchmarks with an absolute average improvement of 3.94% accuracy. In addition, we highlight the robustness and generalizability of the proposed method across different architectures and on out-of-distribution datasets."}, {"title": "Summary of contributions:", "content": "1. We frame the cache model of Tip-Adapter as a Nadaraya-Watson estimator, a classical kernel regression method, and provide a theoretical understanding of how Tip-Adapter functions.\n2. Under this new perspective, we propose multiple improvements on Tip-Adapter either through a closed-form local linear regression fit or by incorporating global information.\n3. Our analysis leads to our new proposed method ProKeR, a training-free method that leverages global regularization in a reproducing kernel Hilbert space. Through extensive experiments, ProKeR outperforms existing methods and sets a new state-of-the-art on standard few-shot classification benchmarks."}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Vision-Language Pre-trained Models", "content": "Visual-language models (VLMs) have gained an increasing popularity lately. These methods, exemplified by CLIP , DeCLIP and ALIGN , employ a contrastive learning framework to learn a vision encoder and a language encoder with a shared representation space between text and images. Trained on large-scale datasets of image-text pairs, VLMs have shown remarkable zero-shot capabilities on downstream tasks without additional fine-tuning, paving the way for open vocabulary recognition. VLMs have been extended to few-shot classification as well as other tasks beyond image classification such as video understanding, image segmentation, image generation and 3D reconstruction ."}, {"title": "2.2. Few-shot Adaptation", "content": "Few-shot adaptation methods in the context of classification can be categorized into prompt learning and adapter based approaches. Inspired by the recent advances in natural language processing , prompt learning aims to learn effective global text or visual prompts for the downstream tasks [2, 6, 23, 45, 47, 49, 58, 61, 66, 67]. Although prompt learning methods have brought significant improvements over the zero-shot baseline, they require back-propagating through the entire text encoder and require having access to the text encoder . On the other hand, adapter-based approaches operate in the feature space and do not require having access to the pre-trained model weights. We distinguish two families of adapter-based approaches. The first one is training-based methods which either train a linear layer or a two-layer MLP such as CLIP-Adapter to perform residual feature blending of the zero-shot classifier.\nWhile fine-tuning adapters have achieved a good level of performance, they still require additional training time which is impractical under limited resources and might suffer from the caveats of gradient optimization. To alleviate these issues, Tip-Adapter emerges as a training-free solution based on a key-value cache model. Building on Tip-Adapter,"}, {"title": "3. Method", "content": "We expose in this section the details of the proposed method. Our starting point consists in framing Tip-Adapter as a kernel method. Thanks to this new perspective, we develop multiple improvements tailored for it. Conclusively, we propose ProKeR, a method that introduces a global proximal regularization in a reproducing kernel Hilbert space (RKHS)."}, {"title": "3.1. Tip-Adapter as a Nadaraya-Watson estimator", "content": "As illustrated in Fig. 2, let $x \\in \\mathbb{R}^D$ be the features of an input query image extracted using the visual encoder of CLIP, $S \\in \\mathbb{R}^{NK \\times D}$ the visual features of the training set and $L \\in \\mathbb{R}^{NK \\times D}$ the associated matrix of one-hot labels where N is the number of classes and K is the number of shots per class. Let $W_{clip} \\in \\mathbb{R}^{D \\times N}$ be the text prototypes of the classes extracted with the text encoder using the standard hand-crafted templates in the form of \u201ca photo of a {CLASS}\" [28, 62, 67]. The zero-shot predictor from CLIP is defined as $f_{clip} : x \\mapsto xW_{clip}$.\nTo alleviate $f_{clip}$ prediction errors due to lack of generalization, Tip-Adapter utilizes a cache model to learn knowledge from the few-shot samples. The predicted logits can be formulated as:\n$\\phi_{Tip}(x) = f_{clip}(x) + \\alpha \\exp(-\\beta (1 - xS^T)) L$,\nwhere $ \\beta$ is a smoothing scalar and $\\alpha$ controls the balance between textual features and few-shot images. Note that since the features in CLIP are normalized (i.e. $|\\vert x \\vert |_2 = 1$), we can rewrite Eq. (1) as:\n$\\phi_{Tip}(x) = f_{clip}(x) + \\alpha \\sum_{i=1}^{NK} \\exp(-\\frac{\\beta}{2} |\\vert S_i-x \\vert |_2^2) L_i$,\nwhere $S_i$ is the i-th few-shot sample. Interestingly, the right term of Tip-Adapter can be seen as a modified version of the well-known Nadaraya-Watson (NW) estimator [33] with a Radial Basis Function (RBF) kernel [37]:\n$\\phi(x) = \\frac{\\sum_{i=1}^{NK} k_\\beta(d(x, S_i))L_i}{\\sum_{i=1}^{NK} k_\\beta(d(x, S_i))}$ where $k_\\beta(\\nu) = (\\frac{\\beta}{2\\pi})^{\\frac{D}{2}} \\exp(-\\frac{\\beta}{2} \\nu)$,\nwhere d is the distance between the query image and the shots in the feature space. In essence, when d is the"}, {"title": "3.2. Training-free few-shot adapters as a Bayes optimal mapping", "content": "We formulate the few-shot visual-language adaptation as a Bayes optimal mapping [17] associated to the following pointwise risk:\n$R(x, \\phi(x)) = E_{Y|X}[s(Y, \\phi(X)) + R_{clip} | X = x]$,\nwhere s is a cost function and $R_{clip}$ is a regularization term using CLIP prediction independent of Y. Here, X and Y are random variables representing the image features and the labels respectively.\nThe pointwise Bayes optimal mapping is defined as [17]:\n$\\phi(x) = arg \\min_{q \\in M} R(x, q)$\n$= arg \\min_{q \\in M} \\int s(y, q)dx(y) + R_{clip}$,\nwhere $d_{\\mu x}$ is the conditional probability of Y conditioned on X = x and M is the output space. Following [17], we leverage kernel estimators to rewrite the adaptation problem as:\n$\\phi(x) = arg \\min_q \\frac{1}{NK} \\sum_{i=1}^{NK} k_s(d(x, S_i)) s(q, L_i) + R_{clip}$.\nThis formulation offers a new perspective on the adaptation, where for each test point the cost function is minimized over the output space, with a weighting from each training sample. The regularization term guarantees that the obtained predictions are not far from zero-shot CLIP.\nInterestingly, this formulation paves the way for considering different choices of cost functions s, regularization terms $R_{clip}$ and kernels $k_s$. The consistency of the obtained estimator is discussed in [17] where the solution of Eq. (7) is obtained using a gradient descent algorithm. While this optimisation can be time consuming, there are cases where a closed form solution can be derived. For instance, when s is the squared Euclidean distance and $R_{clip} = \\frac{\\lambda}{2} |\\vert q - f_{clip}(x) \\vert |^2$, we can derive the following solution:\n$\\phi(x) = \\frac{\\lambda NK}{\\lambda NK + Z(x)} f_{clip}(x) + \\frac{1}{\\lambda NK + Z(x)} \\sum_{i=1}^{NK} k_s(d(x, S_i))L_i$, where $Z(x) = \\sum_{i=1}^{NK} k_s(d(x, S_i))$,\nwhere $ \\lambda$ is a regularization term that balances the predictions of zero-shot CLIP and the local fit. The obtained closed form of the estimator in Eq. (7) is equivalent to Tip-Adapter up to a scaling factor which depends on each input x. The main difference between the two formulations in Eq. (2) and Eq. (8) is that the second term in Eq. (8) is agnostic to the training size and is query dependent.\nAlthough the Nadaraya-Watson estimator is a nonparametric model that allows to capture any type of distribution, it is well known to suffer from poor bias at the boundaries of the training samples . In the following, we propose two methods to alleviate this bias."}, {"title": "3.3. Local Linear Regression", "content": "A standard way to alleviate the boundary bias of the NW estimator is by moving from a local constant fit to a local linear fit around each test point. Instead of using the estimate from Eq. (7), local linear regression (LLR) forms the local estimate $ \\phi(x) = \\tilde{x} A$, where $\\tilde{x} = [1 x]$, and $A\\in \\mathbb{R}^{(d+1)c}$ minimizes the following problem:\n$\\min_A \\frac{1}{NK} \\sum_{i=1}^{NK} k_s(d(x, S_i)) s(\\tilde{S_i} A, L_i) + R_{clip}$,\nwhich is a weighted ordinary least square problem around each test point weighted by the kernel values $k_s(d(x, S_i))$. Using the same cost function as for Eq. (8) and using the regularization term $R_{clip} = \\frac{\\lambda}{2} |\\vert \\tilde{x}A \u2013 f_{clip}(x) \\vert |^2$, we derive a closed form solution for Eq. (21) as follows:\n$\\phi(x) = \\tilde{x} A^{-1}B$, where $A = S^T \\Omega S + \\lambda NK \\tilde{x}^T\\tilde{x}$ , and $B = S^T \\Omega L + \\lambda NK \\tilde{x}^T f_{clip}(x)$,\nwhere $\\Omega$ is the NK \u00d7 NK matrix with ith diagonal element as $k_\\beta(d(x, S_i))$.\nThis method, that we dub henceforth LLR, boils down to fitting a local linear regression while enforcing the obtained fit to be close to the text predictions locally around the query input sample. While LLR usually improves performance, it is time consuming as one needs to fit a linear regression for each input sample. Furtheremore, the parameters of LLR"}, {"title": "3.4. Local methods with a global metric", "content": "While there exists multiple strategies to construct a good metric , using the Mahalanobis distance with the covariance matrix estimated from the training data is a simple yet well-performing one [3]:\n$d(x, S) = |x - S|_\\Lambda$,\nwhere $\\Lambda$ is the estimated precision matrix from the few-shot samples. This metric effectively incorporates global information and captures the geometry of the space which allows to construct a better kernel function tailored to the downstream task. The classical RBF kernel corresponds to using an isotropic covariance matrix. While still being a local method, adapting the metric to the downstream task allows to incorporate global information about the underlying distribution and improves over Tip-Adapter as shown in Fig. 3."}, {"title": "3.5. Proximal Kernel Ridge Regression", "content": "As can be seen in Fig. 3, incorporating global information about the task through a global metric effectively outperforms both Tip-Adapter and LLR. However, the choice of the global metric for NW estimator beyond the Mahalanobis distance remains challenging , especially in the few-shot setting. Furthermore, despite being equipped with a global metric, the NW estimator remains in essence a local method and still lacks a global regularization. Whilst the use of a global regularization has been recently addressed in training-based methods , using a truly global regularization in a training-free manner remains challenging and unexplored.\nThese limitations highlight the necessity for regularization in this adaptation process. The main idea is to balance the need to maintain the expressive capacity of the learned functions while ensuring stability and robustness. To this end we introduce two key design choices: (1) we constrain the hypothesis space of the learned function to a reproducing kernel Hilbert space (RKHS), and (2) we incorporate a proximal regularization term, based on the RKHS norm, to ensure the solution remains close to the base predictor ($f_{clip}$). The RKHS norm's properties guarantee that minimizing the difference between two functions in this space results in pointwise closeness. Consequently, this proximal term serves as a global regularization that preserves prior knowledge from the zero-shot predictor, resulting in more robust solutions that are less prone to overfitting on the few-shot data.\nGiven the multi-output nature of the problem, the employed reproducing kernel $K_\\beta$ is a reproducing kernel for vector valued functions. The main difference is that the kernel is matrix valued. Several instances of multi-output kernels have been proposed in the literature , with separable kernels being among the most widely used for learning vector-valued functions due to their simplicity and computational efficiency. These kernels are formulated as a product of a kernel function for the input space alone and a matrix that encodes the interactions among the outputs. Let us consider $K_\\beta : \\mathbb{R}^D \\times \\mathbb{R}^D \\rightarrow \\mathbb{R}^{N \\times N}$ as a separable kernel of the form:\n$(K_\\beta (x,x'))_{j,j'} = k_\\beta(x,x')B$,\nwhere B is a N \u00d7 N symmetric and positive semi-definite matrix which captures the correlations between the outputs. A simple yet effective choice for B is the identity matrix where all outputs are treated as being unrelated.\nOur goal is to learn a multi-output predictor $\\phi$ using the following objective:\n$\\min_{\\Phi \\in H} \\sum_{i=1}^{NK} ||\\Phi(S_i) - L_i||^2 + \\frac{\\lambda}{2} ||\\Phi - f_{clip}||^2$.\nBy the representer theorem , the unique minimizer of problem in Eq. (13) emerges naturally as the solution of a Kernel Ridge Regression (KRR) problem:\n$\\Phi = f_{clip} + \\sum_{i=1}^{NK} k_\\beta(S_i, .) \\gamma_i$,\nwhere $ \\gamma = (\\Gamma + k_\\beta(S, S))^{-1}(L \u2013 f_{clip}(S))$.\nHere, $(k_\\beta(S, S))_{i,j} = k_\\beta(S_i, S_j)$ and $\\gamma_i \\in \\mathbb{R}^N$. This approach allows to map data to an infinite dimensional space. Furthermore, the regularization term allows the use of a richer model that captures the complex structure of the data while preserving its smoothness, avoiding overfitting on the few-shot data."}, {"title": "3.6. Mercer decomposition of kernel methods", "content": "One additional benefit of this perspective on caching methods is memory reduction which allows to overcome the necessity of storing training data. For positive definite kernel, we can leverage Mercer theorem [13] to write the kernel function as follow:\n$k_\\beta(x, x') = \\psi_\\beta(x)^T \\psi_\\beta(x)$.\nThis allows us to write:\n$\\sum_{i=1}^{NK} k_\\beta(S_i, x) \\gamma_i = \\sum_{i=1}^{NK} \\psi_\\beta(x) \\psi_\\beta(S_i)^T \\gamma_i = \\psi_\\beta(x) [\\Psi_1, ..., \\Psi_N]^T \\Gamma$,\n1"}, {"title": "3.7. Training-based ProKeR", "content": "In addition to our training-free method, we propose a trained version of ProKeR. The optimization problem in Eq. 13 allows for the use of any base learner and is not limited to the zero-shot CLIP. In this training-based version, instead of using the zero-shot CLIP as the base learner to regularize our method in the RKHS, we iteratively optimize for a regularizer and the obtained solution from ProKeR. Our method consists of training a linear classifier using the same loss $L_{CLAP}$ proposed in CLAP [50] while solving the linear system of ProKeR at each iteration as follows:\n$\\min_W L_{CLAP}(W,S,L)$, where $w(x) = xW + \\sum_{i=1}^{NK} k_\\beta(S_i, W) \\gamma_i$, and $\\gamma = (I + \\frac{1}{\\Gamma} k_\\beta(S, S))^{-1}(L \u2013 SW)$.\nBy jointly optimizing the base learner and the solution of the Kernel Ridge Regression problem, our method (ProKeR + CLAP) allows to learn a regularizer specific to the few-shot task which takes into account the latter problem."}, {"title": "4. Experiments", "content": "In this section, we evaluate our method on multiple image classification benchmarks. We compare our results to existing training-free methods, notably Tip-Adapter , which is the baseline of our work. For a fair comparison, we use the same text inputs for all reported methods. We also compare to other state-of-the-art conventional methods, such as APE , Tip-X , GDA , CLIP and CALIP which proposes a parameter-free attention mechanism to improve CLIP in a zero-shot manner. We run APE with the same text templates as Tip-Adapter using their official implementation. Finally, we report the average running times on ImageNet [9] for each method using an NVIDIA RTX A6000 GPU. For completeness, we additionally provide a comparison of our method with existing state-of-the-art training-based methods."}, {"title": "4.1. Datasets and Evaluation Protocol", "content": "For comprehensive evaluation, we adopt 11 image classification benchmarks: ImageNet [9], Caltech101 [11], DTD [8], EuroSAT [18], FGVCAircraft [31], Flowers102 [35], Food101 [4], OxfordPets [40], Stanford-Cars [24], SUN397 [57], and UCF101 [51]. For testing the generalization ability of our method, we further test on ImageNet-Sketch [48] and ImageNet-V2 [44].\nFor a fair comparison with previous work, we use ResNet-50 for the visual encoder of CLIP unless mentioned otherwise. We follow two settings for our experiments. The first setting, initially introduced by [50] for training-based methods, consists of selecting the best hyperparameters of each method on ImageNet and transfer them to the other datasets and report the average performance. This setting reflects real-life scenarios where a validation set may not be available especially in a few-shot regime. The second setting follows CoOp's benchmark [67] where validation shots are used to select the hyperparameters and evaluate the results on the full test set."}, {"title": "4.2. Experiment Results and Analysis", "content": ""}, {"title": "4.2.1. Comparison with alternatives to Tip-Adapter", "content": "We compare in Fig. 3 our result when improving Tip-Adapter using kernel based approaches across 11 datasets. Our reformulation in Eq. (8) outperforms Tip-Adapter for 8 and 16 shots and maintains the same level of performance in the lower shot setting. We argue that this is due to the fact that our reformulation is agnostic to the training size. Debiasing the NW estimator using our regularized LLR in Eq. (21) significantly improves the performance as a first order polynomial fit compared to the constant fit of the NW estimator. Additionally, the use of the Mahalanobis distance as global metric for NW estimator further increases the performance especially with more shots as the estimation of the precision matrix from the training set becomes more accurate. Furthermore, by introducing a proximal regularization in the RKHS, our method ProKeR significantly outperforms Tip-Adapter as well as all the proposed alternatives especially with more shots."}, {"title": "4.2.2. Comparison with training-free methods", "content": "We report in Tab. 1 the performance of different methods in a realistic and practical validation-free experimental setting. First, our method ProKeR outperforms existing alternatives on average by a large margin (2.04% compared to the second best), especially in the low shot regimes (1, 2 and 4) and is only outperformed in the 8 shot setting. Furthermore, when using a Polynomial kernel (as defined in Tab. 5), our method sets a new standard in the 16 shots regime."}, {"title": "4.2.3. Generalization Ability", "content": "In Tab. 3, we test the generalization ability of the different methods on out-of-distribution datasets. Notably, the shots are drawn from ImageNet and the test set is drawn from either ImageNet-V2 or ImageNet-Sketch. ProKeR achieves state-of-the-art performance for training-free methods on both in-distribution and out-of-distribution datasets."}, {"title": "4.2.4. Comparison with training-based methods", "content": "We report in Tab. 4 the comparison of our method with training-based methods. While being training-free, our method ProKeR outperforms existing training-based methods on 8 out of 11 datasets. This emphasizes the effectiveness of incorporating a global regularization using the zero-shot predictor in a reproducing kernel Hilbert space (RKHS).\nThe jointly trained version of ProKeR with CLAP achieves significantly better results compared to existing training-based methods and increases the average performance of the ProKeR by 1.73% accuracy. This shows the importance of using a strong base learner. Furthermore, the joint training allows to leverage both the expressive power of the RKHS and the flexibility of the linear classifier."}, {"title": "4.3. Kernel abltation", "content": "So far, we have performed our analysis using the RBF kernel, a commonly used kernel in the kernel literature and in cache-based methods. Nevertheless, through the lens of our kernel perspective on cache-based methods, different kernels can be considered ranging from a linear kernels to more elaborate ones. We perform in Tab. 5 an ablation study where we discuss different kernel choices. Besides RBF, we consider three commonly used kernels: the Linear kernel, the Epanechnikov kernel and the Polynomial kernel. The RBF"}, {"title": "4.4. Ablation on CLIP architectures", "content": "In Tab. 6, we report the performance of training-free methods using different backbones on ImageNet for 16 shots. Our method consistently outperforms the alternatives across all architectures. While all methods improve with more capable architectures, the gap between our method and the second best one (APE) remains stable."}, {"title": "4.5. Adressing memory limitations of cache-based methods", "content": "In Tab. 2, we report the performance when using Random Fourier features to alleviate the memory limitations of cache-based methods. Using RFFs, we are able to drastically reduce the memory footprint of caching methods while maintaining almost the same performance across different shots. Our method when combined with RFFs still maintains state-of-the-art performance."}, {"title": "4.6. Running times and memory requirements", "content": "Next, we report running time (train and test) for different methods as well as the memory requirements for each method (Tab.7). Our method is on par with Tip-Adapter and GDA in term of speed. Note that APE runs a feature selection step which takes additional time to run, making it slower than the previously mentioned competition. On the other hand, training-based methods are orders of magnitude slower. Regarding the memory complexity, ProKeR stores the training shots similarly to APE and Tip-Adapter. However, when using Random Fourier features, our method does need to store additional training samples."}, {"title": "5. Limitations & Future Work", "content": "Based on our analysis, caching methods can be understood as local non-parametric regressors. These methods lack a global regularization from zero-shot CLIP which limits their generalization ability when performing adaptation. On the other hand, global methods may lack the flexibility when dealing with complex data. As part of future work, we will explore how both local and global methods can be combined to benefit from the best of both worlds. Furthermore, our formulation of caching methods as a Nadaraya-Watson estimator offers multiple options for the choice the regularization term, the metric used in the kernel, as well as the bandwidth selection . These choices constitute different ways to reduce the bias-variance trade-off inherent to these methods ."}, {"title": "6. Conclusion", "content": "In this paper, we proposed a new theoretical understanding of Tip-Adapter, a prominent training-free caching-based approach for LVM few-shot adaptation. Our analysis suggests that Tip-Adapter is a local nonparametric regression that has well-known bias limitation. We proposed multiple angles of improvement that have shown significant amelioration over the Tip-Adapter baseline. Subsequently, we demonstrate that incorporating global information in a training-free method can be achieved using a global regularization in a reproducing kernel Hilbert space (RKHS), which conclusively further improves the state-of-the-art for training-free methods."}, {"title": "A. Detailed derivations", "content": ""}, {"title": "A.1. Nadaraya-Watson estimator", "content": "We first derive the solution of the adaptation problem for the Nadaraya-Waston estimator. The adaptation problem writes:\n$\\phi(x) = arg \\min_q \\frac{1}{NK} \\sum_{i=1}^{NK} k_\\beta(d(x, S_i)) |\\vert q - L_i \\vert |^2 + \\frac{\\lambda}{2} |\\vert q - f_{clip}(x) \\vert |^2$.\nThe derivation of the solution of Eq. 20 is as follows:\n$L = \\frac{1}{NK} \\sum_{i=1}^{NK} k_\\beta(d(x, S_i)) |\\vert q - L_i \\vert |^2 + \\frac{\\lambda}{2} |\\vert q - f_{clip}(x) \\vert |^2$\n$\\frac{dL}{dq} = 0$\n$\\Rightarrow \\frac{1}{NK} \\sum_{i=1}^{NK} k_\\beta(d(x, S_i)) (q \u2013 L_i) + \\lambda q \u2013 \\lambda f_{clip}(x) = 0$\n$\\Rightarrow q \\frac{1}{NK} (\\sum_{i=1}^{NK} k_\\beta(d(x, S_i)) + \\lambda) = \\frac{\\lambda}{NK} f_{clip}(x) + \\frac{1}{NK} \\sum_{i=1}^{NK} k_\\beta(d(x, S_i)) L_i$\n$\\Rightarrow q = \\frac{\\frac{\\lambda}{NK} f_{clip}(x) + \\frac{1}{NK} \\sum_{i=1}^{NK} k_\\beta(d(x, S_i)) L_i}{\\frac{1}{NK} (\\sum_{i=1}^{NK} k_\\beta(d(x, S_i)) + \\lambda)}$"}, {"title": "A.2. Local Linear Regression", "content": "Here, we detail the derivation of the solution of the local linear regression (LLR) in Eq. 10. Let $\\tilde{x} = [1 x]$ and A\u2208 $R^{(d+1)c}$ which minimizes the following problem:\n$\\min_A \\frac{1}{NK} \\sum_{i=1}^{NK} k_\\beta(d(x, S_i)) |\\vert \\tilde{S_i} A \u2013 L_i \\vert |^2 + \\frac{\\lambda}{2} |\\vert \\tilde{x} A \u2013 f_{clip}(x) \\vert |^2$"}, {"title": "B. Sensitivity Analysis of \u5165", "content": "We analyze the sensitivity of A in our method ProKeR (cf Eq. 14) in Tab. 8. We compute the average value for each dataset and study the effect of deviating from this value. Overall, performance is relatively stable in a range between 1/3 and up to 3 times this value, with only a drop of 1.2% in accuracy. Varying lambda up to a fifth or 5 times this value still only leads to a drop of 3%."}, {"title": "C. Comparison per dataset", "content": "To accompany Tab. 2 in the paper, we provide bellow in Fig. 4 per-dataset comparisons of Training-free Methods on 11 image classification datasets on the CoOp's benchmark."}]}