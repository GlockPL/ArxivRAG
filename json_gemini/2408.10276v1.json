{"title": "FEDKIM: Adaptive Federated Knowledge Injection into Medical Foundation Models", "authors": ["Xiaochen Wang", "Jiaqi Wang", "Houping Xiao", "Jinghui Chen", "Fenglong Ma"], "abstract": "Foundation models have demonstrated remarkable capabilities in handling diverse modalities and tasks, outperforming conventional artificial intelligence (AI) approaches that are highly task-specific and modality-reliant. In the medical domain, however, the development of comprehensive foundation models is constrained by limited access to diverse modalities and stringent privacy regulations. To address these constraints, this study introduces a novel knowledge injection approach, FEDKIM, designed to scale the medical foundation model within a federated learning framework. FEDKIM leverages lightweight local models to extract healthcare knowledge from private data and integrates this knowledge into a centralized foundation model using a designed adaptive Multitask Multimodal Mixture Of Experts (M\u00b3OE) module. This method not only preserves privacy but also enhances the model's ability to handle complex medical tasks involving multiple modalities. Our extensive experiments across twelve tasks in seven modalities demonstrate the effectiveness of FEDKIM in various settings, highlighting its potential to scale medical foundation models without direct access to sensitive data.", "sections": [{"title": "1 Introduction", "content": "Similar to large language models (Zhao et al., 2023) and foundation models (Zhou et al., 2023a), medical foundation models (Thirunavukarasu et al., 2023; Moor et al., 2023) have achieved superior performance of handling diverse modalities and tasks within the medical domain. These models have the potential to revolutionize medical diagnostics and treatment by leveraging data-driven insights from large volumes of multimodal healthcare data. Due to the sensitive nature of medical data and the complexity of medical tasks, most existing medical foundation models usually rely on particular public medical datasets. This nature results in limitations of the existing medical foundation models, detailed as follows:\n(1) Unrealistic to conduct large-scale centralized training. The centralized training of medical foundation models presents significant challenges, primarily due to the difficulties in aggregating sensitive healthcare data. Regulations such as the Health Insurance Portability and Accountability Act (HIPAA) in the United States and the General Data Protection Regulation (GDPR) in the European Union impose strict privacy restrictions on the use of personal health information. This regulatory environment makes it impractical to collect and store large amounts of healthcare data in a single location, which is typically required for the effective training of high-performing medical foundation models.\n(2) Limited modality and task adaptability. Current medical foundation models exhibit a high degree of specialization, constraining their effectiveness to a narrow range of downstream tasks within specific modalities, as outlined in Table 1. For instance, MMedLM (Qiu et al., 2024) is tailored for text, while LLava-Med (Liu et al., 2023a) focuses on both image and text modalities. In practical settings, comprehensive medical decisions often require integrating multiple types of health data across various tasks. Yet, by being task or modality-specific, existing models fail to recognize and leverage the intricate relationships between different healthcare data modalities and tasks.\nThe first limitation prevents training a medical foundation model from scratch in a centralized manner, while the second one exacerbates the challenge of developing a multimodal, multi-task medical foundation model. To overcome these obstacles, a viable solution is to scale existing medical foundation models and infuse them with medical knowledge. Given that medical data is stored on private clients, the federated learning (FL) paradigm (McMahan et al., 2017) offers a promising approach, which is a decentralized and collaborative machine learning method where participants do not need to share data directly. Although several recent studies on federated foundation models (Lu et al., 2023; Chen et al., 2024a) have made progress, they primarily focus on enhancing services to local clients using existing foundation models. Importantly, none have specifically tackled the challenge of injecting novel medical knowledge into existing medical foundation models in a federated manner.\nTo tackle this new challenge, in this paper, we propose a novel approach: Federated Knowledge Injection for Medical foundation models (FEDKIM), as shown in Figure 1. FEDKIM adopts a flexible design, allowing it to incorporate various types of medical modalities to handle a variety of medical tasks. Considering the real-world scenarios, FEDKIM deploys the medical foundation model only on the server side and leverages lightweight local models along with classic federated learning approaches to extract healthcare knowledge from private data.\nTo effectively inject extracted medical knowledge into the foundation model, FEDKIM uses knowledge-rich parameters from the modality-specific encoders updated from the local end. To be specific, FEDKIM integrates this knowledge using parameter-efficient fine-tuning technique with a novel multitask multimodal mixture of expert module, namely M\u00b3OE. M\u00b3OE adaptively selects appropriate expert systems for handling specific tasks in given modalities, enabling FEDKIM to deal with tasks in complex medical contexts.\nOur experiments across 12 healthcare tasks with 7 modalities demonstrate the effectiveness of FEDKIM, providing a solid foundation for future exploratory research on the medical knowledge injection problem."}, {"title": "2 The proposed FEDKIM Framework", "content": "In this section, we first introduce the setup of the medical knowledge injection task (Section 2.1). Next, we describe the proposed method, FEDKIM. As depicted in Figure 1, FEDKIM consists of two main components: knowledge extractors (Section 2.2), which are deployed on local clients, and a knowledge injector (Section 2.3), which is deployed on the server."}, {"title": "2.1 Framework Setups", "content": "The goal of this work is to scale and enhance the predictive ability of medical large language models (LLMs) by incorporating medical knowledge from private client data in a federated manner. To achieve this, we employ N clients, each representing a hospital or a medical institute holding private medical data \\(D_n\\). We assume that the private dataset \\(D_n\\) contains all medical modalities \\(\\{M_1,\u2026\u2026,M_M\\}\\) and can perform all tasks \\(\\{T_1,\u2026,T_r\\}\\). Each client trains a model \\(f_n = [\\text{ENC}_n(); \\text{DEC}_n()]\\) using the data \\(D_n\\), where \\(\\text{ENC}_n()\\) is the set of multimodal encoders and \\(\\text{DEC}_n()\\) is the set of multi-task decoders/predictors. Thus, the model parameters \\(\\theta_n\\) of \\(f_n\\) can be divided into \\(\\theta_{\\text{enc}}^n\\) (for the encoder) and \\(\\theta_{\\text{dec}}^n\\) (for the decoder), which will be further upload to the server."}, {"title": "2.1.2 Server Setups", "content": "We deploy a generative medical foundation model on the server, denoted as \\(F\\). We aim to inject medical knowledge represented by \\(\\{\\theta_{\\text{enc}}^1,...,\\theta_{\\text{enc}}^N\\}\\) into \\(F\\) and simultaneously update \\(\\{\\theta_e^0,\\theta_d^0\\}\\) by absorbing new knowledge from \\(F\\). These updated encoders and the aggregated decoders will then be distributed to the corresponding clients for learning in the next communication round. To facilitate the updates of client parameters, we place a small amount of public data on the server, denoted as \\(D_p\\)."}, {"title": "2.2 Client Updates \u2013 Knowledge Extraction from Private Clients", "content": "This framework allows each client to handle T tasks simultaneously. Although these tasks have different training data, the modalities are partially shared, which motivates us to design a simple client model with M modality-specific encoders and T task-specific decoders. Details of the encoders are listed in Appendix D. We then use the following loss to train each client model:\n$$\\min_{\\theta_n} L_n := \\frac{1}{|D_n|} \\sum_{t=1}^T \\sum_{(x,y) \\in D_n^{t,m}} l_t(f_n(x; \\theta_n), y),$$\n$$f_n(x_i; \\theta_n) = \\text{DEC}_{n,t} (\\text{ENC}_{n,m}(x_i; \\theta_{\\text{enc}}^{n,m}); \\theta_{\\text{dec}}^{n,t}),$$where \\(D_n^{t,m}\\) is the task-specific dataset, \\(x\\) and \\(y\\) are the data features and the corresponding ground truths, and \\(l_t\\) is the loss function for a specific task, such as cross-entropy. \\(\\text{ENC}_{n,m} \\subseteq \\text{ENC}_n\\) is the encoder for modality \\(M_m\\) with parameters \\(\\theta_{\\text{enc}}^{n,m}\\). \\(\\text{DEC}_{n,t} \\subseteq \\text{DEC}_n\\) is the decoder for the \\(t\\)-th task with parameters \\(\\theta_{\\text{dec}}^{n,t}\\). The number of modality-level encoders in \\(\\text{ENC}_{n,m}\\) is determined by the input data, while amount of tasks determines the number of task-oriented decoders. After the local training, we will upload the encoder and decoder parameters \\(\\theta_{\\text{enc}}^n\\) and \\(\\theta_{\\text{dec}}^n\\) to the server."}, {"title": "2.3 Server Updates \u2013 Knowledge Injection into Medical LLM", "content": "We assume that the predictive ability of \\(F\\) is better than the uploaded decoders \\(\\{\\theta_{\\text{dec}}^1,..., \\theta_{\\text{dec}}^N\\}\\), and useful knowledge is primarily contained in the encoders \\(\\{\\theta_{\\text{enc}}^1,..., \\theta_{\\text{enc}}^N\\}\\). Thus, on the server side, we aim to inject medical knowledge \\(\\{\\theta_{\\text{enc}}^1,..., \\theta_{\\text{enc}}^N\\}\\) into the LLM \\(F\\) with the help of public data \\(D_p\\). Before the injection, we first aggregate knowledge uploaded from each client in traditional federated learning manners such as FedAvg (McMahan et al., 2017) or FedProx (Li et al., 2020), i.e.,\n$$\\theta_{e} = f_{FL}([\\theta_{\\text{enc}}^1,\u2026\u2026\u2026, \\theta_{\\text{enc}}^N]),$$\n$$\\theta_{d} = f_{FL}([\\theta_{\\text{dec}}^1,\u2026\u2026\u2026, \\theta_{\\text{dec}}^N]),$$where \\(f_{FL}\\) can be flexibly replaced with any federated learning methods, such as personalized FL methods (Jiang et al., 2019; T Dinh et al., 2020), differential privacy-based FL methods (Hu et al., 2020; El Ouadrhiri and Abdelhadi, 2022), or adaptive FL methods (Reddi et al., 2020; Wang et al., 2022b,a)."}, {"title": "2.3.2 Knowledge Injection", "content": "Effectively injecting medical knowledge \\(\\theta_d\\) is challenging since the LLM \\(F\\) cannot directly use these diverse modality-specific encoders. To solve this challenge, we leverage a straightforward yet effective feature alignment strategy that follows the training of LLaVA (Liu et al., 2024) by concatenating the modality embeddings with the task prompt. Subsequently, we embed our original Multimodal Multi-tasking Mixture Of Experts (M\u00b3OE) into the medical foundation model. M\u00b3OE allows the medical foundation model \\(F\\) to adaptively select specific expert system given different combination of tasks and modalities. Next, we detail the process of knowledge injection.\nStep 1: Feature Alignment. For each input data \\((x, y) \\in D\\) from the \\(t\\)-th task, we first obtain its feature representations using the aggregated encoders \\(\\theta^e\\), i.e., \\(e_1^e = [e_1^e];\u2026\u2026 ; e_i^e] = g(\\theta^e(x))\\), where \\(g(\\cdot)\\) is the linear mapping function. We also embed the task prompt \\(P_t\\) using the encoder of \\(F\\), i.e., \\(p_t = \\text{EMB}_F(P_t).\\), where \\(\\text{EMB}_F()\\) is the text embedding layer of \\(F\\). Then, the concatenation of the data feature \\(e\\) and the task prompt feature \\(p_t\\) will be used as the input of the encoder of \\(F\\), denoted as \\(h = [e; p_t]\\).\nStep 2: Multimodal Multi-tasking Mixture of Experts (M\u00b3OE). A naive solution is directly using the aligned feature \\(h\\) to generate the output. However, such a naive end-to-end fine-tuning approach not only has weak distinguishability of different tasks but also ignores the generalization ability of FEDKIM to unseen tasks, even though the modalities have been encountered already. To address this issue, we develop a Multimodal Multi-tasking Mixture Of Experts (M\u00b3OE) module to allow FEDKIM to distinguish tasks dynamically.\nM\u00b3OE takes both the task description \\(T^t\\) and the modality descriptions \\(M^t\\) associated with Task \\(T\\) as inputs to compute the relevance of each expert for the given task and modality, where \\(M^t\\) is the concatenation of descriptions of all modalities concerning Task \\(T^t\\). \\(T^t\\) and \\(M^t\\) are firstly encoded by the embedding layer of the foundation model \\(F\\), and subsequently processed to output weights for expert selection as follows:\n$$\\alpha^t = \\text{softmax} (\\text{MLP} (\\text{Pooling} ([\\mathcal{h}]^3))),$$\n$$\\beta = \\frac{(W_q \\text{EMB}(M^t)^T(W_k \\text{EMB}(T^t))}{\\sqrt{d_k}} \\frac{W_v \\text{EMB}(T^t)}{\\sum_i W_v \\text{EMB}(T^t)},$$where \\(\\alpha^t \\in \\mathbb{R}^P\\) and P is the number of experts. \\(W_q\\), \\(W_k\\), and \\(W_v\\), denote the attention matrices, and \\(d_k\\) is the dimension size.\nThe proposed M3OE effectively integrates the injected knowledge managed by two separate routers, resulting in a more streamlined and contextually aware computation of weights. The output, \\(\\alpha^t\\), represents the attention-weighted selection of experts optimized for both the modality and the specific task. This approach provides the flexibility needed to handle complex medical scenarios by selecting the appropriate experts based on the context.\nStep 3: LORA-M\u00b3OE based Parameter-Efficient Fine-tuning. Finally, we generate the representation of each layer in \\(F\\) for the forward pass based on LORA (Hu et al., 2022) and the learned M\u00b3OE weight using Eq. (4) as follows:\n$$\\mathcal{C} = W_F h + \\sum_{p=1}^P \\alpha_p^t (B_p A_p h),$$where \\(W_F\\) denotes the frozen parameters of \\(F\\), \\(B_p A_p\\) denotes the lower-rank adaptation module serving as the \\(p\\)-th expert system. We will fine-tune the proposed FEDKIM using the final output from \\(F\\) and the ground truth \\(y\\). The design balances efficacy and efficiency during knowledge injection, allowing FEDKIM to decently handle the complex nature of medical applications.\nDuring the training, modality-specific encoders \\(\\theta^e\\) gradually align with the medical LLM \\(F\\) that contains abundant knowledge acquired through pre-training. The alignment indicates prior knowledge in \\(F\\) is also extracted during injection, in the form of adjusted parameters restored in encoders. To benefit local models and boost knowledge injection in the next round, FEDKIM passes the updated encoders \\(\\theta^e\\) and the aggregated decoders \\(\\theta^d\\) back to local ends and performs the knowledge-driven iterative training until convergence."}, {"title": "3 Experiment Setup", "content": "In this study, we have training tasks and validation tasks across different datasets and data modalities. To provide a clear illustration, we present them in Table 2.\nTraining Task. To examine the utility of the proposed FEDKIM, we leverage four classification tasks across six modalities to federatedly inject medical knowledge into the selected foundation model through multi-task training. Details regarding these tasks are available in Appendix A. As emphasized in Section 2, we perform training on this suite of tasks in a multi-task pattern.\nValidation Task. Typical medical foundation models, such as MMedLM2 (Qiu et al., 2024), often struggle with handling unseen tasks involving novel modalities. To evaluate the extent to which knowledge injection enables the medical foundation model to tackle unseen tasks, we compile five classification tasks (ECD, SP, PED, AD, and EBD) and three generation tasks (MR, SNC, and MS). Details on these tasks are provided in Appendix B."}, {"title": "3.2 Data Partition", "content": "For each training task, we divide the data into four parts in a ratio of 7:1:1:1. Specifically, 70% of the data, \\(D_n\\), is private data evenly distributed to N clients for training local models. Another 10% of the data is public data, \\(D_p\\), placed on the server for tuning the foundation model. An additional 10% of the data is development data, \\(D_a\\), kept on the server as a validation set. The remaining 10% of the data, \\(D_t\\), is used as testing data for these tasks. More details regarding the data distribution can be found in Appendix C."}, {"title": "3.3 Baselines", "content": "Since the task of medical knowledge injection is novel and unexplored, there are no existing baselines. Therefore, we establish our own baselines, detailed as follows:\nFedPlug. FedPlug acquires modality-specific encoders through the federated learning process described in Section 2.2. These encoders are then integrated into the foundation model for fine-tuning. By aligning multimodal medical input with the semantic space of the foundation model, FedPlug enables the model to handle multiple modalities. Throughout this process, only the aggregated encoders are trainable.\nFedPlug. Building on the FedPlug framework, FedPlug incorporates the Low-Rank Adaptation (LoRA) technique (Hu et al., 2022) to better integrate multimodal features into the semantic space of the large language model (LLM), thereby optimizing the federated learning process. In addition to the trainable encoders in FedPlug, each layer of the LLM is equipped with a tunable LoRA module."}, {"title": "3.4 FL Backbone Approaches", "content": "We implement our FEDKIM based upon the following backbone approaches:\nFedAVG (McMahan et al., 2017) is a conventional federated learning method, producing a global model by aggregating distributed models \\([\\theta^1,..., \\theta^M]\\) as follows:\n$$f_{\\text{avg}}([\\theta^1,..., \\theta^M]) = \\frac{1}{N} \\sum_{n=1}^N \\theta^n.$$\nFedProx (Li et al., 2020) aims to extend FedAvg by regularizing each local loss function with an L2 term as follows:\n$$\\min_{\\theta_n} L_n (\\theta; \\theta^*) = L_n(\\theta) + \\frac{\\lambda}{2} ||\\theta - \\theta^*||^2,$$\nwhere \\(\\theta^*\\) is the global model, \\(L_n(\\cdot)\\) is the corresponding loss function, and \\(\\lambda\\) is the hyperparameter for weighting.\nMMedLM-2 (Qiu et al., 2024) is an advanced unimodal Large Language Model. Benefiting from multilingual pre-training, MMedLM-2 achieves the state-of-the-art performance in multiple question answering tasks, thus selected as the backbone of our foundation model deployed on the server."}, {"title": "3.5 Implementation Details", "content": "All experiments were conducted in an Ubuntu 20.04 environment using two NVIDIA A100 GPUs. We utilized MMedLM-2, the aforementioned state-of-the-art pre-trained medical language model, as the target of medical knowledge injection. The learning rate was set to \\(5 \\times 10^{-4}\\) for the foundation model and \\(1 \\times 10^{-4}\\) for the local models. \\(\\lambda\\) for FedProx was set to \\(1 \\times 10^{-4}\\). Cross-entropy loss was used for training the local models, while the foundation model was optimized using general autoregressive loss. The number of clients N was set to 5, and the number of experts P was set to 12 for FEDKIM. To ensure a fair comparison, we set the number of communication rounds to 10 for all methods involved in the comparison."}, {"title": "4 Performance Evaluation", "content": "We examine our proposed FEDKIM from the zero-shot evaluation (subsection 4.1) and fine-tuning evaluation (subsection 4.2) perspectives."}, {"title": "4.1 Zero-shot Evaluation", "content": "In the zero-shot evaluation, there is no overlap between the training tasks and evaluation tasks, which targets at examining the zero-shot capability of the medical foundation models enabled by FEDKIM.\nThe experiment results on unseen tasks are shown in Figure 2, with FedAvg (Figure 2a) and FedProx (Figure 2b) as the backbone federated approaches. We use black, orange, blue, and green curves to denote MMedLM2, FedPlug, FedPlug, and FEDKIM, respectively. Accuracy for classification tasks and BLEU for generation tasks are used for visualization. Based on the experiment results, we provide the observations and discussion below:\n(1) The original foundation model MMedLM2 fails to do the zero-shot evaluation on the unseen tasks in the training process. This is due to its extremely limited multimodal capabilities.\n(2) FedPlug, which only incorporates the federated encoder, performs the worst across all tasks, regardless of the type. This observation underscores the necessity of effectively utilizing public data to align the medical foundation model with external knowledge. Without proper integration, external knowledge\u2014although derived through federated approaches on vast amounts of private data\u2014cannot be directly assimilated into the medical foundation model.\n(3) Even though FedPlug approaches FEDKIM's performance on several tasks, it still falls short, particularly in generation tasks like Med-VQA. This indicates that the knowledge injected through FedPlug+LoRA does not fully generalize to unseen tasks, as training was exclusively performed on classification tasks. In contrast, FEDKIM, despite also being trained on classification tasks, achieves better performance on these tasks and maintains superior capability in handling unseen classification tasks. Comparing our FEDKIM with FedPlug\u2081, FEDKIM shows the superior performance on all the tasks, especially on the tasks of SNC (\u2191 82.36% with FedAvg), PED (\u219143.92% with FedAvg), and AD (\u219148.12%). On the other tasks, such as MR, EBD, and ECD, these approaches reach closed performance. This success is attributed to the M\u00b3OE module, which enables FEDKIM to adaptively select appropriate experts to jointly handle novel tasks based on the context. Furthermore, our proposed FEDKIM works well with the federated backbones of FedAvg and FedProx. It also generally maintains the advantages of a more advanced federated learning method (FedProx) over the vanilla approach. Comparing Figure 2a and Figure 2b, the performance with FedProx generally outperforms the one with FedAvg on different tasks, such as PED\u2191 15.25%.\nThese observations further show the adaptability of FEDKIM to enable medical foundation models to have zero-shot capability across different tasks and federated learning frameworks."}, {"title": "4.2 Fine-tuning Evaluation", "content": "While injecting medical knowledge into foundation models demonstrates the potential for handling unseen tasks, it remains uncertain whether the enhanced foundation model can also perform well on previously encountered tasks. To address this, we conducted a fine-tuning evaluation, with the training process detailed in Section 3.1 considered as fine-tuning for these tasks. The test sets for these tasks were used for evaluation, and the fine-tuning results are presented in Table 3. For a comprehensive evaluation, we utilize accuracy, precision, recall, and F1 score as metrics for these tasks.\nCompared to the experiments on unseen tasks, it is evident that the knowledge-injected medical foundation model performs significantly better on familiar tasks. This showcases the explicit utilization of knowledge acquired through federated training. Similar to the zero-shot evaluation, approaches combined with FedProx consistently outperform those with FedAvg, underscoring the importance of effective knowledge extraction during the injection process.\nFurthermore, FEDKIM consistently outperforms the two baselines, FedPlug and FedPlug\u2081. This competitive performance validates the design and effectiveness of the M\u00b3OE module."}, {"title": "4.3 Ablation Study", "content": "We conduct an ablation study on the COVID-19 detection task to assess the impact of each module within our proposed FEDKIM. Retaining all other modules as in the main experiments, we explore the following variant settings: (1) FEDKIMpub: Instead of utilizing knowledge from private datasets \\(D_n\\), this configuration solely leverages public dataset \\(D_p\\) for centralized training. Consequently, the federated training module discussed in Section 2.2 is excluded, with the encoder \\(\\theta^e\\) updated exclusively through public training as detailed in Section 2.3. (2) FEDKIMT: This variant omits the task description module that guides the expert selection process, testing the importance of task-specific information in routing the mixture of experts. (3) FEDKIMM: Similarly, we remove the modality description module to examine its influence on expert selection.\nThe results of the ablation study are presented in Table 4. They indicate that each component significantly enhances FEDKIM's performance. Specifically, a substantial decline in performance with FEDKIMpub highlights the crucial role of knowledge injected from local clients through federated learning. This locally enriched encoder allows the medical foundation model to better adjust to unseen modalities, thereby enhancing its effectiveness compared to models trained without this knowledge. Moreover, the absence of task or modality descriptions diminishes FEDKIM's ability to manage specific tasks through multi-task training, validating the design of the M\u00b3OE module. This module equips FEDKIM to effectively navigate complex healthcare scenarios that involve diverse tasks and modalities. In summary, the synergistic integration of local knowledge, along with the task and modality description modules, crucially bolsters the performance of our proposed FEDKIM."}, {"title": "5 Related Work", "content": "Medical Foundation Models. Foundation models, known for their vast parameters and training datasets, have demonstrated impressive capabilities across domains (Touvron et al., 2023; Zhou et al., 2023a; Yang et al., 2024; Li et al., 2024a; Abbasian et al., 2024), and are becoming increasingly prevalent in healthcare. Thirunavukarasu et al. (Thirunavukarasu et al., 2023) highlight the potential of large language models (LLMs) in clinical settings. Moor et al. (Moor et al., 2023) propose a generalist medical AI for diverse tasks using multimodal data. Specialized medical foundation models have been developed for disease detection (Zhou et al., 2023b), cancer biomarker identification (Pai et al., 2024), echocardiogram interpretation (Christensen et al., 2024), image segmentation (Zhang et al., 2023a), and precision oncology (Truhn et al., 2024). Despite these advancements, there is a research gap in creating datasets and benchmarks to integrate and leverage distributed medical data.\nFederated Fine-tuning with Foundation Models. Fine-tuning foundation models (FMs) with task-specific data is essential for improved performance in specialized tasks. Federated Learning (FL) supports this by utilizing locally stored data and distributed computational resources. Research in this field includes full tuning (Deng et al., 2023; Fan et al., 2023), partial tuning (Peng et al., 2024; Marchisio et al., 2022; Khalid et al., 2023), and parameter-efficient fine-tuning (PEFT) (Lu et al., 2023; Zhang et al., 2023b). Notably, (Lu et al., 2023) involves clients hosting FMs and exchanging adapters with the server, which aggregates and redistributes them. Similarly, FedPETuning (Zhang et al., 2023b) shares parts of client models for pretrained language models in FL. Unlike these studies, which require clients to have FMs, our approach positions the medical FM on the server, facilitating collaborative enhancement of medical FM models without accessing local data.\nParameter-efficient Fine-tuning on Foundation Model Full-parameter Fine-Tuning of foundation models, while promising in terms of performance enhancement, requires extremely extensive computational resources. Consequently, researchers have investigated Parameter-efficient Fine-tuning (PEFT) techniques. PEFT methods aim to adapt pre-trained models to specific tasks using a minimal number of additional parameters. Low-Rank Adaptation (LoRA) (Hu et al., 2022), a widely recognized PEFT method, reduces the number of trainable parameters by factorizing weight matrices into low-rank representations, achieving significant parameter efficiency. Additionally, previous studies have utilized modular approaches, such as adapters (Gao et al., 2023) and the Perceiver Resampler (Alayrac et al., 2022), to adapt new modalities to foundation models.\nResearchers have explored combining the Mixture of Experts (Jacobs et al., 1991) concept with Low-Rank Adaptation (LoRA) for Parameter-efficient Fine-tuning (PEFT) (Li et al., 2024b; Wu et al., 2023). To guide the selection of experts in complex scenarios, they have leveraged modality information (Luo et al., 2024; Li et al., 2024c), instructions (Chen et al., 2023, 2024b; Wu et al., 2023; Li et al., 2024b), or pre-defined task IDs (Liu et al., 2023b). However, these MOE methodologies do not specifically address the complex, modality-diverse scenarios found in the healthcare domain."}, {"title": "6 Conclusion", "content": "This work introduces the concept of knowledge injection into medical foundation models, emphasizing its critical role and potential in the development of comprehensive medical models. We propose a novel approach, FEDKIM, designed to extract and inject healthcare knowledge into foundation models, thereby enhancing their ability to handle multiple tasks and modalities. FEDKIM leverages flexible federated learning techniques to extract knowledge from distributed medical data. The extracted knowledge is then injected into the foundation model using our proposed adaptive M\u00b3OE module. Our exhaustive experimental results on 12 tasks and 7 modalities demonstrate the effectiveness of FEDKIM in diverse settings, showcasing its excellent capability in handling either encountered or unseen healthcare tasks. This study validates the potential of injecting knowledge into foundation models using federated learning, providing a crucial solution for developing a healthcare foundation model without accessing sensitive data."}, {"title": "7 Limitations", "content": "This work explores the problem of medical knowledge injection within the PEFT framework. Due to current computational limitations, we have not yet combined Full-parameter Fine-Tuning with our proposed FEDKIM. Additionally, our study utilizes MMedLM2, which has 7 billion parameters, but injecting knowledge into larger foundation models is restricted by available computational resources. In future research, we plan to investigate the integration of knowledge injection with Full-parameter Fine-Tuning. We also aim to evaluate the efficacy of our approach on larger medical foundation models to further validate its scalability and potential."}, {"title": "A Details of Training Tasks", "content": "COVID-19 Detection (CD) involves identifying COVID-19 symptoms from X-ray images using the COVQU dataset (Rahman et al., 2021) to evaluate the model's ability to interpret medical images.\nLung Opacity Detection (LOD) uses chest X-ray images to classify lung opacity based on data from the RSNA Pneumonia Detection Challenge 2018 (rsn), annotated by medical practitioners.\nECG Abnormal Detection (EAD) is an unimodal binary classification task that determines abnormal patterns in 10-second, 12-lead ECG signals from PTB-XL database (Wagner et al., 2020).\nMortality Prediction (MP) predicts ICU patient survival or death using multimodal dynamic features vital signs, lab tests, input and output, with data sourced from MIMIC-III (Johnson et al., 2016)."}, {"title": "B Details of Validation Tasks", "content": "Enlarged Cardiomediastinum Detection (ECD) (Irvin et al., 2019) aims to assess the presence of an enlarged cardiomediastinum using medical images from clinical evaluations. This task measures the model's capability to interpret radiographic data effectively.\nSepsis Prediction (SP) aims to forecast the likelihood of sepsis during ICU stays, testing the model's ability to understand various clinical features. These features are identical to those used in the mortality prediction task, extracted from the MIMIC-III database through the preprocessing pipeline (van de Water et al., 2024).\nMedical Visual Question Answering on RAD (MR) involves using both visual images and textual questions as inputs to generate answers. This task evaluates the model's ability to align text and image modalities within the medical domain. The VQA-RAD dataset is utilized for this task (Lau et al., 2018).\nSignal Noise Clarification (SNC) is a generative task that focuses on accurately describing noise in ECG signals based on corresponding textual questions. The data is extracted from an existing ECG question-answering dataset (Oh et al., 2024). The signals are recorded in 12 channels and last for 10 seconds, similar to the ECG Abnormal Detection task.\nPleural Effusion Detection (PED) (Irvin et al., 2019) is derived from the CheXpert dataset and involves using X-ray images to identify the presence of pleural effusion, testing the model's ability to interpret radiographic data.\nAtelectasis Detection (AD) (Irvin et al., 2019) also uses X-ray images from the CheXpert dataset to detect atelectasis, evaluating the model's capability in analyzing medical images.\nMedical Visual Question Answering on Slake (MS) (Liu et al., 2021) utilizes both visual images and textual questions from the SLAKE dataset to generate answers, assessing the model's proficiency in aligning text and image modalities in the medical domain.\nEctopic Beats Detection (EBD) aims to identify ectopic beats in ECG signals sourced from the PTB-XL database (Wagner et al., 2020)."}, {"title": "C Dataset Details", "content": "For tasks involved in training, we adopt the data partition setup detailed in Section 3.2. For tasks utilized in zero-shot evaluation, we select a subset of corresponding datasets to facilitate the inference efficiency. We cover 1,000 randomly sampled samples for the tasks of Sepsis Prediction, MedVQA-Slake, MedVQA"}]}