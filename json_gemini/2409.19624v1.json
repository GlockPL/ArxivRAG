{"title": "Storynizor: Consistent Story Generation via Inter-Frame Synchronized and Shuffled ID Injection", "authors": ["Yuhang Ma", "Wenting Xu", "Chaoyi Zhao", "Keqiang Sun", "Qinfeng Jin", "Zeng Zhao", "Changjie Fan", "Zhipeng Hu"], "abstract": "Recent advances in text-to-image diffusion models have spurred significant interest in continuous story image generation. In this paper, we introduce Storynizor, a model capable of generating coherent stories with strong inter-frame character consistency, effective foreground-background separation, and diverse pose variation. The core innovation of Storynizor lies in its key modules: ID-Synchronizer and ID-Injector. The ID-Synchronizer employs an auto-mask self-attention module and a mask perceptual loss across inter-frame images to improve the consistency of character generation, vividly representing their postures and backgrounds. The ID-Injector utilize a Shuffling Reference Strategy (SRS) to integrate ID features into specific locations, enhancing ID-based consistent character generation. Additionally, to facilitate the training of Storynizor, we have curated a novel dataset called StoryDB comprising 100,000 images. This dataset contains single and multiple-character sets in diverse environments, layouts, and gestures with detailed descriptions. Experimental results indicate that Storynizor demonstrates superior coherent story generation with high-fidelity character consistency, flexible postures, and vivid backgrounds compared to other character-specific methods.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in text-to-image diffusion models has sparked considerable interest in generating continuous story images. Maintaining consistency between frames, ensuring natural and flexible character poses, and achieving a clear separation of foreground and background are critical challenges in this domain.\nMany prior works have paid attention to ensuring character consistency. For instance, IP-Adapter [1], Arc2Face [2], and InstantID [3] extract identity features from a reference image and inject them into the diffusion model. While effective in single-character scenarios, these methods often struggle with stiff postures and are limited in handling more complex multi-character interactions.\nOther approaches, such as Mix-of-Show [4] and OMG [5], focus on multi-character generation by utilizing attention maps to position characters within a frame. These methods successfully achieve varied poses and maintain character consistency but lack inter-frame coherence, as they operate on a frame-by-frame basis without ensuring consistency across the sequence.\nTo achieve narrative coherence, methods like ConsiStory [6] and StoryDiffusion [7] have attempted to fuse character features across frames to enhance inter-frame consistency. However, the absence of an identity injection mechanism in these approaches results in inaccurate alignment with reference"}, {"title": "2 Related work", "content": "Text-to-image generative models. Diffusion models have achieved remarkable results in text-to-image generation in recent years [8, 9, 10, 11, 12, 13, 14]. Early works such as DALL-E2 [10] and Imagen [9] utilize original images as the diffusion input, resulting in enormous computational resources and training time. Latent diffusion models (LDMs) [15] have been introduced to compress images into a latent space through a pre-trained auto-encoder [16], instead of operating directly in the pixel space [9, 8]. However, general diffusion models rely solely on text prompts, lacking the capability to generate consistent characters with image conditions.\nConsistent character generation. Subject-driven image generation aims to generate customized images of a particular subject based on different text prompts. Most existing works adopt extensive fine-tuning for each subject [17, 18, 19, 20]. Dreambooth [17] maps the subject to a unique identifier while Textual-Inversion [21] is proposed to optimize a word vector for a custom concept. Moreover, some works [22, 5, 4] put their effort in multi-subject image generation. Custom Diffusion [22] propose to combine multiple concepts via closed-form constrained optimization. OMG [5] and Mix-of-Show [4] propose to optimize the fusion mode during training in circumstance of multi-concept generation. However, these methods necessitate additional training for all subjects, which can be time-consuming in multi-subject generation scenarios. Recently, some methods strive to enable subject-driven image generation without additional training [23, 24, 1, 25, 3, 6]. Most of them explore extended-attention mechanisms for maintaining identity consistency. IP-Adapter [1] and InstantID [3] introduce visual control by separating cross-attention layers for text features and image features. ConsiStory [6] enables training-free subject-level consistency across novel images via cross-frame attention. However, they fail to preserve detailed information according to the inadequate image feature extraction."}, {"title": "3 Method", "content": "We propose a pretraining story generation model called Storynizor, which generates a series of multi-character stories with high inter-frame character consistency, effective foreground background separation, and rich pose variation under a series of prompt conditioning and ID images (optional). To modeling our task, we set a series of prompts T as the following:\n$T = \\{T_n\\}, n = 1, ..., N$                                                                           (1)\nwhere N denotes the total numbers of prompts. To contains the description of characters P and the actions of each characterA:\n$T_n = \\{P_n, A_n\\} = \\{P_m, A_m\\}, m = 1, ..., M,$                                              (2)"}, {"title": "3.1 ID-Synchronizer", "content": "Previous works [6] typically consider a spacial self-attention module to ensure consistency among inter-frames. Given a series of latent noise features $x_t \\in R^{B \\times F \\times H \\times W \\times C}$ and a single text prompts y, they formulate latent noise features as $z_t \\in R^{B \\times FHW \\times C}$ for spacial self-attention to inherit all the module weights from the original 2D self-attention in diffusion model. ID-Synchronizer also begins with this well-explored design. However, the shared visual features across images produce nearly identical backgrounds. While maintaining minimal variation in backgrounds or layout among frames is typical for tasks like video and 3D-object generation, generating narrative images for stories demands vibrant backgrounds tailored to specific text prompts.\nTherefore, we introduce an Auto-mask Self-attention (AMSA) to our ID-Synchronizer to ensure consistent character generation in vivid backgrounds and postures. AMSA leverages attention masks of the primary subjects, acquired from the cross-attention modules of the UNet, to concentrate on regions containing characters. It then employs spatial self-attention to these specific areas within the noise features across frames. AMSA requires precise cross attention maps to achieve an excellent generation of different background and consistent characters across images. Acknowledging the constrained semantic representation of the original text encoder in Stable Diffusion, we introduce a Mask Perceptual Loss to improve the semantic representation of each character.\nAuto-mask Space-Attention. Our aim is to ensure consistent character portrayal across inter-frame generation while integrating lively backgrounds. To achieve this, ID-Synchronizer extends the original self-attention module into a spatial self-attention module. Specifically, we rearrange the latent noise $z_n$ of each frame in the i-th layer of the diffusion model by formulating it as following:\n$z_i = [z_i^1 \\oplus z_i^2 ... \\oplus z_i^N]$                                                                           (4)"}, {"title": "3.2 ID-Injector", "content": "Since the ID-Injector is trained alongside the ID-Synchronizer, it necessitates inter-frame feature injection. Given arbitrary numbers of ID images, Storynizor develops an optional inter-frame ID-Injector, which can receive additional face ID features for continuous story generation across frames. We adopt an ID encoder Ef to extract ID features from given face images IR and a CLIP encoder Er to extract image embeddings of this face. Then we develop a Resampler Pr to project the face images to the condition space of the latent diffusion model. Given a set of reference images IR = {In, n = 1, ..., N, the inter-frame face embedding finally into the diffusion model is defined as the following:\n$C_f = P_r(E_f(I_R), E_I(I_R)),$                                                                              (9)\nwhere cf \u2208 R(B\u00d7N)\u00d7T\u00d7h, T \u00d7 h refers to the dimension of face condition embedding of each frame, B \u00d7 N refers to the batch size and numbers of frames. Subsequently, another inter-frame cross-attention adaptive module is introduced into the latent diffusion model to support face images as prompts.\nShuffling Reference Strategy (SRS). Recent works [25, 23] demonstrate various approaches to inject personalized features into diffusion models, such as original ID embedding, average ID embedding, stacked ID embedding and ID embedding with face keypoints. However, when used with ID-Synchronizer, with the integration of spatial attention modules in AMSA, the generated images are notably influenced by the initial image conditions, leading to consistent facial poses throughout"}, {"title": "4 StoryDB Dataset Construction", "content": "Storynizor aims to generate consistent character images across diverse backgrounds. However, existing open-source datasets lack either rich background variety or fixed character attributes. To address this, we introduce StoryDB, a character-centric image-text pair dataset comprising 10,000 groups, each featuring the same character in consistent attire across different scenes, totaling 100,000 images. Each group contains 5-12 images with corresponding prompts, indexed shared prompt elements, and character mask images. StoryDB not only supports Storynizor's training but also serves as a resource for future research in story generation and IP-consistent content creation.\nImage downloading. Initially, we collect images from the internet and open-source datasets to create a comprehensive character dataset comprising real humans, cartoon characters, and animals. We then calculate the aesthetic score of each image to aid in filtering the dataset during the download process."}, {"title": "5 Experiments", "content": "We utilize the original checkpoint of Stable Diffusion Model-1.5 as the backbone for both ID-Synchronizer and ID-Injector. Training is conducted on 8 NVIDIA A100 GPUs, with 5% probability of dropping out text and face conditions. Inference uses DDIM [12] with 30 steps and a guidance scale of 7.0 on an NVIDIA A30 GPU, with the resolution of 768 \u00d7 768.\nID-Synchronizer. We train the ID-Synchronizer with its UNet parameters frozen, using the StoryDB Dataset. We train 50,000 iterations with a batch size of 4 and learning rate of 5 \u00d7 10-5 at a resolution of 512 x 512. The ID-Synchronizer is further fine-tuned at a resolution of 768 \u00d7 768 for high-fidelity generation, with a batch size of 1 for 50,000 iterations.\nID-Injector. We use a total of 80 million text-image pairs, comprising 50M from LAION-Face[27] and 30M from the internet. We train 2 epochs with a learning rate of 1 \u00d7 10-4 and a batch size of 128 with the resolution of 512 \u00d7 512. In the second stage, we incorporate the pre-trained ID-Injector into Storynizor. We train the ID-Injector with ID-Synchronizer frozen, using the StoryDB for 5 epochs with a learning rate of 1 \u00d7 10-4 and a batch size of 4, at the resolution of 768 \u00d7 768."}, {"title": "5.3 Quantitative Evaluation", "content": "Quantitative results are presented in Tab. 2. For prompt-only generation, our Storynizor achieves optimal performance in both text-image consistency and inter-image coherence. In prompt-ID guided generation, InstantID attains high scores in facial similarity. However, its semantic capability is compromised due to generating images overly similar to the reference, resulting in a lack of diversity, as evidenced by low CLIP-T scores. While PhotoMaker achieves comparable text similarity scores to our Storynizor, it significantly underperforms in story continuity and facial consistency. Overall, Storynizor demonstrates the highest comprehensive score, validating its superior story generation capabilities."}, {"title": "5.5 Human Evaluation", "content": "We conducted a user study with 25 experts to evaluate Storynizor against previous methods. Each expert evaluated the samples used for quantitative comparison. As shown in Table 3, the results indicate a preference for Storynizor over other methods in both text alignments and consistent story generation."}, {"title": "5.6 Ablation Studies", "content": "Influence of AMSA and MPL of ID-Synchronizer. We conduct an ablation study of the following components: (1) Auto-Mask Self-Attention module (AMSA) and (2) Mask Perceptual Loss (MPL). Quantitative results are provided in Tab. 4. As evidenced by the table, both the incorporation of"}, {"title": "6 Conclusion", "content": "In conclusion, we present Storynizor, a model for generating cohesive story images with consistent characters, distinct foreground-background elements, and diverse poses. It combines ID-Synchronizer with AMSA for character consistency and vivid features, and the ID-Injector uses Shuffling Reference Strategy (SRS) for flexible face poses and consistent portrayal. Additionally, we introduce StoryDB, a 100,000-image dataset featuring diverse character sets in various settings, supporting Storynizor's training and future research."}, {"title": "A Appendix", "content": "We have provided supplementary details regarding our Storynizor in this section. Our code will be released at https://anonymous.4open.science/r/Storynizor-0DC3.\nA.1 Implementation details\nA.1.1 Inference setup\nWe utilize the original checkpoint of Stable Diffusion Model-1.5 as the backbone for both ID-Synchronizer and ID-Injector. Inference uses DDIM [12] with 30 steps and a guidance scale of 7.0 on an NVIDIA A30 GPU, with the resolution of 768 \u00d7 768.\nA.1.2 Training setup for ID-Synchronizer\nWe train the ID-Synchronizer on 8 NVIDIA A100 GPUs. with its UNet parameters frozen, using the StoryDB Dataset. We train 50,000 iterations with a batch size of 4 and learning rate of 5 \u00d7 10-5 at a resolution of 512 \u00d7 512. The ID-Synchronizer is further fine-tuned at a resolution of 768 \u00d7 768 for high-fidelity generation, with a batch size of 1 for 50,000 iterations.\nA.1.3 Training setup for ID-Injector\nWe use a total of 80 million text-image pairs, comprising 50M from LAION-Face[27] and 30M from the internet. We train 2 epochs with a learning rate of 1 \u00d7 10-4 and a batch size of 128 with the resolution of 512 \u00d7 512 on 8 NVIDIA A100 GPUs. In the second stage, we incorporate the pre-trained ID-Injector into Storynizor. We train the ID-Injector with ID-Synchronizer frozen, using the StoryDB for 5 epochs with a learning rate of 1 \u00d7 10-4 and a batch size of 4, at the resolution of 768 \u00d7 768.\nA.1.4 Evaluation metrics\nWe employ CLIP ViT-L/14\u2074 to evaluate the similarity between the generated images and the given text prompts (CLIP-T). Subsequently, we utilize the image encoder of the CLIP model to evaluate the correlation between the generated consistent images and the reference images (CLIP-I). Additionally, we employ the DINO score [31] to evaluate image alignment, as DINO is better suited for subject representation (DINO-I). We use Arcface score to evaluate both the similarity between the generated faces and the reference face (Face Sim) and the similarity across the generated frames when evaluating the ID-Injector of Storynizor.\nA.1.5 Ablation\nWe integrated ID features from given reference images into cross-frame story generation through our ID-Injector. An ablation study was carried out to identify the best injection mode. Each training sample contains four text-image-pairs. With Stacked-ID, faces from all training samples are stacked and injected into the Resampler during ID-Injector training, leading to stiff face postures, as illustrated in Fig. 8. In contrast, using our SPS resulted in more flexible face poses. Our quantitative results in the main paper also illustrate that our proposed shuffling reference strategy (SRS) outperforms stacked ID embedding in both facial similarity and textual alignment, affirming the superiority of SRS."}]}