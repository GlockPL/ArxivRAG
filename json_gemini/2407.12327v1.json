{"title": "Spectra: A Comprehensive Study of Ternary, Quantized, and FP16 Language Models", "authors": ["Ayush Kaushal", "Tejas Pandey", "Tejas Vaidhya", "Aaryan Bhagat", "Irina Rish"], "abstract": "Post-training quantization is the leading method for addressing memory-related bottlenecks in LLM inference, but unfortunately, it suffers from significant performance degradation below 4-bit precision. An alternative approach involves training compressed models directly at a low bitwidth (e.g., binary or ternary models). However, the performance, training dynamics, and scaling trends of such models are not yet well understood. To address this issue, we train and openly release the Spectra LLM suite consisting of 54 language models ranging from 99M to 3.9B parameters, trained on 300B tokens. Spectra includes FloatLMs, post-training quantized QuantLMs (3, 4, 6, and 8 bits), and ternary LLMs (TriLMs) our improved architecture for ternary language modeling, which significantly outperforms previously proposed ternary models of a given size (in bits), matching half-precision models at scale. For example, TriLM 3.9B is (bit-wise) smaller than the half-precision FloatLM 830M, but matches half-precision FloatLM 3.9B in commonsense reasoning and knowledge benchmarks. However, TriLM 3.9B is also as toxic and stereotyping as FloatLM 3.9B, a model six times larger in size. Additionally, TriLM 3.9B lags behind FloatLM in perplexity on validation splits and web-based corpora but performs better on less noisy datasets like Lambada and PennTreeBank.\nTo enhance understanding of low-bitwidth models, we are releasing 500+ intermediate checkpoints of the Spectra suite at https://github.com/NolanoOrg/SpectraSuite.", "sections": [{"title": "1 Introduction", "content": "The FLOPs, memory capacity, and memory bandwidth of GPUs keep increasing exponentially, doubling every 1.26, 2, and 2.9 years, respectively [Gholami et al., 2024], i.e. the compute capabilities (FLOPs) are growing faster than memory capacity and bandwidth. In Large Language Models (LLMs) inference, the primary bottlenecks are caused by model size (bits), which affects memory usage (memory capacity) and data transfer to processors (memory bandwidth). These issues are becoming more critical than the growing number of model parameters which affects the computational limits (FLOPs). For instance, state-of-the-art LLMs such as 340B Nemotron 4 [Nvidia et al., 2024] have sizes (in bits) exceeding the memory capacity of data center GPUs, such as 8xH100s. Token generation speed, or latency, is now limited by memory bandwidth [Kim et al., 2024]. Addressing these bottlenecks requires more expensive training, exceeding Chinchilla's compute-optimal regime [Hoffmann et al., 2022], with billion-parameter models being trained on up to 15 trillion tokens [Touvron et al., 2023b]. Another popular but, as we show later, sub-optimal method is post-training quantization during deployment [Zhu et al., 2023].\nIn post-training quantization, LLMs initially trained in 16-bit floating point (FP16/BF16) format (referred to as FloatLM) have their parameters quantized, i.e. converted to a smaller bitwidth after training; we refer to the resulting models as QuantLMs. These models use optimized kernels for deployment, offering speedups nearly proportional to the compression factor [Frantar and Alistarh, 2024]. However, very low bitwidths cause a significant mismatch between the pre-trained FloatLM representations and the deployable QuantLM, resulting in undesired behavior and quality degradation [Li et al., 2024, Huang et al., 2024]. Some of the state-of-the-art methods [Frantar et al., 2022, Egiazarian et al., 2024] mitigate this issue by using calibration and re-training data from target domains; however, this increases the sensitivity to calibration data. For instance, simple choices like whether to length-normalize or not the calibration data can significantly impact QuantLM performance [Malinovskii et al., 2024]. Other works have observed that QuantLM at 4 bits (4-bit QuantLMs) have about 65% lower knowledge capacity per parameter compared to trained and aligned FloatLMs [Allen-Zhu and Li, 2024].\nAnother approach to reducing model size while maintaining parameter count is training neural networks with low effective bitwidths [Zhou et al., 2018]. This approach offers compression benefits beyond post-training quantization without its drawbacks. Typically, we low bitwidths like binary or ternary quantization are used; However, binary quantization usually underperforms compared to regular FP16 models [Liu et al., 2023a], while ternary modeling can perform match performance"}, {"title": "2 Memory Bottlenecks and Low-Bitwidth Language Modelling", "content": "Gholami et al. [2024] recently observed that given the slower pace of improvements in memory and communication as compared to compute (FLOPs), the bottleneck continues to shift away from computation towards memory-related characteristics of hardware for deploying large language models. In this section, we start by expanding their analysis to a wider range of recent datacenter General Purpose GPUs (GPGPUs) used for neural network development and research since 2018 from multiple hardware providers. We consider different configurations across the recent microarchitectures. These include Volta (V100 SXM/PCIe) [Nvidia Team, 2018], Ampere (A100 40GB/80GB SXM/PCIe) [Nvidia Team, 2020], Hopper (H100 SXM/PCIe, H200) [Nvidia Team, 2022, 2023] and Blackwell"}, {"title": "3 TriLM: Ternary Language Model", "content": "In this section, we present the architectural and optimization details of the TriLM (Ternary Language Model). The following subsections provide an in-depth analysis of the architectural choices distinguishing TriLM from BitNet, as well as optimization strategies employed during training."}, {"title": "3.1 Architecture", "content": "TriLM is LLaMa-style [Touvron et al., 2023a] autoregressive transformers [Vaswani et al., 2017] model with RMSNorm [Zhang and Sennrich, 2019] instead of LayerNorm [Ba et al., 2016], SwiGLU Gated MLP [Shazeer, 2020] instead of standard transformer MLP, Rotary Position Embedding (ROPE) [Su et al., 2021], Multi-Headed Attention and no bias terms.\nIn TriLMs, the weights of linear layers are represented in one of three possible ternary states {-1,0,1}, along with an additional floating point number called 'scale' shared across the matrix. During training, the latent (or master) weights are maintained in floating point precision, allowing for the accumulation of small updates over iterations that eventually contribute to a switch in the estimated ternary state of a parameter. During forward pass, the floating point latent weights are ternarized on the fly. This is done by first computing the scale to the absolute mean of the latent weights, then estimating the ternary state of a parameter by rounding off to the nearest ternary state after scaling. In the backward pass, a straight-through estimator is used to estimate backward pass on the floating point latent weights. During inference, ternarized states and scale needs to be estimated only once - allowing for more than 10x reduction in model size and inference time at larger scales.\nA formal description of these forward pass, backward pass, and inference time equations is provided in the Appendix (\u00a7A.1). Across all our experiments the embedding and language model head are represented in half-precision floating point.\nSince, training of TriLMs requires computing of scale on the fly, synchronizing for a single scalar across devices in model parallel training [Shoeybi et al., 2019] can cause significant communication overhead. Thus, we let each device independently compute scale over its own shard of the matrix. These lead to additional artifacts, similar to BitNet, where the number of scalar values for each matrix is same as the degree of model parallelism used during training. This leads to negligible increase in size - in our case, only 6 additional scalar values for each matrix with millions of parameters."}, {"title": "Differences from BitNet Architecture", "content": "TriLM differs from BitNet b1.58 in several ways for better performance as well as for fairer comparison with FloatLMs.\nFollowing are the key differences in TriLM's architecture. We follow GPT3's Pre-Normalization [Brown et al., 2020a] approach to normalize before each linear layer - this was observed to be crucial for stable training in FP16. Thus, normalization is done twice in each transformer layer, at the input representations to the two sub-layers attention and Gated MLP. This is in contrast to BitNet, where before each linear layer (i.e. 4-7 times per transformer layer depending on the implementation), the activation (or intermediate representations) are normalized, scaled and quantized to 8 bits. We use RMSNorm with a scale parameter over the parameterless RMSNorm."}, {"title": "3.2 Optimization Schedule", "content": "Optimization of low bitwidth neural networks (such as in Quantization Aware Training) [Liu et al., 2023b, Yuan et al., 2024, Bethge et al., 2018, Le and Li, 2023] requires a set of consideration like higher initial learning rate and reduced weight decay. Our optimization schedule for TriLM closely follows that of BitNet [Ma et al., 2024] consisting of two interventions in a vanilla linear decay learning rate scheduling with warmup and weight decay (L2 Regularization). (1) Peak LR - at roughly the halfway point, we reduce the peak learning rate. (2) L2 Reg. - at roughly two-thirds of the training, we remove the weight decay regularization as ternarization provides sufficient regularization [Courbariaux et al., 2016]."}, {"title": "4 Spectra Suite: Spanning Parameters and Bitwidths", "content": "The Spectra suite includes comprehensive families of Large language models designed to span different parameter counts and bit-widths. This suite includes three main model families: TriLMs, FloatLMs, and QuantLMs (3, 4, 6, and 8 bits). Drawing inspiration from established model suites such as those by [Biderman et al., 2023, Liu et al., 2023c, Groeneveld et al., 2024], Spectra aims to facilitate scientific research on low-bitwidth LLMs."}, {"title": "4.1 Overview of Spectra Suite", "content": "The Spectra suite stands out with several key properties:\n1. Scale: The suite spans a broad spectrum of scales across parameter count (99M to 3.9B), sizes (9 * 108 to 6.4 * 1010 bits) and bitwidths (1.58 bits to 16 bits).\n2. Uniform Training: All models are trained using identical data sequences.\n3. Public Accessibility: The training data is publicly available for study.\n4. Consistent Model Size Mapping: All models across the families maintain a consistent one-to-one mapping for parameter count.\nEach model family within Spectra spans from 99M to 3.9B parameters, covering nearly two orders of magnitude in size. All the TriLMs and FloatLMs are trained on a standardized 300B subset of Slim Pajama [Soboleva et al., 2023] dataset, ensuring training consistency. QuantLMs undergo quantization using the same calibration data, maintaining uniformity in model quantization procedures. Data ordering and batch sizes are also kept consistent within each model family to support reproducibility and comparability in research efforts."}, {"title": "4.2 FloatLM and QuantLM", "content": "FloatLMs: We utilize LLaMa-style [Touvron et al., 2023a] architecture akin to TriLM. In FloatLMs, parameters in the weight matrices of linear layers are represented as floating-point numbers (FP16/BF16). The optimization schedule for FloatLM follows a cosine decay scheduling with weight decay and includes a learning rate warmup. This methodology is consistent with the practices established in models such as Pythia, OLMO, LLM360. For more details, refer to the Appendix (A.4).\nQuantLMs: Recently, Data-aware quantisation techniques like GPTQ [Frantar et al., 2022] have emerged as efficient solutions for near-lossless weight quantization down to 4-bit precision [Dettmers and Zettlemoyer, 2023]. In our work, we implemented GPTQ post-training quantization to FloatLM, creating the QuantLM family of models across 3, 4, 6, and 8 bits. We quantized all transformer layer weights. For 3-bit and 4-bit quantization, we employ a group size of 128, which results in effective bit rates of 3.25 and 4.25 bits per parameter, respectively. We've refined our approach by incorporating best practices from recent research [Malinovskii et al., 2024], particularly in terms of calibration data and scaling it to a million tokens for improved reconstruction. To ensure a fair comparison with TriLM, we maintain certain components in their original precision. Specifically, we do not quantize the embedding, language model head, or activations. Additionally, we use symmetric quantization (without zero offset) as it is simpler, is supported by fast inference kernels [Frantar and Alistarh, 2024] and offers similar performance to assymmetric quantization (with separate zero offsets in addition to scale for each group). It also offers consistency and a fairer comparison with TriLMs. It's worth noting that our Spectra suite is designed with flexibility in mind, allowing for easy extension to other quantization methods as needed."}, {"title": "4.3 Training Dynamics and Scaling Laws", "content": "Figure 7a shows the training loss curves for all the TriLMs trained and Figure 7b shows relative training loss of a TriLM to two smaller FloatLMs. The loss curves demonstrate a continuous and consistent improvement in TriLMs with increase in parameter count. Furthermore, since the TriLMs were all trained on same data, with same ordering, minor spikes and drops in training loss are consistently observed at all scales at a given token count. It should be noted that the two largest models - TriLM 2.4B and TriLM 3.9B also showcase one large spike in training loss each in the first half of training. Upon dropping the peak learning rate at halfway point, a sharp drop (spanning over a course of only a few hundred million tokens) in training loss is observed. While, for the larger TriLMs (2.4B and 3.9B), rate of decrease in loss after this sudden drop reverts back to the same as before halfway-mark, it plateaus for the smaller ones (1.1B and 1.5B). In fact, for TriLMs with less than a Billion parameters, training loss starts to increase after this. At two-thirds mark, when weight decay is removed, all models start to converge faster, and this is most pronounced for the largest TriLM models."}, {"title": "4.4 Advancing Research through Open Access:", "content": "The open suite of TriLM, FloatLM, and QuantLM families aims to empowers researchers to explore the nuanced impacts of precision levels on model performance and efficiency, thereby catalyzing ongoing advancements in the development and deployment of language models, as well as enhancing their interpretability and safety. By providing a range of publicly accessible models trained on openly available data, the suite offers unprecedented transparency in the training process. Intermediate checkpoints are available for all models, accompanied by detailed documentation of training procedures and hyperparameters. This comprehensive suite enables researchers to investigate the capacities and limitations of TriLMs at various scales, thus facilitating advancements in model development, and safety."}, {"title": "5 Evaluation", "content": "We evaluate the families of LLMs on three aspects - commonsense & reasoning tasks, knowledge based tasks, and toxicity, all of which are crucial measures of their downstream performance. Readers may refer to appendix for more details regarding the benchmarks (\u00a7B)."}, {"title": "5.1 Commonsense and Reasoning", "content": "We assess the models using eight distinct commonsense and reasoning benchmarks consisting of tasks from logical and reasoning questions to grounded and physical commonsense tasks: Arc Easy, Arc Challenge [Clark et al., 2018], BoolQ [Clark et al., 2019], HellaSWAG [Zellers et al., 2019], WinoGrande [Sakaguchi et al., 2021], PIQA [Bisk et al., 2019], LAMBADA [Paperno et al., 2016], LogiQA [Liu et al., 2021], all under zero-shot settings.\nFigures 1a and 1b display the average performance of the LLMs on first six benchmarks (the same benchmarks as those reported for BitNet b1.58) across size (bits) and params. Figures 1c and 1d present the performance for the LAMBADA dataset. TriLMs consistently demonstrate superior performance for their size across all benchmarks at the 2.4B and 3.9B parameter scales. At the largest scale of 3.9B, TriLM surpasses FloatLM on LAMBADA and achieves competitive average scores across six benchmarks. Additionally, TriLMs at the largest scales consistently outperform 4-bit QuantLMs of equivalent parameter count. However, across the considered scales, all LLMs show poor performance on LogiQA, making it difficult to identify a clear performance trend. For detailed benchmarking across all datasets, refer to Tables 6 and 7."}, {"title": "5.2 Knowledge", "content": "Several downstream practical uses of LLMs requires LLMs to have knowledge about common subjects like science or topics like political figures. We evaluate the performance of LLMs on SciQ [Welbl et al., 2017], TriviaQA [Joshi et al., 2017] and MMLU [Hendrycks et al., 2021] benchmarks in zero-shot settings. Figures 9a and 9b shows the accuracy of the LLMs on SciQ across size (bits) and parameter counts. Figures 9c and 9d does the same for TriviaQA, while 10a and 10b does so for MMLU. Across both the benchmarks, at large 2.4B+ scales, TriLMs offer the best performance at a given size (bits). Surprisingly, despite having fewer bits, the knowledge capacity of TriLM do not have any significant degradation as observed in case of QuantLMs [Allen-Zhu and Li, 2024]. Low-bitwidth LLMs like TriLMs have similar knowledge capacity to FloatLMs, indicate that knowledge capacity is parameterized via presence and nature of a connection (+1 or -1), rather than its strength. Tables 7 and 9 expands on these results."}, {"title": "5.3 Toxicity", "content": "We evaluate the Spectra suite across various safety and toxicity benchmarks of TruthfulQA [Lin et al., 2021], Big Bench BBQ Lite [Parrish et al., 2022] and CrowsPairs [Nangia et al., 2020]. These scores are listed in the Appendix in Table 9. We observe that none of the LLMs, even at largest sclaes of 3.9B parameter with 300B tokens perform significantly better than random guessing on TruthfulQA. Across the remaining two datasets, we observe that toxicity and stereotypes correlate with LLMs capability across other tasks. Specifically, TriLMs at less than Billion parameter scale are less stereotyping than FloatLMs of same parameter count, however the difference closes with scale and TriLM 2.4B and TriLM 3.9B start performing equally biased as FloatLM 2.4B and FloatLM 3.9B across these benchmarks. This also highlights that it implies TriLMs are far more stereotyping than FloatLMs of similar size (bits), at par with FloatLMs of similar parameter counts."}, {"title": "6 Related Work", "content": "Training Language Models At Lower Precision: Several notable language models such as GPT [Brown et al., 2020b], NeoX [Black et al., 2022] and Pythia families have been trained using mixed precision (FP32/FP16 or FP32/BF16) [Micikevicius et al., 2018] or fully half-precision (FP16/BF16) [Kalamkar et al., 2019].\nRecent line of works on BitNet [Wang et al., 2023] and BitNet b1.58 [Ma et al., 2024] leverage strategies native to training extremely low bitwidth networks [Courbariaux et al., 2016] for transformer based language models. These studies demonstrate that low-bitwidth language models scaling trends are similar to those of floating point language modeling. In their work, models are trained at low \"effective\" precision of binary and ternary respectively - where the latent (or master) weights during training are maintained in higher precision like FP16. The model weights are binarized or ternarized on the fly during the forward pass and gradients are backpropagated for the latent weights using the straight-through estimator [Courbariaux et al., 2016]. Prior works emphasize the importance of maintaining latent (or master) weights at high precision to allow accumulation of small updates during training - for example, Peng et al. [2023] observed significant performance drop on language model when the latent (or master) model weights were switch from 16-bits (FP16/BF16) to 8-bits (FP8) during training. Concurrent architectural improvements such as Flash Attention [Dao et al., 2022, Dao, 2023], mixture of experts [Zoph et al., 2022] and state space modeling [Gu and Dao, 2024, Dao and Gu, 2024] complement these advancements in lower precision modeling.\nQuantization of Large Language Models after Training: Post-training quantization (PTQ) algorithms convert a pretrained high-precision model (FP32 / FP16 / BF16) into a lower precision format without requiring the original training process[Cai et al., 2020, Hubara et al., 2020, Choukroun et al., 2019]. These methods can be either data-independent or need a small calibration dataset. [Malinovskii et al., 2024] observed the sensitivity to calibration datasets. Post-training quantization of LLMs is additionally difficult due to presence of numerical outliers in weights and activations [Bondarenko et al., 2021].\nGPTQ [Frantar et al., 2022] is a state-of-the-art one-shot weight quantization method aimed at finding a matrix of quantized weights (say \u0174) that minimizes the squared error relative to the full precision layer output. This can be expressed mathematically as: \\(\\min_{\\hat{W}} |Wx - \\hat{W}x|^2\\), where \\(W\\) represents the weight and \\(x\\) the activation. By leveraging second-order information, GPTQ derives a closed-form solution to this optimization problem. Other methods [Dettmers et al., 2023, Lin et al., 2024, Lee et al., 2024] emphasize the importance of outlier weights that correspond to high-magnitude activations. some methods [Xiao et al., 2024, Yao et al., 2022, 2023] also quantised activation along with the weights."}, {"title": "7 Conclusion", "content": "We introduce the Spectra suite, an open family of LLMs across varying bitwidths, consisting of ternary LLMs (TriLMs), FP16 LLMs (FloatLM) as well as their quantized QuantLMs (3, 4, 6 and 8 bits) all pretrained on same 300B tokens of data. We also present our improved and simplified TriLM architecture for ternary language modeling that offers stable training at FP16 precision. Our evaluation of these models demonstrate that low bitwidth language models like TriLMs offer better performance for their size than quantized models at Billion+ parameter count. The TriLM 3.9B specifically achieves competitive performance to FloatLM 3.9B (a model much larger than TriLM 3.9B) across various benchmarks of commonsense & reasoning and knowledge based tasks. These results underscore the potential of TriLMs in addressing bottlenecks in LLM inference, stemming from memory capacity and bandwidth, better than QuantLMs. We open-source over 500 checkpoints (including intermediate training checkpoints) of the Spectra suite to further research on better understanding these models, their training dynamics, current optimization bottlenecks as well as finer-grained interpretability methods that leverages their ternarized structure."}, {"title": "8 Broader Impact", "content": "Interpretability Beyond Neuron Level: While several efforts have been made to understand how language models work and means to steer them without training, these methods have mostly focussed on intervening at neuron level. TriLMs opens a new degree of interpretability - at the connection level. Here, the connections between any two neurons in a layer are in one of the three states - 0 (no connection), -1 (negative connection) and +1 (positive connection), each with equal strength. This is in sharp contrast to FloatLMs, where these connections can be of varying strengths, making it harder to study interpretability beyond neuron level. By releasing the checkpoints across our training runs, we facilitate research along these directions.\nEnvironmental Benefits and Resource Efficiency: The open release of our models mitigates future emissions by allowing others to bypass the need for pretraining models from scratch. Moreover, TriLMs much lesser resource to deploy, and can perform the autoregressive generation as a faster pace - making them critical to scenarios demanding strict latency. Additionally, TriLMs represent a substantial advancement in enhancing performance on resource-constrained edge devices, including smartphones, laptops, and automobiles.\nImpact on Specialised Hardware: While TriLMs offers significant memory reduction and latency improvements on General Purpose GPUs like H100 and RTX4090, certain specialized hardware benefits more from ternary modeling. Hardware (like Cerabras) that support high byte-to-flop ratio computations, can leverage the sparsity stemming from ternarization for speedup in both training as well as inference. On the other hand, hardware with limited Memory/SRAM (like Groq), benefit from reduction in the number of chips needed to deploy an LLMs.\nReduced Training Costs: The Chinchilla scaling laws established that for training compute optimality, it may be recommended to train larger LLMs for lesser tokens than smaller LLMs for more tokens for achieving the desired model performance. However, memory requirements and latency associated with deployment of larger models, has motivated costlier training runs that go far beyond Chinchilla optimality. For example a LLaMa 3 model with only 8B parameter was trained for 15T tokens. Since, TriLM and ternary models in general can reduce the memory requirements and latency, this can motivate a shift inparameter-token tradeoff for efficient training runs towards Chinchilla's compute-optimal regime."}, {"title": "A Architecture and PreTraining Details", "content": "This section provides a comprehensive overview of the architectural design and pretraining for TriLM (Ternary Language Model) and FloatLM (Floating Point Language Model). We outline the forward and backward pass equations specific to their linear layers, highlighting the contrast between the FP16 matrices in FloatLM and the ternary matrices with scalar scaling in TriLM. Additionally, it covers dataset selection, tokenizer usage, and preprocessing methods employed for training data preparation. These discussions provide information on pretraining setups, implementation nuances, and key hyperparameters critical to the models' development."}, {"title": "A.1 Forward Pass, Backward Pass and Inference Equations", "content": "Table 1 show the equations across TriLM vs FloatLM for forward pass, backward pass and inference.\nFloatLM \\(Y = XW^T\\), \\(\\frac{\\partial L}{\\partial X} = \\frac{\\partial L}{\\partial Y} W\\), \\(\\frac{\\partial L}{\\partial W} = X^T \\frac{\\partial L}{\\partial Y}\\), \\(Y = XW^T\\)\nTriLM \\(\\gamma = \\frac{1}{n} \\sum_{i=1}^{n} \\sum_{j=1}^{m} |W_{ij}|\\), \\(W_{ij} = round(\\min(\\max(W_{ij}, -1), 1)) = \\frac{W_{ij}}{\\gamma}\\), \\(W_{ij} = W_{ij} \\gamma\\), \\(Y = XW^T\\), Compute \\(W\\) and \\(\\gamma\\) once and cache, \\(\\frac{\\partial L}{\\partial X} = \\frac{\\partial L}{\\partial Y} W\\), \\(\\frac{\\partial L}{\\partial W} = X^T \\frac{\\partial L}{\\partial Y}\\), \\(W_{ij} = W_{ij}\\), \\(Y = XW^T\\)"}, {"title": "A.2 Data and Tokenizer", "content": "Dataset Selection: Let input be \\(X \\in \\mathbb{R}^{b \\times n}\\) for a linear layer with FP16 weight matrix \\(W \\in \\mathbb{R}^{m \\times n}\\) and \\(Y \\in \\mathbb{R}^{b \\times m}\\) be the output. The same matrix W is also used to denote latent weights in TriLMs during training.\nFor ternarized layers in TriLMs, we also have a scalar scale \\(\\gamma \\in \\mathbb{R}\\), matrix with ternarized states \\(W \\in \\{-1, 0, 1\\}^{n \\times m}\\) and ternarized matrix \\(W \\in \\mathbb{R}^{n \\times m}\\). We set \\(\\epsilon = 1e-5\\).\nDue to lack of availability of Pile 300B [Gao et al., 2020] used in Pythia, we opted to use a 300B token sample of deduplicated Slim Pajama dataset. We sample from each subset with the probability proportional to its size."}, {"title": "Training Data Preparation:", "content": "\u2022 Main experiments (Spectra suite): We used the full 300B token sample\n\u2022 Ablation studies: Training runs with 100B tokens, we sample from these 300B tokens with equal probability weight to each data-point\n\u2022 Fine-Web Edu experiments: We tokenized one-third of a 350B token sample, from which we then sampled 100B tokens for our experiments."}, {"title": "QuantLM:", "content": "For the creation of QuantLM, we utilized a subset of the Slimpajama-627B dataset, consisting of 512 samples with a sequence length of 2048. These samples were normalized for length. Our approach closely follows the methodology outlined in [Malinovskii et al., 2024]."}, {"title": "A.3 PreTraining Setup", "content": "We scale using 2D-parallelism with Megatron-style sharding [Shoeybi et al., 2019] and use ZeRO stage 2 Deepspeed [Rasley et al., 2020] for ZeRO [Rajbhandari et al., 2020]. Our implementation was based on GPT NeoX Codebase [Andonian et al., 2023]. We use AdamW [Kingma and Ba, 2017] for optimization. We train on nodes with with IBM Power9 PC CPUs and 6x16GB V100. Due to lack of BFloat16 support in V100, we train both TriLM and FloatLM in FP16 using Mixed Precision Training and Dynamic Loss Scaling. Please refer to \u00a7A.5 for more implementation specific details. We extensively use Huggingface [Wolf et al., 2020] and Wandb [Biewald, 2020] for handling the checkpoints and experiment tracking."}, {"title": "A.4 Hyperparameters", "content": "Table 3 shows the hyperparameters for TriLM and FloatLM's transformer architecture and their learning rate. We set Adam \\(\\beta\\) are set to (0.9, 0.95) for both families of models and all the reported runs are trained to 2048 sequence length. FloatLM and TriLM are respectively trained with batch sizes of 2M and 1M tokens respectively."}, {"title": "A.5 Known Implementation Artifacts", "content": "\u2022 Similar to BitNet [Wang et al., 2023], our models have artifacts from model parallelism. Specifically, computing the scale \\(\\gamma\\) across all the entire weight matrix - which has been sharded across multiple devices requires a costly communication overhead from all-reduce. In our implementation, we compute these scales over the portion of weight matrix local to each device. Thus, for inference over TriLM models, scales should be independently computed over each model parallel group. It should be noted that this negligible change on effective on bits/parameter of < 10-5, even at highest model parallelism of 6 for our largest model.\n\u2022 Because we train in FP16, we expect some artifacts from training. However, we do not expect a reasonable performance difference from mixed precision training with BF16 or even FP32 because the lowest values of loss scales observed during any of the runs were at least as high as recommended 128 [Micikevicius et al., 2018]. Moreover, in BitNet b1.58 (Section 3), they compared models to their reproduced FP16 LLaMA LLM. Thus, our setting closely resemble theirs."}, {"title": "B Benchmark Details", "content": "We benchmark TriLM, FloatLM and QuantLM across Knowledge, Commonsense, Reasoning and Toxicity benchmarks. We average our scores across 3 different 'seeds' by preparing three different QuantLM models quantized using different calibration sets. We also add Pythia (deduplicated with consistent 2M batch size across families) suite of models (70M to 2.8B params) and BitNet b.158 performance scores from their paper for comparison. We use the LM Evaluation Harness [Gao et al., 2023] to benchmark."}, {"title": "B.1 Commonsense and Reasoning", "content": "We report commonsense and reasoning benchmark scores across 6 benchmarks previously considered by BitNet b.158. Each is considered in a zero shot setting. Following are the details of each of the benchmark considered:\n\u2022 ARC Challenge and Easy: [Clark et al.", "2018": "ARC dataset comprises 7787 multiple-choice science questions divided into two sets: Challenge and Easy. We calculate accuracy and normalised accuracy across both of these sets.\n\u2022 BoolQ: [Clark et al.", "2019": "BoolQ is a reading comprehension dataset consisting of naturally occurring yes/no questions. We calculate the accuracy on this tasks.\n\u2022 HellaSwag: [Zellers et al., 2019"}]}