{"title": "Spectra: A Comprehensive Study of Ternary, Quantized, and FP16 Language Models", "authors": ["Ayush Kaushal", "Tejas Pandey", "Tejas Vaidhya", "Aaryan Bhagat", "Irina Rish"], "abstract": "Post-training quantization is the leading method for addressing memory-related bottlenecks in LLM inference, but unfortunately, it suffers from significant performance degradation below 4-bit precision. An alternative approach involves training compressed models directly at a low bitwidth (e.g., binary or ternary models). However, the performance, training dynamics, and scaling trends of such models are not yet well understood. To address this issue, we train and openly release the Spectra LLM suite consisting of 54 language models ranging from 99M to 3.9B parameters, trained on 300B tokens. Spectra includes FloatLMs, post-training quantized QuantLMs (3, 4, 6, and 8 bits), and ternary LLMs (TriLMs) our improved architecture for ternary language modeling, which significantly outperforms previously proposed ternary models of a given size (in bits), matching half-precision models at scale. For example, TriLM 3.9B is (bit-wise) smaller than the half-precision FloatLM 830M, but matches half-precision FloatLM 3.9B in commonsense reasoning and knowledge benchmarks. However, TriLM 3.9B is also as toxic and stereotyping as FloatLM 3.9B, a model six times larger in size. Additionally, TriLM 3.9B lags behind FloatLM in perplexity on validation splits and web-based corpora but performs better on less noisy datasets like Lambada and PennTreeBank.\nTo enhance understanding of low-bitwidth models, we are releasing 500+ intermediate checkpoints of the Spectra suite at https://github.com/NolanoOrg/SpectraSuite.", "sections": [{"title": "1 Introduction", "content": "The FLOPs, memory capacity, and memory bandwidth of GPUs keep increasing exponentially, doubling every 1.26, 2, and 2.9 years, respectively [Gholami et al., 2024], i.e. the compute capabilities (FLOPs) are growing faster than memory capacity and bandwidth. In Large Language Models (LLMs) inference, the primary bottlenecks are caused by model size (bits), which affects memory usage (memory capacity) and data transfer to processors (memory bandwidth). These issues are becoming more critical than the growing number of model parameters which affects the computational limits (FLOPs). For instance, state-of-the-art LLMs such as 340B Nemotron 4 [Nvidia et al., 2024] have sizes (in bits) exceeding the memory capacity of data center GPUs, such as 8xH100s. Token generation speed, or latency, is now limited by memory bandwidth [Kim et al., 2024]. Addressing these bottlenecks requires more expensive training, exceeding Chinchilla's compute-optimal regime [Hoffmann et al., 2022], with billion-parameter models being trained on up to 15 trillion tokens [Touvron et al., 2023b]. Another popular but, as we show later, sub-optimal method is post-training quantization during deployment [Zhu et al., 2023].\nIn post-training quantization, LLMs initially trained in 16-bit floating point (FP16/BF16) format (referred to as FloatLM) have their parameters quantized, i.e. converted to a smaller bitwidth after training; we refer to the resulting models as QuantLMs. These models use optimized kernels for deployment, offering speedups nearly proportional to the compression factor [Frantar and Alistarh, 2024]. However, very low bitwidths cause a significant mismatch between the pre-trained FloatLM representations and the deployable QuantLM, resulting in undesired behavior and quality degradation [Li et al., 2024, Huang et al., 2024]. Some of the state-of-the-art methods [Frantar et al., 2022, Egiazarian et al., 2024] mitigate this issue by using calibration and re-training data from target domains; however, this increases the sensitivity to calibration data. For instance, simple choices like whether to length-normalize or not the calibration data can significantly impact QuantLM performance [Malinovskii et al., 2024]. Other works have observed that QuantLM at 4 bits (4-bit QuantLMs) have about 65% lower knowledge capacity per parameter compared to trained and aligned FloatLMs [Allen-Zhu and Li, 2024].\nAnother approach to reducing model size while maintaining parameter count is training neural networks with low effective bitwidths [Zhou et al., 2018]. This approach offers compression benefits beyond post-training quantization without its drawbacks. Typically, we low bitwidths like binary or ternary quantization are used; However, binary quantization usually underperforms compared to regular FP16 models [Liu et al., 2023a], while ternary modeling can perform match performance"}, {"title": "2 Memory Bottlenecks and Low-Bitwidth Language Modelling", "content": "Gholami et al. [2024] recently observed that given the slower pace of improvements in memory and communication as compared to compute (FLOPs), the bottleneck continues to shift away from computation towards memory-related characteristics of hardware for deploying large language models. In this section, we start by expanding their analysis to a wider range of recent datacenter General Purpose GPUs (GPGPUs) used for neural network development and research since 2018 from multiple hardware providers. We consider different configurations across the recent microarchitectures. These include Volta (V100 SXM/PCIe) [Nvidia Team, 2018], Ampere (A100 40GB/80GB SXM/PCIe) [Nvidia Team, 2020], Hopper (H100 SXM/PCIe, H200) [Nvidia Team, 2022, 2023] and Blackwell"}, {"title": "3 TriLM: Ternary Language Model", "content": "In this section, we present the architectural and optimization details of the TriLM (Ternary Language Model). The following subsections provide an in-depth analysis of the architectural choices distinguishing TriLM from BitNet, as well as optimization strategies employed during training."}, {"title": "3.1 Architecture", "content": "TriLM is LLaMa-style [Touvron et al., 2023a] autoregressive transformers [Vaswani et al., 2017] model with RMSNorm [Zhang and Sennrich, 2019] instead of LayerNorm [Ba et al., 2016], SwiGLU Gated MLP [Shazeer, 2020] instead of standard transformer MLP, Rotary Position Embedding (ROPE) [Su et al., 2021], Multi-Headed Attention and no bias terms.\nIn TriLMs, the weights of linear layers are represented in one of three possible ternary states {-1,0,1}, along with an additional floating point number called 'scale' shared across the matrix. During training, the latent (or master) weights are maintained in floating point precision, allowing for the accumulation of small updates over iterations that eventually contribute to a switch in the estimated ternary state of a parameter. During forward pass, the floating point latent weights are ternarized on the fly. This is done by first computing the scale to the absolute mean of the latent weights, then estimating the ternary state of a parameter by rounding off to the nearest ternary state after scaling. In the backward pass, a straight-through estimator is used to estimate backward pass on the floating point latent weights. During inference, ternarized states and scale needs to be estimated only once - allowing for more than 10x reduction in model size and inference time at larger scales.\nA formal description of these forward pass, backward pass, and inference time equations is provided in the Appendix (\u00a7A.1). Across all our experiments the embedding and language model head are represented in half-precision floating point.\nSince, training of TriLMs requires computing of scale on the fly, synchronizing for a single scalar across devices in model parallel training [Shoeybi et al., 2019] can cause significant communication overhead. Thus, we let each device independently compute scale over its own shard of the matrix. These lead to additional artifacts, similar to BitNet, where the number of scalar values for each matrix is same as the degree of model parallelism used during training. This leads to negligible increase in size - in our case, only 6 additional scalar values for each matrix with millions of parameters."}, {"title": "Differences from BitNet Architecture", "content": "TriLM differs from BitNet b1.58 in several ways for better performance as well as for fairer comparison with FloatLMs.\nFollowing are the key differences in TriLM's architecture. We follow GPT3's Pre-Normalization [Brown et al., 2020a] approach to normalize before each linear layer - this was observed to be crucial for stable training in FP16. Thus, normalization is done twice in each transformer layer, at the input representations to the two sub-layers attention and Gated MLP. This is in contrast to BitNet, where before each linear layer (i.e. 4-7 times per transformer layer depending on the implementation), the activation (or intermediate representations) are normalized, scaled and quantized to 8 bits. We use RMSNorm with a scale parameter over the parameterless RMSNorm."}, {"title": "3.2 Optimization Schedule", "content": "Optimization of low bitwidth neural networks (such as in Quantization Aware Training) [Liu et al., 2023b, Yuan et al., 2024, Bethge et al., 2018, Le and Li, 2023] requires a set of consideration like higher initial learning rate and reduced weight decay. Our optimization schedule for TriLM closely follows that of BitNet [Ma et al., 2024] consisting of two interventions in a vanilla linear decay learning rate scheduling with warmup and weight decay (L2 Regularization). (1) Peak LR - at roughly the halfway point, we reduce the peak learning rate. (2) L2 Reg. at roughly two-thirds of the training, we remove the weight decay regularization as ternarization provides sufficient regularization [Courbariaux et al., 2016].\nAmong these four runs, we notice the lowest final training loss when both, the L2 Regularization and Peak LR are intervened, closely followed only L2 Regularization being intervened and then only Peak LR being intervened. Dropping the peak LR at halfway point leads to a quick sharp drop in training loss. Similar phenomena have also been observed in schedules with small episodes of fast learning rate decaying like MiniCPM [Hu et al., 2024]. On the other hand, removing L2 regularization, or weight decay, leads to accelerated convergence, which can even mostly have the same effect as lowering peak LR leading to a quick drop in loss. These relative training loss observation at 100B tokens also go hand in hand with relative downstream performance across commonsense and reasoning tasks, which are listed in Table 8. Thus, we fix the TriLM optimization schedule. We drop in the peak learning rate at the halfway mark Weight decay is removed at the two-thirds mark."}, {"title": "4 Spectra Suite: Spanning Parameters and Bitwidths", "content": "The Spectra suite includes comprehensive families of Large language models designed to span different parameter counts and bit-widths. This suite includes three main model families: TriLMs, FloatLMs, and QuantLMs (3, 4, 6, and 8 bits). Drawing inspiration from established model suites such as those by [Biderman et al., 2023, Liu et al., 2023c, Groeneveld et al., 2024], Spectra aims to facilitate scientific research on low-bitwidth LLMs."}, {"title": "4.1 Overview of Spectra Suite", "content": "The Spectra suite stands out with several key properties:\n1. Scale: The suite spans a broad spectrum of scales across parameter count (99M to 3.9B), sizes (9 * 108 to 6.4 * 1010 bits) and bitwidths (1.58 bits to 16 bits).\n2. Uniform Training: All models are trained using identical data sequences.\n3. Public Accessibility: The training data is publicly available for study.\n4. Consistent Model Size Mapping: All models across the families maintain a consistent one-to-one mapping for parameter count.\nEach model family within Spectra spans from 99M to 3.9B parameters, covering nearly two orders of magnitude in size. All the TriLMs and FloatLMs are trained on a standardized 300B subset of Slim Pajama [Soboleva et al., 2023] dataset, ensuring training consistency. QuantLMs undergo quantization using the same calibration data, maintaining uniformity in model quantization procedures. Data ordering and batch sizes are also kept consistent within each model family to support reproducibility and comparability in research efforts."}, {"title": "4.2 FloatLM and QuantLM", "content": "FloatLMs: We utilize LLaMa-style [Touvron et al., 2023a] architecture akin to TriLM. In FloatLMs, parameters in the weight matrices of linear layers are represented as floating-point numbers (FP16/BF16). The optimization schedule for FloatLM follows a cosine decay scheduling with weight decay and includes a learning rate warmup. This methodology is consistent with the practices established in models such as Pythia, OLMO, LLM360. For more details, refer to the Appendix (A.4).\nQuantLMs: Recently, Data-aware quantisation techniques like GPTQ [Frantar et al., 2022] have emerged as efficient solutions for near-lossless weight quantization down to 4-bit precision [Dettmers and Zettlemoyer, 2023]. In our work, we implemented GPTQ post-training quantization to FloatLM, creating the QuantLM family of models across 3, 4, 6, and 8 bits. We quantized all transformer layer weights. For 3-bit and 4-bit quantization, we employ a group size of 128, which results in effective bit rates of 3.25 and 4.25 bits per parameter, respectively. We've refined our approach by incorporating best practices from recent research [Malinovskii et al., 2024], particularly in terms of calibration data and scaling it to a million tokens for improved reconstruction. To ensure a fair comparison with TriLM, we maintain certain components in their original precision. Specifically, we do not quantize the embedding, language model head, or activations. Additionally, we use symmetric quantization (without zero offset) as it is simpler, is supported by fast inference kernels [Frantar and Alistarh, 2024] and offers similar performance to assymmetric quantization (with separate zero offsets in addition to scale for each group). It also offers consistency and a fairer comparison with TriLMs. It's worth noting that our Spectra suite is designed with flexibility in mind, allowing for easy extension to other quantization methods as needed."}, {"title": "4.3 Training Dynamics and Scaling Laws", "content": "Figure 7a shows the training loss curves for all the TriLMs trained and Figure 7b shows relative training loss of a TriLM to two smaller FloatLMs. The loss curves demonstrate a continuous and consistent improvement in TriLMs with increase in parameter count. Furthermore, since the TriLMs were all trained on same data, with same ordering, minor spikes and drops in training loss are consistently observed at all scales at a given token count. It should be noted that the two largest models - TriLM 2.4B and TriLM 3.9B also showcase one large spike in training loss each in the first half of training. Upon dropping the peak learning rate at halfway point, a sharp drop (spanning over a course of only a few hundred million tokens) in training loss is observed. While, for the larger TriLMs (2.4B and 3.9B), rate of decrease in loss after this sudden drop reverts back to the same as before halfway-mark, it plateaus for the smaller ones (1.1B and 1.5B). In fact, for TriLMs with less than a Billion parameters, training loss starts to increase after this. At two-thirds mark, when weight decay is removed, all models start to converge faster, and this is most pronounced for the largest TriLM models.\nFigures 8a and 8b show the final validation loss across size (in bits) and parameters respectively. When measuring performance in terms of size (crucial for output generation phase of inference), TriLMs, with increasing size, offer much better performance at same number of bits. Specifically, at the size of TriLM 3.9B, these ternary models start offering better performance than models, more than five times their size. In this work, scaling laws for FloatLM and TriLMs (up to the 3.9B parameter scale) show FloatLMs as a better choice. The difference between the two, however, considerably narrows at Billion+ parameter scale; the trends show the potential for TriLMs to meet (or even outperform) FloatLMs of same parameter count. Despite the gap in validation loss, we will later observe that TriLMs offer competitive downstream performance with FloatLMs of same parameter count across a variety of benchmarks in commonsense, reasoning and knowledge based tasks. In appendix (\u00a7B.4), we show that that gap in perplexity is also observed across other overlapping web based datasets like (Dolma, RefinedWeb), however the gap is not present for less noisy data, like Penn Tree Bank and OpenAI's Lambada."}, {"title": "4.4 Advancing Research through Open Access:", "content": "The open suite of TriLM, FloatLM, and QuantLM families aims to empowers researchers to explore the nuanced impacts of precision levels on model performance and efficiency, thereby catalyzing ongoing advancements in the development and deployment of language models, as well as enhancing their interpretability and safety. By providing a range of publicly accessible models trained on openly available data, the suite offers unprecedented transparency in the training process. Intermediate checkpoints are available for all models, accompanied by detailed documentation of training procedures and hyperparameters. This comprehensive suite enables researchers to investigate the capacities and limitations of TriLMs at various scales, thus facilitating advancements in model development, and safety."}, {"title": "5 Evaluation", "content": "We evaluate the families of LLMs on three aspects - commonsense & reasoning tasks, knowledge based tasks, and toxicity, all of which are crucial measures of their downstream performance. Readers may refer to appendix for more details regarding the benchmarks (\u00a7B)."}, {"title": "5.1 Commonsense and Reasoning", "content": "We assess the models using eight distinct commonsense and reasoning benchmarks consisting of tasks from logical and reasoning questions to grounded and physical commonsense tasks: Arc Easy, Arc Challenge [Clark et al., 2018], BoolQ [Clark et al., 2019], HellaSWAG [Zellers et al., 2019], WinoGrande [Sakaguchi et al., 2021], PIQA [Bisk et al., 2019], LAMBADA [Paperno et al., 2016], LogiQA [Liu et al., 2021], all under zero-shot settings.\nFigures 1la and 1b display the average performance of the LLMs on first six benchmarks (the same benchmarks as those reported for BitNet b1.58) across size (bits) and params. Figures 1c and 1d present the performance for the LAMBADA dataset. TriLMs consistently demonstrate superior performance for their size across all benchmarks at the 2.4B and 3.9B parameter scales. At the largest scale of 3.9B, TriLM surpasses FloatLM on LAMBADA and achieves competitive average scores across six benchmarks. Additionally, TriLMs at the largest scales consistently outperform 4-bit QuantLMs of equivalent parameter count. However, across the considered scales, all LLMs show poor performance on LogiQA, making it difficult to identify a clear performance trend. For detailed benchmarking across all datasets, refer to Tables 6 and 7."}, {"title": "5.2 Knowledge", "content": "Several downstream practical uses of LLMs requires LLMs to have knowledge about common subjects like science or topics like political figures. We evaluate the performance of LLMs on SciQ [Welbl et al., 2017], TriviaQA [Joshi et al., 2017] and MMLU [Hendrycks et al., 2021] benchmarks in zero-shot settings. Figures 9a and 9b shows the accuracy of the LLMs on SciQ across size (bits) and parameter counts. Figures 9c and 9d does the same for TriviaQA, while 10a and 10b does so for MMLU. Across both the benchmarks, at large 2.4B+ scales, TriLMs offer the best performance at a given size (bits). Surprisingly, despite having fewer bits, the knowledge capacity of TriLM do not have any significant degradation as observed in case of QuantLMs [Allen-Zhu and Li, 2024]. Low-bitwidth LLMs like TriLMs have similar knowledge capacity to FloatLMs, indicate that knowledge capacity is parameterized via presence and nature of a connection (+1 or -1), rather than its strength. Tables 7 and 9 expands on these results."}, {"title": "5.3 Toxicity", "content": "We evaluate the Spectra suite across various safety and toxicity benchmarks of TruthfulQA [Lin et al., 2021], Big Bench BBQ Lite [Parrish et al., 2022] and CrowsPairs [Nangia et al., 2020]. These scores are listed in the Appendix in Table 9. We observe that none of the LLMs, even at largest sclaes of 3.9B parameter with 300B tokens perform significantly better than random guessing on TruthfulQA. Across the remaining two datasets, we observe that toxicity and stereotypes correlate with LLMs capability across other tasks. Specifically, TriLMs at less than Billion parameter scale are less stereotyping than FloatLMs of same parameter count, however the difference closes with scale and TriLM 2.4B and TriLM 3.9B start performing equally biased as FloatLM 2.4B and FloatLM 3.9B across these benchmarks. This also highlights that it implies TriLMs are far more stereotyping than FloatLMs of similar size (bits), at par with FloatLMs of similar parameter counts."}, {"title": "6 Related Work", "content": "Training Language Models At Lower Precision: Several notable language models such as GPT [Brown et al., 2020b], NeoX [Black et al., 2022] and Pythia families have been trained using mixed precision (FP32/FP16 or FP32/BF16) [Micikevicius et al., 2018] or fully half-precision (FP16/BF16) [Kalamkar et al., 2019].\nRecent line of works on BitNet [Wang et al., 2023] and BitNet b1.58 [Ma et al., 2024] leverage strategies native to training extremely low bitwidth networks [Courbariaux et al., 2016] for transformer based language models. These studies demonstrate that low-bitwidth language models scaling trends are similar to those of floating point language modeling. In their work, models are trained at low \"effective\" precision of binary and ternary respectively - where the latent (or master) weights during training are maintained in higher precision like FP16. The model weights are binarized or ternarized on the fly during the forward pass and gradients are backpropagated for the latent weights using the straight-through estimator [Courbariaux et al., 2016]. Prior works emphasize the importance of maintaining latent (or master) weights at high precision to allow accumulation of small updates during training - for example, Peng et al. [2023] observed significant performance drop on language model when the latent (or master) model weights were switch from 16-bits (FP16/BF16) to 8-bits (FP8) during training. Concurrent architectural improvements such as Flash Attention [Dao et al., 2022, Dao, 2023], mixture of experts [Zoph et al., 2022] and state space modeling [Gu and Dao, 2024, Dao and Gu, 2024] complement these advancements in lower precision modeling.\nQuantization of Large Language Models after Training: Post-training quantization (PTQ) algorithms convert a pretrained high-precision model (FP32 / FP16 / BF16) into a lower precision format without requiring the original training process[Cai et al., 2020, Hubara et al., 2020, Choukroun et al., 2019]. These methods can be either data-independent or need a small calibration dataset. [Malinovskii et al., 2024] observed the sensitivity to calibration datasets. Post-training quantization of LLMs is additionally difficult due to presence of numerical outliers in weights and activations [Bondarenko et al., 2021].\nGPTQ [Frantar et al., 2022] is a state-of-the-art one-shot weight quantization method aimed at finding a matrix of quantized weights (say \u0174) that minimizes the squared error relative to the full precision layer output. This can be expressed mathematically as: $min_{\\hat{W}} |W x - \\hat{W}x|^2$, where W represents the weight and x the activation. By leveraging second-order information, GPTQ derives a closed-form solution to this optimization problem. Other methods [Dettmers et al., 2023, Lin et al., 2024, Lee et al., 2024] emphasize the importance of outlier weights that correspond to high-magnitude activations. some methods [Xiao et al., 2024, Yao et al., 2022, 2023] also quantised activation along with the weights."}, {"title": "7 Conclusion", "content": "We introduce the Spectra suite, an open family of LLMs across varying bitwidths, consisting of ternary LLMs (TriLMs), FP16 LLMs (FloatLM) as well as their quantized QuantLMs (3, 4, 6 and 8 bits) all pretrained on same 300B tokens of data. We also present our improved and simplified TriLM architecture for ternary language modeling that offers stable training at FP16 precision. Our evaluation of these models demonstrate that low bitwidth language models like TriLMs offer better performance for their size than quantized models at Billion+ parameter count. The TriLM 3.9B specifically achieves competitive performance to FloatLM 3.9B (a model much larger than TriLM 3.9B) across various benchmarks of commonsense & reasoning and knowledge based tasks. These results underscore the potential of TriLMs in addressing bottlenecks in LLM inference, stemming from memory capacity and bandwidth, better than QuantLMs. We open-source over 500 checkpoints (including intermediate training checkpoints) of the Spectra suite to further research on better understanding these models, their training dynamics, current optimization bottlenecks as well as finer-grained interpretability methods that leverages their ternarized structure."}, {"title": "8 Broader Impact", "content": "Interpretability Beyond Neuron Level: While several efforts have been made to understand how language models work and means to steer them without training, these methods have mostly focussed on intervening at neuron level. TriLMs opens a new degree of interpretability - at the connection level. Here, the connections between any two neurons in a layer are in one of the three states - 0 (no connection), -1 (negative connection) and +1 (positive connection), each with equal strength. This is in sharp contrast to FloatLMs, where these connections can be of varying strengths, making it harder to study interpretability beyond neuron level. By releasing the checkpoints across our training runs, we facilitate research along these directions.\nEnvironmental Benefits and Resource Efficiency: The open release of our models mitigates future emissions by allowing others to bypass the need for pretraining models from scratch. Moreover, TriLMs much lesser resource to deploy, and can perform the autoregressive generation as a faster pace - making them critical to scenarios demanding strict latency. Additionally, TriLMs represent a substantial advancement in enhancing performance on resource-constrained edge devices, including smartphones, laptops, and automobiles.\nImpact on Specialised Hardware: While TriLMs offers significant memory reduction and latency improvements on General Purpose GPUs like H100 and RTX4090, certain specialized hardware benefits more from ternary modeling. Hardware (like Cerabras) that support high byte-to-flop ratio computations, can leverage the sparsity stemming from ternarization for speedup in both training as well as inference. On the other hand, hardware with limited Memory/SRAM (like Groq), benefit from reduction in the number of chips needed to deploy an LLMs.\nReduced Training Costs: The Chinchilla scaling laws established that for training compute optimality, it may be recommended to train larger LLMs for lesser tokens than smaller LLMs for more tokens for achieving the desired model performance. However, memory requirements and latency associated with deployment of larger models, has motivated costlier training runs that go far beyond Chinchilla optimality. For example a LLaMa 3 model with only 8B parameter was trained for 15T tokens. Since, TriLM and ternary models in general can reduce the memory requirements and latency, this can motivate a shift inparameter-token tradeoff for efficient training runs towards Chinchilla's compute-optimal regime."}, {"title": "A Architecture and PreTraining Details", "content": "This section provides a comprehensive overview of the architectural design and pretraining for TriLM (Ternary Language Model) and FloatLM (Floating Point Language Model). We outline the forward and backward pass equations specific to their linear layers, highlighting the contrast between the FP16 matrices in FloatLM and the ternary matrices with scalar scaling in TriLM. Additionally, it covers dataset selection, tokenizer usage, and preprocessing methods employed for training data preparation. These discussions provide information on pretraining setups, implementation nuances, and key hyperparameters critical to the models' development."}, {"title": "A.1 Forward Pass, Backward Pass and Inference Equations", "content": "Table 1 show the equations across TriLM vs FloatLM for forward pass, backward pass and inference.\nType\nFloatLM: Forward Pass: $Y = XW^T$, Backward Pass: $\\frac{\\partial L}{\\partial X} = \\frac{\\partial L}{\\partial Y} W$, Inference: $Y = XW^T$\nTriLM: Forward Pass: $\\gamma = \\frac{1}{n m} \\sum_{i=1}^n \\sum_{j=1}^m |W_{ij}|, \\quad \\bar{W}_{i j} = round(min(max(\\frac{W_{i j}}{\\gamma}, -1), 1))=\\bar{W}_{i j},\\ \\hat{W} = \\bar{W} \\gamma, \\quad Y = X \\hat{W}^T$, Backward Pass: $\\frac{\\partial L}{\\partial \\bar{W}} = \\frac{\\partial L}{\\partial Y} X$,  Inference: Compute $\\hat{W}$ and $\\gamma$ once and cache, $\\quad \\bar{W}_{i j} = \\bar{W}_{i j} , Y = X \\hat{W}^T$\nTable 1: Equations in the Linear Layer of TriLMs and FloatLMs."}, {"title": "A.2 Data and Tokenizer", "content": "Dataset Selection: Let input be $X \\in R^{b \\times n}$ for a linear layer with FP16 weight matrix $W \\in R^{m \\times n}$ and $Y \\in R^{b \\times m}$ be the output. The same matrix W is also used to denote latent weights in TriLMs during training.\nFor ternarized layers in TriLMs, we also have a scalar scale $\\gamma\\in R$, matrix with ternarized states $\\bar{W} \\in \\{-1, 0, 1\\}^{n \\times m}$ and ternarized matrix $\\hat{W} \\in R^{n \\times m}$. We set $ \\epsilon = 1e - 5$.\nDue to lack of availability of Pile 300B [Gao et al., 2020] used in Pythia, we opted to use a 300B token sample of deduplicated Slim Pajama dataset. We sample from each subset with the probability proportional to its size.\nTraining Data Preparation:\nMain experiments (Spectra suite): We used the full 300B token sample\nAblation studies: Training runs with 100B tokens, we sample from these 300B tokens with equal probability weight to each data-point\nFine-Web Edu experiments: We tokenized one-third of a 350B token sample, from which we then sampled 100B tokens for our experiments."}, {"title": "A.3 PreTraining Setup", "content": "We scale using 2D-parallelism with Megatron-style sharding [Shoeybi et al., 2019] and use ZeRO stage 2 Deepspeed [Rasley et al., 2020] for ZeRO [Rajbhandari et al., 2020]. Our implementation was based on GPT NeoX Codebase [Andonian et al., 2023]. We use AdamW [Kingma and Ba, 2017] for optimization. We train on nodes with with IBM Power9 PC CPUs and 6x16GB V100. Due to lack of BFloat16 support in V100, we train both TriLM and FloatLM in FP16 using Mixed Precision Training and Dynamic Loss Scaling. Please refer to \u00a7A.5 for more implementation specific details. We extensively use Huggingface [Wolf et al., 2020] and Wandb [Biewald, 2020] for handling the checkpoints and experiment tracking."}, {"title": "A.4 Hyperparameters", "content": "Table 3 shows the hyperparameters for TriLM and FloatLM's transformer architecture and their learning rate. We set Adam \u03b2 are set to (0.9, 0.95) for both families of models and all the reported runs are trained to 2048 sequence length. FloatLM and TriLM are respectively trained with batch sizes of 2M and 1M tokens respectively."}, {"title": "A.5 Known Implementation Artifacts", "content": "Similar to BitNet [Wang et al., 2023], our models have artifacts from model parallelism. Specifically, computing the scale \u03b3 across all the entire weight matrix - which has been sharded across multiple devices requires a costly communication overhead from all-reduce. In our implementation, we compute these scales over the portion of weight matrix local to each device. Thus, for inference over TriLM models, scales should be independently computed over each model parallel group. It should be noted that this negligible change on effective on bits/parameter of < 10-5, even at highest model parallelism of 6 for our largest model.\nBecause we train in FP16, we expect some artifacts from training. However, we do not expect a reasonable performance difference from mixed precision training with BF16 or even FP32 because the lowest values of loss scales observed during any of the runs were at least as high as recommended 128 [Micikevicius et al., 2018]. Moreover, in BitNet b1.58 (Section 3), they compared models to their reproduced FP16 LLaMA LLM. Thus, our setting closely resemble theirs."}, {"title": "B Benchmark Details", "content": "We benchmark TriLM, FloatLM and QuantLM across Knowledge, Commonsense, Reasoning and Toxicity benchmarks. We average our scores across 3 different 'seeds' by preparing three different QuantLM models quantized using different calibration sets. We also add Pythia (deduplicated with consistent 2M batch size across families) suite of models (70M to 2.8B params) and BitNet b.158 performance scores from their paper for comparison. We use the LM Evaluation Harness [Gao et al., 2023] to benchmark."}, {"title": "B.1 Commonsense and Reasoning", "content": "We report commonsense and reasoning benchmark scores across 6 benchmarks previously considered by BitNet b.158 in Table 6 and rest in Table 7. Each is considered in a zero shot setting. Following are the details of each of the benchmark considered:\nARC Challenge and Easy: [Clark et al., 2018] ARC dataset comprises 7787 multiple-choice science questions divided into two sets: Challenge and Easy. We calculate accuracy and normalised accuracy across both of these sets.\nBoolQ: [Clark et al., 2019] BoolQ is a reading comprehension dataset consisting of naturally occurring yes/no questions. We calculate the accuracy on this tasks."}, {"title": "B.2 Knowledge", "content": "We report performance on SciQ, TriviaQA and MMLU in Tables 7, 9 and 10. Each is considered in a zero shot setting. Following are the details of each of the benchmark considered:\nThe knowledge-based evaluation included the following tasks:\nSciQ: [Welbl et al., 2017] The SciQ dataset contains multiple-choice questions with 4 answer options from crowd-sourced science exams. The questions range from Physics, Chemistry and Biology and several other fields. We calculate the accuracy and length normalised accuracy on this task.\nTriviaQA: [Joshi et al., 2017] TriviaQA is a reading comprehension dataset containing question-answer-evidence triples. We calculate the exact match accuracy on this task.\nMMLU [Hendrycks et al., 2021]: The benchmark aims to assess the knowledge gained during pretraining by evaluating models solely in zero-shot and few-shot scenarios. It spans 57 subjects, including STEM fields, humanities, social sciences, and more."}, {"title": "B.3 Toxicity", "content": "We report toxicity-based evaluation in 9. Each is considered in a zero-shot setting.\nThe toxicity-based evaluation included the following tasks:\nBBQ [Parrish et al., 2022]: The Bias Benchmark for QA (BBQ) dataset, comprises sets of questions developed by its authors, focusing on documented social biases directed towards individuals from protected classes across nine distinct social dimensions pertinent to U.S. English-speaking environments.\nCrows Pairs [Nangia et al., 2020]: proposed a challenge dataset aimed at quantifying stereotypical biases embedded within language models, with a specific emphasis on U.S. contexts. Hosted on GitHub, this dataset serves as a crucial resource for assessing and addressing biases through paired sentences that illuminate societal stereotypes.\nTruthfulQA [Lin et al., 2021]: A benchmark designed to evaluate the truthfulness of language models in generating responses to questions. This benchmark includes 817 questions across 38 categories, such as health, law, finance, and politics."}, {"title": "B.4 Perplexity on other datasets", "content": "We measure perplexity using TriLM 3.9B and FloatLM 3.9B across various other corpora than SlimPajama, which was used for training - OpenAI Lambada, Penn Tree Bank, C4, Cosmopedia, Dolma, S2Orc, Wikipedia, RefinedWeb. A portion of Wikipedia, C4 is included in Slim Pajama. Some other corpora like Dolma and RefinedWeb, may also have overlaps from C4, Wikipedia as well as Common Crawl.\nFigure 11 demonstrates that while TriLM 3.9B is similar or better than FloatLM 3.9B on PTB and Lambada, across the other datasets, with potential overlaps with SlimPajama, it's performance is consistently worse indicating lower capability to memorize training data as well as worse in-distribution performance, despite competitive out of distribution performance."}, {"title": "D Illustrative examples of TriLM 3.9B's completion capabilities", "content": "We showcase instances of outputs produced by TriLM (3.9B) across diverse tasks, highlighting its proficiency in tasks such as comprehension, prompt completion, and creative composition."}, {"title": "Generated Output on Reading Comprehension by TriLM (3.9B)", "content": "Title: The Blitz Background: From the German point of view, March 1941 saw an improvement. The Luftwaffe flew 4,000 sorties that month, including 12 major and three heavy attacks. The electronic war intensified but the Luftwaffe flew major inland missions only on moonlit nights. Ports were easier to find and made better targets. To confuse the British, radio silence was observed until the bombs fell. X- and Y-Ger\u00e4t beams were placed over false targets and switched only at the last minute. Rapid frequency changes were introduced for X-Ger\u00e4t, whose wider band of frequencies and greater tactical flexibility ensured it remained effective at a time when British selective jamming was degrading the effectiveness of Y-Ger\u00e4t.\nQ: How many sorties were flown in March 1941?\nA: 4,000\nQ: When did the Luftwaffe fly inland missions?\nA: Only during moonlit nights\nTitle: Helsinki\nBackground: Helsinki is the capital and largest city of Finland. It is in the region of Uusimaa, in southern Finland, on the shore of the Gulf of Finland. Helsinki has a population of, an urban population of, and a metropolitan population of over 1.4 million, making it the most populous municipality and urban area in Finland. Helsinki is some north of Tallinn, Estonia, east of Stockholm, Sweden, and west of Saint Petersburg, Russia. Helsinki has close historical connections with these three cities. The Helsinki metropolitan area includes the urban core of Helsinki, Espoo, Vantaa, Kauniainen, and surrounding commuter towns. It is the world's northernmost metro area of over one million people, and the city is the northernmost capital of an EU member state. The Helsinki metropolitan area is the third largest metropolitan area in the Nordic countries after Stockholm and Copenhagen, and the City of Helsinki is the third largest after Stockholm and Oslo. Helsinki is Finland's major political, educational, financial, cultural, and research center as well as one of northern Europe's major cities. Approximately 75% of foreign companies that operate in Finland have settled in the Helsinki region. The nearby municipality of Vantaa is the location of Helsinki Airport, with frequent service to various destinations in Europe and Asia.\nQ: What is the most populous municipality in Finland?\nA: Helsinki\nQ: How many people live there?\nA: 1.4 million in the metropolitan area\nQ: What percent of the foreign companies that operate in Finland are in Helsinki?\nA: 75%\nQ: What towns are a part of the metropolitan area?\nA: Espoo, Vantaa, Kauniainen, and surrounding commuter towns"}, {"title": "Auto-complete on a creative writing task by TriLM (3.9B)", "content": "Essay Writing\nIn the year 2150, humanity had established colonies on Mars. Captain Ramirez was on a routine mission to explore an uncharted region of the planet when he and his crew were attacked by Martian natives who wanted them dead for their technology. The captain's ship crashed into the surface of Mars where it exploded in flames.\nThe survivors managed to escape from the wreckage but they soon found themselves stranded with no way out or back home. They would have to survive alone until rescue came. But what if there is nothing waiting for them? What if they are trapped forever?\nPoem Writing\nIn a garden where the roses bloom,\nUnderneath the silver moon,\nA gentle breeze begins to sway,\nAnd whispers secrets of the day.\nThe stars above start to appear,\nWhile shadows dance and disappear,\nThe night unfolds in its own time.\nThe sun rises over the horizon,\nAs it does every morning,\nBut this is no ordinary dawn.\nIt's a new beginning for all who are here today.\nFor those who have come before us,\nAnd those yet to be born.\nWe celebrate life with music and laughter,\nWith friends old and new.\nThis is our moment to shine!"}, {"title": "Auto-complete on various prompts by TriLM (3.9B)", "content": "Making a cake: Several cake pops are shown on a display. A woman and girl are shown making the cake pops in a kitchen. They use a mixer to mix ingredients, then knead dough by hand. The women roll out the dough with a rolling pin, cut it into shapes using cookie cutters, bake them, and decorate them with frosting or sprinkles.\nThe energy given up by electrons as they move through the electron transport chain is used to produce ATP. In this process, oxygen and water are produced in a reaction called cellular respiration.\nWhen talking about sensitive topics, it is crucial to be aware of the fact that there are many different types of people. Some will react negatively and others positively. It's important not to take offense when someone says something you don't like.\nIf someone disagrees with you, it is best to ignore them. The most important thing in life is not what we have but who we are and how we treat others."}]}