{"title": "Achieving Data Efficient Neural Networks with Hybrid Concept-based Models", "authors": ["Tobias A. Opsahl", "Vegard Antun"], "abstract": "Most datasets used for supervised machine learning consist of a single label per data point. However, in cases where more information than just the class label is available, would it be possible to train models more efficiently? We introduce two novel model architectures, which we call hybrid concept-based models, that train using both class labels and additional information in the dataset referred to as concepts. In order to thoroughly assess their performance, we introduce ConceptShapes, an open and flexible class of datasets with concept labels. We show that the hybrid concept-based models outperform standard computer vision models and previously proposed concept-based models with respect to accuracy, especially in sparse data settings. We also introduce an algorithm for performing adversarial concept attacks, where an image is perturbed in a way that does not change a concept-based model's concept predictions, but changes the class prediction. The existence of such adversarial examples raises questions about the interpretable qualities promised by concept-based models.", "sections": [{"title": "1 Introduction", "content": "Understanding model behavior is a crucial challenge in deep learning and artificial intelligence [1-4]. Deep learning models are inherently chaotic, and give little to no insight into why a prediction was made. In computer vision, early attempts of explaining a model's prediction consisted of assigning pixel-wise feature importance, referred to as saliency maps [5-8]. Despite gaining popularity and being visually appealing, a large number of experiments show that saliency maps perform a poor job at actually explaining model behavior [2, 3, 9-13].\nRecently, several concept-based models have been proposed as inherently interpretable [14-18]. These models are restricted to perform the downstream prediction only based on whether it thinks some predefined concepts are present in the input or not, where concepts are defined as human meaningful features. This way, the downstream predictions can be interpreted by which concepts the model thought were in the data.\nHowever, recent experiments have highlighted issues with concept-based models' interpretability. This is mainly due to the concept predictions encoding more information than just the concepts, referred to as concept leakage [19, 20]. We further add evidence to the lack of interpretability in concept-based models by introducing adversarial concept attacks (see Figure 1).\nWe will deviate from the goal of interpretability and use the framework of concept-based models to create models that tackle another crucial challenge in deep learning, namely large dataset requirements. Deep learning models require vast dataset sizes in order to reach good performance. This has resulted in large computational requirements [21], high energy usage [22, 23], and challenges in gathering enough data [24, 25].\nOur proposed model architectures use both concept predictions and information not interfering with the concepts to make the downstream prediction. This way, the models can use the concept predictions if they are helpful for the downstream task, but can also rely on a skip connection to encode information about the data not present in the concepts. We propose these models to better utilize the available information in datasets with concepts."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Concept-based Models", "content": "Concept-based models first predict some predefined concepts in the dataset, then use those concept predictions to predict the downstream task. This way, the final prediction can be interpreted by which concepts the model thought were present in the input. One of the first and most popular concept-based models is the concept bottleneck model (CBM) [15], which is a neural network with a bottleneck layer that predicts the concepts. The model is trained both using the concept labels and the target labels.\nSeveral alternatives to the CBM architecture have been proposed. Concept-based model extraction (CME) [14] may use a different hidden layer for the various concepts. Post-hoc concept bottleneck models (PCBM) [28] first learn the concept activation vectors (CAVs) [32] of the concepts, and then project embeddings down on a space constructed by CAVs. Concept embedding models (CEM) [27] produces vectors in a latent space of concepts that are different for presence and absence of a concept and predicts the probabilities of present concepts. It has been shown that concept-based models are susceptible to adversarial attacks that change the concept predictions [29], which was used to improve the robustness of the models."}, {"title": "3 Methods", "content": ""}, {"title": "3.1 New Model Architectures", "content": "We propose two novel model architectures. The first is based on a CBM [15], but uses an additional skip connection that does not go through the concept bottleneck layer (see Figure 2). The skip connection can be implemented either as a residual connection [33] or a concatenation [34], and we refer to the models as CBM-Res and CBM-Skip, respectively. This way, the model can use both the concept prediction and information not interfering with the concepts to make the final downstream prediction.\nThe other proposed architecture predicts the concepts sequentially throughout the neural network's layers, instead of all at once (see Figure 3). All of the concept predictions are concatenated together, along with the final hidden layer, and given as input to the output layer. We refer to this model as the Sequential Concept Model (SCM).\nWe use a loss function constructed by a weighted sum of a concept loss and a task loss, similarly to the joint bottleneck proposed for the CBM [15].\nAll of the proposed models are compatible with transfer learning, where the first part of the model can be a large pre-trained network. We present the models in the domain of classification, but they can easily be adapted to regression by replacing the output layer with a single node."}, {"title": "3.2 Introducing ConceptShapes", "content": "To accurately assess the performance of concept-based models, we have developed a class of flexible synthetic concept datasets called ConceptShapes. The input images consist of two shapes, where the position and orientation are random, and the downstream task is to classify which combination of shapes that are present (see Figure 4). Some examples of target classes are \"triangle-rectangle\", \"triangle-triangle\" and \"hexagon-pentagon\". Depending on how many shapes that are used, the dataset contains 10, 15 or 21 classes.\nThe key feature of the datasets are that various binary concepts are present, such as the color of the shapes, outlines and background. Given a class, some predefined concepts are drawn with a high probability $s \\in [0.5, 1]$, and the others with a low probability $1-s$. The hyperparameter $s$ can be chosen by the user. When $s = 0.5$, the concepts are drawn independently of the classes, and when $s = 1$, the concepts are deterministic given the class. The datasets can be created with either five or nine concepts.\nThe datasets are flexible with regards to the relationship between the concepts and the classes, and the number of concepts, classes and data. This way, the difficulty of correctly classifying the images can be tuned by the amount of classes and the amount of data, and the information in the concepts can be tuned by the amount of concepts and the value of $s$. Further details about the dataset can be found in Appendix B."}, {"title": "3.3 Adversarial Concept Attacks", "content": "We propose an algorithm for producing adversarial concept attacks, which given a concept-based model and input images, produces identically looking images that give the same concept predictions, but different output predictions. The algorithm is based on projected gradient descent (PGD) [35], which iteratively updates an input in the direction which maximize the classification error of the model, and projects the alteration on an $L$-infinity ball around the original input. In each iteration, we add a step where we check if the alteration causes the model to almost change the predictions of the concepts. If so, we check which pixels that are altered in a direction that changes the concept predictions, and multiply those pixels' alterations by a number in [-1,0]. The complete algorithm is covered in Appendix C.\nOur approach differs from the adversarial attacks for concept-based models done by Sinha et al. [29], which altered the concept predictions, and not the class predictions."}, {"title": "4 Experiments", "content": "We show that the hybrid concept-based models achieve the highest test-set accuracy on multiple datasets. In order to examine how the models perform when the amount of data is sparse, we train and test the models on various smaller subsets of the datasets. We also investigate how well the concepts are learned."}, {"title": "4.1 Datasets", "content": ""}, {"title": "4.1.1 Caltech-USCD Birds-200-2011 (CUB)", "content": "The CUB dataset [26] consists of N = 11788 images of birds, where the target is labeled among 200 bird species. The original dataset contains 28 categorical concepts, which makes 312 binary concepts when one-hot-encoded. The processed version used for benchmarking concept-based models [15] majority voted the concepts so that every class has the exact same concepts and removed sparse concepts, ending up with 112 binary concepts. The dataset is split in a 50%-50% training and test split, and we use 20% of the training images for validation. We train and evaluate on six different subset sizes."}, {"title": "4.1.2 ConceptShapes", "content": "We use different amounts of classes with nine concepts. We set the probability $s$ to be 0.98, in order to make the concepts useful, but not deterministic given the class. The datasets are generated with 1000 images in each class, and we split them into 50%-30%-20% train-validation-test sets. We train and evaluate on subsets sizes with 50, 100, 150, 200 and 250 images in each class, drawn from the 1000 images created. We explore other configurations of ConceptShapes in Appendix A.2."}, {"title": "4.2 Setup", "content": "We compare our proposed models against a CBM, which we refer to as vanilla CBM, and a convolutional neural network (CNN) [36] not using the concepts at all, referred to as the standard model. Additionally, we also include an oracle model, which is a logistic regression model trained only on the true concept labels, not using the input images. We call it an oracle since it uses true concept labels at test time, which are usually unknown, and do this in order to measure how much information there is in the concepts alone. We perform a hyperparameter search for each model and each different subset configuration. The hyperparameter details are covered in Appendix D.\nThe models trained on CUB use a pre-trained and frozen ResNet 18 [33] as the convolutional part of the model, while the models trained on ConceptShapes are trained from scratch. The details about the setup are explained in Appendix E. The code for running the experiments can be found at https://github.com/Tobias-Opsahl/Hybrid-Concept-based-Models.\nThe models' accuracies are evaluated on a held out test-set, which is the same for every subset configuration. We also record the Misprediction overlap (MPO) [14] to measure the quality of the concept predictions. The MPO measures the ratio of images that had m or more concept mispredictions. We use m = 1,2,...,k, where k is the amount of concepts in the dataset.\nWe run a grid search to find the most successful adversarial concept attack ratio, meaning the ratio of images that changes the model's class prediction, but not the concept predictions. We use the best vanilla CBM found in the hyperparameter searches, and compare the results to PGD [35]."}, {"title": "5 Results and Discussion", "content": ""}, {"title": "5.1 Improved Accuracy", "content": "The test-set accuracies can be found in Figure 5 and Figure 7. We observe that the hybrid concept-based models generally have the best performance.\nSince we test on relatively small datasets by deep learning standards, we interpret the results as hybrid concept-based models being able to more efficiently use the available information in sparse data settings."}, {"title": "5.2 CUB Concepts are not Learned", "content": "When inspecting the MPO plots for CUB in Figure 6, we see that none of the concept-based models learn the concepts properly. About 50% of the images are predicted with 15 or more mistakes in the concept predictions for all models, which is about as good as random guessing, since the labels are one-hot-encoded and sparse. Because earlier work has pointed out that the concepts in CUB are ambiguous and sometimes wrong [18], this might not be surprising. When inspecting the MPO plot for ConceptShapes in Figure 8, we do however see that the concepts are properly learned. This suggests that the concepts in ConceptShapes are not ambiguous, and therefore the datasets serve as better benchmark datasets for concept-based models."}, {"title": "5.3 Adversarial Concept Attacks", "content": "The results of the adversarial concept attacks can be found in Table 1. We see that a substantial amount of images are perturbed with success, and the algorithm is more effective than PGD.\nSince the concept predictions are used as the interpretation of the model behavior, but the same interpretation can lead to vastly different model behavior, we suggest that this experiment questions the interpretable qualities of concept-based models."}, {"title": "6 Conclusions", "content": "We proposed new hybrid concept-based models motivated by improving performance, and demonstrated their effectiveness on CUB and several ConceptShapes datasets. The proposed models train using both the class label and additional concept labels. By experimenting with subsets of relatively small datasets, we demonstrated that they were more data efficient than the benchmark models.\nWe also introduced ConceptShapes, a flexible class of synthetic datasets for benchmarking concept-based models. Finally, we demonstrated that concept-based models are susceptible to adversarial concept attacks, which we suggest are problematic for their promised interpretable qualities."}, {"title": "A More Experiments", "content": ""}, {"title": "A.1 Hard Bottleneck", "content": "We also conducted experiments where we rounded off the concept predictions to binary values in the concept-based models, referred to as a hard bottleneck. The results can be seen in Figure A.1 and Figure A.2. We see that the vanilla CBM's performance is reduced, while the hybrid concept-based models are not changed much. Comparing the MPO plots (Figure A.2 and Figure 6) shows little change in how well the concepts were learned."}, {"title": "A.2 ConceptShapes Datasets", "content": "We evaluate with different values of s, and the results can be seen in Figure A.3. With s = 0.5, the concepts bring no information that helps predict the classes. However, the hybrid concept-based models do not perform worse than the CNN, suggesting they are able to assign low weights to the bottleneck layer when the concepts are irrelevant. When s increases, the performance of all models does as well.\nFinally, we use five concepts instead of nine, with the results plotted in Figure A.4. We see that the oracle model has a lower accuracy, indicating there is less information in the concepts alone. Thus, the gap between the standard model and the hybrid concept-based models is a little narrower, although the hybrid concept-based models still perform the best in general."}, {"title": "B ConceptShapes Details", "content": "We now explain the ConceptShapes datasets in greater detail. The crucial feature of the datasets are the concepts. All of them are binary and independent, meaning any combination of concepts are possible. The five first concepts are based on the two shapes in the image, while the last four optional concepts are based on the background. We now describe the concepts one-by-one, and visualizations are available in Table B.1 and Table B.2. We start with the five concepts that influences the shapes:\n1.  Big shapes. Every shape had two intervals of sizes to be randomly drawn from. One interval corresponded to the small figures, and the other to big ones.\n2.  Thick outlines. The outlines of the shapes were drawn from one of two intervals. One corresponded to a thin outline, and the other to a thick one.\n3.  Facecolor. There were two possible colors for the shapes, blue and yellow.\n4.  Outline color. The shapes had two possible outline colors, red and white.\n5.  Stripes. Some shapes were made with stripes, and some were not. The stripes were in the same color as the outline.\nAll of the concepts apply to the whole image. For instance, if the image gets the thick outline concept, both shapes in the image get a thick outline.\nThe datasets that use nine concepts have all five of the concepts above, in addition to four more. While all of the five-concept datasets have black backgrounds, the nine-concept datasets split the background in two and use the color and stripes as concepts."}, {"title": "6. Upper background color.", "content": "The upper-half of the background would either be magenta or pale-green."}, {"title": "7. Lower background color.", "content": "The lower-half of the background would be either indigo or dark-sea-green."}, {"title": "8. Upper background stripes.", "content": "This represented whether there were black stripes present in the upper background or not."}, {"title": "9. Lower background stripes.", "content": "This represented whether there were black stripes present in the lower background or not.\nTo summarize, some of the image's visuals are determined by the concepts, some by the classes and some by randomness. The two shapes (from triangle, square, pentagon, hexagon, circle and wedge) are determined by the class. The shapes' size, color and outline are determined by the concepts. If the dataset uses nine concepts, the background color and stripes are also determined by the concepts. The shapes' position and rotation are determined randomly, regardless of which class or concepts they have."}, {"title": "C Adversarial Concept Attacks", "content": "We describe a high level overview of the algorithm for performing adversarial concept attacks, while the full algorithm can be found in Table 1. We do PGD [35] with an additional step. For each concept, if the concept prediction for the perturbed image is close to changing compared to the original image, we consider the concept as sensitive. We control this with a sensitivity threshold $\\gamma \\in [0,\\infty)$, so that a concept is considered sensitive if its logits is in the interval [-$\\gamma$,$\\gamma$]. We get an initial perturbation in each iteration from following the gradient of the loss with respect to the pixels of the images, similar to PGD. The perturbation is multiplied with a mask M, which is constructed so that it is 1 for pixels that do not influence sensitive concepts to be even closer to a change of prediction, and $\\beta\\in [-1,0]$ for pixels that do. We loop over the sensitive concepts and iteratively update M, and use the notation $\\mathbb{I}_{\\beta}(A = B)$ to denote an elementwise indicator function, so that it is 1 if elements in the same place in A and B are equal, and $\\beta$ if not. The Hadamard product represent elementwise multiplication.\nThe algorithm might terminate with failure due to several reasons. In any steps, if the concept predictions are changed, we terminate. If the mask M has only $\\beta$ as elements, the perturbation will not go in a direction that changes the class, and we therefore stop. Finally, if the maximum amount of steps are taken, we also terminate.\nThe algorithm can be improved in many ways, but our intention is to demonstrate that such adversarial examples are possible and easy to generate, not to make the best algorithm to do so. One possible improvement might for instance be to be able to backtrack if the concept predictions are changed.\nWhen tuning hyperparameters for the adversarial concept attacks, we chose to tune the step size a, which is multiplied with the perturbation in each step, and the sensitivity threshold $\\gamma$, which determines how close a concept prediction needs to be to change before we try to cancel out its changes.\nFor the ConceptShapes datasets, we used a grid search with step size a $\\in$ [0.003, 0.001, 0.00075] and sensitivity threshold $\\gamma \\in$ [0.1, 0.05, 0.01]. For CUB, the values were a $\\in$ [0.0001,0.000075, 0.00005] and $\\gamma \\in$ [0.1, 0.075, 0.05, 0.02]. These values were chosen after some initial experimentation. The best values were a = 0.001, $\\gamma$ = 0.1 and a = 0.000075, $\\gamma$ = 0.1, respectively. In order to reduce the running time, we sampled 200 images from the training set that was correctly predicted. We used 800 max steps for ConceptsShapes and 300 for CUB.\nWe ran the grid search with $\\beta$ = -0.3, which is the weight multiplied with pixels that would make concept predictions change, and $\\epsilon$ = 1, which determines where the adversarial images gets projected back on. After the grid search, we performed a"}, {"title": "Algorithm 1: Adversarial Concept Attack", "content": ""}]}