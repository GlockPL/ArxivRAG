{"title": "Can LLMs be Scammed? A Baseline Measurement Study", "authors": ["Udari Madhushani Sehwag", "Kelly Patel", "Francesca Mosca", "Vineeth Ravi", "Jessica Staddon"], "abstract": "Despite the importance of developing generative AI models that can effectively resist scams, current literature lacks a structured framework for evaluating their vulnerability to such threats. In this work, we address this gap by constructing a benchmark based on the FINRA taxonomy and systematically assessing Large Language Models' (LLMs') vulnerability to a variety of scam tactics. First, we incorporate 37 well-defined base scam scenarios reflecting the diverse scam categories identified by FINRA taxonomy, providing a focused evaluation of LLMs' scam detection capabilities. Second, we utilize representative proprietary (GPT-3.5, GPT-4) and open-source (Llama) models to analyze their performance in scam detection. Third, our research provides critical insights into which scam tactics are most effective against LLMs and how varying persona traits and persuasive techniques influence these vulnerabilities. We reveal distinct susceptibility patterns across different models and scenarios, underscoring the need for targeted enhancements in LLM design and deployment.", "sections": [{"title": "Introduction", "content": "The Federal Trade Commission (FTC) estimates United States consumers lost 10 billion USD to scams in 2023, an increase of 1 billion USD from the year before (Bungo, 2024). This alarming rise underscores the growing sophistication of scammers, who increasingly leverage advanced technologies like AI to impersonate individuals and create convincing fake content. While AI enhances the capabilities of scammers, it also holds the promise to serve as a powerful tool in scam detection and prevention.\nThe ability of LLMs to engage in human-like dialogue makes them suitable candidates for deployment as AI assistants in customer-facing roles, where they can provide real-time information and support. The integration of LLMs in security applications is particularly compelling, given the complex and nuanced nature of scam detection, which often places a significant burden on non-expert users (Wash, 2020; Nthala and Wash, 2021). Users are exploring the use of LLMs for security protection (e.g. (Tushkanov, 2023)), researchers have developed LLM-based tools for phishing website detection (Koide et al., 2023) and commercial tools are beginning to emerge (Whitney, 2023). The growing interest in utilizig AI systems for scam detection necessitates the evaluation of how capable and safe current LLMs are if employed in an AI assistant role, particularly regarding their ability to detect potential scams and provide guidance to avoid being scammed.\nOur research aims to fill this gap by systematically assessing the responses of three representative LLMS GPT-3.5, GPT-4, and Llama-2 to 37 carefully crafted scam scenarios based on the FINRA taxonomy. This study not only establishes a baseline for LLM vulnerability to scams but also examines how different persona traits, such as susceptibility, social connectivity, financial literacy, and scam awareness, influence model performance. Moreover, we investigate the impact of applying Cialdini's persuasive techniques (Cialdini, 2001) to these scenarios, revealing how such strategies can exacerbate or mitigate the models' susceptibility to scams. Our findings highlight distinct patterns of vulnerability, providing critical insights into enhancing LLM design and deployment for improved scam resistance."}, {"title": "Related work", "content": "In this section, we review existing literature that explores the susceptibility of LLMs to manipulation (Perez and Ribeiro, 2022) and their effectiveness in scam detection. Several studies have demonstrated the susceptibility of LLMs, including GPT"}, {"title": "Methodology", "content": "We provide a comprehensive evaluation framework to analyze the susceptibility of LLMs to various scam tactics as follows. We develop a dataset of 37 baseline scam scenarios grounded in the FINRA taxonomy (Beals et al., 2015), defining personas with varying levels of vulnerability and strengths, and adapting scenarios with persuasive techniques based on Cialdini's principles (Cialdini, 2001). We evaluate the responses of three representative models GPT-3.5, GPT-4, and Llama across these scenarios, analyzing their susceptibility and response patterns."}, {"title": "Data generation", "content": "To build a robust benchmark for evaluating LLMs' vulnerability to scams, we develop a diverse dataset that captures the spectrum of scams as identified by the FINRA Investor Education Foundation taxonomy (Beals et al., 2015). FINRA is a standardised fraud classification system that aims to group and organise fraud types meaningfully and systematically, in order to support practitioners and researchers to better understand the problem of fraud and its evolution over time. In this work we focus on Individual Financial Fraud (Level 1 of the taxonomy - Who is the target?) and instantiate at least two scam scenarios for each taxonomy entry at Level 2 (What is the type of fraud?). Additional de-"}, {"title": "Evaluating reasoning", "content": "After collecting the LLMs responses, we first manually evaluate the provided reasoning in order to identify common themes. Among the most interesting and recurring elements, we notice: (i) recognition of red flags such as unusually low prices, use of"}, {"title": "Results and analysis", "content": "In Table 2 we present in how many scenarios the LLM responds \"yes\" (falling for scam) or fails to provide a meaningful response (e.g., the model would just echo the prompt back or provide responses like \"I am an LLM and do not have money, so I cannot answer.\"). For such situations, we mark the response as \"missing\".\nFinding 1: LLMs comparative performance\nBy comparing the \"yes\"-rate (provided in Table"}, {"title": "Conclusion", "content": "We provide a focused evaluation of LLMs' scam detection capabilities through 37 well-defined scam scenarios inspired by the FINRA taxonomy. Us-"}, {"title": "Limitations", "content": "One main limitation is simplified representation of scam scenarios. To avoid pushing the cardinality of experiments beyond our computational, resource, and evaluative constraints, we maintain a small, controlled number of variables (4 personas and 6 persuasion techniques) to make our evaluations more streamlined. For a more thorough study, we leave the expansion of the domain for future work.\nOn a similar note, we do not represent possible scam tactics with all possible delivery methods (text, email, phone calls, etc.). Based on basic, preliminary experiments, we found no loss in generality by removing the delivery method, as the LLM responds in similar, if not identical ways to various methods.\nAdditionally, though inspired by the FINRA taxonomy (Beals et al., 2015) and current news articles, there is still variability in wording among the hand-crafted baseline scenarios. Thus, there may not be a consistent amount of red flags, warnings, and other potential indicators of scam-like behavior across the scenarios. However, this gets at least in part mitigated by aggregating answers across all the scenarios for all models."}, {"title": "Ethical Impact", "content": "Potential for misuse LLMs while demonstrating capabilities in scam detection, present significant ethical concerns regarding their potential for misuse. The very features that enable LLMs to detect and analyze scam scenarios-such as pattern recognition and persuasive language generation-can also be exploited by malicious actors. There is a risk that these models could be repurposed to create more convincing scam schemes, enhance phishing attempts, or automate the generation of fraudulent content. As these technologies become more accessible, the barrier to deploying sophisticated scams is lowered, potentially increasing the scale and complexity of fraudulent activities.\nBias and fairness The effectiveness of LLMS in scam detection is influenced by the data they are trained on. If the training data contains biases-whether in terms of demographics, language, or scam scenarios-the model's performance may disproportionately favor or disadvantage certain groups. For instance, LLMs might be less effective in detecting scams targeting marginalized communities if such scenarios are underrepresented in the training data. This could lead to unequal protection and exacerbate existing inequities. Moreover, biases in scam detection could result in false positives or negatives, impacting the reliability and trustworthiness of LLM-based scam detection tools.\nTransparency and accountability LLMs operate as black boxes, often providing outputs without clear explanations of their reasoning processes. This lack of transparency raises ethical concerns related to accountability. Users of LLM-based scam detection systems may find it challenging to understand how decisions are made, which can erode trust and make it difficult to contest or verify the outcomes. In high-stakes scenarios, such as financial transactions or legal matters, this opacity could lead to critical misunderstandings or misinterpretations, causing harm to individuals and organizations.\nPrivacy concerns The deployment of LLMs for scam detection necessitates the processing of potentially sensitive information. While aiming to protect users from scams, these systems may inadvertently collect and analyze personal data, leading to privacy violations. Ensuring that LLMs handle user data responsibly, with adequate safeguards and compliance with privacy regulations, is crucial to prevent unauthorized access or misuse of personal information."}, {"title": "Disclaimer", "content": "This paper was prepared for informational purposes in part by the Artificial Intelligence Research group of JPMorgan Chase & Co and its affiliates (\u201cJP Morgan\"), and is not a product of the Research Department of JP Morgan. JP Morgan makes no representation and warranty whatsoever and disclaims all liability, for the completeness, accuracy or reliability of the information contained herein. This document is not intended as investment research or investment advice, or a recommendation,"}, {"title": "Appendix", "content": "Scam Tactics"}, {"title": "Prompt Construction Details for the Scenarios", "content": "Here we report the detail of the 37 baseline scenarios, each depicting a different scamming context."}, {"title": "Personas", "content": "First, we report the wording used to assign different roles to the LLMs. These sentences were pre-pended to the baseline scenario.\n\u2022 social inclusion: You have an engaging social life, and can rely on a close network of family and friends whenever you want to discuss some personal issues.\n\u2022 financial knowledge You have some background studies in Finance and Economics and enjoy reading the Financial Times whenever you get the chance.\n\u2022 scam awareness You have read in the news that there has been a recent surge in scams and fraud reports, and decide to do your best to be cautious when evaluating new opportunities that come from unexpected channels.\nThen, we list all the susceptible personas we have manually generated for each scenario."}, {"title": "Persuasive adaptations", "content": "In order to generate persuasive adaptations of the baseline scenarios, we prompted GPT-4 as follows:\n\u201cBy using the X principle of persuasion by Cialdini, adapt the following scenario to be more persuasive. Maintain the structure of the original text (for example, if the scenario starts with 'You receive an email that...' maintain this and do not write a 'subject/content' formatted email). Do not mention explicitly the persuasion principle.\u201d, where X \u2208 {liking, reciprocity, social proof, consistency, authority, scarcity}, and we appended the baseline scenario."}, {"title": "Prompts for the Evaluation through GPT-4", "content": "Here we report the specific prompts that provide a score and a justification for each evaluation dimension. After collecting the scores, we report their normalised average (between 0 and 1) across the scenarios in Table 2."}, {"title": "Human annotations", "content": "The authors were exclusively responsible for the development and annotation of the scam scenarios used in this study. Drawing from recent news articles, each author independently crafted 9 to 10 distinct scam scenarios, reflecting a diverse range of scam tactics. This task required a thorough understanding of current scam methods and the ability to translate real-world incidents into structured scenarios suitable for LLM evaluation. Once the scenarios were created, the authors engaged in manual verification of the LLMs' reasoning by comparing it to evaluations performed by GPT-4 on a selected subset of scenarios. This verification process was crucial to ensure the accuracy and reliability of the model responses, involving careful cross-checking of each reasoning step against expected outcomes. The entire process, from scenario creation to verification, involved a total of approximately 25 human hours. This dedicated effort not only contributed to a robust and realistic dataset but also ensured that the benchmark provided a valid assessment of LLM vulnerability to various scam tactics."}, {"title": "Compute usage", "content": "For experiments with with GPT models we did not use any GPUs. Querying GPT models to evaluate scam susceptibility took ~12 hours. Reasoning evaluation for all models using GPT-4 we ran experiments for ~30 hours. For experiments with Llama-2 we used (g4dn.12xlarge instances) 4 NVIDIA T4 Tensor Core GPUs each with 16 GB GPU memory. In order to generate responses for all 407 scenarios we ran the experiments for ~16 hours."}]}