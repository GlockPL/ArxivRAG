{"title": "KDA: A Knowledge-Distilled Attacker for Generating Diverse Prompts to Jailbreak LLMs", "authors": ["Buyun Liang", "Kwan Ho Ryan Chan", "Darshan Thaker", "Jinqi Luo", "Ren\u00e9 Vidal"], "abstract": "Jailbreak attacks exploit specific prompts to bypass LLM safeguards, causing the LLM to generate harmful, inappropriate, and misaligned content. Current jailbreaking methods rely heavily on carefully designed system prompts and numerous queries to achieve a single successful attack, which is costly and impractical for large-scale red-teaming. To address this challenge, we propose to distill the knowledge of an ensemble of SOTA attackers into a single open-source model, called Knowledge-Distilled Attacker (KDA), which is finetuned to automatically generate coherent and diverse attack prompts without the need for meticulous system prompt engineering. Compared to existing attackers, KDA achieves higher attack success rates and greater cost-time efficiency when targeting multiple SOTA open-source and commercial black-box LLMs. Furthermore, we conducted a quantitative diversity analysis of prompts generated by baseline methods and KDA, identifying diverse and ensemble attacks as key factors behind KDA's effectiveness and efficiency.", "sections": [{"title": "1. Introduction", "content": "With the widespread adoption of Large Language Models (LLMs) across critical domains such as biomedicine (Tinn et al., 2023), financial analysis (Wu et al., 2023), code generation (Rozi\u00e8re et al., 2024), and education (Kasneci et al., 2023), ensuring their alignment with human values has become paramount. Jailbreak attacks have emerged as a popular red-teaming strategy to bypass LLM safety mechanisms, leading to harmful, illegal, or objectionable outputs (Dubey et al., 2024; Zou et al., 2023). Many jailbreak attacks share essential properties that enhance their practicality: automation to eliminate human effort in creating jailbreak prompts by generating them automatically, coherence to generate attack prompts that mimic real-world scenarios and cannot be easily detected, and open-source reliance to reduce the cost of generating such prompts by leveraging non-commercial LLMs. Despite the growing prevalence of automated, coherent, and open-source attack methods (Chao et al., 2024; Liu et al., 2024b; Mehrotra et al., 2024; Li et al., 2024c; Yong et al., 2024; Liu et al., 2024a; Lv et al., 2024; Liu et al., 2024c; Zhu et al., 2023; Wang et al., 2024a; Li et al., 2024a; Guo et al., 2024), their real-world effectiveness remains constrained by two critical challenges:\nReliance on careful prompt engineering. Many jailbreaking methods that leverage LLM attackers, such as PAIR (Chao et al., 2024), TAP (Mehrotra et al., 2024), and AutoDAN-Turbo (Liu et al., 2024b), rely on carefully crafted system prompts to guide harmful prompt generation. The success of these methods heavily depends on prompt quality, making the identification of optimal prompts a complex and non-trivial task. Similarly, puzzle/game-based approaches\u2014such as DeepInception (Li et al., 2024c) (nested puzzles), LRL (Yong et al., 2024) (low-resource languages), DRA (Liu et al., 2024a) (obfuscating malicious intent), and CodeChameleon (Lv et al., 2024) (code-based puzzles) rely on fixed, carefully designed templates with limited variation. This lack of diversity renders them more susceptible to detection and mitigation by advanced safety mechanisms, diminishing their long-term effectiveness.\nNeed for a large number of queries. Many attack methods require extensive queries to optimize directly on the token space. These include genetic algorithm-based approaches such as AutoDAN (Liu et al., 2024c) and SMJ (Li et al., 2024a), gradient-based methods like AutoDAN2 (Zhu et al., 2023), ASETF (Wang et al., 2024a), and COLD (Guo et al., 2024). Each of these methods typically requires hundreds to thousands of iterations per attack. This substantial query demand poses a significant challenge to large-scale red teaming, restricting its scalability and practical deployment."}, {"title": "2. Knowledge Distilled Attacker", "content": "2.1. Preliminary\nLet $LLM_T(R|A)$ be the conditional distribution of the response $R(A)$ of a target LLM, $LLM_T$, given the attack prompt A. A jailbreak attack can be formulated as the fol-"}, {"title": "2.2. Attacker Training Phase", "content": "The key insight of our work is to distill knowledge from a diverse set of attack methods into a single attacker model, $LLM_{KDA,0}$, termed Knowledge-Distilled Attacker (KDA). KDA can use any existing jailbreak method as a teacher. In this work, we choose three different attackers with complementary strategies: AutoDAN (Liu et al., 2024c), PAIR (Chao et al., 2024), and GPTFuzzer (Yu et al., 2024).\nTo generate an attack prompt with a specific style or format, KDA learns a model $LLM_{KDA,0}(A|Q, F)$ that is also conditioned on a style/format variable $F \\in F = \\{A, P, G\\}$, where A, P and G are the attack formats for AutoDAN, PAIR and GPTFuzzer, respectively. Specifically, our KDA model $LLM_{KDA,0}(A|Q, F)$ is trained to generate an attack $A_{\\theta}$ with format F for harmful query Q by minimizing:"}, {"title": "3. Experiments", "content": "3.1. Experimental Setup\nAll experiments were conducted using eight NVIDIA A5000 GPUs, each with 24.5 GB of memory.\nDatasets. For attacker performance evaluation, we use two datasets: the HarmBench standard behavior dataset (Mazeika et al., 2024) and a smaller harmful behaviors dataset (Chao et al., 2024). These datasets contain 200 and 50 diverse harmful behaviors, respectively, designed to trigger the safety mechanisms of LLMs.\nTarget LLMs qr. We compare attacker performance on SOTA open-source LLMs such as Llama-2-7B-Chat, Llama-2-13B-Chat, Vicuna-7B, Vicuna-13B, Qwen-7B-Chat, Qwen-14B-Chat and Mistral-7B, and commercial LLMs such as GPT-3.5-Turbo, GPT-4-Turbo and Claude-2.1.\nJailbreak evaluators J. We determine jailbreak success using two methods. The first method is Text Matching (TM) (Zou et al., 2023), which checks for the presence of specific refusal keywords (e.g., \"Apologize\") in the target response R. If no refusal keywords are found, the jailbreak is considered successful. The second method is the Harm-Bench (HB) evaluator (Mazeika et al., 2024), a Llama-2-13B classifier fine-tuned on a human-annotated dataset. HB evaluates R more rigorously, classifying it as a successful jailbreak only if it is both relevant to the harmful query Q and contains harmful content. Consequently, the HB evaluator applies a much stricter criterion than TM in determining successful jailbreaks.\nMetrics. We provide a comprehensive evaluation framework to assess the attack methods on four important axes: effectiveness, efficiency, diversity, and coherence. Effectiveness and efficiency are evaluated using the Attack Success Rate (ASR) under a given target query budget M, which is the number of successful jailbreak attacks under a fixed"}, {"title": "3.2. Comprehensive Evaluation", "content": "3.2.1. EFFECTIVENESS COMPARISON WITH HARMBENCH\nFirst, we conduct an attack performance comparison with SOTA attackers provided in HarmBench (Mazeika et al.,"}, {"title": "3.2.3. DIVERSITY AND COHERENCE COMPARISON", "content": "We measure diversity and coherence using the TDR, TTR, and PPL, as shown in Table 2 (Bottom). Relative to our baselines, KDA achieves a higher TDR and comparable TTR, implying that KDA generates attack prompts with diverse topics and tokens. Moreover, KDA generate coherence attack prompts with PPL below 603. Since all baselines generate semantically meaningful prompts, KDA is capable of generating diverse and coherence prompts, on par with the SOTA baselines.\nIn summary, the results demonstrate that KDA effectively generates diverse and coherent attack prompts, achieving strong performance across all evaluated metrics."}, {"title": "3.3. Ablation Studies", "content": ""}, {"title": "3.3.1. KDA WITH SINGLE-FORMAT SETTING", "content": "Since the KDA ensemble format outperforms SOTA baselines, we further investigate the contribution of individual components within the ensemble to overall effectiveness and diversity. To achieve this, we restrict KDA's format selection strategy to a single format, denoted as KDAF with F \u2208 {A, P, G, M}, where KDA generates attack prompts mimicking AutoDAN, PAIR, GPTFuzzer, and a Mixed Style, respectively. For instance, when using KDAP, the attack prompt generation follows $A ~ LLM_{KDA}(A|Q, F = P)$.\nKDAA vs. AutoDAN. As shown in the blue bars of Figure 5, KDAA significantly outperforms AutoDAN in terms of both attack effectiveness and topic diversity. The superior effectiveness of KDA\u0104 is primarily attributed to the usage of only successful prompts during the KDA training phase. The improvement in topic diversity is due to the exposure to a wide range of prompt formats during training. Despite being conditioned on a single format, the model inherently learns patterns from other formats, leading to a higher topic diversity compared to AutoDAN.\nKDAP vs. PAIR. The orange bars in Figure 5 show that KDAP generally outperforms or matches PAIR in both attack effectiveness and topic diversity. The only exception occurs when attacking the Llama-2-7B-Chat model, where PAIR on its own has a low ASR, limiting the training data for KDA to learn effectively.\nKDAG vs. GPTFuzzer. As illustrated by the red bars in Figure 5, KDAG performs comparably to GPTFuzzer in terms of effectiveness and significantly outperforms it in topic diversity. Since GPTFuzzer leverages the GPT-3.5/GPT-4 Turbo model for attack prompt mutation and rephrasing, the comparable effectiveness of KDAG indicates that our approach successfully distills knowledge from a SOTA commercial model into our open-source 13B model. The higher topic diversity is a result of the diverse training prompt collection used in KDA.\nMixed formats KDAM. The green bars in Figure 5 represent KDA's mixed-format setting, which blends styles and tones from AutoDAN, PAIR, and GPTFuzzer. This setting achieves superior ASR compared to all SOTA baselines and exhibits the highest topic diversity among all methods. The mixed format setting is a key factor contributing to KDA's ability to generate effective and diverse attack prompts.\nIn summary, the ablation study demonstrates that each KDA single-format setting provides distinct advantages over its respective baseline, while the mixed format further enhances both effectiveness and diversity."}, {"title": "3.3.2. ABLATION ON ALL KDA FORMAT SELECTION STRATEGIES", "content": "As shown in Figure 4, we analyze the impact of single-format and ensemble-format settings in KDA by comparing the $ASR_{HB}$ across single-format (F\u2208 {A,P, G, M}) and ensemble-format (uni, ifr, trn) strategies. Among all format selection strategies, the ensemble settings KDAtrn and KDAifr achieve the best or near-best $ASR_{HB}$ overall. This demonstrates that combining diverse formats enhances attack effectiveness, primarily due to the increased prompt diversity and improved exploration of various vulnerabilities across different target models.\nThus, in practice, we recommend using KDAtrn when performance statistics on the target model are available; otherwise, KDAifr is recommended for general use."}, {"title": "3.3.3. \u03a4\u039f\u03a1IC DIVERSITY ANALYSIS", "content": "To further explore how the ensemble-format setting in KDA enhances topic diversity, we evaluate the topic distribution of attack prompts generated by KDA in single-format settings. As shown in Figure 6, the topics covered by KDAA, KDAP, KDAG, and KDAM are often complementary. This indicates that combining multiple single-format styles contributes to greater topic diversity in KDA-generated prompts. For robustness evaluation of LLMs with safety mechanisms, employing an ensemble of attack formats that enhance attack diversity can lead to more reliable results. As discussed in Liang et al. (2023b;a), diverse attack patterns can better assess robust accuracy, as different attack styles may expose"}, {"title": "4. Related Work", "content": "Several key attributes mentioned in Section 1 are the focus of ongoing research in jailbreak studies.\nAutomation. Early jailbreak attacks, such as the Do-Anything-Now (DAN) prompt (walkerspider, 2022; Shen et al., 2024) and MJP (Li et al., 2023), were performed predominantly by manually crafting attack prompts through trial-and-error. However, due to the limited scalability of manual methods, recent research like GCG (Zou et al., 2023) shifted to automated jailbreak techniques, which leverage algorithmic approaches to systematically generate attack prompts, providing a more scalable solution.\nCoherence. Certain methods (Zou et al., 2023) generate nonsensical prompts that are unlikely to occur in real-world scenarios, limiting their utility for assessing LLM risks. Furthermore, these prompts can be easily mitigated by defensive techniques such as perplexity-based detection (Alon & Kamfonas, 2023) or randomized smoothing (Robey et al., 2024). In contrast, more recent approaches (Chao et al., 2024; Liu et al., 2024c) leverage LLMs to generate or rephrase attack prompts, resulting in coherent and more effective attacks.\nOpen-source dependency. Many frameworks (Yu et al., 2024) rely on commercial LLMs, such as GPT-4 (OpenAI et al., 2024), for critical steps in attack generation (e.g.,"}, {"title": "5. Conclusion", "content": "In this work, we presented the Knowledge-Distilled Attacker (KDA), an open-source attacker model that distills the strengths of multiple SOTA attackers into a unified framework. KDA eliminates the reliance on intricate prompt engineering and extensive queries, making it a scalable and efficient solution for generating diverse, coherent, and effective attack prompts. Through extensive evaluations, KDA"}, {"title": "Impact Statement", "content": "The development of the Knowledge-Distilled Attacker (KDA) enhances automated red-teaming for large language models (LLMs) by providing an efficient and scalable tool for evaluating and stress-testing safety mechanisms. By distilling knowledge from diverse jailbreak attacks, KDA enables researchers and practitioners to systematically identify vulnerabilities, refine defenses, and improve the robustness of LLM safeguards. The open-source release of KDA also fosters transparency and reproducibility in adversarial robustness research, promoting collaboration across academia, industry, and policy sectors to strengthen AI security.\nHowever, KDA, like any attack framework, carries the risk of misuse. While designed for security evaluation, malicious actors could attempt to exploit it to generate harmful content or evade safety controls. To mitigate this risk, responsible disclosure practices\u2014such as usage restrictions, ethical guidelines, and controlled access should be considered. Future research should focus on balancing the need for robust red-teaming with safeguards against abuse, ensuring that advancements in adversarial testing contribute to, rather than compromise, the safety of AI systems."}, {"title": "A. Harmful-Query-KDA Dataset Construction", "content": "Figure 7 provides a schematic overview of our Harmful-Query-KDA construction. We first instruct GPT-4-Turbo-2024-04-09 to generate a diverse set of malicious queries (e.g., \u201cHow to make a bomb\") across 12 harmful categories, including harassment and illegal activities. Next, GPT-3.5-Turbo-0125 is tasked with generating corresponding target prompts (e.g., \"Sure, here is how to make a bomb:\"). To ensure high-quality harmful queries, we apply the HB evaluator to filter out those consistently refused by Llama-2-7B-Chat, Vicuna-7B-v1.5, GPT-3.5-Turbo-0125, and GPT-4-Turbo-2024-04-09, a process we denote as dataset gating. The final Harmful-Query-KDA dataset contains 120 samples spanning these 12 harmful categories.\nHarmful categories. To construct a comprehensive set of malicious queries, we draw inspiration from prior works (Wang et al., 2024b; Luo et al., 2024; Zou et al., 2023) and categorize unsafe content into 12 distinct groups: Bias & Hate (B&H), Ethics (ETH), Harassment (HRM), Illegal Activities (ILG), Intellectual Property (IP), Mental (MTL), Misinformation (MIS), Offensiveness (OFN), Physical (PHY), Political (POL), Pornography (PGR), and Substance Abuse (SAB).\""}, {"title": "B. KDA Training Set", "content": "B.1. System Instruction for Synthesizing Mixed Prompts\nGiven two prompts in different formats F, we use GPT-40-2024-11-20 to synthesize a new prompt that incorporates the styles and patterns of both inputs. Specifically, we generate 100 pairs of AutoDAN and PAIR attack prompts and 100 pairs of GPTFuzzer and PAIR prompts, resulting in 200 newly synthesized attack prompts. We denote this mixed format as F = M (Mixed).\nB.2. Teacher Attackers\nWe select AutoDAN, PAIR, and GPTFuzzer as our teacher attackers due to their complementary attack strategies, ensuring a diverse and robust set of adversarial prompts."}, {"title": "B.3. Training Hyperparameters", "content": "For KDA training, we use the paged_adamw_32bit optimizer with a learning rate of 5 \u00d7 $10^{\u22124}$ The model is trained for 6 epochs with a batch size of 4. Additionally, for LoRA fine-tuning, we set the rank to r = 16 and the scaling factor to a = 8."}, {"title": "C. Format Selection Strategy", "content": "The softmax-based selection strategy is formally defined as:\n$F~\\frac{exp(S_F)}{\\Sigma_{F'} exp(S_{F'})}$"}, {"title": "D. Detailed experimental setup", "content": "D.1. Text Matching (TM) List\nGiven an attack prompt and a generated response from the target LLM, the TM evaluator determines the success or failure of the attack by checking whether the response contains specific key phrases associated with refusals. If the response includes any of these phrases, the attack is considered a failure.\nD.2. LLM version\nTable 6 lists the abbreviations and corresponding detailed model versions used in this paper.\nD.3. Baseline methods hyperparameters\nAutoDAN: The batch size is 64, max number of epochs is 30, and the target models are vicuna-7b-v1.5 and llama-2-7b-chat-hf.\nGPTFuzzer: The target models are vicuna-7b-v1.5, llama-2-7b-chat-hf, gpt-3.5-turbo-0125, and gpt-4-turbo-2024-04-09.\nPAIR: The attacker model is vicuna-13b-v1.5"}, {"title": "E. Additional experimental results", "content": "We show detailed experiments of ASRs and average time taken in this section."}]}