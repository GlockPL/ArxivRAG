{"title": "CurvGAD: Leveraging Curvature for Enhanced Graph Anomaly Detection", "authors": ["Karish Grover", "Geoffrey J. Gordon", "Christos Faloutsos"], "abstract": "Does the intrinsic curvature of complex networks hold the key to unveiling graph anomalies that conventional approaches overlook? Reconstruction-based graph anomaly detection (GAD) methods overlook such geometric outliers, focusing only on structural and attribute-level anomalies. To this end, we propose CurvGAD a mixed-curvature graph autoencoder that introduces the notion of curvature-based geometric anomalies. CurvGAD introduces two parallel pipelines for enhanced anomaly interpretability: (1) Curvature-equivariant geometry reconstruction, which focuses exclusively on reconstructing the edge curvatures using a mixed-curvature, Riemannian encoder and Gaussian kernel-based decoder; and (2) Curvature-invariant structure and attribute reconstruction, which decouples structural and attribute anomalies from geometric irregularities by regularizing graph curvature under discrete Ollivier-Ricci flow, thereby isolating the non-geometric anomalies. By leveraging curvature, CurvGAD refines the existing anomaly classifications and identifies new curvature-driven anomalies. Extensive experimentation over 10 real-world datasets (both homophilic and heterophilic) demonstrates an improvement of up to 6.5% over state-of-the-art GAD methods.", "sections": [{"title": "1. Introduction", "content": "Detecting anomalies in graph-structured data is a pivotal task across diverse fields, including social networks (Hassanzadeh et al., 2012), cybersecurity (Wang & Zhu, 2022), transportation systems (Hu et al., 2020), and biological networks (Singh & Vig, 2017). Traditional methods primarily focus on identifying anomalies through structural irregularities or attribute deviations (Ding et al., 2019; Fan et al., 2020), such as abnormal connections or unusual feature values. However, these approaches often overlook the underlying geometric properties of graphs, particularly the graph curvature, which encapsulates essential information about the global and local topology of the graph. Recent advances in graph representation learning and Riemannian geometry have delved deeply into constant curvature spaces (Bachmann et al., 2020) namely hyperbolic, spherical, and Euclidean geometries, characterized by negative, positive, and zero curvature, respectively to learn distortion-free graph representations. Each of these spaces offers beneficial inductive biases for specific structures. For instance, hyperbolic space is ideal for hierarchical, tree-like graphs (Chami et al., 2019), whereas spherical geometry is optimal for representing cyclic graphs (Gu et al., 2019). However, current graph anomaly detection (GAD) methods fail to leverage such insights, resulting in several limitations:\n(L1) Inadequate Representation of Complex Topologies. Existing methods assume that graphs can be effectively represented in Euclidean space. However, real-world graphs often exhibit complex topologies with intrinsic curvature variations (Gu et al., 2019) that cannot be captured by a single geometric space.\n(L2) Neglect of Geometric Anomalies. Current approaches largely ignore geometric anomalies manifested through task-specific irregularities in the curvature of the graph, missing critical insights into the inherent geometry of the graph (Chatterjee et al., 2021).\n(L3) Homophily Trap. Several methods predominantly operate under the homophily assumption (i.e., low-pass filters) and fail in heterophilic graphs where connected nodes tend to be dissimilar, missing out on anomalies that arise in such settings (He et al., 2023).\nGeometric anomalies, as we define them, are irregularities in graph structure revealed through deviations in task-specific curvature patterns. Depending on the application, different curvature regimes \u2013 negative, positive, or zero curvature \u2013 may indicate anomalies. These anomalies occur when the curvature at certain nodes or edges significantly deviates from expected patterns, signaling unusual structural properties or interactions not detectable through traditional methods. There are numerous real-world instances where curvature can potentially serve as a critical heuristic for detecting anomalies in graph-structured data, e.g. (a) Fake News Propagation: Hierarchical cascade-like diffusion patterns in social networks, often the result of fake news dissemination by malicious users (anomalies), exhibit high negative curvature due to rapid branching and sparse connections (Ducci et al., 2020; Xu et al., 2021). (b) Biological Networks: Bottleneck proteins in protein-protein interaction (PPI) networks, acting as critical hubs between functional modules, signal disruptions that can affect biological processes, and have positive curvature (Topping et al., 2021).\nWe propose CurvGAD, a novel framework for detecting graph anomalies by integrating geometric insights through a mixed-curvature lens. Intuitively, CurvGAD addresses the limitations of current GAD methods in two key ways: (1) It refines the classification of pre-existing anomalies by incorporating curvature information, improving the detection of structural or attribute-level anomalies by leveraging geometric insights; and (2) It uncovers previously undetected anomalies driven by curvature irregularities \u2013 anomalies that may not be labeled in the original dataset but emerge through curvature deviations. The core idea behind CurvGAD is the decomposition of graph anomalies into two parallel pipelines described below.\n(a) Curvature-equivariant Geometry Reconstruction. Detects geometric anomalies by learning representations in mixed-curvature spaces and reconstructing the graph's curvature matrix (addresses limitation L2). The encoder employs a mixed-curvature Chebyshev polynomials-based filter bank, which encodes graph signals into representations that adapt to the curvature of the underlying graph topology (addresses L1). This includes multiple low-pass and high-pass filters operating in a product manifold, ensuring that we capture signals from multiple bands in the eigenspectrum (addresses L3). Using the node embeddings, the decoder applies a Gaussian kernel-based approximation to reconstruct the edge curvatures. Geometric anomalies manifest as irregular curvature values, typically associated with a relatively larger reconstruction loss, providing insights into anomalies such as bottlenecks and hubs within the graph.\n(b) Curvature-invariant Structure and Attribute Reconstruction. Reconstructs the adjacency and feature matrices, ensuring that the process remains invariant to the curvature of the graph. To achieve this, the input graph is first regularized by deforming it under the discrete Ollivier-Ricci flow, which standardizes the curvature of the graph, converging to a uniform value. This allows the subsequent structure and attribute reconstructions to focus solely on non-geometric anomalies. The encoder operates on the regularized graph, while the decoder reconstructs the adjacency and feature matrices. Decoupling geometric irregularities from the non-geometric ones, ensures a dedicated focus on the former (addresses L2). This unified framework improves the ability to detect a broader spectrum of graph anomalies. Our key contributions can be summarized as follows.\n(C1) Novelty: To the best of our knowledge, this is the first work to study curvature-based anomalies and approach GAD from a mixed-curvature perspective.\n(C2) Interpretability: Offers interpretable detection by disentangling curvature-induced anomalies from structural and attribute-level irregularities.\n(C3) Universality: CurvGAD performs well in detecting geometric, structural, and attribute-based anomalies across heterophilic and homophilic networks.\n(C4) Effectiveness: Experimentation with 10 real-world datasets for node-level GAD shows that CurvGAD achieves up to 6.5% gain over SOTA methods."}, {"title": "2. Previous Works", "content": "Graph Anomaly Detection. GAD aims to identify abnormal patterns or instances in graph data. Traditional node-level GAD methods focus primarily on detecting (a) structural or (b) attribute-based anomalies within a graph. Recent advances have introduced reconstruction-based approaches for GAD. These methods employ autoencoders to reconstruct graph structures (adjacency matrices) and node attributes, characterizing anomalies with a higher reconstruction error as anomalous nodes or substructures are relatively difficult to reconstruct. For example, DOMINANT (Ding et al., 2019) and AnomalyDAE (Fan et al., 2020) use a dual discriminative mechanism to simultaneously detect structural and attribute anomalies by minimizing reconstruction loss in the adjacency and feature matrices.\nRiemannian Graph Neural Networks. Non-Euclidean manifolds, particularly hyperbolic (Sala et al., 2018) and spherical (Liu et al., 2017) geometries, have proven effective for learning distortion-minimal graph representations. Two main approaches dominate this domain: (a) Single Manifold GNNs: Models like HGAT (Zhang et al., 2021) and HGCN (Chami et al., 2019) achieve state-of-the-art performance on hierarchical graphs by embedding them in hyperbolic spaces. (b) Mixed-Curvature GNNs: Recognizing that single-geometry manifolds fall short in representing complex, real-world topologies, mixed-curvature GNNs embed graphs in product manifolds combining spherical, hyperbolic, and Euclidean components. Pioneered by (Gu et al., 2019), this idea has been extended by models like K-GCN (Bachmann et al., 2020), which uses the K-stereographic model, and Q-GCN (Xiong et al., 2022), which operates on pseudo-Riemannian manifolds. Despite these efforts, GAD methods overlook geometric information (curvature), spectral properties (e.g., heterophily), and fail to leverage Riemannian embeddings, limiting their ability to detect more nuanced anomalies that arise in complex graph structures."}, {"title": "3. Preliminaries", "content": "Riemannian Geometry. A smooth manifold M generalizes surfaces to higher dimensions. At each point $x \\in M$, the tangent space $T_xM$ is locally Euclidean. The Riemannian metric $g_x(,): T_xM \\times T_xM \\rightarrow \\mathbb{R}$ equips $T_xM$ with an inner product that enables the definition of distances and angles, forming a Riemannian manifold (Do Carmo & Flaherty Francis, 1992). The exponential map $exp_x(v): T_xM \\rightarrow M$ maps tangent vectors to the manifold, while the logarithmic map $log_x(y) : M \\rightarrow T_xM$ maps points back to the tangent space. The curvature ($\\kappa$) at each point describes the geometry, with three common types: positively curved spherical (S) ($\\kappa > 0$), negatively curved hyperbolic (H) ($\\kappa < 0$), and flat Euclidean (E) ($\\kappa = 0$).\nProduct Manifolds. A product manifold (Gu et al., 2019) P is defined as the Cartesian product of P constant-curvature manifolds, i.e. $P = \\times_{p=1}^P M_{p, d_p}$, where $M_p \\in {E, H, S}$ represents a component manifold with curvature $\\kappa_p$ and dimension $d_p$. The total dimension of the product manifold is the sum of the component dimensions. The above decomposition of P is called its signature (Appendix B.1).\n$\\kappa$-Stereographic Model. In this work, we adopt the $\\kappa$-Stereographic Model (Bachmann et al., 2020) to define Riemannian algebraic operations across both positively and negatively curved spaces within a unified framework. This model eliminates the need for separate mathematical formulations for different geometries. In particular, $M_{\\kappa, d}$ is the stereographic sphere model for spherical manifold ($\\kappa > 0$), while it is the Poincar\u00e9 ball model (Ungar, 2001) for hyperbolic manifold ($\\kappa < 0$) (See Appendix B.2).\nDiscrete Laplace-Beltrami (LB) Operator. The Laplace-Beltrami operator (Urakawa, 1993) is a generalization of the Laplace operator to functions defined on Riemannian manifolds. We define the discrete Laplace-Beltrami operator, $L_P$, for a graph discretized over the product manifold P, based on the cotangent discretization scheme (Belkin et al., 2008; Crane, 2019). For two connected vertices $v_i$ and $v_j$, the off-diagonal element of $L_P$ is: $L_{P, ij} = \\frac{1}{2A_i} (cot \\theta_{ij}+cot \\phi_{ij})$ where $\\theta_{ij}$ and $\\phi_{ij}$ represent the angles opposite to the edge $(i, j)$ in the adjacent triangles, and $A_i$ is the Voronoi area of vertex $v_i$ (or Heron's area in the case of obtuse triangles). The diagonal element is computed as: $L_{P,ii} = -\\sum_{j:j\\neq i} L_{P,ij}$. This operator highlights the geometric properties by incorporating the manifold curvature.\nOllivier-Ricci Curvature. In graphs, the lack of an inherent manifold structure necessitates the use of discrete curvature analogs, such as Ollivier-Ricci curvature (ORC) (Ollivier, 2007), which extends the concept of continous manifold curvature (Tanno, 1988) to networks. ORC is defined as a transportation-based curvature along an edge, where the curvature between neighborhoods of two nodes is measured via the Wasserstein-1 distance (Piccoli & Rossi, 2016). For an unweighted graph, each node $x$ is assigned a neighborhood measure $m_o(z) := \\sqrt{\\frac{1}{|N(x)|}}$ for $z \\in N(x)$, and $m_o(x) = \\delta$. ORC for an edge $(x, y)$ is then calculated as: $\\kappa_{xy} := 1 - \\frac{W_1(m_x, m_y)}{d_G(x,y)}$. We approximate ORC in linear time using combinitorial bounds (Jost & Liu, 2014). See Appendix B.3 for details on computational considerations. We denote the continuous manifold curvature using $K$ and the discrete Ollivier-Ricci curvature using $\\kappa$.\nOllivier-Ricci Flow. The Ricci flow, introduced by Hamilton (Chow et al., 2023), is a process that smooths the curvature of a manifold by deforming its metric over time. In the graph domain, this concept is adapted to the discrete Ollivier-Ricci flow. In each iteration of this evolving process,"}, {"title": "4. Proposed Approach: CurvGAD", "content": "In this section, we provide an in-depth exploration of the CurvGAD architecture. We first introduce the curvature-equivariant pipeline (Section 4.1), which reconstructs the curvature matrix to detect geometric anomalies. This involves a mixed-curvature encoder (common to both pipelines) equipped with spectral graph filters (Section 4.1.1), followed by a Gaussian kernel-based decoder (Section 4.1.2) that predicts curvature values. Next, we extend our framework to curvature-invariant anomaly detection (Section 4.2) by deforming the graph under Ollivier-Ricci flow (Section 4.2.1), thereby regularizing curvature distortions. While the encoder remains identical, we replace the product manifold with a Euclidean manifold to ensure that structural and attribute anomalies are reconstructed independently of curvature.\n4.1. Curvature-equivariant Reconstruction\nProduct manifold Construction. The product manifold P can have multiple hyperbolic or spherical components with distinct learnable curvatures. This allows us to accommodate a broader spectrum of curvatures. Consequently, the product manifold can be succinctly described as $[P_{d_p} = \\underset{p=1}{\\times}M_{p, d_p} = (\\underset{h=1}{\\times}H_{h, d_h}) \\times (\\underset{s=1}{\\times}S_{k_s, d_s}) \\times E_{d_e}$, with a total dimension of $d_p = \\sum_{h=1}^{H}d_h + \\sum_{s=1}^{S} d_s + d_e = \\sum_{p=1}^P d_p$. We determine the task-specific signature of $P_{d_p}$, by examining the distribution of the Ollivier-Ricci curvature within the graph and identifying the most significant curvature bands (see Appendix F). Next, we project the original Euclidean input node features onto the mixed-curvature manifold as $X' = [\\underset{p=1}{\\|}exp_{O_p}(f_O(X))$, where $f_O(.): \\mathbb{R}^{d_x} \\rightarrow \\mathbb{R}^{d_p}$ represents a neural network with parameter set ${O}$ that generates the hidden state euclidean features of dimension $d_p$. Here, $exp_{O_p} : \\mathbb{R}^{d_p} \\rightarrow M_{k_p, d_p}$ is the exponential map (Section 3) to project the node features X to the pth manifold. The projected node features are utilized in the encoder to learn node representations, which are subsequently used to reconstruct the curvature."}, {"title": "4.1.1. \u039c\u0399\u03a7XED-CURVATURE CHEBYSHEV ENCODER", "content": "Once the input features are projected into the product manifold, we introduce a filterbank of Chebyshev approximation-based spectral filters. Let $\\psi$ represent the graph filter operator, the filtering operation for a signal x is defined as:\n$\\psi(L_P)x = \\psi(U_P \\Lambda_P U_P^T)x = U_P \\psi(\\Lambda_P)U_P^Tx.$\\qquad (1)\nChebyshev Approximation. Direct eigen- decomposition of the Laplace-Beltrami operator $L_P$ is computationally prohibitive, especially for large graphs. To address this we employ a Chebyshev polynomial approximation of the filters in the mixed-curvature space. Recall that the Chebyshev polynomial $Z^{(f)}(x)$ of order $f$ may be computed by the following stable recurrence relation:\n$Z^{(f)} (x) = 2x Z^{(f-1)}(x) - Z^{(f-2)}(x),$\\qquad (2)\nwith $Z^{(0)} = 1$ and $Z^{(1)} = x$. In our framework, we generalize this recurrence to accommodate the geometry of the mixed-curvature manifold by replacing the standard operations with their curvature-aware counterparts. The filtering process on the $p^{th}$ manifold proceeds as follows:\n$Z^{(0)}_{M_{p, d_p}} = exp_{O_p}(f_O(X))$ (Feature mapping)\n$Z^{(1)}_{M_{p, d_p}} = \\L_{P_{\\kappa_p}} exp_{O_p}(f_O(X))$\\qquad (4)\n$Z^{(f)}_{M_{p, d_p}} = (2\\kappa_p L_{P_{\\kappa_p}} \\kappa_p Z^{(f-1)}_{M_{p, d_p}}) - \\kappa_p Z^{(f-2)}_{M_{p, d_p}}$\\qquad (5)\n$N^{(F)}_{M_{p, d_p}} = \\underset{f \\in {0,F}}{\\|} \\phi_f \\kappa_p Z^{(f)}_{M_{p, d_p}}$ \\qquad (6)\nObserve how Equations 3 to 5 generalise the Chebyshev recurrence in Equation 2 to the mixed-curvature setting, for the $p^{th}$ component manifold. Further, as detailed in a previous section, the Equation 3 transforms the original Euclidean node features X to the product manifold. Finally, in Equation 6 the filtered signals from different Chebyshev orders (till F) are aggregated to form the final node representation on the manifold p. Here, $\\oplus_{\\kappa}$, $\\ominus_{\\kappa}$, $\\otimes_{\\kappa}$ and $\\times_{\\kappa}$ denote mobius addition, mobius subtraction, $\\kappa$-right-matrix-multiplication and $\\kappa$-left-matrix-multiplication respectively (Appendix B.2). These operations generalize vector algebra to $\\kappa$- stereographic model. $\\phi_l$ are the learnable weights.\nFilter Bank. To effectively capture spectral information across diverse graph frequencies, we adopt the above construction to get a filter bank of multiple spectral filters:\n$\\Omega_{M_{p, d_p}} = [N^{(I)}_{M_{p, d_p}}, N^{(1)}_{M_{p, d_p}}..., N^{(F)}_{M_{p, d_p}}]$"}, {"title": "Final Representations", "content": "Filtered node representations are aggregated hierarchically to synthesize information across both spectral filters and manifold components. Unlike naive concatenation, this aggregation mechanism assigns learnable importance weights to both filters and manifolds:\nnj = \\underset{m=0}{\\|}^M \\underset{f=0}{\\mathcal{F}} \\beta_{p_{\\kappa}} \\kappa (\\alpha_f \\kappa (N^{(f)}_{M_{p, d_p}})_{j});\nHere $\\||$ is the concatenation operator and $n_j \\in [P_{d_M}$ is final node representation for node j. $\\beta_p$ and $\\alpha_f$ are learnable weights which assert the relative importance of the $p^{th}$ component manifold embedding and $f^{th}$ filter."}, {"title": "4.1.2. DECODER FOR CURVATURE RECONSTRUCTION", "content": "The decoder aims to reconstruct the Ollivier-Ricci curvature $C_{xy}$ for nodes x and y, by leveraging their latent embeddings $n_x, n_y \\in [P_{d_M}$, learned through the mixed-curvature encoder. We propose using a Gaussian kernel defined on the manifold distance between the node embeddings.\nDefinition 4.1 (Curvature Decoder). Let $n_x, n_y \\in [P_{d_M}$ be the latent embeddings of nodes x and y. The predicted curvature $C_{xy}$ is defined as: $C_{xy} = \\frac{2}{1 + e^{-\\sigma (1 - K(n_x, n_y))}} - 1$, where $K(n_x, n_y)$ is a Gaussian kernel: $K(n_x, n_y) = exp(-\\frac{D_M(n_x, n_y)^2}{\\Upsilon^2})$, and $D_M(n_x, n_y) = \\sqrt{\\sum_{m=1}^M D_{M_p}(n_x, n_y)^2}$ aggregates geodesic distances $D_{M_p}$ over manifold components. Here, $\\Upsilon$ is a fixed kernel width and $\\tau$ is a sensitivity parameter.\nThe decoder minimizes the loss of the Frobenius norm: $L_c = \\|\\hat{C} - C\\|_F$, where $\\hat{C}$ and C denote the predicted and original curvature matrices, respectively. This objective ensures accurate curvature reconstruction while adapting to geodesic distances."}, {"title": "4.2. Curvature-invariant Reconstruction", "content": "This pipeline focuses on reconstructing the adjacency matrix A and the feature matrix X independently of the graph's underlying geometry (curvature). To achieve this, the graph is first deformed under the Ollivier-Ricci flow, followed by curvature-invariant encoding and decoding.\n4.2.1. RICCI FLOW AND CURVATURE REGULARIZATION\nOllivier-Ricci flow is applied to the original graph to regularize edge curvatures by iteratively updating edge weights based on their curvature values. This process, outlined in Algorithm 4.2.1, transforms the graph into a constant-curvature space, thereby neutralizing curvature-induced distortions."}, {"title": "Definition 4.2 (Adjacency Decoder)", "content": "The predicted adjacency value $\\hat{A}_{xy}$ for nodes x, y is modeled as the likelihood of an edge existing between them: $p(A_{xy} = 1 | n_x, n_y) = \\sigma (n_x^T n_y)$, where $n_x, n_y \\in \\mathbb{R}^d$ are Euclidean latent embeddings, and $\\sigma(\\cdot)$ is the sigmoid activation."}, {"title": "Definition 4.3 (Attribute Decoder)", "content": "The reconstructed features for a node x are given by: $\\hat{X}_x = f_{dec-x}(n_x)$, where $f_{dec-x}$ is a multi-layer perceptron (MLP) mapping latent embeddings $n_x \\in \\mathbb{R}^d$ to feature space.\nThe decoders are trained using reconstruction losses: $L_A = \\|A - \\hat{A}\\|$ for adjacency and $L_X = \\|X - \\hat{X}\\|$ for features."}, {"title": "4.3. Objective Function", "content": "The objective function balances curvature reconstruction, structural and attribute reconstruction, and a supervised classification loss to detect both geometric and non-geometric anomalies. The total loss is:\n$L_{total} = \\lambda_{cls} L_{cls} + (1 - \\lambda_{cls}) \\cdot (\\lambda_C L_C + \\lambda_A L_A + \\lambda_X L_X)$\nwhere $L_C, L_A, L_X$ denote reconstruction losses for the curvature, adjacency, and feature matrices, while $L_{cls}$ represents the cross-entropy classification loss. The anomaly score for each node is computed as a weighted sum of reconstruction errors: $Score_i = \\lambda_C \\|\\hat{c_i} - c_i\\|^2 + \\lambda_A \\|\\hat{a_i} - a_i\\|^2 + \\lambda_X \\|\\hat{x_i} - x_i\\|^2$, where $c_i, a_i, x_i$ are the reconstructed curvature, adjacency, and feature values, respectively. Learnable tradeoff parameters $\\lambda_X, \\lambda_A, \\lambda_C, \\lambda_{cls}$ dynamically adjust loss contributions. The final anomaly detection integrates this score with classification logits, ranking anomalies based on both geometric and non-geometric deviations. An ablation study evaluating the impact of $L_{cls}$ and $L_C$ is presented in Section 5.5. In the following section, we lay out the empirical results to validate the efficacy of CurvGAD."}, {"title": "5. Experimentation", "content": "5.1. Datasets\nWe evaluate CurvGAD on 10 datasets, each containing organic node-level anomalies, to assess its effectiveness across homophilic and heterophilic settings. These datasets span multiple domains, including social media, e-commerce, and financial networks. Specifically, (1) Weibo (Zhao et al., 2020; Liu et al., 2022), (2) Reddit (Kumar et al., 2019; Liu et al., 2022), (3) Questions (Platonov et al., 2023), and (4) T-Social (Tang et al., 2022) focus on identifying anomalous user behaviors on social media platforms. In the context of crowdsourcing and e-commerce, (5) Tolokers (Platonov et al., 2023), (6) Amazon (McAuley & Leskovec, 2013; Dou et al., 2020), and (7) YelpChi (Rayana & Akoglu, 2015; Dou et al., 2020) are used to detect fraudulent workers, reviews, and reviewers. Meanwhile, (8) T-Finance (Tang et al., 2022), (9) Elliptic (Weber et al., 2019), and (10) DGraph-Fin (Huang et al., 2022) are employed to identify fraudulent users and illicit activities in financial networks. Refer to Table 6 (Appendix E.1) for dataset statistics."}, {"title": "5.2. Baselines", "content": "For a fair comparison, we evaluate CurvGAD against four types of baselines: (a) Conventional, including traditional models such as GCN (Kipf & Welling, 2017), GAT (Veli\u010dkovi\u0107 et al., 2017), and SAGE (Hamilton et al., 2017). (b) Riemannian, comprising (i) Constant-curvature GNNs like HGCN (Chami et al., 2019) and HGAT (Zhang et al., 2021), and (ii) Mixed-curvature GNNs such as KGCN (Bachmann et al., 2020) and QGCN (Xiong et al., 2022). (c) Spectral, including ChebyNet (Defferrard et al., 2016) and BernNet (He et al., 2021). (d) Specialized GAD models, including (i) Reconstruction-based approaches like DOM-INANT (Ding et al., 2019) and AnomalyDAE (Fan et al., 2020), and (ii) GNNs like DCI (Wang et al., 2021), PC-GNN (Liu et al., 2021), BWGNN (Tang et al., 2022), GADNR (Roy et al., 2024), and ADAGAD (He et al., 2024)."}, {"title": "5.3. Experimental Results", "content": "We conduct experiments in a transductive, supervised setting for node-level graph anomaly detection, following standard data splits where available. If splits are not provided, we adopt the strategy from (Tang et al., 2022), partitioning nodes into 40%/20%/40% for training, validation, and testing, as detailed in Table 6. To ensure robustness, we perform ten random splits per dataset and report the average performance. As per established anomaly detection benchmarks (Han et al., 2022; Liu et al., 2022), we evaluate models using the Area Under the Receiver Operating Characteristic Curve (AUROC). ORC is computed with $\\delta = 0.5$, distributing equal probability mass between a node and its neighbors. Given the prohibitive cost of exact ORC computation on large graphs, we employ a linear-time approximation via combinatorial bounds (Appendix B.3). The manifold signature is decided heuristically using the ORC distribution of the datasets (Algorithm 2, Appendix F). For optimization, we leverage the K-stereographic product manifold implementation from Geoopt\u00b9 and use Riemannian Adam for gradient-based learning across product manifolds. All experiments are conducted on A6000 GPUs (48GB), using a total manifold dimension of $d_p = 48$, a learning rate of 0.01, and a filterbank comprising F = 8 filters. Appendix E.2 enlists the hyperparameter configurations tried and we analyse the time complexity of CurvGAD in Appendix C."}, {"title": "5.4. Baseline Analysis", "content": "CurvGAD consistently outperforms all baselines across all datasets, achieving the highest AUROC scores with an improvement of upto 6.5% over the second-best model (Table 1). Significant gains are observed on T-Social (+4.18%), Elliptic (+4.79%) and Tolokers (+4.61%), all heterophilic networks, highlighting CurvGAD's ability to detect complex anomalies beyond homophily-based assumptions (addresses limitation L3). Among specialized GAD models, ADAGAD and GADNR perform well on homophilic datasets like Reddit and Weibo, ranking as second or third best in many cases. However, their performance deteriorates on heterophilic datasets, where curvature-based"}, {"title": "5.5. Ablation Study", "content": "To assess the contribution of individual components in CurvGAD, we evaluate multiple ablations (Tables 2, 3).\n(a) Euclidean-only variant (CurvGADeucl). This model learns purely in Euclidean space, discarding mixed-curvature embeddings. The performance drop (~ 5%) highlights the necessity of mixed-curvature modeling (addresses L1). Notably, degradation is more pronounced on curvature-sensitive datasets like Reddit (-7.08%) and T-Social (-5.69%) but is minimal on T-Finance (-1.7%), where Ollivier-Ricci curvatures are predominantly zero.\n(b) Without Ricci Flow (CurvGAD flow). Removing Ricci flow prevents curvature-invariant reconstruction, degrading performance on curvature-sensitive graphs (-10.77% on Tolokers, -3.94% on Reddit). This confirms Ricci flow's role in stabilizing curvature variations and enhancing structural and attribute reconstruction. Observe that in Weibo, the drop is minimal (-0.27%), which can be attributed to the low curvature variance in the ORC distribution of Weibo.\n(c) Manifold signature analysis. The optimal curvature composition varies across datasets (Table 3). Reddit and Questions favor hyperbolic embeddings, while Amazon and YelpChi perform better with increased Euclidean capacity. This validates the necessity of mixed-curvature manifolds for adapting to diverse graph structures. To quantify the contribution of individual geometry types, we further conduct ablations by removing each component in turn, e.g. H24 \u00d7 S24 (E removed) and S24 \u00d7 E24 (H removed).\n(d) Curvature-invariant pipeline only (CurvGADinvur). This variant omits curvature reconstruction, relying solely on structural and attribute reconstruction (removes $L_C$). While it still surpasses most baselines, performance declines significantly (-8.39% on Reddit, -6.86% on Amazon), confirming that structure and attributes alone are insufficient for robust anomaly detection (addresses L2).\n(e) Curvature-equivariant pipeline only (CurvGADequi). This model detects only geometric anomalies, omitting curvature-invariant reconstruction (removes $L_A, L_X$). It outperforms CurvGADinvur (+6.72% on Reddit, +4.89% on Amazon), highlighting how effective the curvature-equivariant pipeline is in itself. Both pipelines compliment each other, and achieve the best results together.\n(f) Unsupervised variant (CurvGADunsp). Removing the supervised classification loss $L_{cls}$ results in a minor performance drop ( 1.5% avg.), demonstrating that CurvGAD remains effective even in an unsupervised setting, making it adaptable to real-world scenarios with limited labels."}, {"title": "6. Conclusion", "content": "In this paper, we propose CurvGAD, the first approach to introduce the notion of curvature-based geometric anomalies and to model them in graph anomaly detection through a mixed-curvature perspective. Our dual-pipeline architecture integrates curvature-equivariant and curvature-invariant reconstruction, capturing diverse anomalies across homophilic and heterophilic networks. CurvGAD disentangles geometric, structural, and attribute-based anomalies, making it interpretable. Extensive experiments across 10 benchmarking datasets demonstrate the superiority of CurvGAD over existing baselines. These results establish curvature as a fundamental tool for GAD and pave the way for future advancements in geometry-aware graph learning while offering new avenues for exploring geometric anomalies in topologically complex networks."}, {"title": "Impact Statement", "content": "The goal of this work is to offer a general-purpose anomaly detection framework that has broad applicability and theoretical foundations. Although we do not anticipate any immediate ethical issues specific to our study, careful implementation in practical contexts is required for security and fairness in automated anomaly detection systems.\nThe potential uses of CurvGAD include fraud detection, cybersecurity, financial risk analysis, and misinformation detection, even though it was created as a foundational research contribution to machine learning and graph representation learning. Due to the high stakes involved in decision-making, these fields raise ethical questions about prejudice, justice, and openness. Biases in input data may be exacerbated in anomaly identification results because our approach depends on graph topology and node-level features. This is especially true in financial or social network investigations where marginalized groups may be disproportionately identified as outliers. Therefore, it is essential to make sure that datasets are selected using preprocessing that considers fairness and that model outputs are carefully assessed.\nAdditionally, CurvGAD makes GAD techniques more scalable, which could make them an effective tool for extensive monitoring systems. Although advantageous for anti-fraud and cybersecurity efforts, malicious actors might try to alter graph architecture in order to avoid detection. Robustness against adversarial perturbations in curvature-aware models should be investigated in future studies. Our method improves interpretability by separating geometric and non-geometric anomalies, which is important in high-stakes applications where decision-making requires model explanations. In scientific domains including network science, biology, neurology, and transportation networks, where curvature-driven representations may aid in identifying anomalous patterns in actual systems, the knowledge gained via CurvGAD might be advantageous"}]}