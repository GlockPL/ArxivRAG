{"title": "Contrastive Deep Learning Reveals Age Biomarkers\nin Histopathological Skin Biopsies", "authors": ["Kaustubh Chakradeo", "Pernille Nielsen", "Lise Mette Rahbek Gjerdrum", "Gry Sahl Hansen", "David A Duch\u00eane", "Laust H Mortensen", "Majken K Jensen", "Samir Bhatt"], "abstract": "As global life expectancy rises, so does the burden of chronic diseases, yet individuals exhibit considerable variability in the\nrate at which they age. Identifying biomarkers that distinguish fast from slow ageing is crucial for understanding the biology\nof ageing, enabling early disease detection, and improving prevention strategies. Using contrastive deep learning, we show\nthat skin biopsy images alone are sufficient to determine an individual's age. We then use visual features in histopathology\nslides of skin biopsies to construct a novel biomarker of ageing. By linking with comprehensive health registers in Denmark,\nwe demonstrate that visual features in histopathology slides of skin biopsies predict mortality and the prevalence of chronic\nage-related diseases. Our work highlights how routinely collected health data can provide additional value when used together\nwith deep learning, by creating a new biomarker for ageing which can be actively used to determine mortality over time.", "sections": [{"title": "Introduction", "content": "Background- Ageing\nThe global population is ageing fast due to fast declining mortality rates. As demographic pyramids shift to an older population,\nit is essential to understand what drives the enormous heterogeneity in how people age. From a public health view this is the\nmatter of why, for a particular age, some individuals experience more senescence and frailty, while others remain comparatively\nhealthy\u00b9. The observation that physiological differences underpin this variation has led to a distinction between chronological\nand the so-called biological age.\nVarious forms of epigenetic alterations like loss of histones (proteins around which DNA can wind for gene regulation),\ntelomere length, as well as histone modification, and DNA methylation (methyl groups attached to DNA molecules) have been\nsuggested to be biomarkers of cellular ageing. Existing methods for defining biological age typically rely on linear regression\nfunctions of hallmarks of ageing including sets of biomarkers like e.g., telomere length, DNA methylation, mitochondrial\ndysfunction or various \"-omics\")2\u20135; however, no clock captures all the hallmarks of ageing. It is also difficult to decide which\ncombination of biomarkers should be used to capture ageing clocks6\u201312. The biological age of an individual is, therefore, the\npredicted age estimated by a given model, and the metric that becomes interesting for the individual is reduced to the difference\nbetween biological and chronological age as a way to detect accelerated ageing, in other words, biologically ageing faster than\nthe chronological age peers.\nThe primary question that remains is whether the developed biological age is a better measure of residual age (or \"ageing\nstatus\") as compared to chronological age. Measures of biological age are therefore often evaluated in terms of their ability to\npredict adverse outcomes such as death or incidence of age-related disease4,13. In other words, is biological ageing a better\nmeasure for risk of mortality for a given age, over chronological age? One of the ways an estimate of biological age can be\ndetermined is by using healthy skin samples: trained human observation alone reveals that the skin cell profile changes with\nage14,15. The relative numbers of endothelial cells, endotheliocytes, keratinocytes, mastocytes and fibroblasts in the biopsy"}, {"title": "How can biological age be utilised for prediction tasks?", "content": "Previous studies developing a metric of biological age tend to test ageing against outcome as a criterion validity. However, as\nmentioned earlier, ageing itself can be a critical factor in mechanisms of disease. It is therefore critical to establish whether\nbiological age has a connection with the risk of disease and mortality. Here, we focus on the discovery of biomarkers that\nadd predictive value in terms of age-related health outcomes. Importantly, the predictive power of an estimate of biological\nage is expected to perform better than the predictive power of chronological age and sex alone. Some studies have developed\ndefinitions of biological age which correlate almost perfectly with chronological age, therefore explaining no variance in\noutcome that cannot be explained by age alone. Other studies have developed definitions of biological ages that are based on\ndata that pose a risk to the patient (e.g. X-Ray imaging data20) or data which are financially costly and do not lead to predictive\npower that outperforms readily obtainable physiological measures.\nWhile many definitions of biological age focus on specific biological or physiological processes (telomere length, DNA\nmethylation, -omics, physiological measures, etc.) composite definitions that combine information from several types of\nprocesses into a single index tend to perform better5,21. The propensity for multidimensional markers to perform better\nsupports the notion that ageing is a complex process that manifests at multiple biological scales across all tissues. Here we\nexplore age-related changes in the skin as a low-risk, low-cost, multifaceted manifestation of the ageing process, encompassing\nalterations in tissue structure, cellular composition, and molecular dynamics."}, {"title": "Deep Learning approaches towards pathobiology, ageing and mortality", "content": "By leveraging contrastive deep learning methods to extract visual features in histopathology slides of skin biopsies, we develop\na biomarker of ageing that includes features spanning several physical scales, ranging from nanometres to centimetres. As the\nbody's largest organ, the skin undergoes noticeable changes with advancing age. Biopsies taken for disease detection do not\npose a large risk for patients, thus making them a valuable target for investigating the underlying mechanisms of ageing even as\nthey are used for other purposes.\nThis study utilises the vast collection of tissue biopsies, initially collected for clinical purposes unrelated to research, which\nare stored in the archives at Danish Pathology departments. These biopsies present a unique opportunity, given their diversity in\npatient age, sex and the broad spectrum of diseases they were originally intended to investigate. Additionally, the biopsies can\nbe linked to comprehensive Danish health registers, providing a comprehensive follow-up with detailed disease trajectories for\nthe patients from whom biopsies were retrieved. This adds value to the biopsies as a resource for discovering biomarkers of\nageing and age-related diseases.\nThe use of deep learning (DL), particularly convolutional neural networks (CNNs) in histopathology has led to several\nforms of automated analyses in the field. DL models can identify and classify structures on a tissue level as well as cellular and\nsub-cellular levels. Because of improvements in digital pathology and acquisition of whole-slide images (WSI), as well as\nthe advances in hardware and computing costs, DL models have allowed for leaps in biomedical data analysis22. Along with\nthis, the biggest advances have been possible because of increasingly large open-source datasets, like CAMELYON23, which\nwas a large collection of fully annotated WSIs of sentinel lymph nodes of patients with breast cancer. Two other open source\ncancer-related influential datasets with multiple modalities include the Cancer Genome Atlas (TCGA) and Clinical Proteomic\nTumour Analysis Consortium (CPTAC). With these tools at hand, DL approaches are now being used for tumour detection and\nclassification24\u201337, image segmentation38\u201348, cell detection, counting and mitosis49\u201354, and tumour grading55\u201357.\nHowever, these approaches have several challenges related with the nature of WSIs58. First, the size of WSIs is generally\nconsiderable (up to 1000002 pixels, which is far greater than the typically 2242 pixels that CNNs can handle). This challenge\nhas been overcome by either creating smaller patches and performing analysis on these smaller patches consisting of cellular\nand sub-cellular features59,60. Another approach has been to use streaming CNNs for end-to-end learning instead of using\npatches 61.\nThe biggest challenge, hand-in-hand with size, is the lack of manually annotated training samples. There have been only a\nhandful of open-source datasets with manual annotations54,62,63, making it difficult to train large CNN models. A promising\nsolution comes with the advent of self-supervised learning models, which learn implicit label representations from unstructured\ndata. Even without extensive manual annotations, there is mounting evidence that DL methods are able to predict diseases\nusing WSIs64\u201366. DL methods are also being leveraged to extract ageing-related biomarkers from various medical imaging\nmodalities like X-rays and Magnetic Resonance Imaging, as well as surveys and bio-banks67\u201372."}, {"title": "Results", "content": "We developed a novel deep learning-based system to extract sex-specific age-related feature information from digitised skin\nbiopsies. The added predictive value from the visual features is assessed in several analyses, coupling the biopsy data to\ninformation of disease and survival. We find that the visual features add predictive value above and beyond age and sex alone\n(Fig. 1D). We also investigated which visual features constitute the highest predictive power towards age, and discuss how\nthese features relate to known morphological features in the skin tissue (Fig. 3).\nOur system consists of 4 components (Fig. 1): (a) Data Collection, b) Preprocessing and self-supervised pretraining to\nextract visual information related to age from digitised skin biopsies, c) Prediction of age and d) Downstream tasks using\npredicted age to check for the usefulness of the visual features extracted from the skin biopsies. In a) we collected digitised\nbiopsy samples of 919 males and 868 females from the Danish National Register of Pathology (DNRP). We followed this cohort\nover a span of 20 years since the first biopsy, during which death eventuated for 230 males and 118 females. We preprocessed\nthe biopsy in b) by first creating patches from whole slide biopsy images in different scales depending on the resolution of\nthe digitised biopsies, with one scale having a maximum patch size of 1024x1024 pixels and the second scale having a maximum\npatch size of 4096x4096 pixels. Only the patches containing the biopsy part in the foreground were selected. We downscaled all\npatchs to 224x224 pixels for fit as input separately for the VGG-based self-supervised contrastive deep learning model (CDL).\nThe different scales allow for the model to focus on either tissue information or cellular and sub-cellular information. We ran\nthis CDL twice, once for each scale, and extracted features from the patches in the form of vector embeddings. Using these\nfeature vectors, we used an xGBoost model73 to predict the age of each participant in c). We first clustered these feature vectors\nso that patches with similar features belonged to the same cluster. Taking this predicted age, in d) we performed downstream\ntasks to verify the validity of using predicted age to classify prevalent diseases at the time of biopsy using logistic regression,\nthen using these predicted ages and diseases for predicting survival probabilities using these prevalent diseases over 20 years,\nusing a cox regression model74. To find out the utility of using predicted age versus using the actual ages of the participants,\nwe compared the results of d) with using actual age with the same model settings. The biological plausibility of the link of\nextracted features with age was then checked by visualising the age-related feature information in Fig. 3.\nOur analyses show that features extracted from skin biopsies can give a good indication of the age of participants across all\nages, for both males and females. This can be verified using 3 different criteria; first, visualising the best-performing features in\nterms of their mean absolute error (MAE) between the actual ages and the predicted ages (Fig. 3). We find that these features\nare indicative of biopsy characteristics such as atrophy of the epidermis and damaged collagen. The second criterion we use to\nverify the predicted ages is by taking them as a proxy for actual age in age-related prediction tasks - a common estimation\nquestion in epidemiology. Using MAE as a metric, we find that these predicted ages perform better than using the actual ages\nfor predicting 7 age-related indecent diseases, that is, diseases which participants had at the time of biopsy. Finally, we verify\nthe predictive power of biopsy samples for age by using it instead of actual age to predict the survival probabilities of the\nparticipants. Once again, we find that the extracted features perform either on par or outperform using actual ages in predictions\nof survival. Using these three criteria, we can confirm that features extracted from skin biopsies using deep learning models are\nexplainable, can predict age of participants well, and often perform better for further predictive tasks in comparison to actual\nages."}, {"title": "Features from contrastive deep learning correlate with age", "content": "WSIs from biopsies were first converted into smaller patches, as the large size can complicate their use for contrastive deep\nlearning model (CDL) (Fig. 1 b). In order to extract visual features representing different physical scales, we constructed\npatches of two different scales, 1024x1024 pixels for scale 1 and scale 2 with a maximum size of 4096x4096 pixels. Thus, based\non the dimensions of the patches, keeping the resolution constant, patches in scale 1 contained more tissue-level information,\nwhile scale 2 contained more cellular and sub-cellular information. Using our CDL model we then extracted visual features\nfrom each patch, resulting in many sets of visual features for each biopsy (each WSI). We used this CDL model twice, once for\neach scale, and thus had two sets of visual features, with 512 features for scale 1 and 128 features for scale 2. For each scale,\nwe reduced the amount of patches contributing per patch, by clustering similar patches together using a K-means clustering\nalgorithm and aggregating the features belonging to similar clusters together. Thus, we converted the varying amount of patches\nfor each biopsy to 3 clusters, with their feature sets, over 2 scales. We then also included a third scale for analysis by combining\nscales 1 and 2, testing if a combination of tissue information and cellular and sub-cellular information can work better than\neither individually.\nThe extracted visual features were then used to predict age at the time of biopsy, drawing out any age-related information\nfrom the biopsies using an XGBoost model with 1000 bootstraps. We found that the predicted ages, which were based solely on\nfeatures extracted from the skin biopsies, correlated with the actual ages of the participants. Ages were predicted using different\nscales, as well as combined scales such as scale 3. For both men and for women, we find that scale 1 (containing cellular and\nsub-cellular information) performs better for males and females for all ages, even though there are no significant differences in\nthe MAE for individual age categories. A likely reason is the lower frequency count for individuals in sub-categories. Table 1\nshows individual MAEs for males and females across age categories, as well as for all categories combined.\nAs seen in the table, for overall ages, the smallest scale 1 outperforms the bigger scale 2 for both males and females.\nWhen using the combined scales, the difference is not as great, but still, scale 1 has the most predictive power. That is, the\npredictions based on features from the CDL model having more patches of smaller dimensions, rather than fewer patches\nwith larger dimensions produced a higher accuracy. Seemingly, sub-cellular features carry stronger age-related information\nthan tissue-level information. When the model has only tissue-level information with the same resolution, it can miss the\nsmaller sub-cellular structures, which contain important biomarkers to ageing like epidermis atrophy, collagen deterioration,"}, {"title": "Validating the quality of age-related signals extracted using contrastive deep learning model", "content": "Above, we have seen that age-related signals can be extracted from skin biopsies using CDL. We also saw that the CDL could\nidentify important age-related factors extracted from the biopsies. Now, we validate the quality of this signal. How useful is the\nage information extracted by the CDL? To verify that, we perform 2 classic epidemiological tasks- prediction of prevalent\ndiseases and survival analysis. This is done by predicting the prevalence of several age-related diseases using only the predicted\nage and sex of the patients. The information extracted from the biopsies enters the prediction model through the predicted age\nvalues. We then compare the results to an equivalent model where actual age at the time of biopsy is used instead of predicted\nage - representing the case where no information is gained from the biopsies. This experiment was done to see if the signal of\nage-related obtained from extracting features from biopsies had any significant gains over using the traditional actual ages.\nIn table 2, we compare the classification accuracy for 7 age-related diseases. We applied a logistic regression model to\npredict the existence of age related diseases, at the time of biopsy as seen in the Danish Registries. We compared using actual\nages to predict prevalent disease, vs using the predicted ages (Fig. 1 c). We also used both the predicted age and actual age to"}, {"title": "Discussion", "content": "Using Contrastive Deep Learning we have shown that it is possible to extract age-related information from skin tissues from a\npopulation spanning a wide age range ([7,94]). After condensing the data from tissue into a predicted age, we have shown\nthat the extracted signal correlates strongly with chronological age, and that predicted age is additionally a high-performance\npredictor of prevalent age-related disease and mortality hazard. These age-signals also have high performance in classification\ntasks and survival analysis, comparable to using classical chronological age. Our results provide evidence that easily accessible\ndata from skin tissue have substantial value for capturing the dynamics of human ageing and predicting mortality.\nBeyond predicting chronological age alone, detailed registry data allowed us to examine whether the deviation between\npredicted and chronological ages comprised additional predictive power about disease risk. Indeed, extracted features improved\nthe classification of age-related diseases by up to 4%. These findings provide evidence that not only is it possible to extract\nage-related information from the skin biopsies, but this approach provides additional predictive power related to the dynamics\nof human ageing and mortality risk. Our methods, together with similar alternatives, are likely to cast new light on the concept\nof biological age and on its impact as a tool in clinical and public health settings. However, it has to be said, that biopsies are\nnot performed on normal and healthy skin, but only on participants with some history of skin-related illnesses, usually nevi. All"}, {"title": "Methods", "content": "Study Overview\nSkin biopsies from 1787 individuals (stratified by age and sex, see Table 1) were retrieved from the archives at the Department\nof Pathology, Zealand University Hospital and digital images were produced for analysis using computer vision (CV)89. There\nwere 919 males and 868 females with skin biopsies. The mean observed age (standard deviation) at the time of biopsy of the\npopulation was 48.21 (18.14), 47.25 (18.59) for males and 49.19 (17.63) for females. The period for biopsies was between\n2000 and 2015. Since the date of biopsy, there were 227 deaths, with 93 males and 130 females dying between their biopsy and\n2020, which was chosen as the end of follow-up period in our study."}, {"title": "Selection and Retrieval of Skin Biopsies", "content": "The archives at the Danish Pathology departments store histological tissue samples indefinitely. The histological samples\nare stored in Formalin Fixed Paraffin Embedded blocks (FFPE-blocks). The FFPE-blocks can be retrieved (e.g., for research\npurposes) and new thin slices may be cut from the biopsies in order to produce glass slides that can be inspected in a microscope\nor scanned to save digitised images of the tissue structures.\nSince normal tissues are not biopsied, we selected samples from the Danish National Register of Pathology (DNRP) with a\nnaevus, and for which there was excess skin adjacent to the naevus, such that we were able to sample normal skin with no\nindication of disease.\nFrom DNRP we filtered all skin materials with topology codes for skin from non-sun-exposed areas (SNOMED codes\nT01000, T02403, T02424, T02430(-A,-B), T02470(-A,-B), T02471(-C,-D), T02480 or T02487), with a morphology code for\nNaevus (SNOMED codes M87200, M87230, M87400, M87500, M87600, M87611, M87700), with a suiting procedure code\n(SNOMED codes P11000, P30610, P30611, P30620) and which were requisitioned at the Department of Pathology, Roskilde\nUniversity within the time period 2000-2015. We excluded materials with cancer (SNOMED codes M8xxxx and M9xxxx with\nlast digit not 0) and materials which had other reasons for being not suitable (SNOMED codes M01111, M09000, M09010,\nM09011, M09013, M09014, M09020, M09025, M09039, M09070, M09100, M09145, M09150, M09155). After this initial\nfiltering, we selected the materials which had the highest chance of having excess skin adjacent to the naevus, by counting\nthe number of FFPE blocks and number of materials per requisition. We then stratified the samples by sex and (categorical)\nage and selected a random sample of 100 patients within each stratum, except for the strata with ages >50 years, in which we\nselected all patients. The stored FFPE blocks from these biopsies were retrieved from the hospital archives and we produced\ndigitised images of the skin tissue structures. Very few biopsies were lost due to technical difficulties and in total we retrieved\nbiopsies from 1787 individual patients."}, {"title": "Image Statistics and preprocessing", "content": "A single biopsy is evaluated using a technique called super-resolution microscopy. Digital pathology is the process of evaluating\ndigitised histopathology slides on a computer, rather than on bright-field and immunofluorescent light microscopy (LM)90.\nIn this work, we evaluate these skin biopsies using the help of deep learning techniques. A digitised biopsy, called a whole\nslide image (WSI) can be 50 Mb to over 6 GB in storage size91, or about 800002 pixels. The digitised biopsy slides obtained\nfrom Statistics Denmark were scanned using the Hamamatsu Scanner (model C13220) and stored in a NanoZoomer Digital\nPathology Image (ndpi) format. They were acquired from the Zealand University Hospital, Roskilde. The WSI scans obtained\nwere of two resolutions, 2140 and 4280 pixels per inch (ppi).\nFor deep learning (DL) models for vision, the average input size is 224x224 pixels, which is far smaller than the average\nWSI size. We therefore divided our WSI scans into smaller patches with some overlap of 50 pixels between neighbouring\npatches in order to extract features from each patch instead of the entire image at once. The overlap was used to prevent missing\ninformation around patch edges. For the different resolutions, we used different patch sizes. For the WSIs with resolution 2140,\neach patch had dimensions half that of WSIs with resolution 4280. We created two sets of patches from these WSI slides, first,\nwith a patch size of 512x512 and 1024x1024 pixels, and the second set of patch sizes 2048x2048 and 4096x4096 pixels. The\nlarger patches had a decreased resolution in order to extract features from a larger physical scale. Overall, we extracted features\nfrom patches of sizes 5122 pixels (2.3mm\u00b2) to 40962 pixels (9.57mm\u00b2). We used equation 1 to determine these sizes."}, {"title": "Contrastive Deep Learning (CDL) model for extracting visual features from digitised WSIs of skin biopsies", "content": "Contrastive Deep Learning (CDL) is a type of self-supervised deep learning technique for vision tasks. We train a model to\nalign the representations of augmented views of the same image (positive pairs) while diverging the representations of different\nimages (negative pairs). In other words, the model is trained to put similar images together, so as to learn a better representation\nof the images without labels. The CDL contains two models, an encoder network and a predictor network. We create only\npositive pairs. These pairs are generated using augmentations. A colour augmentation can be obtained by changing the values\nof contrast, intensity, saturation and hue of the images, while geometric augmentations can be achieved by horizontal and\nvertical flipping and rotating the images.\nLet us consider a patch from the WSI, called V. We created two contrasting augmentations to generate two augmented\nviews of V, called v1 and v2. Compared to established contrastive methods which perform random augmentations, contrastive\naugmentations make learning harder for the model during training, thus making the model more robust. Thus, for view v1,\ncolour augmentations were made using a positive delta, and geometric augmentations were made by only flipping vertically and\nrotating clockwise. For view v2, colour augmentations were made using a negative delta, and geometric augmentations were\nmade by only flipping horizontally and rotating anti-clockwise. All augmentations were applied with a probability of 0.75. All\naugmentations were generated dynamically without generating them in advance. Each patch V is augmented right before being\nfed into the neural network for each epoch for training. A full list of augmentations can be seen in table 4."}, {"title": "Loss function", "content": "The CDL method is a self-supervised pretraining step before prediction models. It works by maximising the similarity between\nthe different augmented views (v1 and v2) for the same biopsy patch. This similarity measure is trained using a cosine similarity\nloss function, explained in"}, {"title": "Encoder Architecture", "content": "The two augmented views, v1 and v2 are passed through a VGG-based CNN architecture. The input to this model was a\n224 x 224 x 3 patch of the biopsy. This was propagated through ReLU-activated convolutional blocks of 2, 2, and 3 layers.\nThe resulting 512 dimensional output was compressed using global average pooling before passing through two l2-regularised\nand ReLU-activated fully connected layers. Using the loss function given by 2, both v1 and v2 were passed to the encoder\nmodel. The weights during training were updated for v1, while a stop gradient (stop-grad) method was applied to v2, that is,\nthe weights from v2 are not updated. In other words, the stop-gradient method acted to create implicit negative pairs."}, {"title": "Predictor Architecture", "content": "A 3-layer multilayer perceptron (MLP) was attached to the encoder model. MLP was used to transform the output of v1 to that\nof v2. Since the weights of v2 were not updated, the MLP received no gradients from the output of v2 but receives the gradients\nfrom the output of v1. Thus, the predictor transformed the output of v1 and matched it to v2. The predictor architecture was a\nsimple architecture consisting of two fully connected layers connected with a batch normalised hidden layer. The output of this\npredictor architecture was a vector embedding representation of each image V.\nThe model encoder- predictor architecture is described in figure 5. We call this combined architecture the contrastive deep\nlearning model (CDL)."}, {"title": "Training", "content": "This CDL model was trained for 100 epochs, and the biopsy image patches were transformed into vector embeddings from the\ndeep-learning model. This was represented by 1024 different features for each image; the features combined together described\nthe image. After the pretraining contrastive step, each biopsy image patch was described by 1024 features. Pretraining was\ndone on an NVIDIA A6000 GPU."}, {"title": "Linking CDL output to Demographic Variables from Danish Registries", "content": "Each skin biopsy, or WSI has been linked to a participant with a unique personal identification number in the Danish Registries.\nUsing this ID, we linked each biopsy to the date of biopsy, observed age at the time of biopsy, and sex of the participant.\nAdditionally, we also linked death (whether died, or alive, as of 31 December 2020), date of death for participants who had\ndied, and the time from biopsy to end of follow-up (whether death or reaching the mentioned date alive). We also linked\nif the participant at the time of biopsy had common age-related diseases like heart disease, Chronic Obstructive Pulmonary\nDisease (COPD), cancer (excluding skin cancer), osteoporosis, osteoarthritis, back pain, joint disease and hypertension. The\nInternational Classification of Diseases, Tenth Revision (ICD-10) codes selected are given by table 5.\nSome skin diseases like atopic dermatitis, psoriasis, acne, and rosacea were also considered, to evaluate if the model was\nalso able to extract information related to these diseases. The ICD-10 codes for these diseases are also present in table 5.\nHowever, because of the low power of these disease occurrences in the dataset (< 100 in total for all skin diseases combined),\nthese codes were not ultimately used in predictive models analysis. However, a bigger study with a well-represented sample of\nthese diseases could also involve these diseases along with age biomarkers as a prediction metric.\nNow, this information was linked to the visual feature set extracted from each patch using the CDL. For each WSI, a\nKMeans clustering algorithm was used to cluster patch-data into similar groups. Using the elbow method, 3 clusters were\ncreated and each patch from a WSI belonging to the same cluster was aggregated. After this, the biopsy images were ready for\nthe prediction tasks."}, {"title": "Predicting Age from extracted information from biopsies using CDL", "content": "The first predictive task was the prediction of age from the learned image features. An XGBoost model was created with 1000\nbootstraps, with the pretrained image feature set as predictive variables, with the observed age at the biopsy of the individual as\nan outcome variable, while adjusting for sex. Predicted age was generated for each category of observed ages for males and\nfemales. These predictions were done on three scales, CDL model trained on a scale with a maximum size of 1024x1024pixels\n(scale 1), CDL model trained on a scale with a maximum size of 4096x4096pixels (scale 2), as well as combining features\nfrom scale 1 and scale 2, as explained in table 1 for males as well as females. The accuracy of this model was described using\nthe mean absolute error between observed ages and predicted ages."}, {"title": "Visualising age related features", "content": "For this task, the previous clusters were not used. Instead, for each patch in the biopsy, predictions were made using the model\nin the previous task, using only scale 1. Then, each biopsy patch was ranked according to the difference in the mean absolute\nerror between the actual age and the predicted age on that patch. The best patches contributing towards the age predictions\n(least MAE value) were plotted in figure 3."}, {"title": "Using predicted age to classify age-related diseases", "content": "To evaluate whether the age-related information extracted from biopsy samples using CDL was viable, a second prediction task\nwas performed. Each age-related disease linked from the Danish Registries was now set as the outcome variable, while there\nwere two instances of predictor variables- the observed age at the time of biopsy from the Danish Registries, and the predicted\nage from the previous task. These two instances of predictor variables were compared against each other for each age related\ndisease using a classification task, using a logistic regression model. This model was evaluated using classification accuracy."}, {"title": "Prediction models for survival analysis", "content": "The third task was survival analysis over the next 20 years from the date of biopsy. The long follow-up period was chosen\nbecause of the variation in the dates for biopsy (with the earliest being 2000, and the latest being 2015). Individuals were\nfollowed until their date of death or end of follow-up, stratified by sex. Again, we did a comparison by using a) the observed\nage as a predictor variable, and b) the predicted age from CDL as a predictor variable. The existence of comorbidities that each\nparticipant had (age-related diseases classified in the previous task) were also added as predictor variables. A Cox regression\nwith 12 norm regulariser model was used to compute the survival probabilities and hazard ratios for each disease. This model\nwas also run as a comparison, first using the actual or observed age and incident disease at the time of biopsy, then using the\npredicted age as well as the predicted diseases from the previous tasks."}, {"title": "Supplementary", "content": "Data availability\nThe digitised skin biopsies and registry data from the National Patient Register and other administrative registers used for\nthis manuscript are person sensitive and are not publicly available. The data is stored at Statistics Denmark. Researchers\naffiliated with a Danish research institution can gain access to all non-image data via Statistics Denmark's Research Services.\nAccess to the digitized skin biopsies requires an ethical approval from the Danish Research Ethics Committees. The code for\nthe preprocessing step of contrastive deep learning, as well as the downstream tasks, has been made available at this Github\nrepository. For further information, contact the authors."}]}