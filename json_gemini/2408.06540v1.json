{"title": "Dynamic Exclusion of Low-Fidelity Data in Bayesian Optimization for Autonomous Beamline Alignment", "authors": ["Megha Ragini Narayanan", "Thomas W. Morris"], "abstract": "Aligning beamlines at synchrotron light sources is a high-dimensional, expensive- to-sample optimization problem, as beams are focused using a series of dynamic optical components. Bayesian optimization is an efficient machine learning ap- proach to finding global optima of beam quality, but the model can easily be impaired by faulty data points caused by the beam going off the edge of the sen- sor or by background noise. This study, conducted at the National Synchrotron Light Source II (NSLS-II) facility at Brookhaven National Laboratory (BNL), is an investigation of methods to identify untrustworthy readings of beam quality and discourage the optimization model from seeking out points likely to yield low-fidelity beams. The approaches explored include dynamic pruning using loss analysis of size and position models and a lengthscale-based genetic algorithm to determine which points to include in the model for optimal fit. Each method successfully classified high and low fidelity points. This research advances BNL's mission to tackle our nation's energy challenges by providing scientists at all beam- lines with access to higher quality beams, and faster convergence to these optima for their experiments.", "sections": [{"title": "Introduction", "content": ""}, {"title": "Background", "content": "The invaluable work performed by scientists at the National Synchrotron Light Source II (NSLS-II) is made possible by the high-quality beams produced by each of their beamlines. These beamlines are focused using a series of precision optical components, including crys- tals, mirrors, and aperture shutters. Each component is dynamic, with multiple degrees of freedom, and can be adjusted using precision motors within the beamline to optimize the final beam image. Every time a beamline is used, these components must be precisely re-"}, {"title": "Autonomous Methods", "content": "Currently, we are using Gaussian Processes and Bayesian Optimization to find global op- tima of beam quality, tools which are uniquely suited for expensive-to-sample problems with multiple objectives such as this one.\u00b9 At a high level, we are looking for the input x which leads to the maximum value of the true function f(x), which relates our inputs in terms of component positions to outputs in terms of beam quality. Given that we must sample as few points as possible, we treat this function as a stochastic process and use Bayesian inference to create a posterior distribution, which represents the probability a given function is the true function. This posterior can be visualised as a mean with error bars, and given an input x, our posterior gives us a distribution of what y is likely to be. In each iteration of Bayesian inference, we perform three main steps. First, we create a posterior distribution p(f) based on historical observations (x, y), according to the equation below, where the like- lihood p(y | f, x) is the probability of observing outputs y at inputs x for a given function f, and the prior p(f) is our knowledge of the likelihood of a function f before we have observed any data.\n$$p(f | x,y) = \\frac{p(y | f,x)p(f)}{p(y | x)}$$\nThis is generally done using Gaussian Process (GP) models which optimize hyperparam- eters of a kernel to estimate a covariance matrix and construct a posterior. Second, we use an acquisition function such as Expected Improvement (EI) for single objectives, or Noisy Expected Hypervolume Improvement (NEHVI) for multiple objectives, which estimates how much better than the existing optima a given point will be, to determine the best points to sample. Finally, we sample these points and add them to the historical observations."}, {"title": "Low-Fidelity Data", "content": "The beam is optimized based on images from sensors within the beamline. However, one key complication with autonomous alignment is that these sensors can be finnicky, and beam data noisy. This leads to slower convergence from our model, as it can be impaired by these faulty data points caused e.g. by the beam going off the edge of the sensor or by background noise. Thus, this study is an investigation of methods to identify untrustworthy readings of beam quality and discourage the optimization model from seeking out points likely to yield low-fidelity beams. One elementary solution would be to set a flux cutoff for a point to be considered by our model, which would eliminate images where the beam is no longer on the sensor, or so unfocused that minimal light is hitting the sample. However, different beamlines produce very different measures of flux, and these measurements are often impacted by other conditions which makes this cutoff hard to set. Since our goal is to be able to apply this machine learning framework to alignment at many different beamlines, we present this study to investigate methods to remove these faulty data points in an online fashion, hopefully providing scientists at all beamlines with access to higher quality beams, and faster convergence to these optima for their experiments."}, {"title": "Methods and Results", "content": "All of the models developed were originally tested on a simulated beam function, which takes 8 inputs, which model the motion of two elliptical mirrors and two Secondary Source Apertures (SSAs). Each elliptical mirror has two degrees of freedom: vertical and horizontal. Note that only the horizontal degrees of freedom affect the beam's width, and only the vertical ones affect the height. A fairly high amount of correlated noise is added to the image produced by the simulated beam function."}, {"title": "Image Processing", "content": "The first step in training models to optimize beam quality is processing the beam images into meaningful outputs which represent its quality. Because of the high level of noise in the image, we first perform Singular Value Decomposition on the images, and then reconstruct them from only the most significant singular values, a linear algebra process which reduces images to their most significant features, as seen below."}, {"title": "Exclusion Based on Loss from Size and Position Models", "content": "Due to the nature of the input variables, we expect each input to have a fairly smooth result on the image of the final beam, meaning if we move one motor, the beam should change in size or position in a predictable way. However, for the \u201cjunk\u201d beam images that are mostly noise, these transitions are not smooth, and we expect what the image digestion function finds to be the \u201ccenter\u201d of the beam to jump around. As we can see below, as we change just 1 input, the beam moves predictably until the image becomes purely noise. If we add"}, {"title": "Genetic Algorithms", "content": "Another way we can approach data pruning is as a binary classification problem, trying to find the optimal subset of datapoints to exclude for the best model training. One method of binary classification is to employ a genetic algorithm, which takes inspiration from biology, simulating evolution to reach an optima. In our genetic algorithm, we start with a population of \"individuals\"- each of which has a unique genetic code. The genetic code, in this case, is a binary string the length of our dataset, where the value at each index represents whether or not we are including the data point corresponding to that index. From the initial population, the fitness of each individual is measured by constructing and fitting a model using the"}, {"title": "Conclusions", "content": "This study has been a successful investigation into methods of data exclusion for optimal model training. The dynamic pruning method, utilized with models trained on multiple beam features, proved effective in distinguishing between high and low-quality data. By dynamically excluding points that deviated significantly from model predictions, we achieved faster convergence to optimal beam configurations. This method was particularly adept at handling the complexities of noisy beam image data, ensuring the optimization process remained focused on high-fidelity readings. As a result, a pruning method which applies this algorithm has been developed and integrated into the beamline alignment code at NSLS-II.\nThese research approaches co-align with the overarching goal of Brookhaven National Laboratory to advance scientific research by providing consistently high-quality beams. The integration of these techniques into the beamline alignment code at NSLS-II marks a signifi-"}]}