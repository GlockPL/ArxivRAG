{"title": "Interpretable Dual-Filter Fuzzy Neural Networks for Affective Brain-Computer Interfaces", "authors": ["Xiaowei Jiang", "Yanan Chen", "Nikhil Ranjan Pal", "Yu-Cheng Chang", "Yunkai Yang", "Thomas Do", "Chin-Teng Lin"], "abstract": "Fuzzy logic provides a robust framework for enhancing explainability, particularly in domains requiring the interpretation of complex and ambiguous signals, such as brain-computer interface (BCI) systems. Despite significant advances in deep learning, interpreting human emotions remains a formidable challenge. In this work, we present iFuzzyAffectDuo, a novel computational model that integrates a dual-filter fuzzy neural network architecture for improved detection and interpretation of emotional states from neuroimaging data. The model introduces a new membership function (MF) based on the Laplace distribution, achieving superior accuracy and interpretability compared to traditional approaches. By refining the extraction of neural signals associated with specific emotions, iFuzzyAffectDuo offers a human-understandable framework that unravels the underlying decision-making processes. We validate our approach across three neuroimaging datasets using functional Near-Infrared Spectroscopy (fNIRS) and Electroencephalography (EEG), demonstrating its potential to advance affective computing. These findings open new pathways for understanding the neural basis of emotions and their application in enhancing human-computer interaction.", "sections": [{"title": "I. INTRODUCTION", "content": "EMOTIONS are fundamental to human existence, shaping diverse aspects of life and evolving over millions of years to form the cornerstone of social intelligence and group cohesion [1]\u2013[3]. The ability to recognize and respond to emotional cues across varying contexts\u2014such as how anger can enhance the detection of fear and vice versa-is crucial for survival and social functioning [4]. Advances in neuroimaging have illuminated the neural pathways involved in processing affective stimuli, emphasizing the roles of appetitive and defensive systems rooted in primitive neural circuits [5]. These systems coordinate responses ranging from attentional shifts to physiological changes, highlighting their critical role in survival and the dynamic interplay between physiological states and psychological experiences [6]\u2013[9]. Despite significant progress, mapping specific neural circuits to distinct emotions remains challenging, with only probabilistic patterns identified thus far [8]. Understanding how the human brain processes emotions remains a priority, spurring the development of Affective Computing (AC) systems, particularly Affective Brain-Computer Interfaces (aBCIs). These systems aim to bridge the gap between human emotions and machine understanding by recognizing, interpreting, processing, and simulating emotional states across various modalities [10]\u2013[13].\nThe aBCIs system integrate data from diverse sources, including behavioral cues [14], peripheral nervous system (PNS) signals such as heart rate variability (HRV) [15], and central nervous system signals, including EEG [16] and fNIRS [17]. These approaches offer a comprehensive perspective on the neural and physiological underpinnings of emotions [18]. Recent advancements in aBCIs have demonstrated the effectiveness of Convolutional Neural Networks (ConvNets) for end-to-end emotional decoding from EEG data. For instance, EEGNet has set new benchmarks in performance [19], while the NLSTM model has showcased exceptional sensitivity and specificity across diverse EEG datasets, emphasizing its robustness and generalization capabilities in both controlled and real-world scenarios [20]. Similarly, DBjNet has achieved remarkable decoding accuracy in distinguishing between negative and neutral emotions using fNIRS data [21]. However, despite these advancements, the interpretability of these models remains a significant challenge, limiting the understanding of their decision-making processes and hindering their broader application in real-world contexts.\nAddressing this limitation, Fuzzy Neural Networks (FNNs) combine the adaptability of neural networks with the interpretability of fuzzy logic systems. FNNs leverage fuzzy rules and membership functions (MFs) to process inputs, providing an intuitive framework that contrasts sharply with the opacity of conventional deep learning models [22]\u2013[24]. This transparency facilitates a clear understanding of how inputs are transformed into outputs, enhancing users' trust and interaction in aBCI systems [25]\u2013[27]. FNNs have been successfully applied to pattern classification tasks [28] and extended to adaptively learn spatiotemporal knowledge through advanced architectures like Evolving Fuzzy Neural Networks (EFuNNs) [29] and the interpretable fuzzy transfer learning model (iFuzzyTL) [26].\nBuilding on this foundation, we introduce the Interpretable Dual-Filter Fuzzy Rule-Based Model for aBCIs (iFuzzyAffectDuo), an evolution of the iFuzzyTL model originally designed for transfer learning in (Steady-state visually evoked potential) SSVEP-based EEG tasks. While iFuzzyTL demonstrate strong domain adaptation capabilities, it faces limitations in interpretability and complex pattern recognition due to the relatively simple neural patterns associated with SSVEP [26]. iFuzzyAffectDuo overcomes these limitations by incorporating a dual-filter structure that combines spatial and temporal filters, inspired by EEGNet, to enhance feature extraction and pattern recognition [30], and transiting from Gaussian to Modified Laplace MFs, significantly improving performance.\nFurthermore, iFuzzyAffectDuo integrates a fuzzy attention mechanism, enabling the model to generalize central fuzzy rules while capturing domain-specific spatiotemporal dependencies in brain signals. This novel architecture supports robust feature extraction and excels in emotion classification tasks, making it particularly effective for aBCI applications [31], [32]. To validate its performance, we evaluate iFuzzyAffectDuo on three datasets: two fNIRS datasets (Picture Recognition and Picture Empathy) and one EEG dataset (FACED) for emotion recognition tasks. Experimental results demonstrate that iFuzzyAffectDuo achieves state-of-the-art accuracy and interpretability, effectively capturing affective neural patterns and advancing the performance of aBCI systems across both fNIRS and EEG modalities."}, {"title": "II. METHODOLOGY", "content": "A. Fuzzy Inference Systems\nFuzzy Inference Systems (FISs) are widely used to model uncertainty and imprecision in various domains. A class of FIS, realized using neural architectures, is known as Fuzzy Neural networks (FNNs) [22], which generally use gradient descent optimization for training. This system uses the concept of membership, which quantifies the degree to which an element x belongs to a fuzzy set characterized by a membership function A(x). One prominent approach to fuzzy modeling is the Takagi-Sugeno-Kang (TSK) model [33], which utilizes a set of IF-THEN rules to define the relationship between inputs and outputs. For a Zero-th order TSK fuzzy system, the rules are expressed as:\nIf $x_1$ is $A_{1,r}$,\u2026\u2026,$x_D$ is $A_{D,r}$,\n(1)\nthen the output is $y = u_r$; $r = 1,\u2026, R$,\n(2)\nwhere $x_i$; $i = 1,\u2026\u2026, D$ represent the d input (linguistic) variables and $A_{i,r}$; $i = 1,\u2026\u2026,D$ are D fuzzy sets for the $r^{th}$ rule, which are defined by the membership functions $A_{i,r}(x_i)$; $i = 1,\u2026, D$.\nThe firing strength $\u00b5_r$ of the $r^{th}$ rule is typically computed as the product of the individual MFs, i.e.,\n$\u03bc_{\u03b3}(x) = \u03a0_{i=1}^{D} A_{i,r}(x_i)$\n(3)\nThe final output of the TSK FIS is derived as:\n$y = \\frac{\\sum_{j=1}^{R} \u03bc_j(x)U_j}{\\sum_{i=1}^{R} \u03bc\u03b5(x)}$\n(4)\nwhere R is the total number of rules, and y represents the aggregated output obtained through weighted aggregation of the rule outputs. If the system has multiple outputs, each rule will have multiple consequents.\nB. Enhancing Sensitivity to Scale Parameter with Modified-Laplace Membership Functions\nTraditional Gaussian MFs often exhibit broad widths, which can lead to suboptimal feature representation within models [25]. To mitigate this issue, we introduce a novel MF inspired by the Laplace Distribution, defined by:\n$f(x|m,b) = \\frac{1}{2b} e^{-\\frac{|x-m|}{b}}$\n(5)\nwhere m is the location parameter and b is the scale parameter. We modify this distribution as follows to better suit our fuzzy systems as Modified-Laplace MFs:\n$\u00b5_{ML}(x | m, \u03bb) = e^{-\u03bb|x-m|}$,\n(6)\nwhere da is a width factor within the range of [0, +\u221e). Both $A_a$ and m are trainable parameters. The firing strength $\u03bc_r(x)$ in our TSK model is computed using the product of these Modified-Laplace MFs, following the eq. (3):\n$\u03bc_r(x) = \u03a0_d[e^{-\u03bb_d|x_d-m_{r,d}|}]$,\n(7)\nwhere $x_d$ denotes the d-th feature, $m_{rd}$ is the center of the rule r's fuzzy set for the d-th feature, and $A_a$ dynamically adjusts the sensitivity of the MF, as shown in Fig. 2.\nThe firing strength of a fuzzy rule in the TSK model, $f_{MLi,r}(x)$, is expressed as:\n$f_{MLi,r}(x) = \\frac{\u03bc_\u03b3(x)}{\\sum_{j=1}^{R} \u03bc_j(x)}$\n(8)\n$= \\frac{e^{-\\sum_{d=1}^D \u03bb_{i,d}|x_{i,d}-m_{r,d}|}}{\\sum_{j=1}^R e^{-\\sum_{d=1}^D \u03bb_{j,d}|x_{j,d}-Mr,d|}}$\n(9)\nC. Sensitivity Analysis of Modified-Laplace Membership Functions\nTo conduct the Sensitivity Analysis, we analyze the derivative of the Gaussian MFs with respect to the width parameter \u03c3and the Modified-Laplace MF with respect to the width parameter A to show how sensitive each MF is to changes in its respective width parameters.\nThe derivative of $f(x | m,\u03c3)$ with respect to \u03c3 is given by:\n$\\frac{\u2202\u03bc_G(x|m, \u03c3)}{\u2202\u03c3} = \u03bc_G(x | m, \u03c3)\u00b7\\frac{(x - m)^2}{\u03c3^3}$\n(10)\nThe derivative of $\u00b5_{ML}(x | m, \u03bb)$ with respect to A is given by:\n$\\frac{\u03bc_{ML}(x|m, \u03bb)}{\u2202\u03bb} = -|x - m|\u00b7 \u03bc_{ML}(x | m, \u03bb)$.\n(11)\nThe Gaussian MF is more sensitive to changes in its width parameter o compared to the Modified-Laplace function's sensitivity to A. This higher sensitivity arises because the Gaussian derivative scales with the square of the distance from the mean, (x \u2013 m)2, and inversely with \u03c3\u00b3. In contrast, the ML derivative scales linearly with |x - m. As a result, the Gaussian's sensitivity increases more rapidly as the input deviates from the mean and is more affected by small changes in \u03c3.\nD. iFuzzyAffectDuo Main Structure\nThe proposed model, iFuzzyAffectDuo, is composed of three principal modules, as shown in Fig. 1(D). The first is the Spatial Fuzzy Filter, which is designed to capture spatial patterns within the brain data. This filter facilitates the nuanced detection of region-specific neural activity, enabling a deeper understanding of spatial dynamics associated with emotional responses. The second module, the Temporal Fuzzy Filter, focuses on the dynamic aspects of brain signals. It identifies temporal patterns that correlate with emotional responses, adapting over time to changes in the emotional state of the subject.\nTo enhance information capture, we project x(t) using WV as the value (representing Fuzzy Consequents), while the firing strength and membership degree are computed in the query space via the projection parameter W. Following TSK fuzzy model, the output (Y(t)) for ruler in these two proposed models is then expressed as:\n$Yr(t) = f_{MLi,r}(Wx(t)). WVx(t)$\n(12)\nwhere Yr(t) is the output of the penultimate layer of the network. Both We and WV are of dimension D \u00d7 D, where D is the dimension of the input. To ameliorate issues with gradient descent, we add a In operation after Yr(t), which stabilizes the gradient flow by computing logarithms of probabilities.\nLastly, the Classifier, a single-layer linear network, links the outputs from the spatial and temporal filters to the output nodes. This configuration effectively integrates the processed"}, {"title": "III. EXPERIMENTS AND RESULTS", "content": "A. Dataset\n1) fNIRS Dataset 1: Picture Recognition: The first dataset involves 23 dyads of friends (age = 20.16 \u00b1 2.02) and 26 dyads of strangers (age = 19.47\u00b12.21) participating in an image recognition task. Images were classified as neutral or negative, displayed for two seconds following a fixation point. The NIRScout 32\u00d732 was used to record blood oxygenation at a sampling rate of 7.8125 Hz, covering the prefrontal cortex with a 20-channel setup. Data preprocessing and additional details can be found in [34] and [25]. An example of these brain signals is shown in Fig. 1(A).\n2) fNIRS Dataset 2: Picture Empathy: This dataset examined responses to empathy-inducing images across 180 female participants divided into groups of 90 for social and physiological empathy, approved by the Henan Province Key Laboratory of Psychology and Behavior (20200702002). Each session included eight blocks alternating between negative and neutral images, structured in an ABBA sequence to control for order effects. Ratings of perceived pain were recorded on a 1 to 9 Likert scale. The experiment setup and data processing were consistent with the Picture Recognition dataset.\n3) EEG Dataset 3: FACED: FACED involved eliciting emotional responses through 28 video clips(12 Negative videos, 12 Positive videos, and 4 Neutral videos). A total of 123 participants (average age 23.2 years) viewed videos varying in length while EEG data were collected. The analysis focused on the power spectral densities across five EEG frequency bands for 30-second epochs, generating 150 features per channel. Further methodological details are presented in [35].\nB. Comparative Analysis of other Models\nThis section presents a comparative analysis of the iFuzzyAffectDuo model against existing models such as DBJNet, EEGNET, NLSTM, and Transformer across three datasets, employing paired t-tests with two-sided alternative hypotheses and the False Discovery Rate Benjamini/Hochberg (FDR-BH) correction for multiple comparisons. The results, visualized in Fig. 3, show that the iFuzzyAffectDuo model consistently outperforms the others. On the Picture Recognition dataset, it achieved an accuracy of 80.53% \u00b1 1.55%, significantly higher than the compared models with p < 0.001. Similar superiority is observed in the FACED dataset with an accuracy of 77.19% \u00b1 1.49%, and in the Picture Empathy dataset with 83.88% \u00b13.04%, both also surpassing competing models at p < 0.001 levels.\nThese findings illustrate the iFuzzyAffectDuo model's exceptional ability to manage and analyze diverse datasets, thereby asserting its versatility and effectiveness in complex machine learning landscapes, especially aBCI tasks. The consistently significant performance advantages across varied tasks not only validate the model's robust feature extraction and learning capabilities but also highlight its potential as a benchmark model in aBCI research.\nC. Comparison of Modified-Laplace and Gaussian Membership Functions\nIn our comparative analysis using a model architecture with 5 rules, Modified-Laplace MFs consistently outperformed Gaussian MFs across various datasets. Specifically, within the FACED dataset, Modified-Laplace MFs achieved an accuracy of 77.19% \u00b1 1.49%, significantly higher than the 74.55% \u00b1 1.20% observed for Gaussian MFs, with a notable statistical difference (t(20) = 6.55, p < 0.001, Cohen's d = 1.95). In the Picture Empathy dataset, Modified-Laplace MFs recorded 83.88%\u00b13.04% accuracy, slightly outperforming the 82.92% \u00b13.01% by Gaussian MFs (t(20) = 4.67, p < 0.001, Cohen's d = 0.32). Finally, for the Picture Recognition task, Modified-Laplace MFs demonstrated 80.53% \u00b1 1.55% accuracy, surpassing the 79.77% \u00b1 2.11% achieved by Gaussian MFs (t(20) = 2.75, p < 0.05, Cohen's d = 0.36). These findings underscore the effectiveness of Modified-Laplace MFs in enhancing classification accuracy across diverse settings.\nD. Fuzzy Set Membership and Feature Distribution Across EEG Signals\nThe distributions of fuzzy set memberships depicted in Fig. 4 highlight the operational characteristics of Modified-Laplace MFs within the query space in FACED dataset. Notably, Channel 4 exhibits a broader diversity of rule centers (m), indicating a wide variation in feature representation. In contrast, Channel 11 demonstrates less diversity in rule centers, suggesting a more uniform feature response. Channel 30 reveals that while some rules appear similar, distinct patterns emerge, particularly between rules #1 and #5 versus rules #2, 3, and 4, underscoring the subtle complexities in neural processing across different EEG channels.\nE. Sample-wise Interpretability Analysis\nIn the Picture Empathy dataset, to provide an intuitive illustration of how the fuzzy filter discerns picture types and elucidates the neural patterns of different empathy types from fNIRS data, we present four demonstrative samples from the top-performing model. These samples represent a 2 \u00d72 factorial design (Picture type: Neutral or Negative; Empathy type: Physiological or Social). From Table I, it is evident that Rule #3 predominantly influences border firing strength in all four cases, while Rule #2 consistently plays a minor role. The learned patterns of channels show substantial variety across different rules. Notably, significant contributions are observed from several specific channels: Channel 11, which measures oxy-hemoglobin (HbO) in the Left Frontopolar Area (FPA); Channel 18, which measures deoxy-hemoglobin (HbR) in the Right Ventrolateral Prefrontal Cortex (VLPFC); Channel 5, which measures HbO in the Left Ventromedial Prefrontal Cortex (VmPFC); and Channel 3, which measures HbO in the VLPFC. These channels demonstrate significant contributions across the rules. Particularly, in cases where the Picture is Neutral and the Empathy is Physiological, channel 16 by Rule #5 and channel 20 by Rule #3 are highlighted. Channel 11 is highlighted by Rules #1 and #4, and Channel 18 also receives significant emphasis by these rules. All membership degrees and firing strengths for these cases are depicted in Fig. 5. The demos for the decision-making processes in the Picture Recognition dataset are shown in Fig. 1 (E, F, G, and H). The orientation of the 3D Brain can be found in Fig. 1(C).\nOverall, these findings highlight the nuanced roles of the prefrontal cortex (PFC) in modulating distinct empathetic responses, with heightened activity observed in response to affective cues, illustrating the intricate interplay between neural circuitry and emotional processing [36]\u2013[39]."}, {"title": "IV. CONCLUSIONS", "content": "This study introduces iFuzzyAffectDuo, a novel Fuzzy logic-based model for aBCI tasks, utilizing fNIRS and EEG technologies. It advances interpretability in neural decoding, outperforming Gaussian MFs, CNN-based, and self-attention models in accuracy across two fNIRS and one EEG dataset. The Modified-Laplace distribution enhances the model's ability to detail neural activity patterns, making complex decisions understandable. The potential of iFuzzyAffectDuo extends to Functional Magnetic Resonance Imaging (fMRI) and Electrocorticography (ECoG) applications, potentially enriching theoretical frameworks in neuroscience. Future work will address the model's hyperparameter sensitivity and training duration, explore online testing, and integrate multi-modal technologies to broaden its application."}]}