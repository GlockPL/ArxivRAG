{"title": "MAPF-GPT: Imitation Learning for Multi-Agent Pathfinding at Scale", "authors": ["Anton Andreychuk", "Konstantin Yakovlev", "Aleksandr Panov", "Alexey Skrynnik"], "abstract": "Multi-agent pathfinding (MAPF) is a challenging computational problem that typically requires to find collision-free paths for multiple agents in a shared environment. Solving MAPF optimally is NP-hard, yet efficient solutions are critical for numerous applications, including automated warehouses and transportation systems. Recently, learning-based approaches to MAPF have gained attention, particularly those leveraging deep reinforcement learning. Following current trends in machine learning, we have created a foundation model for the MAPF problems called MAPF-GPT. Using imitation learning, we have trained a policy on a set of pre-collected sub-optimal expert trajectories that can generate actions in conditions of partial observability without additional heuristics, reward functions, or communication with other agents. The resulting MAPF-GPT model demonstrates zero-shot learning abilities when solving the MAPF problem instances that were not present in the training dataset. We show that MAPF-GPT notably outperforms the current best-performing learnable-MAPF solvers on a diverse range of problem instances and is efficient in terms of computation (in the inference mode).", "sections": [{"title": "Introduction", "content": "Multi-agent pathfinding (MAPF) (Stern et al. 2019) is a combinatorial computational problem that asks to find a set of paths for the agents that operate in a shared environment, such that accurately following these paths does not lead to collisions and, preferably, each agent reaches its specified goal as soon as possible. On the one hand, even under simplified assumptions, such as graph representation of the workspace, discretized time, and uniform duration of actions, optimally solving MAPF is NP-Hard (Surynek 2010). On the other hand, efficient MAPF solutions are highly demanded in numerous real-world applications, such as automated warehouses (Li et al. 2021), railway scheduling (Svancara and Bart\u00e1k 2022), transportation systems (Mironov et al. 2023), etc. This has resulted in a noticeable surge of interest in MAPF and the emergence of a large body of works devoted to this topic.\nRecently, learning-based MAPF solvers have come on stage (Skrynnik et al. 2021; Alkazzi and Okumura 2024). They mostly rely on deep reinforcement learning and typically involve additional components to enhance their performance, such as single-agent planning, inter-agent communication, etc. Meanwhile, in the realm of machine learning, currently, the most impressive progress is driven by self-supervised learning (at scale) on expert data and employing transformer-based architectures (Vaswani et al. 2023). It is this combination that recently led to the creation of the seminal large-language models (LLMs) and (large) vision-language models (VLMs) that achieve an unprecedented level of performance in text generation (dialogue) systems (Dubey et al. 2024), image and video generation (Liu et al. 2024; Zhu et al. 2024), etc. Moreover, such a data-driven approach has become widespread in robotics and reinforcement learning, where an agent's imitation policy is trained based on a variety of the expert trajectories (Chen et al. 2021; Chi et al. 2023). Thus, in this work, we are motivated by the following question: Is it possible to create a strong learnable MAPF solver (that outperforms state-of-the-art competitors) purely on the basis of supervised learning on expert data omitting additional decision-aiding routines like communication, single-agent planning, etc.? Our answer is positive.\nTo create our learning-based MAPF solver, which we name MAPF-GPT, we first design a vocabulary of terms, called tokens in machine learning, that is used to describe any observation an individual agent may perceive and any action it can perform while navigating in the environment. Next, we create a large and diverse dataset of expert data, i.e., successful sub-optimal MAPF solutions, utilizing state-of-the-art MAPF solver. Consequently, we convert these MAPF solutions into the sequences of (observation, action) tuples, encoded with our tokens, and utilize a transformer-based non-autoregressive neural network to learn to predict the correct action provided with the observation. Since sub-optimal expert actions are present in the dataset, using a simple cross-entropy loss function allows the agent to update the parameters of the transformer-based policy. In our extensive empirical evaluation, we show that MAPF-GPT notably overpasses the current best-performing learnable-MAPF solvers (without any usage of additional planning or communication mechanisms), especially when it comes to"}, {"title": "Related Works", "content": "Multiagent pathfinding Several orthogonal approaches to tackle MAPF can be distinguished. First, dedicated rule-based MAPF solvers exist that are tailored to obtaining MAPF solutions fast, yet no bounds on the suboptimality of the resultant solutions are guaranteed (Okumura 2023; Li et al. 2022). Second, reduction-based approaches to obtain optimal MAPF solutions are widespread. They convert MAPF to some other well-established computer science problems, e.g., minimum-flow on graphs, boolean satisfiability (SAT), and employ off-the-shelve solvers to obtain the solution of this (converted) problem (Yu and LaValle 2013; Surynek et al. 2016). Next, a plethora of the search-based MAPF solvers exist (Sharon et al. 2015, 2013; Wagner and Choset 2011). They explicitly rely on the graph-search techniques to obtain MAPF solutions and often may provide certain desirable guarantees, e.g., guarantees on obtaining optimal or bounded suboptimal solutions (meanwhile, simplistic search-based planners that lack strong guarantees, like prioritized planning (Ma et al. 2019), are also widespread).\nThere were also multiple attempts to apply machine and reinforcement learning to solve MAPF. One of the first such successful solvers was PRIMAL(Damani et al. 2021) that demonstrated how MAPF problem can be solved in a decentralized manner utilizing machine learning. The recent learnable MAPF solvers such as SCRIMP (Wang et al. 2023), DCC (Ma, Luo, and Pan 2021), FOLLOWER (Skrynnik et al. 2024b), to name a few, typically rely on reinforcement learning and often rely on additional modules, like the communication one, to solve the problem at hand. Orthogonally to these approaches we rely purely on imitation learning from (a large volume) of expert data.\nOffline reinforcement learning Offline deep reinforcement learning develops a policy based on previously collected data without interacting with the environment during the training phase (Levine et al. 2020). This approach allows for the utilization of large amounts of pre-collected data to develop a robust policy. There are numerous effective offline RL approaches, such as CQL (Kumar et al. 2020), IQL (Kostrikov, Nair, and Levine 2022), and TD3+BC (Fujimoto and Gu 2021). Modern approaches often involve using transformers as the architectural backbone. One popular approach is the Decision Transformer (DT), which models the behavior of an expert by conditioning on the desired outcomes, thereby integrating reward guidance directly into the decision-making process. In multi-agent scenarios, there is less diversity in offline RL methods; however, a multi-agent adaptation of the DT exists, known as MADT (Meng et al. 2021).\nThe application of offline RL to solving challenging tasks often requires additional techniques. One commonly used technique is behavioral cloning, which involves supervised learning on large datasets. A notable example of this approach is AlphaStar (Vinyals et al. 2019), where the authors pre-trained a large model (including transformer blocks) on expert gameplay data from StarCraft II and then fine-tuned it using RL. Similarly, VPT (Video Pretraining) (Baker et al. 2022) addresses the task of playing Minecraft by proposing a foundation model trained on a large video dataset collected from YouTube. In the case of AlphaGo (Silver et al. 2016), pretraining on human gameplay data was used before the development of AlphaZero (Silver et al. 2017). Additionally, transformers trained with a combination of behavioral cloning and offline RL losses have demonstrated grandmaster-level chess performance without the use of search techniques (Ruoss et al. 2024).\nMultiagent imitation learning (MAIL) Imitation learning and learning from demonstration are actively used in multi-agent systems (Tang et al. 2024; Liu and Zhu 2024). MAIL refers to the problem of agents learning to perform a task in a multi-agent system through observing and imitating expert demonstrations without any knowledge of a reward function from the environment. It has gained particular popularity in the tasks of controlling urban traffic and traffic lights at intersections (Bhattacharyya et al. 2018; Huang et al. 2023) due to the presence of a large amount of data collected in real conditions and a high-quality simulator (such as Sumo (Lopez et al. 2018)). Among the methods in the field of MAIL, it is possible to note works using the Bayesian approach (Yang et al. 2020), generative adversarial methods (Song et al. 2018; Li et al. 2024), statistical tools for capturing multi-agent dependences (Wang et al. 2021), low-rank subspaces (Shih, Ermon, and Sadigh 2022), latent multi-agent coordination models (Le et al. 2017), decision transformers (Meng et al. 2021), etc. Demonstrations are also often used as an element of pre-training in game tasks (for example, pre-training the value function for AlphaGo (Silver et al. 2016)) and in MAPF tasks (as in SCRIMP (Wang et al. 2023)). However, despite the listed works in this area, a single foundation model has not yet been proposed, the imitation learning of which already gives high results in multi-agent tasks and does not require an additional stage of online learning in the environment. This is largely due to the complexity of behavioral multi-agent poli-"}, {"title": "Background", "content": "Multi-agent pathfinding The classical variant of the MAPF problem is defined by a tuple $(n, G = (V,E), S = {s_1,..., s_n | s_i \u2208 V}, G = {g_1, ..., g_n| g_i \u2208 V})$, where n is the number of agents acting in the shared workspace which is represented as an undirected graph G. At each time step, an agent is assumed to either move from one vertex to the other or wait at the current vertex (the duration of both actions is uniform and equals 1 time step). The plan for the i-th agent, $pl_i$, is a sequence of moves, s.t., each move starts where the previous one ends. Two distinct plans are said to contain a vertex/edge conflict if a time step exists, s.t., the agents following these plans occupy the same graph vertex/traverse the same edge in opposite directions.\nThe problem is to find a set of plans, $P_l = {pl_1, ..., pl_n}$, s.t. each $pl_i$ starts at $s_i$, ends at $g_i$ and each pair of plans in $P_l$ is conflict-free. The objective to be minimized is typically defined as $SoC(P_l) = \\sum_{i=1}^{n} cost(pl_i)$ (called the Sum-Of-Costs) or as $MS(P_l) = max_{i=1,...,n} cost(pl_i)$ (called the Makespan), where $cost(pl_i)$ is the cost of the individual plan which equals the time step when the agent reaches its goal vertex (and does not move away further on).\nNotably, two assumptions on how agents behave when they reach their goals are common in MAPF: stay-at-target and disappear-at-target. In the latter case, the agent is assumed to disappear upon reaching its target and, thus, is not able to cause any further conflicts. In this work, we study MAPF under the first assumption (which is intuitively more restrictive).\nMAPF as a sequential decision-making problem Despite MAPF is typically considered to be a planning problem as defined above, it can also be considered as a sequential decision-making (SDM) problem. Within the SDM framework, the problem is to construct a policy, $\u03c0$, that is a function that maps the current state (current positions of all agents in the graph) to a (joint) action $a = a_1 \u00d7 ... \u00d7 A_n$, where $a_i \u2208 A_i$ and $A_i$ is the set of possible actions for agent i. When $\u03c0$ is obtained, it is invoked sequentially until either all agents reach their goals or the threshold on the number of time steps, $t_{max}$, is reached.\nFor better scalability, the decision-making policy might be decentralized, i.e., each agent chooses its action independently of the other agents. In practice, decentralized agents typically don't have access to the global state of the environment, i.e., positions of the other agents, but rather rely on local observation, $o_t$. For example, if the underlying graph is a 4-connected grid, then the local observation may be a"}, {"title": "Method", "content": "Our approach, MAPF-GPT, is to learn to imitate an expert in solving MAPF. The learning phase of MAPF-GPT consists of the four major steps: creating MAPF scenarios, generating ground truth solutions, tokenizing these solutions, and executing the main training loop \u2013 see Fig. 1. We will now sequentially describe these steps.\nCreating MAPF Scenarios\nA large, curated dataset is crucial for any data-driven method including ours. To create the set of training instances we used POGEMA (Skrynnik et al. 2024a), a versatile tool for developing learnable MAPF solvers that includes utilities to generate maze-like maps and maps with random obstacles,"}, {"title": "Training protocol", "content": "The model was trained to clone the behavior of the expert policy using cross-entropy loss (i.e., log-loss) via mini-batch-based stochastic gradient descent, using AdamW (Loshchilov and Hutter 2019). The target label of such loss is a ground-truth action index provided by the expert policy LaCAM. LaCAM is a centralized approach that builds a path for all agents during the whole episode, providing information about the full environment state. In contrast, the trainable model uses the local observation $o_u$ of the agent at a"}, {"title": "Experimental Evaluation", "content": "Main results In the first series of experiments, we compare 3 variants of MAPF-GPT varying in the number of parameters in their neural networks (2M, 6M, 85M) with the state-of-the-art learnable MAPF solvers: DCC (Ma, Luo, and Pan 2021) and SCRIMP (Wang et al. 2023). We use the pre-trained weights for DCC and SCRIMP. These weights were obtained by the authors while training on the random maps. Additionally, we present the results of LaCAM (Okumura 2024), which served as the expert centralized solver for data collection. For evaluation we used Random, Mazes, Warehouse, MovingAI maps (the latter two are out-of-distribution for all learnable solvers). The details on the maps and problem instances are given in Appendix.\nThe results are presented in Figure 3, where the success rate of all solvers is shown. Clearly, MAPF-GPTs significantly outperform DCC and SCRIMP across all scenarios, with the largest model, MAPF-GPT-85M, being the absolute leader. MAPF-GPT-6M shows slightly better results on the Random and Mazes maps, and comparable results on the Warehouse and MovingAI maps to the smallest MAPF-GPT-2M model.\nFigure 4 shows the Sum-of-Costs (SoC) achieved by the solvers relative to SoC of LaCAM (the lower - the better). As can be seen, MAPF-GPTs again outperform the other approaches and their performance correlates with the number of model parameters. Interestingly, on the Warehouse map, some of the instances are solved better than LaCAM did in terms of SoC.\nRuntime In this experiment, we compare the runtime efficiency of each approach. The results are presented in Figure 5; each data point indicates the average time spent de-"}, {"title": "Ablation study", "content": "In this experiment, we study how each part of the information influences the performance of the MAPF-GPT agent. To address this, we trained a 6M parameter model with certain pieces of information masked that were provided to the original model. We examine four different cases: if there is no goal information for all agents (noGoal), if there is no greedy action provided (noGA), if there is no action history (noAH), and if the agent is trained without cost-to-go information (still retaining information about obstacles). The results are presented in Table 1."}, {"title": "LifeLong MAPF", "content": "In addition to evaluating MAPF-GPT on the MAPF instances it was trained on, we also assessed its performance in a Life-Long MAPF (LMAPF) setup. Unlike regular MAPF problems, in LMAPF, each agent receives a new goal location every time it reaches its current one. In this setup, the primary objective is throughput, which is defined as the average number of goals reached by all agents per time step.\nWe evaluated MAPF-GPT in both zero-shot and fine-tuned configurations. To fine-tune the model, we generated an additional dataset using mazes maps. For expert data, we employed the RHCR approach (Li et al. 2021), as LaCAM is not well-suited for LMAPF. The dataset contains 90 million tensors. We used MAPF-GPT-6M for this experiment.\nThe results are presented in Table 2. Even the zero-shot model is able to compete with other existing learning-based approaches, such as Follower (Skrynnik et al. 2024b) and MATS-LP (Skrynnik et al. 2024c). Moreover, in all cases, fine-tuning improved the results of the MAPF-GPT-6M model, even outperforming other approaches on the warehouse map."}, {"title": "Evaluation on Puzzles", "content": "The Puzzles set is part of the POGEMA benchmark, briefly mentioned in the ablation study section of our paper. However, it provides valuable insights into the cooperation abilities of agents. The Puzzles maps were specifically designed with narrow corridors, cul-de-sacs, and lacunas. The examples of such scenarios are depicted in Figure 7. These challenging instances require agents to cooperate strategically, such as one agent entering a corridor and using a lacuna to let another agent pass."}, {"title": "Limitations", "content": "The main limitation of MAPF-GPT is a generic one, shared with the other learnable methods, i.e. it lacks theoretical guarantees. The next limitation is that training large models, e.g., MAPF-GPT-85M, is quite demanding (requires expensive hardware and prolonged time). Still, our smaller models containing 6M and 2M parameters are much less demanding while providing competitive results. It should also be noted that all MAPF-GPT models are sensitive to the quality of trajectories in the expert data set. As has been shown in previous research on behavior cloning with transformers, e.g (Chen et al. 2021), adding low-quality trajectories to expert data may lead to significant degradation in model's performance. It is also unclear how effectively MAPF-GPT can replicate the behavior of the other existing centralized approaches (such as CBS (Sharon et al. 2015) that is an optimal MAPF solver). This dependence on the type of behavioral expert policy requires further research."}, {"title": "Conclusion", "content": "In this work we have studied MAPF problem as a sequential decision making problem. We proposed an approach to obtain a an individual policy based on state of the art in machine learning, i.e. (supervised) imitation learning on the expert data - MAPF-GPT. To train MAPF-GPT we create"}]}