{"title": "RingFormer: A Ring-Enhanced Graph Transformer for Organic Solar Cell Property Prediction", "authors": ["Zhihao Ding", "Ting Zhang", "Yiran Li", "Jieming Shi", "Chen Jason Zhang"], "abstract": "Organic Solar Cells (OSCs) are a promising technology for sustainable energy production. However, the identification of molecules with desired OSC properties typically involves laborious experimental research. To accelerate progress in the field, it is crucial to develop machine learning models capable of accurately predicting the properties of OSC molecules. While graph representation learning has demonstrated success in molecular property prediction, it remains under-explored for OSC-specific tasks. Existing methods fail to capture the unique structural features of OSC molecules, particularly the intricate ring systems that critically influence OSC properties, leading to suboptimal performance. To fill the gap, we present RingFormer, a novel graph transformer framework specially designed to capture both atom and ring-level structural patterns in OSC molecules. RingFormer constructs a hierarchical graph that integrates atomic and ring structures and employs a combination of local message-passing and global attention mechanisms to generate expressive graph representations for accurate OSC property prediction. We evaluate RingFormer's effectiveness on five curated OSC molecule datasets through extensive experiments. The results demonstrate that RingFormer consistently outperforms existing methods, achieving a 22.77% relative improvement over the nearest competitor on the CEPDB dataset.", "sections": [{"title": "Introduction", "content": "As the demand for renewable energy sources grows, organic solar cells (OSCs) have attracted considerable interest for their ability to convert sunlight into electricity (Wang et al. 2016). Despite their potential, the development of OSCs has been hindered by the reliance on trial-and-error methods, which involve complex and time-consuming synthesis procedures (Sun et al. 2019). To accelerate progress, there is increasing interest in leveraging machine learning models to accurately predict the properties of OSC molecules, promising to expedite the development of OSCs.\nIn this work, we aim to predict OSC properties based on the structure of OSC molecules, i.e., organic small-molecule semi-conducting materials for the active layer of OSCs. Such molecules function as electron acceptors or donors to create photovoltaic effects with efficacy linked to their conjugated structure, such as aromatic rings (Solak and Irmak 2023). In chemistry, rings are closed loops of atoms connected through covalent bonds (Jonathan Clayden 2012), and the design of complex ring systems has become a central focus in OSC research (Schweda et al. 2021). These systems can include various fused and non-fused ring structures (Gao et al. 2023). For example, as shown in Figure 1, the core of OSC molecule 4T-1 (highlighted in yellow) on the left comprises four non-fused thiophene rings connected by single bonds. In contrast, the core of SN6IC-4F on the right side is a fused S,N-heteroacene consisting of six rings, where adjacent rings share a pair of bonded atoms. The backbone of both molecules can be characterized by multiple inter-connected rings connecting each other in various ways, forming a complex ring system. Additionally, different alkyl functional groups (in blue) attached to the core as side chains can further influence OSC performance (Ching and Isabelle 2023). These structural differences ultimately lead to significantly different power conversion efficiency (PCE) values between 4T-1 and SN6IC-4F. Therefore, accurately predicting the properties of OSC molecules requires capturing both the high-level structure of the ring system and the local-level atomic groups within these molecules.\nFor predicting molecular properties, graph neural networks (GNNs) have been widely adopted, they represent chemical molecules as graphs, with nodes for atoms and edges for chemical bonds. While GNNs effectively capture local atomic structures like functional groups, they struggle to model higher-order patterns such as those found in OSC ring systems (He et al. 2023). To address these limitations, graph pooling methods (Gao and Ji 2019; Lee et al. 2019)"}, {"title": "Related Work", "content": "OSC Property Prediction. Organic solar cells (OSCs) have garnered significant research attention as one of the most promising technologies for harnessing solar energy (Eibeck et al. 2021). As conducting laboratory experiments to screen candidate OSC molecules is time and resource-intensive (Xu et al. 2022), researchers have recently turned to machine learning methods for efficient OSC property prediction. Currently, fingerprint-based approaches (Eibeck et al. 2021) are commonly employed. Typically, these methods utilize hand-crafted molecular fingerprints such as MACCS (Durant et al. 2002) and ECFP (Rogers et al. 2010) as molecular features, which are then inputted into off-the-shelf machine learning models like random forest and support vector machine. However, fingerprints represent simplified abstractions of molecular structures, which overlook crucial molecular information and interactions, particularly in OSC molecules with complex structures (Miyake and Saeki 2021). Inspired by the success of GNNs in drug discovery, Eibeck et al. (Eibeck et al. 2021) recently explored the application of GNNs in OSC property prediction. However, they found that conventional GNNs often perform poorly in predicting OSC properties, achieving lower accuracy compared to fingerprint methods. The development of effective models for OSC property prediction remains under-explored.\nGraph Representation Learning on Molecules. As molecules can be naturally represented as graphs, graph neural networks (GNNs) (Hu et al. 2019; Zhu et al. 2023) are widely used for molecule property prediction. However, conventional GNNs struggles to capture high-order structures (He et al. 2023), including important molecular features such as rings (Chen et al. 2020; Loukas 2019). To address the limitations of GNNs, researchers have developed graph pooling methods (Gao and Ji 2019; Lee et al. 2019), motif-based methods (Yu and Gao 2022; Zang et al. 2023), and graph transformers (Rong et al. 2020; Ying et al. 2021; Kim et al. 2022). Although these models capture higher-level molecular patterns beyond localized atomic features, they still fail to emphasize the rings and their connections, which are crucial for accurately predicting OSC molecule properties. To fill the gap, we propose RingFormer, the first graph transformer framework that is specially designed to capture ring systems for OSC property prediction."}, {"title": "Problem Formulation", "content": "Data Model. Following previous works on OSC property prediction (Eibeck et al. 2021), an OSC molecule is represented by an atom-level molecular graph $G_A = (V_A, E_A)$, where $V_A$ is the set of nodes representing atoms and $E_A$ is the set of edges representing chemical bonds. Let $x_{v_i} \\in \\mathbb{R}^{d_{VA}}$ denote node attribute vector of node $v_i \\in V_A$ and $e_{ij} \\in \\mathbb{R}^{d_{EA}}$ denote edge attribute vector of edge $e_{ij} \\in E$, where $d_{VA}$ and $d_{EA}$ are the dimension of node and edge attributes, respectively.\nPrediction Task. Given an OSC molecular graph $G_A$, the objective of our work is to learn a property prediction model $f: G_A \\rightarrow y$ that predicts the target property value $y$, such as power conversion efficiency (PCE), highest occupied molecular orbital (HOMO), lowest unoccupied molecular orbital (LUMO), etc (Miyake and Saeki 2021). Notably, PCE holds particular significance for OSC molecules, as it serves as a pivotal indicator of their efficiency in converting sunlight into electrical energy (Eibeck et al. 2021). Accurate prediction of PCE is essential for assessing the performance of OSCs. In this study, we prioritize the prediction of PCE as our pivotal target property."}, {"title": "The RingFormer Method", "content": "Overview. Aiming to capture both atom and ring-level structural patterns in an OSC molecule, our proposed Ring-Former framework firsts constructs a hierarchical OSC graph, and then holistically encodes the hierarchical graph using RingFormer layers for property prediction. As depicted in Figure 2, the hierarchical OSC graph consists of atom, ring, and inter-level graphs that models OSC molecule structure from multiple levels. The RingFormer layers combine message-passing and global attention mechanisms to further learn node representations at each level and fuse information across hierarchies. Finally, RingFormer aggregates node representations of both atom and ring nodes to generate the graph representation which is used for predicting target OSC properties."}, {"title": "Hierarchical OSC Graph Construction", "content": "In this section, we introduce how to construct the hierarchical OSC graph $G$ for an OSC molecule. As illustrated in Figure 1, the ring system, characterized by the types of rings and their interconnections, plays a crucial role in determining the properties of OSC molecules. To overcome the limitations of a flat molecular graph $G_A$ that solely depicts low-level atom-based structure, our proposed hierarchical OSC graph $G$ comprehensively represents an OSC molecule from three levels. As shown in Figure 2, given an input atom-level graph $G_A$, we construct a ring-level graph $G_R$ above the atom-level $G_A$ to explicitly depict the high-level ring system. After having $G_A$ and $G_R$, we further incorporate a bipartite inter-level graph $G_I$ to describe the connections between atom and ring nodes and represent graph hierarchy. Combining the three levels of graphs, we obtain the hierarchical OSC graph $G = {G_A, G_R, G_I}$. Next, we provide more details on each level of the graph.\nRing-level Graph Construction. Given an input OSC molecule modeled by an atom-level molecular graph $G_A$ (introduced in the problem formulaiton section), we first extract the smallest rings from $G_A$ and represent each ring by a ring node in $V_R = {v_1, v_2, ..., v_{|V_R|}$. Here, the smallest ring is defined as a ring such that no proper subset of its nodes can form a smaller ring.\nDifferent types of rings are characterized by their atom composition. For a ring node $v_i \\in V_R$, we use the one-hot encoding of its ring type as the attribute vector $x_{v_i} \\in \\mathbb{R}^{d_{VR}}$, where $d_{VR}$ represents the total number of ring types. Between a pair of extracted ring nodes, we add an edge in the edge set $E_R$ to suggest the connections between them when (1) the two rings share one or more atoms or (2) the two rings are connected by a single chain of one or more non-aromatic bonds. For an edge $e_{ij} \\in E_R$, its edge attribute is an one-hot vector $e_{ij} \\in \\mathbb{R}^{d_{ER}}$ indicating the connection type, where $d_{ER}$ represents the total number of connection types. We differentiate the type of connection between two ring nodes based on the atoms shared by two rings in condition (1) or the atoms and bonds composition of the connecting chain in condition (2). Having the ring nodes and their interconnections, we construct ring-level graph $G_R = (V_R, E_R)$."}, {"title": "RingFormer Layer", "content": "In this section, we present RingFormer layers that learn expressive representations for both atom and ring nodes on the hierarchical OSC graph $G$. Considering the hierarchical nature of $G$, we combine the power of message-passing and global attention in RingFormer layers to better capture the unique patterns in each graph level. As shown in Figure 2, each RingFormer layer consists of an atom-level message passing module, a ring-level cross-attention module, and an inter-level message passing module. At the end of each RingFormer layer, we fuse the latent information learned from hierarchical levels to generate expressive node representations for atoms and rings, which are then forwarded to the next layer for further updates. Next, we introduce each module in detail.\nAtom-level Message Passing Module. On the atom-level graph $G_A$, we aim to learn atom node representations that capture the local chemical structures in OSC molecules, such as function groups. As GNNs are good at mining patterns in local structures (He et al. 2023), we adopt GNNs to perform message passing on $G_A$. In the $l$-th RingFormer layer, GNNs are applied on the atom-level graph as follows:\n$h_{v_i, A}^{(l)} = GNN(h_{v_i}^{(l-1)}, \\{h_{v_j}^{(l-1)} | v_j \\in N_{GA}(v_i)\\}, \\{e_{ij}\\}),$ (1)\nwhere $v_i, v_j \\in V_A$ denote nodes in $G_A$, $N_{GA}(v_i)$ is the neighborhood of node $v_i$ in $G_A$, $e_{ij}$ is the edge attribute of edge $e_{ij} \\in E_A$, and $h_{v_i}^{(l-1)} \\in \\mathbb{R}^{d}$ is the node representation of node $v_i$ from the preceding RingFormer layer. Specially, we have $h_{v_i}^{(0)} = W_ax_{v_i}$, where $W_a \\in \\mathbb{R}^{d \\times d_{va}}$ are learnable parameters to transform node attribute $x_{v_i}$, $d$ is the hidden dimension. Here, $GNN$ can be arbitrary GNN architecture. Throughout the paper, we use GINE (Hu et al. 2019) as the default GNN backbone. In Appendix, we study the effect of different GNN backbones. $h_{A}^{(l)} \\in \\mathbb{R}^{d}$ is the output node representation of the atom-level message passing module in the $l$-th RingFormer layer.\nRing-level Cross-attention Module. In the encoding of ring-level graph, we aim to learn ring node representations that capture the global structure of the ring system. To achieve this, we introduce a novel transformer-based cross-attention module tailored for the ring-level graph. Recently, graph transformers (Ying et al. 2021; Kreuzer et al. 2021) have shown their superiority to GNNs in modeling long-range node dependencies and capturing structural patterns beyond localized graph structures (Ramp\u00e1\u0161ek et al. 2022). However, existing graph transformers still exhibit limitations in capturing the chemical semantic-rich structures of ring-level graphs that represent ring systems in OSC molecules. A primary limitation of existing graph transformers is their inadequate utilization of edge attributes which contain crucial information about how two rings are connected in the ring-level graph. The graph transformers typically compute self-attention using node representations as queries, keys, and values, with edge attributes merely considered in attention biases. Consequently, valuable chemical semantics in edge attributes of the ring-level graph are not integrated into the output ring node representations.\nMoreover, graph transformers further suffer from efficiency issues when integrating edges attributes. For instance, as Graphormer calculate fully-connected attention among all node pairs in a graph, it also needs to compute the shortest paths among all node pairs as edge attributes, leading to high computation complexity relative to the number of nodes (Shirzad et al. 2023).\nTo address the above limitations, we introduce two key designs in our proposed ring-level cross-attention module: cross-attention mechanism and localized neighborhood attention strategy. These designs optimize the utilization of edge attributes, while maintaining computational efficiency. Firstly, to better leverage the edge attributes, we propose to adopt a cross-attention strategy instead of the node-centric self-attention commonly used in graph transformers. Specifically, for a source ring node $v_j \\in V_R$ connected to the target ring node $v_i \\in V_R$ through edge $e_{ij} \\in E_R$, we regard the node representation of $v_i$ as the query, while the combination of the node representation of $v_j$ and edge attribute $e_{ij}$ serves as the key and value. In such a way, edge attributes are not only considered in the attention scores but also integrated into the target node representation during aggregation. Then, in calculating multi-head attention, the queries, keys, and values for the c-th attention head in the $l$-th layer are calculated as follows:\n$z_{ij}^{(l)} = MLP(h_{v_j}^{(l-1)} || e_{ij}),$ (1)\n$q_{v_i,c}^{(l)} = W_q^c h_{v_i}^{(l-1)}, k_{ij,c}^{(l)} = W_k^c z_{ij}^{(l)}, v_{ij,c}^{(l)} = W_v^c z_{ij}^{(l)},$ (2)\nwhere $||$ indicates concatenation operation, $MLP$ is a multi-layer perception network (MLP) with one hidden layer, $W_q^c, W_k^c, W_v^c \\in \\mathbb{R}^{\\frac{d}{C} \\times d}$ are learnable parameters, and $C$ is the number of attention heads. Specially, we have $h_{v_i}^{(0)} = W_rx_{v_i}$, where $W_r \\in \\mathbb{R}^{d \\times d_{VR}}$ are learnable parameters to transform node attribute $x_{v_i}$.\nFurthermore, to reduce the computation cost, inspired by (Shirzad et al. 2023), we replace fully-connected graph attention with localized neighborhood attention augmented by a virtual molecule node. Specifically, ring-level cross-attention module adds a virtual molecule node to $G_R$ and connects the virtual node to all other ring nodes using virtual edges. In this way, global information can be gathered and spread through the virtual node without the need to compute attention between all pairs of nodes. We initialize the representation of the virtual node using learnable embedding and set the edge attributes of the virtual edges as one-hot vectors different from real edges in $G_R$. Then, we calculate localized neighborhood attention between the target node and its immediate neighbors as follows:\n$\\alpha_{ij}^{(l)} = \\frac{exp(\\frac{q_{v_i,c}^{(l)} k_{ij,c}^{(l)}}{\\sqrt{d}})}{\\Sigma_{v_k \\in N_{GR}(v_i)} exp(\\frac{q_{v_i,c}^{(l)} k_{ik,c}^{(l)}}{\\sqrt{d}})},$ (3)\nwhere $q_{v_i,c}^{(l)}, k_{ij,c}^{(l)}, v_{ij,c}^{(l)} \\in \\mathbb{R}^{\\frac{d}{C}}$ are the query, key, value vectors calculated in Eq.(2), $N_{G_R}(v_i)$ is the neighborhood of node $v_i$ in $G_R$, and $\\sqrt{d}$ is  Then, we update ring node representations based on cross-attention as follows:\n$h_{v_i, R}^{(l)} = W_r h_{v_i}^{(l-1)} + W_s || (\\Sigma_{c=1}^{C} \\Sigma_{v_j \\in N_{G_R}(v_i)} \\alpha_{ij}^{(l)} v_{ij,c}^{(l)}),$ (4)\nwhere $W_r, W_s \\in \\mathbb{R}^{d \\times d}$ are learnable parameters, $h_{OR}^{(l)} \\in \\mathbb{R}^{d}$ is the output node representation of node $v_i$ after multi-head cross-attention. Specially, we calculate $h_{v_i, R}^{(0)} = W_rx_{v_i}||p_{v_i}$, where $p_{v_i} \\in \\mathbb{R}^{d_p}$ is a real-valued embedding vector with dimension $d_p$ working as nodev_i's degree-based position encoding (Ying et al. 2021) and $W_r\\in \\mathbb{R}^{(d-d_p) \\times d_{VR}}$ is learnable parameter.\nFollowing the convention of typical transformer blocks (Vaswani et al. 2017), the node representations generated by cross-attention further go through a feed-forward layer and are updated as follows:\n$h_{v_i, R}^{(l)} = FFN^{(l)}(h_{OR}^{(l)}+h_{v_i}^{(l-1)})$ (5)\nwhere $FFN^{(l)}$ is the feed-forward block in the $l$-th layer, and $h_{v_i, R}^{(l)} \\in \\mathbb{R}^{d}$ is the output node representation of ring-level cross-attention module in the $l$-th RingFormer layer.\nInter-level Message Passing Module. In the inter-level message passing module, we further transfer knowledge between atom and ring nodes on the bipartite graph $G_I$, allowing atom representations to perceive the global structure of the high-level ring system, while ring representations are enriched by the local structure around their constituent atoms. Specifically, GNNs are applied on the inter-level message passing module as follows:\n$h_{v_i, I}^{(l)} = GNN(h_{v_i}^{(l-1)}, \\{h_{v_j}^{(l-1)} | v_j \\in N_{G_I}(v_i)\\}),$ (6)\nwhere $v_i, v_j \\in V_I$ denotes nodes in $G_I$, $N_{G_I}(v_i)$ is the neighborhood of node $v_i$ in $G_I$, and $h_{v_i, I}^{(l)} \\in \\mathbb{R}^{d}$ is the output node representation of inter-level message passing module in the $l$-th RingFormer layer.\nHierarchical Messages Fusion. After learning node representations on the three levels of graphs, the $l$-th RingFormer layer generates two types of node representations from different graph hierarchies for every atom and ring node. Specially, an atom node $v_i\\in V_A$ has $h_{v_i, A}^{(l)}$ generated by atom-level message passing module on $G_A$ and $h_{v_i, I}^{(l)}$ generated by inter-level message passing module on $G_I$. Similarly, an ring node $v_j \\in V_R$ has $h_{v_j, R}^{(l)}$ learned by ring-level cross-attention module on $G_R$ and $h_{v_j, I}^{(l)}$ learned by inter-level message passing module on $G_I$.\nTo facilitate information fusion across the hierarchies, we combine the two representations for each node:\nh_{v_i}^{(l)} = \\begin{cases}MLP(h_{v_i,A}^{(l)} || h_{v_i,I}^{(l)}) & \\text{if } v_i \\in V_A\\\\MLP(h_{v_i,R}^{(l)} || h_{v_i,I}^{(l)}) & \\text{if } v_i \\in V_R\\end{cases}\nwhere $h_{v_i}^{(l)} \\in \\mathbb{R}^{d}$ is the final node representation output by the $l$-th RingFormer layer for node $v_i \\in V_A \\cup V_R$. $h_{v_i}^{(l)}$ is then sent to the next RingFormer layer for further update."}, {"title": "Prediction Layer", "content": "After stacking $L$ RingFormer layers, we aggregate their output node representations to generate graph representation for the hierarchical OSC graph. First, for each node $v_i \\in V_A \\cup V_R$, we concatenate its representations from all RingFormer layers as its final node representation to incorporate structural patterns at different scales:\n$h_{v_i} = CONCAT(h_{v_i}^{(l)}, h_{v_i}^{(l)}, ..., h_{v_i}^{(l)}),$ (8)\nwhere CONCAT indicates concatenation operation, and $h_{v_i} \\in \\mathbb{R}^{d \\times (L+1)}$ is the concatenated node representation. Then, we separately aggregate node representations of atoms and rings in G using sum pooling and concatenate the two to obtain the final graph representation:\n$h_g = POOL(\\{h_{v_i}|V_i \\in V_A\\}) || POOL(\\{h_{v_i}|V_i \\in V_R\\}),$ (9)\nwhere POOL indicates sum pooling operation and $h_g \\in \\mathbb{R}^{2d \\times (L+1)}$ is the final graph representation for the hierarchical OSC graph $G$. As the target properties of OSC molecules considered in this paper are real values, and thus OSC property prediction can be regarded as a regression task, we project the molecular representation into the logits $\\hat{y}_g$ using one linear layer. Given a batch of training molecules, RingFormer is trained with mean absolute error (MAE) loss:\n$L = \\frac{1}{B} \\Sigma_{b=1}^{B} |\\hat{y}_g - y_g|,$ (10)\nwhere $B$ is the batch size, and $\\hat{y}_g$ and $y_g$ represent the predicted and ground-truth molecule properties, respectively."}, {"title": "Experiments", "content": "Experimental Settings\nDatasets and Evaluation Metrics. We curate 5 OSC molecule datasets to evaluate property prediction performance, as listed in Table 1. Specially, CEPDB (Hachmann"}, {"title": "Overall Performance", "content": "PCE prediction. We report the performance of RingFormer in predicting power conversion efficiency (PCE), the primary property of interest for OSC molecules (Solak and Irmak 2023), in Table 2. Firstly, we observe that Ring-Former consistently achieves the best performance, except that RingFormer is the runner-up in PFD. For instance, on CEPDB, RingFormer achieves test MAE 0.189, which indicates 22.8% relative improvement over the best competitor with test MAE 0.244. On NFA, the dataset with the highest average number of rings (See Table 1), RingFormer outperforms the fingerprint-based method ECFP by 4.96%. In contrast, other deep learning models struggle to match the performance of ECFP. Furthermore, across the four experimental datasets (HOPV, PFD, NFA, PD), we observe that GNN-based methods consistently yield inferior performance compared to fingerprint-based methods, indicating a struggle in learning structural patterns from larger and more complicated OSC molecules. However, RingFormer still achieves competitive performance across these datasets, emerging as the top performer in three out of four datasets. The results demonstrate RingFormer's ability to capture structural patterns in OSC molecules for accurate property prediction.\nMulti-task learning. We further evaluate the performance of RingFormer in multi-task learning using CEPDB dataset. Specifically, we aim to predict 5 target properties, resulting in 5 regression tasks. The details of the five target properties are given in Appendix. For training RingFormer and other deep neural network-based competitors, we set the output dimension to be the same as the number of target properties and train the neural network using MAE loss. As the results shown in Table 3, RingFormer consistently outperforms other competitors in all six target properties, often by a significant margin. For instance, RingFormer has test MAE 5.993 in predicting Jsc, achieving 20.24% relative improvement on the best competitor. The results further validate RingFormer's superiority in predicting multiple OSC properties simultaneously. Additionally, we observe GPS consistently achieve promising results across all target properties, only inferior to RingFormer. It is because both RingFormer and GPS combine the power of message passing and global attention, which validates the importance of capturing both local and global structural features in OSC molecules."}, {"title": "Model Analysis", "content": "Ablation on hierarchical OSC graph. We evaluate the effectiveness of hierarchical OSC graph by comparing it with all three levels $G = \\{G_A, G_R, G_I\\}$ to a subset of $G$. As shown in Table 4, we observe a significant performance drop on the incomplete hierarchical OSC graph compared to the full one, which validates the necessity of all levels in $G$. Moreover, observing that $G \\setminus G_I$ is better than $G_A$ and $G_R$, we conclude that encoding structure of both atom and ring level is important in OSC property prediction. Comparing $G$ with $G \\setminus G_I$, we find transferring hierarchical information between atom and ring levels can improve performance.\nEffectiveness of ring-level cross-attention module. We evaluate the effectiveness of the proposed ring-level cross-attention module by replacing it with other graph representation learning layers and report the results in Table 5. Specifically, Cross-attention w.o. virtual is the variant of"}, {"title": "Conclusion", "content": "This paper addresses the under-explored problem of predicting properties of organic solar cells (OSCs) and introduces RingFormer, a novel graph transformer framework designed to capture rings and their interconnections within an OSC molecule to facilitate accurate prediction. RingFormer constructs a hierarchical OSC graph that represents OSC molecular structure at both atom and ring levels, and leverages a combination of local message-passing and global attention mechanisms to learn expressive graph representations. Extensive experiments demonstrate the superiority of RingFormer in OSC property prediction."}, {"title": "Appendix", "content": "Dataset Details\nCEPDB: This dataset is curated from the Harvard's clean energy project database (Hachmann et al. 2011) which contains approximately 2.3 million organic semiconductor molecules with potential applications as donor materials in organic solar cells. Each molecule is initially represented by a SMILES string, and the database includes OSC-related properties calculated using density functional theory (DFT) methods. During dataset curation, we eliminate invalid SMILES strings using RDKit\u00b9 and resulting in a final set of 2,225,974 valid molecules. Six properties are included for OSC property prediction: power conversion efficiency (PCE), highest occupied molecular orbital (HOMO), lowest unoccupied molecular orbital (LUMO), Band Gap, open circuit voltage (Voc), and short-circuit current density (Jsc). The SMILES strings are converted to atom-level molecular graphs using PyTorch-Geometric2.\nHOPV: The Harvard Organic Photovoltaic (HOPV) dataset (Lopez et al. 2016) collects the data of 350 selected small molecules and polymers, represented by SMILES strings, intended for use as p-type constituents in OSCs. The dataset encompasses various experimentally determined properties. During dataset curation, PCE is regarded as the ground truth property, and the SMILES strings are converted to atom-level molecular graphs.\nPFD: This dataset is originally reported by Nagasawa et al. (Nagasawa et al. 2018), comprises 1203 donor polymer molecules with various device parameters, including PCE. These device parameters are experimentally obtained from old-generation polymer:fullerene-based OSC devices. Each molecule is assigned a SMILES string and a nickname. Due to potential duplications arising from one donor polymer molecule working with different acceptors, resulting in different PCE values, molecules with the same nickname are merged. The largest PCE value is considered the ground truth, and after SMILES validity checks, 1055 molecules remain, represented by molecular graphs.\nNFA: This dataset is curated from the Polymer:NFA dataset reported by Miyake et al. (Miyake and Saeki 2021) containing 1318 pairs of polymer donor and non-fullerene acceptor (NFA) molecules, along with experimentally determined PCE values. NFAs represent a new generation of electron acceptors for organic photovoltaics, distinct from fullerene structures, offering significantly enhanced performance for OSC devices. During dataset curation, pairs of molecules with the same acceptor molecule are merged based on SMILES strings, and the largest PCE value is considered the ground truth. After SMILES validity checks, 654 acceptor molecules remain, represented by molecular graphs.\nPD: PD dataset, also obtained from the Polymer:NFA dataset (Miyake and Saeki 2021), differs from NFA in that it focuses on predicting OSC properties based on polymer donor molecules. During dataset curation, pairs of molecules\nOSC Properties Details\nIn this paper, we focus on six key OSC properties: power conversion efficiency (PCE), highest occupied molecular orbital (HOMO), lowest unoccupied molecular orbital (LUMO), band gap, open-circuit voltage (Voc), and short-circuit current density (Jsc). Below, we provide detailed descriptions of each property:\nPCE (%) of an organic solar cell is expressed as the percentage ratio of electrical power produced to optical power impinging on the cell (Sikiru et al. 2022). It is the most critical component of every OSC system, ranging from 0% to 100%.\nThe HOMO (eV) is the highest-energy molecular orbital that has electrons in it and the LUMO (eV) is the next energy orbital level close to HOMO, which always has states that are empty of electrons (Hussain 2018).\nBand Gap (eV) or HOMO-LUMO gap is the energy difference between the HOMO and LUMO. Its size can be used to predict the strength and stability of transition metal complexes (Griffith and Orgel 1957). As a rule of thumb, the smaller a compound's HOMO-LUMO gap, the more stable the compound.\nVoc (V) is the maximum voltage available from a solar cell, and this occurs at zero current (Sebesty\u00e9n 2021). The open-circuit voltage corresponds to the amount of forward bias on the solar cell due to the bias of the solar cell junction with the light-generated current.\nJsc (mA/cm\u00b2) is the current density at zero voltage (Akhtaruzzaman and Selvanathan 2021).\nImplementation Details of Baselines\nWe provide more description and implementation details of the baselines in experiments section.\nMACCS is a fingerprint-based method. MACCS uses fingerprints Molecular ACCess System keys (Durant et al. 2002) which are one of the most commonly used structural keys. These fingerprints are binary in nature, consisting of a fixed-length bitstring (typically 166 bits). Each bit in the MACCS keys represents the presence or absence of a specific substructure or chemical pattern within a molecule.\nECFP is also a fingerprint-based method that uses Extended Connectivity Circular Fingerprints (Rogers et al. 2010). ECFP captures the structural information and connectivity patterns of molecules by considering circular neighborhoods around each atom in a molecule. Within these neighborhoods, it identifies and encodes substructures or fragments. The resulting ECFP fingerprint is a binary representation, with each element indicating the presence or absence of specific substructures within the circular neighborhoods."}, {"title": "Implementation Details", "content": "Implementation Details. In RingFormer, we use GINE (Hu et al. 2019) as GNNs in atom-level message passing module and inter-level message passing module. We fix the number of layers L = 8, dimension d = 512, and the number of attention heads C = 4 in all 5 datasets. We use mini-batch gradient descent to optimize parameters"}, {"title": "Additional Experiments", "content": "Performance under different GNN backbones. In the previous experiments, we regard GINE as the default GNN backbone used in atom-level message passing module and inter-level message passing module components of RingFormer. In this experiment, we assess the performance of RingFormer using various GNN backbones and provide the results in Table 8. Specifically, we implement RingFormer"}]}