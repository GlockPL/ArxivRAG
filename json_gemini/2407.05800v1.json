{"title": "FedMRL: Data Heterogeneity Aware Federated Multi-agent Deep Reinforcement Learning for Medical Imaging", "authors": ["Pranab Sahoo", "Ashutosh Tripathi", "Sriparna Saha", "Samrat Mondal"], "abstract": "Despite recent advancements in federated learning (FL) for medical image diagnosis, addressing data heterogeneity among clients remains a significant challenge for practical implementation. A primary hurdle in FL arises from the non-IID nature of data samples across clients, which typically results in a decline in the performance of the aggregated global model. In this study, we introduce FedMRL, a novel federated multi-agent deep reinforcement learning framework designed to address data heterogeneity. FedMRL incorporates a novel loss function to facilitate fairness among clients, preventing bias in the final global model. Additionally, it employs a multi-agent reinforcement learning (MARL) approach to calculate the proximal term (\\(\\mu\\)) for the personalized local objective function, ensuring convergence to the global optimum. Furthermore, FedMRL integrates an adaptive weight adjustment method using a Self-organizing map (SOM) on the server side to counteract distribution shifts among clients' local data distributions. We assess our approach using two publicly available real-world medical datasets, and the results demonstrate that FedMRL significantly outperforms state-of-the-art techniques, showing its efficacy in addressing data heterogeneity in federated learning. The code can be found here https://github.com/Pranabiitp/FedMRL.", "sections": [{"title": "1 Introduction", "content": "Deep learning (DL) algorithms have demonstrated significant achievements in medical image analysis tasks [15], [19], [18], [17]. However, creating effective DL-based models typically requires gathering training data from various medical centers, such as hospitals and clinics, into a centralized server. Obtaining patient data from multiple centers presents challenges due to privacy concerns, legal restrictions on data sharing, and the logistical difficulty of transferring large data volumes [20]. Researchers have increasingly employed Federated Learning (FL) as a solution, enabling medical image classification with decentralized data from multiple sources while preserving privacy [25], [3]. Unlike models trained independently at individual sites, FL can leverage a more diverse and extensive dataset, resulting in improved performance and increased generalizability [13], [11]. The efficacy of federated training encounters challenges due to data heterogeneity within local hospital datasets, resulting in performance degradation in real-world healthcare applications. This heterogeneity manifests in various forms: some hospitals may possess more data from patients at early stages, while others primarily collect data from patients with severe conditions, leading to label distribution skew. Additionally, variations in data quantity among hospitals, with larger institutions having more patient data compared to community clinics, contribute to quantity skew. Moreover, differences in imaging acquisition protocols and patient populations further exacerbate feature distribution skew [24]. FedAvg, a foundational FL algorithm, while successful in many scenarios, exhibits diminished efficacy in heterogeneous data settings [13]. To address this challenge, FedProx [8] introduced a proximal term (\\(\\mu\\)) into the conventional optimization objective to penalize large updates in the model parameters. However, selecting an optimal value for the proximal term \\(\\mu\\) in FedProx presents a challenge, as traditional methods like trial and error or heuristics may not effectively adapt to heterogeneous data distributions.\nDistribution shifts within each hospital's private dataset often result in scenarios where the global model performs better for certain hospitals but neglects others. The study [9] introduced q-fairness optimization problems in FL, where the parameter q guides the loss function to desirable outcomes. Huang et al. [6] focused on fairness and robustness by dynamically selecting local centers for training, but this approach may not be suitable for the medical domain due to limited participant hospitals. Lyu et al. [12] proposed a collaborative fair FL framework to enforce convergence to different models, addressing fairness differently. Another challenge is that existing methods commonly train the global model by minimizing the average training losses of all local clients [13], [10], [22]. However, these approaches lack performance guarantees for individual hospitals as they prioritize average training results, leading to divergent performance across participants [8]. This issue is exacerbated in real-world scenarios where data from medical centers differ in size and distribution [4]. Motivated by the aforementioned challenges, we introduce FedMRL, a novel framework that addresses these issues through three distinct components. Our main contributions are:\n- The FedMRL framework introduces a novel method for calculating adaptive \\(\\mu\\) values by leveraging the QMIX algorithm from Multi-agent Reinforcement Learning (MARL). This approach accounts for client-specific factors such as data distribution, volume, and performance feedback, facilitating dynamic regularization adjustments during FL training.\n- We propose integrating a novel loss function into the local objectives of each client to foster fairness among them. This aims to minimize the disparity"}, {"title": "2 Problem Statement", "content": "Assuming there are H hospitals, each represented by h\u2208 [1, 2, \u2026\u2026\u2026, H], and possessing privately labeled data denoted by Dh, the aim is to train a generalized global model over the combined dataset \\(D = \\cup_{H-1} D_h\\). The global objective function is represented in Eq. 1.\n\\[\\begin{equation}  \\arg \\min_w L(w) = \\sum_{h=1}^{H} \\sum_{}^{D_h} L_h(w) \\tag{1}  \\end{equation}\\]\nThe local objective function L\u2081(w) in client h, which quantifies the local empirical loss over the data distribution Dh, is represented in Eq. 2.\n\\[\\begin{equation}  L_h (w) = E_{x\\sim D_h} [l_h(W; x)] \\tag{2}  \\end{equation}\\]\nIn this context, lh denotes the loss function utilized by client h, and w represents the global model parameters. While the above fixed weighted averaging method offers an unbiased global model estimation in the presence of independent and identically distributed (IID) training samples across clients, non-IID distributions, stemming from device and user heterogeneity, lead to slower convergence and reduced accuracy [26]. To address this challenge, we propose FedMRL, integrating a novel fairness term into the local objective function, dynamically determining the proximal term for each hospital through a MARL-based approach, and employing a self-organizing map-based aggregation method at the server. The final local objective function is represented as shown in Eq. 3.\n\\[\\begin{equation}  \\arg \\min_w L(w) = \\sum_{h=1}^{H}  \\frac{D_h}{|D|} - L_h(w) + \\frac{\\mu}{2} \\lVert w - w^t \\rVert^2 + L_{fair}(w) \\tag{3}  \\end{equation}\\]"}, {"title": "3 Proposed Framework", "content": "In this section, we provide a comprehensive overview of our proposed approach FedMRL, which consists of three contributions such as calculating adaptive personalized \\(\\mu\\) Value, novel loss function, and server-side adaptive weight Aggregation using SOM. The overall algorithm is represented in Algo. 1. The architecture details are shown in Fig. 2."}, {"title": "3.1 Adaptive Personalized \\(\\mu\\) Value", "content": "For the dynamic adaptation of the proximal term \\(\\mu\\), we frame it as a multi-agent reinforcement learning problem, with each hospital h having an agent on the server side. Each agent h observes its state si from the overall environment state St, selects an action ai based on the current policy \\(\\pi_{\\tau}\\), and the agents' actions collectively form the joint action A. The environment transitions to the next state si+1 according to the state transition function P(si+1|si, ai), iterating until completion or predefined criteria are met. In the proposed work, we integrate QMIX [16], a prominent Q-learning algorithm for cooperative MARL in the decentralized paradigm that represents an advancement over Value-Decomposition Networks (VDN) [21]. Essentially, VDN assesses the influence of each agent on the collective reward, assuming that the joint action-value function Qtot (s, a) can be decomposed into N Q-functions for N agents, with each Q-function relying solely on local state-action history, represented in Eq. 4.\n\\[\\begin{equation}  Q_{tot} (s, a) = \\sum_{j=1}^{N} Q_j (S_j, a_j, \\theta_j) \\tag{4}  \\end{equation}\\]\nState: The state of the environment at round t is st = [St,1, St,2,\u2026\u2026, St,h] represents the data distribution among clients and performance feedback st,i is defined in Eq. 5.\n\\[\\begin{equation}  S_{t,i} = (E_c, P_c, acc_c, loss_c) \\tag{5}  \\end{equation}\\]"}, {"title": "3.2 Loss Function", "content": "Our proposed novel loss function in FedMRL effectively mitigates the impact of distribution shifts in hospitals' datasets, ensuring fairness and consistent performance across all participating institutions. By incorporating a fairness term into the local objective function of individual hospitals, FedMRL adjusts model parameters to achieve uniform training loss across all H hospitals, inspired by the Mean Square Error (MSE) loss function commonly used in regression models. The fairness term is formulated as an optimization problem aiming to minimize the sum of squares of differences in loss between each of the H hospitals and the global loss, as presented in Eq. 12.\n\\[\\begin{equation}  L_{fair} = \\sum_{h=1}^{H} \\sum_{}^{} (F_h(W) - F(w))^2 \\tag{9}  \\end{equation}\\]"}, {"title": "3.3 Server side Weight Aggregation", "content": "FedAvg uniformly averages client model updates, neglecting individual data distributions, which can impede performance in non-IID scenarios. In contrast, SOM-based weight adjustment considers client model similarity to the global model, significantly impacting clients with more representative data [7]. This adaptivity effectively addresses non-IID distribution challenges, allowing personalized adjustments based on model-global similarity, thus enhancing performance for clients with unique data distributions. We initialize the SOM grid shape as (5,5), and its weights are randomly initialized. Distances between the SOM weights and each hospital's local weights determine the Best Matching Unit (BMU) on the SOM grid. The influence of local weights on each SOM neuron is calculated based on its distance to the BMU and current sigma value, updating the neuron weights accordingly. Weights for each local model are computed from the SOM weights and similarity metrics, ensuring accurate representation through normalization. Cosine similarity metrics between local and global models, combined with distances, determine weights(ai) for each local model, favoring higher similarity for increased weight. During SOM weight updates, the influence of each local model scales by its similarity metric, with higher similarity models exerting greater impact. Normalized similarity metrics ensure proportional weighting, are responsive to changes in local models over time, and maintain fairness in highly non-IID data settings. The aggregation is performed according to Eq. 10, where wt+1 denotes the global model parameters for the"}, {"title": "4 Dataset and Experimental Results", "content": "In this section, we present the datasets used for the experiment and the experimental results of the proposed FedMRL."}, {"title": "4.1 Datasets", "content": "We have chosen two distinct benchmark datasets pertinent to real-world medical contexts to evaluate the effectiveness of our proposed FedMRL framework for highly heterogeneous scenarios. The ISIC 2018 dataset, notable for its contributions to skin cancer detection, provides a diverse array of dermoscopy images captured from various anatomical regions. This dataset encompasses 7,200 images categorized into 7 distinct classes [1]. Additionally, we utilized the Messidor dataset [2], consisting of 1,560 authentic fundus images tailored for grading diabetic macular edema across five severity levels. Both datasets were partitioned into 80% for training and 20% for validation to ensure robust evaluation."}, {"title": "4.2 Implementation Details", "content": "We construct non-IID data partitions following the methodology outlined in [14]. Using 80% of each dataset for training purposes, we organize the data based on their labels and segment each class into 200 shards. Subsequently, clients create local datasets by sampling from these shards according to the probabilities represented in Eq. 11.\n\\[\\begin{equation}  pr(x) =  \\begin{cases}   \\eta\\in [0,1], & \\text{if } x \\in \\text{ class j}, \\\\   N(0.5,1), & \\text{otherwise}.   \\end{cases}  \\tag{11}  \\end{equation}\\]\nThe client samples from a specific class j with a constant probability \\(\\eta\\), while samples from other classes follow a Gaussian distribution. Higher \\(\\eta\\) values indicate a greater concentration of samples in a specific class, resulting in more heterogeneous datasets. We have used \\(\\eta\\) = 1.0 for the experiment to access the performance of FedMRL. Following [23], We employ DenseNet121 [5] as the backbone. For Fedprox, we selected the proximal term \\(\\mu\\) from the set 0.001, 0.1, 0.4, and for Fednova, we chose the proximal SGD value from the set 0.001, 0.1, 0.2 to yield the best results."}, {"title": "4.3 Comparison with State-of-the-arts", "content": "To assess the efficacy of the proposed FedMRL methods, we compare them against four baseline approaches: FedAvg, FedProx, FedNova, and FedBN. Table 1 presents the results of all the models in terms of Accuracy (ACC), Area under the ROC Curve (AUC), Precision (Pre), Recall, and F1-score (F1). The results highlight FedMRL's superior performance over baseline algorithms, demonstrating significant accuracy improvements across both datasets. Specifically, FedMRL outperforms state-of-the-art methods by 0.92%, 0.81%, 7.64%, and 2.43% for the ISIC-2018 dataset, and by 3.59%, 1.43%, 13.65%, and 3.59% for the Messidor dataset, compared to FedAvg, FedProx, FedNova, and FedBN, respectively. These improvements benefit from our FedMRL scheme, which explicitly takes advantage of MARL to learn the data heterogeneity in the network and adaptively optimize the local objective of clients. Additionally, our novel loss function promotes fairness among clients, while the SOM-based adaptive weight adjustment method for aggregation enhances convergence to a better global optimum."}, {"title": "5 Conclusion", "content": "This study addresses the challenge of data heterogeneity in federated learning while ensuring fair contributions from the decentralized participants. Our framework demonstrates robustness to non-IID data distribution across clients and outperforms existing benchmarks in two medical datasets. While effective in mitigating the challenges posed by non-IID data distributions, FedMRL may encounter scalability issues when dealing with a large number of clients. Furthermore, the computational overhead associated with calculating personalized \\(\\mu\\) values and performing server-side adaptive weight aggregation using SOM may impose additional computational burdens, particularly in resource-constrained environments. In future work, we envision exploring distributed computing and resource-sharing models to alleviate the computational burden and aim to enhance FedMRL's scalability and efficiency while exploring its applicability in diverse domains."}, {"title": "6 Proof of Proposed Loss Function", "content": "When all H hospitals exhibit identical training loss, the optimal solution for the fairness term emerges. To demonstrate this, we minimize Lfair. Subsequently, we elaborate on the mathematical representation of the fairness term. Further, we discuss the enhancement in performance resulting from integrating the fairness term into the local training loss, as described by Eq. 12.\n\\[\\begin{equation}  L_{fair} = \\sum_{h=1}^{H} (F_h(W) - F(w))^2 \\tag{12}  \\end{equation}\\]"}]}