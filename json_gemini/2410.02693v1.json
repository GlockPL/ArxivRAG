{"title": "DISCOVERING CLUES OF SPOOFED LM WATERMARKS", "authors": ["Thibaud Gloaguen", "Nikola Jovanovi\u0107", "Robin Staab", "Martin Vechev"], "abstract": "LLM watermarks stand out as a promising way to attribute ownership of LLM- generated text. One threat to watermark credibility comes from spoofing attacks, where an unauthorized third party forges the watermark, enabling it to falsely attribute arbitrary texts to a particular LLM. While recent works have demonstrated that state-of-the-art schemes are in fact vulnerable to spoofing, they lack deeper qualitative analysis of the texts produced by spoofing methods. In this work, we for the first time reveal that there are observable differences between genuine and spoofed watermark texts. Namely, we show that regardless of their underlying approach, all current spoofing methods consistently leave observable artifacts in spoofed texts, indicative of watermark forgery. We build upon these findings to propose rigorous statistical tests that reliably reveal the presence of such artifacts, effectively discovering that a watermark was spoofed. Our experimental evaluation shows high test power across all current spoofing methods, providing insights into their fundamental limitations, and suggesting a way to mitigate this threat.", "sections": [{"title": "1 INTRODUCTION", "content": "The improving abilities of large language models (LLMs) to generate human-like text at scale (Bubeck et al., 2023; Dubey et al., 2024) come with a growing risk of potential misuse. Hence, reliable detection of machine-generated text becomes increasingly important. Researchers have proposed the concept of watermarking: augmenting generated text with an imperceptible signal that can later be detected to attribute ownership of a text to a specific LLM (Kirchenbauer et al., 2023; Kuditipudi et al., 2023; Christ et al., 2024). Major LLM companies have pledged to watermark their models (Bartz & Hu, 2023), and regulators actively advocate for their use (Biden, 2023; CEU, 2024). However, recent works have demonstrated targeted attacks on watermarks that allow for removing the watermark or impersonating it (spoofing) (Sadasivan et al., 2023; Jovanovi\u0107 et al., 2024; Gu et al., 2024; Zhang et al., 2024). This implies that watermarks are not as robust as initially thought (Kirchenbauer et al., 2024; Piet et al., 2023).\nRed-green watermarks A well-studied class of LLM watermarking schemes are Red-green wa- termarks. At each step of the generation process, using both a private key \\( \\& \\) and a few previous tokens (context), the watermark algorithm boosts a subset of green tokens, leaving other (red) tokens unchanged. Given a text, the detection first computes, using the private key \\( \\S \\), the color of each token. A high proportion of green tokens in this color sequence indicates that the text is watermarked.\nSpoofing attacks In spoofing attacks, a malicious actor (spoofer) generates text that is detected as watermarked without knowledge of the private key \\( \\S \\). Being able to generate spoofed text at scale poses a serious threat to the credibility of watermarks. Spoofed text can be falsely attributed to the model provider, causing reputational damage, or used as an argument to evade accountability (Zhou et al., 2024). Moreover, in the case of multi-bit watermarks that embed client IDs in generated text (Wang et al., 2024), spoofing attacks can be used to impersonate and incriminate a specific user. Current state-of-the-art spoofing techniques adhere to a common pipeline. First, the malicious actor queries the targeted model to build a dataset \\( D \\) of genuinely watermarked text. Then, either applying statistical methods (Jovanovi\u0107 et al., 2024), integer programming (Zhang et al., 2024), or fine-tuning on watermarked data (Gu et al., 2024), the spoofer learns how to forge the watermark and can generate watermarked text without additional queries to the original model (Step 1 in Fig. 1). In"}, {"title": "2 BACKGROUND", "content": "Given a sequence of tokens (text) from a vocabulary \\( \\Sigma \\), an autoregressive language model (LM) M outputs a logit vector \\( l \\) of unnormalized next-token probabilities, used to sample the following token. LM watermarking is a process of embedding a signal within the generated text \\( w \\) using a private key \\( \\xi \\) (often by modifying \\( l \\) or the sampling procedure, see below and \u00a76), such that this signal is later detectable by any party with access to \\( \\xi \\). In particular, a watermark detector \\( D_{\\xi} : \\Sigma^* \\rightarrow \\{0,1\\} \\) implements a statistical test with the null hypothesis \u201cthe given text was produced with no knowledge of \\( \\xi \\)\u201d. \\( D_{\\xi}(w) = 1 \\) implies that the null hypothesis was rejected, i.e., the text \\( w \\) is watermarked.\nRed-green watermarks We focus on the well-studied class of Red-green watermarks, introduced by Kirchenbauer et al. (2023); we review relevant follow-up work in detail in \u00a76. Let \\( w_t \\in \\Sigma \\) be the token generated by the LM at step \\( t \\), \\( h \\in \\mathbb{N} \\) the watermark's context size (we refer to \\( h \\) previous tokens \\( w_{t-h:t-1} \\) as the context), \\( \\xi \\in \\mathbb{N} \\) the watermark's private key, \\( H : \\Sigma^h \\rightarrow \\mathbb{N} \\) a hash function,"}, {"title": "3 CAN SPOOFING ATTEMPTS BE DISCOVERED?", "content": "In this section, we discuss discoverability of spoofing, introduce the problem statement of distinguish- ing \u00a7-watermarked and spoofed texts, and formalize it within a hypothesis testing framework (\u00a73.1). We then describe the intuition behind our approach (\u00a73.2), that we later present in detail in \u00a74.\n3.1 PROBLEM STATEMENT\nAs previously discussed, current spoofing methods (spoofers) are evaluated in terms of their success rate at generating high-quality watermarked text. We aim to initiate a deeper qualitative study of spoofers, trying to get better insight into how well they mimic watermarked texts, beyond simply fooling watermark detectors. Our hypothesis is that due to the bottleneck of learning from a dataset of watermarked text of limited size, these spoofers, despite adopting fundamentally different approaches, may all leave similar artifacts in spoofed texts. In particular, we ask:\nDo learning-based spoofing techniques leave discoverable artifacts in generated texts?\nShowing existence of such artifacts would provide valuable insight into the shared limitations of current state-of-the-art watermark spoofers. Moreover, reliably identifying them would enable us to distinguish between \u00a7-watermarked and spoofed texts, lowering the effective accuracy of spoofers, without compromising other desirable properties, as is often the case when trying to specifically design watermarking schemes more resistant to spoofing (see \u00a76).\nConcretely, we assume the perspective of the model provider with a private key \\( \\xi \\) and a model \\( M \\). We receive a text \\( w \\in \\Sigma^T \\) that is flagged as watermarked by our detector \\( D_{\\xi} \\), and aim to decide whether it was generated using our private key \\( \\xi \\), or by a spoofing method. We assume that our private key \\( \\xi \\) was not simply leaked; otherwise, spoofed texts are hardly distinguishable from \u00a7-watermarked texts."}, {"title": "4 DESIGNING A TEST STATISTIC", "content": "We proceed to introduce our test statistic \\( S \\), deriving fundamental results regarding its distribution under the independence assumption from Eq. (3b), and in the more general case where it may be violated (\u00a74.1). Then, we present two concrete instantiations of \\( S \\) and discuss their trade-offs (\u00a74.2).\n4.1 CONTROLLING THE DISTRIBUTION\nWe introduce the main results regarding the distribution of \\( S(\\Omega) \\) under the null hypothesis.\nColor-score correlation Let \\( w \\in \\Sigma^T \\), sampled from \\( \\Omega \\), denote the text of length \\( T \\) received by the model provider, \\( x \\in \\{0,1\\}^T \\), sampled from \\( \\mathcal{X} \\), denote its color sequence under \\( D_{\\xi} \\), and \\( y \\in [0,1]^T \\) denote a sequence of scores for each token sampled from a sequence of \\( T \\) random variables \\( Y \\). We defer the construction of \\( Y \\) to \u00a74.2, where we will build on the intuition from \u00a73.2. As the test statistic, we use the sample Pearson correlation coefficient between \\( x \\) and \\( y \\), defined as\n\\[ S(w) = \\frac{\\sum_{t=1}^T(x_t \u2013 \\bar{x}) (y_t \u2013 \\bar{y})}{\\sqrt{\\sum_{t=1}^T(x_t \u2013 \\bar{x})^2 \\sum_{t=1}^T (y_t \u2013 \\bar{y})^2}} \\]        (4)\nIndependence case We first study the distribution of \\( S(\\Omega) \\) under the assumption that \\( X_i \\) and \\( Y_i \\) are independent for all \\( i \\), as in Eq. (3b) (we refer to this as cross-independence between \\( X \\) and \\( Y \\)). From this assumption, we derive the following result:\nLemma 4.1. Under the cross-independence between \\( X \\) and \\( Y \\), and additional technical assumptions (detailed in App. D), we have that\n\\[ Z_S(\\Omega) := \\sqrt{T} S(\\Omega) \\sim \\mathcal{N}(0,1). \\]\nWe defer the proof to App. D for brevity. Therefore, given a text \\( w \\), we can compute a p-value using a two-sided Z-test on the statistic \\( Z_S(w) \\), which is sampled from a standard normal distribution. We will refer to this test as the Standard method.\nGeneral case In practice, however, the cross-independence assumption between \\( X \\) and \\( Y \\) does not always hold (see \u00a73.2). We make a modeling assumption motivated by the results from the independent case. Let \\( \\mu_{\\Omega} := \\mathbb{E}[S(\\Omega)] \\). Under the null hypothesis (and the practical considerations outlined below), we assume that\n\\[ \\sqrt{T} S(\\Omega) \\sim \\mathcal{N}(\\mu_{\\Omega}, 1). \\]        (5)\nCompared to Lemma 4.1, the difference is that the normal distribution is offset by \\( \\mu_{\\Omega} \\). This introduces a key challenge: finding a way to estimate \\( \\mu_{\\Omega} \\). To this end, we propose to use \\( w_{<c} \\), a prefix of \\( w \\) of length \\( c \\), to prompt our model \\( M \\) to generate a new sequence \\( w' \\) of length \\( T' := T \u2013 c \\) (which is a realization of \\( \\Omega' \\)). In practice, we set \\( c = 25 \\). Given the shared prefix, we expect that \\( \\Omega_{>c} \\sim \\Omega' \\) and hence that \\( \\mathbb{E}[S(\\Omega_{>c})] = \\mathbb{E}[S(\\Omega')] = \\mu_{\\Omega} \\). Then we introduce the statistic \\( Z_R(\\Omega, \\Omega') \\), defined by\n\\[ Z_R(\\omega, \\omega') = \\frac{S(\\omega_{>c}) \u2013 S(\\omega')}{\\sqrt{1/(T \u2013 c) + 1/T'}} \\]        (6)\nUnder the null hypothesis, we have that \\( Z_R(\\Omega, \\Omega') \\sim \\mathcal{N}(0,1) \\), as \\( S(\\omega_{>c}) \\) and \\( S(\\omega') \\) are two independent samples from a normal distribution. Therefore, in the general case, at the cost of higher computational complexity (since we need to use the model to generate the new text), we can, as in the independent case, compute a p-value using a Z-test on the statistic \\( Z_R(W,W') \\), which is sampled from a standard normal distribution. We later refer to this test as the Reprompting method. For consistency, in Reprompting experiments in \u00a75, we use \\( T \\) to implicitly refer to \\( T \u2013 c \\)."}, {"title": "5 EXPERIMENTAL EVALUATION", "content": "In this section, we present the results of our experimental evaluation. First, in \u00a75.1, we empirically validate the normality assumptions from \u00a74.1. In \u00a75.2, we validate the control of Type 1 error and evaluate the power of the statistical tests from \u00a74 on both spoofing techniques introduced in \u00a72: Stealing (Jovanovi\u0107 et al., 2024) and Distillation (Gu et al., 2024). In \u00a75.3, we compare the test results across a wider range of spoofer LMs. In App. A, we show additional results with a different watermarked model \\( M \\), parameter combinations, and another prompt dataset."}, {"title": "6 RELATED WORK", "content": "Watermarks for LLM In the class of distribution-modifying watermarks (Kirchenbauer et al., 2023), many schemes have built on the core idea of red-green vocabulary splits (Kirchenbauer et al., 2024; Zhao et al., 2024; Lee et al., 2023; Wu et al., 2023; Yoo et al., 2024; Fernandez et al., 2023; Liu et al., 2023; Fairoze et al., 2023; Ren et al., 2024; Lu et al., 2024; Guan et al., 2024; Zhou et al., 2024). Another prominent approach to LLM watermarking are distortion-free watermarks (Christ et al., 2024; Kuditipudi et al., 2023; Hu et al., 2024) that aim to preserve the distribution of the LM.\nWatermark spoofing Spoofing attacks are considered a threat to watermarks as they can lead to falsely attributing text ownership to a model provider. Sadasivan et al. (2023) presented a proof- of-concept where a dataset is generated by querying the watermarked model, and then used to approximately reverse-engineer the watermark scheme, but did not provide a practically validated method. Follow-up works Jovanovi\u0107 et al. (2024) and Zhang et al. (2024) expanded on this idea, to develop practical methods for spoofing Red-green watermarks. While Jovanovi\u0107 et al. (2024) works across multiple watermarking schemes, Zhang et al. (2024) is restricted to the unigram scheme (Zhao et al., 2024). Additionally, Gu et al. (2024) introduced an alternative spoofing method which distills the watermark into the model weights by fine-tuning a model on a dataset of \u00a7-watermarked text.\nAdditionally, works such as Wu & Chandrasekaran (2024) do not focus explicitly on spoofing but could be easily adapted to the spoofing scenario. However, in contrast with the above spoofers which can produce arbitrarily many spoofed texts once the watermark is forged, such approaches have practical limitations as they require additional queries to the watermarked model at each step of spoofing, inflating the computational cost. Moreover, another range of spoofing attacks are \"piggyback spoofing attacks\" (Pang et al., 2024), where an attacker substitutes a few tokens in a genuinely watermarked sentence to produce a spoofed sentence, simply using the robustness of the watermarking scheme. Piggyback spoofing attacks lack flexibility to produce arbitrary text as they rely on the targeted model for generation. Finally, there are attempts to design watermark schemes that are more resistant to spoofing (Zhou et al., 2024), which often comes at the cost of other desirable scheme properties such as text quality.\nBroader work on LLM watermarking Other directions in the realm of LLM watermarking includes scrubbing attacks (Jovanovi\u0107 et al., 2024; Wu & Chandrasekaran, 2024; Chang et al., 2024), detection of the presence of a watermark (Tang et al., 2023; Gloaguen et al., 2024), and attempts to imprint the watermark into the model weights (Li et al., 2024; Creo & Pudasaini, 2024)."}, {"title": "7 CONCLUSION", "content": "In this work, building upon the intuition that spoofed text contains artifacts reflecting the partial knowledge of the spoofer, we successfully constructed rigorous statistical tests to distinguish between spoofed and genuine watermarked texts. The tests behave similarly across the two fundamentally different spoofers studied, and across a wide range of watermark settings. Our results show that spoofed text can be reliably distinguished from genuine watermarked text, with arbitrary accuracy given a long enough text, and highlight shared limitations of current state-of-the-art spoofers.\nLimitations While we can provide an experimental evaluation of power on current state-of-the-art spoofers, the proposed tests come with no theoretical guarantee of power. We build our tests on reasonable assumptions regarding the limitations of learning-based spoofing techniques. Yet, we hypothesize that spoofing techniques that adaptively learn the vocabulary split may avoid leaving similar artifacts in generated text. Designing such attacks can be an interesting path for future work."}, {"title": "A ADDITIONAL EXPERIMENTAL RESULTS", "content": "In this section, we conduct several thorough ablation studies. We evaluate the test using a different dataset as base prompts (App. A.1), with a different variation of the watermark scheme (App. A.2), and using another watermarked model (App. A.3). In all tested additional settings, the results are similar to those presented in \u00a75, which emphasizes the validity of the test and shows that the spoofing artifacts studied are a fundamental property of learning-based spoofers.\nUnlike in \u00a75, we generate 1,000 continuations per parameter combination for the ablation study. It means that on average we have 105/T samples per parameter combination."}, {"title": "B VALIDATING THE CONCATENATION PROCEDURE", "content": "In this section, we experimentally validate the claim that concatenating texts \\( w \\) according to the procedure from \u00a74.2 has no influence on the resulting distribution of the statistic.\nExperimental setup Let \\( \\mathcal{W} = (W_1, ..., W_n) \\) be a corpus of \\( n \\) texts of the same length, and \\( \\mathcal{W}' \\) the corresponding corpus of Reprompting texts of the same length \\( T \\). Let \\( \\mathcal{X}, \\mathcal{X}' \\in \\{0,1\\}^{n \\times T} \\) be the color matrices of the corpora, and \\( \\mathcal{Y}, \\mathcal{Y}' \\in [0,1]^{n \\times T} \\) be the associated (\\( h \\) + 1)-gram score matrices of the corpora. For permutations \\( \\sigma \\in S_{n \\times T} \\), we define \\( \\sigma(\\mathcal{X})_{i,j} = \\mathcal{X}_{\\sigma((i,j))} \\) and \\( \\sigma(\\mathcal{Y})_{i,j} = \\mathcal{Y}_{\\sigma((i,j))} \\).\nWe define \\( \\sigma(\\mathcal{W}) \\) as the shuffled corpus with the corresponding \\( \\sigma(\\mathcal{X}) \\) color and \\( \\sigma(\\mathcal{Y}) \\) score. Given \\( \\sigma \\in S_{n \\times T} \\), we test the hypothesis that shuffling has no influence on the distribution of \\( Z_R(\\mathcal{W}, \\mathcal{W}') \\),\n\\[ Z_R(\\sigma(\\mathcal{W}), \\sigma(\\mathcal{W}')) \\sim Z_R(\\mathcal{W},\\mathcal{W}'), \\]        (11)\nwhere \\( Z_R(\\mathcal{W}, \\mathcal{W}') := (Z_R(W_1,W'_1), ..., Z_R(W_n, W'_n)) \\). The shuffling operation can be interpreted as a concatenation of texts of length 1. Hence, if the shuffling has no influence, this implies that the"}, {"title": "C DEPENDENCE BETWEEN THE CONTEXT DISTRIBUTION AND THE COLOR", "content": "In this section, we study in detail the dependence between the color of token \\( w_t \\) and \\( I_D(w_{t-h:t}) \\) in \u00a7-watermarked text from \u00a73.2.\nProblem statement We recall that \\( D \\) is the training data of the spoofer, and \\( I_D \\) is the function of frequencies of \\( h \\) + 1-grams in \\( D \\). In \u00a73.2, we hypothesize that low entropy is a common factor that implies \\( I_D (\\Omega_{t-h:t}) \\) is high and \\( P(X_t = 1) \\approx \\gamma \\). Under such an assumption, we therefore expect the correlation between the observed color sequence \\( x \\) and the \\( (I_D(w_{t-h:t}))_{t \\in \\{h,...,T\\}} \\) to be negative. In other words, we expect \\( Z_S(w) \\) with the (\\( h \\) + 1)-gram score to be negative for \u00a7-watermarked text.\nResults We verify this claim by computing \\( Z_S(w) \\) with the (\\( h \\) + 1)-gram score for a corpus \\( \\mathcal{W} \\) of 1000 \u00a7-watermarked texts, each of length \\( T \\) = 500. In Fig. 7, we see the histograms of \\( Z_S(\\mathcal{W}) \\) for different values of \\( h \\). We see that for all \\( h \\), \\( \\mathbb{E}[Z_S(\\mathcal{W})] \\) is indeed negative. Furthermore, we notice that the histograms appear normally distributed, agreeing with the assumption underlying the Reprompting method (Eq. (5)). Therefore, these results show that the proposed intuitive explanation of the dependence due to \\( M \\) is coherent, and further highlight the need for the Reprompting method in order to build a statistic with a known distribution when using the (\\( h \\) + 1)-gram score."}, {"title": "D PROOF OF LEM\u039c\u0391 4.1", "content": "In this section, we detail the proof of Lemma 4.1.\nFirst, let's recall some statistical results that we need.\nTheorem D.1 (Lindeberg CLT). Let \\( X_{n,1}, ..., X_{nn} \\) be independent random variables in \\( \\mathbb{R}^d \\) with mean zero. If for all \\( \\varepsilon > 0 \\)\n\\[ \\sum_{k=1}^n \\mathbb{E}[||X_{n,k}||^2 \\mathbb{1}\\{||X_{n,k}|| > \\varepsilon\\}] \\rightarrow 0, \\quad \\text{(Lindeberg Condition)} \\]        (12)\nand\n\\[ \\sum_{k=1}^n Cov(X_{n,k}) \\rightarrow V, \\]        (13)\nthen\n\\[ \\sum_{k=1}^n X_{n,k} \\xrightarrow{d} \\mathcal{N}(0, V). \\]        (14)\nTheorem D.2 (Delta method). Let \\( X_1, ..., X_n \\) be a sequence of random variables in \\( \\mathbb{R}^d \\), if\n\\[ \\sqrt{n}(X_n \u2013 \\mu) \\xrightarrow{d} \\mathcal{N}(0, V), \\]        (15)\nand \\( u : \\mathbb{R}^d \\rightarrow \\mathbb{R} \\) is differentiable at \\( \\mu \\), with \\( \\nabla u(\\mu) \\neq 0 \\), then\n\\[ \\sqrt{n}(u(X_n) \u2013 u(\\mu)) \\xrightarrow{d} \\mathcal{N}(0, \\nabla u(\\mu)^T V \\nabla u(\\mu)). \\]        (16)\nNow we proceed to prove Lemma 4.1. We first state the result formally.\nLemma 4.1. Let \\( X := X_1, ..., X_T \\) be a sequence of independent (non i.i.d) Bernoulli random variables, and \\( g_i = \\mathbb{P}(X_i = 1) \\). Let \\( Y := Y_1, ..., Y_T \\) be a sequence of i.i.d. random variables. Let \\( \\Omega = (X, Y) \\). Assuming that, for all \\( i \\in \\{0, ...,T\\} \\), \\( X_i \\) and \\( Y_i \\) are independent, that there exist \\( g^{(1)}, g^{(2)} \\in [0, 1] \\) such that\n\\[ \\lim_{T \\rightarrow \\infty} \\frac{1}{T} \\sum_{i=1}^T (g_i \u2013 \\bar{g})^2 = 0 \\quad \\text{(1)} \\quad \\text{and} \\quad \\lim_{T \\rightarrow \\infty} \\frac{1}{T} \\sum_{i=1}^T \\bar{g}^2 = g^{(2)}, \\]        (17)\nand assuming that \\( Y \\) admits at least 4 moments \\( \\mu_{Y}, \\mu_{Y^2}, \\mu_{Y^3}, \\mu_{Y^4} \\). Then, we have that\n\\[ Z_S(\\Omega) := \\sqrt{T} S(\\Omega) \\xrightarrow{d} \\mathcal{N} (0,1) . \\]\nProof. Let \\( w_i := (X_i, Y_i, X_i^2, Y_i^2, X_iY_i) \\). Let \\( X_{n,k} = (w_i \u2013 \\mathbb{E}[w_i]) \\). We recall the definition of \\( S \\),\n\\[ S(\\Omega) = \\frac{\\sum_{i=1}^T(X_i \u2013 \\bar{X}_T) (Y_i \u2013 \\bar{Y}_T)}{\\sqrt{\\sum_{i=1}^T(X_i \u2013 \\bar{X}_T)^2 \\sum_{i=1}^T (Y_i \u2013 \\bar{Y}_T)^2}} \\]        (18)\n\n\\[ \\frac{\\sum X_iY_i \u2013 \\frac{1}{T} (\\sum X_i) (\\sum Y_i)}{\\sqrt{\\left(\\sum X_i^2 - \\frac{1}{T} (\\sum X_i)^2\\right) \\left(\\sum Y_i^2 - \\frac{1}{T} (\\sum Y_i)^2\\right)}} \\]        (19)\nwhere \\( \\bar{X}_T \\) denotes the mean of \\( X_{1:T} \\).\nThe proof goes as follows:\n*   First, we show that the sum of the covariance matrix of \\( X_{n,k} \\) converges (Eq. (13)).\n*   Then, we show that \\( X_{n,k} \\) satisfies the Lindeberg condition (Eq. (12)). We can then apply the Lindeberg theorem to show that \\( w_i \\) converges to a normal distribution.\n*   Finally, we apply the Delta method (Theorem D.2) to show that \\( S(\\Omega) \\) is normally distributed."}]}