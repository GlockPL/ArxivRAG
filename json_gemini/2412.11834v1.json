{"title": "Wonderful Matrices: Combining for a More Efficient and Effective Foundation Model Architecture", "authors": ["Jingze Shi", "Bingheng Wu"], "abstract": "In order to make the foundation model more efficient and effective, our idea is combining sequence transformation and state transformation. First, we prove the availability of rotary position embedding in the state space duality algorithm, which reduces the perplexity of the hybrid quadratic causal self-attention and state space duality by more than 4%, to ensure that the combining sequence transformation unifies position encoding. Second, we propose dynamic mask attention, which maintains 100% accuracy in the more challenging multi-query associative recall task, improving by more than 150% compared to quadratic causal self-attention and state space duality, to ensure that the combining sequence transformation selectively filters relevant information. Third, we design cross domain mixture of experts, which makes the computational speed of expert retrieval with more than 1024 experts 8 to 10 times faster than the mixture of experts, to ensure that the combining state transformation quickly retrieval mixture. Finally, we summarize these matrix algorithms that can form the foundation model: Wonderful Matrices, which can be a competitor to popular model architectures.", "sections": [{"title": "1 Introduction", "content": "The backbone of modern foundation models usually consists of two main parts: one is sequence transformation, which assigns dependencies to elements; the other is state transformation, which assigns knowledge information to elements.\nIn the sequence transformation part, efficient algorithms aim to compress element dependency information in a limited state, while effective sequence transformation algorithms aim to store all element dependencies. Transformer (Vaswani et al. 2017) Architecture is popular in modern language modeling, it directly captures the relationship between any two elements in the sequence by calculating the causal mask matrix, which can effectively handle long-distance dependency problems. However, the architecture has a major drawback: the quadratic complexity of the Quadratic Causal Self-Attention in the sequence transformation part limits the ability to handle long contexts. State Space Model (Dao and Gu 2024) Architecture came into being, it balances the quadratic and linear calculation methods of relevant elements by calculating the semiseparable matrix, which can achieve linear scaling of sequence length during training and maintain a constant state size during generation. However, the architecture also has a major drawback: the dependency state of the State Space Duality in the sequence transformation part does not expand with the sequence length to cause dependency bias.\nIn the state transformation part, efficient algorithms aim to sparsely activate knowledge parameters related to elements, while effective algorithms aim to densely activate knowledge parameters related to elements. Gated Multi-Layer Perceptron (Shazeer 2020) consists of a Linear layer with dense activation and an activation function, it controls the flow of information through gate units, which can suppress the output of certain neurons. However, the structure has a major drawback: each output unit of the Linear layer receives information from all input units, causing the computational complexity to increase with the number of units, leading to difficult to expand. Then a series of sparse mixture of experts structures appeared, among which the most efficient is the Mixture of A Million Experts (He 2024) mainly composed of embedding layers and activation functions, which maintains computational efficiency through parameter-efficient expert retrieval. However, the structure also has a major drawback: one input unit of the Embedding layer only activates one output unit, causing the sharing ratio to not increase with the number of units, leading to redundancy to stored."}, {"title": "2 Related Work", "content": "Quadratic Causal Self-Attention. Self-Attention is a mechanism that computes the relevance scores between each element in the sequence and all other elements, allowing each element to \"attend\" to other elements. The most important variant of attention is the Quadratic Self-Attention. A notable feature of Quadratic Self-Attention is that it can capture dependencies between any positions in the input sequence, without being limited by distance, and the state expands with the sequence length, which gives it an advantage in capturing long-range dependencies in long sequences. In causal language modeling, a causal mask matrix is usually added to the attention score matrix to prevent future information leakage. We refer to it as Quadratic Causal Self-Attention.\n\\(Y = softmax(QK^T) \\cdot V\\)\nState Space Duality. Many variants of Quadratic Causal Self-Attention are proposed based on the calculation improvement of the attention score matrix. The most important variant is linear attention (Katharopoulos et al. 2020), which rewrites \\((QK^T) \\cdot V = Q \\cdot (K^TV)\\) by folding softmax into the kernel feature map and using the kernel properties of matrix multiplication. In the case of causal attention, they show that when the causal mask is merged to the left \\((L \\circ QK^T) \\cdot V\\), where L is a lower triangular matrix, the right side can be expanded into a recursive form, allowing attention to perform linear autoregressive reasoning. In Transformers are SSMs (Dao and Gu 2024), the State Space Duality is used to prove that by implementing the semiseparable matrix \\(M = L \\circ CB^T = L \\circ QK^T\\) and performing quadratic matrix-vector multiplication, the result is equivalent to quadratic causal kernel attention.\n\\((L \\circ QK^T) \\cdot V = (L \\circ CB^T) \\cdot X\\)\nRotary Position Embedding. Position information is very important in language modeling, and there are three mainstream relative positional encodings: convolution, recursive, and inner product. Rotary Position Embedding (Su et al. 2021) adds absolute position information m and n to the Q and K matrices in the Quadratic Causal Self-Attention, and calculates the inner product of QKT to obtain the relative position encoding matrix.\n\\(< f(Q, m), f(K, n) > = g(Q, K, m \u2013 n)\\)\nShared Expert Isolation. The sparse activation mixture of experts architecture aims to train a larger model in fewer training steps with limited computational resources, which often performs better than training a smaller model in more steps. In the routing expert strategy, to ensure that the experts learn non-redundant general knowledge, Shared Expert Isolation (Dai et al. 2024) shares knowledge by isolating k experts e(x), adding the entire sequence state of the isolated experts to the state of each token of the routing expert.\n\\(e(x) = \\sum_{i=1}^{k}e_i(x_i)(x)+\\sum_{i=1}^{n-k}(x) + e_i(x)\\)\nParameter Efficient Expert Retrieval. In knowledge-intensive modeling tasks, the finer the granularity of the sparse activation mixture of experts, the lower the model perplexity, but the retrieval time of the routing expert strategy will also increase significantly. Mixture of A Million Experts (He 2024) proposes parameter-efficient expert retrieval, which maintains computational efficiency with a large number of experts.\n\\(e_i(x) = \\sigma(d_x)u_i\\)"}, {"title": "3 Methods", "content": "Wonderful Matrices is a foundation architecture designed to build efficient and effective models.\nRotary Position Embedding for Hybrid Algorithms. First, we prove the availability of Rotary Position Embedding in the hybrid State Space Duality and Quadratic Causal Self-Attention algorithms. Ensuring that the position encoding is consistent for the hybrid algorithm, whether it is training or inference.\nDynamic Mask Attention. Second, we propose Dynamic Mask Attention with the same selectivity as State Space Duality. Ensuring that Quadratic Causal Self-Attention can selectively filter past states related to the current state, directly masking irrelevant states in the attention score matrix.\nCross Domain Mixture of Experts. Third, we design Cross Domain Mixture of Experts composed of Embedding layers and Linear layers. Ensuring that in dense knowledge tasks such as language modeling, parameters can be fully utilized to store general knowledge and domain-specific knowledge.\nArchitecture Design. Finally, we combine Rotary Position Embedding, State Space Duality, Dynamic Mask Attention, Cross Domain Mixture of Experts to design the Wonderful Matrices architecture in the language modeling task."}, {"title": "3.1 Rotary Position Embedding for Hybrid Algorithms", "content": "For example, in the Self-Attention QKT, the dot product of two vectors Qm \u00b7 Kn is calculated, and the result is a scalar, which represents the correlation between position m and position n. The basic idea of rotary position embedding is to encode the position information as a complex rotary matrix, whose angle is determined by the position index. When QK or CB is applied with the Rotary Position Embedding, if an element position is close to the front, its rotation will affect the direction of the K or B vector multiplied with it, thereby affecting the result of the inner product.\nThe first step is to define the absolute position information embedding function 1. We define four functions to add absolute position information, where the m-th position of the Q and C matrices adds absolute position information m, and the n-th position of the K and B matrices adds absolute position information n. Where @ is the rotation angle.\n\\(f(q, m) = qe^{im\\theta} \\quad f(k, n) = ke^{in\\theta} \\quad f(c, m) = ce^{im\\theta} \\quad f(b, n) = be^{in\\theta}\\) \n\nThe second step is to define the score algorithm of the rotary position encoding for hybrid algorithms 2. In Appendix A, we prove how to achieve rotary position encoding in the semiseparable matrix used in State Space Duality. Then we can use the same method to calculate the score with relative position information for each position of QK or CB. Where \\(R_\\Theta\\) is the rotation matrix, \\(\\Theta\\) is the rotation angle set.\n\\(attn_{score} = f(q, m)R_{\\Theta, m-n}f(k, n) \\quad ssd_{score} = f(c, m)R_{\\Theta, m-n}f(b, n)\\)\n\nThe final step is to apply the causal mask L to the score matrix after position encoding, and output the score matrix with position information to V and X 3 to extract features.\n\\(y = Attn_{score} L \\cdot V \\quad y = SSD_{score} L \\cdot X\\)\n\nIn addition, another important reason for unifying the position encoding of Quadratic Causal Self-Attention and State Space Duality into Rotary Position Embedding is to achieve effective position information for linear fast inference 4. Rotary Position Embedding is a way to achieve relative position encoding with absolute position encoding, without operating the score matrix, it is a relative position encoding method that can be directly used for linear attention, and linear attention"}, {"title": "3.2 Dynamic Mask Attention", "content": "In the Quadratic Causal Self-Attention algorithm, the Q and K related to the entire input sequence are calculated to form the attention score matrix, to extract information from the V related to the input sequence. Linear attention caches the hidden state of KV, only calculates the current Q state with the cached KV state, to achieve linear complexity of autoregressive inference. If there is a gate that can selectively filter the information related to the current state from the cached state, then the attention score mask can be dynamically adjusted.\nThe first step is to define the input-related projection function 5. We define four functions to project the input state, the projection function of Q KV is the matrix multiplication of the input x with the related matrix weight, while the projection function of the dynamic mask A is to use the zero-order hold technique. Here we explain some of the reasons for using the zero-order hold technique for dynamic masks. First, considering that the states usually contain continuous correlation in language modeling, to reduce the computational cost, we will retain its value every time we receive a continuous correlated state until we receive a new non-continuous correlated state. Second, to maintain this value over time, we introduce a learnable parameter W\u2081, which linearly projects the V state to directly obtain the correlation with the past cached state and the current input state. Third, we pass the time step through a non-negative function t\u25b3, because we cannot guarantee that the value domain of the learnable parameter A is non-negative. Finally, we calculate the time step and parameter A through the exponential function to achieve the zero-order hold technique.\n\\(f(x, W_Q) = xW_Q \\quad f(x, W_K) = xW_K \\quad f(x, W_V) = xW_V \\quad f(V_{cache}, V, W_\\Delta, A) = exp(t_\\Delta(concat(V_{cache}, V)W_\\Delta)A)\\) \n\nThe second step is to calculate the attention score and apply the causal mask 6. We first concatenate the current K state with the past K state, then perform dot product calculation with the current Q state to obtain the attention score. Finally, apply the causal mask L to the attention score to obtain the causal mask attention score matrix M.\n\\(M = f(x, W_Q) \\cdot concat(K_{cache}, f(x, W_K))^T \\circ L\\)\n\nThe final step is to apply the dynamic mask to the score matrix and output it to the value state 7. We first concatenate the past V state with the current V state, then perform zero-order hold operation with the time step weight W\u2081 and parameter A to obtain the dynamic mask, and finally apply the dynamic mask to the score matrix and perform matrix multiplication with the V state to obtain the final state. Here we explain some additional operations on how the dynamic mask is applied to the score matrix. First, the time-sampled state will be operated by a non-negative function, only the parameter A will have negative values during continuous learning. Second, after the \\(t_\\Delta A\\) state is operated by the exponential function, the negative values will be converted to values less than 1. Finally, if the causal mask is applied to the attention score using addition, then its value domain will be (-\\infty, 0], we can convert the part of the dynamic mask less than 1 to \u2013\u221e and apply it to the attention score. If the causal mask is applied to the attention score using multiplication, then its value domain will be [0, 1], we can directly perform element-wise multiplication between the dynamic mask and the causal mask, the part originally filled with 0 will not change, while the other parts can be dynamically attenuated or enhanced.\n\\(y = M f(V_{cache}, V, W_\\Delta, A) \\cdot concat(V_{cache}, f(x, W_V))\\)\n\nThis Quadratic Causal Self-Attention algorithm with dynamic mask can selectively filter the information related to the current state from the final superimposed state of self-attention, which can directly mask invalid states, and can attenuate or enhance some states, without causing decay or bias in past states."}, {"title": "3.3 Cross Domain Mixture of Experts", "content": "In the conventional mixture of experts strategy, the tokens assigned to different experts need to have common knowledge, so each expert may have redundant parameters for storing the same information. The proportion of expert parameter redundancy increases with the increase in expert granularity and the decrease in the number of expert activations. In Parameter Efficient Expert Retrieval, due to the high granularity of the experts, even if the expert weight rows are all shared, the proportion of expert parameter redundancy reaches an astonishing height due to the low number of expert column activations. If the tokens assigned to different experts have passed through the parameters for storing common knowledge, the parameter redundancy can be reduced.\nThe first step is to initialize all weights 8. We define query projection matrix weight \\(W_{eQ}\\), learnable key parameter \\(\\theta_{eK}\\), two Embedding matrices \\(W_{edown}\\) and \\(W_{eup}\\), and two Linear matrices \\(W_{cdup}\\) and \\(W_{cddown}\\). Where \\(\\sigma\\) represents the activation function, \\(d_{model}\\) represents the model hidden dimension, \\(d_{cd}\\) represents the cross domain dimension, \\(d_{ret}\\) represents the expert retrieval dimension, \\(n_e\\) represents the number of experts, and \\(n_h\\) represents the number of expert heads.\n\\(W_{eQ} \\in \\mathbb{R}^{d_{model}\\times n_h \\times d_{ret}} \\quad \\theta_{ek} \\in \\mathbb{R}^{\\sqrt{n_e} \\times \\sqrt{n_e} \\times d_{ret}} \\quad W_{edown}, W_{eup} \\in \\mathbb{R}^{n_e \\times d_{model}} \\quad W_{cdup}, W_{cddown} \\in \\mathbb{R}^{d_{cd} \\times d_{model}}\\) \n\nThe second step is to retrieve the expert state 9. First, perform matrix multiplication of the input x with \\(W_{eQ}\\) to obtain the query projection, and perform matrix multiplication with \\(\\Theta_k\\) to obtain the dot product similarity g. Then, take the topk expert scores and indices corresponding to each h in the \\(\\sqrt{n_e}\\) dimension of g, and combine them to obtain the similarity score s and expert index i. Finally, perform matrix multiplication of the index position i with \\(W_{edown}^T\\) and \\(W_{eup}\\), which is the embedding extension hidden dimension, to take out the weight rows of the expert dimension, obtaining the embedding states d and u corresponding to the two index positions.\n\\(g = x \\cdot W_{eQ} \\Theta_k \\in \\mathbb{R}^{2 \\times batch\\_seq \\times n_h \\times \\sqrt{n_e}}\\) \n\\(s, i = topk(g, k, dim = -1) \\in \\mathbb{R}^{batch\\times seq \\times n_h \\times k}\\)\n\\(d, u = i W_{edown}^T i\\cdot W_{eup} \\in \\mathbb{R}^{batch\\times seq \\times n_h \\times k \\times d_{model}}\\) \n\nThe final step is to mix the expert state with the cross domain state 10. First, perform matrix multiplication of the input state x with \\(d^T\\), then perform non-linear activation and multiply with the score s to obtain the expert weight w. The second step is to perform matrix multiplication of the expert weight w with u, obtaining \\(n_h \\times k\\) different expert states, and then summing in the \\(n_h, k\\) dimension to combine these different expert states to obtain the expert state q. The third step"}, {"title": "3.4 Architecture Design", "content": "We designed an architecture using these matrices in the language modeling task: Cheems. It first uses Word Embeddings to convert discrete vocabulary into continuous vectors, and outputs the vocabulary probability distribution after passing through Final Norm and LM Head. In the model backbone part, we use RoPE as the position encoding before each sequence transformation module, and use a CDMoE module as the state transformation after the sequence transformation, with input normalization and residual connection between each sequence transformation and state transformation. In the sequence transformation combination method, we stack 7 SSD modules for each stack, and stack 1 DMAttn module for each stack, to ensure the performance in-context learning."}, {"title": "4 Empirical Validation", "content": "4.1 Effect of Modules"}, {"title": "5 Discussion", "content": "In fact, we encountered many problems when completing this work, including various reasons that caused the mamba-ssm library to not work properly. Before solving this problem, we tried to directly remove the SSD sequence transformation module and modify the architecture to stack multiple MLP or CDMoE state transformation modules after a single DMAttn sequence transformation module as shown in Figure 9. We found that using this model architecture, ensuring that the parameter amount is equal to or less than other architectures for language modeling, there is no significant decrease in most verification metrics. We speculate that on the one hand, DMAttn allows Transformer and SSM to transform each other, and on the other hand, in the current Transformer architecture, there may be some redundancy in the Attn layer. Studying the impact of attention scores on the depth of the layer may be a research direction."}, {"title": "6 Conclusion", "content": "This paper explores the idea of modeling by integrating the state space dual algorithm with the quadratic causal self-attention algorithm. We studied the unified position encoding under the hybrid algorithm, proposed dynamic mask attention that can selectively filter information related to the current state, and designed cross domain mixture of experts to reduce parameter redundancy. Finally, this paper verifies that these algorithms achieve advanced performance in language modeling, promoting the development of language modeling in a more efficient and effective direction."}, {"title": "A ROPE for SSD", "content": "Proof of equation 2. by definition, \\(h_0 = B_0x_0\\). By induction,\n\\(h_t = A_t... A_1B_0x_0 + A_t... A_2B_1x_1 + ... + A_tA_{t-1}B_{t-2}x_{t-2} + A_tB_{t-1}x_{t-1} + B_tx_t\\)\n\\(= \\sum_{s=0}^{t}A_{t:s}B_sx_s\\)\nMultiplying by \\(C_t\\) to produce \\(y_t\\), and vectorizing the equation to \\(t \\in [T]\\) (T is the sequence length), we derive the matrix transformation form of SSD.\n\\(y_t = \\sum_{s=0}^{t}C_t A_{t:s}B_sx_s\\)\n\\(y = SSD(A, B, C)(x) = Mx\\)\n\\(M_{ji} := C_j A_{j:i}B_i\\)\nThen the matrix form of SSD is represented using SSS (Sequentially Semiseparable) as \\(M = SSS(A, B, C)\\), where \\(M_{ji} = C_j A_{j:i}B_i\\), and then considering A is just a scalar, rearranged as\n\\(M_{ji} = A_{j:i} \\cdot (C_j B_i)\\)\nVectorized as\n\\(L := 1SS(a)\\)\n\\(M = L \\circ (CB^T)\\)\nFinally, it is proved that the matrix transformation form of SSD is equivalent to Attention \\((L \\circ QK^T) \\cdot V = (L \\circ CB^T) \\cdot X\\).\nNow we have enough theoretical support to give rotational positional encoding to the C and B matrices in SSD.\n\\(C_m = f_c(x_m, m)\\)\n\\(B_n = f(x_n, n)\\)\n\\(C_m\\) represents the output weight matrix of the m-th token corresponding to the word vector \\(x_m\\) integrated with the position information m, \\(B_n\\) represents the input weight matrix of the n-th token corresponding to the word vector \\(x_n\\) integrated with the position information n.\nTo utilize the relative positional information between tokens, we assume that the inner product operation between the \\(C_m\\) vector and the \\(B_n\\) vector can be represented by a function g, where the input of the function g is the word embedding vectors \\(x_m\\) and \\(x_n\\), and their relative positional information m - n, the inner product of \\(C_m\\) and \\(B_n\\) and their relative positional information m - n is defined as\n\\(< f_c(x_m, m), f(x_n, n) > = g(x_m, x_n, m \u2013 n)\\)"}, {"title": "B.2 SSD", "content": ""}, {"title": "B.3 DynamicMaskAttn", "content": ""}, {"title": "B.4 Cross Domain Mixture of Experts", "content": ""}, {"title": "C.1 Multi-Query Associative Recall", "content": ""}, {"title": "C.2 Downstream Evaluation", "content": "To avoid score bias in downstream tasks due to different training data, we retrain four model architectures, including Llama3 using the QCAttn algorithm, Mamba2 using the SSD algorithm, Jamba using the hybrid of QCAttn and SSD, and our architecture. We train models of two scales, 360M and 1.3B, with parameters referenced in the table7."}]}