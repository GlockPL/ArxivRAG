{"title": "DLO: Dynamic Layer Operation for Efficient Vertical Scaling of LLMS", "authors": ["Zhen Tan", "Daize Dong", "Xinyu Zhao", "Jie Peng", "Yu Cheng", "Tianlong Chen"], "abstract": "In this paper, we introduce Dynamic Layer Operations (DLO), a novel approach for vertically scaling transformer-based Large Language Models (LLMs) by dynamically expanding, activating, or skipping layers using a sophisticated routing policy based on layerwise feature similarity. Unlike traditional Mixture-of-Experts (MoE) methods that focus on extending the model width, our approach targets model depth, addressing the redundancy observed across layer representations for various input samples. Our framework is integrated with the Supervised Fine-Tuning (SFT) stage, eliminating the need for resource-intensive Continual Pre-Training (CPT). Experimental results demonstrate that DLO not only outperforms the original unscaled models but also achieves comparable results to densely expanded models with significantly improved efficiency. Our work offers a promising direction for building efficient yet powerful LLMs. We will release our implementation and model weights upon acceptance.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) (Achiam et al., 2023; Team et al., 2023) have shown remarkable success across various natural language processing (NLP) tasks (Hadi et al., 2023; Tan et al., 2024; Li et al., 2024b,a), leveraging their vast capacity to capture complex patterns in data. Traditional scaling of these models has predominantly focused on horizontal expansion, as seen in Mixture-of-Experts (MoE) architectures (Shazeer et al., 2017; Fedus et al., 2022b; Lepikhin et al., 2020), where the width of the model is increased by adding more experts. This approach primarily optimizes parameter usage and computational cost by activating a fixed portion of parameters conditioned on the given input (Fedus et al., 2022a).\nHowever, the potential for vertical expansion remains underexplored. Inspired by how the human brain allocates more neurons for complex tasks and forms deeper neural chains (Baddeley, 1992; Koechlin et al., 2003), we propose focusing on vertical scaling. Our method dynamically expands, activates, or skips layers to optimize model depth and reduce redundancy, as shown in Figure 1 (a).\nThere are three critical challenges in vertically scaling LLMs: \u25cf Optimization Complexity. Dynamically adding or pruning layers making the process hard to optimize. Obtaining optimal such operations have been proved to be a NP-hard problem (Glorot and Bengio, 2010; Hestness et al., 2017), while an approximation method (Wang et al., 2023a) has shown compromised improvement. \u25cf Computation Cost. The inherent computational cost is associated with processing deeper networks. Each additional layer contributes to the overall latency and resource consumption. \u25cf Feature Collapse. Our analysis in Figure 1 (b) reveals that for a significant number of inputs, the representations across consecutive layers exhibit substantial similarity, suggesting that many layers may be redundant for certain samples.\nTo address these challenges, in this paper, we propose Dynamic Layer Operation (DLO), that consists of three operations: (i) expansion, (ii) activation, and (iii) skipping, for dynamic vertical scaling of LLMs without a proportional increase in computational cost. Our specific designs are as follows: \u25cf Expansion: Additional layers are dynamically expanded from existing ones, easing optimization complexity. \u25cf Activation & Skip: Feature Similarity guides the activation and skipping of layers. We propose similarity-induced labels to train the router that controls these operations. Adaptive FLOPs: Sparsity settings vary for layers facilitate adaptive FLOPs for different tokens, maintaining efficiency. Enhanced Generalizability: Layer-specific learning rates, based on sparsity, further improve the model's ability to generalize across tasks. Note that all modules are trained during the Supervised Fine-Tuning (SFT) stage, eliminating the need for Continual Pre-training (CPT) and simplifying the training process. Our primary contributions are as follows:\n\u2022 Method. We introduce a novel method, DLO, for dynamically scaling LLMs vertically by dynamically expanding, activating, or skipping layers.\n\u2022 Performance & Efficiency. Through rigoerous experiments, we demonstrate that DLO not only surpasses the performance of the original unscaled models but also achieves comparable results to densely expanded models with significantly enhanced efficiency.\n\u2022 Applicability. Fine-tuned on language understanding, math, and coding tasks, we manifest DLO's effectiveness across multiple NLP tasks."}, {"title": "Related Work", "content": "2.1 Mixture-of-Experts (MoE)\nMoE architectures have emerged as a promising approach for enhancing the efficiency and scalability of LLMs (Shazeer et al., 2017). Traditional neural networks activate all parameters for every input, leading to significant computational overhead, particularly as models scale up. In contrast, MoE models activate only a subset of parameters for each input, optimizing computational resource usage and enabling models to scale to billions of parameters without a corresponding increase in computational cost per input (Lepikhin et al., 2020; Fedus et al., 2022b; Zoph et al., 2022; Team, 2023a; ?). This selective activation makes MoE highly efficient for both training and inference by focusing on horizontal expansion and adding more experts. However, MoE's primary aim is to optimize width, potentially leaving layer redundancy unaddressed. Our Dynamic Layer Operation (DLO) approach complements MoE by focusing on vertical scaling through dynamic layer expansion and activation, targeting depth scalability and reducing potential feature redundancy.\n2.2 Efficient Model Stacking\nModel stacking is a common ensemble learning technique that improves predictive performance by combining multiple models to leverage their complementary strengths (Ting and Witten, 1997; Chen et al., 2015). In the context of LLMs, stacking can involve integrating various models into a hierarchical structure, where outputs from one model serve as inputs to another, capturing a broader range of features and patterns (Dabre and Fujita, 2019; Chen et al., 2021a; Wang et al., 2023b; Kim et al., 2023).\nRecent advancements have focused on progressively stacking pre-trained transformer or self-attention layers to create composite language models (Gong et al., 2019; Gu et al., 2020; Shen et al., 2022; Evci et al., 2022; Yao et al., 2023; Du et al., 2024; Wu et al., 2024). This approach reduce training costs by reusing pre-trained components. However, the increased depth and complexity of stacked models lead to high inference latency.\nTo mitigate this issue, layer-skipping methods have been developed, allowing models to \"early exit\" using additional layer-wise classifiers, thereby reducing the number of layers processed during inference (Wang et al., 2022; Chen et al., 2023; Zhang et al.). More recently, conditional computation techniques have been proposed to dynamically skip layers based on token-specific conditions, further enhancing efficiency (Ainslie et al., 2023; Raposo et al., 2024). However, these methods often require modifications during the pre-training stage, adding computation complexity and limiting their application to existing pre-trained LLMs. In contrast, our DLO method focuses on efficiency and scalability through dynamic vertical scaling within a single model during the SFT stage. It provides a comprehensive, high-performance solution to scaling LLMs without the extensive computational demands associated with stacked ensembles."}, {"title": "Methodology", "content": "In this section, we introduce the Dynamic Layer Operation (DLO) framework for efficienct vertical scaling of LLMs. DLO consists of three key operations: expansion, activation, and skipping. These operations dynamically adjust the model structure during the Supervised Fine-Tuning (SFT) phase to optimize computational efficiency and improve performance. A pseudo code style description is included in Appendix A.\n3.1 Layer Expansion\nTo facilitate dynamic depth adjustment, we introduce a group-based layer expansion strategy. Suppose the LLM has R transformer layers, which we group into P groups with Q layers each, such that R = P\u00d7Q. Each group is expanded to Q' = Q+q layers, where q is the number of additional layers introduced per group. The resulting number of layers will be R' = P \u00d7 Q'.\nLet Gi denote the i-th group with layers Li1, Li2, ..., Liq. The expanded group G will contain layers Li1, Li2,\u2026\u2026\u2026, Li(Q+q)\u00b7 The expanded layers are initialized using a policy II, and we consider several initialization strategies:\n\u2022 Random Initialization (Irand): Initialize the new layers' weights \\( \\theta \\) using Xavier initialization (Glorot and Bengio, 2010).\n\\begin{equation}\n\\theta'_{ij} \\sim \\mathcal{U} \\left(\\frac{-6}{\\sqrt{N_{in} + N_{out}}}, \\frac{6}{\\sqrt{N_{in} + N_{out}}}\\right),\n\\end{equation}\nwhere \u2200j \u2208 {Q+1,Q+2,...,Q+q}, \\( \\mathcal{U} \\) denotes the uniform distribution, nin is the number of input units, and nout is the number of output units in the layer.\n\u2022 Copy from Previous Layer (Icopy): Copy the parameters from the preceding layer.\n\\begin{equation}\n\\theta'_{ij} = \\theta_{i(Q+q-1)}, \\forall j \\in {Q + 1,..., Q + q}.\n\\end{equation}\n\u2022 Identity Initialization (Identity) (Wu et al., 2024): Copy from the preceding layer but set the output linear matrix of the multi-head self-attention (MHSA) to zero.\na. Copy the parameters of the previous layer:\n\\begin{equation}\n\\theta'_{ij} = \\theta_{i(Q+q-1)}, \\forall j \\in {Q + 1, ..., Q + q}.\n\\end{equation}\nb. Set the weights of the output linear layer Wout in the MHSA to zero: Wout = 0.\nIn this way, the output of the expanded layers will preserve the features from the original layers.\n\u2022 Linear Merge (\u03a0linear): Merge from the preceding \u03c4 layers using a linear function.\n\\begin{equation}\n\\theta'_{ij} = \\sum_{k=1}^\\tau \\alpha_k \\theta_{i(Q+q-k)}, \\sum_{k=1}^\\tau \\alpha_k = 1.\n\\end{equation}\n\u2022 Spherical Linear Interpolation (SLERP) (Islerp) (Shoemake, 1985): Merge from the preceding \u03c4 layers using SLERP. The SLERP method smoothly interpolates between two weight vectors on a unit sphere, maintaining constant velocity. The interpolation between two weight vectors u and v is defined as:\n\\begin{equation}\nSLERP(u, v, \\alpha) = u\\frac{sin((1 - \\alpha)\\Omega)}{sin(\\Omega)} + v\\frac{sin(\\alpha\\Omega)}{sin(\\Omega)}\n\\end{equation}\nwhere \u03a9 is the angle between u and v:\n\\begin{equation}\n\\Omega = arccos \\frac{u \\cdot v}{\\|u\\| \\|v\\|},\n\\end{equation}\nand \u03b1 \u2208 [0, 1] is the interpolation parameter.\nIn our context, for the new layer j, the parameters are initialized by interpolating between the weights of the previous layers \\( \\theta_{i(Q+q-1)} \\) and \\( \\theta_{i(Q+q-\\tau)} \\):\n\\begin{equation}\n\\theta'_{ij} = SLERP(\\theta_{i(Q+q-1)}, \\theta_{i(Q+q-\\tau)}, \\alpha),\n\\end{equation}\nwhere \u03b1 controls the interpolation. This ensures a smooth transition between layers, aiding in gradient flow and stable training.\nWe conduct comprehensive experiments on the choice of the policy II in Section 4.3.\n3.2 Layer Activation & Skipping\nDLO dynamically skips the multi-layer perceptron (MLP) module within the transformer layer Li for input tokens. To achieve this, we uses a linear router to determine the activation of layers. Suppose we have the set of token embeddings in a"}, {"title": "Training and Integration", "content": "sequence of length S for a given layer Li, that is h\u2081 = {hs \u2208 N*,s < S}, where in following contents we omit the superscript s for better readability. Considering feature redundancy, we use the router weights W\u1d62 to process the input token h\u1d62 and obtain the decision score r\u1d62, which is given by:\n\\begin{equation}\nr_i = \\frac{\\beta + (2\\sigma(h_iW_i) - 1)\\gamma}{2} \\in (\\frac{\\beta - \\gamma}{2}, \\frac{\\beta + \\gamma}{2}),\n\\end{equation}\nwhere \\( \\sigma \\) is the sigmoid function, \\( \\beta \\) and \\( \\gamma \\) are hyperparameters controlling the output range. During the inference stage, this score r\u1d62 determines whether layer L\u1d62 is active: the layer is activated if and only if \\( r_i \\geq \\frac{\\beta}{2} \\), otherwise it's skipped. The final activated output for the layer is:\n\\begin{equation}\nh_{i+1} = \\begin{cases} r_i M_i \\mathcal{A}_i(h_i) & \\text{if } r_i \\geq \\frac{\\beta}{2},\n\\\\  \\mathcal{A}_i(h_i) & \\text{otherwise,}\n\\end{cases}\n\\end{equation}\nwhere M\u1d62, \\( \\mathcal{A}_i \\) are the MLP and the attention modules within layer L\u1d62, respectively. This activation & skipping mechanism aims to encourage the utilization of the most relevant layers, thus reducing unnecessary computation. In this paper we initialize the router weights W\u1d62 as zeros and set \\( \\beta = 2.0, \\gamma = 0.05 \\), so that \\( r_i = 1.0 \\) on the first step and \\( r_i \\in (0.975, 1.025) \\) during training. This ensures benign initialization for activated tokens and avoids excessive disturbance on activated outputs brought by the decision scores.\n3.3 Training and Integration\nSimilarity-induced Label & Router Skip Loss. Given a pre-defined overall spasity \\( \\rho \\), we define a sparsity factor \\( \\rho_i \\) for each layer L\u1d62 that controls the layer-wise actived tokens. To determine the status of token h\u1d62, we utilize predicted router labels \\( \\tilde{\\Lambda}^s = {\\tilde{\\lambda}_1, ..., \\tilde{\\lambda}_i, ..., \\tilde{\\lambda}_{R'}}^s \\), which is obtained through the decision scores \\( r_i \\):\n\\begin{equation}\n\\tilde{\\lambda}_i^s = \\begin{cases} 1 & \\text{if } r_i \\in Top[(1-\\rho_i)s] ({r_i}_{i=1}^s),\n\\\\ 0 & \\text{otherwise.}\n\\end{cases}\n\\end{equation}\nwhere \\( \\tilde{\\lambda}_i^s = 1 \\) indicates layer L\u1d62 is predicted to be activated, and vice versa. To train the routers, we utilize the supervised router labels \\( \\tilde{\\Lambda}^s = {\\tilde{\\lambda}_1, ..., \\tilde{\\lambda}_i, ..., \\tilde{\\lambda}_{R'}}^s \\) to guide the learning of sparsity, i.e., to skip or not. The router labels \\( \\lambda^s \\) of layer L\u1d62 at training step t is determined by the following procedures:\n1. The cosine feature similarity of layer i is calculated through features across MLP M\u1d62:\n\\begin{equation}\n\\mu_i^s = \\frac{\\mathcal{A}_i(h_i) \\cdot M_i \\mathcal{A}_i(h_i)}{\\|\\mathcal{A}_i(h_i)\\| \\|M_i \\cdot \\mathcal{A}_i(h_i) \\|} \\in [0,1].\n\\end{equation}\n2. The similarity are sorted over all the layers, and the similarity-induced label is given as follows:\n\\begin{equation}\n\\lambda_i^s = \\begin{cases} 1 & \\text{if } \\mu_i^s \\in Bottom [(1-\\rho_i)R'S] ({\\mu_i}_{i=1}^{R',S}),\n\\\\ 0 & \\text{otherwise.}\n\\end{cases}\n\\end{equation}\nwhere Bottom \\( [(1-\\rho_i)R'S] \\) indicates the labels for tokens with the least \\( [(1-\\rho_i)R'S] \\) portion of cosine similarity are set to 1s, which are expected to be activated.\n3. A skip loss \\( \\mathcal{L}_{skip} \\) based on the Binary-Cross-Entropy loss LBCE is incorporated to guide the learning of the router attached to each layer:\n\\begin{equation}\n\\mathcal{L}_{skip} = \\frac{1}{R'S} \\sum_{i,s=1}^{R',S} \\mathcal{L}_{BCE} (\\sigma(h_iW_i), \\lambda_i^s).\n\\end{equation}\nGiven the task-specific loss Ltask, the overall loss function for DLO training is:\n\\begin{equation}\n\\mathcal{L} = \\mathcal{L}_{task} + \\mathcal{L}_{skip}.\n\\end{equation}\nSkip Rate Dynamics. The redundancy exhibits an imbalanced distribution across layers, as is evidenced in Figure 1 (b). To this end, we adjust the next-step \\( \\rho_{i,t+1} \\) for each training step t over the total T steps, where the initial skip rate \\( \\rho_{i,1} = \\rho \\). The layer-wise sparsity factor \\( \\rho_{i,t+1} \\) is calculated using the router labels as follows:\n\\begin{equation}\n\\rho_{i,t+1} = \\frac{\\sum_{s=1}^S \\tilde{\\lambda}_{i,t}^s}{S}.\n\\end{equation}"}, {"title": "Experiments", "content": "where \\( \\tilde{\\lambda}_{i,t}^s \\) is the supervised router label of the s-th token in layer L\u1d62 at step t. Additionally, we employ an annealing technique on the skip rate to ensure the warm start. During training, the overall skipping rate gradually increases from an initial low value \\( \\rho \\) to the target sparsity level \\( \\rho \\) over a predefined number of steps T'. The overall skip rate \\( \\rho^t \\) at step t is given by:\n\\begin{equation}\n\\rho^t = \\begin{cases} \\rho + (\\rho - \\rho) \\frac{t}{T'} & \\text{if } t \\leq T',\n\\\\  \\rho & \\text{otherwise,}\n\\end{cases}\n\\end{equation}\nwhere we set \\( \\rho = 0 \\). This annealing process helps the model to progressively adapt to higher sparsity levels with smoother changes, leading to more stable training and better convergence.\nLayer-Wise Learning Rates. DLO also employs layer-wise learning rates \\( \\zeta_{i,t} \\), adjusted based on sparsity to promote generalizability. The learning rate for each layer is defined as:\n\\begin{equation}\n\\zeta_{i,t} = \\zeta \\cdot \\sqrt{\\frac{1 - \\rho_{i,t}}{1-\\rho^t}},\n\\end{equation}\nwhere \\( \\zeta \\) is the base learning rate. It is noteworthy that all DLO components are trained during the Supervised Fine-Tuning (SFT) stage in an end-to-end manner, eliminating the need for Continual Pre-Training (CPT). By integrating DLO, we achieve dynamic vertical scaling, optimizing model depth, and maintaining high performance with reduced computational demands.\n3.4 Adaptive Inference-Time FLOPs\nDuring inference time, DLO uses layer-specific sparsity settings to maintain computational efficiency and ensure adaptive floating-point operations (FLOPs) for different tokens. In other words, the predicted sparsity pi will be determined completely by the router based on Equation (1)-(2). The adaptive FLOPs are computed as:\n\\begin{equation}\n\\text{FLOPS}_i = \\rho_i \\cdot \\text{FLOPS}_{full},\n\\end{equation}\nwhere FLOPSfull represents the FLOPs for a fully active layer. Since \\( \\rho_i \\) is predicetd based on each specific token, DLO acheive adaptive FLOPs that entails better generalizability.\n4 Experiments\nIn this section, we present an empirical evaluation of the proposed DLO framework, detailing the experimental settings, results, and analysis.\n4.1 Experimental Settings\nModel Selection. We utilize LLaMA2-7B (Touvron et al., 2023) as the primary backbone due to its open-source availability and extensive usage. It consists of R = 32 original transformer layers, which we group into P = 4 clusters, each containing Q = 8 layers. For layer expansion, we increase the group size to Q' = 10 layers, resulting in a dense model, LLaMA-DLO, with a total of 40 layers and 8 billion parameters. For comparison, we also employ LLaMA-Pro-8B (Wu et al., 2024), a competitive model trained with Continual Pre-Training (CPT) on specialized datasets. We demonstrate DLO achieves an optimal balance between performance and computational cost in Section 4.4.\nFine-tuning Details. Following common practices (Wu et al., 2024), we fine-tune using a mixture of five instruction tuning datasets: ShareGPT (Team, 2023b), EvolInstruct (Luo et al., 2023), SlimOrca (Team, 2023c), MetaMath (Yu et al., 2023), and Evol-CodeAlpaca (Team, 2022), with ShareGPT replicated three times, totaling approximately 1.44 million instances. We use a batch size of 128 and a maximum sequence length of 4,096 tokens. The learning rate is set to 2e \u2013 5 with a warmup ratio of 0.03 and cosine scheduling, and we utilize AdamW (Loshchilov and Hutter, 2017) as the optimizer. Flash Attention (Dao et al.) and bfloat16 mixed-precision training are adopted to accelerate training. Fine-tuning LLaMA-DLO under different skip ratios yields the sparse models, with each training run taking approximately 36 hours on eight NVIDIA A100 GPUs.\nEvaluation Benchmarks. We assess the fine-tuned models using the EleutherAI LM Harness (Gao et al., 2023) and BigCode Harness (Ben Allal et al., 2022) across three domains: 0 Language [ARC-C (Clark et al., 2018), GLUE (Wang et al., 2018), MMLU (Hendrycks et al., 2020), OBQA (Mihaylov et al., 2018), PIQA (Bisk et al., 2020), SQUAD (Rajpurkar et al., 2016), TruthfulQA (Lin et al., 2021), WinoGrande (Sakaguchi et al., 2021)], Math [GSM8K (Cobbe et al., 2021), MathQA (Amini et al., 2019)], and Code [HumanEval (Chen et al., 2021b), MBPP (Austin et al., 2021)]. Detailed metrics are in Appendix B.\n4.2 Overall Performance\n Dense Models' Superiority: Dense models, indicated by , generally outperform their sparse counterparts across most datasets. For instance, LLaMA-DLO models consistently achieve high average scores, such as 51.31 for LLaMA2-7B+SFT, compared to the baseline LLaMA2-7B's 41.38. This indicates that our DLO-expansion method enhances model performance significantly while leveraging additional parameters effectively. \u2793 Efficiency of Sparse Models: Sparse models, marked with , show a notable reduction in inference-time FLOPs while maintaining competitive accuracy. At 10% sparsity, the LLaMA-DLO model achieves an average score of 50.71 with 34.2T FLOPs, compared to the dense model's 51.31 with 36.5T FLOPs. This demonstrates the efficiency of DLO's activation and skipping operations in optimizing computational resources without significantly sacrificing performance. \u278f DLO-Expansion Advantages: Models expanded using DLO expansion with up to 8B parameters outperform the original LLaMA2-7B model across multiple metrics. For example, dense LLaMA-DLO-8B+SFT achieves a higher average score of 51.24 compared to LLaMA2-7B's 41.38, highlighting the effectiveness of vertical scaling through layer expansion in improving model capacity and performance. On the other hand Balanced Performance of Sparse Models: Sparse models with DLO's dynamic activation and skipping (0) provide a well-balanced trade-off between performance and computational efficiency. At 30% sparsity, LLaMA-DLO models maintain strong performance on tasks like GLUE (51.0 vs. 28.1) and HumanEval (50.5 vs. 21.8), while significantly reducing FLOPs. This makes them suitable for scenarios requiring computational efficiency without substantial performance loss. Effectiveness Inference Optimization: DLO demonstrates effective inference optimization. For instance, the dense LLaMA-DLO model with 8B parameters achieves lower inference-time FLOPs compared to LLaMA-Pro-8B (36.5T vs. 36.4T) while maintaining a competitive average performance (51.24 vs. 50.33). This highlights DLO's capability to enhance model efficiency without compromising accuracy. General Observations: Overall, the DLO framework successfully balances performance and efficiency across various datasets and tasks. The adoption of both expansion and skipping strategies enables LLaMA-DLO to achieve robust performance improvements while maintaining lower computational costs, suggesting that DLO is a viable approach for scalable and efficient LLM deployment.\nWe conduct further analyses of key components of DLO in the subsequent subsections.\n4.3 Ablation Studies\nInitialization Strategies for Expanded Layers. We explored the effectiveness of various layer initialization strategies for the expanded layers, as detailed in Section 3.1. Table 2 evaluates the impact of different initialization strategies on the performance of LLaMA-DLO models with 10% sparsity. The results highlight that the choice of initialization plays a critical role in determining model performance across various tasks.\nThe identity and copy initialization strategies demonstrate the most consistent and high-performing results, suggesting that leveraging existing layer information is beneficial for stabilizing and enhancing model performance. These methods help maintain coherence in the model's internal representations, leading to robust results across a wide"}, {"title": "Scalability", "content": "range of tasks, including GLUE and HumanEval. Interestingly, while linear and SLERP initializations were expected to offer smoother transitions and potentially enhance performance, their results were only moderately effective. This indicates that while sophisticated initialization techniques can offer benefits, they may not always outperform simpler strategies like identity and copy initialization, which directly utilize pre-existing model structures. Random initialization yields the lowest performance. The variability in task performance with this method highlights the challenges of using non-specific weights, which can lead to unstable and suboptimal model behavior, particularly in complex tasks like math and coding.\nOverall, the findings emphasize that initialization strategies that leverage prior information from existing layers tend to provide a better foundation for training expanded models, leading to improved performance. We thus choose Hidentity as the default initialization strategy.\nZeros Router Initialization & Score Rescaling.\nIn this experiment, we investigate the impact of zeros router initialization and score rescaling on mitigating performance degradation.\n\u25b7 Zeros Router Initialization. Initializing the router parameters to zero aims to start the model from a neutral state, avoiding any initial bias towards layer activation or skipping. This method allows the model to learn activation patterns from scratch without being influenced by predefined weights. Results in Table 3 indicate that this approach helps maintain balanced training dynamics and mitigates premature convergence, as reflected in the performance stability observed across tasks.\n\u25b7 Score Rescaling. Score rescaling adjusts the routing scores to maintain them within a specific range, typically 0 to 1. This adjustment is intended to preserve gradient flow and prevent extreme activations, ensuring that the model remains responsive to training signals. Our findings suggest that score rescaling helps avoid over-activation of layers, leading to more efficient use of the model's capacity.\nThe combined use of zeros router initialization and score rescaling appears to prevent performance degradation effectively. As shown in Table 3, models with these techniques generally achieve more consistent accuracy and efficiency across various tasks. These results suggest that careful initialization and rescaling strategies are beneficial for maintaining robust performance during adaptation.\nEfficient Expansion. In addition to the high-cost LLaMA-Pro approach (studied in Table 1), we compare our expansion method with two state-of-the-art efficient vertical expansion baselines: SOLAR (Kim et al., 2023) and Self-Duplicate Stack (SD-Stack) (Team, 2024). These two methods duplicate blocks of transformer layers and stack them together in a training-free manner. As shown in Table 4, the proposed DLO-expansion significantly outperforms both SOLAR and SD-Stack by a considerable margin. This highlights the critical role of Supervised Fine-Tuning (SFT) in adapting the expanded layers effectively. Unlike training-free approaches, DLO-expansion achieves a superior balance between training cost and performance.\n4.4 Scalability\nThe proposed LLaMA-DLO model surpasses the performance of the original dense LLaMA, while also achieving comparable results to the dense LLaMA-Pro. Notably, it does so at a significantly lower training cost by eliminating the need for expensive CPT. Additionally, LLaMA-DLO facilitates efficient inference through adaptively reduced FLOPs, making it a cost-effective choice for both training and deployment.\nFigure 5 illustrates the trade-off between model performance and both training and inference costs. LLaMA-DLO emerges as the optimal solution, achieving the best balance across these metrics. This demonstrates the model's scalability, ensuring that high performance is maintained while keeping computational costs manageable."}, {"title": "Conclusion", "content": "This paper presents LLaMA-DLO, a framework for efficient vertical scaling of LLMs that dynamically expands, activates, and skips layers to optimize computational resources. Our experiments demonstrate that LLaMA-DLO achieves performance on par with expensive dense expansion model like LLaMA-Pro, while significantly reducing training costs and enhancing inference efficiency. These results highlight LLaMA-DLO's potential as a cost-effective solution for scaling LLMs in various NLP tasks, offering a balanced approach between model performance and resource management."}, {"title": "Limitation Discussions & Future Work", "content": "Disentanglement of Routing Decisions and Rescaling Scores. Currently, the routing decisions and rescaling scores in our framework are interdependent, which may impact the model's accuracy. For instance, the skipped outputs are optimized to match the original outputs primarily through cosine similarity, which does not account for the difference in L2 magnitude. This discrepancy could potentially be mitigated by applying an additional rescaling factor to the skipped outputs, ensuring a better match in magnitude and improving the overall performance.\nImproved Supervision for Router Labels. The current method relies on cosine similarity for supervising router labels, which may not be the most effective approach. Exploring alternative supervision methods, such as task-specific metrics or direct gradients, could lead to more accurate router decisions. For example, drawing inspiration from works like Jiang et al. (2023) or employing gradient-based techniques similar to those used in network pruning, could enhance the router's ability to prioritize important tokens and improve performance.\nSkipping Attention Modules. This work primarily focuses on skipping MLP layers due to the observed instability in decoding when skipping attention modules, such as excessive repetition and degraded accuracy (e.g., achieving 0% accuracy on GSM8K). Future work could explore strategies to stabilize the skipping of attention layers, potentially improving model efficiency without compromising output quality significantly."}, {"title": "Ethical Statement", "content": "The development and deployment of large language models, including the LLaMA-DLO framework presented in this work, can raise important ethical considerations. Our research aims to enhance the efficiency and scalability of LLMs while maintaining high standards of responsibility and ethical practice. We recognize the potential impact of our work on various stakeholders and are committed to the following ethical principles:\nFairness and Bias Mitigation. We are aware that language models can inadvertently learn and propagate biases present in training data. Efforts have been made to ensure that LLaMA-DLO is trained on diverse and representative datasets to minimize the risk of bias. Effective ways to mitigate bias is a pressing problem worth further study.\nTransparency and Accountability. We strive to maintain transparency in our research and development processes. Detailed documentation and open access to our methodologies and results will be provided to allow for scrutiny and reproducibility. Accountability mechanisms are in place to ensure that any adverse effects of our technology are promptly identified and addressed.\nSocial Impact. The potential societal impact of LLaMA-DLO is carefully evaluated to prevent misuse or harm. We are committed to the ethical deployment of our technology, ensuring it is used for beneficial purposes such as advancing research, improving accessibility, and enhancing communication. We actively discourage and take steps to prevent the use of our models for malicious activities, misinformation, or any application that could harm individuals or society.\nContinuous Ethical Review. The ethical implications of our work are continually assessed to adapt to evolving norms and expectations. We engage with interdisciplinary experts and stakeholders to identify and address ethical concerns, ensuring that our research and its applications remain aligned with societal values and ethical standards.\nBy adhering to these principles, we aim to contribute positively to the field of artificial intelligence and ensure that the benefits of our research are realized in an ethical and responsible manner."}]}