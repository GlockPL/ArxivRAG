{"title": "The Built-In Robustness of Decentralized Federated Averaging to Bad Data", "authors": ["Samuele Sabella", "Chiara Boldrini", "Lorenzo Valerio", "Andrea Passarella", "Marco Conti"], "abstract": "Decentralized federated learning (DFL) enables devices to collaboratively train models over complex network topologies without relying on a central controller. In this setting, local data remains private, but its quality and quantity can vary significantly across nodes. The extent to which a fully decentralized system is vulnerable to poor-quality or corrupted data remains unclear, but several factors could contribute to potential risks. Without a central authority, there can be no unified mechanism to detect or correct errors, and each node operates with a localized view of the data distribution, making it difficult for the node to assess whether its perspective aligns with the true distribution. Moreover, models trained on low-quality data can propagate through the network, amplifying errors. To explore the impact of low-quality data on DFL, we simulate two scenarios with degraded data quality\u2014one where the corrupted data is evenly distributed in a subset of nodes and one where it is concentrated on a single node\u2014using a decentralized implementation of FedAvg. Our results reveal that averaging-based decentralized learning is remarkably robust to localized bad data, even when the corrupted data resides in the most influential nodes of the network. Counterintuitively, this robustness is further enhanced when the corrupted data is concentrated on a single node, regardless of its centrality in the communication network topology. This phenomenon is explained by the averaging process, which ensures that no single node\u2014however central\u2014can disproportionately influence the overall learning process.", "sections": [{"title": "I. INTRODUCTION", "content": "The Federated Learning paradigm leverages the use of multiple entities connected within a network, empowered with enough computational power to solve the task of knowledge extraction and model training. Each device extracts knowledge from locally available data by a training machine learning model and shares it with its neighbors, resulting in local knowledge spread across the network without the raw data leaving the device. Thus, problems that typically require access to all the data produced by the system, as done in centralized solutions, are solved using multiple learning devices trained in parallel on locally available data. Federated learning is studied within environments where it is not possible or convenient to share raw data. Healthcare facilities have strict privacy-related problems preventing them from sharing patients\u2019 data outside the infrastructure where they are collected or produced. Sharing locally trained neural networks has been one key solution to combine the knowledge coming from multiple sources and improve the predictions' performance [1] [2]. Internet of Things (IoT) networks usually have bandwidth limits that prevent sharing large volumes of data across the network. Sharing a compressed data representation may represent the only solution to learning a shared objective between devices using all the data available to the system. When computing power is not scarce and learning is required, decentralized learning could be a valid approach to avoid sending large quantities of data to a central server where learning is done via a centralized technique. Moreover, end users are increasingly concerned about privacy. Sharing only the knowledge extracted from the end devices without exposing the data used to extract that knowledge offers an effective alternative to centralized learning solutions that require all data to be accessible during training.\nCompared to a centralized solution where data is centrally collected and trained in the cloud, federated learning introduces a greater number of degrees of freedom. The devices in the network can be initialized either homogeneously or heterogeneously, and data distribution across devices can be either identically and independently distributed (IID) or non-IID. In non-IID settings, each node possesses a unique perspective, differing from its neighbors. When devices learn exclusively from their local data, generalization across the entire system suffers, as no single node has access to the full data distribution. In star-like topologies, a central entity\u2014typically called a Parameter Server (PS) can coordinate knowledge sharing, a paradigm known as centrally coordinated federated learning (FL, hereafter). Conversely, when network conditions or topology prevent central coordination, the system operates under a fully decentralized federated learning paradigm (DFL, for short). In this paper, we focus on fully decentralized learning systems with non-IID data distributions and homogeneous initialization, where no central coordination is available.\nPrevious research has demonstrated the robustness and resilience of decentralized learning to structural failures in network topology during training [3]. Specifically, DFL has shown that network disruptions have minimal impact on learning accuracy, provided that the remaining nodes retain sufficiently representative data. Moreover, most of the knowledge acquired before network disruptions is preserved even when some nodes become unreachable. Similarly, DFL can effectively handle highly skewed data distributions, as long as each node has access to a few representative examples [4]. However, to the best of our knowledge, the impact of low-quality data on DFL has not been systematically investigated. Building on these insights, this paper examines the robustness of decentralized federated learning from a data quality perspective, focusing on the widely used yet simple model aggregation method based on averaging. We address the following research questions:\n\u2022 RQ1: How sensitive is average-based decentralized federated learning to low-quality or corrupted data?\n\u2022 RQ2: To what extent is this sensitivity influenced by the underlying network topology?\nTo investigate these questions, we simulate decentralized environments where some nodes' local datasets contain low-quality samples. In real-world scenarios, such data degradation may arise from sensor noise, faulty data augmentation, or intentional adversarial interference aimed at disrupting the learning process. Regardless of the cause, these misleading samples (referred to as corrupted data) can affect DFL, and this paper aims to systematically assess their impact.\nWe generate malformed data using interpolations in the latent space of a pre-trained Generative Adversarial Network (GAN), following the method described in [5]. This interpolation approach enables targeted corruption of specific labels with varying intensities, providing a controlled environment to analyze its effect on model performance. We compare fully decentralized and federated learning approaches against a centralized baseline to assess their resilience to data corruption. Our experiments consider both balanced and imbalanced data distributions, including scenarios where central nodes (e.g., hubs) concentrate most of the available information. Additionally, we introduce increasing levels of feature corruption, ranging from a small fraction of the dataset up to 90%, to evaluate the impact on learning outcomes.\nOur key findings are the following:\n\u2022 Corrupted data primarily affects both the target and collateral classes, while overall accuracy remains largely stable, indicating minimal impact on other classes. Interestingly, corrupted samples from the target class that resemble the collateral class degrade the classifier's performance more on the collateral class than on the target class itself.\n\u2022 The distribution of bad data plays a crucial role in its impact: corruption spread across multiple nodes has a far more detrimental effect than when concentrated in a single node, regardless of that node's influence in the network.\n\u2022 When data corruption is unevenly distributed, decentralized learning proves to be more resilient than its centralized counterpart. While this does not definitively establish decentralized learning as the more robust approach, it highlights the need for further comparative analysis.\n\u2022 Federated learning demonstrates superior long-term robustness to data corruption compared to a fully decentralized learning setup."}, {"title": "II. RELATED WORK", "content": "We emphasize that this paper focuses on the susceptibility and robustness of decentralized learning to low-quality data distributed across the network topology. While this issue may bear some resemblance to adversarial machine learning, our perspective is fundamentally different, as we do not specifically address adversarial attacks. However, given the conceptual overlap, we include, alongside the relevant literature on decentralized learning, a brief overview of the literature on attacks against decentralized learning."}, {"title": "A. Decentralized learning", "content": "Most of the work studying the robustness of decentralized learning against data poisoning mainly focuses on attacks targeting federated learning and trying to compromise the model integrity by sending ad-hoc updates to the parameter server. Federated learning [6], which currently has some of its applications in the field of user keyboard input prediction [7] and healthcare [2], is a form of decentralized learning where the underlying topology is star-like and the central server coordinates the learning.\nDecentralized learning, more broadly, offers a solution to learn within networks of devices, each holding a unique dataset, where constraints prevent the sharing of raw data across the network. The constraints are usually due to bandwidth limits or privacy requirements. Briefly, each device in the network starts by training a local model for a few epochs on the locally available data. Then, the distilled knowledge, usually the locally trained model, is shared with the neighbors. Finally, each device collects the models received by its neighbors and merges them into a new, updated model. Three steps are outlined: i) local training, ii) synchronization, iii) update. The three phases are then repeated until all devices reach convergence and models do not change anymore. It must be specified that local learning is analogous to the centralized paradigm, with all the techniques available such as early stopping on validation data extracted from the local dataset. Within these settings, learning is still possible even with non-IID data distributions and heterogeneous initialization [4]."}, {"title": "B. Attacks against decentralized learning", "content": "Unlike adversarial attacks that target models at test time [8], our work focuses on the local training phase within a decentralized learning paradigm. Most prior research has concentrated on attacks against federated learning, along with corresponding countermeasures and detection mechanisms. In contrast, we aim to analyze overall network performance in a fully decentralized, uncoordinated, and potentially data-imbalanced scenario where nodes hold misleading and incorrectly labeled data.\nOur scenario aligns closely with the category of label flip attacks. In [9], the authors investigated a federated IID setting with a single malicious user who manipulated gradient updates sent to the parameter server. Their approach leveraged either an additional scaling factor or a custom loss function to maximize the stealth and effectiveness of the attack. In [10], the authors proposed VagueGAN, a modified GAN trained on an altered objective function that degrades the generator's capabilities. In their experiments, they augmented the local dataset of each malicious node by 10% using the pre-trained GAN and performed a label flip attack, converting all samples of class 6 to class 0 in the MNIST dataset. Their study examined scenarios with up to 30% malicious participants.\nSimilarly, [11] introduced PoisonGAN to address cases where an attacker lacks access to a pre-trained generator or the necessary data distribution for training it. Their approach leveraged the global model managed by the parameter server as a discriminator to bypass data access limitations. Our work departs from these studies by assuming that nodes have full access to a pre-trained generator, which they use to construct or augment local datasets before training begins.\nIn [12], the authors conducted an extensive study on label-flip poisoning in federated learning with IID data distributions. They examined the effects of poisoning by deploying multiple attackers at different time steps, specifically targeting selected labels. A comprehensive survey on poisoning attacks in federated learning is provided in [13]. Meanwhile, [14] explored fully decentralized learning systems by injecting adversarial samples to induce backdoor behavior, forcing the victim model to respond to specific image triggers with predetermined outputs. This type of attack, known as a backdoor attack, allows an adversary to embed a hidden objective in the model that can be exploited at test time. In contrast, our approach simply shifts the label of one class to another (specifically, class 4 to class 9) without introducing any additional objectives.\nUnlike [15], where malicious nodes execute adversarial code, our setting assumes that nodes have control only over their local dataset and do not run any harmful code. Lastly, we extend the ideas presented in [16], where the impact of corrupted samples in federated scenarios was analyzed, to the extreme case of a fully decentralized setting with imbalanced data distribution. In our scenario, samples from two easily confusable classes are further corrupted using a generative model and assigned to the wrong class.\nTo the best of our knowledge, this is the first study to jointly investigate the effects of data corruption on a complex network topology in fully decentralized federated learning."}, {"title": "III. SYSTEM MODEL", "content": "We start by defining a network G(V,E) where V denotes the set of nodes and E the set of edges. Nodes represent individual learning entities (e.g., devices in a network), and edges represent the connections between them. We will assume that nodes represent devices in a network. Each device follows a three-step algorithm in every communication round: i) the device trains a local model using its locally available dataset for a few epochs; ii) local model's parameters are sent to the neighboring nodes; iii) the models received from the neighbourhood are combined through an aggregation function and used to update the local model. A communication round consists of these three steps executed atomically. Decentralized learning proceeds through multiple communication rounds until the training converges across the network, i.e., the local model updates become negligible.\nThe overall dataset D can be denoted as the union of all local datasets:\nD ~ P = \u222aD\u1d62 ~ P\u1d62 with \u2229D\u1d62 = \u2205,\nwhere Di = {(xj, yj)}\u2c7c=1 represents the local dataset of node i, N is the total number of nodes in the network, and P is the global data distribution. Since local data distribution may vary across devices, we explicitly define P\u1d62, the distribution from which Di is drawn.\nAll devices share the same learning objective and underlying neural network architecture. However, local datasets may differ in size and label distribution. For instance, the hub (i.e., the most connected node in the network) may have a small, uniformly distributed dataset (di : P\u1d62 ~ P) or a large, imbalanced dataset. To isolate the effects of data corruption and avoid confounding factors introduced by model initialization, we assume homogeneous model initialization across devices.\nFollowing [17], let h\u1d62 be the model of device i at communication round 0, parameterized by w\u1d62. The device aims to minimize the local loss function l over its dataset Di:\nw\u1d62 = arg min \u2211 l(yk, h(xk; w\u1d62))."}, {"title": "A. Decentralized learning", "content": "We start by defining a network G(V,E) where V denotes the set of nodes and E the set of edges. Nodes represent individual learning entities (e.g., devices in a network), and edges represent the connections between them. We will assume that nodes represent devices in a network. Each device follows a three-step algorithm in every communication round: i) the device trains a local model using its locally available dataset for a few epochs; ii) local model's parameters are sent to the neighboring nodes; iii) the models received from the neighbourhood are combined through an aggregation function and used to update the local model. A communication round consists of these three steps executed atomically. Decentralized learning proceeds through multiple communication rounds until the training converges across the network, i.e., the local model updates become negligible.\nThe overall dataset D can be denoted as the union of all local datasets:\nD ~ P = \u222aD\u1d62 ~ P\u1d62 with \u2229D\u1d62 = \u2205,\nwhere Di = {(xj, yj)}\u2c7c=1 represents the local dataset of node i, N is the total number of nodes in the network, and P is the global data distribution. Since local data distribution may vary across devices, we explicitly define P\u1d62, the distribution from which Di is drawn.\nAll devices share the same learning objective and underlying neural network architecture. However, local datasets may differ in size and label distribution. For instance, the hub (i.e., the most connected node in the network) may have a small, uniformly distributed dataset (di : P\u1d62 ~ P) or a large, imbalanced dataset. To isolate the effects of data corruption and avoid confounding factors introduced by model initialization, we assume homogeneous model initialization across devices.\nFollowing [17], let h\u1d62 be the model of device i at communication round 0, parameterized by w\u1d62. The device aims to minimize the local loss function l over its dataset Di:\nw\u1d62 = arg min \u2211 l(yk, h(xk; w\u1d62))."}, {"title": "B. Dataset corruption", "content": "We extend the notation introduced in the previous section by defining C(\u00b7; I, s), a function that maps D to D, the corrupted dataset. In our case, D is just D being subject to a post-processing function I which targets a specific label class ct (target class), thus:\nC(D; I, s) = D = (((xi), yi) if y\u1d62 \u2260 ct,\n(xi, yi) otherwise.\nTo analyze the network's behavior under varying levels of corruption, we conducted experiments where samples for label ct were drawn either from D or D with percentage p gradually increasing from 10% to 90%. Given D = {xi, yi}1..n and D = {xi, yi}1..n we define our final dataset, from which local datasets are drawn, as:\nDp = {(xi, yi)}1..\u230ap*n\u230b \u222a {(xi, yi)}\u230ap*n\u230b..n\nOur downstream task is supervised image classification. Within this context, we pick two classes, cc and ct, and we corrupt the features of ct with an interpolation towards class cc. This corruption makes the learning model more prone to errors when discriminating between the two classes [19]. Interpolated images are built upon the latent representation of a pre-trained GAN [20], a special case of artificial curiosity [21] [22]. Let Enet and Dnet be the encoder and decoder of our model trained using an adversarial loss, respectively. For each sample xi subject to corruption, let xc, xt be samples randomly drawn from the collateral class c and target class t, respectively. We obtain the corrupted sample xi by combining their latent representation via a simple linear transformation with parameter \u03b1 \u2208 [0, 1] controlling the interpolation strength. We then used the interpolated sample in place of xi.\nxi = I(xi) = Dnet(\u03b1 * Enet(xt) + (1 \u2212 \u03b1) * Enet(xc))"}, {"title": "IV. EXPERIMENTAL SETTINGS", "content": "We run experiments for scenarios composed of 50 nodes both for the federated learning and the fully decentralized setting. In FL the topology is constrained to being star-like (with the coordinating server at the center). In our federated learning setup, we assume that all devices participate in every communication round, whereas some FL algorithms allow only a fraction of nodes to contribute at each round. This choice removes one degree of variability in our experiments, ensuring a consistent participation pattern across all nodes.\nIn the decentralised scenarios, we employ a Barab\u00e1si-Albert (BA) graph topology [23], which belongs to the family of scale-free networks. These networks are characterized by a few highly connected nodes, known as hubs, which play a crucial role in information spreading due to their extensive connectivity. Given their central position, hubs serve as our primary targets for the investigation, as their models are directly shared with numerous other devices. We generate BA graphs using the NetworkX Python library [24], setting the parameter m to 1, meaning that each newly added node connects to only one existing node. Because the BA model preferentially attaches new nodes to those with higher degrees, this results in a small number of well-connected hubs, while most nodes remain sparsely connected, often with just one link (because m = 1).\nIn both FL and DFL, nodes communicate exclusively with their neighbors in the network. However, in FL, each node has a single neighbor (the central server), whereas in DFL, nodes have varying numbers of neighbors depending on the degree distribution of the communication graph. The BA topology is particularly interesting for studying DFL because its heavy-tailed degree distribution (a common feature in many real-life networks, including social and technological systems [25]) introduces maximum heterogeneity in node connectivity and communication opportunities."}, {"title": "B. Dataset and data distribution", "content": "In our experiments we use the MNIST dataset [26]. Given its simplicity and well-defined classes, it provides an ideal testbed for analyzing and understanding the impact of bad data in a controlled manner. The MNIST dataset consists of 60,000 training samples of handwritten digits from 0 to 9 and a test set containing 10,000 samples. Each sample consists of a 28 \u00d7 28 image and a label reporting the digit number. The local datasets of each node are mutually exclusive partitions of the training set, the test set is common to all the nodes.\nWe investigated a scenario where low-quality data affects a specific class. To this aim, we perform label interpolation modifying images labeled as 9 to resemble those labeled as 4. We explore interpolation strength (i.e. \u03b1 in eq. 5) set to 0.5 and to 0.95, the latter basically resulting in a naive label flip. We refer to class 9 as the target class, as its samples are directly modified, and to class 4 as the collateral class, since the increasing resemblance of corrupted 9s to 4s may indirectly distort the learning process for class 4. The remaining classes are referred to as bystanders or uninvolved classes. While the parameter \u03b1 controls the extent to which corrupted 9s resemble 4s, the actual number of corrupted samples in class 9 is governed by parameter p \u2208 [0, 1]. Consequently, the impact of poor-quality data is maximized when both \u03b1 and p are close to 1, i.e., the totality of the target class sample is indistinguishable from those of the collateral class, but the labels remain unchanged, resulting in a sort of label flip."}, {"title": "C. Learning & simulation settings", "content": "Each node locally trains a CNN configured according to state-of-the-art practices with a cross-entropy supervised learning objective. The CNN consists of two convolution layers (kernel size is set to 5), each followed by max pooling and a ReLu activation function. Dropout interleaves the two layers and the output is given by two fully connected layers with a ReLu activation function and dropout applied between the two. We used the stochastic gradient descent with learning rate set to 1e-3 and momentum set to 0.9. In all our experiments we set the batch size to 32. We also set the local validation size to 20% of all the training set local to each node. We run simulations up to 1,000 communication rounds, with each node training for a maximum of 5 local epochs. Early stopping is applied to prevent local model degradation. We replicate our simulations varying both the seed governing graph generation and the seed controlling other sources of randomness beyond the network topology. We also present the results for a centralized benchmark, where the same CNN is trained on the overall dataset D."}, {"title": "D. Performance metrics", "content": "We evaluated each node's performance using a common test set shared across all devices. At each communication round, we collected the confusion matrix for each node and extracted two key metrics: accuracy and F1 score. Depending on the analysis, we present these metrics in different ways: averaged across all nodes, within the neighborhood of target nodes, or for a specific subset of nodes. We will specify the chosen approach as needed throughout the discussion. When not specified, the results we present in this paper report the mean value at each communication round together with the 95% confidence interval computed over the different seeds used."}, {"title": "V. RESULTS", "content": "Before proceeding with a detailed analysis, we we first highlight a key observation: our experiments indicate that decentralized federated averaging is largely robust to the presence of corrupted data in the system. Specifically, we found that data corruption effects become clearly pronounced when its strength is set to \u03b1 = 0.95. Consequently, unless stated otherwise, all results presented in this work have been obtained using this corruption strength. Refer to Figure 2 for a qualitative illustration of the impact of corruption for different \u03b1.\nWe begin by illustrating the impact of low-quality data over time in Figure 5, where we vary the dataset corruption percentage (p in Dp). The figure presents results for DFL in the most vulnerable scenario identified: balanced corrupted data distribution. The plot tracks performance at each communication round using three key metrics. The first is accuracy, which reflects the overall impact of the attack on all classes. The second is the F1 score for class 9, denoted as F1(9), quantifying degradation in the corrupted class. The third is the F1 score for class 4, denoted as F1(4), capturing potential misclassification side effects in the collateral class. With no corrupted data, the performance of decentralized learning soon approaches that of a centralized approach. However, as p increases, the impact of low-quality data for class 9 is clearly visible, and as expected, its severity increases with the corruption fraction p. As the proportion of bad samples grows, accuracy and F1 scores progressively decline. However, the overall accuracy experiences only a mild drop, with at most a 10 percentage point decrease compared to an uncorrupted scenario. Notably, the corruption of class 9 strongly affects class 4, leading to increasing misclassification, i.e., low F1(4). These results suggest that low-quality data primarily affects both the target class (class 9) directly and the collateral class (class 4) indirectly, while having minimal impact on the remaining classes.\nTable II presents the F1 score for the best-performing round under high corruption conditions, i.e., with a corruption strength of \u03b1 = 95% and p = 90% of class 9 samples globally corrupted. For comparison, the table also includes baseline performance in the absence of corruption. A key observation is that, in a corruption-free scenario, FL, DFL, and centralized learning achieve rather similar performance (highlighted in grey in Table II). However, when corrupted data is introduced, all learning strategies experience performance degradation, as indicated by the green-highlighted cells showing lower F1 scores. Interestingly, the impact of corruption varies depending on the distribution of corrupted data across nodes. In the unbalanced case (highlighted in light green), the performance drop remains relatively small, with a worst-case loss of 8 percentage points (pp) with respect to the corresponding configuration without corruption. In contrast, in the balanced corruption scenario (darker green), the degradation is much more severe, with class 4 (the collateral class) losing approximately 30 pp and class 9 (the target class) losing around 20 pp. Surprisingly, the performance drop is more severe for class 4 than for class 9. This suggests that classifying 4s becomes substantially more challenging due to the presence of corrupted samples labeled as 9s that visually resemble 4s. Conversely, corruption on 9s appears to act\u2014counterintuitively\u2014as a form of generalization, making it easier for the model to handle class 9. As a result, while the model does make mistakes on 9s, it makes significantly more errors on 4s, whose dataset remains unaltered yet is indirectly affected by the misleading samples introduced into class 9.\nFigures 6 and 7 report accuracy, F1(4), F1(9) for both FL and DFL. In the balanced bad data configuration (Fig. 6), we observe curves that are clearly separated for different values of p. From the temporal evolution standpoint, the figure highlights that FL also converges faster than DFL over time, especially for small p. In the unbalanced bad data scenario (Fig. 7), where corrupted samples are concentrated on a single node, performance differences are minimal for low to intermediate values of p, with FL and DFL exhibiting nearly identical behavior. For high p, DFL initially learns faster but is eventually overtaken by FL, which surpasses its performance at later stages. As previously noted, the unbalanced bad data case has a significantly smaller impact on overall performance, as the corruption remains localized and does not propagate widely across the network.\nTaken together, these results indicate that, for the same amount of bad-quality data, the impact is significantly greater when the corrupted samples are distributed across multiple nodes rather than concentrated within a single node, regardless of the node's centrality. Building on this finding, we further argue that having a neighbor with uncorrupted data helps mitigate the effects of corruption. Figure 8 presents an analysis of a DFL simulation with p = 0.9. The leftmost figure visualizes the network topology, where each node is colored based on its mean F1(4). Nodes circled in red represent devices that do not contain corrupted samples. The rightmost figure plots the mean F1(4) score across the entire network against the F1(4) scores of individual nodes, particularly those connected to nodes with uncorrupted data. The results clearly show that having at least one \u201cclean\u201d neighbor significantly mitigates the negative effects of corrupted data, leading to a local F1(4) score that exceeds the network-wide average. From this, we conclude that denser networks may exhibit greater resilience to low-quality data when corruption is confined to a subset of nodes. In such networks, uncorrupted nodes have more connections, potentially acting as stabilizing points in the aggregation process and mitigating the impact of corrupted data.\nThe increased robustness of federated learning compared to a fully decentralized approach becomes less noticeable in milder scenarios. By using a corruption strength of \u03b1 = 0.5 both presented similar convergence for the balanced and unbalanced scenario as shown in Figures 9 and 10.\nTo summarize, the following findings emerged from our observations:\n\u2022 The impact of corrupted data is most pronounced in the F1 scores for class 4 and class 9, while overall accuracy is only mildly affected, suggesting that other classes remain largely unaffected. Surprisingly, corrupted 9s that resemble 4s degrade the performance of the decentralized classifier more on class 4 than on class 9.\n\u2022 The same amount of bad data has a significantly greater impact when distributed across multiple nodes rather than concentrated in a single, even highly influential, node.\n\u2022 Decentralized learning under an unbalanced data distribution outperforms its centralized counterpart. While this does not necessarily imply that decentralized learning is inherently more robust, it warrants further investigation.\n\u2022 Federated learning exhibits greater long-term resilience to corrupted data compared to a fully decentralized approach."}, {"title": "VI. DISCUSSION AND CONCLUSION", "content": "n this work, we investigated the impact of corrupted data on decentralized federated learning (DFL) and compared its resilience to federated learning (FL) and centralized learning. Our evaluation was conducted on a realistic communication network modeled as a heavy-tailed Barab\u00e1si-Albert (BA) graph, with a focus on a standard image classification task. We simulated decentralized environments where some nodes' local datasets contained low-quality samples, mimicking real-world scenarios in which data degradation arises from sensor noise, faulty data augmentation, or intentional adversarial interference. To introduce controlled corruption, we leveraged interpolations in the latent space of a pre-trained GAN, enabling targeted corruption of specific labels with varying intensities. Our experiments examined both balanced and unbalanced corruption distributions, considering scenarios where corrupted data was either spread across multiple nodes or concentrated on a single node, including highly connected hubs. We also explored different corruption levels, ranging from minor perturbations to extreme cases where up to 90% of class 9 samples were corrupted.\nOur key findings indicate that corruption primarily affects the target and collateral classes while leaving other classes largely unaffected. Interestingly, misclassified target samples that resemble the collateral class degrade performance more on the collateral class than on the target class itself. Furthermore, we observed that the distribution of corruption plays a crucial role: when corrupted data is scattered across multiple nodes, the performance drop is significantly larger than when it is concentrated in a single node, even if that node is highly central. This suggests that having uncorrupted neighbors helps mitigate the negative effects of bad data. Additionally, we found that under an unbalanced data distribution, DFL can outperform a naive centralized approach in handling corruption. However, this does not necessarily indicate that decentralized learning is universally more robust; rather, its resilience depends on the corruption pattern and network structure, warranting further investigation. Lastly, while fully decentralized learning initially converges faster, FL exhibits greater long-term robustness, ultimately surpassing DFL in later communication rounds.\nOur findings provide new insights into the resilience of decentralized learning systems to low-quality data, emphasizing the strong interplay between network structure and the distribution of corrupted data. This interplay suggests potential design strategies for peer-to-peer overlays that enhance collaboration among nodes when network constraints allow. Future research should explore alternative aggregation strategies beyond simple averaging, extend the analysis to different network topologies, and evaluate additional datasets. Finally, given recent studies on the decentralized training of diffusion models to alleviate the computational burden of training infrastructure [27], we argue that a deeper understanding of how unbalanced settings impact performance could inform more effective data distribution strategies. This, in turn, may help mitigate the risks associated with training on unverified data sources, such as those scraped from uncontrolled internet environments."}]}