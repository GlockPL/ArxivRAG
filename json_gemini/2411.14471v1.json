{"title": "Leveraging Gene Expression Data and Explainable Machine Learning for Enhanced Early Detection of Type 2 Diabetes", "authors": ["Aurora Lithe Roy", "Md Kamrul Siam", "Nuzhat Noor Islam Prova", "Sumaiya Jahan", "Abdullah Al Maruf"], "abstract": "Diabetes, particularly Type 2 diabetes (T2D), poses a substantial global health burden, compounded by its associated complications such as cardiovascular diseases, kidney failure, and vision impairment. Early detection of T2D is critical for improving healthcare outcomes and optimizing resource allocation. In this study, we address the gap in early T2D detection by leveraging machine learning (ML) techniques on gene expression data obtained from T2D patients. Our primary objective was to enhance the accuracy of early T2D detection through advanced ML methodologies and increase the model's trustworthiness using the explainable artificial intelligence (XAI) technique. Analyzing the biological mechanisms underlying T2D through gene expression datasets represents a novel research frontier, relatively less explored in previous studies. While numerous investigations have focused on utilizing clinical and demographic data for T2D prediction, the integration of molecular insights from gene expression datasets offers a unique and promising avenue for understanding the pathophysiology of the disease. By employing six ML classifiers on data sourced from NCBI's Gene Expression Omnibus (GEO), we observed promising performance across all models. Notably, the XGBoost classifier exhibited the highest accuracy, achieving 97%. Our study addresses a notable gap in early T2D detection methodologies, emphasizing the importance of leveraging gene expression data and advanced ML techniques.", "sections": [{"title": "1 INTRODUCTION", "content": "Diabetes is a chronic metabolic disorder where the human body struggles to regulate blood sugar. More than 537 million individuals were diagnosed with diabetes in 2021, as indicated by recent data from the International Diabetes Federation, and more than 75% of them lived in low-income and middle-income nations. According to projections, by 2030 and 2045, there will be 643 million and 783 million such cases, respectively. There are three main types: Type 1, Type 2, and Gestational diabetes. Among these, T2D is the most common form. It is a condition caused by either resistance to the effects of insulin or inadequate insulin synthesis by the pancreatic cells. As per an article by the World Health Organization, T2D accounts for more than 95% of diabetes cases.\nPreventive measures, including maintaining a healthy weight, regular exercise, and consideration of genetic factors, can often deter the development of T2D. However, symptoms of T2D can initially be subtle and may take years to manifest fully. As a result, diagnosis may be delayed until after the disease has already begun, and in some cases, complications may have already developed. So timely identification of the condition is crucial in averting its severe consequences.\nGenetic predispositions affecting insulin production, insulin re-sistance, and lifestyle choices, including obesity, overeating, and inactivity, are the leading causes of T2D [22]. The traditional way of diagnosing T2D is based on clinical symptoms and blood glucose levels, which may not become apparent until the disorder has advanced considerably. As a result, there is an urgent need for more proactive and accurate approaches to diagnose this condition in the early stages. Using ML models enables timely intervention to reduce the risk of complications."}, {"title": "2 RELATED WORK", "content": "Several studies have investigated non-genetic variables such as blood sugar level, blood pressure, cholesterol, age, and physical activity status in detecting T2D. Fazakis et al. [5] assessed the WeightedVotingLRRFs ensemble model for T2D risk prediction, achieving an AUC of 0.884. Joshi and Dhakal [10] identified five major predictors of T2D with 78.26% accuracy: age, diabetes pedigree function, BMI, glucose, and pregnancy. They also gained a 21.74% cross-validation error rate.\nKarmand et al. [11] compared seven ML methods for T2DM prediction in an Iranian setting, with Gradient Boosting Machine outperforming others with an AUC of 0.75 and 0.76 for males and females respectively, and an F1 score of 0.33 for males and 0.42 for females. Furthermore, major features influencing T2DM prediction were identified using SHAP analysis, which included age, blood type, past hospitalization history, and sugar consumption.\nNipa et al. [17] achieved notable accuracies in outlier detec-tion using classifiers like Extra Tree (97.11%) and Multi-Layer Perceptron (MLP) (96.42%) and found stable predictive capabilities in methods like Light Gradient Boosting Machine and RF. They used patient data from Bangladesh's Sylhet Diabetes Hospital and the ML repository of the University of California, Irvine. Iparraguirre-Villanueva et al. [9] explored KNN (79.6% accuracy) and Bernoulli Na\u00efve Bayes (77.2% accuracy) to identify diabetes using the Pima Indian database. Aguilera-Venegas and their team [2] predicted T2DM incidence seven and a half years in advance by analyzing 18 socio-demographic and clinical factors, with RF attaining accuracy of 92.91%.\nMany researchers have explored the intersection of bioinformat-ics and ML to gain insights into the genetic factors underlying T2D. Saxena et al. [21] aimed to identify crucial metagenes by wielding LASSO feature selection together with five ML models, including ANN which exhibited excellent performance with a 95% AUC and achieved 73% accuracy in differentiating between glucose-tolerance and diabetics. These metagenes were enriched in terms relevant to diabetes along with being identified in earlier genome-wide asso-ciation studies of T2D. This study helps in understanding insulin resistance and T2D pathways. Xiaoting Pei et al. [19] used LASSO regression and RNA sequencing in mice lacrimal glands to identify marker genes associated with T2DM. Wild-type mice were used as controls, and twenty db/db (diabetic) mice were used as T2DM models. Five marker genes were chosen using LASSO regression out of 689 differentially expressed genes. Among these, Synm was downregulated, while Ptprt, Glcci1, Tnks, Elovl6 were upregulated in T2DM mice. This model demonstrated high predictive accuracy with 1.000 and 0.980 AUC respectively in the training and testing datasets. Nevertheless, it has several drawbacks, including a limited sample size that can restrict applicability and a focus on mouse lacrimal glands that might not apply to humans.\nIn a retrospective study by Amos Otieno Olwendo et al. [18], electronic health record (EHR) data from Nairobi Hospital were analyzed to identify confirmed cases of diabetes. Supervised and unsupervised learning algorithms were applied. RF demonstrated the highest accuracy at 95%. GB and MLP yielded 94% accuracy. Rai et al. [20] employed single-nuclei chromatin accessibility profiling to delineate cell-type-specific regulatory signatures in pancreatic"}, {"title": "3 PROPOSED MODEL", "content": "We used the GSE81608 [24] dataset from the Gene Expression Om-nibus (GEO) database [4]. This dataset contains RNA sequencing data derived from a study on 1600 human pancreatic islet cells, encompassing various cell types including \u03b1, \u03b2, \u03b4, and Pancreatic Polypeptide (PP) cells. These cells were obtained from both non-diabetic and Type 2 Diabetes (T2D) organ donors. This data was generated using the Illumina HiSeq 2500 as the sequencing platform. There were 949 T2D samples and 651 non-diabetic samples. Each sample in the dataset quantifies expression levels for 28089 genes.\nThe single-cell RNA sequencing technique helps to see how gene ex-pression varies between different cell types within the islet and how gene expression differs in healthy individuals compared to those with T2D. We downloaded both the normalized gene-level data and the complete metadata from GREIN for easy access and analysis [15]. The metadata contains sample details, such as condition, age, ethnicity, gender, and cell subtype.\nThe dataset obtained from GREIN was preprocessed using the PANDAS library. Techniques such as transposition and modification of column headers were applied to improve dataset organization. Then, the column containing sample conditions (Type 2 Diabetes or non-diabetic) from the metadata was merged with the gene expression data based on sample names to integrate sample conditions"}, {"title": "3.2 Classification Algorithms", "content": "In this study, our objective was to use a variety of commonly used ML models to obtain an optimized performance. We aimed to ex-plore decision-based, ensemble, and linear models to assess their efficiency. As such, we utilized six models for classification: DT, RF, LR, AdaBoost, GB, and XGBoost. Each method has a unique strength: AdaBoost highlights misclassified cases, GB optimizes per-formance through gradient descent, XGBoost iteratively increases performance, DT visualizes decision boundaries, RF combines tree predictions, and LR calculates probabilities. We have used different parameters and values for the classifier. Table 1 shows the classi-fiers' parameters and values.\nDecision Trees: Across regression and classification applica-tions, DTs are flexible and comprehensible ML instruments. Specif-ically in cases of binary classification, DTs work by repeatedly dividing the feature space according to feature thresholds. Decision nodes that match anticipated class outcomes are produced by this procedure, providing a clear understanding of the underlying pat-terns in the data. A particular feature is assessed at each decision node, directing the algorithm along branches according to the fea-ture's result. DTs are known for their simplicity and are excellent at illustrating intricate linkages in the data. Let T be the DT model. At each decision node i, the algorithm selects a feature fi and a threshold \u03b8i. The decision rule at node i can be represented as:\nif $x_f \\leq \\theta_i$ then $T(x) = T_L$ else $T(x) = T_R$\nWhere:\n\u2022 $x_f$ is the value of feature $f_i$ for the input x,\n\u2022 $\\theta_i$ is the threshold value,\n\u2022 $T_L$ and $T_R$ are the left and right child trees, respectively.\nThis process recursively runs until a stopping condition is satis-fied, such as reaching a tree depth of maximum value or no further improvement in classification performance.\nRandom Forest: RF is an ensemble learning method that com-bines multiple DTs to improve predictive accuracy and reduce over-fitting. Mathematically, let $T_1, T_2, ..., T_n$ represent the individual DTs in the RF ensemble. A random portion of the training data and a random subset of the features are used to train every $T_i$. The prediction of the RF ensemble $\\hat{y}_{RF}$ for a given input x is typically obtained by aggregating the predictions of individual trees.\nIn classification tasks, a common aggregation method is the majority vote, where the final predicted class $\\hat{y}_{RF}$ is determined by the class that receives the most votes from the individual trees:\n$\\\u00db_{RF}(X) = argmax_y \\sum_{i=1}^{n} (T_i(x) = y)$"}, {"title": "4 RESULTS AND ANALYSIS", "content": "We carefully examined the research findings in this section. We first evaluated the models' capacity for generalization before delving into the complex worlds of overfitting and underfitting. Using a careful combination of training and testing accuracy, we measured how likely each model was to fall into these traps. After that, we looked at more than just accuracy metrics. We started a thorough investigation, examining the models from the perspective of true"}, {"title": "4.1 Model Generalization Analysis", "content": "A detailed snapshot of the testing and training accuracies, as well as the corresponding execution times for each model, are shown in Table 2. This multifaceted dataset forms the basis of our analysis, providing insight into the inherent complexity involved in the models' execution and their effectiveness in classifying diabetics."}, {"title": "4.3 Agreement and Error Analysis", "content": "Given the similarity in accuracy scores between GB and XGBoost, we compared these two models using Cohen's Kappa statistic and Matthews Correlation Coefficient (MCC).\nIn terms of Cohen's Kappa, XGBoost attained a value of 94.27%. GB achieved a slightly lower value of 93.74%. Both values indicate a high level of agreement between the predicted classifications and the ground truth labels. However, XGBoost outperformed GB by 0.53%.\nThe Matthews Correlation Coefficient (MCC) considers true positives, false positives, and false negatives. XGBoost achieved an MCC value of 94.29%, while GB achieved a slightly lower MCC value of 93.81%. XGBoost's higher MCC value reinforces its superiority over GB in predictive performance.\nXGBoost is the best performer in our study, demonstrating its superior predictive accuracy. A confusion matrix shows how true and predicted classifications interact in Figure 4. The confusion matrix, which captures the complex balance between actual and expected outcomes, is a key performance evaluation tool in the classification models. True Positives (TP) are those cases in which the model agrees with the real positive cases and correctly pre-dicts positive outcomes. True Negatives (TN) represent the model's accurate detection of negative results, reflecting the real negative occurrences. False Positives (FP), or Type I Errors, happen when the model predicts positive results even though the real values are nega-tive. False Negatives (FN), also known as Type II Errors, on the other hand, occur when the model misidentifies positive outcomes and incorrectly labels them as negative. It is essential to comprehend these matrix elements to assess a model's predictive performance. As we evaluated the performance of our model, we found that it was quite accurate in classifying positive instances-177 cases were correctly classified as True Positives or TP. Similarly, the model successfully classified 134 data points (True Negatives, TN), indi-cating that it could identify negative instances. Nonetheless, there"}, {"title": "4.4 Discussion and Analysis", "content": "In this study, we delve into the realm of XAI by examining SHAP (SHapley Additive explanations) values, a popular method for ex-plaining individual predictions of ML models. Specifically, we focus on analyzing the SHAP values derived from a subset of the test data comprising the first 20 samples. We examined the SHAP values for X_test[0:20], a subset of the test data. For individual predictions, SHAP values explain how each feature affects the model's output. The test set's first 20 samples' SHAP values are computed in this instance. This produces a summary plot of the previously deter-mined SHAP values. The summary plot gives a general idea of the features' relative importance by showing the distribution of SHAP values for each feature across the samples. It aids in determining which features-and in what direction-are most important to the model's predictions. In the SHAP summary plot (figure 5), features' effects on the model's predictions are indicated by color coding: red indicates a positive impact, while blue signifies a negative effect. The features are arranged from highest to lowest importance, with \"HLA-A.3\" being the most important and \"GAS2\" being the least important.\nThe presence of red and blue colors for a single feature suggests that its influence on the model's predictions varies across different samples in the dataset. Higher feature values positively contribute to the model's output for some samples, resulting in red-coloured SHAP values. However, for other samples, the output of the model decreases with higher values of the same feature, represented by blue-coloured SHAP values. This variability often arises due to the nonlinear or complex relationship between the feature and the target variable.\nWe have compared our findings with several other noteworthy studies to place our work in the larger context of diabetic pre-diction studies (Table 5). Using a neural network model, Agliata et al. [1] obtained an accuracy of 86%. They were interested in people's health and risk factors, including blood pressure, age, gen-der, weight, cholesterol, and blood pressure. In a different study, researchers [8] used traditional contextual datasets that included demographic, physiological, and clinical data to apply boosted re-gression on the Pima Indian Diabetes database. They achieved an accuracy of 90.91%. With the use of conventional features and a sur-vey, Ganie et al. [6] were able to predict diabetes with a remarkable accuracy of 96.90% using a GB model. Using administrative health-care claim data from the CBHS dataset, Lu et al. [14] employed a RF classifier and achieved an accuracy of 84.95%. On the other hand, re-search on gene expression datasets was done by Srinivasu et al. [23] and [7], who obtained 82% and 95% accuracy rates, respectively. In our study, we harnessed the power of biological mechanism"}, {"title": "5 CONCLUSION", "content": "Research on T2D diagnosis and management has gained significant attention recently. Our research proposes a framework for T2D de-tection utilizing gene expression level data through ML techniques. We have used RNA sequencing data from 1600 human pancreatic islet cells sourced from the GEO database. Our findings from a rig-orous evaluation of various ML models demonstrate that XGBoost performs better than the other models in terms of several perfor-mance evaluation metrics, with 97% accuracy. The suggested model also performs better than previous studies in ML-based diabetes diagnosis. This research highlights the potential of ML in early detection of diseases based on gene expression data. Identification of T2D markers even before full symptom manifestation allows the beginning of treatment and lifestyle adjustments within the early stages of diabetes. It paves the way for improved early diagnosis and intervention strategies for diabetes which can significantly improve patient outcomes, delay disease development, and minimize the likelihood of health complications. In future research, it is impera-tive to broaden model validation by incorporating more datasets to reveal additional diabetes risk factors. Applying deep learning-based pattern recognition models to analyze gene sequences for predicting future illnesses is another promising avenue. Further-more, another avenue worth exploring is integrating techniques to predict future glucose levels."}]}