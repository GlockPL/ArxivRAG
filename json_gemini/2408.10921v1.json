{"title": "MTFinEval:A Multi-domain Chinese Financial Benchmark with\nEurypalynous questions", "authors": ["Xinyu Liu", "Ke Jin"], "abstract": "With the emergence of more and more economy-specific LLMS, how to measure whether they\ncan be safely invested in production becomes a problem. Previous research has primarily fo-\ncused on evaluating the performance of LLMs within specific application scenarios. However,\nthese benchmarks cannot reflect the theoretical level and generalization ability, and the backward\ndatasets are increasingly unsuitable for problems in real scenarios. In this paper, we have compiled\na new benchmark, MTFinEval, focusing on the LLMs' basic knowledge of economics, which can\nalways be used as a basis for judgment. To examine only theoretical knowledge as much as possi-\nble, MTFinEval is build with foundational questions from university textbooks, and exam papers\nin economics and management major. Aware of the overall performance of LLMs do not depend\nsolely on one subdiscipline of economics, MTFinEval comprise 360 questions refined from six major\ndisciplines of economics, and reflect capabilities more comprehensively. Experiment result shows\nall LLMs perform poorly on MTFinEval, which proves that our benchmark built on basic knowl-\nedge is very successful. Our research not only offers guidance for selecting the appropriate LLM\nfor specific use cases, but also put forward increase the rigor reliability of LLMs from the basics.", "sections": [{"title": "Introduction", "content": "In the realm of economics, LLMs offer economists and policymakers unique insights [Li+23a], thereby\nenhancing the efficiency of economic industry development [Zha+23][Zha+24b]. For example, Cat-\nMemo[Cao+24] uses LLM for stock trading, FINANCEBENCH [Isl+23] uses LLM for company earn-\nings analysis, and FinPT[Yin+23] uses LLM for risk forecasting. Financial data grows exponentially\nin both volume and complexity. However, the task-oriented benchmarks[Lei+23] [Ara19], composed\nof specific past events are gradually deviating from the actual situation. No matter a drastic change\nin policy between countries, or disruptive innovation in new technologies such as AI, it will lead to\ndramatic changes in economic phenomena. Furthermore, the comprehensive capabilities of LLMs in\nthe field of economics cannot be fully assessed by a single task requirement. Therefore we create a\nbenchmark, MTFinEval, to examine LLM theoretical knowledge across a wide range of economics\nfields.\nIn this article, we narrow focus to the cognitive level of theoretical knowledge within LLMs because\ntheoretical knowledge is the most fundamental requirement. It not only shapes the model's grasp of\nproblems but also forms the basis for task execution. Our analysis breaks down the comprehensive\ncapabilities of LLMs across numerous sub-aspects. This approach aims to identify the reasons be-\nhind the subpar performance of LLMs when tackling complex tasks, offering a clearer direction for\nunderstanding their limitations.\nThe benchmark, MTFinEval, covers six fields of management, accounting, e-commerce, strategic\nmanagement of enterprise, macroeconomics and microeconomics, and covers multiple dimensions such\nas economic indicators, financial technology, financial law and economic phenomena. For all ques-\ntions, the ability of the model is examined in the form of question and answer, and the correct rate\nof the model is directly calculated. Through this dataset, we aim to assess the multi-faceted capa-\nbilities[Liu+19][Ma+18][KGC17][LJD18] of LLMs in the field of finance, including but not limited to\ndata understanding, logical reasoning, and situational adaptation."}, {"title": "2 Related Work", "content": "In finance and economics, the application of Large Language Models (LLMs) is emerging as an essential\ntool for in-depth market analysis[ZYX23][Yu23][Wan+22], precise investment advice, and effective risk\nassessment[Yan+23]. This paper aims to delve into the specialized development and potential of LLMs\nwithin the financial domain, highlighting the significance of systematic evaluation and theoretical\nintegration.\nFirstly, BloombergGPT [Wu+23], an LLM tailored for the financial sector, exhibits significant po-\ntential in performing financial natural language processing (NLP) tasks. Its capability to manage\ncomplex financial data and tasks, such as analyzing market trends and generating investment re-\nports [Koa+24], illustrates the promise of LLM applications in finance. Similarly, FinMA, through\ncommand tuning, adeptly handles a variety of financial NLP tasks[SK20][ZML21][Mai+18][Li+20]\nincluding sentiment analysis, event_detection[Cor+17], and risk assessment[AVB15][LZ23], further\ndemonstrating the broad application potential of LLMs in the financial sector.\nNonetheless, systematic evaluation of LLMs' performance in finance is crucial. The introduction\nof the EconNLI dataset [GY24], designed to assess LLMs' knowledge and competence in economic\nreasoning, exposes potential deficiencies in LLMs' economic reasoning abilities[Par+23]. This under-\nscores the necessity for thorough evaluation of these models. Additionally, models that incorporate\neconomic theory excel in financial analysis and forecasting, emphasizing the importance of integrating\neconomic knowledge when developing financial LLMs. Evaluating LLMs' performance in specialized\nareas is crucial for gauging their true capabilities. While existing assessments often concentrate on\ngeneral NLP tasks relevant to finance, such as causal reasoning, text classification, and predictive\nanalytics, there is a dearth of systematic evaluations tailored to the financial and economic sectors. It\nbecomes evident that a deep grasp of economic principles significantly enhances LLMs' financial task\nperformance[Zha+24a].\nThe application of multimodal learning in financial forecasting also presents new opportunities. For\ninstance, MONOPOLY[Mat+22] leverages multimodal cues to forecast finances from monetary policy\nmeeting videos, offering a novel perspective on market dynamics understanding. Concurrently, the\nexploration of cross-language and zero-sample learning capabilities[JLT24], such as the zero-sample\ncross-language named entity recognition method introduced by CROP[Yan+22a][Yan+20], opens up\nnew possibilities for multilingual[Yan+22c] financial applications and aids in processing financial doc-\numents in various languages.\nIn conclusion, LLMs in the financial and economic fields demonstrate unique value and potential.\nTo enhance the performance and reliability of these models [Hua+22], a deeper understanding and\nintegration of economic theory[PH20][Elm93][Fan+18] are required."}, {"title": "3 MTFinEval Benchmark", "content": "Our research is dedicated to strengthening the capacity of these models to tackle economic challenges\nfundamentally and systematically.\nThe MTFinEval dataset comprises 360 university economics questions, spanning six major sub-\ntopics: macroeconomics, microeconomics, accounting, management, e-commerce, and strategic management.\nEach subtopic includes single choice, multiple choice, and true or false questions. All questions and\nanswers are manually extracted from college textbooks [Li+23b] and exam papers to ensure they are\nfoundational and introductory. The data collection process has been meticulously scrutinized through\na series of systematic checks. We began by manually entering the paper questions into CSV files,\nensuring the completeness of each entry.\nSubsequently, to verify the accuracy of the questions and answers, we enlisted the expertise of\nsix economists, each with high proficiency in various subfields. They were tasked with reviewing the\nclarity of the descriptions and the correctness of the answers, for which they were compensated at a\nrate of $3 per question. In cases where a question was deemed questionable during the expert review\nstage, a collective discussion was held among all experts to determine the correct answer and to decide\nif the question should be omitted. The types and numbers of questions for all subtopics are shown in\nFigure 1."}, {"title": "3.2 Formulation", "content": "For problem type T, given the large model M, the corpus set d used for training and fine-tuning the\nlarge model, input the question q, and the large model returns the answer a[NCL18].\nIn the zero-shot scenario for true or false statements, the objective function is defined as follows:\n$r = \\arg \\max_{\u03b1\u2208{0,1}}P(a|q^\u00b2; M, d\u00b2) $\nSimilarly, for single-choice questions in the zero-shot scenario, the objective function is specified as:\n$r=\\arg \\max_{a\u2208{A,B,C,D}} P(a|q; M, d\u00b2)$\nLastly, for multiple-choice questions in the zero-shot scenario, the objective function is:\n$r=\\arg \\max_{a\u2208 {A,B,C, AB, AC, BC, ABC...}} P(a\\q; M, d\u00b2)$\nOn the surface, choices and judgments are both classify problems. Fundamentally, for the decoder\nonly model, the underlying task is a generation task. In the training of LLMs, While pursuing to\nreduce cross entropy, maximum likelihood estimation is also used to reproduce similar sentences in the\ncorpus. Therefore, although only the selection of the judgment problem, the bottom layer still recalls\nthe relevant corpus content during the training of the LLMs."}, {"title": "4 Experiments", "content": "For problem type T, given the large model M, the corpus set d used for training and finetuning the\nlarge model, input the question q, and the large model returns the answer a. In the zero-shot scenario\nfor judge questions the objective function is define To simulate the most direct use cases effectively,\nwe introduce a brief prompt to the question. This prompt includes a description of the question type,\nall under zero-shot conditions[Bro+20]. The prompts for the three question types are similiar with\nfollows[Wei+22][Cha+24][Yan+19][Yan+22b][Tai+23]: multiple choice: You are a financial knowledge\nexpert, please combine your financial knowledge to answer the following multiple choice, note that there\nis not only one answer to multiple choice, to think and reason whether each option is correct but do\nnot output thinking process, directly return the option letter in markdown format, each option in the\nanswer is separated by a space. When it comes to judgment questions, the LLM is instructed to provide\nanswers limited to \"yes,\" \"wrong,\" or \"do not know.\". Subsequently, key words from these answers are\nextracted and mapped onto these three specific options. In the realm of traditional economics, LLMS\nare typically based on the Llama series[Tou+23][Dub+24] for processing. To promote decentralization,\nthe models participating in this evaluation are exclusively open-source models that were released in\nthe year 2024."}, {"title": "4.2 Results", "content": "Regardless of little changes in the LLMs' architecture, the diverse answers provided by various LLMS\ncan reveal the extent and quality of the training data used. This insight can then be used to rea-\nsonably gauge the model's capability level. Specialization in Subjects: Certain models demonstrate\nexpertise in specific domains. For instance, the 01-ai/Yi-1.5-9B-Chat-16K[You+24] model excels in\naccounting, e-commerce, and strategic management, suggesting a rich dataset in these areas. This\nproficiency likely stems from its capacity to interpret complex financial data, manage online platforms\nefficiently, and formulate strategic business plans. Macroeconomic Understanding: The THUDM/glm-\n4-9b-chat [Zen+24] [Bom+21] model's elevated score in macroeconomics indicates a robust grasp of\neconomic trends and policy impacts. This proficiency may be due to its exposure to a diverse range\nof economic data, enabling precise assessments. Strategic Management Insight: The google/Gemma-\n2-9B-IT model's high strategic management score reflects its potential to support business planning.\nIts ability to integrate information from various sources to offer valuable strategic insights is likely the\nreason for this performance. Overall Performance: The Qwen/Qwen2-7B-Instruct[Bai+23][Yan+24]\nmodel's lead in overall performance with a 63.33 score signifies its emergence as a leading contender\nin the field of financial language model technology. Its comprehensive score not only reflects its pro-\nficiency in individual subjects but also suggests a robust and well-rounded training approach that\nhas equipped it to handle a variety of economic analyses. This model's performance is indicative of\nits potential to become the new baseline, or pedestal [Nor+23], for financial LLMs [Che+23], possibly\noutperforming or replacing previous models such as Llama [CYY23] in certain applications. Reflection\non Weaker Performances: Models like the meta-llama/Meta-Llama-3-8B-Instruct, which scored lower\noverall, may have been trained on less diverse or pertinent data for certain subjects, or they may have\narchitectural limitations hindering effective processing and analysis of economic information. Implica-\ntions for Model Improvement: To enhance performance in specific subjects, models should be trained\nwith more domain-specific data. Moreover, ongoing updates and refinements to the model architecture,\ninformed by performance data, can further augment capabilities. In conclusion, LLM performance in"}, {"title": "5 Conclusion", "content": "In this study, we introduced MTFinEval, a comprehensive multi-domain benchmark, consisting of 360\nquestions across six major economic disciplines. It provides a rigorous assessment tool to evaluate\nthe fundamental economic knowledge of LLMs, for the purpose of ensuring that the LLM still makes\nthe right analysis in a rapidly changing environment. The experimental results demonstrated that\ncurrent LLMs, generally perform poorly on this benchmark, highlighting significant gaps in their\ntheoretical understanding of economics. This research contributes to the field by offering a robust\nevaluation framework that can guide future improvements in LLMs, ensuring they are better equipped\nfor complex and dynamic economic environments."}]}