{"title": "ENERGY-CONSCIOUS LLM DECODING: IMPACT OF TEXT GENERATION STRATEGIES ON GPU ENERGY CONSUMPTION", "authors": ["Alireza Nik", "Michael A. Riegler", "P\u00e5l Halvorsen"], "abstract": "Decoding strategies significantly influence the quality and diversity of the generated texts in large language models (LLMs), yet their impact on computational resource consumption, particularly GPU energy usage, is insufficiently studied. This paper investigates the relationship between text generation decoding methods and energy efficiency, focusing on the trade-off between generation quality and GPU energy consumption across diverse tasks and decoding configurations. By benchmarking multiple strategies across different text generation tasks, such as Translation, Code Summarization, and Math Problem Solving, we reveal how selecting appropriate decoding techniques with their tuned hyperparameters affects text quality and has measurable implications for resource utilization, emphasizing the need for balanced optimization. To the best of our knowledge, this study is among the first to explore decoding strategies in LLMs through the lens of energy consumption, offering actionable insights for designing resource-aware applications that maintain high-quality text generation.", "sections": [{"title": "Introduction", "content": "In the past few years, we have witnessed a significant increase in the applications of Large Language Models (LLMs). Powerful text generation LLMs such as OpenAI's ChatGPT [1], Google's Gemini [2], and Anthropic's Claude Sonnet [3] have been completely intertwined in our daily lives. The ability of these models to generate high-quality coherent response text is significantly influenced by the choice of decoding strategies during inference. Decoding strategies transform the probability distributions generated by LLMs into fluent and relevant output text [4, 5, 6]. They determine how tokens are selected at each step of text generation. This transformation termed as decoding is as crucial as the model itself in achieving the best quality of the generated text. However, the selection of the appropriate decoding strategy and tuning their specific parameters remain insufficiently explored [4]. In most cases, the evaluations of the newly proposed, state-of-the-art LLMs are only conducted using the default decoding method, instead of studying the effects of adopting different strategies and varying their parameters to achieve the best benchmarking results.\nOn the other hand, the popularity and increasing applications of LLMs across various domains has caused the AI community to continuously scale the model sizes up to hundreds of billion parameters [7]. This growth has led to a significant surge in computational demands causing a considerable energy consumption associated with the hardware these models are being deployed, especially the GPUs [8, 9]. GPUs have become indispensable to modern high- performance computing (HPC) infrastructure due to their efficient parallel processing [10]. This enhanced capability has made them an excellent accelerator for training and deploying LLMs. However, their significant energy consumption has raised serious alarms recently due to their increasing scale and the global push towards sustainable AI [8, 11]. Although several research have studied the energy consumption and environmental footprints regarding training LLMs [12, 13, 14, 15], the energy consumption concerns associated with LLM inference have been studied much less. This contrasts with the fact that LLM inference has become the main contributor of AI energy footprints recently [16]."}, {"title": "Background", "content": ""}, {"title": "Decoding Strategies", "content": "LLMs generate text sequentially in an autoregressive manner. At each step, the model computes a probability distribution over the vocabulary. Decoding strategies determine how the model converts these probabilities into generated text. The choice of decoding method significantly influences the quality, diversity, and coherence of the output. These strategies are generally categorized into two types: deterministic and stochastic."}, {"title": "Deterministic Decoding Strategies", "content": "\u2022 Greedy: The most basic decoding strategy where the model select the token with the highest probability at each generation step [5, 22]. Since tokens with the highest probability are always chosen while the rest are completely ignored, this technique may leads to repetition and degeneration, especially in tasks where diversity in the output text is an important factor.\n\u2022 Beam Search: Instead of greedily selecting the tokens, [23] proposed keeping track of multiple candidate sequences (a beam), expanding them at each generation step, and selecting the top ones based on their overall scores after multiple steps. The number of these candidate sequences is controlled by the beam width, which serves as a hyperparameter.\n\u2022 Diverse Beam Search (DBS): [24] introduced an extension of the original Beam Search method by dividing the candidate sequences into multiple subgroups while enforcing diversity within each group. The key hyperparameters for this approach are the beam width and the number of groups.\n\u2022 Contrastive Search (CS): [19] introduced a contrastive framework to penalize repetitive token selections while maintaining coherence. After selecting the top-k candidates from the model, they are assessed based on the likelihood assigned by the LLM and a degeneration penalty that evaluates how close the token is relative to the previous context. This method have two hyperparameters: k, which controls candidate selection based on top-k, and a, which controls degeneration."}, {"title": "Stochastic Decoding Strategies", "content": "\u2022 Temperature ($\\tau$): This sampling method simply modifies the logits before applying the Softmax [26] function in the decoding process. When the temperature is low, the method becomes more deterministic by increasing the likelihood of selecting high-probability tokens. In contrast, selecting higher temperature values introduce randomness in generation by increasing the likelihood of choosing lower-probability tokens.\n\u2022 Top-k: [27] introduced a method which sampling is performed from the k most probable candidates in the vocabulary at each token generation step. A smaller k makes the output more deterministic, whereas a larger k allows tokens with lower probabilities to be considered in the selection process.\n\u2022 Top-p (Nucleus): Rather than selecting a fixed number of tokens and sampling from them, [28] proposed selecting tokens based on their probability distribution. They suggested choosing the minimal set of tokens whose cumulative probability goes beyond a probability p\n\u2022 Epsilon Sampling: [29] introduced another truncation sampling approach by selecting only the tokens whose conditional likelihoods are above an entropy-dependent threshold $\\epsilon$. This ensures that the highly unlikely tokens are excluded.\n\u2022 Typical Sampling: To generate text that resembles most to human writing, [30] proposed selecting tokens based on information entropy. In the sampling approach, we select tokens based on how close a token's probability is to the expected information content at each step. Thus, tokens that are too predictable or too unlikely are discarded, with the selection process controlled by a typicality threshold.\n\u2022 Min-p Sampling: [21] proposed a dynamic token selection strategy to overcome the limitations of other truncation-based sampling methods. Their approach adjusts the filtering threshold at each step based on the probability of the most likely token. The key hyperparameter is a base probability, which is multiplied by the likelihood of the most probable token to dynamically adjust the threshold for the next generation step."}, {"title": "Experimental Setting", "content": ""}, {"title": "Compute Resources", "content": "All the experiments were conducted on a compute node equipped with two NVIDIA A100-PCIE-40GB GPUs and a Dual 64-c ore AMD EPYC 7763 Processor @ 2.45GHz. The system is managed by SLURM [31], an open-source workload manager that facilitates resource allocation, job scheduling, and queue management. In each job (experiment), we requested only one GPU. Before running the main job, we start monitoring and collecting the GPU resource utilization data over the job's run time. This monitoring is stopped immediately after the main job is finished. The maximum power draw for both GPUs is set to 250 watts.\nAll the experiments were run on an isolated compute node to minimize the variations in the LLM inference measurements. When running the same experiment (a specific decoding method with fixed parameters with same input data) multiple times alongside other users' jobs in a compute node, we sometimes observed significant variations in the total inference time and consumed energy. These fluctuations in LLM inference workloads are due to the shared nature of the system (the potential disruptions of other jobs) and distinct compute characteristics of LLMs [16]. Consequently, we submitted all our jobs on isolated node to mitigate the significant variations."}, {"title": "Monitoring Tools", "content": "We use the nvidia-smi [32] utility to monitor and manage GPUs in our experiments. This command-line utility provides detailed information regarding GPU performance metrics including GPU utilization, temperature, memory usage, memory clock frequency, power consumption, and etc. We use this real-time monitoring utility to record GPU resource usage including utilization and power consumption at 1000 ms intervals for each experiment."}, {"title": "Benchmarks", "content": "For reproducibility, we used LM-evaluation-harness [33] to benchmark and evaluate the chosen LLMs across different decoding methods. This is an open-source framework designed for evaluating and testing the performance of the LLMs across a wide range of tasks and benchmarks. We evaluated our selected decoding strategies across three text generation tasks: Translation, Code Summarization, and Math. For all three tasks, we utilized only a subset of the full datasets. Using the complete dataset would unnecessarily increase inference time and energy consumption during experimentation, without bringing any meaningful insights to the study. We emphasize that we reported only the available performance metrics for the generated texts as provided by the LM-evaluation-harness framework across our selected tasks."}, {"title": "Translation", "content": "We selected WMT16 [34] dataset to evaluate the translation performance. WMT is a widely recognized competition for machine translation tasks, providing high-quality datasets across multiple languages. In our experiments, we only focused on German-English and English-German and only considered the first 500 input prompts of the whole dataset in both cases. Also, the number of few-shots was set to 0 for all translation jobs.\nWe report the BLEU [35] in our translation experiments. This widely-used evaluation metric measures the n-gram overlap between the generated output and the ground truth, with higher scores indicating the better generative performance."}, {"title": "Code Summarization", "content": "The CodeXGLUE [36] benchmark suit is a collection of datasets designed to evaluate the AI models on various code-related tasks. Specifically, we used Code Summarization [37] task to generate concise and coherent descriptions of source code snippets. This task evaluates the LLM's ability to understand the semantics of code and translate it into meaningful summaries. We only summarized the first 100 code snippets of Python and Java Script programming languages in to natural language descriptions while the number of few-shots was set to 0 for all experiments.\nWe use the Smoothed BLEU [38] evaluation metric, a modified version of the standard BLEU, to alleviate the issue of non-overlapping high-order n-grams in the code summarization task [36]."}, {"title": "Math", "content": "We utilized the GSM8K dataset [39] to evaluate the reasoning and mathematical problem-solving capabilities of LLMs under different decoding strategies. Although the popular GSM8K consists of 8K grade-school-level math problems, we only focused on the first 100 problems out of the whole benchmark. In this specific task, we used 5-shot prompts in our experiments.\nFor this benchmark, the Exact Match accuracy is reported as the performance of the generated text compared to the ground truth. As explained in [40], the exact match checks if each token in the a language model's output aligns with the greedy decoding generation of the input at each step. If all tokens align perfectly, the LLM's generated text is considered an exact match. Thus, a higher value indicates better text generation performance."}, {"title": "LLM", "content": "In our experiments, we used Qwen 2.5 [41], a recent series of decoder-only LLMs. Specifically, we experimented different decoding strategies with 7B-instruction-tuned model. We selected this model size due to its robust performance across a wide range of text generation tasks and to accelerate the decoding process in our experiments."}, {"title": "Strategies & Hyperparameters", "content": "We employed the decoding strategies introduced in Sections 2.2 & 2.3 in our study. All adopted strategies have been implemented in the Hugging Face [42] library. We also tuned the hyperparameters of each strategy To evaluate their impact on text quality and energy consumption across our selected tasks. The range of hyperparameter values for each decoding technique was selected based on recommendations from standard practices and recent studies [4, 5]. However, in some cases, we included unconventional hyperparameter values to investigate their impact, particularly on energy consumption. For example, in the Top-K decoding strategy, we conducted experiments with k = 1. While k = 1 is essentially equivalent to the greedy decoding strategy, this allowed us to examine whether differences in implementation between greedy decoding and Top-K decoding result in variations in performance or energy consumption. Table 3 lists the utilized decoding strategies and their associated hyperparameters. Our goal is not to identify the optimal decoding hyperparameters for the entire benchmark suite across different models but rather to examine the relationship between performance and energy consumption in comparable settings for each decoding strategy. By focusing on a limited subset of datasets, we aim to determine which decoding strategy is more energy-efficient during LLM inference."}, {"title": "Experimental Results", "content": "We conducted an extensive benchmarking of different decoding strategies across 5 text generation tasks. The batch size is set to 1 in all experiments, as our selected text generation tasks from LM-Evaluation-Harness do not allow batch size modification for these cases. For each decoding method, we experimented multiple hyperparameters, repeating each configuration 5 times to capture variations in the LLM's inference behavior. Through preliminary testing, we found that 5 repetitions were sufficient to capture variations. During each run, we tracked both text generation quality and energy consumption. After completing each job, we filtered the energy usage data to focus solely on the active GPU processing periods. This approach allowed us to accurately quantify the GPU energy consumption and ensured that our findings are both reliable and reflective of real-world GPU usage during text generation tasks.\nIn the following sections, we examine the trade-offs between text generation quality and GPU energy consumption for each decoding strategy and analyze how hyperparameter tuning impacts energy usage."}, {"title": "Text Generation Quality vs Energy Efficiency", "content": "The text generation quality and energy consumption for various decoding strategies using the Qwen2.5 7B-Instruct model are presented in Table 1. The results were obtained using the best hyperparameter configuration that delivered the highest quality for each decoding method. The corresponding energy consumption for this optimal hyperparameter was averaged across all 5 runs and reported in watt-hours. Additionally, we evaluated the trade-off between quality and energy across different methods using an Efficiency Ratio (ER = Quality/Energy). This metric measures the generative quality achieved per unit of energy consumed, making it valuable for identifying methods that offer high quality while minimizing energy usage.\nIn both translation tasks, Beam Search and DBS achieve the highest BLEU scores-up to about 42.7 for De\u2192En and 29.9 for En\u2192De-but also draw considerable power, pulling down their overall efficiency ratio. In contrast, AD delivers BLEU scores near-or on par with those top-performing beam-based methods while consuming far less power, thereby attaining the best efficiency ratios. Greedy decoding, along with Temperature, Top-k, and Top-p sampling methods, occupies a middle ground: they offer moderately high BLEU at moderate energy costs, resulting in fairly respectable efficiency.\nSimilarly, in GSM8K, Beam search achieves the highest exact-match accuracy (0.90) but requires 20.32 Wh of energy, resulting in a modest efficiency ratio. In contrast, AD maintains solid accuracy (0.80) at just 10.63 Wh, yielding the highest efficiency ratio among all methods. Several stochastic methods also strike a better balance than beam-based approaches, generating high-quality texts at more moderate energy levels.\nFor both Python and JavaScript code summarization tasks, CS achieves the highest smoothed bleu scores, exceeding 1.0 in both cases, but also consumes the highest energy usage, putting it at a disadvantage efficiency-wise. In both cases, CS, DoLa and DBS have the highest amount of energy consumption compared to other methods. In contrast, AD delivers near-competitive smoothed bleu scores at a fraction of the energy, leading to the highest efficiency ratios among the tested strategies. Stochastic sampling techniques fall into a middle zone, similar to translation and math problem-solving tasks, offering moderate generation quality without the extreme energy consumption of beam-based or contrastive methods in deterministic approaches.\nTo better demonstrate how the choice of decoding strategy affects generative quality and energy consumption, we employed the Relative Standard Deviation expressed as RSD(%) = $\\frac{\\sigma}{\\mu}$ \u00d7 100 mathematically. RSD quantifies the variability of the data relative to its mean value."}, {"title": "Hyperparameter Sensitivity Analysis", "content": "To investigate the impact of hyperparameter choices on both text generation quality and energy consumption, it is crucial to determine their sensitivity across different configurations. To quantify this, we calculate the RSDs of both quality and energy to measure their variability and consistency across all hyperparameters for each decoding strategy. In this way, we can capture the degree of sensitivity of each method to hyperparameter changes, allowing for an interpretable comparison between decoding methods in each task. In this scenario, a lower RSD suggests robustness, where hyperparameter changes have minimal effect on the observed metrics.\nWe also employed the Sharpe Ratio [43], a classic statistical metric from finance, to incorporate the recorded energy variations observed in each run for a given hyperparameter within our analysis. Mathematically expressed as $\\frac{\\bar{x}-r_f}{\\sigma}$, Sharpe Ratio measeures how well the average return of an investment compensates the volitality (risks) of returns. This reward-to-variability metric can be adapted in our analysis by treating the average of text generation qualities as the mean return of investments and the variability (standard deviation) of energy consumptions across all hyperparameter runs as the risks for each decoding strategy. This approach provides us a single metric to identify decoding strategies that are less sensitive to hyperparameter changes while simultaneously rewarding strong generative quality and penalizing energy fluctuations."}, {"title": "Key Findings", "content": "Across all tasks, we observe that, except for Temperature sampling, the other stochastic decoding methods achieve some of the highest SR ratios, maintaining good quality with minimal energy fluctuations across different hyperparameter choices. This suggests that these methods are less sensitive to hyperparameter tuning and are more energy-conscious relative to the quality they produce. The reason Temperature sampling exhibits significant variation in both energy consumption and quality is the inclusion of high-temperature values in our choices. Higher temperatures $\\tau > 1$ increase randomness in token selection, reducing output fluency and coherence. This quality loss is further illustrated in Figure 2. Across all selected tasks, the model struggled to enhance diversity without sacrificing accuracy, leading to a noticeable drop in quality and increased energy consumption, particularly in translation and math problem-solving tasks. This is particularly interesting because, while Temperature sampling was among the methods with the highest efficiency ratio in Table 1, the RSD results in Table 2 reveals that it has the greatest variation in both quality and energy across different hyperparameters. This underscores the importance of hyperparameter optimization when using this method.\nIn some cases, such as beam-based methods in the math problem-solving task, we observe significant energy fluctuations despite minimal variation in quality. Conversely, in code summarization tasks, Temperature sampling exhibits the opposite trend, with substantial quality variation but stable energy usage. This proves that variations in quality and energy across hyperparameters do not always correlate.\nAnother notable finding is that, while in Section 4.1 we identified AD as the most energy-efficient method in its best hyperparameter configuration for each task, We observe that only in the German-English translation task does it rank among the best methods with the highest SR ratio. In other cases, it exhibits variations in both generation quality and energy consumption across its selected hyperparameters, implying that AD performs optimally only when properly tuned in some text generation tasks. Additionally, for beam-based methods, we observe considerable energy fluctuations when increasing the beam width in code summarization and math problem-solving tasks (Figure 3). However, in translation tasks, energy consumption remains relatively stable, and the quality does not vary significantly. Both of these findings highlight the importance of task-specific hyperparameter tuning for certain decoding strategies.\nContrastive Search stands out as the most sensitive to hyperparameter changes among all methods, with k = 25 exhibiting significantly greater energy fluctuations than k across different penalty a values. Larger k values can lead to substantial shifts in both performance and energy consumption. Additionally, the impact of the a parameter varies considerably depending on the task, contributing to its high RSD values in both quality (up to 104.95%) and energy consumption (up to 53.45%)."}, {"title": "Limitations & Future Work", "content": "In this study, we confined our experiments to a relatively small-scale LLM, Qwen2.5 7B-Instruct, leaving the in-vestigation of larger models and architectures for future work. While we carefully selected hyperparameters for our experiments, our approach was not exhaustive, leaving a broader range of parameter settings yet to be explored. Additionally, because LM-Evaluation-Harness framework did not offer benchmarking open-ended text generation tasks at the time of conducting this research, we focused on a subset of directed tasks that best suited our objectives. Recent decoding strategies research efforts significantly revolves around open-ended language generation, such as story creation and text continuation; thus, exploring how various decoding techniques influence energy consumption in such scenarios would be a valuable direction to pursue. We also did not combine different decoding strategies, suggesting another avenue for future research. Finally, our analysis focused on GPU energy consumption; however, future work could extend this scope to encompass other factors such as CPU power usage, memory and temperature, providing a more comprehensive understanding of how decoding strategies influence overall resource consumption."}, {"title": "Ethical Considerations and Societal Impact", "content": "\u2022 Ethical Considerations: This research does not involve any private or sensitive data, as all experiments are conducted using publicly available datasets. To ensure transparency and reproducibility, we rely on the LM-Evaluation-Harness framework, which provides a unified approach for benchmarking language models. However, the investigation of different text generation strategies must be accompanied by a careful assessment of potential ethical risks, including bias, misinformation, and harmful content generation.\n\u2022 Societal Impact: This work aims to advance the field of Machine Learning while addressing the growing concerns about the energy consumption and environmental impact of large-scale AI models. Recently, there has been growing concern about the high energy consumption and sustainability challenges of AI models, emphasizing the need for efficient inference methods. Selecting decoding strategies that preserve model quality while reducing energy usage can help mitigate the environmental footprint of large-scale language models. Our research encourages the AI community to not only prioritize output quality but also adopt a more energy-conscious approach when using LLMs, contributing to greater sustainability in AI."}, {"title": "Conclusion", "content": "In this study, we investigated the impact of decoding strategies and hyperparameter choices on both generative quality and GPU energy consumption during LLM inference across various text generation tasks. Through a comprehensive evaluation, we found that, in all tasks, the choice of decoding strategy significantly affects GPU energy consumption during inference, even in cases where it has minimal impact on output quality. Additionally, we observed that different decoding methods present distinct trade-offs between text quality and energy efficiency. While deterministic methods generally achieve top-tier quality in most generation tasks, they also tend to consume more energy and exhibit higher variability across their hyperparameters. In contrast, stochastic approaches appear to be more energy-efficient, often providing a better quality-to-energy ratio and demonstrating lower sensitivity to hyperparameter changes relative to their output quality. Overall, no single decoding strategy outperforms all others across every metric. Instead, the optimal choice depends on the specific task and whether the priority is maximizing generation quality, minimizing energy consumption, or achieving a stable balance between both. Our findings highlight the importance of carefully selecting decoding strategies, taking into account not only output quality but also computational and energy costs."}, {"title": "Appendix", "content": ""}, {"title": "Detailed Results", "content": ""}, {"title": "energy across hyperparameters:", "content": "In this section, we present figures illustrating how average energy consumption changes with varying hyperparame-ters for each decoding strategy. Additionally, The marker sizes in each graph represent generation quality, with larger markers indicating higher quality and smaller markers in-dicating lower quality."}]}