{"title": "Upstream and Downstream Al Safety: Both on the Same River?", "authors": ["John McDermid", "Yan Jia", "Ibrahim Habli"], "abstract": "Traditional safety engineering assesses systems in their context of use, e.g. the operational design domain (road layout, speed limits, weather, etc.) for self-driving vehicles (including those using Al). We refer to this as \"downstream\u201d safety. In contrast, work on safety of frontier Al, e.g. large language models which can be further trained for downstream tasks, typically considers factors that are beyond specific application contexts, such as the ability of the model to evade human control, or to produce harmful content, e.g. how to make bombs. We refer to this as \u201cupstream\u201d safety. We outline the characteristics of both upstream and downstream safety frameworks then explore the extent to which the broad Al safety community can benefit from synergies between these frameworks. For example, can concepts such as common mode failures from downstream safety be used to help assess the strength of Al guardrails? Further, can the understanding of the capabilities and limitations of frontier Al be used to inform downstream safety analysis, e.g. where LLMs are fine-tuned to calculate voyage plans for autonomous vessels? The paper identifies some promising avenues to explore and outlines some challenges in achieving synergy, or a confluence, between upstream and downstream safety frameworks.", "sections": [{"title": "Introduction", "content": "There is currently global interest in safety of artificial intelligence (Al), particularly safety of foundation or frontier Al models, also known as general purpose Al (GPAI). The trigger for this was the emergence of ChatGPT in late 2022 and the rapidly advancing capabilities of the ChatGPT family and other large language models (LLMs). Indeed, some are concerned that GPAI may, in time, evade human control and could pose existential risks [1, 2]. There are also more specific concerns, e.g. the ability of such models to assist \u201cbad actors\u201d in developing harmful capabilities, e.g. bioweapons. These concerns have led to international agreement on the need for controls and the emergence of frameworks for assessing and managing safety of such Al models [3]. These frameworks tend to focus on how the capabilities of the Al models are developed, e.g. training and fine-tuning, and the risk controls that can be introduced by the model developers; we refer to these as upstream Al safety models, as they address the development of general Al capabilities rather than the multifarious services provided using those models, e.g. through user-facing apps.\nIn contrast, safety engineering (in the traditional sense) is a much older discipline which originated around 1950 in response to the risks associated with space flight and some military capabilities. The early processes focused on failure, e.g. loss of structural integrity or thermal runaway of a chemical process. There have been major changes since these early days. First, software has become a critical component of such systems, with approaches to improving the integrity of software/minimise the likelihood of software-related failures originating in the 1980s [4, 5]. Second, the introduction of autonomy (often enabled by Al) has led to a focus on assessing the safety of the intended function (hitherto assumed to be safe) [6]. Throughout this evolution, safety engineering practice focused on the product in its operational context, for example the same humanoid robot in a care home or a factory poses quite different risks and requires different risk controls. We use the term downstream safety to reflect assessment in context, drawing on traditional safety engineering.\nThis paper explores the similarities and differences between upstream and downstream safety; we hope that by clarifying concepts and identifying commonalities of concerns and practices we can facilitate constructive dialogue. More ambitiously, we hope to identify ways of improving traceability between upstream and downstream safety; we believe this is fundamental for safety assurance, governance and transparency, and for allocating responsibility (or more positively for empowering the right parties to take responsibility for their roles and actions). In this sense, we hope to show whether the upstream and downstream safety models are on the same river or at least there is a confluence in sight.\nWe start by presenting some of the relevant concepts of downstream safety and reinforce the value of analysing safety in context, as this determines the potential nature and extent of harms. We then consider upstream safety, including re-articulating some of the emerging concepts using downstream safety terminology. This leads on to a discussion of the potential for a confluence between the two approaches, what would be needed to achieve it, some views on future research directions and the implications for Al regulation."}, {"title": "Downstream Safety", "content": "Downstream safety has traditionally focused on physical harm (to humans and sometimes the environment) [7]. It analyses systems or operations, starting from the concept of a hazard a situation which could give rise to harm, e.g. wrong dosage rate for a syringe pump, then identifying failures that could cause the hazard (whether human or technical) down to \"basic\u201d faults, e.g. a valve sticking or a resistor failing open circuit. Controls are identified, e.g. redundancy, diversity and developing components to high levels of integrity to reduce the risk to an acceptable level. The notion of design integrity applies to physical components but is also widely used for software, with the controls involving increased rigour and depth of verification evidence as the consequences of failure/malfunction increase [8]. Approaches to achieving and assuring software integrity are often defined in standards, and these vary across application domain, e.g. aerospace [9] vs healthcare [10].\nWith the growing introduction of autonomous capabilities, e.g. in cars and unmanned aircraft, there has been a more explicit recognition of the safety of the intended function (SOTIF), i.e. assessing whether the system is safe if it operates as intended, rather than the traditional focus on failures (along with the implicit assumption that working as intended is safe). Work on SOTIF is most explicit in automotive, which had perhaps overlooked the issue, and a recent standard [11] sets out the need to reduce uncertainty when a system (perhaps including Al) is operating in a complex and only partially knowable environment [12]. Further, there is work on assurance of Al in several embedded systems domains, e.g. automotive\u00b9 and aerospace, and some domain-agnostic approaches\u00b2. These are generally extensions of well-established safety engineering approaches, considering Al as part of a"}, {"title": "Upstream Safety", "content": "Following the Al Safety Summits in the UK and Korea, 16 major GPAI developers agreed to publish their Al safety frameworks before the next summit (in France in February 2025). Our use of the term upstream safety is intended to encompass the work on these frameworks. There are several major differences to downstream frameworks.\nFirst, the concerns go wider than physical harm (although they typically include it), for example addressing the ability for GPAI to help bad actors to develop bioweapons, the ability to manipulate political processes, e.g. through deepfake videos, and the ability to assist organisations in mounting cyber-attacks.\nSecond, the frameworks reflect much more dynamic processes. Downstream safety has, historically, focused on assuring safety in support of a deployment decision. This has been seen as the critical decision point, with lower levels of scrutiny applied to updates to the system. In contrast, the upstream frameworks must deal with much faster (perhaps DevOps) development models and the fact that the deployed models can be adapted by users.\nThird, the focus is (largely) on the capabilities of the Al models themselves, with less focus on actual application and context of use (this is unsurprising, given the generality of the technology). In terms of the downstream model there is a focus on SOTIF.\nThe emerging frameworks vary, but there are commonalities, reflecting the National Institute of Standards and Technology (NIST) Assessing Risks and Impacts of AI (ARIA) framework which focus on evaluation (evals) to inform model refinement and to identify the need to introduce risk controls. Typically the evals encompass:\nModel testing including benchmarking on tasks intended to reflect real-world usage.\nRed teaming using experts to stress test and to seek to find flaws or weaknesses.\nField-testing in conditions representative of operational use (but usually controlled).\nLong-term impact assessments.\nTo our knowledge, no-one has proposed an \u201cunderlying safety science\u201d for the upstream safety model. We suggest, in order to prompt discussion, that this might be characterised as capability evaluation (reflecting point 3 above) and evolutionary thinking (reflecting point 2 above and the iterative model refinement process supported/guided by evals)."}, {"title": "Level of capability and capability evaluation", "content": "We use two examples to illustrate the ways in which GPAI developers characterise and evaluate model capabilities. Google DeepMind has set out a frontier safety framework\u00ae which they aim to implement by early 2025 before the considered risks materialise. It uses the term critical capability levels (CCLs) defined as \u201cthresholds at which models may pose heightened risk without additional mitigation\". They identify CCLs in four risk domains:\nAutonomy \u2013 models capable of expanding their capacity by acquiring resources to run additional copies of themselves.\nBiosecurity \u2013 ability to develop known biothreats easily (non-expert users) or novel biothreats (expert users).\nCybersecurity \u2013 ability to fully automate cyber-attacks opportunistically or on critical national infrastructure.\nML R&D \u2013 ability of the models to significantly accelerate the development of Al capability (presumably most powerful if applied recursively).\nIn the downstream model's terminology these are not hazards or failure modes; they are perhaps best thought of as particular risks. ML R&D can be thought of as a (potential) common cause (enabler) for the other three risk domains (and more). Similarly, autonomy has the potential to be a common cause for many other harms, or it could be benign (aside from resource consumption), e.g. producing pictures of cats, or beneficial, e.g. searching for biomarkers for types of cancer. In the downstream world view, the benefit or risk cannot be assessed except in the application context.\nFinally, Google DeepMind discuss evaluation to provide early warnings of potential violations of CCLs. This is intended, inter alia, to allow researchers time to develop new mitigations before the risks materialise \u2013 which is a form of evolutionary thinking.\nOpenAl has also developed guidelines on Al Safety and their evolving position is reflected on their website'. Their approach includes a Preparedness Framework\u00b9\u00ba which includes what they term catastrophic risks against which they evaluate models prior to release. The four catastrophic risks they identify, which can also be seen as capabilities, are:"}, {"title": "Risk controls", "content": "There seems to be more variation in the definition of risk controls than there are in the levels of capabilities and evaluation strategy.\nGoogle DeepMind identify two classes of mitigation: security and deployment controls. The security controls focus on preventing exfiltration of model weights as this would allow others to replicate the models15 and potentially to exploit the full capability of the models.\nThe deployment controls can be summarised as follows:\nSafety finetuning of models and use of filters.\nFull suite of prevailing industry safeguards plus periodic red teaming to assess the adequacy of the mitigations.\nSafety case to keep numbers of incidents below a prescribed level with red team validation pre-deployment.\nPrevention of access to the model capabilities (an open research problem).\nThe description of level 2 doesn't use the downstream terminology for safety cases but implicitly it includes goals (what is to be demonstrated), strategies for decomposing goals into subgoals, and evidence to show that the goals have been met reflecting the standard structure of a safety case16. The underlying intent of level 2 deserves some scrutiny. Given the severity of the harms associated with the higher CCLs any number of incidents above zero would seem to be highly undesirable. One way of rationalising this would be to construct the safety case to include the mitigation measures, e.g. guardrails, and to identify \"incidents\" as triggering of the mitigations. Assuming there are multiple mitigations for the more severe outcomes, then setting targets for incidents in terms of mitigation measures should still result in acceptable risk, as several mitigations need to fail (or be \u201cbreached\u201d) to allow adverse effects. This might also link to Safety-II \u2013 the mitigations acting to prevent harm are part of the system \u201cworking right\u201d and thus should be reinforced.\nOpenAl focus on training of models and the use of guardrails providing a hierarchy of controls which aligns with the use of classical downstream representations of safety controls, such as bow tie models\u00b9\u201d. The training reflects a Model Spec18 which includes:\nObjectives including benefitting humanity, reflecting social norms and complying with applicable laws.\nRules including protecting people's privacy and respecting creators' rights.\nDefault behaviours which are as much about the way developers and users conduct themselves as the Al models themselves.\nThe Model Spec is mainly intended to be used to guide developers. OpenAl also refer to sharing real-world feedback which is similar to the NIST ARIA long-term impact assessment.\nSome recent work on safety of GPAI builds on the concept of safety cases, adapted from the downstream safety world. One approach identifies \u201cbuilding block arguments\u201d 19, see Figure 1 below, with the expectation that the arguments used will vary as the models become more powerful. This approach strongly links capabilities and controls.\nThe building block arguments are, in summary:\nInability \u2013 the model cannot cause unacceptable outcomes in any realistic setting.\nControl - unacceptable outcomes are prevented by existing control measures.\nTrustworthiness even though the models could cause catastrophic outcomes, they won't as they robustly behave as intended (there is an obvious analogy with SOTIF).\nDeference an Al advisor asserts that the Al model doesn't pose a catastrophic risk (and the Al advisors are at least as credible as human decision-makers).\nAs GPAI models will evolve rapidly the safety case will need to be dynamic [22]."}, {"title": "Observations", "content": "Despite the differences in the details the approaches reviewed above have some common elements, viz: a focus on evals (e.g. testing against a range of benchmarks), use of red teaming and an emphasis on fine-tuning to achieve model alignment. They can perhaps be thought of as refinements of NIST's ARIA model. Also, they share a concern about model transparency (aka visibility or explainability) but the approaches, perhaps excepting the use of CoT reasoning with OpenAl's 01, do not consider the issues of achieving transparency with large-scale GPAI models20.\nFrom a downstream safety perspective, where unsafe failure rate targets might be one in a million operations/hours or less, the evaluations, e.g. showing jailbreak \u201csuccesses\u201d in the 10s of percent, seem a very long way away from acceptable levels of risk. Or are they? That depends on the context of use, which is largely lacking in these upstream frameworks."}, {"title": "Comparison and Analysis", "content": "We first set out a comparison between the upstream and downstream models, then consider what each might learn from the other. Table 1 summarises our views on the core characteristics of upstream and downstream safety, including identifying similarities. We briefly discuss the key points in the table, then consider what it would mean for upstream and downstream safety to be \u201con the same river\u201d."}, {"title": "Insights and challenges for upstream safety", "content": "There are three key take-aways, or insights, from upstream safety assurance:\nRisk is viewed as an aspect of capability and is generally assessed independent of context.\nConfidence in the models comes from post-development assessment (evals) and the subsequent refinement of the models based on adverse findings in the evals.\nThere is nascent work on understanding failure modes of GPAI, which might be informative for downstream safety analysis and assurance.\nThe general challenge is coping with the capabilities and pace of change of GPAI models."}, {"title": "Insights and challenges for downstream safety assurance", "content": "There are four key take-aways, or insights, from downstream safety assurance:\nThe importance of analysing risk in context, including identifying hazards which can be viewed as the proximate cause of harm.\nThe need to identify failure modes, both top down and bottom up, and to understand how these can combine to give rise to hazards (and harm).\nThe need to identify \u201csafety architectures\u201d employing redundancy and diversity, but also the requirement for high integrity elements in the architecture to achieve safety.\nThe need to identify and understand common mode or common cause failures, and potential SPOF, which might undermine the safety architecture.\nIn terms of challenges, how can downstream safety cope with GPAI which is beyond the ability of established analysis methods to assess?"}, {"title": "The Same River or a Confluence?", "content": "End-user applications of Al can arise in different ways, as shown in Figure 2. First, specific-purpose Al, e.g. for an SDV or for clinical decision-support (CDS) can be derived from GPAI, including frontier models. Second, such applications can be developed using more traditional Al, e.g. neural networks or reinforcement learning. Third, GPAI can be used \"directly"}, {"title": "Conclusions", "content": "We have explored the world views of the Al community when discussing Al safety and that of the more traditional safety community when considering the impact of Al and GPAI; in doing so we have sought to find commonalities as well as differences. We argue that downstream safety is essential to understand the potential adverse impact of GPAI behaviour and failures in context. We are not alone. A recent Chinese report on their Al safety governance framework29 also distinguishes inherent GPAI safety risks (upstream) and safety risks in GPAI applications (downstream). Thus, we conclude that the downstream model of safety is still relevant in the GPAI world, but we also believe it needs to (continue to) adapt to survive.\nTo this end, we sought to identify whether there is a confluence between upstream and downstream safety including identifying some potentially unifying terminology. More importantly, we have identified some possible ways that analyses of GPAI models could be used to inform the safety analysis of their applications. In essence this would be through providing guidance on classes of deviation from intent, and their causes, that can arise from GPAI models to inform downstream analysis, e.g. through adaptations of methods such as HAZOP. There is also a need to identify what might be seen as \u201cparticular risks\u201d in the applications of the GPAI models. This requires collaboration between the two communities as knowledge and understanding of each is needed to make substantive progress. We see a research programme on adapting classical safety methods to reflect the characteristics of GPAI as being vital to progress.\nIn practice, a key issue will also be mechanisation. Classical safety work is largely carried out by hand; this is intractable at the scale of GPAI applications and the pace of change. Thus, for example, any work on deviation classes needs to be supported by tooling that allows experiments, e.g. injecting simulated deviations into models, and perhaps by using GPAI itself to search for and characterise GPAI deviation classes.\nWe have also discussed fault-tolerant system architectures and the ability to protect against (or control) failures through use of redundancy and diversity and the need to avoid common modes of failure. This needs to be extended into resilience \u2013 how systems can recover from failures \u2013 and incident investigation (to avoid recurrence of problems). This would build on and extend the notions of monitoring which are in most GPAI safety frameworks. Again, collaboration between the Al safety and traditional safety communities is needed here.\nAssessing safety and regulating GPAI is challenging due to the complexity of the models and the uncertainties around their behaviour. There is no simple solution to this problem and the philosophy must involve reducing uncertainty prior to deployment and monitoring in operation to detect and respond to \"the ones that got away\u201d. However, one potentially attractive pre-deployment activity is to use a dialectic approach \u2013 developers/would be deployers of GPAI present a safety case, and the red team presents counterarguments (a risk case) although this will mean changes in practice for existing (domain-based) regulators.\nWe hope that our discussion will have a further benefit in that it identifies the way in which established regulators in specific domains who are confronted with uses of GPAI can work with emerging national Al safety regulators, thus helping to shape the evolving Al safety regulation ecosystem."}]}