{"title": "Distance between Relevant Information Pieces Causes Bias in Long-Context LLMs", "authors": ["Runchu Tian", "Yanghao Li", "Yuepeng Fu", "Siyang Deng", "Qinyu Luo", "Cheng Qian", "Shuo Wang", "Xin Cong", "Zhong Zhang", "Yesai Wu", "Yankai Lin", "Huadong Wang", "Xiaojiang Liu"], "abstract": "Positional bias in large language models (LLMs) hinders their ability to effectively process long inputs. A prominent example is the \"lost in the middle\" phenomenon, where LLMs struggle to utilize relevant information situated in the middle of the input. While prior research primarily focuses on single pieces of relevant information, real-world applications often involve multiple relevant information pieces. To bridge this gap, we present LONGPIBENCH, a benchmark designed to assess positional bias involving multiple pieces of relevant information. Thorough experiments are conducted with five commercial and six open-source models. These experiments reveal that while most current models are robust against the \"lost in the middle\" issue, there exist significant biases related to the spacing of relevant information pieces. These findings highlight the importance of evaluating and reducing positional biases to advance LLM's capabilities.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) (Zhao et al., 2023; Minaee et al., 2024) have made significant progress in various natural language processing tasks (Hendrycks et al., 2021; Han et al., 2021). In particular, applications such as code repository analysis (Chen et al., 2021) and information extraction (Ko\u010disk\u00fd et al., 2018) often require processing long texts, with context lengths reaching up to 200,000 tokens (Li et al., 2024; Zhang et al., 2024). To address these demands, researchers have focused on enhancing LLMs' ability to handle extended inputs effectively (Chen et al., 2023; Han et al., 2024). As a result, multiple LLMs have been developed (Dubey et al., 2024; Team et al., 2024; OpenAI, 2024) which support context lengths of up to one million tokens.\nRecent studies have shown that the position of relevant information significantly affects the performance of long-context LLMs (Liu et al., 2023; Lei et al., 2024; Hsieh et al., 2024). In \"needle in a haystack\" tasks, models struggle to utilize information located in the middle of the input, a phenomenon known as the \"lost in the middle\" effect (Liu et al., 2023). This evaluation method is commonly used to analyze positional bias (Hengle et al., 2024; Nelson et al., 2024). These analyses (Liu et al., 2023) focused on single relevant information pieces and their positions in the input sequence (front, middle, back), referred to as absolute positions.\nHowever, real-world tasks like data analysis (Zhang et al., 2024) often involve multiple pieces of relevant information. This introduces a new characteristic: the distance between relevant information pieces, or how densely they are distributed, termed as relative position. Evidence from two types of extreme cases indicates that varying relative position may lead to significant bias, impairing LLMs' long-context performance (Lei et al., 2024). However, this kind of biases have not been systematically studied so far, which highlights the need for thorough investigation.\nTo bridge the gap, we introduce LONGPIBENCH, a benchmark designed to evaluate positional bias with multiple relevant pieces. It assesses positional bias in two categories: (1) absolute positions, referring to the location of relevant information within the entire context, and (2) relative positions, referring to the distribution and distance between multiple relevant information pieces. It includes diverse tasks of different complexity and spans four input lengths from 32K to 256K tokens. To the best of our knowledge, LONGPIBENCH is the most comprehensive benchmark for isolating and ana-"}, {"title": "2 LONGPIBENCH", "content": "LONGPIBENCH is a dataset designed to evaluate positional bias with multiple relevant information pieces. As shown in Figure 2, we first manually annotated several seed examples and then augmented them by varying the positions of relevant information. More details can be found in Appendix A."}, {"title": "2.1 Core Statistics", "content": "LONGPIBENCH contains 3 different tasks, 4 different input length levels: (32k, 64k, 128k, and 256k). To comprehensively analyze the impact of positional bias, we set 16 different absolute and relative location levels respectively. The benchmark is composed of 7,680 instances, each containing 10 pieces of relevant information. The whole dataset comprises to 922M tokens."}, {"title": "2.2 Seed Data Annotation", "content": "We manually labeled 20 seed data points for three tasks: Table SQL, Timeline Reordering, and Equation Solving, which cover a range of complexities in long-context tasks. Each instance contains 10 relevant pieces of information. This selection was based on an examination of long-context application scenarios, where the number of relevant elements typically falls around the order of magnitude of ten, although it varies across different tasks (Bai et al., 2023; Wang et al., 2024; Dong et al., 2024). Detailed task definitions, examples, and other pertinent details are provided in Appendix A."}, {"title": "2.3 Data Augmentation", "content": "To analyze the positions of relevant information, we augmented the data by altering the absolute and relative positions of the relevant pieces while keeping all other features unchanged.\nWe broke down the context into elements based on natural information units: table entries for Table SQL, event entries for Timeline Reordering, and equation lines for Equation Solving. We labeled each element as relevant or irrelevant by selecting relevant elements, forming queries around them, and adding irrelevant ones. By introducing varying amounts of irrelevant information, he context lengths are controlled at four levels: 32K, 64K, 128K, and 256K. We then shuffled the element positions to introduce positional variations.\nAbsolute Position To analyze the impact of absolute position on LLM performance, we manipulated where relevant information appears in the context. Each context was divided into 16 equal segments from start to end. We placed all 10 relevant pieces within a single segment to keep their relative positions consistent. By moving this segment from the first to the last position, we varied the absolute position from the start to the end of the input. The average position of these relevant pieces served as the absolute position metric which is calculated as:\nAverage Location = $\\frac{1}{N} \\sum_{i=1}^{N} l_i \\times L$ ,\nwhere l is the current level, N is the total number of levels (16), and L is the length of the context.\nThis setup allowed us to assess how model performance changes as relevant information is placed further back in the context.\nRelative Position To examine the effect of spacing between relevant information pieces on LLM performance, we created 16 levels of distribution density. Each level represents a different spacing configuration among the 10 relevant pieces. At the densest level, all relevant pieces are adjacent with no irrelevant information between them. At the sparsest level, they are evenly distributed throughout the context with equal intervals of"}, {"title": "2.4 Quality Control", "content": "To ensure the integrity and reliability of the benchmark, we implemented comprehensive quality control measures throughout its construction.\nManual Correction Automatically synthesized data generated by rules or language models (LLMs) can be noisy and may not accurately reflect real-world scenarios. To address this, we manually annotated queries and answers based on the provided context, ensuring that the data is both accurate and contextually relevant.\nKnowledge Masking In knowledge-intensive tasks like the Timeline Reordering task, there's a risk of data leakage if models rely on pre-existing knowledge rather than the provided context. To prevent this, we implemented knowledge masking by anonymizing or fictionalizing events using GPT-4 (OpenAI et al., 2024). This approach ensures that models cannot leverage memorized information and must rely solely on the context presented to them."}, {"title": "3 Experimental Setup", "content": "To evaluate the influence of context information positioning on long-text large language models (LLMs), we conducted experiments using popular long-context language models on a subset of LONGPIBENCH."}, {"title": "3.1 Models", "content": "We assessed a total of eleven LLMs, comprising six open-source and five commercial options. The selection of open-source models includes the 70B model from Llama-3.1-Instruct series (Dubey et al., 2024), the 7B, 14B, 32B, 72B models from Qwen-2.5 family (Qwen, 2024), the 8\u00d722B model of WizardLM-2 (Xu et al., 2023). The commerical models we selected are GPT-40-mini (OpenAI, 2024), Claude-3-Haiku (Anthropic, 2024), Gemini-1.5-Flash (Team et al., 2024), GLM-4-air (GLM et al., 2024), and Deepseek-Chat-v2 (DeepSeek-AI et al., 2024). The selected models are good representatives of popular and top-performance long-context models."}, {"title": "3.2 Configuration", "content": "During the pre-test stage, we found that Timeline Reordering and Equation Solving proved to be excessively challenging, with even the top-performing LLMs achieving accuracy rates between 0.0 and 0.2 (more in Appendix B). Therefore, we reserve these two tasks for more advanced LLM analysis in the future. We currently focus our experiments on the Table SQL task. Additionally, since 32k tokens is the minimal context length supported by all tested LLMs, we standardized the context length to 32k\u00b3 tokens for all experiments. This uniform configuration ensures consistency across different models, allowing for a fair comparison of their performance under identical conditions. Detailed discussions on statistics, evaluation metrics, parameter settings, and prompt configurations are provided in Appendix C."}, {"title": "4 Results and Discussion", "content": "In this section, we analyze the impact of absolute and relative positional bias. And we further analyze these phenomena from two perspectives: the number of parameters and query-aware contextualization. Detailed Experimental Results are available in Appendix D."}, {"title": "4.1 Impact of Absolute Position", "content": "As illustrated by the blue lines in Figure 3, we progressively shift the interval of relevant information from the beginning to the end and observed that while a few open-source models like Qwen 2.5 (7B) (Qwen, 2024) and WizardLM 2 (8\u00d722B) (Xu et al., 2023) still suffer from the severe \"lost in the middle\" phenomenon, commercial models and larger open-source models do not exhibit effects related to absolute position.\nThis outcome significantly surpasses previous evaluations (Liu et al., 2023), indicating that current long-context models have achieved greater robustness against variations in absolute position of relevant information."}, {"title": "4.2 Impact of Relative Position", "content": "As illustrated by the orange lines in Figure 3, we progressively increase the distance between relevant pieces of information and observe that all open-source and commercial models exhibit a significant bias toward different relative positions. This bias is characterized by an initial rapid decline in performance followed by a more gradual decrease. Even in straightforward retrieval tasks, relative position bias can lead to a 20\u201330% reduction in recall rates for competent commercial models.\nThese findings indicate that the relative positioning among multiple relevant pieces of information is a serious and unresolved issue, which may substantially undermine the effectiveness of long-text language models in practical applications."}, {"title": "4.3 Further Analysis", "content": "Effect of Parameter Size When selecting models for evaluation, we included four variants from the Qwen 2.5 Family (Qwen, 2024) with differing parameter sizes. These models exhibit no significant differences in architecture, training methods, or training data. By analyzing their performance under identical positional information features, we can isolate the impact of parameter size on the robustness to positional bias.\nAs illustrated in Figure 3, for absolute position bias, we found that simply increasing the model parameters from 7B to 14B-while keeping architecture, training methods, and data constant substantially mitigates the \"lost in the middle\" (Liu et al., 2023) issue. This suggests that robustness to absolute positions may be an \"emergent ability\" (Wei et al., 2022) and increasing the number of parameters can significantly enhances it.\nIn contrast, regarding biases related to relative positional information, augmenting the number of parameters only yielded minor quantitative improvements and did not alter the pronounced bias trend. This trend remains largely unchanged even in commercial models with approximately hundreds of billions of parameters. These findings indicate that merely increasing parameter size is insufficient to develop robustness to relative positions, and new techniques may be necessary.\nEffect of Query-Aware Contextualization Liu et al. (2023) demonstrated that the placement of the query"}, {"title": "5 Conclusion", "content": "This study investigates a new category of positional bias involving multiple relevant pieces of information in long-context LLMs through three key contributions. (1) Benchmark Development: We introduce LONGPIBENCH, the most comprehensive benchmark for evaluating positional bias in long-text LLMs, assessing both absolute and relative biases.\n(2) Comprehensive Evaluation: Using LONGPIBENCH, we evaluated eleven popular LLMs, investigated the \"lost in the middle\" phenomenon, and identified novel yet significant biases related to the relative positioning of multiple relevant pieces of information.\n(3) Insightful Findings: Our experiments show that while modern LLMs have improved robustness against absolute positional biases, they are highly sensitive to the distance between relevant pieces of information. Performance declines sharply as the distance increases before stabilizing. We also explore how model size and query-aware contextualization impact these biases. These findings emphasize the necessity of continuously mitigating positional biases in long-text models."}, {"title": "Limitation", "content": "Lack of In-depth Analysis Our systematic experiments demonstrate that two types of positional bias exist when multiple related pieces of information are present in the context. We also analyzed how these biases relate to the number of parameters and query contextualization. However, we are currently unable to explain the reasons behind these two positional biases.\nFocus on Specific Models The evaluation was conducted on a set of eleven popular large language models (LLMs), including both open-source and commercial options. However, the findings are limited to these models. The study does not account for the performance of other emerging or less popular models, which might exhibit different results regarding positional biases."}, {"title": "Ethical Considerations", "content": "Human Annotation Our seed construction process involves manual annotation. This annotation was carried out by some of the authors, who are researchers with substantial knowledge in LLM evaluation. Consent was obtained from the individuals whose data we are using or curating. The data collection protocol was approved.\nData Security Some data in our Table SQL task may appear to pertain to personal information. However, this data is not actual personal information. Instead, it is generated by us through specific heuristics, eliminating the risk of personal information leakage.\nUse of AI assistants We use GPT-4o (OpenAI, 2024) for expression modification and grammar sanity check during the composition process."}, {"title": "A Details of LONGPIBENCH", "content": "A.1 Task Definitions\nTable SQL This task involves retrieving entries containing specific features from a table with a large number of entries. This task is considered the simplest among the three tasks because the LLMs only needs to perform accurate retrieval based on the corresponding key without requiring additional processing either before or after the retrieval. The prototype of this task is primarily derived from experiments in S3Eval (Lei et al., 2024), specifically those examining information distributions with extreme positional variability.\nTimeline Reordering This task involves sorting specific events based on their occurrence within lengthy event descriptions. This task is considered more challenging than the Table SQL task because the LLM must determine which sections of the comprehensive event description pertain to the occurrence of the specific event during retrieval. Additionally, after retrieval, the model must sort the events according to their chronological order. The corpus for this task is partially sourced from Wikipedia (Wikipedia, 2024), which provides descriptions of various historical events, and partially generated by GPT-4 (OpenAI, 2024) to create fictitious scenarios. This approach is employed to prevent data leakage that could arise from real historical events being present in the model's pre-training knowledge. Both the questions and answers are manually annotated.\nEquation Solving This task involves identifying equations related to target variables within a large set of linear equations and performing elimination calculations to determine the exact values of the variables. This task is considered the most difficult for two main reasons. First, the relevant information exhibits strong dependencies, necessitating a sequential retrieval process where an error in any step can lead to the failure of the entire chain. In contrast, the previous two tasks allow for parallel processing. Second, after retrieving all necessary information, the model must perform numerical computations on the results."}, {"title": "A.2 Task Examples", "content": "Here are some examples of the three tasks in LONGPIBENCH. Queries are placed both before and after the context for better query contextualization.\nA.2.1 Table SQL\nInput You are given a table of entries with the following columns: Country, Name, Birth Year, Birth Month, Blood Type. Your task is to find all the entry with the following Country: China. You should return all the entries that match the query as a python list. For example, ['l China | Hong Liang | 1991 | August | Al', ...]. You should not generate anything else. Here is the table:\n| Country | Name | Birth Year | ... | Blood Type |\n| Italy | Ginevra | 2009 | February | O |\n| Argentina | Martina | 1966 | March | B |\n| Egypt | Salma | 1985 | July | B |\n| China | Zhang Wei | 2006 | November | O |\n| China | Wang Wei | 1966 | February | AB |\n| Australia | Emily | 1983 | December | O |\n| Italy | Leonardo | 1985 | November | O |\nYou are given a table of entries with the following columns: Country, Name, Birth Year, Birth Month, Blood Type. Your task is to find all the entry with the following Country: China. You should return all the entries that match the query as a python list. For example, ['| China | Hong Liang | 1991 |\nAugust | A |', ...]. You should not generate anything\nelse.\nGround Truth\n[\n\"| China | Zhu Wei | 1992 | September | B |\",\n\"| China | Zhang Wei | 1955 | March | O |\",\n\"| China | Zhang Wei | 2006 | November | O |\",\n\"| China | Wang Wei | 2001 | September | B |\",\n\"| China | Yang Wei | 2016 | November | AB |\",\n\"| China | Li Na | 1974 | January | B |\",\n\"| China | Liu Wei | 1975 | November | O\",\n\"| China | Gao Wei | 1954 | August | BI\",\n\"| China | Zhu Wei | 1989 | September | AB |\",\n\"| China | Wang Wei | 1966 | February | AB |\"\n],\nA.2.2 Timeline Reordering\nInput Please find the following events list and reorder them in chronological order:\n0: The unification of the Kingdom of Aedoria was achieved through the Edict of Unison.;\n1: The Aerial Flight of the Albatross, the first recorded successful manned flight.;\n2: The discovery of the Cerulean Mineral significantly advanced technological progress.;\n3: The signing of the Treaty of Greenwater ends the War of the Roses.;\n4: The discovery of the lost city of Subterracopia changed historical narratives.;\n5: The fictional Great Reformation of the Church of Light heralded a new era of spiritual practices.;\n6: The foundation of the fictional City of Harmony symbolized a new era of urban planning and societal integration.;"}, {"title": "A.2.3 Equation Solving", "content": "Input Here is a list of equations. Every variable\nxi is well-defined and solvable from the given list.\nPlease find out the exact int value of 149. You may\nthink step by step. Your response should end with\n'Therefore, the answer is (the int value)'.\nEquation 1: 3 * x1 = 3\nEquation 2: -5 * x2 - 4 * x3 + 6 * x4 = -14\nEquation 3: -1 * x1 - 2 * x5 = -9\nEquation 4: -5 * x6 - 4 * x7 = 37\nEquation 5: -1 * x5 + 5 * x8 = 21\nEquation 1520: 3 * x3079 - 5 * x3080 + 1 * x3081 =\n-24\nEquation 1521: -6 * x3082 + 3 * x3083 = 3\nEquation 1522: 6 * x3084 - 6 * x3085 = -36\nHere is a list of equations. Every variable xi\nis well-defined and solvable from the given list.\nPlease find out the exact int value of x49. You may\nthink step by step. Your response should end with\n'Therefore, the answer is (the int value)'.\nGround Truth 6"}, {"title": "B Details of the Unused Tasks", "content": "During the pre-training phase, we found that the two tasks of Timeline Reordering and Equation Solving are extremely challenging. Even for the commercial models we tested, which have the best performance, they could only achieve very low accuracy (ranging from 0.0 to 0.2). For open-source models, examples with different absolute and relative positions achieved almost entirely zero experimental results. Therefore, we believe these two tasks are currently too challenging and should be reserved for future long-text models with greater capabilities. Here, we provide quantitative and qualitative results for these two tasks."}, {"title": "B.1 Quantitative Examples", "content": "As shown in Figure 5, the results presented here evaluate the impact of absolute positions by testing GPT-40-mini (OpenAI, 2024) and Gemini-1.5-Flash (Team et al., 2024) on all examples of the History Reordering task with a length of 32k."}, {"title": "B.2 Qualitative Examples", "content": "In this section, we show some typical failure output of LLMs on these two challenging tasks.\nB.2.1 Timeline Reordering\nInput Please find the following events list and reorder them in chronological order:\n0: The unification of the Kingdom of Aedoria was achieved through the Edict of Unison.;\n1: The Aerial Flight of the Albatross, the first recorded successful manned flight.;\n2: The discovery of the Cerulean Mineral significantly advanced technological progress.;\n3: The signing of the Treaty of Greenwater ends the War of the Roses.;\n4: The discovery of the lost city of Subterracopia changed historical narratives.;\n5: The fictional Great Reformation of the Church of Light heralded a new era of spiritual practices.;\n6: The foundation of the fictional City of Harmony symbolized a new era of urban planning and societal integration.;\n7: The landmark decision from the Court of Harmony transformed civil rights.;\n8: The fictional Upheaval of the Red Monarchy.;\n9: The radical filmmaker Arman Dorset premiered his highly controversial movie, 'The Last Hope'.,\nYou should only output the order and should not output anything else. For example, if the correct order is 2, 0, 1, you should directly answer '2, 0, 1'.\"\nHere is the context:\nEvent 1\nIn the spring of 1462, the Kingdom of Aedoria reached a pivotal moment in its history with the signing of the Edict of Unison by King Alaric IV. Aedoria, previously a collection of feuding city-states, was ravaged by internal strife for centuries. Seeking to end the perpetual conflict, King Alaric IV employed a mix of diplomacy, strategic marriages, and tactful displays of military might to persuade the city-state rulers to consolidate power under a central monarchy. The Edict of Unison laid the groundwork for a unified legislative system, common currency, and an overarching defense strategy that played a substantial role in mitigating future conflicts among the states. This period marked the beginning of the Aedoric Golden Age, during which the arts, trade, and sciences flourished due to the domestic peace and stability created by the edict."}, {"title": "C Details of Experimental Setup", "content": "C.1 Evaluation Metrics\nTable SQL In the Table SQL task, we evaluate the correctness of generated SQL responses by calculating the recall of reference labels present in the generated response. The metric measures the proportion of reference labels that appear in the generated text. Formally, given a generated response R and a set of reference labels L = {l1, l2, ..., ln }, the metric MSQL is defined as:\nMSQL = $\\frac{1}{|L|} \\sum_{i=1}^{|L|} 1[l_i \\in R]$,\nwhere 1[] is the indicator function that returns 1 if the condition is true and 0 otherwise, and |L| is the total number of reference labels. This metric yields a score between 0.0 and 1.0, representing the fraction of labels correctly recalled in the generated response.\nTimeline Reordering For the Timeline Reordering task, we assess the correctness of the event sequence in the generated response compared to the ground truth sequence. The metric calculates the proportion of events that are correctly ordered. Given the ground truth sequence G = [g1, g2, ..., gn] and the generated sequence R = [r1, r2, ..., rn], the metric MReorder is computed as:\n MReorder = $\\frac{1}{n} \\sum_{i=1}^{n} 1[g_i=r_i]$,\nwhere n is the number of events, and 1[gi = ri] indicates whether the i-th event in the generated sequence matches the ground truth. The metric ranges from 0.0 to 1.0, reflecting the proportion of events in the correct order.\nBefore computing the metric, the generated response is cleaned to retain only numeric indices, commas, and spaces. If the generated sequence does not match the expected format or the lengths differ, the metric is assigned a score of 0.0.\nEquation Solving In the Equation Solving task, we evaluate whether the numerical answer provided in the generated response matches the ground truth. The metric is binary, assigning a score of 1.0 if the predicted answer equals the ground truth and 0.0 otherwise. If the generated response contains the phrase \"the answer is\" followed by the numerical value, we extract this value and compare it to the ground truth label L:\nMEquation = 1[P = L],\nwhere P is the predicted value extracted from the response. If the predicted value cannot be extracted or converted to an integer, the metric returns a score of 0.0."}, {"title": "C.2 Inference Parameters", "content": "To ensure consistency and reproducibility in our experiments, we standardized the inference parameters across all models during the inference phase. Specifically, we set the temperature parameter (temp) to 0.0 and the top-p sampling parameter (top) to 0.9. This unification of inference settings facilitates the replication of experiments and establishes a consistent evaluation standard across different models."}, {"title": "C.3 Prompt Template", "content": "For the three tasks, we used the following prompt templates respectively. Notice that we place queries both before and after the context body for better query contextualization.\nC.3.1 Table SQL\nInput You are given a table of entries with the following columns: Country, Name, Birth Year, Birth Month, Blood Type. Your task is to find all the entry with the following Country: {country}.\nYou should return all the entries that match the query as a python list. For example, ['l China | Hong Liang\n| 1991 | August | A |', ...]. You should not generate anything else. Here is the table:\n{context}\nYou are given a table of entries with the following columns: Country, Name, Birth Year, Birth Month, Blood Type. Your task is to find all the entry with the following Country: {country}. You should return all the entries that match the query as a python list.\nFor example, ['l China | Hong Liang | 1991 | August\n| Al', ...]. You should not generate anything else.\nC.3.2 Timeline Reordering\nInput Please find the following events list and reorder them in chronological order:\n0: {Event Name 0}\n9: {Event Name 9}\nYou should only output the order and should not output anything else. For example, if the correct order is 2, 0, 1, you should directly answer '2, 0, 1'.\"\nHere is the context:\n{context}"}, {"title": "C.3.3 Equation Solving", "content": "Input Here is a list of equations. Every variable\nxi is well-defined and solvable from the given list.\nPlease find out the exact int value of {target}.\nYou may think step by step. Your response should\nend with 'Therefore, the answer is (the int value)'.\n{context}\nHere is a list of equations. Every variable xi\nis well-defined and solvable from the given list.\nPlease find out the exact int value of {target}.\nYou may think step by step. Your response should\nend with 'Therefore, the answer is (the int value)'."}, {"title": "D Details of Experimental Results", "content": "In the main text, for better readability, we presented the experimental results in the form of line charts. However, it is difficult to obtain the specific values of the different experimental groups from the line charts. To address this, we present all the experimental results here in tabular form. This will better facilitate the precise display of the experimental results.\nThe results of Figure 3 that depicts the impact of absolute positions and relative positions are exhibited in Table 1 and Table 2. The results of Figure 4 that depicts the impact of query contextualization are exhibited in Table 3 and Table 4."}]}