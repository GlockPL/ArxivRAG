{"title": "No Size Fits All: The Perils and Pitfalls of Leveraging LLMs Vary with Company Size", "authors": ["Ashok Urlana", "Charaka Vinayak Kumar", "Bala Mallikarjunarao Garlapati", "Ajeet Kumar Singh", "Rahul Mishra"], "abstract": "Large language models (LLMs) are playing a pivotal role in deploying strategic use cases across a range of organizations, from large pan-continental companies to emerging star-tups. The issues and challenges involved in the successful utilization of LLMs can vary significantly depending on the size of the organization. It is important to study and discuss these pertinent issues of LLM adaptation with a focus on the scale of the industrial concerns and brainstorm possible solutions and prospective directions. Such a study has not been prominently featured in the current research literature. In this study, we adopt a threefold strategy: first, we conduct a case study with industry practitioners to formulate the key research questions; second, we examine existing industrial publications to address these questions; and finally, we provide a practical guide for industries to utilize LLMs more efficiently.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have recently garnered significant attention due to their exceptional performance in various predictive and generative tasks (Hadi et al., 2023; Kar et al., 2023). Extensive research has been conducted to harness LLMs across diverse domains and tasks (Raiaan et al., 2024), including medicine (Thirunavukarasu et al., 2023), finance (Li et al., 2023b), and reasoning tasks (Huang and Chang, 2023; Qiao et al., 2023). Despite their unprecedented adaptation to numerous industrial applications, there is a notable lack of studies examining the potential challenges and risks associated with LLMs, which can vary depending on the size of the organization. Such studies would not only be valuable for industries seeking informed adaptation but also help shape research focus to address the key challenges and obstacles faced in real-world scenarios.\nThe challenges and bottlenecks faced by organizations of different sizes are not uniform. Factors such as funding availability, workforce size, skill and training deficits, ethical and regional considerations, and access to adequate hardware can all influence how these challenges manifest. Previous research has largely addressed general challenges (Raiaan et al., 2024) with LLMs, such as multi-lingual support, domain adaptation, and compute requirements. However, there is a lack of studies specifically focusing on the industrial perspective and the unique challenges of implementing LLMs in this context.\nTo this end, we conduct a study with a threefold strategy, firstly, we conduct a rigorous case study of real-world practitioners from the IT industry, who are trying to work on AI adaptation and formulate three guiding research questions. \nHow have industries adopted LLMs so far, and what challenges do they face? \nWhat are the barriers hindering the full utilization of LLMs in industrial applications, and how can these barriers be overcome? \nHow can various industries advance to maximize the utility of LLMs in practical applications? Subsequently, with an aim to address guiding research questions, we perform a thorough scoping survey of existing research publications from industrial entities of all sizes. Finally, we discuss our takeaways and insights and present a practical pilot scenario-based guide for industries to adapt to LLMs in a more informed manner.\nThe key contributions of this work can be summarised as: this study identifies various categories of challenges associated with LLMs for industrial adoption and proposes potential solutions. These challenges broadly relate to data confidentiality, reliability of LLM responses, infrastructure bottlenecks across industries, domain-specific adoption, synthetic data generation, and ethical concerns. Additionally, we offer a practical guide tailored for small, medium, and large industries to maximize the utilization of LLMs."}, {"title": "Related Work", "content": "In the literature, numerous studies focus on practical and ethical challenges associated with LLMs across diverse application domains includes education (Yan et al., 2024), finance (Li et al., 2023d), healthcare (Zhou et al., 2023) and security (Shao et al., 2024). Additionally, several studies address the task-specific challenges for LLMs' adoption in areas such as spoken dialog systems (Inoue, 2023), mathematical reasoning (Ahn et al., 2024), mining software repositories (Abedu et al., 2024). Moreover, studies explore the challenges based on LLMs capabilities with explanations generation (Kunz and Kuhlmann, 2024), data augmentation (Ding et al., 2024), support for multilingual context (Shen et al., 2024) and compliance with ethical challenges (Jiao et al., 2024).\nClose to our work, Gallagher et al. (2024) addresses a few concerns on the adoption of LLMs for specific high-stake applications, particularly intelligence reporting workflows. In contrast to existing studies, our work specifically concentrates on the utilization of LLMs for industrial applications. Moreover, this study provides a comprehensive overview of several roadblocks to LLMs adoption for industrial use cases and corresponding potential solutions. Additionally, our study offers a suggestive guide to maximize the utilization of LLMs for various industries."}, {"title": "Methodology", "content": "This section aims to understand, how have industries adopted LLMs so far, and what challenges they face? (RQ1)."}, {"title": "Industrial Case Study on LLMs", "content": "We conduct an industrial case study to understand, how the LLMs are shaping industry practices, identifying both challenges and benefits. Through a meticulous process of expert consultation and iterative refinement, the questionnaire was designed to capture insightful data and serve as a tool for understanding the evolving role of LLMs in the industry. This case study covers a multitude of aspects related to LLM usage for specific application domains, corresponding risks, trust attributes, and challenges. In crafting a succinct questionnaire, our objective was to gauge the adoption and impact of LLMs in various industries. These questions can be found in Appendix B Table 4. We receive 26 responses in total from real-world practitioners of the IT industry."}, {"title": "Quantitative Analysis", "content": "Based on the responses obtained from the industrial case study, we make the following observations.\nParticipants of the case study. We shared the questionnaire with the IT professionals, who are either working on LLMs or developed some solutions. The participants are industry professionals and practitioners ranging in their expertise from beginner to expert level.\nWidely adapted applications by leveraging LLMs. Even though LLMs are being utilized for various applications, we observe that the majority of the industrial applications are related to financial, retail, security, and healthcare domains.\nModality of the datasets. More than 60% of the industry practitioners prefer to use either textual or tabular data as shown in Figure 1.a.\nWidely used LLMs. Our case study suggests that more than 50% of the applications utilize the GPT-3.5 and GPT-4 models. Recently, researchers have been assessing the capabilities of LLaMA-2 (Touvron et al., 2023) and Mistral (Jiang et al., 2023a).\nPrompting strategy. We observe that zero-shot, few-shot, and in-context learning prompting strategies are widely adapted compared to Fine-tuning.\nRisks associated with LLMs. Based on our case study, LLMs pose risks associated with security and safety, quality of service, and license-related challenges as depicted in Figure 1.b.\nTrust attributes to be considered. We observe that robustness, security, and hallucination are the major attributes that need to be considered to utilize the LLMs as shown in Figure 1.c.\nMoreover, to gain a better understanding of the barriers to leverage the LLMs for industrial use cases, we survey 68 research papers specifically from the industry. With this study, we compile several prominent challenges and collate potential solutions to address them. The selection criteria for the papers can be referred to in the Appendix A."}, {"title": "Challenges and Potential Solutions", "content": "In this section, we explore several barriers to leveraging LLMs for industrial applications and discuss potential solutions (RQ2)."}, {"title": "Data Confidentiality", "content": ""}, {"title": "Pre-training data issues", "content": "Potential privacy risks. To deploy large language models (LLMs) on cloud platforms, robust data privacy protocols are required to handle extensive sensitive datasets while pre-training. Key challenges include mitigating data breaches and preventing unauthorized extraction of sensitive information. Despite the adoption of LLMs in applications like disaster response management (Goecks and Waytowich, 2023), public health intervention (Jo et al., 2023), and assisting AAC users (Valencia et al., 2023), there is a noticeable lack of focus on privacy and security aspects. Moreover, it is imperative to address the potential risks associated with deploying LLMs in high-stakes scenarios.\nRegulations. GDPR in Europe and CCPA in California introduce stringent guidelines for deploying LLMs by enforcing strict data handling and intellectual property rules for transparency and fairness. As highlighted by Mesko and Topol (2023), adhering to these laws in sensitive domains like healthcare is crucial to avoid harm and protect privacy.\nPotential solution. Developing a comprehensive framework that aids in LLM compliance is essential for responsible use and interaction with users."}, {"title": "Usage of APIs", "content": "To access the closed-source LLMs, passing the commercial data through third-party APIs raises potential privacy concerns (Laskar et al., 2023).\nPotential solutions. 1. Implementing robust security and privacy techniques, such as federated learning, is essential to safeguard user data while maintaining the functionality of LLMs, 2. A strategic way of crafting prompts is essential to avoid PII leakage (Kim et al., 2023)."}, {"title": "Reliability of LLMs' Responses", "content": "Control the level of AI proactivity. LLMs should reduce social awkwardness, improve expressiveness, and adapt to different scenarios (Liu et al., 2023c; Urlana et al., 2023). The open-ended generation of LLMs makes it challenging to customize dialog systems for public health intervention applications (Jo et al., 2023).\nOutdated knowledge. The open-endedness of LLMs often leads to hallucinations due to a lack of an updated knowledge base (Faizullah et al., 2024). Additionally, the training data might contain errors and become outdated over time.\nPotential solutions. Techniques such as Retrieval-Augmented Generation (RAG) are effective in reducing hallucinations. However, RAG systems struggle with complex questions requiring additional information and generate out-of-context content. Moreover, techniques such as diverse beam search (Vijayakumar et al., 2016), confident decoding (Tian et al., 2019) are promising in mitigating hallucinations. Additionally, model editing techniques (Hoelscher-Obermaier et al., 2023) can address the unintended associations, enhancing the practical usage of LLMs."}, {"title": "Infrastructure Accessibility", "content": "Carbon emissions. Infrastructure is crucial for deploying LLMs, influencing factors like processing speed, latency, cost, and training needs. High-performance hardware is necessary to boost speed and reduce latency, enhancing user experience but demanding careful budgeting due to high costs. Achieving an optimal balance between cost and performance is crucial for the efficient training and scalability of LLM applications.\nPotential Solution. Implementing robust small language models lead to reduced carbon emissions."}, {"title": "Domain Adaption", "content": "Lack of domain-specific datasets. The ability of LLMs in the finance and medical domains is lacking due to insufficient (Liu et al., 2023a) domain-specific training data in the foundation models (Li et al., 2023c). Consequently, the current versions of GPT-4 and ChatGPT do not meet the industrial requirements to build financial analyst agents (Li et al., 2023c). While LLMs can generate relevant reasoning, they fall short of the desired standard, indicating significant room for improvement.\nLack of expert domain data. Majority of the LLMs struggle to perform the expert data domain tasks (Alfassy et al., 2022), such as image-text and text-image retrieval in technical documentation.\nDiversity. Currently, LLMs fail to mitigate social bias due to the absence of diverse demographic groups (Lee et al., 2023) data in the training data. Foundation models should give equal importance to aspects such as ethnicity, nationality, gender, and religion. At present, most LLMs reflect the collective intelligence of Western society.\nIn-context learning (ICL). The scope of in-context learning is limited by its pretraining data (Han et al., 2023). It is unlikely that any model will perform well when using ICL with data significantly different from its pretraining data.\nPotential Solution. 1. Pre-training data should consist of various domain mixtures; however, finding the right mixture is an open challenge. 2. LLMs should be carefully tested to ensure they treat marginalized individuals and communities equally (Kotek et al., 2023). 3. Continuous pre-training can help overcome the drawbacks of the in-context learning strategy."}, {"title": "Data Creation Using LLMs", "content": "Few works attempt to generate synthetic datasets by utilizing LLMs. However, four major concerns exist with using LLMs for synthetic data creation/annotation; 1). Lack of diversity. Synthetic datasets may lack diversity due to the limited knowledge base (Ramakrishna et al., 2023) of LLMs, 2). Quality and resource requirements. The quality of the annotated data might increase with the size of the LLM used for the annotation (Sun et al., 2023). However, leveraging large LLMs requires higher computational resources, 3). In-context learning (ICL) challenges. ICL is a widely adopted approach for textual task data annotation tasks (Li et al., 2023c). However, the challenge lies in responsibly incorporating the model's output to deliver value to users without misleading them or inadvertently amplifying malicious behavior (Deng et al., 2023), 4). Performance of complex tasks. The performance of models degrades when using synthetic data for complex tasks.\nPotential solution. Currently, tools like FABRICATOR (Golde et al., 2023), support tasks like classification, sentence similarity and QA for data labeling and other tasks should be explored."}, {"title": "Sub-standard Performance of LLMs", "content": "Code generation. LLMs' coding ability is limited to generate general-purpose coding tasks. However, the generation of high-quality code for complex network management tasks remains challenging (Mani et al., 2023). Moreover, LLMs have limited capabilities in repository-level coding tasks except in C and Python languages (Bairi et al., 2023) and fail to complete code with potential bugs (Dinh et al., 2023). Most of the code-LLMs struggle with code completion tasks, with undefined names and unused variables (Ding et al., 2023) being the most prominent static error cases.\nConversational applications. LLMs face challenges in providing emotional support and maintaining long-term memory, impacting their effectiveness in conversational applications (Jo et al., 2023). Future research on a longitudinal deployment of LLM-driven chatbots for public health interventions would help understand how users' engagement changes over time.\nMultilingual. Maximizing LLMs' potential in healthcare, retail, and finance requires high-quality, labeled datasets. Additionally, with most LLMs being limited to English, there is significant room for creating robust multilingual models.\nMulti-modal. With the rising demand for multimodal applications (Feng et al., 2023; Lu et al., 2023b), only a few studies focused on utilizing LLMs for such industrial applications. More efforts are needed to integrate LLMs with voice assistants and Robotics (Yamazaki et al., 2023)."}, {"title": "Evaluation of LLMS", "content": "In sectors like legal, finance, and healthcare, blending LLMs with human feedback is crucial to lowering false positives, underscoring the importance of human oversight in safety-critical applications (Liu et al., 2023b). Moreover, our analysis (see Appendix C) reveals that less than 15% of studies conduct human evaluations to assess LLM outputs, indicating a need for more rigorous validation methods. Evaluating long-form question answering is challenging for LLMs (Zhao et al., 2023), as additional contextual information may not always be available in practical QA scenarios. Current metrics like BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) primarily evaluate the explanation task, but these measures are insufficient for assessing the reasonableness of LLM responses."}, {"title": "Ethical Concerns", "content": "The most common ethical challenges with LLMs are violation of the model license, model theft, copyright infringement, producing harmful content, and trustworthiness (Foley et al., 2023) and following are the potential solutions .\nProtecting LLMs. Watermarking techniques (Peng et al., 2023) are essential for copy-right protection of industrial LLMs, aim to minimize the adverse impact on the original LLM.\nBuilding trust. LLMs face gender bias issues while generating professional documents like recommendation letters (Li et al., 2023c). Further, most of the studies are limited to understanding gender bias in male and female pronouns, indicating a need for more research on marginalized groups, such as the LGBTQIA+ community. Moreover, AI-assisted writing should be employed judiciously to prevent reinforcing gender stereotypes (Wan et al., 2023). Increasing trustworthiness involves prioritizing the addressing, opposing, and limiting of further proliferation of bias (Kotek et al., 2023). Recent studies also suggest to conduct experiments with LLMs' at least three times to validate the results (Chen et al., 2023).\nEnhancing creativity. The involvement of AI models should not aim to replace the author's thinking but rather enhance it by producing new ideas and discover connections or findings (Shen et al., 2023).\nFairness in data visualization. Developing interactive data visualization techniques/frameworks to identify the hidden biases and ensure fairness (Kwon and Mihindukulasooriya, 2023).\nLinking models. Techniques such as LLM Attribution (Foley et al., 2023) are promising for linking fine-tuned models with their pre-trained versions.\nProtecting integrity. To maintain the integrity of LLMs, implementing guardrails such as NeMo (Rebedea et al., 2023), LangKit, and TrustLLM (Sun et al., 2024) helps to prevent the generation of biased, offensive, or inaccurate responses.\nAddressing these challenges requires a combination of technical expertise, ethical considerations, and ongoing research efforts. As shown in Figure 2, we categorize each paper (total of 68) based on its application life cycle and observed that, due to the above-mentioned pitfalls, more than 70% of LLM-based studies are still in the conceptual phase."}, {"title": "Maximizing LLM Utilization Across Industries", "content": "This section offers a suggestive guide to various industries to maximize the utilization of LLMs for Natural Language Generation (NLG) applications"}, {"title": "Limitations", "content": "Our study has the following limitations.\nScope. To provide a practical guide to various industries, we restrict our scope to only Natural Language Generation (NLG) applications. Prospective works should focus on providing an extensive guide to various other tasks as well.\nCoverage. With the rapid development of LLMs and the voluminous research in this field, it's not feasible to comprehensively cover all the papers. Recognizing this, our survey has focused specifically on industry-related papers. This allowed us to delve deeper and gain an understanding of the unique requirements and challenges faced within industrial applications of LLMs.\nConfidentiality. Due to the confidential nature of the industrial applications not many details were available for specific scenarios or challenges. Hence, we only focused on providing recommendations/insights that can be applicable to a broad range of industrial applications."}, {"title": "Ethics Statement", "content": "To our knowledge, this study presents minimal ethical concerns. However, to maintain transparency, we provide a detailed analysis of all 68 papers present in the survey in Section C. Each paper is reviewed by at least three individuals to validate its claims and findings. We conduct the industrial case study, following the guidelines outlined by the ACL ethics review policy 2, thereby Ethics Review Boards (ERB) approval is not necessary. It's important to note that our research involving human subjects does not entail the collection of any medical or sensitive information from the users."}]}