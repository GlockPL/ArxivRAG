{"title": "Improving Trust Estimation in Human-Robot Collaboration Using Beta Reputation at Fine-grained Timescales", "authors": ["Resul Dagdanov", "Milan Andrejevi\u0107", "Dikai Liu", "Chin-Teng Lin"], "abstract": "Abstract\u2014When interacting with each other, humans adjust\ntheir behavior based on perceived trust. However, to achieve\nsimilar adaptability, robots must accurately estimate human\ntrust at sufficiently granular timescales during the human-\nrobot collaboration task. A beta reputation is a popular way to\nformalize a mathematical estimation of human trust. However,\nit relies on binary performance, which updates trust estimations\nonly after each task concludes. Additionally, manually crafting a\nreward function is the usual method of building a performance\nindicator, which is labor-intensive and time-consuming. These\nlimitations prevent efficiently capturing continuous changes in\ntrust at more granular timescales throughout the collaboration\ntask. Therefore, this paper presents a new framework for the\nestimation of human trust using a beta reputation at fine-\ngrained timescales. To achieve granularity in beta reputation,\nwe utilize continuous reward values to update trust estimations\nat each timestep of a task. We construct a continuous reward\nfunction using maximum entropy optimization to eliminate\nthe need for the laborious specification of a performance\nindicator. The proposed framework improves trust estimations\nby increasing accuracy, eliminating the need for manually\ncrafting a reward function, and advancing toward developing\nmore intelligent robots. The source code is publicly available.", "sections": [{"title": "I. INTRODUCTION", "content": "Human decisions are often influenced by their perceptions\nof how trustworthy they are perceived by others [1], [2].\nResearch in human-robot collaboration (HRC) indicates that\nwhen robots act in accordance with a human co-worker's\ntrust, collaboration effectiveness is enhanced [3]\u2013[5]. How-\never, to make trust-aware decisions, robots need to accurately\nestimate how much their co-worker trusts them [6], [7].\nTrust in a robot can change throughout a task, making it\nessential for the robot to estimate trust in real-time at fine-\ngrained timescales. By continuously estimating trust during\nthe task rather than only at its conclusion, the robot can adapt\nits behavior immediately, either enhancing or reducing trust\nto address the pitfalls of overtrust or undertrust [8]\u2013[10].\nThere is growing HRC research interest in computational\nmodels to estimate human trust toward robots [11]\u2013[14].\nThese models are based on robot performance, which is the\nmost significant factor influencing human trust [10], [15],\n[16]. Furthermore, probabilistic models that capture uncer-\ntainty and bias in human subjectivity show great promise\nin this context [17]\u2013[20]. Consequently, the proposed frame-\nwork in this paper entails a probabilistic estimation of human\ntrust based on robot performance, as illustrated in Fig. 1.\nThe probabilistic models proposed in [17]\u2013[20] fail to\ncapture the continuous changes in human trust as a robot\nperforms a task. This limitation arises because human co-\nworkers assess performance in a binary manner (e.g., success\nor failure) only after task completion, neglecting performance\nchanges during the task. This results in a static estimation of\ntrust dynamics, often referred to as a \"snapshot\" view [19].\nAn intelligent robot needs to adjust its behavior in real-\ntime in response to changes in human trust in order to address\nthe pitfalls of overtrust or undertrust. For instance, trust may\nshift during a collision avoidance task if the robot navigates\ntoo close to obstacles, raising concerns about its reliability\nand safety. Capturing these trust dynamics in real-time is es-\nsential because it could enable the robot to adapt its behavior\nimmediately rather than wait until the task is complete. In\nthis example, the robot could deliberately navigate around the\nobstacle to prevent further deterioration of trust. The robot\ncould account for performance and trust-related objectives if\nit estimates trust at fine-grained timescales."}, {"title": "II. LITERATURE REVIEW", "content": "A computational model is necessary for the estimation of\nhuman trust. One straightforward way is to construct trust\nestimation as a linear combination of performance features\nthat influence trust in HRC [13], [24]. Such models do\nnot incorporate uncertainty, which is advantageous in some\ntasks. For example, in repetitive assembly tasks where the\nrobot's reliability is consistent and predictable, deterministic\nmodels of trust are applied [24], [25]. However, in cases\nwhere it is important to capture human subjectivity, these\nmodels fall short. This is because human perceptions and\ndecision-making typically involve uncertainty [1], [2], [26].\nTherefore, human perceptions of trustworthiness most likely\ninclude subjective uncertainty, which informs their decisions.\nSimilarly, for robots to adjust their behavior based on human\ntrust, they must capture uncertainty in their estimations.\nTo capture uncertainty in trust estimations, it is required\nto apply probabilistic models. A typical probabilistic model\nis a dynamic Bayesian network. However, this model lacks\na mathematical framework to describe how human trust\nstabilizes over time through repeated collaborations with\nthe same robot [27]. A more suitable alternative is the\nbeta reputation, which has been proposed to address this\nlimitation [19]. In this model, a beta distribution offers two\nmain advantages [28]. Firstly, this model limits an estimation\ninterval to 0 and 1, creating consistency with the trust mea-\nsurement scale. Secondly, this model accounts for a historical\nreputation by accumulating the number of successful and\nunsuccessful collaborations. These advantages make the beta\nreputation suitable for the probabilistic estimation of trust.\nTo reduce the labor-intensive workload in designing trust\nmodels, effective use of co-worker demonstrations is re-\nquired. These demonstrations provide insight into how a\nco-worker expects a robot to perform tasks [29]. Note that\ntrust dynamics in HRC closely depend on the co-worker's\nexpectations of the robot's capabilities [15], [30]. Thus, these\ndemonstrations are a critical resource for modeling trust, as\nthey reflect co-worker expectations of the robot's behavior.\nThe maximum entropy optimization method can quantify\nthe similarity mismatch between co-worker demonstrations\nand a robot's capabilities [31]. It was applied to construct re-\nward and trust estimation models using demonstrations [18].\nHowever, by clustering similar states into a fixed number\nof decision-making policies, this method loses flexibility\nin environments where decision-making parameters are not\nconstant. Furthermore, recent work in [20] has applied this\nmethod to learn personalized weights in trust estimation. For\nexample, one co-worker may prioritize safety, while another\nmay emphasize a robot's speed as a critical trust indicator."}, {"title": "III. PROBLEM FORMULATION", "content": "Let us consider a decision-making policy where an in-\ntelligent robot performs the task in alignment with human\ndemonstrations. The expression for this process is a Markov\ndecision process (MDP), denoted as M := (S, A, r, f, T). At\neach timestep (step number) t in a task that concludes after a\ntotal of T timesteps, $s_t \\in S$ represents a robot state vector,\nand $a_t \\in A$ represents a robot action vector. An intelligent\nrobot performs an action under the decision-making policy\nand then transitions to a new state $s_{t+1} = f(s_t, a_t)$ after\nreceiving a reward $r(s_t, a_t)$, where f is a transition function.\nRobot operations can take various forms, such as kines-\nthetic navigation, audio, and visual communication. In this\nwork, operations performed by a robot are robotic arm\nmanipulations under the control of a decision-making pol-\nicy. As a robot performs actions, it transitions to a new\nstate based on transition function dynamics, similar to\nthose described in [21]. A consecutive sequence of these\ntransitions is a spatial trajectory, which is represented as\na finite set of T timestep state-action pairs, specifically\n$\\xi = \\{(s_1, a_1), (s_2, a_2), \\dots, (s_T, a_T)\\} \\in \\Xi$. To simplify\nthe notation, as adopted from [21], a trajectory can be\ndenoted in a compact form as $\\mathbf{s} = (s_1, a_1, a_2, \\dots, a_T)$.\nHuman demonstrations refer to samples where a co-\nworker physically shows the robot how to perform a task\nby manipulating the robotic arm, like the kinesthetic inter-\nactions described in [13]. The notation for a dataset of N\ndemonstrations is $D = \\{\\xi^1, \\xi^2, \\dots, \\xi^N\\}$, where $\\xi^n$ is\na sample trajectory demonstration by a human co-worker.\nIn this paper, a human is the trustor, and a robot is the\ntrustee. The concept of human trust is defined in [30] as \"the\nattitude that an agent will help achieve an individual's goals\nin a situation characterized by uncertainty and vulnerability\".\nThis definition of trust aligns with the engineering aspects\nand the goal-oriented nature of a robot in HRC.\nIn HRC literature [7], models of human trust estimations\nare categorized into relation-based and performance-based\nmodels. Performance-based models estimate trust primarily\nbased on the capability and reliability of a robot. In contrast,\nrelation-based models use data on the societal and ethical\nnorms of a human as a trust estimation feature. This paper\npresents a performance-based model of human trust because\nthe engineering objective is to enhance the ability of a robot\nto perform physical tasks with high performance.\nThe performance of the robot in HRC is the most dominant\nfactor affecting human trust in the robot [10], [15], [16].\nTrust depends on the successful and unsuccessful reputation\nof collaboration with the robot. In this paper, a success\nmetric of collaboration is the compatibility between human\nco-worker expectations and robot capabilities, which serves\nas a key indicator of changes in trust [29]. Based on these\nfindings, we present a performance-based model of trust."}, {"title": "D. Robot Decision-Making Policy", "content": "To enhance the robot's generalization capabilities, partic-\nularly in high-dimensional action spaces, a learning-based\nmethod for robot decision-making needs to be implemented.\nOne common approach to doing this is to maximize the sim-\nilarity between expert demonstrations and robot operations,\nalso known as behavior cloning (BC). This approach comes\nwith a major limitation. Mimicking human actions may result\nin a suboptimal decision-making policy for the robot, as\nhumans do not always act optimally. However, there is also a\nbig advantage to this approach. As discussed in Section III-C,\nthe similarity mismatch between co-worker demonstrations\nand robot operations can serve as a key indicator of changes\nin trust, which can be captured via the reward function.\nFor this reason, in our framework, the human co-worker's\ndemonstrations are treated as optimal (i.e., expert).\nA common practice in formulating a robot decision-\nmaking policy is to assume that human demonstrations\nresemble a Gaussian distribution [21], [23]. Consequently,\na robot policy $\\pi_\\theta(a_t | s_t) \\sim \\mathcal{N}(\\mu_\\theta(s_t), \\sigma_\\theta(s_t)^2)$, where\n$\\mu_\\theta(s_t)$ and $\\sigma_\\theta(s_t)$ denote the mean and variance of the policy\ndistribution, respectively. In this manner, $\\mu_\\theta$ and $\\sigma_\\theta$ serve as\nparameters of a nonlinear neural network.\n$\\underset{\\theta}{argmin} \\frac{1}{N_D} \\sum_{(s_t, a_t) \\sim D} \\left[ \\frac{\\eta}{2} \\cdot \\log \\sigma_\\theta(s_t)^2 + \\frac{(a_t - \\mu_\\theta(s_t))^2}{2 \\cdot \\sigma_\\theta(s_t)^2} \\right]$\nEq. 1 represents a minimization objective function for\noptimizing a robot decision-making policy. The $\\eta$ coefficient\nacts as a scaling factor for regularizing a policy variance.\nWhen $\\eta$ is large, a policy optimization prioritizes minimizing\na decision-making uncertainty. In contrast, when $\\eta$ is small, a\nrobot performs more stochastic actions, leading to increased\nexploration. Section IV-A provides a detailed description\nof how $\\eta$ varies throughout this data-driven optimization\nprocess, which occurs in Stage-2 of Fig. 2."}, {"title": "E. Reward Function Optimization", "content": "Formulating a reward function that accurately captures\nthe context-dependent performance of a task is laborious\nand time-consuming [21]\u2013[23]. This complexity arises from\nthe challenge of determining appropriate weights that align\nthe reward function with desired outcomes. One solution to\nthis problem is to use a data-driven approach, specifically\nmaximum entropy (MaxEnt) optimization [31], to construct\na reward function that accurately captures the performance\nof robotics applications [23], [32]. So, to eliminate the need\nfor laborious performance specifications, we use MaxEnt\noptimization to construct a continuous reward function.\nIn the early learning epochs of BC in Stage-2 of Fig. 2,\na robot performs suboptimal actions. As the optimization\nprocess in Eq. (1) proceeds, a decision-making policy gets\ncloser to an optimal policy. The main idea behind Max-\nEnt optimization is to iteratively sample trajectories from\n$p(\\xi) \\sim exp(R(\\mathbf{g}))$. The goal is to match features between\nrobot trajectories $\\mathbb{E}_R$ and human demonstrations $\\mathbb{D}$.\nThe proposed framework does not focus on learning a\nrobot policy by maximizing cumulative rewards. Instead, the\naim of this paper is to learn a robot policy through BC by\nenabling it to mimic demonstrations (see Section III-D).\n$R(\\xi) = \\frac{1}{T} \\sum_{(s_t, a_t) \\in \\xi} r_\\psi : (S, A) \\rightarrow [-1, 1]$\nFormulating a linear reward function in high-dimensional\nenvironments is challenging and often impractical. A non-\nlinear approach is necessary to construct a reward function\nfor these environments. A widely adopted method for cap-\nturing performance is the use of nonlinear neural networks,\nwhich provide flexibility and adaptability in data-driven\nsolutions [23]. Therefore, we construct a reward function in\nEq. (2) using a neural network with parameters $\\psi$.\n$L_{\\text{MaxEnt}} = - \\mathbb{E}_{D} \\left[\\log p(\\xi | \\psi) \\right] = - \\frac{1}{|D|} \\sum_{\\xi \\in D} \\log \\frac{exp(R_{\\psi}(\\xi))}{\\mathcal{Z}(\\psi)}$\nEq. (3) is a loss function of MaxEnt optimization. There\nare infinitely many discrete possible states and actions for\nthe background partition function $\\mathcal{Z}(\\psi) = \\int exp(R_{\\psi}) d\\xi$\nto calculate when S and A are both continuous.\n$\\mathcal{Z}(\\psi; \\theta) \\approx \\frac{1}{|\\Xi_R|} \\sum_{\\xi \\in \\Xi_R} \\frac{exp(R(\\xi))}{p(\\xi; \\theta)}$\nA stochastic sampling-based method is a popular tech-\nnique for approximating $\\mathcal{Z}(\\psi; \\theta)$, as proposed in [23]. This\nmethod, as shown in Eq. (4), approximates close to the\nexpectation of the negative log-likelihood loss in Eq. (3). In\nthis paper, $\\mathbb{D}$ and $\\Xi_R$ are the sets of N demonstrations and\nM robot trajectories, respectively. Note that it is common\npractice to generate robot trajectories from $p_\\theta(\\xi)$ [21], [32].\nTo address the exploration-exploitation dilemma inherent\nin a decision-making policy and to enhance the generality of\na reward function by approximating $\\mathcal{Z}(\\psi; \\theta)$ in Eq. (4), this\npaper demonstrates an application of a dynamically anneal-\ning/interpolating linear weight $\\eta$ (see Eq. (1)). Section IV-A\ncomprehensively explains the rationale behind this choice."}, {"title": "IV. PROBABILISTIC FRAMEWORK FOR GRANULAR ESTIMATION OF HUMAN TRUST", "content": "This section outlines the mathematical foundations for\nrobot learning of human trust in a probabilistic manner. The\ndivergence between human expectations and robot actions\nin HRC are key factors influencing the dynamics of human\ntrust [7], [15], [29]. Accordingly, this section provides a\nmathematical framework for the estimation of human trust at\ngranular timescales based on a continuous reward function.\nThe denominator in Eq. (4) represents the MDP collection\nof $\\pi_\\theta(a_t | s_t, s_{t-1},...)$, which denotes the probability of\ntaking action $a_t$ at state $s_t$ according to the decision-making\npolicy $\\pi_\\theta$. Finding the exact value of the background partition\nfunction $\\mathcal{Z}(\\psi; \\theta)$ with $p_\\theta(\\xi)$ is infeasible, especially when\nthe ideal reward parameters $\\psi^*$ are unknown.\n$\\eta = \\eta_{\\text{min}} + \\frac{k}{K}(\\eta_{\\text{max}} - \\eta_{\\text{min}})$                                      \nIn [23], researchers employed an iterative method for\na decision-making policy that explored the task environment\nby performing random actions. To minimize the frequency\nof the random actions, [32] investigated the use of anneal-\ning and interpolation methods to determine the exploration\nweight factor. The framework in this paper utilizes a linear\ninterpolation method to find dynamic importance weight $\\eta$.\nThe method is given in Eq. (5), where k is the current epoch\nand K is the maximum number of learning epochs.\nIn order for a data-driven reward function to represent\ntask performance, it is necessary for a robot to explore the\ntask environment widely via random actions. During the\nearly epochs of learning, a decision-making policy under the\nobjective function in Eq. (5) prioritizes a wider exploration of\nthe task environment. Consequently, $\\eta$ gradually increases as\nlearning epochs increase. The selection of the hyperparam-\neters $\\eta_{\\text{min}} = 0.05$ and $\\eta_{\\text{max}} = 1.00$ in Eq. (5) is based on\nproblem-specific trials. It is important to note that increasing\n$\\eta_{\\text{max}}$ could lead to a less generalized reward function.\nThe objective is to gradually increase the loss function\nvariance (uncertainty) factor during the learning process of a\nrobot decision-making policy. This strategy ensures the robot\nexhibits significant uncertainty but explores more in the early\npolicy and reward learning epochs. Adequate exploration\nis crucial for achieving a generalizable reward function in\nMaxEnt optimization. This process of learning a decision-\nmaking policy takes place in Stage-2 of Fig. 2.\nThe collection of human demonstration data occurs in\nStage-1 of Fig. 2. Subsequently, the learning processes for\nthe reward function and robot decision-making policy occur\nin Stage-2. The total number of human demonstrations\nremains constant throughout the proposed framework. The\nparameters $\\theta^*$ and $\\psi^*$ represent the optimized decision-\nmaking policy and reward function obtained through MaxEnt\noptimization and BC, respectively. Once optimized, $\\theta^*$ and\n$\\psi^*$ remain fixed. Therefore, for a given state $s_t$, the frame-\nwork ensures that the reward value $r_{\\psi^*}$ is reproducible and\nthe decision-making policy $\\pi_{\\theta^*}$ always takes action $a_t$."}, {"title": "B. Beta Reputation Model at Fine-Grained Timescales", "content": "An important theoretical part of the proposed framework\nis an estimation of human trust by updating a beta probability\ndistribution at each timestep of the task.\nStage-3 of Fig. 2 involves a human co-worker testing the\ncapability of a decision-making policy and a reward function.\nEach complete cycle in Stage-3 represents one experiment,\nafter which self-reported trust is collected based on a 7-\npoint Likert scale shown in Fig. 3. While a human co-\nworker reports trust at the end of each task, a reward function\ncontinuously assigns reward values to each state-action pair.\nThis aspect enables granularity in human trust estimation.\nA beta reputation system [28]\nis a beta probability distribution. It is a common choice to\nestimate trust probabilistically [17], [19], [20], as it captures\nsubjective uncertainty and variability of human trust.\n$\\tau_q(s_t, a_t) \\sim Beta(\\alpha_n, \\beta_m)$                                                     \nEq. (6) is a formulation of a continuous probability dis-\ntribution to represent human co-worker's trust $\\tau_q(s_t, a_t)$ at\nrobot state $s_t$ and robot action $a_t$ at timestept < T of\ntrajectory $\\xi_R$. q is the total number of timesteps in all tasks.\n$\\hat{\\tau}_q(s_t, a_t) = \\frac{\\alpha_n}{\\alpha_n + \\beta_m}$                                                  \nThrough the experience\nof a human co-worker with a robot, this paper presents the\nmathematics of updating $\\alpha_n$ and $\\beta_m$ of a beta probability\ndistribution based on a reward function $r_{\\psi^*} := r_{\\psi^*}(s_t, a_t) |$\n$(s_t, a_t) \\in \\xi_R$ output at each timestep q. In Eq. (8), the\nsubscripts n and m indicate the total number of successful\nand unsuccessful state-action counts, respectively.\n$\\alpha_n =\\begin{cases}\\sum_{i=0}^{n-1}(\\gamma^i \\cdot \\alpha_{n-i-1}), & \\text{if } r_{\\psi^*} < \\epsilon \\\\ \\sum_{i=0}^{n-1}(\\gamma^i \\cdot \\alpha_{n-i-1}) + \\omega_s \\cdot r_{\\psi^*}, & \\text{if } r_{\\psi^*} > \\epsilon\\end{cases}$\n$\\beta_m = \\begin{cases}\\sum_{j=0}^{m-1}(\\gamma^j \\cdot \\beta_{m-j-1}), & \\text{if } r_{\\psi^*} > \\epsilon \\\\ \\sum_{j=0}^{m-1}(\\gamma^j \\cdot \\beta_{m-j-1}) + \\omega_f \\cdot e^{-r_{\\psi^*}}, & \\text{if } r_{\\psi^*} \\leq \\epsilon\\end{cases}$\nEq. (8) shows a threshold parameter\n$\\epsilon$ in the non-differentiable piece-wise function for the updates\nat granular timescales. We use a popular derivative-free\ndifferential evolution method [34] to optimize $\\lambda$.\nThe target objective in MLE is to minimize the negative\nlog-likelihood between a trust measurement $\\tau^*$ and a trust\nestimation $\\hat{\\tau}_q$ in Eq. (7) at the end of each experiment in\nStage-3"}, {"title": "C. Major Empirical Findings on Trust Dynamics", "content": "We review major findings from the literature on trust\ndynamics and relate them to the proposed beta reputation.\nHuman trust at the previous\ntimestep, $\\tau_{q-1}$, influences the immediate next trust, $\\tau_q$.\nResearch in [25] highlights this history-dependent nature of\ntrust. The proposed beta reputation, described in Eq. (8),\nmathematically captures this characteristic of human trust\nthrough the use of an aging factor, $\\gamma$.\nIn inference experi-\nment 3, the robot failed to bring the tile to the target position,\nas shown in Fig. 6. Trust estimation at the first timestep of\nthis experiment was 30.19%, and it continued to decrease,\nreaching 18.92% by the end of the experiment. For verifica-\ntion, human trust was self-reported as \"Moderate Distrust\"\nafter inference experiment 2 and \"High Distrust\" at the end\nof inference experiment 3. This suggests that the human\ntrust was affected by an adverse experience. This observation\naligns with previous findings in the literature, which indicate\nthat a negative experience significantly impacts trust [10].\nWhen $n, m\\rightarrow \\infty$,\n$\\sum_{i=0}^{n-1}(\\gamma^i \\cdot \\alpha_{n-i-1})$ and $\\sum_{j=0}^{m-1}(\\gamma^j \\cdot \\beta_{m-j-1})$ in Eq. (8)\ndiminish due to a discount factor $(0 < \\gamma \\leq 1)$. As a result,\nthe contribution of earlier interactions gradually becomes\nnegligible. Additionally, the terms $\\omega_s r_{\\psi^*}$ and $\\omega_f e^{-r_{\\psi^*}} |$\nin Eq. (8) only shift the beta distribution by a constant\nfactor without introducing instability in the proposed beta\nreputation. Consequently, because parameters $\\theta^*$ and $\\psi^*$ are\nconstant in Stage-3 and Stage-4, trust estimations converge\nto a stable condition after repeated collaboration with the\nsame robot, as stated in the previous research in [27]."}, {"title": "V. EXPERIMENTAL EVALUATION AND RESULTS", "content": "We conducted a case study to verify our framework for\nhuman trust estimation in HRC construction tasks. In these\ntasks, an intelligent robot transferred a tile to a target position\nwhile avoiding collisions with an obstacle (see Fig. 1). The\nrobot's state vector $(s_t \\in S)$ included distances to the\nobstacle, the ground, and the target position, following a\nconfiguration similar to those used in [21], [22]. The action\nvector $(a_t \\in A)$ represented the x, y, and z positions of the\nrobot's end-effector at each timestep t.\nIn each cycle of Stage-3 within the proposed framework\n(see Fig. 2), a human co-worker randomly set the starting\nposition of the robot's end-effector before the experiment\nbegan. An intelligent robot then utilized its decision-making\npolicy, $\\pi_{\\theta^*}(a_t | s_t)$, to plan a sequence of actions aimed\nat transporting a tile to a target position while avoiding an\nobstacle. The reward function, $r_{\\psi^*}(s_t, a_t)$, assigned a reward\nat each timestep. After the experiment, a co-worker self-\nreported trust in the robot using the scale shown in Fig. 3.\n Notably, trust measurements\ndid not always align with reward values. This discrepancy\noccurs because the reward function does not incorporate\nthe history of collaboration between the human co-worker\nand the robot. Trust estimation, however, should reflect this\nhistory-dependency [25]. As discussed in Section IV-C, the\nbeta reputation model captures the historical dynamics of\ntrust by considering cumulative interactions with the robot."}, {"title": "C. Analysis of Verification Results", "content": "In the Stage-4 experiments, the robot successfully com-\npleted the task in experiments 1 and 2, while it was unable\nto do so in experiments 3 and 4. As illustrated in Fig. 6, the\nrobotic arm failed to reach the target position in experiments\n3 and 4. In these experiments, the target was at ground level.\nAt the end of inference exper-\niment 1, the trust estimation was 20.68%, while at the\nfirst timestep of inference experiment 2, it was 19.78%,\nas shown in Fig. 6. As illustrated in Fig. 7, our model\ncontinuously updates the trust distribution at each timestep.\nThis pattern is observed in all Stage-4 experiments, high-\nlighting the history-dependent continuity in human trust\nestimations. These results are in line with the view that trust\nin human-machine systems should be continuously updated\nover interactions [25].\nDuring Stage-4 of the\nproposed framework, the parameters $\\lambda^*, \\psi^*$, and $\\theta^*$ remained\nfixed. After the MLE process in Stage-3, the success and\nfailure weight parameters were optimized as $\\omega_s^* = 3.7897$\nand $\\omega_f^* = 4.5390$. These results align with previous research\nfindings in [10], [19] that unsuccessful interactions have a\ngreater impact on trust than successful ones $(\\omega_f^* > \\omega_s^*)$ (as\ndiscussed in the previous Section IV-C).\nIn inference ex-\nperiment 1, trust estimation values steadily decreased from"}, {"title": "D. Comparison with Binary Performance-Based Trust Model", "content": "To evaluate the granular trust estimation model, we con-\nducted comparative experiments in Stage-4 of the pro-\nposed framework using the binary performance-based trust\nmodel [19]. Since the source code was not readily available,\nwe implemented the comparison model ourselves and opti-\nmized both models during the MLE process using differential\nevolution [34]. A key advantage of the proposed framework\nis its ability to provide trust estimations at more fine-grained\ntimescales without requiring a labor-intensive specification\nof real-time performance indicators. Since the human co-\nworker only self-reported trust at the end of each task, we\ncalculated the absolute error as the difference between trust\nmeasurement and trust estimation at the last timestep of\neach experiment, as detailed in Table I. The granular trust\nestimation model showed a lower average absolute error\n($\\mu_{\\text{average}} = 4.55\\%$) compared to the binary performance-\nbased model ($mu_{average} = 12.33\\%$). This improvement in\naccuracy is likely due to the adaptation of a real-time con-\ntinuous reward function, which was constructed using only\nhuman demonstrations. Additionally, the maximum variance\nin estimation error was lower for our model, indicating more\nconsistent accuracy. Overall, these results highlight that the\nproposed model outperformed the binary performance-based\nmodel in terms of accuracy, granularity, and labor efficiency."}, {"title": "VI. CONCLUSION", "content": "The proposed framework introduces a mathematical model\nfor estimating human trust toward a robot at each timestep\nduring the HRC task. By providing estimations at more\nfine-grained timescales, this model provides a more accurate\nrepresentation of human trust dynamics. Additionally, it\neliminates the laborious crafting of performance metrics by\nutilizing maximum entropy optimization to create a continu-\nous reward function, which is then used to formulate a fine-\ngrained beta reputation model.\nFuture work will focus on measuring human trust at each\ntimestep, allowing for continuous error evaluation. We also\naim to develop a real-time trust-aware robot decision-making\npolicy, enabling the robot to adapt its behavior deliberately\nto enhance or reduce its trustworthiness and immediately\naddress the pitfalls associated with overtrust and undertrust."}]}