{"title": "CORRECT: Context- and Reference-Augmented Reasoning and Prompting for Fact-Checking", "authors": ["Delvin Ce Zhang", "Dongwon Lee"], "abstract": "Fact-checking the truthfulness of claims usually requires reasoning over multiple evidence sentences. Oftentimes, evidence sentences may not be always self-contained, and may require additional contexts and references from elsewhere to understand coreferential expressions, acronyms, and the scope of a reported finding. For example, evidence sentences from an academic paper may need contextual sentences in the paper and descriptions in its cited papers to determine the scope of a research discovery. However, most fact-checking models mainly focus on the reasoning within evidence sentences, and ignore the auxiliary contexts and references. To address this problem, we propose a novel method, Context- and Reference-augmented Reasoning and Prompting. For evidence reasoning, we construct a three-layer evidence graph with evidence, context, and reference layers. We design intra- and cross-layer reasoning to integrate three graph layers into a unified evidence embedding. For verdict prediction, we design evidence-conditioned prompt encoder, which produces unique prompt embeddings for each claim. These evidence-conditioned prompt embeddings and claims are unified for fact-checking. Experiments verify the strength of our model.", "sections": [{"title": "Introduction", "content": "The proliferation of misinformation has posed growing challenge in the realm of information reliability. There is a need to develop automated fact-checking methods (Guo et al., 2022) to verify the truthfulness of real-world claims using evidence.\nExisting fact-checking models (Zhou et al., 2019; Liu et al., 2020) have shown promise in aggregating and reasoning over multiple evidence sentences to verify a claim. However, the evidence sentences retrieved from a large corpus may contain incomplete information when they are taken out-of-corpus. We need to refer to additional contexts and references from elsewhere to understand coreferential expressions, acronyms, and the scope of a reported finding. For example, Fig. 1(a) illustrates context-dependent evidence, where undefined acronym \u201cMNC\u201d in evidence sentence from a paper abstract requires additional context from the abstract to jointly interpret the meaning of acronym \u201cMNC\u201d. Fig. 1(b) presents reference-dependent evidence, where we need to check the cited paper to understand that \u201cSARS-CoV-2 infection\" and \u201cCOVID-19 infection\" are coreferential expressions, so that we could accurately fact-check the claim. Such scenario also exists in general domain where evidence sentences from a Wikipedia page may need contextual sentences in the same page and text in the hyperlinked pages to complement the insufficient information in the evidence.\nChallenges and Approach. To overcome the limitations of existing methods, we propose Context- and Reference-augmented Reasoning and prompting for fact-checking (CORRECT), to address two open questions.\nFirst, how to aggregate both contextual and referential documents into evidence reasoning? Some models are proposed to capture contextual documents, e.g., MultiVerS (Wadden et al., 2022). Some others are designed for referential documents, e.g.,"}, {"title": "Related Work", "content": "Multi-hop fact-checking. Complex claims usually require reasoning over multiple evidence sentences. Many methods are based on Language Models (Vaswani et al., 2017; Devlin et al., 2019) and Graph Neural Networks (Hamilton et al., 2017), such as GEAR (Zhou et al., 2019), KGAT (Liu et al., 2020), DREAM (Zhong et al., 2020), SaGP (Si et al., 2023), DECKER (Zou et al., 2023), CausalWalk (Zhang et al., 2024a), etc. However, they mainly focus on the reasoning within evidence sentences. They ignore the auxiliary contextual and referential documents. Methods incorporating contextual documents are proposed, e.g., ParagraphJoint (Li and Peng, 2021), ARSJoint (Zhang et al., 2021), MultiVerS (Wadden et al., 2022), etc. Some others integrating referential documents include Transformer-XH (Zhao et al., 2020) and HESM (Subramanian and Lee, 2020). However, they incorporate either contextual or referential documents, but not both. In contrast, we construct a three-layer evidence graph to model evidence sentences, contexts, and references. There are fake news detection models where auxiliary graph with Wikidata is used (Hu et al., 2021; Whitehouse et al., 2022). Fake news detection aims to detect the whole article with meta-data, while fact-checking focuses on claim sentences with retrieved evidence.\nSome fact-checking works are based on retrieval-augmented generation (Zeng and Gao, 2024). They unify evidence retrieval and claim verification as a joint approach, while our model mainly focuses on verification, and relies on external tool for evidence retrieval. Our setting is consistent with existing works (Wadden et al., 2022; Zhang et al., 2024a).\nPrompt-based fact-checking. Some models verify claims by prompting LLMs (Achiam et al., 2023). ProToCo (Zeng and Gao, 2023) inputs both evidence sentences and claim to T5 (Raffel et al., 2020). ProgramFC (Pan et al., 2023) decomposes complex claims into simpler sub-tasks and uses natural language to prompt LLMs. Varifocal (Ousidhoum et al., 2022) formulates fact-checking as question generation and answering. They rely on handcrafted natural language as prompt. The performance heavily relies on the choice of prompt, and it is difficult to design a prompt that produces a decent result, as shown in (Zhou et al., 2022b). Our model is designed with learnable prompt embeddings where the prompting instruction is naturally learned by embeddings through optimization.\nPrompt learning. Prompting (Brown et al., 2020) uses natural language as the input to language models to fulfill certain tasks. Many prompting models have been proposed, including natural language prompt (Gao et al., 2021; Shin et al., 2020) and prompt embeddings (Lester et al., 2021; Liu et al., 2023, 2022; Li and Liang, 2021). Prompting also benefits many tasks (Zhou et al., 2022a; Tan et al., 2022). However, no one has explored"}, {"title": "Model Architecture", "content": "We introduce Context- and Reference-augmented Reasoning and prompting for fact-checking (CORRECT). \n3.1 Problem Formulation\nWe are given a fact-checking dataset $\\mathcal{D} = {\\mathcal{X},\\mathcal{E},\\mathcal{C},\\mathcal{R}}$. Claim set $\\mathcal{X} = {x_i}_{i=1}^N$ contains a set of $N$ claims. Evidence set $\\mathcal{E} = {e_j}_{j=1}^E$ is a corpus of $E$ evidence sentences. For each evidence sentence $e \\in \\mathcal{E}$, we have its contextual document $c \\in \\mathcal{C}$. Usually, an evidence sentence has only one contextual document, from which this sentence is retrieved. We also have $e$'s referential documents $\\mathcal{N}_{ref}(e) = {r_{e,n}}_{n=1}^{R_e} \\subset \\mathcal{R}$. Here $R_e$ is the number of $e$'s referential documents. Evidence sentence $e$ may have multiple referential documents, such as papers cited by $e$'s paper or Webpages hyperlinked by $e$. We use $\\mathcal{N}_{ref}(e)$ to represent the set of $e$'s referential documents. We use $\\mathcal{N}_{evid}(x) \\subset \\mathcal{E}$ to denote the set of evidence sentences for a claim $x$.\nGiven $\\mathcal{D}$ as input, we design a model that uses evidence sentences from $\\mathcal{E}$ together with their contextual documents in $\\mathcal{C}$ and referential doc-"}, {"title": "Three-layer Evidence Graph Reasoning", "content": "Graph construction. For each claim $x \\in \\mathcal{X}$ and its evidence sentences $\\mathcal{N}_{evid}(x) \\subset \\mathcal{E}$, we construct a three-layer graph with evidence, context, and reference layers in Fig. 2(a). We consider evidence sentences, contextual documents, and referential documents as three types of vertices. Each type of vertices reside on their own layer. Cross-layer links between evidence layer and context layer connect each evidence sentence with its contextual document. Each evidence sentence and its referential documents are connected by cross-layer referential links. Green links in Fig. 2(a) are cross-layer links. For multi-evidence reasoning, we add intra-layer links on evidence layer where evidence sentences of a claim are fully connected, shown by black links in Fig. 2(a). The purpose of constructing three layers instead of mixing all vertices into one layer is to better differentiate three types of vertices.\nIntra-layer reasoning. Evidence reasoning includes intra- and cross-layer reasoning. We first show intra reasoning (orange arrows in Fig. 2(b)). For each evidence sentence $e \\in \\mathcal{N}_{evid}(x)$, we let $H_e^{(1)} = [h_{e,CLS}^{(1)}, h_{e,1}^{(1)}, h_{e,2}^{(1)}, ...]$ denote the output from the $l$-th Transformer step. Note that previous works call it the $l$-th layer, but to distinguish it from our three-layer graph, we instead call it the $l$-th step. $h_{e,i}^{(1)} \\in \\mathbb{R}^d$ is $d$-dimensional token embedding. We use graph neural network to aggregate different evidence sentences of a claim. For each evidence sentence $e$, we first project it by\n$h_{e,CLS}^{(1)} = W_1 h_{e,CLS}^{(1)}$\nThe [CLS] token is taken as the evidence sentence embedding, and $W_1 \\in \\mathbb{R}^{d \\times d}$ is type-specific parameter. We design type-specific attention.\n$\\alpha_{e,e'} = softmax (LeakyReLU([h_{e,CLS}^{(1)} || h_{e',CLS}^{(1)}]^T b_1)).$\n$e' \\in \\mathcal{N}_{evid}(x)\\backslash e$ is another evidence sentence for the same claim $x$, $[\\cdot || \\cdot]$ is concatenation, and $b_1 \\in \\mathbb{R}^{2d}$"}, {"title": "Evidence-conditioned Prompting", "content": "Now we integrate evidence reasoning into claim embedding to fully integrate their information for fact-checking. Prompting (Liu et al., 2023) is a powerful method in fact-checking (Zeng and Gao, 2023). However, existing models are mainly based on natural language as input prompt to language models for verdict prediction. Handcrafted discrete prompt has two disadvantages: First, it is difficult to manually design a prompt that provides a decent performance. Previous works (Zhou et al., 2022b) have shown that the change of a single word in the prompt may lead to significant deterioration of the results, and it is time-consuming to enumerate every prompt. Second, discrete natural language prompt is difficult to optimize, since language models are intrinsically continuous.\nTo mitigate these problems, we explore learnable and continuous prompt embedding. Below we design a prompt encoder, which takes evidence embedding $h_E$ as input, and produces evidence-conditioned prompt embeddings.\nEvidence-conditioned prompt encoder. We consider below continuous embeddings as prompt.\n$\\mathcal{P}_x = [h_{x,CLS}, \\pi_1, \\pi_2, ..., \\pi_M, h_{x,1}, h_{x,2}, ...].$\nHere ${\\pi_m}_{m=1}^M$ where $\\pi_m \\in \\mathbb{R}^d$ is a set of $M$ learnable evidence-conditioned prompt embeddings to be explained shortly, and $M$ is a hyperparameter, indicating the number of prompt embeddings. Each $h_{x,i} \\in \\mathbb{R}^d$ is a $d$-dimensional embedding of the $i$-th text token in claim $x$. In language models, there is an embedding look-up table before language model encoder. In this look-up table, input text tokens are first mapped to the vocabulary to obtain their token embeddings, which are then summed up with positional encodings. $h_{x,i}$ in Eq. 9 is obtained by this look-up table.\nNow we explain prompt embeddings ${\\pi_m}_{m=1}^M$, output from an evidence-conditioned prompt encoder. We first initialize $M$ base prompt embeddings, ${h_m}_{m=1}^M$. We then project evidence embedding $h_E$ in Eq. 8 to the prompt embedding space, followed by element-wise product and summation.\n$\\alpha_x = tanh (W_a h_E + b_a)^T (W_B h_E + b_B)$,\n$\\beta_x = tanh (\\frac{\\alpha_x}{\\mathcal{T}} M^T b)$,\n$\\pi_m = h_m \\odot (\\alpha_x + \\mathbf{1}) + \\beta_x.$"}, {"title": "Experiments", "content": "We conduct extensive experiments and ablation analysis to evaluate the effectiveness of the proposed model CORRECT.\nDatasets. We use 4 datasets in Table 2. FEVEROUS (Aly et al., 2021) is a general-domain dataset. Each claim is annotated in the form of sentences and/or cells from tables in Wikipedia pages. Since we focus on textual fact-checking, we follow (Pan et al., 2023) and select claims that only require sentences as evidence. We call this subset FEVEROUS-S. BearFact (Wuehrl et al., 2024) is a biomedical dataset with sentences from papers as evidence. Its original dataset does not have evidence for claims in NEI class. We follow (Zeng and Gao, 2023) and select sentences that have the highest tf-idf similarity with those claims as evidence. Check-COVID (Wang et al., 2023) contains claims about COVID-19. SciFact (Wadden et al., 2020) is a dataset with sentences in papers as evidence. As in its original paper, for claims in NEI class, we choose sentences from the cited abstract with top-3 highest tf-idf similarity with the claim as evidence.\nBaselines. We have 4 categories of baselines.\ni) Multi-hop fact-checking, KGAT (Liu et al., 2020), HESM (Subramanian and Lee, 2020), Transformer-XH (Zhao et al., 2020), MultiVerS (Wadden et al., 2022), and the recent CausalWalk (Zhang et al., 2024a). MultiVerS models contextual documents, and HESM and Transformer-XH incorporate referential documents. By comparing to them, we highlight the advantage of three-layer graph for modeling both contextual and referential documents. Since our model is built on Transformer-XH, we further extend it by modeling both contextual and referential documents, and name it Transformer-XH++. The comparison showcases the effect of evidence-conditioned prompting.\nii) Few-shot fact-checking, GPT2-PPL (Lee et al., 2021), ProToCo (Zeng and Gao, 2023), and ProgramFC (Pan et al., 2023). They are mainly designed for few-shot setting. By increasing their training set, we could also compare to them on fully supervised setting. ProToCo and ProgramFC are proposed with handcrafted natural language prompt. By comparison, we verify the usefulness of our evidence-conditioned prompt embedding.\niii) Prompt tuning is not for fact-checking. But for completeness, we convert P-Tuning v2 (Liu et al., 2022), a continuous prompting, to our task.\niv) Retrieval-augmented generation for fact-checking. Though our model is not designed with retrieval-augmented generation, we still compare to JustiLM (Zeng and Gao, 2024) for completeness.\nImplementation details. Following (Vaswani et al., 2017), we set L to 12 and d to 768. Number of prompt embeddings M is 8. Temperature T in Eq. 10 is 100. For both our model and language model-based baselines, we initialize the model with pre-trained parameters in biomedical domain (Gu et al., 2021) for scientific datasets, and in general domain (Devlin et al., 2019) for FEVEROUS-S. Each result is obtained by 5 independent runs. Experiments are done on 4 NVIDIA A100 80GB GPUs.\nWe present two experimental settings below.\nFully supervised v.s. Few-shot. For fully supervised setting, we train the model on the training set. If the dataset provides data split, we follow the split and obtain training and test sets. Otherwise, we split the dataset into 80:20 for training and test, respectively. Among training set, we further reserve 10% for validation. For few-shot setting, we report 5-shot experiments as the main results, i.e., for each class in the label set y \u2208 Y, we randomly sample"}, {"title": "Empirical Evaluation", "content": "Fully supervised setting. We follow (Wadden et al., 2022) and report Macro F1 score for both gold and retrieved evidence settings in Table 3. We also show Micro F1 score in Table 4. Transformer-XH++ consistently outperforms Transformer-XH, verifying that contextual and referential documents bring useful information. By comparing CORRECT to Transformer-XH++, we design evidence-conditioned prompting to integrate evidence and claim embeddings, and further improve the performance. Models with handcrafted prompt do not predict verdict as accurately as our model, which showcases the advantage of continuous prompt embeddings. Overall, the results on gold evidence setting are higher than on retrieved evidence setting, because the retrieved evidence sentences may not be always correct and may contain noisy information. The only exception is Check-COVID, because the retrieved evidence setting has only two labels, making the prediction task easier. MultiVerS is slightly better than CORRECT on SciFact, because the evidence sentences in SciFact contain sufficient information for fact-checking as shown in (Wadden et al., 2020, 2022), and referential documents do"}, {"title": "Model Analysis", "content": "Effect of intra- and cross-layer reasoning. We respectively remove each graph layer from the com-"}, {"title": "Conclusion", "content": "We propose a context- and reference-augmented reasoning and prompting model for fact-checking. To model contextual and referential documents, we construct a three-layer graph with intra- and cross-layer reasoning. To integrate evidence into claims, we design evidence-conditioned prompting, which produces unique prompt embeddings for each claim. A future work is to extend three-layer graph to a multi-modal graph for fact-checking."}, {"title": "Limitations", "content": "Here we identify two limitations of our work in terms of dataset and evidence type.\nDataset. Our model is proposed to incorporate contextual and referential documents of evidence sentences. We assume that the contextual and referential documents of evidence sentences are available in the dataset, or the dataset provides identifiers for evidence sentences, such as PubMed ID, so that we can use these identifiers to search their contextual and referential documents online. In Appendix B, we provide details on how to use identifiers to obtain contextual and referential documents. If the given dataset does not provide contextual or referential documents, or the identifiers of evidence sentences are not available, our model will reason within evidence sentences for fact-checking.\nEvidence type. Following existing textual fact-checking models, we propose our model to reason over textual evidence sentences only. Our model is not proposed for tabular or multi-modal evidence, thus cannot reason over these types of evidence for fact-checking. One potential future work would be to extend our three-layer evidence graph to a multi-modal graph for evidence reasoning."}, {"title": "Ethics Statement", "content": "We do not foresee any undesired implications stemming from our work. Conversely, we hope that our work can advance AI Ethics research."}, {"title": "Pseudo-code of Training Process", "content": "We summarize the training process at Algo. 1."}, {"title": "Dataset Preprocessing Details", "content": "Here we present details of dataset preprocessing.\nFEVEROUS (Aly et al., 2021) is a general-domain dataset, and each claim is annotated in the form of sentences and/or cells from tables in Wikipedia pages. In this paper we mainly focus on textual evidence sentences, thus we follow ProgramFC (Pan et al., 2023) and obtain claims that only require textual evidence for verification, and name this subset FEVEROUS-S. Claims in this dataset have two labels only, SUPPORT and REFUTE. In the original dataset, each evidence sentence may contain hyperlinks to other Wikipedia pages, and such hyperlinks in sentences are indi-"}]}