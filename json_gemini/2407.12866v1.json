{"title": "Beyond KV Caching: Shared Attention for Efficient LLMs", "authors": ["Liao Bingli", "Danilo Vasconcellos Vargas"], "abstract": "The efficiency of large language models (LLMs) remains\na critical challenge, particularly in contexts where compu-\ntational resources are limited. Traditional attention mecha-\nnisms in these models, while powerful, require significant\ncomputational and memory resources due to the necessity\nof recalculating and storing attention weights across differ-\nent layers. This paper introduces a novel Shared Attention\n(SA) mechanism, designed to enhance the efficiency of LLMS\nby directly sharing computed attention weights across mul-\ntiple layers. Unlike previous methods that focus on sharing\nintermediate Key-Value (KV) caches, our approach utilizes\nthe isotropic tendencies of attention distributions observed\nin advanced LLMs post-pretraining to reduce both the com-\nputational flops and the size of the KV cache required dur-\ning inference. We empirically demonstrate that implementing\nSA across various LLMs results in minimal accuracy loss on\nstandard benchmarks. Our findings suggest that SA not only\nconserves computational resources but also maintains robust\nmodel performance, thereby facilitating the deployment of\nmore efficient LLMs in resource-constrained environments.", "sections": [{"title": "Introduction", "content": "The rapid growth of large language models (LLM) has\nbrought forth significant challenges in terms of computa-\ntional and memory efficiency during inference. Traditional\napproaches, such as Multi-Query Attention (MQA) (Shazeer\n2019) and Grouped-Query Attention (GQA) (Ainslie et al.\n2023), have made strides in reducing the key-value (KV)\ncache size by sharing keys and values across multiple\nheads within a layer. More recently, Cross-Layer Attention\n(CLA) has extended this concept by sharing keys and val-\nues across adjacent layers, further reducing memory require-\nments without substantially impacting model performance\n(Brandon et al. 2024). Despite these advancements, the need\nfor more efficient methods continues to grow, particularly as\nmodels scale and are deployed in resource-constrained envi-\nronments.\nIn this paper, we introduce a novel method termed Shared\nAttention (SA), which significantly reduces the KV cache\nrequirements and computational load during inference for\nLLMs. Unlike previous methods that focused on sharing KV\ncaches either within the same layer or between adjacent lay-\ners, our approach inspired by the inherent similarity of at-"}, {"title": "Shared Attention", "content": "In this section we demonstrate motivation, Shared Atten-\ntion (SA) method, and the comparison to existed KV-sharing\nmechanisms."}, {"title": "Motivation", "content": "The self-attention mechanism in transformer models is typ-\nically defined as $\\text{softmax}(\\frac{QK^T}{\\sqrt{d}})V$, where Q, K, and V rep-\nresent the query, key, and value matrices respectively, and d\nis the dimension of the key vectors. This formulation neces-\nsitates the recomputation of attention weights at each layer, a\ncomputationally intensive task, particularly when the model\nis deployed in inference mode. To mitigate this, the concept\nof a KV-cache is employed, reducing the need to recompute\nK and V matrices for previously encountered tokens.\nWhile prior methodologies have focused on sharing KV\ncaches at different levels to minimize memory overhead,\nthey predominantly operate under the assumption that atten-\ntion weights differ significantly across layers, thereby ne-\ncessitating individual computations to capture diverse con-\ntextual dependencies effectively. This assumption prompts a"}, {"title": "Shared Attention", "content": "Based on the observed uniformity in attention weights,\nwe propose a novel algorithm as shown in Algorithm 1,\nShared Attention, which utilizes a single shared attention\nmatrix across multiple layers. This approach fundamentally\nredefines the operational paradigm by maintaining a con-\nsistent attention mechanism across various contextual lay-\ners, thereby reducing redundancy and enhancing inference\nspeed."}, {"title": "Comparison with Existing Approaches", "content": "The original self-attention mechanism in Transformers,\ncharacterized by the Multi-Head Attention (MHA) model,\nnecessitates caching the keys (K) and values (V) in each\nhead and layer to accelerate inference (Vaswani et al. 2017).\nThis requirement has historically imposed a significant\nmemory overhead, prompting a series of innovations aimed\nat reducing this burden.\nAmong these, Multi-Query Attention (MQA) and its more\ngeneralized counterpart, Grouped-Query Attention (GQA),\nconsolidate the KV cache by allowing multiple query heads\nwithin the same layer to share a singular set of K and V\nmatrices. This approach effectively reduces the number of\nunique key and value pairs that must be stored and retrieved\nduring the computation process. Subsequently, Cross-Layer\nAttention (CLA) extends this concept by facilitating the\nsharing of K and V matrices across different layers, thereby\noffering further reductions in the memory footprint required\nfor KV storage.\nOur method, however, introduces a fundamentally differ-\nent paradigm in addressing the challenges of self-attention."}, {"title": "Isotropic Attention Distribution", "content": "In an extensive analysis of layer-specific attention weights\nacross a spectrum of LLMs, we explored the attention dy-\nnamics within models such as Llama2-7B-chat, Llama3-8B-\ninstruct, Llama3-70B-instruct, Baichuan2-7B-chat, Qwen2-\n7B-instruct, and Qwen2-72B-instruct (Touvron et al. 2023;\nYang et al. 2023; Bai et al. 2023). These models were eval-\nuated using the MMLU.\nOur investigations reveal a self-organization pattern in the\nattention weights across these diverse models. As depicted\nin Figure 2, there exists a consistent global similarity pat-\ntern in the layers' attention weights across all tested models.\nThis pattern suggests an inherent structural characteristic in\nthe way LLMs process information, which can be broadly\nsegmented into four distinct groups:\n\u2022 Group 1: Comprising the initial layers (indices 0 and\n1), this group is situated closest to the input tokens and\nprimarily focuses on abstracting token-level semantic in-\nformation. These layers exhibit data-dependent attention\npatterns that are crucial for the initial semantic process-\ning of the inputs.\n\u2022 Group 2: This group includes layers immediately fol-\nlowing the first group and extends up to layer index 5.\nLayers in this segment demonstrate high internal similar-\nity in attention weights but are markedly different from\nthose in other groups. These layers likely serve as tran-\nsitional zones where intermediate semantic features are\nrefined."}, {"title": "Dynamics During Pretraining", "content": "To elucidate the formation and evolution of attention weight\npatterns during the pretraining phase of LLMs, we utilized\nintermediate checkpoints of the Baichuan 7B model, pro-\nvided by the model developers. These checkpoints, spanning\nfrom 0.2T to 2.6T tokens processed, offer a unique point of\nview to observe the dynamic shifts in attention mechanisms\nas the model gains exposure to an increasing volume of data.\nWe applied a consistent metric for measuring the simi-\nlarity of attention weights across layers at each pretraining\ncheckpoint. Additionally, the final chat model, fine-tuned\nto align with human reference responses, was included to\nbenchmark the evolution against practical application out-\ncomes. The dynamics of these attention weights are visual-\nized in Figure 3, which illustrates the progressive differentia-\ntion and stabilization of attention patterns across the model's\nlayers.\nAs observed in the early pretraining stage at 0.2T tokens,\nGroups 1 and 2 appear merged, indicating a less differen-\ntiated processing strategy across these initial layers. This\ncombination suggests that early in training, the model does\nnot distinctly separate token-level semantic processing from\nintermediate semantic refinement. However, as the model\nprogresses to 1.0T tokens, a clear division emerges between\nGroups 1 and 2. This separation aligns with the model be-\nginning to form more specialized and efficient strategies for\nhandling different types of information across its architec-\nture.\nThe similarity within Group 3, which encompasses the\nbulk of the model's layers, shows a marked improvement\nfrom a similarity score of 0.8 to 0.9. This increase is in-\ndicative of the model's attention mechanism stabilizing and\nbecoming more consistent in its approach to processing the\nbulk of contextual information.\nThe training advancements observed across the pretrain-\ning checkpoints not only demonstrate significant shifts in the\ninternal structure of the model's attention mechanisms but\nalso correlate positively with performance improvements on\nmultiple benchmarks. This includes results on the MMLU,\nCMMLU (Li et al. 2023), and C-Eval (Huang et al. 2024)\n5-shot accuracy tests, which have reportedly improved from\na baseline accuracy of 0.25 to 0.50 (Yang et al. 2023). This\nnotable enhancement underscores the intrinsic link between\nthe refinement of attention mechanisms within LLMs and\ntheir enhanced capabilities in natural language understand-\ning tasks.\nMoreover, further examination of the model's develop-\nment, as observed in supplementary material, reveals that\nthe similarity within Group 3-comprising the core contex-\ntual processing layers of the model\u2014continues to enhance\nafter the alignment stage. This observation suggests that the\nalignment process, typically aimed at fine-tuning the model\nto more closely mirror human-like understanding and re-\nsponse generation, also contributes to the stabilization of the\nmodel's attention mechanisms."}, {"title": "Experiments and Discussion", "content": "To validate the efficacy of our proposed Shared Attention\n(SA) method, we conducted series of experiments. These ex-\nperiments were designed to test the robustness of SA under\nvarious configurations and to evaluate its performance on\nwidely recognized benchmarks.\nInitially, we applied the SA mechanism directly to ad-\nvanced LLMs without any prior training to assess its im-\npact on pre-trained models. This experiment aimed to un-\nderstand the immediate effects of SA when integrated into\nexisting model architectures. We evaluated the performance\nof these models on standard LLM benchmarks, including\nGLUE (General), GSM8k (Arithmetic), HellaSwag (Rea-\nsoning), and MMLU (Knowledge) (Wang et al. 2018; Cobbe\net al. 2021; Zellers et al. 2019). As anticipated, the direct\napplication of SA resulted in a loss of accuracy on some\nbenchmarks. This outcome is consistent with our expecta-\ntions given the lack of retraining to adapt the models fully\nto the nuances of the Shared Attention mechanism. Due to\ncomputational constraints, it was impractical for our team to\npretrain an LLM from scratch incorporating SA.\nTo further probe the capabilities of SA under a training\nregimen, we fine-tuned base LLMs equipped with Shared\nAttention on the publicly available Instruct dataset (Taori\net al. 2023). Post fine-tuning, these models were tested\nagainst the same benchmarks to find out any performance\nchanges. This approach allowed us to measure the adapt-\nability of SA when models are trained to accommodate its\ndynamics.\nThese experiments collectively demonstrate the potential\nof Shared Attention to modify the traditional attention mech-\nanism in LLMs, showing a promising avenue for reduc-\ning computational demands while maintaining, and in some\ncases enhancing, model performance. The detailed results\nand further discussion on each benchmark and dataset are\nprovided in the subsequent sections."}, {"title": "Direct Application of Shared Attention", "content": "The application of SA was tested across discrete segments of\nlayers within the Llama2-7B and Llama3-8B models, each\ncomprising 32 layers in total. To evaluate the robustness and\nadaptability of SA as shown in Figure 4, it was implemented\nin varying layer segments, ranging from narrower spans such\nas four layers (e.g., SA:15~18) to broader spans such as\neight layers (e.g., SA:23~30).\nPreliminary assessments of SA in the earlier layers of\nLlama2-7B (e.g., layers 3 to 6) resulted in an explosion of\nperplexity, indicating significant disruptions in the model's\nability to predict subsequent tokens accurately. This phe-\nnomenon underscores the crucial role that attention score\nvariances play in the model's early stages of processing,\nwhich are essential for initial context setting and feature ex-\ntraction. To quantitatively assess the impact of attention vari-\nance throughout the model, we conducted a detailed vari-\nance analysis. We applied the same computational method\nused to obtain attention mean scores to calculate the variance\nof attention weights in Llama2-7B and Llama3-8B while\nprocessing the MMLU dataset. We further explored the po-\ntential influence of attention variance in downstream layers\nby computing a weighted cumulative variance. This metric\naggregates the variances of all downstream layers starting\nfrom each specific layer, weighted by the average of these\nsummed variances. As illustrated in Figure 5, the analy-\nsis revealed that early layers exhibited significantly higher\nweighted variances compared to latter layers. This variance\ntends to decrease as one progresses through the model's\narchitecture, suggesting a stabilization of attention mecha-\nnisms in the latter layers. Given these results, our experi-\nments predominantly focused on the application of SA in\nthe latter layers, where such variances appear to stabilize.\nThe outcomes of these experiments reveal interesting patterns. For the Llama2-7B model,\nimplementing SA in the latter layers (e.g., SA:23~26 and\nSA:27~30) maintained relatively stable performance across\na variety of benchmarks, including GLUE and MMLU. Con-\nversely, extending the scope of SA to encompass more lay-\ners, particularly mid-level layers such as SA:15~18, led to a\nnoticeable degradation in tasks requiring mathematical rea-\nsoning (GSM8K).\nIn comparison, the Llama3-8B model, which inherently\nshowed higher layer-wise attention similarity as discussed"}, {"title": "Fine-Tuning on Instruct Dataset", "content": "Given the computational constraints that preclude the pre-\ntraining of LLMs with SA from scratch, we adopted to fine-\ntune existing LLMs to evaluate whether fine-tuning could\nameliorate the performance deficits observed with the di-\nrect application of SA. This approach was particularly aimed\nat understanding the adaptability of SA under a more con-\ntrolled learning regimen.\nFine-tuning was conducted on the publicly available In-\nstruct dataset, which is designed to evaluate models on tasks\nthat require following complex instructions. This dataset\nwas chosen because it challenges the models to utilize their\nlearned representations effectively, making it an ideal bench-\nmark for testing the efficacy of modifications like SA.\nThe results, demonstrate a nar-\nrowed performance gap between the original models and\nthose modified with SA. For instance, while the original\nLlama2-7B model outperformed the SA version in direct\napplication tests, the fine-tuned Llama2-7BSA:23~30 showed\nsignificant improvements across multiple metrics. This sug-\ngests that fine-tuning enables the model to better integrate\nand leverage the Shared Attention mechanism, effectively\nregaining some of the lost performance noted in the initial\napplication of SA.\nThese findings indicate the potential of fine-tuning as a\nviable method for integrating new architectural changes like\nSA into existing models. The recovery in performance indi-\ncates that with adequate training, the initial disadvantages of\ndirectly applying SA can be mitigated, leading to enhanced\nmodel capabilities that more closely align with or even ex-\nceed their original configurations."}, {"title": "Future Directions", "content": "Our experimental investigations have demonstrated that im-\nplementing Shared Attention (SA) across multiple latter lay-\ners in LLMs arouses minimal accuracy loss, making it a\npromising approach for enhancing model efficiency. Fur-\nthermore, our analysis reveals a trend towards isotropic at-\ntention patterns during the pretraining process, indicating\nthat the models' attention mechanisms tend to stabilize as\nthey process more data.\nGiven these insights, integrating SA from the pretraining\nappears to be a particularly beneficial strategy. This early\nintegration could allow models to better adapt to the stream-\nlined attention mechanism, potentially improving perfor-\nmance and efficiency across various tasks. The foundational\nembedding of SA might simplify later adaptations and in-\nherently supports efficient attention dynamics."}, {"title": "Related Work", "content": "Efficient memory management in transformers is a critical\narea of research with diverse objectives ranging from reduc-\ning memory bandwidth and storage requirements to optimiz-ing computational costs during both training and inference\nphases. Notably, our work focuses on minimizing the size of\nthe inference Key-Value (KV) cache that persists between\nmodel passes, thereby enhancing model efficiency without a\nsignificant compromise in performance."}, {"title": "Memory Efficiency in Attention Mechanisms", "content": "Significant efforts have been made to address the efficiency\nof the KV cache post-training. Techniques such as KV cache\ncompression have been explored extensively. For instance,\nmethods like KVQuant (Hooper et al. 2024) and KIVI (Liu\net al. 2024b) employ quantization strategies to reduce the\nmemory footprint of KV pairs to just a few bits. Moreover,\nworks such as AttentionSink (Xiao et al. 2023) and Scis-\nsorhands (Liu et al. 2024a) introduce sparsity into the KV\ncache by selectively storing elements based on their proxim-\nity or importance to the generation token, thus reducing the\noverall storage requirements."}, {"title": "Architectural Innovations for Reducing KV Cache", "content": "Architectural modifications aimed at reducing the KV cache\nsize are pivotal in enhancing the efficiency of large lan-\nguage models. Such strategies include limiting the effective\nsequence length, as seen in Sparse Attention (Child et al.\n2019), which constrain attention to local windows to reduce\nboth computational load and memory overhead. Another\napproach involves replacing traditional softmax attention\nwith scalable alternatives like linear attention (Katharopou-\nlos et al. 2020), which maintains constant space complexity\nand offers more graceful scaling with respect to the token\ncount. Additionally, methods such as Grouped-Query At-\ntention (GQA) (Ainslie et al. 2023) and Multi-Query Atten-\ntion (MQA) (Shazeer 2019) aggregate attention across mul-\ntiple queries, significantly decreasing the memory footprint\nby sharing KV pairs across attention heads. These innova-\ntions collectively contribute to reducing the redundancy in\nattention calculations and are directly relevant to our work,\ninforming our development of the Shared Attention mecha-\nnism that further optimizes memory usage by sharing atten-\ntion weights across layers."}, {"title": "Conclusion", "content": "In this paper, we explored the attention dynamics within ad-\nvanced LLMs and observed that the attention distribution\nacross layers tends to isotropize following extensive pre-\ntraining. This isotropic pattern of attention, where layers\nexhibit similar attention mechanisms, inspired a novel ap-\nproach to attention sharing that departs from conventional\nmethods.\nTraditionally, methods like MQA and CLA have focused\non sharing KV caches to reduce memory overheads but\nstill required the computation of attention weights indepen-\ndently across each layer. Our proposed Shared Attention\n(SA) method bypasses this redundancy by directly sharing\nthe computed attention weights across multiple layers. This\napproach not only significantly reduces the size of the KV\ncache but also decreases the computational FLOPs required\nduring model inference.\nThe introduction of Shared Attention represents a\nparadigm shift in the design of attention mechanisms in neu-\nral networks, emphasizing efficiency without compromising\nthe model's performance. By reducing both the computa-\ntional burden and memory requirements, SA enables more\nscalable and efficient deployment of LLMs, particularly in\nenvironments where resources are constrained.\nThis research paves the way for further explorations into\nefficient model architectures and opens up new possibilities\nfor the application of LLMs across a broader spectrum of\ntasks and datasets. Future work will focus on expanding the\napplicability of Shared Attention, exploring its integration\nduring the initial phases of model training, and combining it\nwith other optimization techniques to maximize the opera-\ntional efficiency of LLMs."}]}