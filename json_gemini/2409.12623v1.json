{"title": "CamelEval: Advancing Culturally Aligned Arabic Language Models and Benchmarks", "authors": ["Zhaozhi Qian", "Faroq Altam", "Muhammad Saleh Saeed Alqurishi", "Riad Souissi"], "abstract": "Large Language Models (LLMs) are the cornerstones of modern artificial intelligence systems. This paper introduces Juhaina, a Arabic-English bilingual LLM specifically designed to align with the values and preferences of Arabic speakers. Juhaina inherently supports advanced functionalities such as instruction following, open-ended question answering, information provisioning, and text processing. Our model contains 9.24 billion parameters and is trained on a context window of up to 8,192 tokens. This paper details the creation process of Juhaina and provides an extensive empirical evaluation. Furthermore, we identify the limitations of widely-adopted Open Arabic LLM Leaderboard (OALL) and propose a new evaluation benchmark, CamelEval. Our findings demonstrate that Juhaina surpasses existing LLMs of comparable sizes, such as the Llama and Gemma families, in generating helpful responses in Arabic, providing factually accurate information about the region, and understanding nuanced cultural aspects. We aspire for Juhaina to democratize cutting-edge AI technologies, serving over 400 million Arabic speakers by offering LLMs that not only communicate in their language but also comprehend their culture. We publicly release all models on Huggingface\u00b9.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have emerged as transformative tools in artificial intelligence. LLMs are believed to significantly enhance productivity by automating complex tasks such as content creation and customer service [4]. Moreover, they enable more intuitive and accessible interactions with technology, from personalized virtual assistants to advanced language translation services.\nCurrently, approximately 400 million people worldwide speak Arabic [8]. However, Arabic content constitutes only a small fraction of the web corpus used for pre-training large language models (LLMs). For example, Common Crawl contains merely 0.6% of documents in Arabic [5]. This scarcity is compounded by the unique linguistic patterns and the rich cultural and historical connotations inherent in the Arabic language. Consequently, many existing Arabic-centric [19, 12] or multilingual LLMs [3, 17, 15] struggle to generate responses that are both useful and culturally aligned in Arabic [16, 13].\nTo better serve the Arabic-speaking community, we set the goals as follows:"}, {"title": "2 Creating Juhaina LLMs", "content": "Juhaina is created by performing rigorous post-training on the Gemma 2 family of models, which were pre-trained on up to 13 trillion tokens of primarily-English data [22]. Our post-training process comprises two stages: the supervised fine-tuning (SFT) stage and the alignment with human preference stage.\nIn the SFT stage, the Juhaina LLM is trained to reproduce responses crafted by human annotators. This involves exposing the model to a diverse set of high-quality, human-generated text examples, ensuring it learns to generate coherent and contextually appropriate responses. The goal here is to refine the model's understanding of language and improve its ability to generate text that closely mirrors human communication.\nIn the alignment with human preference stage, the LLM is further refined to adapt its style and tone to align with human preferences. This stage involves using techniques such as reinforcement learning from human feedback (RLHF), where the model's outputs are evaluated and ranked by human reviewers. The model is then adjusted based on this feedback to better match the desired style, tone, and appropriateness of responses. This ensures that the LLM not only produces accurate"}, {"title": "2.1 Data Collection", "content": "Collecting Arabic datasets was one of the main bottlenecks for developing Juhaina. Most of the open Arabic datasets are translated from other languages and are subject to translation biases or fail to reflect cultural context appropriately [23, 21, 24, 16]. Moreover, regional datasets are extremely scarce and with limited quality [20, 2]."}, {"title": "2.1.1 Data Sources", "content": "To systematically collect data, we first identified a comprehensive list of subject categories, such as \"Science\" and \u201cHumanities.\" For each subject, we further delineated sub-categories, such as \"Physics\" and \"History.\" For each identified category, we conducted a thorough search to locate and collect relevant datasets. Our search methodology encompassed the following data sources:\n1. Internal Datasets: We identified existing datasets within our database. These datasets were evaluated for relevance and quality to ensure they met our requirements.\n2. Open Web Search: We performed extensive searches on the open web to find websites containing useful data that could be scraped. This involved using advanced search techniques and tools to identify and extract data from various online sources. The data obtained from these sources was then cleaned, validated, and integrated into our dataset.\n3. Translatable Datasets: We located datasets available in different languages. These datasets were flagged for translation to ensure their effective utilization within our project. The translation process involved both automated tools and manual verification to maintain accuracy.\nDuring the search, we considered a variety of criteria, such as relevance, accuracy, and timeliness. The full list of criteria is available in the Appendix. We used a tagging system to annotate various aspects of the data and flagged data with high uncertainty for human review. We prioritized Modern Standard Arabic (MSA) data, ensured sources were reputable, and preferred cleaned data. We collected the resulting dataset into a corpus with the associated category tags for further processing."}, {"title": "2.1.2 Data Cleaning", "content": "To ensure the quality and relevance of our dataset, we implemented a multi-step data cleaning process. Initially, we applied basic filters to eliminate empty and invalid rows, which helped in removing obvious errors and gaps in the data. Following this, we employed advanced filtering techniques, including semi-regular expressions (semi-regex) and fuzzy matching. These methods allowed us to perform more nuanced cleaning, such as removing unwanted characters like emojis and filtering out non-Arabic data records, which could otherwise introduce noise and inaccuracies. We have also take care to avoid data contamination by removing data that duplicate or paraphrase the benchmarks."}, {"title": "2.1.3 Prompt Generation", "content": "We defined a list of LLM capabilities that we would like Juhaina to achieve, such as factuality and reasoning (the full list of capabilities is provided in the Appendix). For each document in the data corpus, the annotators generated prompts that invoke different LLM capabilities. Each prompt may contain a context, which is an extract or summary of the original document, followed by the task instruction or question. In some cases, the annotator also provided a \"system prompt\" that specifies the role of the LLM and the expected style of the response.\nAfter curating the initial set of prompts, we used an automated tool to tag the quality and difficulty of each prompt. Prompts with low quality are often ambiguous and lack clear instructions. We removed these low-quality prompts from the final set."}, {"title": "2.1.4 Answer Generation", "content": "We split the set of prompts into two parts: one for supervised fine-tuning (SFT) and the other for alignment through human feedback. For the SFT data, the annotators provided the ground truth"}, {"title": "2.1.5 Postprocessing", "content": "The dataset then undergoes three postprocessing pipelines: filtering, transformation, and balancing.\nThe filtering pipeline cleans the data by removing duplicates, unnecessary white spaces, foreign language content, and other irrelevant elements. It also verifies the overlap between each sample and the evaluation data to ensure there is no contamination with the test set. The transformation pipeline converts the cleaned data into the desired alignment format. During each stage, native speakers conduct manual investigations to address any non-trivial cases.\nOnce the data is processed, a further level of re-sampling is performed to improve data distribution. We sub-sample an equal number of rows for each subject category, resulting in a final dataset comprising approximately 600,000 rows for SFT and 40,000 rows for alignment."}, {"title": "2.1.6 Learnings on Data Collection", "content": "We have learned several key lessons from our data curation process.\nFocus on Data Quality. There is a well-known trade-off between quality and quantity in data collection. We found that quality matters much more than quantity in post-training. In fact, our experience shows that including low-quality data significantly hurts benchmark performance. However, in practice, we tend to overemphasize data quantity simply because it is easier to measure (i.e., just count the tokens or rows), whereas understanding data quality is quite non-trivial. Based on the problems we have encountered and resolved during the curation process, we created a checklist of common data quality issues with multi-lingual datasets:\nTag the Data. We found that extensively tagging the data greatly facilitates the curation process. It not only exposes data quality issues but also informs data selection and sampling. We adopted a variety of automated tagging systems and human annotations to understand various aspects of the data, including subject category and sub-category, task type, task difficulty, and more.\nEffective Communication with Annotators. We extensively used human annotation and feedback in the curation process. We found that effective communication with the annotators is vital for success. Care needs to be taken to ensure that the annotators understand and adhere to the desired annotation practices. Providing practical examples and a walk-through of the process helps align expectations and behaviors. Failure or delay in communication could lead to systematic errors or biases in the data, rendering the effort worthless."}, {"title": "2.2 Post-training Procedure", "content": "We note that the typical constraints associated with using LLMs as evaluators also apply to Camel-Eval. For example, there's a possibility that the judge LLM might show a preference for answers it generates itself. Additionally, other factors, like the length of the response, could introduce biases into the evaluation process. We aim to tackle these and other unresolved challenges in future updates of CamelEval."}, {"title": "2.2.1 Supervised Finetuning (SFT)", "content": "We used the Llama Pro algorithm [25] for supervised fine-tuning (SFT). Llama Pro performs block expansion on the base model to introduce additional free parameters. During the training stage, only these newly added parameters are trained. Our experiments indicate that, compared to standard SFT, Llama Pro enables the LLM to acquire additional capabilities in the Arabic language without compromising its existing capabilities in English. The training settings are detailed in Table 2."}, {"title": "2.2.2 Alignment with Human Feedback", "content": "At the conclusion of the Supervised Fine-Tuning (SFT) stage, the Juhaina model has demonstrated proficiency in following instructions and adapting to domain-specific knowledge. However, additional post-processing is necessary to ensure safety and adherence to desired behaviors, such as compliance with specific cultural norms.\nThis requirement is particularly important given the flexibility of the Arabic language and the diversity of its dialects and cultural contexts. To address these challenges, we have implemented a bespoke data curation approach aimed at achieving high data quality. In particular, the alignment data is meticulously collected and validated by native Arabic speakers to ensure accuracy and cultural relevance.\nWe used the Monolithic Odds Ratio Preference Optimization (ORPO) algorithm [10] to align Juhaina with human preference data. ORPO is inspired by the Direct Preference Optimization (DPO) algorithm [18]. Based on empirical evidence from our pilot studies, we selected ORPO for two primary reasons. Firstly, unlike DPO, ORPO does not require a reference model, which significantly reduces GPU memory usage during training. Secondly, ORPO naturally incorporates a supervised fine-tuning (SFT) loss, which is known to stabilize the training process [6].\nSimilar to the SFT stage, we adopted freeze tuning, adapting only a subset of the model parameters. Our pilot studies indicated that freeze tuning achieves better performance compared to full parameter tuning or other parameter-efficient methods, such as LoRa [11]. The training parameters are detailed in Table 2. The computing setup is the same as the SFT stage."}, {"title": "2.2.3 Infrastructure and Computing", "content": "The SFT was conducted on our computing cluster, which consists of 8 nodes, each equipped with 8 GPUs featuring 80GB of VRAM. We utilized PyTorch's Fully Sharded Data Parallel (FSDP) to efficiently manage the parallel training process. The training was completed in 3 days."}, {"title": "3 Evaluation of Arabic LLMs", "content": ""}, {"title": "3.1 Open Arabic LLM Leaderboard (OALL)", "content": "The Open Arabic LLM Leaderboard (OALL) is one of the few close-ended Arabic benchmarks [7]. It encompasses three primary benchmarks: AlGhafa [1], ACVA [9], and Arabic MMLU [13], which are all curated specifically for Arabic LLMs. OALL also include translated versions of standard LLM benchmarks such as EXAMS, ARC, BOOLQ, COPA, HELLASWAG, OPENBOOK-QA, PIQA, RACE, SCIQ, and TOXIGEN. They assess the model's capability in selecting the correct answer from yes-no or multiple-choice options in the zero-shot setting. The normalized log-likelihood of the target is used to measure the model's ability to generate the correct choice. The OALL has a centralized and managed process to evaluate all submitted LLMs. This ensures reproducibility and standardization of the results."}, {"title": "3.2 CamelEval", "content": "To address the limitations in OALL, we developed a new evaluation benchmark, CamelEval. CamelEval is based on AlpacaEval [14], an LLM-as-a-judge automatic evaluation framework for instruction-following LLMs. Essentially, two competing LLMs provide responses to a set of test prompts, and they are subsequently evaluated by a \"judge\u201d LLM to decide the win rate.\nCamelEval enables a better coverage of LLM capabilities, especially the ability to generate helpful conversations and following user instruction. Using a LLM as a judge, CamelEval avoids much"}, {"title": "4 Discussion", "content": "In this study, we have detailed the development of Juhaina, a leading open-weight Arabic LLM, and introduced CamelEval, a new benchmark for Arabic LLMs that serves as a complement to the existing OALL benchmarks. We aspire that the release of Juhaina's LLM weights, along with the CamelEval benchmark and the insights shared in this work, will assist the community in advancing the creation of improved and more culturally attuned LLMs for the Arabic-speaking world."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Criteria for Data Search", "content": "1. Relevance to Topic Criteria: The data must be directly related to our subject categories.\n2. Timeliness Criteria: The data should be up to date.\n3. Completeness Criteria: The dataset should be comprehensive enough to support robust analysis.\n4. Granularity Criteria: The data should have the appropriate level of detail.\n5. Availability and Accessibility Criteria: The data should be accessible and has an open license.\n6. Bias and Objectivity Criteria: The data should be free from bias or, if biased, the bias should be understood and accounted for.\n7. Cost: The estimated cost for accessing and curating the data."}, {"title": "A.2 List of LLM Capabilities", "content": "1. Information Provision: Delivering specific information or facts on various topics.\n2. Reasoning: Engaging in logical thinking, problem-solving, or managing complex tasks.\n3. Planning: Assisting in the creation of plans or strategies for activities and projects.\n4. Editing: Editing, rephrasing, proofreading, or performing other tasks related to the composition of written content.\n5. Coding & Debugging: Assisting with writing, reviewing, or fixing code in programming.\n6. Math: Providing help with mathematical concepts, problems, and calculations.\n7. Role Playing: Interacting with users by adopting a character or persona.\n8. Data Analysis: Interpreting data, analyzing statistics, or performing analytical tasks.\n9. Creative Writing: Crafting stories, poems, or other creative texts.\n10. Advice Seeking: Offering recommendations or guidance on various personal or professional issues.\n11. Brainstorming: Generating ideas, engaging in creative thinking, or exploring possibilities.\n12. Other: Addressing queries that do not fit into the above categories."}, {"title": "A.3 Examples of Translation Issues in OALL", "content": "Example One:\nExample Two:"}, {"title": "A.4 Subject Categories in CamelEval Curated Set", "content": ""}]}