{"title": "Provocations from the Humanities for Generative AI Research", "authors": ["Lauren Klein", "Meredith Martin", "Andre Brock", "Maria Antoniak", "Melanie Walsh", "Jessica Marie Johnson", "Lauren Tilton", "David Mimno"], "abstract": "This paper presents a set of provocations for considering the uses, impact, and harms of generative AI from the perspective of humanities researchers. We provide a working definition of humanities research, summarize some of its most salient theories and methods, and apply these theories and methods to the current landscape of AI. Drawing from foundational work in critical data studies, along with relevant humanities scholarship, we elaborate eight claims with broad applicability to current conversations about generative AI: 1) Models make words, but people make meaning; 2) Generative AI requires an expanded definition of culture; 3) Generative Al can never be representative; 4) Bigger models are not always better models; 5) Not all training data is equivalent; 6) Openness is not an easy fix; 7) Limited access to compute enables corporate capture; and 8) AI universalism creates narrow human subjects. We conclude with a discussion of the importance of resisting the extraction of humanities research by computer science and related fields.", "sections": [{"title": "1 Introduction", "content": "As humanities researchers with long histories of engaging with computational technologies, who are currently witnessing the proliferation of AI (and AI \u201chype\u201d) across nearly every aspect of society, we care deeply about its uses, impact, and harms. Whether we must learn to inhabit physical and social landscapes forever changed by AI, or whether we will soon be required to contend with the residue of a burst AI bubble, many important questions have come to the fore: What are the social, political, and historical conditions that have brought us to this point? What are the boundaries between social, political, economic, and technical systems? How are the ideas of \u201cculture\u201d and \u201ccommunity\u201d being redefined by AI researchers or the models they develop? How can we ensure that communities in the world are helped and not harmed by these models? Is scholar-driven or community-led work even possible in a research environment dominated by corporations? And what of the environmental impact of these (and our) research efforts? Is any future that might be enhanced by new developments in AI doomed to fail because of outsized corporate power? Because of the extreme environmental footprint of data centers or the exploitation of data workers? Because of narrow or unreflective assumptions about what human intelligence truly entails?"}, {"title": "2 Background: What are the humanities anyway?", "content": "The humanities have an extensive institutional and disciplinary history [26, 109, 126], but here we focus on humanities research as it is practiced in the present: to study the humanities is to investigate the human: to investigate people and groups, and the cultural objects they create [67, 117]. We do so in order to understand how these cultural objects create or reflect new forms of knowledge. In order to do so, we are trained in the history of knowledge production within our individual and overlapping fields of expertise. We are also trained in multiple languages and are trained to translate cultural and historical meaning across objects, audiences, and in an array of forms.\nHumanities research requires both minute specificity and sweeping breadth. To engage with Claude McKay's \u201cConstab Ballads\u201d (1912), a researcher must first read both normative English and Jamaican dialect filled with elisions and diacritical marks. They must then be able to analyze poetry in the context of Caribbean history, American literature, and Black studies. They must know both the material culture that informs the work's physical form as well as text encoding and web development. The humanities encompass the range of cultural objects that humanities researchers analyze, including language, history, philosophy, theory, aesthetics, and phenomena. Humanities researchers are trained to understand these concepts by examining and interpreting specific examples, and by asking what ways of knowing and thinking (individually and collectively) might have gone into their creation [4].\nWhat methods do we employ to conduct this work? Methods shared across humanities fields include theorization, interpretation, contextualization\u2014and, increasingly but not universally, collaboration. Some concrete examples might include: archival research, textual explication, disciplinary techniques such as close reading (literary studies), historical synthesis (historical fields), and media-specific analysis (film and media studies), among others. These methods are what generate the bulk of the evidence that appears in humanities scholarship. However, what more commonly travels to technical fields are the theories that this evidence points towards. These are humanistic theories: written articulations of complex ideas that, until that point, we did not yet fully understand (or we thought we did, but the evidence shows that there is still more to learn). In this paper, we summarize and synthesize many of these theories. We encourage readers who want to learn more about the methods and mechanisms of humanities research to consult the original books and papers in which they appear.\nAs should be clear, we reject formulations of humanistic thinking that have been proffered by scholars in computer science as, for example, \u201cbasically a style or mode\u201d [17]. As outlined above, our methods are instilled through rigorous training, refined through ongoing, reflection-driven interpretation, and grounded in the history of how that object or culture or concept has been interpreted to that point. Like the sciences, the humanities are also iterative; humanities disciplines build on past knowledge and they grow and they change. And the impact of this research is substantial. Humanities research teaches us what specific cultural objects might have represented to the people who created them or used them, and about the cultures that gave rise to them. These objects and cultures, in turn, point to how humans have existed in cultural and social contexts, both past and present, and how these contexts are part of what defines us at any given moment. Also key to these contexts is an understanding of how they consist of that which has not yet been discovered, digitized, transcribed, or annotated, as well as the of unrecoverable voices, the silenced narratives, and the stolen artifacts that have become symbols of empire. Humanities researchers track these complex meanings across time and place, and we share a commitment to protecting the stories of the few over the many. As part of this work we study technologies to understand how they affect societies and cultures [23, 69, 96, 111] and we use technologies in order to tell"}, {"title": "3 Related Work", "content": ""}, {"title": "3.1 Humanities Research at FAccT", "content": "Historically, humanities research has not featured prominently within the FAccT conference. Most mentions of the humanities that appear in FAccT publications appear in the umbrella term of \u201csocial science and humanities research,\u201d or because of the ACM category of \u201carts and humanities\u201d in which papers on ML, AI, and artistic practice appear. There has been a consistent acknowledgment of the unique perspectives and areas of expertise brought by humanities researchers, however, e.g. Ganesh et al.'s paper documenting a workshop in which researchers from different disciplines were brought together to discuss issues of AI fairness [56], L\u00fcnich and Keller's paper documenting a similar workshop on the topic of explainable AI [89], Bates et al.'s auto-ethnographic account of incorporating critical data studies perspectives into the computer science classroom [19], and Dotan et al.'s survey of approaches to incorporating generative AI responsibly in the classroom [48]. Most pointedly, and ironically, Raji et al.'s quantitative analysis of AI ethics syllabi across the academy, ", "Us'": "Exclusionary Pedagogy in AI Ethics Education,\u201d makes explicit mention of multiple humanities fields in the service of an argument about how, in refusing to look outside the discipline of computer science for their assigned readings, AI ethics courses are reproducing existing academic hierarchies [107].\nAn exception is philosophy, which is one of the core disciplines of the FAccT community. A recent retrospective of the FAccT conference categorized 11% of papers presented at FAccT as belonging to philosophy, and philosophy was the only humanities discipline prominent enough to be categorized individually Laufer et al. [84]. But why is philosophy privileged in this way, not only in publications at FAccT but also in public discourse about AI? Unlike other humanities disciplines, which, as discussed above, prioritize the human and the specific, philosophy can allow researchers to abstract away from the messiness of real humans toward scenarios that isolate variables of interest and are more easily interpretable; a tempting proposition for scientists who value more tractable theories of knowing, and a useful method that can more easily mesh with mathematical and technical solutions. We can see this pattern reflected in works published at FAccT which have brought philosophical approaches to algorithmic decision making [39, 71, 120]. The contributions of philosophy to FAccT and to the field of AI have been significant, but we clarify that the humanities has much more to offer than philosophy alone.\nWith that said, concepts and ideas from the humanities have consistently made their way into papers published at FAccT, often in substantive and impactful ways. A 2020 CRAFT workshop centered on speculative, queer, and feminist engagements with archives [106] while that same year saw Jo and Gebru\u2019s \u201cLessons from Archives: Strategies for Collecting Sociocultural Data in Machine Learning,\u201d a paper that has only grown in significance as the issue of LLM training data has entered broad consciousness [72]. Additional papers have focused on specific theoretical constructs from the humanities, using them to question core concepts in ML/AI research (e.g. Corbett and Denton on transparency [40] and Stark on animation [119]), and to imagine alternatives to existing approaches (e.g. Klumbyt\u00e9 et al., who explore the feminist and Black feminist concepts of situatedness, figuration, diffraction, and critical fabulation to think beyond existing modeling approaches [81]. Indeed, feminist and gender theories have perhaps received the most substantial"}, {"title": "3.2 Humanities Research on Al", "content": "The launch of ChatGPT, in November 2022, brought humanities researchers to the forefront of national conversations about generative AI and its impact on research, writing and creativity (e.g. [30, 43, 74, 113, 124, 128]. Those in the fields of digital humanities, digital pedagogy, and the history of technology, in particular-fields that require both technical and humanistic expertise-became some of the earliest expositors of its limitations and its potential e.g. ([77, 79, 110]). While these fields continue to lead within the humanities-for example, the task force convened jointly by the MLA and CCCC in Spring 2023 that has since published a series of working papers on AI and writing ([1, 2, 34]-the national conversation about AI has been usurped by corporate spokespeople parroting Al hype, whose claims in turn convince academic administrators to adopt costly and potentially harmful AI systems [114]. In fact, the increasingly politicized relationship between for-profit corporations, public resources, public institutions (like universities and research centers) and political power should make humanists, computer scientists, and researchers broadly alarmed.\nAt the same time, humanities researchers have continued to push forward research about and/or involving AI in their own fields. This includes new journals (e.g. Critical AI, edited by two literary scholars [60]), and themed sections and special issues of flagship humanities journals (e.g. American Literature's special issue on \u201cCritical AI: A Field in Formation", "AI and the University as Service": 78}, {"title": "4 Provocations from the Humanities for Al Research", "content": "In the sections that follow, we employ boyd and Crawford's framework of \u201cprovocations\u201d for big data 2012 to develop a new set of provocations that encompass the expanded research landscape of generative AI. Ours are intended to establish a stronger foundation for future AI research and cross-disciplinary collaboration, to allow for the development of more informed research questions, and to prevent non-experts from making thin claims about the full breadth of human culture, past and present. We list the full set of provocations in Table 1."}, {"title": "4.1 Models make words, but people make meaning", "content": "Humanists have wrestled for decades with the interplay between language and intention. What is the implication of language that is produced in a way that is\u2014by construction-devoid of human intention? The field of natural language processing has been shaped by decades of research in both machine learning and linguistics. This has led to explanations (and rejections) of the language generated by LLMs and related AI systems that rely on linguistic theories of \u201ccommunicative intent", "hallucinations": "hat have become a topic of research interest [82] and (justifiable) public concern. The theories cited above allow us to understand how the human tendency to create meaning through the interpretive expectations of intention and care can confer the illusion that the model \u201cknows\u201d something or someone. This is compounded by our current climate of \u201cAI hype\u201d and the nonconsensual infusion of AI into our lives, which have together naturalized artificial \u201cknowing\u201d over the complicated, messy, and polyrhythmic meaning-making conducted by humans when using language (oral, written, and otherwise) in social contexts. Humanities scholars have already been at the forefront of analyzing the output of genAI models, both text and image, not for what they \u201cknow\u201d but for what meaning their output elicits about the cultures from which they emerged.\nMeaning is always people-fueled, socially-driven, irreverent, and impossible to map consistently. Especially in the context of the Global Minority, making meaning from language also requires an awareness of the history of evading and erasing dominant meanings, and extracting additional meaning from guarded words. Take for example the work of the Translation Lab, which set out to translate a famed interview with Ousmane Semb\u00e8ne, the Senegalese filmmaker, using SmartCat, a professional AI translation platform. The team quickly found that there were few Twi and Wolof terms on suggestion, and the terms that were suggested varied widely from the meaning expressed in the original French. As humanists, we are not surprised. AI \u201cknowing\u201d is an easier solution to a complicated reality of meaning forged between language, place, time, and social relations."}, {"title": "4.2 Al requires an understanding of culture", "content": "Several years after the release of generative AI models to the public, we have ample evidence of how the text and images that they produce do not perform well in the context of non-dominant"}, {"title": "4.3 Al can never be \u201crepresentative\u201d", "content": "A recognition of the cultural complexity of historical sources, including training data, also offers a fundamentally different way to approach the issue of bias. As is widely recognized, gaps in training data, coupled with the range of biases embedded in existing training datasets, have contributed to a range of harms perpetuated by AI systems [33, 52, 99]. In response, researchers and government agencies alike often call for strategies of bias mitigation, ranging from attempts to \u201cde-bias\u201d datasets and AI systems [28], to improved documentation of both datasets [59] and models [95], to research into model interpretability [24] and explainability [44]. Each of these interventions are necessary, but the issue of bias reflects a deeper structural problem, one that will never be fixed unless the power differentials that cause structural inequalities are challenged at their source."}, {"title": "4.4 Bigger models are not always better models", "content": "As boyd and Crawford assert in their original provocations, \u201cbigger data is not always better data\u201d [29]. This applies to models as well. For the first several years of LLM development, the growth in numbers of parameters was rivaled only by the growth in the number of charts documenting the rising number of those parameters. But the axiom that bigger is better has now met its match."}, {"title": "4.5 Not all training data is equivalent", "content": "A related point to the previous provocation has to do with the nature of training data. Up until very recently, advances in generative AI have relied upon on vast, heterogeneous pretraining datasets. These draw from a wide range of sources: internet content, copyrighted books, scientific articles, and now even synthetic data from other AI models, not to mention a variety (but to be clear, not the entirety) of languages, time periods, communities, and genres [13, 32, 100]. Despite this range, pretraining data is often treated as a homogenous, undifferentiated resource\u2014often likened to \u201coil\u201d-where individual pieces of data are assumed to be interchangeable and valued primarily for their downstream utility. When individual points within these massive datasets are considered, they are typically evaluated through metrics like \u201ctoxicity\u201d or \"quality\" [88], framings that prioritize how the data will impact a model's performance. This shallow approach to pretraining data is further compounded by \u201cdocumentation debt\u201d [16]; the actual contents of pretraining data and why specific sources are chosen are rarely disclosed.\nThis reduction and obfuscation of training data is now widely recognized as an ethical problem as well as a technical one. Inspired by practices in the electronics industry, the influential \u201cDatasheets for Datasets", "\n    },\n    {\n      ": "itle", "4.6 Openness is not an easy fix": "content"}, {"title": "4.7 Limited access to compute enables corporate capture", "content": "There is no question that the capabilities of AI systems have grown tremendously, especially over the past 3-5 years. As noted by Whittaker [134], however, much of this epochal growth in capability has been fueled not by new insights, but by the concentration of resources. While the capabilities of these new models are qualitatively different, the change that enabled it is almost purely quantitative: massive datasets, massive computation clusters, and massive neural network models that are well-adapted to take advantage of current hardware. Universities and even governments have difficulty participating in this resource-intensive computing ecosystem; individual scholars have no chance."}, {"title": "4.8 Al universalism creates narrow human subjects", "content": "Generative AI has further embroiled society in a \u201cdata episteme\u201d [83], where we know ourselves through the acquisition of data and our knowledge can only be validated by gathering more and more data. Where we began with indexing ourselves, our economies, and our institutions, we are now in a moment where culture is \u201ccontent\u201d; moments of ingenuity, mundanity, and deviance"}, {"title": "5 Conclusion: Against Humanities Extraction", "content": "Against the dispiriting backdrop of late capitalism and its fueling of the corporate capture of technical research, it has been heartening to see how the substance if not the development of this research has brought attention to the role and importance of humanistic thinking in ways that"}, {"title": "Addendum", "content": "this markdown structure follows"}]}