{"title": "Improving the Stability of GNN Force Field Models by Reducing Feature Correlation", "authors": ["Yujie Zeng", "Wenlong He", "Ihor Vasyltsov", "Jiaxin Wei", "Ying Zhang", "Lin Chen", "Yuehua Dai"], "abstract": "Recently, Graph Neural Network based Force Field (GNNFF) models are widely used in Molecular Dynamics (MD) simulation, which is one of the most cost-effective means in semiconductor material research. However, even such models provide high accuracy in energy and force Mean Absolute Error (MAE) over trained (in-distribution) datasets, they often become unstable during long-time MD simulation when used for out-of-distribution datasets. In this paper, we propose a feature correlation based method for GNNFF models to enhance the stability of MD simulation. We reveal the negative relationship between feature correlation and the stability of GNNFF models, and design a loss function with a dynamic loss coefficient scheduler to reduce edge feature correlation that can be applied in general GNNFF training. We also propose an empirical metric to evaluate the stability in MD simulation. Experiments show our method can significantly improve stability for GNNFF models especially in out-of-distribution data with less than 3% computational overhead. For example, we can ensure the stable MD simulation time from 0.03ps to 10ps for Allegro model.", "sections": [{"title": "Introduction", "content": "The development and innovation of semiconductor devices rely deeply on the study of semiconductor material properties (Kim et al. 2022; Bez, Fantini, and Pirovano 2022; Orji 2019; Nakamae 2021). This research requires accurate and effective experiments and visualization of atomic scale interaction and formation. However, natural experiments are costly and time-consuming. Thus, Molecular Dynamics simulation has emerged to be a cost-effective way to study material properties and reduce detrimental defects in semiconductor materials area (Gu 2022; Zhou 2019). MD simulation is a widely used theoretical method to simulate the motion of a system of interacting particles such as atoms. It can represent the simulation results of nanomaterials depending on the availability of proper potential functions/force fields modeling interatomic forces (Thompson et al. 2022; Alder and Wainwright 1959; Rahman 1964; Frenkel and Smit 2002). These results are useful in laboratory and industrial applications in material and biology science.\nVarious Force Fields (FF) models were developed to study different aspects of material properties (Gu 2022; Zhou 2019). Classical FF can be obtained from first principles using a quantum mechanical method such as Density Functional Theory (DFT) (van Mourik, Bhl, and Gaigeot 2014). This is called Ab Inito MD (AIMD). AIMD can provide extremely high accuracy with theoretical considerations rather than empirical fitting (Iftimie, Minary, and Tuckerman 2005). However, the significant disadvantage of AIMD is that it calculates the potential with treating the electronic degrees of freedom, therefore it's limited to short simulations due to the huge computation cost. Moreover, AIMD is limited to systems that contain several hundreds of atoms. However, the demand for large-scale atom system simulations in industry has been increasing recently. Accordingly, more and more Machine Learning (ML) and Deep Learning (DL) methods are researched and applied in MD area (Anstine and Isayev 2023; Jia et al. 2020) due to the high accuracy and better scalability for large atomic systems. Among these ML based Force Field (MLFF) models, Graph Neural Networks based Force Field (GNNFF) models have shown its ability to capture the atomic interaction with graph-based system modeling (Batzner et al. 2022; Musaelian et al. 2023; Gasteiger, Becker, and G\u00fcnnemann 2021; Sch\u00fctt et al. 2017; Mailoa et al. 2019; Park et al. 2021). GNNFF models take particle position, particle features and spatial features as input to model the interactions of atoms and learn to predict particle energy and forces of the whole system. The predicted forces then are used in MD simulation tools (e.g., LAMMPS (Thompson et al. 2022)) to calculate particle positions after a time step. Recently, many GNNFF models are developed and used, such as NequIP (Batzner et al. 2022), Allegro (Musaelian et al. 2023), Gem-Net (Gasteiger, Becker, and G\u00fcnnemann 2021) and SCHNet (Sch\u00fctt et al. 2017). In this paper, we mainly focus on NequIP, Allegro and GemNet models because of their superior accuracy and scalability."}, {"title": "Motivation and key contributions", "content": "Generally, the accuracy of atom energy and forces is the most important metric for GNNFF model since a more accurate prediction result can better reveal the macroscopic properties for materials and provide valuable insights to users. However, a model with good accuracy value in energy/force MAEs\u00b9 cannot ensure stability since the accuracy value only guarantees that trained model learns the knowledge from the training data, which often can be incomplete, or biased. Thus, simulation stability is an important challenge to MD simulation methods especially in long-time simulations (Stocker et al. 2022; Fu et al. 2022; Bihani et al. 2023; Fu et al. 2023). GNNFF models may produce unstable or wrong prediction result when the learned force field is not robust enough. The simulation can enter nonphysical states and MD simulation will end up as system crash as shown in Figure 1. Therefore, improving the stability of GNNFF model is important in real application scenarios.\nBesides, in real application scenarios of MD simulation, GNNFF models are expected to be robust in as many scenarios as possible including in-distribution (ID) data and Out-Of-Distribution (OOD) data (Rajak et al. 2021). Non-stoichiometric compounds material is useful in new material property research. They exhibit different properties such as conductivity, magnetism, catalytic nature, and other unique solid-state properties, which have important technological applications (Rogacheva 2012; Kim et al. 2023; Orlov et al. 2015; Rogacheva and Nashchekina 2006; Dubey and Kaurav 2019; Kostenko et al. 2021). Therefore, it would be worthwhile if a model with high generalization can be learned and applied to different atom compositions. Meanwhile, it is necessary and crucial to improve the stability of GNNFF models, especially in the OOD dataset. Another important challenge is how to evaluate the stability of a GNNFF model in MD simulation. Since the metrics in training process cannot be applied in MD simulation, the current measurement of GNNFF model is insufficient for stability evaluation (Kim et al. 2023).\nIn this paper, we target on improving the stability of GNNFF models in MD simulation especially over OOD datasets, and propose a GNN feature correlation based method in GNNFF training. Our key contributions are as follows:\n\u2022 We analyze the stability performance of GNNFF models with different structures and reveal the negative relationship of feature correlation and stability of MD simulation with GNNFF models.\n\u2022 To improve the stability of GNNFF models, we design a loss function to reduce feature correlation that can be applied during GNNFF model training.\n\u2022 To alleviate the accuracy drop involved by extra loss function, we design a scheduler to dynamically adjust loss coefficient during the training.\n\u2022 To better evaluate the effectiveness of our method, we design an empirical metric based on multiple physical values extracted from simulation results."}, {"title": "Related Work", "content": "GNNFF models\nGraph Neural Network based Force Field: Given an atomic system with n atoms, each atom has an atomic number and position $r_i \\in \\mathbb{R}^{n \\times 3}$, and Force Field (FF) models learn from the interactions of atoms to predict the system potential energy E and force $F_i$ for each atom. Typically, the forces on each particle are obtained as $F_i = -\\frac{\\delta E}{\\delta r_i}$ (Fu et al. 2022). In GNNFFs, atoms are considered to be nodes and the interaction or bonds between two atoms are considered to be edges. An edge is built when the distance of two atoms is less than a predefined cutoff threshold. GNNFF learns knowledge from atoms' spatial information like distances, angles between atom pairs, and dihedral of atom groups. The accuracy of FF model is usually evaluated by Energy MAE (EMAE) and Force MAE (FMAE) per-atom, with the unit of meV/atom and meV/\u00c5.\nNequIP (Batzner et al. 2022) is an E(3)-equivariant Message Passing Network employing E(3)-equivariant convolutions for interactions of geometric tensors. It achieves state-of-the-art accuracy on a challenging and diverse set of molecules and materials with remarkable data efficiency.\nAllegro (Musaelian et al. 2023) is a local interaction based-FF model. It predicts the energy as a function of the final edge embedding rather than the node embeddings. All the pairwise energies are summed to obtain the total energy of the system. Allegro shows high accuracy and great scalability with its local interaction architecture. GemNet (Gasteiger, Becker, and G\u00fcnnemann 2021) is a Message Passing Network based on directed edge embeddings and two-hop message passing. GemNet and its variants shows high accuracy in OC20 (Chanussot et al. 2021) leaderboard but lower scalability than Allegro.\nMD simulation stability\nRecently, the stability of MD simulation when using MLFF/GNNFF models to describe atomic interaction is actively discussed in the field. MLFFs may produce unstable prediction result when the learned force field is not robust to"}, {"title": "Methodology", "content": "Our method is inspired by the performance deterioration of deep GNNs (Li, Han, and Wu 2018). The potential issue of deep GNNs lies in over-smoothing (Zhao and Akoglu 2020; Chen et al. 2020) and over-correlation (Jin et al. 2022). Over-smoothing indicates the learned node representations become highly indistinguishable when stacking too many GNN layers. Over-correlation indicates that deeply stacking GNN layers renders the learned feature dimensions highly correlated. High correlation indicates high redundancy and less information encoded by the learned dimensions, which can harm the generalization and performance of downstream tasks.\nTherefore, in order to understand the trends of stability performance with different GNN architectures over ID and OOD datasets, we have trained NequIP, Allegro and GemNet-T models with different layers over the hafnium oxide (HfO) ID dataset released in (Kim et al. 2023). HfO is typically used as a high-k material and a crucial ferroelectric material in complementary metal-oxide-semiconductor technology, showing great potential for emerging electronics applications. The ID training dataset comprises 96 atoms, exhibiting a 1:2 ratio of 32 Hf atoms to 64 O atoms, and OOD datasets exhibiting a 1:1.1 and 1:1.55 ratio of Hf atoms to O atoms. OOD dataset is added in our benchmark because different types of atom compositions for HfO material exists in real application scenario.\nWe benchmarked the simulation stability with the trained model over both ID and OOD datasets for 40,000 steps in LAMMPS \u00b2. The result in Table 1 shows shallow GNNFFs"}, {"title": "Feature correlation calculation", "content": "Supposing that a GNN model has L layers and each layer will produce a set of edge features to pass the message to the next layer, we denote the edge features as $X_1, ..., X_l, ..., X_L$. Each $X_l$ has shape [f, dim], where f is the number of edges and dim is the dimension of edge features. We define feature correlation as the correlation value between each dimension of edge feature, so the correlation matrix $Corr_l \\in \\mathbb{R}^{dim \\times dim}$ is shaped like [dim, dim], and can be calculated from the l-th GNN layer. $Corr_l[k, j]$ is the element located at row k and column j of $Corr_l$, which means the correlation value between feature dimension k and feature dimension j, and can be calculated by:\n$Corr_l[k, j] = |\\rho(X_l(:, k), X_l(:, j))|$, (1)\nwhere $\\rho(X, Y)$ is the Pearson correlation coefficient (Benesty et al. 2009), which measures linear correlation between column vectors X and Y. In ideal case, we expect that the correlation coefficient between any pair of different feature dimensions to be 0, which implies there is no linear dependency between them.\nFor equivariant GNNFFs (Batzner et al. 2022), features are geometric objects that comprise a direct sum of irreducible representations of the O(3) symmetry group. Therefore, we need to do extra processing on features to select 10 features to calculate the feature correlation.\nComputing all edge features of all atoms from all training samples is time-consuming, so we randomly sample $\\sqrt{f}$ edges from all f edges in a sample to calculate correlation value. The number of multiplications to compute the covariance matrix of edge features decreases from $dim^2 f$ to $dim^2 \\sqrt{f}$."}, {"title": "Correlation loss function", "content": "We expect the feature correlation of each layer can be as low as possible, so the target is to optimize $Corr_l$ to an identity matrix $Corr_{target}$. The loss function is:\n$loss_{corr} = \\frac{\\sum |Corr_l - Corr_{target}|}{dim(dim - 1)}$ (2)\nFinally, we sum $loss_{corr}^l$ of all layers to $loss_{corr}$, and our final optimizing target is to minimize $loss_{corr}$:\n$loss_{corr} = \\sum loss_{corr}^l$ (3)\nTo measure the correlation value of a model on a specified dataset, only the correlation matrix $Corr_L$ at the last layer of the model is taken. If we suppose there are B samples in the dataset, the final correlation value is:\n$Corr = \\sum Corr_L^b$ (4)"}, {"title": "Dynamic coefficient scheduler", "content": "Combining the two loss functions (force and energy) is tricky since focusing on one metric may lead to performance degradation on the other, not to mention the extra correlation loss involved by our method. Thus, it is necessary to balance stability, energy accuracy and force accuracy. Therefore, we propose a dynamic coefficient scheduler to balance those objectives:\n$C_{corr} = C_{min} + \\frac{C_{max} - C_{min}}{2} (1 + cos(\\pi \\frac{t}{t_{cycle}}))$ (5)\nBefore each training epoch starts, the current loss coefficient $c_{corr}$ is updated. $[C_{min}, C_{max}]$ is the range of correlation loss coefficient during training; $t_{cycle}$ is the update epoch cycle interval, and t is the current epoch; $C_{min}$ $C_{max}$, and $t_{cycle}$ are hyperparameters to be set before training. In our experiments, $C_{min} = 0, C_{max} = 0.1, t_{cycle} = 100$.\nOur correlation loss coefficient scheduler is similar to cyclic cosine annealing learning rate scheduler (Loshchilov and Hutter 2017), but our coefficient scheduler is gradually increasing instead of decreasing in one cycle due to the priority given to force and energy accuracy. In the early stage of training process, the model can quickly converge to the minimum. If the correlation loss coefficient is high, the correlation loss acting as a regular term will put the model into a poor local minimum. Similarly, periodically restarting the coefficient can help jump out of current local minimum and find a lower local minimum to improve accuracy. The total loss is\n$loss = c_f \\cdot loss_f + c_e \\cdot loss_e + C_{corr} loss_{corr}$ (6)\nFinally, backward calculation is proceeded and gradients are updated according to the loss value calculated by Eq.6 until the model is fully trained. Empirically, $C_{max}$ should not be set bigger than $c_f$ and $c_e$ to avoid accuracy drop. For example, if $c_f$ and $c_e$ are 1 respectively, 1 or 0.1 is preferred for $C_{max}$."}, {"title": "Empirical metric to evaluate MD stability", "content": "Physical values and atom information in simulation results (such as temperature, force, number of atoms, length of bonds, etc.) can be used to evaluate the stability of a model in simulation experiments. However, evaluating with multiple non-consecutive values would be confused for users. Therefore, we propose a unified metric to quantify the stability performance with all the meaningful simulation values.\nThe empirical metric considers atom number, forces' abnormality and distance between pairs of atoms to quantify the stability of GNNFF models over a dataset in MD simulation. Moreover, system temperature is proportional to kinetic energy, which explains why unstable simulation always shows unusually high temperatures.\nSimulations usually crash because model predicts forces that are extremely huge, and so atoms fly out of simulation space and lost. Then crash happens because of unmatched atom number. Our metric, the stability index, takes all the above situations into account, as shown in Eq.7.\n$S_{index} = \\frac{1}{num} \\sum S_{index}^n$ (7)\nSpecifically, to estimate the typical and physically correct values of MD simulation, first we run MD simulation for a certain number of steps with the trained model in LAMMPS framework. Then, we dump the simulation trajectory data with a fixed simulation step interval such as saving for every 100 steps. The saved trajectory data should include atom number N, atom positions r and temperatures T. After simulation, num snapshots are saved. For each snapshot, a stability index $S_{index}$ should be calculated and accumulated together. Second, we calculated $r_{min}$, the minimum distance between atoms for different atomic species pairs using atom positions r which is just dumped. Just like RDF value, with total C atomic species in a system, combining pairwise species can get $\\frac{C(C+1)}{2}$ total number of the atom pair composition, so we need to calculate C sets of $r_{min}$ values. To calculate the stability index of the n-th snapshot, take current atom number $N_n$ and initial atom number $N_0$, set simulation temperature $T_{set}$, current temperature $T_n$, current minimum distance $r_{min}^n$ and last minimum distance $r_{min}^{(n-1)}$ into Eq.8\n$S_{index} = \\alpha (\\frac{N_n}{N_0}) + \\beta (\\frac{T_{set}}{T_n}) + \\prod (\\frac{r_{min}^n}{r_{min}^{(n-1)}})$ (8)\nwhere $\\alpha$ is the scale factor for atom number, and $\\beta$ is the scale factor for temperature. In our experiments we have used $\\alpha = 1$, and $\\beta = 1/4$. The higher $S_{index}$ is, the more stable the simulation is."}, {"title": "Experimental validation", "content": "We conducted a number of experiments to show the effectiveness of our method, evaluated model accuracy over ID and OOD datasets and stability performance in LAMMPS simulation, and estimated the overhead of our method. All the LAMMPS simulations are conducted with Langevin thermostat and with timestep equals 2.5fs. Since \"fix langevin\" command does not perform time integration (it only modifies forces to effect thermostatting) (Thompson et al. 2022), we use a separate time integration with microcanonical NVE ensemble to actually update the velocities and positions of atoms using the modified forces. We also conducted ablation study to show the effectiveness of each component in our method. All the experiments were done on the Super-computing Center, where each server node has 8 NVIDIA A100-SXM4-80GB GPU connected in series via NVLink. The software versions are: PyTorch 12.1 and CUDA 11.4.\nEvaluation on GNNFF models\nWe take NequIP, Allegro and GemNet-T models to evaluate the performance with correlation method applied. We use the default model configurations from original proposed paper. We fit our GNNFF models with the newly released HfO dataset designed for semiconductor advanced material called the SAMD23 dataset (Kim et al. 2023). To assess the accuracy of GNNFF models, we used EMAE and FMAE over ID test datasets, similarly to (Kim et al. 2023). Furthermore, in order to better evaluate the stability, we use the proposed metric Sindex to quantify the stability of MD simulations.\nAccuracy\nFMAE and EMAE columns in Table 2 shows the accuracy of the GNNFF models on HfO ID dataset of baseline and our method. Corr value is the correlation value of last layer calculated by equation 1. GemNet-T model with our method achieves lower Force MAE. However, NequIP and Allegro model trained with our method suffer accuracy loss. This is because GemNet-T used more geometric features and interaction information applied with full graph structure while NequIP only used a pair of atom interaction information and Allegro only used local geometric information. Therefore, our method has less impact on the accuracy of GemNet-T model than that of NequIP and Allegro.\nStability\nThe stability experiments are conducted over ID and OOD datasets with different Hf:O ratios. We expect to perform stable MD simulation over both ID and OOD with one unified model rather than train different models for different compositions. We perform 40,000 steps of simulation with temperature of 1,200K, 1,800K and 2,400K in LAMMPS with baseline model and the model trained by our method respectively.\nAs shown in Table 2, baseline NequIP model can only successfully perform MD simulation with ID dataset, while other two cases with OOD dataset get sindex = 0, which indicating system crash because of lost atoms. NequIP model trained with our method can complete MD simulation with both ID and OOD dataset. Furthermore, all cases achieve reasonable physical values during simulation. We also get similar result for Allegro model. With our method, the simulation time can be extended from 0.03ps to 10ps as shown in Figure 1. As shown in Figure 5, optimized Allegro model gets more reasonable and stable force values in all simulation steps while baseline model crashed because of unreasonable force values (red spheres represent Hf atom, purple spheres represent O atom and green-colored vectors represent the force of the atoms during MD simulation). For GemNet-T model, the baseline completes the simulation, but the distance between close atoms shows abnormality as shown in Figure 6. GemNet-T model trained with our method can run simulation successfully with all 3 cases and achieve reasonable physical values including forces and atom distances for all the cases. We list the result from the simulation temperature of 1,200K, but we see the similar trend with the simulation temperature of 1,800K and 2,400K. Figure 7 shows the RDF curves for HfO (1:2) dataset of GemNet-T model baseline and optimized with our method. We can see that RDF curve with optimized model involves less noisy compared with the baseline. We have got similar RDF results for NequIP and Allegro models.\nAll above shows that our proposed method remove all non-physical results from the simulated structures, and thus provide stable MD simulation results. More information on stability experiments are shown in section and detailed physical metric values of MD simulation are presented in Table A4."}, {"title": "Ablation study", "content": "Computation overhead. To reduce the computation overhead involved in correlation calculation, only a small portion of sampled features are used to compute correlation, so there is little overhead even an extra loss function is involved in training. In our experiments we randomly choose features of 1,024 edges among the total edges to calculate the correlation. Results in Table 3 show there is only up to 3% extra overhead in NequIP, Allegro and GemNet-T.\nCorrelation calculation. Different from Allegro and GemNet-T model, features in NequIP are geometric objects that comprise a direct sum of irreducible representations of the O(3) symmetry group (Batzner et al. 2022). Therefore, we tried the following four types of combination with different orders and parities of features: a) mixing up all edge feature with different parities and rotation orders; b) only taking 0e (l=0 and parity is even) features; c) only taking 1o (l=1 and parity is odd) features; d) summing the correlations of 0e and 1o features. Besides, we reduce correlation of node features to see if stability is improved. Table 4 shows reducing correlation of 1o features can achieve higher accuracy and better stability in MD simulation. The result comparison of reducing correlation of edge features and node features also shows reducing correlation of edge features are more useful which proves that the information in edge features are more crucial for GNNFFs.\nCoefficient scheduler. We experimented with two types of scheduler: linear scheduler and cosine scheduler. The former uses a linear increasing coefficient with training epochs; the latter uses a cycling increasing coefficient with a fixed epoch cycle. The result in Table A3 shows reducing feature correlation with both linear and cosine scheduler can help Allegro model to improve the generalization and achieve more stable MD simulation over OOD dataset. Furthermore, we can see cosine scheduler can achieve lower energy MAE than linear scheduler."}, {"title": "Summary and Limitations", "content": "This paper presents a method to improve GNNFF model's generalization and stability in MD simulation. Our method reduces GNN feature correlation by adding a correlation loss and dynamically scheduled coefficient. Evaluation results verify that our method can improve the simulation stability for GNNFF models both on ID and OOD datasets with less than 3% computational overhead. Besides, this paper proposes a new metric to reveal the robustness of MD simulation with more physical information in simulation trajectory data.\nLimitations. Admittedly, the main limitation of the present study is that the motivation and studies are base on the GNN structures for MD tasks only, therefore the effectiveness of our method is validated over GNNFFs. Also, main focus of our work is semiconductor applications, where long-term simulations are critically needed. In the future, we aim to validate the generalization of our approach on numerous other GNNFF datasets and assessing the impact of model and data scaling."}]}