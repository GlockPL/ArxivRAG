{"title": "BatchTopK Sparse Autoencoders", "authors": ["Bart Bussmann", "Patrick Leask", "Neel Nanda"], "abstract": "Sparse autoencoders (SAEs) have emerged as a powerful tool for interpreting language model activations by decomposing them into sparse, interpretable features. A popular approach is the TopK SAE, that uses a fixed number of the most active latents per sample to reconstruct the model activations. We introduce BatchTopK SAEs, a training method that improves upon TopK SAEs by relaxing the top- k constraint to the batch-level, allowing for a variable number of latents to be active per sample. As a result, BatchTopK adaptively allocates more or fewer latents depending on the sample, improving reconstruction without sacrificing average sparsity. We show that BatchTopK SAEs consistently outperform TopK SAEs in reconstructing activations from GPT-2 Small and Gemma 2 2B, and achieve comparable performance to state-of-the-art JumpReLU SAEs. However, an advantage of BatchTopK is that the average number of latents can be directly specified, rather than approximately tuned through a costly hyperparameter sweep. We provide code for training and evaluating BatchTopK SAEs at https://github. com/bartbussmann/BatchTopK.", "sections": [{"title": "Introduction", "content": "Sparse autoencoders (SAEs) have been proven effective for finding interpretable directions in the activation space of language models [1, 2, 9, 6]. SAEs find approximate, sparse, linear decompositions of language model activations by learning a dictionary of interpretable latents from which the activations are reconstructed.\nThe objective used in training SAEs [1] has both a sparsity and a reconstruction term. These are naturally in tension as, for an optimal dictionary of a given size, improving the reconstruction performance requires decreasing sparsity and vice versa. Recently, new architectures have been proposed to address this issue, and achieve better reconstruction performance at a given sparsity level, such as Gated SAEs [6], JumpReLU SAEs [7], and TopK SAEs [3].\nIn this paper, we introduce BatchTopK SAEs, a novel variant that extends TopK SAEs by relaxing the top-k constraint to a batch-level constraint. This modification allows the SAE to represent each sample with a variable number of latents, rather than assuming that all model activations consist of the same number of units of analysis. By selecting the top activations across the entire batch, BatchTopK SAEs enable more flexible and efficient use of the latent dictionary, leading to improved reconstruction without sacrificing average sparsity. During inference we remove the batch dependency by estimating a single global threshold parameter.\nThrough experiments on the residual streams of GPT-2 Small [5] and Gemma 2 2B [8], we show that BatchTopK SAEs consistently outperform both TopK and JumpReLU SAEs in terms of reconstruction performance across various dictionary sizes and sparsity levels, although JumpReLU SAEs have less downstream CE degradation in large models with a high number of active latents. Moreover, unlike"}, {"title": "Background: Sparse Autoencoder Architectures", "content": "Sparse autoencoders aim to learn efficient representation of data by reconstruction inputs while enforcing sparsity in the latent space. In the context of large language models, SAEs decompose model activations x \u2208 Rn into sparse linear combinations of learned directions, which are often interpretable and monosemantic.\nAn SAE consists of an encoder and a decoder:\n\\begin{equation}\nf(x) := \u03c3(W_{enc}x + b_{enc}),\n\\end{equation}\n\\begin{equation}\n\\hat{x}(f(x)) := f(x) W_{dec} + b_{dec}.\n\\end{equation}\nwhere f(x) \u2208 Rm is the sparse latent representation and \\hat{x}(f) \u2208 R\" is the reconstructed input. Wenc is the encoder matrix with dimension n \u00d7 m and benc is a vector of dimension m; conversely Wdec is the decoder matrix with dimension m x n and bdec is of dimension n. The activation function \u03c3 enforces non-negativity and sparsity in f(x), and a latent i is active on a sample x if f(x) > 0.\nSAEs are trained on the activations of a language model at a particular site, such as the residual stream, on a large text corpus, using a loss function of the form"}, {"title": "BatchTopK Sparse Autoencoders", "content": "We introduce BatchTopK SAEs as an improvement over standard TopK SAEs. In BatchTopK, we replace the sample-level TopK operation with a batch-level BatchTopK function. Instead of selecting the top k activations for each individual sample, we select the top n \u00d7 k activations across the entire batch of n samples, setting all other activations to zero. This allows for a more flexible allocation of active latents, where some samples may use more than k latents while others use fewer, potentially leading to better reconstructions of the activations that are more faithful to the model.\nThe training objective for BatchTopK SAEs is defined as:\n\\begin{equation}\n\\mathcal{L}(X) = ||X - BatchTopK(W_{enc}X + b_{enc})W_{dec} + b_{dec}||_2 + \\mathcal{L}_{aux}\n\\end{equation}\nHere, X is the input data batch; Wenc and benc are the encoder weights and biases, respectively; Wdec and bdec are the decoder weights and biases. The BatchTopK function sets all activation values to zero that are not among the top n \u00d7 k activations by value in the batch, not changing the other"}, {"title": "Experiments", "content": "We evaluate the performance of BatchTopK on the activations of two LLMs: GPT-2 Small (residual stream layer 8) and Gemma 2 2B (residual stream layer 12). We use a range of dictionary sizes and values for k, and compare our results to TopK and JumpReLU SAEs in terms of normalized mean squared error (NMSE) and cross-entropy (CE) degradation. For the experimental details, see Appendix A.1.\nWe find that for a fixed number of active latents (L0=32) the BatchTopK SAE has a lower normalized MSE and less cross-entropy degradation than TopK SAEs on both GPT-2 activations (Figure 1) and Gemma 2 2B (Figure 2). Furthermore, we find that for a fixed dictionary size (12288) BatchTopK outperforms TopK for different values of k on both models.\nIn addition, BatchTopK outperforms JumpReLU SAEs on both measures on GPT-2 Small model activations at all levels of sparsity. On Gemma 2 2B model activations the results are more mixed: although BatchTopK achieves better reconstruction than JumpReLU for all values of k, BatchTopK only outperforms JumpReLU in terms of CE degradation in the lowest sparsity setting (k=16).\nTo confirm that BatchTopK SAEs make use of the enabled flexibility to activate a variable number of latents per sample, we plot the distribution of the number of active latents per sample in Figure 3. We observe that BatchTopK indeed uses a wide range of active latents, activating only a single latent on some samples and activating more than 80 on others. The peak on the left of the distribution are model activations on the <BOS>-token. This serves as an example of the advantage of BatchTopK: when the model activations do not contain much information, BatchTopK does not activate many latents, whereas TopK would use the same number of latents regardless of the input. This corroborates our"}, {"title": "Conclusion", "content": "In this work, we introduced BatchTopK sparse autoencoders, a novel variant of TopK SAEs that relaxes the fixed per-sample sparsity constraint to a batch-level constraint. By selecting the top activations across the entire batch rather than enforcing a strict limit per sample, BatchTopK allows for a variable number of active latents per sample. This flexibility enables the model to allocate more latents to complex samples and fewer to simpler ones, thereby improving overall reconstruction performance without sacrificing average sparsity. We evaluated BatchTopK SAEs using the standard metrics of reconstruction loss and sparsity. We evaluated BatchTopK SAEs using standard metrics of reconstruction loss and sparsity, and while we did not directly assess human interpretability, the architectural similarity to TopK SAEs suggests that these latents would remain comparably interpretable. Our results demonstrate that small modifications to the activation function can have significant effects on SAE performance and expect that future work will continue to find improvements that better approximate the latent structure of model activations."}, {"title": "Supplemental Material", "content": "In this appendix, we provide details about the datasets used, model configurations, and hyperparameters for our experiments.\nWe trained our sparse autoencoders (SAEs) on the OpenWebText dataset\u00b9, which was processed into sequences of a maximum of 128 tokens for input into the language models.\nAll models were trained using the Adam optimizer with a learning rate of 3 \u00d7 10-4, \u03b2\u2081 = 0.9, and \u03b22 = 0.99. The batch size was 4096, and training continued until a total of 1 \u00d7 109 tokens were processed.\nWe experimented with dictionary sizes of 3072, 6144, 12288, and 24576 for the GPT-2 Small model, and used a dictionary size of 16384 for the experiment on Gemma 2 2B. In both experiments, we varied the number of active latents k among 16, 32, and 64.\nFor the JumpReLU SAEs, we varied the sparsity coefficient such that the resulting sparsity would match the active latents k of the BatchTopK and TopK models. The sparsity penalties in the experiments on GPT-2 Small were 0.004, 0.0018, and 0.0008. For the Gemma 2 2B model we used sparsity penalties of 0.02, 0.005, and 0.001. In both experiments, we set the bandwidth parameter to 0.001."}]}