{"title": "PHI-S: DISTRIBUTION BALANCING FOR LABEL-FREE MULTI-TEACHER DISTILLATION", "authors": ["Mike Ranzinger", "Jon Barker", "Greg Heinrich", "Pavlo Molchanov", "Jan Kautz", "Bryan Catanzaro", "Andrew Tao"], "abstract": "Various visual foundation models have distinct strengths and weaknesses, both of which can be improved through heterogeneous multi-teacher knowledge distillation without labels, termed \u201cagglomerative models.\u201d We build upon this body of work by studying the effect of the teachers' activation statistics, particularly the impact of the loss function on the resulting student model quality. We explore a standard toolkit of statistical normalization techniques to better align the different distributions and assess their effects. Further, we examine the impact on downstream teacher-matching metrics, which motivates the use of Hadamard matrices. With these matrices, we demonstrate useful properties, showing how they can be used for isotropic standardization, where each dimension of a multivariate distribution is standardized using the same scale. We call this technique \u201cPHI Standardization\" (PHI-S) and empirically demonstrate that it produces the best student model across the suite of methods studied.", "sections": [{"title": "1 INTRODUCTION", "content": "A body of work recently emerged on the topic of agglomerative models Ranzinger et al. (2024), which is fusing multiple heterogeneous visual foundation models Awais et al. (2023) into a single model via multi-teacher knowledge distillation Hinton et al. (2015); Zuchniak (2023) without labels. Starting with AM-RADIO Ranzinger et al. (2024), and followed by Theia Shang et al. (2024), and UNIC Sariyildiz et al. (2024). Theia and UNIC apply feature standardization to the teacher output, and demonstrate how important it is.\nWhile knowledge distillation has a large body of literature (e.g. Bucilu\u0103 et al. (2006); Ahn et al. (2019); Heo et al. (2019); Huang & Wang (2017); Romero et al. (2014); Sun et al. (2021); Wei et al. (2022a); Zagoruyko & Komodakis (2017)), agglomerative models - dealing with multiple teachers coming from different modeling domains (e.g. vision-language contrastive Radford et al. (2021), self-supervised learning Oquab et al. (2023); Zhou et al. (2022); Assran et al. (2023), and segmentation Kirillov et al. (2023)) without ground truth labels was new territory. In AM-RADIO, the"}, {"title": "2 METHOD", "content": "The goal of the agglomerative student model is to produce activations $x^{(t)}$ that match the teacher activations $y^{(t)} \\in \\mathcal{Y}^{(t)}$ as closely as possible for each teacher $t \\in \\mathcal{T}$, and the loss is (usually) linearly aggregated using weights $\\alpha^{(t)}$. Finding these $\\alpha^{(t)}$ is difficult due to the size of the design space, so current methods typically default to $\\alpha^{(t)} = 1$ and focus on conditioning $\\mathcal{Y}^{(t)}$ to handle distributional differences. For simplicity, we drop the $.(t)$ superscript as the same type of normalization is applied for every teacher, and each teacher has independent normalization parameters. Throughout this paper, we refer to $\\text{Var} [Z]$ as the diagonal of the covariance matrix $\\Sigma [Z]$ for some distribution $Z$.\n2.1 BASELINE\nWe start with the MSE (mean squared error) loss serving as the baseline for feature matching:\n$L_{mse} (x, y) = \\frac{1}{N} \\sum_{n=1}^{N} (x_n - y_n)^2$ (1)\nBecause AM-RADIO Ranzinger et al. (2024) doesn't use MSE as their loss function, but rather a hybrid cosine + Smooth-L1 loss, we also consider a few of these variants. For example, the vanilla cosine distance loss, which is identical to what AM-RADIO uses for the summary loss. While we expect this to do poorly on the task of exactly matching the teacher distribution (due to magnitude invariance), it's not clear how this will affect the downstream tasks, so we include it.\n$L_{cos}(x, y) = \\frac{1}{N} \\sum_{n=1}^{N} (1 - \\frac{x^T y}{||x|| ||y||})$ (2)\nWe also consider the exact loss function proposed in AM-RADIO which is a hybrid of cosine distance and smooth-L1:\n$L_{hyb-sml1} (x, y) = \\beta \\cdot L_{cos}(x, y) + (1 - \\beta) \\cdot \\text{SmoothL1}(x, y)$ (3)\nFor completeness, we ablate whether MSE vs Smooth-L1 has an effect on the evaluation criteria:\n$L_{hyb-mse} (x, y) = \\beta \\cdot L_{cos}(x, y) + (1 - \\beta) \\cdot L_{mse}(x, y)$ (4)\nIn AM-RADIO, the authors used $$\\beta$ to interpolate between cosine and smooth-L1 loss. Instead of searching the space for the optimal $$\\beta$, we analyzed the setting they chose ($$\\beta = 0.9$), and also note"}, {"title": "2.2 NORMALIZATION", "content": "Instead of balancing the different heads through loss weighting, we can alter the targets themselves. In Wei et al. (2022a), the authors explore this to condition their single teacher's distribution, however, they use the non-invertible LayerNorm operator to rescale the teacher features. Because we want to maintain compatibility for the student to replace the teacher in downstream tasks (by replacing only the vision encoder part of the model), we require the student to still estimate the true teacher distribution. To achieve this, during training, we use an invertible linear mapping $f_k(\\cdot)$ such that $T_h(x) = f_k (T_k(x))$ and $T_k(x) = f_k^{-1} (T(x))$, where the student model learns to match teacher $(T(x))$ for each of the $k$ teachers.\n2.2.1 STANDARDIZATION\nWe first consider the simplest case of standardization, which is to use a single scalar $\\mu_g$ and std. dev. $\\sigma_g$ across the entire feature map. These represent the global statistics of the teacher distribution. In contrast to Wei et al. (2022a), we seek an invertible linear mapping, which excludes LayerNorm. We can, however, estimate the $\\mu_{xy}$ and $\\sigma_{xy}$ of each position, or, because we want to preserve resolution flexibility, estimate them across all positions and channels, yielding global $\\mu_g$ and $\\sigma_g$.\nLet $\\mu_g$ and $\\sigma_g$ be the global mean and standard deviation estimate of the teacher distribution. Then\n$L_{gs} (x, y) = L_{mse} (X, \\frac{y - \\mu_g}{\\sigma_g})$ (5)\nwhich we call Global Standardization. We also explore regular multivariate standardization where we normalize each channel of the teacher distribution independently. Let $\\mu_c = E [Y_c]$ and $\\sigma_c = \\sqrt{\\text{Var} [Y_c]}$, then standardization is defined as\n$L_s (x, y) = L_{mse} (x, y'), y'_c = \\frac{y_c - \\mu_c}{\\sigma_c}$ (6)\n2.2.2 WHITENING\nWhile standardization normalizes the individual feature variances, it doesn't correct for any covariance between dimensions. We can expand on standardization by also eliminating the covariance between features, called whitening. Let $\\Sigma [Y]$ be the covariance matrix for $Y$ where $y \\sim Y$. Following Kessy et al. (2018), we want to find the $W$ in\n$z = Wy$ (7)\nwith $z \\sim Z$ and $\\Sigma [Z] = I$. $W = \\Sigma [Y]^{-1/2}$ is one such valid matrix, called ZCA Whitening Bell & Sejnowski (1997), and takes the form"}, {"title": "2.2.3 ESTIMATION ERRORS", "content": "Each feature in $\\Sigma[Y']$ is linearly independent and has uniform scale. And so $L_w(x,y) = L_{mse} (x, Wy - \\mu)$ for any whitening method $w$. $y$ and $y'$ are related to each other as\n$y' = \\Sigma [Y]^{-1/2} (y - \\mu)$ (8)\n$y = \\Sigma [Y]^{1/2} y' + \\mu$ (9)\n2.2.3 ESTIMATION ERRORS\nFollowing the whitening notation of Kessy et al. (2018), given some orthogonal matrix $Q$, then $QW$ is also a valid whitening matrix, as $Q^TQ = I$, therefore $(QW)^T QW = \\Sigma[Y]^{-1}$. Kessy et al. (2018) then demonstrate the properties of certain choices of $Q$, and we focus on PCA Whitening (PCA-W) and ZCA in this paper. With\n$\\Sigma [Y] = U \\Lambda U^T$ (10)\n$W_{pca-w} = Q_{pca-w} \\Lambda^{-1/2}U^T, Q_{pca-w} = I$ (11)\n$W_{zca} = Q_{zca} \\Lambda^{1/2}U^T, Q_{zca} = U$ (12)\nwhere $U$ and $\\Lambda$ are the eigenvectors and eigenvalues for the covariance matrix of $Y$ respectively. $\\Lambda = \\text{diag-embed} (\\lambda_1, ..., \\lambda_c)$ where diag-embed ($\\cdot$) forms a diagonal matrix with the vector argument along the diagonal. From equation 9, an issue naturally arises, which is the estimation error of our student network. Let $\\epsilon \\in \\mathbb{R}$ be the estimation error of the student s.t. $y' = x + \\epsilon$ where $x$ is the student prediction for a given normalized teacher, forming the exact equality\n$y = W^{-1} (x + \\epsilon) + \\mu$ (13)\n$= W^{-1}x + W^{-1}\\epsilon + \\mu$ (14)\n$\\epsilon_{pca-w} = U \\Lambda^{1/2} \\epsilon$ (15)\n$\\epsilon_{zca} = U \\Lambda U^T \\epsilon$ (16)\nWe can also use the same $\\epsilon$ to study standardization (equation 6), taking the form\n$\\epsilon_{std} = \\text{diag-embed} (\\sigma_1, ..., \\sigma_c) \\epsilon$ (17)\nAs is clear from equations 15, 16 and 17, the choice of normalization will have an impact on the error profile of the model, unless $\\epsilon$ counteracts the distortion. We next introduce another $Q$ not studied in Kessy et al. (2018), which is to use a scaled Hadamard matrix, based on this idea.\n2.2.4 HADAMARD WHITENING (HCA)\nIn PCA Whitening, each successive dimension explains the next-largest variance in the data. While this can be a very useful form, we hypothesize that this sort of dimensional loading might not be healthy for a model to learn to match, as effects such as regularization, step size, gradient clipping, etc. may impact the ability of the model to learn each dimension. Instead of ranking the dimensions, we'd like to do the opposite, and find a $Q$ that explains exactly the same amount of variance irrespective of channel index. It follows that if we could construct an orthogonal basis where each axis captures an identical amount of energy from the diagonal $\\Lambda^{-1/2}$ matrix, then we are able to achieve this balance. First, this matrix $R$ must be orthogonal for it to be a valid $Q$. Second, in order for the same proportion of the diagonal $\\Lambda$ to be captured by each row, then each cell must have the same magnitude. Specifically, $R_{ij} = \\pm \\frac{1}{\\sqrt{c}}$. These matrices are called Hadamard matrices, and the following is called Sylvester's construction Sylvester (1867), valid when $C$ is a power of 2:"}, {"title": "2.2.5 PCA-HADAMARD ISOTROPIC STANDARDIZATION (PHI-S)", "content": "A key issue with the previous normalization procedures (aside from global standardization) is that they place disproportionate weight on lower-variance axes. To avoid this distortion, we present the following theorem, and then describe how we apply it as a novel form of standardization:\nTheorem 2.1. For any mean-centered normal data distribution $X \\in \\mathbb{R}^{C \\times N}$ with satisfiable Hadamard-matrix dimension $C$, there exists an orthogonal transform $R \\in \\mathbb{R}^{C \\times C}$ and scalar $\\alpha \\in \\mathbb{R}$ such that $\\text{diag} (\\Sigma [\\alpha RX]) = \\mathbb{I}_C$.\nProof. Let $\\Sigma [X]$ be the covariance matrix of $X$, and let $\\Sigma [X] = U \\Lambda U^T$ where $U$ is an orthogonal matrix, and $\\Lambda = \\text{diag-embed} (\\lambda_1, ..., \\lambda_c)$, with $\\lambda_i$ being the eigenvalues of $\\Sigma [X]$. (called PCA).\nFirst, note that $\\Sigma [UX] = U^T (U \\Lambda U^T) U = \\Lambda$.\nNext, let $H \\in \\mathbb{R}^{C \\times C}$ be a normalized Hadamard matrix, and recall each cell in $H$ has value $\\pm \\frac{1}{\\sqrt{C}}$.\nUsing the orthogonal transform $HU^T$, we get $\\Sigma [HUX] = H \\Lambda H^T$.\nLet\n$\\text{diag} (H \\Lambda H^T)_r = \\sum_{i=1}^{C} (\\frac{1}{\\sqrt{C}} \\lambda_i \\frac{1}{\\sqrt{C}}) = \\frac{1}{C} \\sum_i \\lambda_i$ $ \\forall r \\in C$ (21)\n$\\phi = \\sqrt{\\frac{1}{C} \\sum_{i=1}^{C} \\lambda_i}$ (22)\n$\\Sigma [\\phi^{-1}M] = \\phi^{-2}M = \\frac{1}{C} \\sum \\lambda_i M$ (23)\nfor some matrix $M$. For $H \\Lambda H^T$, we have\n$\\text{diag} (\\Sigma [\\phi^{-1}HUX]_r) = \\text{diag} (\\phi^{-2}H \\Lambda H^T)_r = \\frac{\\frac{1}{C}\\sum_{i=1}^{C} \\lambda_i}{\\frac{1}{C}\\sum_{i=1}^{C} \\lambda_i} = 1$ $ \\forall r \\in C$ (24)\nTherefore"}, {"title": "3 IMPLEMENTATION DETAILS", "content": "R = HU \u03b1 = \u03c6\u207b\u00b9  (25)\nFor PHI-S, following equation 25 we use\nWship = R (26)\nEssentially, we first mean center and then rotate the distribution in such a way (R) that the variance along each resulting dimension is identical, allowing us to uniformly scale by \u03b1 to achieve a standardized distribution.\nWe generally follow the procedure outlined in AM-RADIO, however we make some changes that reduce the computational cost of training, which was necessary to cover all of the ablations we studied. Namely, we:\n\u2022 Add SigLIP as a teacher.\n\u2022 Train the student model at 256px resolution, and downsample the teacher features to match.\n\u2022 Train for 300k steps instead of the 600k steps originally proposed.\n\u2022 Split each teacher into their own partition, resulting in each teacher receiving a batch of 256 images, with a total of 1024 images per iteration.\n\u2022 Initialize from TIMM Wightman (2019) \u201cvit_[base,large]_patch16_224\u201d pretrained models.\nWe found that downsampling SAM features degrades their quality, so instead we pad the small image and crop out the features. Further details, and specifically for table 2, are presented in appendix A.7."}, {"title": "4 RESULTS", "content": "In figure 3 we display our model's ability to estimate the teacher distributions during training. For any of the transforms that project the teacher features into a different space, we apply the inverse operation so that all methods are measured in the original space. As can be seen, \u201cBaseline\u201d is much worse than any other method, and it's intuitive because it allows the relative difference in magnitudes between the different teachers to implicitly weight the loss. SAM has much larger activation variance than any other model, which results in the Baseline model spending most of its energy trying to match SAM. Overall, the PHI Standardization method produces the best results, as it's able to simultaneously beat any other method on DFN CLIP, SigLIP, second best on DINOv2, while remaining competitive on SAM. We show the final MSEs in table 3.\nIn tables 1 and 4, we display the average benchmark ranks across different benchmarks and methods for ViT-B/16 and ViT-L/16 students, respectively. For LLaVA, we first average the two GQA and"}, {"title": "and TextVQA", "content": "fixed radius) in the learned distribution versus the denormalized distribution. Using the Hadamard matrix as Q is the only choice that doesn't place extra error on a particular learned dimension.\nIn figure 8 we display the radius of the denormalized error circle. An interesting property of stan-dardization becomes apparent, which is that the error magnitude of standardization is bounded be-tween PCA-W and PHI-S, with equality at $\u03a3 [Y] = \\Lambda$ for the former and $\\text{diag} (\u03a3 [Y]) = \\phi_{ship}I$ for the latter. One hypothesis for why the standardization transforms (Global Standardization, Stan-dardization, PHI-S) work best in our study is because the error amplitudes are \u201cless extreme\u201d than whitening in general. With MSE being sensitive to outliers, this property is likely important. Be-cause the whitening methods only differ by an orthogonal transform, their errors are phase shifted relative to each other.\nmulti-view"}, {"title": "5 RELATED WORK", "content": "Egs = ags \u03b5  (27)\nEship = \u03c3ship\u03b5 \nfor global standardization and PHI-S respectively. We used this error profile to motivate the in-troduction of Hadamard matrices for whitening in section 2.2.4, as it distributes the error variance equally through all channels of the denormalization projection. In table 5 we display the empirical error variance ranges for each studied method and for each teacher. Intriguigingly, both methods that employ the Hadamard matrix (HCA and PHI-S) have very low variance ranges compared to the other methods. This implies that the student model is making errors of roughly uniform magnitude across all channels. Unfortunately, in the case of HCA, this property isn't borne out in a useful way in the benchmarks (table 1). Table 5 shows that the loss landscape and/or the optimizer are adapt-ing to normalization distortions and baking the non-uniform nature of the variances into the student model. For PHI-S, the student model still has nearly uniform error variance in the normalized space, but also has the lowest (or nearly lowest) range in the denormalized (original) space. This isn't sur-prising given that a unit change in any dimension of the normalized space has an identical effect as any other dimension, thus there's no incentive to prefer one dimension to another.\nIn figure 9 we show the loss distributions for the core normalization methods we studied. It helps us understand not only the magnitudes of the errors, but also showcases how different normalization methods affect the behavior of outliers. It's very apparent that \u201cBaseline\u201d has uncontrolled magni-tudes, with SAM having quite extreme losses, especially relative to DFN CLIP. This is also where we can really see how \u201cGlobal Standardize\u201d and \u201cPHI-S\u201d differ in behavior, owing to PHI-S equial-izing the variance across channels. The purple curve shows how global standardization is still very susceptible to outlier errors. As predicted in section 2.2.3, the methods that use Hadamard matrices (PHI-S and HCA) have the tightest error bounds between channels. Finally, it's also apparent how well PHI-S works for balancing across teachers, as the losses all have the most similar distributions compared against the other methods."}, {"title": "6 CONCLUSION", "content": "Through our experiments, we have conclusively demonstrated that using plain MSE without balancing has a large negative effect on the resulting quality of the student model. Among normalization methods, standardization worked better than whitening, which was an initially surprising result. We hypothesize that the major issue with whitening is that the teacher models aren't producing full rank distributions (appendix, table 6), which makes the normalization factors unstable. Regular standardization is resistant to this because the principal components of the distribution are spread out across all of the dimensions, preventing degenerate $\\Lambda^{-1/2}$ solutions. We found two novel ap-plications of Hadamard matrices with respect to distribution normalization: HCA and PHI-S. At the ViT-B/16 model scale, we found that isotropic normalization methods (Global Standardize and PHI-S) worked the best, and for ViT-L/16, PHI-S remained the best. On the topic of reconstruc-tion errors, we found no significant result across the whitening methods with respect to downstream metrics, and also found that the per-channel estimation errors were not uniform in general, unless uniform is the optimal choice (HCA and PHI-S), implying that the student model is able to be robust to the potentially high-distortion nature of the different transforms. Overall, PHI-S appears to be the best normalization method studied, and it allowed us to produce ViT-B and ViT-L models that are competitive with the original AM-RADIO Ranzinger et al. (2024) ViT-H model.\nFuture Work We've solely explored the use of PHI-S for agglomerative modeling, however it's a general standardization technique when certain assumptions about the data hold such as normality and dimensionality of the distribution. PHI-S could additionally be used to post-hoc standardize the output of existing models. Lastly, an opportunity arises when combining PHI-S with quantization practices (similar to Ashkboos et al. (2024)) in the information retrieval domain as it balances the information across all channels evenly, potentially unlocking higher fidelity quantizers."}, {"title": "A APPENDIX", "content": "A.1 HADAMARD MATRICES\nA.1.1 CONSTRUCTING HADAMARD MATRICES\nSylvester's construction gives us a convenient way to construct a Hadamard matrix when $C$ is a power of 2. Unfortunately, many of the $C$'s we care about aren't such a power. More generally, the Hadamard Conjecture hypothesizes that there exists a valid Hadamard matrix for any $C$ that is divisible by 4. If true, then there are significantly more valid matrices, and in particular, common deep learning choices will be a multiple of 4. While not proven in general, the literature has found a way to construct many non-power-of-2 sized matrices using some of the following rules:\n\u2022 If $H_n$ and $H_m$ are Hadamard matrices, then $H_n \\otimes H_m$ is also a Hadamard matrix.\n\u2022 If $3 = q^k \\mod 4$ for some prime $q$ and integer $k > 0$, then we can use Paley's first construction Paley (1933) to produce a Hadamard matrix of size $q + 1$.\n\u2022 If $1 = q^k \\mod 4$ for some prime $q$ and integer $k > 0$, then we can use Paley's second construction to produce a Hadamard matrix of size $2 (q + 1)$.\nwhere $\\otimes$ is the Kronecker product. For our purposes, there are common feature dimensions that we want to be able to produce:\n\u2022 ViT-B: 768 [S(2) $\\otimes P_1(384)$]\n\u2022 ViT-L: 1024 [S(1024)]\n\u2022 SigLIP-L: 1152 [S(32) $\\otimes P_2(36)$]\n\u2022 ViT-H: 1280 [S(64) $\\otimes P_1 (20)$]\n\u2022 ViT-g: 1408 [S(32) $\\otimes P_1(44)$]\nWhere $P_i (x)$ is a Paley construction $i$ of size $x$, and $S(x)$ is a Sylvester construction of size $x$. In the case of Sylvester, we're referring to when $2^k = x$ for some $k \\in \\mathbb{N}_0$. For $P_1(384)$, we have the prime $q = 383$, which $3 = 383^1 \\mod 4$. For 1280, we can use (possibly among other options) $P_1(1280)$ as we have $q = 1279$, and thus $3 = 1279^1 \\mod 4$, or the compound version shown above. Finally, for $P_1(44)$ we have $q = 43$ and $3 = 43^1 \\mod 4$. So, by some stroke of luck, we have known constructions of Hadamard matrices for the major ViT widths. There are even more methods for constructing these matrices, and at the time of this writing, the smallest unknown Hadamard matrix is 668. While not exhaustive, for our purposes, the Sylvester and Paley constructions were sufficient to cover the models we studied."}, {"title": "A.2 HADAMARD WHITENING", "content": "A.2.1 PROOF OF HCA UNIFORM ERROR PROFILE\nReferring to equation 20:\n$\\epsilon_{hada} = UA^T H^T \\epsilon$ (20 revisited)\nwe demonstrate that each column of $UA^T H^T$ has identical magnitude, and further, that an error step of size $\\delta$ along any single dimension has identical magnitude in the original space."}, {"title": ""}]}