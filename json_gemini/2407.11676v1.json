{"title": "SKADA-Bench: Benchmarking Unsupervised Domain Adaptation Methods with Realistic Validation", "authors": ["Yanis Lalou", "Theo Gnassounou", "Antoine Collas", "Antoine de Mathelin", "Oleksii Kachaiev", "Ambroise Odonnat", "Alexandre Gramfort", "Thomas Moreau", "R\u00e9mi Flamary"], "abstract": "Unsupervised Domain Adaptation (DA) consists of adapting a model trained on a labeled source domain to perform well on an unlabeled target domain with some data distribution shift. While many methods have been proposed in the literature, fair and realistic evaluation remains an open question, particularly due to method-ological difficulties in selecting hyperparameters in the unsupervised setting. With SKADA-Bench, we propose a framework to evaluate DA methods and present a fair evaluation of existing shallow algorithms, including reweighting, mapping, and sub-space alignment. Realistic hyperparameter selection is performed with nested cross-validation and various unsupervised model selection scores, on both simulated datasets with controlled shifts and real-world datasets across diverse modalities, such as images, text, biomedical, and tabular data with specific feature extraction. Our benchmark highlights the importance of realistic validation and provides practical guidance for real-life applications, with key insights into the choice and impact of model selection approaches. SKADA-Bench is open-source, reproducible, and can be easily extended with novel DA methods, datasets, and model selection criteria without requiring re-evaluating competitors. SKADA-Bench is available on GitHub at https://github.com/scikit-adaptation/skada-bench.", "sections": [{"title": "1 Introduction", "content": "Given some training -or source- data, supervised learning consists in estimating a function that makes good predictions on target data. However, performance often drops when the source distribution used for training differs from the target distribution used for testing. This shift can be due, for instance, to the collection process or non-stationarity in the data, and is ubiquitous in real-life settings. It has been observed in various application fields, including tabular data [16], clinical data [18], or computer vision [15].\nDomain adaptation. Unsupervised Domain Adaptation (DA) addresses this problem by adapting a model trained on a labeled source dataset -or domain\u2013 so that it performs well on an unlabeled target domain, assuming some distribution shifts between the two [2, 41, 42]. As illustrated in Figure 1, source and target distributions can exhibit various types of shifts [36]: changes in feature distributions (covariate shift), class proportions (target shift), conditional distributions (conditional shift), or in distributions in particular subspaces (subspace shift). Depending on the type of shift, existing DA methods attempt to align the source distribution closer to the target using reweighting [46, 50], mapping [7, 52], or dimension reduction [12, 39] methods. More recently, it has been proposed to mitigate shifts in a feature space learned by deep learning [9, 15, 29, 53], primarily focusing on computer vision applications. Regardless of the core algorithm used to address the domain shift, hyperparameters must be tuned for optimal performance. Indeed, a critical challenge in applying DA methods to real-world cases is selecting the appropriate method and tuning its hyperparameters, especially given the unknown shift type and the absence of labels in the target domain.\nModel selection in DA settings. Without distribution shifts, classical model selection strategies -including hyperparameter optimization\u2013 rely on evaluating the generalization error with an independent labeled validation set. However, in DA, validating the hyperparameters in a supervised manner on the target domains is impossible due to the lack of labels. While it is possible to validate the hyperparameters on the source domain, it generally leads to a suboptimal model selection because of the distribution shift. In the literature, this problem is often raised but not always addressed. Some papers choose not to validate the parameters [39], while others validate on the source domain [52] or propose custom cross-validation methods [51]. Few papers focus specifically on DA model selection criteria, which we will call scorers in this paper. These scorers are used to select the methods' hyperparameters, and mainly consists of reweighting methods on source [49, 59], prediction entropy [37, 45] or circular validation [3]. One of the goals of our benchmark is to evaluate these approaches in realistic scenarios.\nBenchmarks of DA. As machine learning continues to flourish, new methods constantly emerge, making it essential to develop benchmarks that facilitate fair comparisons [22, 31, 35, 40]. In DA and related fields, several benchmarks have been proposed. Numerous papers focus on Out-of-distribution (OOD) datasets for different modalities: computer vision, text, graphs [25, 44], time-series [14], AI-aided drug discovery [23] or tabular dataset [16]. Due to the type of data considered, existing benchmarks are mainly focused on Deep DA methods [11, 24, 38, 56], offering an incomplete evaluation of DA literature. Moreover, only a few benchmarks propose a comparison of Deep unsupervised DA methods with realistic parameters selection, on computer vision [20, 38] and time series [11] data. Those benchmarks have shown the importance of validating with unsupervised scores and reveal that deep DA methods achieve much lower performance in realistic scenarios. In the present work, we focus on \u201cshallow\u201d DA methods, addressing a gap in the DA literature and its evaluation.\nContributions. In the following, we propose SKADA-Bench, an ambitious and fully reproducible benchmark with the following features: 1. a set of 4 simulated and 8 real-life datasets with different modalities (CV, NLP, tabular, biomedical) totaling 51 realistic shift scenarios, 2. a wide range of 20 shallow DA methods designed to handle different types of shifts, 3. a realistic model selection procedure using 5 different unsupervised scorers with nested cross-validation for hyperparameter selection, 4. an open-source code and publicly available datasets, easy to extend for new DA methods and datasets without the need to re-run the whole experiment.\nIn addition, we provide a detailed analysis of the results and derive guidelines for practitioners to select the best methods depending on the type of shifts, and the best scorer to perform unsupervised model selection. In particular, the effects of model selection and the scorer's choice on the final performances are highlighted, showing a clear gap between the unsupervised realistic scorers versus using target labels for supervised validation."}, {"title": "2 Domain adaptation and model selection without target labels", "content": "In this section, we first discuss the specificities of the unsupervised domain adaptation problem and introduce several types of data shift and their corresponding DA methods. Next, we discuss the different validation strategies used in the literature and the need for realistic scorers to compare DA methods.\n2.1 Data shifts and DA strategies\nDomain Adaptation problem and theory. The theoretical framework of DA is well established [2, 41, 42]. The main results highlight that the performance discrepancy of an estimator between the source and target domains is linked to the divergence between both distributions. This has motivated the majority of DA methods to search for a universal (or domain invariant) predictor by minimizing the divergence between the two domains through the adaptation of the distributions. This is done in practice by modeling and estimating the shift between the source and target distributions and then compensating for this shift before training a predictor.\nData Shifts and DA methods. A wide variety of shifts between the source and target distributions are possible. They are usually expressed as a relation between the joint distributions $P_s(x,y) = P_s(x|y)P_s(y) = P_s(y|x)P_s(x)$ in the source domain and $P_t(x,y) = P_t(x|y)P_t(y) = P_t(y|x)P_t(x)$ in the target domain. We now discuss the main types of shifts and the strategies proposed in the literature to mitigate them. Figure 1 illustrates these shifts.\nIn Covariate shift the conditionals probabilities are equal (i.e., $P_s(y|x) = P_t(y|x)$), but the feature marginals change (i.e., $P_s(x) \\neq P_t(x)$). Target shift is similar, but the label marginals change $P_s(y) \\neq P_t(y)$ while the conditionals are preserved. For classification problems, it corresponds to a change in the proportion of the classes between the two domains. Both of those shifts can be compensated by reweighting methods that assign different weights to the samples of the source domain to make it closer to the target domain [46, 50].\nIn Conditional shift, conditional probabilities differ between domain (i.e., $P_s(x|y) \\neq P_t(x|y)$ or $P_s(y|x) \\neq P_t(y|x)$). This shift is typically harder to compensate for, necessitating explicit modeling to address it effectively. For instance, several approaches model the shift as a mapping $m$ between the source and target domain such that $P_s(y|m(x)) = P_t(y|x)$ [7, 52]. The estimated mapping is then applied to the source data before training a predictor.\nSubspace shift, also known as domain invariant representation, assumes that while probabilities are different between the domains, there exists a subspace $Z$ and a function $\\varphi : X \\rightarrow Z$ such that $P_s(y|\\varphi(x)) = P_t(y|\\varphi(x))$. This implies that a classifier trained on this subspace will perform well across both domains. Subspace methods aim to identify the subspace $Z$ and the function $\\varphi$, as developed in [12, 39]. Note that, as discussed in the introduction, a natural extension of this idea is to learn a non-linear invariant subspace using deep learning [15, 53].\n2.2 DA model selection strategies\nAs seen above, the different DA methods are usually designed for one type of shift, yet in a practical problem, one does not know what shift is present. This raises the problem of method and parameter selection when facing a new problem. In this section, we discuss the validation strategies proposed in the literature to compare DA methods, focusing on realistic scorers that do not use target labels.\nRealistic DA scorers. In the literature, few papers propose realistic DA scorers to validate the parameters of the methods, i.e., unsupervised scorers that do not require target labels. The Importance Weighted (IW) scorer [49] computes the score as a reweighted accuracy on labeled sources data. The Deep Embedded Validation (DEV) [59] can be seen as an IW in the latent space with a variance reduction strategy. DEV was originally proposed for deep learning models but can be used on shallow DA methods that compute features from the data (mapping/subspaces). The Prediction Entropy (PE) scorer [37] measures the uncertainty associated with model predictions on the target data. Soft Neighborhood Density (SND) [45] also computes an entropy but on a normalized pairwise similarity matrix between probabilistic predictions on target. The Circular Validation (CircV) scorer [3] performs DA by first adapting the model from the source to the target domain and predicting"}, {"title": "3 A realistic benchmark for DA", "content": "In this section, we present our benchmark framework. First, we introduce the parameter validation strategies. Then, we present the compared DA methods followed by a description of the datasets used in the benchmark.\n3.1 Nested cross-validation loop and implementation\nWe discuss below the nested cross-validation and the implementation details of the benchmark.\nHyperparameter validation loop. We propose a nested loop cross-validation procedure, depicted in Figure 2. First, the source and target data are split into multiple outer test and train sets (outer loop in Figure 2). The test sets are kept to compute the final accuracy for both the source and target domains. For each split in the outer loop, we use a nested loop to select the DA methods' parameters. Here, the training sets are further divided into nested train and validation sets (nested loop in Figure 2). Note that no labels are available for the target nested train and validation sets in this loop. The target training set is used to train the DA method, while the target validation set allows to compute the unsupervised score and select the best model.\nFor both loops, the data is split randomly 5 times using stratified sampling with an 80%/20% train/test split. For one given method, we evaluate all the unsupervised scorers discussed earlier, as well as a supervised scorer that uses target labels, over all the nested splits. After averaging, the scores over the splits, the best hyperparameters are selected according to each scorer and then used to train a final classifier on the outer training sets. Although the supervised scorer cannot be used in practice, it is included in our results to actually evaluate the performance drop due to the absence of target labels.\nTo limit complexity and perform a fair comparison of the methods, we set a timeout of 4 hours for performing the nested loop.\nBase estimator. Existing domain adaptation methods typically rely on either a base estimator trained on the adapted data or an iterative estimation process to adapt this estimator to the target data. The choice of the base estimator is crucial, as it significantly impacts the final performance. Before validating the hyperparameters of the DA methods, we determined the best estimator for each dataset using a grid-search on the source data. We tested multiple hyperparameters for Logistic Regression, SVM with RBF kernel, and XGBoost [5], selecting the ones that maximize the average accuracy on the source test sets. Note that for some methods that specifically require an SVM estimator (i.e., JDOT and DASVM), we only validate SVM as the base estimator. We validated the base estimator separately from the DA methods parameters to reduce computational complexity and avoid too complex hyperparameter grids that can compromise the reliability of DA scorers.\nBest scorer selection and statistical test. For all methods, we select the best validation scorer as the one that maximizes the averaged accuracy on the target domains for all real datasets. This provides a reasonable and actionable choice of scorer for each DA method for practitioners. For all methods and datasets, we perform a paired Wilcoxon signed-rank test at the 0.05 level to detect significant gain or drop in performance with respect to the no DA approach, denoted by \u201cTrain Src\" in the following. The test is done using the accuracy measures of the DA method with the selected scorer and the Train Src for all shifts and outer splits, ensuring between 10 and 60 values depending on the dataset.\nPython implementation. The benchmark code will be made available on GitHub upon publication of the paper. Our benchmark is implemented following the benchopt framework [35], which provides standardized ways of organizing and running benchmarks for ML in Python. This framework facilitates reproducing the benchmark's results, with tools to install the dependencies, run the methods in parallel, or cache the results to prevent redundant computations. It also makes it easy to extend the benchmark with additional datasets and methods, enabling it to evolve to account for the advances in the field. In the supplementary materials, we provide examples demonstrating how to add DA methods or datasets to the benchmark. Using this framework, we aim to make SKADA-Bench a reference benchmark to evaluate new DA methods in realistic scenarios with valid performance estimations.\n3.2 Compared DA methods\nIn this section, we present the different families of domain adaptation methods that we compare in our benchmark. We group the methods into four categories: reweighting methods, mapping methods, subspace methods, and others. We provide a brief description of each method and the corresponding references.\nReweighting methods. These methods aim to reweight the source data to make it closer to the target data. The weights are estimated using different methods such as kernel density estimation (Dens. RW) [50], Gaussian estimation (Gauss. RW) [46], discriminative estimation (Discr. RW) [46], or nearest-neighbors (NN RW) [30]. Other reweighting estimate weights by minimizing a divergence between the source and target distributions such as Kullback-Leibler Importance Estimation Procedure (KLIEP) [51] or Kernel Mean Matching (KMM) [21]. Finally, we also include the MMDTarS method [60] that uses a Maximum Mean Discrepancy (MMD) to estimate the weights under the target shift hypothesis.\nMapping methods. These methods aim to find a mapping between the source and target data that minimizes the distribution shift. The Correlation Alignment method (CORAL) [52] aligns the second-order statistics of source and target distributions. The Maximum Mean Discrepancy (MMD-LS) method [60] minimizes the MMD to estimate an affine Location-Scale mapping. Finally, the Optimal Transport (OT) mapping methods [7] use the optimal transport plan to align with a non-linear mapping of the source and target distributions with exact OT (MapOT), entropic regularization (EntOT), or class-based regularization (ClassRegOT). Finally, the Linear OT method [13] uses a linear mapping to align the source and target distributions, assuming Gaussian distributions.\nSubspace methods. These methods aim to learn a subspace where the source and target data have the same distribution. The Transfer Component Analysis (TCA) method [39] searches for a kernel\""}, {"title": "3.3 Compared datasets", "content": "In this section, we present the datasets used in our experiments. We first introduce the synthetic datasets that implement different known shifts. Then, we describe the real-world datasets from various modalities and tasks such as Computer Vision (CV), Natural language Processing (NLP), tabular data, and biosignals.\nSimulated datasets. The objective of the simulated datasets is to evaluate the performance of the DA methods under different types of shifts. Knowing that multiple DA methods have been built to handle specific shifts, evaluating them with this dataset will demonstrate whether they perform as expected and if they are properly validated.\nThe 4 simulated shifts in 2D, covariate (Cov. shift), target (Tar. shift) conditional (Cond. shift) and Subspace (Sub. shift) shift are illustrated in Figure 1. The source domain is represented by two non-linearly separable classes generated from one large and several smaller Gaussian blobs. In the experiments, the level of noise has been adjusted from Figure 1 to make the problem more difficult. For the subspace shift scenario, the source domain consists of one class represented by a large Gaussian blob and another class comprising Gaussian blobs positioned along the sides of the large one. The target domain is flipped along the diagonal, making the task challenging in the original space but feasible upon diagonal projection.\nReal-word datasets. The real-world datasets used in our benchmark are summarized in Table 1. We select 8 datasets from different modalities and tasks: Computer Vision (CV) with Office31 [26], Office Home [55], and MNIST/USPS [28], Natural Language Processing (NLP) with 20Newsgroup [27] and Amazon Review [33], Tabular Data with Mushrooms [8] and Phishing [34], and Biosignals with BCI Competition IV [54]. The datasets are chosen to represent a wide range of shifts and to evaluate the performance of the methods on different types of data.\nThe datasets are preprocessed with feature extraction to ensure reasonable performance when trained on each domain. For example, images are embedded using pre-trained models followed by a PCA (except MNIST/USPS where only PCA is used), and textual data is embedded using Large Language Models (LLM) [43, 57] before applying a PCA. The tabular data are one-hot encoded to transform categorical data into numerical data. The biosignals from Brain-Computer Interface (BCI) data are embedded using the state-of-the-art tangent space representation proposed in [1]. The datasets are"}, {"title": "4 Benchmark results", "content": "We now present the results of the benchmark. Training and evaluation across all experiments required 1,215 CPU-hours on a standard Slurm [58] cluster. We first discuss and compare the performances of the methods on the different datasets. Then, a detailed study of the unsupervised scorers is provided.\n4.1 Performance of the DA methods\nResults table. First, we report the realistic performances of the different methods when using their selected scorer on the different datasets in Table 2. The cells showcasing a significant change in performance with the Wilcoxon test are highlighted with colors. Blue indicates an increase in performance, while red indicates a loss. The intensity of the color corresponds to the magnitude of the gain or loss - the darker the shade, the larger the positive or negative change. Cells with a NA values indicate that the method was not applicable to the dataset (DASVM is limited to binary classification) or that the method has reached a timeout. We also report the best scorer and the average rank of the methods for all datasets. In addition to Table 2 providing realistic performance estimations, we also report in Table D.12 (Appendix D) the results when using the non-realistic supervised scorer.\nSimulated data with known shifts. On simulated data with known shifts (i.e., underlined datasets), DA methods tend to show a small but significant gain on the shift they were designed for. However, they rarely reach the Train Tgt performances, and as expected, the methods perform poorly on the shift they were not designed for. For Cov. shift, the improvement with reweighting methods is very limited. We believe that using a complex base estimator (SVM with an RBF kernel) enables us to train an estimator that works well on both source and target, leading to less impressive results. The Appendix D contains detailed tables for two other base estimators (i.e., Logistic Regression in Table D.9 and XGBoost in Table D.11). These tables show that the gain is larger for Logistic"}, {"title": "4.2 Study of validation scorers", "content": "We now investigate the performance of the various scorers to select hyperparameters of the DA method. First, we consider the relationship between the cross-val score and the accuracy for each inner split. In Figure 3, we plot for each scorer the cross-val score as a function of the accuracy computed on the test set and report the Pearson correlation coefficient $\u03c1$. As expected, the supervised scorer is highly correlated with the accuracy ($\u03c1$ = 0.98), as it has access to the target labels. We observe that SND, DEV, and PE do not provide a good proxy to select hyperparameters that give the best-performing models ($\u03c1$ < 0.06). On the contrary, IW and CircV are correlated with the accuracy, $\u03c1$ = 0.53 and $\u03c1$ = 0.71 respectively. This is coherent with their selection as the best scorer in most scenarios in Table 2. Still, while those scorers are well correlated with the target accuracy, it is important to note that they have a large variance. For instance, a score close to 1 in IW or CircV corresponds to an accuracy between 0.5 and 1.0.\nFurthermore, we provide in Figures D.3 and D.4, from Appendix D, several visualizations that illustrate the relationship between the accuracy achieved when using a supervised scorer and the accuracy obtained when using different unsupervised scorers. We also visualize in Figure D.2 the drop in performance when using the best-unsupervised scorer instead of the supervised scorer. Interestingly some methods such as KMM, EntOT, and ClassRegOT can lose up to 10% accuracy when using realistic scorers, which might come from their high number of parameters or their sensitivity to them.\nOur results thus show that most scorers have poor results when evaluated on many datasets. Of the five methods under consideration, only two achieve satisfactory performance, although incurring large variance in their results. This shows that proper hyperparameter selection is still an open question, that needs attention from the research comunity to guide practitioners toward real life applications of unsupervised DA technics."}, {"title": "5 Conclusion", "content": "In this work, we introduced SKADA-Bench, a comprehensive benchmark for unsupervised domain adaptation, focusing on shallow DA methods and the impact of their model selection. Our findings reveal that few DA methods consistently perform well across diverse datasets and that model selection scorers significantly influence their effectiveness. For each DA method, we provide the optimal model selection scorer for unsupervised hyperparameter tuning based on our experiments. We believe this benchmark will aid the community in understanding the critical role of model selection in DA performance and inspire the development of methods that are more robust to hyperparameter choices."}]}