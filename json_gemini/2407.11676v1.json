{"title": "SKADA-Bench: Benchmarking Unsupervised Domain Adaptation Methods with Realistic Validation", "authors": ["Yanis Lalou", "Theo Gnassounou", "Antoine Collas", "Antoine de Mathelin", "Oleksii Kachaiev", "Ambroise Odonnat", "Alexandre Gramfort", "Thomas Moreau", "R\u00e9mi Flamary"], "abstract": "Unsupervised Domain Adaptation (DA) consists of adapting a model trained on a labeled source domain to perform well on an unlabeled target domain with some data distribution shift. While many methods have been proposed in the literature, fair and realistic evaluation remains an open question, particularly due to methodological difficulties in selecting hyperparameters in the unsupervised setting. With SKADA-Bench, we propose a framework to evaluate DA methods and present a fair evaluation of existing shallow algorithms, including reweighting, mapping, and subspace alignment. Realistic hyperparameter selection is performed with nested cross-validation and various unsupervised model selection scores, on both simulated datasets with controlled shifts and real-world datasets across diverse modalities, such as images, text, biomedical, and tabular data with specific feature extraction. Our benchmark highlights the importance of realistic validation and provides practical guidance for real-life applications, with key insights into the choice and impact of model selection approaches. SKADA-Bench is open-source, reproducible, and can be easily extended with novel DA methods, datasets, and model selection criteria without requiring re-evaluating competitors. SKADA-Bench is available on GitHub at https://github.com/scikit-adaptation/skada-bench.", "sections": [{"title": "Introduction", "content": "Given some training -or source- data, supervised learning consists in estimating a function that makes good predictions on target data. However, performance often drops when the source distribution used for training differs from the target distribution used for testing. This shift can be due, for instance, to the collection process or non-stationarity in the data, and is ubiquitous in real-life settings. It has been observed in various application fields, including tabular data [16], clinical data [18], or computer vision [15].\nDomain adaptation. Unsupervised Domain Adaptation (DA) addresses this problem by adapting a model trained on a labeled source dataset -or domain\u2013 so that it performs well on an unlabeled target domain, assuming some distribution shifts between the two [2, 41, 42]. As illustrated in Figure 1, source and target distributions can exhibit various types of shifts [36]: changes in feature distributions (covariate shift), class proportions (target shift), conditional distributions (conditional shift), or in distributions in particular subspaces (subspace shift). Depending on the type of shift, existing DA methods attempt to align the source distribution closer to the target using reweighting [46, 50],\nmapping [7, 52], or dimension reduction [12, 39] methods. More recently, it has been proposed to mitigate shifts in a feature space learned by deep learning [9, 15, 29, 53], primarily focusing on computer vision applications. Regardless of the core algorithm used to address the domain shift, hyperparameters must be tuned for optimal performance. Indeed, a critical challenge in applying DA methods to real-world cases is selecting the appropriate method and tuning its hyperparameters, especially given the unknown shift type and the absence of labels in the target domain.\nModel selection in DA settings. Without distribution shifts, classical model selection strategies -including hyperparameter optimization\u2013 rely on evaluating the generalization error with an independent labeled validation set. However, in DA, validating the hyperparameters in a supervised manner on the target domains is impossible due to the lack of labels. While it is possible to validate the hyperparameters on the source domain, it generally leads to a suboptimal model selection because of the distribution shift. In the literature, this problem is often raised but not always addressed. Some papers choose not to validate the parameters [39], while others validate on the source domain [52] or propose custom cross-validation methods [51]. Few papers focus specifically on DA model selection criteria, which we will call scorers in this paper. These scorers are used to select the methods' hyperparameters, and mainly consists of reweighting methods on source [49, 59], prediction entropy [37, 45] or circular validation [3]. One of the goals of our benchmark is to evaluate these approaches in realistic scenarios.\nBenchmarks of DA. As machine learning continues to flourish, new methods constantly emerge, making it essential to develop benchmarks that facilitate fair comparisons [22, 31, 35, 40]. In DA and related fields, several benchmarks have been proposed. Numerous papers focus on Out-of-distribution (OOD) datasets for different modalities: computer vision, text, graphs [25, 44], time-series [14], AI-aided drug discovery [23] or tabular dataset [16]. Due to the type of data considered, existing benchmarks are mainly focused on Deep DA methods [11, 24, 38, 56], offering an incomplete evaluation of DA literature. Moreover, only a few benchmarks propose a comparison of Deep unsupervised DA methods with realistic parameters selection, on computer vision [20, 38] and time series [11] data. Those benchmarks have shown the importance of validating with unsupervised scores and reveal that deep DA methods achieve much lower performance in realistic scenarios. In the present work, we focus on \u201cshallow\u201d DA methods, addressing a gap in the DA literature and its evaluation.\nContributions. In the following, we propose SKADA-Bench, an ambitious and fully reproducible benchmark with the following features: 1. a set of 4 simulated and 8 real-life datasets with different modalities (CV, NLP, tabular, biomedical) totaling 51 realistic shift scenarios, 2. a wide range of 20 shallow DA methods designed to handle different types of shifts, 3. a realistic model selection procedure using 5 different unsupervised scorers with nested cross-validation for hyperparameter selection, 4. an open-source code and publicly available datasets, easy to extend for new DA methods and datasets without the need to re-run the whole experiment.\nIn addition, we provide a detailed analysis of the results and derive guidelines for practitioners to select the best methods depending on the type of shifts, and the best scorer to perform unsupervised model selection. In particular, the effects of model selection and the scorer's choice on the final performances are highlighted, showing a clear gap between the unsupervised realistic scorers versus using target labels for supervised validation."}, {"title": "Domain adaptation and model selection without target labels", "content": "In this section, we first discuss the specificities of the unsupervised domain adaptation problem and introduce several types of data shift and their corresponding DA methods. Next, we discuss the different validation strategies used in the literature and the need for realistic scorers to compare DA methods."}, {"title": "Data shifts and DA strategies", "content": "Domain Adaptation problem and theory. The theoretical framework of DA is well established [2, 41, 42]. The main results highlight that the performance discrepancy of an estimator between the source and target domains is linked to the divergence between both distributions. This has motivated the majority of DA methods to search for a universal (or domain invariant) predictor by minimizing the divergence between the two domains through the adaptation of the distributions. This is done in practice by modeling and estimating the shift between the source and target distributions and then compensating for this shift before training a predictor.\nData Shifts and DA methods. A wide variety of shifts between the source and target distributions are possible. They are usually expressed as a relation between the joint distributions $P_s(x,y) = P_s(x|y)P_s(y) = P_s(y|x)P_s(x)$ in the source domain and $P_t(x,y) = P_t(x|y)P_t(y) = P_t(y|x)P_t(x)$ in the target domain. We now discuss the main types of shifts and the strategies proposed in the literature to mitigate them. Figure 1 illustrates these shifts.\nIn Covariate shift the conditionals probabilities are equal (i.e., $P_s(y|x) = P_t(y|x)$), but the feature marginals change (i.e., $P_s(x) \\neq P_t(x)$). Target shift is similar, but the label marginals change $P_s(y) \\neq P_t(y)$ while the conditionals are preserved. For classification problems, it corresponds to a change in the proportion of the classes between the two domains. Both of those shifts can be compensated by reweighting methods that assign different weights to the samples of the source domain to make it closer to the target domain [46, 50].\nIn Conditional shift, conditional probabilities differ between domain (i.e., $P_s(x|y) \\neq P_t(x|y)$ or $P_s(y|x) \\neq P_t(y|x)$). This shift is typically harder to compensate for, necessitating explicit modeling to address it effectively. For instance, several approaches model the shift as a mapping $m$ between the source and target domain such that $P_s(y|m(x)) = P_t(y|x)$ [7, 52]. The estimated mapping is then applied to the source data before training a predictor.\nSubspace shift, also known as domain invariant representation, assumes that while probabilities are different between the domains, there exists a subspace $Z$ and a function $\\phi : X \\rightarrow Z$ such that $P_s(y|\\phi(x)) = P_t(y|\\phi(x))$. This implies that a classifier trained on this subspace will perform well across both domains. Subspace methods aim to identify the subspace $Z$ and the function $\\phi$, as developed in [12, 39]. Note that, as discussed in the introduction, a natural extension of this idea is to learn a non-linear invariant subspace using deep learning [15, 53]."}, {"title": "DA model selection strategies", "content": "As seen above, the different DA methods are usually designed for one type of shift, yet in a practical problem, one does not know what shift is present. This raises the problem of method and parameter selection when facing a new problem. In this section, we discuss the validation strategies proposed in the literature to compare DA methods, focusing on realistic scorers that do not use target labels.\nRealistic DA scorers. In the literature, few papers propose realistic DA scorers to validate the parameters of the methods, i.e., unsupervised scorers that do not require target labels. The Importance Weighted (IW) scorer [49] computes the score as a reweighted accuracy on labeled sources data. The Deep Embedded Validation (DEV) [59] can be seen as an IW in the latent space with a variance reduction strategy. DEV was originally proposed for deep learning models but can be used on shallow DA methods that compute features from the data (mapping/subspaces). The Prediction Entropy (PE) scorer [37] measures the uncertainty associated with model predictions on the target data. Soft Neighborhood Density (SND) [45] also computes an entropy but on a normalized pairwise similarity matrix between probabilistic predictions on target. The Circular Validation (CircV) scorer [3] performs DA by first adapting the model from the source to the target domain and predicting"}, {"title": "A realistic benchmark for DA", "content": "In this section, we present our benchmark framework. First, we introduce the parameter validation strategies. Then, we present the compared DA methods followed by a description of the datasets used in the benchmark."}, {"title": "Nested cross-validation loop and implementation", "content": "We discuss below the nested cross-validation and the implementation details of the benchmark.\nHyperparameter validation loop. We propose a nested loop cross-validation procedure, depicted in Figure 2. First, the source and target data are split into multiple outer test and train sets (outer loop in Figure 2). The test sets are kept to compute the final accuracy for both the source and target domains. For each split in the outer loop, we use a nested loop to select the DA methods' parameters. Here, the training sets are further divided into nested train and validation sets (nested loop in Figure 2). Note that no labels are available for the target nested train and validation sets in this loop. The target training set is used to train the DA method, while the target validation set allows to compute the unsupervised score and select the best model.\nFor both loops, the data is split randomly 5 times using stratified sampling with an 80%/20% train/test split. For one given method, we evaluate all the unsupervised scorers discussed earlier, as well as a supervised scorer that uses target labels, over all the nested splits. After averaging, the scores over the splits, the best hyperparameters are selected according to each scorer and then used to train a final classifier on the outer training sets. Although the supervised scorer cannot be used in practice, it is included in our results to actually evaluate the performance drop due to the absence of target labels.\nTo limit complexity and perform a fair comparison of the methods, we set a timeout of 4 hours for performing the nested loop."}, {"title": "Compared DA methods", "content": "In this section, we present the different families of domain adaptation methods that we compare in our benchmark. We group the methods into four categories: reweighting methods, mapping methods, subspace methods, and others. We provide a brief description of each method and the corresponding references.\nReweighting methods. These methods aim to reweight the source data to make it closer to the target data. The weights are estimated using different methods such as kernel density estimation (Dens. RW) [50], Gaussian estimation (Gauss. RW) [46], discriminative estimation (Discr. RW) [46], or nearest-neighbors (NN RW) [30]. Other reweighting estimate weights by minimizing a divergence between the source and target distributions such as Kullback-Leibler Importance Estimation Procedure (KLIEP) [51] or Kernel Mean Matching (KMM) [21]. Finally, we also include the MMDTarS method [60] that uses a Maximum Mean Discrepancy (MMD) to estimate the weights under the target shift hypothesis.\nMapping methods. These methods aim to find a mapping between the source and target data that minimizes the distribution shift. The Correlation Alignment method (CORAL) [52] aligns the second-order statistics of source and target distributions. The Maximum Mean Discrepancy (MMD-LS) method [60] minimizes the MMD to estimate an affine Location-Scale mapping. Finally, the Optimal Transport (OT) mapping methods [7] use the optimal transport plan to align with a non-linear mapping of the source and target distributions with exact OT (MapOT), entropic regularization (EntOT), or class-based regularization (ClassRegOT). Finally, the Linear OT method [13] uses a linear mapping to align the source and target distributions, assuming Gaussian distributions.\nSubspace methods. These methods aim to learn a subspace where the source and target data have the same distribution. The Transfer Component Analysis (TCA) method [39] searches for a kernel"}, {"title": "Compared datasets", "content": "In this section, we present the datasets used in our experiments. We first introduce the synthetic datasets that implement different known shifts. Then, we describe the real-world datasets from various modalities and tasks such as Computer Vision (CV), Natural language Processing (NLP), tabular data, and biosignals.\nSimulated datasets. The objective of the simulated datasets is to evaluate the performance of the DA methods under different types of shifts. Knowing that multiple DA methods have been built to handle specific shifts, evaluating them with this dataset will demonstrate whether they perform as expected and if they are properly validated.\nThe 4 simulated shifts in 2D, covariate (Cov. shift), target (Tar. shift) conditional (Cond. shift) and Subspace (Sub. shift) shift are illustrated in Figure 1. The source domain is represented by two non-linearly separable classes generated from one large and several smaller Gaussian blobs. In the experiments, the level of noise has been adjusted from Figure 1 to make the problem more difficult. For the subspace shift scenario, the source domain consists of one class represented by a large Gaussian blob and another class comprising Gaussian blobs positioned along the sides of the large one. The target domain is flipped along the diagonal, making the task challenging in the original space but feasible upon diagonal projection.\nReal-word datasets. The real-world datasets used in our benchmark are summarized in Table 1. We select 8 datasets from different modalities and tasks: Computer Vision (CV) with Office31 [26], Office Home [55], and MNIST/USPS [28], Natural Language Processing (NLP) with 20Newsgroup [27] and Amazon Review [33], Tabular Data with Mushrooms [8] and Phishing [34], and Biosignals with BCI Competition IV [54]. The datasets are chosen to represent a wide range of shifts and to evaluate the performance of the methods on different types of data.\nThe datasets are preprocessed with feature extraction to ensure reasonable performance when trained on each domain. For example, images are embedded using pre-trained models followed by a PCA (except MNIST/USPS where only PCA is used), and textual data is embedded using Large Language Models (LLM) [43, 57] before applying a PCA. The tabular data are one-hot encoded to transform categorical data into numerical data. The biosignals from Brain-Computer Interface (BCI) data are embedded using the state-of-the-art tangent space representation proposed in [1]. The datasets are"}, {"title": "Adding new methods and datasets to SKADA-Bench", "content": "Using the benchopt framework for this benchmark allows users to easily add novel domain adaptation (DA) methods and datasets. To that end, users should adhere to the benchopt [35] conventions. We provide below the guidelines with examples in Python to add a new DA method and a new dataset to SKADA-Bench."}, {"title": "Adding a new DA method", "content": "A new DA method can be easily added with the following:\n\u2022 Create file with a class called Solver that inherits from DASolver and place it in the solvers folder.\n\u2022 This class should implement a get_estimator() function, which returns a class inheriting from sklearn. BaseEstimator and accepts sample_weight as fit parameter. In the benchmark we used the Domain Adaptation toolbox SKADA [17] that provides many esDA estimatos with correct interface.\nWe provide below an example of Python implementation to add a new DA method to SKADA-Bench."}, {"title": "Adding a new dataset", "content": "A new DA dataset can be easily added with the following:\n\u2022 Create a file with a class called Dataset that inherits from BaseDataset and place it in the datasets folder.\n\u2022 This class should implement a get_data() function, which returns a dictionary with keys X, y, and sample_domain.\nWe provide below an example of Python implementation to add a new dataset to SKADA-Bench."}, {"title": "Benchmark detailed results", "content": "In Table 2 of the main paper, the reported performance for each method on a given dataset is an average over the number of shifts, i.e., the number of source-target pairs denoted by #adapt in Table 1. In this section, we provide additional details on the performance of methods for each shift in each dataset\nThese detailed tables where cell in green denote a gain wrt Train Src (average outside of standard deviation of Train Src) better illustrate the challenges of domain adaptation (DA) methods. They show that not all shifts are equivalent within a given dataset. For example, Table D.5 reveals that only 4 shifts in the AmazonReview dataset present a DA problem (defined as a > 3% difference in accuracy between Train Src and Train Tgt). While for the other shifts, we achieve similar performance whether we train on source or target data. Additionally, some specific shifts present a DA problem that no method can successfully address. This can be seen in the dsl \u2192 amz shift in the Office31 dataset, as shown in Table D.2. Finally, some DA methods perform consistently across all shifts within a dataset, as demonstrated by the results for the 20Newsgroup dataset in Table D.4."}, {"title": "Impact of the base estimators on the simulated datasets", "content": "As mentioned in the main paper, it is possible to partly compensate for the shift by choosing the right base estimator. In this part, we provide the results on the Simulated dataset for three different base estimators: Logistic Regression (LR) in Table D.9, SVM in Table D.10, and XGBoost in Table D.11. Observing the two first rows for covariate shift, we see that with LR (Table D.9), there is a significant drop in performance between training on the source v.s. training on the target (~ 10%), while using SVC (Table D.10) only leads to a drop (~ 3%). Finally, using XGBoost (Table D.11) maintains the performance. The reweighting DA methods help compensate for the shift when using a simpler LR estimator. However when using an SVC, as shown in the main paper, the reweighting does not help to compensate for the covariate shift. If we look at the other shifts, the problem is harder. The subspace methods help with subspace shift, and the mapping methods help with the conditional shift.\nThese Tables show the importance of choosing the right base estimator. It is clear that choosing an appropriate base estimator can partially compensate for some shifts."}, {"title": "Unrealistic validation with supervised scorer", "content": "Table D.12 shows the results when we choose the supervised scorer that is when validating on target labels. It is important to highlight that this choice is impossible in real life applications due to the lack of target labels.\nWhen using the target labels, the method's parameters are better validated. This can be seen by the significant increase in the table (blue values), which are numerous in this table compared to the one with the selected realistic scorer. For example, the method MMDTarS, which is made for Target shift, compensates all the shift simulated covariate shifts when we select the model with a supervised scorer.\nWhen looking at the rank, 11 DA methods have a higher rank than Train Src compared to 8 when using realistic scorer."}, {"title": "Comparions between supervised and unsupervised scorers", "content": "Impact on the cross-validation score. We observe in Figure D.1 the cross-validation score as a function of the final accuracy for various DA methods type and for both supervised and unsupervised scorers. As expected, we observe a good correlation between accuracy and cross-validation score with the supervised scorer. An important remark is that the Circular Validation (CircV) [4] shows some correlation between accuracy and cross-validation score. It indicates that this unsupervised scorer might be the most suitable choice for hyperparameter selection. This is supported by our extended experimental results in Table 2 for which the CircV is selected as the best scorer the most often. A similar trend can be observed for the Importance Weighted (IW) [49] which is also confirmed in Table 2."}]}