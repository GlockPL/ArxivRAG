{"title": "HEIE: MLLM-Based Hierarchical Explainable AIGC Image Implausibility Evaluator", "authors": ["Fan Yang", "Ru Zhen", "Jianing Wang", "Yanhao Zhang", "Haoxiang Chen", "Haonan Lu", "Sicheng Zhao", "Guiguang Ding"], "abstract": "AIGC images are prevalent across various fields, yet they frequently suffer from quality issues like artifacts and unnatural textures. Specialized models aim to predict defect region heatmaps but face two primary challenges: (1) lack of explainability, failing to provide reasons and analyses for subtle defects, and (2) inability to leverage common sense and logical reasoning, leading to poor generalization. Multimodal large language models (MLLMs) promise better comprehension and reasoning but face their own challenges: (1) difficulty in fine-grained defect localization due to the limitations in capturing tiny details; and (2) constraints in providing pixel-wise outputs necessary for precise heatmap generation. To address these challenges, we propose HEIE: a novel MLLM-Based Hierarchical Explainable image Implausibility Evaluator. We introduce the CoT-Driven Explainable Trinity Evaluator, which integrates heatmaps, scores, and explanation outputs, using CoT to decompose complex tasks into subtasks of increasing difficulty and enhance interpretability. Our Adaptive Hierarchical Implausibility Mapper synergizes low-level image features with high-level mapper tokens from LLMs, enabling precise local-to-global hierarchical heatmap predictions through an uncertainty-based adaptive token approach. Moreover, we propose a new dataset: Expl-AIGI-Eval, designed to facilitate interpretable implausibility evaluation of AIGC images. Our method demonstrates state-of-the-art performance through extensive experiments.", "sections": [{"title": "1. Introduction", "content": "Recently, Artificial Intelligence Generated Content (AIGC) has rapidly advanced, leading to the widespread use of AIGC images across various fields. However, they often face quality issues such as artifacts, unnatural textures, and problematic regions. Accordingly, many studies focus on"}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. AI-Generated Images", "content": "Advancements in text-to-image generation models have significantly impacted creative industries like art, design, and advertising. Stable Diffusion [30]uses denoising processes to produce high-quality images. It is open-source, encouraging academic and industrial applications. DALL-E 3 [4]"}, {"title": "2.2. AIGC Image Evaluation", "content": "Effective evaluation are required to optimize generative models. Many studies have released datasets to emulate human assessment [14, 19, 21, 42, 44]. Large-scale visual understanding models are now commonly used for reliable evaluation due to their strong semantic capabilities [28, 41, 50]. Prior research has shown that MLLMs are proficient in both low-level visual evaluations [37, 39] and high-level semantic comprehension [49, 52]. Some studies [17, 34, 38, 40] activate the visual evaluation capabilities of multimodal MLLMs through data fine-tuning.\nExisting models primarily aim to align their scores with human preferences [38, 42] or aesthetic evaluation [15, 16]. These approaches fail to identify specific fine-grained problem areas within images, which hinders users and researchers from making targeted improvements.\nRecently, HumanRefiner [10] and RichHF [23] have introduced tasks focused on detecting fine-grained prob-"}, {"title": "3. Method", "content": ""}, {"title": "3.1. Overview", "content": "As shown in Fig. 2, HEIE first employs an Adaptive Hierarchical Implausibility Mapper to generate an image implausibility region heatmap. Subsequently, a verisimilitude scorer is used to predict the image score. Furthermore, with the aid of a Co-driven Explainable System, we stimulated the potential of the LLM to achieve mutual enhancement and joint output of the heatmap, score, and explanation."}, {"title": "3.2. Adaptive Hierarchical Implausibility Mapper", "content": ""}, {"title": "3.2.1. MLLM-Based Implausibility Mapper", "content": "To obtain fine-grained, precise image implausibility regions, pixel-level mask outputs are necessary. However, most MLLMs only provide text information and cannot produce heatmap predictions. To address this, we design the"}, {"title": "3.2.2. Hierarchical Implausibility Mapper", "content": "Implausibility detection is not the primary objective of mainstream general-purpose MLLMs. Defect localization often requires detailed information, including small defects like fingers and eyes, making it crucial to detect these nuances. We developed a method targeting both global and local defects by segmenting images into adaptive patches according to resolution [7]. The image encoder processes the thumbnail and N segmented patches, generating global features $F_g$ and local features $F_l$. The LLM then outputs N local information-enhanced tokens, $[MAP]_i$, for each patch. Tokens $T_i$ combine with local features $F_i$ and input into the Implausibility Mapper (see Sec. 3.2.1), where cross-attention mechanisms generate local implausibility heatmaps $L_i$. These heatmaps are concatenated to form a complete heatmap:\n$H_l = F_{concat}(L_1, L_2, ..., L_N)$"}, {"title": "3.2.3. Adaptive Implausibility Mapper", "content": "AIGC images vary in resolution, aspect ratio, and content, necessitating adaptive strategies for handling implausibility. Adaptive special map token: We propose an adaptive special map token to manage local information for AIGC images. Our hierarchical design adjusts the local token count"}, {"title": "3.3. CoT-Driven Explainable Trinity Evaluator", "content": ""}, {"title": "3.3.1. Verisimilitude Scorer", "content": "Our model generates a heatmap of implausible regions and an overall verisimilitude score for comprehensive evaluation. LLMs struggle with numerical outputs, so we propose the Verisimilitude Scorer method to regress the score.\nWe define a special score token [SCORE] within the LLM, capturing the image's verisimilitude. The hidden state of the [SCORE] token is processed by a Feed-Forward Network (FFN) to regress an initial score, $S_{token}$.\nOur experiments (Tab. 6) show a strong correlation between the heatmap and verisimilitude score. So we employ several convolution and FFN layers to get the heatmap score $S_{map}$ from the predicted heatmap. The final score S is derived by calibrating $S_{token}$ and $S_{map}$:\n$S = Calib(S_{token}, S_{map})$. In our implementation, we explored various calibration functions, such as weighted summation and dynamic fusion. They exhibited similar performance."}, {"title": "3.3.2. CoT-Driven Explainable System", "content": "In addition to generating the implausibility region heatmap and verisimilitude score, it is essential to analyze and explain these outputs for AIGC system improvement. We introduce the CoT-Driven Explainable Trinity Evaluator, depicted in Fig. 2. The LLM is guided through a Chain of Thought (CoT) prompting process in five steps:\n1. Image Description: The LLM describes the image, capturing key elements in natural language.\nThe Laplace distribution is $f_X(x) = \\frac{1}{2\\lambda} exp(-\\frac{|x-\\mu|}{\\lambda})$, where $\\mu$ and $\\lambda$ are parameters. The standard deviation $\\sigma$ is $\\sigma = \\sqrt{2}\\lambda$."}, {"title": "3.4. Expl-AIGI-Eval Dataset", "content": "Existing AIGC image implausibility datasets, such as RichHF-18K [23] and AbHuman [10], include AIGC images and annotated defect regions but lack interpretability explanations. To address this, we introduce Expl-AIGI-Eval: Explainable AIGC Image Implausibility Evaluation. We provide detailed analyses and explanations for each implausibility region. The annotation process is conducted in three stages, shown in Fig. 3."}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Experimental Setup", "content": ""}, {"title": "4.1.1. Implementation Details", "content": "Our model is based on the InternVL-8B [8]. Our Expl-AIGI-Eval dataset is constructed by expanding upon two existing datasets, RichHF-18K and AbHuman, by adding explanatory and analytical annotations. We follow the original train-validation split. We use Deepspeed to finetune the MLLM, with a learning rate of $3 \\times 10^{-4}$, a warmup ratio of"}, {"title": "4.1.2. Evaluation Metrics", "content": "We follow the method in [23] for evaluation. We report the MSE for all samples, including images with no problematic region. For images with implausibility regions, we provide evaluation metrics for the saliency heatmaps, such as KLD, AUC-Judd, SIM, and CC, as referenced in [5]. For the score prediction tasks, we report the Pearson linear correlation coefficient (PLCC) and Spearman rank correlation coefficient (SRCC) [5]. Additionally, since the RAHF model [23] is not open source and its training is complex, we can only provide the results reported in the RichHF paper and cannot test the actual performance of the RAHF model."}, {"title": "4.2. Comparison with State-of-the-art", "content": "We evaluate our method against the RichHF-18K benchmark [23], as presented in Tab. 1. Conventional vision models typically lack physical common sense and struggle with high-level semantic understanding, complicating their"}, {"title": "4.3. Zero-shot Domain Generalization Results", "content": "Tab. 4 presents zero-shot cross-domain generalization experiments. We train on either the RichHF-18K or Ab-Human datasets and perform zero-shot inference on the other. Cross-domain tasks [13, 48], common in real-world applications [46, 47], test a model's fundamental world knowledge beyond domain-specific fitting [12]. As shown, smaller models suffer significant accuracy drops, losing defect assessment ability. In contrast, our model excels, leveraging MLLMs' strong common-sense knowledge and zero-shot capabilities."}, {"title": "4.4. Ablation Study", "content": "Ablation Study for Adaptive Hierarchical Implausibility Mapper. As shown in Tab. 5, Exp. (a) uses only a special global map token and global image features for implausibility prediction, resulting in poor accuracy due to lost"}, {"title": "4.5. Visualization Analysis", "content": "Evaluation Results of HEIE: As shown in Fig. 4, by using CoT, HEIE implements comprehensive and explainable AIGC image implausibility evaluation. Our model does not produce false positive predictions for defect-free images.\nComparison with Baseline: As Fig. 5 illustrates, predicting image implausibility heatmaps is challenging, and baselines perform poorly. Our model, however, shows excellent predictive performance. As well, we cannot obtain RAHF's prediction results due to the unavailability of RAHF's open model.\nResults of Hierarchical Implausibility Mapper: As shown in Fig. 6, global heatmaps tend to predict obvious, coarse-grained regions, while local heatmaps focus on subtler, finer details. Their adaptive combination yields the final heatmap."}, {"title": "5. Conclusion", "content": "In summary, we introduce the CoT-Driven Explainable Trinity Evaluator, which guides LLM to generate the heatmap, score, and explanation for image implausibility. We propose an Adaptive Hierarchical Implausibility Mapper capable of enhancing the prediction of subtle implausibilities. Additionally, we present the Expl-AIGI-Eval dataset, which offers high-quality, explainable implausibility annotations. Our work facilitates users in under-"}]}