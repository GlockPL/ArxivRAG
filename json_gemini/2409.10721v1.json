{"title": "A Missing Data Imputation GAN for Character Sprite Generation", "authors": ["Fl\u00e1vio Coutinho", "Luiz Chaimowicz"], "abstract": "Creating and updating pixel art character sprites with many frames spanning different animations and poses takes time and can quickly become repetitive. However, that can be partially automated to allow artists to focus on more creative tasks. In this work, we concentrate on creating pixel art character sprites in a target pose from images of them facing other three directions. We present a novel approach to character generation by framing the problem as a missing data imputation task. Our proposed generative adversarial networks model receives the images of a character in all available domains and produces the image of the missing pose. We evaluated our approach in the scenarios with one, two, and three missing images, achieving similar or better results to the state-of-the-art when more images are available. We also evaluate the impact of the proposed changes to the base architecture.", "sections": [{"title": "1 INTRODUCTION", "content": "Asset creation is a vital part of the game development process, and it usually takes up a large portion of the project schedule. In particular, the task of character design is seldom executed in a forward-only way, typically involving a lot of going back and forth [24]. In pixel art games, in which the color of each pixel is thoughtfully picked, even small changes to a character might require updating many sprites, especially if characters can face multiple directions and contain different animation sequences spanning many frames [30].\nDespite the character creation process requiring high creativity and being an established and well-suited responsibility for artists, some involved tasks can become repetitive. For instance, creating normal maps [20] from colored sprites, designing every animation frame [6], or propagating changes to the many sprites of a character. In that context, recent Procedural Content Generation techniques can help streamline the pipeline, particularly those involving Machine Learning (PCGML). Different works approached character generation through PCGML techniques using Variational Autoencoders (VAEs) [18, 23], Generative Adversarial Networks (GANS) [3, 6, 8, 12, 25], and Convolutional Neural Networks (CNNs) [26], and all of them posed their problems as an image to image translation task that generates an image given another (e.g., a normal map from a shaded character). However, if more information is available to the model, it can be leveraged to potentially generate better images.\nIn this work, we tackle the problem of generating a character sprite in a target pose as a missing data imputation task, using all the images of the character available in other poses. In particular, we propose a model that uses images of pixel art characters in source poses (e.g., facing left, right, back) to impute a missing target direction (e.g., facing front). Figure 1 illustrates our approach.\nWe propose a generative adversarial network model based on the CollaGAN [16] architecture, with changes to the generator topology and the training procedure. Compared to the baselines using the metrics Frech\u00e9t Inception Distance (FID) [11] and L1 distance, the images produced by our model are similar or better than the state-of-the-art. When fewer images are available, the model still produces feasible images, but with less quality. In an ablation study, we show how each of the proposed changes to the original CollaGAN influenced the improved results we achieved.\nThus, our main contributions in this work are:"}, {"title": "BACKGROUND", "content": "In this section we describe some concepts related to generative adversarial networks and then present how such models can approach the image-to-image translation problem."}, {"title": "2.1 Generative Adversarial Networks", "content": "Goodfellow et al. [10] introduced the concept of generative adversarial networks (GANs) as a framework for generating content through an adversarial training process. It consists of two models playing different roles in a minimax game: a generator G that evolves to create new content similar to the examples seen during training and a discriminator D that learns to distinguish between real and generated (fake) examples. If an optimal state is reached, G captures the distribution of the training data and can produce new samples that are indistinguishable from the real ones. At the same time, D cannot tell whether an observation is real or fake.\nThe training algorithm traverses the set of examples for a number of epochs. At each step, G receives a noise prior z and produces new examples, while D is called once to discriminate a minibatch of generated samples and a second time with real ones. The discriminator's loss function LD is the mean value (of all examples) of (the log of) the probability of incorrectly labeling an input example x as fake and (the log of) the probability of incorrectly labeling a fake example G(z) as real. D updates its weights ascending the stochastic gradient of the following value function, while G updates descending it:\n$\\min_{G}\\max_{D} L(G, D) = E_{x} [log D(x)] + E_{z} [log (1 \u2013 D(G(z)))]$\nIn this work, we propose a GAN model to generate pixel art character sprites. In particular, it generates a character in a target pose given input images of it facing other source directions. Because it uses multiple images as input to generate a missing one, we approach the problem as a missing data imputation task. However, it can also be regarded as an image-to-image translation problem with multiple images as input and the target as the missing domain.\nNext, we define the image-to-image translation task and present some of the proposed deep generative architectures using GANS."}, {"title": "2.2 Image-to-Image Translation", "content": "Pang et al. [21] define image-to-image translation as the process of converting an input image xa in a source domain a to a target b while keeping some intrinsic content from a and transferring it to the extrinsic style of b. The meaning of a domain, style and content differ according to the task. To illustrate, if we want to create a cartoon version (domain b) from pictures of faces (domain a), we are translating faces xa to b, keeping the person's identity (intrinsic content) but using cartoonish techniques (extrinsic style). Different problems have been approached as image-to-image translation using deep generative models (e.g., GANs, VAEs), such as image colorization [9, 15], semantic image synthesis [14, 25], style transfer [36], attribute manipulation [4, 5, 16], and pose transfer [6, 8, 12].\nThe diversity of the presented problems involves different characteristics of the task and the proposed solution. A first important property is the use of supervision (label/annotated examples) for training, which largely depends on the availability of such data. For instance, in a translation from grayscale to colored pictures, it is easy to have pixel-wise aligned examples, but that is not the case if we want to transform horses into zebras, as the cost of acquiring completely registered pairs of photos of horses and zebras in the same position in the same environment is impractical. Hence, when paired data is available for some task, we can use supervised training [14, 16], whereas when it is not, the algorithm needs to train in an unsupervised fashion [1, 4, 36].\nA second characteristic of the tasks is the number of domains involved in the translation and how the proposed architecture can deal with them. For instance, many problems consist of only two domains (e.g., grayscale to color, photo to painting, semantic labels to photographs). In contrast, others involve multiple (e.g., translating a neutral face to one smiling, angry, or crying). Hence, the proposed architectures can be two-domain [14, 36] or multi-domain, supporting the translation among all directions [4, 5, 16]. Additionally, the architectures for two-domain translation can generate images in a single direction [14] or in both [36, 37].\nAuthors have proposed architectures for tasks with different sets of characteristics. Here, we propose a model based on the Collaborative GAN (CollaGAN) to generate missing poses of pixel art characters. In our experiments, we compare the proposed architecture to baselines consisting of models based on Pix2Pix [8] and StarGAN [4].\nPix2Pix trains with supervision (paired images). It can translate images from one domain into another in a single direction. In contrast, StarGAN trains unsupervisedly but supports multiple domains with a single generator and discriminator pair. CollaGAN, in turn, requires supervision and is multi-domain, with the additional difference that it uses images from multiple domains as input."}, {"title": "3 RELATED WORK", "content": "As we investigate the generation of character sprites, we first describe some recent works that tackle the automatic creation of characters. Most also deal with pixel art imagery and use deep generative models. In sequence, we present some works related to the missing data imputation problem, which is how we frame the generation of missing character poses."}, {"title": "3.1 Sprite Generation", "content": "Some works propose generating characters in a target pose using a bone graph to indicate the desired positions of each body part. Hong et al. [12] approached that task with a multiple discriminator GAN (MDGAN). It translates images of a character (representing its shape and color) and a target bone-graph sprite into a target of that same character in the new pose. The model consists of a generator and two discriminators, one to determine if two images share the same color and shape, while the other tells whether a character's pose is correct according to some bone-graph sprite."}, {"title": "3.2 Missing Data Imputation", "content": "Data analysis can be drastically hindered when relevant parts of information are missing. That can happen for various reasons: data can be absent because it was never collected or produced, it might have been lost, or it might contain errors [35]. Researchers have proposed different missing data imputation techniques to replace absent data with plausible substitutions. The choice of such techniques depends on the data type, among other characteristics. It can be one or a mix of categorical [27, 35], sequential [17], and image [16, 27-29].\nInaugurating the use of deep learning-based techniques for missing data imputation, Yoon et al. [35] proposed a generalization of the original GAN to deal with imputing missing values, which they called Generative Adversarial Imputation Nets (GAIN). The generator receives three inputs: the sample with missing values, a mask indicating which values are present, and a random vector of the same dimension that introduces noise. As output, it produces a version of the sample with replaced values for those missing. The discriminator, in turn, tries to distinguish which of the categorical variables are imputed and which are from the original sample.\nThe imputation task becomes more challenging when the missing data are images due to the higher dimensionality. Some works approach the problem using GANs [16, 27-29]. An example is the View Imputation GAN (VIGAN) [27], that can generate missing values in a target domain by combining a modified CycleGAN [36] with a Denoising Autoencoder in a three-step training process. A shortcoming of VIGAN is that it performs bi-directional imputation between only two domains. When the task involves more domains, other architectures are better suited. The Multi-Modal GAN (MM-GAN) [28], CollaGAN [16], and ReMIC [29] can impute missing images among multiple domains and use the information of all available sources as input to the generator.\nMM-GAN's generator [28] has an equal number of inputs and outputs, receives samples with images missing in random domains, and outputs imputed values. The discriminator distinguishes between real and imputed same-size patches of a full sample comprising all domains. CollaGAN [16] works similarly and was proposed in the same year as MM-GAN. However, it produces an image of a single target domain. Its generator varies depending on the task, but it also receives the images in all available domains, concatenated with the index of the target domain spread spatially and through the channels dimension. Both architectures presented good results in their respective experiments. ReMIC [29] also takes the inputs from all available domains and generates the missing ones, like MM-GAN. However, unlike the other two, it disentangles the images and extracts a shared content encoding and a separate style encoding for each domain.\nAll multi-domain architectures that deal with missing image data imputation [16, 28, 29] were tested either with medical or natural images, but not with pixel art or other styles. In the next section, we present a modified architecture based on CollaGAN to generate missing pixel art characters."}, {"title": "4 ARCHITECTURE", "content": "We propose an architecture based on CollaGAN [16] to impute images of pixel art characters in a missing pose (target domain). Considering that there are domains N = {a, b, c, d}, one representing each pose. The architecture consists of a single generator and discriminator pair that creates an image xt of a character in the missing pose t using the available images from all of the other source S poses:\n$x_{t} = G(x_{S}, t)$, with $t \u2208 N, S = N - {t}$\nOur generator has one encoder branch to process the input for each domain, a single decoder branch with concatenated skip connections, and outputs an image in the missing domain. The discriminator distinguishes images as real or fake, as well as determines their domain through an auxiliary classifier output. Figure 2 shows the topology of both networks."}, {"title": "4.1 Objective Function", "content": "As usually done with GANs, we train both networks adversarially, but also with additional objectives. The generator's loss function has five terms: regressive, cycle consistency, structural similarity, adversarial, and domain classification. In turn, the discriminator trains with adversarial and domain classification objectives.\nTraining requires a forward and a backward pass. In the first step, a minibatch of paired images with a random missing domain t is fed to the generator G, which synthesizes an image corresponding to the missing t domain. For example, if S = {a, b, c} and t = d, the images xa, xb, xc are available and we want the model to generate xd as close as possible to the real xd:\n$\\hat{x_{d}} = G({x_{a}, x_{b}, x_{c}, x_{zero} }, d)$,\nin which xzero is a tensor filled with zeros.\nSubsequently, to ensure cycle consistency, the backward step comprises synthesizing |N| - 1 images with each domain in S = {a, b, c} as a target, using the generated xd instead of the real xd.\nThe outputs of this pass, in our example, would be:\n$x_{a|d} = G({x_{zero}, x_{b}, x_{c}, \\hat{x_{d}} }, a)$\n$x_{b|d} = G({x_{a}, x_{zero}, x_{c}, \\hat{x_{d}} }, b)$\n$x_{c|d} = G({x_{a}, x_{b}, x_{zero}, \\hat{x_{d}} }, c)$,\nand should reconstruct the original images xa, xb, and xc.\nA regressive loss term Lreg steers the generator towards using the information from the source domains to translate an image to the target, whereas a multiple cycle consistency loss Lmcyc leads it into encoding in xt enough information to allow cyclical reconstruction of the original inputs. Both losses are pixel-wise L1 distances between the generated and the real images:\n$L_{reg} = E_{x_{t}, x_{S}} [||x_{t} - \\hat{x_{t}}||_{1}]$\n$L_{mcyc} = E_{x_{t}, x_{S}} [||x_{S} - \\hat{x_{S|t}}||_{1}]$\n$S \\in S$\nBesides Lmcyc, an additional objective Lssim is used to improve the quality of the images generated in the backward pass. It uses the structural similarity index measure (SSIM) [34] to compose a loss term between the cyclically generated xs and the real source images xs. Its formulation is the same as in the CollaGAN paper and is omitted here for brevity.\nThe discriminator also uses the other two objectives for the generator: adversarial and domain classification. The adversarial loss uses the one from Least Squares GAN [19], which optimizes the square of the errors of the discriminator classification of real and fake images. The discriminator $L_{D}^{adv}$, and generator $L_{G}^{adv}$ adversarial losses are:\n$L_{D}^{adv} = E_{x_{t}} [(D_{adv}(x_{t}) \u2013 1)^{2}] + E_{x_{S|t}} [(D_{adv}(x_{S|t}))^{2}]$\n$L_{G}^{adv} = E_{x_{S|t}} [(D_{adv}(x_{S|t}) \u2013 1)^{2}]$\nThe domain classification objective leads the generator to synthesize images classified as having the intended target domain. For the generator, $L_{G}^{fake}$ considers only generated images, whereas for the discriminator, $L_{D}^{real}$ uses only real images. As a classification, they are calculated using cross entropy, given as:\n$L_{D}^{real} = E_{x_{t}} [-log(D_{dmn}(x_{t}))]$\n$L_{G}^{dmn} = E_{x_{t}} [-log(D_{dmn}(\\hat{x_{t}}))]$\nTo summarize, the full objectives of the generator LG and the discriminator LD are sums weighted by a scalars given as:\n$L_{G} = L_{G}^{adv} + \\lambda_{reg}L_{reg} + \\lambda_{mcyc}L_{mcyc} + \\lambda_{ssim} L_{ssim} + \\lambda_{dmn}L_{G}^{dmn}$\n$L_{D} = L_{D}^{adv} + \\lambda_{dmn}L_{D}^{real}$"}, {"title": "4.2 Generator", "content": "The generator has four encoder branches, each receiving a source image from a particular domain and a channelized and spatially spread one-hot encoded label of the target domain. There are four downsampling blocks for each branch and a bottleneck layer that concatenates the activation maps from all encoder branches and further processes it. The data is then passed onto a single decoder composed of four upsampling blocks. There are skip connections from"}, {"title": "4.3 Discriminator", "content": "The discriminator receives a batch of images and outputs values Dadv that should be one for real images and zero for the generated ones. In addition, it classifies the domain of the image, yielding probabilities Ddmn of images having each domain.\nThe network topology is the same as in the original, with 6 downsampling blocks, each consisting of a convolution that halves the resolution while increasing the number of channels, with a leaky ReLU activation. The last block also contains a dropout layer. Following it, two parallel convolutions represent the Dadv and the Ddmn outputs, with linear and softmax activations, respectively."}, {"title": "4.4 Training Procedure", "content": "At each training step, we select a batch of paired images xs with random target domains t. The generator receives the batch of (xs, t) and creates the missing t, in the forward pass. Next, xt is used in place of xt to create a number of new batches equal to |N| - 1, in which each domain in S becomes the target, in the backward (or cyclical) pass. The generator then creates xs|t, \u2200s \u2208 S that must be as close as possible to the original xs, \u2200s \u2208 S.\nThe CollaGAN architecture authors observed that images are much worse as the number of available sources decreases. However, it is common to have use cases in which more than one domain is missing. Hence, they proposed a batch selection strategy called input dropout, in which the model trains with one or more missing domains. For instance, for |N| = 4 and t = d, when a batch (xs, t) is selected using the input dropout strategy, xs can have zero, one or two withdrawn images and be one of the following:\n$x_{S} = {x_{a}, x_{b}, x_{c}}$\n$x_{S} = {x_{zero}, x_{a}, x_{c}}$\n$x_{S} = {x_{a}, x_{zero}, x_{c}}$\n$x_{S} = {x_{a}, x_{b}, x_{zero}}$\n$x_{S} = {x_{zero}, x_{zero}, x_{c}}$\n$x_{S} = {x_{zero}, x_{b}, x_{zero}}$\n$x_{S} = {x_{a}, x_{zero}, x_{zero}}$\nIn the original CollaGAN, the number of images to be dropped out is chosen uniformly, leaving a 33% chance of having the full source domain set. That strategy improved the results in our task too. However, we observed that a more conservative approach in which the model trains more frequently dropping out few images yields even better results in the scenario of having fewer available images. We adopted chances of 10%, 30%, and 60% to have two, one, and zero images dropped out. Figure 3 (left) compares the three strategies (no dropout, original dropout, conservative dropout) with different numbers of missing images.\nAnother change we made to the training procedure relates to the backward generation pass. When the cycled images xs|t, \u2200s \u2208 S (e.g., xa|d, xb|d, and xc|d) are generated in the original implementation, the image xt generated in the forward pass replaces not only the original target image xt, but also all images that have been dropped out due to the batch selection strategy. We experimented with having xt replace only xt and observed better results.\nTo illustrate the difference, considering a batch with t = d and the domain c dropped out, the backward generated images xs|t, \u2200s \u2208 S for the original (left) and our implementation (right) would be as shown next. We highlighted the differences in color:\n$x_{a|d} = G({x_{zero}, x_{b}, \\hat{x_{d}}, \\hat{x_{d}}}, a)$\n$x_{b|d} = G({x_{a}, x_{zero}, \\hat{x_{d}}, \\hat{x_{d}}}, b)$\n$x_{c|d} = G({x_{a}, x_{b}, x_{zero}, \\hat{x_{d}}}, c)$\n$x_{a|d} = G({x_{zero}, x_{b}, x_{zero}, \\hat{x_{d}}}, a)$\n$x_{b|d} = G({x_{a}, x_{zero}, x_{zero}, \\hat{x_{d}}}, b)$\n$x_{c|d} = G({x_{a}, x_{b}, x_{zero}, \\hat{x_{d}}}, c)$\nFigure 3 (right) compares the generated images when the model trains using the original replacement procedure for the dropped-out images versus our version where only the forward target image is replaced by the one generated in the forward pass. We can note that with the original procedure, the generator produces images with artifacts from domains other than the target one, mostly noticeable through the wrong number of eyes in the examples.\nRegarding the number of trainable parameters, the generator contains 104,887,616 values, and the discriminator has 44,726,272.\nAfter training, the generator takes ~116 ms to produce an image using a GeForce GTX 1050 GPU."}, {"title": "5 METHODOLOGY", "content": "We start by presenting the datasets used in the experiments to propose and evaluate models for translating pixel art characters in different poses. Next, we describe the metrics L\u2081 and FID used to analyze the quality of the generated images using each model. Finally, we conclude the section by presenting the baseline models used in the experiments."}, {"title": "5.1 Dataset", "content": "Unlike tasks that are more commonly tackled in Computer Vision research, we found only one character sprite dataset readily available: TINY HERO\u00b9, which contains 912 paired images of characters facing the back, left, front, and right directions. To increase the number of training examples, we scraped character sprite sheets from different sources from the web, splitting them into individual character sprites, and generated characters modularly by assembling various parts. The dataset contains 14,202 paired images of characters in four directions spanning different art styles. They primarily comprise humanoid characters of different sizes and art styles, but also a few sprites of animals, vehicles, and monsters. Figure 4 shows examples depicting the high variability of the samples.\nImages from each source had different character sizes, so the smaller ones were transparency-padded to the largest size, 64\u00d764. We also created an alpha channel with the character shape for the images that lacked one. The training set contains 12,074 examples, and the test set contains 2,128 examples (85% split). During training, we applied hue rotation to each character as data augmentation."}, {"title": "5.2 Evaluation Metrics", "content": "The evaluation of generative models is an active research problem with different metrics proposed over the recent years [2]. We evaluate the quality of a model by how close the generated images are to their ground truth. However, a qualitative analysis is important as the metrics do not always converge.\nHence, we analyze the results qualitatively through visual inspection and quantitatively using the L\u2081 distance to the target images and the Fr\u00e9chet Inception Distance (FID) [11]. The L\u2081 distance measures the absolute difference between the colors of pixels of two image sets (the generated and target). In turn, FID uses the Inception v3 network (proposed for image classification) to get the distance between the feature vectors of the two image sets [31]. As"}, {"title": "5.3 Baseline Models", "content": "We compare our model with two other architectures proposed for the image-to-image translation task: Pix2Pix [14] and StarGAN [4]. Pix2Pix. We trained a modified version of the architecture proposed in [8] for generating pixel art characters in a target pose given an image of it in a source one. Differently from the referenced work, we use 12 such models to support translation from and to all four poses: back, left, right, and front, excluding models from and to the same direction. Each generator has 29,307,844 trainable variables, so the model collection contains 351,694,128 parameters.\nStarGAN. We trained a StarGAN-based model to perform multi-domain translation using a single generator and discriminator pair. The generator typically receives the source image and a label indicating the target direction. Still, we found that providing a label of the source domain increases the quality of the generated characters. In turn, the original critic receives only the image to be evaluated, but we got better results by sending the source image too (before translation), which makes it perform a conditional discrimination. In that case, the network indicates whether the provided image is real/fake considering that it is a translation of the source image. For a fair comparison, we train the model using supervision (the original trains without paired images). The generator contains 134,448,128 parameters."}, {"title": "6 EXPERIMENTS", "content": "The model trained with the pixel art characters dataset for 240,000 generator update steps in minibatches of 4 examples, which is equivalent to ~80 epochs. It took 01:20h to train using a GeForce GTX 1050 GPU. We used early stopping to select the model that had the best metrics on its test set instead of getting the one in the end to prevent overfitting. At every 1,000 update steps, we evaluate the model and select the one with the lowest (best) L1"}, {"title": "6.1 Missing Image Imputation", "content": "We trained our proposed model using conservative input dropout and the forward-only replacer strategy. Tables 1 and 2 show the values of FID and L\u2081 for our proposed model and the baselines, with the rows representing the target pose and the columns displaying the metrics for the baselines Pix2Pix and StarGAN, averaged considering the translation from the other source domains and CollaGAN with the three other domains as input.\nRegarding FID, CollaGAN-3 had the lowest (best) values of the evaluated models in all target poses and, hence, on average too: 1.508 (CollaGAN-3) versus 2.288 (StarGAN) and 4.091 (Pix2Pix). Also, the L\u2081 distance for CollaGAN was the lowest (best), with the averages: 0.04078 (CollaGAN-3), 0.05273 (Pix2Pix), and 0.06577 (StarGAN). Next, we visually analyze the generated images."}, {"title": "6.2 Generating from Fewer Domains", "content": "Even though we propose a model to impute a single missing domain, we also evaluate it in scenarios where it receives two (CollaGAN-2) or only one image (CollaGAN-1). The metrics' values are averaged among all targets and all available sources for each model and scenario (i.e., CollaGAN-3, 2, and 1).\nTable 3 compares the proposed model in those situations. We can observe that both FID and L\u2081 metrics progressively improve as the number of available domains increases, with CollaGAN-2 still having better L\u2081 than Pix2Pix and StarGAN."}, {"title": "6.3 Input Dropout", "content": "We evaluated the impact of different batch selection strategies on presenting examples to the proposed model: Should it always see the three available domains, or should they sometimes be omitted?\nWe investigated always showing all available domains (none), the original input dropout strategy proposed in [16], a curriculum learning approach suggested by [28], and our proposed conservative tactic. The original approach has an equal chance of presenting three, two, or a single image in a training step. The curriculum learning approach starts training with easier tasks (using three images) and progressively makes it harder (using a single input) until half of the training, then it randomly chooses between the number of domains to drop out for the second part. Lastly, the conservative approach randomly selects the number of images to drop, but with higher probabilities to keep more images: 60% with 3 images, 30% with 2, and 10% with a single image.\nTable 4 presents the results from the models trained with the different input dropout strategies (columns) in the scenarios of having three, two, or one available image as input (rows). We can observe that using any input dropout yields better results than always showing all domains (none). Compared to the original and curriculum learning strategies, our proposed conservative tactic has better FID and L\u2081 metrics on the average of the three scenarios. In particular, regarding FID, the model trained with the conservative input dropout worsens its performance less drastically with the decrease of input domains. Regarding L\u2081, its metrics are better than the other models when two and three images are available."}, {"title": "6.4 Ablation Study", "content": "To understand the impact of our changes to the original CollaGAN architecture, we trained and evaluated models that progressively added each modification. Table 5 shows the FID and L\u2081 values of the generated images averaged over all domains and among the scenarios of the model receiving three, two, and one input domains. The rows show the results of each modification cumulatively: the first one is the original CollaGAN model without any of our proposed changes, the second introduces the first modification, the third uses two changes, and the last includes all three (our final model).\nThe original model had 6,565,712 trainable variables, but with the increased capacity, there are 104,887,616 parameters. That change alone improved L\u2081 but worsened FID. The replacement strategy of substituting only the original target with the image generated in the forward step improves both metrics' results. Lastly, training with the proposed conservative input dropout further enhances the results, with FID and L\u2081 values that are 46.7% and 14.53% better than the original architecture."}, {"title": "7 FINAL REMARKS", "content": "We posed the task of generating pixel art characters as a missing data imputation problem and approached it using a deep generative model. It is based on the CollaGAN architecture, from which we proposed changes involving a capacity increase, a conservative input dropout strategy, and a different replacement tactic during the backward step of the training procedure. The experiments showed that all of the changes contributed to achieving better results.\nCompared to the baseline models, our approach produces images with similar or better quality when using three domains as input. The model can still produce feasible images in scenarios with fewer available images but with increasingly lower quality.\nIn future work, we propose the study of other missing image imputation architectures to the same task tackled here, such as ReMIC [29] and MM-GAN [28]. Differently from CollaGAN, both methods can receive and generate images in any number of domains. Another line of investigation is to approach the task with architectures that disentangle the source images into content and style codes [13] and also latent diffusion models [22]. An interesting outcome of such architectures is their multi-modality nature, in that they can generate different suggestions for the same input."}]}