{"title": "DELMAN: Dynamic Defense Against Large Language Model Jailbreaking with Model Editing", "authors": ["Yi Wang", "Fenghua Weng", "Sibei Yang", "Zhan Qin", "Minlie Huang", "Wenjie Wang"], "abstract": "Large Language Models (LLMs) are widely applied in decision making, but their deployment is threatened by jailbreak attacks, where adversarial users manipulate model behavior to bypass safety measures. Existing defense mechanisms, such as safety fine-tuning and model editing, either require extensive parameter modifications or lack precision, leading to performance degradation on general tasks, which is unsuitable to post-deployment safety alignment. To address these challenges, we propose DELMAN (Dynamic Editing for LLMS JAilbreak Defense), a novel approach leveraging direct model editing for precise, dynamic protection against jailbreak attacks. DELMAN directly updates a minimal set of relevant parameters to neutralize harmful behaviors while preserving the model's utility. To avoid triggering a safe response in benign context, we incorporate KL-divergence regularization to ensure updated model remains consistent with original model when processing benign queries. Experimental results demonstrate that DELMAN outperforms baseline methods in mitigating jailbreak attacks while preserving the model's utility, and adapts seamlessly to new attack instances, providing a practical and efficient solution for post-deployment model protection.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) play a significant role in decision-making, underscoring the importance of aligning LLMs with safety standards and human values. To ensure that generated content aligns with human values and avoids harmful information, various safety alignment methods are employed throughout the model production pipeline, including pre-training by model providers, task-specific adaptations by secondary developers, and deployment for user interactions (illustrated in the upper part of Figure 1). Among these three phases, the deployment stage poses the greatest safety risk, as adversarial users can launch \u201cjailbreak attacks\" by crafting prompts or optimized suffixes to bypass safety measures (Zou et al., 2023; Liu et al., 2023; Zhou et al., 2024b; Chao et al., 2023). \nConsidering that large-scale modifications to a model's architecture or parameters become impractical once deployed, and adversarial users represent only a minority, which making it infeasible to construct sufficient labeled datasets for fine-tuning, safety alignment in the deployment phase must meet three essential requirements: (1) Minimal model modifications to ensure efficiency; (2) Targeted defenses that address adversarial queries without compromising regular user interactions; (3) Dynamic adaptability to continuously counter emerging jailbreak examples without requiring extensive retraining. Existing defense mechanisms such as safety fine-tuning (Wang et al., 2022; Ganguli et al., 2022; Xu et al., 2024a) and model decoder modification (Wang et al., 2024; Zhao et al., 2024) are unsuitable due to their extensive changes to model architecture or parameters. Model editing, originally designed for knowledge correction (Zhu et al., 2020; Lee et al., 2022; De Cao et al., 2021; Mitchell et al., 2021; Meng et al., 2022a,b), has also been explored as a defense against jailbreak attacks. Approaches like DINM and LED (Wang et al., 2024; Zhao et al., 2024) rely on indirect model editing that fine-tunes specific layers, but they often lack precision in targeting harmful regions and risk degrading overall model performance. \nA dynamic jailbreak defense mechanism is essential, one that is timely, precise, and minimal in required modifications to the deployed model while effectively countering adversarial attacks. To achieve this, our key motivation is to utilize direct editing that focuses on minimal parameter updates, minimizing interference with the model's overall performance. Specifically, in this work, we introduce DELMAN (Dynamic Editing for LLMs"}, {"title": "Methods", "content": "The idea behind DELMAN is to mitigate a model's harmful behavior by directly modifying the weights of specific layers, establishing a direct association between harmful tokens and safe responses. Factual knowledge is stored in the MLP of specific layer l (Meng et al., 2022a). The MLP acts as two-layer key-value memories where the neurons of the first layer $W_{gate}$ generate a key k, with which the $W_{down}$ retrieves an associated value v. The MLP layer can be expressed as:\n$k = \\sigma(W_{gate} (a^{l} + h^{l-1})), v = W_{down}k$,(1)\nwhere $a^{l}$ is the attention output at layer l, $h^{l-1}$ is the hidden state of previous layer l \u2212 1, \u03c3 is the activation function and y is the layernorm. DELMAN aims to edit $W_{down}$ to rebuild the connection between harmful-token-related key representation k* and safe-response-related representation v*. As illustrated in Figure 2, DELMAN achieves this through five key steps. In the following of this section, we first outline the process of identifying k* through harmful token extraction and random sequence generation. Then, we describe how to estimate the v* to establish its connection to k* that can generate safe responses. Last, we explain how to update the $W_{down}$, the MLP of specific layer l* (directly adopted from MEMIT (Meng et al., 2022b)) accordingly."}, {"title": "Identify Key Representation k*", "content": "To identify the harmful-token-related key representation k*, we first extract the harmful tokens from input queries that may trigger unsafe responses. To improve the stability of model editing on a specific harmful token, we generate multiple sequences that incorporate these tokens in varied contexts. Following that, we perform forward propagation for each sequence through the language model f and use the internal representations at layer l* as harmful-token-related key representation k*.\nHarmful tokens extraction. We automate this process using GPT-4 as a token extraction assistant, which analyzes each query to pinpoint tokens likely to trigger harmful outputs. Formally, for each query in a set of harmful queries q \u2208 Qharm, we extract a harmful token or phrase t, forming a set of consecutive harmful tokens Th = {t1, t2,..., tn}, which can be defined as: $T_{h}$ = Extraction(Qharm)."}, {"title": "Estimate v* of Safe Response Ytarget", "content": "To establish the connection to k* that determines the model's likelihood of generating safe response, we optimize v* with the following loss function:\n$L_{safe} = -log P_{f(m*:=v)} [Y_{target} | q]$,(3)\nwhere m refers to the MLP output activation at layer l* and position i, and f(m* := v) indicates the model f with the specified activation replaced by vector v, and q represents the harmful query in Qharm introduced in Section 3.1.\nTo prevent unintended triggers of the safe response in ordinary contexts where the harmful token might appear benignly, we want the updated model to remain consistent with its original distribution when asked a benign query, thus avoiding the over-activation of the safe response in normal conversation. We use KL-divergence to achieve this, which can be formulated as:\n$L_{utility} = KL(P_{f(m* :=v)} [\u00b7 | q_{u}] || P_{f} [ \u00b7 q_{u}])$,(4)"}, {"title": "Weight Update of W down", "content": "After obtaining the pair (k*, v*), we incorporate this new key-value association into the MLP at layer l* by editing the matrix $W_{down}$ via solving the least-squares problem (Belinkov and Glass, 2019):\n$\\underset{W^{l*}_{down}}{min} || W^{l*}_{down} KD-VD||^{2}$(6)\nsubject to $W_{down}k* = v*$.(7)\nHere, $K_{D} = [k_{1}, k_{2}^{*}, . . .]$ is a matrix of key vectors, and $V_{D} = [v_{1}, v_{2}^{*}, . . .]$ is the matrix of their corresponding value vectors. Eq.6 can be solved with this closed form solution:\n$W_{down} = W_{down} + RD KDT (C^* + KD KDT)^{-1}$,(8)\nwhere $C^{l*} = KKT$ denotes the covariance matrix of K, which is the key of original knowledge pair K and V at layer l*, pre-cached from Wikipedia dataset. The term $R_{D}$ is defined as\n$RD = V_{D}- W^{l*}_{down}K_{D}$,(9)\nwhich measures the residual error between the desired values $V_{D}$ and the model's current outputs $W^{l*}_{down}K_{D}$ at target layer l*.\nPractical scheme. In practice, instead of updating a single layer l*, we spread the updates over a range of crucial layers R = {l1, l2, ..., L} to limit the magnitude of parameter changes in a single layer, which results for better robustness (Zhu et al., 2020). For example, we directly adopt the finding in MEMIT and use the 7th and 8th layer as the crucial layers for Llama2 and Vicuna. The v* and the residual in Eq.10 is only estimated for the last crucial layer L. This residual is then distributed to the lower layer with a factor L \u2212 l + 1, which can be expressed as:\n$R_{D} = \\frac{VD-W^{l*}_{down}KD}{L-l+1}$.(10)\nBy ensuring smaller changes in lower layers, DELMAN can promote stability and avoid abrupt changes in a single layer. A detailed description of the algorithm is provided in Appendix A."}, {"title": "Experiments", "content": "We begin this section by detailing the configuration of our experiments, including evaluated datasets, jailbreak attacks, and models, along with compared baselines and evaluation metrics. Then, we present the effectiveness of DELMAN in terms of defense performance and utility preservation. Next, we demonstrate the impact of single-behavior edit of DELMAN, highlighting its transferability across datasets and harmful behaviors. Last, we use a consecutive edit case study to illustrate that each edit, once applied, does not interfere with the edit established in previous phases."}, {"title": "Experiment Setup", "content": "Datasets. To ensure a comprehensive evaluation of defense effectiveness against jailbreak attacks, we use the HARMBENCH (Mazeika et al., 2024) dataset for editing and evaluate across multiple testing benchmarks: HARMBENCH (HB), ADVBENCH (AB) (Zou et al., 2023), JAILBREAKBENCH (JBB) (Chao et al., 2024), and MALICIOUSINSTRUCT (MI) (Huang et al., 2023). To comprehensively assess potential side effects of model editing on LLMs' general utility, we evaluate DELMAN using MT-bench (Zheng et al., 2023) and seven downstream tasks: Closed-domain QA, Dialogue, Named entity recognition (NER), Natural language inference (NLI), Reasoning, Sentiment analysis and Summarization. The detail of the datasets and their evaluation metrics are presented in the appendix B.3.\nEvaluated jailbreak attacks and models. We use three leading jailbreak attack methods to demonstrate the defense performance of DELMAN: two optimization based attack GCG (Zou et al., 2023), AutoDAN (Liu et al., 2023) that search for adversarial suffix, and prompt-based attack PAIR that rewrite the prompt to adversarial form (Chao et al., 2023). Our evaluation focuses on a strong aligned model, Llama-2-7B-chat (Touvron et al., 2023), and a weak aligned model Vicuna-7B-v1.5 (Zheng et al., 2023). A detailed description of attack setup is provided in Appendix B.1.\nBaselines and evaluation metrics. We consider three different defense methods as baselines, SafeDecoding (Xu et al., 2024a) an decoder modification method, Safety fine-tuning with LoRA (Hu et al., 2021), as well as LED (Zhao et al., 2024), an indirect editing method. For all baseline methods, we follow their original papers' suggested hyperparameter settings. A detailed description of baseline setup is provided in Appendix B.2. We employ HARMBENCH classifier (Mazeika et al., 2024) to detect the harmful content in model responses. The primary evaluation metric is the Attack Success Rate (ASR), which measures the proportion of successful attacks over all tested examples. For a dataset Qharm containing harmful queries q, ASR is formally defined as:\nASR(Qharm) = $\\frac{1}{\\|Q_{harm}\\|} \\sum_{q \\in Q_{harm}} \\mathbb{I}(f(q))$,(11)\nwhere II is the indicator function that returns 1 for successful attacks and O otherwise."}, {"title": "Effectiveness of DELMAN", "content": "Safety evaluation. Figure 3 compares DELMAN with baselines and the Original Model under three jailbreak attacks across four datasets. DELMAN edits the model according to HARMBENCH (HB) data, and evaluates the edited model performance on AB, JBB and MI, showing its generalization ability on unseen datasets. The exact value of reduced ASR is relegated to Appendix D.1. We observe several key findings. First, compared to the original model, DELMAN significantly reduces the ASR across all datasets (HB, AB, JBB, and MI) and against different attack types, including optimized suffix attacks (GCG, AutoDAN) and promptrewriting attacks (PAIR), and in many cases DELMAN is able to completely mitigate jailbreak attacks, reducing ASR to 0. Second, among baselines, LED also demonstrates some defensive capability, even surpassing DELMAN in certain scenarios within HB. However, LED struggles on unseen datasets, indicating a lack of generalization. In contrast, LoRA and SafeDecoding perform worse, failing to bring ASR down to an acceptable level. Last, since Llama2 already exhibits strong safety alignment, PAIR has little effect on it. As a result, the improvements from DELMAN in this case are less pronounced.\nUtility evaluation. We summarizes the performance of DELMAN and baselines on generalpurpose tasks with Vicuna-7B and Llama2-7B on MT-Bench, along with seven downstream tasks to comprehensively evaluate the model's utility in Table 1. The highest utility scores are highlighted in bold (except LoRA which has the highest ASR), and scores that exceed those of the Original Model"}, {"title": "Edit According to Harmful Behavior", "content": "In this section, we investigate the effect of DELMAN edit on individual harmful behavior and its impact on defending other unedited behavior.\nEffectiveness of DELMAN on each harmful behavior. Figure 5 compares the performance of DELMAN across individual HARMBENCH behavior, including chemical and biological (CheBio), cybercrime intrusion (CybIn), harassment and bullying (HaraBull), general harmful (GenHarm), il-"}, {"title": "Understanding the DELMAN Transferability Across Datasets and Behaviors", "content": "DELMAN establishes a direct link between harmful tokens and specific responses to modify the model parameters effectively. To explain why modifying"}, {"title": "Consecutive Edits with DELMAN", "content": "In real-world deployment, adversarial parties may repeatedly attempt to jailbreak the model, making it crucial for dynamic and consecutive edits to maintain the effects of earlier modifications without interference. To evaluate the robustness of DELMAN under consecutive edits, we conduct an experiment where edits are applied sequentially across different harmful behavior categories. Specifically, we select one category each from the HB, AB, JBB, and MI datasets and perform DELMAN edits in succession. After each edit, we evaluate:"}, {"title": "Conclusion", "content": "In this work, we introduce DELMAN, a novel defense mechanism that directly edits model parameters to neutralize harmful behaviors by forming explicit connections. DELMAN brings minimal parameter modification, preserving the utility on normal tasks and is capable of dynamic and consecutive edits. Extensive experiments demonstrate superiority over existing baselines in terms of defense performance and utility preservation, as well as strong transferability. Overall, DELMAN demonstrates how token-level editing method can effectively enhance model safety while maintaining performance. In the future, it would be interesting to investigate more efficient methods for harmful token identification, for instane, using a minimal set of tokens (e.g., 20-30 Tokens) to effectively cover the majority of harmful scenarios, which would significantly reduce computational costs. Additionally, exploring the application of DELMAN to domain-specific LLMs and VLMs would validate its generalizability across different domains and modalities."}, {"title": "Limitations", "content": "The limitations of our study are as follows:\n1.  Our evaluations are currently restricted to general-purpose LLMs, leaving the applicability to domain-specialized models (e.g., medical or legal LLMs) and larger-scale models (e.g., 70B parameters) unexplored. Further investigation is required to assess its defense capabilities against domain-specific jailbreak attacks and potential impacts on domain expertise after editing.\n2.  DELMAN relies on GPT-4 for harmful token extraction and context generation, which introduces dependency on external models and potential cost barriers.\n3.  The stability of consecutive edits, though preliminarily validated, needs deeper analysis to assess potential performance drift over extended deployment."}, {"title": "Ethics Statement", "content": "DELMAN directly edits parameters linked to harmful tokens, raising concerns about potential misapplication or unintended bias introduction. We advocate for responsible deployment where practitioners thoroughly validate parameter modifications and strictly limit edits to well-defined harmful content categories. While our approach offers fine-grained, post-deployment protection, it should be viewed as one component within a comprehensive safety framework that includes human oversight and established moderation systems to ensure ethical and harm-free interactions."}, {"title": "Algorithm", "content": "Algorithm 1 demonstrates the detailed procedure of DELMAN."}, {"title": "Detail Experiment Setting", "content": ""}, {"title": "Attack Setup", "content": "For GCG (Zou et al., 2023), AutoDAN (Liu et al., 2023), and PAIR (Chao et al., 2023), we leverage multiple harmful query datasets: the complete HARMBENCH (200 queries), a subset of ADV-BENCH (100 queries), the complete JAILBREAK-BENCH (100 queries), and the complete MALICIOUSINSTRUCT (100 queries). For each model under test, we generate specific attack prompts using every query from these datasets, following the original hyperparameter configurations as described in their respective papers."}, {"title": "Baseline Setup", "content": ""}, {"title": "Original Model", "content": "Recent work (Inan et al., 2023; Xu et al., 2024b) has demonstrated that system prompts and prompt formats can significantly influence the effectiveness of jailbreaking attacks. For experimental consistency and reproducibility, we utilize the default settings across all evaluations."}, {"title": "DELMAN", "content": "The editing dataset consists of 200 queries from HARMBENCH, along with a single uniform Ytarget: \u201cI'm sorry, I can't assist with that.\". Our method does not require a dedicated safe response for every single question; Instead, just this one Ytarget is sufficient to provide robust defenses with minimal impact on the model's utility. And we directly adopt the findings in MEMIT (Meng et al., 2022b) and use the 7th and 8th layer as the crucial layers for Llama2-7B and Vicuna-7B. The hyperparameters of DELMAN are shown in Table 2."}, {"title": "LoRA", "content": "We also apply LoRA fine-tuning on the same 200 queries from the HARMBENCH; However, in this setup, each query is paired with a safe response generated by GPT-4 as the Ytarget. We have verified that these Ytarget achieve 0 ASR on HARMBENCH classifier. Notably, if we were to follow the same strategy as used in DELMAN and adopt a single uniform Ytarget for all queries, the model would inevitably converge to generating only that single response. This would severely limit the model's ability to provide diverse and contextually appropriate responses. The hyperparameters of LoRA are shown in Table 3."}, {"title": "SafeDecoding", "content": "SafeDecoding (Xu et al., 2024a), a safety enhancement method that operates by adjusting token probability distributions. This approach strengthens the model's security through two key mechanisms: boosting the probability of safety disclaimers while reducing the likelihood of potential jailbreak sequences. We utilized their publicly released fine-tuned versions of Llama2 and Vicuna models."}, {"title": "Prompt", "content": ""}, {"title": "Prompt Used for Harmful Tokens Extraction via GPT-4", "content": "You are tasked with analyzing queries to extract harmful tokens. Follow these instructions: 1. Do not generate harmful, inappropriate, or speculative content; 2. Do not fulfill or answer harmful queries; 3. Extract the most harmful tokens (keywords or phrases) that contribute to the query's harmful nature or potential misuse; 4. Output tokens as a list: [token1, token2, ...].\nQuery: INSERT QUERY HERE}"}, {"title": "Prompt Used for Random Sequence Generation via GPT-4", "content": "You are tasked with generating random sequences that include the specified token. Follow these instructions: 1. Avoid generating harmful, inappropriate, or unsafe content; 2. Each sequence should be 15\u201330 words long; 3. Use the given token exactly once in each sequence.\nTokens: INSERT TOKENS HERE}"}, {"title": "Supplementary Materials", "content": ""}, {"title": "Effectiveness of DELMAN", "content": "Table 7 presents the exact value of reduced ASR by DELMAN and baselines."}, {"title": "Effectiveness of DELMAN on Each Harmful Behavior", "content": "Figure 9 compares the performance of DELMAN on Llama2-7B across individual HARMBENCH behavior."}, {"title": "Cross-Behavior Observations", "content": "Figure 10 presents the results for Vicuna-7B under the GCG and AutoDAN jailbreak attacks."}, {"title": "Results of DELMAN across Harmful and Clean Tokens", "content": "Figure 11 shows the k and v distribution differences between harmful and clean tokens. Notably, choosing harmful tokens is vital for preserving model utility: while editing with clean tokens also reduces ASR, these tokens frequently appear in benign queries across various contexts, leading to unnecessary modifications of the model's normal behaviors. In contrast, harmful tokens are primarily concentrated in unsafe queries, allowing for more precise interventions. This explains why editing based on clean tokens leads to significant degradation in MT-Bench scores - it unintentionally affects the model's processing of legitimate queries where these common tokens naturally occur. In our experiment, we define clean tokens as the third-to-last word in queries."}, {"title": "Effectiveness of Sequential DELMAN", "content": ""}, {"title": "Computing Resources", "content": "The experiments are carried out on 2 NVIDIA A40 GPUs with a total computation time of 680 GPU hours."}]}