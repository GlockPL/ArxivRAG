{"title": "Spoken Grammar Assessment Using LLM", "authors": ["Sunil Kumar Kopparapu", "Chitralekha Bhat", "Ashish Panda"], "abstract": "Spoken language assessment (SLA) systems restrict themselves to evaluating the pronunciation and oral fluency of a speaker by analysing the read and spontaneous spoken utterances respectively. The assessment of language grammar or vocabulary is relegated to written language assessment (WLA) systems. Most WLA systems present a set of sentences from a curated finite-size database of sentences thereby making it possible to anticipate the test questions and train oneself. In this paper, we propose a novel end-to-end SLA system to assess language grammar from spoken utterances thus making WLA systems redundant; additionally, we make the assessment largely unteachable by employing a large language model (LLM) to bring in variations in the test. We further demonstrate that a hybrid automatic speech recognition (ASR) with a custom-built language model outperforms the state-of-the-art ASR engine for spoken grammar assessment.", "sections": [{"title": "I. INTRODUCTION", "content": "The demand for second language (L2) learners to study foreign languages, especially English, leads to the imminent need for the development of language proficiency assessment systems or tools [1], [2]. While several English language assessment tools exist, the assessments are often lengthy because they have separate assessment modules to assess different aspects of language proficiency. The spoken language proficiency assessment is often restricted to assessing the speech articulation of the speaker in terms of pronunciation [3]-[5] and speech delivery in terms of oral fluency [6], [7], which includes speaking rate [8], [9], recognition of pauses, filler words, and analysis of intonation [10] etc. The other important aspects of language like grammar or vocabulary are assessed separately through a written language proficiency assessment. Spoken language assessment (SLA) and written language assessment (WLA) complement each other, providing a comprehensive evaluation of overall language proficiency. Separate SLA and WLA assessments not only extend testing time but may also encourage learners to neglect grammar. In practical settings like call centers and virtual interviews, spoken language communication is important. This highlights the need for a comprehensive SLA system that assesses all aspects of language proficiency. The primary obstacle to integrating grammar assessment into current SLA systems is the limited availability or accuracy of speech analysis tools. Accurate grammar assessment requires precise identification of spoken words by ASR engines, which can be challenging due to the limitations of ASR, especially with spontaneous speech. As a result, grammar assessment is often delegated to WLA systems. For further insights on the differences between spoken and written language, refer to For details on the difference between spoken and written language text see [11]. The study [12] compared a cascaded system with separate modules for ASR disfluency removal, and grammar error correction, to an end-to-end system and demonstrated that the performance of the latter system was comparable to that of the former. With current advancements in ASR technology, often it can be believed that these systems could capture spoken grammatical errors in the decoded text. However, these systems have an inherent bias from the language model (LM) towards the grammatically correct text. The study [13] found that a deep learning-based grammatical error detection (GED) system, fine-tuned on free speech data, improved performance on non-native spoken English. However, challenges in ASR and disfluency detection limited accurate feedback. The work [14] evaluated the impact of ASR errors on GED using a deep learning-based system originally trained on written text. ASR confidence scores were integrated into the GED system to address the grammatical errors stemming from incorrect transcriptions rather than learner mistakes. In [15], the authors evaluated ASRbased methods for spoken GED, finding that a score-based classification outperforms the cascaded approach. They also found that LM and N-best hypotheses had minimal impact on decoding-based likelihood classification. The above two studies highlight the issue with the current spoken GED systems that use SOTA ASR and the need for a system using custom-built LMs.\nIn this paper, we introduce an end-to-end SLA system to enable GED or assessment of language grammar from spoken speech. Further, the use of a large language model (LLM) makes the SLA system scalable and practical because no two assessment instances are the same; ensuring that the student cannot be coached for the assessment. The main contribution of the paper is (a) designing a SLA system that can robustly evaluate all aspects of language proficiency, without employing additional WLA tools, thereby significantly reducing the time taken to take the test, (b) proposing a mechanism to incorporate language grammar assessment by exploiting the superior performance of available speech analysis tools on read speech, (c) automatic grammar assessment using a custom-built LM on top of a readily available hybrid ASR system, (d)"}, {"title": "II. SPOKEN LANGUAGE GRAMMAR ASSESSMENT", "content": "The block diagram of the end-to-end SLA system is shown in Fig. 1a. It has two parts, the first part, allows for the generation of a paragraph P (example Fig. 4(a)) by prompting an LLM, and the second part takes the audio S(t), spoken by the candidate, corresponding to Pd (example, 4(b)) and assesses for language grammar using Pg (Fig. 4(c)). Unlike traditional SLA systems which take an audio input S(t) and use the output Ps of a standard ASR to automatically compute the pronunciation or oral fluency [16], [17] only, in this paper, we enable grammar assessment on spoken speech. The grammar scoring acts on the output of the ASR, namely, Ps and the gold truth P, (details mentioned later). This is done by displaying the paragraph Pa generated by an LLM using prompt engineering. We would like to emphasize that we do not focus on oral fluency and pronunciation (red dotted lines in Fig. 1a) which is common in SLA systems in this paper. Further, we do not delve into literature to focus on the proposed SLA system; an implementation is shown in Fig. 1b."}, {"title": "A. Generation of Paragraph", "content": "A sample P generated by prompting a LLM [18] is shown in Fig. 2a, 5a and 5b. The tags \"<grammar> </grammar>\" correspond to the words or phrases that are to be evaluated for grammar. The tag \"<correct> </correct>\" shows the correct choice. The correct choice of grammar usage is studying corresponding to study/studied/studying displayed to the student. In practice, both Pa (Fig. 2b) and Pg (Fig. 2c) can be obtained by a simple text parser applied on P (Fig. 2a)."}, {"title": "B. Spoken Language Grammar Scoring", "content": "The student is shown a paragraph Pa on a web interface (Fig. 1b) containing |Pa| words in language L. Of the Pa words, a small subset of words Gw (\u2208 Pd,\u226a Pa) help determine the student's grammar proficiency. The student (s) is given time to familiarize themselves with Pa and then reads it into a microphone, generating the audio S(t). The SLA system performs grammar scoring in the following steps.\n#1 Building a customized LM (CLM) specific to the para- graph P to enhance the performance of the ASR (ASR-CLM). Let Ps = ASR-CLM(S(t)) be the transcript of S(t).\n#2 Compute the grammar score (S3)"}, {"title": "C. Speech to Text (ASR)", "content": "The most crucial block is the ASR, which converts the spoken paragraph S(t) into text Ps (see Fig. 1a) because ASR outputs are erroneous [19] leading to an error in grammar assessment. Let *Ps be the true transcript (human transcribed) of S(t). Let es be the error due to ASR, generally captured as the word error rate [20] (WER) between Ps and *Ps,\n\n$E_s = WER(P_s, *P_s).$\n\nUnless \u20ac5 = 0, the audio grammar assessment score S would be different from the true grammar assessment score,\n\n$^*S = G-SCORE(P^*_s, P_d, G_w).$\n\nThe error in grammar scoring due to an error (es) in ASR is\n\n$\u20ac_g = |S - ^*S|.$\n\nWe hypothesize that in addition to the way G-SCORE is deter- mined (1), the construction of CLM tightly coupled with the assessment paragraph P performs better than even the state- of-the-art ASR (we use whisper [21] in our experiments). This is due to the fact that a LM plays a significant role in improving the accuracy of an ASR engine. While whisper is trained on extremely large and varied sets of text data, they are likely to lack grammatically incorrect sentences.\nAs an illustration (see Fig. 3) there are three possible options for both the preposition (a/an/the) and the verb (study/studied/studying). Hence, the total number of possible sentences using all options is nine. Most of these (eight of the nine) sentences will rarely occur, in any text databases since they are grammatically incorrect. Hence, text corpora used for training whisper will not include these sentences. Shallow fusion is the most popular approach to combine pre-trained ASR model and LM [22]. Shallow fusion can be expressed mathematically as:\n\n$score(P_s|S(t)) = log (p(P_s|S(t))) + \\gamma \\cdot log(p(P_s)) $\n\nwhere Ps is the spoken paragraph, p(Ps|S(t)) is acoustic score, y is a scaling factor and p(Ps) is LM score. If Ps is not present in the training text, then p(P\u2084) = 0, which will make score(Ps|S(t)) very small. This results in the ASR"}, {"title": "III. EXPERIMENTAL ANALYSIS", "content": "We first describe how to generate a unique assessment para- graph P for each student using ChatGPT. This ensures that the students cannot be coached for the assessment. Subsequently, we experiment with an instance of P to validate the use of an ASR engine equipped with a custom-built LM based on the generated paragraph, namely, ASR-CLM."}, {"title": "A. Generating Pusing ChatGPT", "content": "We adopt 1-shot learning prompting style for generating new paragraphs (P1, P2,) as described in Fig. 4."}, {"title": "B. ASR performance", "content": "We used whisper speech recognition engine and a Kaldi- based ASR with a custom-built LM (ASR-CLM) for compar- ison. The acoustic model of the Kaldi ASR was trained on 960 hours of speech data from Librispeech database [24]. The custom LM was trained on the text comprising all possible variations of the given sentences (example Fig. 3b). We recorded speech corresponding to all variations of the below sentence, \"It (was/is/am) a late afternoon probably (on/in/of) the 15th of February, 2019. (I and my friend/my friend and I) (was/were/will be) walking on the footpath (in/inside/into) central Bangalore.\" namely, 3(was/is/am) \u00d7 3(on/in/of) \u00d7 2(I and my friend/my friend and I) \u00d7 3(was/were/will be) \u00d7 3(in/inside/into) = 162 utterances. We found that ASR-CLM was able to exactly transcribe the utterance (even when there was an error in grammar) while whisper \"corrected\" the grammatical error. Overall, the ability of ASR-CLM to recognize what was spoken is 84.7% while that of whisper was 46%. The performance was computed on 137 utterances; 25 of the 162 utterances were discarded because of noise. The poor accuracy of the SOTA ASR highlights the need for a CLM-ASR for the purpose of SLA of grammar.\nTo the best of our knowledge, a standard speech dataset for spoken grammar assessment with manual annotations of grammatical errors in conversational or read speech is cur- rently unavailable. To evaluate our SLA system, we used an in- house dataset consisting of audio recordings from 17 students speaking a generated paragraph, which was manually assessed by a linguist to mark the grammar score (*S). We used both whisper and ASR-CLM to convert the spoken paragraph to text and compute S. The error in assessment is captured in parenthesis for each student in Table II. Larger grammar assessment errors (\u20acg = 20) due to whisper are observed compared to 6g = 3 for a custom-built LM ASR (ASR-CLM)."}, {"title": "IV. CONCLUSIONS", "content": "Language proficiency assessment is a common requirement for L2 speakers of English. There exist several SLA tools to assess pronunciation and oral fluency but none of them venture into assessing language grammar, instead, they depend on WLA systems. We designed and implemented a practical, scal- able and robust SLA system to assess grammar. The design, to display the paragraph with options, made sure the audio obtained for assessment had no spontaneous speech charac- teristics like filler words, or repetitions and resembled \"read\" speech thereby enhancing the ASR performance. Additionally, the use of a custom LM in ASR-CLM leads to improved ASR performance, resulting in robustness in grammar assessment. The use of LLM enables the generation of paragraphs that are largely non-repetitive thereby making the proposed system hard to be memorized by students. We can observe that the grammar scoring mechanism, by design, is not affected by ASR mis-recognition of non Gw words."}]}