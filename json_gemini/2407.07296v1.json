{"title": "Large Language Model-Augmented Auto-Delineation of Treatment Target Volume in Radiation Therapy", "authors": ["Praveenbalaji Rajendran", "Yong Yang", "Thomas R. Niedermayr", "Michael Gensheimer", "Beth Beadle", "Quynh-Thu Le", "Lei Xing", "Xianjin Dai"], "abstract": "Radiation therapy (RT) is one of the most effective treatments for cancer, and its success relies on the accurate delineation of targets. However, target delineation is a comprehensive medical decision that currently relies purely on manual processes by human experts. Manual delineation is time-consuming, laborious, and subject to interobserver variations. Although the advancements in artificial intelligence (Al) techniques have significantly enhanced the auto-contouring of normal tissues, accurate delineation of RT target volumes remains a challenge. In this study, we propose a visual language model-based RT target volume auto-delineation network termed Radformer. The Radformer utilizes a hierarchical vision transformer as the backbone and incorporates large language models to extract text-rich features from clinical data. We introduce a visual language attention module (VLAM) for integrating visual and linguistic features for language-aware visual encoding (LAVE). The Radformer has been evaluated on a dataset comprising 2985 patients with head-and-neck cancer who underwent RT. Metrics, including the Dice similarity coefficient (DSC), intersection over union (IOU), and 95th percentile Hausdorff distance (HD95), were used to evaluate the performance of the model quantitatively. Our results demonstrate that the Radformer has superior segmentation performance compared to other state-of-the-art models, validating its potential for adoption in RT practice.", "sections": [{"title": "I. INTRODUCTION", "content": "RADIATION therapy (RT) is a widely used modality for the treatment of cancer [1]\u2013[3]. In RT, ionizing radiation is used to kill the cancerous cells by inflicting damage to their DNA. The therapeutic efficacy of the RT is achieved by administering sufficient radiation doses to the tumor target while minimizing the exposure to normal tissues [4]. In RT, precision delineation of the treatment target plays an important role, and it directly impacts the treatment outcomes. Furthermore, advanced RT treatment plans, such as the volumetric-modulated arc therapy (VMAT), are more susceptible to contouring inaccuracies. However, manual contouring of target volume is a complex, laborious process, subject to intra- and inter-observer variations [5]. Moreover, studies have demonstrated that a fire amount of the manually delineated target volumes are subjected to changes during the peer review process [6]\u2013[8]. Over the last decade, deep learning (DL) has achieved significant progress in medical image segmentation tasks. Convolutional neural network (CNN) has demonstrated significant achievements in medical image segmentation [9]\u2013[16]. Among CNNs, UNet stands out as one of the most extensively employed networks for segmentation tasks. For instance, a UNet-based hybrid densely connected network has been proposed for hepatic tumor segmentation from the combination of magnetic resonance (MR) and computed tomography (CT) images[17]. Similarly, a three- branch two-dimensional (2D) U-Net termed multiple branch UNet (MB-UNet) has been proposed for the concurrent segmentation of the prostate and lesions from the T2-weighted, diffusion weighted (DWI), and apparent diffusion coefficient (ADC) MR images [18]. To facilitate rapid target contouring, U-Net has also been extended to 3D volumetric data. For instance, 3D-UNet has been proposed for segmenting the prostate and the related organs for dose optimization in radiation therapy [19]. Moreover, a two-channel input 3D-UNet has also been proposed for the target volume segmentation in head and neck cancer [20]. In another implementation, CUNet, a modified 3D U-Net with residual block integrated with attention center block, has been proposed for prostate segmentation from CT images [21]. Similarly, DSD-UNet, a 3D-UNet-inspired architecture incorporating residual connection and dilated convolution, has been implemented for target volume segmentation in brachytherapy [22]. However, these methods suffer from limited delineation accuracy and are deemed unacceptable in routine clinical practice.\nOver recent years, attention-based transformers have significantly advanced natural language processing (NLP) and computer vision domains [23]\u2013[28]. Moreover, transformer- based backbones have achieved comparable or better performance than those of CNN-based backbones. Inspired by their success, attention-based transformers have also been adapted for various medical segmentation tasks. For instance, CoTr model interleaves transformers between the CNN decoder and encoder to enhance the tumor and organ segmentation performance [29]. In another development, TransUNet employs"}, {"title": "II. MATERIALS AND METHODS", "content": "The overview of the Radformer is shown in Figure 1. The Radformer utilizes an encoder-decoder architecture, utilizing hierarchical vision transformers [33] in the encoder to generate cross-modal alignments between visual and textual information. A CNN-based decoder is utilized to generate 3D segmentation maps. In this section, we begin with the introduction of Language-Aware Visual Encoding (LAVE), followed by the Vision Language Attention Module (VLAM), the Language Gating Unit (LGU), and the CNN-based decoder."}, {"title": "1) Language- Aware Visual Encoding (LAVE)", "content": "The encoder of the Radformer is designed to extract and fuse the features from the 3D images and the text-rich clinical data. Typically, a patient's clinical data comprises extensive unstructured information. To extract the features that are relevant to defining the treatment targets, we leverage the potential of the Generative Pre-trained Transformer 4 (GPT-4) [41]. We utilize tailored prompts to guide the GPT-4 to extract tumor-related information from the clinical data. A domain- specific Bidirectional Encoder Representations from Transformers (BERT) model pre-trained on the PubMed corpus called PubMed-BERT [42] was utilized to contextually embed the data extracted by GPT-4. The embedded high-dimensional word vector is represented as \\(L\u2208 R^{C_t \u00d7T}\\), where \\(C_t\\) represents the number of channels (768, corresponding to the size of the PubMed BERT hidden layer) and T represents the number of words.\nFollowing the language feature extraction, we employ a SWIN-UNETR [33] based encoder to perform the combined visual feature fusion and visual language feature fusion. The encoder layer comprises four stages \\(i \u2208 {1, 2, 3, 4}\\). Each stage of the encoder has two transformer blocks, and it is designed to take two inputs, a 3D image of dimension \\(H \u00d7 W \u00d7 D\\) (\\(V_e \u2208 R^{H\u00d7W\u00d7D}\\)) and vectorized tumor information of dimension \\(C_t\u00d7T\\) (\\(L\u2208 R^{C_t \u00d7T}\\)). A patch partition layer is used to divide the 3D image volume into patches of dimension \\(\\frac{H}{2} \times \\frac{W}{2} \times \\frac{D}{2}\\) (\\(P\u2208R^{\\frac{H}{2} \times \\frac{W}{2} \times \\frac{D}{2}}\\)). The partitioned patches are then projected into an embedding space dimension \\(C_i\\). In the encoder, the first three stages consist of two SWIN transformer blocks \\(\u03b1_i\\), a multimodal feature fusion module \\(\u03b2_i\\), and a gating unit \\(\u03d5_i\\). In these stages, the generation of semantic-aware visual features involves a three-step process inspired by language-aware vision transformer (LAVT) model [39]. In each of the initial three stages, the SWIN transformer blocks \\(\u03b1_i\\) utilize the features generated by the preceding stage as input to generate visual features as output \\(I_i \u2208 R^{C_i\u00d7H_i \u00d7 W_i\u00d7 D_i}\\). The resultant visual features \\(I_i\\) are then integrated with the language features L via a multimodal feature fusion module termed VLAM to generate multimodal attention features \\(M_i \u2208 R^{C_i\u00d7H_i \u00d7 W_i\u00d7 D_i}\\). A learnable gating unit \\(\u03d5_i\\) called LGU weighs the attention features \\(M_i\\); the weighted features \\(G_i\\) are then combined elementwise to the"}, {"title": "2) Visual Language Attention Module (VLAM)", "content": "The schematic of the VLAM is shown in Figure 2. The VLAM utilizes the visual features as the query \\(I_i \u2208 [R^{C_i\u00d7H_i \u00d7 W_i\u00d7 D_i}\\)and language features \\(L \u2208 R^{C_t \u00d7 T}\\) as the key and value to generate position-specific sentence-level feature vectors \\(F_i \u2208 R^{C_i \u00d7 H_i\u00d7 W_i\u00d7 D_i}\\) as follows.\n\\(I_{iq} = flatten (p_{iq}(I_i))\\) (1)\n\\(I_{ik} = p_{ik} (L)\\) (2)\n\\(I_{iv} = p_{iv}(L)\\) (3)\n\\(F_i = softmax(\\frac{I_{iq}I_{ik}^T}{\\sqrt{C_i}}) I_{iv}\\) (4)\n\\(F_{ij} = p_{iw}(unflatten(F_i^T))\\) (5)\nWhere, \\(p_{iq}\\), \\(p_{iw}\\), \\(p_{ik}\\), \\(p_{iv}\\), are projection functions. The projection functions \\(p_{iq}\\) and \\(p_{iw}\\) are implemented as 1 \u00d71 \u00d7 1 convolutions followed by instance normalizations to generate projections resulting in channels \\(C_i\\). The projection functions \\(p_{ik}\\) and \\(p_{iv}\\) are implemented as 1 \u00d71\u00d71 convolutions to"}, {"title": "3) Language Gating Unit (LGU)", "content": "Figure 3 illustrates the schematic of the LGU. The LGU is designed as a language gate that adaptively regulates the amount of information flowing to the subsequent stage of the transformer. The LGU is implemented as follows:\n\\(H_i = \u03b3_i(M_i)\\) (8)\n\\(G_i = H_i \u2295 F_i + I_i\\) (9)\nWhere \\(\u03b3_i\\) refers to a two-layer perceptron; the first layer consists of 1\u00d71\u00d71 convolution paired with RELU activation, and the second layer comprises 1 \u00d71 \u00d71 convolution followed by hyperbolic tangent function. The \\(\u2295\\) in Eq. (9) signifies the element-wise multiplication."}, {"title": "4) Decoder", "content": "The multimodal feature maps \\(M_i\\) generated by the encoder are utilized to generate the 3D segmentation maps using a CNN- based decoder. The decoding process at each stage of the decoder is implemented as follows:\n\\(S_0 = R(V), V = V_1 + V_2\\) (10)\n\\(S_1 = R(P), P = P_1 + P_2\\) (11)\n\\(S_j = R(M_j), j\u2208 { 2,3,4}\\) (12)\n\\(Y_i = R[v(Y_{i+1}); S_i], i\u2208 {0,1,2,3,4}\\) (13)\n\\(Y_5 = S_5\\) (14)\nWhere, R represents a residual block implemented with two 1 x 1 x 1 convolutions with instance normalization u. The \\(v\\) signifies the deconvolution based up sampling operation. The symbol [;] represents the concatenation among the channel dimensions. The final 3D segmentation outputs are generated from the feature maps \\(Y_0\\) utilizing a 1 \u00d7 1 \u00d7 1 convolution with a SoftMax activation."}, {"title": "B. Data", "content": "We evaluated the Radformer on a public head-and-neck cancer dataset (RADCURE) [43]. The dataset comprises CT volumes and gross tumor volume (GTV) contours of patients who underwent radiation therapy, along with clinical data such as demographic details, clinical histories, and treatment specifics. The average age of the patients in the dataset was 63 years and the dataset consists of individuals diagnosed with oropharyngeal, larynx, nasopharynx, and hypopharynx cancer. The clinical information was based on the 7th edition TNM staging system and was standardized to adhere to the American Association of Physicists in Medicine (AAPM) Task Group report no.263 (TG263) nomenclature. To optimize the Radformer, we preprocessed the CT images and the GTV contours. The patient cases with data inconsistencies and corrupt labels were removed during the preprocessing. The CT images were center cropped to a size of 336 \u00d7336 \u00d7 64 voxels to ensure all regions of interest were included. The contours were used to generate segmentation masks, including the background and targets. The dataset was randomly divided into two groups: one comprising 2,388 datasets for training and validating the model, and another group with 597 datasets designated for testing."}, {"title": "C. Training", "content": "The loss function employed is a composite loss function comprising dice focal loss and Tversky loss from the MONAI library [44]. The composite loss function is formulated as:\n\\(L_{tot} = L_{DFC}(Y_G, Y_P) + L_{TV}(Y_G, Y_P)\\) (15)\nWhere, \\(Y_G\\) denotes the true label and \\(Y_P\\) denotes the predicted label. The dice focal loss, and Tversky loss are represented as \\(L_{DFC}\\) and \\(L_{TV}\\)."}, {"title": "2) Implementation", "content": "Besides our introduced multimodal model, we also implemented advanced models including SWIN-UNETR and 3D-UNETR for comparisons. Each model (Radformer, SWIN-UNETR, and 3D-UNETR) was implemented using the PyTorch deep learning library [45] on an Nvidia RTX 4090 GPU with a batch size of 1. Adam optimizer with a learning rate of 4 \u00d7 10-4 was utilized with a weight decay of 1 \u00d7 10-3. All the models were trained for 100 epochs. We utilized HuggingFace's Transformer library [46] to integrate the PubMed BERT into the Radformer. The PubMed BERT utilized is a base model comprising of 12 layers with a hidden size of 768. The SWIN transformer layers of the Radformer are initialized with pre-trained weights from the BraTS dataset [32]. The remaining weights are randomly initialized."}, {"title": "D. Evaluation Metrics", "content": "For evaluating the segmentation performance, we chose commonly used metrics including the dice similarity coefficient (DSC), intersection over union (IOU), and 95th percentile Hausdorff distance (HD95) [44]. These metrics were calculated on 3D volumes. Statistical analysis such as paired t-test was performed between our proposed method and other models across all metrics (DSC, IOU, and HD95)."}, {"title": "III. RESULTS", "content": "The performance of the Radformer was assessed utilizing the public RADCURE dataset as described in the above session. We also compared the performance of the Radformer with other state-of-the-art DL-based segmentation approaches including SWIN-UNETR and 3D-UNETR. The SWIN-UNETR and 3D- UNETR networks are DL approaches widely utilized for 3D segmentation tasks. As summarized in Table 1, the Radformer significantly outperforms other networks over all the metrics (p < 0.05). In particular, Radformer achieved a mean DSC score of 0.76, a mean IOU of 0.69, and a mean HD95 of 7.82 mm. The Radformer outperformed the baseline 3D-UNETR by 15% in terms of mean DSC and 17% in terms of mean IOU. Furthermore, a 45% improvement in the boundary accuracy (HD95) was noted compared to that of the 3D-UNETR."}, {"title": "IV. DISCUSSION", "content": "Delineation of the treatment target is a crucial step in RT treatment planning, and its accuracy significantly affects the quality of the RT plan. Currently, the delineation of target volume purely relies on human expert's manual process, which is time-consuming, laborious, and subject to intra- and inter- observer variability depending on the individual's knowledge and experience. Although DL-based segmentation approaches have made considerable advancements in recent years, its success in RT target delineation compared to normal organ segmentation is limited. At present, the delineation of RT treatment targets is still clinically unacceptable in most situations. The inferior performance of DL-based approaches for RT target delineation compared to normal organ segmentation could be attributed to the complexity of target delineation in RT. In fact, RT target delineation is a comprehensive medical decision-making process. In routine clinical practice, radiation oncologists consider the entire medical history and diagnosis of the disease when performing target delineation. In this study, we tested our hypothesis that, in addition to images, radiation oncologists leverage clinical data for RT target delineation. Therefore, integrating both text- rich clinical information and imaging data will improve DL- based auto-segmentation methods for RT target delineation.\nWe developed a VLM-based 3D medical image segmentation network called Radformer. The Radformer utilizes the SWIN transformer as the backbone to extract visual features from images and LLMs to extract text-rich features from clinical data. The visual and linguistic features are fused through language-aware visual encoding and then leveraged by a CNN- based decoder to output segments. The Radformer was trained on a dataset from 2388 patients and evaluated on a test dataset comprising of 597 patients. Our results demonstrate that the Radformer achieves superior performance in comparison with the other state-of-the-art architectures purely leveraging visual features (SWIN-UNETR, and 3D-UNETR). Moreover, the statistical analyses using paired t-tests over the metrics DSC, IOU, and HD95 further substantiate the significant improvement achieved by Radformer. Furthermore, the enhanced accuracy in segmenting the GTV between the Radformer and Radformer without LAVE (Table 2) underscores the importance of linguistic and visual feature integration for GTV segmentation.\nFigure 7 depicts the violin plots of segmentation performance distribution on all patient test cases, evaluated over the metrics including DSC, IOU, and HD95. These plots reveal the spread and density of the segmentation performance scores across ~600 patient cases. Figures 7(a) and 7(b) highlight that the Radformer architecture generally surpasses the performance of other models in most patient cases for both DSC and IOU metrics. This suggests a consistent, robust, and generalized segmentation capability. Furthermore, over other architectures such as Radformer without LAVE, SWIN-UNETR, and 3D- UNETR, it can be noted that the performance scores are tightly clustered within the interquartile range and are characterized by lower median values, indicating a relative underperformance in certain patient scenarios. This variability in performance substantiates the advantages of integrating linguistic and visual features. Furthermore, the violin plots representing performance over the HD95 metric indicate that the majority of the Radformer's performance is skewed towards a lower median value, suggesting enhanced accuracy in contour segmentation compared to other methods. This skewing towards lower values indicates the tendency of the Radformer to yield more precise segmentations with less deviation from the ground truth, as lower HD95 values correspond to smaller distances between predicted and actual contours. Moreover, the improvement in segmentation performance when linguistic information can be noted by comparing the performance distribution between the Radformer and Radformer without LAVE over all the metrics.\nAt present, the clinical data associated with the medical images are unstructured and extensive. In this study, we leverage the potential of LLMs to extract valuable information from the clinical data through customized prompts. In particular, we used GPT-4 to extract tumor-related information from clinical data. As GPT-4 only generates output texts for public access, we utilized PubMed BERT (an open-source model) to contextually embed the lesion information provided by GPT-4. Ideally, the process of extracting tumor-related information and contextual embedding can be masked by a single LLM. The proposed approach is generic and has the great potential to be adaptable to the segmentation of various cancer types in radiation oncology.\nMoving forward, integrating our proposed method into standard radiotherapy treatment planning could transform"}, {"title": "V. CONCLUSION", "content": "In conclusion, in this study, we propose a large language model-augmented approach for RT treatment target auto- delineation. Breaking away from the conventional architectures relying only on visual features, our model leverages text-rich clinical information and visual features to enhance the accuracy of target delineation. The proposed method could be adopted into routine radiotherapy treatment planning, offering a means to rapidly contour the target volumes with high precision."}]}