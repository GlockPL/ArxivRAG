{"title": "LATENT WEIGHT DIFFUSION:\nGENERATING POLICIES FROM TRAJECTORIES", "authors": ["Shashank Hegde", "Gautam Salhotra", "Gaurav S. Sukhatme"], "abstract": "With the increasing availability of open-source robotic data, imitation learning has\nemerged as a viable approach for both robot manipulation and locomotion. Cur-\nrently, large generalized policies are trained to predict controls or trajectories using\ndiffusion models, which have the desirable property of learning multimodal ac-\ntion distributions. However, generalizability comes with a cost - namely, larger\nmodel size and slower inference. Further, there is a known trade-off between\nperformance and action horizon for Diffusion Policy (i.e., diffusing trajectories):\nfewer diffusion queries accumulate greater trajectory tracking errors. Thus, it is\ncommon practice to run these models at high inference frequency, subject to robot\ncomputational constraints.\nTo address these limitations, we propose Latent Weight Diffusion (LWD), a\nmethod that uses diffusion to learn a distribution over policies for robotic tasks,\nrather than over trajectories. Our approach encodes demonstration trajectories\ninto a latent space and then decodes them into policies using a hypernetwork. We\nemploy a diffusion denoising model within this latent space to learn its distri-\nbution. We demonstrate that LWD can reconstruct the behaviors of the original\npolicies that generated the trajectory dataset. LWD offers the benefits of con-\nsiderably smaller policy networks during inference and requires fewer diffusion\nmodel queries. When tested on the Metaworld MT10 benchmark, LWD achieves\na higher success rate compared to a vanilla multi-task policy, while using models\nup to ~18x smaller during inference. Additionally, since LWD generates closed-\nloop policies, we show that it outperforms Diffusion Policy in long action horizon\nsettings, with reduced diffusion queries during rollout.", "sections": [{"title": "1 INTRODUCTION", "content": "The recent increase in open-source robotic data has made imitation learning an attractive prospect\nto solve robot manipulation and locomotion tasks (Collaboration et al., 2023; Peng et al., 2020).\nWhile traditional supervised learning methods like Behavioral Cloning (Florence et al., 2022) and\ntransformer-based models such as RT-1 (Brohan et al., 2022) have demonstrated some success, they\nare unable to capture the multimodal nature of robotic action distributions (e.g., while avoiding an\nobstacle in a navigation task with two optimal actions in opposing directions 'turn left' and 'turn\nright', the learned action \u2018go straight' is a suboptimal average of the two). Recently, diffusion-based\nmethods have emerged as a promising alternative for robot control (Tan et al., 2024), offering the\nadvantages of continuous outputs and the capacity to learn multimodal action distributions.\nInspired by the success of latent diffusion in vision (Rombach et al., 2022b) and language (Lovelace\net al., 2024), we explore it here for robotics. We introduce a novel method that utilizes diffusion"}, {"title": "2 RELATED WORK", "content": ""}, {"title": "2.1 IMITATION LEARNING FOR ROBOTICS", "content": "With the availability of vast swaths of open-source robotic data, imitation learning has emerged as a\nviable approach for robot control. Original imitation learning approaches were simple \u2013 behavioral\ncloning agents that learned to predict controls or trajectories from expert demonstrations. With the\nadvent of transformer-based methods, imitation learning has grown in popularity. Methods like\nPerAct (Shridhar et al., 2022) and RT-1 (Brohan et al., 2022) perform well on various tasks. Brohan"}, {"title": "2.2 DIFFUSION", "content": "Diffusion models have emerged as a leading approach in image generation, with Denoising Diffu-\nsion Probabilistic Models (DDPM) being a prominent class of generative models (Ho et al., 2020).\nThese models generate images by progressively denoising samples drawn from an isotropic Gaus-\nsian distribution. Furthermore, Rombach et al. (2022a) demonstrated that diffusion can be effec-\ntively applied in the latent space of a pre-trained Variational Autoencoder. Recently, diffusion-based\nmethods have shown promise in solving robotic tasks. The seminal work Chi et al. (2024) showed\nthat diffusion models can be used to learn multimodal action distributions for a task by diffusing tra-\njectories for control up to a defined action horizon. Urain et al. (2022) showed that diffusion models\ncan be used to learn smooth cost functions for the joint optimization of grasp and motion plans.\nFor long horizon skills, Mishra et al. (2023) showed how to use diffusion to chain skills together to\nsolve a larger task. Diffusion models have been used to formulate policies to control a quadruped\nrobot Huang et al. (2024), although the length of the trajectories diffused was still relatively short,\ntherefore requiring a higher diffusion inference frequency. Tan et al. (2024) showed that latent dif-\nfusion could be applied to multi-task manipulation action trajectory generation. A key limitation of\nusing diffusion models to generate trajectories is the degradation in performance for longer action\nhorizons, which is caused by both modeling and trajectory tracking errors."}, {"title": "2.3 HYPERNETWORKS", "content": "Hypernetworks, introduced by Ha et al. (2016), are neural networks that can estimate the weights\nof a secondary network. Following their inception, they have been extended and applied in multiple\nsettings. In meta-learning, Bertinetto et al. (2016) proposed a model where a learner network pre-\ndicts the parameters of another network for one-shot learning tasks, sharing conceptual similarities\nwith hypernetworks. The concept of dynamically generating network parameters is also related to\nDynamic Filter Networks by Jia et al. (2016), where filters are generated on the fly based on the\ninput. This method aligns with the principles of hypernetworks, emphasizing adaptability and ef-\nficiency in processing varying inputs. It has also been shown that hypernetworks can be used for\nrobot policy representation (Hegde et al., 2024)."}, {"title": "2.4 MULTI-TASK LEARNING", "content": "Metaworld (Yu et al., 2020) and RLBench (James et al., 2019) are popular benchmarks for multi-task\nlearning. Many recent transformer-based methods have shown good performance on various tasks,\ngiven task conditioning during training and testing, such as Shridhar et al. (2022) and the RT family\nof models. GNFactor (Ze et al., 2023) uses a generalizable neural feature field to learn a volumetric\nrepresentation of the environment, which can be used to synthesize different views of the environ-\nment. Another way to solve multi-task learning is through modularity. Devin et al. (2016) showed\nhow to split networks into modules that are task-specific and robot-specific. Naturally, the task-\nspecific modules can be shared across robots, and the robot-specific modules can be shared across\ntasks on a robot, enabling the transfer of learned behaviors across tasks or robot embodiments."}, {"title": "3 PROBLEM FORMULATION & METHOD", "content": "This work builds on Hegde et al. (2023), which demonstrates the capacity of latent diffusion mod-\nels to generate policies from a policy dataset while addressing its key limitation - the reliance on\noften unavailable policy datasets - by utilizing trajectory datasets instead. LWD employs a two-\nstep process. A variational autoencoder (VAE) with a weak KL-regularization coefficient encodes\ntrajectories into a latent space that can be decoded into a trajectory. A diffusion model learns the dis-\ntribution of this latent space, enabling policy sampling from the learned distribution (see Figure 2)."}, {"title": "3.1 LATENT POLICY REPRESENTATION", "content": "Consider \u03c0(\u00b7, \u03b8) as a stochastic policy, parameterized by 0, that interacts with the environment and\ngenerates trajectories 7. Suppose there exists a distribution of policy parameters p(0), and we sample\na policy parameter from this distribution and collect a trajectory for this sampled policy parameter.\nThis process of sampling the policy parameter is done for all trajectories collected. We assume that\nfor a given 0, at = \u03c0(st, 0)+e, where e is normally distributed around 0. i.e., at ~ \u039d(\u03c0(st, 0), \u03c3\u00b2)\nOur goal is to learn the distribution of policy parameters p(0) that produced the trajectory dataset.\nWe assume that a latent variable z exists that contains information required to identify different\npolicy behaviors. Since trajectories are generated using parameters 0, we can use conditional inde-\npendence, p(T | z,0) = p(\u03c4 | 0). Considering that our dataset consists of trajectories, we want\nto maximize the likelihood of sampling 7, therefore maximizing logp(t). We derive a modified\nversion of the Evidence Lower Bound (ELBO) to incorporate p(\u03b8). This is shown below.\nlog p(t) = log \u222b\u222b p(\u03c4, 0, z) dz d0 (Introduce policy parameter @ and latent variable z)\n= log \u222b p(\u03c4 | z,0)p(0| z)p(z) dz de (Apply the chain rule)\n= log \u222b \u222b  p(\u03c4 | z, 0)p(0 | z)p(z) / q(z | T) dz d\u0142\n q(z | T)\n(1a)\n(Introduce a variational distribution q(z | T), approximating the true posterior p(z | T))\n= log Ep(0|2) [\u222b  p(\u03c4 | z, 0)p(z) / q(z|t) dz ]\n p(0) \n(1b)\n= log Eq(z|t) [\u222b [Ep(0|z) [p(T | z, 0)] p(z)] / q(z T)  dz]\n q(z | T)\n(1c)\n\u2265 Eq(2/7) log [\u222b [Ep(0|2) [p(T | 2,0)]p(2)] / q(z T) ] (Jensen's inequality)\n= Eq(z|t) [log (Ep(0|z) [p(T | z, 0)])] \u2013 Eq(z|7) [log (q(z | r)) \u2013 log (p(z))]\n(1d)\n= Eq(2/7) [log (Ep(0|z) [p(\u03c4 | 0)])] \u2013 KL(q(z | T) || p(z)) (conditional independence)\n(le)\n\u2265 Eq(2/7) [Ep(0|2) [log (p(\u03c4 | \u03b8))]] \u2013 KL(q(z | T) || p(z)) (Jensen's inequality)\n(1f)"}, {"title": "3.2 VARIATIONAL AUTOENCODER FOR POLICIES", "content": "Since we now have a modified ELBO objective, we shall now try to approximate its components\nwith a variational autoencoder. Let \u00d8enc be the parameters of the VAE encoder that variationally\nmaps trajectories to z, and dec be the parameters of the VAE decoder. We assume the latent z is\ndistributed with mean zero and unit variance. We construct the VAE decoder to approximate p(0 | z)\nwith podec (0 | z). Since at ~ N(\u03c0(st,0), \u03c3\u00b2):\np(at | St, 0) = 1/\u221a2\u03c0\u03c32 exp( -(at \u2013 \u03c0(st, 0))2/2\u03c32)\n(5)\nOur objective is to maximize the mELBO. The likelihood of trajectory Tk = {s\u0142, a\u00a5}=1 for the\ngiven VAE parameters is:\nL({st, a}=1 | \u0424\u0435\u043f\u0441, \u0424dec) = \u2211 E 94enc (2\\{87,471) [Ep@dec(0|2) [logp (ast,0)]]\n\nKL (qenc (z | {st, a}=1) || p(z))\n=C- 202Bgene (21) Eggene (218,071) [Epodec (02) [(\u03b1 - \u03c0(8,0))2]]\n\n- KL (qenc (z | {st, a}=1) || p(z))\n(6)\nFor computational stability, we construct our decoder to be a deterministic function f\u00f8dec, i.e.,\nPodec (0 | z) becomes d(0 \u2013 f\u00f8dec (z)), therefore:\nL({st, a}=1 | \u0424\u0435\u043f\u0441, \u0424dec) = C- 202 \u03a3 E penc (21871) [(\u03b1- \u03c0(84, fodec (2)))2]\n\n- KL (qenc (z | {s, a}=1) || p(z))"}, {"title": "3.3 POLICY DIFFUSION", "content": "In practice, we see that approximating p(z) = N(0, I) is suboptimal, and therefore we set \u03b2\u03ba\u03b9 to\na very small number ~ (10-9,10-6). After training the VAE to maximize the objective provided\nin Equation 7 with this \u03b2\u03ba\u03b9, we have access to this latent space z and can train a diffusion model to\nlearn its distribution p(z). The diffusion process in the latent space involves gradually adding noise\nto the latent variable zo = E(\u03c4) over a sequence of time steps t\u2208 {0,1, . . ., T}. This process can\nbe described by a forward noising process:\nEt = 9(zt | zt-1) = N (zt; Zt-1\u221a1-Bt, BrI)\n(8)\nmaking the forward process Markovian. The reverse or generative process P\u00f3aif (ZT) reverts noise\nfrom an isotropic Gaussian into a sample zo in q(z) and contains a similar Markov structure.\nPodif (20) = P(ZT) Podif (Zt-1|2t), Podif (Zt-1|zt) = N(zt\u22121; \u03bc\u03c6\u03b1if (Zt, t), \u03a3\u03c6dif (zt, t))\n(9)\nWe can condition the latent denoising process on the current state and/or the task identifier c of the\npolicy required. Therefore the model shall be approximating Podif (Zt\u22121 | zt, c). After denoising for\na given state and task identifier, we can convert the denoised latent to the required policy.\nTherefore, to sample from p(\u03b8), first sample z using the trained diffusion model z ~ Ppdif (20), and\nthen apply the deterministic function fodec to the sampled z."}, {"title": "4 EXPERIMENTS", "content": "We first analyze the behavior reconstruction of LWD, followed by the effect of breaking down\nthe trajectory into shorter snippets. Then, we perform an ablation over the size of LWD, showing\nthat a larger model can mitigate the problems introduced by snipping trajectories. After this, we\nbenchmark LWD on the MT10 suite of tasks in Metaworld, showcasing multitask advantages, as\nwell as a benchmark on human-generated data in the PushT environment, showcasing its ability to\nrun at longer action horizons. All our numerical results are shown across three seeds.\nWe focus on demonstrating results in state-based low-dimension observation spaces. Thus, the\ngenerated policies are always Multi-Layer Perceptrons (MLP) with 2 hidden layers with 256 neurons\neach. In the VAE, the encoder is a sequential network that flattens the trajectory and compresses it\nto a low-dimension latent space. The decoder is a conditional hypernetwork from the hypernettorch\npackage (Ehret et al., 2021). For the diffusion model, we adapt the model provided by Rombach\net al. (2022a). For all experiments the latent space is R256, the KL regularization weight \u03b2\u03ba\u03b9 = 10-8,\nthe learning rate is 10\u20134 with the Adam optimizer."}, {"title": "4.1 BEHAVIOR RECONSTRUCTION ANALYSIS", "content": "Here, we ask \u2013 how does LWD perform in reconstructing the behavior of the original policies that\ngenerated the trajectory dataset? Is it able to reproduce different behaviors for the same task?\nFirst, we analyze the behavior reconstruction capability of different components of LWD . For this\nexperiment, we use the D4RL (Fu et al., 2020) halfcheetah dataset. Each trajectory in this dataset\nhas a length of 1000. We combine trajectory data from three original behavior policies provided in\nthis dataset: expert, medium, and random. Following Batra et al. (2023), we track the foot contact\ntimings of each trajectory as a metric for measuring behavior. For each behavior policy, we get 32\ntrajectories. These timings are normalized to the trajectory length and are shown in Figure 3. For\neach plot, the x-axis denotes the foot contact percentage of the front foot, while the y-axis denotes\nthe foot contact percentage of the back foot. We first visualize the foot contact timings of the original\npolicies in Figure 3a. Then, we train the VAE model on this dataset to embed our trajectories into\na latent space. We then apply the hypernetwork decoder to generate policies from these latents.\nThese policies are then executed on the halfcheetah environment, to create trajectories. We plot the\nfoot contact timings of these generated policies in Figure 3b. We see that the VAE captures each\nof the original policy's foot contact distributions, therefore empirically showing that the assumption\nPodec (0 | z) = \u03b4(0 \u2013 f\u00f8dec (z)) is reasonable. Then, we train a latent diffusion model conditioned on\na behavior specifier (i.e., one task ID per behavior). In Figure 3c, we show the distribution of foot\ncontact percentages of the policies generated by the behavior specifier conditioned diffusion model.\nWe see that the diffusion model can learn the conditional latent distribution well, and the behavior\ndistribution of the decoded policies of the sampled latent matches the original distribution. Note that\nto sample policies during inference, we do not need to encode trajectories; rather, we need to sample\nlatents using the diffusion model and use the hypernetwork decoder to decode a policy from it.\nAnother way to analyze the behavior reconstruction capability of LWD is to compare the rewards\nobtained during a rollout. Figure 4 shows us the total objective obtained by the original, VAE-\ndecoded, and diffusion-denoised policies. We see that the VAE-decoded and diffusion-generated\npolicies achieve similar rewards to the original policy for each behavior.\nApart from these plots, we use Jensen-Shannon divergence to quantify the difference between two\ndistributions of foot contact timings. Table 1 shows the JS divergence between the empirical distri-\nbution of the foot contact timings of the original policies and those generated by LWD. The lower\nthis value is, the better. As a metric to capture the stochasticity in the policy and environment, we get\nthe JS divergence between two successive sets of trajectories generated by the same original policy,\nwhich we shall denote SOS (Same as source). A policy having a JS divergence score lesser than\nthis value indicates that that policy is indistinguishable from the original policy by behavior. As a\nbaseline for this experiment, we train a large (5-layer, 512 neurons each) behavior-conditioned MLP"}, {"title": "4.2 ENCODING TRAJECTORY SNIPPETS", "content": "Here, we ask the question can LWD generate policies that are faithful to the original policies,\neven when provided with only a snippet of the trajectory data? For most robotics use cases, it\nis impossible to train on long trajectories due to the computational limitations of working with\nlarge batches of long trajectories. Therefore, we analyze the effect of sampling smaller sections\nof trajectories from the dataset. After training a VAE for the D4RL halfcheetah dataset on three\npolicies (expert, medium, and random), we encode all the trajectories in the mixed dataset to the\nlatent space. We then perform Principal Component Analysis (PCA) on this set of latents and select\nthe first two principal components. Figure 5a shows us a visualization of this latent space. We see\nthat the VAE has learned to encode the three sets of trajectories to be well separable. Next, we run\nthe same experiment, but now we sample trajectory snippets of length 100 from the dataset instead\nof the full-length (1000) trajectories. Figure 5b shows us the PCA on the encoded latents of these\ntrajectory snippets. We see that the separability is now harder in the latent space. Surprisingly, we\nnoticed that after training our VAE on the snippets, the decoded policies from randomly snipped\ntrajectories were still faithfully behaving like their original policies. We believe that this is because\nof the cyclic nature of the halfcheetah task, and all trajectory snippets have enough information to\nindicate its source policy.\nTo validate this hypothesis, we analyze our method on trajectory snippets for non-cyclic tasks. We\nchoose the MT10 suite of tasks in Metaworld (Yu et al., 2020). We utilize the hand-crafted expert"}, {"title": "4.3 VAE DECODER SIZE ABLATION", "content": "As noted in subsection 4.2, the size of the hy-\npernetwork decoder influences the quality of\ndecoded policies for the MT10 task suite, when\ntrained on trajectory snippets. Here we conduct\nan ablation on the decoder size, evaluating the average success rate of decoded policies across all\nMT10 tasks. Figure 7 illustrates the performance of decoders with varying sizes, denoted as xs\n(3.9M parameters), s (7.8M parameters), m (15.6M parameters), and l (31.2M parameters). It's\nimportant to note that despite the substantial parameter count of the hypernetwork decoder, the re-\nsulting inferred policy remains relatively small (< 100K parameters, see Table 2). The results\ndemonstrate that increasing the decoder size consistently improves the average success rate of the\ndecoded policies."}, {"title": "4.4 METAWORLD MT10 SIZE BENCHMARK", "content": "In the previous experiments, we have shown that\nLWD can learn a distribution of policies from a tra-\njectory dataset. However, another common use case\nfor robotic learning is multi-task imitation learning.\nIn this experiment, we study the ability of LWD to\nlearn a task conditional distribution of policies. We\ntest the performance of LWD on the Metaworld\nMT10 benchmark and compare it to a vanilla multi-\ntask MLP policy trained on the same dataset. The\nbaselines are three MLP policies, each with 5 hid-\nden layers of the same size (512, 256 and 128 per-\nceptions). As seen in Table 2, LWD outperforms the\nvanilla multi-task policy in the average success rate\nacross all tasks. We also see that the policy gener-\nated by LWD has a smaller parameter count than the\nvanilla multi-task policy, with about 1/18th the number of parameters."}, {"title": "4.5 EFFECT OF ACTION HORIZON", "content": "The previous experiments have shown that LWD can learn a distribution of behaviors for a task or a\ndistribution of policies for a set of tasks. All these experiments were conducted with a single policy\nbeing generated at the start of the episode. However, for environments where the training dataset\nhas a high variance, it is important to diffuse out locally optimal policies. That is, we need to diffuse\na policy out every Ha steps, where Ha is the action horizon. This is what we observe for Diffusion\nPolicy (Chi et al., 2024) as well. We use their PushT task, whose dataset has fewer trajectories\nwith a higher variance. We compare the performance of LWD to Diffusion Policy on this task.\nSpecifically, we compare the performance change as we change the action horizon from 8 to 128,\nrelative to the best value. We now condition our latent diffusion model on the current state. We see\nin Figure 8 that Diffusion Policy had the best performance at a horizon of 16, whereas LWD had the\nbest performance at a horizon of 8. But note that as the action horizon increases, the performance of\nDiffusion Policy degrades much faster than LWD . Therefore, for a relative performance reduction\nof ~ 25%, LWD requires 1/4th the number of expensive diffusion model queries."}, {"title": "5 LIMITATIONS AND FUTURE WORK", "content": "Although LWD has promising results, it has some limitations and avenues for future work.\nLWD needs a lot of trajectory data to learn the distribution of policies accurately. Moreover, by\nvirtue of generating closed-loop policies, LWD is more prone to see out-of-distribution states when\ncompared to methods that diffuse multi-step trajectories. These limitations serve as potential di-\nrections for future work. For example, it would be interesting to investigate methods to learn the\ndistribution of policies from fewer demonstration trajectories. Another avenue is to use our method\nwith a hypernetwork that can output weights for a transformer network, to take sequence data as\ninput, or a vision transformer network, to take image data as input."}, {"title": "6 CONCLUSION", "content": "We present LWD, a method to learn a distribution of policies from a heterogeneous set of demon-\nstration trajectories. We first embed trajectories into a latent space and then learn the distribution of\npolicies in this latent space. We then decode these latents to generate policies using a hypernetwork\ndecoder. We show that LWD can reproduce the original policies present in the demonstration tra-\njectories, in two cases. First, we show that we can reproduce multiple behaviors for the same task.\nWe also show that we can reproduce policies used for multi-task learning. Finally, we discuss how\nLWD can be used with high-variance data, and compare it to baselines. We believe that LWD can be\na useful tool for learning from demonstration, and can be used in a variety of applications, including\nrobotics, reinforcement learning, and imitation learning."}]}