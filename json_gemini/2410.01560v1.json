{"title": "OpenMathInstruct-2: Accelerating AI for Math with Massive Open-Source Instruction Data", "authors": ["Shubham Toshniwal", "Wei Du", "Ivan Moshkov", "Branislav Kisacanin", "Alexan Ayrapetyan", "Igor Gitman"], "abstract": "Mathematical reasoning continues to be a critical challenge in large language model (LLM) development with significant interest. However, most of the cutting-edge progress in mathematical reasoning with LLMs has become closed-source due to lack of access to training data. This lack of data access limits researchers from understanding the impact of different choices for synthesizing and utilizing the data. With the goal of creating a high-quality finetuning (SFT) dataset for math reasoning, we conduct careful ablation experiments on data synthesis using the recently released Llama3.1 family of models. Our experiments show that: (a) solution format matters, with excessively verbose solutions proving detrimental to SFT performance, (b) data generated by a strong teacher outperforms on-policy data generated by a weak student model, (c) SFT is robust to low-quality solutions, allowing for imprecise data filtering, and (d) question diversity is crucial for achieving data scaling gains. Based on these insights, we create the OpenMathInstruct-2 dataset which consists of 14M question-solution pairs (\u2248 600K unique questions), making it nearly eight times larger than the previous largest open-source math reasoning dataset. Finetuning the Llama-3.1-8B-Base using OpenMathInstruct-2 outperforms Llama3.1-8B-Instruct on MATH by an absolute 15.9% (51.9% \u2192 67.8%). Finally, to accelerate the open-source efforts, we release the code, the finetuned models, and the OpenMathInstruct-2 dataset under a commercially permissive license.", "sections": [{"title": "1. Introduction", "content": "Synthetic data has emerged as a key technique for building large language models due to its cost-effectiveness and scalability. In particular, synthetic data is well suited for mathematical reasoning where the performance improvements with synthetic data scaling are yet to saturate. However, access to this progress is limited because the current largest math datasets remain closed-source. The closed nature of these datasets introduces two major issues. First, concerns over data leakage erode trust in reported benchmark results. E.g., Zhang et al. show a drop of more than 10% for popular LLMs on an unpublished test set which is distributionally similar to the popular grade school math benchmark GSM8K. Second, it prevents practitioners from fully understanding the impact of data composition and algorithmic choices.\nAmong open-source alternatives, the recent NuminaMath dataset has the largest collection of questions collected from diverse sources. However, its restrictive license\u2014likely due to the use of GPT-40 in data processing and synthesis-limits its broader use. Similarly, other popular math instruction tuning datasets, such as MetaMathQA and MathInstruct, have also utilized GPT models for data synthesis, which prohibits their usage in non-commercial settings. A notable exception is the OpenMathInstruct-1 dataset, one of the biggest open-source math reasoning datasets, where solutions are synthesized using open-weight models. However, OpenMathInstruct-1 has two key limitations. Firstly, its question diversity is limited, since all the questions in the dataset are drawn from the training sets of MATH and GSM8K. Secondly, at the time of its release, there was a sizable gap in the math reasoning capabilities of open and closed-source models. As a result, the dataset underrepresents more challenging problems compared to its GPT-based counterparts.\nThe recent emergence of frontier open-weight models has made it possible to create high-quality, commercially permissible math reasoning datasets. In this paper, we use the recently released Llama3.1 family of models to generate synthetic math instruction tuning (SFT) data, and evaluate the quality of the math reasoning data by finetuning the smaller 8B and 70B base models. To create OpenMathInstruct-2, we conduct careful ablation studies using the MATH dataset to determine design choices that impact the final SFT performance. The highlights of our findings include:"}, {"title": "2. Data: Solution Augmentation", "content": "In this section, we focus on the Solution Augmentation part of the OpenMathInstruct-2 construction pipeline, shown in Figure 3. We first give a brief overview of how solutions are synthesized for existing questions, and then present ablation studies designed to understand the impact of the different dataset design choices."}, {"title": "2.1. Solution Augmentation Preliminaries", "content": "Let $X = \\{(q_i, a_i)\\}_{i=1}^I$ represent a typical mathematical reasoning dataset, where $q_i$ and $a_i$ denote the $i$th question and answer respectively. To synthesize solutions for this dataset, a teacher LLM $M$ is prompted as follows:\n$I(q_1, s_1), ..., (q_K, s_K), q'$\nwhere $I$ represents the instruction to answer the given math question, $\\{q_1, ..., q_K\\}$ represent $K$ questions representative of the dataset, $\\{s_1, ..., s_k\\}$ represent their respective solutions, and $q'$ represents a question from the training set. Given this prompt, multiple candidate solutions are sampled using $M$. The high-quality solutions, usually those that lead to the correct answer, along with the prompt question $q'$, are added to the SFT dataset."}, {"title": "2.2. Ablation Studies", "content": "In the previous section, we gave an abstract overview of the solution augmentation pipeline. In practice, several design decisions impact the final SFT dataset, such as the solution format of the few-shot examples $\\{s_1,..., s_K\\}$, the choice of the teacher model $M$, and the solution filtering mechanism. In this section, we study the impact of these different design choices on the SFT performance to guide the dataset construction.\nFor these ablation experiments, we use the 1K validation split created from MATH training set by Toshniwal et al. The remaining 6.5K MATH training set problems are used to create the SFT dataset. The solutions are generated using nucleus sampling with a temperature of 1.0 and top-p of 0.95. The Llama3.1-8B-Base model is used as the student model in all the ablation experiments. For SFT, the model is trained for 4 epochs, with a batch size of 256, using the AdamW optimizer with a constant learning rate of 5e-6 and a weight decay of le-2. To account for the variance in performance across runs, we report the performance averaged across 4 runs.\nData Downsampling For efficiency or experiment design reasons, we sometimes need to downsize an SFT dataset to a specific size or to match another SFT dataset in ablation experiments. We introduce the concept of coverage and the two downsampling operations used in the paper.\nCoverage of a SFT dataset $D = \\{(q_i, s_i)\\}_{i=1}^I$ synthesized using dataset $X = \\{(q_i, a_i)\\}_{i=1}^I$ is the fraction of questions in $X$ with at least one solution in $D$:\n$Coverage(D, X) = \\frac{\\{q : (q, s) \\in D\\}}{\\{q : (q, a) \\in X\\}}$\nFair Downsampling is a question-dependent downsampling method introduced by Toshniwal et al. Due to the varying difficulty of questions, the representation of \"easier\" ones can often dominate an SFT dataset, as generating high-quality solutions for them is \"easier\". The goal of fair downsampling is to sample question-solution pairs from the original SFT dataset in a way that ensures all questions are as equally represented in the downsampled dataset as possible.\nMatching Coverage: The different design choices explored in the ablation studies result in SFT datasets of varying sizes. However, to compare the quality of the datasets, we want to control for the dataset size. To this end, we introduce the Matching Coverage operation, where SFT datasets are matched at the level of questions. Put simply, after matching coverage, the number of unique questions as well as the number of solutions for each individual question in two dataset is the same."}, {"title": "2.2.1. Solution Format", "content": "Finetuning with synthetic chain-of-thought (CoT) solutions has been the key to strong performances of small models on math reasoning tasks. We find the Llama's CoT format to be quite verbose, and propose an alternate CoT format, OpenMath CoT, which is detailed as well but less verbose. Figure 4 shows a sample solution in the two CoT formats.\nTo compare the two CoT formats, we generate SFT data using the Llama3.1-405B-Instruct model. For generating solutions in the Llama CoT format we simply use the zero-shot prompt setup as the model was trained on those kinds of solutions. However, even when prompting the model with few-shot OpenMath CoT solutions, a substantial number of generations - 57% in our experiment - still follow the Llama CoT format. This tendency of aligned models reverting to their trained behavior when encountering inputs seen during training has also been observed in prior work. We find an interesting workaround to this issue by dropping the special tokens used by Llama-Instruct models. Prompting the model with"}, {"title": "2.2.2. Choice of Teacher Model", "content": "Prior work has shown that with repeated sampling, even weak models can match or outperform much stronger/bigger models. In fact, for a fixed compute budget, a weaker model can be a better choice for a teacher model. But data synthesis is a one-time expense and a small portion of the overall compute budget of training LLMs. We instead ask the following question: Can a student model learn better from its own generated solutions us solutions generated by a strong teacher model when matching the SFT data coverage?\nIn this ablation, we compare Llama3.1-8B-Base and Llama3.1-405B-Instruct as teacher models. We sample solutions using the two models and perform the Matching Coverage operation to match the final SFT datasets precisely. The SFT results show that even when controlling for the SFT data size, Llama3.1-405B-Instruct is a far superior data generation model. Our preliminary analysis suggests that the reason is weaker models generate more noisy solutions that use incorrect reasoning yet end up with the right answer and, ultimately, part of the SFT dataset (Appendix B). We leave a more detailed analysis regarding this for future work. Next, we investigate the impact of these noisy solutions among solutions generated by Llama3.1-405B-Instruct."}, {"title": "2.2.3. Impact of Low-Quality Solutions", "content": "Data quality plays an important role in the accuracy of LLMs. We explore the impact of data quality on the final SFT performance in our setup. First, we employ automated LLM-based methods to filter out solutions that, despite reaching the correct answer, use incorrect reasoning. Second, we investigate the effects of intentionally incorporating incorrect solutions into the SFT dataset.\nRemoving Low-Quality Solutions. Synthetic solutions produced in our pipeline may include examples where the intermediate steps are incorrect, yet still lead to the right final answer. For simplicity, we refer to these instances as \u201clow-quality\" data. In this section, we will discuss how we identify and remove low-quality data, followed by an investigation into its impact on the SFT performance.\nWe employ two methods to identify low-quality data: LLM-as-a-Judge and reward model. In the LLM-as-a-Judge approach, we design two prompts for the Llama3.1-405B-Instruct to determine whether the generated solutions contain incorrect intermediate steps, providing a binary outcome. For the reward model labeling method, we use Nemotron-4-340B-Reward to evaluate the quality of the generated solutions based on factors like helpfulness (the overall usefulness of the response to the prompt) and correctness (the inclusion of all relevant facts without errors). Helpfulness and correctness are rated on a scale from 0 to 4, where a higher score indicates better data quality. For the reward model filtering, we used a threshold of 3 based on small-scale tuning experiments."}, {"title": "2.2.4. Impact of Question Diversity", "content": "To investigate the impact of question diversity on SFT performance, we construct finetuning datasets with 256K question-solution pairs with the number of unique questions varying from {1K, 2K, 4K, 6.5K}. Figure 6 shows a clear trend that the SFT performance improves with an increase in the number of unique questions, with a drop of more than 10 points when the number of unique questions is limited to 1K. This result highlights the potential of generating new questions, and we describe the Question-Solution Augmentation pipeline next."}, {"title": "3. Data: Question-Solution Augmentation", "content": "In this section, we describe the Question-Solution Augmentation component of the OpenMathInstruct-2 construction pipeline, illustrated in Figure 3. This process consists of two stages: (i) question augmentation, and (ii) solution augmentation.\nFor question augmentation, we utilize the training splits of MATH and GSM8K as seed datasets to generate new questions. We use simple few-shot prompting showing 5 examples of original questions and the new questions written by us that are similar in some aspect. We do not add explicit instructions to increase difficulty or add new conditions, instead relying on the inherent variance of the nucleus sampling that we use to generate new problems. After filtering out syntactically ill-formed questions, we check the generated questions for potential contamination with test sets of evaluation benchmarks, described in detail in the next section. To generate solutions for the new synthesized questions, we use the solution augmentation pipeline from Section 2.1, generating 32 solutions for each question with a temperature of 0.7. Since the newly synthesized questions don't have ground-truth answers to filter solutions, we instead use majority voting among the 32 generations as a proxy for the ground-truth answer. For more details on question-solution augmentation, see Appendix C."}, {"title": "3.1. LLM Decontamination", "content": "It has been noted that many widely used benchmarks and datasets suffer from data contamination, where information from the test set unintentionally leaks into the training data. This can result in an overly optimistic assessment of the model's performance. The most commonly used methods, such as n-gram overlap and embedding similarity search, are susceptible to simple variations in test data (e.g., paraphrasing, translation), allowing rephrased samples to bypass these basic detection techniques easily.\nWe adopt the approach suggested by Yang et al. to remove all potential paraphrases of evaluation benchmark questions from the synthesized questions. In our setup, we use the test sets of four evaluation benchmarks, namely GSM8K , MATH , AMC 2023 , and AIME 2024 .\nThe LLM-based decontamination process consists of two main steps. First, for each synthesized question, use embedding similarity search to identify the top-k most similar test examples from all benchmark datasets. Second, create question pairs by matching the synthesized question with each of these top-k test examples. An advanced LLM then evaluates whether any of these pairs are paraphrases via zero-shot prompting. To mitigate any positional bias, we generate two pairs for each match: one in which the synthesized question appears first and another in which the test set question is presented first. If any of the 2k pair is determined to be a paraphrase, the synthesized question is removed.\nWe use a popular Sentence Transformer model for embedding, and Llama3.1-405B-Instruct for paraphrase detection (details on the prompt are provided in Appendix D.4). In our experiment, we use k = 5, which results in 10 LLM inference calls for each generated question. To emphasize the importance of using an LLM in the decontamination pipeline, we provide multiple examples of questions flagged as contaminated that cannot be found via n-gram matching (see Table 10 in the Appendix). Overall, our decontamination pipeline removes about 50K questions out of the 569K new questions synthesized (569K \u2192 519K)."}, {"title": "4. Results", "content": "Training Details. All the models are trained with a batch size of 512, using the AdamW optimizer with a constant learning rate of 2e-5 and a weight decay of le-2. For the 8B model, we train the model on 1M, 2M, and 5M fair downsampled versions of OpenMathInstruct-2 to understand the impact of the data scaling. Due to computational constraints, we train the 70B model only on the 5M subset with a learning rate of le-5. The models are trained for 2 epochs, and we save 6 equally spaced checkpoints during the training runs, which are averaged to create the final model (See Appendix A.4 for performance gains with checkpoint averaging).\nEvaluation Details. We evaluate our models on a set of common benchmarks that consists of GSM8K (1.3K examples), MATH (5K examples), AMC 2023"}, {"title": "5. Related Work", "content": "In recent years, significant progress has been made in developing datasets to enhance mathematical reasoning abilities of LLMs. NuminaMath contains a collection of 860K pairs of competition-level math problems and solutions, annotated with chain-of-thought traces. Skywork-MathQA collects 2.5M question-solution pairs, incorporating three different augmentation techniques and a diverse seed problem set. MuggleMath is created by complicating and diversifying queries, as well as sampling multiple reasoning paths from existing datasets. MetaMathQA introduced a dataset with 395K entries created by bootstrapping questions from MATH and GSM8K, employing techniques such as semantic rephrasing, self-verification, and backward reasoning. MAmmoTH2 introduced a paradigm for efficiently extracting 10 million naturally occurring instruction data points from pre-training web corpora, enhancing LLM reasoning and improving benchmark performance without the need for in-domain training. Li et al. expanded the MATH dataset to 480K and the GSM8K dataset to 960K by generating both questions and CoT-based solutions, resulting in significant accuracy improvements for fine-tuned models.\nTool-integrated methods for math problem-solving have also become prevalent. Chen et al. pioneered the Program of Thoughts (PoT) approach, combining text and programming language statements to arrive at solutions. Building on similar concepts, other datasets have been developed. For instance, OpenMathInstruct-1 introduced a math instruction tuning dataset of 1.8 million examples, synthesizing code-interpreter solutions for GSM8K and MATH benchmarks. InfinityMATH developed a scalable instruction tuning dataset for programmatic mathematical reasoning, consisting of 100K data points.\nSimilar to prior work, we also leverage CoT-based solutions and question augmentation to construct a novel dataset. Yet our approach distinguishes itself in several important ways: (a) we leverage open-weight models instead of proprietary closed-source LLMs allowing us to release the dataset under a permissive license; (b) we offer novel insights into the impact of low-quality data, effectiveness of on-policy training and the design of solution format; (c) we ensure our results are accurate by performing a comprehensive decontamination process using an LLM-based pipeline that can detect rephrased variations of test set questions."}, {"title": "6. Conclusion", "content": "Recent advances in LLM mathematical reasoning have mostly been closed-source since instruction tuning data is often not shared or has restrictive license. In this paper we contribute towards open-source progress by sharing the OpenMathInstruct-2 dataset and all the code necessary to reproduce our work. Besides releasing high-performing models and data, we also conduct detailed ablations that advance our understanding of how to best construct such datasets. In summary, we show that:\na) Not all chain-of-thought formats are equally effective, and longer solutions are not necessarily better.\nb) On-policy data for SFT is perhaps less effective than previously suspected.\nc) Data filtering has limited utility for math reasoning datasets as models are quite robust to the presence of incorrect solutions during SFT.\nd) Training on a diverse set of questions is crucial, but proper decontamination has to be performed to ensure the benchmark evaluations accurately represent model strengths."}, {"title": "A. Miscellaneous", "content": "When we prompt the Llama3.1-405B-Instruct model with few-shot examples in OpenMath CoT format from Appendix D.1 in tandem with the instruct prompt, shown in Figure 7, almost 57% of the generated solutions are in the Llama CoT format on which the model is most likely trained on. We find that dropping the Llama special tokens for marking roles in the prompt, as shown in Figure 8, results in much better adherence to our proposed few-shot prompt with only 0.1% generations in the Llama CoT format.\nWe remove or modify solutions based on the following criteria:\n\\begin{itemize}\n    \\item Remove solutions with multiple \\boxed entries.\n    \\item Remove prefix My Solution: from solutions.\n    \\item Truncate the solution till the first sentence with \\boxed.\n    \\item Remove incorrect arithmetic calculations.\n    \\item Split complex arithmetic calculations to step-by-step calculations to make it easier for the model to generate.\n    \\item Remove solutions longer than 1024 Llama3.1 tokens.\n    \\item Remove solutions with less than 200 characters.\n\\end{itemize}"}, {"title": "A.3. Composition of OpenMathInstruct-2", "content": "Table 5 represents the composition of OpenMathInstruct-2. The dataset consists of about 592K new synthetically-generated questions which contribute about 11M new question-solution pairs."}, {"title": "A.4. Checkpoint Averaging", "content": "We have found consistent gains in our setup with checkpoint averaging. Figure 9 shows a gain of more than 2% for one of our ablation runs when the final checkpoint is created using the average of the last 4 checkpoints in comparison to using only the last checkpoint."}]}