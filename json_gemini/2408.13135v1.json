{"title": "Deep Learning at the Intersection: Certified Robustness as a Tool for 3D Vision", "authors": ["Gabriel P\u00e9rez S.", "Juan C. P\u00e9rez", "Motasem Alfarra", "Jes\u00fas Zarzar", "Sara Rojas", "Bernard Ghanem", "Pablo Arbel\u00e1ez"], "abstract": "This paper presents preliminary work on a novel connection between certified robustness in machine learning and the modeling of 3D objects. We highlight an intriguing link between the Maximal Certified Radius (MCR) of a classifier representing a space's occupancy and the space's Signed Distance Function (SDF). Leveraging this relationship, we propose to use the certification method of randomized smoothing (RS) to compute SDFs. Since RS' high computational cost prevents its practical usage as a way to compute SDFs, we propose an algorithm to efficiently run RS in low-dimensional applications, such as 3D space, by expressing RS' fundamental operations as Gaussian smoothing on pre-computed voxel grids. Our approach offers an innovative and practical tool to compute SDFs, validated through proof-of-concept experiments in novel view synthesis. This paper bridges two previously disparate areas of machine learning, opening new avenues for further exploration and potential cross-domain advancements.", "sections": [{"title": "1. Introduction", "content": "This paper explores preliminary observations on a connection between certified robustness and 3D object modeling in machine learning. Certified robustness studies guarantees about model prediction stability under input variations [13, 2], while 3D object modeling focuses on computational representations like meshes, voxel grids [16], and signed distance functions (SDFs) [3, 11].\nOur work centers on the link we observe between SDFs and a fundamental concept in certified robustness. In particular, we consider the concept of \"Maximal Certified Radius\" (MCR) [17] for certifying a classifier f at an input x: that is, the radius r of the largest ball, around x, inside of which f's predictions remain constant. Based on that concept, our main theoretical observation is that computing the MCR for a classifier f that represents the space's occupancy is equivalent to computing the space's SDF. We visualize this notion in Figure 1 (left), where we consider\n*These authors contributed equally to this work"}, {"title": "2. Preliminaries: Certified Robustness", "content": "Given the vulnerability of Deep Neural Networks (DNNs) to small imperceptible perturbations known as adversarial attacks [4, 15], several works devised approaches with provable robustness guarantees. That is, given a classifier $f_o : X \\rightarrow R^k$ where X is the input space and k is the number of classes, certified robustness aims at making $f_o$ output a fixed prediction for an $||.||_p$ region around a given x. In other words, we say that $f_o$ is certifiably robust at a given x with radius r > 0 if the following statement is achievable:\n$\\arg \\max f(x) = \\arg \\max f(x + \\delta) \\forall||\\delta|| \\leq r$,\nwhere $f_i(x)$ is the $i^{th}$ element of the vector $f_o(x)$. We note that quantifying the certified radius r is, in general, a very challenging problem for DNNs. However, recently, a tractable approach, known as Randomized Smoothing [2], alleviated this problem via a probabilistic approach."}, {"title": "2.1. Randomized Smoothing", "content": "Randomized Smoothing (RS) [2] is a technique that constructs a smoothed classifier g derived from a base classifier $f_o$. At its core, g assigns an input x the class most likely predicted by the base classifier f when x is subjected to perturbations with isotropic Gaussian noise. More formally, and for a binary classifier $f_o : X \\rightarrow {0,1}^*$, the smoothed classifier is given by:\n$g(x) = arg \\max_{c \\in {0,1}} P_{\\epsilon \\sim N(0,\\sigma^2I)}(f_o(x + \\epsilon) = c)$.\nwhere $\\sigma^2$ controls the trade-off between robustness and accuracy for the smooth classifier. Cohen et al. [2] showed\n*Extension to a larger number of classes follows directly (see [2])."}, {"title": "3. Efficient Randomized Smoothing in Low-dimensional Spaces", "content": "Let k \u2208 [0, 1]d be a grid representing the soft occupancy of an object, where $k_i \\in [0, 1]$ represents the probability of the $i^{th}$ voxel being occupied with an object. Then, $f_o(x)$ may continuously represent the soft occupancy of an object by trilinearly interpolating voxel values of k around x. We observe that the certified radius of each voxel, the distance for the predicted class to flip, represents the Signed Distance Function (SDF). Our main aim is to leverage RS to compute the SDF. However, conducting the Monte-Carlo approach from [2] on each voxel results in intractable computation.\nTo that end, we leverage the equivalency between subjecting the input of the base classifier to isotropic Gaussian noise, and convolving the output with a Gaussian distribution [14]. That is, one can rewrite the smooth classifier as: $g(x) = arg \\max f(x)$ with\n$\\hat{f}(x) = (f_o * N(0, \\sigma^2I)) (x)$,\nwhere * denotes the convolution operator.\nWhile such computation is impractical for classification problems, we unlock its potential in low-dimensional spaces. In particular, we note that $\\hat{f}(x)$ can be efficiently approximated via inexpensive Gaussian smoothing on a voxel grid discretizing space, which is tractable for low-dimensional spaces. This formulation allows efficient calculation of certified radii via RS, offering a promising avenue for applying to new domains."}, {"title": "4. SDF as Certified Radius: Applications in 3D Vision", "content": "Our key insight is the equivalence between a space's Signed Distance Function (SDF) and the Maximal Certified Radius (MCR) of the space's occupancy function. In particular, we note that computing the SDF value at a point in space is equivalent to computing the MCR of the occupancy function at that same point. Formally, for any given x in a space whose occupancy is described by the occupancy function $f_{occ}$, i.e. a binary classifier, the following holds:\n$SDF(x) = MCR(f_{occ}, x)$,"}, {"title": "4.1. Novel View Synthesis", "content": "An important application in 3D computer vision which can benefit from the use of SDFs is novel view synthesis. This task consists on predicting novel views of a scene from a set of posed images of the scene. NeRFs [9] successfully tackled this task by learning two fields in 3D: a radiance field $L_o(x, d;\\theta): R^3 \u00d7 R^2 \\rightarrow R^3$, representing outgoing radiance in each point x in direction d, and a density field $\\sigma(x;\\theta) : R^3 \\rightarrow R$ which captures the scene's geometry. Given these functions, one can leverage the volume rendering integral\n$\\hat{C}(r; \\theta) = \\int_{t_n}^{t_f} T(t) \\sigma(r(t)) L_o(r(t), d) dt$,\nto compute pixel colors C'(r) along rays r(t) in space. The parameterized radiance and density functions can be trained by minimizing a reconstruction loss measuring dissimilarity between rendered ray values and original image pixels. Please refer to [9] for further details.\nWe demonstrate an application of our method by generating a density field from the occupancy we learn, and incorporating this density within a popular framework for novel view synthesis [10]. Our rendering pipeline works as follows:\n1.  Learning a Voxel Grid: We train a voxel grid, representing a field, which we denote as $f_e$. By design, the values of $f_e$ are clamped between 0 and 1.\n2.  Gaussian Smoothing: We perform Gaussian smoothing with a kernel of standard deviation \u03c3 to derive $\\hat{f}$. Given the constraints on $f_e$, $\\hat{f}$ invariably retains values within the 0 to 1 range.\n3.  Deriving the weak SDF: At any point x of interest, a weak Signed Distance Function (SDF) can be computed through the equation:\n$SDF(x) = \\sigma \\times \\Phi^{-1}(\\hat{f}(x))$\nwhere \u03a6 denotes the CDF of the Gaussian distribution.\n4.  Achieving Hard Classification for Occupancy: Direct extraction of a hard version from $\\hat{f}$ would involve the argmax operation. However, to allow for back-propagation, we approximate the argmax operator with a high-eccentricity sigmoid function\n$G(x) = occupancy(x) = sigmoid (\\alpha \\cdot (\\hat{f}(x) - 1/2))$.\n5.  Mapping to Density: Rendering requires density, which we model as a (monotonically increasing) transformation of occupancy via a differentiable function h:\n$g(x) = density(x) = -30 \\cdot ln(1 + \\epsilon \u2013 G(x))"}, {"title": "5. Experiments & Results", "content": "5.1. Setup and Implementation Details\nDataset We demonstrate our approach using synthetic scenes from NeRF [9]. This dataset consists of 8 synthetic scenes with 100 training images rendered from various poses around the object.\nImplementation We build on top of Nerfacc's [6] PyTorch code base [12] implementation of I-NGP [10] with its default parameters. We set the Gaussian smoothing's \u03c3 to 1.1, sigmoid's \u03b1 to 19, use 10k optimization iterations for I-NGP, 5k iterations for fine-tuning the voxel grid, and 500 optimization iterations for the final fine-tuning. We test three resolutions for the discrete learned representation F: 1923, 2563 and 3843.\n3D object extraction We extract object meshes by employing marching cubes on top of the weak SDF our method generates. In our experiments, we run marching cubes (MC) searching for a slightly negative isosurface of SDF = -0.1, with the purpose of compensating for locally sharp irregularities in points with certified radii of exactly 0. Comparison to I-NGP requires extracting a surface from its density field via MC. To determine a good density threshold for I-NGP, we run MC with threshold values in multiples of 10 from 0 to 100, and report its best-performing numbers."}, {"title": "5.2. Results and Analysis", "content": "We report proof-of-concept experiments demonstrating our method's capability to generate guaranteed weak SDFs while being able to generate renders of competitive quality in novel view synthesis.\nQualitative Results We report qualitative results in Figure 3. We show comparisons between GT test set images, the rendered image with our pipeline, and the depth map of the learned density field. These results show that our method is capable of correctly capturing the scene's geometry as well as generating competitive renders. We highlight that our method's most important capacity is providing inexpensive access to a weak SDF of the scene. We visualize this in subfigures (d), (e) and (f) of Figure 3 with color-mapped transversal cuts of the values computed by running RS via our efficient algorithm from Section 3.\nSDF Quality We report Chamfer distances in Table 3. With respect to this metric, our method displays clear superiority against I-NGP. That is, even after choosing the best-performing density threshold for I-NGP for each scene, the average Chamfer distance is usually at least three times as large as that of our approach.\nRendering Quality Table 1 reports an average PSNR of 30.09 when using a resolution of 384, which represents a drop of 2.51 points of PSNR compared to I-NGP. This drop in rendering quality is to be contrasted with the fact that our method guarantees the output of a weak SDF.\nEfficiency The correlation between resolution and both quality and performance becomes critical, as shown Table 2. Space complexity scales cubically, and time complexity seems to follow a similar trend. Our results show that, when considering training time against rendering quality, our method rapidly achieves diminishing returns."}, {"title": "6. Conclusions", "content": "In summary, our work demonstrates a novel connection between certified robustness and 3D object modeling, leading to an efficient algorithm for computing weak SDFs. We showcase the utility of this connection by employing it for the task of novel view synthesis where it guarantees learning a weak SDF while maintaining rendering quality. We anticipate that this synergy between robustness and geometry will drive further advancements in machine learning, computer graphics, and related domains, inspiring researchers to explore the untapped potential at this intersection."}]}