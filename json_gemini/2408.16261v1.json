{"title": "Evaluating Time-Series Training Dataset through Lens of Spectrum in Deep State Space Models", "authors": ["Sekitoshi Kanai", "Yasutoshi Ida", "Kazuki Adachi", "Mihiro Uchida", "Tsukasa Yoshida", "Shin'ya Yamaguchi"], "abstract": "This study investigates a method to evaluate time-series datasets in terms of the performance of deep neural networks (DNNs) with state space models (deep SSMs) trained on the dataset. SSMs have attracted attention as components inside DNNs to address time-series data. Since deep SSMs have powerful representation capacities, training datasets play a crucial role in solving a new task. However, the effectiveness of training datasets cannot be known until deep SSMs are actually trained on them. This can increase the cost of data collection for new tasks, as a trial-and-error process of data collection and time-consuming training are needed to achieve the necessary performance. To advance the practical use of deep SSMs, the metric of datasets to estimate the performance early in the training can be one key element. To this end, we introduce the concept of data evaluation methods used in system identification. In system identification of linear dynamical systems, the effectiveness of datasets is evaluated by using the spectrum of input signals. We introduce this concept to deep SSMs, which are nonlinear dynamical systems. We propose the K-spectral metric, which is the sum of the top-K spectra of signals inside deep SSMs, by focusing on the fact that each layer of a deep SSM can be regarded as a linear dynamical system. Our experiments show that the K-spectral metric has a large absolute value of the correlation coefficient with the performance and can be used to evaluate the quality of training datasets.", "sections": [{"title": "1 Introduction", "content": "Time-series data are ubiquitous in various fields [37], such as health-care [11, 40], industrial IoT [16, 19], and finance [31]. To analyze time-series data, machine learning methods continue to be studied and explored [4, 15, 30], and recent deep neural network (DNN)-based methods have enabled us to analyze complicated time-series data [16, 25, 31]. Especially, DNNs with structured state space models (SSMs), e.g., S4 [9] and S5 [33], have attracted much attention because they can address long-term dependencies [7, 8]. However, DNNs with SSMs (deep SSMs) require a large data sample size, which can be a bottleneck in practical data analysis applications. Moreover, when encountering a new task, we do not know whether a prepared dataset has sufficient information to solve the task. We can determine whether the dataset is effective to solve the given task only after training. Therefore, data scientists often need to collect data and train iteratively until a satisfactory performance is obtained. In fact, MLOps has a feedback loop from the training process to the data engineering process [12]. The cost of such it-erative trial-and-error runs can be reduced if we can estimate the performance of trained deep SSMs early in the training.\nTo evaluate the effectiveness of training datasets, one candidate metric is the data sample size. Rosenfeld et al. [29] have presented fitting a power law function to show the relation between the performance and data sample size [2]. Mahmood et al. [18] have presented fitting more general functions. Another candidate is validation loss at the first few epochs. However, since these approaches implicitly assume that the information about the tasks is uniformly distributed over data samples, they do not necessarily evaluate the effectiveness of real-world training datasets precisely. Real-world data can lack specific data samples due to bias in the data collection process. For example, sensor data collection of running chemical plants is difficult to include all reachable states of the plant because there are plant-friendly constraints, e.g., minimizing variability in product quality [26]. Additionally, simply collecting and integrating all of data may have a negative affect on model training in some cases [27]. Thus, methods need to be developed that can evaluate various training datasets including biased datasets.\nIn this paper, we propose a new metric called the K-spectral metric that correlates with the test performance of deep SSMs trained on various training datasets on the basis of the concept in lin-ear system identification. To evaluate the effectiveness of training datasets, we introduce the concepts of the optimal input design and Persistence of Excitation (PE) in system identification [17]. In system identification, we need to collect training datasets to build a model of a target physical system by applying input signals to the system and observed output signals. The optimal input design explores the input signal to minimize the estimation errors of parameters. The optimality is determined by the spectrum rather than the shape of input signals when the target system is a linear dynamical system. Roughly speaking, the optimal input signals have a large magnitude on the sensitive frequency area of the system [17, 20, 28]. If there is no a priori knowledge about target systems, PE becomes the metric of the informativeness of training data [17]. The PE condition cor-responds to the number of frequency components of input signals, and it should be large enough for identifying higher-order systems. Though the spectrum is an important metric for linear systems as above, it is not obvious whether it is also a useful metric for training datasets of deep SSMs, which are nonlinear systems. Since deep SSMs have linear systems inside the model architecture, we investigate the following questions:\n\u2022 Do the frequency components of intermediate signals before SSMs represent the effectiveness of training data when used in DNNs?\n\u2022 If so, how can we use them to evaluate training datasets?\nTo answer these questions, we empirically investigate the rela-tionship between the performance and the frequency components. Specifically, we evaluate the sum of top-K magnitudes of frequency components of intermediate signals that are applied to SSMs in deep SSMs on the basis of the concept of optimal input design and PE (Fig. 1). We name this metric as the K-spectral metric and ex-perimentally show that it can evaluate the effectiveness of training datasets on nonlinear system identification, classification, and fore-casting problems of time-series data. The main contributions of this paper are as follows:\n\u2022 We propose K-spectral metric, a performance-correlated met-ric that can evaluate the effectiveness of training datasets. Its correlation coefficients are larger than those of the data sample size and validation loss when we compare training datasets that lack data samples uniformly and biasedly.\n\u2022 We empirically reveal that the spectrum of intermediate signals of deep SSMs (i.e., the K-spectral metric) is important for solv-ing complex tasks although the whole model architecture is a nonlinear dynamical system.\n\u2022 Our experiments reveal that a flatter spectrum of the interme-diate signals is required for deep SSMs to achieve good perfor-mance early in the training. After the middle of the training, the spectrum should be the specific shape needed for the problem."}, {"title": "2 Preliminaries", "content": "After Gu et al. [7] have revealed that certain SSMs called HiPPO address the long-term dependency, several studies used SSMs in DNNs for time-series data [6, 8, 9, 22, 33, 39]. S4 [9] initializes their parameters to satisfy the HiPPO framework and update them to learn time-series data. Whereas S4 addresses the multiple inputs by using several single-input and single-output (SISO) SSMs, S5 [33] extends S4 to address multiple inputs with the HiPPO initialization by using only one multiple-input and multiple-output (MIMO) SSM.\nLet $u\\in \\mathbb{R}$, $y \\in \\mathbb{R}$, and $x \\in \\mathbb{R}^{d}$ be an input, output, and state vector of the l-th layer at a discrete time-step t, respectively. A linear time-invariant SISO SSM is written as:\n$x_{t}^{l} = Ax_{t-1}^{l} + bu_{t-1}^{l}$  (1)\n$y_{t}^{l} = c^{T}x_{t}^{l} + Du_{t}^{l}$, (2)\nwhere $A^{l} \\in \\mathbb{R}^{d \\times d}$, $b^{l} \\in \\mathbb{R}^{d \\times 1}$, $c^{l} \\in \\mathbb{R}^{d \\times 1}$, and $D^{l} \\in \\mathbb{R}$ are parameters. The input $u_{t}^{l}$ of the l-th layer is the output of the $(l-1)$-th layer after an activation function $\\phi$: $u_{t}^{l} = \\phi(y_{t-1}^{l-1})$. $\\phi(y_{t-1}^{l-1})$ is generally a vector, and thus, input can also be a vector: $u_{t}^{l} = [u_{t}^{l,1},...,u_{t}^{l,d_{in}}]^{T}$. S4 con-siders $d_{in}$ SISO SSMs for one layer, and we consider our metric for each element of the input vector independently even when using S5.\nSince a SSM is one of the representations of a linear dynamical system, it can be written by another representation. Let q be shift operator as $qu_{t} = u_{t+1}$ and $q^{-1}u_{t} = u_{t-1}$. A discrete SSM can be written as a discrete transfer function $G_{\\Theta}(q)$:\n$Y_{t} = G_{\\Theta}(q)u_{t}$,  (3)\n$G_{\\Theta}(q) = c^{T}(qI \u2013 A)^{-1}b + D$. (4)\nwhere $\\Theta$ denotes parameters of a system and $\\Theta$ = $[vec(A)^{T}, b^{T}, c^{T}, D]^{T}$ in this case. Similarly, a SSM can be also approximated by a finite impulse response model (FIR):\n$y_{t} \\approx Du_{t} + c^{T}bu_{t-1} + c^{T}Abu_{t-2} + \u00b7\u00b7\u00b7 +c^{T} A^{d'-1}bu_{t-d'}$ (5)\n$= \\sum_{i=0}^{d'-1}\\theta_{i}u_{t-i}$. (6)\nIf the SSM is stable, $\\lim_{d' \\rightarrow \\infty} c^{T} A^{d'} b$ converges to zero. Thus, a SSM can be written by the FIR with sufficient large d'. We use these representations to explain the optimal input design and PE simply."}, {"title": "2.1 State Space Models in DNNs", "content": "After Gu et al. [7] have revealed that certain SSMs called HiPPO address the long-term dependency, several studies used SSMs in DNNs for time-series data [6, 8, 9, 22, 33, 39]. S4 [9] initializes their parameters to satisfy the HiPPO framework and update them to learn time-series data. Whereas S4 addresses the multiple inputs by using several single-input and single-output (SISO) SSMs, S5 [33] extends S4 to address multiple inputs with the HiPPO initialization by using only one multiple-input and multiple-output (MIMO) SSM.\nLet $u\\in \\mathbb{R}$, $y \\in \\mathbb{R}$, and $x \\in \\mathbb{R}^{d}$ be an input, output, and state vector of the l-th layer at a discrete time-step t, respectively. A linear time-invariant SISO SSM is written as:\n$x_{t}^{l} = Ax_{t-1}^{l} + bu_{t-1}^{l}$  (1)\n$y_{t}^{l} = c^{T}x_{t}^{l} + Du_{t}^{l}$, (2)\nwhere $A^{l} \\in \\mathbb{R}^{d \\times d}$, $b^{l} \\in \\mathbb{R}^{d \\times 1}$, $c^{l} \\in \\mathbb{R}^{d \\times 1}$, and $D^{l} \\in \\mathbb{R}$ are parameters. The input $u_{t}^{l}$ of the l-th layer is the output of the $(l-1)$-th layer after an activation function $\\phi$: $u_{t}^{l} = \\phi(y_{t-1}^{l-1})$. $\\phi(y_{t-1}^{l-1})$ is generally a vector, and thus, input can also be a vector: $u_{t}^{l} = [u_{t}^{l,1},...,u_{t}^{l,d_{in}}]^{T}$. S4 con-siders $d_{in}$ SISO SSMs for one layer, and we consider our metric for each element of the input vector independently even when using S5.\nSince a SSM is one of the representations of a linear dynamical system, it can be written by another representation. Let q be shift operator as $qu_{t} = u_{t+1}$ and $q^{-1}u_{t} = u_{t-1}$. A discrete SSM can be written as a discrete transfer function $G_{\\Theta}(q)$:\n$Y_{t} = G_{\\Theta}(q)u_{t}$,  (3)\n$G_{\\Theta}(q) = c^{T}(qI \u2013 A)^{-1}b + D$. (4)\nwhere $\\Theta$ denotes parameters of a system and $\\Theta$ = $[vec(A)^{T}, b^{T}, c^{T}, D]^{T}$ in this case. Similarly, a SSM can be also approximated by a finite impulse response model (FIR):\n$y_{t} \\approx Du_{t} + c^{T}bu_{t-1} + c^{T}Abu_{t-2} + \u00b7\u00b7\u00b7 +c^{T} A^{d'-1}bu_{t-d'}$ (5)\n$= \\sum_{i=0}^{d'-1}\\theta_{i}u_{t-i}$. (6)\nIf the SSM is stable, $\\lim_{d' \\rightarrow \\infty} c^{T} A^{d'} b$ converges to zero. Thus, a SSM can be written by the FIR with sufficient large d'. We use these representations to explain the optimal input design and PE simply."}, {"title": "2.2 Optimal Input Design", "content": "System identification is a research area that builds mathematical models for dynamical systems to control them [17]. In system identi-fication, we apply input signals $u_{t}$ to a physical system and observe output signals $y_{t}$. Then, we estimate parameters of the model from the datasets ${(u_{t}, y_{t})}_{t=0}^{T-1}$. Input signals are designed to obtain accurate models. To minimize estimation errors of $\\Theta$, we need to con-sider the spectrum of input signals rather than their waveforms [17]. The following objective function (A-optimality [1, 21, 28]) is often used to design input signals $u_{0:T-1}$ = $[u_{0}, ..., u_{T-1}]$ as:1\n$u_{0:T-1} = arg \\underset{u_{0:T-1}}{max} tr(M)$, (7)\n$M = \\frac{1}{2\\pi} \\int_{-\\pi}^{\\pi} M(\\omega) d\\omega$, (8)\n$M(\\omega) = Re\\{\\frac{\\partial G_{\\Theta}(e^{j\\omega})}{\\partial\\Theta} [\\frac{\\partial G_{\\Theta}(e^{j\\omega})}{\\partial\\Theta}]^{H}|G_{2}(e^{j\\omega}, \\gamma)|^{-2} \\Phi_{u_{0:T-1}}(\\omega)\\}$, (9)\nwhere $\\omega$, j, and H are angular frequency, imaginary unit, and Hermitian transpose, respectively. $\\Phi_{u_{0:T-1}}$ is the spectral density:\n$\\Phi_{u_{0:T-1}}(\\omega) = \\lim_{T \\rightarrow \\infty} \\frac{1}{T} |U_{T}(\\omega)|^{2}$, (10)\nwhere $U_{T}$ is Fourier transform of $u_{0:T-1}$, which is a continuous-time input signal. The objective function (Eq. (7)) is derived through the Fisher information matrix (Appendix B), which determines the bound of variance of unbiased estimators of $\\Theta$ [20, 28].\nEq. (8) indicates that the optimal input signals should have the spectral density depending on the target system in the frequency domain: M can be regarded as the sensitivity of the frequency response to $\\Theta$ [17]."}, {"title": "2.3 Persistency of Excitation", "content": "PE is another important condition for input signals of system identi-fication problems. In system identification", "17": ".", "example": "identification of a FIR with white Gausssian noise $e_{t"}, "n$y_{t} = \\sum_{i=1}^{d}\\theta_{i}u_{t-i} + e_{t}$. (11)\nWe can estimate $\\Theta$ by using $u_{t}$ and $y_{t}$ for T time steps as:\n$\\Theta = (U_{T}^{T}U_{T})^{-1}U_{T}^{T}Y_{T}$\n$Y_{d+1}$\\\n$y_{T}$\\\n$U_{d-1} U_{d-2} ... U_{1}$\\\n...,U_{T} = \\\\\n$y_{T-1}$  (12)\nTo obtain the unique solution of Eq. (12), the condition of rank $(U_{T}^{T}U_{T})= d$ should be satisfied. This condition corresponds to PE. The PE is generally defined by using a covariance function:\nDefinition 2.1 ([17"]}