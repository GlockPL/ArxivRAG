{"title": "Evaluating Time-Series Training Dataset through Lens of Spectrum in Deep State Space Models", "authors": ["Sekitoshi Kanai", "Yasutoshi Ida", "Kazuki Adachi", "Mihiro Uchida", "Tsukasa Yoshida", "Shin'ya Yamaguchi"], "abstract": "This study investigates a method to evaluate time-series datasets in terms of the performance of deep neural networks (DNNs) with state space models (deep SSMs) trained on the dataset. SSMs have attracted attention as components inside DNNs to address time-series data. Since deep SSMs have powerful representation capacities, training datasets play a crucial role in solving a new task. However, the effectiveness of training datasets cannot be known until deep SSMs are actually trained on them. This can increase the cost of data collection for new tasks, as a trial-and-error process of data collection and time-consuming training are needed to achieve the necessary performance. To advance the practical use of deep SSMs, the metric of datasets to estimate the performance early in the training can be one key element. To this end, we introduce the concept of data evaluation methods used in system identification. In system identification of linear dynamical systems, the effectiveness of datasets is evaluated by using the spectrum of input signals. We introduce this concept to deep SSMs, which are nonlinear dynamical systems. We propose the K-spectral metric, which is the sum of the top-K spectra of signals inside deep SSMs, by focusing on the fact that each layer of a deep SSM can be regarded as a linear dynamical system. Our experiments show that the K-spectral metric has a large absolute value of the correlation coefficient with the performance and can be used to evaluate the quality of training datasets.", "sections": [{"title": "1 Introduction", "content": "Time-series data are ubiquitous in various fields [37], such as health-care [11, 40], industrial IoT [16, 19], and finance [31]. To analyze time-series data, machine learning methods continue to be studied and explored [4, 15, 30], and recent deep neural network (DNN)-based methods have enabled us to analyze complicated time-series data [16, 25, 31]. Especially, DNNs with structured state space models (SSMs), e.g., S4 [9] and S5 [33], have attracted much attention because they can address long-term dependencies [7, 8]. However, DNNs with SSMs (deep SSMs) require a large data sample size, which can be a bottleneck in practical data analysis applications. Moreover, when encountering a new task, we do not know whether a prepared dataset has sufficient information to solve the task. We can determine whether the dataset is effective to solve the given task only after training. Therefore, data scientists often need to collect data and train iteratively until a satisfactory performance is obtained. In fact, MLOps has a feedback loop from the training process to the data engineering process [12]. The cost of such iterative trial-and-error runs can be reduced if we can estimate the performance of trained deep SSMs early in the training.\nTo evaluate the effectiveness of training datasets, one candidate metric is the data sample size. Rosenfeld et al. [29] have presented fitting a power law function to show the relation between the performance and data sample size [2]. Mahmood et al. [18] have presented fitting more general functions. Another candidate is validation loss at the first few epochs. However, since these approaches implicitly assume that the information about the tasks is uniformly distributed over data samples, they do not necessarily evaluate the effectiveness of real-world training datasets precisely. Real-world data can lack specific data samples due to bias in the data collection process. For example, sensor data collection of running chemical plants is difficult to include all reachable states of the plant because there are plant-friendly constraints, e.g., minimizing variability in product quality [26]. Additionally, simply collecting and integrating all of data may have a negative affect on model training in some cases [27]. Thus, methods need to be developed that can evaluate various training datasets including biased datasets.\nIn this paper, we propose a new metric called the K-spectral metric that correlates with the test performance of deep SSMs trained on various training datasets on the basis of the concept in linear system identification. To evaluate the effectiveness of training datasets, we introduce the concepts of the optimal input design and Persistence of Excitation (PE) in system identification [17]. In system identification, we need to collect training datasets to build a model of a target physical system by applying input signals to the system and observed output signals. The optimal input design explores the input signal to minimize the estimation errors of parameters. The optimality is determined by the spectrum rather than the shape of input signals when the target system is a linear dynamical system. Roughly speaking, the optimal input signals have a large magnitude on the sensitive frequency area of the system [17, 20, 28]. If there is no a priori knowledge about target systems, PE becomes the metric of the informativeness of training data [17]. The PE condition corresponds to the number of frequency components of input signals, and it should be large enough for identifying higher-order systems.\nThough the spectrum is an important metric for linear systems as above, it is not obvious whether it is also a useful metric for training datasets of deep SSMs, which are nonlinear systems. Since deep SSMs have linear systems inside the model architecture, we investigate the following questions:\n\u2022 Do the frequency components of intermediate signals before SSMs represent the effectiveness of training data when used in DNNs?\n\u2022 If so, how can we use them to evaluate training datasets?\nTo answer these questions, we empirically investigate the relationship between the performance and the frequency components. Specifically, we evaluate the sum of top-K magnitudes of frequency components of intermediate signals that are applied to SSMs in deep SSMs on the basis of the concept of optimal input design and PE (Fig. 1). We name this metric as the K-spectral metric and experimentally show that it can evaluate the effectiveness of training datasets on nonlinear system identification, classification, and forecasting problems of time-series data. The main contributions of this paper are as follows:\n\u2022 We propose K-spectral metric, a performance-correlated metric that can evaluate the effectiveness of training datasets. Its correlation coefficients are larger than those of the data sample size and validation loss when we compare training datasets that lack data samples uniformly and biasedly.\n\u2022 We empirically reveal that the spectrum of intermediate signals of deep SSMs (i.e., the K-spectral metric) is important for solving complex tasks although the whole model architecture is a nonlinear dynamical system.\n\u2022 Our experiments reveal that a flatter spectrum of the intermediate signals is required for deep SSMs to achieve good performance early in the training. After the middle of the training, the spectrum should be the specific shape needed for the problem."}, {"title": "2 Preliminaries", "content": "2.1 State Space Models in DNNs\nAfter Gu et al. [7] have revealed that certain SSMs called HiPPO address the long-term dependency, several studies used SSMs in DNNs for time-series data [6, 8, 9, 22, 33, 39]. S4 [9] initializes their parameters to satisfy the HiPPO framework and update them to learn time-series data. Whereas S4 addresses the multiple inputs by using several single-input and single-output (SISO) SSMs, S5 [33] extends S4 to address multiple inputs with the HiPPO initialization by using only one multiple-input and multiple-output (MIMO) SSM.\nLet \\(u \\in \\mathbb{R}\\), \\(y \\in \\mathbb{R}\\), and \\(x \\in \\mathbb{R}^d\\) be an input, output, and state vector of the l-th layer at a discrete time-step t, respectively. A linear time-invariant SISO SSM is written as:\n\\(x_t = A x_{t-1} + b u_{t-1}\\) (1)\n\\(y_t = c^T x_t + D u_t\\), (2)\nwhere \\(A^l \\in \\mathbb{R}^{d \\times d}\\), \\(b^l \\in \\mathbb{R}^{d \\times 1}\\), \\(c^l \\in \\mathbb{R}^{d \\times 1}\\), and \\(D^l \\in \\mathbb{R}\\) are parameters.\nThe input \\(u^l\\) of the l-th layer is the output of the (l \u2212 1)-th layer after an activation function \\(\\phi\\): \\(u^l = \\phi(y^{l-1})\\). \\(\\phi(y^{l-1})\\) is generally a vector, and thus, input can also be a vector: \\(u^l = [u_1^l, ..., u_{d_{in}}^l]^T\\). S4 considers \\(d_{in}\\) SISO SSMs for one layer, and we consider our metric for each element of the input vector independently even when using S5.\nSince a SSM is one of the representations of a linear dynamical system, it can be written by another representation. Let q be shift operator as \\(q u_t = u_{t+1}\\) and \\(q^{-1} u_t = u_{t-1}\\). A discrete SSM can be written as a discrete transfer function \\(G_{\\theta}(q)\\):\n\\(y_t = G_{\\theta}(q) u_t\\), (3)\n\\(G_{\\theta}(q) = c^T (qI - A)^{-1} b + D\\). (4)\nwhere \\(\\theta\\) denotes parameters of a system and \\(\\theta = [vec(A)^T, b^T, c^T, D]^T\\) in this case. Similarly, a SSM can be also approximated by a finite impulse response model (FIR):\n\\(y_t \\approx D u_t + c^T b u_{t-1} + c^T A b u_{t-2} + ... + c^T A^{d'-1} b u_{t-d'}\\) (5)\n\\(= \\Theta^T \\Psi_t\\) (6)\nIf the SSM is stable, \\(\\lim_{d' \\rightarrow \\infty} c^T A^{d'} b\\) converges to zero. Thus, a SSM can be written by the FIR with sufficient large \\(d'\\). We use these representations to explain the optimal input design and PE simply."}, {"title": "2.2 Optimal Input Design", "content": "System identification is a research area that builds mathematical models for dynamical systems to control them [17]. In system identification, we apply input signals \\(u_t\\) to a physical system and observe output signals \\(y_t\\). Then, we estimate parameters of the model \\(\\theta\\) from the datasets \\(\\{(u_t, y_t)\\}_{t=0}^{T-1}\\). Input signals are designed to obtain accurate models. To minimize estimation errors of \\(\\theta\\), we need to consider the spectrum of input signals rather than their waveforms [17]. The following objective function (A-optimality [1, 21, 28]) is often used to design input signals \\(u_{0:T-1} = [u_0, ..., u_{T-1}]\\) as:\n\\(u_{0:T-1} = \\arg \\max_{u_{0:T-1}} tr(M)\\), (7)\n\\(M = \\frac{1}{2 \\pi} \\int_{- \\pi}^{\\pi} M(\\omega) d \\omega\\), (8)\n\\(M(\\omega) = Re\\left[\\frac{\\partial G_{\\theta}(e^{j \\omega})}{\\partial \\theta} \\left[\\frac{\\partial G_{\\theta}(e^{j \\omega})}{\\partial \\theta}\\right]^H \\Phi_{u_{0:T-1}}(\\omega)\\right]\\), (9)\nwhere \\(G_{\\theta}(e^{j \\omega})\\) is a rational transfer function parameterized by \\(\\theta\\), which is a representation of the linear system in frequency domain.\nWe consider an identification problem with additive white Gaussian noise \\(e_t\\) as \\(y_t = G_{\\theta}(e^{j \\omega}) u_t + e_t\\)."}, {"title": "2.3 Persistency of Excitation", "content": "PE is another important condition for input signals of system identification problems. In system identification, \\((u_t, y_t)\\) should be informative, i.e., the data allow discrimination between any two different models in a model set [17]. To achieve this, input signals \\(u_t\\) should excite various oscillation modes of systems. The informative input is guaranteed by the metric called PE. To grasp the concept of PE, we explain the PE condition by using a concrete example: identification of a FIR with white Gausssian noise \\(e_t\\)\n\\(y_t = \\sum_{i=1}^{d} \\theta_i u_{t-i} + e_t\\). (11)\nWe can estimate \\(\\theta\\) by using \\(u_t\\) and \\(y_t\\) for T time steps as:\n\\(\\hat{\\theta} = (U_T^T U_T)^{-1} U_T^T Y_T\\), (12)\n\\(U_T = \\begin{bmatrix} u_d & u_{d-1} & ... & u_1 \\\\ u_{d+1} & u_d & ... & u_2 \\\\ ... & ... & ... & ... \\\\ u_T & u_{T-1} & ... & u_{T-d} \\end{bmatrix}, Y_T = \\begin{bmatrix} y_{d+1} \\\\ y_{d+2} \\\\ ... \\\\ y_T \\end{bmatrix}\\)\nTo obtain the unique solution of Eq. (12), the condition of \\(rank(U_T^T U_T)= d\\) should be satisfied. This condition corresponds to PE. The PE is generally defined by using a covariance function:\nDefinition 2.1 ([17]). Let \\(\\overline{u}\\) and \\(r_u(l)\\) be an average and covariance function for input \\(u_t\\) as \\(\\overline{u} = \\lim_{T \\rightarrow \\infty} \\frac{1}{T} \\sum_{t=0}^{T-1} u_t\\) and \\(r_u(l) = \\lim_{T \\rightarrow \\infty} (u_{t+l} - \\overline{u})(u_t - \\overline{u})\\), respectively. The input \\(u_t\\) is persistently exciting of order d if we have \\(r_u(d) > 0\\) where\n\\(r_u(d) = \\begin{bmatrix} r_u(0) & r_u(1) & ... & r_u(d-1) \\\\ r_u(1) & r_u(0) & ... & r_u(d-2) \\\\ ... & ... & ... & ... \\\\ r_u(d-1) & r_u(d-2) & ... & r_u(0) \\end{bmatrix}\\) (13)\nFor instance, one sinusoidal signal satisfies PE of order two, and a sum of m sinusoidal signals satisfies PE of order 2m. White noise has all frequency components and satisfies PE of the infinite order: it satisfies \\(r_u(d) > 0, \\forall d\\). PE also indicates that the spectral information of input signals can be the metric for evaluating datasets."}, {"title": "2.4 Related Work", "content": "Deep SSM. Gu et al. [7] have presented an SSM-based architecture called HiPPO. Since HiPPO captures the dynamics of coefficients for the polynomial series restoring the original time transition function, HiPPO can memorize the information of the original function. After the HiPPO framework has been presented, several studies have presented deep SSMs [8, 9, 33]. S4 [9] outperforms recurrent neural networks and Transformer variants in terms of time-series forecasting [9], anomaly detection [39], audio generation [6] and long-form speech recognition [22]. Smith et al. [33] have extended S4 to address multiple inputs by one MIMO SSM and call this method S5.\nEvaluation of training dataset. Roh et al. [27] identified data evaluation as a future research challenge in data collection for machine learning: how to evaluate whether the right data was collected with sufficient quantity is an open question. The relationship between the dataset size and performance can fit a power law function [2, 29] and more general functions [18]. However, these approaches implicitly assume that the information for the tasks is uniformly distributed over data samples and cannot evaluate the effectiveness of biased training datasets such that specific data samples are not obtained. Gupta et al. [10] have presented a toolkit for assessing various qualities of training datasets, such as class overlap. Since building the toolkit using several metrics is out of our research scope, we do not compare our metric with this toolkit. While Sheng et al. [32] investigates labeling quality and propose repeated labeling, we focus on input data points rather than target labels.\nPE in deep learning. Some studies have applied the concept of PE to deep learning [14, 24, 34, 41] for various purposes. Nar and Sastry [24] and Sridhar et al. [34] have introduced the concept of PE in the dynamics of gradient descent, and Lekang and Lamperski [14] have investigated the PE condition for the rectified linear unit (ReLU) activation. To the best of our knowledge, this is the first study to investigate the relation between the performance of the deep SSMs with the spectrum of intermediate signals."}, {"title": "3 Proposed Metrics", "content": "Deep SSMs require a larger data sample size than traditional models, e.g., ARIMA for time-series data, and the necessary quality of training datasets is not known. We consider the following problem: Let \\(u_{0:T-1}^n = [u_0^n, ..., u_{T-1}^n]\\) and \\(y_{0:T-1}^n = [y_0^n, ..., y_{T-1}^n]\\) be input data point and the target output, respectively. An L-layer deep SSM is trained on training dataset \\(\\{(u_{0:T-1}^n, y_{0:T-1}^n)\\}_{n=1}^N\\) by the objective function \\(\\sum_{n=1}^N L(y_{0:T-1}^n, \\hat{y}_{0:T-1}^n)\\) where \\(\\hat{y}_{0:T-1}^n\\) is output of the deep SSM. Can the training dataset be evaluated with respect to the performance of deep SSMs?\nAs discussed in Sections 2.2 and 2.3, we are able to compare the given training datasets (input and output signals) for system identification of linear systems in terms of optimality and PE before the parameter estimation. However, deep SSMs are generally nonlinear dynamical systems because \\(\\phi\\) contains nonlinear functions. This makes their performance difficult to predict in advance of training. Even so, linear dynamics of SSMs in the models are dominant in the dynamics of S4 and S5 because the state transition of each layer is linear computations. Thus, we hypothesize that the spectrum of the input signal for each intermediate SSM is related to the training performance like the optimal input design and the PE condition.\nHowever, the optimal input design and PE condition are difficult to apply to deep SSMs directly. Since we have no a priori knowledge of the target SSMs, we do not know \\(M\\) and cannot use the objective of optimal input design Eq. (7). Regarding PE, the computation of \\(rank(r_u(d))\\) is numerically unstable and incurs high computation costs. Thus, we consider the alternative computation by using the magnitudes of frequency components."}, {"title": "3.1 K-spectral Metric", "content": "We propose the K-spectral metric \\(R(\\overline{U}_T, K)\\), which is the sum of the top-K magnitudes of FFT of normalized input signals:\n\\(R(\\overline{U}_T, K) = \\sum_{k \\in topk(\\overline{U}_T, K)} |\\overline{U}_k|\\) (14)\nwhere topk(S, K) is the function that outputs the index set that has indices of the largest K elements for S. \\(\\overline{U}_T = [|\\overline{U}_0|, ..., |\\overline{U}_{T-1}|]\\) is the magnitudes of the frequency components as\n\\(\\overline{U}_s = \\sum_{t=0}^{T-1} \\overline{u}_t e^{-j \\omega t}\\), for \\(\\omega = \\frac{2 \\pi s}{T}, s = 0, ..., T-1\\). (15)\n\\(\\overline{u}_t\\) is the normalized intermediate signal \\(\\overline{u}_t = \\frac{u_t}{\\|u_{0:T-1}\\|_2}\\) where \\(u_{0:T-1} = [u_0, ..., u_{T-1}]^T\\). Note that we omit the superscript of l for simplicity. Since a deep SSM can have multiple SSM layers and \\(d_{in}\\) intermediate signals in one layer, we average the K-spectral metric over all intermediate signals as:\n\\(R = \\frac{1}{N_{ssm}} \\sum_{i=1}^{N_{ssm}} R(\\overline{U}_T, K)\\), (16)\nwhere \\(N_{ssm}\\) is the number of input signals for SSMs in a deep SSMs. If \\(L_{ssm}\\) is the number of SSM layers, \\(N_{ssm} = d_{in} L_{ssm}\\). The hyperparameter K is set to d as explained in Section 3.1.2. If R correlates with the test performance, we can use R as the metric of training datasets to estimate whether a training dataset has useful information for the task. R is expected to correlate with performance because of the following two relationships.\n3.1.1 Relationship with Optimal Input Design. The optimal input design suggests that the spectrum \\(\\Phi_{u}(\\omega)\\) for continuous-time signals determines the optimality of input signals, i.e., training datasets, to estimate parameters as Eqs. (7) and (8). On the other hand, the K-spectral metric is based on the spectral density \\(|U_k|\\) for discrete-time signals because SSMs inside DNNs are always discrete-time systems. Although the spectrum for continuous-time signals and the spectrum for discrete-time signals are different, they evaluate the effectiveness of training datasets through the magnitudes of the frequency domain rather than its phase, i.e., specific waveforms.\nThe spectrum of Eq. (8) is weighted by M because system identification often uses a priori knowledge of the target system. On the other hand, the K-spectral metric (Eq. (14)) computes the sum of the spectral density of K points without weighting because we have no a priori knowledge. Thus, a high K-spectral metric indicates that the input signals are optimal for estimating SSMs that have uniformly broad sensitivity M across frequency domains. In other words, if the SSMs should have a uniform sensitivity to solve a task, the K-spectral metric should become high, i.e., the K-spectral metric positively correlates with the performance. On the other hand, if the SSMs should have a non-uniform sensitivity after training, the K-spectral should be low, i.e., the K-spectral metric negatively correlates with the performance. While correlation coefficients can be both positive and negative, our method can be used to estimate the performance because we observed that the absolute values of correlation coefficients are high enough in Section 4.\n3.1.2 Relationship with PE. A sinusoidal signal is persistently exciting of order two, and the sum of m sinusoidal signals satisfies PE of 2m. Conversely, if the input signal contains m sinusoidal signals (i.e., spectrum is nonzero at 2m points) this signal satisfies PE of order 2m. In other words, PE of higher order corresponds to more frequency components in the inputs. If \\(|U_k| > 0\\) for K points in frequency domain, input signals have K frequency components and are expected to satisfy PE of order K. We set K to d since SISO SSMs with d states require input signals satisfying the PE of at least order d. \\(U_s\\) of the real signal is symmetric for s, but we use all of them. The K-spectral metric has the following property:\nTheorem 3.1. \\(R(\\overline{U}_T, K)\\) in Eq. (14) achieves the maximum value \\(R_K = max_{\\overline{U}} R(\\overline{U}_T, K)\\) if and only if \\(|\\overline{U}_i| = |\\overline{U}_j|\\) for all \\(i, j \\in topk(\\overline{U}_T, K)\\), and \\(|\\overline{U}_i| = 0\\) for \\(i \\notin topk(\\overline{U}_T, K)\\).\nThe proof is provided in Appendix A. This theorem indicates that the K-spectral metric has the maximum value if the magnitudes at K points in the frequency domain have the same values and the others are zero. This implies that if input signals are persistently exciting of higher order, our metric \\(R(\\overline{U}_T, K)\\) also becomes higher for sufficiently large K. Since the K-spectral metric measures the informativeness of input signals without the difficulty of rank computation, we use it for evaluating the training dataset quality."}, {"title": "3.2 Implementation of K-spectral Metric", "content": "Since deep SSMs generally contain learnable layers before SSMs, the learned layers affect the spectrum of the intermediate signals to train SSMs. Thus, the training dataset affects the intermediate signals through both the current data sequence and layers learned past data sequences. In other words, even if the intermediate signals are not optimal before training, the training dataset can make layers generate the optimal intermediate signals through the training, which means that the training dataset has sufficient information to solve the task. Thus, our metric is measured with parameter updates."}, {"title": "4 Experiments", "content": "We conducted experiments to evaluate the effectiveness of the K-spectral metric in system identification, classification, and forecasting of time series data. If our metric at the early epoch highly correlates with the test performance after training, it can evaluate the effectiveness of training data before full-training.\nFirst, we investigate the effectiveness of the proposed metric through nonlinear identification problem using S4 in Section 4.2. We generated various input signals for identification since we need to apply the input signals and observe output signals for identification problems as explained in Section 2.2. Next, we solved the classification task and forecasting tasks on public time-series data in Section 4.3. We made various reduced training datasets and evaluated the correlation between the performance and the K-spectral metric. Finally, we investigate behavior of the K-spectral metric in training and the effect of K in Section 4.4.\n4.1 Common setup\nWe set K to the number of states d.\n4.1.1 Models. We used S4 [9] for system identification, classification, and forecasting problems and used S5 [33] for classification problems. For the system identification problem, we used three-layer deep SSMs, which are composed of one SSM layer and two input and output linear layers with SiLU activation functions. The length of state vectors of all layers was set to four. For classification"}, {"title": "4.2 System Identification", "content": "The major difference between system identification and machine learning is that system identification involves creating a training dataset, where we apply input signals to a physical system and observe its output signals. The optimal input design is not obvious when the target is an unknown nonlinear system. In this experiment, we evaluate the metrics of the training dataset when we train deep SSMs for modeling two ground-truth systems that have linear dynamics and static non-linearity.\n4.2.1 Set up. We assume that the true systems are a Wiener model [3, 38] and Hammerstein model [3, 5] with the additive Gaussian noise. The Wiener-model has the static nonlinear component after the linear dynamical system. Following [3, 38], we used the mathematical model for the control valve for fluid flow:\n\\(y(t) = \\frac{\\sqrt{v(t)}}{0.10 + 0.90 v(t)}\\), (18)\nwhere u(t), v(t), and y(t) are the signal applied to the stem, the stem position, and the resulting flow, respectively. On the other hand, the Hammerstein-model is also composed of the static nonlinear component and linear system but nonlinearity is before the linear system. Following [3, 5], we used the polynomial nonlinearity with FIR:\n\\(v(t) = \\sum_{i=1}^{3} p_i u(t)^i\\) (19)\n\\(y(t) = \\sum_{i=1}^{8} \\theta_i v(t - i)\\)\nTraining datasets We made 5,000 training datasets for computation of the correlation coefficients. We generated 5,000 input signals and observed the output signal for each input signal. The i-th input signal has i frequency components:\n\\(u(t) = c \\sum_{j=0}^{i-1} sin(\\frac{2 \\pi \\omega_j}{T} t + \\frac{4 \\pi \\psi_i}{T})\\) (20)\nwhere \\(\\omega_j\\) and \\(\\psi_i\\) are sampled from U(0, T/2). \\(c \\in \\mathbb{R}\\) is set to satisfy \\(\\|u_{0:T-1}\\|_2 = 100\\). \\(y_t\\) is obtained by applying \\(u^\\circ(t)\\) to the ground-truth systems (Eqs. (18) and (19)).\nTest datasets Since we do not know input signals for the running plants and controlled fluid flow in advance, test input signals should be different from training input signals for nonlinear system identification problems. We prepared two input signals as the test dataset. Test input I is the same as the input signals (Eq. (20)) when i = 5000. Test input II is a signal that takes a constant value sampled from a uniform distribution U(-1, 1) for each interval. We set the length of the interval to 20 and T to 10,000. Finally, \\(u^\\circ(t)\\) is normalized to satisfy \\(\\|u_{0:T-1}\\|_2 = 100\\). We observed the true output signals for each input signal.\nWe evaluate MSE between the true outputs and the outputs of deep SSMs after the training of each training datasets and compute \\(\\rho\\) with 5000 training datasets (Nd = 5000) by Eq. (17).\n4.2.2 Results. Tab. 1 lists the correlation coefficients between the K-spectral metric and MSE for the test dataset and between baselines and MSE. The K-spectral metric achieves the highest absolute values of correlation coefficients among the metrics. Validation loss does not correlate with the test loss because a validation dataset is a subset of a training dataset: if a training dataset is biased and does not have sufficient information to identify nonlinear systems, the behavior of validation loss is different from the behavior of test loss. Since dataset sizes T are constant across training datasets, correlation coefficients cannot be computed. This is a drawback of using dataset size to evaluate the effectiveness of training datasets. In contrast, the K-spectral metric can evaluate the effectiveness of training datasets even when the training dataset sizes are constant and the data collection is biased."}, {"title": "4.3 Classification and Forecasting", "content": "4.3.1 Setup. We used CIFAR10 [13", "23": "and Speech Commands (SC) [36", "33": ".", "99%": "of the training datasets. Validation datasets are subsets of these reduced training datasets. For (ii)"}, {"33": "and randomly removed [2", "10": "target labels and their data for CIFAR10 and ListOps", "35": "target labels for SC. In all cases", "43": "following [9", "following": [3000, 6500, 10000, 13500, 17050, 20550, 24050], "17900": "for ECL", "6000": "for ETTh1", "25400": "for ETTm1. The original full test datasets were used for the performance metrics.\n4.3.2 Correlation between K-spectral Metric and Performance. Tab. 2 lists the correlation coefficients between test accuracy and each metric for classification problems", "datasets": "i) randomly reduced datasets, (ii) datasets lacking certain classes, and the joint set of (i) and (ii). Regarding dataset size, the correlation coefficients are large when using only (i) and only (ii). Especially, the relationship between dataset size and the performance of (ii) is almost linear. This is because test accuracy linearly decreases when training datasets lack classes one by one. However, for the joint set of training datasets (i)+(ii), dataset size loses the linear correlation. Additionally, dataset size does not necessarily correlate highly with the performance for only (i) because the relation is a power law rather than a linear correlation [29"}]}