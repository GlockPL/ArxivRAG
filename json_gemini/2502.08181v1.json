{"title": "Latest Advancements Towards Catastrophic Forgetting under Data Scarcity: A Comprehensive Survey on Few-Shot Class Incremental Learning", "authors": ["M. Anwar Ma'sum", "Mahardhika Pratama", "Igor Skrjanc"], "abstract": "Data scarcity significantly complicates the continual learning problem, i.e., how a deep neural network learns in dynamic environments with very few samples. However, the latest progress of few-shot class incremental learning (FSCIL) methods and related studies show insightful knowledge on how to tackle the problem. This paper presents a comprehensive survey on FSCIL that highlights several important aspects i.e. comprehensive and formal objectives of FSCIL approaches, the importance of prototype rectifications, the new learning paradigms based on pre-trained model and language-guided mechanism, the deeper analysis of FSCIL performance metrics and evaluation, and the practical contexts of FSCIL in various areas. Our extensive discussion presents the open challenges, potential solutions, and future directions of FSCIL.", "sections": [{"title": "1 Introduction", "content": "The few-shot class incremental learning (FSCIL) problem simulates a real-world problem where a machine learning model is deployed to learn dynamic environments with very few samples [Tao et al., 2020]. FSCIL problem is broadly applied in various area e.g. medical[Sun et al., 2024b], remote sensing[Zhao et al., 2022], audio processing[Wang et al., 2021], and data mining [Tan et al., 2022]. The presence of the data scarcity constraint in FSCIL hardens the catastrophic forgetting problem to be solved. As a clear proof, rehearsal-based class incremental learning (CIL) methods [Boschini et al., 2022], and event joint-training mechanism fail to overcome the FSCIL problem. Thus, it calls for advanced solutions beyond CIL methods. On the other sides, popular rehearsal-based methods in the CIL problem cannot be applied in the FSCIL problem because it might result in all samples to be stored in the memory.\nIn the earlier stages, various FSCIL methods have been developed, offering different approaches i.e. backbone tuning [Shi et al., 2021], meta-learning [Chi et al., 2022], prototype-tuning [Chen and Lee, 2021], and dynamic network approach [Zhang et al., 2021]. Most of the methods fully train the backbone parameters in the base task, then train a small subset of the backbone or part of the model e.g. projection and weighting in the few-shot tasks. This implies that the model relies too much on the base task performance. In addition, the model barely achieves plasticity in the new tasks, while suffering from a high risk of forgetting on the base task.\nMeanwhile, the use of the foundation model and the parameter-efficient fine-tuning (PEFT) approach offer a breakthrough solution to the catastrophic forgetting problem [Wang et al., 2022c]. The pre-trained model that has strong background knowledge handles the dependency on the base task. In addition, the approach offers far smaller learnable parameters than the previous approaches and reduces the model training time by a large margin. The second advancement is the language-guided learning approach pioneered by [Khan et al., 2023] that contributes to the advancement of the FSCIL method. Language-guided learning offers discriminative representations through encoded text embedding of respective classes by a pre-trained vision-language model (VLM) such as CLIP[Radford et al., 2021]. That is, the use of language modality alleviates the data scarcity problem because it offers complementary information of those input images.\nThe recent survey [Tian et al., 2024] systematically discussed the four approaches of FSCIL methods from the theoretical perspective and application. However, the survey doesn't cover the latest advancement of the PEFT approach and language-guided learning of FSCIL. Second, the survey also doesn't yet cover the important advancements of FSCIL settings i.e. semi-supervised (SSFSCIL)[Cui et al., 2021], unsupervised (UFSCIL)[Ma\u2019Sum et al., 2024b], complete (CFSCIL)[Lin et al., 2024] and cross-domain (CDFSCIL) [Yang et al., 2023a] that have been applied in various research areas. Third, prototype bias and rectification[Liu et al., 2020] that are the important aspects of data scarcity haven't been discussed yet in the survey. Last but not least, the subtle details such as backbone and parameters, details of base classes, and novel classes performance i.e., stability-plasticity performance haven't been discussed as well. These intriguing factors motivate us to write a comprehensive survey on FSCIL. We offer a comprehensive FSCIL (sub-)settings, detailed topology, and formal objectives of each FSCIL approach, broad yet detailed aspects in FSCIL methods comparison, accommodating the latest advancement of FSCIL. Then, we present the open problem, potential solution and future direction of FSCIL. Specifically, our contributions are summed up:"}, {"title": "2 Preliminaries", "content": ""}, {"title": "2.1 Problem Formulation", "content": "Few-Shot Class-Incremental Learning (FSCIL) is defined as: Given a sequence of tasks [0, 1, 2, ..., T] where each task t carries a labeled training set $T^t = \\{(x,y)\\}$, where $x \\in X^t$ denotes an input image and $y \\in Y^t$ denotes its label, and |.| denotes the cardinality. Each task t is disjoint with another task t'. i.e. $\\forall t,t' T^t \\cap T^{t'} = ()$. The first task (t = 0) carries abundant training samples while the remaining tasks (t > 0) carries only far smaller samples than the first task i.e $|T^0| >> |T^{t>0}|$. For a convenient way, task-0 is called the base task while the rest is called the few-shot task (FS task). Let a deep neural network $g_{\\phi}(f_{\\theta}(.))$ be parameterized by $\\theta$ and $\\phi$ where $f(.)$ and $g(.)$ are the feature extractor and classifier respectively. The objective of FSCIL is to achieve the optimum parameters $\\theta^*$ and $\\phi^*$ so that $g_{\\phi^*}(f_{\\theta^*}(.))$ recognizes all the learned classes from the first task until the current task t i.e. {$T^0$, ..., $T^t$}. Formally, FSCIL objective is defined as in equation 1."}, {"title": "2.2 Types of FSCIL Setings", "content": "There are 3 types of FSCIL setting based on the training mechanism with respect to the number of images and labels, i.e. X and Y as follows:"}, {"title": "3 FSCIL Approaches", "content": ""}, {"title": "3.1 Backbone Tuning Approach", "content": "The backbone tuning approach trains the whole backbone and classifier parameters in the base task, then trains the classifier and some of backbone parameters in the few-shot tasks as illustrated in fig. 2 (a). Suppose that $\\theta = \\theta_{\\alpha} \\theta_{\\beta}$, where $\\theta_{\\alpha}$ and $\\theta_{\\beta}$ are front part (earlier layers) and back part (latest layers) of the feature extractor parameters, and denotes concatenation. In the base task, the method optimizes $\\theta$ and $\\phi$, while in the few-shot tasks, the method freezes $\\theta_{\\alpha}$ and optimizes $\\theta_{\\beta}$ and $\\phi$. In general, the objective of backbone tuning approach is defined as in equation 2.\nSeveral strategies are enforced in this aproaches i.e. GP-Tree [Achituve et al., 2021] utilizes Gaussian Process to approximate the distribution of classes with few-shot labeled"}, {"title": "3.2 Meta-Learning Approach", "content": "Inspired from MAML[Finn et al., 2017] meta learning approach (fig. 2 (b)) trains both meta learner and base learner with the different sets of data i.e. training and validation set. Validation set can be obtained from auxiliary (open) dataset or by transforming the training set with sequence of operations. Suppose that $f_{\\psi}(.)$ is the meta learner parametrized by $\\psi$, and $T^\\prime = Transform(T^t)$ is validation set transformed from $T^t$. then the dual objective of meta learning-based FSCIL is formulated in eq. 3."}, {"title": "3.3 Prototype-Tuning Approach", "content": "Prototype tuning approach is the most common method in FSCIL due to its simplicity and scalability. Suppose that $z_c$ is the prototype of class c, $Z^t = \\{z^t_c\\}$ is the prototype set of task t contains prototypes of any class c exists in $T^t$, and $Z = Z^0 \\cup Z^1, ... \\cup Z^t$ is the prototype set of all learned classes from task 0 to task t. The approach utilizes prototype-based classifier $g_Z(.)$ instead of standard MLP classifier $g_{\\phi}(.)$. Thus it optimizes Z in every task, while feature extractor parameter $\\theta$ is optimized only in the base task. A prototype-based method may utilize an additional network $f_W(.)$ parameterized by W between backbone $f_{\\theta}(.)$ and prototype-based classifier $g_Z(.)$ for projection, weighting, or refinement. The objective of the prototype tuning approach is defined in eq. 2."}, {"title": "3.4 Dynamic Architecture Approach", "content": "Dynamic architecture approach tackles FSCIL challenges by incrementally evolving its architecture to adapt to the new tasks. Fig. 2 (d) shows that this approach can be implemented into two types of growing component, i.e. growing representation W or growing classifier $\\phi$. TOPIC[Tao et al., 2020] grows neural gas W, a graph-like structure to obtain a more discriminative class representation. BiDist[Zhao et al., 2023a] adds learnable weight for each task $W_t$ then uses bilateral distillation of current and previous tasks rep-"}, {"title": "3.5 Parameter Efficient Fine Tuning (PEFT) Approach", "content": "The PEFT approach proposes a unique new solution for FSCIL by training small-sized parameters (prompts) on top of frozen pre-trained backbone e.g. visual transformer (ViT). Different to CNN, ViT has sequence of layers, where a layer contains Multiples Self Attention (MSA) heads combined with other components, each performing self-attention function as in eq. 6:\nwhere Q, K, and V are query, key, and value vectors obtained by processing an input patch into respective weights i.e. $x.W_Q, x.W_K$, and $x.W_V$, D and h denote patch dimension and number of heads respectively. Prompt-based method attaches a small learnable prompts into a frozen pre-trained ViT model (fixed $\\phi$) [Wang et al., 2022c]. The attachment techniques are mainly divided into 2 types i.e. prompt tuning and prefix tuning [Li and Liang, 2021]. Prompt tuning as in L2P [Wang et al., 2022c] inserts a prompt vector P into query, key, and value of MSA, then the attention mechanism is computed as $A([P \\oplus Q], [P \\oplus K], [P \\oplus V])$, where $\\oplus$ denotes concatenation operation. Prefix tuning as in DualP [Wang et al., 2022b] divides P into pair of $P_K$ and $P_V$ for key and value respectively, then the attention mechanism becomes $A(Q, [P_K \\oplus K], [P_V \\oplus V])$. The structure of P can be defined by several options, i.e. pool-based structure as in L2P, task-wise structure as in DualP, growing component structure as in CODA-P[Smith et al., 2023], or any customized structure.\nASP[Liu et al., 2024] deploys task-aware task-invariant prompts to improve prompt selection during the testing phase. Besides, it utilizes a prompt encoder that produces an additional (second) prompt. Privilege [Park et al., 2024] leverages vision and language modalities and prepends concatenation of vision and language prompts. Similarly, FineFMPL[Sun et al., 2024a] performs language-guided prompting i.e. tuning vision prototype by language prototypes. In addition, it utilizes two types of vision prototypes i.e. global level and object level prototypes. ApproxFSCIL[Wang et al., 2025] concatenates ViT layers with norm layers and tanh function instead of prompt, then bounds training process with transfer risk and consistency risk. Formally, the objectives of PEFT approach is presented in eq.7."}, {"title": "4 Trends and Finding", "content": ""}, {"title": "4.1 Backbone, Parameters, and Pre-trained Model (PTM)", "content": "As shown in tab. 1, ResNet18 is the preferred backbone for backbone tuning, meta-learning, prototype-tuning, and dynamic structure approach following the pioneer of FSCIL setting[Tao et al., 2020]. However, the recent advancement of PEFT approach utilized ViT instead of ResNet. This is one of the clear impacts of the advancement of the foundation model. Despite utilizing a bigger backbone i.e. ViT-B/16 with 86M parameters, PEFT approach has the smaller trainable parameter i.e. 0.2-11.9M as all the pre-trained backbone weights are frozen, except YourSelf. YourSelf fine-tunes its backbone in the base classes resulting a bigger trainable parameters, albeit utilizing fewer layers of ViT.\nThe other 4 approaches have bigger trainable parameters i.e. 11.7-41.7M albeit utilizing the smaller backbone i.e. 11.7M as these approaches tune the whole backbone in the base task. Note that a method usually has more parameters than the backbone and classifiers e.g. projection, weighting, self-attention, or rectifier. Thus, these approaches train more parameters. PEFT utilizes a PTM for all scenarios, while the rest utilizes PTM in the CUB dataset. This fact gives us 2 insights, first, training the backbone from scratch isn't enough for a dataset with small (30 or fewer) samples per class, and second we can compare all the methods head to head in CUB dataset as all the methods utilize PTM."}, {"title": "4.2 Prototype-based over Network-based Classifiers", "content": "The table shows that the prototype-based classifier is highly preferable than the network-based classifier such as MLP or reconstructed linear network [Zhang et al., 2021]. A prototype-based classifier uses classes' prototypes where a prototype is optimized only in a particular task. Thus, it won't be adjusted in the upcoming tasks. Thus, it reduces the risk of forgetting. On the contrary, a network-based classifier is adjusted in each task, thus it has a higher chance of forgetting. Several methods utilize combined or customized classifiers. GKEAL and Comp-FSCIL utilize a combination of linear and prototype-based classifiers. The methods have a selection mechanism to choose which classifiers will be utilized in the inference phase. PriViLege and FineMPL utilize both vision and language prototypes since the methods adopt language-guided learning. S3C generates multiple prototypes with angular rotation, and then performs a stochastic classification based on the similarity to the prototypes."}, {"title": "4.3 Image vs Feature Augmentation", "content": "Several FSCIL methods utilize augmentation to handle data scarcity in FSCIL, aside from standard augmentation such as random crop and random flip, commonly used in a data loader setting. Image augmentation offers a low-level and simple procedure but consumes higher system memory. On the contrary, augmentation in the feature space requires a lower memory size, but it needs more complex operation and validation. For example, image-space augmentation can be"}, {"title": "4.4 The Importance of Prototype Rectification", "content": "As shown in tab.1 all the methods that utilize prototype-based classifiers perform prototype rectification to maximize class discrepancies. As emphasized in [Liu et al., 2020], and visualized in fig.3(a) a few observed samples have a high chance of producing a biased prototype i.e. a prototype that only represents the few observed samples but not the whole distribution. Thus a rectification is necessary to adjust the biased prototype into the optimal prototype.\nThe existing FSCIL methods perform various types of rectification as illustrated in fig.3(b)-(d). The simplest mechanism is rectification by loss where classes' prototypes are adjusted based on similarity to the input features or between classes prototypes. There are many types of loss that can be utilized to perform this rectification e.g. prototypical-cross-entropy $L_{pce}$, self-supervised loss $L_{ss}$, or contrastive loss based on positive and negative prototypes i.e. $L_{p+}$ and $L_{p-}$. The second type is rectification by pseudo prototypes. Pseudo-prototypes can be obtained by generating several prototypes from different layers, model perturbations, or augmentation. Some methods like OrCo[Ahmed et al., 2024] utilize orthogonal loss $L_{ort}$ to minimize dot product between classes' prototypes. Rectification by training-free calibration [Wang et al., 2024a] adjusts a prototype of novel classes based on weighted similarities to the base classes' prototypes. While rectification by trainable networks utilizes an extra trainable model to calibrate the prototype [Tang et al., 2024]. Training-free calibration is less complex than rectification by pseudo-prototypes or trainable networks since it doesn't need extra operations for generating pseudo-prototypes."}, {"title": "4.5 The Use of Language-Guided Learning", "content": "The advancement of the vision-language model (VLM) such as CLIP [Radford et al., 2021] drives the utilization of language-guided learning in FSCIL. PriViLege[Park et al., 2024] deploys language prototypes along with vision prototypes as loss anchors and classifiers, while FineMPL[Sun et al., 2024a] merges both prototypes into one long prototype and then uses it for the same purpose. Despite only 2 methods that utilize language-guided learning, the studies show a promising future for the approach."}, {"title": "4.6 Performance", "content": "Tab.1 shows the overall performance on 3 main FSCIL benchmark datasets, in terms of average accuracy(AA) from the first until the last task, performance drop (PD) between the first and the last task and average harmonic accuracy (AHM) that represent stability-plasticity balance from the second until the last task. In general, AA in three datasets that is still below 90% shows that FSCIL offers a big opportunity for improvement. The high accuracy (> 90%) in MiniImageNet by Privilege, FineMPL and ApproxFSCIL are common-sense results as the methods utilize pre-trained ViT on ImageNet (the superset of MiniImageNet). The results on the CUB dataset, where the PEFT method achieves relatively higher AA show the impact of the bigger backbone i.e. ViT (86.7M) over ResNet18(11.7M) since all methods utilize PTM. The high magnitude of PD i.e. up to 42% shows that catastrophic forgetting with data scarcity remains a difficult problem.\nPEFT methods e.g. ASP and Privilege confirm the superiority of the PEFT approach to the other 4 approaches with a significant gap i.e. up to 20%. The improvement from L2P, DualP, and CodaP to L2P+, DualP+, and CodaP+ confirms the significant impact of the prototype-based classifier for FSCIL methods. The other 4 approaches have competitive performance in those 3 datasets, with similar trends i.e. the latest methods outperform the earlier methods. Both from AA and PD measurement, the best performers of those 4 approaches are below the best performer of the PEFT approach."}, {"title": "4.7 Stability-Plasticity Balance", "content": "The stability-plasticity balance of FSCIL methods is shown in average harmonic mean accuracy (AHM) in tab.1. The table shows that stability-plasticity is still an open problem in FSCIL, as almost all the methods achieve less than 80% AHM, and less than 60% AHM for non-PEFT methods. In addition, the huge different i.e. up to 35% between AA and AHM of those methods indicates the failure in maintaining stability-plasticity, and the average accuracy relies too much on base classes' accuracy. It indicates that the methods barely achieve plasticity with few available samples. Tab. 2 shows the accuracy of base and novel classes accuracy in the last task. The table clearly shows the huge (up to 60%) difference between base and novel classes' accuracy, indicating that the"}, {"title": "4.8 Application", "content": "Table 3 summarizes the latest advancement of FSCIL studies in various areas i.e. medical(red), remote sensing (green), NLP (yellow), and graph (blue). Medical and remote sensing typically utilize image data type, while NLP and graph FSCIL typically utilize audio and graph data types respectively. The other data types are time-series, hyperspectral, and text that are utilized in medical, remote sensing, and NLP FSCIL respectively. The majority of the studies were conducted in supervised FSCIL, only few studies were conducted in different settings i.e. UFSCIL[Ma\u2019Sum et al., 2024b], CFSCIL[Tai et al., 2023],[Si et al., 2024], and CDFSCIL[Yang et al., 2023a]. At the moment, backbone tuning, prototype tuning, and dynamic networks are preferable in these areas among the 5 FSCIL approaches, with ResNet as the preferable backbone for all the research areas, except graph FSCIL that utilizes graph neural networks (GNN). The other backbone types are CRF, CNN, and 1DCNN utilized for text, audio, and times series data respectively.\nPrototype-based method is preferred over network-based classifiers due to its resilience against data scarcity and its compatibility to constrastive learning and self-supervised learning. About half of the methods utilize augmentation, but almost none of the methods deploy prototype rectification. It emphasizes the importance of rectification as discussed in the previous analysis. Only of the study utilizes PTM as none of the methods uses PEFT approach where PTM is an important element. None of the studies utilize language-guided learning that shows an opportunity for the approach to be deployed."}, {"title": "5 Open Challenges and Potential Solutions", "content": "a). Stability-Plasticity Dilemma remains the most important open challenge in FSCIL. The PEFT approach may achieve far better stability-plasticity balance than the other 4 approaches, but the gap between base and novel classes' performance is still high. One of the potential solutions is improving prototype rectification for the PEFT approach since most of its methods utilize rectification by loss only.\nb). Prototype Bias has a bigger part in the stability-plasticity dilemma as in point a, as the existing models struggle to achieve plasticity. It shows the needs for more effective prototype rectification for FSCIL methods."}, {"title": "6 Future Direction", "content": "a). Toward Data Privacy: Data privacy now attracts a concern on FSCIL following real-world scenarios where the learning process is conducted by collaborative distributed clients instead of one standalone agent. The agents hold only a partial of the data and class labels. The agents are allowed to exchange their locally trained models but not the data due to data privacy constraint. In a such scenario, FSCIL training is executed in a federated way, and lead to new problem named federated FSCIL (FFSCIL)[Ma'Sum et al., 2025a].\nb). Toward Data Streams: As in CIL[Xinrui et al., 2024], learning in data streams is bound to be a concern in FSCIL. The learning agent can't be assumed to always have storage to save the samples and labels, and the learning process will be executed in an online way. This highly possible setting lead to new problem named online FSCIL (OFSCIL) that will be even more challenging than online CIL.\nc). Toward Open World Setting: Nowadays, open-world setting have been a new focus on CIL studies[Kim et al., 2025] and eventually will be applied in FSCIL in the future. The out-of-distribution (OOD) detection capability is an important feature for a continually learning agent. The agent should be able to differentiate between learned knowledge and not yet learned knowledge. This leads to new sub-study of FSCIL named open-world FSCIL (OWFSCIL).\nd). Towards Foundation Models and PEFT: The advancement of foundation models impacts the FSCIL study significantly as explained in the previous analysis. There is still a room for improvement in its exploration e.g. which pre-trained model is suitable for which dataset, how the pre-trained model contributes in the cross-domain setting, and the development of foundation model for particular domains such as medical images, audio, and graph. Meanwhile, there are many opportunities to develop new PEFT methods such as new promoting structure and learning, or PEFT based on Low-Rank Adaptation (LORA) as in [Liang and Li, 2024].\ne). Toward Language-Guided Learning: The recent study[Ma'sum et al., 2025b] highlights new advancements of language-guided learning in CIL that are likely useful for FSCIL, i.e. the synergy of class-wise and task-wise language prototypes, mixed-up language-and-vision prototype, generative language prototype by LLM decoder, or language as prompt generator offering future opportunity in FSCIL study."}, {"title": "7 Conclusion", "content": "In this study, we propose a comprehensive survey on the latest advancement of the FSCIL topic. We define formal attributes of the setting, a richer yet clear FSCIL topology, and formal objectives of FSCIL approaches. We emphasize the importance of prototype rectification and present its topology. We analyze the broader aspects of FSCIL methods, from their attributes to the detailed performance. We highlight remarkable FSCIL open challenges and potential solutions, and the future direction of FSCIL towards various aspects that open wide opportunities for future study."}]}