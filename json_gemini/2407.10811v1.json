{"title": "GuideLight: \u201cIndustrial Solutions\u201d Guidance for More Practical Traffic Signal Control Agents", "authors": ["Haoyuan Jiang", "Xuantang Xiong", "Ziyue Li", "Hangyu Mao", "Guanghu Sui", "Jingqing Ruan", "Yuheng Cheng", "Hua Wei", "Wolfgang Ketter", "Rui Zhao"], "abstract": "Currently, traffic signal control (TSC) methods based on reinforcement learning (RL) have proven superior to traditional methods. However, most RL methods face difficulties when applied in the real world due to three factors: input, output, and the cycle-flow relation. The industry's observable input is much more limited than simulation-based RL methods. For real-world solutions, only flow can be reliably collected, whereas common RL methods need more. For the output action, most RL methods focus on acyclic control, which real-world signal controllers do not support. Most importantly, industry standards require a consistent cycle-flow relationship: non-decreasing and different response strategies for low, medium, and high-level flows, which is ignored by the RL methods. To narrow the gap between RL methods and industry standards, we innovatively propose to use industry solutions to guide the RL agent. Specifically, we design behavior cloning and curriculum learning to guide the agent to mimic and meet industry requirements and, at the same time, leverage the power of exploration and exploitation in RL for better performance. We theoretically prove that such guidance can largely decrease the sample complexity to polynomials in the horizon when searching for an optimal policy. Our rigid experiments show that our method has good cycle-flow relation and superior performance. The code is available here.", "sections": [{"title": "I. INTRODUCTION", "content": "Traffic signal control (TSC) is a critical chapter to maintain traffic efficiency and safety in smart sustainable mobility [1]. Recently, employing reinforcement learning (RL) to solve TSC has become a research hotspot since it does not require too much artificial priori and can improve performance significantly through the paradigm of trial and error. Despite all efforts in this field, there is no real-world deployment of an actual RL-based TSC to date. In fact, it has not yet been proven that RL is even applicable as TSC in a real-world setting. Every investigation so far has been done without considering real-world deployment requirements. In this paper, we aim for a practical RL-based traffic signal control method, considering the following industry requirements:\n\u2022 R1: Input (State): The RL-based methods usually need multiple environmental observations, which are rather chal- lenging to obtain in the real world. Specifically, they need a combination of queue length, traffic flow, wait or delay time, car position, and so on as the observation space [2], [3], [4], [5], [6], [7]. However, traffic flow is the only available and stable index in most cities. SCATS, SCOOT [8], and other traditional industrial solutions thus only use traffic flow to provide stable and good enough performance.\n\u2022 R2: Output (Action): The learned RL policies do not align with the industry practice. Firstly, Fig. 1 shows how traffic signals are usually controlled with the basic concept of \"phase\", i.e., a combination of two non-conflicting traffic movements (more details in Sec III). As shown in Fig. 2(a1), the real-world solution is to directly determine the cycle time for each of the fixed-order four phases, e.g., A-D-E-H, and then repeat, known as \"Cyclic Control\". (Details in Sec III.) In cyclic control, the task is to determine the phase duration for each phase of A, D, E, and H within a cycle. Thus, we can also sum up to get the whole cycle length. Cyclic control is known to be friendly and intuitive for drivers to follow. As depicted in Fig. 2(b1), almost all the RL models [4], [5], [9], [7], [10], [11], [12], [13], [14], [15], [16], [17], [18] follow an \"Acyclic Control\", which is still conceptual in simulation systems and not validated in real life: the action is to choose one phase from 8 available phases (with fixed 10 seconds to release) for every 15 seconds. Thus, the resulting phase sequence can be quite random and unstable, causing drivers' confusion and even potential traffic accidents.\n\u2022 R3: Cycle-Flow Relation: Most importantly, the industry requires that for each intersection: 1) the trends of the traffic flow and the cycle time should be positively synchronized, which means the cycle-flow curve shown in Fig. 2(a2) should always be non-decreasing, between the required minimal and maximal cycle time. A straightforward explanation is that as the traffic flow regulated by this phase increases, the duration required to fully clear this phase of the traffic flow will also extend. Otherwise, the start time and stop time of the unreleased vehicle will be lost; 2) moreover, the relation between the traffic flow and the cycle time should be three-stage [19]: as shown in Fig. 2(a2), both logistic model [20] and SCATS [21] show that when the flow is medium (yellow region, climbing up to the peak), cycle time should response very sensitively, and the other regions such as the purple region where the flow is reaching the traffic capacity, the cycle time change should be smooth, to avoid further delay. Mechanisms of R2 and R3 cultivate stable behaviors and response patterns from the driver to the traffic lights and avoid confusion and even potential traffic accidents. In contrast, as shown in Fig. 2(b2), the RL- based methods such as DDPG (Deep Deterministic Policy Gradients) [22], FRAP (short for Flipping and Rotation and considering All Phase configurations) [5], and MetaGAT [10], have quite unreasonable cycle-flow relations, heavily confusing the drivers.\nIn this paper, we present a practical RL model to tackle all three criteria above. We will use the traffic flow as state input and output cyclic phases as action so the trained RL model is campatible with industry practice. Moreover, we also aim to maintain a rigid and rule-based-like cycle-flow relation in Fig. 2 by combining the traditional solution with the RL method, to obtain a solution that meets industrial needs and has good performance at the same time. In this paper, we propose, GuidedLight, an RL-based agent guided by the \"industrial solutions\". Specifically, we use Behaviour Cloning (BC) to encourage the agent to learn from the teacher, e.g., SCATS, and we also let the learning process be gradual and progressive: we adopt curriculum learning [23], which teaches the agent from easy to advanced. In summary, the contributions are three-fold:\n\u2022 Our GuideLight lays a significant foundation for the practical implementation of RL-based solutions to be adopted in practice. Technically, our RL framework is purely designed in a way that matches the industry standards in terms of state, action, rewards, and cycle- flow relation.\n\u2022 To guarantee the rigid cycle-flow relation, we innova- tively propose to use industrial rule-based solutions to guide the agent via behavior cloning and curriculum learning. Moreover, we also theoretically prove that with guidance from the traditional methods, our method guar- antees a polynomial sample complexity in the horizon.\n\u2022 Experiments show that the proposed method not only re- spects the cycle-flow relations, but also achieves higher performance, thanks to the \u201cindustrial\" guidance and the agent's own exploration and exploitation.\nThe rest of the paper is organized as follows: In Section II, we will review the traditional methods and the RL-based methods for traffic signal control; In Section III, we will give the key background information and preliminaries; Section IV will officially design the proposed \u201cGuideLight\u201d and specifically, Section IV-D will prove that such guidance can largely decrease the sample complexity to polynomials in the horizon when searching for an optimal policy. Section V will present the rigid experiments, Section VI discusses generalization, and Section VII will conclude."}, {"title": "II. RELATED WORK", "content": "A. Traditional methods\nTraditional methods are widely used in the real world, most of them are cyclic control, which is friendly and con- sistent to drivers; however, they rely on strong assumptions and manually specified rules. Fixed-time control [24] is one of the earliest cycle-based methods: it configures the traffic signal plan as a fixed cycle length, which is inflexible and cannot automatically adapt its policy to the actual situation. Actuated control model [25], [26] instead used pre-defined sets of rules to decide whether to adjust the current phase.\nLater on, methods with analytic solutions were proposed: Webster [27] directly calculates a cycle length using the flow ratio as input to minimize the delay time:\n$C=\\frac{1.5L+5}{1-Y}$,\nwhere L is the loss time, and Y is the sum of the critical flow ratio. Logistic method [27] uses the logistic curve to better respond to different types of traffic flow. SCATS [21] is the most used one worldwide, which does not use a mathematical model but a set of heuristic rules: it uses a set of simple algebraic expressions to describe the traffic characteristics and operating rules of the current road network. As shown in Fig. 2(a2)'s red curve, SCATS handles (1) non-peak traffic flow with \"stair-like\" rules, with indexes such as Min CT, Alt Min 1, Alt Min 2 to be specified, (2) climbing traffic with rapid response up to Stretch CT, and (3) peak traffic with a flatter curse, capped by Max CT. All of these traditional methods are simple and interpretable, but their assumption dependencies are relatively straightforward and inflexible. Actual situations may not conform to them, so traditional methods often cannot guarantee optimal results.\nB. RL methods.\nIn recent years, numerous RL-based TSC methods [28], [29] have achieved significant outcomes. As mentioned be- fore, most RL-based methods are acyclic control, whose action is to decide which phase to choose. In [30], [12], [31], [2], a single agent is employed to manage all intersections within a given scenario. However, due to the joint of their state space or action space, these often encounter issues such as the curse of dimensionality [13] and instability [10]. A different approach is taken by [14], [4], [32], which utilizes image-based states for decentralized intersection control. [5], further proposed FRAP, a widely followed method for acyclic control. It inputs the traffic features of all eight movements, combines two non-conflicting movements into a phase (8 phases obtained in total), and uses deep Q-learning to decide which phase will be selected for the next time step. Building upon this, [9] further presents a General Plug-In (GPI) module, where any arbitrary and various intersections can be mapped into the standard one, thus having a unified state and action space for various intersections. This facilitates the co-training of large-scale scenarios.\nNonetheless, this acyclic control has quite a wide gap to real-world applications, and a major reason is the hardware: most traffic signal controllers only support cyclic phase plans. A few methods have been tried to solve it. For instance, [33] used DDPG [22] to determine each phase duration in the next cycle. Nonetheless, this method requires the total cycle time to be fixed. Alternatively, [34] uses a strategy of deciding whether to remain in the present phase or transition to the subsequent phase during each short interval. However, this approach neglects various practical constraints, such as ensuring that the discrepancy between the durations of two consecutive cycles is not excessively large. An innovative strategy is presented in [35], which modifies one phase's duration during each timestep. This approach accommodates certain constraints and allows for dynamic adjustments in both the complete cycle duration and the duration of individual phases. However, when confronted with a limited variety of observations, e.g., only flow is available and a preferred rigid cycle-flow relationship, this method may be incompetent."}, {"title": "III. PRELIMINARIES", "content": "Some necessary domain information is given here.\nDefinition 1. Intersection is where two or more roads cross. A 4-arm intersection is shown in Fig. 1, where each arm (N, S, W, E) has entering lanes and exiting lanes.\nDefinition 2. Traffic Movement is defined as the direction in which a vehicle crosses from an entering lane to an exiting lane, including go-through, turn-right, and turn-left in each arm. In Fig 1, the numbers 1-8 represent the eight movements controlled by the signal (right movements are usually free).\nDefinition 3. Phase is a combination of two non- conflicting movements that could be released together. For instance, movements 1 and 2 conflict, thus not being able to form a phase; but movements 1 and 5 can form W-E- through phase, denoted as phase-A. In reality, not all the intersections have all four legs with all movements and phases; in that case, zero-padding will be used to mask the missing movement or phase.\nDefinition 4. Cyclic Control In real-world solutions like SCATS, four phases are usually controlled in a cyclic way: A\u2192D\u2192E\u2192 H and then repeat, where in each phase, the phase duration needs to be decided. The cycle time is the summation of all phase durations plus red and yellow time. Our RL method follows the four-phase cyclic control."}, {"title": "IV. METHOD", "content": "Sec. IV-A will introduce the state space, action space, and reward design. Sec. IV-B introduces the network design. Sec. IV-C introduces the training based on behavior cloning and curriculum learning. Sec. IV-D will theoretically prove the improved sample complexity with horizon H.\nA. RL Setup\nGenerally, the single-agent TSC problem is formulated as a Markov decision process (S, A, r, \u03b3, \u03c0). Every agent controls an intersection independently.\nTo perfectly match our proposed algorithm with real-world situations, the state, action, and reward are all carefully designed.\nState S: Only traffic flow and last step action-based observations (e.g., sampled every 5min or 15min) are chosen since commonly they are the only available index; Thus, the state space is continuous.\nAction A: Instead of choosing the next phase in the acyclic control, our action is designed to directly output the phase duration for each phase in a cyclic control. Besides, there are two more constraints: the total cycle time should change smoothly and should stay within the minimal and maximal thresholds. Thus, we translate the action into 3 categories, thus being discrete actions, which are adding/cutting 5 sec- onds (+5s/-5s) or non-changing (0s) for each phase: such a design frees us from the computation burden of the original purely continuous action space, and the small change of 5s on each phase ensures the smoothness of cycle time. To consider the constraint of the total cycle time staying within the minimal and maximal thresholds, once the cycle time hits the min (max) threshold, the action of -5s (+5s) will be masked. Thus, we have:\n$a =\\begin{cases}\\lbrace +5s, -5s, 0s \\rbrace , & \\text{if } CT \\in \\text{[min-threds, max-threds]} \\\\ \\text{masked}, & \\text{otherwise} \\end{cases}$     (1)\nReward r(s, a): we define the reward based on throughput v (\u2191, the higher the better), queue length l (\u2193), and two factors that matter in the industry, i.e., green-light utilization rate gr (\u2191), and green imbalance gi (\u2193). Our reward function is the weighted sum of the four factors, i.e.,\nr = wv \u2022 v + wl \u2022 l + wgr.gr + Wgi.gi     (2)\nwith weights tuned and specified in Sec. V-C. Throughput v is the number of vehicles passing through the intersection per unit of time. Queue length measures the length of the vehicle queue at the intersection. Greenlight utilization rate measures the portion of greenlight that is really used to release vehicles, which is inferred as follows for each phase:\n$gr=\\frac{v \\times 2.5s}{\\text{phase duration}}$    (3)\nwhere v is the through-put of this phase and 2.5 seconds (a common setting in traffic control manual [27]) is usually the time for a vehicle to pass the intersection. Green imbalance gi evaluates the standard deviation of all the phases' gr:\n$gi = \\text{standard deviation}(\\lbrace gr \\rbrace_{\\text{all phases}})$     (4)\nIt is worth mentioning that in the reward, we use metrics beside traffic flow, such as queue length, the reason is: during training in the simulation, the queue length is obtainable and it is beneficial to be part of the reward to give the agent better-informed feedback, so that when being deployed in the real world, even only traffic flow is observable, but agent's action can still tend not to cause long-queue.\nOptimal policy \u03c0*(as): At each intersection i and each timestep t the goal of the agent is to find a policy \u03c0(as) that maximizes the expected return $G_t := \\sum_{t^{\\prime}} E[\\gamma^{t^{\\prime}} r_i]$, where \u03b3 is the discount factor.\n$\\pi^*(as) = \\underset{\\pi}{arg \\max}\\sum_{t} E[\\gamma^t r_i]$     (5)\nB. Network design\nThe model is shown in Fig. 3. When deciding for t + 1, we utilize the movement-level and the phase-level features from t. Then the fused feature is fed into LSTM [36] to capture long-term decision dynamics, and finally, the actor- critic network makes the action [9].\nFor the movement-level feature: the i-th intersection in the scenario, the m-th movement (m \u2208 {1, ..., 8}) observes K features. In our case, K = 3 includes only traffic dynamics, i.e., traffic flow, and two additional static indices, i.e., road capacity and indicator of whether the movement exists. For any traffic movement, we get its embedding,\nwhere ||, Sigmoid(\u00b7), MLP(\u00b7) are concatenation, activation function, and multilayer perceptron, respectively.\n$e_{i,j} = \\underset{k=1}{\\|K} \\text{Sigmoid}(MLP_k(S_{i,m,k}))$     (6)\nThen, we use part of FRAP to first aggregate two non- conflicting movements' embeddings into a phase embedding, and then also to embed the phase conflict information into the phase embeddings. (1) Add: FRAP adds the embedding of movements as a phase representation. For the p-th phase which consists of movements j, j', p \u2208 [A, B, . . ., H]: $\\overline{e}_{i,p}$ = $e_{i,j}$ + $e_{i,j'}$, where the two movements j, j' \u2208 [1,2, ..., 8]. (2) Phase-pair representation: Once obtaining the phase embedding, a phase-pair representation is constructed to capture the pairwise relations for competition: $\\hat{e}_{i}$ = $\\overline{e}_{i,l}$||$\\overline{e}_{i,l'}$. (3) Phase competition: To avoid phase conflict, pairwise competition scores [5] are learned as a competition mask, denoted as \u03a9. Thus, the phase-pair representations $\\hat{e}_{i}$ will go through 1 \u00d7 1 convolution and multiply with the phase competition mask to yield the masked phase-pair embedding: $e_i=\\text{Conv}_{1 \\times 1}(\\hat{e}_i)$, and is Kronecker product. We denote the whole FRAP module as FRAP(\u00b7) [5]:\n$e_i = \\text{FRAP}(e_{i,1},..., e_{i,8})$, where $e_i \\in R^{4 \\times 4 \\times d_f}$,   (7)\n$e_i$ is 4-phase embedding tensor (phase A-D-E-H), with em- bedding dimension as $d_f$. We sum it along the row direction and get the phase embedding with traffic flow information.\n$p_{i,f} = sum(e_i)$, where $p_{i,f} \\in R^{4 \\times d_f}$     (8)\nFor the phase-level feature: \u00a7 denotes the phase-level raw observations with K features in these observations. In our case, K = 3, including more contexts of each phase's duration at the last cycle time, greenlight utilization rate, and green imbalance. We use each MLP to embed them.\n$p_{i,c} = \\underset{l=1}{\\|1}MLP(\\S)$,    (9)\nWe concatenate $p_{i,f}$ and $p_{i,c}$ as the intersection i's feature:\n$p_i = \\| (p_{i,f}, p_{i,c})$  (10)\nLastly, we adopt the LSTM to perceive long-term historical states, so that the agent can use historical trends to achieve better consistency between phase duration and traffic flow. We take the phase-level feature p at this time step t of the intersection i and last time step LSTM hidden state $c_{t-1}$ to the LSTM and get the $h^t_i$ and $c^t_i$.\n$h^t_i, c^t_i = LSTM(p^t_i, c^t_{t-1})$,    (11)\nFor the decision policy \u03c0, it receives the $h^t_i$ as input and outputs the action $a^t_i$.\n$a^t_i = \\pi(\\cdot h^t_i)$.    (12)\nAs mentioned in Sec IV-A about action, $a^t_i$ is defined as +5s/-5s/0s for each phase.\nC. Training\nOur experiments show that solely optimizing the agent us- ing the RL rewarding mechanism only yielded unsatisfactory results regarding the industry requirement (consistent cycle- flow relation). The reasons are two-fold: (1) the agent only perceives the traffic flow from the dynamic road network. This restricted input hampers the agent's ability to explore and identify a superior policy. (2) moreover, the reward is hard to align with the stringent requirements of the industry.\nBehavior Cloning: To address this predicament, we em- ploy BC to steer the agent's actions: namely, we employ the traditional and market-proved solutions to guide our agent. This approach substantially narrows the agent's exploration domain, facilitating faster optimization [37].\nHowever, the well-developed solutions such as SCATS directly output a continuous phase duration, and ours is a discrete action of +5s/-5s/0s. How can we design a loss to penalize the difference between the two? We translate them into logits. The action of the p-th phase from our agent's decision policy \u03c0 can be sampled from below logits:\n$l^t_{i,p} = MLP_p(h^t_i)$, where $l^t_{i,p} \\in R^3$,    (13)\nThe label of BC guidance can be generated as:\n$\\hat{l}^t_{i,p} = \\begin{cases} 0 & E^t_{i,p} - 5  \\le T^t_{i,-1}  \\\\ 1 & E^t_{i,p} + 5  > T^t_{i,0}  \\\\ 2 & \\text{otherwise} \\end{cases}$   (14)\nwhere $E^t_{i,p}$ is the phase duration decided by the expert model of at time t, the $T^t_{i,0}$ is the duration time at last time step, the  \\hat{$l^t_{i,p}$} 0/1/2 represents the ground truth is +5s/-5s/0s respectively. Lastly, we employ the Cross Entropy as the BC loss:\n$L_{BC} = CrossEntropy(\\hat{l}^t_{i,p}, l^t_{i,p})$,   (15)\nCurriculum Learning: Delving deeper into the BC pro- cess, we adopt the Curriculum Learning approach to ensure the agent's progressive learning trajectory. We gradually introduce teachers from easy to advanced to generate the label in Eq. (14).\n\u2022 Easy guide: we choose a linear model that posits a straightforward linear correlation between cycle dura- tion and traffic flow. Phase duration \u2248 0.35 \u00d7 v, where 0.35 is roughly estimated from regression based on large amounts of offline data.\n\u2022 Medium guide: logistic curve based on [27] could be used, where the cycle time and traffic flow follow a logistic relation between the Max CT and Min CT, as shown in Fig. 2(a2).\n\u2022 Advanced guide: In contrast, SCATS exhibits a more intricate behavior, with the three-stage cycle-flow re- lations, as shown in Fig. 2(a2), thus being the most advanced guide.\nTotal Loss: The Behavior Cloning and Curriculum Learn- ing are combined with the Actor-Critic to formulate the total training loss:\n$L = \u03b1L_{Actor} + \u03b2L_{Critic} + \u03baL_{BC}$. (16)"}, {"title": "D. Theoretical Analysis", "content": "Will introducing such guidance even speed up learning? We prove that our approach, under the assumption of guided policy efficacy, can significantly reduce the sample complex- ity from exponential in the horizon to polynomial.\nTheorem 1. For 0-initialized e-greedy, where initial esti- mates for action probabilities start at zero, there exists an MDP instance where the sample complexity scales exponen- tially with the total time horizon H to identify a policy with a suboptimality less than 0.5. [39]\nAssumption 1. Assume that the optimal policy as \u03c0*, with its visited state distribution as d*. The policy updated based on the guidance loss is denoted as \u03c0\u207a with its visited state distribution as d\u00ba. We make the assumption that d\u00ba cover the states visited by \u03c0* subject to an upper limit:\n$\\underset{s \\in S}{sup} \\frac{d^*(s)}{d^+(s)} < C$\nThis ratio, also referred to as the distribution mismatch coefficient, is used in gradient descent methods [40].\nWe explain the advantage of guidance based on an online contextual bandit scenario [41], and we look at regret, a measure of the difference between the reward that could have been obtained by always taking the best possible action and the reward that was actually obtained by the policy used.\nAssumption 2. We assume that there is a guarantee for the exploratory policy. Within the confines of an online contextual bandit scenario characterized by a stochastic context s ~ Po and a reward function r(s, a) spanning [0, R], an exploratory policy exists in every round, such that the total regret is bounded:\n$\\sum_{t=1}^T E_{s \\sim p_0}[r(s, \\pi^*(s)) - r(s, \\pi^+(s))] \\le f(T, R)$\nThis exploratory policy guarantee is a prevailing assump- tion in extensive literature, whether in tabular methods [42] or in methods similar to ours using general function approx- imation [43], [44].\nBased on the Performance Difference Lemma proposed by Kakade et al. [45], we can establish a relationship between the iterations I and the discrepancy between the optimal value function Vo and the current value function V.\n$E_{s_0 \\sim d_0} [V_0^*(s_0) - V_0^\\pi(s_0)]$\n$= \\sum_{i=0}^I E_{s \\sim d} [Q_i(s, \\pi^*(s)) - Q_i(s, \\pi^i(s))]$   (17)\nConsequently, we can derive an upper bound for regret. Let T denote the total exploration steps and E represent the number of epochs after which the policy is updated. Hence, the final number of policy iterations is T/(EH).\n$E_{s_0 \\sim d_0} [V_0^*(s_0) - V_0^\\pi(s_0)]$\n$= \\sum_{i=0}^{T/EH} E_{s \\sim d} [Q_i(s, \\pi^*(s)) - Q_i(s, \\pi^i(s))]$   (18)\n$\\le \\sum_{i=0}^{T/EH} E_{s \\sim d} [Q_i(s, \\pi^*(s)) - Q_i(s, \\pi^+(s))]$   (19)\n$< C \\cdot \\sum_{i=0}^{T/EH} E_{s \\sim d} [Q_i(s, \\pi^+(s)) - Q_i(s, \\pi^i(s))]$  (20)\n$\\le C\\sum_{i=0}^{T/EH} f(H, R)$  (21)\nIn the derivation, inequality (20) arises from Assumption 1, while inequality (21) is predicated on Assumption 2. Drawing on the results presented [46], it can be inferred that under the setting of asymptotic functions, $f(H, R) = R \\cdot (AEF(H))^{1/2}$, where & is a polynomial function. Con- sequently, the rate of our algorithm is up to a factor of C.poly(H) [43]."}, {"title": "V. EXPERIMENT", "content": "A. Towards Real-world Dataset and Simulation\nSeveral efforts have been made to ensure that our eval- uation scenario is as consistent as possible with the real- world scenario. (1) Road Network: we built one simulation scenario in SUMO based on a real-world case: Fenglin West Road, Shaoxing, Zhejiang, as shown in Fig. 4, which is a corridor with 10 intersections to be signal-controlled. More details are in [9]; (2) Traffic flow: most SUMO scenarios only support 1-hour-long flow data, which is not enough to train agents to learn to respond to all levels of flow, i.e., the 3-stage cycle-flow relations shown in Fig. 2. Thus, we collect the real 24-h flow data, e.g., shown in Fig. 4(c), and based on the real-world traffic flow distribution, we generated 1400 different flow patterns for each intersection, maximally enhancing the agents' generalizability; (3) More configurations: more detailed designs such as aligning the SUMO's flow sampling rate with the real-world sensors (e.g., every 5min or 15min), following the pre-defined 4-phase plan (i.e., Phase A-D-E-H), supporting the output of phase duration from agent (instead of choosing the next phase), and so on, have been made. The dataset is open-sourced here for the community.\nB. Benchmark Methods\nWe compare our GuideLight with both traditional and RL- based methods:\nTraditional Methods:\n\u2022 Fixed Time Control (FTC) [24] with random offset executes each phase within a loop, utilizing a pre- defined phase duration.\n\u2022 SCATS [21] The most widely used conventional traffic control algorithms based on real-time traffic data, road characteristics, and manually designed operational rules.\nRL-based Methods:\n\u2022 DDPG [33] A robust RL model widely employed across diverse problem domains and settings, which is meticulously tailored to suit our setting.\n\u2022 FRAP* [5]: It is the ground-breaking acyclic RL based on phase competition. We implement it in a cyclic way based on ours without BC.\n\u2022 MetaGAT* [10] combines contextual meta- reinforcement learning based on GRU to improve generalization and GAT for cooperation. We also implement it in a cyclic way.\n\u2022 GuidedLight Our proposed method in this paper. We used the linear guide and SCATS guide.\nC. Implementation details of GuidedLight\nThe implementation details of GuidedLight are summa- rized in Table II. The parameters are fine-tuned mainly based on grid search.\nD. Evaluation metrics\nWe use two metrics to evaluate these methods. Firstly, we evaluate the results based on traffic flow, queue length, and two factors that matter in the industry, i.e., green-light utilization rate, and utilization balance. These factors form our reward function, which our agent receives as input. Here, traffic flow refers to the volume of vehicles passing through the intersection per unit of time. Green-light utilization measures the proportion of effective time when vehicles pass through the intersection during the green light phase. Utilization balance evaluates the level of balance among the four phases of the intersection. Queue length measures the length of the vehicle queue at the intersection.\nAdditionally, we visualize the synchronization between cycle time and traffic flow. In practice, this is an important indicator of the TSC system. Cycle time tends to increase within a bounded range as traffic flow increases.\nE. Results\n1) Main Results: Quantitative metrics are displayed in Table I, demonstrating that our method consistently attains the highest overall scores, showing substantial advantages over other algorithms. Particularly, in the crucial metric of green-light utilization rate, which is highly important in real- world applications, our algorithm markedly surpasses both the SCATS and FTC methods. Although our approach only achieves the second-best throughput and green imbalance, the gap between our performance and the best results in these categories is relatively minor. Detailed analysis is as follows:\nBehavior Cloning and Curriculum Learning helps GuidedLight learn. Via the delicately designed heuristics, such as the three stages and parameter specifications, SCATS achieves the highest throughput, being the most efficient controller. Thanks to our Behavior Cloning and Curriculum Learning, GuidedLight successfully mimics SCATS's behav- ior and achieves the second. As illustrated in Fig. 2(b2), our GuidedLight also achieves a non-decreasing cycle time- traffic flow relation, and good-shaped three-stage controlling behavior for non-peak, climbing, and peak traffic.\nRL-based agent boosts GuidedLight's performance in other metrics. Through the Actor-Critic-based agent, GuidedLight further optimizes industrial-concerned metrics such as green utilization rate and green imbalance, as well as the queue length, crowning our GuidedLight as the best overall performer.\nAcademic solutions based on RL have rather dis- counted performance and confusing cycle time-traffic flow relations. The well-recognized top performers, FRAP and MetaGAT, when applied in practical industrial settings, suffered performance drops and cannot beat industrial solu- tions such as SCATS and ours. This is because their states, actions, and rewards are all derailed from being practical. Furthermore, as illustrated in Fig. 2(b2), they even have decreasing cycle time-traffic flow relation, meaning higher traffic demand can trigger shorter cycle time and vice versa, which can be rather confusing for drivers.\n2) Visualization of Synchronization: We visualize the synchronization of cycle time and traffic flow in Fig. 5 based on two intersections. It shows that our method and the SCATS system achieve notable synchronization; namely, the cycle time co-varies responsively with fluctuations in traffic flow. On the contrary, the FTC and DDPG approaches remain indifferent to such dynamics. Synthesizing the afore- mentioned results, it becomes evident that RL algorithms tend to outshine traditional methods in terms of quantitative evaluation scores. However, the conventional SCATS method demonstrates superior synchronicity of cycle time and flow. Our method, underpinned by BC guidance, excels in quanti- tative scoring and synchronicity, thus delivering an optimized performance.\n3) Visualization of Curriculum Training Process: Fig. 6 also shows the training process of the curriculum learning. As we can observe, introducing the linear and logistic guides has quite similar and non-distinguishable effects on the training process, and the main reason is that both linear and logistic guides are smooth (in an extreme case, a logistic function with a tiny slope is almost equivalent to a linear function ).\nThus, linear and logistic guides constantly help to reduce the BC loss further and increase the reward; when we introduce the advanced SCATS guide, the loss and reward get worse first for a while, mainly due to the SCATS guide being non-smooth, but then, the training gets immediately improved since SCATS is a well-tailored industrial solution for traffic signal control, being configured perfectly for the task.\n4) Ablation Studies: Ablation studies are conducted to as- sess the impact of each component within our GuidedLight."}, {"title": "VI. DISCUSSION", "content": "Choices of Phase Plan: This paper uses a 4-phase plan as an example, which covers most intersections. It can also handle 3-phase plans by adding masks in```json\n state and action. For the 5-phase plan, state and action need to be extended. In addition, due to the integrated features of FRAP [5], our method can also handle various types of intersections.\nGeneralization to Different Intersection Structures: Given different intersections can have quite different struc- tures, with different numbers of lanes, movements, legs, and shapes (each as T-shape, Y-shape), for a more generalized solution, we recommend using a general plug-in module from GESA [9], which maps various intersections to a unified 4-leg intersection."}, {"title": "VII. CONCLUSIONS", "content": "This paper proposed GuidedLight, an RL-based model for TSC. GuidedLight is delicate to further improve RL meth- ods' applicability in the real world. Several contributions have been made. From the model's perspective: (1) the RL setting is strictly designed following the industry standards in terms of input, output, evaluation, and so on; (2) moreover, to pursue the standard cycle-flow relation, we innovatively incorporate traditional yet effective methods such as SCATS as the guidance and use behavior cloning and curriculum learning to teach the agents. From the data's perspective, our simulation scenario is strictly designed to replicate the real-world case. We theoretically prove that the guided RL can guarantee a polynomial sample complexity, and rigid experiments confirm that our GuidedLight could achieve higher performance and uphold industry requirements.\nFuture work: we will deploy the GuildedLight model in real cities and consider utilizing multi-agent reinforcement learning to facilitate collaborative coordination among mul- tiple intersections, further improving traffic efficiency while ensuring alignment with industry needs."}]}