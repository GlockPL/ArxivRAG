{"title": "HOW TO CORRECTLY DO SEMANTIC BACKPROPAGATION\nON LANGUAGE-BASED AGENTIC SYSTEMS", "authors": ["Wenyi Wang", "Hisham A. Alyahya", "Dylan R. Ashley", "Oleg Serikov", "Dmitrii Khizbullin", "Francesco Faccio", "J\u00fcrgen Schmidhuber"], "abstract": "Language-based agentic systems have shown great promise in recent years, tran-\nsitioning from solving small-scale research problems to being deployed in chal-\nlenging real-world tasks. However, optimizing these systems often requires sub-\nstantial manual labor. Recent studies have demonstrated that these systems can be\nrepresented as computational graphs, enabling automatic optimization. Despite\nthese advancements, most current efforts in Graph-based Agentic System Opti-\nmization (GASO) fail to properly assign feedback to the system's components\ngiven feedback on the system's output. To address this challenge, we formalize\nthe concept of semantic backpropagation with semantic gradients\u2014a generaliza-\ntion that aligns several key optimization techniques, including reverse-mode auto-\nmatic differentiation and the more recent TextGrad by exploiting the relationship\namong nodes with a common successor. This serves as a method for comput-\ning directional information about how changes to each component of an agentic\nsystem might improve the system's output. To use these gradients, we propose\na method called semantic gradient descent which enables us to solve GASO ef-\nfectively. Our results on both BIG-Bench Hard and GSM8K show that our ap-\nproach outperforms existing state-of-the-art methods for solving GASO problems.\nA detailed ablation study on the LIAR dataset demonstrates the parsimonious na-\nture of our method. A full copy of our implementation is publicly available at\nhttps://github.com/HishamAlyahya/semantic_backprop", "sections": [{"title": "1 INTRODUCTION", "content": "Language-based agentic systems are being hailed as a major breakthrough in artificial intelligence,\nwith real-world deployment well underway and numerous companies already being founded based\non this technology (e.g., Pythagora-io (2023)). Such agentic systems typically consist of multiple\ncomponents. These components are selected to perform specific tasks, such as question answering,\nimplementing and executing computer programs, or performing web searches (Wang et al., 2024;\nGuo et al., 2024). Due to the strength of Large Language Models (LLMs) in doing a wide array of\ntasks, agentic systems typically have most of their key components rely on querying LLMs. This\nresults in communication between the components of such systems being handled with free-form\nnatural language (Zhuge et al., 2023). However, while relying on LLMs does partially alleviate the\nengineering burden of building such systems, designing agentic systems remains nontrivial.\nAgentic systems are often modeled as computational graphs, with components involving frozen large\nmodels having auxiliary optimizable parameters (Zhuge et al., 2024). When the graph topology is\nfixed, the challenge of optimizing these parameters to enable an agentic system to solve a specific\nproblem can be modeled as the Graph-based Agent System Optimization (GASO) problem."}, {"title": "2 BACKGROUND", "content": ""}, {"title": "2.1 THE GRAPH-BASED AGENTIC SYSTEM OPTIMIZATION PROBLEM", "content": "The Graph-based Agentic System Optimization (GASO) problem aims to optimize a system capable\nof delivering precise answers to a variety of user queries through a structured computational ap-\nproach. We formalize this system using a directed acyclic computational graph (V, \u0395, \u0397, \u0398). In this\ngraph, V represents the set of vertices or nodes, with each node v \u2208 V being a variable within the\nsystem. E denotes the set of directed edges between nodes, H is a set of forward functions assigned\nto certain nodes, and \u2295 \u2282 V consists of variables that are optimizable parameters.\nFor any vertex v in the graph, let Predecessors(v) denote its predecessor vertices and Successors(v)\ndenote its successor vertices. If a node v \u2208 V has no predecessors, it either holds a specified user\nquery Q or an optimizable parameter \u03b8\u2208 \u0398. Conversely, if a node has predecessors, it contains the\nresult of computations performed by the function hr \u2208 H on its predecessors, expressed as:\n$\\forall v \\in V s.t. Predecessors(v) \\neq \\emptyset, v = h_r (Predecessors(v)).$  (1)\nThe final response A to the user queryQ is produced by a special output node where\nSuccessors(v) = (). The notation A(Q, \u0398) highlights the functional relationship between the graph\noutput and the user query-parameter pair. To obtain the final response, each node of the graph is\nexecuted in topological order.\nThe objective in the GASO problem is to find a set of parameters * that minimizes the expected\nloss EQ~D [l(Q, A(Q, \u0398*))] over a distribution of queries D. Here, I represents a loss function (e.g.,\nnegative utility) defined for a query and its corresponding response. In addition to the loss l(Q, \u0391),\na semantic alphanumeric feedback F(Q, A) is provided as an additional signal for optimization. In\nthis paper, we focus on cases where variables are represented in free-form natural language; that is,\nQ, A, O, and the outputs of h\u2082 are composed of alphanumeric strings. To effectively process the\nsemantic content, the functions hr often require querying LLMs. For instance, consider a variable\nva linked to its predecessors vq (a user query) and v\u0259 (an additional optimizable instruction that can\naffect the response to vq). The function for va could be expressed as hva (vq, ve) = LLM(vq \u2295 vo),\nwhere denotes the concatenation operator, and the LLM function returns an LLM response. In\npractical applications, ve is often a prompt prefix or suffix optimized to improve the LLM's response\nto the task. In the general case, forward functions could take more complex forms, such as accessing\na file, executing a command line and reading the result, and managing some internal thought (Wang\net al., 2024; Zhang et al., 2024; Jin et al., 2024)."}, {"title": "2.2 REVERSE-MODE AUTOMATIC DIFFERENTIATION", "content": "When the forward functions and the loss function are differentiable, first-order optimization methods\n(see the work of Beck (2017)) compute the gradient of the loss with respect to all optimizable\nparameters to solve GASO. Reverse-mode automatic differentiation (Linnainmaa, 1970; 1976), or"}, {"title": "3 METHODS", "content": "Building on the foundational work of Linnainmaa (1970; 1976), Pryzant et al. (2023), and Yuksek-\ngonul et al. (2024), we introduce the concept of Semantic Backpropagation over Semantic Gradients.\nIn the context of the GASO problem, the semantic gradient of a loss function l(Q, A(Q, \u04e8)) with\nrespect to a variable v, denoted as Vul\u0119, provides directional information on how altering v can\nimprove system performance for a query Q. Semantic backpropagation employs the final semantic\ngradient AlQ-typically derived from the answer feedback F(Q, A)\u2014to generate semantic gra-\ndients Vul for all variables v \u2208 V. Specifically, for a given variable v, the backward functions\u2014\nacting as a generalization of the chain rule-are used to partially compute the semantic gradients\nfor Predecessors(v) using the semantic gradient with respect to v. This procedure is systematically\napplied to all variables v \u2208 V, proceeding in reverse topological order.\nUsing semantic gradients, we propose Semantic Gradient Descent to address the GASO problem.\nGeneralizing from numerical gradient descent (Lemar\u00e9chal, 2012), this method involves the follow-\ning iterative steps: (1) Sample a query Q from a distribution D, (2) Apply semantic backpropagation\nto compute the semantic gradients Vul\u0119 for all variables v \u2208 V if l(Q, A(Q, \u04e8)) exceeds a speci-\nfied threshold, (3) Use an optimizer & on each parameter \u03b8 \u2208 \u0398, guided by its semantic gradients,\nto update the optimizable parameters. The subsequent sections detail the mechanisms of semantic\nbackpropagation and semantic gradient descent."}, {"title": "3.1 SEMANTIC BACKPROPAGATION", "content": "Given a computational graph as described in Section 2, in our approach, we generalize the\nterm Jhin Equation (2) by introducing a set of backward functions {h : w \u2208 V,v \u2208\nPredecessors(w)}. Each backward function \u0125 serves as an analogue to the product of the deriva-\ntive and the Jacobian in RMAD, extending it to arbitrary forward functions hw that might incorporate\nnatural language. Specifically, for any query Q, node w \u2208 V, and v \u2208 Predecessors(w), h maps\nthe values of Predecessors(w), w, and the gradient \u2207wl\u0119, to a direction \u2207ul, in the space of v.\nThis direction represents how a change in v would affect w = hw (Predecessors(w)) in alignment\nwith ulo, while keeping the other predecessors fixed.\nInstead of the summation over successors in RMAD, we introduce an aggregation function A that\ncombines the set of directions {ulo : w \u2208 Successors(v)} into a single semantic gradient Vulq\nfor each variable v. This generalizes the summation operator in RMAD, allowing for more flexible\nand problem-specific methods of combining gradients. Formally, we have:\n$\\nabla_v l_Q = A_r(\\lbrace \\nabla_w l_Q : w \\in Successors(v) \\rbrace )$, where $\\nabla_u l_Q = \\hat{h}_{vw} (Predecessors(w), w, \\nabla_w l_Q)$\nfor all w \u2208 Successors(v). Algorithm 1 shows the procedure for backpropagation of semantic gra-\ndients. Our formulation thus extends the chain rule and RMAD to accommodate arbitrary functions\nand aggregation mechanisms."}, {"title": "3.2 SEMANTIC GRADIENT DESCENT", "content": "In this section, we first formalize the notion of parameter updating given semantic gradients, and\nthen present in detail the procedure in which it is applied."}, {"title": "3.2.1 PARAMETER UPDATE FUNCTION", "content": "Similar to numerical gradient descent, semantic gradient descent also requires a parameter update\nfunction, denoted as $. Given an optimizable parameter @ and a set of semantic gradients Go =\n{Volq; : i \u2208 N, i < k} of losses lq\u2081 = l(Qi, A(Qi, \u25d5)) for queries Qi, the function & updates\nthe parameter value by moving @ according to Ge. Analogously to numerical gradient descent, the\nparameter @ is updated by applying the formula 0 \u2013 0 \u2013\u03b1 \u03a3=1 Volq. One method to implement\ninvolves querying an LLM for an improved version of 0, conditioned on Ge. We adopt this strategy\nand detail our implementation of the parameter update function in Implementation 2."}, {"title": "3.2.2 THE OPTIMIZATION PROCEDURE", "content": "Given a parameterized graph, a loss function as described in Section 2, and a parameter update\nfunction 4, semantic gradient descent solves the agentic graph optimization problem as follows.\nThe optimizable parameters are first initialized. Then, we iteratively execute the following steps.\nFirst, we repeatedly sample a query Q and only compute the semantic gradients of lo with respect\nto the variables if l(Q, A(Q, \u04e8)) is above a certain threshold. Move to the next step if this threshold\ncondition is met b times for some batch size b. Second, we apply & to each parameter-gradients pair\n(0, Go) to obtain an alternative parameter value \u03b8' for each parameter \u03b8 \u2208 \u0398. Lastly, we apply an\nupdate gate\u00b3 so that the parameters values are updated if the newly generated values outperform the\ncurrent values on a validation set. Formally, define a validation function\n$L_{val}(\\Theta) = \\sum_{Q \\in Val} l(Q, A(Q, \\Theta)),$\nwhere elements of Val are samples from the query distribution D. We update \u0472 \u2190 {\u00a2(0, Go) : 0\u2208\n\u04e8} if Lval({(0, Go) : 0 \u2208 \u0398}) < Lval(\u0398). See Algorithm 2 for details.\nRemark Unlike numerical gradient descent, which keeps updating the solution in each iteration,\nour practical experience suggests that the update gate is essential to avoid the solution deviating to\nless favored regions. We argue that this gating process is necessary for consistent performance im-\nprovement against the always-update strategy implemented by many first-order optimization meth-\nods since there is a lack of theoretical justification for semantic gradient descent to improve. See\nempirical evidence for the significance of this gating in Section 5.2."}, {"title": "3.3 DIFFERENCE WITH TEXTGRAD", "content": "Here we present our difference with TextGrad (Yuksekgonul et al., 2024). Inspired by backpropa-\ngation, TextGrad aims to solve the GASO problem by propagating\"textual gradients\" in the reverse\ndirection of the computational graph. A textual gradient of a variable is defined as a criticism of the\nvariable presented in natural language. See Section 4.1 for more details on textual gradients. Given\na query Q, TextGrad can be implemented as a special case of semantic backpropagation by having\nA as an identity function and the backward functions h satisfying:\n$\\hat{h}_{vw}(\\cdot, w, \\nabla_w l_Q) = \\hat{h}(., w, \\nabla_w l_Q)$, and  (3)\n$\\hat{h}_{vu}(Predecessors (w), w, \\nabla_w l_Q) = \\hat{h}(v, w, \\nabla_w l_Q)$  (4)\nfor all u \u2208 V, w \u2208 Successors(v), and u \u2208 Predecessors(w).\nNote that while TextGrad does not include such a mechanism, the official implementation of TextGrad\ndoes include it."}, {"title": "4 RELATED WORKS", "content": ""}, {"title": "4.1 TEXTUAL GRADIENTS FOR PROMPT OPTIMIZATION", "content": "The concept of textual gradient was first introduced in the context of prompt optimization as \u201ca local\nloss signal which contains information on how to improve the current prompt\" (Pryzant et al., 2023).\nFurthermore, Pryzant et al. (2023) propose ProTeGi, an optimization method that improves prompt\nparameters that are used to instruct an LLM to produce an answer. Given some query-expected-"}, {"title": "4.2 OTHER BACKPROPAGATION-INSPIRED METHODS FOR GASO", "content": "Trace (Cheng et al., 2024) models the execution trace of computer programs as computational graphs\n(i.e., trace graphs). In Trace, the authors introduce OptoPrime as an optimization algorithm similar\nto TextGrad and semantic backpropagation that uses a feedback signal propagated backwards in the\ntrace graph and then used for optimization. For each node v in the trace graph, a subgraph that\ncontains all nodes u such that there is a path from v through u and then to the output node is as-\nsigned as feedback to v. Conceptually, this subgraph includes all computations influenced by v that\nhave an effect on the output. Then, an optimizer (typically based on large language models) lever-\nages the subgraphs as feedback signals to improve the optimizable nodes. Although the subgraph\nof v contains all relevant information on how v influences the output, it does not directly indicate\nhow improvements can be made (i.e., there is no gradient-like information). Without gradient-like\ninformation gained from a backpropagation-like process, an optimizer must itself implicitly try to es-\ntimate such information\u2014a very non-trivial task. Another issue with the aforementioned subgraphs\nis that the size of these graphs scales linearly with the depth of the overall computational graph. This\nmakes Trace incompatible with large graphs when the optimizer uses transformer-based language\nmodels (Vaswani, 2017; Schmidhuber, 1992; Schlag et al., 2021). Unlike in Trace, semantic back-\npropagation explicitly backpropagates this gradient-like information and is not limited by the same\nlinear scaling requirement. Zhou et al. (2024)\u2014a concurrent work\u2500introduces a backpropagation-\nlike method for GASO which includes edge optimization. However, this approach is limited to\nchain-structured agentic systems, with edge optimization involving the addition, removal, or rear-\nrangement of agents within the chain."}, {"title": "4.3 OTHER METHODS FOR GASO", "content": "DSPy (Khattab et al., 2024) attempts to abstract executions of language model pipelines into text\ntransformation graphs. Each node in such a graph is defined as a declarative module that takes a\ntype of text as input and transforms it to another type of text as output. These nodes are parameter-\nized by the instructions and sets of demonstrations that are prefixed to the input text before querying\nan LLM. DSPy has many implementations of optimizers for finding the best values for these pa-\nrameters. Two such optimizers are BootstrapFewshot, which provides few-shot demonstrations for\neach step in the pipeline, and COPRO, which does coordinate-ascent optimization on each of the\noptimizable instruction prompts in the pipeline. These two optimizers are locally focused, i.e. each\nstep is only implicitly aware of the entire pipeline.\nGPTSwarm's node optimization method Zhuge et al. (2024) solves GASO when the edges are fixed,\nwhich is the focus of this paper. Compared to COPRO, in each iteration of GPTSwarm's node\noptimization method, each node's parameter is updated with respect to a local objective function\nspecific to the node. Such local objective functions are not always available and require an accurate\nunderstanding of the function of each specific node. Optimizing such a local objective function\nalso limits the possibility that a node could change its function through global optimization. On the\nother hand, GPTSwarm offers an edge optimization method that can be used as a complement of our\nmethod when approaching the GASO problem with optimizable edges."}, {"title": "5 EXPERIMENTAL RESULTS", "content": "In this section, we evaluate semantic gradient descent on BIG-Bench Hard (BBH) (Suzgun et al.,\n2023), GSM8k (Cobbe et al., 2021) (for comparing with TextGrad, Trace's OptoPrime and DSPy's\nCOPRO), BigCodeBench (BCB) (Zhuo et al., 2024), and LIAR (Wang, 2017) (for performing abla-\ntion and evaluating ProTeGi) datasets. Using these datasets allows for multi-domain benchmarking\nof our method. In all experiments, the number of forward computations required is significantly\nhigher than the number of backward computations and optimizer calls. Therefore, for consider-\nation of the cost-quality balance, unless otherwise specified, we use gpt-40-mini (a relative\ncheap language model) when performing forward execution and gpt-4-turbo when executing\nthe backward computation or the parameter update function. See Appendix B.1 for more discussion\non the choice of language models."}, {"title": "5.1 GENERAL QUESTION ANSWERING", "content": "In the GSM8K, BBH, and BCB datasets, we\ndo not provide any a priori information to the\nagentic system regarding the task (e.g., we do\nnot tell the system whether \"True\" should be\nrepresented by a \"1\" or by the word \"True\").\nThe computational graph consists of seven vari-\nables with three optimizable parameters, initial-\nized identical across tasks. The initialization is\nchosen to be generic. Specifically, the first two\nparameters are initialized to \"Work out an in-\ntermediate step that helps solve the problem\"\nand the last parameter is initialized to \"Solve\nthe problem\" (Figure 2, a).\nTasks. We experiment with the GSM8K, BBH,\nand BCB datasets. The GSM8K dataset in-\ncludes samples of mathematical problems with\na numerical answer. The BBH dataset con-"}, {"title": "5.2 SPECIALIZED INITIALIZATION EXPERIMENTS AND ABLATION STUDY", "content": "In this section, we perform an ablation study to validate the importance of each component in the\nsemantic gradient descent pipeline. We look at a more realistic scenario where the variables of the\ninitial graph are highly specialized. This matches better the contemporary usage of agentic systems,\nwhere the components of the systems are assigned to implement specific and fine-grained functions\n(Wang et al., 2024), e.g., considering the question from a particular role's perspective (Li et al.,\n2023). Please also refer to Appendix A for additional ablation experiments using different network\narchitectures and using a different forward engine.\nTask. In the LIAR dataset (Wang, 2017), the task is to decide whether a political statement is a lie or\nnot. Each sample in the dataset consists of five attributes, i.e., (i) the statement, (ii) the political party\nof the speaker, (iii) the job title of the speaker, (iv) the state from which the speaker comes from, and\n(v) the source from which this statement is released. This five-attribute structure leads to an intuitive\ndecomposition of the problem, where each component of an agentic system analyzes an attribute\nand then merges the analysis(Figure 2, b). This intuitive decomposition is desirable here as it allows\nfor a relatively naive yet practically plausible agentic system architecture and prompts. Thus we can\nfocus our evaluation on the performance of the optimizers. We use the binary classification version\nof LIAR as done by Pryzant et al. (2023).Here, the prefix of the response (required to be either \u201cYes\u201d\nor \u201cNo\u201d) is used to determine how the agentic system has classified a query.\nExperiment Design. Following the aforementioned decomposition strategy, we optimize a graph\nof 13 variables, of which six are optimizable parameters. These six optimizable parameters serve as\ninstructions for an LLM. Five are initialized to guide the LLM in analyzing specific attributes of a\nsample, while the last parameter instructs the LLM to formulate a final answer based on the previous\nanalyses. See Figure 2 for the visualization of the initial graph.\nWe compare our optimization method with four variants: (1) optimizing without semantic gradients\nby removing the feedback (see Implementation 1) as input of the parameter update function; (2)\noptimizing one parameter only (running this variant six times with a different parameter each time\nand reporting the average); (3) optimizing with semantic gradients computed without conditioning\nthe neighborhood (i.e., as in Equation (3)), emulating TextGrad in our implementation; and (4)\noptimizing without the update gate introduced in Section 3.2, where update gate accepts parameter\nupdates only if they performs better on a validation set.\nEach variant is applied for 8 iterations. We run all the variants five times with different random seeds\nexcept for the variant that optimizes one parameter only. For this variant, we try optimizing each\nof the six parameters separately (once each) and report the average of these six optimizations. We"}, {"title": "6 CONCLUSION", "content": "In this work, we tackled the challenge of optimizing language-based agentic systems by introduc-\ning semantic gradients and semantic backpropagation. These concepts generalize existing credit-\nassignment methods, such as reverse-mode automatic differentiation and TextGrad, by incorporating\nneighborhood conditioning to compute directional information which can be leveraged to improve\neach optimizable component of the system. This framework enabled us to propose semantic gra-\ndient descent, effectively solving the Graph-based Agentic System Optimization (GASO) problem.\nAltogether, our results indicate that semantic gradients can significantly reduce the manual effort\nneeded to optimize agentic systems, paving the way for greater scalability in AI solutions."}]}