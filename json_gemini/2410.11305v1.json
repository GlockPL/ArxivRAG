{"title": "QSPEC: SPECULATIVE DECODING WITH COMPLEMENTARY QUANTIZATION SCHEMES", "authors": ["Juntao Zhao", "Wenhao Lu", "Sheng Wang", "Lingpeng Kong", "Chuan Wu"], "abstract": "Quantization has been substantially adopted to accelerate inference and reduce memory consumption of large language models (LLMs). While activation-weight joint quantization speeds up the inference process through low-precision kernels, we demonstrate that it suffers severe performance degradation on multi-step reasoning tasks, rendering it ineffective. We propose a novel quantization paradigm called QSPEC, which seamlessly integrates two complementary quantization schemes for speculative decoding. Leveraging nearly cost-free execution switching, QSPEC drafts tokens with low-precision, fast activation-weight quantization, and verifies them with high-precision weight-only quantization, effectively combines the strengths of both quantization schemes. Compared to high-precision quantization methods, QSPEC empirically boosts token generation throughput by up to 1.80\u00d7 without any quality compromise, distinguishing it from other low-precision quantization approaches. This enhancement is also consistent across various serving tasks, model sizes, quantization methods, and batch sizes. Unlike existing speculative decoding techniques, our approach reuses weights and the KV cache, avoiding additional memory overhead. Furthermore, QSPEC offers a plug-and-play advantage without requiring any training. We believe that QSPEC demonstrates unique strengths for future deployment of high-fidelity quantization schemes, particularly in memory-constrained scenarios (e.g., edge devices).", "sections": [{"title": "INTRODUCTION", "content": "Large language models (LLMs) have demonstrated remarkable abilities across various domains, including mathematics, coding, and planning (Shao et al., 2024b; Guo et al., 2024a; Huang et al., 2024). Nonetheless, their immense scales pose substantial challenges for deployment due to high memory and computational demands, especially in resource-limited scenarios (e.g., inference on edge devices). Quantization has been an effective compression technique to facilitate LLM inference with limited resources (Lin et al., 2024a; Ashkboos et al., 2024; Zhao et al., 2024c; Lin et al., 2024b). By converting high-precision values (e.g., FP16) into their lower-precision counterparts (e.g., INT4), quantization effectively lowers memory and computational requirements, allowing for larger serving batches and model sizes. Furthermore, the reduced memory footprint boosts token generation throughput by accelerating the typically memory-bound autoregressive decoding process (Zhao et al., 2024a).\nBased on the quantized objects, recent quantization algorithms can be broadly classified into two categories: weight-only and WXAX: (1) Weight-only quantization, represented by W4A16 (Lin et al., 2024a), quantizes model weights to low precision (e.g., 4-bit) for storage, and then dequantizes them to a higher precision (i.e., FP16) during inference; (2) WXAX methods, such as W4A4 (Ashkboos et al., 2024; Zhao et al., 2024c) and W8A8 (Xiao et al., 2023), simultaneously quantize both weights and activations, and leverage low-precision hardware support for faster execution without dequantizing them to higher precision. Nevertheless, WXAX schemes generally suffer model performance"}, {"title": "MOTIVATION", "content": ""}, {"title": "COMPROMISED PERFORMANCE OF ACTIVATION QUANTIZATION", "content": "State-of-the-art (SOTA) activation-weight joint quantization methods, like Atom (Zhao et al., 2024c) and QuaRot (Ashkboos et al., 2024), achieve notable speed-ups with negligible performance loss compared to weight-only ones. However, we argue that this conclusion is skewed by the limited evaluation benchmarks, which fail to capture the negative impacts of activation quantization.\nTo substantiate this claim, we conduct experiments on Llama-3-8B-Instruct models (Dubey et al., 2024) quantized with W16A16, W4A16, and W4A4 methods across four task datasets: PIQA (Bisk et al., 2020), WikiText-2 (Merity et al., 2016), GSM8K (Cobbe et al., 2021), and MBPP (Austin et al., 2021). PIQA is a two-choice commonsense reasoning benchmark for physical knowledge, evaluated using classification accuracy, while WikiText-2 comprises a collection of high-quality Wikipedia articles, assessed for language fluency via perplexity (Jelinek et al., 1977). Both are commonly adopted in current quantization evaluations. GSM8K includes diverse grade school mathematical problems, evaluated by \u201cexact match\u201d metrics; MBPP focuses on crowd-sourced Python programming challenges, assessed by accuracy. Unlike the former two benchmarks, both GSM8K"}, {"title": "HIGH-SIMILARITY TOKEN PREDICTIONS", "content": "Despite the notable performance decline caused by activation quantization, we observe, more microscopically, high similarity in top-1 token predictions between quantization schemes with high and low precision activations. Specifically, we first employ Atom-based W4A16 greedy sampling to generate the golden token sequences for the GSM8K test set, obtaining the prediction probabilities for each top-1 answer token. Subsequently, we perform one Atom-based W4A4 forward pass (i.e., prefill) on the concatenated input of each question and its corresponding golden answer to acquire the token probabilities as well. This allows us to assess the prediction discrepancy between W4A4 and W4A16. As illustrated in Figure 2, we observe that (1) the majority of token prediction probabilities of both W4A4 and W4A16 exceed 80%, and most of the tokens associated with high probabilities are accepted. (2) Compared to accepted tokens, the number of rejected ones is negligible, underscoring the high similarity between the two quantization methods. Combined with the analysis in Sec. 2.1, this can be interpreted that a small set of salient token variations can trigger a snowball effect of errors, especially on multi-step reasoning tasks where the subsequent steps are closely conditioned on the previous ones, akin to findings in Zhang et al. (2023), thus impairing the performance of the low-precision activation scheme. Prior studies indicate that low similarity leads to frequent token rejections, thereby diminishing the efficiency of speculative decoding (Leviathan"}, {"title": "\u041c\u0415\u0422\u041dOD", "content": "Targeting an efficient quantization scheme without sacrificing performance or increasing memory consumption, we propose a new quantization paradigm called speculative decoding with complementary quantization execution (QSPEC). As shown in Figure 1, QSPEC employs and-verpipeline for next-token prediction with varying activation precisions and shared low-precision quantized weights, instead of a constantsingleaquantization scheme. The details are elaborated below, adhering to the core component breakdown of regular speculative decoding (Leviathan et al., 2023)."}, {"title": "QSPEC", "content": "Draft Phase. Current LLMs typically utilize an autoregressive process for next-token prediction, where a new token is drawn from a probability distribution conditioned on all previously generated tokens. This process can be formulated as:\n$t_{i+1} \\sim P_{i+1}(t) := M(t_{i+1}|T_{<i}),$ \nwhere M denotes the model including the weight and activation configurations, while $t_{i+1}$ and $T_{<i}$ represent the next predicted token and the preceding token sequence ($t_0, t_1,..., t_i$), respectively.\nCompared with previous research (Leviathan et al., 2023; Chen et al., 2023), on one hand, we employ a weight-shared quantization scheme with low-precision activations, rather than one standalone small-sized model, to speculate the next y tokens $T_{i+1:i+y}$ and their associated distributions $P_{i+1:i+y}(t)$. In $T_{i+1:i+y}$, each token $t_j$ is sampled from $M_r(t_j|T_{<i}, T_{i+1:j-1})$, where $j \\in [i + 1, i + y]$ and $M_r$ represent our quantized model executed with low-precision activation. On the other hand, our low-precision quantization scheme shares similar attributes as the draft model in Leviathan et al. (2023), as both can generate tokens rapidly, though with reduced quality.\nVerify Phase. To compensate for the performance decline incurred by excessive quantization, we employ a high-precision weight-only quantization scheme to verify the proposed draft token sequence. This ensures that the final generation quality aligns with that of a high-precision activation quantization scheme. All drafted tokens are verified in parallel for higher efficiency.\nFormally, the high-precision quantization scheme $M_h$ receives as input the concatenation of $T_{<i}$ and $T_{i+1:i+y}$, producing high-quality prediction probabilities $P_{i+1:i+y+1}(t)$ through a single forward pass. Following this, an acceptance policy $A$, which will be detailed later, is applied to rectify each drafted token sequentially. Once a token $t_{i+j}$ is rejected, all subsequent tokens are discarded, and token $t_{i+j}$ is resampled according to the distribution $p_{i+j}(t)$. In the optimal scenario, all drafted"}, {"title": "ADVANTAGE ANALYSIS", "content": "As shown in Table 2, we compare QSPEC with individual quantization schemes (i.e., W4A4 and W4A16) as well as speculative decoding across the dimensions of memory, computation, and generation. QSPEC offers several key advantages over these methods, detailed as follows:\n\u2022 Memory-efficient. Quantization is often motivated by memory constraints, rendering regular speculative decoding unsuitable due to the additional memory allocation for the weights and KV caches of the draft model. However, QSPEC addresses these memory overheads by sharing weights and overwriting KV caches, aligning with the costs associated with standalone high-precision activation quantization.\n\u2022 No efficiency-efficacy trade-off. Leveraging the speculative decoding framework, QSPEC achieves efficiency gains without any quality sacrifice, thereby avoiding the trade-off between efficiency and efficacy. In contrast, individual quantization methods either endure significant performance degradation or accept reduced inference speed.\n\u2022 High acceptance rate. The shared weights inherently enable a strong similarity between the two quantization methods. Besides, the KV cache overwriting further enhances the consistency of subsequent predictions. Both factors collectively contribute to a high token acceptance rate of QSPEC."}, {"title": "EXPERIMENTS", "content": "Our evaluation answers three key questions:\nQ1: Does QSPEC serve as a general solution without compromising quality compared to high-precision weight-only quantization methods? (Sec. 4.2)\nQ2: Does QSPEC consistently accelerate high-precision weight-only quantization methods? (Sec. 4.3)\nQ3: What is the common acceptance rate of QSPEC, and how does the length of draft tokens affect it? (Sec. 4.3)"}, {"title": "GENERAL SETUP", "content": "Benchmarks. We assess QSPEC with two primary criteria: (1) generation fidelity and (2) end-to-end serving speedup. For fidelity evaluation, we adopt not only traditional tasks, including PIQA (500, 10-shot) (Bisk et al., 2020), WinoGrande (500, 5-shot) (Sakaguchi et al., 2019), and Wiki-Text2 (Merity et al., 2016), but also challenging multi-step reasoning tasks such as GSM8K (All, 8-shot) (Cobbe et al., 2021), MATH (All, 4-shot) (Hendrycks et al., 2021), MBPP (200, 0-shot) (Austin et al., 2021), and HumanEval (All, 0-shot) (Chen et al., 2021). To measure the acceleration, we use all the above reasoning tasks and two additional chatbot datasets, namely ShareGPT (RyokoAI, 2021) and LMsys-1K (Zheng et al., 2023). Following the setup of Atom (Zhao et al., 2024c), we randomly sampled the dataset for the request prompts to reduce the workload. Due to memory limitations, we vary the batch size from 8 to 32 and serve all requests in a first-come, first-served (FCFS) manner. Once any request is finished, we refill the batch, adhering to the continuous batching approach of ORCA (Yu et al., 2022). We use greedy sampling for token generation.\nBase Models. To assess the effectiveness and scalability of our approach, we conduct experiments using multiple models from the Llama family (Dubey et al., 2024)\u00b9 with varying scales and capacities: Llama3.2-3b, Llama2-7b, Llama3-8b-instruct, and Llama2-13b.\nImplementation. All experiments are performed on a node equipped with four NVIDIA A100 GPUs (40GB HBM each) running CUDA 12.5. To demonstrate the versatility of QSPEC, we implement two SOTA 4-bit quantization methods, namely Atom (Zhao et al., 2024c) and QuaRot (Ashkboos et al., 2024). For W4A16 configurations, we incorporate AWQ-style (Lin et al., 2024a) weight dequantization logic for runtime inference. We select Atom to showcase the acceleration of QSPEC. We use these Group-wise quantization schemes with a group size of 128. With the draft token length y as 3, we simulate the performance of QSPEC by initially employing fake quantization to fully emulate the execution flow, encompassing both the draft and verify stages of QSPEC. Subsequently, we replay the collected traces with real kernel execution to accurately reproduce the latency."}, {"title": "FIDELITY EVALUATION", "content": "QSPEC effectively maintains the generation quality of W4A16, whereas W4A4 does not. As listed in Table 3, with the draft verification of W4A16, QSPEC exhibits only minimal performance fluctuations compared to W4A16. This negligible variation may stem from the nondeterministic algorithms of PyTorch\u00b3 or occasional cases where two tokens have the same maximum prediction probability. In contrast, W4A4 experiences a substantial performance decline exceeding 10% across most tasks, with the reduction becoming more pronounced as task difficulty increases. For instance,"}, {"title": "ACCELERATION EVALUATION", "content": "QSPEC exhibits a substantial efficiency boost compared to W4A16. In Table 4, we present the token generation throughput for both QSPEC and W4A16 across different model sizes, quantization configurations, and batch sizes on diverse datasets. On average, QSPEC achieves a throughput increase of 1.38\u00d7 over W4A16 across all settings, with a peak improvement of 1.80\u00d7."}, {"title": "Ablation on Draft Token Length", "content": "To assess parameter sensitivity, we vary the draft token lengths y, the sole hyper-parameter of QSPEC, from 2 to 7 across all the benchmarks using Llama3-8b-instruct and Llama3.2-3b models. As depicted in Figure 5, an increase in y leads to a gradual decline in the token acceptance rate, since all subsequent tokens are discarded once a token is rejected. Nevertheless, even at y = 7, the token acceptance rate remains relatively high, approximately 70%, compared to 28 ~ 58% in 160m-7b draft-target model pair under y = 5 in conventional speculative decoding (Liu et al., 2024). Additionally, a consistent improvement in throughput is observed compared to W4A16, indicating the robustness of QSPEC with respect to \u03b3."}, {"title": "RELATED WORK", "content": "Quantization is a common technique for deploying LLMs on resource-limited scenarios. Broadly, recent quantization algorithms can be classified into two categories: weight-only W4A16 and weight-activation joint W4A4. Notably, AWQ (W4A16) (Lin et al., 2024a) redistributes the quantization burden by scaling salient weight channels to protect them from degradation. In contrast, W4A4 aggressively quantizes activations to leverage low-precision hardware for improved speed at the cost of model quality degradation. To address this challenge, Atom (Zhao et al., 2024c) proposes reordering outlier channels in the activation through offline profiling. Similarly, QuaRot (Ashkboos et al., 2024) employs Hadamard matrices to apply computational invariance on weights. Despite these advancements, our observations indicate that W4A4 methods still exhibit substantial degradation compared to weight-only quantization approaches across multi-step reasoning tasks. On the other hand, adaptive quantization aims to optimize the trade-off between quantization-induced quality degradation and computational acceleration by mixed precision. LLM-PQ (Zhao et al., 2024a) proposes an adaptive layer-wise bitwidth selection approach, while QAQ (Dong et al., 2024) focuses on KV-cache bitwidth optimization. Other works operate at finer granularity to address outliers (Lee et al., 2024). However, these methods cannot fully recover the generation quality of higher precision.\nSpeculative Decoding leverages a draft model to generate candidate tokens, which are then validated by a target model (Leviathan et al., 2023). Recent research has primarily focused on improving the acceptance rate and generation speed of candidate tokens. SpecInfer (Miao et al., 2024) introduces a boost-tuned small language model to generate candidate tokens in tree structures, enabling single-pass verification. In contrast, EAGLE (Li et al., 2024) adopts an aggressive pruning strategy for the draft model's architecture, allowing penultimate layer feature prediction with minimal computational overhead. Self-speculative decoding, a subset of this technique, employs a single model for both draft generation and verification. LayerSkip (Elhoushi et al., 2024) introduces a training methodology for early exit with layer drop, subsequently verifying partially generated tokens through full model inference. Medusa (Cai et al., 2024) constructs a generation tree of multiple candidate continuations by augmenting the original LLM with additional heads atop the final hidden state while relaxing the acceptance policy. However, these approaches inevitably require retraining of the original model, which can be computationally expensive and time-consuming.\nParameter Sharing has been extensively applied for various purposes in previous research. Targeting parameter savings, Universal Transformer (Dehghani et al., 2018) shares all layers within a transformer model, while Subformer (Reid et al., 2021) shares its middle layers without sacrificing performance. Similarly, DictFormer (Lou et al., 2021) reparameterizes the model using a shared dictionary alongside unshared coefficients and indices, achieving reduced parameter redundancy and faster computations. Pires et al. (2023) enhances both accuracy and latency by implementing a single, larger shared feed-forward network across the encoder. In a different domain, Wang et al. (2024b;a) and Kopiczko et al. (2023) leverage parameter sharing in low-rank adaptation (LoRA) (Hu et al., 2021) to improve parameter efficiency. Unlike these methods, our focus is on sharing low-precision weights from two quantization schemes to maintain memory overhead."}, {"title": "CONCLUSION", "content": "In this paper, we begin by validating that multi-step reasoning tasks can capture performance degradation incurred by activation quantization more sensitively and consistently than current evaluation protocols, advocating for their incorporation for a more comprehensive assessment. With nearly cost-free execution switching and high token-level similarities, we introduce QSPEC, a novel quantization paradigm that seamlessly synergizes two complementary weight-shared quantization schemes with speculative decoding. Empirically, QSPEC achieve up to 1.80\u00d7 acceleration without any quality sacrifice across diverse settings. Alongside consistent memory consumption and a plug-and-play property, these advantages distinguish QSPEC from any existing solution, promising it for high-fidelity quantization deployment, particularly in memory-constrained scenarios."}, {"title": "EVALUATION DATASETS", "content": "We utilize the following datasets for evaluation:"}, {"title": "LANGUAGE MODELING AND COMMONSENSE REASONING", "content": "\u2022 WikiText-2 (Merity et al., 2016): A language modeling dataset comprising over 100 million tokens extracted from high-quality Wikipedia articles.\n\u2022 PIQA (Bisk et al., 2020): A benchmark for physical commonsense reasoning, focusing on questions about everyday physical interactions.\n\u2022 Winogrande (Sakaguchi et al., 2019): A dataset of 44,000 problems inspired by the Winograd Schema Challenge, designed to test commonsense reasoning with reduced linguistic biases."}, {"title": "MATHEMATICAL REASONING", "content": "\u2022 GSM8K (Cobbe et al., 2021): A collection of 8,500 linguistically diverse grade school math problems that require multi-step reasoning using basic arithmetic operations.\n\u2022 MATH (Hendrycks et al., 2021): A dataset of 12,500 challenging competition-level math problems, each accompanied by detailed step-by-step solutions."}, {"title": "PROGRAMMING PROBLEMS", "content": "\u2022 MBPP (Austin et al., 2021): A benchmark of approximately 1,000 beginner-level Python programming problems, each with a task description, solution code, and automated test cases.\n\u2022 HumanEval (Chen et al., 2021): An evaluation set of 164 original programming problems used to assess functional correctness in code synthesis from docstrings."}, {"title": "CHATBOT DATASETS", "content": "\u2022 ShareGPT52K (RyokoAI, 2021): This dataset comprises approximately 52,000 conversations collected via the ShareGPT API before it was discontinued. The dataset captures both user prompts and the corresponding responses from OpenAI's ChatGPT, providing insights into human-AI dialogue dynamics.\n\u2022 LMsys-chat-1M-1K Zheng et al. (2023): Gathering one million authentic conversations with 25 leading large language models (LLMs), this dataset was sourced from over 210,000 unique IP addresses interacting with the Vicuna demo and Chatbot Arena websites."}, {"title": "DATASETS SAMPLING", "content": "To construct our test sets, we randomly sampled from the original datasets using torch.sample with a fixed seed of 42, and constructed them as prompt instructions following the Open-Instruct templates (Wang et al., 2023). The sample sizes for each dataset are as follows:\n\u2022 Fidelity Evaluation:\nWikiText-2: All samples\nPIQA: 500 samples\nWinogrande: 500 samples\nGSM8K: 200 samples\nMATH: 700 samples, balanced by selecting 100 questions from each of the seven distinct question types\nMBPP: 200 samples\nHumanEval: All samples\n\u2022 Acceleration Evaluation: 100 samples from each dataset, maximum output length set to 200 tokens."}, {"title": "RECOMMENDED APPLICATIONS", "content": "We further analyze the best practice for efficiently deploying QSPEC based on our empirical results:\nHigher Matrix Multiplication Cost. The primary advantage of low-precision quantization over weight-only quantization lies in their ability to better utilize non-attention compute kernels for speedup. However, certain architectural choices, such as Group-Query Attention (Ainslie et al., 2023) reduce the general matrix multiplication (GEMM) cost in the QKV (query, key, value) computations, which in turn diminishes the overall speedup of Llama3-8b compared to Llama2-7b.\nAppropriate Batch Size. The optimal performance of QSPEC requires an appropriately sized batch. Specifically, the batch size should be neither too large nor too small. Excessively large batch sizes can cause the performance W4A16 verify to degrade as it scales proportionally with \u03b3. Conversely, very small batch sizes are also suboptimal because low-precision kernels are most effective when computations are compute-bound; only memory footprint reductions offer benefits if computations are memory-bound. Moreover, excessive small batch size may not meet the tensor shape requirements necessary to exploit tensor cores (Zhao et al., 2024b) fully.\nAdditionally, in our experiments, we observed that some requests fail to generate the correct termination token and instead continue generating until they are truncated at the maximum generation length. As batch size increases, these excessively long requests are redundantly computed up to their maximum sequence length within the computing batch due to the first-in, first-out mechanism. This negative impact is magnified with larger batch sizes and draft-verify mechanism, reducing overall pipeline efficiency. This effect is particularly pronounced for the MBPP and HumanEval datasets, where the ratio of maximum length to average generation length is relatively large. Consequently, pipeline efficiency decreases further, leading to significant performance degradation with larger batches. Implementing appropriate early exit criteria would address this issue.\nThese observations suggest that QSPEC is particularly well-suited for applications involving, non-attention compute-intensive and moderate batch sizes, where its advantages can be fully leveraged for optimal performance."}]}