{"title": "FLASHRNN: OPTIMIZING TRADITIONAL RNNS ON MODERN HARDWARE", "authors": ["Korbinian P\u00f6ppel", "Maximilian Beck", "Sepp Hochreiter"], "abstract": "While Transformers and other sequence-parallelizable neural network architectures seem like the current state of the art in sequence modeling, they specifically lack state-tracking capabilities. These are important for time-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs, as well as modern variants like sLSTM do have these capabilities at the cost of strictly sequential processing. While this is often seen as a strong limitation, we show how fast these networks can get with our hardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the register level on modern GPUs. We extend traditional RNNs with a parallelization variant that processes multiple RNNs of smaller hidden state in parallel, similar to the head-wise processing in Transformers. To enable flexibility on different GPU variants, we introduce a new optimization framework for hardware-internal cache sizes, memory and compute handling. It models the hardware in a setting using polyhedral-like constraints, including the notion of divisibility. This speeds up the solution process in our ConstrINT library for general integer constraint satisfaction problems (integer CSPs). We show that our kernels can achieve 50x speed-ups over a vanilla PyTorch implementation and allow 40x larger hidden sizes compared to our Triton implementation. Our open-source kernels and the optimization library are released here to boost research in the direction of state-tracking enabled RNNs and sequence modeling: https://github.com/NX-AI/flashrnn", "sections": [{"title": "Introduction", "content": "Sequence models are at the core of many applications like time-series modeling, natural language processing, text, audio and video models, and predictions for physical systems based on ODEs or PDEs [Vaswani et al., 2017, Degrave et al., 2022, Nearing et al., 2024]. While there are modern sequence-parallelizable architectures like the Transformer [Vaswani et al., 2017], Mamba [Gu and Dao, 2023] or mLSTM [Beck et al., 2024], these lack the state-tracking capabilities [Merrill et al., 2024, Merrill and Sabharwal, 2023, Del\u00e9tang et al., 2023] of traditional RNNs like LSTM [Hochreiter and Schmidhuber, 1997], GRU [Cho et al., 2014] and sLSTM [Beck et al., 2024].\nTraditional RNNs include a recurrent connection or memory mixing, that connects the previous hidden state in a non-linear way to the current state update and this way mixes the states of different memory cells. While the sequence has to be processed step by step, computed hidden states and the recurrent matrix weights can stay cached, enabling a large speed optimization. In this work, we introduce FlashRNN as a generic hardware-optimized library for these RNN-style architectures.\nOur library facilitates research in the direction of state-tracking enabled RNN architectures, in two ways: Firstly, it enables easier and more efficient use of recent RNN-architectures like sLSTM [Beck et al., 2024]. This includes the notion of block-diagonal recurrent matrices that can speed up networks while lowering the number of parameters. Secondly, it can be easily extended to novel RNN-like architecture variants, as it supports generic state and gate numbers per cell. The LSTM [Hochreiter and Schmidhuber, 1997, F.A. Gers, 1999], with its two states and four gates (we consider the cell update as a fourth \"gate\" for simplicity here), can be implemented as easy as a simple Elman-RNN with one state and one gate [Elman, 1990], or sLSTM with its three states and four gates [Beck et al., 2024]."}, {"title": "Related work", "content": "Hardware-aware algorithms and their open-source implementations of common sequence modeling primitives have been focused primarily around the Transformer architecture [Vaswani et al., 2017] and its attention operation because of its ubiquity in language modeling. FlashAttention [Dao et al., 2022] introduced an IO-aware attention algorithm and CUDA implementation that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM, and achieves significant memory savings. FlashAttention2 [Dao, 2024] improves FlashAttention with better work partitioning and the additional parallelization over the sequence dimension. FlashAttention3 [Shah et al., 2024] takes advantage of new capabilites, such as asynchrony and FP8 low precision support of the recent NVIDA Hopper GPU generation.\nRecently, novel sequence models taking inspiration of Linear Attention [Katharopoulos et al., 2020] have shown promising performance compared to Transformer Attention [Beck et al., 2024, Yang et al., 2024, Dao and Gu, 2024]. Yang et al. [2024] provide an hardware-efficient algorithm and implementation in Triton for Gated Linear Attention that trades off memory movement against parallelizability and show that it is faster than FlashAttention2.\nTraditional RNNs like LSTMs [Hochreiter and Schmidhuber, 1997] or GRUs [Cho et al., 2014] are still widely used in many applications, such as for example time series modeling or reinforcement learning [Nearing et al., 2024, Degrave et al., 2022]. Many of these applications rely on optimized closed-source implementations of these RNN operations such as in the NVIDIA cuDNN 1 library, which is integrated in PyTorch. Sharvil [2020] provide an open-source alternative in CUDA for specific LSTM and GRU variants in their HASTE library, which served as inspiration for this work. HASTE is limited in speed due to a sequence of alternating calls of matrix multiplication and point-wise kernels, as well as its limitation to higher (but slower) precision."}, {"title": "Generic Recurrent Neural Network architecture with memory mixing", "content": "A generic RNN architecture that we aim to optimize has $N_s$ states $s^{(i)}_t \\in \\mathbb{R}^d$, and $N_g$ gates (or pre-activations) $g^{(i)}_t \\in \\mathbb{R}^d$, with $d$ being the embedding dimension or hidden size of the RNN. For example the LSTM [Hochreiter and Schmidhuber, 1997] has $N_s = 2$ states and $N_g = 4$ gates.\nEach gate receives an input $x^{(i)}_t \\in \\mathbb{R}^d$. As learnable parameters, the gates have a recurrent matrix $R^{(i)} \\in \\mathbb{R}^{d \\times d}$ that models the dependency on the previous hidden state $s^{(i)}_{t-1}$, and a bias $b^{(i)} \\in \\mathbb{R}^d$. The state sequence of the RNN is then defined as:\n$g^{(i)}_t = x^{(i)}_t + R^{(i)} s^{(0)}_{t-1} + b^{(i)}$,\n$s^{(i)}_t = \\mathcal{P}^{(i)} \\Big( \\{ s^{(i')}_{t-1} \\}_{i'} , \\{ g^{(i')}_t \\}_{i'} \\Big)$,\nwhere a point-wise / element-wise function $\\mathcal{P}^{(i)}$ that does not mix different cells along the vector dimension (unlike the recurrent weight). In Appendix A, we show how this generic formulation translates to the most common RNN variants.\nUsually for these networks, the input is modified with another weight matrix $W$. We omit this here as it can be moved outside of the basic kernels. In the common training setting, where the whole sequence is given as input, the weight matrix $W$ can be applied in parallel to all timesteps before processing a sequence in the RNN. Our runtime experiments in Section 6.1 show that this operation has only marginal impact on the overall runtime."}, {"title": "Generic gradient for back-propagation through time", "content": "In back-propagation through time [Mozer, 1995], the backward pass of this RNN architecture can be recursively defined as well. The backward pass reads:\n$\\frac{\\partial \\mathcal{P}^{(i)}}{\\partial s^{(k)}_{t-1}} \\Big( \\{s^{(i')}_{t-1} \\}_{i'} , \\{g^{(i')}_t \\}_{i'} \\Big)$\n$\\delta s^{(k)}_{t-1} = \\sum_j \\frac{\\partial g^{(j)}_t}{\\partial s^{(k)}_{t-1}} \\delta g^{(j)}_t$,\n$\\frac{\\partial \\mathcal{P}^{(i)}}{\\partial s^{(k)}_{t-1}} \\Big( \\{s^{(i')}_{t-1} \\}_{i'} , \\{g^{(i')}_t \\}_{i'} \\Big)$\n$\\delta g^{(i)}_{t-1} = \\begin{cases}\n\\frac{\\partial s^{(i)}_t}{\\partial g^{(i)}_t} \\Big( R^{(i)T} \\delta g^{(i)}_t \\Big) & \\text{if } i = 0 \\\\\n\\delta s^{(i)}_{t-1} & \\text{otherwise}\n\\end{cases}$.\nThe structure of the gradient shows that, also for the backward pass, we have an alternation of point-wise operations (left) and matrix multiplication (right).\nThe input gradient is equal to the gate gradients, the bias gradient is the sum of the input gradients and the recurrent weight matrix gradient is the time-wise sum of the outer product of gate gradients with the state values:\n$\\delta x^{(j)}_t = \\delta g^{(j)}_t$\n$\\delta b^{(j)} = \\sum_t \\delta g^{(j)}_t$\n$\\delta R^{(j)} = \\sum_t \\delta g^{(j)}_t s^{(0)T}_{t-1}$"}, {"title": "Vanishing and Exploding gradients and Gradient Modifications", "content": "For a neural network to be stably trainable, there must not be exploding gradients, also vanishing gradients should be prohibited for long context sequence modeling [Hochreiter and Schmidhuber, 1997]. Still, for the generic structure of Equations 3, there can be exploding components: Firstly, one or more eigenvalues of the point-wise function Jacobian"}, {"title": "Head-wise parallelization", "content": "When increasing the size of a neural network, typically the width, i.e. the embedding dimension or hidden size is increased. Vaswani et al. [2017] found that for the attention operation it is beneficial to linearly project the input embedding vectors into multiple smaller input vectors, the so called heads, and then perform attention on each of these small vectors in parallel. This parallelization primitive enables also efficient implementations on GPUs, since each head can be computed in different thread blocks of the GPU [Dao et al., 2022] in parallel (see also Section 5.1).\nMany more recent architectures also rely on this head-wise parallelization primitive [Beck et al., 2024, Yang et al., 2024, Dao and Gu, 2024], where the embedding or hidden vector of dimension $d$ is split into $N_{heads}$ heads of smaller dimension $d_{head} = d/N_{head}$, each of which is processed independently inside the sequential part. In FlashRNN, we apply this primitive to traditional RNNs by dividing the recurrent matrix $R$ into multiple blocks or heads $R_{head} \\in \\mathbb{R}^{d_{head} \\times d_{head}}$, rendering the recurrent matrix $R$ as a block-diagonal matrix."}, {"title": "Hardware-Efficient Implementation", "content": "Modern compute hardware in the form of GPUs enables massive parallelization and accelerated matrix multiplication. This means that both point-wise (scalar) operations can be parallelized and matrix multiplications have good support via BLAS-like libraries [Lawson et al., 1979, Thakkar et al., 2023], as used for RNN training workloads as defined above."}, {"title": "GPU-acclerated computing", "content": "Modern compute hardware in the form of GPUs enables massive parallelization and accelerated matrix multiplication. This means that both point-wise (scalar) operations can be parallelized and matrix multiplications have good support via BLAS-like libraries [Lawson et al., 1979, Thakkar et al., 2023], as used for RNN training workloads as defined above."}, {"title": "Execution Model", "content": "Specifically, a modern GPU consists of larger computational super-units (i.e. streaming multiprocessors (SMs)) that have some faster memory attached to them. There are three levels of memory, the large HBM which allows global random access from all computational units at the cost of low speed (still fast compared to CPU RAM access), the SRAM which is attached to one computational super-unit and the registers which are tied to a smallest computational unit (i.e. thread). One super-unit usually supports up to 1024 threads in parallel (with varying register sizes) which are typically referred to as a block or thread block. Multiple blocks executed in parallel on multiple super-units are called the grid. 2 An NVIDIA H100, for example, consists of 132 streaming multiprocessors, with 256 KB SRAM per SM and a SRAM bandwidth of around 33 TB/s [Spector et al., 2024], compared to the up to 3 TB/s for access to the 80 GB of HBM. Starting from the NVIDIA Ampere Architecture and newer, there is hardware acceleration for asynchronous loading and SRAM interconnection, which we did not utilize in this work. Beyond the memory levels, a computational super-unit allows for hardware-accelerated matrix multiplication (e.g. via TensorCores, \"wmma\" operation). Typically, it is divided into sub-units (warps) of a certain number of threads (NVIDIA: 32) that act as one for a matrix multiplication. There are certain size limitations for this acceleration, which have to be considered in the kernel optimization process. For a NVIDIA H100, this means that only minimal matrices of sizes 32x16x8, 16x16x16 or 8x16x32 can be multiplied for the low-precision bfloat16 or float16 dtypes, larger matrix multiplications have to be composed of those, by parallelization along the outer dimensions and summation along the accumulating dimension."}, {"title": "Performance measures", "content": "The specific limitation of a computational load falls into two regimes: Being compute-bound or being memory-bound. In the former case, the arithmetic intensity is high, there are many compute operations per loaded byte and therefore, the main limitation is the computational part. In the latter case, arithmetic intensity is low and the bottleneck is the memory access to load inputs and store outputs [Williams et al., 2009]. Small operations, like applying an activation function in parallel are typically memory bound and should be grouped together into a fused kernel."}, {"title": "Fused Kernels", "content": "To minimize HBM memory accesses, one combines multiple arithmetic operations in one GPU kernel. A kernel is a set of instructions on the GPU which is executed in parallel on its parts. Only within the execution of one kernel SRAM and registers are kept and can serve as a cache. Therefore, for memory-bound operations it is helpful to fuse multiple arithmetic operations into one kernel to leverage these lower cache levels. While compilers can fuse point-wise operations, an alternation of both point-wise computations and matrix multiplication is non-trivial."}, {"title": "FlashRNN kernels", "content": "As the RNN operations of Equations 1 and 3 are a sequential alternation between matrix multiplication and pointwise non-linearities, there is a simple speed up variant that optimizes these two primitives separately. Our library implements this variant, in the alternating backend. This enables arbitrarily large head dimensions (to the limits of HBM GPU memory). Also, a vanilla PyTorch implementation relying on auto-grads will work in this primitive, but for every time step a separate state is saved for the backward pass, leading to inefficiencies beyond memory accesses. We show that moving the time-loop into CUDA can already give large speedups over the vanilla PyTorch implementation.\nThe downside of the alternating implementation is that there are no I/O optimizations beyond a single time step. For every time step, the current input and last state, as well as the recurrent matrix and the biases have to be re-loaded. However, both the recurrent matrix $R$ and the biases remain the same for the whole time loop and the previous states can stay in memory as they were computed in the previous time step. Since the structure of the computation remains the same over the time steps, one can even store most of these values in registers. Registers have the highest memory bandwidth and, while they can only be used within the lowest computation unit (threads), their total size on a GPU is comparable to the SRAM (both 256 KB per SM on H100).\nTo reach the maximum speed, we implement FlashRNN fused kernels that store the recurrent matrix $R$ and the biases $b$ in registers (and SRAM if register memory is exceeded). The matrix multiplication results are stored and accumulated in shared memory (or HBM if SRAM sizes are exceeded). In the forward pass, the computations are mainly tiled along the gate dimension (or the dimension of the new hidden states). This way, we use the maximum amount of memory along the previous state dimension. This dimension is the accumulating dimension of the recurrent matrix multiplication. For the backward pass, the computations are typically tiled along the previous state gradient dimension, such that the gate"}, {"title": "Triton Implementation", "content": "With FlashRNN we also implement a Triton4 variant of the fused FlashRNN kernel. Triton is a domain specific language and compiler for parallel programming that provides a Python-based environment for writing custom GPU kernels.\nFor the Triton kernel we parallelize the computation over two dimensions the batch dimension and the head dimension. See Appendix E for a detailed description of the Triton implementation in Algorithm 5 of the FlashRNN algorithm. As described in section 4.2 we partition the embedding dimension into multiple heads and compute each head in parallel in different programs (or thread blocks) with no synchronization in between these programs. In Triton each program (which corresponds to a thread block in CUDA) will hold its recurrent weight matrix $R_{head}$ and bias $b_{head}$ in SRAM. In contrast to CUDA, Triton gives no access to registers on the GPU. Therefore, we cannot apply the custom caching strategy of the fused CUDA kernels and instead rely on Triton for managing the shared memory and register cache. Additionally, there is no (grid) synchronization between programs in Triton, which makes it impossible to communicate values between different programs over HBM. In section 6.1 we find that this poses a limitation on the maximum head dimension of 128 for the forward pass and 64 for the backward pass on a NVIDIA H100 GPU.\nThe recurrent matrix multiply in equation 1 and 3 is implemented with Triton's matrix multiply operation tl.dot which gives an interface to the Tensor Core units on GPUs. In Triton minimum block size of these matrix multiplies is 16x16, which gives a limit on the minimum batch size. In practice, we enable smaller batch sizes by padding zeros at the cost of efficiency."}, {"title": "Automatic tuning of tiling and looping dimensions", "content": "While Algorithm 1 describes the algorithmic behaviour, the tile, block and grid sizes and loop iterations depend on the specific hardware architecture, i.e. the number of computational super-units (streaming multiprocessors), the SRAM per super-unit, the sizes of matrix-multiplication units, threads (warps and threads) per super-unit and the number of registers per thread. On NVIDIA H100s (and most other NVIDIA GPUs), there is a varying amount of registers per thread, depending on the block size used. The total number of registers on chip per streaming multiprocessor is physically fixed.\nThese physical constraints can now be reformulated as equalities, inequalities and divisibility constraints inside an integer constraint satisfaction problem (integer CSP). Typically this optimization is done via polyhedral constraint optimization in compilers [Baghdadi et al., 2018]. For solving these constraints in FlashRNN, we implement an efficient solver ConstrINT in Python for general integer CSPs going over large number ranges and including the notion of divisibility constraints, which are needed to model the minimal matrix sizes."}, {"title": "Experiments", "content": "In Section 6.1 we benchmark the runtime of our FlashRNN kernels and compare against the LSTM and Attention implementations provided in PyTorch. In Section 6.2 we measure training time with FlashRNN kernels on language modeling. Finally, in Section 6.3 we confirm that traditional RNNs like LSTM and more recent variants like SLSTM implemented in FlashRNN can solve state tracking problems."}, {"title": "Runtime Benchmark", "content": "We evaluate the runtime of all backends of our FlashRNN library that implement the LSTM operation:\n\u2022 CUDA fused: CUDA implementation that fuses matrix multiplication and pointwise operations of the LSTM in a single kernel that is persistent over all time iterations.\n\u2022 CUDA alternating: CUDA implementation that implements the time loop in C++ and alternates between a matrix multiply kernel and a LSTM pointwise kernel.\n\u2022 Triton fused: Triton implementation that fuses matrix multiplication and pointwise operations similar to CUDA fused."}, {"title": "Vanilla PyTorch", "content": "PyTorch implementation of the LSTM operation with our custom backward pass implementation, which is faster than the PyTorch autograd backward pass. We do not use torch.compile due to very long compile times."}, {"title": "FlashAttention2", "content": "PyTorch Attention with FlashAttention2 backend. Note that FlashAttention2 is not a recurrent operation and can be parallelized across batch, head, and sequence dimension on the GPU. FlashAttention2 does not fall into the category of RNNs, which FlashRNN aims to speed up, and is not able to solve state tracking tasks. Therefore, in our benchmarks it should be seen as a widely adopted reference to better interpret the runtimes instead of a direct baseline that we aim to outperform."}, {"title": "nn.LSTM", "content": "PyTorch LSTM with NVIDIA cuDNN as backend. In contrast to our FlashRNN LSTM, nn.LSTM also integrates the gate pre-activation computation into the function call (not kernel call), which we do not (see Section 3). In Section H.4 in the appendix, we provide a comparison to the combination of a linear layer and our FlashRNN LSTM kernel with nn.LSTM. Moreover, nn.LSTM does not support multiple heads on the embedding dimension as described in Section 4.2. nn.LSTM always uses a single head."}, {"title": "haste", "content": "The haste library is an implementation of LSTM and GRU and variations in CUDA, using alternating kernels between pointwise and matrix multiplication operations. Its last release was in 2020, with no compilation support for Ampere or later architectures in the standard setting. It solely supports float32 and float64 precision and does not have a multi-head option."}, {"title": "Setup", "content": "We assess the impact of the input dimensions batch size (B), sequence length (T) and head dimension (DH) and number of heads (NH). The number of heads together with the head dimension give the embedding dimension $d = NH \\times DH$. Except for PyTorch nn.LSTM we run all runtime experiments with bfloat16 precision. For nn.LSTM we use float16 precision, since this precision yielded the fastest runtimes. For every runtime measurement we do 25 warmup iterations and then report the average across 1000 iterations on NVIDIA H100 GPUs. We use PyTorch 2.4 and with CUDA version 12.4 for our experiments. Further details and additional experiments can be found in Section H in the appendix."}, {"title": "Head dimension", "content": "We measure the runtime of all of our FlashRNN kernels and our two references FlashAttention2 and PyTorch nn.LSTM for different head dimensions. We fix the embedding dimension $d = NH \\times DH$ to 768 and vary the head dimension from 16 to 768. We use batch size 16 and sequence length 1024. In Figure 2 we report the runtime of each the forward pass only on the left and the forward combined with the backward pass. FlashAttention2 does not allow for head dimension larger than 256, due shared memory limitation. The PyTorch nn.LSTM does not support multiple heads or blockdiagonal recurrent matrices. Therefore, we only report the runtime for a single head of dimension 768, including the gate pre-activation computation. At this dimension, nn.LSTM is about 3 times faster than CUDA fused. The Triton kernels are limited to head dimension 128 and 64, but are about two times faster than CUDA fused for small head dimensions 16 and 32. The fused CUDA kernels support all head dimensions up to 768 (actually more, see Appendix Section H.1) and are about two to three times faster than the alternating kernels."}, {"title": "Batch size", "content": "We measure the runtime of all LSTM kernels while varying the batch size (B) from 2 to 256 at sequence length 1024. Figure 3 shows the results for NH=12 heads with head dimension DH=64. The CUDA fused backend is optimized for smaller batch sizes and shows a 2x speed up over the alternating backend for batch sizes up to 32. For larger batch sizes than 128 CUDA alternating is faster. Figure 4 shows the results for a single head with head dimension DH=768. At this head dimension CUDA fused is still faster than CUDA alternating up to batch size 32. For larger batch sizes, CUDA alternating is more than two times faster. Comparing to the PyTorch nn.LSTM, we find for medium batch sizes from 8 to 64 it is about 2-3 times faster than and CUDA fused and for larger batch sizes about about 30% faster than CUDA alternating."}, {"title": "Additional Runtime Experiments", "content": "In section H.3 in the appendix, we include experiments on varying sequence lengths. We see the expected linear runtime scaling for our FlashRNN kernels and validate that the above findings transfer to other sequence lengths. In addition, in section H.4 we compare the FlashRNN LSTM kernel in combination with a linear layer that computes the gate pre-activations externally to the PyTorch nn.LSTM baseline which integrates the gate pre-activation computation. We find that the gate pre-activation computation has only marginal impact on the overall runtime. Finally, in section H.5, we provide all runtime results also for the sLSTM [Beck et al., 2024]."}, {"title": "Language Modeling", "content": "Even though we do no expect traditional RNNs to outperform Transformers, the language modeling setting serves as an important benchmark for speed on larger scales. Here, we train models at the 165M parameter scale for a Llama-style Transformer without weight tying, i.e. 12 Transformer blocks with Pre-LayerNorm and a Swish-Gated MLP after the attention layer. We replace attention with FlashRNN LSTM and sLSTM layers for a speed comparison. The results show a slowdown of roughly 25% over attention for equal head dimensions or 140% for one RNN head, see Table 6.2 (H100) and Appendix Table I (A100). In our experiments, we also compare to the cuDNN implementation of LSTM integrated in PyTorch (torch.nn.LSTM). While it's integration into PyTorch is considerably faster, there are numerical differences to the FlashRNN implementation. With same initialization, FlashRNN LSTMs converge faster in our language experiments (both bfloat16 and float32), even though the differences in a single kernel call are at the expected levels of numerical precision. This deviation should be investigated further and suggests the use of FlashRNN even for the established LSTM architecture. We provide an analysis of our kernel precision compared to a float64 baseline in section H.6."}, {"title": "State Tracking Task", "content": "To show state tracking capabilities of traditional RNNs in contrast to Transformers and State Space Models experimentally, we train our implementation on the Parity task and evaluate on longer sequences to measure extrapolation capabilities [Zhou et al., 2024]. This serves as a litmus test for state tracking capabilities [Merrill et al., 2024]."}, {"title": "Conclusion", "content": "The FlashRNN library serves as a fast and extendable implementation of traditional RNNs with a recurrent connection or memory mixing. It extends RNNs with the multi-head paradigm introduced by Beck et al. [2024] for sLSTM. FlashRNN provides a speed-up of up to 50x over vanilla PyTorch implementations of RNNs and may serve as a backbone for future RNN architectures that have a recurrent connection.\nFlashRNN implements two variants, an alternating version switching between point-wise and matrix-multiplication kernels and a fused implementation - optimizing memory transfers, while using hardware-optimized matrix-multiplication. The second leads to a further 3-4x speed-up over the alternating option for small batch sizes. The implementation auto-optimizes its internal sizes for different cache levels via the ConstrINT library - a custom library solving general integer constraint satisfaction problems with equality, inequality and divisibility constraints. This library may be re-used for other optimization problems regarding cache sizes on hardware platforms and beyond.\nWe show that with FlashRNN, traditional RNNs are not too far in speed from Transformers in practice, even though they are not parallelizable along the sequence dimension. In the future, it may be optimized to leverage asynchronous memory operations and inter-SRAM connections - recent hardware features that promise further speed ups not realized in this work."}, {"title": "Ethics Statement", "content": "We use an open dataset (SlimPajama) that uses publicly crawled internet data for Language Model training. Our implementation speeds up a certain class of Machine Learning models. This may reduce the environmental impact of the research field, in case these architectures remain important in future research. Also, it may speed up development of Machine Learning research in the direction of recurrent sequence modeling with state tracking capabilities. The further implications of these impacts may or may not be a benefit for society."}, {"title": "Reproducibility Statement", "content": "We provide the source code for your implementations along with this paper. The detailed training setup for speed tests is described in Section 6.1. For Language Modeling this setup description is provided in Appendix Section J and uses the open SlimPajama dataset, for the parity task experiments in Appendix Section K, the training and test data can be synthetically generated using the mentioned distributions.\nThe observed deviations in language model training compared to the PyTorch LSTM based on cuDNN should be further investigated. The results on A100 and H100, as well as across our different kernels are within the expected small-scale numerical deviations.\nThe code will also be released on GitHub including training scripts."}, {"title": "Details on Triton Implementation", "content": "Algorithm 5 provides details on the Triton implementation. It shows the computation for a single program or thread block, which computes one head of dimension d and block $B_y$ of the batch dimension b. We run a grid of $(n_{head} \\times B)$ of these programs in parallel for FlashRNN forward pass in Triton. We load the recurrent weights R and biases b only once from HBM to SRAM and keep them in SRAM throughout the time loop.\nOn a higher level the main differences to the CUDA implementation in algorithm 1 are that in CUDA we can use multiple thread blocks for a single head and we can force the kernel to keep the recurrent weights R in registers instead of SRAM. One can see this difference for example in the kernel launch grid, which parallelizes only over number of heads and blocks of batch size in Triton, while it has two more parallelization dimensions in CUDA (see Algorithm 1)."}, {"title": "Computational Complexity", "content": "Traditional RNNs go over the sequence step by step, while applying a recurrent matrix multiplication and a pointwise activation function at each step. For the back-propagation in time, all past state values are usually stored. In this paper, we implement the head-wise notion limiting the recurrent matrix to a block diagonal form. The computational complexity is therefore: $O(T n_{heads} d_{head})$, with head size $d_{head}$, $N_{heads}$ the number of heads and T the sequence length. The matrix vector product at each step is the dominant computational factor (for large head sizes). For inference, the memory needed is defined by the state of the RNN which is $O(n_{heads} d_{head})$\nIn contrast, Attention computes a weighted sum over past inputs at each step, with the weight defined by the softmax over scalar products between query and key vectors. This leads to a computational complexity $O(T^2 n_{heads} d_{head})$. The space complexity is $O(T n_{heads} d_{head})$ [Vaswani et al., 2017]. In conclusion, RNNs are more compressive, while their computational complexity is higher when computing only a few steps. For training with BPTT the space complexity of RNNs matches the one of Attention, as all past states have to be stored."}, {"title": "Roofline Analysis", "content": "As mentioned in Section 5.1, kernel speed is fundamentally limited by two factors: computation and memory bandwidth. This is usually visualized in the Roofline-Plot, showing the position of a kernel in terms of its computation throughput and arithmetic intensity. We use NVIDIA NSight Compute, to analyse this for the alternating and fused FlashRNN LSTM kernel compared to the nn.LSTM (cuDNN) baseline on a H100-SXM:"}, {"title": "Additional Benchmark Experiments", "content": ""}, {"title": "Fused Kernel Limits", "content": "Since the fused CUDA kernel of FlashRNN is based on keeping the recurrent memory matrix in registers and shared memory", "1280)": "n\u2022 RTX3090: [1280", "1824": "n\u2022 A40: [1280"}, {"1824": "n\u2022 A100: [1280, 1312, 1344, 1376, 1408, 1440, 1472, 1504, 1536, 1568, 1600, 1632, 1664, 1696, 1728, 1760, 1792, 1824, 1920, 2016, 2080, 2112, 2304"}]}