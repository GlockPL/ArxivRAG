{"title": "Can LLMs Explain Themselves Counterfactually?", "authors": ["Zahra Dehghanighobadi", "Asja Fischer", "Muhammad Bilal Zafar"], "abstract": "Explanations are an important tool for gain-\ning insights into the behavior of ML models,\ncalibrating user trust and ensuring regulatory\ncompliance. Past few years have seen a flurry\nof post-hoc methods for generating model ex-\nplanations, many of which involve computing\nmodel gradients or solving specially designed\noptimization problems. However, owing to the\nremarkable reasoning abilities of Large Lan-\nguage Model (LLMs), self-explanation, that\nis, prompting the model to explain its outputs\nhas recently emerged as a new paradigm. In\nthis work, we study a specific type of self-\nexplanations, self-generated counterfactual ex-\nplanations (SCEs). We design tests for measur-\ning the efficacy of LLMs in generating SCEs.\nAnalysis over various LLM families, model\nsizes, temperature settings, and datasets re-\nveals that LLMs sometimes struggle to gener-\nate SCEs. Even when they do, their prediction\noften does not agree with their own counterfac-\ntual reasoning.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) are rapidly gaining\nadoption due to their remarkable capabilities across\na wide spectrum of real-world tasks (Bommasani\net al., 2021; Bubeck et al., 2023; Maynez et al.,\n2023; Wei et al., 2022a). Recent analyses show that\nLLMs can lead to significant productivity gains,\nand can match or even surpass human-level perfor-\nmance (Luo et al., 2024; Peng et al., 2023; Yang\net al., 2024). These impressive achievements are\noften attributed to large datasets, model sizes (Hoff-\nmann et al., 2022; Kaplan et al., 2020) and the ef-\nfect of alignment with human preferences (Guo\net al., 2025; Ouyang et al., 2022). The resulting\nmodel complexity, however, means that the LLM\noutputs can be difficult to explain.\nA number of recent studies have looked into ex-\nplaining predictions of LLMs (Bricken et al., 2023;\nTempleton et al., 2024; Zhao et al., 2024, inter\nalia). Explainability had been a thoroughly stud-\nied area of ML even before the advent of modern\nLLMs (Gilpin et al., 2018; Guidotti et al., 2018).\nConsequently, a large number of attempts to ex-\nplain LLM outputs build upon those designed for\nnon-LLM models. For the most part, these tech-\nniques operate by computing model gradients or\nsolving specially designed optimization problems\nto find input features (Cohen-Wang et al., 2025),\nmodel neurons (Meng et al., 2022; Templeton et al.,\n2024), abstract concepts (Kim et al., 2018; Xu et al.,\n2025) or training data points (Park et al., 2023) that\ncaused the model to depict a certain behavior.\nIn parallel, inspired by the reasoning abilities of\nLLMs (Wei et al., 2022b), recent work has started\nexploring whether LLMs can explain themselves\nwithout the need for costly operations like comput-\ning gradients or solving optimization problems. For\ninstance, Bubeck et al. (2023) show that GPT-4 can\nprovide reasoning for its answers and in the pro-\ncess, can even admit its mistakes. A fast-emerging\nbranch of explainability literature focuses on meth-\nods for producing and evaluating self-generated ex-\nplanations (Agarwal et al., 2024; Guo et al., 2025;\nLanham et al., 2023; Tanneru et al., 2024; Turpin\net al., 2023).\nIn this work, we focus on a specific type of\nself-generated explanations called self-generated\ncounterfactual explanations (SCEs). Given an\ninput x and the model output \u0177, the counterfac-\ntual XCE is a modified version of the input that\nwould lead the model to generate another output\nYCE \u2260 \u0177 (Wachter et al., 2017). Prior work ar-\ngues that the contrastive nature of counterfactual\nexplanations means that they are better aligned with\nhuman reasoning (Miller, 2019), are better suited\nto regulation around right-to-explanation (Wachter\net al., 2017) and constitute a better test of knowl-\nedge (Ichikawa and Steup, 2024) than other input-\nfeature based explanations like LIME (Ribeiro\net al., 2016) and SHAP (Lundberg and Lee, 2017).\nOur goal in this paper is to study the efficacy of\nLLMs in generating SCEs. We break down our\ninvestigation into three research questions (RQs):\nRQ1 Are LLMs able to generate SCEs at all?\nRQ2 Do LLM predictions on SCEs actually yield\nthe target label?\nWe find that when generating the model predic-\ntion on a SCE, the presence of the original pre-\ndiction (Figure 1a) and the instruction to generate\nthe SCE (Figure 1b) in the context window has a\nlarge impact on the eventual prediction on SCE,\npointing to flaws in the internal reasoning process\nof LLMs. Within the same dataset, models show a\nRQ3 Do LLMs need to make large-scale changes\nto the input to generate SCEs?\nlarge spread in the amount of changes they make to\nthe original input when generating the SCE. Over-\nall, our results show that despite their impressive\nreasoning abilities, modern LLMs are far from\nperfect when explaining their own predictions\ncounterfactually."}, {"title": "2 Related work", "content": "Explainability in ML. The literature on ML ex-\nplainability is vast. There are several ways to cate-\ngorize explainability methods, e.g., perturbation v.s.\ngradient-based, feature v.s. concept v.s. prototype-\nbased, importance v.s. counterfactual-based and\noptimization v.s. self-generated. See Gilpin et al.\n(2018), Guidotti et al. (2018), and Zhao et al. (2024)\nfor details and alternative categorizations."}, {"title": "Counterfactual explanations in ML", "content": "See Sec-\ntion 1 for a comparison between counterfactual ex-\nplanations (CEs) and other forms of explainability.\nGenerating valid and plausible CEs is a longstand-\ning challenge (Verma et al., 2024). For instance,\nDelaney et al. (2023) highlight discrepancies be-\ntween human-generated and computationally gen-\nerated CEs. Through two user studies, they find\nthat humans tend to make larger, more meaning-\nful modifications to misclassified images, whereas\ncomputational methods prioritize minimal edits.\nPrior work has also highlighted the need for gener-\nating on-manifold CEs to ensure plausibility and ro-\nbustness (Slack et al., 2021; Tsiourvas et al., 2024).\nModeling the data manifold, however, is a challeng-\ning problem, even for non-LLM models (Arvani-\ntidis et al., 2016)."}, {"title": "Self-explanation by LLMs", "content": "While a large class\nof explainability methods require whitebox model\naccess SEs can be deployed in a balckbox manner.\nSEs can take many forms, e.g., chain-of-thought\n(CoT) reasoning (Agarwal et al., 2024) and fea-\nture attributions (Tanneru et al., 2024). However,\nresearch indicates that CoT and feature attribu-\ntions are not always a faithful representation of the\nmodel's decision-making process (Lanham et al.,\n2023; Tanneru et al., 2024; Turpin et al., 2024). Our\nprotocol for testing SCEs is distinct from both CoT\nand feature-attribution based self-explanations.\nChen et al. (2023) argue that effective explana-\ntions should empower users to predict how a model\nwill handle different yet related inputs, a concept\nreferred to as simulatability. Their experiments\ntested whether GPT-3.5's ability to generate CEs\ndepends on the quality of the examples provided.\nInterestingly, GPT-3.5 was able to produce compa-\nrable (to humans) CEs even when presented with\nillogical examples, suggesting that its CEs gener-\nation capabilities stem more from its pre-training\nthan from the specific examples included in the\nprompt. Unlike Chen et al. (2023), our focus is not\non human simulatability of SCEs."}, {"title": "LLMs for explanations", "content": "LLMs are also used to\ngenerate explanations for other models (Bhattachar-\njee et al., 2024; Gat et al., 2023; Li et al., 2023;\nNguyen et al., 2024; Slack et al., 2023). Unlike\nthese studies, our focus is on explaining the LLM\nitself. Additionally, the approach of Nguyen et al.\n(2024) and Li et al. (2023) involved explicitly pro-\nviding the model with the original human gold la-\nbels in the prompt, without assessing the model's"}, {"title": "3 Generating and evaluating SCEs", "content": "We formally describe the process of generating\nSCEs and then list the metrics used to evaluate\ntheir quality."}, {"title": "3.1 Generating counterfactuals", "content": "We consider datasets of the form D =\n{(xi, Yi)}1. x are input texts, e.g., social me-\ndia posts or math problems. Yi \u2208 Y are either\ndiscrete labels, e.g., sentiment of a social media\npost, or integers from a predefined finite set, e.g.,\nsolution to a math problem. The model prediction\nand explanation process consists of the following\nsteps. Precise prompts for each step are shown\nin Appendix C.\nStep 1: Input and output. Given the input x, we\ndenote the model output by \u0177 = f(x) \u2208 y. For\ninstruction-tuned LLMs, this step involves encap-\nsulating the input x into a natural language prompt\nbefore passing it through the model, see for exam-\nple the work by Dubey et al. (2024). We detail\nthese steps in Appendix C. The outputs of LLMs\nare often natural language and one needs to em-\nploy some post-processing to convert them to the\ndesired output domain Y. For the sake of nota-\ntional simplicity, we do not explicitly state these\npost-processing steps here but describe them in Ap-\npendix D.\nStep 2: Generating SCEs. A counterfactual ex-\nplanation XCE is a modified version of the original\ninput x that would lead the model to change its de-\ncision, that is f(x) \u2260 f(XCE). A common strategy\nfor generating counterfactuals is to first identify\na counterfactual output YCE \u2260 y and then solve\nan optimization problem to generate XCE such that\nf(XCE) = YCE (Mothilal et al., 2020; Verma et al.,\n2024; Wachter et al., 2017). YCE is either chosen at\nrandom or in a targeted manner. Since we are inter-\nested in self-explanation properties of LLMs, we\ndo not solve an optimization problem and instead\nask the model itself to generate the counterfactual\nexplanation.\nA key desiderata for counterfactual explanations\nis to keep the differences between x and XCE to"}, {"title": "3.2 Evaluating CEs", "content": "We use the following metrics for evaluating SCEs.\nGeneration percentage (Gen) measures the per-\ncentage of times a model was able to generate\na SCE. In a vast majority of cases, the models\ngenerate a SCE as instructed. The cases of non-\nsuccessful generation include the model generating\na stop-word like \u201c.\u201d or \u201c!\u201d or generating a XCE that\nis much shorter in length than x. We describe the\ndetailed filtering process in Appendix D.\nCounterfactual validity (Val) measures the per-\ncentage of times the SCE actually produces the\nintended target label, i.e., f(xCE) = YCE. As de-\nscribed in Step 3 in Section 3.1, this final prediction\ncan be made either with Steps 1 and 2 in context or\nwithout. We denote the validity without context as\nVal and with context as Valc.\nEdit distance (ED) measures the edit distance be-\ntween the original input x and the counterfactual\nXCE. Closeness to the original input is a key desider-\natum of a counterfactual explanation (Wachter\net al., 2017). Our use of edit distance as the close-\nness metric is inspired by prior studies on evaluat-\ning counterfactual generations (Chatzi et al., 2025).\nWe only report the ED for valid SCEs. Since the va-\nlidity of SCEs is impacted by the presence of Steps"}, {"title": "4 Experimental setup", "content": "In this section, we describe the datasets, models\nand parameters used in our experiments."}, {"title": "4.1 Datasets", "content": "To gain comprehensive insights into the counter-\nfactual generation ability of LLMs, we consider\ndatasets from four different domains: decision mak-\ning, sentiment classification, mathematics and nat-\nural language inference. We describe each dataset\nin detail.\n1. DiscrimEval (decision making) by Tamkin et al.\n(2023) is a benchmark featuring 70 hypothetical\ndecision-making scenarios. Each prompt instructs\nthe model to make a binary decision regarding an\nindividual, e.g., whether the individual should be\ngranted a loan, whether the individual should re-\nceive medical treatment. The prompts are designed\nsuch that a yes decision is always desirable. The\ndataset replicates the 70 scenarios several times by\nsubstituting different values of gender, race and\nage. We fix these features to arbitrarily selected\nvalues of female, white, and 20 years old.\n2. FolkTexts (decision making) by Cruz et al.\n(2024) is a Q&A dataset suite derived from the\n2018 US Census PUMS data. Each instance con-\nsists of a a textual description of an individual's\nfeatures like their educational status, age, and occu-\npation. The modeling task is to predict whether the\nyearly income of the individual exceeds $50,000.\n3. Twitter financial news (sentiment classifica-\ntion) by ZeroShot (2022) provides an annotated\ncorpus of finance-related tweets, specifically cu-\nrated for sentiment analysis in the financial domain.\nEach tweet is labeled with one of three sentiment\nclasses: Bearish, Bullish, or Neutral.\n4. SST2 (sentiment classification) or Stanford Sen-\ntiment Treebank by Socher et al. (2013) consists\nof single sentence movie reviews along with the\nbinary sentiment (positive and negative).\n5. GSM8K (math) or Grade School Math 8K by\nCobbe et al. (2021) consists of mathematical prob-"}, {"title": "4.2 Models, infrastructure, and parameters", "content": "We aim to gain insights into the counterfactual gen-\neration ability of models across various architec-\ntural aspects. The models we study are:\nSmall models namely Gemma-2-9B-it henceforth\nreferred to as GEMs, Llama-3.1-8B-Instruct\n(LAMs), Mistral-7B-Instruct-v0.3 (MSTs).\nMedium models consist of Gemma-2-27B-it\n(GEM), Llama-3.3-70B-Instruct (LAMm), and\nMistral-Small-24B-Instruct-2501 (MSTm).\nReasoning model. We only consider\nDeepSeek-R1-Distill-Qwen-32B (R1m).\nAll experiments were run on a single node with\n8x NVIDIA H200 GPUs. The machine was shared\nbetween multiple research teams. We ran all the\nmodels in their original 16-bit precision and did not\nemploy any size reduction strategies like quantiza-\ntion. For each model, we consider two temperature\nvalues, T = 0 and T = 0.5.\nFor generating the counterfactuals, one needs to\nprovide the model with the target label YCE. For\nclassification datasets, we select YCE from the set\nY - {y} at random. For the GSM8K data, we\ngenerate YCE = \u0177 + \u0454 with \u0454 was sampled from a\nuniform distribution Unif(1, 2, . . ., 10).\nGiven the high cost of LLM inference, we\nsubsample datasets as follows: For classification\ndatasets, we select 250 samples at random from\neach class. For the non-classification, GSM8K\ndata, we also randomly select 250 samples. While\nwe did not track the precise time, the experiments\ntook several days on multiple GPUs to complete.\nWe occasionally used ChatGPT for help with\ncoding errors."}, {"title": "5 Results", "content": "Tables 1 and 2 show the results when using uncon-\nstrained prompting and rationale-based prompting,\nrespectively at T = 0. Tables 4 and 5 showing\nresults with a higher temperature T = 0.5 are in-\ncluded in Appendix B and discussed under each\nRQ."}, {"title": "RQ1: Ability of LLMs to generate SCEs", "content": "Most models are able to generate SCEs in a\nvast majority of cases with the exception of GEMS\nmodel for the DiscrimEval, FolkTexts, and GSM8K\ndatasets. At a higher temperature of 0.5 the ability\nof GEMs to generate SCEs goes up slightly (Table 4),\nwhereas for LAM5, it decreases slightly on average.\nThe fraction roughly remains the same for\nrationale-based prompting as shown in Tables 2\nand 5. The affect of temperature on the ability of\nGEMs and LAMs to generate SCEs remains the same."}, {"title": "RQ2: Do SCEs yield the target label?", "content": "Model generated SCEs yield the target label in most\ncases, however, there are large variations. The\nmost prominent variation is along the task level.\nFor the more challenging mathematical reasoning\ntasks in GSM8K, the percentage of times the mod-\nels generate valid SCEs in only in single digits\nfor most models. For the FolkTexts tasks which\nrequires the model to reason through the Census-\ngathered data of individuals, the validity in many\ncases is low.\nWe also see a somewhat expected variation at\nmodel-size level. The smaller models, GEM5 (9B\nparameters), LAMs (8B), and MST5 (7B) tend to gen-\nerate valid SCEs at a higher rate than the larger\nversions GEMm (27B), LAMm (70B), and MSTm (24B).\nThere are only few exceptions like MGNLI where\nGEMm performs better. The reasoning model R1m\n(32B) does not always perform better than the simi-\nlarly sized models GEM\u339c and MSTm. For instance, for\nFolkTexts, the percentage of valid SCEs generated\nby R1m is much lower than for GEMm or MSTm.\nPresence of the original prediction and coun-\nterfactual generation in the context window has\na large impact on validity as shown by the com-\nparison of Val and Valc in Tables 1 and 2. Most\nprominently, on the GSM8K dataset, the validity\nincreases significantly meaning that the mathemat-\nical reasoning ability of the model is impacted\nby information that should be irrelevant. On the\nFolkTexts data too the model deems more SCES\nas valid when the prediction and SCE generation\nare provided in the context. However, for some\ntask and model combinations, the validity rate de-\ncreases with the presence of additional information"}, {"title": "RQ3: Changes required to generate SCEs", "content": "For a given task and dataset, different LLMs re-\nquire different amount of changes to generate SCEs,\neven for a similar level of validity. Consider for\nMSTm, LAMs and LAMm models for DiscrimEval data.\nThe validity of SCEs ranges between 89% to 100%\nwhile the normalized edit distance ranges between\n14 and 61.\nThe required changes also depend on the task"}, {"title": "Are invalid SCEs statistically different?", "content": "In this section, we aim to characterize the differ-\nences between valid and invalid SCEs. Specifi-\ncally, we investigate if the lengths of SCEs can\nprovide a clue on their validity. Our question is\ninspired by previous work on detecting LLM hallu-\ncinations (Azaria and Mitchell, 2023; Snyder et al.,\n2024; Zhang et al., 2024) which shows that incor-\nrect model outputs show statistically different pat-\nterns from correct answers. Figure 2 shows that\nthe length distribution of valid and invalid SCEs is\nindeed different.\nNext, we systematically analyze these differ-\nence in SCE lenghts. Specifically, for each model,\ndatasest and SCE generation configuration, we\ncompute the normalized difference in lengths as\n|Lval-Linval \u00d7 100 where Lval is the average\nmax(Lval, Linval)\nlength of valid SCEs. The normalization ensures\nthat the metric lies in the range [0, 100]. The results\nin Table 3 show that in many dataset and model\ncombinations, the SCEs lengths can indeed provide\na signal on their validity."}, {"title": "6 Conclusion and future work", "content": "In this study, we examined the ability of LLMs\nto produce self-generated counterfactual explana-\ntions (SCEs). We design a prompt-based setup for\nevaluating the efficacy of SCEs. Our results show\nthat LLMs consistently struggle with generating\nvalid SCEs. In many cases model prediction on\na SCE does not yield the same target prediction\nfor which the model crafted the SCE. Surprisingly,\nwe find that LLMs put significant emphasis on the\ncontext-the prediction on SCE is significantly im-\npacted by the presence of original prediction and\ninstructions for generating the SCE. Based on this\nempirical evidence, we argue that LLMs are still\nfar from being able to explain their own predic-\ntions counterfactually. Our findings add to similar\ninsights from recent studies on other forms of self-\nexplanations (Lanham et al., 2023; Tanneru et al.,\n2024).\nOur work opens several avenues for future\nwork. Inspired by counterfactual data augmenta-\ntion (Sachdeva et al., 2023), one could include the\ncounterfactual explanation capabilities a part of\nthe LLM training process. This inclusion may en-\nhance the counterfactual reasoning capabilities of\nthe LLM. Follow ups should also explore the ef-\nfect of prompt tuning, specifically, model-tailored\nprompts for generating SCEs. These approaches\nmight lead to better quality SCES.\nWe limited our investigation to open source mod-\nels of upto 70B parameters. Extending our analysis\nto larger and more recent models, e.g., DeepSeek\nR1 671B, and closed source models like OpenAI\n03 would be an interesting avenue for future work.\nFinally, our experiments were limited to rela-\ntively simple tasks: classification and mathematics\nproblems where the solution is an integer. This\nlimitation was mainly due to the fact that it is diffi-\ncult to automatically judge validity of answers for\nmore open-ended language generation tasks like\nsearch and information retrieval. Scaling our anal-\nsis to such tasks would require significant human-\nannotation resources, and is an important direction\nfor future investigations."}, {"title": "7 Limitations", "content": "Our work has several limitations. First, explainabil-\nity and privacy can sometimes be at odds with each\nother. Even if LLMs are able to provide comprehen-\nsive and faithful explanations, this can introduce\nprivacy and security concerns (Grant and Wischik,\n2020; Pawlicki et al., 2024). Detailed explanations\nmay inadvertently expose sensitive information or\nbe exploited for adversarial attacks on the model\nitself. However, our work focuses on publicly avail-\nable models and datasets, ensuring that these risks\nare mitigated.\nSimilarly, savvy users can strategically use coun-\nterfactual explanations to unfairly maximize their\nchances of receiving positive outcomes (Tsirtsis\nand Gomez Rodriguez, 2020). Detecting and limit-\ning this behavior would be an important desidera-\ntum before deployment of LLM counterfactuals.\nOur analyses in this paper solely focused on auto-\nmated metrics to evaluate quality of SCEs. Future\nstudies can conduct human surveys to assess how\nplausible the explanations appear from a human\nperspective. This feedback can then be used to en-\nhance the model's performance through methods\nsuch as direct preference optimization (Rafailov\net al., 2024)."}, {"title": "A Reproducibility and licenses", "content": "Dataset Licenses and Usage.\n1. DiscrimEval: We utilize the dataset ver-\nsion made available by the authors at\nhttps://huggingface.co/datasets/\nAnthropic/discrim-eval. It is distributed\nunder the CC-BY-4.0 license.\n2. Folktexts: The dataset version we refer-\nence is the one provided by the authors,\naccessible at https://huggingface.co/\ndatasets/acruz/folktexts.\nFolkTexts\ncode is made available under the MIT\nlicense. The dataset is licensed under\nthe U.S. Census Bureau's terms (https:\n//www.census.gov/data/developers/\nabout/terms-of-service.html).\n3. Twitter Financial News: We employ\nversion 1.0.0 of the dataset, as released\nby the authors, available at https:\n//huggingface.co/datasets/zeroshot/\ntwitter-financial-news-sentiment.\nThe dataset is distributed under the MIT\nLicense.\n4. SST2: The dataset version used in our\nwork is the one published by the Stan-\nfordNLP team at https://huggingface.\nco/datasets/stanfordnlp/sst2. The\ndataset itself does not provide licens-\ning information. However, the whole\nStanfordNLP toolkit is available under\nApache2.0 license, see https://github.\ncom/stanfordnlp/stanza.\n5. GSM8K: We make use of the dataset\nversion released by the authors, accessible\nat https://huggingface.co/datasets/\nopenai/gsm8k?row=3. It is licensed under\nthe MIT License.\n6. Multi-Genre Natural Language Infer-\nence (MultiNLI): Our work relies on\nthe dataset version shared by the authors\nat https://huggingface.co/datasets/\nnyu-mll/multi-nli. It is available under\nthe CC-BY-SA-3.0 license.\nModel Licenses. We utilize the original providers'\nmodel implementations available on HuggingFace\n(https://huggingface.co).\n1. Mistral models (Jiang et al., 2023) are released\nunder the APACHE-2.0 license.\n2. Gemma models are released under the custom\nGemma-2 license.\n3. LLaMA models (Dubey et al., 2024) are re-\nleased under the custom LLaMA-3.1 license.\n4. DeepSeek-R1-Distill-Qwen-32B (Guo et al.,\n2025), derived from the Qwen-2.5 series, re-\ntains its original APACHE-2.0 license."}, {"title": "B Additional results", "content": "Table 4 shows the SCE evaluation metrics for direct\nprompting when using a temperature of 0.5. Ta-\nble 5 shows the metrics when using rationale-based\nprompting with temperatures of 0.5."}, {"title": "C Prompts for generating and evaluating SCES", "content": "Recall from Section 3.1 that we can generate SCES\nthrough: (i) unconstrained prompting where we\nsimply ask the model to generate counterfactuals,\nor (ii) rationale-based prompting by asking the\nmodel to first select decision rationales (DeYoung\net al., 2019) and then generating counterfactuals by\nlimiting the changes to these rationales only. For\neach dataset, we show prompts separately for each\nprompt type."}, {"title": "C.1 DiscrimEval", "content": "C.1.1 Unconstrained prompting\n1. You will be given a decision making scenario\nfollowed by a question about the scenario. An-\nswer the question with 'Yes' or 'No'. Do not\ninclude any additional words in your answer.\nYour answer should start with 'ANSWER:'.\nThe scenario is: {SCENARIO}\nThe question is: {QUESTION}\n2. Now revise the original scenario such that\nyour answer to the question on the revised\nscenario becomes <Complement>. Share the\nrevised scenario. The revised scenario should\nstart with 'REVISED SCENARIO:'"}, {"title": "C.1.2 Rationale-based prompting", "content": "1. You will be given a decision making scenario\nfollowed by a question about the scenario. An-\nswer the question with 'Yes' or 'No'. Do not\ninclude any additional words in your answer.\nYour answer should start with 'ANSWER:'"}, {"title": "C.2 FolkTexts prompts", "content": "We adapt the prompts from Cruz et al. (2024).\nC.2.1 Unconstrained prompting\n1. You will be provided data corresponding to\na survey respondent. The survey was con-\nducted among US residents in 2018. Please\nanswer the question based on the information\nprovided by selecting from one of the two\nchoices. The data provided is enough to reach\nan approximate answer. Do not include any"}, {"title": "C.2.2 Rationale-based prompting", "content": "1. You will be provided data corresponding to\na survey respondent. The survey was con-\nducted among US residents in 2018. Please\nanswer the question based on the information\nprovided by selecting from one of the two\nchoices. The data provided is enough to reach\nan approximate answer. Do not include any"}, {"title": "C.3 SST2", "content": "C.3.1 Unconstrained prompting\n\u2022 You will be given a movie review. Assess its\nsentiment and classify it as 'Positive' or 'Neg-\native.' Do not include any additional words in\nyour answer. Your answer should start with\n'ANSWER:'\n\u2022 Now revise the original review so that the\nsentiment of the revised review becomes\n. Share the revised review. The\nrevised review should start with 'REVISED\nREVIEW:'"}, {"title": "C.3.2 Rationale-based prompting", "content": "\u2022 You will be given a movie review. Assess its\nsentiment and classify it as 'Positive' or 'Neg-\native.' Do not include any additional words in\nyour answer. Your answer should start with\n'ANSWER:'\n\u2022 Now, identify the 'rationales' behind your an-\nswer. The rationales are words, phrases or\nsentences in the original review that led you\nto answer with . Share\na list of rationales with one rationale per line.\nThe list should start with 'RATIONALS:'\n\u2022 Alter the rationales in the original review so\nthat your answer on the altered review be-\ncomes . Keep the changes to\na minimum. The altered review should start\nwith 'ALTERED REVIEW:'"}, {"title": "C.4 Twitter Financial News", "content": "C.4.1 Unconstrained prompting\n1. You will be given a finance-related news post\nfrom X (formerly Twitter) followed by a ques-\ntion about its sentiment. Respond with \u2018Bear-\nish,' 'Bullish,' or 'Neutral.' Do not include"}, {"title": "C.4.2 Rationale-based prompting", "content": "1. You will be given a finance-related news post\nfrom X (formerly Twitter). Assess its senti-\nment and classify it as 'Bearish,' \u2018Bullish,' or\n'Neutral.' Do not include any additional words\nin your answer. Your answer should start with\n'ANSWER:'\n2. Now, identify the 'rationales' behind your an-\nswer. The rationales are words, phrases or sen-\ntences in the original Twitter post that led you\nto answer with . Share\na list of rationales with one rationale per line.\nThe list should start with 'RATIONALS:'\n3. Alter the rationales in the original Twitter post\nso that your answer on the altered Twitter post\nbecomes . Keep the changes to\na minimum. The altered Twitter post should\nstart with 'ALTERED TWITTER POST:'"}, {"title": "C.5 GSM8K", "content": "C.5.1 Unconstrained prompting\n1. You will be given a math problem. The solu-\ntion to the problem is an integer. Your task\nis to provide the solution. Only"}]}