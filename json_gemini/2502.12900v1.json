{"title": "Soundwave: Less is More for Speech-Text Alignment in LLMs", "authors": ["Yuhao Zhang", "Zhiheng Liu", "Fan Bu", "Ruiyu Zhang", "Benyou Wang", "Haizhou Li"], "abstract": "Existing end-to-end speech large language models (LLMs) usually rely on large-scale annotated data for training, while data-efficient training has not been discussed in depth. We focus on two fundamental problems between speech and text: the representation space gap and sequence length inconsistency. We propose Soundwave, which utilizes an efficient training strategy and a novel architecture to address these issues. Results show that Soundwave outperforms the advanced Qwen2-Audio in speech translation and AIR-Bench speech tasks, using only one-fiftieth of the training data. Further analysis shows that Soundwave still retains its intelligence during conversation. The project is available at https://github.com/FreedomIntelligence/Soundwave.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have profoundly transformed the paradigm of natural language processing (NLP) due to their remarkable abilities in understanding and reasoning (Achiam et al., 2023; Touvron et al., 2023). Recently, multi-modal LLMs have also shown rapid development, with the success of GPT-4o highlighting the potential of speech-focused LLMs (Hurst et al., 2024). A fundamental requirement for achieving seamless communication with LLMs is their ability to accurately interpret speech-essentially enabling LLMs to \u201chear\u201d.\nHowever, most speech-based LLMs rely on massive labeled datasets and substantial computational resources to enable speech perception (D\u00e9fossez et al., 2024; Chen et al., 2025). For example, the Qwen2-Audio (Chu et al., 2024) model requires approximately 500,000 hours of data to achieve cross-modal functionality, while 1,000 hours needed by advanced automatic speech recognition models to achieve comparable results (Gulati et al., 2020). This discrepancy underscores the need for more efficient methods to develop speech-capable LLMs.\nWe first identify two fundamental challenges to achieve alignment between speech and text (Zhang et al., 2023): (1) the representation space gap and (2) sequence length inconsistency. The former challenge arises from the two modalities being developed independently, while the second challenge stems from the difference in modeling units-speech is typically represented at the frame level, whereas LLMs operate at the sub-word level. We then decouple the process of building speech LLMs to achieve more efficient training. Specifically, we propose a two-stage training framework designed to efficiently overcome these challenges. The first stage focuses on resolving the representation space gap, while the second stage aims to reduce the sequence length of speech.\nFurthermore, to efficiently bridge the gap between speech and text, the quality of alignment data plays a crucial role. To address this, we collect high-quality speech recognition data and manually annotate audio labels to support the first stage. For the second stage, we analyze the proportion of"}, {"title": "2 Methodology", "content": "2.1 Overall Design\nThe training process consists of three stages, as shown in Fig. 2. Stage I aims to align the representation between speech and text, addressing the representation space gap problem. Stage II primarily shrinks the speech sequence and mitigates the sequence length inconsistency. The supervised fine-tuning (SFT) stage (Wei et al., 2021) enables the speech LLMs to generalize across diverse tasks.\nThe input to the model consists of speech FBank features, which are then processed by the pretrained audio encoder. To efficiently align the representation with that of LLMs, we use the audio encoder that produces semantic features (e.g., Whisper (Radford et al., 2023) or Seamless (Communication, 2025)), rather than vector quantization features (D\u00e9fossez et al., 2022) or self-supervised"}, {"title": "2.2 Stage I: Alignment", "content": "We use the auxiliary CTC loss to improve training efficiency, as it can achieve alignment without the involvement of LLMs. Additionally, we use high-quality data to speed up the convergence rate.\n2.2.1 Auxiliary CTC loss\nThe audio encoder and LLMs have a gap in their representation spaces due to separate pre-training. One direct approach is to use ASR tasks for alignment. We design an adapter and utilize CTC loss (Graves et al., 2006) to achieve efficient cross-modal training. Specifically, the adapter consists of a linear layer followed by a Transformer layer (Vaswani, 2017). The linear layer transforms the audio sequence A \u2208 R^{l\u00d7h_a} where l is the length of the speech sequence and h_a is the hidden size of the audio encoder. We concatenate adjacent features and adjust the dimensionality to match that of the LLMs, resulting in A' \u2208 R^{l/2\u00d7h_{ilm}} where h_{ilm} is the hidden size of the LLMs. A Transformer layer then converts the features into the representation space of LLMs. Finally, we use CTC loss to train the adapter, aligning the shared space of the LLMs.\n2.2.2 High-quality Alignment Data\nWe believe that improving data quality is crucial to training efficiency for alignment. We apply data strategies for two types of data (ASR and sound data), as outlined below. The adapter is trained without the LLMs at this stage, thus the alignment training is fast. Our later experiments in Sec. 5.4 confirm the benefits to training efficiency.\nVerified ASR Data At this training stage, we use transcriptions from ASR data as the target, which we found to be crucial for improving convergence ratio. The selected high-quality data is all verified by advanced ASR model (Radford et al., 2023) with a Word Error Rate (WER) lower than 10%.\nStandardized Sound Data Another challenge is processing sound due to the inherent background noise and the diversity of labeling information. To address this, we annotate about 8k pieces of sound category data. We further select clear 20k sound samples, then unify label format and audio length."}, {"title": "2.3 Stage II: Shrinking", "content": "After aligning the data representation, we focus on reducing the length of the speech as detailed in Sec. 2.3.1. Additionally, at this stage, we include various types of foundational audio tasks to better generalize downstream tasks. This introduces a data mixture problem, which is solved by a dynamic data mixture strategy (see Sec. 2.3.2).\n2.3.1 Dynamic Shrinking\nThere are two essential aspects to shrinking the audio sequence: final length determination and lossless information retention.\nFinal Length Determination For the first aspect, we utilize the probability from CTC. The CTC predicts the corresponding word for each position. We then remove duplicate predictions from adjacent positions to obtain the final sequence. Since the sequence has been aligned to text in Stage I, the decoded result can indicate the final length.\nLossless Information Retention For the second aspect, we select the content based on the CTC output as the query, and then use attention mechanisms to gather related information, such as tone and pitch, in order to prevent information loss.\nAssume the speech features x have been aligned to the representation space of LLMs, then, we select the features based on the CTC probability to compress the sequence x'.\nX_{out} = norm (x' + cross_attn(x',x,x))  (1)\nwhere norm is the layer norm operation. X_{out} is the final output of the shrinking adapter. x' can be viewed as the content feature, while the gathered information, calculated by cross-attention, serves as auxiliary data for the selected features. The whole processing is shown in Fig. 3.\n2.3.2 Dynamic Data Mixture\nWe select both audio data (involving three basic audio tasks) and text data to enable LLMs to generalize to downstream speech understanding. Training with mixed data may be biased by dominant tasks due to data imbalance (see Table 2), and existing work has adopted curriculum learning (Das et al., 2024; Tang et al., 2023), though it requires considerable prior knowledge for proper design.\nInspired by temperature-based data sampling, which has previously been used to address multilingual data imbalance (Arivazhagan et al., 2019), we propose a dynamic data mixture guided by sampling temperature. Specifically, the sample rate for each task k is as follows:\nP_k = \\frac{\\sqrt[T]{D_k}}{\\sum_i \\sqrt[T]{D_i}}  (2)\nwhere the Dk denotes the data size of task k and T denotes the temperature. T is initially set to 1 and gradually increases. This causes the training to start with a sample-level uniform distribution and gradually shift to a task-level uniform distribution. Training at the former stage might be dominated by rich-source tasks, while at the latter stage, training might be more balanced among tasks, potentially alleviating the over-fitting issue.\nAdditionally, Chen et al. (2024) shows that text-related tasks aid instruction following for multi-modal LLMs. We also introduce the text task to ensure a smoother cross-modal process. We incorporate the"}, {"title": "3 Data Engineering", "content": "We introduce the data details for the three stages, respectively, see the summary in Tab. 3. The data shown in the table has been cleaned and filtered, and the details of strategies can be found in App. A.2. We sample some speech from several dataset to control the quality and training cost.\n3.1 Data During Stage I and II\nASR Data We choose high-quality datasets and filter the data with a WER of less than 10%, as tested by Whisper medium. We apply SpecAugment (Park et al., 2019) to enhance the robustness of the model towards speech. To help LLMs understand the conversation and the number of speakers, we splice speech from different speakers. We denote the output format as 'The first speaker says The second speaker says ...'.\nSound Data The sound data is often too short and may be viewed as noise, which causes the model to fail in perceiving it. To address this problem, we embedded environmental sounds into the audio to construct the data. For example: \u2018But there was a passenger dropped off for you, a little girl. <throat_clearing> It's a boy I've come for.' The special token is added to the conversation, and the model needs to transcribe both the sound and speech simultaneously. This method of learning both speech and audio also makes training efficient."}, {"title": "3.2 Instruction Data During Stage III", "content": "Text Instructions The text-based instruction is designed to understand and analyze speech. We have created three types of QA formats, as shown in Tab. 4. The first requires the model to directly answer, which is the most difficult. The second provides detailed choices, and the last requires the model to output the answer in a natural format.\nSpeech Instructions If speech LLMs are to communicate directly with humans, it is essential for them to follow speech instructions. Once the speech is well-aligned, we can achieve this by using text-based dialogue data and synthesizing text into speech. We use AnyInstruct speech subset (Zhan et al., 2024), which is built using this approach."}, {"title": "4 Experiments", "content": "4.1 Settings\nTraining The audio encoder is Whisper Large V3 (Radford et al., 2023), and the foundation model is Llama-3.1-8B-Instruct (Dubey et al., 2024). The alignment adapter is a projection where the output size is 4096. We apply LoRA to the Attention module, where rank and \u03b1 are set to 64 and 16, respectively. Both alignment and shrinking stages consist of 6,000 steps, with the SFT stage set to around 4,000 steps. The sample temperature T at Stage II starts at 1 and increases by 5 per training epoch. The experiments are conducted on 32 A800 GPUs for training on 10k hours of data. The training time for the two stages is approximately four days, and the SFT requires an additional day. App. B shows more details about training settings.\nEvaluation We evaluate Soundwave on several basic tasks and the open-ended AIR-Bench. We also remove repeated samples (see App. A.4) before training to avoid data leakage. We primarily compare Soundwave with Qwen2-Audio, an advanced model for various audio processing tasks."}, {"title": "4.2 Results", "content": "Basic Audio Tasks We show the results on foundational audio tasks in Tab. 5. We find that our model demonstrates a significant advantage on the ST and SER tasks, which heavily rely on the understanding ability of speech LLMs. We also observe that our model shows strong performance on zero-shot tasks, such as translation tasks in other languages. On the other hand, our model still underperforms the SOTA model on the ASR task, indicating that massive training data is essential for ASR. We only used about 244 hours of sound data, which is dozens of times less than the SOTA, thus there is still a gap on the VSC task.\nAIR-Bench We compare our model on AIR-Bench across speech foundation, sound foundation, and speech chat tasks. As shown in Tab. 6, our model demonstrates SOTA performance on average speech foundation tasks with only about 10k of training data. Specifically, we outperform the best of previous speech LLMs on six sub-tasks. Since 98.61% of the training data consists of English speech, our model performs worse on the language identification task. This highlights that the proportion of different languages remains important.\nResults of the sound foundation task are shown in Tab. 7. Although only around 244 hours of data were used, our model is still superior to other models, except Qwen2-Audio, which is trained with 10k hours. Moreover, our single-encoder architecture performs better than the two-encoder model (Tang et al., 2023), indicating that fewer encoders can process both speech and sound simultaneously. Our model also performs well in AIR-Bench speech chat task, ranking second only to Qwen2-Audio among open source models."}, {"title": "5 Analysis", "content": "Considering that analysis based on full data requires massive training cost, we analyze our method based on Librispeech data. The experiments are trained on 8 A800 GPUs with 4,000 steps. We use Adapter (xn) to denote that the adapter architecture is the same as Qwen2-Audio, where n is the down-sampling rate.\n5.1 Convergence Rate\n5.2 Effect of Alignment\nWe randomly sampled 200 items from the Librispeech test clean set and then extracted text and speech representations. The similarity of speech and text after average pooling is compared, as shown in Fig. 6. We found that the representation of Soundwave with the alignment adapter is significantly higher than that of other methods. In addition, we further compare the average training speed under the same batch conditions. The training speed in the alignment stage is nearly three times faster than that of other methods. Whether due to the alignment effect or the training method, the alignment adapter shows obvious advantages.\n5.3 Effect of Shrinking\nWe compare the performance and compression ratios of different strategies on ASR tasks. We found that our approach compresses significantly based on text length. Our method maintains stable performance with 2.5% compression ratios. However, the compression method leads to performance degradation on other test tasks without the aid of auxiliary information. This demonstrates that auxiliary information can compensate for missing features, allowing the LLMs to receive complete information."}, {"title": "5.4 Data Quality", "content": "The training loss for Stage I, with and without cleaning the speech and sound data, is compared in Fig. 7. When uncleaned speech is used, the training process becomes unstable. Additionally, if the sound data is not properly processed, it significantly worsens the overall training. Given that the alignment stage only trains a few parameters to align the two pretrained large models, the quality of the training data is crucial."}, {"title": "5.5 Data Scaling", "content": "We compare the performance from 1k to 10k hours of data, and the results are shown in Fig. 8. Our model, using only 1k hours of training data, achieves performance comparable to previous speech LLMs. Note that we use only the ASR task as the SFT data, yet our model demonstrates decent capability in instruction following. This demonstrates that the speech representation is well aligned with the text representation. When we further scale up the training data, all tasks show consistent improvements."}, {"title": "5.6 Knowledge-Based QA", "content": "We present a case of using the speech instruction to ask complex questions in Fig. 9. We find that Soundwave inherits the rich knowledge of LLMs during the conversation. For more examples of performance in physics, chemistry, finance, mathematics, and other fields, refer to App. E."}, {"title": "6 Related Work", "content": "Speech contains rich non-semantic information compared to text (Wang et al., 2024; Bu et al., 2024; Huang et al., 2024). For LLMs to achieve an accurate understanding of audio, they must have a comprehensive perception of speech rather than relying solely on text (Ji et al., 2024a; Ao et al., 2024). As a result, many researchers have studied how to build end-to-end speech LLMs (Hu et al., 2024; Tang et al., 2023; Chu et al., 2024; Ghosh et al., 2024; Fang et al., 2024; Geng et al., 2025).\nSome studies have found the less is more phenomenon in LLMs with respect to data usage (Zhou et al., 2024; Song et al., 2025), meaning that efficient use of data can also achieve good performance."}, {"title": "7 Conclusion", "content": "Speech understanding is a core capability for multi-modal LLMs, yet current speech LLMs often rely on enormous amounts of training data, putting them out of reach for most academic researchers due to the high costs involved. To address this, we developed a more data-efficient solution: a three-stage training strategy paired with a model architecture that incorporates two adapters. This approach effectively tackles the mismatches in representation and length between speech and text. The trained Soundwave delivers top-tier performance on the AIR-Bench speech tasks, while requiring significantly less training data."}, {"title": "Limitations", "content": "Our work still has some limitations, specifically in the following three aspects:\n\u2022 We have not verified the feasibility of our approach on larger models with more parameters.\n\u2022 Due to time and manpower limitations, the amount of sound data we have labeled from the scene dataset is still relatively small. As a result, we are unable to conduct in-depth experiments to determine the optimal amount of sound data to include.\n\u2022 Due to the lack of relevant data, our model does not perform well in music understanding tasks and has limited support for multiple languages.\nNext, we will expand the parameter size of our model to verify the feasibility of our approach on larger models. We will also incorporate music understanding and multilingual data to enhance these capabilities. In addition, we will continue annotating the sound data to further validate the optimal data ratio. We also hope that other researchers in the community will conduct related studies."}, {"title": "Ethical Considerations", "content": "Use of Artifacts Ours study employs Whisper Large V3 as the audio encoder to extract and process speech input data and utilizes Llama-3.1-8B-Instruct as the foundation model for downstream tasks. In using these models, we adhere to academic standards and have cited their original papers and relevant documentation to ensure proper scholarly attribution. Additionally, Whisper is released under MIT License, while Llama-3.1-8B-Instruct is subject to Llama 3.1 Community License. We have ensured that our application of the model does not violate any of the specified restrictions, thereby maintaining compliance with the license terms.\nData Collection All the datasets used in our study are publicly released open-source datasets, and we strictly adhere to the corresponding open-source license agreements to ensure the legality and compliance of the data sources. In addition, the supplementary data annotation work we conducted did not involve any data privacy or sensitive information. Detailed procedures and workflow of the data annotation work can be found in Section A.1. The content related to Statistics For Data can be found in Section A.3."}, {"title": "A Data Construction and Preparation", "content": "A.1 Sound Re-annotation\nSince a scene segment may contain multiple sounds, we allow users to select multiple sound labels. In the end, we labeled 7,863 audio files, totaling 7.30 hours, with 5,197 files being single-label, totaling 4.81 hours. Specific details can be found in the following sections. We will release all of these labeled data.\nA.1.1 Data Splitting\nWe divide the original 10s data into combinations of 3s-3s-4s for annotation.\nA.1.2 Data Annotation\nIn accordance with the pre-existing scene labels, we established detailed sound annotations, such as waves and birdsong for a beach scenario. Subsequently, we recruited a number of volunteers to perform data annotation tasks. The interface utilized for this process is illustrated in Fig. 10.\nA.1.3 Volunteer Sources and Salaries\nWe extensively recruited volunteers for this project, comprising 70% undergraduate students, 25% graduate students, and 5% individuals who have already graduated. Each participant was compensated with a one-time payment of 200 RMB, which aligns with the prevailing wage levels in mainland China.\nA.1.4 Volunteer Authorization\nAll volunteers have agreed to the public release of their labeled data to promote academic research within the community.\nA.2 Data Process\nThe process from raw data to final application in this paper includes two steps: Data Selection and Data Filtration.\nData Selection\nWe performed data selection on the following dataset:"}, {"title": "A.3 Data Statistics", "content": "We present the usage of audio data and the total amount for each dataset in Table 3. It is important to note that in this statistical process, the same data is counted only once across different stages, only once across different tasks, and only once even if constructed using different methods within the same task. If you are interested in the specific data usage for each stage and task, please refer to the subsequent section.\nA.3.1 Alignment Stage Data\nIn Tab. 9, we present the datasets used during the alignment stage, along with their respective quantities and durations.\nA.3.2 Shrinking Stage Data\nIn Tab. 10, we present the datasets used during the shrinking stage, along with their respective quantities and durations.\nHere, we have two points that need clarification: 1) Why is the ASR data reduced compared to the first stage? 2) Why has the \"TUT w./ LibriTTS\" data increased compared to the first stage?\nHere, we will explain why the ASR data in this stage is reduced. In this stage, the LoRA parameters of the large model need to be trained. To ensure stable training, it is important to avoid overlap between tasks executed by different instructions. As shown in Figure 1, we have a dedicated task for"}, {"title": "A.3.3 SFT Data", "content": "Our SFT data statistic details are shown in Table 12. To further clarify, we will specify in the following two paragraphs how we constructed the training data and tested these tasks.\nTraining Data Examples of the input and output for our SFT data can be found in Table 13. We employed two question-answer construction schemes: direct Q&A and multiple-choice. However, some questions are not suitable for direct Q&A. For instance, in tasks like Age Prediction, direct answers might confuse the model. Therefore, we only constructed multiple-choice format data for such tasks.\nTo enhance the instruction-following capability of our model, we do not rely on a single instruction to construct the training data. Instead, we use at least 50 different instructions for each task to build the dataset. Due to space limitations, we are unable to list all these instructions individually. We will subsequently release our complete training data.\nA.4 Mitigating Data Leakage Risks\nIn this section, we will discuss the risks of data leakage in several parts. For one set of tasks, we used non-homogeneous training data, while for another set, although we employed homogeneous data, we rigorously considered the issue of data leakage."}, {"title": "A.4.1 Non-homogeneous Training Data", "content": "Speech Gender Recognition The task involved in the test set is AIR-Bench(Yang et al., 2024), which uses Common Voice(Ardila et al., 2019) and MELD(Poria et al., 2019) to construct the data. We use TextrolSpeech(Ji et al., 2024b) to construct the data, which is considered non-homogeneous data in comparison.\nSpoken Language Identification This task involves a total of 7 languages: Chinese, English, Italian, German, French, Spanish, and Japanese. AIR-Bench (Yang et al., 2024) uses Covost2 (Wang et al., 2021) in its construction, which is sourced from Common Voice (Ardila et al., 2019). The construction of data in English, Italian, German, French, and Spanish, we used Europarl-ASR (Garc\u00e9s D\u00edaz-Mun\u00edo et al., 2021), while for Chinese data, we used AISHELL3 (Shi et al., 2020). These sources are different from Common Voice, so there is no data leakage. For Japanese data construction, we used Common Voice, which is the same source as AIR-Bench, so we paid special attention to potential leakage issues. We noticed that there were only two Japanese samples in AIR-Bench, so we manually removed these two entries.\nA.4.2 Homogeneous Training Data\nSpeech Grounding Since our training set is constructed using the same dataset as in the AIR-Bench test, we made sure that the test set was not included in the training. We removed data where the same word in the same position was queried in the audio. Specifically, due to the difficulty of ensuring that randomly selected data doesn't overlap during the selection process, we adopted a post-processing approach where we deleted training data with the same filename and identical queries.\nEmotion Recognition During the construction of our training dataset, we utilized the TextrolSpeech (Ji et al., 2024b), RAVDESS (Livingstone & Russo, 2018), CREMA-D(Cao et al., 2014), IEMOCAP (Busso et al., 2008), and MELD (Poria et al., 2019) datasets. Notably, TextrolSpeech is composed of multiple datasets, including ESD, MEAD, MESS, SAVEE, and TESS. Given that AIR-Bench incorporates data from IEMOCAP and MELD, we have entirely excluded these datasets from our training data. Additionally, the test set of MELD has also been removed to ensure data integrity and prevent potential data leakage.\nSpeech Entity Recognition and Intent Classification We used the same source data as AIR-Bench for construction, both utilizing SLURP (Bastianelli et al., 2020), so we paid special attention to data leakage issues. Since AIR-Bench retained the original file names for all its files, we directly removed this portion of the data from our dataset."}, {"title": "B Training Configurations", "content": "The training settings of different stages are shown in Tab. 14. For all training and decoding processes, we set 'You are a helpful language and speech assistant. You are able to understand the speech content that the user provides and assist the user with a variety of tasks using natural language.' as the system prompt."}, {"title": "C Instructions used in the experiment", "content": "Table 15 presents the instructions we used in the experiment on \"Performance on Foundation Tasks\".\nIn order to align with the previous test results, we have fully adopted the instructions from its test data for AIRBench.\nD Other AIR-Bench Evaluation\nD.1 AIR-Bench Sound and Music Foundation Tasks\nThe AIR-Bench sound and music tasks evaluate models on various auditory capabilities. Sound tasks focus on identifying, classifying, and reasoning with environmental sounds, while music tasks\ninvolve classifying musical elements, analyzing pitch and velocity, and understanding emotional content. As Table 17 shown, Soundwave demonstrates exceptional performance in vocal sound and acoustic scene classification, achieving impressive accuracy. Though few sound data is used, sound average score of Soundwave still ranks second. This highlights its strong performance across sound-related tasks, even with limited data.\nD.2 AIR-Bench Chat Tasks\nThe AIR-Bench chat tasks evaluate language models' ability to generate conversational responses based on speech, music, environmental sounds, and mixed audio. Evaluation is conducted using GPT-4-0125-preview, which rates the model's responses on accuracy, relevance, usefulness, and comprehensiveness, on a scale from 1 to 10. The evaluation results are shown in Table 18.\nE Speech Instruction\nWe demonstrate our model's ability to follow voice commands from two aspects: generation tasks and knowledge question-answering tasks.\nE.1 Generation Tasks\nWe selected some commonly used generation tasks in daily life, which demonstrate our model's ability to assist in handling everyday affairs. Our presentation results are shown in Figures 12, 13, and 14.\nE.2 Knowledge-based QA Tasks\nWe will categorize the knowledge to showcase our model's QA ability across various domains of knowledge. Specifically, for math, please refer to Fig. 15. For physics, please refer to Fig. 16. For biology, please refer to Fig. 17. For history, please refer to Fig. 18."}]}