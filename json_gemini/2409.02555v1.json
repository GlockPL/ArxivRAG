{"title": "Low-Resolution Object Recognition with Cross-Resolution Relational Contrastive Distillation", "authors": ["Kangkai Zhang", "Shiming Ge", "Ruixin Shi", "Dan Zeng"], "abstract": "Recognizing objects in low-resolution images is a challenging task due to the lack of informative details. Recent studies have shown that knowledge distillation approaches can effectively transfer knowledge from a high-resolution teacher model to a low-resolution student model by aligning cross-resolution representations. However, these approaches still face limitations in adapting to the situation where the recognized objects exhibit significant representation discrepancies between training and testing images. In this study, we propose a cross-resolution relational contrastive distillation approach to facilitate low-resolution object recognition. Our approach enables the student model to mimic the behavior of a well-trained teacher model which delivers high accuracy in identifying high-resolution objects. To extract sufficient knowledge, the student learning is supervised with contrastive relational distillation loss, which preserves the similarities in various relational structures in contrastive representation space. In this manner, the capability of recovering missing details of familiar low-resolution objects can be effectively enhanced, leading to a better knowledge transfer. Extensive experiments on low-resolution object classification and low-resolution face recognition clearly demonstrate the effectiveness and adaptability of our approach.", "sections": [{"title": "I. INTRODUCTION", "content": "WITH the rapid development of deep learning, deep models have demonstrated remarkable success in various visual recognition applications [1]-[4]. For example, EfficientNet [1] delivers a top-1 classification accuracy of 88.61% on ImageNet [3] in large-scale visual recognition, Groupface [5] gives an extreme high accuracy of 99.85% on LFW [6] in face verification, cross-domain methods deliver impressive performance in gait recognition [2] and micro-expression recognition [4]. These achievements can be attributed to the ability of deep models with massive parameters to extract rich knowledge from extensive high-quality datasets. However, it may suffer from a sharp drop in accuracy when directly applying these models in practical scenarios due to domain distribution difference, i.e., the identified objects lack informative details due to occlusion [7] or low resolution [8]. Meanwhile, it is difficult to correct sufficient low-resolution training data in practical scenarios. Thus, it is necessary to explore a feasible solution that can address a key challenge in low-resolution object recognition: How to effectively transfer knowledge from high-resolution source domain to low-resolution target domain with minimal accuracy loss?\nAs shown in Fig. 1, in spite of the missing of many informative details, low-resolution objects still can be well recognized by subjects when they are familiar with the corresponding high-resolution objects. Recent works [9]\u2013[12] have shown that it is feasible to improve the recognition capacity of a model by knowledge transfer from high-resolution domain to low-resolution one. According to the level of this cross-resolution knowledge transfer, current approaches can be mainly grouped into sample-level or relation-level approaches. For sample-level knowledge transfer, Wang et al. [13] first proposed to use the corresponding high-resolution images to facilitate the model to extract features from low-resolution images. Subsequently, by learning low-resolution face representations and mimicking the adapted high-resolution knowledge, a light-weight student model can be constructed with high efficiency and promising accuracy in recognizing low-resolution faces [9], [11]. However, sample-level knowledge is limited and insufficient to help the model extract sufficiently discriminative features, especially for cross-resolution knowledge transfer. Therefore, the researchers explored the"}, {"title": "II. RELATED WORKS", "content": "relation-level knowledge transfer. Some recent works have shown that transferring the structural similarity instead of representation is beneficial to student learning [14]\u2013[17]. Ge et al. [10] proposed a hybrid order relational distillation to distill richer knowledge from pretrained high-resolution models to facilitate low-resolution object recognition. In general, these approaches have achieved impressive performance. However, they all use low-order relation knowledge to model the mutual information, which may ignore complex high-order inter-sample interdependencies, e.g., contrastive relation, and lead to insufficient knowledge transfer for object recognition.\nRecently, contrastive learning approaches [18]\u2013[21] have been widely used to learn feature representations from data samples by comparing the data with the positive and negative samples in the feature space. These approaches only need to learn discrimination in the feature space. Thus, they will not pay too much attention to pixel details, but can focus on more abstract semantic information, leading to simpler processing than pixel-level reconstruction [18]. Recent contrastive learning is combined with knowledge distillation, and these contrastive-based distillation approaches [19]\u2013[21] aim to capture the correlations and higher-order output dependencies for each sample. Typically, contrastive-based distillation approaches can facilitate cross-resolution knowledge transfer, since they essentially preserve the inter-sample relations which usually are more valuable than the sample representations themselves, especially in visual recognition tasks. The key is the relation modeling for effective knowledge transfer.\nTo transfer high-order dependency within the representation in both relation estimation and knowledge distillation, we propose a teacher-student learning approach for low-resolution object recognition via cross-resolution relational contrastive knowledge distillation with two streams, as shown in Fig. 2. The teacher stream is initialized with a complex pretrained model for high-resolution recognition and the student stream trains a compact model with the help of structural relational knowledge between different resolution samples. By making the high-order relation between low-resolution samples and other high-resolution samples mimic the high-order relation between corresponding high-resolution sample and other high-resolution samples, the student can pay more attention on semantic information instead of pixel details, and then learn the distinction between low-resolution images in the feature space to improve low-resolution object recognition.\nOur main contributions are three folds: 1) we propose a cross-resolution relational contrastive distillation approach that is able to distill richer structural knowledge from pre-trained high-resolution models to facilitate low-resolution object recognition, 2) we propose a relational contrastive module to extract relational knowledge in contrastive representation space, and 3) we conduct extensive experiments to show the state-of-the-art performance and good adaptability of our approach in low-resolution object recognition."}, {"title": "A. Low-Resolution Object Recognition", "content": "The recognition of low-resolution visual objects is attracting increasing interest due to its widespread applica-"}, {"title": "B. Contrastive Learning", "content": "Contrastive learning is regarded as a very important part of self-supervised learning, which builds representations by learning to encode what makes two things similar or different. Recent works [18], [44], [45] have been widely used to learn the feature representations of samples by comparing the data with positive and negative samples in the feature space. Contrastive losses such as NCE [46] and infoNCE [18] measure the similarities of data samples in a deep representation space, which learn representations by contrasting positive and negative representation pairs. One of the major difficulties in contrastive learning is how to construct the positive and negative samples. Deep InfoMAX [46] takes local features of training images and different images as positive and negative samples respectively. Instance Discrimination [47] learns to contrast the current embedding with previous embeddings from an online memory bank. The MOCO [44] and Sim-CLR [45] apply augmentation to train samples and requires the network to match original image and transformed images through contrastive loss. These methods only need to learn in the feature space, thus avoiding focus too much on pixel details but paying more abstract semantic information instead.\nFor knowledge distillation, Tian et al. [19] proposed to combine contrastive learning with knowledge distillation, and Xu et al. [20] represented contrastive task as a self-supervised pretext task to facilitate the extraction of richer knowledge from the teacher to the student. They show that incorporating contrastive learning loss into knowledge distillation can help student learn higher-order structural knowledge which can promote cross domain knowledge transfer. They are based on samples and the mutual relations are still insufficient. Thus, it is necessary to explore more effective forms to model the mutual relations of deep representations instead of the representations themselves. Zheng et al. [48] proposed relation knowledge distillation by linking cluster-based and contrastive-based self-supervised learning. However, such methods often suffer from poor generalization. To address that, we take into account higher-order relational information between the samples across different image resolutions."}, {"title": "III. THE APPROACH", "content": "The objective of our cross-resolution relational contrastive distillation (CRRCD) is sufficiently distilling high-order relational knowledge from a pretrained teacher for high-resolution recognition and effectively transferring it to learn a compact student for low-resolution recognition. Toward this end, we build the training instances by taking massive pairs of high-resolution images and corresponding low-resolution images in a self-supervised manner, and utilize vectors to define the representation relations. A feature relation module is utilized to estimate the teacher relation vector in teacher space and the student relation vector in cross-resolution space, respectively. The module is a simple learnable network that consists of two linear layers and a nonlinear activation layer. It is employed to estimate the relation vector between sample representations. Additionally, the cross-resolution relation vector is supervised by its corresponding vector in teacher space. In this manner, relation estimation and representation learning is performed in a unified way. In general, the student is trained on the images from source domain but deployed in target domain, and these two domains often exist large representation discrepancy. Therefore, our relation modeling manner needs to address cross-resolution knowledge transfer with good adaptability."}, {"title": "A. Problem Formulation", "content": "We denote the training set as D = {(x_i^h, x_i^l, y_i)}, where x_i^h represents the ith high-resolution sample with class label y_i \\in {1, 2, ..., c} and x_i^l corresponds to the corresponding low-resolution sample. Here c is the number of classes. Given a teacher network t with parameters W_t and a student network s with parameters W_s, we denote the representation of a sample pair (x_i^h, x_j^l) produced by the two networks as e_i^t = \\phi_t(W_t; x_i^h) and e_j^s = \\phi_s(W_s; x_j^l), respectively. Let (x_i^h, x_j^h) and (x_i^h, x_j^l) be two sample pairs randomly chosen from the training set. The relation between x_i^h and x_j^h in teacher space can be modeled as v_{i,j}^t, where v is a relation vector produced by the feature relation module F that takes e_i^t and e_j^t as inputs. Similarly, we denote v_{i,j}^{t,s} as the relation vector across the teacher and student space, the inputs of feature relation module are e_i^t and e_j^s, respectively. The specific form is v_{i,j}^{t,s} = \\varphi (\\sigma (\\varphi (\\phi_t(x_i^h) \u2013 \\phi_s(x_j^l)))), where \\varphi and \\sigma denote the linear transformation and the ReLU function, respectively. We hope that the cross-space relation v_{i,j}^{t,s} can be consistent with v_{i,j}^t with the help of relational contrastive distillation loss."}, {"title": "B. Cross-Resolution Relational Contrastive Distillation", "content": "Let x represent the input, we denote its empirical data distribution as p(x). For the conditional marginal distributions p(v_{i,j}^t|x), p(v_{i,j}^{t,s}|x), the sampling procedure is described as:\nx_i^h, x_j^h, x_i^l, x_j^l \\sim p(x)\nv_{i,j}^t = F_t (\\phi_t (W_t; x_i^h), \\phi_t (W_t; x_j^h))\n$$ (1) $$\nv_{i,j}^{t,s} = F_{t,s} (\\phi_t (W_t; x_i^h), \\phi_s (W_s; x_j^l)),\nwhere F_t and F_{t,s} are two learnable networks for computing the relation vectors. v_{i,j}^t and v_{i,j}^{t,s} represent the relationship between the i-th and j-th samples in teacher space and cross-resolution space, respectively. Intuitively, by maximizing Kullback-Leibler (KL) divergence between the joint distribution p(v_{i,j}^t, v_{i,j}^{t,s}|x) and the product of marginal distributions p(v_{i,j}^t|x)p(v_{i,j}^{t,s}|x), we can maximize the mutual information (MI) I between student and teacher representations [19]:\n$$ I(v_{i,j}^t, v_{i,j}^{t,s}) = E_{p(v_{i,j}^t,v_{i,j}^{t,s}|x)} log \\frac{p(v_{i,j}^t, v_{i,j}^{t,s}|x)}{p(v_{i,j}^t|x)p(v_{i,j}^{t,s}|x)}\n$$ (2)"}, {"title": "MI lower bound.", "content": "To setup an appropriate loss to maximize the mutual information, we define a distribution q with latent variable b which indicates whether the relation tuple (v_{i,j}^t, v_{i,j}^{t,s}) is drawn from the joint distribution (b = 1) or the product of marginal distributions (b = 0):\n$$ q(v_{i,j}^t, v_{i,j}^{t,s} | b = 1) = p(v_{i,j}^t, v_{i,j}^{t,s}) $$\n$$ q(v_{i,j}^t, v_{i,j}^{t,s} | b = 0) = p(v_{i,j}^t)p(v_{i,j}^{t,s}). $$\nHere, b = 1 means v_{i,j}^t and v_{i,j}^{t,s} are computed based on the same input pair, and b = 0 means v_{i,j}^t and v_{i,j}^{t,s} are independently selected. Now, suppose in our data, we give 1 relevant relation pair (b = 1) with n irrelevant relation pairs (b = 0). Then the priors on the latent b are q(b = 1) = 1/(n + 1) and q(b = 0) = n/(n + 1). By combining the priors with the Bayes' rule, the posterior for b = 1 is given by:\n$$ q (b = 1 | v_{i,j}^t, v_{i,j}^{t,s}) = \\frac{p (v_{i,j}^t, v_{i,j}^{t,s})}{p (v_{i,j}^t, v_{i,j}^{t,s}) + np (v_{i,j}^t) p (v_{i,j}^{t,s})}. $$\nThen the mutual information is defined as:\n$$ log q (b = 1 | v_{i,j}^t, v_{i,j}^{t,s}) \\leq - log n + log \\frac{p (v_{i,j}^t, v_{i,j}^{t,s})}{p (v_{i,j}^t) p (v_{i,j}^{t,s})}. $$\nTaking the expectation on both sides, Eq. (5) is rewritten as:\n$$ \u2161(v_{i,j}^t, v_{i,j}^{t,s}) > log \\frac{1}{n+1} + E_{q(v_{i,j}^t,v_{i,j}^{t,s}|b=1)} log q(b = 1 | v_{i,j}^t, v_{i,j}^{t,s}), $$\nwhere I(v_{i,j}^t, v_{i,j}^{t,s}) is the mutual information between the relation distributions of the teacher and student embedding. Thus maximizing E_{q(v_{i,j}^t,v_{i,j}^{t,s}|b=1)} log q(b = 1 | v_{i,j}^t, v_{i,j}^{t,s}) the parameters of the student network will increase a lower bound on mutual information."}, {"title": "Relation contrastive loss.", "content": "Actually, we maximize the log like-lihood of the data under the model to estimate true distribution, which is defined as:\n$$ L_{critic}(h) = E_{q(v_{i,j}^t,v_{i,j}^{t,s}|b=1)} [log h(v_{i,j}^t, v_{i,j}^{t,s})] + nE_{q(v_{i,j}^t,v_{i,j}^{t,s}|b=0)} [log(1 \u2013 h(v_{i,j}^t, v_{i,j}^{t,s}))].\n$$ (7)\n$$ h^* = arg max_h L_{critic} (h) < optimal critic. $$\nWe term h the critic since the representations are learned to optimize the critic's score. Considering that the bound in Eq. (6) and the E_{q(v_{i,j}^t,v_{i,j}^{t,s}|b=1)}[log h(v_{i,j}^t, v_{i,j}^{t,s})] is non-positive, we weaken the bound in Eq. (6),\n$$ \u2161(v_{i,j}^t, v_{i,j}^{t,s}) > log \\frac{1}{n} + L_{critic}(h). $$\nWe may choose to represent h with any family of functions that satisfy h: {v_{i,j}^t, v_{i,j}^{t,s}} \u2192 [0,1]. In practice,\n$$ h (v_{i,j}^t, v_{i,j}^{t,s}) = \\frac{e^{h1 (v_{i,j}^t)h2(v_{i,j}^{t,s})/T}}{e^{h1(v_{i,j}^t)h2(v_{i,j}^{t,s})/T} + n/m}, $$\nwhere n is the number of negatives, m is the dataset cardinality and \\tau is a temperature for adjusting concentration level. h_1 and h_2 first perform the linear transformation on relations, then normalize the transformed relations with l2 norm.\nIn our approach, the inputs for the function h are teacher-space relation v_{i,j}^t and cross-space relations v_{i,j}^{t,s}. We aim to maximize the mutual information, which is equivalent to minimizing the relation contrastive loss L_{rcd}:\n$$ L_{rcd} = - \\sum_{q(b=1)} logh (v_{i,j}^t, v_{i,j}^{t,s}) - n \\sum_{q(b=0)} log [1 \u2212 h (v_{i,j}^t, v_{i,j}^{t,s})], $$\nwhere {(v_{i,j}^t, v_{i,j}^{t,s}) | b = 1} acts as positive pairs while {(v_{i,j}^t, v_{i,j}^{t,s}) | b = 0} acts as negative pairs.\nTo achieve superior performance and conduct fair comparisons, we also incorporate the naive knowledge distillation loss L_{kd} along with our relation contrastive loss. Given the presoftmax logits z^t for teacher and z^s for student, the naive knowledge distillation loss can be expressed as\n$$ L_{kd} = p^2H (\\sigma (z^t/\\rho), \\sigma(z^s/\\rho)),\n$$ (12)"}, {"title": "IV. EXPERIMENTS", "content": "where \u03c1 is the temperature, H refers to the cross-entropy and \u03c3 is softmax function. The complete objective is:\n$$ L = L_{cls} + \u03b1L_{kd} + \u03b2L_{rcd}, $$\nwhere L_{cls} represents the arcface loss for face recognition, or cross-entropy loss for object classification. We experimentally determine a best combination of the three loss terms, and set \u03b1 = 0.5 and \u03b2 = 2 in our approach.\nRelationships to similar distillation approaches. Like CRD [19] and CRCD [21], our CRRCD is also based on contrastive learning and has a certain similarity in analysis such as a lower bound on the mutual information. Different from them, our approach is designed for cross-quality knowledge transfer in low-resolution recognition task, and the modeling granularity of relational knowledge between samples is finer and the order is higher. Specifically, compared with CRD, CRRCD takes into account higher-order information between samples in different resolution data and requires less negative samples for training. The main differences from CRCD include: 1) CRRCD focuses on the relation between sample representations, while CRCD calculates the relation between sample gradients which may affect the performance of student model detrimentally on low-resolution recognition and increase the cost, 2) CRRCD facilitates cross-resolution knowledge transfer by modeling the relation between samples in different resolution data, while CRCD only transfers information from the same data resolution, 3) CRRCD uses a more efficient critic function Eq. (10) to estimate the distribution q (b = 1 | vt, vt,s), which helps to maximize a lower bound on the mutual information. Therefore, our CRRCD can achieve better performance on low-resolution object recognition."}, {"title": "A. Low-resolution Object Classification", "content": "Object classification is a general visual recognition task and has very important applications under the low-resolution condition like industrial inspection and medical diagnosis. In the"}, {"title": "B. Low-resolution Face Recognition", "content": "Low-resolution face recognition is a specified and challenging object recognition task and has very helpful applications like recognizing surveillance faces in the wild. In practical scenarios, the facial images often have low resolution, uneven light intensity, diverse facial posture and facial expression. These will have a huge impact on the recognition accuracy. In our experiments, we take CASIA-WebFace as training set, which contains 10575 categories and a total of 494414 images collected from the web. The teacher is trained on CASIA-WebFace with ResNet50 under the high-resolution of 112 x 112, and the students are trained on low-resolution CASIA-WebFace with ResNet18. Then, the trained students are used to evaluate face verification on LFW, face identification on UCCS and face retrieval on TinyFace, respectively. In order to verify the validity of the low-resolution students, we emphatically check the accuracy when the input resolution is 16\u00d716 produced by bilinear downsampling. All approaches use the same experimental settings to ensure fair comparisons."}, {"title": "C. Ablation and Further Analysis", "content": "Effect of negative number. An important part of knowledge distillation based on contrastive learning is to construct positive and negative sample pairs, and the negative number has"}, {"title": "V. CONCLUSION", "content": "In this paper, we propose cross-resolution relational contrastive distillation, a novel approach to improve low-"}]}