{"title": "On Instruction-Finetuning Neural Machine Translation Models", "authors": ["Vikas Raunak", "Roman Grundkiewicz", "Marcin Junczys-Dowmunt"], "abstract": "In this work, we introduce instruction finetuning for Neural Machine Translation (NMT) models, which distills instruction following capabilities from Large Language Models (LLMs) into orders-of-magnitude smaller NMT models. Our instruction-finetuning recipe for NMT models enables customization of translations for a limited but disparate set of translation-specific tasks. We show that NMT models are capable of following multiple instructions simultaneously and demonstrate capabilities of zero-shot composition of instructions. We also show that through instruction finetuning, traditionally disparate tasks such as formality-controlled machine translation, multi-domain adaptation as well as multi-modal translations can be tackled jointly by a single instruction finetuned NMT model, at a performance level comparable to LLMs such as GPT-3.5-Turbo. To the best of our knowledge, our work is among the first to demonstrate the instruction-following capabilities of traditional NMT models, which allows for faster, cheaper and more efficient serving of customized translations.", "sections": [{"title": "1 Introduction", "content": "Instruction-finetuned Large Language Models (LLMs) demonstrate the remarkable ability of instruction-following (Wei et al., 2021), which makes them amenable to tackle any task cast as natural language generation, even under a zero-shot setting. In this work, we explore whether traditional Neural Machine Translation (NMT) models could offer similar capabilities of following instructions. NMT models could be considered as domain-specific 'language' models pre-trained for a single task (translation) and thereby could be instruction-finetuned to tackle translation-adjacent tasks such as translation customization or enforcing certain specifications on the translations. Such tasks, e.g., formality-controlled translation (Schioppa et al., 2021), multi-modal translation (Elliott et al., 2016) or gender-based translation rewriting (Kuczmarski and Johnson, 2018), have typically been tackled through specialized models or algorithms in prior literature, rather than a single instruction-following NMT model. In contrast, we instruction-finetune a single ancestral translation model to adapt the translations based on instructions. Our contributions are as follows:\n\n1.  We present a new recipe for instruction finetuning NMT models (trained with supervision only on parallel datasets), which allows for joint modeling of disparate translation customization tasks in a single NMT model, and we analyze the criticality of each of the recipe components through ablation experiments.\n\n2.  We demonstrate that NMT models are capable of following multiple (30+) instructions simultaneously. We also find that NMT models show abilities of zero-shot composition of instructions, as an effect of finetuning.\n\n3.  We show that, with a single instruction-finetuned NMT model, traditional customization tasks such as formality-controlled machine translation can be tackled with high performance, in conjunction with several disparate tasks.\n\nAdditionally, our proposed finetuned NMT model outperforms GPT-3.5-Turbo on average on the IWSLT-22 Formality Control Shared Task (Antonios et al., 2022), while simultaneously achieving high-performance on others & demonstrating a few other desirable properties vis-\u00e0-vis much larger LLMs. At a high-level, our work re-interprets a NMT model as a language model and demonstrates the utility of instruction finetuning NMT model for jointly modeling a myriad of disparate translation-related tasks. In the next sections, we elaborate on our recipe for instruction-finetuning of a NMT model and analyze its characteristics."}, {"title": "2 Related Work", "content": "Our work is at the intersection of two key themes: instruction finetuning\u2014primarily developed in the context of LLMs\u2014and customizing NMT models for specific tasks."}, {"title": "2.1 Instruction Finetuning of LLMs", "content": "Instruction finetuning refers to the supervised finetuning of a language model on task-specific input-output pairs by explicitly describing the task through instructions. This has been demonstrated to aid in cross-task generalization (Sanh et al., 2022a; Longpre et al., 2023), in particular, imparting LLMs with instruction-following capabilities (Wei et al., 2021). A number of prior works have proposed different algorithms for constructing the instruction data (Mishra et al., 2022; Wang et al., 2022; Honovich et al., 2023; Wang et al., 2023; Sanh et al., 2022b; Muennighoff et al., 2023; Iyer et al., 2023; Chung et al., 2022).\n\nIn our recipe, we rely on a combination of parallel data filtering and synthetic data generation through LLMs to construct the instruction dataset that is leveraged for finetuning NMT models. Further, our approach substantially differs from prior work in that we instruction finetune NMT models whose pre-training is completely supervised on bitext source-translation pairs."}, {"title": "2.2 Customizing Translation Models", "content": "There exists a large body of work in adapting NMT models and customizing them for specific use cases such as for achieving high-performance on specific domains (Saunders, 2022), tones or registers in the target language (N\u0103dejde et al., 2022) as well as for tasks such as gender-based translation rewriting (Rarrick et al., 2023). Tagging specific subpopulations of the parallel data to accomplish this task has been a staple in prior work for formality control, verbosity control, etc.\n\nOur work is related to the tagging approaches developed in the literature but differs in two key aspects: (a) task diversity and scale: typically, tagging is only applied to supply information pertaining to a single task, while instruction finetuning as"}, {"title": "3 Instruction Finetuning of NMT models", "content": "In this section, we describe the problem setting along with our instruction finetuning recipe and evaluation protocol."}, {"title": "3.1 Problem Setting", "content": "For instruction finetuning, we take a pre-trained NMT model and finetune it with instruction annotated source-translation pairs. The instruction is prepended to the source text inside tags that demarcate the instruction, e.g., <instruction> informal </instruction>. Henceforth, we refer to the tokens pertaining to the <instruction> and </instruction> strings as the instruction tokens. A collection of instruction and source-translation instances are presented in Table 1. Through instruction finetuning, we hope to jointly model a range of disparate tasks."}, {"title": "3.2 Instruction Finetuning Recipe", "content": "We present our simple recipe for instruction finetuning NMT models in Algorithm 1. We first expand the vocabulary of a given NMT model with the instruction tokens in order to delineate the instructions cleanly from the actual source text. Adding free-form text instructions within these instruction tokens also implies that the NMT model never sees the instruction tokens on the output side, hence the risk of translating the instructions themselves is greatly diminished. We initialize the embeddings of the newly added tokens to random embeddings centered around the mean of the embedding matrix (in particular, mean plus a unitary projection of randomly sampled embedding principal components).\n\nThe next step in the recipe is to curate both task-specific and parallel datasets used for finetuning. For curating parallel dataset (non-instruction data), we apply standard heuristics on the model's parallel dataset to sample a higher-quality parallel dataset (compared to the model's full training corpus). The details of the heuristics are presented in Appendix D. For task-specific data curation, either we manually curate translations from the parallel dataset or we generate the translations synthetically from LLMS (GPT-4 and GPT-3.5-Turbo). We describe task specific dataset curation in section 3.4.\n\nFinally, the NMT model is finetuned on a mix (2:1) of parallel and task data\u2014the mixing ratio is a hyperparameter in our recipe and we tune it so that we observe no degradation in general translation performance as measured on the WMT'20 validation set. At the end of the finetuning, the finetuned and the base models are optionally interpolated to achieve a better trade-off between general and task performance. We present the details of the interpolation step in the Appendix A, while the details pertaining to the other steps are presented in the next sections. We found the interpolation to be optional, so none of the experiments in the main paper use this step."}, {"title": "3.3 Evaluation Protocol", "content": "For the instruction finetuned NMT model, we have the choice of either translating an input without any instruction (the general case) or using a particular instruction (the instruction case). Throughout this work, we report the following measurements in order to evaluate the instruction finetuned NMT model:\n\n1.  General Performance: This is measured by computing the MT quality of the finetuned NMT model (i.e., the original translation task) on a standard test set. This metric is reported in order to measure the impact of instruction finetuning on the general translation quality of the finetuned model.\n\n2.  Task-Specific Performance: On a per-task basis we report two measurements:\n\na.  Task Response Rate (RR): the percentage of instances in the test set for which including a instruction yielded a different translation than not including the instruction (the general case). This offers us a crude measure to evaluate how"}, {"title": "4 Experiments", "content": "In this section we describe all experimental settings, from model architecture to data curation and evaluation."}, {"title": "4.1 Experimental Settings", "content": "We conduct experiments on the WMT'20 News Translation (English-German) task benchmark (Barrault et al., 2020). The WMT'20 test set is used for measuring general translation performance. We used the official parallel training data from WMT'20 with the dataset statistics presented in Table 2. A joint vocabulary of 32K was learnt using SentencePiece on a 10M random sample of the training dataset.\n\nThe trained model is a Transformer-Big (225M parameters) with the hyperparameters described exactly in Vaswani et al. (2017). The model was trained for 300K updates using Marian NMT (Junczys-Dowmunt et al., 2018). The metrics BLEU, ChrF2, TER (Papineni et al., 2002; Popovi\u0107, 2015; Snover et al., 2006) for the trained model on the WMT'20 validation and test sets (under beam size of 1) as"}, {"title": "4.2 Task-Specific Data Curation", "content": "The first column of Table 3 shows the list of task instructions. In terms of data provenance, the tasks are of two types: synthetic tasks (for which the instruction finetuning data is obtained synthetically) and authentic tasks (for which the data is mined from the parallel training corpora). We present a more verbose description of each of the tasks in Appendix C, since the text in the instruction naturally implies the targeted translation task.\n\nFor each of the 30 tasks, we curate instruction data using filters applied on the parallel data or through synthetic data generation using GPT-3.5-Turbo or GPT-4. In particular, the data for instructions pertaining to generating active voice, passive voice, simplifying, complexifying and obsfuscating translations were obtained synthetically through GPT-3.5-Turbo, whereas formal and informal translation data was obtained using GPT-4."}, {"title": "4.3 Finetuning and Evaluation Settings", "content": "The last checkpoint of the trained WMT'20 model is finetuned for 3 data epochs. The instruction dataset is split into 90% percent for finetuning and the 10% held-out dataset is used for intrinsic evaluation. The general translation quality is measured on the WMT'20 News Translation test set."}, {"title": "5 Results and Analysis", "content": "In this section, we characterize the behavior of the instruction finetuned NMT model using both intrinsic and extrinsic evaluations. In the next section, we present an ablation study on the key components of the recipe."}, {"title": "5.1 Instruction-Following Performance", "content": "Table 3 presents the results that characterize the instruction-following performance of the finetuned NMT model. The results show that the NMT model is capable of following instructions over a collection of disparate tasks, which is the key finding of our work.\n\nIn particular, both rule-based tasks such as leetify (which inserts leet-speak in the translation) as well as tasks which are more distributional and style-based in nature, such as complexify, are remarkably well learned by the NMT model. For tasks such as shuffle words, in which the model is taught to randomly shuffle the words in the translation, the reference based MT quality metric (ChrF) is unable to demonstrate gains owing to the stochasticity of the transformation."}, {"title": "5.2 Zero-Shot Composition of Instructions", "content": "Additionally, we investigate whether the model, trained on individual task instructions can compose two instructions. Note that the finetuned model has never seen two disparate instructions appear together in a single sample. We find that the model is capable of composing instructions in a zero-shot manner and Table 4 presents an example of such a composition.\n\nTo further investigate this behavior, in Table 4, we present additional metric named Task Success Rate (SR), which provides a binary measure of"}, {"title": "5.3 Extrinsic Evaluations", "content": "We conduct extrinsic evaluation on the WMT' 22 Shared Task for formality on English\u2013German translations. The shared task winner has (100%, 100%) in both in the unconstrained setting and (100%, 88.6%) in the constrained setting (Antonios et al., 2022). The instruction-finetuned model does not use any training data at all from WMT\u201922, relying only on the synthetic task data curated from GPT-4 and is evaluated on the test set directly. The results in Table 5 show that the instruction finetuned model is quite competitive with the WMT\u201922 task winner and achieves better performance that GPT-3.5-Turbo (evaluated in the zero-shot setting)."}, {"title": "5.4 General Translation Quality", "content": "The ChrF2 of the finetuned model on the WMT'20 test set is 61.9, which is +0.3 over the base WMT'20 model. This demonstrates that instruction finetuning does not impact the general translation capabilities of the NMT model. Similar trends hold for other metrics as well."}, {"title": "6 Ablation Study", "content": "In this section, we present an ablation study on the instruction finetuning recipe presented in Algorithm 1, wherein we remove the addition of explicit instruction tokens and the addition of parallel data from our recipe. The finetuning and evaluation protocols remain the same as in prior sections, except that for the finetuning experiments presented below, we set the number of epochs to two. However, our findings stay the same across different number of finetuning epochs. Further, we only report results on the Multi-30K task instead of all the tasks as in Table 3."}, {"title": "6.1 Ablating Parallel Data", "content": "Our recipe mixes task-specific and standard parallel data for finetuning. Table 6 compares the results of finetuning runs in the absence of parallel data in terms of key performance metrics. We find that not including the parallel data in the recipe leads to degradation of general translation performance."}, {"title": "6.2 Ablating Vocabulary Expansion", "content": "Our recipe expands the vocabulary of the NMT model with new instruction tokens. Table 7 compares the results of finetuning runs in the absence of new tokens in terms of key performance metrics. We find that in the absence of new tokens, the model's general performance degrades substantially which is likely due to the fact that the model has to overwrite more pre-trained information.\n\nAltogether, the above ablations point that both the key elements of our recipe are quite important. We hypothesize that this is owing to the fact that both of these components allow the model to overwrite less of its pre-training knowledge, which helps the model strike a better trade-off between"}, {"title": "7 Discussion", "content": "To conclude, we presented a simple yet effective instruction-finetuning recipe for unified modeling of multiple disparate translation-specific tasks in a single NMT model. Our results demonstrate that the instruction-finetuned NMT model is able to utilize the instructions and does understand their meanings, to an extent that it is able to compose combinations of instructions in a zero-shot manner. Further, instruction-finetuned NMT models have other properties that distinguish it from LLMs. Table 8 presents such a comparison on a few properties of interest:\n\n1.  Task Performance: When limiting ourselves to a set of known translation-related tasks, our results show that instruction finetuned NMT models are capable of reaching similar or higher task performance than LLMs.\n\n2.  Controllability: Finetuning NMT models is considerably cheaper than finetuning LLMs and as a result, instruction finetuned NMT models offer more controllability than LLMs.\n\n3.  Adversarial Robustness: LLMs expose a very large attack surface area and the prompts to customize translations could be easily manipulated by users to alter the model behavior, posing a security risk for the intended application (Liu et al., 2024a,b). However, instruction-finetuned NMT models, by default expose a much smaller attack surface area and thereby are less vulnerable to adversarial attacks\u2014some examples highlighting the differences with respect to prompt injection and intent misclassification attacks are in Table 9.\n\n4.  Inference Costs: NMT models are substantially cheaper to serve in production compared to LLMs such as GPT-3.5-Turbo, owing to smaller parameter sizes.\n\nAs such, instruction following NMT models which can broadly adapt translations based on desired user specifications for a large number of translation specific tasks might offer a better cost to quality and cost to security trade-off when compared to orders-of-magnitude larger LLMs."}, {"title": "8 Conclusion and Future Work", "content": "In this work, we presented a simple recipe for instruction finetuning NMT models. Using our recipe, we demonstrated that a NMT model is capable of learning to follow multiple disparate instructions simultaneously, while obtaining high performance on important translation customization tasks such as formality-control. Further, even though we experimented only on English-German as the language pair, our proposed recipe is quite general and language-pair agnostic. Our work opens up an interesting research direction\u2014on building instruction following NMT models which could leverage both the cheaper inference costs of NMT models as well as the broad customization capabilities of LLMs."}, {"title": "A Appendix A", "content": "We describe the interpolation step equation 1. This step interpolates between the parameters of the base model ($\\Theta_{base}$) and the finetuned model ($\\Theta_{finetuned}$) using a scalar interpolation weight $\\alpha$ which is applied for all common parameters between the base and the finetuned model (Ilharco et al., 2022). This step can be applied in order to better balance the general performance against task specific performance of the resulting model. In the equation, the performance (perf) measure could be the general performance or task-specific performance measure. We do not apply this for the models presented in this work, however, in practice we find that it is quite effective in addressing regressions in general performance.\n\n$\\Theta_{\\alpha}=max_{ \\alpha}{perf\\{ (1 - \\alpha) \u00b7 \\Theta_{base} +\\alpha\u00b7 \\Theta_{finetuned}\\}}$\n\n(1)"}, {"title": "B Appendix B", "content": "The metrics BLEU, ChrF2, TER (Papineni et al., 2002; Popovi\u0107, 2015; Snover et al., 2006) for the WMT20 trained model (under beam size of 1) as"}, {"title": "C Appendix C", "content": "We present a brief characterization of the different tasks here, along with some example input-output pairs in Table 10.\n\n\u2022 Rule Based Tasks: A number of tasks are rule based, e.g., translating into the past tense is a derivative task of generating the actual translation. Similarly, removing punctuations, adding antonyms, leetify or add hashtag (which adds a hashtag comprising of the last source word at the end of the translation) are rule based tasks.\n\n\u2022 Distributional Style Based Tasks: We include tasks such as generating translation in a particular style, which can be learned based on the synthetic LLM-generated translations.\n\n\u2022 Contrastive Tasks: Tasks such as length control in which the model is taught to control the verbosity of the translation is an example of a task in which the model is taught to generate translations which do not have any absolute property - but possess characteristics against some constrastive examples.\n\n\u2022 Multi-modal Task: Multi-30K represents the multi-modal translation tasks wherein an image accompanies the source input."}, {"title": "D Appendix D", "content": "For parallel data filtering, we replicate the bitext filtering pipeline of Wu et al. (2020). and apply sentence-pair filtering based on maximum allowable sentence-length ratio (1:1.3) and reverse sentence-length ratio (1.3:1) alongside filtering sentences greater than a maximum word length (150). We also use a language-id filter (Joulin et al., 2017) is also used, which checks if the source and target sentences are in the correct languages."}]}