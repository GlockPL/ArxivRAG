{"title": "IMPROVING ANOMALOUS SOUND DETECTION VIA LOW-RANK ADAPTATION\nFINE-TUNING OF PRE-TRAINED AUDIO MODELS", "authors": ["Xinhu Zheng", "Anbai Jiang", "Bing Han", "Yanmin Qian", "Pingyi Fan", "Jia Liu", "Wei-Qiang Zhang"], "abstract": "Anomalous Sound Detection (ASD) has gained significant in-\nterest through the application of various Artificial Intelligence\n(AI) technologies in industrial settings. Though possessing\ngreat potential, ASD systems can hardly be readily deployed\nin real production sites due to the generalization problem,\nwhich is primarily caused by the difficulty of data collection\nand the complexity of environmental factors. This paper in-\ntroduces a robust ASD model that leverages audio pre-trained\nmodels. Specifically, we fine-tune these models using ma-\nchine operation data, employing SpecAug as a data augmen-\ntation strategy. Additionally, we investigate the impact of uti-\nlizing Low-Rank Adaptation (LoRA) tuning instead of full\nfine-tuning to address the problem of limited data for fine-\ntuning. Our experiments on the DCASE2023 Task 2 dataset\nestablish a new benchmark of 77.75% on the evaluation set,\nwith a significant improvement of 6.48% compared with pre-\nvious state-of-the-art (SOTA) models, including top-tier tra-\nditional convolutional networks and speech pre-trained mod-\nels, which demonstrates the effectiveness of audio pre-trained\nmodels with LoRA tuning. Ablation studies are also con-\nducted to showcase the efficacy of the proposed scheme.", "sections": [{"title": "1. INTRODUCTION", "content": "In the realm of industrial automation, the ability to detect\nanomalous sounds is crucial for maintaining operational in-\ntegrity and preventing potential failures. The complexity of\nthis task arises from the need to distinguish between normal\noperational noise and genuine anomalies, a process that re-\nquires sophisticated algorithms capable of learning from di-\nverse acoustic patterns. In actual production environments,\nthe diversity of equipment types, the complexity of the sur-\nroundings, and the presence of domain shift issues in sound\ndata make it challenging to develop systems that can accu-\nrately identify and classify abnormal sounds across different\ndevices and environments.\nAnomalous sound detection tasks are distinct from other\naudio or speech tasks in several key ways [1, 2, 3]. While\ntasks like speech recognition, speaker identification, and\nacoustic scene classification focus on identifying and catego-\nrizing sounds based on their content or context, ASD tasks are\nconcerned with identifying sounds that deviate from a nor-\nmative baseline. In ASD, the system must discern between\nnormal operational sounds and those that are anomalous,\nwhich could indicate potential issues or malfunctions. This\nrequires the system to have a deep understanding of what\nconstitutes normalcy within a specific context, which is not\nalways explicitly labeled in the training data. In contrast,\nother audio tasks often have more clearly defined categories\nand labeled datasets for training purposes. This makes ASD\na unique and challenging area of research within the broader\nfield of audio signal processing.\nThe field of anomalous sound detection has seen a surge\nof innovative research aimed at improving the accuracy and\nreliability of detecting abnormal sounds in various environ-\nments. Recent studies have explored a range of method-\nologies. In terms of unsupervised learning, traditional ap-\nproaches have utilized statistical models and machine learn-\ning algorithms [4, 5], with recent advancements incorporat-\ning deep learning techniques such as Autoencoders (AE) and\nConvolutional Neural Networks (CNN) for feature extrac-\ntion and anomaly scoring [6]. These methods often rely on\nthe reconstruction error of AEs or the discriminative power\nof CNNs to differentiate between normal and anomalous\nsounds.\nOn the other hand, self-supervised techniques show great\npotential in recent works [7, 8, 9]. These techniques of-\nten involve data augmentation and pretext tasks that en-\nable the model to learn from the data's inherent structure.\nFor instance, the top-ranking team of DCASE 2020 Chal-\nlenge [8] introduced an unsupervised anomalous sound de-\ntection method based on MobileNetV2 that leverages self-\nsupervised classification and a group-masked autoencoder"}, {"title": "2. METHOD", "content": "The system architecture is inspired by the 3rd ranking team [18]\nof the DCASE 2023 Task 2 challenge, which is composed of\ntwo main components, namely the front-end and the back-\nend, as shown in Fig. 1. The front-end is responsible for\nprocessing the original audio signals. It includes several\npre-processing steps such as segmentation, windowing, en-\nhancement, and converting to log-mel spectrogram. The\nprocessed audio is then fed into an audio model to extract\nsemantic audio embeddings. During training, the audio em-\nbedding is further processed by a fully connected layer to\nperform classification tasks.\nIn the anomaly detection phase, the audio embedding is\npassed to the back-end, which consists of an anomaly detec-\ntor. This detector processes the input audio embedding to\ndetermine whether it is anomalous and outputs an anomaly\nscore. For practical applications of anomaly detection sys-\ntems, an anomaly score thresholding approach is used to\nmake judgments. In this paper, to evaluate the general per-\nformance, the area under the curve (AUC) and partial-AUC\n(pAUC) scores are then calculated as performance metrics."}, {"title": "2.2. Data augmentation", "content": "In this work, we mainly utilize SpecAug [19] as the data aug-\nmentation method. SpecAug operates directly on the feature\ninputs of a neural network, specifically the filter bank coeffi-\ncients. It employs a straightforward augmentation policy that\nincludes warping the features, masking blocks of frequency\nchannels, and masking blocks of time steps. By manipulating\nthese aspects of the audio signal, SpecAug effectively simu-\nlates various acoustic conditions, thereby enriching the diver-\nsity of the training dataset. However, only frequency masking\nand time masking are applied to the machine audio, while\ntime warping is not applied due to the degradation of detec-\ntion performance. Since Wav2Vec2 uses raw audio as input,\nSpecAug is not applied for this model."}, {"title": "2.3. Pre-trained models", "content": "In this section, we will provide an overview of the pre-trained\nmodels employed in our system.\nWav2Vec2 [15] is a model for speech recognition tasks\nthat employs self-supervised learning to extract speech repre-\nsentations from large amounts of unlabeled audio data. This\nallows for efficient semi-supervised training, where the model\nis first pre-trained on unlabeled data and then fine-tuned on a\nsmaller labeled dataset. Remarkably, Wav2Vec2 can achieve\nstate-of-the-art performance with as little as one hour of la-\nbeled data, demonstrating its ability to learn from limited la-\nbeled data and its potential for practical applications in ASD\nsystems.\nQwen-Audio [16] is a multimodal large audio language\nmodel developed by Alibaba Cloud, which extends the Qwen\nseries. The encoder component of Qwen-Audio is a pivotal\nelement of its architecture, utilizing the Whisper-large-v2\nmodel for initialization. It is designed to process a diverse ar-\nray of audio inputs, including human speech, natural sounds,\nmusic, and songs. It is trained in a multi-task training frame-\nwork The model achieves impressive performance across\ndiverse benchmark tasks without requiring any task-specific\nfine-tuning.\nBEATS [14] introduces a novel approach to audio pre-\ntraining by utilizing discrete label prediction loss, which\naligns with human auditory perception by focusing on high-\nlevel audio semantics rather than low-level time-frequency\ndetails. This method allows for the generation of discrete\nlabels with rich audio semantics. This method allows for\nthe generation of discrete labels with rich audio semantics.\nBEATS achieves this without the need for extensive training\ndata or complex model parameters, setting new records such\nas a mean average precision (mAP) of 50.6% on AudioSet-\n2M for audio-only models and 98.1% accuracy on ESC-50.\nCED [13] introduces a novel approach to audio tagging\nby combining augmentation and knowledge distillation (KD)\ntechniques. CED utilizes the Vision Transformer (ViT) as its\nfoundational model for audio tagging tasks. The ViT's abil-\nity to handle variable-length inputs and its scalability makes\nit an effective choice for the CED framework, which aims\nto improve audio tagging performance through consistent en-\nsemble distillation."}, {"title": "2.4. Fine-tuning methods", "content": "Two distinct fine-tuning strategies are employed to optimize\nthe model's performance. Firstly, we conducted full model\nfine-tuning. Secondly, we implemented Low-Rank Adapta-\ntion (LoRA) fine-tuning, a technique that introduces low-rank\nmatrices to adapt the model's weights without altering the\noriginal architecture. This method is particularly advanta-\ngeous as it significantly reduces the computational resources\nrequired for fine-tuning, making it an efficient alternative to\nfull model fine-tuning. More importantly, LoRA's support for\nincremental learning enables the model to adapt to new tasks\nor data without complete retraining. By focusing on low-rank\nupdates, LoRA helps preserve the knowledge captured during\npre-training, maintaining performance on downstream tasks."}, {"title": "2.5. Loss function and classification", "content": "Given the unpredictable and diverse nature of industrial\nequipment failures, the DCASE dataset [20, 21] has not\nprovided labels indicating whether devices are operating nor-\nmally or not, nor has it supplied labeled anomalous data for\nmodel fine-tuning. Since our front-end model is a classifier,\nan appropriate proxy task must be identified. The DCASE\ndataset includes device type and attribute labels, with at-\ntributes encompassing specific device identifiers (A, B, C,\nD) and operational states (e.g., speed, gear position). These\nlabels are utilized for the classification tasks, helping the\nmodels to learn the hidden features of the audio.\nThe choice of loss function is crucial during training.\nThis paper employs the ArcFace loss [22], an initially face\nrecognition-focused loss function. The mathematical repre-\nsentation of ArcFace Loss is given by:\n$L_{ArcFace} = \\frac{1}{N} \\sum_{i=1}^{N} -log \\frac{e^{s(cos(\\theta_{y_i}+m))}}{F}$  (1)\nwhere $F = e^{s(cos(\\theta_{y_i}+m))} + \\sum_{j=1, j\\neq y_i}^{C} e^{s cos(\\theta_j)}$, $\\theta_{y_i}$ is the\nangle between the feature vector $x_i$ and the weight vector $W_{y_i}$\nof the ground-truth class $y_i$, and $m$ is the margin penalty.\nIn sound anomaly detection tasks, distinguishing between\nnormal and abnormal sounds relies on the distance between\naudio features. ArcFace loss reinforces model discrimina-\ntion by ensuring tight intra-class feature angles and dispersed\ninter-class feature angles, thus amplifying the difference be-\ntween normal and abnormal samples and aiding in their effec-\ntive separation."}, {"title": "2.6. Back-end and anomaly detection", "content": "In the back end of the system, we implement the K-Nearest\nNeighbors (KNN) algorithm as our anomaly detector, a\nmethod commonly utilized in ASD tasks. We have con-\nfigured the hyper-parameter $k$ to a value of 1 and selected\ncosine distance as our chosen metric for measuring distances.\nAdditionally, in alignment with the approach detailed in [23],\nwe have trained our detector on datasets from both the source\nand target domains. The minimum distance recorded by these\ndetectors is then employed as the anomaly score for each\nitem. The final anomaly score for one item can be given by:\n$S_{Anomaly} = min(d_{source}, d_{target})$ (2)\nwhere $d_{source}$ is the distance given by the detector trained in\nthe source domain, and $d_{target}$ is the distance given by the\ndetector trained in the target domain."}, {"title": "3. EXPERIMENTS", "content": "In this paper, the DCASE 2023 Task 2 dataset is leveraged\nfor evaluating the proposed scheme and comparison with pre-\nvious SOTA models. The dataset features audio recordings\nfrom 14 machine types for both training and evaluation. Each\nmachine type provides 1000 clips of 10-second sounds for\ntraining, including 990 clips in the source domain and 10 clips\nin the target domain. Additionally, the validation data is com-\npletely separate from the evaluation data based on machine\ntypes, and no domain information is provided for either set.\nThe dataset is more challenging than classic anomaly detec-\ntion datasets due to domain shift and rare data availability."}, {"title": "3.2. Implementation details", "content": "In the data processing phase, the input configurations differ\nacross models. For Wav2Vec2 and Qwen-Audio, which pro-\ncess the original audio files, each input segment is set to 2\nseconds in duration. For BEATs and CED, which utilize Mel-\nfrequency cepstral coefficients (MFCCs) as input, each frame\nis defined by a window length of 25 milliseconds, a hop length\nof 10 milliseconds, and utilizes a Hamming window function.\nThe MFCCs employed are 128-dimensional.\nFor the training process, the learning rate scheduler is set\nto Warmup, with an initial learning rate of 0.00005 and an\nADAM optimizer. We set the batch size to 8 and trained the\nmodel for 30 epochs.\nFor the results, the evaluation metrics are in line with the\nofficial evaluation metrics of the DCASE Challenge. AUC in\nboth source and target domain and p-AUC are calculated, with\np in the range [0,0.1]. The final score is the harmonic mean of\nthe three scores above across all machine types, which is also\nthe official score used for ranking."}, {"title": "3.3. Results and analysis", "content": "We initially compared the outcomes of the selected pre-\ntrained models, including Wav2Vec2, Qwen-Audio, BEATS,\nand CED. The findings are summarized in Table 1. For our\nbaseline system, we selected the top-performing team from\nthe DCASE 2023 Challenge, which utilized a multi-branch\nnetwork incorporating CNN and Transformer architectures.\nThe results revealed that models pre-trained on Audio-Set\nconsistently outperformed all other models. In contrast,\nmodels pre-trained on speech-related tasks exhibited inferior\nperformance compared to the baseline model which trained\ndirectly on the DCASE dataset.\nNext, we tried using LoRA fine-tuning instead of full fine-\ntuning on the best-performing model, BEATs. Following the\nguidance of the LoRA authors, we initially introduced LoRA\nparameters to the q and v matrices of the Transformer archi-\ntecture and set the hyper-parameter r to 4. Given that LoRA\nwas initially developed for NLP tasks, and considering the\ndistinct nature of audio-related tasks, we explored various\nvalues for r, including 8, 16, 32, 64, and 128. The results\npresented in Table 2 indicate that for the task addressed in\nthis paper, the optimal value of r is 64. This outcome can\nbe attributed to the fact that tasks such as ASD or similar\naudio-related challenges necessitate a greater number of hid-\nden states compared to NLP tasks. Furthermore, the disparity\nbetween ASD tasks and pre-trained tasks contributes to this\nphenomenon, necessitating additional parameters for model\nadjustment to ASD-specific tasks.\nBesides this, we also conduct ablation studies. Since dif-\nferent layers and the various parameters within the same layer\nof a Transformer model will have distinct impacts on the re-\nsults [25], this section continues to use the BEATs model as\nan example, by fine-tuning different layer numbers and differ-\nent Transformer parameter matrices, to investigate how differ-\nent layers and parameter matrices within the model affect the\noutcomes in ASD tasks. The following experiments first set\nsome or all of the k, q, and v parameters in each Transformer\nlayer's matrix to be trainable while freezing the rest of the pa-\nrameters. Additionally, a series of experiments are conducted\nwhere LoRA parameters are introduced into different Trans-\nformer layers. The specific experimental results are detailed\nin the table below.\nFrom the results in Table 3 and Table 5, it is evident that\namong the k, q, and v parameter matrices of the Transformer,\nthe v matrix holds the greatest importance. Adjusting the k\nmatrix tends to result in a relative performance decline. For\nthe layers, those closer to the output tend to show more impor-\ntance. However, adding LoRA parameters across all layers\nyields the best results. Combining these findings with pre-\nvious LoRA tuning experiments, the final results achieve an\noptimal r value of approximately 64. Consequently, three ad-\njustments are proposed for LoRA parameter dimensions: in-\ncreasing the v vector dimension by 1.5 times(1), increasing\nthe dimension of the latter half of the layers by 1.5 times(2),\nand increasing the dimension of the latter half of the v matrix\nby 1.5 times(3).\nFrom the results in Table 6, although these adjustment\nmethods do not surpass previous methods in terms of aver-\nage AUC scores on both training and testing sets, method\n(3) mentioned above results in a new high on the testing set.\nTherefore, by enhancing the number of trainable parameters\nfor matrices that have a greater impact on results, it is possi-\nble to improve the model's generalization ability to a certain\nextent. Also, we compare our results with the systems of the\ntop 3 ranking teams in the DCASE 2023 Challenge and recent\npapers in Table 4. Our method exceeds all the previous works\non the evaluation set."}, {"title": "4. CONCLUSION", "content": "This paper presents a robust ASD model that capitalizes on\naudio pre-trained models, which are fine-tuned with machine\noperation data and enhanced with SpecAugment for data aug-\nmentation. The research also explores the efficacy of Low-\nRank Adaptation (LoRA) tuning as an alternative to full fine-\ntuning, addressing the challenge of limited data availability.\nThrough ablation studies, we have optimized the integration\nof LORA parameters. Our experimental results on the DCASE\n2023 Task 2 dataset confirm that our methods surpass tradi-\ntional convolutional networks and speech pre-trained models,\nvalidating the utility of audio pre-trained models and LoRA\ntuning. The achievement of a harmonic score of 77.75% on\nthe final evaluation dataset demonstrates the model's supe-\nrior performance. This work underscores the potential of au-\ndio pre-trained models and LoRA tuning in enhancing ASD's\ngeneralization capabilities in complex industrial settings."}]}