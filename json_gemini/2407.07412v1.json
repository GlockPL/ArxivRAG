{"title": "Pseudo-RIS: Distinctive Pseudo-supervision Generation for Referring Image Segmentation", "authors": ["Seonghoon Yu", "Paul Hongsuck Seo", "Jeany Son"], "abstract": "We propose a new framework that automatically generates high-quality segmentation masks with their referring expressions as pseudo supervisions for referring image segmentation (RIS). These pseudo supervisions allow the training of any supervised RIS methods without the cost of manual labeling. To achieve this, we incorporate existing segmentation and image captioning foundation models, leveraging their broad generalization capabilities. However, the na\u00efve incorporation of these models may generate non-distinctive expressions that do not distinctively refer to the target masks. To address this challenge, we propose two-fold strategies that generate distinctive captions: 1) 'distinctive caption sampling', a new decoding method for the captioning model, to generate multiple expression candidates with detailed words focusing on the target. 2) 'distinctiveness-based text filtering' to further validate the candidates and filter out those with a low level of distinctiveness. These two strategies ensure that the generated text supervisions can distinguish the target from other objects, making them appropriate for the RIS annotations. Our method significantly outperforms both weakly and zero-shot SOTA methods on the RIS benchmark datasets. It also surpasses fully supervised methods in unseen domains, proving its capability to tackle the open-world challenge within RIS. Furthermore, integrating our method with human annotations yields further improvements, highlighting its potential in semi-supervised learning applications.", "sections": [{"title": "1 Introduction", "content": "Referring image segmentation (RIS) task aims to segment the object referred by a given natural language description and has substantial applications across various fields such as human-object interaction [63], image editing [9]. While RIS is particularly challenging because it requires the model to map and understand both visual data and textual descriptions simultaneously, recent fully supervised approaches [12, 33, 44, 89, 95, 100] have shown remarkable advances."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Pseudo Supervision Generation.", "content": "Several works have explored generating pseudo supervision [11, 19, 31, 49, 51] without human labels to address expensive annotation costs in tasks like natural language video localization [60], video sentence localization [114], video corpus moment retrieval [29], and referring expression comprehension [27]. In video-related tasks [29,60,114], pseudo temporal event boundaries are generated within videos, followed by pseudo language queries using captioning models [41]. In referring expression comprehension task, Pseudo-Q [27] extract object bounding boxes with their category and attributes utilizing pre-trained detector [2] on Visual Genorm dataset [37], then they use heuristic algorithms to produce pseudo language queries based on the template comprising class name, attribute, and relationships. Unlike Pseudo-Q [27], instead of heuristic methods, we automatically generate pseudo language queries using an image captioning model [106] combined with our proposed decoding strategy."}, {"title": "2.2 Decoding Strategy.", "content": "Decoding strategy [35, 40, 55, 65, 74, 79,99] is a vital in auto-regressive language modeling like GPT [4,68] for progressing from previously generated words to the next work. The decoding strategies can be grouped into search [42,62,81] or sampling approaches [3,20,56]. In search-based approaches, beam search maximizes the multiple of the probabilities of each word in a sequence of words. Contrastive decoding [42] maximizes the word probability difference between larger and smaller language models. In sampling-based approaches, top-k and top-p [20] sampling sample words from the fixed top-k or top-p word probability distribution. Mirostate sampling [3] uses the statistical information of previously generated words to chose the next word. Our proposed strategy utilizes the word distributions of other images to sample the next word focused on a target image."}, {"title": "2.3 Referring Image Segmentation.", "content": "Fully supervised RIS approaches [7, 8, 10, 12, 15, 16, 21\u201323, 25, 26, 28, 33, 34, 38, 44, 47, 48, 52, 66, 75, 76, 83\u201385, 88, 89, 93, 95, 96, 98, 100, 102\u2013104, 115] have achieved remarkable performance by effectively fusing two different modalities. However, they highly depend on expensive human annotated data. To handle this, weakly and zero-shot RIS methods have emerged as potential solutions, recently. Weakly supervised methods [32, 39, 45, 80] use only image-text pairs for training. They mainly deal with overcoming the lack of localization for referred instances. Lee et al. [39] utilize Grad-CAM [73] for localization, and SaG [32] exploits slot attention [50] to capture individual entities. Zero-shot RIS methods [61,82, 107] solve the RIS task without any additional training by leveraging the vision-language foundation models [67,71] with the instance segmentation model [36, 86]. Unlike current weakly supervised and zero-shot RIS methods, we generate precise pseudo-supervisions for RIS to handle costly human annotation."}, {"title": "3 Method", "content": "In this section, we present Pseudo-RIS, the novel framework for RIS that automatically generates distinctive referring expressions paired with their corresponding target segmentation masks. We note that getting a distinctive referring text for each target region is critical particularly in RIS, as the resulting mask should change when a non-distinctive caption is given. For instance, in Fig. 1(a), the mask for the non-distinctive caption should change to cover all cows that have a tail. The proposed Pseudo-RIS consists of three steps: 1) precise mask extraction, 2) distinctive caption candidate generation, and 3) distinctiveness-based text filtering. In the mask generation step, we first obtain precise instance-level segmentation mask using a segmentation foundation model such as SAM [36]. Then, a text-generative foundation model, i.e., large-scale captioning model [106], is used to sample an extensive set of candidate referring expressions for each of the generated masks using our proposed distinctive caption sampling method. Finally, we further evaluate the captions generated for the given masks using our distinctiveness metric derived from CLIP [67] scores, and filter out ambiguous and imprecise captions [24]. In the following sections, we further elaborate on each step in more detail."}, {"title": "3.1 Mask Extraction", "content": "We first extract segmentation masks from an unlabeled image by exploiting a segmentation foundation model, SAM [36]. It allows us to obtain hiqh-quality and class-agnostic masks in the wild, leading to valuable pseudo-supervisions appropriate for the open-world nature of RIS."}, {"title": "3.2 Distinctive Caption Candidates Generation", "content": "Given extracted segmentation masks within an image, we employ an image-text foundation model, such as CoCa [106], to generate multiple distinctive caption candidates for each mask. \u03a4\u03bf generate distinctive captions, we propose a new decoding strategy, termed distinctive caption sampling based on the captioning model. In the proposed decoding strategy, the captioning model is encouraged to produce target-specific words capturing dicriminative features of the target image region, which allows the generated captions to distinctively refer only to the target region. For example, among several people as in Fig. 2, a target is uniquey identified by the description \"man wearing a red tie\" due to the target-specific word \"red\". In contrast, more generic words like \"tie\" or \"suit\" do not provide the same level of distinctiveness. In this section, we demonstrate how to generate such distinctive captions using the frozen captioning model with our distinctive caption sampling.\nGenerating Caption for a Target Mask. Given a cropped patch for a given target mask as visual context, the image captioning model generates a sequence of m words y = (Y1, Y2,\u2026\u2026\u2026, \u0423m) from the vocabulary V (y \u2208 V), that form the description representing the image region of the target mask. Consequently, the frozen auto-regressive captioning model computes a joint probability distribution of the sequence of words, P(y|xi), in a left-to-right and token-by-token manner:\n$$P(y|x_i) = \\prod_{t=1}^{m} P(y_t|y_{<t}, x_i, C),$$"}, {"title": "Distinctive Caption Sampling.", "content": "This aims to sample the next word that is more specific to a target mask at each step. To this end, we calibrate the probabilities of the words for a target patch by penalizing high-probability words in other patches and re-warding low-probability words in other patches. In detail, for a given target cropped patch xi, we calculate the probability distribution of the next word P(yt|y<t, xi), which is obtained by applying the softmax function to model output lozit z\u0142 (unnormalized score) at t step. Simultaneously, we also compute this probability for all other cropped patches as P(yt|y<t, xj), where xj \u2208 C\\{x}. Notice that y<t is previously generated words for a target patch xi. Then, the calibrated word probability at t step is given by:\n$$P(y_t|y_{<t}, x_i, C) = \\sigma(P(y_t|y_{<t}, x_i) - \\frac{1}{n} \\sum_{x_j\\in C\\\\{x_i\\}} S_{ij}P(y_t|y_{<t}, x_j)),$$\nwhere \u03c3(\u00b7) is the softmax function with a temperature, n = |C| - 1 denotes the number of the extracted masks except a target i in a given image, and sij is a cosine similarity between visual embeddings of xi and xj, extracted from the visual encoder of the image captioning model. sij modulates the influence of other patches, increasing it for patches similar to a target patch xi, and decreasing it for those not similar, thereby aiding in sampling words that distinguish similar objects better. We sample the next word yt from this calibrated distribution:\n$$y_t \\sim P(y_t|y_{<t}, x_i, C).$$"}, {"title": "3.3 Distinctiveness-based Text Filtering", "content": "In this section, we present our distinctiveness-based text filtering method, that filter out ambiguous and imprecise captions given all the generated caption candidates for each mask, by utilizing the discriminative power of a foundation model, CLIP [67], to validate whether the captions correctly describes the target masks. The generated caption candidates may sometimes ambiguously refer to the unintended object or even incorrectly describe a non-targeted mask located around a target mask within a cropped patch. This is because, (1) we employ a captioning model trained only on noisy non-curated web datasets, causing noisy captions, (2) a cropped patch for a target could contain other masks, (e.g. a horse appears in a cropped patch for a girl, as in Fig. 3, thus a captioning model could describe these non-targeted masks. To tackle these issues, we introduce a distinctiveness score that evaluates how well the generated captions uniquely and correctly identify the target mask. Using this score, misleading caption candidates that have low distinctiveness scores can be effectively filtered out.\nUniqueness. First, we demonstrate how to measure a description uniquely referring to its target. We calculate the CLIP score between a caption and all masks and then divide the CLIP score for a target mask by the highest CLIP score among all other masks. This is formulated as:\n$$UoS = \\frac{\\theta(x_i, y)}{max_{j\\in C\\\\{x_i\\}} \\theta(x_j, y)},$$\nwhere \\theta(x, y) is a function calculating the CLIP score between inputs. xi and y are a cropped patch and a description for a target i. xj is a cropped patch for other instance j, where j\u2208 C\\{x}. By dividing the CLIP score for the target mask with a given description by the maximum CLIP score among other masks, the UoS score reflects the relevance of the caption to the target compared to other masks. If this ratio is close to or lower than 1, it indicates that the caption may refer to at least one other unintended mask. However, the limitation of uniqueness is its inadequate ability to capture captions that correctly point to the target, caused by the simple calculation of the CLIP score between visual and textual embeddings without any in-depth consideration of whether the correct target object is being described. This issue, depicted in Fig. 3(a), highlights the need for another term to capture a description that correctly points to a target.\nCorrectness. To assess whether a caption describes the correct target or non-targeted object, we again employ the CLIP score. But this time, we focus on the masked region of objects, excluding the surrounding context, and on a target noun phrase that identifies the subject described in the caption. Formally, the correctness score is computed as follows:\n$$CoS_i = \\theta(x_i \\odot m_i, TNP(y)),$$\nwhere \\odot is a Hadamard product operation, mi is a segmentation mask for a target i, and TNP(\u00b7) is a function that extracts a target noun phrase, as in [107].\nDistinctiveness. We finally define distinctiveness scores by extending uniqueness scores using the correctness term in Eq (5) as follows:\n$$DoS_i = \\frac{CoS_i \\times \\theta(x_i, y)}{max_{j\\in C\\\\{x_i\\}} CoS_j \\times \\theta(x_j,y)}.$$"}, {"title": "4 Implementation Details", "content": "We utilize SAM [36] as a mask generator and CoCa [106] trained only on web-crawled datasets (LAION-2B [72] and CC3M [77]) as a captioning model. For distinctiveness-based text filtering, we adopt CLIP [67] with ResNet-50 [18] and set a threshold 7 to 1.3. We generate our pseudo supervision from 16,992 unlabeled images in the training set of RefCOCO+. We evaluate our pseudo supervisions using two RIS models: CRIS [89] and ETRIS [100]. In all our experiments, we follow their configurations without modifications.\nMask Generation. We extract segmentation masks using SAM [36]. It generates an excessive number of masks and results in 2,026,942 masks from 16,992 raw images, making a significant computational challenge. To address this, we reduce the mask count from overwhelming ~120 SAM masks per image to a manageable range of ~5 masks by taking overlaid SAM masks with CutLER [87] masks, which is an unsupervised segmentation model. The visualization of this process and the performance using CutLER masks are reported in the supplementary.\nText Generation. For each segmentation mask, we generate 4 distinct types of cropped patches, each with a different margin setting which is 0%, 10%, 20%, and a special 0% with the masked region of objects, influencing the contents of the generated caption. For each cropped patch, we generate 11 caption candidates using a captioning model by employing three decoding strategies (Beam search and our distinctive caption sampling with fixed top-k or top-p words), each with different hyperparameters (top-k words by k \u2208 {5, 7, 9, 11, 13} or top-p words [20] by p\u2208 {0.4,0.5,0.6, 0.7, 0.8}). Therefore, we generated a total of 44 caption candidates, and those captions with distinctiveness scores below the threshold T are discarded. The temperature is used as a default value in CoCa."}, {"title": "5 Experiments", "content": ""}, {"title": "5.1 Datasets and Metrics.", "content": "In our experiments, we use RefCOCO [59], RefCOCO+ [59], RefCOCOg [30,54] and PhraseCut [91] datasets. RefCOCO, RefCOCO+, RefCOCOg, and Phrase-Cut provide 19,994, 19,992, 26,711, and 77,262 images and 50,000, 49,856, 54,822, and 354,496 annotated instances with multiple referring descriptions, respectively. In particular, PhraseCut offers a large number of instance categories (3101 classes), in contrast RefCOCO datasets which handle only 80 categories. The referring descriptions in each dataset have different characteristics. RefCOCO provides positional cues of objects within the image such as left and right, it is banned in RefCOCO+, and RefCOCOg has longer and more complex sentences than others. PhraseCut presents a structured text form that includes attributes, categories, and relationships. For evaluation, standard evaluation metrics in RIS are used: oIoU (overall Intersection over Union) and mIoU (mean Intersection over Union). oIoU tends to penalize more in cases of failure in larger instances, because it calculates the total intersection divided by the total union across all predictions and ground truths masks. In contrast, mIoU treats each instance equally, regardless of its size as it computes the IoU for each individual sample and then averages these values across all samples."}, {"title": "5.2 Results", "content": "Main Results. In Table 1 and 2, we compare our pseudo-supervised method with zero-shot, weakly and fully supervised methods, in terms of oIoU and mIoU, respectively. We perform a zero-shot evaluation on RefCOCO, RefCOCO+ and RefCOCOg using two RIS models, ETRIS [100] and CRIS [89], trained with our pseudo supervisions generated from 16,994 raw images in the training set of RefCOCO+. For a fair comparison with zero-shot methods, we re-implement Global-Local CLIP [107] using SAM. As shown in Table 1 and 2, our method significantly surpasses both zero-shot [61,107], and weakly supervised RIS methods [32,39,45] by large margins. We also report the scores of the fully supervised SOTA RIS models as the upper bound.\nCross-domain Results. The robustness of a model to work with consistent performance across domains is crucial for real-world applications. In this context, we report the performances of both our pseudo-supervised and fully supervised ETRIS models trained on two different domains (e.g. RefCOCO variants and PhraseCut) in Table 3 and 4 for RefCOCO and PhraseCut datasets, respectively. Our pseudo-supervisions are generated from raw images in a train set of the image dataset. We observe that the performances of the supervised model trained with human-labeled data from PhraseCut or RefCOCO datasets drop significantly when evaluated on different domains. This performance degradation is due to domain gaps [78] caused by differences in textual styles and object categories handled in each dataset. In contrast, our pseudo-supervised model shows consistent performances across different domains, validating a level of robustness superior to that of the fully supervised model trained with manual annotations.\nOpen-world Results. To validate the generalization ability of our Pseudo-RIS model compared to the fully supervised one, we extend our zero-shot evaluation to an unseen domain, PhraseCut [91], as shown in Table 4. Since PhraseCut has a larger number of instance categories (i.e. 3,103 classes) than RefCOCO datasets (80 classes from MS-COCO [43]), we choose PhraseCut to validate the generalization ability in open-world setting. In Table 4, Unseen column denotes when evaluated on a subset of instance categories not included in RefCOCO datasets as in [107]. Note that the fully supervised model shows a significant drop in performance on the unseen subset, whereas our pseudo-supervised models are robust and even improve performance on this subset. These results highlight the superior generalization ability of our pseudo-supervised model in light of the open-world nature of RIS.\nResults on Semi-supervised Learning. Fig. 4 illustrates a comparison between our method and a supervised approach in a semi-supervised setting, both using the ETRIS model. In this comparison, our method is first trained with our pseudo supervisions and then fine-tuned with labeled data, while the supervised method is trained solely on proportions of human-labeled data (0.1%, 1%, 5%, 10%, and up to 100%). Remarkably, our approach achieves promising results even when just 0.1% labeled data. As the proportion of labeled data increases to 1%, 5% and 10%, we observe a consistent improvement in our performance, demonstrating the effectiveness of our method even with minimal labeled data. These results highlight the potential of our method, especially in real-world scenarios where obtaining large amounts of labeled data is expensive and challenging. Additionally, we provide a performance comparison with other semi-supervised RIS methods [58, 101, 109] in our supplementary materials."}, {"title": "5.3 Ablation Study", "content": "We conduct ablation studies on the val set of each RefCOCO dataset to validate the effectiveness of our method. We adopt ETRIS as our base model trained with our pseudo supervisions in all experiments.\nImpact of Proposed Components. In this ablation study, we analyze the impact of each component in our method, as illustrated in Table 5. The results show that the integration of our distinctive caption sampling into the na\u00efve method significantly improves the performances across all datasets in terms of mIoU scores. We also demonstrate that our distinctiveness-based text filtering improves mIoU scores. Furthermore, combining distinctiveness-based text filtering with distinctive caption sampling leads to further improvements in mIoU scores. These results indicate that our two strategies are more effective than the na\u00efve method in generating distinctive captions for each mask.\nEffects on GT segmentation masks. In Table 6, we show the impact of quality on segmentation masks using ETRIS as a base model. By using GT masks to generate our distinctive text supervisions, we achieve remarkable performances that are close to a fully supervised model. These results highlight the powerful ability of our method to generate distinctive expressions for RIS.\nAblation on threshold T. We conduct an ablation study on the threshold T, which is crucial for filtering out misleading captions through our distinctiveness metric. The mIoU scores under various threshold values are reported in our supplementary material. We choose T = 1.3 for all datasets.\nQualitative Analysis. We visualize the expressions generated by our method and those generated by the na\u00efve method in Fig. 5a, as well as compare ours with GT expressions in RefCOCO datasets in Fig. 5b. A na\u00efve method generates non-distinctive captions, which make it hard to distinguish the target object well from other objects. In contrast, our method produces distinctive captions describing the details of the target mask. Compared to GT expressions, ours provide more variety of visual concepts in the target mask, making the RIS models robust and general to different domains."}, {"title": "6 Conclusion", "content": "In this paper, we propose a novel framework, Pseudo-RIS, to generate pseudo-supervisions for RIS, where two techniques are introduced: (1) generating multiple candidates with distinctive words, and (2) filtering out the misleading captions, resulting in distinctive captions, like manual supervisions. Our approach outperforms SoTA weakly and zero-shot approaches, even fully supervised methods on an unseen domain. Furthermore, we show effectiveness of our method in a semi-supervised setting with a few labeled data."}, {"title": "L Social Impacts.", "content": "Since our method relies on foundation models that are trained on large-scale datasets, it also inherits their limitations, such as dataset bias and unfairness issues. Specifically, we utilize the image captioning model trained with non-curated web-crawled large-scale datasets, and it may generate captions that include discriminatory or biased languages present in the dataset."}]}