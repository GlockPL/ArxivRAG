{"title": "AccDiffusion v2: Towards More Accurate Higher-Resolution Diffusion Extrapolation", "authors": ["Zhihang Lin", "Mingbao Lin", "Wengyi Zhan", "Rongrong Ji"], "abstract": "Diffusion models suffer severe object repetition and local distortion when the inference resolution differs from its pre-trained resolution. We propose AccDiffusion v2, an accurate method for patch-wise higher-resolution diffusion extrapolation without training. Our in-depth analysis in this paper shows that using an identical text prompt for different patches leads to repetitive generation, while the absence of a prompt undermines image details. In response, our AccDiffusion v2 novelly decouples the vanilla image-content-aware prompt into a set of patch-content-aware prompts, each of which serves as a more precise description of a patch. Further analysis reveals that local distortion arises from inaccurate descriptions in prompts about the local structure of higher-resolution images. To address this issue, AccDiffusion v2, for the first time, introduces an auxiliary local structural information through ControlNet during higher-resolution diffusion extrapolation aiming to mitigate the local distortions. Finally, our analysis indicates that global semantic information is conducive to suppressing both repetitive generation and local distortion. Hence, our AccDiffusion v2 further proposes dilated sampling with window interaction for better global semantic information during higher-resolution diffusion extrapolation. We conduct extensive experiments, including both quantitative and qualitative comparisons, to demonstrate the efficacy of our AccDiffusion v2. The quantitative comparison shows that AccDiffusion v2 achieves state-of-the-art performance in image generation extrapolation without training. The qualitative comparison intuitively illustrates that AccDiffusion v2 effectively suppresses the issues of repetitive generation and local distortion in image generation extrapolation. Our code is available at https://github.com/lzhxmu/AccDiffusion_v2.", "sections": [{"title": "1 INTRODUCTION", "content": "The emergence of diffusion models has significantly advanced the generation field, thanks to techniques such as DDPM [1], DDIM [2], ADM [3], and LDMs [4]. These models are known for their outstanding generative abilities and diverse applications. However, these models perform well only at their pre-trained resolution. To generate higher-resolution images, we must train the model at that resolution. Nonetheless, stable diffusion (SD) models demand extensive high-quality datasets for training and entail tremendous training costs. For example, SD 1.5 trained with 512 \u00d7 512 resolution entails 150,000 A100 GPUs hours [5], while SD 2 trained with 768 \u00d7 768 resolution entails 200,000 A100 GPUs hours [6]. The training cost is even higher for SDXL [7] which is trained with 1024 \u00d7 1024 resolution. The extremely high training cost restricts current open-source SD models to a maximum training resolution of 1024 \u00d7 1024 [7]. However, higher-resolution generation finds numerous applications in advertising, gaming, and wallpaper design. On one hand, large high-resolution image datasets are scarce. On the other hand, the training cost increases quadratically with resolution. The above two factors make it infeasible and unaffordable to train ultra-high resolution generative models, such as 4K, directly. Therefore, exploring how to use pre-trained SD with relatively low resolution for generating ultra-high-resolution images is a valuable research topic for both industry and academia.\nRecently, there has been an explosive increase in research on image generation extrapolation, using either fine-tuning [12], [13] or training-free approaches [8]\u2013[11], [14]\u2013[22]. Previous methods explore image generation extrapolation from various perspectives: attention entropy [8], frequency-domain [18], feature map size [17], and the receptive field of U-Net [9]. However, these methods have shown practical limitations in two folds, as illustrated in"}, {"title": "2 RELATED WORK", "content": ""}, {"title": "2.1 Diffusion Models", "content": "Probabilistic generative models like DDPM [1], DDIM [2], and LDMs [4] are diffusion models that transform Gaussian noise into samples through iterative denoising. DDPM stands out for its impressive image generation ability, leveraging Markovian forward and reverse processes. DDIM further enhances DDPM by employing non-Markovian reverse processes, cutting down sampling time significantly. By integrating the diffusion process into latent space, LDMs achieve more efficient training and inference. Consequently, several open-source LDMs-based stable diffusion models have achieved state-of-the-art performance in image synthesis. This progress has led to widespread applications of diffusion models across various generative tasks, including image [1]\u2013[3], [25], [26], audio [27], [28], video [29], [30], and 3D object [31]\u2013[33], etc."}, {"title": "2.2 Training-Free Higher-Resolution Image Generation", "content": "While stable diffusion delivers remarkable results, the high training cost limits it to low resolutions, leading to low-quality images when the inference resolution differs from the training resolution [8], [9], [11]. Recent studies explore using pre-trained diffusion models to generate higher-resolution images. These approaches are two folds: image-wise generation [8], [9], [15]\u2013[18] and patch-wise generation [10], [11], [14], [19]\u2013[22].\nImage-wise generation methods either directly [8], [9], [15], [17] or gradually [16], [18] scale the input of diffusion models to the target resolution before applying forward and reverse processes on the latent space. These methods often require architectural modifications, such as adjusting the attention scale factor [8], the feature map size of U-Net [17], and the receptive field of convolutional kernels [9], to prevent repetitive generation. SelfCascade [16] and Diffuse-High [18] upsample the generated pre-trained resolution images and refine their details through forward and reverse processes. UG [15] employs a pre-trained diffusion model with an additional term called upsample guidance during sampling to create higher-resolution images. However, these methods often fail to achieve the desired high-resolution details and encounter out-of-memory errors when generating ultra-high resolution images (e.g., 8K) on consumer-grade GPUs due to the exponential increase in memory requirements as the latent space size increases.\nPatch-wise generation produces higher-resolution images through patch-wise denoising and can generate images of any resolution on consumer-grade GPUs. However, these methods [10], [14] struggle with object repetition and local distortion. Du et al. [11] and Tragakis et al. [21] attempt to reduce repetitive generation by incorporating global structural information from lower-resolution images. Haji-Ali et al. [22] separate high-resolution image generation into local and global signals to address distortion but only support up to 4\u00d7 higher resolution. Lin et al. [20] split the patch-wise denoising process into comprehensive structure denoising and specific detail refinement to tackle the local repetition issue. Kim et al. [19] use a staged and hierarchical approach for human-centric scenes."}, {"title": "3 BACKGROUNDS", "content": ""}, {"title": "Latent Diffusion Models (LDMs)", "content": "LDMs perform the diffusion process in latent space. For an image $x_0 \\in \\mathbb{R}^{H \\times W \\times 3}$, an autoencoder $E$ encodes it into latent space as:\n$z_0 = E(x_0),$\nwhere $z_0 \\in \\mathbb{R}^{h \\times w \\times c}$ is the latent representation of an image. Then the diffusion process of LDMs can be formulated as:\n$z_t = \\sqrt{\\bar{\\alpha}_t}z_0 + \\sqrt{1-\\bar{\\alpha}_t} \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I),$\nwhere ${\\lbrace \\alpha_t \\rbrace}_{t=1}^{T}$ is a set of prescribed variance schedules and $\\bar{\\alpha}_t = \\Pi_{i=1}^{t} \\alpha_i$. Then a network $\\epsilon_{\\theta}$ is trained to perform conditional sequential denoising by predicting added noise, with the training objective defined as follows:\n$\\min_{\\epsilon_{\\theta}} \\mathbb{E}_{z_0,\\epsilon \\sim \\mathcal{N}(0,I),t} [ ||\\epsilon - \\epsilon_{\\theta}(z_t, t, \\tau_{\\theta}(y))||_2^2],$\nin which $t \\sim \\text{Uniform}(1,T)$, $\\tau_{\\theta}(y) \\in \\mathbb{R}^{M \\times d_+}$ is an intermediate representation of condition $y$ and $M$ is the number of word tokens in the prompt $y$. In the cross-attention of U-Net, $\\tau_{\\theta}(y)$ is subsequently mapped to keys and values as:\n$Q = W_Q(z_t), K = W_K \\cdot \\tau_{\\theta}(y), V = W_V \\cdot \\tau_{\\theta}(y),$\n$M = \\text{Softmax}(\\frac{QK^T}{\\sqrt{d}}), \\text{Attention}(Q, K, V) = M \\cdot V.$\nHere $(z_t) \\in \\mathbb{R}^{N \\times d_e}$ represents an intermediate noise representation within the U-Net. And $N=h \\times w$ denotes the pixel number of the latent noise $z_t$. The matrices $W_Q \\in \\mathbb{R}^{d_+\\times d_e}$, $W_K \\in \\mathbb{R}^{d_+\\times d}$, and $W_V \\in \\mathbb{R}^{d_+\\times d}$ are learnable projections, while $M \\in \\mathbb{R}^{N \\times M}$ is the cross-attention maps. Without loss of generality, we omit the expression of multi-head cross-attention for conciseness.\nDuring denoising process, diffusion model estimates the noise in $z_t$ and recovers the cleaner version $z_{t-1}$ through:\n$z_{t-1} = \\alpha \\cdot z_t + \\beta \\cdot \\epsilon_{\\theta}(z_t, t, \\tau_{\\theta}(y)),$\n$\\alpha = \\sqrt{\\frac{a_{t-1}}{A_t}},$\n$\\beta = \\sqrt{1-\\frac{1}{A_{t-1}}},$\nBy iteratively denoising through Eq. (5), a noise-free latent $z_0$ is decoded to image $x_0$ through decoder $D(\\cdot)$ as:\n$x_0 = D(z_0).$\nControlNet. For controllable image generation, ControlNet [23] adds an additional condition encoder on pre-trained diffusion models. The denoising process of ControlNet can be represented as follows:\n$z_{t-1} = \\hat{\\alpha} \\cdot z_t + \\beta \\cdot \\epsilon_{\\theta_1}(z_t, t, \\tau_{\\theta}(y), q).$\nHere, $\\epsilon_{\\theta_1}$ represents ControlNet and $q$ denotes extra conditions, such as canny edges [34], human poses [35], depth maps [36]. Note that the introduced ControlNet is a plug-and-play extension without altering the parameters of pre-trained diffusion models.\nPatch-wise Denoising. MultiDiffusion [10] first uses a shift window to sample overlapped patches and then fuses"}, {"title": "4 ACCDIFFUSION V2", "content": "This section formally introduces AccDiffusion v2, a plug-and-play extension for diffusion models that enables accurate higher-resolution image generation. Similar to recent works [10], [11], [18], AccDiffusion v2 adapts a progressive recipe to conduct image generation extrapolation in a patch-wise fashion, which can generate ultra-high resolution images on one consumer-grade GPU. The framework of AccDiffusion v2 is illustrated in Fig. 3. The major differences between AccDiffusion v2 and recent methods are three folds: (1) AccDiffusion v2 uses patch-content-aware prompts for each patch to conduct accurate higher-resolution image generation, while recent works [11], [18] use image-content-aware prompt for all patches. (2) AccDiffusion v2 innovatively integrates ControlNet [23] during the patch-wise denoising to alleviate local distortion. (3) AccDiffusion v2 uses dilated sampling with interaction to generate accurate global semantic information, while recent methods [11] independently denoise dilation samples without interaction."}, {"title": "4.1 Patch-Content-Aware Prompts", "content": "While DemoFusion showcases the potential of leveraging pre-trained LDMs to generate higher-resolution images, the persistent issue of small object repetition poses a challenge to its performance, as depicted in Fig. 1(d). To pinpoint the cause of this repetition, we design two ablation experiments. In the first one, we exclude the text prompt during higher-resolution generation of DemoFusion. The result in Fig. 4(a) shows that the removal of prompts completely eliminates repetitive objects but results in a noticeable loss of detail; In the second one, we exclude the operations of residual connection & dilated sampling in DemoFusion. The result in Fig. 4(b) suffers severe large object repetition. From these results, it is reasonable to conclude that small object repetition arises as an adverse effect from using the same text prompt across all patches, as well as from residual connection and dilated sampling operations. While the former promotes object repetition, the latter diminishes it. As a result, De-moFusion tends to generate small repetitive objects."}, {"title": "4.2 More Accurate Generation of Local Content", "content": "Patch-content-aware prompts effectively suppress the repetitive generation in higher-resolution diffusion extrapolation [24]. Despite this improvement, local distortion persists in the results, as illustrated in Fig. 6(a). Drawing parallels to the analysis in Sec 4.1, we speculate that the patch-content-aware prompts are not enough to accurately describe the content of the patches. In Fig. 6(a), we use the patch corresponding to the astronaut's hand to give an in-depth analysis. This patch tends to generate a complete structure (astronaut) conditioned by the word \"astronaut\" in the prompt, but global semantic information tends to generate local structures (hand). Consequently, the clash between the two leads to a local distortion. A simplistic remedy would involve excluding the inaccurate prompt during higher-resolution diffusion extrapolation. However, we have demonstrated in Sec.4.1 that prompts significantly contribute to the details of results, playing a crucial role in image generation. Therefore, the challenge of local distortion must be approached from another perspective while retaining the prompt.\nAs the structure of images in pre-trained resolution is rational, the structure of relatively low-resolution images can serve as a reference during higher-resolution diffusion extrapolation. First, the denoised latent $z_0 \\in \\mathbb{R}^{h \\times w \\times c}$ is decoded to low-resolution image $I = D(z_0) \\in \\mathbb{R}^{H \\times W \\times 3}$. Next, the image $I$ is interpolated to higher resolution $I' \\in \\mathbb{R}^{H' \\times W' \\times 3}$ with $H' > H$ and $W' > W$. Subsequently, the canny edge detector [34] is used to detect the edges $C \\in \\mathbb{R}^{H' \\times W' \\times 3}$ in images $I'$. Similar to Eq. (8), we use a shifted window to sample patches from $C$, resulting in a series of patches ${\\{C^i\\}}_{i=1}^{P_1}$, where $C^i \\in \\mathbb{R}^{H \\times W \\times 3}$ and $P_1$ is the total number of patches. So far, each patch $z^i$ has a corresponding prompt $\\gamma^i$ and local structure information $C^i$. By integrating the ControlNet [23] $\\epsilon_{\\theta_1}$, the denoising process of patch $z^i$ can be expressed as:\n$z_{t-1} = \\hat{\\alpha} \\cdot z_t + \\beta \\cdot \\epsilon_{\\theta_1}(z_t, t, \\tau_{\\theta}(\\gamma^i), C^i).$"}, {"title": "4.3 Dilated Sampling with Window Interaction", "content": "Both our analysis in Sec. 4.1 and recent works [11], [22] show that global semantic information effectively suppresses object repetition. Dilated sampling is a feasible way to inject global semantic information during higher-resolution extrapolation [11]. Given a higher-resolution latent representation $Z_t \\in \\mathbb{R}^{h' \\times w' \\times c}$, a set of patch samples ${\\{D^k\\}}_{k=1}^{P_2}$ are dilated sampled as:\n$D^k = (Z_t)_{i::h_s, j::w_s, :},$\nwhere k is defined as $k = i \\times w_s + j + 1$, ranging from 1 to $P_2$. The indices $i$ and $j$ vary from 0 to $h_s - 1$ and $w_s - 1$, respectively. The sampling stride is calculated as $h_s = \\frac{h'}{h}$ and $w_s = \\frac{w'}{w}$, with ${\\{h', w'\\}}$ and ${\\{h, w\\}}$ representing the height and width of higher and low resolution latent representation. DemoFusion performs denoising on $D^k$ independently via Eq. (5) to obtain $D_{t-1} \\in \\mathbb{R}^{P_2 \\times h \\times w \\times c}$. Next, the denoised outputs ${\\{D_{t-1}^k\\}}_{k=1}^{P_2}$ are combined to reconstruct $G_{t-1} \\in \\mathbb{R}^{h' \\times w' \\times c}$, which are added to patch-wise denoised latent representation $Z_{t-1}$ as:\n$Z_{t-1} = (1 - \\eta) \\cdot Z_{t-1} + \\eta \\cdot G_{t-1},$\nwhere $(G_{t-1})_{i::h_s, j::w_s, :} = D_{t-1}^k$ and $\\eta$ decreases from 1 to 0 following a cosine schedule. As shown in Fig. 4(c), we find that the global semantic information is non-smooth, due to the lack of interaction among different samples. To solve this issue, as illustrated in Fig. 7, we enable window interaction among different samples prior to each denoising process through a bijective function:\n$D_{t,h,w}^k = D_{t,h,w}^{f_{h,w}(k)},$\nwhere $f_{h,w}$ is a bijective function, with the mapping varying on the specific position or time step. We then perform standard denoising progress on ${\\{D\\}}_{k=1}^{P_2}$ to obtain ${\\{D_{t-1}^k\\}}_{k=1}^{P_2}$. Before applying Eq. (15) to ${\\{D_{t-1}^k\\}}_{k=1}^{P_2}$, we recover the position by using the inverse mapping $(f_{h,w})^{-1}$ of $f_{h,w}$ as:\n$D_{t-1,h,w}^k = D_{t-1,h,w}^{(f_{h,w})^{-1}(k)},$\nwhich yields more smooth global semantics like Fig. 4(d)."}, {"title": "5 EXPERIMENTATION", "content": ""}, {"title": "5.1 Experimental Setup", "content": "Since AccDiffusion v2 has not been fine-tuned on any higher-resolution image dataset, we select only training-free comparison methods, including: SDXL-DI [7], Attn-SF [8], ScaleCrafter [9], MultiDiffusion [10], HiDiffusion [17], DiffuseHigh [18], DemoFusion [11], and AccDiffusion [24]. Although both image super-resolution and diffusion extrapolation aim to generate high-resolution images, they differ in that one uses images as input and the other uses text. Thus, we did not compare AccDiffusion v2 with super-resolution methods. Previous works have shown that super-resolution generates inferior details than diffusion extrapolation methods [9], [11]. To verify the effectiveness of AccDiffusion v2, we select the widely used SDXL [7] for quantitative and qualitative comparisons. For quantitative comparison, we set the hyperparameter c to 0.3. The ControlNet checkpoint used is available on Hugging Face at https://huggingface.co/xinsir/controlnet-canny-sdxl-1.0."}, {"title": "5.2 Quantitative Comparison", "content": "We employ three widely-used metrics: Frechet Inception Distance (FID) [40], Inception Score (IS) [41], and CLIP Score [42] for quantitative evaluations. Specifically, FID assesses the Frechet Inception Distance between generated high-resolution images and real images, while IS calculates the Inception Score for these generated high-resolution images. Notably, both FID and IS require resizing images to 2992 resolutions, which may not provide optimal assessments for high-resolution images. To address this, inspired by methods [11], [43], we crop 10 local patches at native resolution (1x) from each generated high-resolution image before resizing, yielding FID and ISc. The CLIP Score is calculated based on the cosine similarity between image embeddings and text prompts, providing an additional alignment metric. For quantitative comparison, we randomly selected 10,000 images from the Laion-5B dataset [44] as the real image set and used 1,000 randomly chosen text prompts from Laion-5B as input for AccDiffusion v2, generating a corresponding set of high-resolution images.\nAccDiffusion v2 achieves state-of-the-art performance in diffusion extrapolation tasks, as shown in Table 1. More accurate patch-content-aware prompts, enhanced accuracy in local content generation, and improved integration of global structure information enabled by dilated sampling with interaction contribute to the improvements. These improvements are especially effective for high-resolution image generation (16\u00d7). In comparison with other training-free image generation extrapolation methods, AccDiffusion v2 produces quantitative results that more closely align with those at pre-trained resolutions, underscoring its robust extrapolation capabilities in generating high-quality images beyond pre-trained resolutions. The inference time of AccDiffusion v2 is slightly higher than that of AccDiffusion due to the additional cost of suppressing local distortion through ControlNet [23]. Note that FID, IS, and CLIP-Score may not directly indicate the presence of repetitive generation or local distortion in the generated images. Therefore, we perform a qualitative comparison in next section to confirm the efficacy of AccDiffusion v2 in reducing such artifacts."}, {"title": "5.3 Qualitative Comparison", "content": "Fig. 8 shows a comparison between AccDiffusion v2 and other training-free text-to-image generation extrapolation methods, including Attn-sf [8], ScaleCrafter [9], DiffuseHigh [18], HiDiffusion [17], MultiDiffusion [10], DemoFusion [11], and AccDiffusion [24]. As the resolution increases, Attn-SF suffers from severe structural distortion and a significant decline in visual quality. ScaleCrafter avoids object repetition but experiences detail degradation at 3072 \u00d7 3072 resolution and structural distortions at 4096 \u00d7 4096 resolution, as highlighted in the red box. DiffuseHigh can generate high-fidelity images at 2048 \u00d7 2048 and 3072\u00d73072 resolutions, but it still suffers from local distortion at the higher resolution of 4096 \u00d7 4096, also highlighted in the red box. Though HiDiffusion is an efficient image generation extrapolation method but suffers from severe object repetition and local distortion at high resolutions, such as 3072 \u00d7 3072 and 4096 \u00d7 4096. MultiDiffusion can generate seamless images but also suffers from significant repetitive and distorted generation. DemoFusion tends to generate small repetitive objects, like the small wolf at 3072 \u00d7 3072 and small cats and dogs at 4096 \u00d7 4096, with the frequency of repetition escalating with image resolution. It also suffers local distortion, such as the tail of the cat at 4096 \u00d7 4096, both of which significantly degrade image quality. AccDiffusion demonstrates superior performance in generating high-resolution images without such repetitions. However, it still suffers from local distortion in the foreground, such as the eye on the leg of the wolf and the strange shape of the cat's tail. In contrast, AccDiffusion v2 can conduct more accurate higher-resolution extrapolation without repetitions or local distortion, leading to high-quality results. We provide"}, {"title": "5.4 More Stable Diffusion Variants", "content": "AccDiffusion v2 is a plug-and-play framework that can be easily used to conduct higher-resolution diffusion extrapolation for different diffusion models. Thus, we implement AccDiffusion v2 for other latent diffusion models (LDMs), specifically Stable Diffusion 1.5 (SD 1.5) [5] and Stable Diffusion 2.1 [6] (SD 2.1). As demonstrated in Fig. 11, AccDiffusion v2 effectively generates high-resolution images without noticeable repetition or localized distortion. However, it's crucial to consider that AccDiffusion v2's results are influenced by the foundational quality of the LDMs used. Consequently, the visual fidelity of outputs with SD 1.5 and SD 2.1 is lower than those generated with the more advanced SDXL [7]."}, {"title": "5.5 Ablation Study", "content": "This section begins with ablation studies on the three core modules introduced in this paper, followed by a discussion on the threshold settings for the binary mask in Eq. (9) and the patch-content-aware prompt threshold c in Eq. (12). All experiments use a resolution of 40962 (16\u00d7). Since current quantitative metrics cannot intuitively reflect the extent of object repetition or local distortion, we provide visualizations to show how our core modules effectively prevent repetitive generation and local distortion."}, {"title": "5.5.1 Ablations on Core Modules.", "content": "Fig. 10 illustrates that removing any module reduces generation quality. Excluding patch-content-aware prompts leads to numerous small, repetitive object repetitions, emphasizing the role of patch-content-aware prompts in preventing repetitive generation. When dilated sampling with window interaction is removed, small objects in the image appear"}, {"title": "5.5.2 Ablations on Hyper-Parameters.", "content": "Table 2 illustrates a significant variation in the range of different cross-attention maps Mj. Two potential scenarios arise when a fixed threshold is applied to these maps. In the first case, if the threshold is set too high, some words may lack highly responsive regions in their corresponding attention maps, leading to their exclusion from the patch-content-aware prompt. In the second case, if the threshold is set too low, the entire attention map may consist of highly responsive regions, resulting in those words being included in the patch-content-aware prompt all the time. By taking into account the average M:,j, we can ensure that each word is associated with appropriate highly responsive regions, as shown in Fig. 5(b).\nReferencing Eq. (12), the parameter c dictates whether the percentage of a highly responsive region for a word \\gamma_j exceeds the threshold necessary for incorporation into the prompts of patch z\u012f. When c is set to a very small"}, {"title": "6 LIMITATIONS AND FUTURE WORK", "content": "While providing valuable insights, AccDiffusion v2 is not without its shortcomings: Firstly, the inference latency of AccDiffusion v2 is high as shown in Table 1, akin to other patch-wise extrapolation methods [11], [24], due to inefficient progressive upscaling and overlapped patch-wise denoising. Additionally, the use of ControlNet to suppress local distortion further adds to this delay. However, users can choose the method that best fits their needs with this trade-off between performance and inference latency. Secondly, the fidelity of extrapolation results heavily relies on"}, {"title": "7 CONCLUSION", "content": "This paper proposes AccDiffusion v2, a plug-and-play module, that enables higher-resolution diffusion extrapolation without repetitive generation or local distortion. To improve patch-wise denoising accuracy, AccDiffusion v2 introduces patch-content-aware prompts, effectively addressing the issue of repetitive generation from the root. Additionally, to mitigate local distortion, AccDiffusion v2 integrates more precise local structural information through ControlNet during the higher-resolution diffusion extrapolation. Moreover, we propose dilated sampling with window interaction to improve global consistency while generating high-resolution images. Comprehensive experiments demonstrate that AccDiffusion v2 achieves state-of-the-art performance, successfully generating higher-resolution images without object repetition or local distortions."}]}