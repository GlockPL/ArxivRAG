{"title": "Universal Pooling Method of Multi-layer Features from Pretrained Models for Speaker Verification", "authors": ["Jin Sob Kim", "Hyun Joon Park", "Wooseok Shin", "Sung Won Han"], "abstract": "Recent advancements in automatic speaker verification (ASV) studies have been\nachieved by leveraging large-scale pretrained networks. In this study, we analyze\nthe approaches toward such a paradigm and underline the significance of interlayer\ninformation processing as a result. Accordingly, we present a novel approach for\nexploiting the multilayered nature of pretrained models for ASV, which comprises a\nlayer/frame-level network and two steps of pooling architectures for each layer and\nframe axis. Specifically, we let convolutional architecture directly processes a stack\nof layer outputs. Then, we present a channel attention-based scheme of gauging\nlayer significance and squeeze the layer level with the most representative value.\nFinally, attentive statistics over frame-level representations yield a single vector\nspeaker embedding. Comparative experiments are designed using versatile data\nenvironments and diverse pretraining models to validate the proposed approach.\nThe experimental results demonstrate the stability of the approach using multi-layer\noutputs in leveraging pretrained architectures. Then, we verify the superiority of the\nproposed ASV backend structure, which involves layer-wise operations, in terms of\nperformance improvement along with cost efficiency compared to the conventional\nmethod. The ablation study shows how the proposed interlayer processing aids in\nmaximizing the advantage of utilizing pretrained models.", "sections": [{"title": "1 Introduction", "content": "Automatic speaker verification (ASV) technology has undergone a dramatic evolution in recent years.\nBehind such trends, advancements in deep neural networks (DNNs) support this background. With\nthe quantitative improvement of data resources, supervised learning systems based on DNNs have\nrapidly replaced conventional statistical techniques in ASV. Past the transitional form in which the\ntwo techniques were combined, the entirely DNN-based format has been settled in the ASV field\nwith learning from speaker classification given a single utterance [1]. Based on this framework,\nmany studies have proposed advanced ASV systems with sophisticated network architectures [2-7],\nimproved pooling techniques [7\u20139], and well-fitting loss objectives [10-13].\n\nMeanwhile, the pretraining paradigm, which is attested to in natural language processing [14, 15],\nhas exhibited significant success in automatic speech recognition [16]. Diverse speech pretraining\nmethods have been introduced [17-19]. Accordingly, attempts have been made to establish an ASV\nsystem that leverages the domain-general representations learned from large-scale pretraining. The\nfine-tuning of Wav2vec 2.0 [17], which is a representative self-supervised model, has been discussed\nfor transition to ASV systems [20,21]. Some researchers have developed an external backend module\nto transform the final layer output of Wav2vec 2.0 into a speaker-discriminative feature [22, 23].\nIntroduced as a broad-task evaluation benchmark for speech-pretrained models, SUPERB [24] has\ninspired several authors to incorporate layer-wise outputs through weighted summation. Based on"}, {"title": "2 Related Works", "content": "2.1 Statistical Modeling Approaches toward ASV\n\nGaussian mixture models (GMMs) were widely used in early speaker recognition systems. First\nmotivated by [28-30] for text-independent speaker identification, a GMM was trained to maximize\nthe likelihood of the given feature vectors from speaker utterances. Subsequently, [31] introduced\nspeaker-independent GMM training, referred to as the universal background model (UBM), and\nemployed Bayesian adaptation for speakers. The GMM-UBM has become a fundamental element in\nsubsequent speaker recognition studies [32-36].\n\nLater studies have explored the improvement of GMM supervectors through dimensionality reduction\nand refined speaker information. The eigenvoice coefficients were estimated to project the GMM\nsupervectors onto a reduced subspace [33]. The separation of the speaker factor and session variability\nwas discussed in [35] through joint factor analysis and eigenchannel modeling. Furthermore, the\ni-vector framework [36] utilizes factor analysis to extract the combined variability space of training\nutterances. In combination with a probabilistic linear discriminant analysis [37] scoring backend,\nthis framework has become a fundamental basis for text-independent speaker verification (SV)\nsystems [38-40].\n\n2.2 Deep Learning-based ASV Systems\n\nWith the widespread success of DNNs, researchers have explored their usage in ASV systems. Initial\nefforts were made to incorporate DNNs into the i-vector system [41-43]. As one of the earliest\nmethods, d-vector [44] proposed DNN-based speaker feature extraction, which is trained to classify\nspeakers from frame-level features. The embeddings [45] and x-vector [1] introduced a primary\nstructure of text-independent ASV using DNN, consisting of a backbone network to process the\nframe-level features, a statistical pooling layer, and a segment (utterance)-level process.\n\nThe disclosure of the large-scale speaker recognition dataset VoxCelebs [2, 3] has further promoted\nthe development of DNN-based SV systems. The baseline systems of VoxCelebs were built using the\nVGG-M [46] or ResNet [47] architectures. Around the same time, Inception-ResNet-v1 [48] was\ntrained as an end-to-end SV model [10, 11]. In addition to exploiting ready-made networks, various\nproposals have included sophisticated structures, improved from the vanilla x-vector, which is a stack\nof time-delay neural networks (TDNNs) [49]. E-TDNN [4] extended the original form in depth and\ncontext, and D-TDNN [5] considered the dense connectivity [50] of TDNN layers. ARET [6] adopts a\nsplit-transform-merge strategy and the residual transformation introduced in ResNeXt [51]. Similarly,\nECAPA-TDNN [7] leverages the concept of Res2Net [52] followed by a Squeeze-and-Excitation\n(SE) [53] block to build a frame-level neural network."}, {"title": "2.3 Leveraging Pretrained Models for Speaker Embedding", "content": "Self-supervised learning has become a broad paradigm in several data-driven research domains [14,\n15,55-58]. Particularly in speech processing, the pretraining of Transformer-based [59] architectures\nis in the spotlight, as domain-general representations can replace manual acoustic features [16\u201319,60].\nAccordingly, various studies have exploited pretrained models in SV, and the methods can be broadly\ndivided into two categories.\n\n2.3.1 Self-fitting Approaches\n\nThis approach retrains the pretrained model itself to output speaker embeddings directly and perform\nASV without additional parameters. SincNet [61] is first trained in an unsupervised manner and\nthen fine-tuned for speaker recognition tasks [56]. Both [20] and [21] involve fine-tuning Wav2vec\n2.0 [17] as an ASV system, but differ in the strategy for obtaining speaker representations. One\nuses average pooling over the Transformer layers output [20], and the other inserts a constant cls\ntoken [14] into the latent representation sequence before the Transformer layers [21].\n\n2.3.2 Feature Extraction Approaches\n\nAn additional module is trained to transform the output of the pretrained model into a speaker\nrepresentation. [22] and [23] developed backend models for extracting speaker embeddings that were\nattached on top of pretrained Wav2vec 2.0. Other studies have used the concept of SUPERB [24],\nsetting learnable parameters for the weighted summation of the latent representations from each layer.\nOff-the-shelf ASV models, such as X-vector and ECAPA-TDNN, have been adopted to process the\nweighted sum of feature maps [19,24,25]. Multi-head factorized attention pooling was proposed\nbased on this idea [26, 27]."}, {"title": "3 Methodology", "content": "3.1 Preliminaries and Background\n\nAs noted in Section 2.3, leveraging pretrained models has already shown promising results in a\nbroad range of areas, including ASV. Better performance and training efficiency in downstream\ntasks are common advantages that can be expected from this paradigm. Recent ASV studies have\nexploited several pretrained models; however, few have attempted to comprehend how to achieve\nsuch advantages. This study aimed to maximize the benefits of using pretrained models for ASV.\n\nTo offer insights, we discuss the potential of large-scale self-trained representations of ASV. Motivated\nby the feasibility analysis in [20], zero-shot evaluations were conducted on benchmark datasets in\nvarying auditorial environments. Figure 1 shows the equal error rates (EERs) of the evaluations subject\nto layer-wise outputs from the renowned speech-processing models Wav2vec 2.0 [17], HuBERT [18],\nand WavLM [19]. Speaker embeddings were pooled on the mean over the frame axis and the cosine\ndistance was used to measure the similarity between two audio samples. Further experimental details\nare presented in Section 4.\n\nAccording to the results, the layer outputs differed in terms of potential and created distinctive\ndistributions in each model, dependent on the pretraining schemes. In addition, given the individual\nlayer outputs, no other model apart from Wav2vec 2.0 could significantly benefit from pretraining,\nexcept when using the VCTK [62] setup. This explains why few pretrained models have been explored\nin self-fitting approaches. Consequently, when the method is focused on a particular model, it likely\nfails on other models, although it could be commonly applied to Transformer-based structures.\n\nIn contrast, SUPERB [24] has inspired many researchers to reap the benefits of several pretrained\nmodels with a weighted summation of layer-wise outputs [19,25\u201327]. Along with this discovery, we\ncould presume that a sole layer is limited in capturing speaker information and speaker-distinctive\nfeatures are hidden over multiple layers. SUPERB is intended to verify pretraining proposals in broad\nspeech-processing tasks. Hence, the linear combination of layer outputs may not be the optimal\nmethod for extracting speaker representations, thus leaving room for improvement.\n\n3.2 Pooling Speaker Information from Pretrained Model\n\nWe propose a backend module to extract speaker features from pretrained models. As shown in\nFigure 2, the module encodes a multidimensional feature map into a speaker-representative vector\nin three steps. Given the waveform input, we start with a stack of layer-wise outputs $x \\in \\mathbb{R}^{C \\times L \\times T}$\nprocessed from the pretrained model, where C, L, and T denote the hidden dimension, number of\nlayers, and frames, respectively. An initial point-wise convolution (1\u00d71 Conv) adjusts the hidden\nsize to C = 512.\n\n3.2.1 Layer- and Frame-level Processing Network\n\nAs opposed to compressing the layer level instantly, we allow the convolutional neural network\n(CNN) to directly process a stack of layer-wise outputs from the pretrained network. Hence, it creates\nmore representative features for the speaker from adjacent relationships of layer- and frame-wise\ninformation. With some modifications, the D3Net [63] architecture is adopted to capture speaker\nfeatures from diverse receptive areas, based on the dense connections of the multidilated convolutions"}, {"title": "3.2.2 Layer Attention Pooling", "content": "When the preceding network captures speaker representations processed from varied perspectives,\nthe following step involves aggregating and pooling their portrayal as a single vector. We begin\nby pooling the most representative features at the layer level. Although it differs in fields, the\nmultiheaded channel attention scheme (MCA) [64] has proven effective in aggregating multilayered\nfeatures. We apply this method to pool the features over the layer dimensions, as shown in Figure 4.\nThe attention module learns to determine the significance of each layer depending on the layer-wise\nstatistics, allowing elasticity of gauging interlayer relationships with the expansion of heads.\n\nGiven an output $x \\in \\mathbb{R}^{C \\times L \\times T}$ from the preceding dense architecture, point-wise convolution\nproduces a multi-headed representation $x \\in \\mathbb{R}^{(h \\cdot d) \\times L \\times T}$, where we set $h = 4$ and $d = 128."}, {"title": "3.2.3 Attentive Statistic Pooling", "content": "The self-attention mechanism has proven successful in aggregating speaker embeddings from a\nsequence of frame-level features [7\u20139]. We exploit this approach in the same manner as in ECAPA-\nTDNN [7], which expands the concept of the weighted mean and standard deviation for channel-\ndependent statistics. Given the input $x \\in \\mathbb{R}^{C \\times T}$, we use the attention-weighted statistics $\\tilde{\\mu}, \\tilde{\\sigma} \\in \\mathbb{R}^{C}$,\nand the linear transformation $W\\in \\mathbb{R}^{2C\\times R}$ and batch normalization follow, where C = 512 and\nR = 192. Finally, AAM-softmax [13] with a scale of 30 and a margin of 0.2 is used to train\nthe system; otherwise, we use the cosine distance to measure the similarity between two speaker\nembeddings."}, {"title": "4 Experimental Setup", "content": "4.1 Datasets\n\nMultiple datasets were employed to compose various model training scenarios and evaluate the ASV\nsystems for each setup. We explain the configuration of each dataset and its subsets in detail. Table 1\nsummarizes the overall information, where the scale of the speaker pool (low- and high-resource)\nand the auditorial environment (clean and natural) specify the divisions. The audio sources were\npreprocessed with 16 kHz resampling and silence trimming at the beginning and end.\n\n4.1.1 VCTK CSTR Corpus [62]\n\nThis dataset consists of high-quality audio recordings from 108 native English speakers of varying ac-\ncents, ages, and regions. Each speaker reads approximately 400 sentences, with minimal background\nnoise in the controlled environment. VCTK Corpus has a low-resource setup compared to the others.\nMeanwhile, we removed the samples in which the same transcripts were used across speakers (nos.\n000-024) with the aim of text-independent speaker verification.\n\nThe speaker pool was segmented to split the dataset into training, validation, and test sets. A stratified\nsplit was used for gender, age, and accents, whereas binning was applied to age. This demographic\ninformation was also considered when selecting negative-paired trials to prevent evaluation bias.\nWhile ensuring that the trial set included every combination of speakers, we balanced the number of\nnegative pairs based on whether they had the same gender or accent.\n\n4.1.2 LibriSpeech [65]\n\nThis dataset comprises approximately 1,000 h of speech recorded by more than 2K speakers, repre-\nsenting a high-resource configuration. It is segmented into distinct partitions for training, validation\n(dev), and testing. The recordings are generally of high quality, providing clear speech with transcripts.\nLibriSpeech has been adopted in many previous studies addressing various speech processing tasks\nowing to its extensive size and diverse range of speakers. The trials were composed in the same\nmanner as in VCTK Corpus, using speaker gender information, for each validation and testing speaker\npool.\n\n4.1.3 VoxCeleb 1 and 2 [2,3]\n\nBoth datasets serve as benchmarks for evaluating speaker recognition tasks by ensuring a variety\nof acoustic environments and background noise. Each comprises training (dev) and testing splits\ndistinguished by speakers. The first offers three verification trials for the ASV task, and we used\nthe speaker-distinguished version (also known as \u201c-O\u201d) in this study. The second dataset provides\nthe most extensive data for training or testing without any overlapping individuals from the first one.\nHowever, no evaluation protocol is provided for the second dataset; therefore, the datasets are often\nmerged, extending the training data.\n\nTo diverse experimental environments for this study, the two datasets were separated into low- and\nhigh-resource setups. We built a custom evaluation set from the VoxCeleb2 test speakers considering\na balanced label distribution and speaker combination. We then cross-used the evaluation sets of\nVoxCeleb1 and 2 for validation.\n\n4.2 Evaluation Metrics\n\nThe EER and minimum detection cost function (minDCF) are typical measures for evaluating ASV\nsystems [66]. The SpeechBrain toolkit 2 was used for the implementation. These metrics consider\nthe posterior assessment of the evaluation data. In particular, the model first produces the similarity\nscores of the trial pairs, following which each metric derives a decision threshold to accept or reject\nthe claimed authority. Therefore, we devised an additional measure (EER*) to maintain the test data\nunseen and evaluate the ASV systems more practically. All measures reported the test set evaluation\nscore when the best EER performance was achieved on the validation split. The details of each metric\nare described below.\n\nEER\n\n$N = \\{i | y_i = 0: i \\in \\mathbb{Z}\\}, P = \\{i | y_i = 1: i \\in \\mathbb{Z}\\}$\n$FN = \\{i | \\hat{y}_i \\geq \\tau: i \\in N\\}, FP = \\{i | \\hat{y}_i < \\tau: i \\in P\\}$\n$FAR(\\tau) = \\frac{FN}{|N|}$\n$FRR(\\tau) = \\frac{FP}{|P|}$\n\nGiven the evaluation dataset (Z) comprising binary authentication labels $y_{\\forall i} \\in \\{0, 1\\}$, the EER is\na representative measure of biometric system evaluation. Two different error rates are considered,\ndepending on the label. The false acceptance rate (FAR) is the ratio at which the system predicts a\nhigher similarity score than the threshold $(\\hat{y}_i \\geq \\tau)$ that should actually be rejected (N). In contrast,\nthe false rejection rate (FRR) is the ratio at which the system rejects trials $(\\hat{y}_i < \\tau)$ among those that\nshould be accepted (P). The FAR and FRR exhibit a tradeoff relationship according to $\\tau$, and the\nEER occurs when they result in the same value.\n\nEER*\n\n$EER^* = \\frac{FAR_Z(\\tau) + FRR_Z(\\tau)}{2}$\n$s.t. EER_U = FAR_U(\\tau) = FRR_U(\\tau)$\n\nWe conducted the test data evaluation (Z) using the threshold ($\\tau$) obtained from the EER measurement,\ngiven the validation dataset (U). Under the given threshold condition, the FAR and FRR can differ\nsuch that EER* averages the two error rates. The error rates are denoted as FAR* and FRR* in Figure\n5, respectively.\n\nminDCF\n\n$\\min_{\\tau} C_{Det} = C_{Miss} P_{Miss} (\\tau) \\cdot P_{Target} + C_{FA} P_{FA}(\\tau) \\cdot (1 - P_{Target})$\n\nThe minDCF is an evaluation metric employed in the speaker recognition evaluation conducted by\nthe US National Institute of Standards and Technology. This metric assigns different weights to each\ntype of error with the parameters $C_{Miss}, C_{FA}$, and $P_{Target}$. The probability $P_{Miss}$ corresponds to the\naforementioned FRR, whereas $P_{FA}$ represents the FAR. The detection cost ($C_{Det}$) is calculated as\ndefined in (4), and the minDCF metric captures the minimum value over a range of thresholds ($\\tau$). In\nthis study, $C_{Miss}, C_{FA} = 1$, and $P_{Target} = 0.01$ were used [7].\n\n4.3 Comparison Entries and Pretrained Models\n\nDiverse comparison models were employed to verify the proposed method. X-vector [1] and ECAPA-\nTDNN [7] are representative ASV neural networks that use the Mel-frequency cepstral coefficients\n(MFCCs) as input. The X-vector uses the most basic neural architecture for extracting speaker\nrepresentations from acoustic features. The ECAPA-TDNN comprises a more sophisticated structure\nfor each component, such as Res2Block [52] and attention-based temporal pooling [8,9]. Meanwhile,\nSincNet [61] takes a raw waveform as the input and includes a distinctive convolution layer to learn a\nmeaningful band-pass. These models can be implemented using Standalone training processes.\n\nSome studies have presented to adjust pretrained Wav2vec 2.0 [17] self to perform speaker verification\n(Self-fitting) [20,21]. Both have exploited the Transformer output to be trained to become speaker\nembedding. In the former, average pooling was adopted after the Transformer layer. The latter,\nspeaker information was aggregated via a constant cls token inserted before the Transformer.\n\nThe Feature extraction approach is another way discussed for leveraging pretrained models in the\nASV field. We considered the most widely exploited format as a comparison, the weighted summation\nof multilayer outputs, suggested by SUPERB [24]. X-vector and ECAPA-TDNN were employed as\nbackend ASV systems, for processing the aggregated multilayer representation.\n\nWe employed several pretrained models to verify methods of leveraging those. Wav2vec 2.0 [17],\nHuBERT [18], and WavLM [19] are representative state-of-the-art models that process human speech\ndata. They share the structural similarity of Transformer encoder layers above the CNN; however,\nthey vary in their training strategies and objectives. This study limited the pretraining data to the\nLibriSpeech dataset for every model and fixed the pretrained weights for the feature extraction\napproaches. Such restrictions were set to facilitate comparison and identification of the cause and\neffect."}, {"title": "4.4 Hyperparameters and Implementation Details", "content": "For all experiments, the training minibatch comprised 128 audio samples truncated to 3s, and the\ncosine distance was adopted to measure the evaluation trial scores. No data augmentation was used\nother than SpecAugment [67] as specified.\n\nThe optimal number of training iterations (steps) was searched in every data setup for all standalone\nASV and self-fitting approaches (Tables 2 and 3). Combinations of {1, 3, 5} and powers of 10 were\nused, ranging from 1K to 300K. Based on the greedy algorithm, the iteration number that achieved\nthe lowest EER from the validation data was adopted. For the methods exploiting pretrained models,\nthe hyperparameters were adjusted using Wav2vec 2.0 and shared with the other pretrained models.\nThe rest of the hyperparameters, including training strategies and loss functions, were maintained as\nspecified in the original.\n\nThe Adam [68] optimizer with a one-cycle learning rate schedule [69] was used to train our model.\nThe maximum learning rate was set to 0.003, warming up for the first 10% of the training iterations.\nThe same strategies were used for training in the feature extraction type comparisons (Table 4). Codes\nare available here.3"}, {"title": "5 Results", "content": "We verified our method by comparing it with several ASV approaches in diverse data environmental\nsetups. Every entry is reported as the mean score of three different seeds and the standard deviation\nis marked in gray. The unit is denoted in parentheses to the right of the corresponding metric. (L)\nindicates a LARGE model; otherwise, the BASE size is implied.\n\n5.1 Comparison with Diverse Types of ASV\n\nTable 2 reports the experimental results from the VCTK Corpus and LibriSpeech datasets, where\nthe comparison includes standalone ASV networks (SincNet, X-vector, and ECAPA-TDNN) and\nself-fitting approaches (Avg. pool and cls. token) for exploiting pretrained models. We observed that\nthe use of pretrained models could benefit the convergence speed compared to training from scratch\nin general. However, the shortcomings of the self-fitting approaches are prominent in two aspects.\nFirst, as anticipated from Section 3.1, the methods failed with other pretrained models than what\nwas proposed, even though each is generally applicable to such a common architecture. Second, the\nself-fitting models still underperformed compared to the standalone model (ECAPA-TDNN), even on\nthe dataset where pretraining was conducted. In contrast, our proposal showed stability regardless of\nthe pretraining type and further improved the performance by using multilayered features.\n\nAs shown in Table 3, the same comparisons were conducted using datasets representing more natural\nspeech environments. The standalone models required substantially more training resources to\nconverge from recording environments with proactive background noise. On the other hand, despite"}, {"title": "5.2 Comparison with Feature Extraction Approaches", "content": "As reviewed in Section 2.3, most feature extraction approaches are based on the weighted summation\nof the multilayer representations presented by SUPERB. Table 4 presents a comparison of the\nproposed method with representative off-the-shelf ASV models trained on top of a layer-wise\naggregated representation. We measured the number of parameters for the learnable speaker pooling\ncomponent, excluding the frozen feature extraction process. The experimental results confirmed that\nusing multilayered features could reliably benefit from pretraining, regardless of the model types. On\nthe other hand, the results differ in the degree of pulling out the advantage of pretrained features, in\nterms of the number of parameters used and performance. Our approach achieved the best overall\nperformance with the minimum parameter training, particularly for the EER* metric, in every entry.\nFor the reason for such improvements, we underline the operation of interlayer representation to\ncapture speaker-relevant information and the advancement of the layer pooling mechanism. The\nstrengths of our method were more pronounced when fewer data resources were available."}, {"title": "5.3 Ablation Study", "content": "The preceding experimental results demonstrated advancement through direct training on the mul-\ntilayered output of pretrained models, rather than linearly combining layer-wise representations.\nAccordingly, our method contains interlayer operations such as the layer pooling procedure, which\noff-the-shelf ASV approaches do not incorporate. Table 5 presents the results of the ablation experi-\nments for such operations. Two types of ablation components were considered: the layer/frame-level\nprocessing network (L/F-network) and layer pooling (L-pool) strategy. We established a baseline\nwith the most fundamental form for aggregating layer-level features, namely SUPERB without the\nL/F-network. Subsequently, we combined the proposed method step by step and determined the\nchanges in the EER* and EER performance. We report both the absolute and relative performances,\nwhere the relative score is shown in percentile and enlarged for intuitive comparison. The relative\nmeasurement was calculated as $(\\delta_B - \\delta_A)/\\delta_B$, where $\\delta_B$ is the absolute score of the baseline and $\\delta_A$\nis the comparison entry."}, {"title": "6 Discussion", "content": "This section discusses the limitations of this study and presents directions for future research. We\nintroduced a novel approach for extracting speaker discriminative embeddings that is universally\napplicable to diverse pretrained models. Accordingly, we verified the superiority of this methodology\nin diverse training environments. However, our setups differed from those of other studies aimed at\nstate-of-the-art performance on benchmark evaluation protocols.\n\nThe major distinction is the pretrained models. As mentioned in Section 4.3, we used models that were\npretrained on the same data while fixing the weights during the downstream task training. Therefore,\nwe corroborated the impact of changes in the representation extraction approaches, excluding other\nexternal factors. In contrast, those who aim for high performance adopt front-end models that are\npretrained on several large-scale datasets while considering budget-free fine-tuning. In addition, noise\nand room impulse response datasets with data augmentation were adopted, and score calibration\ntechniques were utilized to push the performance limits.\n\nThe proposed backend pooling structure for multilayered outputs from pretraining was shown to be\neconomical as well as to improve the performance compared to existing backend approaches. As the\nmethod is universally applicable to broad pretraining architectures, we believe that it could further\nboost state-of-the-art performance along with previously introduced techniques in future studies."}, {"title": "7 Conclusion", "content": "Recent advances in speaker verification research have highlighted the exploitation of pretrained\nmodels. As demonstrated in diverse data-driven domains, such approaches can benefit from cost-\nefficient training and improved performance. This study has introduced a novel method for extracting\nspeaker-unique representations from the multilayered architectures of pretrained models.\n\nWe presented an analysis and offered insight into how existing approaches deal with pretrained\nmodels. Consequently, we argued for capturing speaker representations hidden in the layer-frame-\nlevel output from the pretrained model and advancing layer-level pooling techniques. We designed\ncomparative experiments using multiple data environments and pretraining model types to validate\nthe universality of the proposed pooling architecture. The experimental results demonstrated that our\nmethod can reliably leverage pretraining in various setups and surpass existing approaches in terms\nof performance and efficacy. The ablation study verified that both the layer-frame-level processing\nnetwork and layer attention pooling strategy contribute significantly to improving performance. In\nthe future, we will discuss incorporating various training techniques for state-of-the-art recordings to\ntranscend the limits of speaker verification performance further."}]}