{"title": "Improving Generalization of Speech Separation in Real-World Scenarios: Strategies in Simulation, Optimization, and Evaluation", "authors": ["Ke Chen", "Jiaqi Su", "Taylor Berg-Kirkpatrick", "Shlomo Dubnov", "Zeyu Jin"], "abstract": "Achieving robust speech separation for overlapping speakers in various acoustic environments with noise and reverberation remains an open challenge. Although existing datasets are available to train separators for specific scenarios, they do not effectively generalize across diverse real-world scenarios. In this paper, we present a novel data simulation pipeline that produces diverse training data from a range of acoustic environments and content, and propose new training paradigms to improve quality of a general speech separation model. Specifically, we first introduce AC-SIM, a data simulation pipeline that incorporates broad variations in both content and acoustics. Then we integrate multiple training objectives into the permutation invariant training (PIT) to enhance separation quality and generalization of the trained model. Finally, we conduct comprehensive objective and human listening experiments across separation architectures and benchmarks to validate our methods, demonstrating substantial improvement of generalization on both non-homologous and real-world test sets.", "sections": [{"title": "1. Introduction", "content": "Speech source separation aims to extract the audio tracks of each individual speaker in an audio mixture. Deep neural networks have emerged as a modeling cornerstone for achieving high-quality separation results. State-of-the-art single-channel speech separation models typically employ an end-to-end architecture that directly processes the time-domain mixture samples, with prominent works such as ConvTasNet [1], Dual-path RNN [2], DPTNet [3], WaveSplit [4], Sepformer [5], and Mossformer [6]. Some models operating in the time-frequency domain, such as TF-GridNet [7], also demonstrated exceptional performance. The application scope of speech separation systems is vast, encompassing areas such as speech enhancement, speech translation, voice conversion, and beyond.\nAn inherent challenge in achieving robust speech separation lies in adapting the model to diverse real-world acoustic environments and conversational scenarios. Researchers have proposed datasets representing various environments, from clean acoustic environments (wsj0-2mix [8]), to noisy surroundings (WHAM! [9]), and even the environment featuring both noise and reverberation (WHAMR! [9] and SMS-WSJ [10]). Furthermore, previous studies show drops in performance when separation models trained on datasets like wsj0-2mix are tested on comparable datasets. Multiple efforts have been made to address these issues, such as the introduction of the LibriMix [11] dataset that provides a wide range of speakers. Despite this promising progress, there remain open challenges in improving the robustness of speech separators.\nFirst, the generalization of speech separators across different acoustic environments needs further exploration. Currently, researchers often train distinct models tailored to specific datasets such as wsj0-2mix, WHAM!, and WHAMR!. However, these datasets typically focus on singular acoustic environments, overlooking the complexity of real-world scenarios. Hence, there is significant value and broad applicability in developing an all-in-one speech separator capable of handling any possible acoustic environment. Further, the remarkable advancements in speech enhancement and de-reverberation models [12, 13] provide an opportunity to reconsider the focus of speech separation in reverberant environments. Preserving the acoustics of the speech in the mixture (i.e., its reverberation and equalization) in separation results could offer substantial support for a variety of applications, including voice conversion and speech-to-speech translation.\nSecond, the generalization of speech separators to diverse speech content needs further exploration. Real-world speech scenarios span from dialogues where participants take turns speaking (e.g., interviews and presentations), to more volatile situations where speakers talk over each other (e.g., debates and talk shows). Each type of scenario presents unique characteristics, such as varying levels of cross-talk, presence of different non-speech audio events, levels of static noise, and instances of isolated single speakers. Developing a separator capable of generalizing across these variations in content requires careful consideration not only in data simulation techniques, but also in designing loss functions: traditional source-to-distortion ratio (SDR) and SI-SDR [14] cannot explicitly optimize for single speaker samples or mixtures with little overlapping regions. Thus, it is essential to re-consider the loss function for achieving robust separation performance and desirable perceptual quality.\nIn this paper, we propose methods to address challenges:\n\u2022 We introduce Acoustic-Content Simulation (AC-SIM), a comprehensive data simulation process that incorporates both acoustic and content variations during the training of speech separators. This approach aims to align separators to a diverse range of real-world speech separation scenarios.\n\u2022 We integrate multiple training objectives into the permutation invariant training (PIT) [15] scheme for speech separation to enhance the separation quality and model capabilities.\n\u2022 We conduct comprehensive objective and subjective experiments to demonstrate that the proposed methods improve the generalization of separators to multiple speech scenarios by achieving consistent performance on our evaluation sets, conventional benchmarks, and real-world cases."}, {"title": "2. Methodology", "content": "2.1. Problem Formulation\nIn this paper, we focus on single-channel speech source separation, addressing a common case involving up to two speakers. Given an audio mixture track $m \\in [-1,1]^T$ with the sample length T, the model separates out a 2-channel track $\\hat{s} \\in [-1,1]^{2 \\times T}$, with one channel for each speaker.\nPrior work in speech separation defines the separation objective as isolating the clean speech tracks of individual speakers, typically under the assumption that noise is the primary interference to the speech mixture. However, this separation objective becomes less effective in speech scenarios involving reverberation and equalization, as they introduce ambiguity in predicting clean speech and combine both separation and de-reverberation tasks into a single target. Instead, we redefine the separation target across all scenarios as speech tracks of individual speakers with acoustic characteristics (i.e., preserving the reverberation and equalization correlated with speech, but suppressing noise). This target remains consistent with previous work under reverb-free clean or noisy scenarios but diverges in addressing noisy-reverberant scenarios. It clearly distinguishes the separation task from the enhancement task, thereby simplifying problem-solving approaches. The preserved acoustic characteristics also benefit a wide range of downstream applications that require or can leverage their acoustic properties.\n2.2. Acoustic-Content Data Simulation\nWe propose Acoustic-Content Simulation (AC-SIM) for data generation, comprising content and acoustic simulation processes. The content simulation contains two stages, namely content selection (section 2.2.1), and crosstalk simulation (section 2.2.3). The acoustic simulation serves as the intermediate processing step to augment the selected content (section 2.2.2).\n2.2.1. Content Selection\nContent selection is based on dynamic mixing (DM) [4], which dynamically determines what samples to include in the audio mixture. We categorize three types of audio samples:\n\u2022 Speech: clean speech samples of different speakers.\n\u2022 Static Noise: generic noise samples as part of background.\n\u2022 Event Noise: specific event samples as part of background.\nIn contrast to prior works [8, 9] which train models in one specific scenario, all samples in our selection, except the first speaker, are chosen with independent probabilities to maximize the diversity of speech scenarios. Furthermore, the inclusion of event noise provides a more dynamic sound background jointly with the static noise. This selection allows us to investigate whether a single separator can effectively handle multiple and complex speech scenarios.\n2.2.2. Acoustic Simulation\nThe acoustic simulation involves manipulating audio samples based on their types. The middle part of Figure 1 illustrates the use of speed change, dynamic volume normalization, reverberation and equalization. Similar to content selection, all acoustic manipulations are randomly chosen with probabilities to enrich the training data with diverse acoustic environments, thus enhancing the adaptability of the separator.\nThe speed change module, dynamic volume normalization and reverberation are applied exclusively to speech samples. The reverberation is guided by a randomly selected room impulse response (RIR) and augmented by scaling DRR and RT60 properties [16] from 0.5 to 2.0. Speed change module stretches speech samples with a scale of 0.9 to 1.2. Dynamic volume normalization involves randomly inserting 0-3 anchor points and linearly changing the volume level to the target level in the range of -10 to 10 dB. Equalization (EQ) is applied to all speech, static noise, and event noise samples, employing a random seven-band filter with the gain for each band set between -5 and 5 dB. Notably, we applied an independent EQ before reverberation on speech to increase the simulation diversity.\n2.2.3. Crosstalk Simulation\nThe crosstalk simulation, as part of content simulation, is applied on acoustically-processed audio samples (as ap.). It consists of two steps: random split and event overlap removal.\nMost speech datasets contain continual speech without long pauses between utterances, whereas real-world conversations often involve turn-taking (i.e., crosstalk) and silent moments when speakers are not actively engaged. Naively mixing speech samples from these datasets results in excessive overlap of speakers, creating a distribution different from real-world data. Instead, we propose randomly splicing segments of speech together with intervals of silence, as shown in Algorithm 1: for each audio component x, we randomly draw a segment with a random length between $l_1 \\times T$ and $l_2 \\times T$ from x and append it to the output samples y. The process continues each time with probability $p_{seg}$ until either input or output samples reach their end. In Algorithm 1, we also describe how splitting and splicing operations are randomly conducted to create crosstalk scenes.\nAnother content factor we consider is whether speech and non-speech events are disjoint. Some transitions occur in scenarios like talk shows or interviews where speakers remain silent until non-speech events cease. Therefore, when mixing segmented speech samples with event samples, we implement an event overlap removal mechanism with a certain probability. This involves removing all the segments in the event track that overlap with speech occurrences. By incorporating crosstalk simulation, our training data more accurately reflects the dynamics between speech and non-speech events in real-world scenarios, enhancing the robustness of the separator.\n2.3. Training Objectives and PIT\nSpeech separation models are typically optimized with SDR or SI-SDR loss functions. However, neither explicitly optimizes for silence frames, which frequently occur in single-speaker or sparse talking scenarios, nor prioritizes the clarity of separation results. Inspired by [17, 12, 18], we introduce three additional loss functions that better align with human perception of acoustic similarity into Permutation Invariant Training (PIT): multi-resolution STFT magnitude loss, mel-spectrogram loss, and time-domain L2 loss:\n$L_{mstft} = \\Sigma_i || log |STFT(s_i, 0_i)| \u2013 log |STFT(\\hat{s}_i, 0_i) |||_1$ (1)\n$L_{mel} = || log Mel(s) \u2013 log Mel(\\hat{s}) ||_1$ (2)\n$L_{time} = ||s - \\hat{s}||_2$ (3)\nWe adopt the multi-resolution STFT magnitude loss with three filter lengths: 512, 1024, and 2048. The mel-spectrogram loss is conducted using the STFT with a filter length 1024 and 128 mel-bins. The total loss for training the separator is the combination: $L = \\lambda_{mstft} L_{time} + \\lambda_{mstft} L_{mstft} + \\lambda_{mel} L_{mel} + \\lambda_{sdr} L_{sdr}$. We use coefficients $\\lambda_{mstft}$ = 10, $\\lambda_{mel}$ = 10, $\\lambda_{time}$ = 100, and $\\lambda_{sdr}$ = 1.\nWe replace $L_{sdr}$ with the combined loss L during the permutation invariant training (PIT) of the speech separator."}, {"title": "3. Experiments", "content": "3.1. Datasets, Backbones, and Training Setup\nWe train and validate speech separators on simulated samples at a sample rate of 16000 Hz. We collect 6149 hours speech data from VCTK [19], DAPS [20] and Spotify Podcast datasets [21]; 4.36 hours static noise data from the REVERB Challenge [22], the ACE Challenge [23] and the ambient sound databases [24]; 39 hours ten-class event noise data from ESC-50 [25], BBC Sound Effects [26], VGGSound [27], UrbanSound [28], and Freesound datasets [29]; and RIR data for reverberation from MIT IR Survey dataset [30] and the Echothief IR Library [31]. Each of them is split into the training set and the validation/evaluation sets.\nTo fully validate the effectiveness of AC-SIM and training paradigm, we choose three models from the prominent and easily-accessible speech separation work as our backbones: ConvTasNet, Dual-Path RNN, and Sepformer. They represent common choices of network architectures for the speech separation task. We then train these models using several variants of our proposed data simulation. Each training data sample lasts 5.0 seconds. We use the Adam [32] optimizer (\u03b2\u2081=0.9, \u03b22=0.99) for training all separators with the batch size of 16 and up to 200 000 steps until convergence on 4 NVIDIA A100 GPUs. Due to the page limitation, we attach the hyper-parameters of these models, detailed configurations of AC-SIM and separation demos in the anonymous appendix page.\nTo fully evaluate the separation performance of the models in different scenarios, we use AC-SIM to generate validation and evaluation sets shown in the leftmost part of Table 1, with the total duration of 6.8 hours. The prefixes D and S denote 2-speaker (double) and 1-speaker samples (single) respectively, and the suffixes denote different acoustic scenarios: All (static noise, event noise, and reverberant speech); NE (static noise, event noise, and clean speech); NR (static noise and reverberant speech); and N (static noise and clean speech). Necessarily, we assess the generalization of the models using wsj0-2mix, WHAM!, WHAMR!, and Libri2Mix (both clean and noisy versions) evaluation sets as benchmarks.\n3.2. Objective Evaluations and Ablation Studies\nTable 1 presents the primary evaluation of three separation models across 12 evaluation sets. The \"Vanilla\" models serve as a baseline and are trained and validated on the WHAMR! dataset.\n3.3. Subjective Listening Test\nWe evaluate the separation accuracy and perceptual quality with a Mean Opinion Score (MOS) test on the evaluation sets as well as a collection of 16 real-world speech recordings, and present results in Table 3. The test was conducted on Prolific [34]. In each question, a listener rates the separation quality of two isolated tracks individually on a scale from 1 (Bad) to 5 (Excellent) with the original mixture track provided as a reference. We collected MOS scores of the four Sepformer models in Table 1, along with the input mixture and the ground-truth reference, totalling 13401 valid responses from 275 unique listeners (~2200 responses per model).\nFrom the table, our ACM-SIM-ML Sepformer outperforms other models across all evaluation sets and real-world cases except WHAMR!, with a consistent improvement from DM to ACM-SIM-ML setting. Conversely, the Vanilla Sepformer, trained on WHAMR!, only fits its homologous evaluation set but performs poorly on the others. This indicates: (1) WHAMR! may not be a representative dataset for real-world scenarios, as models excelling in real-world cases and other evaluation sets tend to under-perform on WHAMR!, and vice versa; (2) Our AC-SIM helps generalize the model to real-world cases as well as different benchmarks via a comprehensive simulation process. Besides, the visualizations of a real-world example in Figure 2 further demonstrates that AC-SIM-ML enhances clarity of audio and rectifies issues encountered in single-speaker cases."}, {"title": "4. Conclusion", "content": "We propose a combination of acoustic-content simulation, and multiple loss training paradigms to address the challenge of generalizing speech separation models across diverse acoustic environments and content. Our experiments confirm the effectiveness of our methods across various models and benchmarks, showcasing substantial improvements in separation quality and generalization on both synthetic and real-world test sets. This underscores further research to bridge the gap between speech separation technologies and real-world applications."}]}