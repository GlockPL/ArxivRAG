{"title": "BENCHMARKING ROBUSTNESS OF CONTRASTIVE LEARNING\nMODELS FOR MEDICAL IMAGE-REPORT RETRIEVAL", "authors": ["Demetrio Deanda", "Yuktha Priya Masupalli", "Jeong Yang", "Young Lee", "Zechun Cao", "Gongbo Liang"], "abstract": "Medical images and reports offer invaluable insights into patient health. The heterogeneity and\ncomplexity of these data hinder effective analysis. To bridge this gap, we investigate contrastive\nlearning models for cross-domain retrieval, which associates medical images with their corresponding\nclinical reports. This study benchmarks the robustness of four state-of-the-art contrastive learning\nmodels: CLIP, CXR-RePaiR, MedCLIP, and CXR-CLIP. We introduce an occlusion retrieval task to\nevaluate model performance under varying levels of image corruption. Our findings reveal that all\nevaluated models are highly sensitive to out-of-distribution data, as evidenced by the proportional\ndecrease in performance with increasing occlusion levels. While MedCLIP exhibits slightly more\nrobustness, its overall performance remains significantly behind CXR-CLIP and CXR-RePaiR. CLIP,\ntrained on a general-purpose dataset, struggles with medical image-report retrieval, highlighting the\nimportance of domain-specific training data. The evaluation of this work suggests that more effort\nneeds to be spent on improving the robustness of these models. By addressing these limitations, we\ncan develop more reliable cross-domain retrieval models for medical applications.", "sections": [{"title": "1 Introduction", "content": "The rapid growth of medical data, including images and reports, presents both opportunities and challenges for\nhealthcare professionals. While these data sources offer valuable insights into patient health, their heterogeneity, and\ncomplexity can hinder effective analysis and decision-making [1]. To bridge this gap, there is a pressing need for AI\nmodels capable of jointly understanding both modalities [2].\nCross-domain retrieval, which involves establishing connections between data from distinct sources, has the potential to\nrevolutionize medical research and practice. By combining information from multiple domains, healthcare providers can\ngain a more comprehensive understanding of patient conditions, leading to more accurate diagnoses and personalized\ntreatment plans [3, 4]. Furthermore, cross-domain retrieval can facilitate the discovery of new medical insights\nby revealing patterns and trends that might otherwise be obscured. In addition, cross-domain retrieval for medical\nimaging-report can also facilitate the automated generation of medical imaging reports [5, 6, 7].\nContrastive learning has emerged as a promising technique for cross-domain retrieval in medical imaging and reports [8,\n9, 6]. While neural networks have demonstrated impressive performance in various tasks, such as cyber security [10, 11],\nhealthcare [12, 13], public transportation [14, 15], and astrophysics [16, 17, 18], modern neural networks are suffering\nfrom issues like miscalibration [19, 20, 21], bias [22], reliability [23], and vulnerability to adversarial attacks [24, 25].\nTo address these limitations, it is crucial to benchmark the robustness of different models.\nThis paper investigates the robustness of the contrastive learning-based cross-domain retrieval models, including\nCLIP [9], CXR-RePaiR [5], MedCLIP [6], and CXR-CLIP [7], for cross-domain retrieval in medical imaging and"}, {"title": "2 Problem Definition", "content": "In the context of medical imaging, cross-domain retrieval involves associating medical images with their corresponding\nclinical reports [7, 5]. This task is challenging due to the inherent differences in the nature of these data modalities.\nHowever, recent advancements in contrastive learning has enabled significant progress in this area [9, 8]."}, {"title": "2.1 Contrastive learning", "content": "Contrastive learning is a technique that learns general data representations by comparing similar and dissimilar samples.\nIn the imaging domain, Siamese networks, composed of two identical subnetworks, are commonly used for this\npurpose [26]. These networks process pairs of images and output feature vectors, which are then compared using a loss\nfunction like triplet loss [27] or contrastive loss [28, 29]. Alternatively, a binary cross-entropy loss can be used to train\nSiamese networks as a binary classification problem [30, 31].\nTo jointly understand medical images and their corresponding reports, a Siamese-style network with distinct subnetworks\ncan be employed. Given a dataset X of image-text pairs {(Ij, Tk)}, where Ij \u2208 I is an image and Tk \u2208 T, I and\nT denoting a set of images and a set of textual reports, respectively. If j = k, the image and textural report are\nassociated (i.e., the image and report are matching). The goal of contrastive learning is to learn a model h(\u00b7) that pulls\nthe embeddings of matching image-text pairs closer in the feature space and pushes those of non-matching pairs further\napart.\nThe model h(\u00b7) typically consists of two branches: hi(\u00b7) for processing images and ht(\u00b7) for processing text. The image\nbranch often employs a convolutional neural network (CNN) or a Vision Transformer (ViT), such as ResNet [32] or\nVision Transformer [33], to extract visual features vi. The text branch, usually is a large language model (LLM) like\nBERT [34] or RoBERTa [35], extracts textural features vt. These features are then projected to the same space and\ncompared using contrastive losses, such as triplet loss, or classified using a binary classifier."}, {"title": "2.2 Contrastive Learning for Cross-Domain Retrieval", "content": "Contrastive learning is a powerful technique for training cross-domain retrieval models, which can be employed in two\nways: similarity comparison and binary classification.\nContrastive learning brings together samples from the same group (e.g., a matching image and report) in the feature\nspace, while simultaneously pushing apart samples from different groups (e.g., non-matching pairs). This enables"}, {"title": "3 Method", "content": "This work investigates the robustness of four contrastive learning models applied to medical image-report retrieval tasks.\nGiven a query image, the objective is to retrieve the most relevant report. This section outlines the detailed evaluation\nmethodology."}, {"title": "3.1 Robustness Evaluation", "content": "To assess the robustness of the pre-trained contrastive learning-based cross-domain retrieval methods, we intro-\nduced an occlusion retrieval task. During evaluation, we systematically occluded a portion (p) of the image\n(p = {0%, 0.25%, 1%, 4%, 9%, 25%, 49%, 81%}) at random locations, generating out-of-distribution data for the\npre-trained models. These occluded images were then used as input to the models for retrieval tasks.\nTo evaluate the robustness of the models, we calculated Recall@k, a metric that measures the proportion of relevant\nitems retrieved within the top k results. We varied the value of k ({5, 10, 20, 30, 50, 100}) to assess performance at\ndifferent retrieval depths. Recall@k is calculated as follows:\nRecall@k =\n# of relevant items retrieved in top k\nTotal # of relevant items\n                                        (1)\nIdeally, a robust model should exhibit similar Recall@k values across different occlusion levels, especially for smaller\nocclusion percentages p.\nAlgorithm 1 provides a detailed description of occlusion retrieval with a specific occlusion ratio."}, {"title": "3.2 Cross-Domain Retrieval Models", "content": "This work evaluates four contrastive learning-based models for cross-domain retrieval tasks: CLIP [9], CXR-RePaiR [5],\nMedCLIP [6], and CXR-CLIP [7]."}, {"title": "3.2.1 CLIP (Contrastive Language-Image Pre-training)", "content": "This neural network learns a shared feature space for images and text. Trained on a massive dataset of image-text pairs,\nCLIP maximizes similarity between semantically related pairs while minimizing it for unrelated ones. This allows\nCLIP to understand the connection between visual and textual information, enabling tasks like image classification and\nzero-shot learning. In our work, we leverage CLIP's learned embeddings for image-text retrieval by calculating cosine\nsimilarity between image and text embeddings generated by the pre-trained model."}, {"title": "3.2.2 CXR-RePaiR (Contrastive X-ray-Report Pair Retrieval)", "content": "This method generates chest X-ray reports through a retrieval-based fashion. It fine-tuned CLIP on the MIMIC-CXR [36]\ndataset for report-level or sentence-level retrieval. Report-level retrieval selects the entire best-matching report from\nthe candidate set, while sentence-level retrieval constructs a new report by selecting sentences from multiple reports.\nFor consistency with other methods, we employ the report-level retrieval in this work, calculating cosine similarity\nbetween query image embeddings and textual report embeddings generated by a CLIP model that was initialized with\nCXR-RePaiR weights (available on their official GitHub repository\u00b9)."}, {"title": "3.2.3 MedCLIP", "content": "This neural network model is jointly trained on medical images and their corresponding text reports. Unlike previous\nmethods, MedCLIP utilizes unpaired data, reducing the need for large amount of paired data. Designed as a general-\npurpose medical imaging model, MedCLIP may perform various tasks like zero-shot learning, supervised classification,"}, {"title": "3.2.4 CXR-CLIP", "content": "Similar to MedCLIP, CXR-CLIP aims to train a general-purpose image-text model using limited data. However,\ninstead of unpaired data, CXR-CLIP leverages Large Language Models (LLMs) to expand image-label pairs into\nnatural language descriptions. Additionally, it utilizes multiple images and report sections for contrastive learning. To\neffectively learn image and textual features, CXR-CLIP introduces two novel loss functions: ICL and TCL. ICL focuses\non learning study-level characteristics of medical images, while TCL focuses on learning report-level characteristics.\nPre-trained CXR-CLIP models can perform both zero-shot learning and image-text retrieval. We evaluate CXR-CLIP's\nimage-text retrieval capabilities using the official code and pre-trained model available on CXR-CLIP's official GitHub\nrepository\u00b3."}, {"title": "3.3 MIMIC-CXR Dataset", "content": "The MIMIC-CXR dataset [36] is a dataset that widely used for contrastive learning and image-text retrieval in the\nmedical domain. The dataset contains 227,835 radiographic studies from 64,588 patients, encompassing 368,948 chest\nX-rays and their corresponding radiology reports. The dataset also provides 14 labels (13 for abnormalities and one for\nnormal cases) derived from radiology reports using NLP tools like NegBio [37] and CheXpert [38].\nThe official validation set includes 2,991 imaging studies, each containing one or more chest X-rays paired with a\nsingle textual report (e.g. Figure 2). Each report is divided into sections such as History, Comparison, Findings, and\nImpression. To ensure data quality, we filtered out reports missing the Findings or Impression sections, resulting in a\nfinal validation set of 994 studies with 1,770 X-rays. This filtered dataset is, then, used in our experiments."}, {"title": "4 Result", "content": "Table 1 presents the occlusion retrieval results of the four evaluated models for various occlusion percentages. Bold\ntext highlight the best performance for each occlusion ratio and recall threshold. Blue text indicates the second-best\nperformance, while red text denotes the worst performance.\nThe table reveals that CXR-CLIP consistently achieves the best performance across most occlusion ratios and recall\nthresholds, except for the 81% occlusion level for Recall@5. CXR-RePaiR consistently achieves the second-best\nperformance for all occlusion ratios, except for the 81% occlusion level. MedCLIP generally ranks third, but it\nachieves the second-best performance five times at the 81% occlusion level across six different recall thresholds. CLIP\nconsistently performs the worst, with most results aligning with random performance."}, {"title": "4.1 Cross-Domain Retrieval", "content": "While CLIP's poor performance is expected due to its training on natural images, MedCLIP's relatively weaker\nperformance is surprising, given its training on medical data. However, this aligns with the performance trends reported\nin the MedCLIP paper, where MedCLIP outperforms CLIP by approximately two times [6]. We believe MedCLIP's\nweaker retrieval performance stems from its integration of unpaired images, texts, and labels using a rule-based labeler,\nwhich may hinder the model's ability to accurately associate images with their corresponding reports due to the\ndecoupling of image-text pairs."}, {"title": "4.2 Robustness Analysis", "content": "Figure 3 visualizes the performance of CXR-RePaiR (Figure 3 top), CXR-CLIP (Figure 3 middle), and MedCLIP\n(Figure 3 bottom), respectively. All three models exhibit a decrease in performance as the image occlusion percentage\nincreases. The performance degradation is generally proportional to the occlusion level, with MedCLIP showing a\nslightly slower decline (approximately 20%) compared to the other two models. This near-proportional performance\ndecrease suggests that none of the models are robust to handle occluded or out-of-distribution data.\nBetween CXR-RePaiR and CXR-CLIP, CXR-RePaiR shows a slightly steeper decline in performance, indicating lower\nrobustness compared to CXR-CLIP.\nWhile MedCLIP exhibits a weaker overall retrieval performance, its slower decline in performance suggests potential\nrobustness. Especially for low occlusion levels (less than 4%), slight occlusions may even improve MedCLIP's retrieval\nperformance. We hypothesize that this is due to the model's training on unpaired images, texts, and labels. Slight\nocclusions may act as a form of noise reduction, smoothing out potential overfitting and improving generalization."}, {"title": "5 Conclusion", "content": "This study investigates the robustness of contrastive learning-based cross-domain retrieval models for medical image-\nreport retrieval tasks. By introducing an occlusion retrieval task, we assessed the performance of CLIP, CXR-RePaiR,\nMedCLIP, and CXR-CLIP under varying levels of image corruption."}]}