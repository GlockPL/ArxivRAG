{"title": "FCL-ViT: Task-Aware Attention Tuning for Continual Learning", "authors": ["Anestis Kaimakamidis", "Ioannis Pitas"], "abstract": "Continual Learning (CL) involves adapting the prior Deep Neural Network (DNN) knowledge to new tasks, without forgetting the old ones. However, modern CL techniques focus on provisioning memory capabilities to existing DNN models rather than designing new ones that are able to adapt according to the task at hand. This paper presents the novel Feedback Continual Learning Vision Transformer (FCL-ViT) that uses a feedback mechanism to generate real-time dynamic attention features tailored to the current task. The FCL-ViT operates in two Phases. In phase 1, the generic image features are produced and determine where the Transformer should attend on the current image. In phase 2, task-specific image features are generated that leverage dynamic attention. To this end, Tunable self-Attention Blocks (TABs) and Task Specific Blocks (TSBs) are introduced that operate in both phases and are responsible for tuning the TABS attention, respectively. The FCL-ViT surpasses state-of-the-art performance on Continual Learning compared to benchmark methods, while retaining a small number of trainable DNN parameters.", "sections": [{"title": "I. INTRODUCTION", "content": "Human learning is a challenging concept to understand and analyze. However, the human ability to learn novel tasks, while leveraging the knowledge of previous ones is undeniable. On the other hand, Deep Neural Networks (DNNs) have not yet achieved this summit. As a result, DNN catastrophic forgetting [8] can occur, when DNNs display a profound decrease in their performance of previous tasks upon learning new ones. This phenomenon can be caused by the feed-forward nature of most DNN architectures involved in CL, which typically cannot provide human \"thinking\" options. For example, DNNs cannot reiterate, re-filter, and further analyze a given prompt, while humans reiterate the same thought to come to a safe conclusion. Such a process can be modeled in DNNs using a DNN feedback mechanism. Admittedly, to leverage the knowledge of previous tasks and infer dynamically on incoming images a DNN should be able to assess the nature of the task at hand dynamically. Then the DNN should filter the neural features accordingly to generate on-the-fly an image analysis strategy customized for the incoming image.\nMany recent studies [6, 7, 14, 23, 32, 35] have sought to tackle the CL challenges. Among these approaches, some [14, 23, 35] have utilized the concept of knowledge distillation [13], which involves transferring knowledge from a Teacher DNN model to a Student DNN model to preserve the prior knowledge captured in the network's output logits. In contrast, other methods [7, 32] have leveraged dynamic expandable networks to address CL challenges. These techniques focus on dynamically enhancing network architectures, such as feature extractors, by incorporating additional DNN parameters and memory. Despite recent progress in tackling the CL problem, forgetting rates reported in CL methods are still high. This occurs because the CL goal is to teach the DNN model where to attend according to the new task at hand. This cannot happen dynamically using the traditional feed-forward DNN architectures. There is a need for novel DNN architecture that can act sequentially by: 1) assessing the task at hand and, 2) tuning Attention features accordingly.\nTo address this challenge, the Feedback Continual Learning Vision Transformer (FCL-ViT) is introduced which operates in two Phases. Phase 1 produces generic image features, i.e. the attention features of the current image. After obtaining the generic image features, phase 2 produces task-specific features to aid the inference of the current image. Thus, FCL-ViT uses Tunable self-Attention Blocks (TABs) that provide the ability to generate both generic and task-specific attention features. FCL-ViT also uses Task Specific Blocks (TSBs) to translate the generic attention features to task-specific attention-tuned features. The TSBs need a CL regularizer, e.g. Elastic Weight Consolidation (EWC) [16], to retain previous knowledge while learning a new task. Such a ViT design significantly alleviates catastrophic forgetting and leverages the Attention mechanism feature of tuning the focus according to the needs of the new task. A similar mechanism has worked significantly well for the Transfer Learning (TL) problem, TOAST [24] proposed a feedback mechanism that refocuses the Transformer encoder features according to the new task dataset at hand. The key difference with FCL-ViT is that TOAST element-wise adds the value variable of each self-attention layer with the vector produced by the decoder introduced by the TOAST method. On the other hand, the FCL-ViT uses the TSBs to produce vectors that tune features using cross-attention layers. Another difference is that TOAST neither uses a regularizer nor Dropout layers in their implementation, as the nature of their problem (Transfer Learning) is different than CL.\nThe key contribution of this paper is the proposal of a novel framework for CL that uses TABs and TSBs as fundamental blocks and, operates in two Phases: 1) Generating generic image features and 2) Generating task-specific features by reprogramming the TABs feature representations dynamically. Extensive experimental FCL-ViT evaluation proves that FCL- ViT goes well beyond the state-of-the-art performance on the"}, {"title": "II. RELATED WORK", "content": "Continual Learning methods can be summarized into three main categories: regularization-based techniques, replay-based techniques, and dynamic expandable networks.\nReguralization-based methods primarily use a regularizer to mitigate vast changes to the most important weights per task. A regularization-based method is Elastic Weight Consolidation (EWC), which calculates important weights and adds a regularizer loss to prevent these weights from changing upon learning new tasks. Another approach is called Learning without Forgetting (LwF) [19] where the DNN model is jointly optimized both for obtaining high classification accuracy in the new task and for preserving accuracy in the old one, without requiring access to the old training data. This method actually employs KD to achieve CL, since it involves optimizing the model on new data according to both ground truth and to the original network's response to the new data.\nIn contrast to previously described CL methods, replay-based CL approaches retain a portion of previous data, referred to as exemplars. Subsequently, a DNN model is trained on both a new training dataset and the stored exemplars to prevent catastrophic forgetting of previous tasks. One such method is Incremental Classifier and Representation Learning (iCaRL) [23], which utilizes a memory buffer derived from LwF [19]. iCaRL employs a herding data sampling strategy in classification, where mean data features for each class are calculated, and exemplars are iteratively selected to bring their mean closer to the class mean in feature space. An alternative algorithm is End-to-End Incremental Learning (EEIL) [1], which introduces a balanced training stage to finetune the DNN model on a dataset with an equal number of exemplars from each class encountered thus far.\nDynamic Expandable Networks, such as DER [32] offer another strategy by dynamically expanding the model's architec- ture with a new feature extractor for each task, which then feeds into a unified classifier. Although this method has achieved state-of-the-art performance, the continuous expansion of the model can lead to excessive overhead, and the pruning methods used to manage this, like HAT, demand high computational resources. Furthermore, while DER eliminates the need for task identifiers during inference, its hyperparameter-sensitive pruning poses practical challenges in real-world applications. In contrast, more recent works, such as Simple-DER [20], aim to resolve these limitations by adopting dynamic architectures specialized for each task. Though effective, they either struggle with uncontrolled parameter growth or require hyperparameter tuning. To address these shortcomings, new approaches focus on task-dynamic strategies that avoid the need for task identifiers and minimize memory overhead, often utilizing architectures based on Transformers and class-agnostic layers similar to TreeNets [18].\nWhile vision transformers have achieved notable success, their use in Continual Learning remains relatively unexplored. Recent methods like DyTox [7] aim to address this by expanding task-specific tokens or employing inter-task attention mechanisms to incorporate previous task information, prevent- ing performance degradation between tasks. However, these approaches require additional memory for storing past tasks' training instances. MEAT [31] tackles this by utilizing learnable masks to preserve key parameters for previous tasks, though its non-expandable architecture limits the number of tasks it can handle. Moreover, many approaches, such as L2P [28] and DualPrompt [27], rely on soft prompts and thus present scalability issues when the number of tasks increases.\nTransformers were first introduced for machine translation tasks [25] Since then, they have become the state-of-the- art model for various Natural Language Processing (NLP) tasks [4, 9, 21, 22]. The key element of the Transformer is the attention module, which gathers information from the entire input sequence. Recently, the Vision Transformer (ViT) [5] adapted the Transformer architecture for image classification, achieving scalability when working with large datasets. Following this, significant efforts have been made to enhance Vision Transformers in terms of data and model efficiency [11, 15, 33]. A prominent research direction involves incorporating explicit convolutions or convolutional properties into the Transformer architecture [2, 26, 30, 34]. For example, CoaT [29] introduces a conv-attention module that uses convolutions to implement relative position embeddings. LeViT [10] adopts a pyramid structure with pooling to capture convolutional-like features instead of the uniform Transformer architecture. Similarly, CCT [12] removes the need for class tokens and positional embeddings by using a sequence pooling strategy and convolutions instead."}, {"title": "III. FCL-VIT METHODOLOGY", "content": "The CL goal is to progressively update a single DNN image classification model to accommodate new classes as they become available. In a series of classification tasks T1, T2,...,Tt, task dataset Di = {(x,y)}^N\u1d62_{j=1} comprises N\u1d62 samples, and x and y denote the j-th image and its corresponding label. One task may comprise data coming from various classes. The class set associated with the training task Ti is denoted by Ci. We assume that there is no overlap between the classes of different tasks, meaning that \u2200i, j, Ci \u2229 Cj = \u00d8. For the current task, only samples from Ct, are available as this method is not using any kind of rehearsal memory. The DNN"}, {"title": "B. Feedback Continual Learning Vision Transformer (FCL-ViT)", "content": "Most DNN models are feed-forward ones. Therefore, DNN inference is performed entirely on one forward pass. As CL aims to reduce catastrophic forgetting upon learning new tasks, this is a rather hard task for feed-forward classification DNNs, as no feedback is provided upon inference. To address CL needs, this paper proposes a novel CL ViT architecture, the Feedback Continual Learning Vision Transformer (FCL-ViT). The FCL-ViT is a Transformer architecture using feedback for CL classification problems. The FCL-ViT architecture comprises Tunable self-Attention Blocks (TABS) and Task-Specific Blocks (TSBs) for image feature extraction. TSBs are entirely responsible for dynamically tuning the FCL- ViT features during inference.\nThe FCL-ViT architecture uses a feedback mechanism to tune the features during inference. FCL-ViT Inference is conducted in two phases: a) the generic image features phase and b) the task-specific features phase. As a result, given a data sample x, the patch positional embeddings p\u2208 RD are produced, where D denotes the dimension. In FCL-ViT Phase 1, p passes through d TABs to generate the generic image features representations r. The representations r are passed through the TSBs to generate the task-specific features for TABs {gj, j = 1, ...,d}, where d denotes the ViT depth. In Phase 2, p is passed again through the TABs to generate the task-specific features z, to be used for classifying image x. For simplicity, the following equations describe the architecture using one attention head per block.\nThe TABs play a dual role as they participate in both FCL- ViT phases. The structure of a TAB allows it to switch between the two phases. As a result, a TAB is a typical self-attention module in Phase 1:\nQj = Wqjfj-1,\nKj = Wkjfj-1,\nVj = Wvjfj-1,\nAj = Softmax(Q\u2c7cK\u2c7c\u1d40/\u221a{D/h})\nfj = Woj Aj Vj + bo, fo = p, r = f_d \u2208 R^D.\nWhere, fj, j = 1, ..., d are the TABs outputs during phase 1. The generic image vector r is equal to the d-th TAB output r = fq. During, phase 2 the j-th TAB operates as a typical cross-attention module between fj\u22121 and the output gj of the j-th TSB:\nQj = Wqjfj-1,\nKj = Wkjgj,\nVj = Wvjgj,\nAj = Softmax(Q\u2c7cK\u2c7c\u1d40/\u221a{D/h})\nfj = WojAjVj + bo, fo = p, z = fa \u2208 R^D.\nAt the end of the second phase the finetuned feature vector z of x is produced for image classification. The TSBs have a specific structure to capture the task-specific information and refine the TAB features. The j-th TSB consists of two linear layers each equipped with a Dropout function. The first one filters the output of the previous TSB 1; to be passed to the TAB in the same depth j = 1, ..., d, and the other one produces the output 1-1 to be passed to the next TSB j 1:\ngj = Dropout(H\u2081\u2c7clj; p),\nlj\u22121 = Dropout(H\u2082\u2c7clj; p), l_d = r \u2208 R^D"}, {"title": "IV. EXPERIMENTAL FCL-VIT EVALUATION", "content": "Following common CL evaluation protocols [7, 23, 32], the FCL-ViT is evaluated on different splits of the CIFAR-100 [17]. A pretrained ViT [5] base is employed as a backbone to initialize the TABs weights. The chosen FCL-ViT contains d = 12 TABs with an embedding dimension of D = 768 and 12 attention heads. The FCL-ViT also contains d = 12 TSBs with a dropout rate of p = 0.5. After a hyperparameter search, the batch size was set to 128 and Adam was adopted as an optimizer, with an initial learning rate of 10-3. The number of epochs was set to 100. Different performances are measured for CIFAR100 on 10 tasks (10 new classes per task), 20 tasks (5 new classes per task), and 50 tasks (2 new classes per task).\nFCL-ViT outperforms all competing methods. Although all other methods' performance is decreased with increasing task numbers on both the \"Avg\" and the \u201cLast\" classification accuracy, FCL-ViT performance is stable. This proves that the FCL-ViT architecture is tolerant to learning multiple tasks continually without exhibiting catastrophic forgetting behavior. The same results are seen in Figure 2 where the average Top-1 accuracy after learning a task is reported for FCL-ViT and all benchmark methods. As the number of seen classes increases, FCL-ViT retains its classification accuracy much better than its competitors. It is noted here that most of the benchmark methods use rehearsal memory to avoid catastrophic forgetting. FCL-ViT replays none of the samples learned in previous tasks.\nFCL-ViT achieves superior performance compared to the baseline methods because of the nature of regularization methods and the Transformer adaptability. The performance boost comes from the fact that the EWC method operates in a higher-dimensional space, allowing it to better discriminate FCL-ViT features across tasks. This level of discrimination is not possible in traditional feed-forward DNNs, which are limited by the lower dimensionality of their latent space. Additionally, the ability to tune a Transformer's features and steer the Attention allows the FCL-ViT to perform this better than its competitors.\nTo showcase the extended capabilities of the FCL-ViT architecture, it is showcased in a Natural Disaster Management (NDM) scenario. The BLAZE wildfire classification dataset is employed, which contains wildfire pictures. Such images have significant differences in the input images from CIFAR100 image entries. The BLAZE classification dataset contains 3850 train images and 1567 test images for the classification of areas during or after a wildfire. The classes are Fire, Burnt, Half- Burnt, Non-Burnt and Smoke. As a result, the experiment using CIFAR100 with 20 tasks (5 new classes per task) is repeated after FCL-ViT has first learned the BLAZE classification dataset.\nThe EWC regularize plays a crucial part in retaining TSB previous knowledge. To study this further, the A hyperparameter is ablated. This experiment examines how strong the EWC regularizer influence should be on TSB training. The ablation experiment is the same as the one presented in Figure 2b, for various A hyperparameter values. The ablation study results are shown and are quite expected. The small \u5165 values cause the first tasks learned to be forgotten. Thus the average Top-1 Accuracy decreases. On the other hand, the highest A values constrain the gradients during learning. Therefore,"}, {"title": "V. CONCLUSION", "content": "In conclusion, this work introduces FCL-ViT, a robust Vision Transformer-based framework that addresses the unique challenges of Continual Learning (CL) through dynamic task- specific attention mechanisms. By employing Tunable self- Attention Blocks (TABs) and Task Specific Blocks (TSBs), FCL-ViT effectively adapts to new tasks while preserving previously learned information, thereby reducing catastrophic forgetting. The dual-phase structure leverages both general and task-specific attention, which allows for targeted and efficient learning across diverse datasets, as demonstrated through extensive evaluations on CIFAR100 and Blaze datasets.\nThe results consistently highlight FCL-ViT's superior perfor- mance in comparison to established CL methods, showcasing its resilience and adaptability across multiple task splits. This resilience underscores the effectiveness of the feedback mechanism and EWC-based regularization in maintaining prior knowledge. Moreover, the model's capability to operate without rehearsal memory marks a significant advancement in CL, particularly in real-world scenarios, e.g. in embedded computing, where memory constraints are critical."}]}