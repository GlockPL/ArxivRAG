{"title": "Disentangled Generative Graph Representation Learning", "authors": ["Xinyue Hu", "Zhibin Duan", "Xinyang Liu", "Yuxin Li", "Bo Chen", "Mingyuan Zhou"], "abstract": "Recently, generative graph models have shown promising results in learning graph representations through self-supervised methods. However, most existing generative graph representation learning (GRL) approaches rely on random masking across the entire graph, which overlooks the entanglement of learned representations. This oversight results in non-robustness and a lack of explainability. Furthermore, disentangling the learned representations remains a significant challenge and has not been sufficiently explored in GRL research. Based on these insights, this paper introduces DiGGR (Disentangled Generative Graph Representation Learning), a self-supervised learning framework. DiGGR aims to learn latent disentangled factors and utilizes them to guide graph mask modeling, thereby enhancing the disentanglement of learned representations and enabling end-to-end joint learning. Extensive experiments on 11 public datasets for two different graph learning tasks demonstrate that DiGGR consistently outperforms many previous self-supervised methods, verifying the effectiveness of the proposed approach.", "sections": [{"title": "1 Introduction", "content": "Self-supervised learning (SSL) has received much attention due to its appealing capacity for learning data representation without label supervision. While contrastive SSL approaches are becoming increasingly utilized on images [Chen et al., 2020] and graphs [You et al., 2020], generative SSL has been gaining significance, driven by groundbreaking practices such as BERT for language [Devlin et al., 2018], BEiT [Bao et al., 2021], and MAE [He et al., 2022a] for images. Along this line, there is a growing interest in constructing generative SSL models for other modalities, such as graph masked autoencoders (GMAE). Generally, the fundamental concept of GMAE [Tan et al., 2022] is to utilize an autoencoder architecture to reconstruct input node features, structures, or both, which are randomly masked before the encoding step. Recently, various well-designed GMAEs have emerged, achieving remarkable results in both node classification and graph classification [Hou et al., 2022, Tu et al., 2023, Tian et al., 2023]."}, {"title": "2 Related works", "content": "Graph Self-Supervised Learning: Graph SSL has achieved remarkable success in addressing label scarcity in real-world network data, mainly consisting of contrastive and generative methods. Contrastive methods, includes feature-oriented approaches[Hu et al., 2019, Zhu et al., 2020, Veli\u010dkovi\u0107 et al., 2018], proximity-oriented techniques [Hassani and Khasahmadi, 2020, You et al., 2020], and graph-sampling-based methods [Qiu et al., 2020]. A common limitation across these approaches is their heavy reliance on the design of pretext tasks and augmentation techniques. Compared to contrastive methods, generative methods are generally simpler to implement. Recently, to tackle the challenge of overemphasizing neighborhood information at the expense of structural information [Hassani and Khasahmadi, 2020, Veli\u010dkovi\u0107 et al., 2018], the Graph Masked Autoencoder (GMAE) has been proposed. It applies a masking strategy to graph structure [Li et al., 2023a], node attributes [Hou et al., 2022], or both [Tian et al., 2023] for representation learning. Unlike most GMAEs, which employ random mask strategies, this paper builds disentangled mask strategies.\nDisentangled Graph Learning: Disentangled representation learning aims to discover and isolate the fundamental explanatory factors inherent in the data [Bengio et al., 2013]. Existing efforts in disentangled representation learning have primarily focused on computer vision [Higgins et al., 2017, Jiang et al., 2020]. Recently, there has been a surge of interest in applying these techniques to graph-structured data [Li et al., 2021, Ma et al., 2019, Mercatali et al., 2022, Mo et al., 2023]. For example, DisenGCN [Ma et al., 2019] utilizes an attention-based methodology to discriminate between distinct latent factors, enhancing the representation of each node to more accurately reflect its features across multiple dimensions. DGCL [Li et al., 2021] suggests learning disentangled graph-level representations through self-supervision, ensuring that the factorized representations independently capture expressive information from various latent factors. Despite the excellent results achieved by the aforementioned methods on various tasks, these methods are difficult to converge in generative graph SSL, as we demonstrated in the experiment of Table.3. Therefore, this paper proposes a disentangled-guided framework for generative graph representation learning, capable of learning disentangled representations in an end-to-end self-supervised manner."}, {"title": "3 Proposed Method", "content": "In this section, we propose DiGGR (Disentangled Generative Graph Representation Learning) for self-supervised graph representation learning with mask modeling. The framework was depicted in Figure 2, comprises three primary components: Latent Factor Learning (Section 3.2), Graph Factorization (Section 3.2) and Disentangled Graph Masked autoencder (Section 3.3). Before elaborating on them, we first show some notations."}, {"title": "3.1 Preliminaries", "content": "A graph G can be represented as a multi-tuple $G = {V, A, X}$ with N nodes and M edges, where $|V| = N$ is the node set, $|A| = M$ is the edge set, and $X \\in R^{N \\times L}$ is the feature matrix for N nodes with L dimensional feature vector. The topology structure of graph G can be found in its adjacency matrix $A \\in R^{N \\times N}$. $z\\in R^{N \\times K}$ is the latent disentangled factor matrix, and K is the predefined factor number. Since we aim to obtain the z to guide the mask modeling, we first utilize a probabilistic graph generation model to factorize the graph before employing the mask mechanism. Given the graph G, it is factorized into ${G_1, G_2, ..., G_K }$, and each factor-specific graph $G_k$ consists of its factor-specific edges $A^{(k)}$, node set $V^{(k)}$ and node feature matrix $X^{(k)}$. Other notations will be elucidated as they are employed."}, {"title": "3.2 Latent Factor Learning", "content": "In this subsection, we describe the latent factor learning method. In this phase, our objective is to derive factor-specific node sets ${V^{(1)}, V^{(2)}, ..., V^{(K)} }$ and adjacency matrices ${A^{(1)}, A^{(2)}, ..., A^{(K)} }$, serving as basic unit of the graph to guide the subsequent masking. The specific approach involves modeling the distribution of nodes and edges, utilizing the generative process developed in EPM [Zhou, 2015]. The generative process of EPM under the Bernoulli-Poisson link [Zhou, 2015, Duan et al., 2021] can be described as:\n$M_{uv} \\sim Poisson(\\sum_{k=1}^{K}(\\gamma_k z_{uk} z_{vk})), z_{uk} \\sim Gamma (\\alpha, \\beta), u, v \\in [1, N]$\nwhere K is the predefined number of latent factors, and u and v are the indexes of the nodes. Here, $M_{uv}$ is the latent count variable between node u and v; $\\gamma_k$ is a positive factor activation level indicator, which measures the node interaction frequency via factor k; $z_{uk}$ is a positive latent variable for node u, which measures how strongly node u is affiliated with factor k. The prior distribution of latent factor variable $z_{uk}$ is set to Gamma distribution, where $\\alpha$ and $\\beta$ are normally set to 1. Therefore, the intuitive explanation for this generative process is that, with $z_{uk}$ and $z_{vk}$ measuring how strongly node u and v are affiliated with the k-th factor, respectively, the product $\\gamma_k z_{uk} z_{vk}$ measures how strongly nodes u and v are connected due to their affiliations with the k-th factor.\nNode Factorization: Equation 1 can be further augmented as follows:\n$M_{uv} = \\sum_{k} M_{ukv}, M_{ukv} \\sim Poisson (\\gamma_k z_{uk} z_{vk})$\nwhere $M_{ukv}$ represents how often nodes u and v interact due to their affiliations with the k-th factor. To represent how often node u is affiliated with the k-th factor, we further introduce the latent count $M_{uk\u00b7} = \\sum_{v \\neq u} M_{ukv}$. Then, we can soft assign node u to multiple factors in ${k : M_{uk\u00b7} } \\ge 1$, or hard assign node u to a single factor using $arg \\max_{k} (M_{uk\u00b7})$. However, our experiments show that soft assignment method results in significant overlap among node sets from different factor group, diminishing the distinctiveness. Note that previous study addressed a similar issue by selecting the top-k most attended regions [Kakogeorgiou et al., 2022]. Thus, we choose the hard assign strategy to factorize the graph node set V graph into factor-specific node sets ${V^{(1)}, V^{(2)},..., V^{(K)}}$."}, {"title": "Edge Factorization:", "content": "To create factor-specific edges $A^{(k)}$ for a factor-specific node set $V^{(k)}$, a straightforward method involves removing all external nodes connected to other factor groups. This can be defined as:\n$A_{uv}^{(k)} = \\begin{cases}\nA_{uv}, \\forall u, v \\in V^{(k)}; u, v \\in [1, N]; \\\\\n0, \\exists u, v \\notin V^{(k)}; u, v \\in [1, N] .\n\\end{cases}$\nBesides, the global graph edge A can also be factorized into positive-weighted edges [He et al., 2022b] for each latent factor as:\n$A_{uv}^{(k)} = A_{uv} \\frac{exp (\\gamma_k z_{uk}z_{vk})}{\\sum_{k'} exp (\\gamma_{k'} z_{uk'} z_{vk'})}; k\\in [1, K], u, v \\in [1, N]$\nApplying Equation 4 to all pairs of nodes yields weighted adjacency matrices ${A^{(k)}}_{k=1}^{K}$, with $A^{(k)}$ corresponding to latent factor $z_k$. Note that $A^{(k)}$ has the same dimension as A and Equation 4 presents a trainable weight for each edge, which can be jointly optimized through network training, showcasing an advantage over Equation 3 in this aspect. Therefore, we apply Equation 4 for edge factorization.\nVariational Inference: The latent factor variable z determines the quality of node and edge factor-ization, so we need to approximate its posterior distribution. Denoting $z_u = (z_{u1}, ..., z_{uK}), z_u \\in R$, which measures how strongly node u is affiliated with all the K latent factors, we adopt a Weibull variational graph encoder [Zhang et al., 2018, He et al., 2022b]:\n$q(z_u | A, X) = Weibull(\\kappa_u, \\lambda_u), (\\kappa_u, \\lambda_u) = GNNEPM(A, X), u \\in [1, N]$\nwhere GNNEPM(\u00b7) stands for graph neural networks, and we select a two-layer Graph Convolution Networks (i.e., GCN [Kipf and Welling, 2016a]) for our models; $\\kappa_u, \\lambda_u \\in R$ are the shape and scale parameters of the variational Weibull distribution, respectively. The latent variable zu can be conveniently reparameterized as:\n$z_u = \\lambda_u (- ln(1 - \\epsilon))^{1/\\kappa_u}, \\epsilon \\sim Uniform(0, 1)$.\nThe optimization objective of latent factor learning phase can be achieved by maximizing the evidence lower bound (ELBO) of the log marginal likelihood of edge log p(A), which can be computed as:\n$\\mathcal{L}_{z} = E_{q(Z |A,X)}[lnp (A | Z)] - \\sum_{u=1}^{N}E_{q(z_u |A,X)} [In \\frac{q(z_u | A, X)}{p(z_u)}]$\nwhere the first term is the expected log-likelihood or reconstruction error of edge, and the second term is the Kullback\u2013Leibler (KL) divergence that constrains q(zu) to be close to its prior p(zu). The analytical expression for the KL divergence and the straightforward reparameterization of the Weibull distribution simplify the gradient estimation of the ELBO concerning the decoder parameters and other parameters in the inference network."}, {"title": "3.3 Disentangled Grpah Masked Autoencoder", "content": "With the latent factor learning phase discussed in 3.2, the graph can be factorized into a series of factor-specific subgraphs ${G_1, G_2, ..., G_K }$ via the latent factor z. To incorporate the disentangled information encapsulated in z into the graph masked autoencoder, we proposed Disentangled Graph Masked Autoencoder in this section. Specifically, this section will first introduce the latent factor-wise GMAE and the graph-level GMAE."}, {"title": "3.3.1 Latent Factor-wise Grpah Masked Autoencoder", "content": "To capture disentangled patterns within the latent factor z, for each latent subgraph $G_k = (V^{(k)}, A^{(k)}, X^{(k)})$, the latent factor-wise GMAE can be described as:\n$H_d^{(k)} = GNN_{enc}(A^{(k)}, \\tilde{X}^{(k)}), \\tilde{X}_d = GNN_{dec}(A, H_d).$"}, {"title": "3.4 Joint Training and Inference", "content": "Benefiting from the effective variational inference method, the proposed latent factor learning and dsientangled graph masked autoencoder can be jointly trained in one framework. We combine the aforementioned losses with three mixing coefficient $\\lambda_D, \\lambda_G$ and $\\lambda_Z$ during training, and the loss for joint training can be written as\n$\\mathcal{L} = \\lambda_D \u00b7 L_D + \\lambda_g \u00b7 L_G + \\lambda_Z \u00b7 L_Z.$\nSince Weibull distributions have easy reparameterization functions, these parameters can be jointly trained by stochastic gradient descent with low-variance gradient estimation. We summarize the"}, {"title": "4 Experiments", "content": "We compare the proposed self-supervised framework DiGGR against related baselines on two fundamental tasks: unsupervised representation learning on node classification and graph classification. We evaluate DiGGR on 11 benchmarks. For node classification, we use 3 citation networks (Cora, Citeseer, Pubmed [Yang et al., 2016]), and protein-protein interaction networks (PPI) [Hamilton et al., 2017]. For graph classification, we use 3 bioinformatics datasets (MUTAG, NCI1, PROTEINS) and 4 social network datasets (IMDB-BINARY, IMDB-MULTI, REDDIT-BINARY and COLLAB). The specific information of the dataset and the hyperparameters used by the network are listed in the Appendix A.2 in table 5 and 6. We also provide the detailed experiment setup in Appendix A.2 for node classification (4.1) and graph classification (4.2)"}, {"title": "4.1 Node Classification", "content": "The baseline models for node classification can be divided into three categories: i) supervised methods, including GCN [Kipf and Welling, 2016a], DisenGCN[Ma et al., 2019], VEPM[He et al., 2022b] and GAT [Velickovic et al., 2017]; ii) contrastive learning methods, including MVGRL [Hassani and Khasahmadi, 2020], InfoGCL [Xu et al., 2021], DGI [Veli\u010dkovi\u0107 et al., 2018], GRACE [Zhu et al., 2020], BGRL [Thakoor et al., 2021] and CCA-SSG [Zhang et al., 2021]; iii) generative learning methods, including GraphMAE [Hou et al., 2022], GraphMAE2[Hou et al., 2023], Bandana[Zhao et al., 2024], GiGaMAE[Shi et al., 2023], SeeGera[Li et al., 2023b], GAE and VGAE [Kipf and Welling, 2016b]. The node classification results were listed in Table 1. DiGGR demonstrates competitive results on the provided dataset, achieving results comparable to those of supervised methods."}, {"title": "4.2 Graph Classification", "content": "Baseline Models We categorized the baseline models into four groups: i) supervised methods, including GIN [Xu et al., 2018], DiffPool[Ying et al., 2018] and VEPM[He et al., 2022b]; ii) classical graph kernel methods: Weisfeiler-Lehman sub-tree kernel (WL) [Shervashidze et al., 2011] and deep graph kernel (DGK) [Yanardag and Vishwanathan, 2015]; iii) contrastive learning methods, including GCC [Qiu et al., 2020], graph2vec [Narayanan et al., 2017], Infograph [Sun et al., 2019], GraphCL [You et al., 2020], JOAO [You et al., 2021], MVGRL [Hassani and Khasahmadi, 2020], and InfoGCL [Xu et al., 2021]; iv) generative learning methods, including graph2vec [Narayanan et al., 2017], sub2vec [Adhikari et al., 2018], node2vec [Grover and Leskovec, 2016], GraphMAE [Hou et al., 2022], GraphMAE2[Hou et al., 2023], GAE and VGAE [Kipf and Welling, 2016b]. Per graph classification research tradition, we report results from previous papers if available."}, {"title": "4.3 Exploratory Studies", "content": "Visualizing latent representations To examine the influence of the learned latent factor on classification results, we visualized the latent disentangled factor z, which reflects the node-factor affiliation, and the hidden representation H used for classification. MUTAG is selected as the representative for classification benchmarks. We encodes the representations into 2-D space via t-SNE [Van der Maaten and Hinton, 2008]. The result is shown in Figure 3(a), where each node is colored according to its node labels. The clusters in Figure 3(a) still exhibit differentiation in the absence of label supervision, suggesting that z obtained through unsupervised learning can enhance node information and offer a guidance for the mask modeling. We then visualize the hidden representation used for classification tasks, and color each node according to the latent factor to which it belongs. The results are depicted in Figure 3(b), showcasing separability among different color clusters. This illustrates the model's ability to extract information from the latent factor, thereby enhancing the quality of the learned representations.\nTask-relevant factors To assess the statistical correlation between the learned latent factor and the task, we follow the approach in [He et al., 2022b] and compute the Normalized Mutual Information (NMI) between the nodes in the factor label and the actual node labels. NMI is a metric that ranges from 0 to 1, where higher values signify more robust statistical dependencies between two random variables. In the experiment, we utilized the MUTAG dataset, comprising 7 distinct node types, and the NMI value we obtained was 0.5458. These results highlight that the latent factors obtained through self-supervised training are meaningful for the task, enhancing the correlation between the inferred latent factors and the task.\nDisentangled representations To assess DiGGR's capability to disentangle the learned representation for downstream task, we provide a qualitative evaluation by plotting the correlation of the node representation in Figure 4. The figure shows the absolute values of the correlation between the elements of 512-dimensional graph representation and representation obtained from GraphMAE and DiGGR, respectively. From the results, we can see that the representation produced by GraphMAE exhibits entanglement, whereas DiGGR's representation displays a overall block-level pattern, indicating that DiGGR can capture mutually exclusive information in the graph and disentangle the hidden representation to some extent. Results for more datasets can be found in Appendix A.3.\nWhy DiGGR works better: We provide a theoretical explanation for DiGGR's performance improvement in the Appendix A.5. Additionally, we conduct quantitative experiments here to"}, {"title": "5 Conclusions", "content": "In this paper, we propose DiGGR (Disentangled Generative Graph Representation Learning), designed to achieve disentangled representations in graph masked autoencoders by leveraging latent disentangled factors. In particular, we achieve this by two steps: 1) We utilize a probabilistic graph generation model to factorize the graph via the learned disentangled latent factor; 2) We develop a Disentangled Graph Masked Autoencoder framework, with the aim of integrating the disentangled information into the representation learning of Graph Masked Autoencoders. Experiments demonstrate that our model can acquire disentangled representations, and achieve favorable results on downstream tasks."}, {"title": "A.6 Broader Impacts", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."}, {"title": "A.7 Limitations", "content": "Despite the promising experimental justifications, our work might potentially suffer from limitation: Although the complexity of the model is discussed in Section 3.4, and it is comparable to previously published work, extending DiGGR to extremely large graph datasets remains challenging at this stage due to the incorporation of an additional probabilistic model into the generative graph framework. One potential solution to this problem could be utilizing PPR-Nibble [Andersen et al., 2006] for efficient implementation, a method that has proven effective in some graph generative models [Hou et al., 2023]. This approach will be pursued in our future work."}, {"title": "A.1 Ablation Study", "content": ""}, {"title": "A.1.1 Number of factors", "content": "One of the crucial hyperparameters in DiGGR is the number of latent factors, denoted as K. When K = 1 DiGGR degenerates into ordinary GMAE, only performing random masking over the entire input graph on the nodes. The influence of tuning K is illustrated in Figure 5. Given the relatively small size of the graphs in the dataset, the number of meaningful latent disentangled factor z is not expected to be very large. The optimal number of z that maximizes performance tends to be concentrated in the range of 2-4."}, {"title": "A.1.2 Representation for downstream tasks", "content": "We investigate the impact of various combinations of representation levels on downstream tasks. As illustrated in Table 4, for the node classification task, both Ha and Hg are required, i.e., concatenating them in feature dimension, whereas for the graph classification task, Ha alone is sufficient. This difference may be due to the former not utilizing pooling operations, while the latter does. Specifically, the graph pooling operation aggregates information from all nodes, providing a comprehensive view of the entire graph structure. Thus, in node classification, where the node representation has not undergone pooling, a graph-level representation (Hg) is more critical. In contrast, in graph classification, the node representation undergoes pooling, making disentangled information Ha more effective."}, {"title": "A.2 Implementation Details", "content": "Environment All experiments are conducted on Linux servers equipped with an 12th Gen Intel(R) Core(TM) i7-12700, 256GB RAM and a NVIDIA 3090 GPU. Models of node and graph classification are implemented in PyTorch version 1.12.1, scikit-learn version 1.0.2 and Python 3.7.\nExperiment Setup for Node Classification The node classification task involves predicting the unknown node labels in networks. Cora, Citeseer, and Pubmed are employed for transductive learning, whereas PPI follows the inductive setup outlined in GraphSage [Hamilton et al., 2017]. For evaluation,"}, {"title": "A.3 Disentangled Representations Visualization", "content": "We chose PROTEINS and IMDB-MULTI as representatives of the graph classification dataset, and followed the same methodology as in Section 4.3 to visualize their representation correlation matrices on GraphMAE, and community representation correlation matrices on DiGGR, respectively. The feature dimensions of PROTEINS and IMDB-MULTI are both 512 dimensions, and the number of communities is set to 4."}, {"title": "A.4 Training Algorithm", "content": ""}, {"title": "A.5 Theoretical Approve of Why DiGGR Works", "content": "DiGGR is built on an graph autoencoder (GAE)-based framework. Recent studies [Zhang et al., 2022, Li et al., 2023a] have demonstrated a direct connection between GAE and contrastive learning through a specific form of the objective function. The loss function can be rewritten as follows:\n$\\mathcal{L^{+}} = \\frac{1}{\\varepsilon^{+}}\\sum_{(u,v) \\in V^{+}} log f_{dec}(h_u, h_v)$\n$\\mathcal{L^{-}} = \\frac{1}{\\varepsilon^{-}}\\sum_{(u^{'},v^{'}) \\in V^{-}} log(1 - f_{dec}(h_{u^{'}}, h_{v^{'}}))$\n$\\mathcal{L}_{GAE} = -(\\mathcal{L^{+}} + \\mathcal{L^{-}})$\nwhere $h_u$ and $h_v$ are the node representations of node u and node v obtained from an encoder $f_{enc}$ respectively (eg., a GNN); $\\varepsilon^+$ is a set of positive edges while $\\varepsilon^-$ is a set of negative edges sampled from graph, and $f_{dec}$ is a decoder; Typically, $\\varepsilon^+ = \\varepsilon$.\nBuilding on recent advances in information-theoretic approaches to contrastive learning [Tsai et al., 2020, Tian et al., 2020], a recent study [Li et al., 2023a] suggests that for SSL pretraining to succeed in downstream tasks, task-irrelevant information must be reasonably controlled. Therefore, the following proposition is put forward:\nThe task irrelevant information $I (U; V\\mathcal{T})$ of GAE can be lower bounded with:\n$I(U; V\\mathcal{T}) \\geq \\frac{(E[N_k])^2}{N_k}$.\nminimizing the aforementioned $L_{GAE}$ is in population equivalent to maximizing the mutual infor-mation between the k-hop subgraphs of adjacent nodes, and the redundancy of GAE scales almost linearly with the size of overlapping subgraphs.\nAccording to this lower bound, we need to reduce the task-irrelevant redundancy to design a better graph SSL methods. In DiGGR, we first factorize the input graph based on latent factor learning before feeding it into the masked autoencoder. Take Fig. 7 from the PDF in global rebuttal as an example. Nodes a and b have overlapping 1-hop subgraphs. However, after graph factorization, the"}]}