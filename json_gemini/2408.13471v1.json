{"title": "Disentangled Generative Graph\nRepresentation Learning", "authors": ["Xinyue Hu", "Zhibin Duan", "Xinyang Liu", "Yuxin Li", "Bo Chen", "Mingyuan Zhou"], "abstract": "Recently, generative graph models have shown promising results in learning graph\nrepresentations through self-supervised methods. However, most existing gener-\native graph representation learning (GRL) approaches rely on random masking\nacross the entire graph, which overlooks the entanglement of learned representa-\ntions. This oversight results in non-robustness and a lack of explainability. Fur-\nthermore, disentangling the learned representations remains a significant challenge\nand has not been sufficiently explored in GRL research. Based on these insights,\nthis paper introduces DiGGR (Disentangled Generative Graph Representation\nLearning), a self-supervised learning framework. DiGGR aims to learn latent\ndisentangled factors and utilizes them to guide graph mask modeling, thereby\nenhancing the disentanglement of learned representations and enabling end-to-end\njoint learning. Extensive experiments on 11 public datasets for two different graph\nlearning tasks demonstrate that DiGGR consistently outperforms many previous\nself-supervised methods, verifying the effectiveness of the proposed approach.", "sections": [{"title": "1 Introduction", "content": "Self-supervised learning (SSL) has received much attention due to its appealing capacity for learning\ndata representation without label supervision. While contrastive SSL approaches are becoming\nincreasingly utilized on images [Chen et al., 2020] and graphs [You et al., 2020], generative SSL has\nbeen gaining significance, driven by groundbreaking practices such as BERT for language [Devlin\net al., 2018], BEiT [Bao et al., 2021], and MAE [He et al., 2022a] for images. Along this line, there is\na growing interest in constructing generative SSL models for other modalities, such as graph masked\nautoencoders (GMAE). Generally, the fundamental concept of GMAE [Tan et al., 2022] is to utilize\nan autoencoder architecture to reconstruct input node features, structures, or both, which are randomly\nmasked before the encoding step. Recently, various well-designed GMAEs have emerged, achieving\nremarkable results in both node classification and graph classification [Hou et al., 2022, Tu et al.,\n2023, Tian et al., 2023]."}, {"title": "2 Related works", "content": "Graph Self-Supervised Learning: Graph SSL has achieved remarkable success in addressing label\nscarcity in real-world network data, mainly consisting of contrastive and generative methods. Con-\ntrastive methods, includes feature-oriented approaches[Hu et al., 2019, Zhu et al., 2020, Veli\u010dkovi\u0107\net al., 2018], proximity-oriented techniques [Hassani and Khasahmadi, 2020, You et al., 2020], and\ngraph-sampling-based methods [Qiu et al., 2020]. A common limitation across these approaches\nis their heavy reliance on the design of pretext tasks and augmentation techniques. Compared to\ncontrastive methods, generative methods are generally simpler to implement. Recently, to tackle the\nchallenge of overemphasizing neighborhood information at the expense of structural information\n[Hassani and Khasahmadi, 2020, Veli\u010dkovi\u0107 et al., 2018], the Graph Masked Autoencoder (GMAE)\nhas been proposed. It applies a masking strategy to graph structure [Li et al., 2023a], node attributes\n[Hou et al., 2022], or both [Tian et al., 2023] for representation learning. Unlike most GMAEs, which\nemploy random mask strategies, this paper builds disentangled mask strategies.\nDisentangled Graph Learning: Disentangled representation learning aims to discover and isolate\nthe fundamental explanatory factors inherent in the data [Bengio et al., 2013]. Existing efforts in\ndisentangled representation learning have primarily focused on computer vision [Higgins et al.,\n2017, Jiang et al., 2020]. Recently, there has been a surge of interest in applying these techniques\nto graph-structured data [Li et al., 2021, Ma et al., 2019, Mercatali et al., 2022, Mo et al., 2023].\nFor example, DisenGCN [Ma et al., 2019] utilizes an attention-based methodology to discriminate\nbetween distinct latent factors, enhancing the representation of each node to more accurately reflect\nits features across multiple dimensions. DGCL [Li et al., 2021] suggests learning disentangled\ngraph-level representations through self-supervision, ensuring that the factorized representations\nindependently capture expressive information from various latent factors. Despite the excellent results\nachieved by the aforementioned methods on various tasks, these methods are difficult to converge\nin generative graph SSL, as we demonstrated in the experiment of Table.3. Therefore, this paper\nproposes a disentangled-guided framework for generative graph representation learning, capable of\nlearning disentangled representations in an end-to-end self-supervised manner."}, {"title": "3 Proposed Method", "content": "In this section, we propose DiGGR (Disentangled Generative Graph Representation Learning) for\nself-supervised graph representation learning with mask modeling. The framework was depicted\nin Figure 2, comprises three primary components: Latent Factor Learning (Section 3.2), Graph\nFactorization (Section 3.2) and Disentangled Graph Masked autoencder (Section 3.3). Before\nelaborating on them, we first show some notations."}, {"title": "3.1 Preliminaries", "content": "A graph G can be represented as a multi-tuple $G = {V, A, X}$ with N nodes and M edges, where\n$|V| = N$ is the node set, $|A| = M$ is the edge set, and $X \\in R^{N \\times L}$ is the feature matrix for N\nnodes with L dimensional feature vector. The topology structure of graph G can be found in its\nadjacency matrix $A \\in R^{N \\times N}$. $z\\in R^{N \\times K}$ is the latent disentangled factor matrix, and K is the\npredefined factor number. Since we aim to obtain the z to guide the mask modeling, we first utilize a\nprobabilistic graph generation model to factorize the graph before employing the mask mechanism.\nGiven the graph G, it is factorized into ${G_1, G_2, ..., G_K }$, and each factor-specific graph $G_k$ consists\nof its factor-specific edges $A^{(k)}$, node set $V^{(k)}$ and node feature matrix $X^{(k)}$. Other notations will be\nelucidated as they are employed."}, {"title": "3.2 Latent Factor Learning", "content": "In this subsection, we describe the latent factor learning method. In this phase, our objective is to\nderive factor-specific node sets ${V^{(1)}, V^{(2)}, ..., V^{(K)} }$ and adjacency matrices ${A^{(1)}, A^{(2)}, ..., A^{(K)} }$,\nserving as basic unit of the graph to guide the subsequent masking. The specific approach involves\nmodeling the distribution of nodes and edges, utilizing the generative process developed in EPM\n[Zhou, 2015]. The generative process of EPM under the Bernoulli-Poisson link [Zhou, 2015, Duan\net al., 2021] can be described as:\n$M_{uv} \\sim Poisson(\\sum_{k=1}^{K} \\Upsilon_k z_{uk} z_{vk}), z_{uk} \\sim Gamma (\\alpha, \\beta), u, v \\in [1, N]$\nwhere K is the predefined number of latent factors, and u and v are the indexes of the nodes. Here,\n$M_{uv}$ is the latent count variable between node u and v; $\\Upsilon_k$ is a positive factor activation level indicator,\nwhich measures the node interaction frequency via factor k; $z_{uk}$ is a positive latent variable for node\nu, which measures how strongly node u is affiliated with factor k. The prior distribution of latent\nfactor variable $z_{uk}$ is set to Gamma distribution, where $\\alpha$ and $\\beta$ are normally set to 1. Therefore, the\nintuitive explanation for this generative process is that, with $z_{uk}$ and $z_{vk}$ measuring how strongly\nnode u and v are affiliated with the k-th factor, respectively, the product $\\Upsilon_k z_{uk} z_{vk}$ measures how\nstrongly nodes u and v are connected due to their affiliations with the k-th factor.\nNode Factorization: Equation 1 can be further augmented as follows:\n$M_{uv} = \\sum_{k} M_{ukv}, M_{ukv} \\sim Poisson (\\Upsilon_k z_{uk} z_{vk})$\nwhere $M_{ukv}$ represents how often nodes u and v interact due to their affiliations with the k-th factor.\nTo represent how often node u is affiliated with the k-th factor, we further introduce the latent count\n$M_{uk\\cdot} = \\sum_{v\\neq v} M_{ukv}$. Then, we can soft assign node u to multiple factors in {$k : M_{uk\\cdot} \\geq 1$}, or\nhard assign node u to a single factor using $\\arg \\max_{k} (M_{uk\\cdot})$. However, our experiments show that\nsoft assignment method results in significant overlap among node sets from different factor group,\ndiminishing the distinctiveness. Note that previous study addressed a similar issue by selecting the\ntop-k most attended regions [Kakogeorgiou et al., 2022]. Thus, we choose the hard assign strategy to\nfactorize the graph node set V graph into factor-specific node sets ${V^{(1)}, V^{(2)},..., V^{(K)}}$."}, {"title": "Edge Factorization", "content": "To create factor-specific edges $A^{(k)}$ for a factor-specific node set $V^{(k)}$, a\nstraightforward method involves removing all external nodes connected to other factor groups. This\ncan be defined as:\n$A^{(k)} =\\begin{cases}A_{uv}, \\forall u, v \\in V^{(k)}; u, v \\in [1, N];\\\\0, \\exists u, v \\notin V^{(k)}; u, v \\in [1, N] .\\end{cases}$\nBesides, the global graph edge A can also be factorized into positive-weighted edges [He et al.,\n2022b] for each latent factor as:\n$A^{(k)}_{uv} = A_{uv} \\frac{exp (\\Upsilon_k z_{uk}z_{vk})}{\\sum_{k'} exp (\\Upsilon_{k'} z_{uk'} z_{vk'})}; k\\in [1, K], u, v \\in [1, N]$\nApplying Equation 4 to all pairs of nodes yields weighted adjacency matrices ${A^{(k)}}_{k=1}^K$, with $A^{(k)}$\ncorresponding to latent factor $z_k$. Note that $A^{(k)}$ has the same dimension as A and Equation 4\npresents a trainable weight for each edge, which can be jointly optimized through network training,\nshowcasing an advantage over Equation 3 in this aspect. Therefore, we apply Equation 4 for edge\nfactorization.\nVariational Inference: The latent factor variable z determines the quality of node and edge factor-\nization, so we need to approximate its posterior distribution. Denoting $z_u = (z_{u1}, ..., z_{uK}), z_u \\in R^K$,\nwhich measures how strongly node u is affiliated with all the K latent factors, we adopt a Weibull\nvariational graph encoder [Zhang et al., 2018, He et al., 2022b]:\n$q(z_u | A, X) = Weibull(k_u, d_u), (k_u, d_u) = GNN_{EPM}(A, X), u \\in [1, N]$\nwhere $GNN_{EPM}(\\cdot)$ stands for graph neural networks, and we select a two-layer Graph Convolution\nNetworks (i.e., GCN [Kipf and Welling, 2016a]) for our models; $k_u, d_u \\in R$ are the shape and\nscale parameters of the variational Weibull distribution, respectively. The latent variable $z_u$ can be\nconveniently reparameterized as:\n$z_u = d_u(-ln(1 - \\varepsilon))^{1/k_u}, \\varepsilon \\sim Uniform(0, 1)$.\nThe optimization objective of latent factor learning phase can be achieved by maximizing the evidence\nlower bound (ELBO) of the log marginal likelihood of edge log p(A), which can be computed as:\n$\\mathcal{L}_z = E_{q(Z |A,x)} [ln p (A | Z)] - \\sum_{u=1}^{N} E_{q(z_u |A,X)} \\left[ln \\frac{q(z_u | A, X)}{p(z_u)} \\right]$\nwhere the first term is the expected log-likelihood or reconstruction error of edge, and the second\nterm is the Kullback\u2013Leibler (KL) divergence that constrains q(zu) to be close to its prior p(zu). The\nanalytical expression for the KL divergence and the straightforward reparameterization of the Weibull\ndistribution simplify the gradient estimation of the ELBO concerning the decoder parameters and\nother parameters in the inference network."}, {"title": "3.3 Disentangled Grpah Masked Autoencoder", "content": "With the latent factor learning phase discussed in 3.2, the graph can be factorized into a series of\nfactor-specific subgraphs ${G_1, G_2, ..., G_K }$ via the latent factor z. To incorporate the disentangled\ninformation encapsulated in z into the graph masked autoencoder, we proposed Disentangled Graph\nMasked Autoencoder in this section. Specifically, this section will first introduce the latent factor-wise\nGMAE and the graph-level GMAE."}, {"title": "3.3.1 Latent Factor-wise Grpah Masked Autoencoder", "content": "To capture disentangled patterns within the latent factor z, for each latent subgraph\n$G_k = (V^{(k)}, A^{(k)}, X^{(k)})$, the latent factor-wise GMAE can be described as:\n$H^{(k)}_d = GNN_{enc}(A^{(k)}, \\tilde{X}^{(k)}), \\tilde{X}^{(k)}_d = GNN_{dec}(A, H_d)$"}, {"title": "3.4 Joint Training and Inference", "content": "Benefiting from the effective variational inference method, the proposed latent factor learning and\ndisentangled graph masked autoencoder can be jointly trained in one framework. We combine the\naforementioned losses with three mixing coefficient $a_d$, $a_g$ and $a_z$ during training, and the loss for\njoint training can be written as\n$\\mathcal{L} = a_d \\cdot L_D + a_g \\cdot L_G + a_z \\cdot L_z$.\nSince Weibull distributions have easy reparameterization functions, these parameters can be jointly\ntrained by stochastic gradient descent with low-variance gradient estimation. We summarize the"}, {"title": "4 Experiments", "content": "We compare the proposed self-supervised framework DiGGR against related baselines on two funda-\nmental tasks: unsupervised representation learning on node classification and graph classification.\nWe evaluate DiGGR on 11 benchmarks. For node classification, we use 3 citation networks (Cora,\nCiteseer, Pubmed [Yang et al., 2016]), and protein-protein interaction networks (PPI) [Hamilton et al.,\n2017]. For graph classification, we use 3 bioinformatics datasets (MUTAG, NCI1, PROTEINS) and 4\nsocial network datasets (IMDB-BINARY, IMDB-MULTI, REDDIT-BINARY and COLLAB). The\nspecific information of the dataset and the hyperparameters used by the network are listed in the\nAppendix A.2 in table 5 and 6. We also provide the detailed experiment setup in Appendix A.2 for\nnode classification (4.1) and graph classification (4.2)"}, {"title": "4.1 Node Classification", "content": "The baseline models for node classification can be divided into three categories: i) supervised methods,\nincluding GCN [Kipf and Welling, 2016a], DisenGCN[Ma et al., 2019], VEPM[He et al., 2022b]\nand GAT [Velickovic et al., 2017]; ii) contrastive learning methods, including MVGRL [Hassani and\nKhasahmadi, 2020], InfoGCL [Xu et al., 2021], DGI [Veli\u010dkovi\u0107 et al., 2018], GRACE [Zhu et al.,\n2020], BGRL [Thakoor et al., 2021] and CCA-SSG [Zhang et al., 2021]; iii) generative learning\nmethods, including GraphMAE [Hou et al., 2022], GraphMAE2[Hou et al., 2023], Bandana[Zhao\net al., 2024], GiGaMAE[Shi et al., 2023], SeeGera[Li et al., 2023b], GAE and VGAE [Kipf and\nWelling, 2016b]. The node classification results were listed in Table 1. DiGGR demonstrates\ncompetitive results on the provided dataset, achieving results comparable to those of supervised\nmethods."}, {"title": "4.2 Graph Classification", "content": "Baseline Models We categorized the baseline models into four groups: i) supervised methods,\nincluding GIN [Xu et al., 2018], DiffPool[Ying et al., 2018] and VEPM[He et al., 2022b]; ii) classical\ngraph kernel methods: Weisfeiler-Lehman sub-tree kernel (WL) [Shervashidze et al., 2011] and\ndeep graph kernel (DGK) [Yanardag and Vishwanathan, 2015]; iii) contrastive learning methods,\nincluding GCC [Qiu et al., 2020], graph2vec [Narayanan et al., 2017], Infograph [Sun et al., 2019],\nGraphCL [You et al., 2020], JOAO [You et al., 2021], MVGRL [Hassani and Khasahmadi, 2020],\nand InfoGCL [Xu et al., 2021]; iv) generative learning methods, including graph2vec [Narayanan\net al., 2017], sub2vec [Adhikari et al., 2018], node2vec [Grover and Leskovec, 2016], GraphMAE\n[Hou et al., 2022], GraphMAE2[Hou et al., 2023], GAE and VGAE [Kipf and Welling, 2016b]. Per\ngraph classification research tradition, we report results from previous papers if available."}, {"title": "4.3 Exploratory Studies", "content": "Visualizing latent representations To examine the influence of the learned latent factor on classifi-\ncation results, we visualized the latent disentangled factor z, which reflects the node-factor affiliation,\nand the hidden representation H used for classification. MUTAG is selected as the representative\nfor classification benchmarks. We encodes the representations into 2-D space via t-SNE [Van der\nMaaten and Hinton, 2008]. The result is shown in Figure 3(a), where each node is colored according\nto its node labels. The clusters in Figure 3(a) still exhibit differentiation in the absence of label\nsupervision, suggesting that z obtained through unsupervised learning can enhance node information\nand offer a guidance for the mask modeling. We then visualize the hidden representation used for\nclassification tasks, and color each node according to the latent factor to which it belongs. The results\nare depicted in Figure 3(b), showcasing separability among different color clusters. This illustrates\nthe model's ability to extract information from the latent factor, thereby enhancing the quality of the\nlearned representations.\nTask-relevant factors To assess the statistical correlation between the learned latent factor and the\ntask, we follow the approach in [He et al., 2022b] and compute the Normalized Mutual Information\n(NMI) between the nodes in the factor label and the actual node labels. NMI is a metric that ranges\nfrom 0 to 1, where higher values signify more robust statistical dependencies between two random\nvariables. In the experiment, we utilized the MUTAG dataset, comprising 7 distinct node types,\nand the NMI value we obtained was 0.5458. These results highlight that the latent factors obtained\nthrough self-supervised training are meaningful for the task, enhancing the correlation between the\ninferred latent factors and the task.\nDisentangled representations To assess DiGGR's capability to disentangle the learned represen-\ntation for downstream task, we provide a qualitative evaluation by plotting the correlation of the\nnode representation in Figure 4. The figure shows the absolute values of the correlation between the\nelements of 512-dimensional graph representation and representation obtained from GraphMAE and\nDiGGR, respectively. From the results, we can see that the representation produced by GraphMAE\nexhibits entanglement, whereas DiGGR's representation displays a overall block-level pattern,\nindicating that DiGGR can capture mutually exclusive information in the graph and disentangle the\nhidden representation to some extent. Results for more datasets can be found in Appendix A.3.\nWhy DiGGR works better: We provide a theoretical explanation for DiGGR's performance\nimprovement in the Appendix A.5. Additionally, we conduct quantitative experiments here to"}, {"title": "5 Conclusions", "content": "In this paper, we propose DiGGR (Disentangled Generative Graph Representation Learning), de-\nsigned to achieve disentangled representations in graph masked autoencoders by leveraging latent\ndisentangled factors. In particular, we achieve this by two steps: 1) We utilize a probabilistic graph\ngeneration model to factorize the graph via the learned disentangled latent factor; 2) We develop a\nDisentangled Graph Masked Autoencoder framework, with the aim of integrating the disentangled in-\nformation into the representation learning of Graph Masked Autoencoders. Experiments demonstrate\nthat our model can acquire disentangled representations, and achieve favorable results on downstream\ntasks."}, {"title": "A Appendix / supplemental material", "content": "Optionally include supplemental material (complete proofs, additional experiments and plots) in\nappendix. All such materials SHOULD be included in the main submission.\nA.1 Ablation Study\nA.1.1 Number of factors\nOne of the crucial hyperparameters in DiGGR is the number of latent factors, denoted as K. When\nK = 1 DiGGR degenerates into ordinary GMAE, only performing random masking over the entire\ninput graph on the nodes. The influence of tuning K is illustrated in Figure 5. Given the relatively\nsmall size of the graphs in the dataset, the number of meaningful latent disentangled factor z is\nnot expected to be very large. The optimal number of z that maximizes performance tends to be\nconcentrated in the range of 2-4.\nA.1.2 Representation for downstream tasks\nWe investigate the impact of various combinations of representation levels on downstream tasks. As\nillustrated in Table 4, for the node classification task, both Ha and Hg are required, i.e., concatenating\nthem in feature dimension, whereas for the graph classification task, Ha alone is sufficient. This\ndifference may be due to the former not utilizing pooling operations, while the latter does. Specifically,\nthe graph pooling operation aggregates information from all nodes, providing a comprehensive\nview of the entire graph structure. Thus, in node classification, where the node representation has\nnot undergone pooling, a graph-level representation (Hg) is more critical. In contrast, in graph\nclassification, the node representation undergoes pooling, making disentangled information Ha more\neffective.\nA.2 Implementation Details\nEnvironment All experiments are conducted on Linux servers equipped with an 12th Gen Intel(R)\nCore(TM) i7-12700, 256GB RAM and a NVIDIA 3090 GPU. Models of node and graph classification\nare implemented in PyTorch version 1.12.1, scikit-learn version 1.0.2 and Python 3.7.\nExperiment Setup for Node Classification The node classification task involves predicting the\nunknown node labels in networks. Cora, Citeseer, and Pubmed are employed for transductive learning,\nwhereas PPI follows the inductive setup outlined in GraphSage [Hamilton et al., 2017]. For evaluation,"}, {"title": "A.3 Disentangled Representations Visualization", "content": "We chose PROTEINS and IMDB-MULTI as representatives of the graph classification dataset, and\nfollowed the same methodology as in Section 4.3 to visualize their representation correlation matrices\non GraphMAE, and community representation correlation matrices on DiGGR, respectively. The\nfeature dimensions of PROTEINS and IMDB-MULTI are both 512 dimensions, and the number of\ncommunities is set to 4."}, {"title": "A.4 Training Algorithm", "content": ""}, {"title": "A.5 Theoretical Approve of Why DiGGR Works", "content": "DiGGR is built on an graph autoencoder (GAE)-based framework. Recent studies [Zhang et al.,\n2022, Li et al., 2023a] have demonstrated a direct connection between GAE and contrastive learning\nthrough a specific form of the objective function. The loss function can be rewritten as follows:\n$\\mathcal{L}^+ = \\frac{1}{\\varepsilon^+} \\sum_{(u,v)\\in \\mathcal{V}^+} log f_{dec}(h_u, h_v)$\n$\\mathcal{L}^- = \\frac{1}{\\varepsilon^-} \\sum_{(u',v')\\in \\mathcal{V}^-} log(1 - f_{dec}(h_{u'}, h_{v'}))$\n$\\mathcal{L}_{GAE} = -(\\mathcal{L}^+ + \\mathcal{L}^-)$\nwhere $h_u$ and $h_v$ are the node representations of node u and node v obtained from an encoder $f_{enc}$\nrespectively (eg., a GNN); $\\varepsilon^+$ is a set of positive edges while $\\varepsilon^-$ is a set of negative edges sampled\nfrom graph, and $f_{dec}$ is a decoder; Typically, $\\varepsilon^+ = \\varepsilon$.\nBuilding on recent advances in information-theoretic approaches to contrastive learning [Tsai et al.,\n2020, Tian et al., 2020], a recent study [Li et al., 2023a] suggests that for SSL pretraining to succeed\nin downstream tasks, task-irrelevant information must be reasonably controlled. Therefore, the\nfollowing proposition is put forward:\nThe task irrelevant information $I (U; V|T)$ of GAE can be lower bounded with:\n$I(U; V\\vert T) \\geq \\frac{(E[N_{kv}])^2}{N_k}$\nminimizing the aforementioned $\\mathcal{L}_{GAE}$ is in population equivalent to maximizing the mutual infor-\nmation between the k-hop subgraphs of adjacent nodes, and the redundancy of GAE scales almost\nlinearly with the size of overlapping subgraphs.\nThe above proposition has been proved in detailed in [Li et al., 2023a], where $I(.; .)$ is the mutual\ninformation, U and V be random variables of the two contrasting views, and T denote the target\nof the downstream task. $N_k$ is the size of the overlapping subgraph of $G^k(u)$ and $G^k(v)$, and the\nexpectation is taken with respect to the generating distribution of the graph and the randomness in\nchoosing u and v.\nAccording to this lower bound, we need to reduce the task-irrelevant redundancy to design a better\ngraph SSL methods. In DiGGR, we first factorize the input graph based on latent factor learning\nbefore feeding it into the masked autoencoder. Take Fig. 7 from the PDF in global rebuttal as an\nexample. Nodes a and b have overlapping 1-hop subgraphs. However, after graph factorization, the"}, {"title": "A.6 Broader Impacts", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many\npotential societal consequences of our work, none which we feel must be specifically highlighted\nhere."}, {"title": "A.7 Limitations", "content": "Despite the promising experimental justifications, our work might potentially suffer from limitation:\nAlthough the complexity of the model is discussed in Section 3.4, and it is comparable to previously\npublished work, extending DiGGR to extremely large graph datasets remains challenging at this stage\ndue to the incorporation of an additional probabilistic model into the generative graph framework.\nOne potential solution to this problem could be utilizing PPR-Nibble [Andersen et al., 2006] for\nefficient implementation, a method that has proven effective in some graph generative models [Hou\net al., 2023]. This approach will be pursued in our future work."}]}