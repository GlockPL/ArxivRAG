{"title": "Two-level deep domain decomposition method", "authors": ["Victorita Dolean", "Serge Gratton", "Alexander Heinlein", "Valentin Mercier"], "abstract": "We have presented a two-level approach to improve the convergence of the one-level Deep-DDM method. The additional coarse networks facilitates faster global transport of information and enhances the scalability of the Deep-DDM method to larger numbers of subdomains. The cost of training the coarse network is relatively low compared with the total cost of the method and the method is well-suited for a parallel implementation with only small differences in wall time (1) with a non optimized parallelization.", "sections": [{"title": "1 Introduction", "content": "The successful application of machine learning in image and language processing has extended its reach to solving physical equations. One example for the combination of scientific computing and machine learning is the field of physics-informed machine learning, with physics-informed neural networks (PINNs [10]) becoming its most prominent example. The idea of this method is to integrate the partial differential equation directly into the loss function for training a neural network to approximate the solution of a boundary value problem (BVP). Therefore, let us consider the generic BVP:\n\nN[u](x) = f(x) \\forall x \\in \\Omega\nB[u](x) = g(x) \\forall x \\in \\partial\\Omega\n\nwith $\\Omega \\in \\mathbb{R}^d$ and $\\partial\\Omega$ its boundary, $N$ a differential and $B$ a boundary operator.\nThe approximation of this problem using a neural network $u_{\\theta}$ parametrized by weights and biases gathered in $\\theta$ can be found by the following optimization problem:"}, {"title": "2 The Deep Domain Decomposition Method", "content": "Let the computational domain $\\Omega$ be decomposed into $S$ overlapping subdomains $\\Omega_1, ..., \\Omega_S$. Then, we consider the classical Schwarz iteration [11], which involves an independent problem on each subdomain $\\Omega_s$: find $u_s$\n\nN(u_s) = f \\quad in \\Omega_s,\nB(u_s) = g \\quad on \\partial\\Omega_s \\backslash \\Gamma_s,\nD(u_s) = D(u_r) \\quad on \\Gamma_s,\n\nwhere $D$ is an operator for the transmission conditions (e.g., Dirichlet, Neumann, or Robin) on the subdomain boundary, and $\\Gamma_s$ is the interface between the subdomain $\\Omega_s$ and the neighboring subdomains $\\Omega_r$, with $\\Omega_r \\cap \\Omega_s \\neq 0$. For convenience, we define the multiple overlapping subdomains as a single subdomain $\\Omega_r$ with a corresponding solution network $u_r$. In order to train a PINN model to solve eq. (2), we incorporate an additional term accounting for the transmission conditions into the loss function eq. (1). In particular, the loss term is computed by sampling points on the interface denoted $\\{x_i\\}^{\\text{Nr}}_{i=1} \\subset \\partial\\Omega_s$. A visualization of all sampling points is shown in fig. 1 (2). For the transmission to $\\Omega_i$, with $\\Omega_r \\cap \\Omega_s \\neq 0$, we incorporate the loss term:\n\nM_{\\Gamma}(\\theta) = \\frac{1}{N_{\\Gamma}} \\sum_{i=1}^{N_{\\Gamma}}(u_s(x_i) - W_i)^2\n\nIn this term, we minimize the difference between the trained network $u_s$ and the neighboring networks $u_r$ on the interface with respect to the transfer operator $D$. Here,\n\nW_i = D(u_r(x_i))\n\nis defined based on the network $u_r$ from the previous outer Schwarz iteration. Once all the local subnetworks have been trained up to a certain stopping criterion (e.g., number of iterations or tolerance), the interface values $W_i$ are being updated, and we proceed to the next outer iteration.\nThe one-level Deep DDM algorithm, without the red parts, is shown in algorithm 1. Initially, two stopping criteria were used to assess convergence: differences in the network solution between subsequent iterations, both in the interior and at the boundary. In addition, a loss criterion variation was used to terminate the training of each network, alongside a maximum epoch limit. This method made comparisons between runs difficult as the number of epochs varied. Here we simplify by using a fixed number of epochs per training and fixed outer iterations. For further details on the original criteria, refer to [8]."}, {"title": "3 Extension via a Coarse Network", "content": "Our coarse level corresponds to training a neural network acting on the entire domain, which we will denote as the coarse network. The convergence of the two-level method will then depend on the coarse network solution as well as the exchange of information with the local networks on the first level of the method. In particular, as the coarse network, we train a classical PINN model on the global domain and add an additional loss term incorporating the local subdomain networks. Conversely, after training the coarse network, we will incorporate the coarse network into the loss function for the training of the local networks.\nExtension operators and a partition of unity [3]: Let us define an extension operator\n\nE_s(w_s) = \\begin{cases} w_s \\quad in \\Omega_s \\\\ 0 \\quad otherwise. \\end{cases}\n\nHere, $w_s$ is a function defined on $\\Omega_s$. Moreover, we define partition of unity functions $\\chi_s$ with $\\chi_s \\geq 0$, supp($\\chi_s$) $\\subset \\Omega_s$, $\\chi_s(x) = 0$ for $x \\in \\partial \\Omega_s \\backslash \\partial\\Omega$, and\n\n\\omega = \\sum_{s=1}^S E_s(\\chi_s\\omega).\n\nfor any function $\\omega$ defined on $\\Omega$. We then add the term\n\nM_{\\text{fine}}(\\theta_c) = \\frac{1}{N_{\\Omega_c}} \\sum_{k=1}^{N_{\\Omega_c}} |u_c(x_k) - \\sum_{s=1}^S E_s(\\chi_s u_s(x))|^2,\n\nto the loss function eq. (1) for the training of the coarse network. Here, $\\{x_k\\}_{k=1}^{N_{\\Omega_c}}$ are the sampling points for the coarse network $\\Omega$ related loss (example in fig. 1 (2)), and $u_c$ and $\\theta_c$ are the coarse network and its network parameters, respectively. This term which transfer information from the fine solution to the coarse network is weighted with $\\lambda_f$. As we aim to train the coarse network concurrently with the fine networks, making our algorithm fully parallelizable, we use the $u_s$ from the previous outer iteration. This information is unavailable for the first iteration, so in this case, we set $\\lambda_f$ to 0.\nIn order to transfer information from the coarse to the local networks, we introduce a loss term into the $M_{\\Gamma}$ loss of the local networks via the interface term eq. (4):"}, {"title": "4 Numerical Results", "content": "In this section, we will conduct tests on the previously defined Poisson's problem using the following settings. The collocation points are sampled using Latin hypercube sampling in $\\Omega$ and on $\\partial\\Omega$ and $\\Gamma$. For our strong scaling tests, we fix $N_\\Omega = 30000$ and $N_{\\partial\\Omega} = N_{\\Gamma} = 16000$ for the whole problem while increasing the number of sub-domains. All networks are trained using the Adam optimizer with an initial learning rate of $2 \\times 10^{-4}$ and an exponential decay of 0.999 every 100 epochs. Each neural network is composed of two hidden layers with 30 neurons. The overlap is set to 30% of the subdomain larger side. In each Schwarz iteration, each local and coarse network is trained for 2 500 epochs. The weight controlling the impact of the coarse network on the local networks is set to $\\lambda_c = 1 \\times 0.9^{I}$ where $I$ is the index of the Schwarz iteration. This increases the impact of the coarse network during the course of the outer Schwarz iteration. The coefficient $\\lambda_f$ controlling the impact of the fine networks on the coarse network is set to a fixed value of 0.5. All weights $\\lambda$ have been optimized using a rough grid search to obtain good performance.\n\nOur implementation uses TensorFlow2 (version 24.02) and runs numerical experiments on a single Nvidia A100-80 GPU. Both the fine and coarse network training processes run concurrently on the GPU using multiprocessing. Although this setup is suboptimal due to the unaddressed GPU load and the complex interaction between TensorFlow and multiprocessing, it allows us to compare the wall time between one-level and two-level methods. The results presented are the median of three independent training runs, each initialized with different seeds.\n\nWe focus on investigating the impact of our two-level domain decomposition on the spectral bias or f-principle [14, 13] of neural networks; this refers to the observation low-frequency components of the target functions are learned much faster than the high-frequency components. The Deep-DDM method tackles this issue by splitting the global problem into smaller subproblems, allowing for better approximation of high frequency components, while the coarse network is supposed to learn the low frequency components.\n\nIn order to investigate this, we consider two different pairs of coefficients $(\\omega_1, \\omega_2) \\in \\{(1,3); (1,6)\\}$. For the lower frequency pair of coefficients, that is, $(\\omega_1, \\omega_2) = (1,3)$, we observe a noticeable improvement in convergence with the addition of a coarse level; cf. fig. 2. In particular, the convergence seems to be inde-"}, {"title": "5 Conclusion", "content": "We have presented a two-level approach to improve the convergence of the one-level Deep-DDM method. The additional coarse networks facilitates faster global transport of information and enhances the scalability of the Deep-DDM method to larger numbers of subdomains. The cost of training the coarse network is relatively low compared with the total cost of the method and the method is well-suited for a parallel implementation with only small differences in wall time (1) with a non optimized parallelization."}]}