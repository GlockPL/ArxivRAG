{"title": "Two-level deep domain decomposition method", "authors": ["Victorita Dolean", "Serge Gratton", "Alexander Heinlein", "Valentin Mercier"], "abstract": "The article proposes a Two-level deep domain decomposition method.", "sections": [{"title": "1 Introduction", "content": "The successful application of machine learning in image and language processing has extended its reach to solving physical equations. One example for the combination of scientific computing and machine learning is the field of physics-informed machine learning, with physics-informed neural networks (PINNs [10]) becoming its most prominent example. The idea of this method is to integrate the partial differential equation directly into the loss function for training a neural network to approximate the solution of a boundary value problem (BVP). Therefore, let us consider the generic BVP:\n\n$N[u](x) = f(x) \\forall x \\in \\Omega$\n$B[u](x) = g(x) \\forall x \\in \\partial\\Omega$\n\nwith $\\Omega \\in R^d$ and $\\partial\\Omega$ its boundary, $N$ a differential and $B$ a boundary operator.\nThe approximation of this problem using a neural network $u_\\theta$ parametrized by weights and biases gathered in $\\theta$ can be found by the following optimization problem:"}, {"title": "2", "content": "where\n\n$\\theta^* = arg \\min_\\theta M(\\theta)$,\n\n$M(\\theta) = \\sum_{i=1}^S \\lambda_\\Omega \\sum_{i=1}^{N_\\Omega} (N[u_\\theta](x_i) - f(x_i))^2 + \\frac{\\lambda_{\\partial\\Omega}}{N_{\\partial\\Omega}} \\sum_{j=1}^{N_{\\partial\\Omega}} (B[u_\\theta](x_j) - g(x_j))^2$ (1)\n\nHere, ${x_i}_{i=1}^{N_\\Omega}$ and ${x_j}_{j=1}^{N_{\\partial\\Omega}}$ are sets of collocation points sampled in $\\Omega$ respectively on $\\partial\\Omega$. The back-propagation algorithm allows for both the evaluation of the residual of the partial differential equation (PDE) and the optimization of the loss function eq. (1) with respect to the network parameters $\\theta$.\nWhile domain decomposition methods (DDMs) are well-established solvers for PDEs using classical discretizations, the use of neural network-based discretizations, in particular, PINNs has been explored more recently. This concept has been explored in various studies like for example in [12], where the authors discuss non-overlapping DDMs for parallel training. Extensions to Schwarz methods for Deep Ritz networks, which integrate the variational form into the loss function, are presented in [7]. The coupling in the finite basis PINNs (FBPINNs) approach in [9] differs from the aforementioned approaches; the authors introduce an overlapping domain decomposition, and the coupling is performed via a corresponding partition of unity scaling and hard enforcement of boundary conditions. For a broader overview over the combination of domain decomposition and machine learning methods, we refer to the review [6].\nOur focus is on employing PINNs as the subdomain solver in a classical Schwarz approach iteration; this is known as the Deep Domain Decomposition Method (Deep-DDM) introduced in [8]. Given that the subdomain problems can be solved sufficiently accurately, the convergence properties are the same as for the classical Schwarz iteration. As a result, the method is not numerically scalable when increasing the number of subdomains. In this paper, we will incorporate a coarse level, in order to retain numerical scalability. A related two-level Schwarz approach, which uses a different coupling between the two levels of a classical Schwarz iteration has been presented in [5]. Other related multilevel approaches are, for instance, multi-level FBPINNs [1, 2] and multilevel optimization methods using frequency-aware networks [4]."}, {"title": "2 The Deep Domain Decomposition Method", "content": "Let the computational domain $\\Omega$ be decomposed into $S$ overlapping subdomains $\\Omega_1, ..., \\Omega_S$. Then, we consider the classical Schwarz iteration [11], which involves an independent problem on each subdomain $\\Omega_s$: find $u_s$"}, {"title": "3", "content": "$N(u_s) = f \\quad in \\quad \\Omega_s,$\n$B(u_s) = g \\quad on \\quad \\partial\\Omega_s \\backslash \\Gamma_s,$\n$D(u_s) = D(u_r) \\quad on \\quad \\Gamma_s, $\n(2)\nwhere $D$ is an operator for the transmission conditions (e.g., Dirichlet, Neumann, or Robin) on the subdomain boundary, and $\\Gamma_s$ is the interface between the subdomain $\\Omega_s$ and the neighboring subdomains $\\Omega_r$, with $\\Omega_r \\cap \\Omega_s \\neq 0$. For convenience, we define the multiple overlapping subdomains as a single subdomain $\\Omega_r$ with a corresponding solution network $u_r$. In order to train a PINN model to solve eq. (2), we incorporate an additional term accounting for the transmission conditions into the loss function eq. (1). In particular, the loss term is computed by sampling points on the interface denoted ${x_i}_{i=1}^{N_\\Gamma} \\subset \\partial\\Omega_s$. A visualization of all sampling points is shown in fig. 1 (2). For the transmission to $\\Omega_i$, with $\\Omega_r \\cap \\Omega_s \\neq 0$, we incorporate the loss term:\n\n$M_\\Gamma(\\theta) = \\frac{1}{N_\\Gamma} \\sum_{i=1}^{N_\\Gamma} |u_s(x_i) - W_i|^2$\n\n(3)\nIn this term, we minimize the difference between the trained network $u_s$ and the neighboring networks $u_r$ on the interface with respect to the transfer operator $D$. Here,\n\n$W_i = D(u_r(x_i))$\n(4)\nis defined based on the network $u_r$ from the previous outer Schwarz iteration. Once all the local subnetworks have been trained up to a certain stopping criterion (e.g., number of iterations or tolerance), the interface values $W_i$ are being updated, and we proceed to the next outer iteration.\nThe one-level Deep DDM algorithm, without the red parts, is shown in algorithm 1. Initially, two stopping criteria were used to assess convergence: differences in the network solution between subsequent iterations, both in the interior and at the boundary. In addition, a loss criterion variation was used to terminate the training of each network, alongside a maximum epoch limit. This method made comparisons between runs difficult as the number of epochs varied. Here we simplify by using a fixed number of epochs per training and fixed outer iterations. For further details on the original criteria, refer to [8]."}, {"title": "Algorithm 1 Two-level DeepDDM; coarse level", "content": "1: Sampling the fine and the coarse collocation points\n2: Initialization of the network parameters $\\theta_f$ and $\\theta_c$\n3: Initialization of interface values $W = [W_1, ..., W_s]$\n4: Initialization of weights $\\lambda_f$ and $\\lambda_c$\n5: while Iteration limits not reached do\n6: Local network training\n7: Coarse network training\n8:  for $s=1$ do\n9: Compute $\\sum_{s=1}^S E_s(\\chi_s u_s(x_{i, coarse}))$ for each coarse points\n10: Update of $W_k$ values at interfaces with eq. (3) or eq. (5)\n11: Update $\\lambda_f$ and $\\lambda_c$"}, {"title": "4", "content": "* Strong scalability: Strong scalability is defined as how the solution time varies with the number of cores for a fixed total problem size. Ideally, the elapsed time is inversely proportional to the number of processing units.\n* Weak scalability: Weak scalability is defined as how the solution time varies with the number of cores for a fixed problem size per core. Ideally, the elapsed time is constant for a fixed ratio between the size of the problem and the number of processing units.\nSince these terms are not clearly defined for of deep learning, we will assume that the size of our problem is the number of points sampled to solve the problem, and the number of processing units is the number of PINNs models used to solve the problem (and therefore the number of subdomains)."}, {"title": "5", "content": "We consider a Poisson equation with Dirichlet boundary conditions:\n\n$\\Delta u = r(x) \\quad in \\quad \\Omega = [0, 1] \\times [0, 1],$\n$u = g(x) \\quad on \\quad \\partial\\Omega.$\n\nWe choose $r$ and $g$ such that the exact solution is $u(x) = sin(\\omega_1\\pi x_1)sin(\\omega_1 \\pi x_2) + sin(\\omega_2 \\pi x_1)sin(\\omega_2 x_2)$, in this paper we will perform tests for several values of $\\omega_1$ and $\\omega_2$. To investigate the scalability of the one-level method we choose $\\omega_1 = \\omega_2 = 1$ and we perform 1500 epochs per training (other settings are the same as in 4). Here, we test the strong scalability on regular rectangle domain decomposition. A regular rectangle domain decomposition with overlap divides $\\Omega = [L_{x0}, L_{x1}] \\times [L_{y0}, L_{y1}]$ into $N_x \\times N_y$ subdomains, each overlapping by $a_x \\Delta x$ and $a_y \\Delta y$. The total number of points sampled remains the same in each experiment, corresponding to our problem size, while we increase the number of subdomains and thus the number of processing"}, {"title": "5 Extension via a Coarse Network", "content": "Our coarse level corresponds to training a neural network acting on the entire domain, which we will denote as the coarse network. The convergence of the two-level method will then depend on the coarse network solution as well as the exchange of information with the local networks on the first level of the method. In particular, as the coarse network, we train a classical PINN model on the global domain and add an additional loss term incorporating the local subdomain networks. Conversely, after training the coarse network, we will incorporate the coarse network into the loss function for the training of the local networks.\nExtension operators and a partition of unity [3]: Let us define an extension operator\n\n$E_s (w_s) = \\begin{cases}\nW_s \\quad in \\quad \\Omega_s \\\\\n0 \\quad otherwise.\n\\end{cases}$\n\nHere, $w_s$ is a function defined on $\\Omega_s$. Moreover, we define partition of unity functions $\\chi_s$ with $\\chi_s \\geq 0, supp(\\chi_s) \\subset \\Omega_s, \\chi_s(x) = 0$ for $x \\in \\partial\\Omega_s \\backslash \\partial\\Omega$, and\n\n$\\omega = \\sum_{s=1}^S E_s(\\chi_s|\\omega_s)$\n\nfor any function $\\omega$ defined on $\\Omega$. We then add the term\n\n$M_{fine} (\\theta_c) = \\frac{1}{N_\\Omega} \\sum_{k=1}^{N_\\Omega} |u_c(x_k) - \\sum_{s=1}^S E_k(\\chi_s u_s(x))|^2,$\n\nto the loss function eq. (1) for the training of the coarse network. Here, ${x_k}_{k=1}^{N_{Oc}}$ are the sampling points for the coarse network $\\Omega$ related loss (example in fig. 1 (2)), and $u_c$ and $\\theta_c$ are the coarse network and its network parameters, respectively. This term which transfer information from the fine solution to the coarse network is weighted with $\\lambda_f$. As we aim to train the coarse network concurrently with the fine networks, making our algorithm fully parallelizable, we use the $u_s$ from the previous outer iteration. This information is unavailable for the first iteration, so in this case, we set $\\lambda_f$ to 0.\nIn order to transfer information from the coarse to the local networks, we introduce a loss term into the $M_\\Gamma$ loss of the local networks via the interface term eq. (4):"}, {"title": "6", "content": "$W_i = \\lambda_c D(u_r(x_i)) + (1 - \\lambda_c)D(u_c(x_i)),$\n(5)\nwhere $\\lambda_c \\in [0, 1]$ is a weight balancing the impact of the coarse network on the local networks.\nNote that, since the accuracy of the local networks is often poor in the first iterations, we adjust the corresponding weight in the loss function $\\lambda_f$ during the Schwarz iteration. The final two-level algorithms is given in algorithm 1, including the red parts."}, {"title": "4 Numerical Results", "content": "In this section, we will conduct tests on the previously defined Poisson's problem using the following settings. The collocation points are sampled using Latin hypercube sampling in $\\Omega$ and on $\\partial\\Omega$ and $\\Gamma$. For our strong scaling tests, we fix $N_\\Omega = 30000$ and $N_{\\partial\\Omega} = N_\\Gamma = 16000$ for the whole problem while increasing the number of subdomains. All networks are trained using the Adam optimizer with an initial learning rate of $2 \\times 10^{-4}$ and an exponential decay of 0.999 every 100 epochs. Each neural network is composed of two hidden layers with 30 neurons. The overlap is set to 30% of the subdomain larger side. In each Schwarz iteration, each local and coarse network is trained for 2 500 epochs. The weight controlling the impact of the coarse network on the local networks is set to $\\lambda_c = 1 \\times 0.9^I$ where $I$ is the index of the Schwarz iteration. This increases the impact of the coarse network during the course of the outer Schwarz iteration. The coefficient $\\lambda_f$ controlling the impact of the fine networks on the coarse network is set to a fixed value of 0.5. All weights $\\lambda$ have been optimized using a rough grid search to obtain good performance.\nOur implementation uses TensorFlow2 (version 24.02) and runs numerical experiments on a single Nvidia A100-80 GPU. Both the fine and coarse network training processes run concurrently on the GPU using multiprocessing. Although this setup is suboptimal due to the unaddressed GPU load and the complex interaction between TensorFlow and multiprocessing, it allows us to compare the wall time between one-level and two-level methods. The results presented are the median of three independent training runs, each initialized with different seeds.\nWe focus on investigating the impact of our two-level domain decomposition on the spectral bias or f-principle [14, 13] of neural networks; this refers to the observation low-frequency components of the target functions are learned much faster than the high-frequency components. The Deep-DDM method tackles this issue by splitting the global problem into smaller subproblems, allowing for better approximation of high frequency components, while the coarse network is supposed to learn the low frequency components.\nIn order to investigate this, we consider two different pairs of coefficients $(\\omega_1, \\omega_2) \\in {(1,3); (1,6)}$. For the lower frequency pair of coefficients, that is, $(\\omega_1, \\omega_2) = (1,3)$, we observe a noticeable improvement in convergence with the addition of a coarse level; cf. fig. 2. In particular, the convergence seems to be inde-"}, {"title": "7", "content": "pendent of the number of subdomains. However, when we increase the frequency of the solution, that is, $(\\omega_1, \\omega_2) = (1, 6)$, we notice that it takes longer for the two-level method before it converges; cf. fig. 3 (2). We observed that this problem can be easily solved by improving hyper parameter settings. In particular, for 6\u00d76 subdomains, we observe that we can significantly improve the scalability by increasing the number of epochs for each subproblem; cf. fig. 3 (3). Notably, with 5 000 and 7 500 epochs per subproblem, the two-level methods clearly outperforms the one-level Deep-DDM."}, {"title": "5 Conclusion", "content": "We have presented a two-level approach to improve the convergence of the one-level Deep-DDM method. The additional coarse networks facilitates faster global transport of information and enhances the scalability of the Deep-DDM method to larger numbers of subdomains. The cost of training the coarse network is relatively low compared with the total cost of the method and the method is well-suited for a parallel implementation with only small differences in wall time (1) with a non optimized parallelization."}]}