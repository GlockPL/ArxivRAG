{"title": "Guide-LLM: An Embodied LLM Agent and Text-Based Topological\nMap for Robotic Guidance of People with Visual Impairments", "authors": ["Sangmim Song", "Sarath Kodagoda", "Amal Gunatilake", "Marc G. Carmichael", "Karthick Thiyagarajan", "Jodi Martin"], "abstract": "Navigation presents a significant challenge for\npersons with visual impairments (PVI). While traditional aids\nsuch as white canes and guide dogs are invaluable, they fall\nshort in delivering detailed spatial information and precise\nguidance to desired locations. Recent developments in large\nlanguage models (LLMs) and vision-language models (VLMs)\noffer new avenues for enhancing assistive navigation. In this pa-\nper, we introduce Guide-LLM, an embodied LLM-based agent\ndesigned to assist PVI in navigating large indoor environments.\nOur approach features a novel text-based topological map\nthat enables the LLM to plan global paths using a simplified\nenvironmental representation, focusing on straight paths and\nright-angle turns to facilitate navigation. Additionally, we utilize\nthe LLM's commonsense reasoning for hazard detection and\npersonalized path planning based on user preferences. Simu-\nlated experiments demonstrate the system's efficacy in guiding\nPVI, underscoring its potential as a significant advancement\nin assistive technology. The results highlight Guide-LLM's\nability to offer efficient, adaptive, and personalized navigation\nassistance, pointing to promising advancements in this field.", "sections": [{"title": "I. INTRODUCTION", "content": "Navigating everyday environments can be particularly\nchallenging for persons with visual impairments (PVI), who\noften depend on specialized tools, help from others, or\nfamiliar routes to get around [1], [2], [3], [4], [5]. Traditional\naids, such as white canes and guide dogs are essential part\nof the navigation, however, with the technological develop-\nments, further assistance could be possible to improve user\nconfidence in navigation [6]. .\nRecent breakthroughs in artificial intelligence, especially\nin large language models (LLMs) [7], [8] and vision-\nlanguage models (VLMs) [9], [10], [11], have created new\nopportunities in human-robot interaction [12], task planning\n[13], [14], and navigation [15], [16]. Despite these advance-\nments, their application in assisting PVI with navigation is"}, {"title": "II. RELATED WORK", "content": "Recent advancements in assistive technology have greatly\nenhanced the navigation and mobility of PVI. For instance,\nthe robotic white cane [26] employs multimodal sensing and\nsteering assistance to aid users in navigating their surround-\nings. Additionally, wearable systems have been developed\nto improve situational awareness by delivering real-time\nfeedback through auditory or haptic signals [27], [28], [29],\n[30]. [31] introduced a method that allows PVIs to jog on\nathletic tracks. Despite these advancements, many of these\nsystems are pre-programmed with fixed rules and behaviors,\nwhich can limit their adaptability to dynamic scenarios.\nTo address these limitations, dialogue-based robots have\nbeen proposed as an alternative form of assistance, using\nconversational interfaces to help users reach their destina-\ntions [17]. However, these systems still encounter significant\nchallenges, particularly in natural language interaction and\nadaptability. Their rule-based nature also limits their ability\nto generalize and adjust to changing environments, which\nis essential for offering personalized and flexible support to\nPVI users."}, {"title": "B. Large Language Models in Robotics", "content": "The integration of LLMs into robotics has demonstrated\nsignificant potential in areas such as task planning, au-\ntonomous driving, multimodal reasoning, and navigation.\nFrameworks like SayPlan and ReAct enhance robotic capa-\nbilities by breaking down complex instructions into action-\nable sub-tasks and combining reasoning with action [13],\n[32]. DriveLLM [16] showcased how LLMs can improve\ninterpretability and decision-making in autonomous driving\nby integrating object-level vector modalities with LLMs,\nthus enhancing context understanding and explainability in\ndriving scenarios. RoboVQ [33] combined LLMs with vision\ninputs, enabling robots to perform complex, long-horizon\ntasks and demonstrating LLMs' multimodal reasoning capa-\nbilities. [34] utilized LLMs as navigation agents, leveraging\ntheir reasoning capabilities to develop search heuristics for"}, {"title": "C. Vision and Language Navigation", "content": "Vision and language navigation (VLN) integrates visual\nperception with natural language understanding to enable\nagents to navigate based on spoken or written instructions.\nA significant challenge in VLN is that natural language\ninstructions often emphasize high-level decisions and land-\nmarks, frequently lacking detailed, low-level movement guid-\nance [39]. Attention-based mechanisms [40], [41], [42] and\nreinforcement learning approaches [43], [44] have shown\npromising results in addressing this issue. Recently, the rise\nof LLMs and VLMs has sparked interest in leveraging these\nmodels to enhance VLN capabilities. For instance, LM-Nav\n[15] uses LLMs to extract landmarks from user queries\nand VLMs to ground these landmarks in the environment\nfor navigation. Similarly, [45] employ LLMs to translate\nuser queries into actionable navigation tasks and use image\nsegmentation techniques to create a topological map for nav-\nigation. [38] combined LLMs commonsense reasoning with\nVLMs to navigate towards uniquely described objects, show-\ncasing LLMs potential in understanding nuanced language.\n[46] utilized LLMs to guide the agent by processing action\nsequences based on text prompts that include navigation in-"}, {"title": "III. PROPOSED SYSTEM", "content": "In this section, we provide a detailed overview of the\nGuide-LLM framework. Figure 2 illustrates the primary com-\nponents of the framework, which enable our LLM agent to\nhandle decision-making, high-level planning, and navigation."}, {"title": "A. Core Components", "content": "1) Agent: We employed GPT-40 [47] as the central com-\nponent of our framework. GPT-40 is tasked with interpreting\nuser inputs and determining appropriate actions to assist PVI.\nWhen a user query is received, the LLM is provided with a\nsystem prompt containing instructions to process the query\nwithin a specific context. As illustrated in Fig. 4, this prompt\nis crafted to guide the LLM in assisting the user effectively\nwhile addressing potential errors and safety concerns. The\nsystem prompt is based on the Chain-of-Thought (CoT)\nmethod [48], which enables the LLM to break down tasks\ninto manageable, intermediate steps.\n2) Text-Based Topological Map: The framework features\na text-based topological map (Fig. 3) that illustrates the spa-\ntial relationships between various nodes in the environment\n(e.g., RoomA is connected to HallwayA, 0.5m north). Nodes\nare positioned at critical points where the robot needs to\nturn or take action. This map acts as a schematic reference\nfor the agent, aiding navigation through complex spaces by\noffering a structured and queryable representation of the\nenvironment. It is specifically designed to emphasize straight\npaths between nodes, as people with visual impairments\ngenerally find straight trajectories more intuitive than curved\nones [24], [23]."}, {"title": "3) Path Planning Module:", "content": "This module works in tan-\ndem with the agent to plan efficient routes to designated\ndestinations. When the agent initiates a navigation query,\nthe module first consults the text-based topological map to\ndetermine potential routes. It calculates the shortest path\nto the destination and generates a textual description of\nthe route for the agent to process. To enhance flexibility,\nthe module uses a depth-first search (DFS) algorithm to\nexplore all possible routes. This approach allows the agent to\nassess multiple paths and adapt in real-time based on hazards\nor user preferences, such as avoiding obstacles, selecting\nquieter routes, or prioritizing safety concerns. By combining\npath planning with textual feedback, the agent can make\nmore informed decisions and offer a personalized navigation\nexperience for the user."}, {"title": "4) Embedding Module:", "content": "We use pre-trained CLIP [10] to\ngenerate embeddings from visual data, which are then stored\nin a vector database alongside relevant metadata, including\nlocation and orientation information. This metadata allows\nthe LLM to reference the location and orientation of specific\nimages upon retrieval. The embedding module is essential\nfor localization and subgoal selection, as it supports both\ntext-to-image and image-to-image similarity searches. This\nfunctionality enables the agent to localize itself accurately\nand select appropriate subgoals."}, {"title": "5) Vector Database:", "content": "The vector database manages high-\ndimensional vector representations of the environment, which\nare generated by the embedding module. This setup enables\nefficient comparison of current observations with previously\nstored data. The database is crucial for real-time decision-"}, {"title": "6) Low-Level Planner:", "content": "The low-level planner converts\nthe high-level decisions made by the agent into executable\nactions for the robot. It ensures that these commands are\nphysically feasible, considering the robot's movement ca-\npabilities. By connecting high-level decision-making with\npractical execution, the low-level planner enables the robot\nto operate safely and efficiently within various environments."}, {"title": "B. Global Path Planning", "content": "Figure 2 illustrates the overall process of the proposed\nframework. We start with the assumption that the text map\nand images in the vector database are pre-labeled. The\nprocess begins with the LLM processing the user's query\nand the system prompt. The LLM then generates high-level\nplans, image query commands, and responses for the user.\nAs shown in Fig. 2, each output is communicated sepa-\nrately for clear communication between modules. High-level\nplans and user responses are delivered through a voice-to-\ntext interface, while image queries retrieve navigation-related\nimages from the vector database. These images are then\nembedded into a secondary vector database, which refines\nthe navigation process by reducing ambiguity and ensuring\nthat only relevant images are retrieved."}, {"title": "C. Topological Navigation", "content": "The agent begins navigation by querying the vector\ndatabase to retrieve the image for the next node. This image\nis compared to the current observation using cosine similarity\nfor localization. To improve place recognition accuracy, a\nsimilarity check is performed each time the robot has traveled\na specified distance or made a turn. The distance to the next\nnode is measured by the robot's odometer through the low-\nlevel planner, and a message is sent to the agent upon arrival.\nIf the similarity score exceeds a predefined threshold, the\nLLM concludes that the target node has been reached and\ngenerates the next set of movement commands to proceed.\nThis process continues until the final destination is reached."}, {"title": "D. Localization and Error Handling", "content": "Localization in our agent framework employs a dual-\nl layered approach. Initially, the agent verifies its arrival at the\ndesired node by querying the navigational vector database\nand comparing the retrieved image with the robot's current\nobservation. If the similarity score between these images\nfalls below a set threshold, the agent detects a potential\nlocalization error, suggesting that the robot may be in the\nwrong location. To address this, the agent initiates a broader\nsearch by querying the main vector database, which contains"}, {"title": "E. Utilizing LLM's Commonsense and Reasoning", "content": "Our framework utilizes the LLM's commonsense rea-\nsoning to improve navigation safety and decision-making.\nUnlike traditional systems that depend on predefined rules,\nthe agent can interpret dynamic, real-world contexts to\nanticipate potential risks. For instance, if the agent identifies\nhazards such as a wet floor, warning tape, or unexpected\nobstacles through visual data or environmental descriptions,\nit proactively alerts the user and suggests an alternative route.\nThe LLM's reasoning capabilities allow it to detect potential\nhazards even if they are not explicitly mentioned what is\nhazard. This flexibility enables the agent to adapt to changing\nconditions that rule-based systems might fail. By integrating\ncommonsense knowledge, the agent enhances both the safety\nand overall reliability of the navigation experience."}, {"title": "F. Personalization Potential", "content": "A key strength of our agent is its ability to personalize\nthe navigation experience according to each user's specific\npreferences and needs. The agent's natural language interac-\ntion capabilities enable it to adjust its behavior in real-time,\nfacilitating this personalization. For example, it can modify\nits path-planning to match a user's preferred walking speed,\nroute preferences (e.g., avoiding stairs or choosing quieter\nareas), or specific safety concerns, such as steering clear of\npotential hazards. Additionally, the system can incorporate\nfeedback from previous interactions, progressively refining\nits decisions to align with the user's habits and preferences.\nFor instance, if a user consistently opts for longer, less\ncrowded routes over shorter ones, the agent can integrate this\npreference into its future planning. This adaptability extends\nbeyond navigation; the agent can engage in personalized\ndialogue, adjusting the level of detail and communication\nstyle to meet different users' needs, whether they prefer brief\ninstructions or more detailed explanations."}, {"title": "IV. RESULTS", "content": "We evaluated the capabilities of our Guide-LLM by testing\nits ability to guide visually impaired individuals through\nsimulated environments. The simulations are divided into\nfour key scenarios, each designed to highlight distinct aspects\nof the LLM's utility in navigation, decision-making, error\nhandling, and hazard detection. All tests were conducted in\na simulated environment using the iGibson simulator with a\nTurtleBot platform."}, {"title": "A. Experiment Setup", "content": "We used iGibson [49] simulator to validate our framework.\nTwo distinct environments of varying sizes were selected"}, {"title": "B. LLM Guided Navigation Ablation Study", "content": "The objective of this experiment was to assess whether\nour LLM agent could successfully navigate to a destination\nby interpreting user queries without detailed instructions,\nshowcasing its potential to assist PVI. To evaluate the con-\ntributions of the core components in our agent framework,\nwe performed an ablation study by systematically removing\nsystem prompts and path planning module individually to an-\nalyze the impact of each removal on navigation performance.\nAs shown in Table I, when all components were active,\nGuide-LLM achieved an 83.33% success rate in navigation\nwithin an office environment. Removing the system prompt\nresulted in a 0% success rate, indicating that the system\nprompt is crucial for guiding the agent's understanding and\nresponses by providing necessary context and instructions.\nTo assess the impact of the path planning module, we\nremoved it and directly input the text map into our agent. The\nsuccess rate of navigation decreased to 40%. This decrease\nis attributed to several factors. First, directly providing the\ntext map along with the system prompt increased the prompt\nlength, which degraded the agent's reasoning capability due\nto processing bottlenecks associated with large prompts [50],\n[51], [52]. Without path planning module, the agent is more\nsuccessful when following alphabetical paths (e.g., Hallway\nA to Hallway C) but struggled with reversed paths (e.g.,\nHallway E to Hallway A). Removing the image retrieval\nsystem forced users to specify the starting location, altering\nthe experiment and adding user burden. This configuration\nwas excluded from comparative analysis to maintain exper-\nimental fairness, as it no longer tested the agent's ability to\ninfer starting points visually."}, {"title": "C. Localization Error Detection and Recovery", "content": "To evaluate our agent's error handling and recovery ca-\npabilities, we conducted an experiment where the robot\nwas placed in random node during navigation. When the\nsimilarity score between the robot's current observation and\nthe expected navigational image fell below 0.94, the agent\nattempted to recover by querying the main vector database to\nre-localize itself and then restarted the navigation planning\nprocess from the recovered location. To further illustrate\nthe system's behavior, we provide an example chat box\ninteraction in Fig. 3, showing how the agent detects the\nerror, responds to the unexpected outcome, and initiates the\nrecovery process by updating the plan based on the new\nlocation. This experiment was carried out 30 times in an\noffice environment to thoroughly test the robustness of the\nagent. Experiment results are shown in Table.II To detect\nand recover from situations where the robot hits a wall, we\nmodified the TurtleBot to output a message when it remains\nstationary for a certain amount of time after receiving a\nmovement command. This allows the agent to recognize\nthe issue and initiate a recovery process by adjusting the\nnavigation plan. During the experiment, we observed that\nthe localization error occasionally happens when the agent\nis in a visually similar area, but managed to recover from\nsome cases using commonsense by considering the past path\nit took."}, {"title": "D. Commonsense Reasoning for Hazard Detection", "content": "The objective of this experiment is to evaluate the agent's\nability to identify potential hazards in the environment,\ncommunicate these risks to the user effectively, and adapt\nits actions based on the user's decisions. To test this, images\nof obstacles and potential hazards were placed along a\nnavigation path, including common dangers such as warning\nsigns, physical barriers, and overhanging objects. The agent\nwas evaluated on its ability to detect these hazards and pro-\nvide appropriate warnings to the user, suggesting alternative\nroutes when necessary. As shown in Table.III key metrics\nincluded hazard detection accuracy, with a focus on true\npositive rates and false positives, such as instances where\nthe agent incorrectly identified non-hazardous objects (e.g.,\na can of coke near a wall) as potential dangers. An example\nscenario is reflected in Fig. 3 where the agent demonstrates\nits commonsense reasoning by going beyond predefined\nnavigation instructions. Upon detecting potential hazards,\nthe agent warns the user and provides possible alternatives.\nFor example, if an obstacle blocks the path or if a risky\nsurface (such as a wet floor sign) is identified, the agent\nalerts the user with a warning message and suggests a safer\nalternate route. Once the hazard is communicated, the system"}, {"title": "E. Personalization Potential", "content": "Research on navigational aid preferences shows that per-\nsonalization is crucial for improving the user experience\n[53]. The LLM-based system is capable of meeting these\ndiverse preferences through natural language interaction,\nallowing users to easily communicate their specific needs.\nBy simply talking to the agent, users can adjust parameters\nsuch as preferred walking speed, route type (e.g., avoiding\nstairs), or safety concerns (e.g., avoiding potential hazard).\nThis flexibility in tailoring the experience highlights the\npersonalization potential of the LLM system. To evaluate\nhow user preferences are reflected, we re-named some spaces\nin text topological map for example, concert hall, food court,\nnoisy area and quiet area. We evaluated the agent's routing\nperformance based on user preferences, testing it 10 times\non the same route to determine if it made different choices.\nThe results were as follows: users preferring detailed step-\nby-step navigation instructions during navigation had their\npreferences met 10 out of 10 times, those preferring quiet\nroutes had their preferences met 10 out of 10 times, and users\nwho preferred stairs over elevators saw their preferences\nmet 3 out of 10 times. The primary reason for the high\nfailure rate in the last experiment was the excessive number\nof route options available to reach the destination, which\nmay have exceeded the maximum allowed prompt, leading\nto the failure. As shown in Fig.3, left corner of the map\nhad rectangular shape causing path planning module produce\n8 route options causing agent failing to reflect on user\npreferences."}, {"title": "V. CONCLUSION AND FUTURE WORK", "content": "We introduced Guide-LLM, an innovative framework\nleveraging Large Language Models (LLMs) and novel text-\nbased topological map to assist persons with visual im-\npairments (PVI) in navigating large indoor environments.\nOur system successfully demonstrated the ability to provide\nefficient, adaptive, and personalized navigation, significantly\nreducing the need for detailed user instructions. Future work\nwill focus on expanding the system's capabilities, including\nautonomous exploration and map generation, as well as\naddressing real-time challenges such as obstacle avoidance.\nTesting in real-world scenarios with PVI will be on future\nworks to improve and refine the system. These advancements\nwill bring us closer to providing a comprehensive assistive\nsolution that empowers PVI with greater independence and\nconfidence in navigating complex environments."}]}