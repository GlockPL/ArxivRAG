{"title": "Flat'n'Fold: A Diverse Multi-Modal Dataset for Garment Perception and Manipulation", "authors": ["Lipeng Zhuang", "Shiyu Fan", "Yingdong Ru", "Florent Audonnet", "Paul Henderson", "Gerardo Aragon-Camarasa"], "abstract": "We present Flat'n'Fold, a novel large-scale dataset\nfor garment manipulation that addresses critical gaps in\nexisting datasets. Comprising 1,212 human and 887 robot\ndemonstrations of flattening and folding 44 unique garments\nacross 8 categories, Flat'n'Fold surpasses prior datasets in size,\nscope, and diversity. Our dataset uniquely captures the entire\nmanipulation process from crumpled to folded states, providing\nsynchronized multi-view RGB-D images, point clouds, and\naction data, including hand or gripper positions and rotations.\nWe quantify the dataset's diversity and complexity compared\nto existing benchmarks and show that our dataset features\nnatural and diverse manipulations of real-world demonstra-\ntions of human and robot demonstrations in terms of visual\nand action information. To showcase Flat'n'Fold's utility, we\nestablish new benchmarks for grasping point prediction and\nsubtask decomposition. Our evaluation of state-of-the-art mod-\nels on these tasks reveals significant room for improvement.\nThis underscores Flat'n'Fold's potential to drive advances in\nrobotic perception and manipulation of deformable objects. Our\ndataset can be downloaded at https://cvas-ug.github.\nio/flat-n-fold", "sections": [{"title": "I. INTRODUCTION", "content": "Manipulating garments remains a significant challenge in\nrobotics. Tasks such as flattening and folding require under-\nstanding the vast space of configurations that garments can\nadopt [1], [2], and planning complex sequences of actions\naccordingly [3], [4]. This makes it vital to have large, diverse\ndatasets from which to learn effective perception models and\nmanipulation strategies. However, current efforts to capture\na representative dataset of garments being manipulated [2],\n[5], [6] are limited in their utility for developing advanced\nrobotic capabilities, due to their relatively small scope and\ndiversity, as well as a lack of ground-truth action annotations.\nIn this paper, we introduce a novel dataset, Flat'n'Fold\n(Fig. 1), which includes 1,212 human and 887 robot demon-\nstration sequences across 44 unique garments within eight\ncategories and surpasses existing datasets in size, scope,\nand diversity (Tab. I). Flat'n'Fold is a multi-view, multi-\nmodal dataset that captures the process of flattening and\nfolding different garments, starting from a crumpled garment.\nSpecifically, our dataset includes:\n\u2022 Human Demonstrations, where twenty human partici-\npants performed the flattening and folding tasks. Our\nobjective is to capture a wide array of human strategies\nin garment handling and the diversity and complexity\nof garment manipulation tasks.\n\u2022 Human-controlled Robot Demonstrations, where an ex-\npert human operator controls a robot to execute similar\ngarment manipulation tasks, aiming to replicate natural,\nhuman-like approaches within the robot's operational\nlimitations.\nMoreover, our dataset is the first to include synchronized\nRGB-D image sequences with the position and rotation\nof the participants' hands (in human demonstrations) and\nrobot grippers (in robot demonstrations), as well as camera\nparameters. To show the utility of our dataset, we set new\nbenchmarks and baseline metrics for:\n\u2022 Grasping point detection with 6,329 annotated point\nclouds from the human demonstrations and 5,574 from\nthe robot demonstrations dataset. For this, we evaluated\ntwo popular models, PointNet++ and Point-BERT, and\nestablished a performance baseline.\n\u2022 Subtask decomposition from human and robot demon-\nstrations with ~20,000 annotated sub-task boundaries\ncorresponding to 'pick' and 'place' actions."}, {"title": "II. RELATED WORK", "content": "Human demonstrations significantly enhance robotic train-\ning by providing detailed examples of natural, dexterous\nmanipulations from which to learn (e.g. [4], [11]\u2013[13]),\ninstead of defining inflexible manipulation strategies by hand\n[1], [3], [14], [15]. Human demonstrations have shown to be\nessential for long-horizon tasks for manipulating rigid and\nsimple deformable objects [16] and have enabled robots to\nanticipate future states and make decisions by optimizing\npolicies throughout the task's duration [17], [18].\nSeveral existing datasets of human demonstrations of\nmanipulating deformable objects have been used to train\nrobotic systems for various tasks. However, these datasets are\nlimited and have not let robots acquire all the skills necessary\nto manipulate garments, as shown in Fig. 1. For example,\nthe MIME dataset [10] provided video demonstrations of\n20 tasks but focused on rigid objects. That is, only one task\ninvolved a deformable object, wiping with a cloth, which was\ncaptured using single-view RGB-D data. DeepFashion [19]\nincluded only clothing and was intended for recognition\ntasks such as classification and attribute recognition rather\nthan manipulation. Similarly, DeepDeform [5] presented an\napproach for non-rigid reconstruction of dynamic objects\nusing RGB-D cameras. Tzelepis et al. [6] introduced a\ndataset of RGB images of human demonstrations while\nhandling a tablecloth with the aim of semantic state esti-\nmation. However, the tasks considered were relatively simple\n(e.g. lifting and diagonal fold) and lacked complex manipula-\ntion sequences. The Video Human Demonstration dataset [2]\nconsisted of Kinect cameras mounted on a folding table, and\n1,000 folding demonstrations were captured from multiple\nangles and participants. While this dataset provides RGB-\nD images and skeleton keypoints of human actions, it did\nnot include 3D spatial position and rotation information (nor\ncamera parameters that could be used to infer these), which\nare required for imitation learning and policy learning in\nrobotics.\nSimilarly, existing datasets relevant to deformable objects\nthat use robot demonstrations or surrogate systems remain\nlimited in scope and diversity. For instance, GarmentTrack-\ning [7] introduced the VR-Garment recording system, which\nenabled users to interact with virtual garment models via\na VR interface. The VR-Folding dataset was introduced\nand included complex garment poses for tasks such as\nflattening and folding. However, the VR-Folding dataset\nwas inherently bound to virtual reality (VR) environments,\nwhich introduced limitations in terms of its relevance to\npractical robotic operations in real-world scenarios. For real-\nworld applications, Boix-Granell et al. [8] designed a Unity-\nbased 3D platform using an HTC Vive Pro system for\ntablecloth manipulations only. Similarly, Moletta et al. [15]\nattempted to automate cloth folding with a system that\nemployed skeleton representations to generate folding plans.\nAlthough this helped replicate specific folding techniques,\nonly three garment classes and fixed folding procedures were\nconsidered, reducing its adaptability to the diverse techniques\nrequired in real-world settings. Another significant work,\nthe Roboturk dataset [9], used a smartphone-based 6-DoF\ncontroller with cloud integration, and a robotic arm was used\nto smooth out garments such as hand towels, jeans, or t-shirts\non a table. Roboturk captured these sequences from a single\ncamera perspective, restricting its application for complex\ntasks that require dual-arm coordination or depth perception.\nTo overcome the limitations of existing datasets, we in-\ntroduce Flat'n'Fold, a dataset comprising human and robot\ndemonstrations, including various garment types, manipula-\ntion tasks, and garment flattening and folding sequences,\nproviding visual and action information. Tab. I compares\nour Flat'n'Fold dataset against existing datasets focused on\nhuman demonstrations and robotic manipulation of garments.\nIn the table, 'Agent' represents whether it is human demon-\nstrations or robot demonstrations; 'Hum. Action' means\nwhether human action data is recorded and saved during\nhuman demonstrations; 'Ann.' represents extra annotations.\nFlat'n'Fold has a clear advantage in terms of data volume,\ndiversity, and modalities recorded."}, {"title": "III. MATERIALS AND METHODOLOGY", "content": "The hardware used is shown in Fig. 2. The setup for the\nhuman demonstration dataset integrates visual and motion\ninformation from three ZED2i cameras, a VR controller,\nand three mechanical pedals interfaced via USB. The ZED2i\ncameras were positioned to provide multiple views and\nminimize occlusions from human arms during garment ma-\nnipulation. These views encompass the front, top, and back of\nthe garment. The ZED2i cameras recorded 1080p images at\n15Hz. For motion tracking, we used SteamVR [22] with two\nHTC Vive Trackers, where the VR headset [20] served as the\nworld coordinate frame to which robot and tracker reference\nframes are linked. Two HTC Vive Trackers were affixed\nto the backs of participants' hands, capturing position and\nrotation in 3D space relative to the headset at 15Hz. Three\npedals were used to record the timing of actions involving\nthe left or right hands and both hands, annotating when\nparticipants grasped or released the garment."}, {"title": "B. ZED2i Camera Calibration", "content": "We place calibration boards at various positions on the\ntable (see Figs. 2 and 1) for camera calibration and use\nOpenCV's PnP solver [25] to estimate the relative camera\npose with respect to the table. To establish the calibration\nboard's position relative to the headset, a tracker was posi-\ntioned in the right corner of each board (designated as the\norigin in OpenCV). The collected data was averaged over 10\nrepetitions to robustly estimate the relative positions of each\ncamera to the headset, ensuring accurate spatial data in our\nexperiments."}, {"title": "C. Methodology", "content": "For the human demonstration dataset, we invited 20 partic-\nipants to capture how they manipulate garments from crum-\npled to folded states. Participants, equipped with trackers on\nboth hands, were asked to stand behind a tablewere briefed\non movement guideguidelines before data collection com-\nmenced. These guidelines included avoiding finger bending\nand only performing pick-and-place actions, minimizing arm\ncrossing, and gently shaking the garment for easier handling.\nThey were also advised to keep their hands within the bounds\nof the table as much as possible. For robot demonstrations, an\noperator familiar with the robotic arm controlled the robot's\narms and grippers (Fig. 2-9).\nThe human and robot demonstration stages are illustrated\nin Fig. 1. For both, demonstrations began with the par-\nticipants or robot operator grasping a crumpled garment\nfrom any point using either hand or gripper and lifting\nthe garment from the table. The flattening stage was not\nstandardized; hence, participants or robot operator grasped\nand manipulated the garment in the air until it was nearly\nflat and then laid on the table. After, they were allowed to\nmake further adjustments to enhance the garment's flatness.\nWe split participants in terms of whether they followed a\nfixed folding strategy (i.e. Fixed) or their usual, natural way\nto fold garments in their daily life (i.e. Daily-life). We must\nnote that 7 participants performed both strategies; hence, the\nFixed strategy comprised 17 participants, while the Daily-life\nstrategy comprised 10 participants. The participants carrying\nout the Fixed strategy adhered to a structured folding ap-\nproach such that all participants had a similar folded garment\nat the end of the manipulation. This approach differs from\nthe simpler folding methods used in previous studies and\nis intended to reflect realistic, practical folding techniques.\nThe robot operator followed both strategies while folding\ngarments.\n1) Data Format: Both visual and action data were cap-\ntured and stored using ROS2 bag files. We use nearest-\ntimestep matching to synchronize images with action data\n(similar to [16]). This synchronization ensures that for each\ncamera, the number of RGB images equals the number of\ndepth images. Moreover, the quantity of synchronized action\ndata matches the number of images, ensuring that camera\ndata aligns with the corresponding action data.\nIn the human demonstration dataset, the action data com-\nprises the position and rotation of the tracker's center. For"}, {"title": "D. Dataset overview", "content": "Our human and robot demonstration datasets use 44 di-\nverse garments from 8 categories: napkins, towels, pants,\nlong-sleeved shirts (LS-shirt), short-sleeved shirts (SS-shirt),\nlong-sleeved t-shirts (LS-tshirt), short-sleeved t-shirts (SS-\ntshirt), and sweaters. Each category includes five items, ex-\ncept for pants, which consist of six. The garments within each\ncategory vary in size and stiffness, covering a wide range of\nfabrics and material properties. The dataset consists of 1,212\nindividual flattening and folding sequences of human and 887\nrobot demonstrations. The data distribution across different\ncategories and individual garments is shown in Tab. II.\n1) Human Demonstration Dataset: Each sequence com-\nprises RGB images and depth images from three cameras,\naction sequences (capturing both the position and rotation\nof trackers related to the headset), and the timing of finger\ninteractions with the garment. Each sequence documents\na complete process of flattening and folding the garment.\nAdditionally, each sequence records the position and rotation\nat the gripping points and is annotated with multiple labels:\nclothing type, clothing name, and the folding strategy of the\naction (Fixed or Daily-life). As described in Sec. III-C, the\nFixed strategy followed a standardized folding method, and\nthe Daily-life strategy followed a natural, everyday folding\ntechnique.\n2) Robot Demonstration Dataset: For the robot dataset,\nwe collected 887 sequences using the same 44 garments as in\nthe human demonstration dataset. This dataset comprises the\nrobot's joint states and the position and rotation at the center\nof the wrist reference frame relative to its world origin, in\naddition to the multi-view RGB images and depth images.\nWe also recorded gripper opening and closing states."}, {"title": "IV. EXPERIMENTS", "content": "We first compare the diversity and complexity of\nFlat'n'Fold to existing datasets (Sec. IV-A). Then, we define\ntwo benchmarks for evaluating grasp prediction (Sec. IV-B)\nand sub-task decomposition (Sec. IV-C)."}, {"title": "A. Quantifying the diversity of the dataset", "content": "We first measure the diversity and complexity of our\ndataset compared to RoboTurk [9], MIME [10], VideoFold-\ning [2], and Semantic State Estimation [6], considering both\naction and visual information. As shown in Tab. III, we\nmeasure complexity and diversity for action information.\nFor complexity, we calculate the variance of positions and\nrotations across different time intervals within each action\nsequence. Then, we take the mean across all videos to\nmeasure the dataset's average complexity. To measure the\naverage diversity of action sequences, we uniformly sam-\npled each sequence for 300 time ticks across datasets. The\nvariance across sequences at individual time points was\nthen calculated and averaged over the sequence duration\nto determine the extent of variation among different action\nsequences.\nFor visual information (Tab. IV), we extract features\nfrom each video using a pre-trained I3D model [26] after\nstandardizing to 256 frames across sequences for all datasets.\nWe then calculate the global standard deviation of these\nfeatures to measure diversity.\nIn Tables III and IV, a lower value means the dataset\nis less complex and diverse. For action diversity, both the\nhuman and robot demonstration datasets exhibit significantly\nhigher position standard deviation than other datasets. Specif-\nically, the position standard deviation for average complexity\nin human demonstrations (Tab. III) surpasses that of Robo-\nTurk by 133.33% and MIME by 3219.51%. Similarly, the\nrotation standard deviation (Euler angle) for average com-\nplexity in human-controlled robot demonstrations exceeds\nthat of RoboTurk by 14.63% and MIME by 38.56%. This\ntrend can also be observed for visual diversity, where the\nstandard deviation in our human demonstration dataset is\n7.91% greater than that observed in VideoFolding for com-\nparable tasks. Similarly, our human-controlled robot dataset's\nstandard deviation exceeds RoboTurk's by 5.33%. We can,\ntherefore, conclude that Flat'n'Fold shows a broader range\nof visual and action data and demonstrates the applicability\nof our dataset in diverse settings for garment perception and\nmanipulation."}, {"title": "B. Grasping Point Prediction Benchmark", "content": "Grasping point prediction is an essential subtask for gar-\nment manipulation but remains challenging in part due to\na lack of comprehensive training datasets with annotated\nground truth. Research into garment grasping points has been\nconfined mainly to simplistic scenarios, such as picking up\ncentrally located points on crumpled fabrics, garments on\na flat surface [27] or grasping garments hung on hangers\n[28]. Our dataset addresses this gap by providing annotated\ngrasping points while manipulating garments from a crum-\npled state to a folded state.\nTo establish a benchmark for subsequent studies, we define\na subset of our dataset comprising points in time when\nthe hand/gripper is about to grasp the garment. We extract\nground truth information at the instant of grasping, i.e. which\nhand grasps the garment (left/right), its position and its\nrotation. We extract point-clouds from 8 to 10 frames earlier,\nfusing the three views and segmenting to include only the\ngarment; this avoids leakage of the arm position for grasping\nprediction. This yields 6,329 annotated point clouds from the\nhuman subset and 5,574 from the robot subset. The goal is\nthen to predict the optimal grasp location given a point-cloud.\nAs metrics, we measure the classification accuracy (left vs\nright hand), the L1 error of positions and the geodesic error\nof rotations. The dataset is divided into training, validation,\nand testing sets in a 7:1:2 ratio across all garment types.\nAs baselines, we evaluate two popular models operating\non point-clouds, PointNet++ [29] and Point-BERT [30]. Both\nwere pretrained on ModelNet-40, and we added two fully-\nconnected layers to predict hand position (supervised with\nan L1 loss), rotation quaternion (with geodesic loss), and\nleft/right hand (with cross-entropy loss). We conducted two\nexperiments using these models to examine the effects of\nvarying dataset sizes and garment types on the outcomes.\nIn Tab. V, we show results when training on different frac-\ntions of our dataset. All metrics consistently improve with the\namount of training data in human and robot demonstrations,\nwhich shows the value of using a large-scale dataset for\ngrasping garments. For example, on the human demonstra-\ntions subset, PointNet++'s classification accuracy increases\nfrom 55.7% using 20% of training data to 59.9% with the full\ntraining set, with corresponding reductions in position error\n(from 0.103 meters to 0.1 meters) and rotation error (from\n0.028 radians to 0.023 radians). However, even with the full\ndataset, the results are still far from perfect which indicates"}, {"title": "C. Automated Subtask Decomposition Benchmark", "content": "Mastering long-horizon manipulation tasks such as flat-\ntening and folding remains challenging. One approach is\nto break them into smaller, more manageable subtasks to\neffectively learn these tasks and generalize to new situations.\nWe, therefore, use our dataset to define a benchmark for task\ndecomposition methods; an example of task decomposition\ncan be found in Fig. 3.\nWe define 'pick' and 'place' actions ground-truth sub-task\nboundaries. Specifically, we divide each image sequence into\ntwo phases. During the flattening phase, we treat every pick\naction as a subtask but not place since these often cause no\nchange to the garment. During the folding phase, we treat\nevery pick or place as a subtask. During the evaluation, we\nconsider a predicted subtask boundary correct if it is within\n10 frames of the annotated ground truth; based on this, we\ncalculate the precision, recall, and F1 score.\nAs a baseline, we evaluate the state-of-the-art unsuper-\nvised task decomposition UVD [31], using VIP [32] as the\nvisual encoder Results are shown in Tab. VII. The precision\nfor both human demonstration and robot demonstration is\nrelatively high at 0.713 and 0.621, respectively. This shows\nthat the method accurately identifies relevant subtasks when\nthe actions involve direct manipulation on a stable surface.\nHowever, lower recall rates of 0.620 for humans and 0.361\nindicate that many subtask boundaries are missed. For both\nsubsets, precision varied across phases, with a lower rate of\n0.520 and 0.416 during flattening but a much higher rate of\n0.905 and 0.825 during folding. When comparing the two\nsubsets, the UVD method demonstrates better performance\nacross all metrics in human demonstrations.\nWe also further analyzed whether different folding strate-\ngies affect the effectiveness of UVD. The results from\nTab. VIII show that varied folding approaches introduce com-\nplexities that affect the UVD method's performance. Specif-\nically, participants folding garments in their style (Daily-\nlife) demonstrated lower precision (0.894) and recall (0.571)\ncompared to those following predefined rules (Fixed), which\nachieved a precision of 0.905 and a recall of 0.852, resulting\nin an F1 score of 0.684. The latter indicates that there is a\nneed to develop approaches that can capture more diverse\nmanipulation strategies."}, {"title": "V. CONCLUSION", "content": "We have introduced Flat'n'Fold, a dataset comprising over\n1,212 human and 887 robot demonstrations of flattening\nand folding 44 unique garments spanning eight categories.\nFlat'n'Fold contains high-resolution multi-view RGB-D and\npoint cloud data, as well as ground-truth actions. We have\nalso defined two new benchmarks for garment manipulation\ntasks: grasping point prediction and subtask decomposition.\nhe future, the size and scope of our dataset can enable\ntraining and benchmarking on new tasks. By providing\ndiverse human actions, the dataset can be used for imitation\nlearning, enabling robots to learn complex manipulation\ntasks. Additionally, Flat'n'Fold's multimodality can support\nthe future development of models that accurately perceive\nand predict the state of garments, which is essential for\nimproving the performance of cloth pose estimation and\nplanning manipulation tasks."}]}