{"title": "Deep Learning-based Dual Watermarking for Image Copyright Protection and Authentication", "authors": ["Sudev Kumar Padhi", "Archana Tiwari", "Sk. Subidh Ali"], "abstract": "Advancements in digital technologies make it easy to modify the content of digital images. Hence, ensuring digital images' integrity and authenticity is necessary to protect them against various attacks that manipulate them. We present a Deep Learning (DL) based dual invisible watermarking technique for performing source authentication, content authentication, and protecting digital content copyright of images sent over the internet. Beyond securing images, the proposed technique demonstrates robustness to content-preserving image manipulations. It is also impossible to imitate or overwrite watermarks because the cryptographic hash of the image and the dominant features of the image in the form of perceptual hash are used as watermarks. We highlighted the need for source authentication to safeguard image integrity and authenticity, along with identifying similar content for copyright protection. After exhaustive testing, we obtained a high peak signal-to-noise ratio (PSNR) and structural similarity index measure (SSIM), which implies there is a minute change in the original image after embedding our watermarks. Our trained model achieves high watermark extraction accuracy and to the best of our knowledge, this is the first deep learning-based dual watermarking technique proposed in the literature.\nImpact Statement-Watermarking is widely used for authentication and copyright protection. Lately, deep learning has been leveraged to perform watermarking, which yields a higher accuracy. However, existing deep learning techniques can only perform either authentication or copyright protection and are vulnerable to overwriting and surrogate model attacks. In this work, we used deep learning to perform image copyright protection and authentication simultaneously while achieving a high level of accuracy, performance and robustness against content-preserving image manipulation, overwriting attack and surrogate model attack. The watermarked images are indistinguishable from the original image, with an average peak signal-to-noise ratio (PSNR) of 46.87 dB and structural similarity index measure (SSIM) of 0.94 while maintaining a high accuracy of 97% for copyright protection and 95% accuracy for authenticating images.", "sections": [{"title": "I. INTRODUCTION", "content": "THE rapid advancement of digital technology in the domain of image processing has paved the way for creating high-resolution digital content. Side-by-side, this advanced technology also helps the attacker to launch attacks on digital content, such as illegally manipulating, duplicating, or accessing information without the owners' knowledge, violating the integrity, authenticity, and copyright protection of the target digital content. Therefore, developing secure copyright protection and authentication has become a challenging task.\nDigital watermarking is an effective technique for achieving copyright protection and authentication. The process of digital watermarking involves two primary operations: watermark embedding and watermark extraction. The sender embeds the watermark into the image to be protected (cover image) and sends the watermarked image to the receiver. The original watermark is provided to the receiver or verifier in advance to perform digital content authentication or copyright protection. In order to validate the authenticity or copyright, the verifier has to extract the watermark from the received watermarked image and compare it to the original watermark.\nIn general, there are two types of digital watermarks addressed in the existing literature: visible and invisible watermarks [1], [2]. A visible watermark typically contains a visible message or a logo as a watermark in digital content. On the other hand, in invisible watermarking, a watermark is embedded and remains invisible, such that the digital media appears visually similar to the original image. Invisible watermarks can be broadly classified into fragile and robust watermarks. Fragile watermarking is used in applications where even a single-bit change in the content is not allowed [3], [4], [5], [6], as the single-bit change in the digital content will alter the watermark and will make it invalid. Therefore, fragile watermarking is used in digital content authentication and integrity verification. Robust watermarking shows robustness against content-preserving image manipulations, such as resizing, rotation, flipping, format conversion, etc [7], [8], [9], [10], which makes it the most suitable watermarking technique for copyright protection. However, the degree of robustness is application-specific.\nDual watermarking is another type of watermarking technique where two watermarks are embedded in the cover image to accomplish two different tasks [11], [12]. The most prevalent dual watermarking techniques combine a robust watermarking technique with a fragile watermarking technique [13], [14], [15], [16], [17], [18], [19], [20], [21]. Traditional image watermarking techniques are specific to their applications and not easily adaptable to other applications. For example, the watermarking used in medical imaging can not be directly applicable to images used in remote-sensing applications. In the case of invisible watermarking, its adaptability is further aggravated as it hides the watermark at the cost of sacrificing the image quality. Recently, deep learning technique has evolved significantly and has wide applications. The image watermarking community has also started using it as a viable tool for watermarking as it not only provides significant improvement in performance and efficiency in comparison to traditional watermarking but is also adaptable to different applications [22], [23], [24], [25]. However, it also has the"}, {"title": "following challenges:", "content": "1) Difficulty in source authentication: Determining whether the watermarked image is generated from the desired source [26] is challenging.\n2) Vulnerable to overwriting attack: The watermark embedded by a DNN can also be overwritten using another DNN to claim ownership of the image (violating copyright protection) [26], [27].\n3) High overhead: It incurs high communication overhead in the form of input watermarks, which should be present with the receiver for verification [28], [29].\nWe propose a deep learning-based dual watermarking technique to overcome the above issues. Our contributions to the proposed work include the following:\n\u2022 We propose a new dual watermarking-based image authentication technique to verify content, source authentication, and protect digital content copyright.\n\u2022 We combine the benefits of hashing and watermarking techniques. Instead of using a random watermark, we used the image's cryptographic and perceptual hash as watermarks, which eliminates the need for the original watermark and facilitates auto-verification.\n\u2022 In our dual watermarking technique, the two extraction phases are independent of each other. The first extraction phase protects digital content copyright, while the second is used for content and source authentication.\n\u2022 Our technique is robust against content-preserving image manipulations such as noise additions, scaling, rotation, filtering, compression, change in brightness, and contrast and also secure against surrogate model attacks and overwriting attacks.\n\u2022 We demonstrated the efficacy of the proposed technique through extensive experimental results.\nThe rest of the paper is organized as follows. In Section-II, the background related to our technique is explained. Section- III gives the motivation behind the proposed work, followed by the proposed technique in Section-IV. The experimental setup and model training are presented in Section-V. The results and efficacy of our technique are discussed in Section-VI. We also discussed a comparative study of our work with some existing techniques in Section-VII. Finally, the conclusions and future scope are in Section-VIII."}, {"title": "II. BACKGROUND", "content": "A. Perceptual Hash\nSecure Hash Algorithm (SHA) is one of the standard cryptographic hash functions with a strong avalanche effect, which means any tiny change to the input will lead to a significant change at the output (hash value) [30], [31], [32]. When it comes to image processing, cryptographic hash becomes less useful, specifically where certain perceptual content-preserving image manipulations, such as JPEG compression, noise, blurring, cropping, deformations, etc, are allowed as these manipulations will alter the hash value. In this line, the perceptual hash is a class of comparable hash functions that generate a distinct and unique hash using robust or invariant features present in the input image [33], [34], [35], [36], [37], [38] such that two visually similar images should produce the same perceptual hash value. There are four basic techniques for generating perceptual hash: frequency transformations, dimensionality reduction, statistical methods, and extracting the local features of an image [39].\nThe most prominent technique for calculating the perceptual hash is the Discrete Cosine Transformation (DCT) technique [40], [41], [42]. In this technique, the input image is expressed in terms of cosine functions oscillating at different frequencies. Frequency coefficients representing the lower frequency components contain important features of an image that are robust against distortions. In DCT-based perceptual hash, the input color image is first converted into a grayscale image and resized to 32 \u00d7 32 pixels. The grayscale image is then converted to the 32\u00d732 frequency domain image through DCT transformation. The 8 \u00d7 8 block of pixels in the upper-left portion of the image represents the low-frequency block of the resized grayscale image. Each frequency coefficient present in the low-frequency block of pixels is compared to the mean value of all the frequency coefficients of pixels in the low-frequency block. If the value of an individual frequency coefficient is greater than the mean value, its phase is taken as 1, and if it is not, it is taken as 0. The final hash is obtained by arranging the 64-bit binary values sequentially, thus representing the perceptual hash of an image.\nB. Deep learning-based watermarking\nDeep learning-based watermarking techniques are the newest approach that can efficiently watermark images by overcoming the drawbacks of traditional watermarking techniques [22], [23], [24], [25]. In deep learning-based watermarking, encoder and decoder networks are used to perform watermarking, as shown in Fig-1. The encoder network constructs a watermarked image by embedding the watermark into the cover image, as shown in the upper part of Fig- 1. Once the watermarked image is constructed, the decoder network (shown at the bottom part of Fig-1) is used to extract the watermark from the watermarked image. Deep learning-based watermarking has its own challenges. The fragile nature of DNN can sometimes cause the model to fail [43]. As a result, fully utilizing the ability of deep neural networks to learn automatically and generalizing it to both watermark embedding and extracting processes is difficult. The lack of a good balance of imperceptibility and robustness is also a major concern in deep learning-based watermarking [44], [45], [23], [46], [47]."}, {"title": "III. MOTIVATION", "content": "Traditional image watermarking methods face challenges due to their limited robustness against content-preserving image manipulation. For example, extraction can only tolerate certain types of attacks; thus, rotation to certain angles or blurring of the image to a larger degree may result in distortions in embedded watermarks. Deep learning-based watermarking is more robust to content-preserving image manipulations [44], [43], [45], [46], however, maintaining a balance between robustness and imperceptibility is a challenge [23]. Although the"}, {"title": "IV. PROPOSED METHODOLOGY", "content": "We propose a deep learning-based invisible dual watermarking technique that has all the features mentioned above. This dual watermarking technique serves three objectives at once: 1) protecting digital content copyright, 2) content, and 3) source authentication. Perceptual hash is used as the first watermark for protecting digital content copyright, whereas cryptographic hash is used as the second watermark for content and source authentication.\nA. Watermark Embedding\nThe watermark embedding process is shown in the upper part Fig-2. The first step in our technique is to find the perceptual hash of the cover image \\(I\\), which is our first watermark. There are different techniques for calculating the perceptual hash of an image. We have chosen DCT-based perceptual hash as discussed in Section-II-A. It has advantages in terms of robustness against distortions and attacks caused by content-preserving image manipulation. We compute 64-bit DCT-based perceptual hash \\(ph\\) of \\(I\\), which is passed into a pre-trained Fully Connected layer (FC) to produce 256 x 256 x 1 tensor and then upsampled to a 400 \u00d7 400 \u00d7 1 tensor. The upsampled tensor is concatenated to \\(I\\) as the fourth channel and passed to the encoder \\(E_1\\). The job of \\(E_1\\) is to embed \\(ph\\) into \\(I\\) as a watermark and produce a 400 \u00d7 400 \u00d7 3 image as output, which is our first stage watermarked image \\(W_1\\). It is ensured that \\(W_1\\) will have the same perceptual hash as \\(I\\) by using three loss functions, which are residual regularization, LPIPS perceptual loss, and critic loss. More details regarding these loss functions are discussed in Section- V-B.\nNext, we compute the 256-bit cryptographic hash \\(h\\) of \\(W_1\\) using SHA-3 and use it as our second watermark. The \\(h\\) is passed through the same pre-trained FC to produce a 256 \u00d7 256 \u00d7 1 tensor that is upsampled to 400 \u00d7 400 \u00d71 tensor. The upsampled tensor is concatenated to \\(W_1\\) as the fourth channel and 400 \u00d7 400 \u00d7 4 tensor is given as input to the second encoder \\(E_2\\), which produces a residual image \\(R\\) of resolution 400 \u00d7 400 \u00d7 3 as the output. The obtained \\(R\\) is added to \\(W_1\\), which yields the second stage watermarked image \\(W_2\\) that is perceptually similar to the \\(I\\). This addition of residual image (R) does not change the perceptual hash and perceptual similarity of \\(W_2\\) with respect to \\(I\\). This is ensured by using appropriate loss functions while training. Adding to"}, {"title": "tication into the watermarking technique will ensure protection against surrogate model attacks.", "content": "4) Auto verification of watermark: The watermark verification should be performed by a unique decoder automatically at the receiver's end without requiring original watermarks. Hence, communication overhead is reduced.\n5) Robustness against content-preserving image manipulation: Deep learning-based watermarking techniques should perform watermark verification successfully even if the watermarked images are maliciously manipulated or tampered with to a certain extent."}, {"title": "IV. PROPOSED METHODOLOGY", "content": "We propose a deep learning-based invisible dual watermarking technique that has all the features mentioned above. This dual watermarking technique serves three objectives at once: 1) protecting digital content copyright, 2) content, and 3) source authentication. Perceptual hash is used as the first watermark for protecting digital content copyright, whereas cryptographic hash is used as the second watermark for content and source authentication.\nA. Watermark Embedding\nThe watermark embedding process is shown in the upper part Fig-2. The first step in our technique is to find the perceptual hash of the cover image \\(I\\), which is our first watermark. There are different techniques for calculating the perceptual hash of an image. We have chosen DCT-based perceptual hash as discussed in Section-II-A. It has advantages in terms of robustness against distortions and attacks caused by content-preserving image manipulation. We compute 64-bit DCT-based perceptual hash \\(ph\\) of \\(I\\), which is passed into a pre-trained Fully Connected layer (FC) to produce 256 x 256 x 1 tensor and then upsampled to a 400 \u00d7 400 \u00d7 1 tensor. The upsampled tensor is concatenated to \\(I\\) as the fourth channel and passed to the encoder \\(E_1\\). The job of \\(E_1\\) is to embed \\(ph\\) into \\(I\\) as a watermark and produce a 400 \u00d7 400 \u00d7 3 image as output, which is our first stage watermarked image \\(W_1\\). It is ensured that \\(W_1\\) will have the same perceptual hash as \\(I\\) by using three loss functions, which are residual regularization, LPIPS perceptual loss, and critic loss. More details regarding these loss functions are discussed in Section- V-B.\nNext, we compute the 256-bit cryptographic hash \\(h\\) of \\(W_1\\) using SHA-3 and use it as our second watermark. The \\(h\\) is passed through the same pre-trained FC to produce a 256 \u00d7 256 \u00d7 1 tensor that is upsampled to 400 \u00d7 400 \u00d71 tensor. The upsampled tensor is concatenated to \\(W_1\\) as the fourth channel and 400 \u00d7 400 \u00d7 4 tensor is given as input to the second encoder \\(E_2\\), which produces a residual image \\(R\\) of resolution 400 \u00d7 400 \u00d7 3 as the output. The obtained \\(R\\) is added to \\(W_1\\), which yields the second stage watermarked image \\(W_2\\) that is perceptually similar to the \\(I\\). This addition of residual image (R) does not change the perceptual hash and perceptual similarity of \\(W_2\\) with respect to \\(I\\). This is ensured 3) Source authentication : Incorporating source authen- by using appropriate loss functions while training. Adding to"}, {"title": "that, we have also used two different techniques to embed the watermarks, thus reducing the chance of overlapping i.e,", "content": "one watermark distorting the other. The technique followed for embedding \\(h\\) into \\(W_1\\) is inspired by the approach of Stegastamp [50]. Dual watermarking makes it convenient to perform different verification in one watermarked image. In our case, \\(W_2\\) protects digital content copyright, content and source authentication.\nB. Watermark Extraction\nThe watermark extraction and verification process is shown in the bottom part of Fig-2, which consists of two independent extraction phases:\n1) Extraction phase 1: is used for digital content ownership verification. We compute the perceptual hash (\\(h'\\)) of \\(W_2\\) using DCT transformation. Then \\(D_1\\) extracts the embedded perceptual hash (\\(h\\)) from \\(W_2\\). If the extracted and the computed perceptual hashes of \\(W_2\\) match, i.e., \\(ph = ph'\\), the image ownership is claimed.\n2) Extraction phase 2: is used for content and source authentication for the given dual watermarked image \\(W_2\\). It uses two decoders: \\(D_2\\) and \\(D_3\\). \\(D_2\\) reconstructs a residual image (\\(R'\\)) from given \\(W_2\\). \\(R'\\) is passed into \\(D_3\\), which decodes the cryptographic hash (\\(h\\)) present in \\(R'\\). Subsequently, \\(R'\\) is subtracted from \\(W_2\\), and the SHA-3 hash value (\\(h'\\)) of the result is computed. If the cryptographic hash (\\(h\\)) extracted by \\(D_3\\) matches with the computed cryptographic hash (\\(h'\\)), i.e., \\(h = h'\\), the authenticity of the content and source of the image is verified.\nA robust copyright protection mechanism should be able to extract the watermark successfully from the watermarked image and claim ownership even if the watermarked image is tampered with. Hence, \\(D_1\\) used in the Extraction phase 1 should successfully extract the watermark from the tampered image to protect digital content copyright. To achieve this, we have used a DCT-based perceptual hash, which is robust against distortions, compression, white Gaussian noises, and Gaussian blurring. Moreover, the spatial transformer provides invariance against various image transformations such as translation, scaling, rotation, cropping, change in brightness and contrast and deformations, as discussed in the next Section- V-B. To verify the authenticity of the content and source for an image, the hash of the image should not change. Therefore \\(D_2\\) and \\(D_3\\) are used in the Extraction phase 2. It is ensured that \\(D_2\\) fails to reconstruct the residual image if the image is tampered with. Thus both \\(D_2\\) and \\(D_3\\) are trained on the images without adding any distortions."}, {"title": "V. EXPERIMENTAL SETUP", "content": "In this section, we present the experimental results of our technique. The model is trained in Python 3.6 with Pytorch [51]. Experiments are conducted on a machine with two 14-core Intel i9 - 10940X CPUs, 128 GB RAM, and two Nvidia RTX-5000 GPUs with 16 GB VRAM each.\nA. Dataset\nTo explore the generality of our technique, we performed our evaluations on two datasets, CelebA [52] and Mir-flickr [53]. CelebA contains 200k face images of various celebrities with different poses and backgrounds at a resolution of 178 x 218. Facial images are the ones that are mostly tampered and identifying the original source image will be a great help in discarding the fabricated media. During training,"}, {"title": "the images are upsampled to 400 \u00d7 400, which shows that our", "content": "technique can also be used for embedding dual watermarks in low-resolution images without creating distortions. The Mirflickr dataset consists of one million images from the social photography site Flickr, from which we have chosen 200k random images for training and testing our technique. It consists of different types of images with varied contexts, lighting, and themes, perfect for exhibiting our techniques' generalization. The images are of resolution 500 \u00d7 500, which is downsampled to 400 \u00d7 400 for training.\nWe have three instances of our proposed deep learning- based dual watermarking technique. The first instance of the model is trained on CelebA, the second on Mirflickr, and the third on the dataset consisting of images from both CelebA and Mirflickr. While training the first two instances of the model, 150k images are chosen as the training set and the remaining 50k images are chosen as the test set. For the third instance of our model, we have taken 100k random images each from CelebA and Mirflickr datasets. From the combined 200k images, 150k images are chosen as the training set and the remaining 50k images are chosen as the test set.\nB. Network Architecture\nOur model consists of five networks (\\(E_1, E_2, D_1, D_2\\) and \\(D_3\\)), which are trained in an end-to-end manner. We employ the U-Net [54] architecture for both \\(E_1\\) and \\(E_2\\) with skip connections between the encoder and decoder layers of each U-net. It helps \\(E_1\\) and \\(E_2\\) to generate watermarked images that are perceptually similar to the input cover image. Furthermore, the skip connections also preserve crucial spatial data and aid in recovering minute features that would have been lost during the downsampling of the image by the U-net encoders.\nWe employ a spatial transformer [55] to extract the perceptual hash, which takes \\(W_2\\) as an input and extracts its embedded perceptual hash \\(ph\\). The spatial transformer used in \\(D_1\\) provides invariance against various image transformations such as translation, scaling, rotation, cropping, deformations, change in brightness and contrast, etc. Decoding the cryptographic hash involves reconstructing \\(R'\\) using decoder \\(D_2\\) and extracting \\(h\\) from \\(R'\\) using \\(D_3\\). U-Net is used to reconstruct \\(R'\\) using \\(D_2\\), whereas we use a simple convolution neural network with five convolution layers followed by two fully connected layers for extracting the hash from \\(R'\\) using \\(D_3\\).\nC. Loss Function and Scheme Objective\nAs mentioned earlier, the model is trained in an end-to-end manner. Generally, embedding watermarks distorts the image, however, in our case, the perceptual hash and cryptographic hash introduce a small distortion in the cover image (\\(I\\)) such that the perceptual hash remains the same. Embedding the first watermark requires \\(I\\) and the perceptual hash (\\(ph\\)) of \\(I\\), which outputs \\(W_1\\). To ensure minimal distortion while embedding \\(ph\\) in \\(I\\) as a watermark, we used \\(L_2\\) residual regularization (\\(L_R\\)), the LPIPS perceptual loss (\\(L_P\\)) and a critic loss (\\(L_C\\)) functions. \\(L_R\\) reduces the chance of the encoder to overfit while \\(L_P\\) and \\(L_C\\) increases the perceptual similarity between \\(I\\) and \\(W_1\\). \\(L_P\\) help us to compute the"}, {"title": "perceptual similarity between the two images corresponding to a predefined network. We use VGG-19 in our case [56] as", "content": "the predefined network. For given two images \\(A\\) and \\(B\\), the \\(L_p\\) corresponding to the VGG-19 is computed as follows:\n\\(L_p = \\sum_k \\gamma^k (VGG_k(A) - VGG_k(B))\\) (1)\nIn Equation (1), features are extracted from \\(k\\) layers of the VGG-19 network. The function \\(\\gamma\\) transforms deep embedding to a scalar LPIPS score. The final score between two images (A and B) is computed and averaged for \\(k\\) layers.\n\\(L_C\\) is calculated using a simple five-layer deep convolution neural network which classifies cover images and watermarked images using Wasserstein loss [57]. It predicts whether a watermark is encoded in an image such that the embedding of the watermark can be improved.\n\\(LE_1 = L_R(E_1) + L_p(W_1, I) + L_c(I, W_1)\\) (2)\nEquation-2 refers to the loss function used for \\(E_1\\). Similarly, embedding the second watermark requires cryptographic hash (\\(h\\)) of \\(W_1\\) and \\(W_1\\), which outputs \\(W_2\\). Thus we have used the same \\(L_R\\), \\(L_P\\), and \\(L_C\\) loss for the encoder \\(E_2\\) as referred in Equation-3.\n\\(LE_2 = L_R(E_2) + L_p(W_2, I) + L_C(I, W_2))\\) (3)\n\\(D_1\\) uses binary cross-entropy loss to extract the embedded perceptual hash (\\(ph\\)) from \\(W_2\\) (Equation-4). Decoder \\(D_2\\) is used for reconstructing \\(R'\\) from \\(W_2\\) while decoder \\(D_3\\) is used for extracting the embedded cryptographic hash (\\(h\\)) from \\(R'\\). After reconstructing \\(R'\\) using \\(D_2\\) it is compared with the original \\(R\\) using \\(L_1\\) loss, LPIPS loss, and Mean Square Error (MSE) (Equation-5). Then \\(D_3\\) is used to extract \\(h\\) from \\(R'\\), thus binary cross-entropy loss is used (Equation-6).\n\\(LD_1 = L_{BCE}(D_1(W_2), ph)\\) (4)\n\\(LD_2 = L_1(R, R') + L_p(R, R') + MSE(R, R')\\) (5)\n\\(LD_3 = L_{BCE}(D_3(R'), h)\\) (6)\nTraining loss(Equation-7) is the sum of all five loss components (\\(LE_1, LE_2, LD_1, LD_2\\), and \\(LD_3\\)), which is minimized and \\(\\lambda_1, \\lambda_2, \\lambda_3, \\lambda_4\\),and \\(\\lambda_5\\) are weight factors.\n\\(L = \\lambda_1LE_1 + \\lambda_2LE_2 + \\lambda_3LD_1 + \\lambda_4LD_2 + \\lambda_5LD_3\\) (7)\nFor integrity and authentication, the reconstruction of the residual image should be identical. We have used \\(L_1\\), MSE and LPIPS loss for that. Adding to that, SHA-256 is not directly applied to the first stage watermarked image. Rather, a pre-trained autoencoder (CNN-based) is used, which obtains the continuous-valued representation of the image in the latent space. This representation is a set of real numbers that captures essential features of the image. Then, these numbers are normalized before the quantization step to ensure a consistent mapping of each continuous value to a specific discrete value. Then, SHA-256 is applied to generate a fixed-size hash value. As D2 will reconstruct identical residual images, which will have the original hash that is extracted by D3."}, {"title": "D. Training of network", "content": "In order to protect digital content copyright, \\(D_1\\) should extract the watermark even if \\(W_2\\) is subject to content- preserving image manipulation such as JPEG compression, noise, blurring, cropping, deformations, etc. Hence, while training, we performed random blurring, adding random noise, flipping, rotating, cropping, and compression to random wa- termarked images in the datasets such that \\(D_1\\) can be robust in extracting the embedded perceptual hash. To reduce the impact of resizing and cropping on watermarked images, we have restricted the embedding of perceptual hash in the centre portion of the images while training [50]. Decoders \\(D_2\\) and \\(D_3\\) are trained on the watermarked images without adding any distortions. This is due to the fact that minute changes in the watermarked image should be detected for successful content and source authentication. We have trained three instances (CelebA, Mirflickr and combined 200k images) of our model for 200 epochs with a batch size of 32. Early stopping and 10-fold cross-validation are employed to prevent the model from overfitting. We have used Adam optimizer to optimize the parameters of all five sub-models in our technique. The learning rate is set as 1.0 \u00d7 10\u22125, and we decay the learning rate by 0.2 if the loss does not decrease within 10 epochs.\nE. Challenges Faced During Training\nThe main challenge in our proposed technique is to embed two watermarks without degrading the quality of the cover image and using different decoders to extract both watermarks independently. The performance of our model depends on the successful convergence of the five loss functions used, which will ensure high accuracy. The steps involved in embedding and extracting the perceptual hash are similar to the techniques used in generic deep learning-based watermarking [24], [23], [25], [22]. The challenge here is to extract the perceptual hash from the second stage watermarked image after embedding the cryptographic hash. We must ensure that the perceptual and the cryptographic hash embedding do not overlap and are independent of each other. Hence to extract the perceptual hash successfully, both watermarks are embedded using different techniques. In the first stage, the encoder reconstructs \\(W_1\\). In contrast, for the second stage, the encoder generates \\(R\\), which is added to \\(W_1\\) to obtain \\(W_2\\). This assures that the watermarks do not overlap and the model can easily extract both watermarks. The difficulty is also faced while embedding the cryptographic hash and extracting it. As we know, the cryptographic hash changes with the slightest change in the image. Thus, \\(D_2\\) must reconstruct the \\(R'\\) identical to \\(R\\) before decoding the cryptographic hash. To achieve this, we have used \\(L_1\\), LPIPS, and MSE loss as the metrics while training the model to make \\(R'\\) identical to \\(R\\)."}, {"title": "VI. RESULTS AND DISCUSSIONS", "content": "Fig-3 shows the visual quality of the cover image and the watermarked images after embedding the perceptual and the cryptographic hash as the watermark. To verify the received watermarked image quality, we used structural similarity index (SSIM) [58] and peak signal-to-noise rate (PSNR) matrices. SSIM to compare local patterns of pixel intensities, which represent the attributes that represent the structure of objects in the image, along with luminance and contrast. The higher the value of SSIM, the better and the value closer to one represents the two images are structurally the same. On the other hand, PSNR measures the distortion between the cover and the watermarked image such that the quality of the reconstructed image can be validated. The higher the PSNR (value greater than 30 dB is considered good [59]), the better the quality of the reconstructed image. Mean squared error (MSE) is used to find the change in pixel distribution. Thus, the level of distortion at the pixel level between the cover and the watermarked image is validated. The lower value of MSE represent less distortion in the reconstructed image.\nOur technique works only if the computed perceptual and cryptographic hash match bit by bit with the extracted perceptual and cryptographic hash, respectively. Hence, the accuracy of our technique means successful matching between extracted and computed hashes. Table-I shows the training and testing accuracy of our model for various instances. The higher accuracy for decoders shows the success of extracting and comparing the watermark with more than 96% accuracy for \\(D_1\\) and more than 94% for \\(D_2\\) and \\(D_3\\). Table-I also demonstrates the similarity between the cover image and the watermarked image, which is shown quantitatively as SSIM, PSNR, and MSE. Higher SSIM and PSNR values show that the watermarked image has good visual quality, and low MSE indicates that our technique adds little distortion to the watermarked image with respect to the cover image."}, {"title": "A. Robustness Against Content Preserving Image Manipula- tions", "content": "The most crucial issue in copyright protection systems is ro- bustness, which signifies a watermarking system's capacity to withstand various attacks. Attackers may attempt to remove the embedded watermark by applying content-preserving image manipulations in the watermarked image. \\(D_1\\) is trained against various content-preserving image manipulations as shown in the first column of Table-II, which are standard across multiple watermarking techniques [23], [22], [13], [15], [16]. For JPEG compression, three different image quantization factor (Q) values of 10, 30, and 50 are used to control the amount of lossy compression applied to the image. A higher Q-value results in less compression and better image quality, and vice versa. Gaussian blur with window sizes 3 x 3,5 \u00d7 5, and 7 \u00d7 7 are applied. Random crops of size 20 \u00d7 20, 40 \u00d7 40, and 60 \u00d7 60 are performed. Cropping the edges also brings"}, {"title": "the image's resolution to 390 \u00d7 390 and 375 \u00d7 375 from", "content": "400 \u00d7 400. Table-II demonstrate that, despite the significant distortion in the watermarked images, the recovered watermark is still intact, and the values for SSIM are close to one. Table- II depicts the accuracy of \\(D_1\\) in extracting the watermark (perceptual hash) and successful bit-by-bit comparison with the computed perceptual hash for different content-preserving image manipulations. Fig-4 depicts the value of computed perceptual hash after applying different content-preserving image manipulations."}, {"title": "B. Robustness Against Overwriting", "content": "In the case of an overwriting attack", "watermarking": "in the first stage, \\(E_1\\) reconstructs \\(W_1\\) while"}]}