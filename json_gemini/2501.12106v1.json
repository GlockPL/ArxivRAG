{"title": "Can open source large language models be used for\ntumor documentation in Germany? - An evaluation on\nurological doctors' notes", "authors": ["Stefan Lenz", "Arsenij Ustjanzew", "Marco Jeray", "Torsten Panholzer"], "abstract": "Background: Tumor documentation in Germany is currently a largely manual process. It involves\nreading the textual patient documentation and filling in forms in dedicated data bases to obtain\nstructured data. Advances in information extraction techniques that build on large language\nmodels (LLMs) could have the potential for enhancing the efficiency and reliability of this\nprocess. Evaluating LLMs in the German medical domain, especially their ability to interpret\nspecialized language, is essential to determine their suitability for the use in clinical\ndocumentation. Due to data protection regulations, only locally deployed open source LLMs are\ngenerally suitable for this application.\nMethods: The evaluation employs eleven different open source LLMs with sizes ranging from 1\nbillion to 70 billion model parameters. Three basic tasks were selected as representative\nexamples for the tumor documentation process: identifying tumor diagnoses, assigning ICD-10\ncodes, and extracting the date of first diagnosis. For evaluating the LLMs on these tasks, a\ndataset of annotated text snippets based on anonymized doctors' notes from urology was\nprepared. Different prompting strategies were used to investigate the effect of the number of\nexamples in few-shot prompting and to explore the capabilities of the LLMs in general.\nResults: The models Llama 3.1 8B, Mistral 7B, and Mistral NeMo 12 B performed comparably\nwell in the tasks. Models with less extensive training data or having fewer than 7 billion\nparameters showed notably lower performance, while larger models did not display\nperformance gains. Examples from a different medical domain than urology could also improve\nthe outcome in few-shot prompting, which demonstrates the ability of LLMs to handle tasks\nneeded for tumor documentation.\nConclusions: Open source LLMs show a strong potential for automating tumor documentation.\nModels from 7-12 billion parameters could offer an optimal balance between performance and\nresource efficiency. With tailored fine-tuning and well-designed prompting, these models might", "sections": [{"title": "Background", "content": "The goal of the tumor documentation in Germany is to capture tumor diagnoses, the treatment\nhistory and the corresponding outcome for all patients in a detailed and structured form. The\ncompiled data should increase the transparency and quality in oncological care, improve\ntreatment outcomes, and support research. All the data items necessary for describing the\nmedical history of cancer patients are defined together with an overarching data structure in a\nunified dataset description, which is mandated by German federal law [1].\nSpecialized tumor documentation units in the hospitals collect and structure the information\nfrom medical records. The personnel in these units must read and interpret the medical\ndocumentation, which primarily comprises doctors' notes written in natural language. In\ndoctors' notes, ICD-10 codes (International Classification of Diseases, 10th revision, [2]) or codes\nfrom other medical classification systems are often not mentioned. Instead, the diagnoses are\nmainly described in text form, often using abbreviations and very heterogeneous wording. The\npresence of orthographic errors further complicates the situation. This way, coding information\nin a standardized way remains a predominantly manual task.\nLarge language models (LLMs), which are pre-trained on vast amounts of text data, have\ndemonstrated significant success in a variety of text-based tasks across different domains. In\nareas like programming, LLMs shown impressive results in code generation [3,4] and fixing\nerrors in code [5]. LLMs have also been highly effective at tasks such as summarization [6],\ntranslation, and answering complex questions [3].\nThe ability of LLMs to perform well in diverse, knowledge-intensive tasks suggests their\npotential for streamlining processes like medical coding and information extraction [7\u20139]. This\nnew technological development is an opportunity for improving the tumor documentation\nprocess. It could significantly increase the accuracy and the amount of structured\ndocumentation as well as improve the timely availability of data, while reducing the workload\nfor medical documentation staff. However, the application of LLMs to medical documentation in\nthe German language is currently underexplored.\nFoundational large language models, which are pre-trained on extensive text corpora, offer\nstrong reasoning capabilities [10]. Very large LLMs such as GPT-4 exhibit a very strong\nperformance across a variety of tasks [3]. Yet, the high level of data protection for healthcare\ndata prohibits the upload of patient data to cloud-based models such as GPT-4 in clinical routine\nin Germany. For this reason, only open source models that can be executed and fine-tuned\nlocally actually have the potential to be used in practice. The models are required to have some"}, {"title": "Methods", "content": "When writing this paper, we followed the TRIPOD-LLM statement [11] for reporting large\nlanguage model research.\nData set preparation\nThe initial data basis comprises 153 doctors' notes in the form of anonymized PDF documents.\nThe primary purpose of the original data set was the use in a doctoral thesis evaluating GPT-4\nfor tumor documentation [12]. Regarding data protection, particular care was required when\nselecting the data for this purpose, as ultimately control over the data entered is lost when\nusing Open Al's LLM. In order to avoid any data protection problems and violations of general\npersonal rights, the data was selected and processed with the highest caution. The letters come\nfrom former patients with prostate cancer who have been deceased for at least 10 years in\n2023. Before the doctors' letters were extracted from the clinical information system, names,\naddresses, dates of birth, and other personal identifying data were removed. These files were\nthen exported from the hospital information system in form of PDFs and redacted to ensure the\nanonymity of the letters. Therefore, further data was removed, such as the exact date of\nadmission or examination, as well as information that could allow conclusions to be drawn\nabout genetic diseases, family members or third parties.\nThe text was extracted from the PDFs with the information about the positions of the text lines\non the pages. Based on this positional information, we clustered the text into text blocks and\nidentified the headings of the blocks. Subsequently, only the blocks with the German word for\ndiagnosis in the header were used further. This process was sufficient to identify almost all text\nblocks that summarized the diagnoses and the corresponding treatment in the documents.\nThese text snippets were put into a Microsoft Excel document. There, one annotator added ICD-\n10 codes for the tumor diagnoses and the corresponding first diagnosis dates in a different\ncolumn. The Excel document was then transformed into an XML file that served as the primary\nbasis for the experiments. For the publication of the data, the data set was finally transformed\ninto the standard dataset format on HuggingFace using the Python \u201cDatasets\u201d package.\nThe product of the text extraction and annotation process was a set of 149 annotated text\nsnippets. The total number of diagnoses in the collection of text snippets is 151. Of those tumor\ndiagnoses, 84 have a first diagnosis date annotated. The dataset comprises 25 patients in total,\nwith some overlap in terms of content between snippets derived from the same patient history.\nThe distribution of the text snippets and the different types of tumor diagnoses can be found in\nTable 1.\nICD-10 codes for the diagnoses do not appear in the texts in most cases. In one of the coded\ndiagnoses, an outdated ICD-10 code (C83.3 instead of C82.2 for coding a \u201cLarge-cell B-cell Non\nHodgkin-Lymphoma\u201d) was used. This was corrected in the text because the purpose of the\nevaluation is the exploration of the coding capabilities with respect to the current ICD-10\ncatalogue. The other ICD-10 codes used in the texts have remained consistent across different\nICD-10 versions.\nTo further strengthen the anonymization in the final data set, the dates in the texts were\naltered: For each snippet, all the dates therein were shifted by a random amount of time that is\nconstant in the snippet. Thereby, the time spans of events in each text snippet remain constant\nand realistic, while a hypothetical use of the dates for patient reidentification is prevented.\nExcept for the modifications described above, the dataset was retained in the form that was\nreturned by the extraction mechanism during conversion from PDF to text. No further cleaning\nwas performed. In particular, special characters have not been removed and no additional\nmanual formatting for clarification was performed. This way, the dataset remains a realistic real-\nworld output from a PDF text extraction process.\nModel selection\nDue to data protection constraints, only models that can be executed locally can be used to\nimprove the tumor documentation process in German hospitals. Therefore, our evaluation\nfocused solely on open-source models, as these are the only practically relevant options. In this\nstudy we define open source models as those models with freely available weights and a license\nallowing modification and commercial use. We also considered only models that can be run on\nour local infrastructure at a reasonable speed. These requirements excluded very large open\nsource models such as Command R+ [13] or Llama 3.1 405B [14].\nFor using LLMs for the tumor documentation in Germany, it is necessary that the LLMs have\nbeen trained on German texts. In this study, Mistral 7B [15] from Mistral Al and Llama 3.1 8B\n[14] from Facebook/Meta were utilized as popular open source LLMs that have been trained on\na large corpus of data, including German texts.\nWe also wanted to examine variants of these models that have received additional training with\nGerman texts or with texts from the medical domain to see whether they can improve the\nresults in the experiment. We included BioMistral [16] as a variant of Mistral 7B that has been\nfine-tuned on texts from the medical domain. We furthermore included LeoLM [17], which is a\nmodel trained on basis of Llama 2 [18] with more German text.\nThe company VAGOsolutions, a software company specializing in LLMs, uses a proprietary\ndataset for additionally training LLMs with the intention to improve the performance on\nGerman text [19]. VAGOsolutions brands these models with \u201cSauerkrautLM\u201d. We included their\nLlama 3.1 8B variant to see whether we can get an improvement compared to the base model in\nthis scenario with German clinical documentation.\nBesides the mentioned models, which have sizes of seven or eight billion parameters, we also\nevaluated both larger and smaller models. This includes the 12-billion-parameter Mistral NeMo\n[20] as well as two larger open-source models Mixtral 8x7B [21] and Llama 3.1 70B [14]. As\nsmaller models, EuroLLM 1.7B [22], Llama 3.2 1B [23], and Llama 3.2 3B [24] were examined.\nOur evaluation used the open source models as they are provided. No fine-tuning was\nperformed on the models. For all models, the instruction-tuned variants, if available, were used,\nas the instruction tuning makes the models better at adhering to instructions given in the\nprompt [33]."}, {"title": "Prompt design", "content": "There are three steps in the evaluation. Each of the steps uses different prompts for obtaining\nstructured information from the text snippet and the output from previous steps. The core parts\nof the prompts and the corresponding objectives are shown in . The original prompts are in\nGerman and contain more detailed explanations. These complete prompts can be found in\nAppendix B.\nThe core parts of the prompts that are shared among different prompt variants explain only\ngeneral concepts that are relevant for the task. In particular, they explain the concept \"tumor\ndiagnosis\" (Step 1) in the context of tumor documentation and what counts as a \u201cfirst diagnosis\ndate\" (Step 3) here. The core prompts were not designed for detecting tumor diagnoses specific\nto urology and contain no instructions specific to this domain. Thereby, we aimed to measure\nthe performance of LLMs for detecting all sorts of tumor diagnoses instead of detecting specific\nones."}, {"title": "Results", "content": "Results for Step 1: Detecting tumor diagnoses\nIn Step 1, the models are asked to return a JSON array with all the tumor diagnoses in the text\nsnippet as strings. If the JSON array is empty, there are no tumor diagnoses in the text. To\nevaluate the ability of the models to find tumor diagnoses in the texts, we examine the\nperformance measures for the test whether a tumor diagnosis occurs in the text snippet or not.\nThe test considers the presence of data in the JSON array as the predicted result. The predicted\nresult is compared to the presence of annotated tumor diagnoses as the ground truth.\nThe accuracy of this test is used to determine the best model for extracting the tumor\ndiagnoses. Non-interpretable values are counted as wrong results for this purpose since a good\nmodel should be reliable with respect to correctness and usability of the answers. In addition to\nthe accuracy, we use the sensitivity and specificity to further investigate whether the model is\nable to understand and apply the concept of a tumor diagnosis as used in the tumor\ndocumentation process.\nLooking at the determined performance metrics of the LLMs (Fig. 2), Llama 3.1 8B is in Step 1\nthe overall best model in terms of accuracy. This means that the largest models Mixtral 8x7B\nand Llama 3.1 8B are surprisingly not also the best models in this scenario. The SauerkrautLM\nvariant with a focus on the German language is very close to the Llama 3.1 8B model but does\nnot improve upon the base model. Mistral NeMo 12B performs also very similar to Llama 3.1 8B\nin this test.\nLeoLM and BioMistral perform clearly worse than the other models with at least 7 billion\nparameters. They also produce more values that cannot be parsed, which indicates that they\nhave more trouble understanding the instructions and following them closely. The two smallest\nmodels, EuroLLM 1.7B and Llama 3.2 1B, are furthermore clearly weaker than all the other,\nlarger models. Llama 3.2 3B shows a performance that is very close to the larger model Mistral\n7B.\nIn Figure 2 all the values for the different prompting variants are shown. The variance of the\nperformance measures across different prompt variants allows to draw conclusions about the\nmodels' general capabilities. The more knowledge the models possess, the less dependent they\nare on additional instructions, making it easier for them to interpret explanations and examples.\nThere is less impact of the prompting variants on the performance of the larger models.\nEuroLLM 1.7B and LeoLM 7B Chat show the most variance in the performances of the\nprompting variants. The impact of foundational knowledge and general comprehension of\ninstructions is clearly reflected in both the performance and its stability in these results."}, {"title": "Discussion", "content": "Limitations of the study and generalizability of results\nIn this study we aimed to highlight the capabilities of LLMs with respect to tumor\ndocumentation. A limitation concerning this is the fact that we only have doctors' notes from\nurology. We tried to address this by phrasing the core prompts in a general way without\nreferring to things specific to urology. We then integrated information from either urology or a\ndifferent medical domain into the examples that we added to the core prompts. By comparing\nthe performance of symmetrical prompts with examples and instructions belonging to two\ndifferent domains, we could investigate the impact of domain-specific instructions. The results\ntherefore also allow some conclusions about the performance of the models and prompting\ntechniques for tumor documentation in general.\nAccording to the rather small size of our data set, the absolute numbers of the different metrics\nreported here might not reflect the exact overall performance of the models. We also could not\ncover all tasks that are part of the tumor documentation process. However, the comparison of\ndifferent models and different prompting strategies for finding diagnoses and first diagnosis\ndates gives valuable insights into the behavior of the different models. This comparison can help\nidentify which models are ideal starting points for fine-tuning towards information extraction\nfor the purpose of tumor documentation.\nThe manual annotations of the text snippets have been created by one person. The annotation\nof the tumor diagnoses in the text snippets is rather simple and should not differ that much\nbetween different annotators. The decision for attributing a first diagnosis date has more\nshades of grey. This difficulty might be one reason why the results of most models in Step 3 are\nsurprisingly not significantly better than the simple rule-based approach.\nModel performance and its connection to training data and model size\nThe results show that foundation models trained on a large amount of data from different\ndomains like Llama 3.1 8B, Mistral 7B, and Mistral 12B are able to perform very specific tasks\nlike tumor documentation. For performing the tasks presented here, the models need to\nunderstand the definition of a primary tumor diagnosis that is relevant for tumor\ndocumentation. Then they also need to connect the tumor diagnosis with the date of the initial\ndiagnosis. With proper prompt design, they are able to grasp these abstract concepts from\ninitial prompts that contain a task description. Short sequences of examples used in few-shot\nprompting further help their understanding.\nWe found that a plateau is quickly reached in few-shot-prompting where most models do not\nprofit from more examples. If the concepts in the prompt are known well-enough to the models\nzero-shot prompting is also sufficient, which seems to be the case with ICD-10 coding: Most\nmodels already had the best performance using zero-shot prompting in the ICD-10 coding task.\nMore examples on an already clear concept seem to confuse the models, as the results here\nshow. An exception to that could be seen in LeoLM 7B Chat and in the smaller models. In Step 1\nand Step 2, LeoLM profited from more examples given in few-shot-prompting than the other\nmodels. Presumably, this is the case because LeoLM saw much less training data in total\ncompared to the other models with at least 7 billion parameters: LeoLM has received training\nwith a dataset of 65 billion tokens in addition to the base model Llama 2 [37], which in turn has\nbeen trained on 2 trillion tokens [18]. In contrast to that, Llama 3.1 has been trained on more\nthan 15 trillion tokens [38].\nEuroLLM 1.7B was trained with 4 trillion tokens, with 50% of the training data being in English\nand 6% in German [22]. This means that EuroLLM has received more German training data than\nLeoLM and a similar amount of English training data. However, the results from LeoLM are\nclearly better by comparison. This may be due to the fact that a model with more parameters\ncan hold the information from the training data better [39], e.g., remember the ICD-10 codes for\nthe diseases, which are contained in the training data of all models as they are all trained on\nWikipedia articles. Another factor may be that a certain model size is required for the models to\nperform well enough on such a complex task as the one presented here [39].\nIt has also been noted [40] that larger models are not always better in tasks with complex\ninstructions. The resources needed for training and inference of larger models as well as the\ntime and energy that is consumed by the models are also important points to consider. It is\ntherefore important to find foundational models that are as small as possible but big enough to\nbe trained and utilized for tumor documentation tasks. Our results point to models of 7-12\nbillion parameters as good candidates for this purpose.\nThe results also show that it is not so simple to improve upon foundation models by simply\ntraining them with some more data related to the area of expertise needed for the task. This\ncan be seen in the results of the SauerkrautLM variant of Llama 3.1 8B, which does not improve\nupon the base model in this study, and in the results of BioMistral, which does not outperform\nMistral 7B. However, numerous techniques are available to fine-tune such models, enhancing\ntheir performance on specific tasks substantially. In particular, there are techniques for\nparameter-efficient fine-tuning, which modify only a subset of the weights of the larger models\n[41]. A targeted fine-tuning of larger models seems more feasible and promising compared to a\nfurther training of foundational models with larger amounts of additional data that are broadly\nrelated to the topic in question.\nContribution to openly available German clinical data\nEspecially in the medical domain, getting data for developing or evaluating methods is hard due\nto privacy protection constraints. There is only a small amount of datasets available but there\nare initiatives to publish more anonymized German clinical text documents [42,43].\nThe largest currently publicly available German dataset based on clinical documentation is the\nCARDIO:DE dataset, which comprises 500 discharge letters with 993,143 tokens from a\ncardiology department [44]. In the oncological domain, there is only one other publicly available\nGerman dataset containing anonymized real-world texts, which consists of shuffled sentences\nfrom 150 discharge summaries and has 89,942 tokens in total [45]. The number of letters in our\ndata set is very similar (153) and the number of tokens in our data set is in the same order of\nmagnitude (26,546 tokens using the Llama 3.1 tokenizer and 31,110 tokens using the tokenizer\nof Mistral 7B v0.3). Other data sets with German clinical texts contain texts translated from\nEnglish [46] or synthetic data [47].\nTherefore, the annotated data set of snippets from real-world doctors' notes that we release\nalongside this paper is in itself a valuable contribution to the field of clinical NLP with German\ndata.\nDistinction to named entity recognition (NER) tasks and classification tasks\nThe objective of the tumor documentation task investigated here is to identify the tumor\ndiagnoses associated with the patient. This makes the task different from named entity\nrecognition (NER) tasks: NER could be used to find terms for tumors where they occur in the\npatient history. However, there are situations in which the occurrence of the term for a tumor in\npatient documentation does not imply a tumor diagnosis for the patient. This is, e.g., the case\nfor suspected diagnoses, excluded diagnosis or tumor diagnoses from close relatives mentioned\nin the texts.\nThe task examined here can also not be solved via document-level classification. The first two\nsteps can be addressed using multi-label classification but the step for identifying the initial\ndiagnosis date, which relies on the extracted diagnosis text, cannot be solved this way.\nAs the evaluation scenario uses complex prompts to achieve the three different tasks, we did\nnot include a comparison to BERT-based models [48] such as BioGottBERT [49] and medBERT.de\n[50], which were also trained on German medical data but with the purpose of NER tasks and\ntext classification.\nICD-10 coding\nThe BERT-based models BioGottBERT and medBERT.de have been evaluated on the ICD-10\nclassification on German discharge notes [50]. Precision values around 40% and recall\n(sensitivity) below 20% were reached in this study. Another comparison of the models Flan-T5\n[51] and GermanBERT [52], which were trained on a set of 100,672 radiology reports, showed a\nmaximum accuracy of 72.2% for predicting the correct three-character ICD-10 codes on the 50\nmost-used ICD-10 codes therein [53]. We achieved a higher performance of more than 85%\naccuracy for ICD-10 coding on our dataset. This performance is still not sufficient for most use\ncases. Yet, the fact that our results were achieved without any additional training or fine-tuning\non the specific task could suggest that models of the size of 7-12 billion parameters can achieve\na performance that is acceptable for real world use if trained properly.\nICD-10 coding for medical billing can be challenging even for big state-of-the-art LLMs such as\nGPT-4 [54]. It should be noted, however, that coding for medical billing differs substantially from\nthe ICD-10 coding of tumors for the purpose of tumor documentation. In practice, these tasks\nare performed by completely different departments/people in the hospitals in Germany. The\ngoal of coding for medical billing is to combine different ICD-10 codes for describing the patient\ncondition in a way that leads to an optimal compensation from the cost bearer. Moreover, the\nnumber of relevant diagnoses for medical billing is, of course, much larger than the number of\ncodes for tumor diagnoses in the ICD-10 catalogue. Therefore, coding for medical billing is a\ndifferent and more difficult task than coding for tumor documentation. Indeed, this task is\nchallenging even for humans: A study with a random sample of three departments of a German\nhospital found that only up to 56.7% of primary diagnoses were correctly coded there, and only\nup to 37.5% of secondary diagnoses [55].\nLLMs such as GPT-4 have been evaluated on medical exams for students and they performed\neven better than many human professionals in these tests [56,57]. Therefore, even if the\ncurrent performance of LLMs needs improvement on certain specialized tasks such as medical\ncoding or tumor documentation, LLMs should be able to achieve at least a human-level\nperformance in these areas in the future. The question remains how to achieve this goal.\nOptimal training techniques, the required training data, and the size of the models have to be\ndetermined for this purpose. There are also approaches for increasing the performance of LLMs\non knowledge-intensive tasks which could be beneficial for ICD coding. A popular method for\nenhancing the knowledge of LLMs is retrieval augmented generation (RAG) [58], which has also\nbeen applied successfully in the biomedical domain in general [59,60] for improving the\nperformance of ICD coding in particular [61]."}, {"title": "Conclusions", "content": "Based on a dataset of urological doctors' notes, this study aimed to evaluate the applicability of\nopen-source large language models (LLMs) for tumor documentation in Germany. The results\nshow that models such as Llama 3.1 8B, Mistral 7B, and Mistral NeMo 12B can be assigned the\ntasks of identifying tumor diagnoses, coding them with ICD-10, and finding the corresponding\ninitial diagnosis dates. This performance was achieved through the use of carefully designed\nprompts, without any explicit training or fine-tuning of the models. The achieved accuracy\nvalues of around 90% for finding and coding diagnoses are not yet suitable for the real-world\napplication of the models. A tailored fine-tuning of the models could, however, substantially\nimprove the performance and make them viable tools for tumor documentation tasks.\nWe found that models of sizes in the range of 7-12 billion parameters offer a good balance of\nperformance and resource efficiency. Larger models did not consistently outperform their\nsmaller counterparts. The smaller models (Llama 3.2 1B and EuroLLM 1.7B) and the model\ntrained with the least amount of data in total (LeoLM 7B Chat) struggled the most with the\ntasks. Llama 3.2 3B was not far behind the models of 7-12 billion parameters, and might also\nbecome a viable option if improved with other techniques that compensate for the smaller\namount of knowledge contained in the model. Additional domain-related training or more\ntraining data in the target language seems not always to lead to an improved performance, as\ncould be seen with BioMistral and Llama 3.1 SauerkrautLM. This indicates that the way of\ntraining has also a very big impact on the performance in addition to the size of the training\ndata.\nThe results also underline the necessity of having independent validation datasets for evaluating\nthe model performance on real clinical data. Together with the code for the evaluation, we\nrelease the anonymized and annotated dataset that served as the data basis for this study. This\ncan serve as a valuable benchmark for the German clinical NLP community.\nOur findings suggest that with proper training and prompt design, models in the 7-12 billion\nparameter range could be well-suited for the task of tumor documentation. However, further\nresearch is needed to enhance model reliability and performance for tumor documentation in\npractice. Future work could explore techniques like parameter-efficient fine-tuning or retrieval-\naugmented generation, which have already shown success in similar tasks. With continued\nadvancements in this area, LLMs could become powerful tools for supporting medical\nprofessionals in tumor documentation."}, {"title": "Declarations", "content": "Ethics approval and consent to participate\nThis study is based on a dataset of anonymized doctors' letters of patients that had passed away\nmore than 10 years before the anonymization of the letters. The letters were collected and\nanonymized for a previous study. As there was no additional data acquisition or involvement of\npatients, an ethics approval was waived.\nConsent for publication\nNot applicable. The manuscript does not contain data from any individual person.\nAvailability of data and materials\nThe code for the evaluation is released under the MIT license and available from\nhttps://github.com/stefan-m-lenz/UroLlmEval. The data set used for this study is available from\nhttps://huggingface.co/datasets/stefan-m-lenz/UroLlmEvalSet.\nCompeting interests\nThe authors declare no competing interests.\nFunding\nThis research has been supported by the Federal Ministry of Education and Research (BMBF) in\nGermany in the project \u201cDigitaler FortschrittsHub Gesundheit \u2013 DECIDE\u201d (FKZ 01ZZ2106A) of the\nGerman national medical informatics initiative. The project aims to facilitate data transfer and\ninteroperability among different stakeholders in the healthcare sector and to provide decision\nsupport based on structured patient information.\nAuthors' contributions\nSL performed the text extraction from the PDFs, annotated the dataset, designed and\nconducted the experiments, and wrote the manuscript. AU was involved in method\ndevelopment and research and helped conducting experiments. MJ provided the anonymized\ndata set and gave advice on legal aspects. TP supervised the work and provided the funding for\nthe research. AU, MJ and TP further contributed by editing the manuscript. All authors read and\napproved the final version of the manuscript.\nAcknowledgements\nWe thank Lakisha Ortiz Rosario for supporting in various parts of the data analysis and in\nparticular for implementing the plot design of the graphs in the paper."}, {"title": "Appendix B: Prompt templates (in German)", "content": "Prompt templates for Step 1\nPrompt template \"0\"\nUser: Gibt es eine oder mehrere Tumordiagnosen in folgendem Diagnosetext? Als\nTumordiagnose z\u00e4hlen hier Diagnosen", "Diagnosetext": "n[SNIPPET-TEXT", "2-gyn\"\nUser": "Gibt es eine oder mehrere Tumordiagnosen in folgendem Diagnosetext? Als\nTumordiagnose z\u00e4hlen hier Diagnosen", "sind.\nAssistant": "Ich werde auf die folgenden Diagnosetexte kurz mit einem JSON-Array antworten\nund nur Tumordiagnosen ber\u00fccksichtigen.\nUser: Gibt es eine oder mehrere Tumordiagnosen in folgendem Diagnosetext?\nDiagnosetext: 2005: Invasives lobul\u00e4res Mammakarzinom links", "II\nSekund\u00e4rbefunde": "Verdacht auf axill\u00e4re Lymphknotenmetastasen links\nBRCA1-Genmutation positiv\nAssistant: [\"Mammakarzinom\"", "nUser": "Gibt es eine oder mehrere Tumordiagnosen in folgendem Diagnosetext?\nDiagnosetext: CA-125-Wert im Normbereich", "ein\nOvarialkarzinoms.\nAssistant": [], "Ovarialkarzinom\",\nwird nicht best\u00e4tigt.)\nUser": "Gibt es eine oder mehrere Tumordiagnosen in folgendem Diagnosetext?\nDiagnosetext: [SNIPPET-TEXT", "2-uro\"\nUser": "Gibt es eine oder mehrere Tumordiagnosen in folgendem Diagnosetext? Als\nTumordiagnose z\u00e4hlen hier Diagnosen, die im Kapitel II (Neubildungen/neoplasms) der\nInternational Classification of Diseases 10th revision (ICD-10) in den Kategorien C00-D48\nbeschrieben sind. Eine Tumorerkrankung in diesem Sinn ist eine Neubildung abnormen\nGewebes aus k\u00f6rpereigenen Zellen im K\u00f6rper des Patienten. Es ist entscheidend, nur\nDiagnosen als Tumordiagnosen zu werten, bei denen klar eine solche Tumorerkrankung\nbeschrieben wird. Aussagen \u00fcber Symptome, Therapien oder andere Erkrankungen sollten\nnicht als Tumordiagnosen interpretiert werden. Denken Sie nach, ob es sich bei den\\"}]}