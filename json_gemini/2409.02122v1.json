{"title": "DEEP KNOWLEDGE-INFUSION FOR EXPLAINABLE DEPRESSION\nDETECTION", "authors": ["Sumit Dalal", "Sarika Jain", "Mayank Dave"], "abstract": "Discovering individuals' depression on social media has become increasingly important. Re-\nsearchers employed ML/DL or lexicon-based methods for automated depression detection. Lexicon-\nbased methods, explainable and easy to implement, match words from user posts in a depression\ndictionary without considering contexts, and little attention has been paid to how the word can be\nassociated with the depression-related context. While the DL models can leverage contextual in-\nformation, their black-box nature limits their adoption in the domain. Though surrogate models\nlike LIME and SHAP can produce explanations for DL models, the explanations are suitable for\nthe developer and of limited use to the end user. We propose a Knolwedge-infused Neural Net-\nwork (KiNN) incorporating domain-specific knowledge from DepressionFeature ontology (DFO) in\na neural network to endow the model with user-level explainability regarding concepts and processes\nthe clinician understands. Further, commonsense knowledge from the Commonsense Transformer\n(COMET) trained on ATOMIC is also infused to consider the generic emotional aspects of user\nposts in depression detection. The model is evaluated on three expertly curated datasets related to\ndepression. We observed the model to have a statistically significant (p<0.1) boost in performance\nover the best domain-specific model, MentalBERT, across CLEF e-Risk (25% MCC\u2191, 12%F1\u2191). A\nsimilar trend is observed across the PRIMATE dataset where the proposed model performed better\nthan MentalBERT (2.5% MCC\u2191, 19%F1\u2191). The observations confirm the generated explanations to\nbe informative for MHPs compared to post-hoc model explanations. Results demonstrated that the\nuser-level explainability of KiNN also surpasses the performance of baseline models and can provide\nexplanations where other baselines fall short. Infusing the domain and commonsense knowledge in\nKiNN enhances the ability of models like GPT-3.5 to generate application-relevant explanations.", "sections": [{"title": "1 Introduction", "content": "Large population suffering from various mental disorders join social communities, a popular means of online commu-\nnication for sharing and helping others. People tend to write daily posts covering feelings, physical moments, food\nhabits, exercise, and music choices. Information from these posts is considered for the user's mental health assess-\nment. Researchers observed signs of depression in user's social data well before their first diagnosis. Moreover, the\nsocial data can be collected non-intrusively. These advantages help health professionals detect the user's mental health\nwithout much interference in the user's life.\nML/DL or lexicon-based approaches are manipulated to analyze the big data generated on social media platforms for\ndepression detection. In lexicon-based approaches, users' social posts are searched for terms from specific dictionaries\n(like antidepressant or depression-related). If the frequency of the terms from these dictionaries in the user's posts\ncrosses a threshold, the user is assigned a depressed label. While applying the lexicon-based method has been known\nto be explainable and easy to implement [1, 2]. Their possible disadvantage, however, is that they only consider if"}, {"title": "2 Literature", "content": null}, {"title": "2.1 Knowledge-infusion in Depression Detection", "content": "Various aspects of a social media user, along with the user's social postings, have been utilized in detecting depression,\nfor example, the social media user's profiles and his/her behavior on the social platform [17, 18]. The downside is\nthat these models are trained on several irrelevant contents, which might not be crucial for detecting a depressed user.\nBesides, this content hurts the overall efficiency and effectiveness of the model. To improve the model performance,\nauthors infused knowledge at various levels using dictionaries/lexicons, which were reported to be effective in cap-\nturing specific linguistic or domain-specific characteristics to detect depression [19, 20]. The authors of [21, 20, 22]\nmapped the words from users' textual posts into Lexicons like Linguistic Inquiry and Word Count (LIWC) [23], Af-\nfective norms for English words (ANEW) [24], and NRC [25] for infusing linguistic knowledge in the classification\nprocess. While the authors of [26, 27, 28, 29, 19, 20] focussed on domain-specific knowledge like antidepressants.\nFew other works which considered domain-specific knowledge like exercise-related information are [30], and disease\nor symptoms-related information are [31, 32, 28, 29]."}, {"title": "2.2 Explainable Depression Detection", "content": "Sometimes, the explanation is confused with the interpretation work; however, both are closely related but not the\nsame. Like in [33], authors explored and interpreted multi-variate time series features for depressed versus non-\ndepressed users in depression detection. For this purpose, they exploited the descriptive text of the depression symp-\ntoms to check the similarity index between each user post and the symptoms to calculate different time series features\nfor a user. Here, the time-series features are interpreted, but the reason for the classification output is not produced.\nAuthors of [34] provided a glimpse of the role of attention weights in explaining depression classification results by\nhierarchical attention neural model. In [27], multiple features besides the similarity index of user posts with depression\nsymptoms and antidepressants have been considered for depression detection from Reddit posts. The relation between\nthe features and the representations learned by the post-level attention network is studied to interpret the depression\nclassification results. Similarly, the authors of [35] emphasized post-level attention weights from hierarchical attention\nnetworks for producing depression classification explanations. The authors in [36] attempted to generate explanations\nusing representation learned by attention mechanism only on the words in a user post, with accompanying limitations.\nAlso, in [37], word-level attention representations from hierarchical attention networks are inspected to analyze uni-\ngrams and bigrams relevant for classification. However, the authors of [38] visualized the significant words and posts\nbased on the attention weights from a hierarchical attention network of word and post levels to explain the classifica-\ntion results. They employed multiple features, including symptoms and antidepressant count, in depression detection\nfrom social media posts. Though a BiLSTM-CNN with Attention Network is provided for depression classification\nin [39], they considered the word frequency of depressive and non-depressive samples from the training set to find\nfeature-level explanations. These works focus on attention weights, but authors in [40] designed a new classifier. They\nupdated the SS-3 classifier to accommodate grams of various lengths instead of uni-grams only from training samples\nfor depression detection. The classifier works on a decision tree where n-gram nodes are given some weight depending\non their participation in the classification process. All works mentioned here are considered user posts for explanation\nexcept the work in [41]. Numerical features like the number of followers, positive/negative emotion words, and posts\nhave been deployed for explanation generation by TreeSHAP, a game approach for explaining decision tree outputs."}, {"title": "3 Knowledge-infused Neural Network (KINN)", "content": null}, {"title": "3.1 Architecture", "content": "KiNN is a deep neural model with domain and commonsense knowledge infusion through multiple dense and attention\nlayers. KiNN has a unique aspect of three-stage knowledge infusion: shallow infusion via domain-specific embed-\ndings, semi-deep infusion via knowledge incorporation during neural network weight learning, and deep infusion via\nmultiple layers of knowledge introduction through attention mechanism at various levels. The attention layers enable\nthe model to compute the significance score of words/phrases from user posts compared to the CPGs (here Depres-\nsionFeature and UMLS) and sentimental aspects (like COMET). It has three self-attention layers as shown in Figure\n2 which focuses on depression and sentiments. The first two layers independently find attention scores of phrase-\ntagged user posts related to depression concepts from DepressionFeature ontology (DFO) and emotional aspects from\nCOMET. The third attention layer takes the concatenated output from the two and decides upon the importance of\nwords/phrases compared to the depression and emotional aspects cumulatively.\nKiNN is supported by two external modules: Knowledge Infusion from Domain-specific Knowledge Graphs\u00b9 and\nExplanation Visualizer. Figure 2 shows the architecture of KiNN with DepressionFeature Ontology and COMET\ntransformer. User posts collected from social platforms, Reddit in this case, have been passed to the Phrase Tagging\nmodule. The module employs the DepressionFeature Ontology to tag the domain-specific phrases from user posts.\nMentalBERT [42] is a BERT-based large language model pre-trained on mental health-specific data from social media\nplatforms. BERT tokenizer, which considers words or sub-words as tokens, not phrases. Moreover, depending on the\ncontext, a single word could have more than one representation in the dimensional space. Hence, to calculate a fixed\nvector, phrases are considered to be present standalone, and representation from the last four layers of the encoder was\nadded and averaged. Phrases are considered over words as tokens because phrase tagging preserves the meaning of\nsentences following the transformer tokenization process [43].\nThe DepressionFeature Ontology (DFO) is a structured person and disease-specific data model to detect and monitor\nmental disorders in social media users'. It contains distinct features from multi-modal data regarding various mental\ndisorders [44, 45]-for example, depression-related text phrases or image characteristics from social media users'\npostings. DepressionFeature ontology is populated with significant topics and static and dynamic-length phrases."}, {"title": "3.2 Algorithm", "content": "Depending on the attention layers KiNN has two variants named KiNN 1 and KiNN 2. KiNN 1 has cross-attention for\n\u00db \u00ca while KiNN 2 has self-attention network. Algorithm 1 presents the working process of KiNN.\nUser posts in the form of paragraphs or sentences represented as ui are passed to KiNN, which outputs a contextual-\nized representation zu\u2081. Depending on the downstream task of detecting depression or causes of depression (see the\nDatasets section), the representation is given to a feed-forward neural network (FFNN) classifier or the output layer.\nThe process involves transforming ui into ut, where \"pt\" stands for phrase tagging. The phrases were identified\nand tagged using multiple domain-specific knowledge sources, as illustrated in Figure 2. Phrase tagging helps infuse\ndepression knowledge in user posts. Domain-specific LM, MentalBERT [42] embeds the user posts (ut) into a vec-\ntor (\u00fbt) to pass it through a self-attention layer. This attention layer focuses on significant parts of the post while\nconsidering the infused knowledge via tagged phrases.\nIn parallel, the emotional aspects along with commonsense are extracted for a user document (ui) through pre-trained\nCOMET on the ATOMIC knowledge graph. Each user document and corresponding text descriptions of five selected\naspects (Erw, EEW, ERW, EEL, and ERL) returned by COMET are concatenated (u\u00bfE) and embedded using\nMentalBERT encoding (\u00fb\u00bfE). This time, the user documents are not phrase tagged as the COMET is trained using\nBERT tokenizer, which considers words or sub-words as tokens, not phrases. The domain knowledge is infused with\nphrases; in mental health, phrases have more significance than words and sub-words. This significance is also visible\nin our previous studies [49].\nThe embedded representation of u\u017c and emotional aspects \u00fb\u2081\u00ca is passed through the self-attention network. The\nnetwork provides a significance score to each aspect according to their relevance to the user post. Further, the output\nfrom self-attention layers is merged and normalized before being transferred to a single attention layer to decide the\nrelevance of domain-specific and emotional (commonsense) aspects. This way KiNN finds if a user document is\nmental health-related or general emotion (commonsense) related. Depending on the task, hidden representation, Zur,\nof user posts generated by KiNN is fed into a classification head to obtain the final classification label or vector, \u011di (Ui).\nWe can interpret which aspects contributed the most predictive value toward the final classification by examining the\nattention matrices. Algorithm 1 provides a formal pseudocode for the complete process within KiNN. Notably, line 4\nand 6 the associated domain knowledge source, which at present is DepressionFeature, can be changed.\nAlgorithm 2 discusses the process of mapping the user posts into DepressionFeature Ontology (DFO). Phrases from\nposts unavailable in DFO are searched in UMLS and the top three concepts are extracted and tagged in the correspond-\ning user post. If UMLS also does not contain any phrase then MentalBERT is employed to look for similar phrases.\nThese will further be looked into UMLS for top concepts and tagged in user posts for processing by Algorithm 1."}, {"title": "4 Experimental Setup", "content": null}, {"title": "4.1 Datasets", "content": "CLEF e-Risk (Type: Binary; Context: Depression): CLEF e-Risk sourced from r/depression subreddit consists\nof user posts and comments. We have considered the CLEF e-Risk dataset released in 2021. This version of CLEF\nis annotated with the Beck Depression Inventory (BDI), a CPG used by MHPs for detecting depression [55, 56].\nHowever, there is no existing lexicon for BDI, so we leverage an existing PHQ lexicon created by Yazdavar as the\nsource of clinical groundedness [31]. The dataset comprises at most 2000 Reddit posts per user for 828 users, out of\nwhich 79 users have self-reported clinical depression and 749 are control users. The control set consists of random\nRedditors interested in discussing depression.\nPRIMATE (Type: Multi-label; Context: Depression): The PRIMATE dataset was constructed to train conversa-\ntional agents to judge which PHQ-9 questions are answerable from the user's online post. The dataset comprises\n2,003 posts sourced from Reddit's r/depression_help subreddit. Each post has been labeled with nine binary annota-\ntions that correspond to whether the post addresses one of the nine PHQ-9 items. The dataset has been established\nas a gold standard for assessing depression severity, demonstrating an inter-annotator agreement (using Fleiss Kappa\n[57]) of 0.85 among six MHPs affiliated with the National Institute of Mental Health and Neurosciences (NIMHANS)\nin Bangalore, India. The anonymized dataset is made publicly available by [58].\nCAMS (Type: Multi-Class; Context: Mixed Depression and Suicide): The CAMS dataset comprises 5051 instances\nand is designed to identify the primary causes of MH problems by categorizing social media posts into six causal\nclasses [59]. These classes, which are based on the underlying reasons for mental illness and derived from relevant\nliterature, include 'No reason' (C0), 'Bias or abuse' (C1), 'Jobs and careers' (C2), 'Medication' (C3), 'Relationship'\n(C4), and 'Alienation' (C5). The dataset is presented in a \u00a1text, cause, inference\u00bf format, where 'text' is the user\npost with 'cause' referring to the labeled reason behind the mental disorder mentioned in the post, and 'inference'\nindicates the key phrases (or relevant concepts) in the post that expert annotators considered when assigning labels.\nThe annotations were reviewed by a clinical psychologist and a rehabilitation counselor and validated using Fleiss'\nKappa interobserver agreement, achieving a substantial agreement of 0.61."}, {"title": "4.2 Implementation and Training Details:", "content": "While training KiNN on CLEF e-Risk Dataset user post length is taken as 2000 i.e. a maximum of 2000 tokens\n(grams) are embedded for a user. Other parameters are two classes (Depressed vs. Non-Depressed), sixteen training\nand validation batches, fifteen training epochs, and a learning rate of one e-03. For PRIMATE dataset the parameters\nwere 9 classes, 150 user post length, 16 training and validation batches, 25 training epochs, and le-03 learning rate.\nFurther, to train KiNN on CAMS Dataset of 6 classes (C0-C5), maximum text length is taken as 50 with a training\nand validation batch size of 128. The training epochs were 25 and the learning rate is placed at 1e-03. The training is\ncompleted on an Intel Xeon server (8GB) which took about 8 hours."}, {"title": "4.3 Evaluation Metrics:", "content": "Four evaluation metrics have been chosen to assess the effectiveness of KiNN across the three datasets. Quantitative\nevaluation is done using standard performance indicators such as Precision (P), Recall (R), F1, and Matthew Cor-\nrelation Coefficient (MCC) scores. The creators of the original dataset employed these metrics to guarantee a fair\nbenchmark and enable insightful comparisons. The micro average for P and R is found to be the same in most of the\ncases. So we have calculated the macro average for P, R, and F1 which provides equal weight to each class in multiple\nclasses."}, {"title": "5 Results & Discussion", "content": "Results from KiNN on multiple depression-related datasets are compared with popular generic and domain-specific\ntransformers. This section presents qualitative results along with a user-level explanation generation process and\nresults from the proposed model and one best baseline model."}, {"title": "5.1 Quantitative Results", "content": "KINN on CLEF e-Risk Dataset: Table 2 reveals consistent improvement from KiNN over baseline methods in the\nCLEF e-Risk dataset. In terms of precision, recall, and F1 score, KiNN 2 outperforms all other models, followed\nby KiNN 1. However, the MCC scores reveal a slightly different pattern. KiNN 1 achieves the highest MCC score,\nindicating better overall classification performance in terms of true and false positives and negatives. In comparison\nwith the best generic transformer model BERT, KiNN 2 showed especially remarkable benefit, particularly in MCC\n(19%) compared to precision (30.6%), recall (41.6%), and F1 (34.4%) scores. However, while comparing KiNN\n2 with the second-best domain-specific transformer model, MentalBERT, significant enhancements were observed\nacross various standard performance metrics. Specifically, in the case of MCC (25.2%) compared precision (4.5%),\nrecall (20.5%), and F1(13%) scores. Similar results were obtained in the case of Naive Bayes (NB) and textCNN\nmodels. MCC is high for both the KiNN variants which signifies better overall performance of the classifier.\nKINN on PRIMATE: For precision, recall, and F1 score, KiNN 2 again emerges as the top-performing model, with\nKINN 1 following closely. Interestingly, in terms of MCC, KINN 1 achieves the highest score, indicating better overall\nclassification performance. From generic transformer models, BERT gives higher scores than other models. KiNN 2\nhas provided an approximate gain of 10%, 14%, 10%, and 1% over BERT in terms of precision, recall, F1, and MCC\nscores respectively. From domain-specific models MentalBERT second-best model after KiNN.\nKINN on CAMS: provided \u201cinference,\u201d reported by annotators as an explanation behind their labeling. Such ground-\ntruth explanations enable a more thorough assessment of the fidelity of the KiNN's attention allocation. Table 2 shows\nthe performance gains of KiNN over CAMS baselines.\nKINN: Though MentalBERT performed best in terms of F1 and MCC scores while BioBERT is best in precision and\nrecall, KiNN variants demonstrated competitive performance across precision, recall, F1 score, and MCC. However,\nKINN 1 achieves the higher precision and F1 score on this dataset, while KiNN 2 achieves the higher MCC."}, {"title": "5.2 User-level Explanations", "content": "For producing user-level explanations of depression classification results GPT 3.5 is employed. The \"text-DaVinci-\n003\" model is prompted in Python through the LangChain library [60]. The prompt is provided with user posts\nand significant words/phrases from MentalBERT or concepts from KiNN. Depression-related concepts are assigned\nhigher attention scores by KiNN as compared to the MentalBert. GPT considers these concepts or words during\nexplanation generation (see italicized words in Table 3). However, phrases from the MentalBERT model did not\nproduce a comparable pattern in GPT 3.5's behavior. The reason could be observed from Figure 1 which displays\nhighlighted text from MentalBERT and the proposed KiNN model. However, most of the time MentalBERT highlights\nall text with equal likelihood, and GPT 3.5 focuses on certain terms during the explanation-generating process.\nThe method establishes a connection between attention words and concepts in DepressionFeature Ontology by com-\nputing their cosine similarity. The visualizer then extracts top-ranked concept(s) with a similarity greater than 0.80,\nconsidering them potential concepts for generating explanations using GPT 3.5. GPT 3.5 utilized the mapped concepts,\nas illustrated in Figure 1, to generate an explanation, as shown in Table 3."}, {"title": "6 Conclusion", "content": "The BlackBox nature of AI models limits their practical implementation in the healthcare domain specifically in\nmental health assessment due to its subjective behavior. Banking upon a system that does not provide reasoning for\nits decisions could be too costly in the mental health domain costing someone's life. Further explanations produced\nby ad-hoc models are rarely understandable to end users. Present models miss domain knowledge and other aspects\nof a person's personality like commonsense or emotional. To handle the challenges, we presented KiNN, a deep\nknowledge-infused learning network that considers domain knowledge (e.g., DepressionFeature, UMLS) and multiple\nemotional aspects of users' text (COMET) infused through at multiple levels through attention networks to provide\nuser-level explainability in critical domains like MH.\nThe performance of our inherently explainable model is comparable to the latest Language Model Models (LLMs)\ntrained on both generic and domain-specific datasets, including BERT, BioBERT, and MentalBERT. The model's\nperformance on all three datasets can be found in Table 2 and Table 3. Moreover, the LLMs are trained on very large\ndatasets and costly resources along with large time and space requirements compared to our model.\nThe inclusion of clinical domain knowledge, such as Clinical Practice Guidelines (CPG), is crucial for the identification\nof various symptoms and experiences within unstructured text. LM algorithms can enhance their learning capabilities\nby incorporating a wider range of mental health (MH) targets. This expansion enables them to provide improved\nsupport for diagnosis, treatment, and intervention strategies.\nThe KiNN model demonstrated superior performance in identifying relevant concepts in unstructured text compared\nto the baseline models. The infusion of knowledge in natural language processing for mental health involves the\nincorporation of logical reasoning, such as clinical practice guidelines and emotional aspects. This enables the AI\nmodel to engage in continuous learning, ensuring its robustness and resistance to errors. In addition, the establishment\nof a shared semantic understanding between human experts and language models facilitates collaborative decision-\nmaking."}]}