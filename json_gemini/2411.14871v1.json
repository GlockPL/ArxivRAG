{"title": "Prioritize Denoising Steps on Diffusion Model Preference Alignment via Explicit Denoised Distribution Estimation", "authors": ["Dingyuan Shi", "Yong Wang", "Hangyu Li", "Xiangxiang Chu"], "abstract": "Diffusion models have shown remarkable success in text-to-image generation, making alignment methods for these models increasingly important. A key challenge is the sparsity of preference labels, which are typically available only at the terminal of denoising trajectories. This raises the issue of how to assign credit across denoising steps based on these sparse labels. In this paper, we propose Denoised Distribution Estimation (DDE), a novel method for credit assignment. Unlike previous approaches that rely on auxiliary models or hand-crafted schemes, DDE derives its strategy more explicitly. The proposed DDE directly estimates the terminal denoised distribution from the perspective of each step. It is equipped with two estimation strategies and capable of representing the entire denoising trajectory with a single model inference. Theoretically and empirically, we show that DDE prioritizes optimizing the middle part of the denoising trajectory, resulting in a novel and effective credit assignment scheme. Extensive experiments demonstrate that our approach achieves superior performance, both quantitatively and qualitatively.", "sections": [{"title": "1. Introduction", "content": "Diffusion models have achieved remarkable success in text-to-image generation [7, 17, 20]. A key challenge in generative modeling is alignment [12, 25], which focuses on improving a model's ability to better align with human preferences. Alignment training has been extensively explored in the context of large language models [14, 22, 30]. Initially driven by Reinforcement Learning from Human Feedback (RLHF), alignment techniques have evolved to include approaches such as Direct Preference Optimization (DPO) [16]. The latter has gained significant attraction, inspiring a series of subsequent studies [1, 3, 29].\nDespite the variety of emerging approaches, few studies have attempted to adapt Direct Preference Optimization (DPO) to text-to-image diffusion models. The primary challenge lies in the sparsity of preference labels. Specifically, human evaluators can only provide preference labels for the final noiseless output of generative models. In contrast, diffusion models generate images progressively, producing large numbers of noisy intermediate results that are difficult to label. So the question is: How can credit be assigned across the denoising steps when preference labels are only available at the terminal of the denoising trajectory?\nRecent studies have explored two main approaches to address this challenge. One approach relies on auxiliary models, such as reward models [2, 6] or noisy evaluators [11]. These models essentially learn a weighting function to adaptively assign credit from the final output to each denoising step. However, this approach introduces additional training complexity, undermining the simplicity of DPO. The second approach requires no auxiliary models, maintaining the simplicity of DPO but limiting the ability to perform effective credit assignments. These methods typically rely on simple strategies, such as uniform assignment [24, 27] or discounted assignment [28] (i.e. placing more weight on the initial denoising steps), which may restrict the alignment potential of the model.\nIn this paper, we propose Denoised Distribution Estimation, a DPO method without auxiliary models for diffusion model alignment training. Instead of directly designing a hand-crafted credit assignment scheme, DDE advocates deriving the assignment mechanism via explicit estimation to the terminal denoised distribution from the perspective of each step. To this end, we propose two estimation strategies: stepwise and single-shot estimation. The stepwise estimation uses the ground-truth conditional distribution (q(xt-1|xt, xo)) to estimate the model distribution (po(xt-1|xt)), while the single-shot estimation utilizes DDIM modeling to directly estimate the terminal distribution (po(xo|xt)) based on an intermediate noisy latent state. By integrating these two estimation strategies, we can evaluate"}, {"title": "2. Preliminaries", "content": "Diffusion Models. Traditional Denoising Diffusion Probabilistic Model (DDPM) [7] defines a forward process, which incrementally transforms a noiseless image (denoted as xo) to pure noise (denoted as xT) by injecting Gaussian noise as:\nq(xt|xt\u22121) = N(xt; \u221a 1 \u2013 \u1e9etxt\u22121, \u1e9etI), (1)\nwhere \u1e9et and at = 1-\u1e9et are hyperparameters of controlling the diffusion process. The forward process can be expressed non-iteratively as:\nq(xt|xo) = N(xt; \u221a\u0101txo, (1 \u2013 \u0101t)I), (2)\nwhere \u0101t = \u03a0i=1\u03b1i denotes the cumulative product of the scaling factor \u03b1t.\nDDPM requires a model to estimate the denoising distribution po (Xt-1|xt), which enables it to reverse the diffusion process and convert noise back into images. The training objective involves learning the distribution of xt-1 conditioned on xo, modeled as a Gaussian distribution:\nq(xt-1|Xt, xo) = N(xt\u22121; \u03bc(xt, xo), \\frac{1- \u1fb6\u03c4\u22121 \u03b2\u03b5}{1-\u0101t}I), (3)\nwhere \u03bc(xt, xo) = \\frac{\u221aat(1\u2212\u0101t\u22121)}{1-\u0101t} xo + \\frac{\u221aat\u22121\u00dft}{1-\u0101t} \u03f5\u03b8. By minimizing the KL-divergence between predicted noising distribution Po(Xt-1|xt) and the true distribution above, the diffusion model learns to denoise the randomly sampled noise xt to xo progressively.\nDue to the Markov property, DDPM is constrained to denoise samples incrementally, one step at a time. In contrast, Denoising Diffusion Implicit Model (DDIM) [20] incorporates a non-Markovian sub-chain, redefining the denoising process as:\nPo(xt'|Xt) = N(xt'; \u00b5\u0473,t' (xt, t), \u03c3\u03c4I), (4)\nwhere t' and t are two consecutive vertices within a sub-chain of the original DDPM denoising Markov chain, \u00b5\u0473,\u03c4' (xt,t) = \\frac{\u221aat'(1\u2212\u0101t\u22121)}{Vat} (xt - \\sqrt{(1-at)}\u03f5\u03b8(xt, t)) + \\sqrt{1 \u2212 \u1fb6\u03c4' \u2212 \u03c3\u03be}\u03f5\u03c1(xt, t) and ot = \\frac{\u221a1-at' \u03b2t}{V1-at}.\nDirect Preference Optimization. DPO [16] aims to optimize generative models to better align with human preferences. This objective is achieved by establishing a connection between the reward model and the generative model, resulting in the following loss function:\nLDPO = Exw,xl~D[\u2212 log \u03c3(\u03b2(log \\frac{Po(xw)}{Pref (xw)} -log \\frac{Po(x')}{Pref ef (xl)}))], (5)\nwhere pe and pref represent the target and reference model respectively. The samples xw and x\u00b9 constitute a paired preference set, where xw > x\u00b9 indicate that the win sample xw is preferred over the lose sample x\u00b9. The sigmoid function is denoted as \u03c3. In this loss formulation, the differential term (log \\frac{Po(xw)}{Pref (xw)} -log \\frac{Po(x')}{Pref(x)})) serves to adjust the model to increase the probability of generating the winning sample (xw), while decreasing that of the losing sample (x\u00b9). The fractional components pref (\u00b7) keep the optimization close to the reference model, thus preventing excessive deviations."}, {"title": "3. Method", "content": "3.1. Overview\nWe introduce our Denoised Distribution Estimation (DDE) method, which adapts Direct Preference Optimization (DPO) to diffusion models. The key challenge is the sparsity of preference labels. These labels are only tagged on the terminal of denoising trajectories (i.e. noiseless images xw > x\u00b9), leaving these intermediate results (xt, \u2200t \u2260 0) unlabeled. Previous research has addressed this issue by either utilizing auxiliary models, such as reward model [2, 6], or employing hand-crafted assignment schemes, including uniform [24, 27] and discounted strategies [28]. Our approach falls into the second category (without auxiliary models), aiming to preserve the simplicity inherent in DPO training.\nDistinct from existing methods that directly target the assignment scheme design, we advocate to investigate the final impact from the perspective of each step.\nTo this end, the most straightforward approach would involve calculating the entire denoising trajectory. However, this would incur prohibitive training costs due to the iterative nature of the generation process. Training feasibility limits the number of model inferences that can be executed. Consequently, the final denoised distribution must be estimated with a reduced number of inferences, or preferably, in a single inference. This constraint motivates us to establish a connection between xt and x0. We begin by representing po(x0) in terms of po(xt) as follows:\nP\u04e9 (\u0445\u043e) = \u222bX1:TPo (x0:T)dx1:T = \u222bX1:Tq(x\u0442)\u0440\u04e9(XT-1|XT)...Po(xo| |x1)dx1:T (6)\nwhere q(xT) is a Gaussian distribution and po(xt-1|Xt) denotes the learned denoising distribution which constitutes the denoising trajectory.\nGiven a randomly sampled step t, the denoising trajectory from T to 0 can naturally be split into two segments: from T to t and from t to 0, denoted as T \u2192 tandt \u2192 0, respectively. We propose two distinct estimation strategies for each segment, as outlined below (also shown in Fig. 1).\n\u2022 Stepwise estimation for segment T \u2192 t. We estimate each step term po (Xt|Xt+1) by q(xt|Xt+1, xo) (defined as Eq. 3). To further reduce the estimation error, we incorporate an importance ratio for correction.\n\u2022 Single-shot estimation for segment t \u2192 0. We directly estimate the terminal denoised distribution of x0 from that of xt by using DDIM with a single model inference.\nHere, we present several key observations regarding our DDE method:\n\u2022 Our method explicitly builds estimation to the learned denoised distribution. Unlike previous methods where the denoising process is modeled as a Markov sequential decision problem, our DDE explicitly estimates the learned"}, {"title": "3.2. Stepwise Estimation for Segment T \u2192 t", "content": "The stepwise estimation uses q(xt|xt+1,xo) (defined as Eq. 3) to estimate po(xt|xt+1). This approach is justifiable as the model is optimized during pretraining to minimize the KL-divergence between the above two distributions [7]. To enhance the accuracy of the estimation, we define the importance ratio as follows:\nrt = log \\frac{Po(XtXt+1)}{q(xt/xt+1,xo)}\nFor any k within the interval T \u2192 t, substituting Po(Xk|Xk+1) with exp{rk}q(xk|Xk+1, Xo) yields\nPo(xo) \u2248 exp{\u2211Tk=t rk} \u222bX1:t \\frac{q(xt|xo)}{q(xt|xo)} \u03a0tk=tPo(Xk\u22121|Xk)dx1:t (8)\nA comprehensive derivation is provided in Suppl. 7.1.\nBy adopting the stepwise estimation, the denoising segment from T to t can be simplified to a single term q(xt|xo), with an additional correction term exp{\u2211Tk=tr}. Term"}, {"title": "3.3. Single-shot Estimation for Segment t\u2192 0", "content": "Based on the Markov property of the inverse process, we have \u222btk=t\u22121 Okt Po(Xk\u22121|xk)dx1:t-1 = Po(xo|xt). Hence, Eq. 8 can be rewritten as follows:\nPo (xo) \u2248 exp{\u2211Tk=t rk} \u222bXtq(xt|xo)Po(Xo|xt)dxt = exp{\u2211Tk=t rk}Ext~q(xt|20) [Pd(X0|Xt)]. (9)\nBy viewing xt and xo as consecutive vertices in a DDIM sub-chain of the original Markov chain, we have\nPo(xo|xt) = N(x0; \u00b5\u0473,t'=0(xt, t), \u03c3\u03c4I), (10)\nwhere pot'=0(xt, t) = \\frac{\u221a\u00e3o}{Vat}(xt-\\frac{1-at}{tee(x,t)}) +\\frac{Vat-1 Bt}{V1-at}. Therefore, the po(xo|xt) in Eq. 9 can be calculated with only one single pass of the neural network.\nBuilding on the above two estimation strategies, the loss can be derived as follows:\nLDDE = Exw,x,xx ~q(x\u00a5|x),x\u00a6~q(x}\\x6) [\u2212log \u03c3(\u03b2(||xw - o,t'=o(x,t)||2 + ||x - pref,t'=0(x,t)||2 + ||x - \u03bco\u03c4'=o(x, t)||2 - ||x - pref,t'=0(x,t)||2 + \u2211Tk=t (rok - rref,k-ro,k + rref,k)))], (11)\nwhere \u00fb\u03b8 and pref denote using DDIM to estimate x0 by xt, as introduced in Sec. 2. The detailed derivation can be found at Suppl. 7.2.\nWe provide an intuitive explanation for each term. For Mean Square Error (MSE) terms relating to 0, ||xw-\u03bco,t'=0(x,t)|| guides the optimizer to increase the output probability of po(x|x), while ||x \u2212 po,t'=o(x, t)||2 decreases the model output samples like xl. MSE terms with respect to the reference model penalize deviations from the reference, with large deviations pushing the optimizer toward the saturation region of the negative log-sigmoid function, thus weakening the optimization. The importance ratio terms correct these deviations, slightly modifying the MSE terms in the negative log-sigmoid function, which adjusts the optimization weight."}, {"title": "3.4. Prioritization Property of Our DDE Method", "content": "Our method prioritizes middle denoising steps. We find that two estimation strategies of our DDE method naturally introduce a credit assignment scheme that prioritizes the optimization of the middle steps of denoising trajectories. Stepwise and single-shot estimation weakens the optimization of steps around 0 and T, respectively.\nThe stepwise estimation strategy introduces correction terms to the loss function. A larger correction term pushes the MSE terms closer into the gradient saturation region of the negative log-likelihood function, shown in Fig. 2(b), reducing the optimization effectiveness. In practice, we observe that the correction term will quickly converge, with its value diminishing for larger steps (see Sec. 4.4 for more details). This is because when the sampled t is closer to time steps around 0 (i.e. noiseless image), the segment T \u2192 t becomes longer, leading to higher estimation errors and larger correction terms. Consequently, this pushes the whole term into the gradient saturation region. Essentially, the stepwise estimation weakens the optimization of steps around 0.\nThe single-shot estimation strategy introduces weight coefficients to the predicted noise due to the DDIM modeling. These coefficients can be directly computed and increase with the rise in t, which is shown in Fig. 2(a). Larger coefficients magnify even a slight difference between the training and the reference model, pushing the term into the saturation region of the negative log-sigmoid function, as shown in Fig. 2(b). Consequently, the single-shot estimation weakens the optimization for steps near T, where the coefficients are maximal. It is critical to note that larger coefficients do not necessarily bring stronger optimization, as they influence both the training and reference model terms.\nIn summary, by combining both estimation strategies, our method weakens the optimization at both ends of the denoising trajectory, effectively strengthening the optimization of the middle part. We refer to this as the \"prioritization property\" of our DDE method.\nAssigning credits more on the middle part is reasonable. The prioritization property distinguishes our method from existing approaches. We initiate our comparison with the uniform credit assignment scheme [24, 27], which defines the loss function as follows:\nLUni = Exw,xl~D[-log \u03c3(\u03b2log \\frac{Po(x-1x)}{Pref(x_1x)} - Blog \\frac{Po(x-1/x)}{Pref (12)})]. (12)\nWe conduct experiments based on Stable Diffusion 1.5 (SD15) [17] using Uniform loss and the performances are evaluated by CLIP model [15], which is shown in Fig. 3. The red dashed line denotes the performance of original SD15 with a score of 0.320. Training SD15 with Uniform loss for 2000 iterations yields an improvement in score to 0.331 (black dashed line). By leveraging the prioritization property inherent in our DDE method, we specifically opti-"}, {"title": "4. Experiments", "content": "4.1. Implementation Detail\nWe finetune the models using the Pick-a-Pic-V2 dataset 1 with the popular text-to-image models, Stable Diffusion 1.5 (SD15) and Stable Diffusion XL (SDXL). All parameters of the U-Net are trained. SD15 and SDXL are trained for 2000 and 1500 iterations, respectively. Training is conducted on 8 Nvidia H20 GPUs, each with 96GiB memory, and a batch size of 2048. Please refer to Suppl. 8.1 for more implementation details.\n4.2. Quantitative Validation\nTo validate our approach quantitatively, we employ CLIP [15], HPS [26], and PS [9] models as annotators, each representing different types of preference. Based on these annotators, we train our methods (DDE) alongside baseline methods: Supervised Finetune (SFT), Uniform (Uni, [24, 27]), and Discounted (Disc, [28]).\nWe conducted an extensive comparison on both SD15 and SDXL, illustrated in the upper sections of Table 2 and Table 3, respectively. Our proposed approach not only enhances the performance of the base models but also surpasses all other baselines. Specifically, our DDE improves SD15 by 6.7%, 3.9%, and 3.3% in terms of CLIP, HPS and PS, respectively. Furthermore, it improves SDXL by 1.4%, 1.0%, and 3.1% across the same metrics.\nIn addition to comparing the mean value of all validation images, we also analyze the beat ratios. As shown"}, {"title": "4.3. Qualitative Validation", "content": "In this section, we conduct a qualitative comparison between our model and the baseline models and provide some illustrative examples. More generated cases are also available in Suppl. 8.3.\nAs shown in Fig. 4, our model surpasses SD15 in generating images with superior details, structure, and text alignment. For instance, the \u201cpanda scientist\u201d produced by using our approach holds on flasks and tubes, exhibiting enhanced detail generation capability compared to the base models. The head portrait generated by the base model suffers eye structure collapse, which our model successfully avoids. Furthermore, our model produces rabbit wearing an armor that is more aligned with the given prompt. Additionally, the generated image has a better view of the surfboard as the prompt request, and the room corner we produce displays more details and a clearer view.\nOur model also enhances the SDXL model, as illustrated in Fig. 6. For the city view generation, we maintain the detail of windows on the buildings more effectively than the base model. Our approach accurately generates a human hand with the right number of fingers, and a dancer with the correct body structure. The text generated by our model is clearer, aligning precisely with the prompt. Moreover, the inverted reflections in water generated by our model exhibit more accurate structural details."}, {"title": "4.4. How the Correction Terms Prioritize Steps", "content": "We now explain that our correction terms brought by our stepwise estimation strategy weaken optimizing those steps around 0. It can be shown in Fig. 7(a) that the correction terms quickly converge as training iteration reaches 100. The distribution of values is illustrated in Fig. 7(b), clearly indicating a decrease in values as the number of steps increases. Notably, a larger correction term pushes the value into the saturation domain of the negative log-sigmoid function, thus weakening the optimization. Therefore, our correction terms weaken the optimization of steps around 0."}, {"title": "4.5. Ablation study", "content": "We conduct experiments using only one of the two estimation strategies described above. Specifically, \"DDE-Step\" denotes only adopting the stepwise estimation, while \"DDE-Single\" refers to using the single-shot estimation alone. The results are shown in the bottom section of Table 2.\nWhen applying only the single-shot estimation, all metrics exhibit performance at or above the level of DDE, demonstrating the effectiveness of this strategy. In contrast, when only the stepwise estimation is used, all metrics show a decrease in performance, even falling below the baseline model. This observation is consistent with the analysis in Sec. 3.4. It is important to note that the stepwise estimation weakens optimization at the initial stage while strengthening it at the steps around T. Consequently, optimizing only the first or last 10% of steps either improves or harms performance, respectively. Therefore, using the stepwise estimation alone weakens the critical steps essential for optimal performance, leading to suboptimal outcomes.\nHowever, this does not mean the stepwise strategy is ineffective. In fact, when both strategies are combined, performance enhancements are observed in 2 out of 3 metrics. This improvement arises from the single-shot estimation, which strengthens optimization at the initial steps, thereby mitigating the negative impact of the stepwise estimation and ultimately resulting in superior outcomes."}, {"title": "5. Related work", "content": "Reinforcement Learning from Human Feedback (RLHF). Using human feedback as a supervision signal in Reinforcement Learning (RL) has been proposed [10] and evolved to a preference-based paradigm for better robustness [4, 8, 13]. RLHF has been recently utilized in generative models, particularly large language models [14, 22, 30]. A typical RLHF framework trains a reward model using preference data, and then applies a proximal policy optimization (PPO) algorithm [19] to optimize the policy (i.e., the generative model). One key drawback of this approach is the high computational cost. To address this issue, Direct Preference Optimization (DPO) was proposed [16] based on the con-"}, {"title": "6. Conclusion", "content": "In this paper, we introduce Denoised Distribution Estimation (DDE), a novel direct preference optimization method tailored for diffusion models. Our DDE addresses the challenge of credit assignment across denoising steps, an issue stemming from the sparsity of preference labels, by leveraging an enhanced insight into the denoising process. DDE incorporates two estimation strategies that evaluate the impact of each denoising step on the final outcome. Our analysis reveals that these two strategies implicitly prioritize the optimization of intermediate steps within the denoising trajectory, which is a key distinction from existing methods. Experimental evaluations show that our approach outperforms previous methods. It generates images with superior metrics of CLIP, HPS, and PS and achieves better alignment of human preference in terms of detail, text-image coherence, and structures."}, {"title": "7. Derivation", "content": "7.1. Derivation of the Loss Function Defined in Eq. 8\nBy substituting Po(XtXt+1) with extq(x+|Xt+1,xo) in Eq. 6, we obtain:\nP\u04e9 (\u0445\u043e) = \u222bX1:Tq(x\u0442)\u0440\u04e9(\u0425\u0422-1|XT)...Po(xo| |x1)dx1:T = \u222bX1:Texp{\u2211Tk=t rk} \u222bX1:t \\frac{q(xt|xo)}{q(xt|xo)} \u03a0tk=tPo(Xk\u22121|Xk)dx1:T(13)\nWe now consider the term q(x\u0442) \u03a0Tk=T\u22121q(xk|xk+1,Xo). Noting that q(xT) is independent of 20, we have q(xT) = q(xT xo). Applying Bayes' theorem, it follows that:\n\u222b q(XT|xo)q(XT-1|XT,X0)...dxt:T\u22121dxT\u222b q(XT, XT-1/20\u222b q(XT-1|xo)q(XT\u22122|XT\u22121,xo)...dxt:T\u22122dxT-1\u222bq(XT-2/xo)... .dxt:T-2\u222b q(xt|xo)dxt (14)"}, {"title": "7.2. Derivation of Eq. 11", "content": "By replacing po (x0) and pref(x0) in the logarithmic term of Eq. 5 with Eq. 9, we obtain:\nlog \\frac{Po(xw)}{Pref(x)} = \u2211(rok - ref,k) + log \\frac{Exy~q(x|xo) [P\u04e9 (xw|x+)]}{Exy~q(xx|xo) [Pref (XU|Xt)]}(16)\nTo avoid the high computation cost of the integral calculation related to the expectation, we employ the Monte Carlo method, using a single point xt ~ q(xt|xo) for estimation. Applying this estimation as model input to both the target\nPo(xo)\nPref(x)\nand reference models, we derive:\n\u222bExy~q(x|x0) [P\u04e9 (xW|Xt)]\u222bExy~q(xx|xo) [Pref (XW|Xt)] = \\frac{ ||x\u03c9 \u2013 \u03bc\u03bf,\u03c4'=0(x, t)||2}{||x - pref,t'=0(x, t)||2}(17)\nThe derivations of the terms\nLDDE = Exw,x,xx ~q(xx|x),x\u00a6~q(x}\\x6) [-log \u03c3(\u03b2( ||xw \u2013 \u03bc\u03bf,\u03c4'=0(x, t)||2 + ||x - pref,t'=0(x,t)||2 + ||x - \u03bco\u03c4'=0(x, t)||2 - ||x - pref,t'=0(x,t)||2 +(rok - ref,k - ro,k + rref,k)))] (18)"}, {"title": "8. Extended Experiments", "content": "8.1. Implementation Details\nWe employ a constant learning rate with a warm-up schedule, finalizing at 2.05 \u00d7 10\u22125. The hyper-parameters \u03b2 and \u03bc are set to 5000 and 0.1 respectively. To optimize computational efficiency, both gradient accumulation and gradient checkpointing techniques are utilized. The validation set of Pick-a-pic dataset contains over 300 prompts. For each prompt, we generate eight images and subsequently evaluate the scores using various models.\nOur source code is available at https://anonymous.\n4open.science/r/DDE-B6C8/README.md.\n8.2. More Quantitative Evaluation\nWe conduct a quantitative evaluation of the Inception Score (IS) [18] for both SD15 and SDXL models. Each model is executed with generating 50 images for each class in the ImageNet dataset, resulting in a total of 50,000 images. The prompt used for generation is formatted as \"A photo of <class name>\\\u201d. To address ambiguity, we specify \"bird crane\" and \"machine crane\" for class \"crane\". For the duplicated class \"maillot\", we distinguish between \"maillot swimsuit\" and \"maillot tanksuit\". These minor modifications do not significantly impact the results. The Inception Scores are calculated using the Inception-V3 model [23]. A comparative analysis of the scores is presented in Table 4, indicating that our alignment training enhances the Inception Score.\nIt is important to note that these models are not trained on ImageNet. Therefore, we consider this comparison as a weak validation, which is included in the supplementary material.\n8.3. Extended Qualitative Evaluation\nIn this subsection, we present additional generated case comparisons to substantiate the superior quality of our method.\nA comparison of the SD15 model can be found in Fig. 8. Our model demonstrates superior proficiency in generating intricate content. For instance, the beds produced by our model display quilts and pillows with finer folds. Moreover, the images generated by our method exhibit increased coherence, as the pizza appears to float without the presence of human hands and the background stars exhibit a more natural look. The dinosaurs' heads our model generate possess finer structures and textures compared to the base model. The cat stickers generated by our DDE maintain the structural integrity of the cat head. Additionally, our model excels in generating images with a more appropriate layout, as evidenced by the two cats playing chess with a clearer chessboard and more natural positions than those generated by the base model. The dancing monkeys created by our method better preserve body structures than those produced by the base model.\nThe comparative analysis of the SDXL model is depicted in Fig. 9. For cars made out of woods, our model exhibits superior detail generation. It also demonstrates enhanced comprehension of prompts, accurately depicting dogs wearing suits, and astronauts with heads composed of cereal balls, as specified. Our generated human figures exhibit postures and positions that better adhere to the prompt requirements compared to those produced by the SDXL model. Additionally, our model effectively generates cartoon figures, avoiding structure collapse and providing a clearer interface between the man's head and the bird."}]}