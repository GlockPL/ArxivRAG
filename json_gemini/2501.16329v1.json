{"title": "sDREAMER: Self-distilled Mixture-of-Modality-Experts Transformer for Automatic Sleep Staging", "authors": ["Jingyuan Chen", "Yuan Yao", "Mie Anderson", "Natalie Hauglund", "Celia Kjaerby", "Verena Untiet", "Maiken Nedergaard", "Jiebo Luo"], "abstract": "Automatic sleep staging based on electroencephalography (EEG) and electromyography (EMG) signals is an important aspect of sleep-related research. Current sleep staging methods suffer from two major drawbacks. First, there are limited information interactions between modalities in the existing methods. Second, current methods do not develop unified models that can handle different sources of input. To address these issues, we propose a novel sleep stage scoring model SDREAMER, which emphasizes cross-modality interaction and per-channel performance. Specifically, we develop a mixture-of-modality-expert (MoME) model with three pathways for EEG, EMG, and mixed signals with partially shared weights. We further propose a self-distillation training scheme for further information interaction across modalities. Our model is trained with multi-channel inputs and can make classifications on either single-channel or multi-channel inputs. Experiments demonstrate that our model outperforms the existing transformer-based sleep scoring methods for multi-channel inference. For single-channel inference, our model also outperforms the transformer-based models trained with single-channel signals.", "sections": [{"title": "I. INTRODUCTION", "content": "Sleep is one of the most basic animal behaviours with wide biological implications. Classifying sleep into stages (typically Wakefulness, REM and non-REM) is an important aspect of sleep-related research [37]. Currently, one of the most common automated sleep staging techniques is based on electrophysiological time-series data. Polysomnography (PSG) by experts requires specialized knowledge of sleep architecture, as well as considerable time (i.e.experts have to observe the whole sleep process, which lasts about several hours), thus validating the necessity of automatic sleep staging.\nIn recent years, deep-learning-based methods have shown potential in sleep staging. Most of these works involve the electroencephalography (EEG) time series signals signals [6], [12], [29]. However, it has been shown that sleep staging with only EEG signals is insufficient for sleep classification (typically in distinguishing REM sleep and non-REM sleep) [4], [17], thus necessitating multi-modality sleep staging (i.e. staging with multiple types of input signals) [1], [15], [30], [32]. Although these multi-modality methods have improved the performance of sleep staging, there are two major drawbacks.\nFirst, previous methods suffer from limited information interactions between modalities. While previous studies have attempted to develop multi-modal models that integrate information across different modalities, they ignore the possible interrelationship of different electrophysiological signals. Specifically, these models embed different signal inputs into separate feature spaces and use a late fusion scheme to generate classification results. Since the electrophysiological signals are quite noisy (see Fig. 1), the cross-reference from multiple input signals can be essential in improving the robustness of automatic sleep staging models.\nSecond, previous methods have not developed unified models that can handle different sources of input, such as multi-channel and single-channel signals. For instance, a model trained on EEG and EMG signals cannot perform reasonable inferences on new data with only EEG signals. This limitation is brought naturally since such models focus on multi-channel joint embedding, but neglect the quality of per-channel embedding. As a result, the performance of these models is restricted, and their potential applications are hindered. Therefore, there is a need for a unified model that can handle different sources of input and improve the performance of automatic sleep staging models.\nTo address these problems, this study proposes a novel sleep stage scoring model that emphasizes the cross-modality alignment and per-channel performance. To achieve this, our model introduces the mixture-of-modality-experts (MoME) transformer. More specifically, we develop a model consisting of three pathways for EEG, EMG, and mixed signals with partially shared weights. Specifically, the attention weights are shared across the three paths, and the weights of the feed-forward networks are not fully shared (see Section IV for details). The partially shared weights implicitly instruct our model to align across different modalities. The three pathways ensure that our model can promote high-quality per-channel performance. Furthermore, we propose a self-distillation method to ensure better information interaction across modalities. We build an epoch MoME transformer for one-to-one sleep staging and a sequence MoME transformer for many-to-many sleep staging. Although our model is trained with multi-channel time-series signals (i.e. EEG and EMG), our model can accept single-channel (i.e. EEG or EMG) as well as multi-channel time-series signal as input in the inference phase.\nWe demonstrate the effectiveness of our proposed self-distilled MOME transformer, referred to as SDREAMER (self-distilled Mixture-of-Modality-Experts Transformer), through experiments on a public mice sleep dataset [18] labeled with sleep stages by our expert. For multi-channel sleep staging, our epoch MoME model and sequence MoME model outperform the transformer-based automatic sleep scoring method by significant margins, respectively. For single-channel sleep staging, our sequence MoME model also produces better inference results than the transformer-based models trained with single-channel signals.\nIn summary, the main contributions of our work can be summarized as follows:\n\u2022 We propose the unified sleep staging framework SDREAMER that can handle either single-channel or multi-channel input based on the MoME transformer.\n\u2022 We propose a self-distillation method for our proposed MOME transformer to ensure the multi-modality information interaction across different pathways (i.e.EEG, EMG, and mixed).\n\u2022 We demonstrate through extensive experiments that our proposed model is effective for both single-channel and multi-channel sleep staging."}, {"title": "II. RELATED WORK", "content": "A. Deep-learning-based Sleep Staging\nThe success of deep learning has inspired several deeplearning-based sleep staging algorithms based on electrophysiological signals. Basically, there are two ways to deal with the raw data. One way is to directly learn from the unprocessed signals [25], [34], [38]. Another way is to transfer them to spectrograms and design models to learn from the spectrograms using computer vision-based techniques [22], [42]. Although introducing spectrograms may produce more interpretable visual understanding for humans, Phan et al. have shown that introducing spectrogram inputs in addition to raw signal inputs may actually lead to a performance drop [30]. Therefore, we focus on automatic sleep staging based on raw electrophysiological signals in our work.\nMost automatic sleep staging works involve the time-series EEG signals [6], [10], [12], [26], [27], [29], [31], [36]. However, previous works have shown that sleep staging with only EEG signals is insufficient for sleep classification (typically in distinguishing REM sleep and non-REM sleep) [4], [17], thus validating the necessity of multi-modality sleep staging (i.e.staging with multiple types of input signals) [1], [15], [30], [32]. One common way of modeling the different types of input signals is the late fusion strategy [14], [21]. Despite their success in introducing different time-series signals into sleep staging, one major problem of these methods is the lack of information interaction across modalities. Although Jathurshan et al. have proposed cross-modal attention [32], the shallow design is insufficient for information interactions.\nB. Transformer-based Cross-modality Learning\nTransformer [41] is a deep-learning framework that can handle sequence input. The recent success in vision-language has demonstrated the power of transformer in cross-modality learning [19], [33], [43]. Typically, there are two types of structures for cross-modality learning: 1) fusion encoder, where different input modalities share the same encoder structure [39], [48] and 2) dual encoder, where different input modalities have different encoder structure but are mixed together in the feature space for cross-modal interactions [13], [44], [46]. Bao et al. suggest that mixture-of-modality-experts transformer can facilitate cross-modality learning efficiently and competitively [5]. Despite the effectiveness of cross-modality learning, most works on sleep staging ignore such techniques for modeling different types of input signals.\nC. Self Distillation\nSelf distillation is a type of knowledge distillation where the teacher network and the student network share the same model. In one type of self-distillation methods, knowledge can be transferred from the earlier epochs to the later epochs [35], [45], [51]. Knowledge can also be transferred from deeper parts of a neural network to the more shallow parts [11], [47]. Other self-distillation methods focus on the data and label augmentation [8], [50] or weak labels [3], [49]. The effectiveness of self-distillation has been analyzed and validated by some works [2], [23]. Recently, Wang et al. have proposed to distill across vision and language domains [44]. Inspired by their work, we propose a novel knowledge distillation scheme in the MoME module to distill across signal modalities for sleep staging."}, {"title": "III. DATA COLLECTION AND PREPROCESSING", "content": "A. Data Collection\nThis section presents the data collection procedures of the mouse sleep staging dataset. The dataset is a publicly available mice sleep dataset [18], while our experts manually labeled the dataset with sleep stages. All the sleep monitoring data are recorded from wildtype C57BL/6 mice subjects including both male and female ones. In particular, the EEG and EMG signals are collected when the subjects are placed in the chamber room. To denoise the collected EEG signals, experts apply filters including a high-pass filter at 1 Hz and a low-pass filter at 100 Hz. For the EMG signals, they apply a high-pass filter at 10 Hz and a low-pass filter at 100 Hz. In addition, a notch filter of 50 Hz is applied to eliminate the power line noise. Based on these raw data and the videos of sleeping mice, our experts annotate each collected signal epoch with its corresponding sleep stage. Specifically, wake stages show high muscle tonus and a high-frequency, low-amplitude EEG pattern. SWS sleep shows no muscle tonus and low-frequency, high-amplitude EEG pattern. REM sleep also shows no muscle tonus but with high frequency, low-amplitude EEG. A detailed description of the dataset can be found in Section V-A.\nB. Data Preprocessing\nThe dataset mainly contains the EEG and EMG signal records collected from sleeping mice, as well as the sleep stage labels for every second. These raw data may suffer from ambiguity in labels, data noise and excessive sequence length. Data preprocessing is applied to the raw data, including irregularity removal, subject-wise normalization, and temporal slicing.\nIrregularity Removal. During the whole span of mice sleep, there exists some time periods when experts are not sure which stage these periods belong to. Such periods are not labeled, which causes data irregularity. To handle these missing values, a neurally controlled differential equation approach [16] was considered. However, since missing values only occurs in a small percentage of the total data, it is unnecessary to apply this method to the large electrophysiological signal dataset. Instead, we remove missing values from the time series during loss calculation.\nSubject-wise Normalization. Raw signal normalization is performed using a subject-wise normalization approach, which normalizes the signals of the same subject with their mean and standard deviation. This method facilitates model learning as electrophysiological signal features often differ greatly in amplitude among individual subjects.\nTemporal Slicing. To obtain input data of a fixed time span, temporal slicing is performed. As our model aims to offer accurate prediction at the second level, the input signals are sliced into per-second signals. The sampling frequency for both signals are equal, denoted as T. Since we choose one second as the slicing time span, the sliced EEG or EMG time-series signal can be denoted as $x^{EEG} \\in \\mathbb{R}^{T \\times 1}$ or $x^{EMG} \\in \\mathbb{R}^{T \\times 1}$, respectively."}, {"title": "IV. METHODOLOGY", "content": "In this study, we propose sDREAMER, an efficient multimodal learning framework to stage sleep using a self-distilled mixture-of-modality-experts transformer. We first establish the problem hierarchically into epoch and sequence settings. Then we present the data input under these two settings. Afterward, the MoME module is introduced as the core for the unified multi-modal learning framework. The MoME module also serves as the foundation for our proposed Epoch SDREAMER and Sequence sDREAMER. Both models support multi-modal and mono-modal input cases. To further enhance the performance of mono-modality input, we propose a self-distillation pipeline that employs a multi-modal expert to guide the mono-modality expert.\nA. Problem Setup\nSleep stage scoring in mice is commonly approached as a multi-class classification task, requiring the model to make predictions according to the electrophysiological signals. Here we introduce two different settings for sleep stage scoring: epoch and sequence setup.\nEpoch Setting. For sleep staging, the smallest annotation window (one second for our dataset) of a signal is referred to as an epoch. As previously defined, the sliced signals $x^{EEG}$ or $x^{EMG}$ belongs to $\\mathbb{R}^{T \\times 1}$, where T is the size of time dimension, representing number of signal data points in an epoch. Since the input signals can be single-channeled or multi-channeled, an additional modality dimension is introduced, and the epoch extends to $x \\in \\mathbb{R}^{M \\times T \\times 1}$, where M is the size of modality dimension. The epoch setting can then be formulated as producing a sleep stage prediction $\\hat{y}$ for each epoch signal $x \\in \\mathbb{R}^{M \\times T \\times 1}$.\nSequence Setting. The sequence setting differs in considering multiple epochs as input and producing multiple predictions as output. A sequence is a larger window within the whole signal trace. It consists of multiple consecutive epochs. A K-size sequence is denoted as $(x_{i})_{i=1}^{K}$ where $x_{i}$ is the i-th epoch within the sequence. The sleep staging for a sequence is defined as the process of using a set of epoch signals, represented as $(x_{i})_{i=1}^{K}$, to predict a corresponding set of predictions, denoted as $(y_{i})_{i=1}^{K}$ by the model. These two settings are also referred to as one-to-one and many-to-many predictions conventionally [28] [32].\nB. Data Input\nPatching Operator To process the epoch signal $x_{i}$ in a K-size sequence $(X_{1},X_{2},\\cdots, x_{K})$, a patch operator is used to divide each epoch signal into a series of non-overlapping windows of equal length W. This operation generates a patched signal $X_{i} \\in \\mathbb{R}^{P \\times W}$, where $P = \\lfloor \\frac{T}{W} \\rfloor$.\nThe patch operator reduces the number of tokens for transformer input by a factor of P, resulting in a significant decrease in the computation complexity for multi-head selfattention by a factor of $P^{2}$, thereby enabling more efficient modeling.\nInitial Representation Given an EEG and EMG epoch x, we apply the aforementioned patching operator to obtain their signal patches. The resulting patches are then embedded in latent spaces to form intermediate initial tokens $I^{m}$ as in Equation 1, where m denotes the modality, $m_{i}$ denotes the i-th epoch of the modality, and $E_{trans}^{m}$ is the linear transformation matrix of this modality.\n$I^{m(i)} = [m_{1}^{i}E_{trans}^{m}; m_{2}^{i}E_{trans}^{m}; ...; m_{P}^{i}E_{trans}^{m}], E_{trans}^{m} \\in \\mathbb{R}^{W \\times D}$ (1)\nFollowing the Vision Transformer [9], we initialize two [CLS] tokens for EEG and EMG semantic representation within an epoch. To preserve the temporal and modality relationships among patch tokens, we introduced joint attribute encoding which combines learnable positional and modality encodings. The intermediate initial representations are concatenated with the [CLS] tokens, and the attribute encoding is then added elementwise to the combined representation as shown in Equation 2.\n$E_{attr}^{pos}, E_{attr}^{mod} \\in \\mathbb{R}^{P \\times D}$\n$E_{attr} = E_{attr}^{pos} + E_{attr}^{mod},$\n$T_{attr}^{m(i)} = Concat( (T_{CLS}^{m(i)}; (T_{CLS}^{m(i)}; I^{m(i)})) + E_{attr}$ (2)\nHere, $T_{CLS}^{s}, T_{CLS}^{ms}, E_{attr}^{pos}, E_{attr}^{ms}, E_{attr}^{mod}$ and $T_{attr}^{m(i)}$ represents the [CLS] token, positional encoding, modality encoding and final initial representations for modality m. Afterward, the initial tokens for EEG and EMG at i-th epoch are created and denotes as $T_{attr}^{eeg(i)}$ and $T_{attr}^{emg(i)}$, respectively.\nC. Mixture-of-Modality-Experts Module\nThe success of mixture-of-modality-experts (MoME) [5] in the visual language understanding task led us to investigate its potential strength in mice sleep scoring. In our paper, the MoME module is a module of three pathways for EEG, EMG and mix data, with partially shared weights. Our proposed epoch SDREAMER and sequence sDREAMER models are all built upon the MoME module to learn the contextual information within a given signal window. In this section, we discuss a unified token processing framework for the MoME module that applies to both models.\nSpecifically, a MoME module is composed of multiple MOME layers, as shown in Fig. 2. Each MoME layer is an efficient variant of the transformer layer but differs in the use of modality-agnostic shared multi-head self-attention (MSA) and modality-aware feedforward neuron networks (FFN). These modality-aware FNNs that capture modality-specific latent features are conventionally referred to as modality experts. To specify, our sleep staging model has three modality experts: EEG, EMG, and mix experts. By designing different forward propagation pathways that pass through these modality experts, the MoME module is able to generate both mono-modal and multi-modal representations.\nGiven the initial representations of N tokens from a modality m denoted as $T^{m}$, a pathway of L layers is represented as follows:\n$T^{m} = [t_{0}^{m}; t_{1}^{m};...; t_{N}^{m}]$ (3)\n$I_{l}^{m} = MSA (LN (T_{l-1}^{m})) + T_{l-1}^{m}$, l = 1...L (4)\n$T_{l}^{m} = \\psi_{(m, l)} (LN (I_{l}^{m})) + T_{l}^{m'}$, l = 1...L (5)\nHere, the $\\psi_{(m, l)}$ is a mapping function from the layermodality joint space D to the modality experts space F as shown in Equation 6. In other words, the activation of the modality experts is determined by the input modality and layer index. Such a mapping design allows the model to handle various inputs and produce multiple outputs with a unified model. Fig. 2 provides a graphical illustration of this mapping function.\n$\\psi_{(m, l)}: D \\rightarrow F, D = \\{(m, l)|m \\in M, l \\in \\{i\\}_{i=1}^{L}\\}$ (6)\n$F = \\{FFN_{eeg}, FFN_{emg},FFN_{mix}\\}, M = \\{eeg, emg, mix\\}$\nEEG and EMG Pathways. Our MoME modules consist of two mono-modal pathways, namely the EEG-pathway and the EMG-pathway. The EEG/EMG pathway takes the input tokens corresponding to each modality and feeds them through each MoME layer to learn mono-modal contextual information. In each MoME layer, the representations first pass through the multi-head self-attention layer to learn the temporal dependencies between tokens. Next, the representations are projected into modality-specific latent space. Finally, the token representations $T_{L}^{eeg}$ and $T_{L}^{emg}$ at the last MoME layer L are outputted.\nMix Pathway. The mix pathway serves to capture both intramodal and cross-modal information by concatenating tokens from EEG and EMG and feeding them to the MoME layers. Initially, the mix tokens $T_{mix}$ pass through MSA in the early MoME layers before being split and mapped back to their original latent space to maintain modality coherence. Using EEG/EMG modality experts ensures stable modality interaction, as earlier layers' low-level features are susceptible to noise. By projecting representations to their own space, the potential noise entering another space is constrained to a certain degree. However, at later layers, token representations are mapped to a multi-modal space with mixed experts to enable deeper modality interaction. The MoME module's output is the token series from the last layer, denoted as $T_{L}^{mix}$.\nD. Epoch Mixture-of-Modality-Experts Transformer\nFollowing epoch settings in previous works [28], [32], we proposed an Epoch SDREAMER model for sleep state classification with a one-to-one paradigm. The Epoch SDREAMER, depicted in Fig. 2, consists of an epoch-level MoME module with three pathways, each assigned a modality-specific classification head. The model takes the initial representations for patches of a given epoch signal as input, and the epoch-level MOME module aggregates representative information among patches to mine epoch context.\nDuring training, all three pathways are enabled. The EEG and EMG initial representations propagate along their respective pathways to produce the final representations, and their corresponding [CLS] tokens $T_{cls}^{eeg}$ and $T_{cls}^{ems}$ are fed to their respective classifiers for prediction. However, the model still lacked cross-modal information, so we leveraged the mix pathway to learn rich cross-modal information while maintaining the information within each modality. The final $T_{cls}^{mix}$ token is utilized, which is a combination of the EEG and EMG representations. During inference, the model can flexibly select the pathway according to the specific token type and generate predictions using only the relevant information within the input token.\nE. Sequence Mixture-of-Modality-Experts Transformer\nTo tackle sleep staging in a sequence setting, we proposed a sequence SDREAMER model that captures contextual information hierarchically. Unlike the epoch setting, where each epoch is considered as a separate sequence, Sequence SDREAMER views each epoch in relation to its neighboring epochs. The architecture of the proposed Sequence SDREAMER model is illustrated in Fig. 3, which employs an epoch-level MoME module at a lower hierarchy to capture epoch-level contexts. Next, a sequence-level MoME transformer is used to capture sequence-level contexts at a higher hierarchy.\nEpoch Encoder. Although the epoch-level MoME module in Epoch sDREAMER and Sequence sDREAMER transformer shared the same network architecture, their information flow differs for both training and inference stages. The mix pathway is not enabled in the epoch-level MoME module because multi-modality information is better modeled during the sequence stage at a higher level. This design decision ensures robustness to noise during training and speeds up inference. Thus, the epoch-level MoME module functions as a context extractor for EEG and EMG epoch signals. To compute the initial representations for an EEG/EMG sequence consisting of K epochs, we concatenate the initial tokens from each epoch to form the whole sequence's epoch-level initial representations, denoted as $H_{0}^{m}$. Each epoch's initial representation is then passed through the EEG/EMG pathway. Finally, the resulting [CLS] tokens that have passed through the MoME module for all K epochs are concatenated to form the sequence-level initial representations of the given modality.\n$H_{0}^{m} = [T_{cls}^{m(1)}; T_{cls}^{m(2)}; ... ;T_{cls}^{m(K)}]$ (7)\n$T_{cls}^{m(i)} = Epoch \\text{-} MOME (T_{attr}^{m(i)}), i = 1 ... K$ (8)\n$Z^{m} = Concat (T_{cls}^{m(1)}; T_{cls}^{m(2)}; ... ;T_{cls}^{m(K)})$ (9)\nAfter passing through both pathways, two sequence-level tokens are generated: $Z^{eeg}$ and $Z^{emg}$.\nSequence Encoder. Considering a series of EEG/EMG tokens outputted from the epoch-level MoME module as the context features of each epoch, we now transform them into initial representations to sequence MoME module. To denote the temporal relationship between epochs, two sequence-level positional embeddings $E^{eeg} \\in \\mathbb{R}^{K \\times D}$ and $E^{emg} \\in \\mathbb{R}^{K \\times D}$ are added elementwise to EEG and EMG tokens, respectively.\nNext, the sequence-level MoME blocks begin to learn dependencies between epochs within the same sequence. Like the Epoch SDREAMER, the EEG and EMG pathways process the input tokens separately and map their learned representations to modality-specific latent space. The output tokens from each pathway are then projected with pathwayspecific classification heads to the label space. Regarding the mix pathway, the mixed EEG-EMG tokens are first projected separately before being mapped to a multi-modal latent space. To better aggregate the multi-modal information learned in the mix pathway, the output EEG-EMG tokens from the same epoch are concatenated along the feature dimension. These concatenated tokens are then mapped to the label space with another pathway-specific head. Finally, three sets of output predictions are generated: $\\{z_{i}^{eeg}\\}_{i=1}^{K}$, $\\{z_{i}^{emg}\\}_{i=1}^{K}$ and $\\{z_{i}^{mix}\\}_{i=1}^{K}$.\nF. Self-Distillation\nProvided a multi-pathways structure, we seek an effective approach that enables such pathways to co-improve with each other. To this end, a self-distillation framework for better mono-modal and multi-modal representation learning is presented. To specify, the output from the mix pathway is leveraged to distill the outputs from EEG and EMG pathways. Given the outputs of each pathway, the self-distilled loss formulates as the KL-divergence between the predictions from EEG/EMG and pseudo-targets provided by the mixed pathway. Let $p_{T}$ denotes the softmax function with temperature factor T defined as\n$P(z_{i}^{m(k)}) = \\frac{exp(z_{i}^{m(k)}/T)}{\\sum exp(z_{i}^{m(j)}/T)}$ (10)\nwhere $z_{i}^{m(k)}$ denotes the k-th dimension of the logits vector for i-th prediction given by m modality pathway. The selfdistillation loss for EEG and EMG formulates as shown in Equation 11 and 12, respectively.\n$L_{sd-eeg} = KL (P_{T_{eeg}}(z_{i}^{eeg}) || P_{T_{eeg}}(z_{i}^{mix}))$ (11)\n$L_{sd-emg} = KL (P_{T_{emg}}(z_{i}^{emg}) || P_{T_{emg}}(z_{i}^{mix}))$ (12)\nThe logits vectors of predictions from three pathways are denoted as $z_{i}^{eeg}, z_{i}^{emg}$, and $z_{i}^{mix}$. $T_{eeg}$ and $T_{emg}$ are temperature factors for the distillation of EEG and EMG. Along with the self-distillation loss, a cross-entropy loss is also incorporated into the optimization process.\n$L_{ce} = - \\sum_{i=1}^{L} y_{i} log(P_{T=1}(z_{i}^{mix}))$ (13)\nThe $y_{i}$ and $z_{i}^{mix}$ indicate the i-th ground truth and prediction from the mix pathway. L is the number of epoch signals. The total loss for MoME models, shown in Equation 14, is a linear combination of self-distillation and cross-entropy loss.\n$L_{mome} = (1 - \\alpha) L_{ce} + \\frac{\\alpha}{2} (L_{sd-emg} + L_{sd-emg})$ (14)\nHere, the $\\alpha$ is a scaling factor used to balance the weightage of the loss terms."}, {"title": "V. EXPERIMENTS", "content": "A. Experiment Setup\nDataset. We conduct an evaluation of our proposed epoch SDREAMER and sequence sDREAMER models using the mouse sleep staging dataset, which so far comprises EEGEMG paired signals collected from mice during sleep and will include additional modalities in the future. Experts have labeled the sleep stages of the mice every second based on these electrophysiological signals, assigning each second of recorded data one of the following labels: Wake, SWS (slowwave sleep), or REM (rapid eye movement). An illustrative example of the data is shown in Fig. 4.\nThe dataset consists of EEG and EMG records of 16 mouse subjects, with each record spanning approximately 4 hours. Given that the EEG and EMG signals are sampled at 512 Hz, there can be about 5-10 million EEG/EMG data points for each mouse record. To align with the expert labeling span, we set the epoch window to 1 second, resulting in a total of 10,000 epoch data samples. To evaluate the performance of our model, we adopted subject-wise split criteria, where the records of 12 mice subjects were used for training, while the records of 4 mice subjects were used for testing. The distribution of stages in the training set and test set can be seen in Fig. 5a and Fig. 5b, respectively.\nB. Implementation Details\nIn this section, we explain the epoch and sequence settings of our SDREAMER model. The epoch MoME model involves a 4-layer MoME module, with the first three layers having only EEG and EMG experts, and the last layer incorporating a mixed expert. The feed-forward network dimension for each expert is 512. The Sequence sDREAMER model comprises a 2-layer MoME module for epoch modeling and a 3-layer MOME module for sequence modeling. The entire epoch-level MoME module and the first two layers of the sequence MOME module have only EEG and EMG experts, and the final layer of the sequence MoME module has an additional mix expert. All experiments are performed on an NVIDIA RTX 3090 GPU with AdamW [20] as the optimizer. The learning rate and weight decay are set to 1e-3 and 1e-4, respectively. The non-hierarchical model utilizes a batch size of 256, while the hierarchical model uses a batch size of 16. The EEG and EMG self-distillation temperature factors are set to $T_{eeg} = 1.0$ and $T_{emg} = 3.0$, respectively, and the distillation weight $\\alpha$ is 0.33.\nC. Baselines\nWe compare our method with both machine learning-based and deep learning-based methods. Due to the limited work for mice sleep staging, we implemented multiple baseline model architectures with different settings, from simple to complex. We also include a state-of-the-art cross-modal transformer method [32] as one of the baselines.\nMachine-learning-based Methods. Decision tree, AdaBoost, XGBoost, and Random forest were chosen as the baselines for the machine learning-based methods. However, due to the high dimensionality of the EEG-EMG signal, capturing the underlying patterns within the raw signal is challenging for these methods. Therefore, we engineered several features of EEG-EMG traces. In particular, statistical features(e.g. mean, max, min, std, and skew) for the raw signal and its first-order derivative are leveraged. Additionally, we applied a discrete Fourier transform to both the raw signal and its derivative and incorporated the statistics in the frequency domain as well.\nDeep-learning-based Methods. For deep-learning methods, we developed several baseline models and also compared others' methods. To start with, MLP is a straightforward model with multiple feedforward layers. Bi-LSTM is a sequential modeling model that captures temporal contexts from the forward and backward directions. The channel-independent transformer [24] is an architecture that shares the attention weight for multi-channel input signals. A dual-encoder transformer represents a typical late-fusion model that uses two independent encoders for each input signal and merges the information late in the architecture. The cross-attention transformer is a variant of the dual-encoder architecture that add the cross-attention layer to capture cross-modal information. Lastly, the state-of-the-art cross-modal transformer [32], another variant of the dual-encoder model, uses a self-attention layer to integrate information from two modalities.\nD. Quantitative Results\nMulti-modal Comparison. In order to demonstrate the efficacy of our model, we conducted a comparative analysis against the aforementioned baseline methods on the multimodal input setting. Table I presents the experiment results for EEG-EMG paired input, where we report the accuracy and F1-score for each model. Results show that our proposed epoch-level sDREAMER outperforms the machine-learningbased methods by a significant margin. Furthermore, our SDREAMER model outperforms all the deep-learning-based methods, including the state-of-the-art method cross-modal transformer, on both epoch level and sequence level.\nMono-modal Comparison. To demonstrate our model's generalizability on single-modal inputs, we evaluated our model's ability to produce accurate sleep stage predictions using a single EEG or EMG signal input in the inference stage. It is noteworthy that none of the baseline methods trained with multi-modal input can make inference with single-modal input. Therefore, we compare our model's performance with the baseline dual-encoder transformer model trained solely on EEG or EMG. Results are presented in Table II. The results indicate that our proposed model was able to learn a superior mono-modal representation by utilizing the multi-modality as a form of supervision during training.\nE. Ablation Studies\nIn this section, we present the ablation studies for our model. Specifically, as the effectiveness of the MoME transformer has been validated, we hereby conduct an ablation study regarding positional and modality embedding to investigate the contribution of different attribute embedding. As shown in Table IV, the joint attribute encoding improves the performance by an increase of approximately 1.0% in accuracy. Among the two encodings, positional encoding is of greater importance than modality encoding. One possible explanation for this is that there are only two kinds of electrophysiological signals incorporated within this sleep staging task, thus weakening the impacts of modality encoding. In addition, we also perform an ablation study on the contribution of self-distillation. Table III indicates that the self-distillation not only improved the monomodality performance significantly but also promoted the multi-modal performance by a large margin.\nF. Visualizations\nt-SNE Analysis. To explore the embeddings learned by our model, we utilized t-SNE [40] to visualize the highdimensional embedding in a two-dimensional space. The resulting visualization, shown in Fig. 6, demonstrates the disentangled and aligned latent space learned by Sequence SDREAMER. In particular, the representations spaces for all three pathways are visualized, and they indicate a similar spatial pattern. This pattern alignment is non-trivial because we did not provide any supervision directly that would promote such alignment in the latent space-suggesting shared attention mechanism and self-distillation have implicitly benefited the multi-modal alignment. The t-SNE plot also reveals spatial continuity among the Wake, REM, and SWS sleep stages in the embedding space, consistent with real-world scenarios. Our model implicitly learns to maintain the relative spatial relationships in the ground truth space without injecting any prior knowledge of actual spatial relativity. These findings highlight the effectiveness and interpretability of our proposed method in sleep stage scoring.\nVisualizations. In Fig. 7, we give some predictions results of our sequence SDREAMER model. From the visualization results, we see that our model predicts most of the sleep stages correct."}, {"title": "VI. CONCLUSIONS AND DISCUSSIONS", "content": "Automatic sleep staging is a crucial first step for sleeprelated research. In this paper, we propose sDREAMER, a sleep staging model that can handle both single-channel and multi-channel inputs. Our sDREAMER emphasizes the crossmodality interaction and per-channel performance, and therefore generates high-quality staging results on the sleep staging task. Extensive experiments demonstrate the effectiveness of our proposed sDREAMER.\nThere are some limitations in our work. The performance of"}]}