{"title": "Autonomy-of-Experts Models", "authors": ["Ang Lv", "Ruobing Xie", "Yining Qian", "Songhao Wu", "Xingwu Sun", "Zhanhui Kang", "Di Wang", "Rui Yan"], "abstract": "Mixture-of-Experts (MoE) models mostly use a router to assign tokens to specific expert modules, activating only partial parameters and often outperforming dense models. We argue that the separation between the router's decision-making and the experts' execution is a critical yet overlooked issue, leading to suboptimal expert selection and ineffective learning. To address this, we propose Autonomy-of-Experts (AoE), a novel MoE paradigm in which experts autonomously select themselves to process inputs. AoE is based on the insight that an expert is aware of its own capacity to effectively process a token, an awareness reflected in the scale of its internal activations. In AoE, routers are removed; instead, experts pre-compute internal activations for inputs and are ranked based on their activation norms. Only the top-ranking experts proceed with the forward pass, while the others abort. The overhead of pre-computing activations is reduced through a low-rank weight factorization. This self-evaluating-then-partner-comparing approach ensures improved expert selection and effective learning. We pre-train language models having 700M up to 4B parameters, demonstrating that AoE outperforms traditional MoE models with comparable efficiency.", "sections": [{"title": "1. Introduction", "content": "Large language models (LLM) built on Mixture-of-Experts techniques (MoE, Shazeer et al., 2017; Lepikhin et al., 2021; Fedus et al., 2022) have gained increasing research and industrial attention (Jiang et al., 2024; Dai et al., 2024; Team, 2024; Sun et al., 2024). The core idea of MoE in LLMs involves dividing a large feed-forward network (FFN) into smaller FFNs, known as experts, and activating different experts' parameters for different inputs. The decision on which experts process which inputs are made by a router, typically an MLP-based classifier. Compared to dense models, MoE models are more efficient due to their sparse activation, and their ability to flexibly combine expert knowledge enhances downstream performance."}, {"title": "2. Background: Mixture-of-Experts (MoE)", "content": "We focus on studies of sparse MoE models, treating each feed-forward network (FFN) module as an expert. Each FFN, or expert, is expected to possess diverse and distinct abilities, enabling the model to process inputs effectively by activating only the experts with the necessary capabilities, thereby improving efficiency. Some studies (Chen et al., 2024; Lin et al., 2024) on dense MoE do not reduce the parameter activation ratio, which is not the primary concern of this paper. In this paper, when we refer to MoE, we mean sparse MoE.\nMoE-based LLMs (Jiang et al., 2024; Dai et al., 2024; Team, 2024; Lieber et al., 2024; Sun et al., 2024; Abdin et al., 2024) typically follow the FFN design in the Llama models (Touvron et al., 2023) as an expert module. The i-th expert within a specific layer can be formulated as:\n$E(x) = (SiLU(xW) (x)) W$,\nwhere $x \u2208 R^{dmodel}$ is the input hidden state; $W_g, W_p \u2208 R^{dmodel\u00d7dfin}$, and $W_o\u2208 R^{dfin\u00d7dmodel}$ are the expert weights. This paper focuses on this classical FFN formulation.\nA router (or gate) R determines which expert processes which hidden state. Many studies have proposed various routing strategies, such as token choosing top experts (Shazeer et al., 2017; Lepikhin et al., 2021), expert choosing top tokens (Zhou et al., 2022; 2023), dynamic expert calls (Raposo et al., 2024; Gong et al., 2024), and refining expert selection by solving mathematical problems (Lewis et al., 2021; Clark et al., 2022), among others. Without loss of generality, our discussion focuses on token choosing the Top-K experts (Shazeer et al., 2017; Lepikhin et al., 2021), but our experiments consider various strategies. Algorithm 1 presents a working pipeline of an MoE layer with a total of n experts. The \"[i]\" notation in the algorithm follows Python syntax, indicating the selection of the i-th element in a vector or a matrix.\nA challenge faced by MoE is the imbalanced expert load. MoE routers tend to disproportionately favor specific experts, resulting in suboptimal parameter utilization. Fedus et al. (2022) incorporate a load-balancing loss, controlled by a hyperparameter weight, $C_{aux}$, to ensure that each expert receives a similar load for a batch B with T tokens:\n$L_{aux} = C_{aux} \u00b7 \u03b7\u00b7 \\sum_{i=1}^{n} f_i \u00b7 P_i$, where\n$f_i = \\frac{1}{T} \\sum_{x\u2208B} 1 \\{i \u2208 argtopK(R(x))\\},\\\\P_i = \\frac{1}{T} \\sum_{x\u2208B} Softmax(R(x)) [i].$\nSeveral variants of this auxiliary loss have been proposed (Zuo et al., 2022; Wang et al., 2024a;b; Huang et al., 2024), but they all share the common goal of maintaining a balanced load. Therefore, our discussion focuses on the balancing loss presented above."}, {"title": "3. Method", "content": "We begin by introducing preliminary experiments that motivate the development of Autonomy-of-Experts (AoE) in Section 3.1. In Section 3.2, we refine the straightforward implementation from the preliminary experiments, improving the expert architecture to address efficiency concerns and, finally, deriving the AoE method."}, {"title": "3.1. An Insight: Experts \u201cKnow\u201d What They Know", "content": "We present the experiment that motivated the development of AoE models.\nGeva et al. (2021) interpret FFN layers as key-value memory networks, where inputs are projected into a \u201ckey\u201d vector (e.g., $(SiLU(xW_g) (xW_p))$). The \u201ckey\u201d vector retrieves knowledge or abilities stored in the parameters through a key-value matching mechanism (e.g., multiplying by $W_o$). If the experts can effectively handle the input, the \u201ckey\u201d should be highly activated, allowing for effective retrieval. Note that this example is purely analogical; there are no defined rules to determine which internal activations behave more like the \u201ckey\u201d and which behave more like the \u201cvalue,\u201d as models are not trained with constraints that would regularize these roles.\nInspired by (Geva et al., 2021), we conducted preliminary experiments to explore whether experts in pre-trained MoE-LLMs \"know\" their capabilities\u2014that is, whether the scale of their activation norms reflects their ability to handle specific inputs."}, {"title": "3.2. Autonomy-of-Experts (A0E)", "content": "The following paper centers on using the norm of $xW_g$ to guide expert selection in our new MoE language models pre-trained from scratch. There is no technical difference or challenge in applying our method to any other node, regardless of the architecture. However, utilizing nodes other than $xW_g$ or $xW_p$ is not cost-effective.\nThe efficiency of the rudimentary method in Section 3.1 must be improved. The primary overhead arises from all experts computing activations for a given token, even though not all results contribute to the final MoE output. Additionally, large $d_{ffn}$-dimensional activations (14,336 for Mixtral 8 \u00d7 7B and 6,400 for Phi-3.5-MoE) at the pause node are cached, leading to significant memory usage.\nA factorization of the $W_g$ matrix can address these two issues. We decompose $W_g$ into two low-rank matrices: $W_{down} \u2208 R^{dmodel \u00d7 dlow}$ and $W_{up} \u2208 R^{dlow\u00d7dwid}$, where $d_{low} < d_{model} < d_{wide}$. The i-th AoE expert can be formulated as:\n$E_i(x) = (SiLU (xw_{down} W_{up}) (xW_p)) W_o$,\nwhere $W_p \u2208 R^{dmodel \u00d7dwid}$, and $W_o \u2208 R^{dwid \u00d7 dmodel}$.\nAlgorithm 2 formulates the pipeline within an AoE layer. In each expert, $W_{down}$ first compresses the input vectors into low-dimensional activations. These activations are cached as C, and their $L^2$ norms are used to rank the experts. Given an input, the experts with the top-K norms use the cache to continue the forward computation within the expert, while unchosen experts abort processing. The compressed activations significantly reduce both the cache size and the computational overhead from unselected experts. This factorization does not impair the model's expressiveness, as the weights are inherently low-rank in large language models (Li et al., 2018; Aghajanyan et al., 2021; Hu et al., 2022).\nFurthermore, to enhance efficiency, the loop for calculating the activation cache (Line 2 in Algorithm 2) can be eliminated by combining the $W_{down}$ matrices of all experts into a single large matrix. This allows the cache to be obtained through a single multiplication:\n$W_{down} = [W^{down}_1, W^{down}_2,..., W^{down}_n] \u2208 R^{dmodel x (ndlow)}\nC = xW_{down}.$\nThe resulting $C \u2208 R^{ndlow}$ is then reshaped into an n \u00d7 $d_{low}$ matrix for subsequent computations.\nIn Section 4.1, we demonstrate that an AoE model achieves up to 97% of the throughput of a traditional MoE model while also delivering superior downstream performance."}, {"title": "4. Experiments", "content": "We begin by providing a detailed analysis of our method through ablation experiments on pre-trained small language models using AoE and traditional MoE. These experiments enable us to answer key research questions related to AoE. Based on the insights gained, we scale up the language models to 4 billion parameters, demonstrating AoE's scalability."}, {"title": "4.1. Method Analysis through Small Language Models", "content": null}, {"title": "4.1.1. GENERAL SETUP", "content": "We train small language models consisting of 12 layers, each containing 12 attention heads. Each layer contains 8 experts, with the top-K = 2 experts selected. Models use the Llama (Touvron et al., 2023) vocabulary of size 32,000 and the same pre-RMSNorm (Zhang & Sennrich, 2019) module. We set $d_{model}$ = 768 and $d_{ffn}$ = 3,072 for traditional MoE models, while the values of $d_{low}$ and $d_{wide}$ for AoE models are variable. Specifically, in all experiments below, to ensure that the total number of parameters in an AoE model is comparable to that of an MoE model, when we adjust $d_{low}$, $d_{wide}$ is set as follows:\n$d_{wide} = \\frac{3d_{model} d_{ffn} - d_{low} d_{model}}{d_{low} +2d_{model}}$\nThe total number of model parameters is 732 million, and the number of activated parameters is 247 million.\nWe train models on 100 billion tokens from RedPa-jama (Computer, 2023), with a batch size of 4.2 million tokens, a learning rate of 2 \u00d7 $10^{-4}$, and a linear warmup over the first 4,800 steps, followed by a cosine decay schedule that reduces the learning rate to 1.28 \u00d7 $10^{-5}$ (Tow et al., 2024). The AdamW optimizer (Loshchilov & Hutter, 2019) is employed with ($\u03b2_1$, $\u03b2_2$) = (0.9, 0.95), a gradient norm clipping threshold of 1, and a weight decay of 0.1.\nWe conduct a comprehensive evaluation of language models across a range of widely used tasks, including ARC-easy (Clark et al., 2018), PIQA (Bisk et al., 2020),"}, {"title": "4.1.2. RESOLVING QUESTIONS REGARDING AOE", "content": "We investigate the following questions related to AoE through a series of ablation experiments."}, {"title": "Question 1: How does the downstream performance of AoE compare with traditional MoE models?", "content": "We evaluated various configurations of AoE (Configs. 5 to 12) and traditional MoE models (Configs. \u2460 to \u2463). Every AoE setup outperforms the best-performing MoE configuration in terms of average accuracy across eight tasks. Notably, AoE without any auxiliary loss surpasses traditional MoE models, which enhances the simplicity of training an MoE model. Additionally, AoE exhibits lower training loss, suggesting more efficient training. We elaborate on this in Question 2."}, {"title": "Question 2: What is the impact of varying $d_{low}$?", "content": "We adjusted $d_{low}$ to values of 64, 128, 256, and 512, corresponding to Configs. 6, 8, 10, and 12, respectively. The combined impact of $C_{aux}$ and $d_{low}$ will be discussed in the next question. All of these variants outperform the traditional MoE model in downstream performance. The performance differences among these configurations are relatively small. The maximum performance gain occurs when $d_{low}$ is approximately one-third of $d_{model}$ (256/768). Both smaller and larger values of $d_{low}$ result in lower performance, though they still surpass the baselines. The suboptimal performance with smaller $d_{low}$ may be due to the factorization of $W_g$ into $W_{down} W_{up}$ being a lossy approximation when $d_{low}$ is below the true rank of $W_g$. Conversely, larger $d_{low}$ introduce more noise into the activation, potentially hindering the effectiveness of the norm-based selection measure.\nIn Figure 2, we present the negative log-likelihood (NLL) loss during training for traditional MoE (Config. 2) and AoE models (Configs. 6, 8, \u2469, and \u246b). AoE models exhibit more effective expert learning, as evidenced by lower loss values. However, when $d_{low}$ = 64 (Config. 6), the loss is comparable to that of traditional MoE models, suggesting that smaller $d_{low}$ values hinder AoE performance. In contrast, $d_{low}$ = 256 (Config. \u2469) results in the lowest training loss overall, reinforcing the finding that setting $d_{low}$ to approximately one-third of $d_{model}$ yields the most benefits."}, {"title": "Question 3: Is AoE compatible with other expert-selection strategies?", "content": "We also train language models using the Top-P token-choice (Huang et al., 2024) and the Top-K expert-choice strategy (Zhou et al., 2022). Table 3 shows that AoE outperforms traditional MoE models, demonstrating its generality. Details are provided in Appendix B."}, {"title": "Question 4: How is the load balancing of AoE?", "content": "There are three main findings regarding load balancing.\nFinding 4.1: AoE improves load balancing compared to traditional MoE models, with or without $L_{aux}$.\nAoE can incorporate $L_{aux}$ with minor modifications to Eq. 2, as shown below:\n$L_{aux} = C_{aux}\u00b7\u03b7\u00b7 \\sum_{i=1}^{n} f_i\u00b7P_i$, where\n$f_i = \\frac{1}{T} \\sum_{x\u2208B} 1 \\{i \u2208 argtopK (L^2-Norm (xW_{down} ))\\},\\\\P_i = \\frac{1}{T} \\sum_{x\u2208B} Softmax(L^2-Norm (xW_{down}) )) [i].$\n$C_{aux}$ is determined using a validation set comprising 5 billion tokens from (Gokaslan & Cohen, 2019). Experiments indicate that $C_{aux}$ = 0.01 is effective for both traditional MoE and AoE models. We adopted this value across all configurations without further hyperparameter tuning.\nFigure 3 illustrates expert load statistics on the SST-2 dataset (Socher et al., 2013) for Configs. \u2460, \u2461 (Traditional MoE with and without $L_{aux}$), \u2468, and 10 (AoE with and without $L_{aux}$). We report both the load distribution $f_i$ (as defined in Eqs. 2 and 6), representing the percentage of tokens processed by expert i, and the entropy of the load distribution within each layer:\n$Ent_{load} = - \\sum_{i=1}^{n} f_i log f_i.$"}, {"title": "Finding 4.2: AoE models exhibit stronger confidence in expert selection.", "content": "We introduce the confidence entropy, denoted as $Ent_{conf}$. For each layer, we have:\n$Ent_{conf} = - \\sum_{i=1}^{n} P_i log P_i$,\n$P_i = Softmax(L^2-Norm (xw_{down})),$ for AoE\n$Softmax(R(x)),$ for traditional MoE\nThis entropy quantifies the confidence in expert selection: lower entropy indicates a distribution closer to a one-hot vector, signifying more confident expert selection, while higher entropy reflects greater uncertainty in expert decisions. AoE exhibits significantly lower entropy, demonstrating stronger confidence in selecting experts. Furthermore, its confidence increases from shallow to deep layers, aligning with the intuitive inductive bias that shallow layers perform fundamental, non-specialized functions, whereas deeper layers handle specialized and abstract tasks (Wang et al., 2023; Lv et al., 2024). In contrast, MoE models do not display this trend, potentially suggesting more homogeneous expertise within and across layers (Wang et al., 2024a)."}, {"title": "Finding 4.3: Beyond improved load balancing, AoE with $L_{aux}$ achieves better downstream performance.", "content": "In general, $L_{aux}$ benefits both traditional MoE and AoE models. However, when $d_{low}$ = 128, applying $L_{aux}$ results in a decrease in accuracy, which we attribute to task-specific variations. In conclusion, as addressed in response to Question 4, AoE exhibits strong potential for advancing MoE-based LLMs, owing to its improvements in both load balancing and downstream performance."}, {"title": "Question 5: Do improvements stem from the factorization of $W_g$?", "content": "We examined the impact of factorizing the experts' weight matrix on performance by comparing Configurations and \u2461. The factorization does not significantly influence performance, as expected in Section 3.2, based on findings that the weights of LLMs are inherently low-rank (Li et al., 2018; Aghajanyan et al., 2021; Hu et al., 2022). Therefore, the improvements observed with AoE are not attributed to the factorization of model weights."}, {"title": "Question 6: Does the improvement of AoE come from involving more parameters in expert selection?", "content": "We increased the size of the router in MoE to include n\u00b7$d_{low}$\u00b7$d_{model}$ parameters, ensuring that the number of parameters involved in expert selection remains consistent with that of AoE models. Note that in this setup, traditional MoE models have more activated parameters in total. Comparing Config. \u2463 and, the larger router provides a slight performance benefit. However, every AoE setup still outperforms this configuration. Thus, the improvement in AoE is not primarily due to involving more parameters in expert selection."}, {"title": "Question 7: How aligned are the self-evaluation criteria among experts?", "content": "In AoE models, each expert independently develops self-evaluation criteria for processing tokens, as reflected in their activation scales. This might raise concerns that some experts could become overly \u201cegoistic,\u201d meaning their internal activations are consistently larger than those of others. For example, one expert might produce activations with norms ranging from 10 to 20, while an \u201cego\u201d expert produces activations with norms from 20 to 30, leading to biased selections that favor the \u201cego\u201d expert.\nWe track dynamics of activation norms during pre-training. Figure 4 shows the details for Configs. and 10. Except for the very initial period, experts' self-evaluation criteria are well aligned, as evidenced by clusters of same-colored plots (representing experts within the same layer). In the early stages of training without the auxiliary loss, some middle-to-upper-layer experts exhibit significantly lower activation. However, AoE naturally resolves this imbalance in activation scales during training. Alternatively, $L_{aux}$ can address this imbalance earlier because it acts as a regularizer for activation norms, increasing the norm scales of underactive experts and ensuring they are used more often."}, {"title": "Question 8: How Efficient is AoE?", "content": "Table 5 shows the maximum training throughput (tokens processed per second per GPU) and memory usage for both traditional MoE models and various AoE models. Here are the key findings:\nFinding 8.1: AoE achieves up to 97% of the throughput of the traditional MoE model, with the added cost of memory.\nAdditionally, note that experts in our experiments work sequentially within the same layer but in practical deployments of MoE-LLMs, experts are typically distributed across different devices and operate in parallel. Consequently, experts must wait for the most loaded expert to finish computation, resulting in idle time that can be quantified by the difference between the maximum and minimum expert loads. The total differences across layers are 1.49 for Figure 3(c) (traditional MoE) and 1.41 for Figure 3(d) (AoE). In this case, AoE can achieve an additional time reduction equivalent to processing 8% of the total tokens through a single MoE layer. Assuming an ideal load distribution where each of the 8 experts processes 12.5% of the total tokens, this reduction translates to a 64% decrease in the running time of one MoE layer. This advantage, however, is not reflected in the reported efficiency metrics.\nFinding 8.2: In AoE, memory usage and throughput are influenced by $d_{low}$, presenting trade-offs.\nIn terms of incremental memory, a smaller $d_{low}$ requires a larger $d_{wide}$, thereby increasing the memory consumption of $xW_{up}$ to T\u00b7$d_{wide}$, where T is the number of tokens. Conversely, a larger $d_{low}$ results in a larger activation cache, raising memory usage to n\u00b7T\u00b7$d_{low}$. For Configs. \u2465 to 10, n$d_{low}$ < $d_{wide}$, making the primary memory cost stem from the larger up-projection. In contrast, Config. 11 and 2 satisfy n\u00b7$d_{low}$ > $d_{wide}$, meaning the increased memory usage is more attributable to the larger activation cache. In terms of throughput reduction, a smaller $d_{low}$ requires more computational resources for the up-projection, while a larger $d_{low}$ leads to a higher unused activation cache."}, {"title": "4.2. Pre-training Large Language Models", "content": "We pre-train LLMs with a total of 4 billion parameters, of which 1.18B are activated. The initial learning rate is 3.2 \u00d7 $10^{-4}$ (Tow et al., 2024). Each model has 24 layers, with 20 attention heads per layer. For traditional MoE models, we set $d_{model}$ = 1,280 and $d_{ffn}$ = 5,120. Considering the trade-offs between efficiency overhead and performance gain, we set $d_{low}$ = 400 and, according to Eq. 5, derive $d_{wide}$ = 6,470. Other settings follow those in Section 4.1. Both models are enhanced by $L_{aux}$ with $C_{aux}$ = 0.01. Table 4 demonstrates that AoE outperforms traditional MoE models as they scale, with the performance improvement being more pronounced in LLMs compared to smaller models. This highlights the potential of AoE to drive advancements in larger and more powerful MoE-based LLMs."}, {"title": "5. Conclusion", "content": "We introduce Autonomy-of-Experts (AoE), a novel Mixture-of-Experts (MoE) paradigm that addresses a crucial yet widely overlooked issue: the separation between the router's decision-making and the experts' execution, which leads to suboptimal expert selection and learning. AoE selects experts based on their internal activation scales. Several architectural modifications ensure efficiency. Language models based on AoE outperform traditional MoE models in many aspects. This paper highlights the advantages of enabling MoE experts to self-select and aims to inspire the community to develop more powerful MoE-like models."}, {"title": "A. Re-running Experiments in Section 3.1 Using Alternative Expert-Selection Metrics", "content": "We also use the $L^1$ and $L^\\infty$ norms as expert-selection metrics in pre-trained LLMs, which resulted in poorer performance preservation compared to the $L^2$ norm. The time costs for each configuration are identical to those presented in Table 1 and are therefore omitted here for clarity. The results are shown below."}, {"title": "B. Pre-training Language Models Using Alternative Expert-Selection Strategy", "content": "We train both small traditional MoE and AoE language models by assigning tokens using the Top-P strategy (Huang et al., 2024). All hyperparameters and dataset details closely follow Section 4.1, except that we replace the Top-K = 2 strategy with Top-P = 0.6 following (Wang et al., 2024a). Models utilizing the Top-P strategy require an additional auxiliary loss equivalent to minimizing our introduced $Ent_{conf}$ (Eq. 8). This ensures that the model does not learn shortcuts by assigning uniform probabilities to all experts, which would activate too many parameters to achieve lower loss. Following (Huang et al., 2024), we set the weight of this regularization term to $10^{-4}$.\nExpert-choice (Zhou et al., 2022) is similar to the Top-K token-choice strategy. Consider an expert-selection matrix in the shape of T \u00d7 n (i.e., the router outputs in traditional MoE or the activation norms in AoE). The token-choice strategy applies the Top-K operator along the n dimension, whereas expert-choice applies it along the T dimension. Models trained using the expert-choice strategy do not require auxiliary losses. We set the \"capacity factor\" to 2 (see (Zhou et al., 2022) for details), allowing each expert to process 25% of the tokens in a batch.\n$d_{low}$ for AoE in these two experiments is 256. Results are shown in Table 3. AoE outperforms traditional MoE models, demonstrating its generality across various expert-selection strategies."}, {"title": "C. Additional Interpretation of AoE's Advantage", "content": "We provide some intuitive insights into AoE's strengths by developing a fully controlled classification task and monitoring training dynamics of both tiny AoE and MoE models. We provide details here for interested readers. This experiment is of a toy nature and not intended as a major claim or contribution.\nIn our setup, inputs are multivariate Gaussian vectors belonging to three classes. Classes one and two have distinct positive and negative means, respectively, while class three has a zero mean. We adjust their standard deviations to ensure no overlap within a three-sigma range. Initially, we train both tiny AoE and MoE classifiers to distinguish between classes one and two;"}]}