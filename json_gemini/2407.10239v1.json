{"title": "What is Reproducibility in Artificial Intelligence and Machine Learning Research?", "authors": ["Abhyuday Desai", "Mohamed Abdelhamid", "Nakul R. Padalkar"], "abstract": "In the rapidly evolving fields of Artificial Intelligence (AI) and Machine Learning (ML), the reproducibility crisis underscores the urgent need for clear validation methodologies to maintain scientific integrity and encourage advancement. The crisis is compounded by the prevalent confusion over validation terminology. Responding to this challenge, we introduce a validation framework that clarifies the roles and definitions of key validation efforts: repeatability, dependent and independent reproducibility, and direct and conceptual replicability. This structured framework aims to provide AI/ML researchers with the necessary clarity on these essential concepts, facilitating the appropriate design, conduct, and interpretation of validation studies. By articulating the nuances and specific roles of each type of validation study, we hope to contribute to a more informed and methodical approach to addressing the challenges of reproducibility, thereby supporting the community's efforts to enhance the reliability and trustworthiness of its research findings.", "sections": [{"title": "1 Introduction", "content": "The AI/ML domain has witnessed an explosive growth in research publications over the past few years. With major conferences receiving thousands of submissions annually, the sheer volume of research outputs has made it challenging to ensure consistent reproducibility. Conferences like NeurIPS, ICML, ICLR, and AAAI have seen significant growth in paper submissions. The number of papers submitted to these four conferences has increased by 169% from 2018 to 2023. \nWhile the number of publications has surged, attempts to reproduce the findings of these papers have often been met with challenges. Numerous studies have highlighted this critical issue in recent years. For example, a survey of 1,576 researchers in the Nature journal indicated that more than 70% of researchers have tried and failed in their attempt to reproduce another researcher's experiments, and more than half have failed to reproduce their own experiments Baker (2016).\nCollberg and Proebsting (2016) attempted to execute the code from 601 papers from computer systems research. This study only involved attempts to execute the code, not verify the correctness of the published results. These attempts to re-execute the code were divided into three categories: category 1: time to examine a research artifact is limited and communicating with the author is not an option; category 2 involves the scenario where ample time is available, but the lead author is not available for consultation; category 3 represents the case where ample time is available, and the author is available to correspond. They achieved success in 32.3% in category 1 attempts, 48.3% in category 2, and 54.0% in category 3.\nWithin the machine learning research domain, a comprehensive study by Raff (2019) involving an attempt to reproduce results from 255 papers from 1984 until 2017 yielded a success rate of 63.5%. This study involved independent reproduction attempts, where the original authors' code, even if available, was not used.\nA study by Islam et al. (2017) investigated the reproducibility of benchmarked deep reinforcement learning tasks and found a wide range of results reported in the literature for the same baseline algorithms, highlighting the difficulty in reproduction attempts. These variations were attributed to factors such as external randomness, under-reporting of hyperparameters, and a narrow range of tasks under the benchmark.\nPahm et al. (2020) revealed a striking statistic highlighting the reproducibility challenges in deep learning research. They reported that for 16 identical training runs for a popular deep learning network architecture called LeNet5, the accuracy of the resulting 16 models ranged from 8.6% to 99.0% a difference of 90.4% across the runs.\nThe reproducibility challenges in AI/ML research highlighted above, spanning from code execution issues to the variability in experimental outcomes, emphasize the critical need for a systematic approach to ensure validation efforts are both effective and reliable. This necessity brings us to another contributing factor in this crisis: the ambiguity surrounding key terminology in research validation."}, {"title": "2 Terminology Confusion", "content": "Addressing the reproducibility crisis in AI and ML research requires not only clear validation methods but also a precise understanding of fundamental terms like 'repeatability', 'reproducibility', and 'replicability'. Despite their significance in establishing the trustworthiness of research, there exists a notable confusion over these terms' meanings. For instance, Hunold (2015) highlighted findings from a survey conducted at the Euro-Par conference on reproducibility in parallel computing, where only 32% of respondents indicated they could accurately differentiate between replicability, repeatability, and reproducibility, pointing to a significant gap in understanding that complicates the validation process.\nGundersen (2021) highlights this issue further by reviewing the 34 diverse definitions and interpretations of 'reproducibility', 'replication', and related terms across numerous research papers, concluding that a single, agreed-upon definition does not exist. Another study by Barba (2018) on the disparities in terminologies for reproducible research reveals how the application and understanding of these terms can vary significantly across researchers in different scientific fields, contributing to ongoing debates and misunderstandings. Barba's study identifies three distinct approaches to usage of these terms:\n\u2022 Approach A: No distinction is made between 'reproducibility,' 'replicability,' and 'repeatability.'\n\u2022 Approach B1: 'Reproducibility' is defined as using the original data and code to regenerate the results, whereas 'replicability' refers to generating similar scientific findings with new data.\n\u2022 Approach B2: Conversely, 'reproducibility' implies that independent researchers achieve the same results using their own methods and data, while 'replicability' involves using the original study's artifacts."}, {"title": "3 The Reproducibility Framework", "content": "Gundersen (2021) proposes his definitions for reproducibility terms by referring to the general scientific method in research studies. We use Gundersen's approach while narrowing the focus on the key components incorporated in a research publication.\nA research publication consists of the components, as visualized in Figure 2:\n1. Claim/hypothesis: Represents the central assertion or prediction that the study aims to investigate. It is the result that the authors are trying to prove.\n2. Experiment design: This is the structured plan devised to test the claim or hypothesis. It encompasses the selection of methodologies for data collection, data preprocessing, model building, and validation. This stage establishes the research protocol, detailing how the experiment will be conducted to ensure valid and reliable results.\n3. Experiment implementation: Refers to the practical execution of the experiment according to the predefined design. This phase involves the operational aspects of the study, such as collecting data, coding and scripting for analysis or model building, and utilizing necessary hardware or software resources.\n4. Experiment outcomes: These are the raw results obtained from the experiment. These outcomes are the direct observations or data points collected during the implementation phase, before any analysis or interpretation.\n5. Analysis of outcomes: This is the step of examining and interpreting the experiment outcomes through various analytical techniques. This may include statistical testing, the creation of summary tables and charts, and other methods to extract insights and understand the data's implications regarding the hypothesis.\n6. Study conclusion: Refers to the final assessment of the hypothesis based on the analyzed outcomes. This stage involves synthesizing the findings to determine whether the initial claim is supported or refuted by the experimental evidence, culminating in a clear statement about the validity of the claim.\nWe now define the key types of validation studies as follows. Repeatability refers to the ability to obtain consistent results by the same team using the same experimental setup, including the claim or hypothesis, experiment design, implementation, outcomes, analysis of outcomes, and conclusions. This means that when the original researchers re-execute their experiment under the same conditions, they achieve the same findings.\nReproducibility: Reproducibility involves external researchers validating the correctness of an original experiment's findings by following the documented experimental setup. This can be achieved through direct use of the original data and code, called dependent reproducibility, or by reimplementing the experiment, titled independent reproducibility, ensuring the results are reliable and applicable across different teams. Importantly, validating the correctness of the implementation in both dependent and independent reproducibility distinguishes these efforts from mere repeatability, enhancing the study's scientific rigor.\nDirect Replicability: Direct Replicability is when an independent team intentionally varies the implementation of an experiment while keeping the hypothesis and experimental design consistent with the original study to verify the results. This deliberate alteration may entail using different datasets, methodologies, or analytical approaches. The objective is to affirm the original findings under slightly altered experimental conditions, but not entirely new ones, ensuring that the study's conclusions are not solely contingent on the original set of experimental parameters.\nConceptual Replicability: Conceptual Replicability refers to the process where an independent team tests the same hypothesis through a fundamentally new experimental approach. Unlike direct replicability, which alters only the implementation, conceptual replicability involves redesigning the experimental setup itself. This approach aims to validate the hypothesis in broader or different contexts by significantly deviating from the original study's design. Let us now explore these validation types in more detail."}, {"title": "3.2 The Role of Repeatability", "content": "Repeatability is the exercise of ensuring that results are consistently achievable under the same experimental conditions by the original researchers. When different researchers repeat the experiment without verifying its correctness, we term this external repeatability.\nExternal repeatability, while extending the concept to include independent researchers, does not materially enhance the trustworthiness or reliability of the findings beyond what is achieved by the original team's repetition. The absence of a thorough examination of the experiment's design or its outcomes means that this effort remains a basic repetition rather than a deepened form of validation."}, {"title": "3.3 Reproducibility for Validating Experiment Correctness", "content": "Reproducibility transcends the mere blind regeneration of results; it fundamentally involves a critical validation of the original study's implementation correctness. This process ensures that, when an experiment implementation is as described and without material flaws, it consistently leads to the same conclusions, affirming the reliability of the original findings.\nReproducibility focuses on validating the correct implementation of the experiment design and its ability to consistently lead to the original study's conclusions. It does not assess whether the experimental design is the most appropriate or the only option for testing the hypothesis; this validation is categorized under replicability in our framework."}, {"title": "Pathways to Achieving Reproducibility", "content": "There are two main pathways to pursue a reproducibility study:\n1. Dependent Reproducibility: This approach uses the original study's code and data to recreate the results and relies on the availability of these original artifacts.\n2. Independent Reproducibility: Entails a researcher independently reimplementing the experiment based on the published descriptions, without reliance on the original code and data.\nIndependent reproduction stands as a more rigorous validation method compared to dependent reproduction due to its requirement for the reproducer to independently parse and execute the original experiment's design. This method deepens the validation process, making it more probable to identify flaws or limitations in the original findings than when relying on dependent reproduction."}, {"title": "Navigating Common Scenarios", "content": "For further clarity around our definition of reproducibility, consider the following scenarios:\nDiscovering Flaws in the Original Study:\nReproduction attempts of AI/ML research may reveal flaws in the original study, such as data leakage or insufficient control of experimental variables, affecting outcomes significantly. If corrections to these flaws change the study's conclusions, it indicates a failure in reproducibility, pointing to problems with the original implementation's accuracy. This underscores that reproducibility goes beyond mere re-execution of an experiment by different individuals. It requires an independent verification of the experiment's implementation for correctness."}, {"title": "Adherence to Experiment Methodology", "content": "Reproducibility is achieved by closely following the original experiment's methodology, where minor modifications are permitted but should not be confused with direct replication. For instance, translating code from TensorFlow to PyTorch, while a technical modification, falls under the reproduction category if it follows the original experiment design and hypothesis testing. These minor changes do not constitute direct replication, which involves a deliberate alteration in the experiment's implementation to test the robustness of findings under varied conditions."}, {"title": "3.4 Replicability for Robustness and Generalization", "content": "The overarching goal of replicability, encompassing both direct and conceptual approaches, is to validate the robustness and generalization of a study's findings. By testing the original results through intentional alterations in experiment implementation, called direct replicability, or entirely new experimental designs, titled conceptual replicability, researchers aim to confirm that the conclusions drawn are not merely artifacts of specific experimental setups but hold across different contexts and approaches. Replicability can help understand the boundaries within which the original findings hold.\nDirect replicability focuses on intentional modifications to an experiment's implementation while adhering to its original design and hypothesis. These modifications may include using alternative datasets, analytical methods, computational tools, or statistical techniques, aiming to test the stability of the findings under slight but deliberate variations.\nConversely, conceptual replicability represents a more extensive departure from the original experiment by adopting entirely new designs to explore the same hypothesis. This approach allows for testing the hypothesis under significantly different conditions or assumptions, potentially uncovering new insights, or challenging the original study's conclusions."}, {"title": "4 Implications for Scientific Rigor and Reliability", "content": "Let's now explore the implications of the different types of validation studies on scientific rigor and reliability. As illustrated in Figure 3, the validation hierarchy from repeatability to conceptual replicability represents a continuum of increasing scientific rigor and reliability.\nRepeatability establishes the baseline integrity of research findings, ensuring that results are not artifacts of chance. It's the essential first check for any study, affirming internal consistency.\nDependent Reproducibility and Independent Reproducibility elevate scrutiny by testing if the findings hold when the experiment is recreated. Dependent reproducibility involves using the original materials and validating the correctness of the implementation as described in the study. Independent reproducibility is achieved by reconstructing the experiment based on the original study's methodology and similarly validating the implementation's correctness. Both approaches ensure that the experimental results are not only repeatable but also methodologically sound, reinforcing the reliability of the findings.\nDirect and Conceptual Replicability represent further steps in validating research findings. Direct replicability tests the original conclusions against variations in methodology, probing the findings' stability across different implementations. Conceptual replicability, the pinnacle of validation efforts, explores the hypothesis in entirely new contexts, assessing the generalizability and versatility of the conclusions.\nThis validation hierarchy illustrates that while repeatability and reproducibility are fundamental for trust in the original experiment's findings, direct and conceptual replicability are crucial for demonstrating the findings' robustness and generalization to methodological changes. Each step increases the rigor and provides a deeper understanding of what the research achieves and its limitations in the face of expanding scientific scrutiny."}, {"title": "5 Conclusions", "content": "In response to the challenges posed by the reproducibility crisis within the artificial intelligence and machine learning domain, our work has established a clear framework and definitions for critical terms such as repeatability, reproducibility, and replicability. By differentiating between these key concepts and exploring their implications for scientific rigor and reliability, we aim to clarify their meanings and underline their vital role in enhancing trust in scientific research. Ultimately, our goal is to foster a scientific environment where findings are not only repeatable and reproducible but also robustly replicable across various contexts, thereby advancing the field with trustworthy and verifiable knowledge."}]}