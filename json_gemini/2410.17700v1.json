{"title": "Scalable Random Feature Latent Variable Models", "authors": ["Ying Li", "Zhidi Lin", "Yuhao Liu", "Michael Minyi Zhang", "Pablo M. Olmos", "Petar M. Djuri\u0107"], "abstract": "Random feature latent variable models (RFLVMs) represent the state-of-the-art in latent variable models, capable of handling non-Gaussian likelihoods and effectively uncovering patterns in high-dimensional data. However, their heavy reliance on Monte Carlo sampling results in scalability issues which makes it difficult to use these models for datasets with a massive number of observations. To scale up RFLVMs, we turn to the optimization-based variational Bayesian inference (VBI) algorithm which is known for its scalability compared to sampling-based methods. However, implementing VBI for RFLVMs poses challenges, such as the lack of explicit probability distribution functions (PDFs) for the Dirichlet process (DP) in the kernel learning component, and the incompatibility of existing VBI algorithms with RFLVMs. To address these issues, we introduce a stick-breaking construction for DP to obtain an explicit PDF and a novel VBI algorithm called \"block coordinate descent variational inference\" (BCD-VI). This enables the development of a scalable version of RFLVMs, or in short, SRFLVMs. Our proposed method shows scalability, computational efficiency, superior performance in generating informative latent representations and the ability of imputing missing data across various real-world datasets, outperforming state-of-the-art competitors.", "sections": [{"title": "1 INTRODUCTION", "content": "NE classic problem in the field of machine learning is to learn useful latent representations of high-dimensional data in order to facilitate critical downstream tasks, e.g., classification [1], [2]. Latent variable models (LVMs) [3], [4] are particularly useful due to their superior performance in unsupervised learning, especially in the task of dimensional reduction [3].\nLVMs posit a relationship between the low-dimensional latent variables and the high-dimensional observations via a mapping function [5], [6], [7]. The latent variable, in essence, distills the information of the high dimensional data into a low dimensional representation [8], [9], [10], [11]. The choice of the mapping is a key factor in LVMs. When a linear mapping is employed, linear LVMs such as principal component analysis (PCA) [12], canonical correlation analysis (CCA) [13], factor analysis (FA) [14] among others, are incapable of capturing the intricate and complex patterns inherent in the observed data. To address this issue, neural networks and Gaussian processes (GPs) [15], [16], which possess a universal function approximation capability, are a typical choice of non-linear mapping functions, leading to well-known LVMs such as variational autoencoders (VAEs) [17] and Gaussian process latent variable models (GPLVMs) [18].\nVAEs are a type of LVM where the mapping of the observed data to the latent space (and vice versa) are parameterized by a neural network [17]. Despite the fact that VAEs exhibit extraordinary representational capabilities, they are still plagued by an issue known as \"posterior collapse.\" This issue amounts to the failure to capture the essential information contained within the observational data in certain scenarios [19], [20]. This problem is, in part, attributed to overfitting [21], [22].\nThe overfitting issue could be naturally addressed by GPLVMs [18], which are Bayesian models and where the mapping function is assumed to be sampled from a GP prior [15]. Unfortunately, GPLVMs still exhibit inferior performance in learning an informative latent space, primarily due to limited approximation capacity of the typically used kernel function [18], [23]. To address this issue, random Fourier feature latent variable models (RFLVMs) [24] employ a random Fourier feature approximation of the kernel function in order to facilitate a Markov chain Monte Carlo (MCMC) sampling algorithm for posterior inference under various likelihoods, in conjunction with a Dirichlet process (DP) kernel mixture for modeling flexibility [25], [26].\nNevertheless, the application of the MCMC inference to RFLVMs presents several significant challenges. First, MCMC inference-characterized by its high computational costs-limits the applicability of RFLVMs to large data sets [27], which renders RFLVMs prohibitive to use in the big data era. Second, the necessity for ongoing monitoring of Markov chain convergence introduces hyperparameters, e.g., burn-in steps, to the inference process. Optimizing and tuning these hyperparameters complicates model selection, consequently increasing resource and time consumption in computational efforts [28].\nDue to the drawbacks of employing an MCMC sampling algorithm, we explore a variational Bayesian inference (VBI) approach [29]. The fundamental idea behind VBI is first to reformulate posterior inference as a deterministic optimization problem. This approach aims to approximate the true posterior with a surrogate distribution (or optimal variational distribution), which is identified from a predefined family (or"}, {"title": "1.1 Contributions", "content": "While recent advancements in VBI have been dedicated to enhancing its applicability, it remains incapable of being applied to RFLVMs. To address this limitation, this paper proposes a novel VBI algorithm designed to facilitate its utilization in general models, including RFLVMs. The key contributions of our work are:\nStick-breaking construction of the DP. We introduce the stick-breaking construction of DPs [35], assuming a multinomial distribution for indicator variables. This approach enables the establishment of explicit PDFs for the indicator variables in the DP mixture used in RFLVMs, allowing us to explore the space of stationary kernels.\nBlock coordinate descent variational inference. We introduce a novel algorithm for VBI termed \"block coordinate descent variational inference\u201d (BCD-VI). The fundamental idea behind BCD-VI is to partition the variational variables into distinct blocks, and then sequentially optimize the subproblems formulated by each block with a suitable solver (RGVI, or proposed MFVI-based) that is carefully selected. More specifically, it uses RGVI for the non-conjugate terms, while retaining the computational efficiency of conjugate computations on the conjugate terms. Consequently, our proposed BCD-VI significantly simplifies problem-solving, improves algorithmic efficiency, and thus, extends the applicability of VBI to a wider array of models.\nScalable RFLVMs. We employ the BCD-VI algorithm on RFLVMs with the stick-breaking construction. We, facilitate the development of VBI for RFLVMs, ultimately leading to a scalable version of RFLVMs, namely SRFLVMs. Our proposed SRFLVMs method demonstrates scalability, computational efficiency, and superior performance in learning informative latent variables and imputing missing data, surpassing state-of-the-art (SOTA) LVM variants across a diverse set of datasets."}, {"title": "1.2 Paper Organization", "content": "The structure of the remaining sections in this paper is summarized as follows. Section 2 discusses prior work in LVMs and highlights challenges encountered in the application of VBI to RFLVMs. In Section 3, we introduce the stick-breaking construction and the BCD-VI algorithm to address those challenges, which leads to the development of SRFLVMs in Section 4. Based on the proposed approach, numerical results and discussions are presented in Section 5. Finally, Section 6 summarizes the conclusions derived from our work."}, {"title": "2 PRELIMINARIES AND PROBLEM STATEMENT", "content": "Section 2.1 presents an introduction to RFLVMs and outlines limitations associated with the existing sampling-based inference methods [24]. To address these limitations, Section 2.2 discusses an alternative approach known as VBI. We detail the foundational concepts underlying VBI and examine the challenges it faces when integrated with RFLVMs."}, {"title": "2.1 Random Feature Latent Variable Models", "content": ""}, {"title": "2.1.1 Gaussian Process Latent Variable Models", "content": "A GPLVM [18] establishes a relationship between an observed dataset $Y \\in \\mathbb{R}^{N\\times M}$ and latent variables $X \\in \\mathbb{R}^{N\\times Q}$ using a set of GP-distributed mappings, $\\{f_m(\\cdot) : \\mathbb{R}^Q \\rightarrow \\mathbb{R}, m \\in 1, ..., M\\}$. Formally, a GPLVM with Gaussian likelihood can be expressed as\n\\begin{align}\ny_{:,m}|f_m(X) &\\sim \\mathcal{N}_{N}(f_m(X), \\sigma^2 I_N), & m\\in 1,..., M, \\tag{1a}\\\\\nf_m &\\sim GP(0, K), & m\\in 1,..., M, \\tag{1b}\\\\\nx_n &\\sim \\mathcal{N}_{Q}(0, I_Q), & n\\in 1,..., N, \\tag{1c}\n\\end{align}\nwhere the subscript $m$ indexes the element of the output vector $y$, $GP(0, K)$ stands for a GP with zero mean and kernel function $k$, and the subscript of $N$ denotes the dimension of the distribution. Here, $\\sigma^2$ represents the noise variance, and $y_{:,m} \\in \\mathbb{R}^N$ denotes the vector of observations for the $m$-th dimension. The function $f_m(X)$ is defined as $f_m(X) = [f_m(x_1),..., f_m(x_N)] \\in \\mathbb{R}^N$. Additionally, we define $K_X \\in \\mathbb{R}^{N\\times N}$ as a covariance matrix evaluated on the finite input $X$ with a positive definite kernel function $k(x, x')$, i.e., $[K_X]_{i,j} = k(x_i, x_j)$. In the context of GPLVMs, the radial basis function (RBF) is a typical choice of kernel function [36]. Due to the conjugacy between the GP prior and the Gaussian likelihood, each mapping can be marginalized out, resulting in the marginal likelihood:\n\\begin{equation}\ny_{:,m} \\sim \\mathcal{N}_{N}(0, K_X + \\sigma^2 I_N), \\quad m\\in 1,..., M. \\tag{2}\n\\end{equation}\nBuilt upon this likelihood and the prior from Eq. (1c), we can derive the maximum a posteriori (MAP) estimate for the latent variable $X$ using gradient-based optimization methods, with computational complexity that scales $O(N^3)$ for $N$ observations [18]. A critical limitation of the GPLVM is its reliance on Gaussian likelihood as it may be an unrealistic assumption for certain data types (like count data, for example). To overcome this, RFLVMs integrate RFFs with weight-space approximations of GPs, thereby generalizing GPLVM to support diverse data likelihoods [24], [25]; see further discussions of related work on GPLVMs in Section D."}, {"title": "2.1.2 Random Fourier Features", "content": "According to Bochner's theorem [37], a continuous stationary kernel function $k(x,x') = k(x - x')$ is positive definite if and only if $k(\\cdot)$ is the Fourier transform of a finite positive measure $p(w)$. If the kernel is properly scaled, $p(w)$ is guaranteed to be a density [37], that is\n\\begin{equation}\nk(x - x') = \\int p(w) \\exp(iw^T(x-x')) dw.\n\\tag{3}\n\\end{equation}\nBuilt upon Eq. (3), the RFFs methods try to obtain an unbiased estimator of the kernel function [25], which is summarized in the following theorem.\nTheorem 1. Let $k(x, x')$ be a positive definite stationary kernel function, and let $\\varphi(x)$ be an associated randomized feature map defined as follows:\n\\begin{equation}\n\\varphi(x) = \\sqrt{\\frac{2}{L}} \\big[\\sin(w_1^T x), \\cos(w_1^T x),...,\\sin(w_{L/2}^T x), \\cos(w_{L/2}^T x)\\big]^T,\n\\tag{4}\n\\end{equation}\nwhere $W = \\{w_l\\}_{l=1}^{L/2} \\in \\mathbb{R}^{L/2 \\times Q}$ are independent and identically distributed (i.i.d.) random vectors drawn from the spectral density $p(w)$. Then, an unbiased estimator of the kernel $k(x,x')$ using RFFs is given by:\n\\begin{equation}\nk(x, x') \\approx \\varphi(x)^T\\varphi(x').\n\\tag{5}\n\\end{equation}\nProof. See Appendix A or [25]."}, {"title": "2.1.3 Weight-Space View of GPs", "content": "The representer theorem [36] states that the optimal solution for the mapping in kernel-based methods can be expressed as a linear combination of kernel evaluations between data points, specifically as $f(x) = \\sum_{n=1}^N \\alpha_{nm}k(x_n, x)$. By integrating the unbiased estimator provided by RFFs (Eq. 5) with this optimal mapping $f(x)$, a weight-space approximation of GP is obtained:\n\\begin{equation}\nf_m^*(x) \\approx \\sum_{n=1}^N \\alpha_{nm}k(x_n, x) \\approx \\sum_{n=1}^N \\alpha_{nm}\\varphi(x_n)^T\\varphi(x) = \\varphi(x)^T h_m \\varphi(x),\n\\tag{6}\n\\end{equation}\nwhere $h_m = [\\alpha_{1m},..., \\alpha_{Nm}] \\in \\mathbb{R}^N$, and where we define $H \\triangleq \\{h_m\\}_{m=1}^M \\in \\mathbb{R}^{M \\times L}$. Substituting this approximation into the GPLVM (see Eq. (1)) gives rise to the generalized GPLVMs as follows [24]:\n\\begin{align}\ny_{:,m}|h_m &\\sim p(g(\\Phi h_m), \\zeta), & m\\in 1,..., M, \\tag{7a}\\\\\nh_m &\\sim \\mathcal{N}_L(0, I_L), & m\\in 1,..., M, \\tag{7b}\\\\\nx_n &\\sim \\mathcal{N}_Q(0, I_Q), & n\\in 1,..., N, \\tag{7c}\n\\end{align}\nwhere $p(\\cdot,\\cdot)$ stands for some probability distribution, $\\Phi = [\\varphi(x_1),...,\\varphi(x_N)] \\in \\mathbb{R}^{N \\times L}$, $g(\\cdot)$ denotes an invertible link function that maps real vectors onto the support of the parameters of $p(\\cdot,\\cdot)$, and $\\zeta$ represents other parameters specific to $p(\\cdot,\\cdot)$. However, despite the versatility of generalized GPLVMs in handling diverse data, their reliance on a primary kernel function, such as the RBF kernel, often results in inaccurate learning of the underlying function map, thereby degrading the quality of latent variable inference [23]."}, {"title": "2.1.4 Random Feature Latent Variable Models", "content": "To enhance kernel flexibility, RFLVMs integrate kernel learning within generalized GPLVMs [24]. Particularly, by leveraging Bochner's theorem (Eq. (3)), which establishes a duality between the spectral density and the kernel function, a category of kernel learning methods approximates the spectral density of the underlying stationary kernel. In RFLVMs, a DP mixture kernel is employed, with its spectral density $p(w)$ defined as follows:\n\\begin{align}\nW_l|z_l &\\sim \\mathcal{N}_Q(\\mu_{z_l}, \\Sigma_{z_l}), & l\\in 1,...,L/2, \\tag{8a}\\\\\nz_l &\\sim CRP(\\alpha), & l\\in 1,..., L/2, \\tag{8b}\\\\\n\\alpha &\\sim Ga(\\alpha_0, \\beta_0), &  \\tag{8c}\n\\end{align}\nwhere each spectral frequency vector $w_l$ is distributed with a mixture component which is decided by the associated integer variable (or indicator variable) $z_l = k \\in \\{1, ... K\\}$, and $K$ is the number of clusters. Each mixture component is characterized by the mean vector $\\mu_k \\in \\mathbb{R}^Q$ and the covariance matrix $\\Sigma_k \\in \\mathbb{R}^{Q \\times Q}$, collectively denoted as $\\theta_{dpm} = \\{\\mu_k, \\Sigma_k\\}_{k=1}^K$. Each integer variable $z_l$ in $z \\triangleq \\{z_l\\}_{l=1}^{L/2}$ follows the Chinese restaurant process (CRP) with a concentration parameter $\\alpha$, which governs the degree of association for a set of indicator variables [30]. The parameter $\\alpha$ is further drawn from a Gamma distribution with shape hyperparameter $\\alpha_0 \\in \\mathbb{R}$ and rate hyperparameter $\\beta_0 \\in \\mathbb{R}$. In summary, the graphical model of RFLVMs is shown in Fig. 1. Next, we provide two examples of different data distributions in RFLVMs."}, {"title": "2.2 Variational Bayesian Inference and Challenges", "content": "Let $\\Theta$ represent all variational variables, i.e., $\\{W, X, z, \\alpha\\}$. The variational method converts inference into an optimization problem by aiming to identify an optimal variational distribution, denoted by $q^*(\\Theta)$, from a predetermined distribution family $q(\\Theta; \\vartheta)$ indexed by variational parameters $\\vartheta$. This identified variational distribution should closely approximate the posterior distribution $p(\\Theta|Y)$ in terms of Kullback-Leibler (KL) divergence. Mathematically, the optimization task can be formulated as follows:\n\\begin{equation}\n\\min_{\\vartheta} KL(q(\\Theta; \\vartheta) || p(\\Theta | Y)),\n\\tag{12}\n\\end{equation}\nwhere $KL(.,.)$ denotes the KL divergence between the two arguments. Because of the intractability of the posterior distribution, the equivalent optimization problem is to maximize the ELBO [27], $\\mathcal{L}(q(\\Theta; \\vartheta)$, i.e.,\n\\begin{equation}\n\\max_{\\vartheta} \\mathcal{L}(q(\\Theta; \\vartheta) = E_{q(\\Theta; \\vartheta)} \\Big\\{\\log \\frac{p(\\Theta, Y)}{q(\\Theta; \\vartheta)} \\Big\\}.\n\\tag{13}\n\\end{equation}\nUnfortunately, in RFLVMs, the optimization problem in Eq. (13) presents several key challenges: 1) Lack of explicit PDFs: The indicator variables defined in DP are assumed to follow a CRP, a stochastic sampling procedure that lacks explicit PDFs [30]. As a result, the ELBO, defined in Eq. (13), becomes incomputable. 2) Numerous variational variables: In VBI, one typically updates numerous variational variables jointly using a single optimization algorithm, unexpectedly overlooking the potential to exploit the unique optimization structures of the individual variables. For illustrating this point, we give an example below, which will be clearly demonstrated in the context of RFLVMs later in Section. 4."}, {"title": "3 METHODOLOGY", "content": "This section details two methods to address the challenges encountered when implementing VBI for RFLVMs. In Section 3.1, we introduce the utilization of the stick-breaking construction of DP to obtain explicit PDFs for indicator variables. Then in Section 3.2, we introduce the foundational principle of novel BCD-VI, which is motivated by RFLVMs."}, {"title": "3.1 Stick-breaking Construction of DP", "content": "As mentioned in Section 2.2, lacking explicit PDFs for $z$ renders ELBO incomputable [38]. This limitation does not affect MCMC sampling, as it only requires sampling from the posterior of $z$, represented as a stochastic process that can be easily sampled (see Eq. (10) in [24]). In contrast, in the context of VBI, an incomputable ELBO leads to an intractable problem (Eq. (13)). To tackle this issue, we utilize a well-known reformulation of the stick-breaking construction of a DP, also referred to as the Blackwell-MacQueen urn scheme [35]. Specifically, within the stick-breaking construction, each $z_l$ is generated from a multinomial distribution with a probability mass function $\\pi(v)$ that depends on $v \\in \\mathbb{R}^K$, i.e.,\n\\begin{equation}\nz_l \\sim Mult(\\pi(v)) = const \\times \\prod_{k=1}^K (\\pi_k)^{\\mathbb{I}[z_l=k]} = V \\prod_{k=1}^K (1-v_k)^{\\mathbb{I}[z_l\\neq k]},\n\\tag{14a}\n\\end{equation}\nwhere, the $k$-th element of the probability $\\pi(v)$ is given by\n\\begin{equation}\n[\\pi(v)]_k \\triangleq \\pi_k = v_k \\prod_{j=1}^{k-1} (1 - v_j), \\qquad v_k \\sim Beta(1, \\alpha).\n\\tag{14b}\n\\end{equation}\nIntuitively, each $\\pi_k$ can be treated as sampling from a stick-breaking process. This process involves conceptualizing a stick of length 1, where at each step $k$, a fraction $v_k$, sampled from a Beta distribution, is broken off. The length of the broken-off fraction is treated as the value of $\\pi_k$, i.e., $\\pi_k = v_k \\prod_{j=1}^{k-1} (1 - v_j)$. The remaining portion is retained for subsequent steps. Note that the distribution over $\\pi_k$ is sometimes denoted as $\\pi_k \\sim GEM(\\alpha)$, with \u201cGEM\" stands for Griffiths, Engen, and McCloskey [35]. Utilizing such stick-breaking construction within RFLVMs (see Eq. (11) & (7c)), the graphical model for RFLVMs is depicted in Fig. 2, and the joint distribution for the RFLVM, denoted as $p(Y, \\Theta)$, is expressed as:\n\\begin{equation}\np(Y, \\Theta) = \\prod_{m=1}^M \\prod_{n=1}^N p(y_{n,m} | x_n, W) p(x_n)\n\\prod_{l=1}^{L/2} p(w_l | z_l) p(z_l | v) p(v | \\alpha) p(\\alpha),\n\\tag{15}\n\\end{equation}\nwhere $\\{\\Theta \\triangleq W, X, z, v, \\alpha\\}$. The variational variables within each have explicit PDFs, thereby resulting in a computable ELBO.\""}, {"title": "3.2 Block Coordinate Descent Variational Inference", "content": "As discussed in Section 2.2, each variational variable in the VBI framework is typically solved jointly using the same optimization algorithm (e.g., MFVI or RGVI). This approach overlooks the potential benefits of exploiting the specific optimization structures of different variational variables as illustrated in Example 3. A potential solution is alternating optimization, which enables the use of the most appropriate solver for each variational variable, thus not only improving computational efficiency for conjugate terms but also ensuring tractable inference for non-conjugate terms.\nInspired by this, we incorporate the concept of block coordinate descent (BCD) into VBI using a divide and conquer strategy. Specifically, we first partition the variational variables $\\Theta$ into $B$ blocks, denoted as $\\{\\Theta_b\\}_{b=1}^B$, with each block containing $J_b \\neq 0$ variational variables. Such block partitioning then divides the optimization problem defined in Eq. (13) into B subproblems, that is\n\\begin{equation}\n\\max_{\\vartheta_b} \\mathcal{L} (q(\\Theta_b; \\vartheta_b)), \\qquad b = 1, ..., B,\n\\tag{16}\n\\end{equation}\nwhere $\\vartheta_b$ denotes the variational parameters associated with the block $b$. The optimization objective, termed the $b$-th local ELBO, includes the terms within the ELBO given by Eq. (13) that are related to block $b$. To solve these subproblems, we propose using the MFVI-based method for those with conjugate priors, and RGVI for those with non-conjugate priors, as illustrated in the remaining text of this section."}, {"title": "4 SCALABLE RFLVM (SRFLVM)", "content": "In this section, we detail our proposed computationally efficient BCD-VI algorithm for RFLVMs, which we term SRFLVM (scalable RFLVM). Following the philosophy mentioned in Section 3.2, we first preselect the following block-wise variational distribution, $q(\\Theta)$,\n\\begin{equation}\nq(\\Theta) = q(X)p(W|z)q(z)q(v)q(\\alpha),\n\\tag{20}\n\\end{equation}\nwhere $q(X) = \\prod_{n=1}^N \\mathcal{N} (x_n|\\mu_n, S_n)$ with $\\mu_n \\in \\mathbb{R}^Q$, $S_n \\in \\mathbb{R}^{Q \\times Q}$, and $q(v) = \\prod_{k=1}^K Beta(\\alpha_{v_k}, b_{v_k})$ with $\\alpha_{v_k}, b_{v_k} \\in \\mathbb{R}$. The distribution $q(\\alpha)$ is a Gamma distribution with shape and rate parameters $a_{\\alpha}$ and $b_{\\alpha}$, respectively. Furthermore, we define the probability of the $l$-th spectral point $w_l$ being sampled from the $k$-th cluster as $\\varphi_{l,k}$, i.e., $q(z_l = k) = \\varphi_{l,k}$. In addition, we set $q(W|z)$ to its prior $p(W|z)$ to capture the dependence between $W$ and $z$, and achieve a more tractable ELBO (see Eq. (22)).\nBy combining the variational distributions $q(\\cdot)$ (Eq. (20)) with the model joint distribution (Eq. (15)), we immediately obtain the following concrete optimization problem for SRFLVMs as described in Eq. (13):\n\\begin{equation}\n\\max_{\\theta,\\vartheta} E_{q(X)p(W/z)q(z)} \\Big\\{\\log \\frac{p(Y | X, W)p(X)p(W | z)}{q(X)q(W | z)} \\Big\\}\n+ E_{q(z)q(v)q(\\alpha)} \\Big\\{\\log \\frac{p(z|v)p(v|\\alpha)p(\\alpha)}{q(z)q(v)q(\\alpha)} \\Big\\},\n\\tag{21}\n\\end{equation}\nwhere the model hyperparameters, $\\theta$, encompass $\\theta_{dpm}$ from the kernel function learning and likelihood parameters, and the variational parameters $\\vartheta$ encompass $\\{\\{\\mu_n, S_n\\}_{n=1}^N, \\{\\alpha_{v_k}, b_{v_k}\\}_{k=1}^K, a_{\\alpha}, b_{\\alpha}, \\{\\varphi_{l,k}\\}_{l=1}^{L/2,K}\\}$. Given the fact that we set $q(W | z) = p(W | z)$ for tractability and dependency, the first term (namely likelihood term) in this optimization problem can be expressed as\n\\begin{equation}\nE_{q(X)p(W/z)q(z)} \\Big\\{\\log \\frac{p(Y |X, W)p(X)}{q(X)}\\Big\\}.\n\\tag{22}\n\\end{equation}\nWe now turn to the implementation details of BCD-VI for solving the aforementioned optimization problem. To this ends, careful analysis of the optimization objective structure is essential for effective variable block optimization. Specifically, for evaluating the likelihood term, it becomes manageable once $W$, $X$ and $z$ are divided into distinct blocks. As for $\\alpha$ and $v$, which can have conjugate priors, they are each treated as separate blocks. In summary, we partition the variational variables into four blocks:\n\\begin{equation}\n\\underbrace{\\{W,X\\}}_{\\text{likelihood block}}, \\underbrace{\\{z\\}}_{\\text{DP kernel blocks}}, \\underbrace{\\{v\\}}, \\underbrace{\\{\\alpha\\}},\n\\tag{23}\n\\end{equation}\nwhere the set $\\{W, X\\}$ is named the \u201clikelihood block\" simply because the data likelihood is dependent on $W$ and $X$. For the logistic likelihood, the main difference is that the likelihood block includes additional stochastic weights $H$. Consequently, next, we will discuss the inference processes of Gaussian and logistic likelihood blocks in Section 4.1 and 4.2, respectively. Finally, Section 4.3 will introduce the inference process of the shared DP kernel blocks."}, {"title": "4.1 Gaussian Likelihood Blocks", "content": "In the case of Gaussian likelihood, the related subproblem is,\n\\begin{equation}\n\\max_{\\theta,\\vartheta} E_{q(X)p(W/z)q(z)} \\Big\\{\\log \\frac{p(Y|X, W)p(X)}{q(X)} \\Big\\},\n\\tag{24}\n\\end{equation}\nwhere $\\vartheta = \\{\\mu_n, S_n\\}_{n=1}^N$. Through some algebraic calculations, the subproblem (see Eq. (24)) can be reformulated as follows:\n\\begin{equation}\n\\underbrace{E_{q(W)q(X)} [\\logp(Y|W, X)]}_{\\text{term (a)}} - \\underbrace{KL(q(X)||p(X))}_{\\text{term (b)}},\n\\tag{25}\n\\end{equation}\nwhere\n\\begin{equation}\nq(W) = \\int p(W|z)q(z)dz= \\prod_{l=1}^{L/2} \\Big(\\sum_{k=1}^K \\varphi_{lk} \\mathcal{N}(w_l|\\mu_k, \\Sigma_k)\\Big).\n\\tag{26}\n\\end{equation}\nThe interpretability of Eq. (25) is as follows: Term (a) represents the data reconstruction error, encouraging accurate reconstruction of the observed data using any $X$ and $W$ samples drawn from their variational distributions. Term (b) serves as a regularizer, penalizing substantial deviations of $q(X)$ from the prior distribution $p(X)$.\nDue to the highly nonlinear nature of the GP prior, the prior for $X$ and $W$ and their likelihoods are non-conjugate. Thus, we adopt the RGVI-based approach to solve the subproblem defined in Eq. (25). We numerically evaluate the objective function Eq. (25), which is\n\\begin{equation}\n\\frac{1}{M} \\sum_{m=1}^M \\frac{1}{\\mathcal{I}} \\sum_{i=1}^{\\mathcal{I}} \\Big[\\log\\mathcal{N} \\big(y_{:,m} |0, K^{(i)}+\\sigma^2 I_N\\big)\\Big] - KL(q(X)||p(X)),\n\\tag{27}\n\\end{equation}\nwhere $\\mathcal{I}$ denotes the number of MC samples drawn from $q(X)$ and $p(W)$. The second term, a KL divergence between two Gaussian distributions, can be computed analytically. Further computational details are provided in Appendix C.1.1. Thanks to the reparameterization trick, we can apply gradient-based optimization methods, e.g., Adam [33], to update the model hyperparameters $\\theta$ and the variational parameters $\\vartheta$."}, {"title": "4.2 Logistic Likelihood Block", "content": "The distinction in logistic likelihood from the Gaussian case lies in the introduction of stochastic weights $H$ (see Eq. (6)), resulting in a likelihood term formed as $p(Y|X, W, H)$ and a manageable KL divergence term involving $H$. Assuming $q(h_m)$ for all $m \\in \\{1, ..., M\\}$ as independent multivariate Gaussian variables and incorporating $H$ into the likelihood block, which now includes $\\{W, X, H\\}$, the corresponding subproblem can be solved using the RGVI-based method.\nUnfortunately, the empirical results reveal that using approximate inference on the stochastic weights $H$ significantly degrades the quality of learned kernel matrices, regardless of likelihood types (see Appendix. C.2). To address this issue, we integrate P\u00f3lya-gamma (PG) augmentation [41] into SRFLVMs. By augmenting a set of PG distributed variables into the model, we can transform the logistic likelihood into a quadratic form with respect to $H. This transformation ultimately yields a closed-form solution for the posterior of $H$, thereby improving both tractability and performance."}, {"title": "4.2.1 P\u00f3lya-gamma Distribution", "content": "If $w\\in \\mathbb{R}$ follows a PG distribution (PGD) with parameters $b > 0$ and $c \\in \\mathbb{R}$, denoted as $w \\sim PG(b, c)$, then it is equivalent in distribution to an infinite weighted sum of Gamma:\n\\begin{equation}\n\\omega \\stackrel{d}{=} \\frac{1}{2} \\sum_{k=1}^{\\infty} \\frac{g_k}{\\pi^2 (k - 1/2)^2 + c^2/(4\\pi^2)},\n\\tag{28}\n\\end{equation}\nwhere $\\stackrel{d}{=}$ denotes equality in the cumulative distribution function (CDF), and $g_k \\sim Ga(b,1)$, $k \\in 1,...,\\infty$ are independent Gamma random variables. Building on the equality established in Eq. (28), several useful properties are proposed in [41]. First,\n\\begin{equation}\n\\frac{(e^{\\psi})^a}{(1+e^{\\psi})^b} = 2^{-b}e^{\\kappa \\psi}  \\int_0^{\\infty} e^{-\\omega \\psi^2/2} p(\\omega)d\\omega,\n\\tag{29}\n\\end{equation}\nwhere $\\kappa = a - b/2$ and $p(\\omega) = PG(\\omega | b, 0)$. And second,\n\\begin{equation}\np(\\omega | \\psi) \\sim PG(b, \\psi).\n\\tag{30}\n\\end{equation}\nGiven $w$, the left-hand side of Eq. (29), which resembles the logistic likelihood expression (Eq. (9)), can be represented in a quadratic (or Gaussian) form with respect to $\\psi$. The corresponding posterior distribution of $\\omega$ is provided by Eq. (30)."}]}