{"title": "Multimodal Laryngoscopic Video Analysis for Assisted Diagnosis of Vocal Cord Paralysis", "authors": ["Yucong Zhang", "Xin Zou", "Jinshan Yang", "Wenjun Chen", "Faya Liang", "Ming Li"], "abstract": "This paper presents the Multimodal Analyzing System for Laryngoscope (MASL), a system that combines audio and video data to automatically extract key segments and metrics from laryngeal videostroboscopic videos for clinical assessment. MASL integrates glottis detection with keyword spotting to analyze patient vocalizations and refine video highlights for better inspection of vocal cord movements. The system includes a strobing video extraction module that identifies frames by analyzing hue, saturation, and value fluctuations. MASL also provides effective metrics for vocal cord paralysis detection, employing a two-stage glottis segmentation process using U-Net followed by diffusion-based refinement to reduce false positives. Instead of glottal area waveforms, MASL estimates anterior glottic angle waveforms (AGAW) from glottis masks, evaluating both left and right vocal cords to detect unilateral vocal cord paralysis (UVFP). By comparing AGAW variances, MASL distinguishes between left and right paralysis. Ablation studies and experiments on public and real-world datasets validate MASL's segmentation module and demonstrate its ability to provide reliable metrics for UVFP diagnosis.", "sections": [{"title": "I. INTRODUCTION", "content": "vocal cord Paralysis (VP) is a condition where one of the vocal cords fails to move properly, leading to voice changes, difficulty swallowing, and potential breathing problems [1], [2]. VP can result from nerve damage due to surgery, injury, infection, or tumors, significantly impacting a patient's quality of life [3]. Accurate diagnosis of VP is crucial, as it informs the appropriate medical or surgical intervention, which can restore vocal function, improve airway protection, and enhance overall patient outcomes. Clinicians often use laryngeal videostroboscopy to check the vocal cord vibration in details. Laryngeal videostroboscopy is a specialized diagnostic tool used to evaluate the function of the vocal cords and the larynx. The stroboscopic component is key because it allows for the visualization of vocal cord vibrations, which are often too rapid to be observed directly [4]. Stroboscopy uses a light source that flashes at a slightly different frequency than the vocal cord vibration. This creates a slow-motion effect, enabling the clinician to see the individual phases of vocal cord vibration in great detail.\nWith the advent of the artificial intelligence, deep learning methods are developed to extract useful parameters [5] to assist clinicians, track the motion of vocal cords [6]\u2013[10], and even make predictions [11], [12] or classifications [13] to help clinicians to conduct diagnosis. However, the diagnosis of VP requires to inspect the complete phonation cycles of patients. Previous works default to using video frames or images that contain the vocal cords, but in most of the time, the raw recordings from the endoscopic inspection carry useless information other than patient's phonation cycles. For instance, no phonation cycles in the beginning of the inspection, as the laryngoscope is not in its position, still finding the vocal cords.\nAlthough experienced experts can provide valuable insights by analyzing the video captured by the endoscope, this methodology heavily relies on personal diagnosis and lacks objectiveness, which on one hand reduces patient's confidence, on the other hand increases the risk of misdiagnosis. Thus, analysis tools [14]\u2013[17] are developed to assist experts in making better diagnoses with the endoscopic video. [14] relies on physical features like color and shape to segment glottis area. [15], [16] uses supervised method to keep track of the vocal cords' movement to extract the anterior glottic angle (AGAW). However, those three tools [14]\u2013[16] focus solely on one single modality to assist the clinicians and cannot be used for left or right VP detection. Researchers in [17] extract more than one features from the laryngoscope, but those features all come from the AGA between two vocal cords, which cannot be used to detect LVP or RVP. Moreover, current tools are restricted to pre-processed laryngoscopic video or short video segments without stroboscopic examinations, which cannot be directly used on raw and long laryngoscopic videos. As a result, labor works are still needed beforehand, which makes it inconvenient for the experts to use.\nIn addition to video processing, audio-related methods have shown great potential to detect voice pathology [18], [19] as well. Such method typically involves converting audio clips to spectrograms using short-time Fourier transform (STFT), which are then fed into various models for downstream tasks. For instance, deep neural networks (DNN) is utilized to predict"}, {"title": "II. RELATED WORK", "content": ""}, {"title": "A. U-Net Based Glottis Segmentation", "content": "U-Net's encoder-decoder structure with skip connections has proved to be a robust framework for biomedical image segmentation [27], [28]. Modifications to this architecture, such as the introduction of separable convolutions in S3AR U-Net [29] and the incorporation of attention mechanisms in Attention U-Net [30], have demonstrated improved performance by capturing more salient features and reducing computational demands. The pursuit of efficiency and clinical applicability has led to the development of models like Efficient U-Net [31], which offers a practical inference time while maintaining high segmentation quality. Yet, the trade-off between segmentation accuracy and computational cost is a recurring theme, with models like VGG19 U-Net [31] showing longer inference times. Despite the computational efficiency of U-Net variants, the need for substantial training data and the potential for overfitting remain concerns [32]. To counter this, weakly supervised learning approaches, like the one proposed by [33], have gained traction, requiring only point annotations and demonstrating a remarkable balance between segmentation accuracy and convergence speed. However, these methods may still struggle with complex anatomical structures and the need for precise boundary localization.\nWhile the field has made significant strides, challenges persist in achieving a harmonious balance between segmentation accuracy, and generalizability across varied clinical datasets. The limited size of the glottis segmentation mask often leads to a high incidence of false positives when using U-Net, resulting in segmentation outputs even in the absence of glottal regions. This can significantly impair the accuracy of subsequent analyses. To address this issue, we incorporate a diffusion-based refinement stage, which enhances the precision of the initial U-Net segmentation."}, {"title": "B. AGAW Extraction and Glottal Midline Estimation", "content": "In the early years, researchers find out that by analyzing the maximum separation between vocal cords, one can effectively detect UVP [34]. Later on, researchers begin to use AGA to represent the separation between vocal cords, and AGAW analysis begin to play a critical role for VP detection. However, the existing methods typically assess AGAWs without differentiating between the left or right vocal cords [12], [15], [16], [35]. To tackle this, our proposed approach introduces a"}, {"title": "III. MULTIMODAL KEY FRAMES EXTRACTION", "content": ""}, {"title": "A. System Design", "content": "Our system aims to facilitate efficient clinical examinations by extracting key segments from laryngoscopic videos and providing objective indicators for specific laryngeal diseases. As Fig. 1 shows, comprising two main modules \u2013 the voice module, the video module \u2013 our system ensures the accurate observation of vocalization cycles and the clear visualization of the glottal area.\nThe voice module initially processes the audio extracted from the video using short-time Fourier transform to obtain spectrograms. Through keyword spotting (KWS) technique, each frame is analyzed to detect patient vocalizations. This enables the preliminary segmentation of vocalization segments within the video.\nSubsequently, the video module refines these vocalization segments to obtain key frames to form laryngoscopic highlights, ensuring the visibility of the vocal cords. Specifically, by utilizing the glottis detection model, MASL can identify regions containing the vocal cords and glottis in each frame. Moreover, given the importance of the stroboscopic portions in laryngoscopic videos for subjective analysis by physicians, we also include a stroboscopic video extraction method into our system."}, {"title": "B. Audio Processing Module", "content": "To get a better examination of the status of the vocal cords, clinicians often request the patients to pronounce \"ee\". This pronunciation is easy to be made and can largely enable the vibration of the vocal cords. Hence, by capturing the segments that pronounce \"ee\", phonation cycles might be captured. To accomplish this, we developed a KWS model that is used to detect keywords in a sentence, such as \"Hey Siri\", \"Ok Google\", and in our case, the word \"ee\".\nThe overall pipeline is illustrated in Fig. 2. Initially, the input audio is transformed into a spectrogram using the Short-Time Fourier Transform (STFT), converting the time-domain signal into a time-frequency representation for more effective analysis. The spectrogram is then segmented into chunks along the time axis, with each chunk containing a fixed number of frames. During the training phase, these chunks are randomly selected from each audio clip to ensure a diverse set of training samples. This method enhances the model's generalization capabilities by incorporating different spectrogram chunks from various audio clips into a robust batch of training data.\nThe spectrogram chunks are subsequently fed into the KWS model, the architecture of which is detailed in TABLE I. The model comprises multiple convolutional blocks and residual blocks [38], designed to efficiently extract relevant features from the spectrogram chunks. Within the model, we use max pooling to progressively reduce the spatial dimensions. In this way, the features are compressed and become more meaningful. In the end, we use an adaptive average pooling layer to aggregate the features before they pass through two fully connected layers, with the final layer producing the classification score output.\nDuring inference, the input spectrogram is sliced into chunks using a sliding window. Each chunk is processed by the trained KWS model to generate decision results, as depicted in Fig. 2. This carefully constructed pipeline ensures that the KWS model reliably detects the \"ee\u201d vocalization, providing critical prior knowledge for subsequent analysis."}, {"title": "C. Video Processing Module", "content": "Although the KWS model is employed to detect the vocalization video frames of the phonation of \"ee\", these detected frames may not always successfully capture the vocal cords and glottis. To address this issue, we train a vocal cord detection model, utilizing the famous object detection model structure YOLO-v5 [39], to further refine the time masks.\nGiven the limitations of the open-source data for training a detection model, we utilize the public glottis segmentation dataset, BAGLS [40], to construct our training dataset. To generate labels for vocal cord detection, we designed an automatic bounding box generator. As illustrated in Fig. 3, the process begins by obtaining the coordinates of the top, bottom, left, and right vertices from the glottis mask. These coordinates are denoted as U(x1, y1), D(x2, y2), L(x3, y3), and R(x4, y4), respectively. Next, these vertices are manually expanded by a fixed number of pixels to ensure adequate coverage around the vocal cords. Using the expanded points, a bounding box is computed, as shown on the right side of Fig. 3.\nAs depicted in Fig. 1, our pipeline integrates a strobing video extraction module alongside the detection modules. This module is utilized to isolate the strobing segments of laryngeal videos by analyzing hue, saturation, and value (HSV) of video frames. The HSV parameters represent type, intensity, and"}, {"title": "IV. GLOTTIS SEGMENTATION", "content": "In the evaluation of laryngeal function and pathology, accurate segmentation of the glottis is essential. This segmentation enables the extraction of objective metrics that clinicians can utilize for diagnosis and review. In our system, we implement a two-stage approach comprising a U-Net-based method followed by a diffusion-based refinement. The initial segmentation is performed using a naive U-Net model, which is simple yet effective in medical image segmentation tasks. This model provides a robust initial estimate of the glottis boundaries.\nHowever, the U-Net model tends to produce false positives, especially in cases where the glottal area is not visible (see Table V). These incorrect segmentations can complicate subsequent analysis stages. To address this issue and enhance the precision of the segmentation results, we further refine the U-Net outputs using a diffusion model. This additional step helps to correct any inaccuracies and produce a more accurate and reliable glottis mask. The combination of these two methods ensures high-quality segmentation, which is crucial for subsequent analysis and metric computation.\nThe following subsections provide detailed descriptions of each component in our glottis segmentation pipeline, including the U-Net-based method and the diffusion-based refinement."}, {"title": "A. U-Net-based Method", "content": "The U-Net model is a convolutional neural network specifically designed for biomedical image segmentation. Owing to its proven effectiveness and accuracy, we employ a U-Net as our segmentation model. As detailed in Table III, the model comprises a series of convolutional blocks, referred to as ConvBlocks (see Table II), which include convolution operations followed by batch normalization and ReLU activation functions. The U-Net architecture is symmetrical, featuring a contracting path that captures contextual information and an expansive path that facilitates precise localization through upsampling operations. The contracting path consists of ConvBlocks with progressively increasing numbers of channels (64, 128, 256, and 512), interspersed with MaxPool2D layers for downsampling. This is followed by a sequence of upsampling operations and ConvBlocks that reduce the feature map dimensions, effectively reconstructing the image to its original resolution. The final layer utilizes a ConvBlock with a single output channel to generate the segmentation mask.\nThere are several advantages to using a U-Net model with this structure. First, the architecture captures both low-level and high-level features through its deep structure and skip connections, which concatenate features from the contracting path to the corresponding layers in the expansive path. This facilitates precise segmentation by preserving spatial information. Second, the use of ConvBlocks with batch normalization"}, {"title": "B. Diffusion-based Refinement", "content": "The U-Net model excels at extracting image masks. However, the generated masks may tend to yield false alarms (see Table V). To further enhance the quality, we explore the integration of the diffusion-based model. By incorporating the diffusion model, we can inject more diversity into the masks extracted by U-Net, making the generated images richer and more diverse.\nDiffusion models consist of two stages: forward diffusion and reverse diffusion. During the forward process, Gaussian noise is gradually added to the segmentation label x0 over a series of steps T. Conversely, in the reverse process, a neural network is trained to recover the original data by reversing the noise addition, as represented by the following equation:\n$P_{\\theta}(x_{0:T-1}|x_{T}) = \\prod_{t=1}^{T}P_{\\theta}(x_{t-1}|x_{t}),$ (1)\nwhere \u03b8 stands for the parameters for the reverse process. Consistent with the conventional implementation of the Diffusion Probability Model (DPM) [41], a U-Net model is employed for training. Following the idea of MedSegDiff [42], we incorporate the original glottis images as priors for the step estimation function and employ dynamic conditional encoding to fuse the encoding outcomes from both the raw image and the segmentation mask at each step. Hence, for each step, the estimation function \u03f5 is written as:\n$\\epsilon_{\\theta}(x_{t}, I, t) = D((E_{I} + E_{x,t}), t),$ (2)\nwhere \u03b8 stands for the learning parameters, D is the decoder, I is the raw image prior, t is the current diffusion step. $E_{I}$ and $E_{x,t}$ are the embeddings encoded from the raw image and segmentation mask at step t respectively.\nDifferent from the traditional training procedure, we do not start the diffusion process from a standard Gaussian noise,"}, {"title": "V. MULTIMODAL VOCAL CORD PARALYSIS ANALYSIS", "content": "To detect VP, we adopt a two-stage process. First, we build a binary classification model to check if it belongs to VP. Then, with our proposed metrics, LVP and RVP can be identified by comparing the left and right vocal cord movement via the AGA."}, {"title": "A. Anterior Glottic Angle Extraction with Quadratic Fitting", "content": "To diagnose the laryngeal paralysis in a more detailed way, instead of the whole glottal angle, we extract the glottal angles for left and right vocal cords separately. As a result, we can provide the contrastive diagnosis of laryngeal paralysis. The procedure for extracting glottal angles from a segmented glottis involves a systematic series of steps, detailed in Algorithm 1, with visual representations of the intermediate results provided in Fig. 6.\nInitially, the algorithm acquires the coordinates of the top, bottom, left, and right vertices from the glottis mask, computing their center point denoted as U(x1,y1), D(x2, y2), L(x3,y3), R(x4,94), and C(xc, yc) respectively (see Fig. 6(a)). Subsequently, a line connecting points C and D is established, and the function f(x) passing through C and D is computed.\nEquidistant points C1, C2, ..., CN-1 are then positioned along the line segment intercepted by the glottal mask. For each Ck, orthogonal functions f'(x) to f(x) are calculated, with their intersection points Lk and Rk with the glottis mask determined for k \u2208 [1, N \u2013 1] (see Fig. 6(b)).\nTo ensure uniformity, the coordinate system is rotated by an angle \u03b3, aligning all intersection points along the y-axis after rotation, denoted as Ly,k and Ry,k. Leveraging these points, a quadratic curve qy(x) is approximated in the rotated coordinate system. The lowest point Dq of qy(x) is identified and mapped back to the original coordinate system (see Fig. 6(c)).\nSubsequently, a calibrated middle line f*(x) connecting points C and Dq is established, intersecting the glottis mask"}, {"title": "B. Multimodal vocal cord Paralysis Detection", "content": "We developed a model that integrates both audio and video modalities for enhanced diagnosis. As illustrated in Fig. 7, the model utilizes the audio spectrogram and the AGA movements as inputs. The audio spectrogram is encoded using EfficientNet-b0 [43], a compact and efficient model renowned for its performance in image classification tasks. Given that multiple video highlights are extracted from a single laryngoscope video, with corresponding AGA movements for each, multiple AGA movement time series is generated. These time series are treated as multi-channel inputs to a ConvLSTM [44] model, comprising a convolutional layer followed by an LSTM layer. The ConvLSTM model is adept at handling multi-channel time series; the convolutional layer captures features across all channels, while the LSTM layer processes temporal information effectively.\nTo further differentiate between LVP and RVP, we analyze the variance of the AGA movements for the left and right vocal cords. Intuitively, the paralyzed side exhibits less activity during phonation, leading to a smoother AGA movement time sequence and consequently a lower variance. Therefore, by simply comparing the variance of the left and right AGA movements, we distinguish between LVP and RVP."}, {"title": "VI. EXPERIMENTS AND RESULTS", "content": ""}, {"title": "A. BAGLS Dataset", "content": "We use the Benchmark for Automatic Glottis Segmentation (BAGLS) [40] as the dataset for glottis segmentation. It consists of 59,250 endoscopic glottis images acquired from hundreds of individuals at seven hospitals, which is split into 55,750 training images and 3,500 test images."}, {"title": "B. SYSU Dataset", "content": "This is the dataset collected from real-world scenario by Sun Yat-sen Memorial Hospital of Sun Yat-sen University (SYSU). The dataset contains 520 video samples, including 106 normal samples and 414 paralysis samples (257 LVP and 157 RVP). All the video clips are recorded using a laryngeal videostroboscope, including one or two strobing video segments for each clip."}, {"title": "C. Keyword Spotting Model", "content": "The detailed architecture of the KWS model is presented in Table I. For audio input, we utilize a Mel-spectrogram with 80 Mel filters, with the number of FFT points and hop length set to 1024 and 512, respectively. A sliding window of 400 samples and a hop length of 64 samples are employed to extract frame-level information. The model is trained for 100 epochs, optimized using the Adam optimizer with a learning rate of 0.005.\nSince the KWS model outputs a posterior score rather than a direct decision value, a threshold is necessary to determine the final result. Table IV lists the thresholds applied in the system. The precision-recall (PR) curves and receiver operating characteristic (ROC) curves in Fig. 8 illustrate the classification performance across different thresholds. Consequently, the threshold is selected based on the highest F1-score, a balanced metric that considers both precision and recall, providing a comprehensive measure of classification performance.\nTable IV displays the results for both doctor and patient. The detection performance for the doctor's voice is inferior to that for the patient's voice due to the doctor's broader range of spoken words compared to the patient's singular utterance of \"ee\". Our system prioritizes frames where the patient is speaking \"ee\u201d, resulting in a high F1-score on the test dataset, which underscores the efficacy of our KWS model."}, {"title": "D. Glottis Segmentation", "content": "We train the U-Net baseline with the same strategy as [40]. For the diffusion model, it is built on the model of [42], except that we slightly modify the noise to meet the prior knowledge of U-Net results. The diffusion step is set to 1,000. We train the diffusion model with 100,000 steps. The model is optimized by a AdamW optimizer with a learning rate of 0.0001.\nWe evaluate our segmentation model on the public glottis dataset BAGLS. We use intersection over union (IoU) as the metric, and the results are shown in Table V. From the table, we can see that the diffusion model can have a slightly better IoU performance than the traditional U-Net, By refining the U-Net results with the diffusion ones, the IoU performance can be further improved.\nBesides IoU, we also compute the false alarm rate (FAR) when using different segmentation methods. The false alarm (FA) is defined as the situation where the glottis mask is generated when no glottis is seen. The FAR is calculated as the number of FA divided by the total number of images with no glottis to be seen. We care about FA as we need the segmentation masks for later analysis. FAs might produce wrong measurements and mislead the prediction results of VP.\nFrom Table V, the traditional U-Net model shows a tendency to produce FA masks with a leading FAR of 15.8%. By contrast, diffusion-based segmentation model has a much lower FAR of 4.5%, which is more than two times decrease. The result may indicates that, in terms of the glottis segmentation, the diffusion model performs more robust when dealing with image with no targets. Hence, after we refine the U-Net result with the diffusion model, we can further achieve a result with a lower FAR but a better IoU."}, {"title": "E. vocal cord Paralysis Detection", "content": "We manually divided the SYSMH dataset into training and testing subsets. The training set comprises 470 samples (81 normal, 389 paralysis), while the testing set includes 50 samples (25 normal, 25 paralysis). The split was performed randomly, ensuring an equal number of normal and paralysis samples in the test set to facilitate a balanced evaluation of our model\u2019s performance. We conduct the same experiment for 5 times with 5 different random seeds, so that the train-test-split operation is different for each run, resulting in performance variability. This approach allows for a comprehensive assessment of the model\u2019s overall performance, independent of the specific data subsets used in each iteration.\nAblation studies are conducted using different system configurations to show the effectiveness of our proposed system, as shown in Table VI. The naive system uses the images segmented by the U-Net baseline without quadratic fitting or diffusion refinement. Table VI summarizes the performance for vocal cord paralysis classification across various system configurations. Each configuration's precision, recall, and F1-score are reported for both normal and paralysis classes, as well as their weighted averages. In the naive settings, the system achieves a precision of 82.0% and recall of 69.0% for normal cases, and a precision of 73.2% and recall of 84.0% for paralysis, with a weighted average F1-score of 76.5%.\nSteady and accurate middle line prediction is critical for analyzing left and right vocal cord movement, since every time when measuring the AGA movement, distance between the sample points on the vocal cord and middle line is computed. However, the laryngeal camera is not always in fixed positions in real-world application. As a result, using the traditional way to estimate the middle line may introduce high variations in middle line positions, producing less robust results. It turns out that calibrating the middle line by fitting a quadratic function might be a good way to enhance the middle line prediction, resulting in a better classification performance. The results from Table VI shows that the proposed quadratic fitting method is able to improve the performance by a large margin, improving the balanced precision and recall to approximately 81.5% for both classes.\nAfter further incorporating the diffusion refinement to the glottis segmentation process, the classification results can be improved further, slightly increasing the weighted average F1-score to 82.5%. The diffusion refinement only involves the images where glottis cannot be detected by the YOLO-v5 detector (described in Section III-C), since we want to use this method to reduce FAs (described in Section VI-D).\nWe also conduct experiments on the effect of multimodality. As a result, by removing the AGA movement metrics, the classification performance drops dramatically, with the weighted average F1-score falling to 72.5%. This result demonstrate the critical role of integrating both audio and AGA movement modalities for robust performance.\nWe argue that the high standard deviation over all runs due to the fact that we randomly split the datasets for train and test for each run in the experiments. The SYSMH dataset is a challenging dataset with various kinds of recording situations, such as various types of laryngeal videostroboscopes, recording environment, etc., causing domain shifts.\nIn summary, the comparison shown in Table VI underscores the efficacy of quadratic fitting and diffusion refinement in enhancing classification accuracy and also show the critical role of integrating both audio and AGA movement modalities for better performance."}, {"title": "F. Potential of Unilateral Vocal Cord Paralysis Detection", "content": "Fig. 9 illustrates the glottal area and the AGA movement time series of a patient with RVP. These metrics, extracted by MASL, are derived from a single video clip in the SYSU dataset. As three video highlights are extracted from the clip, three distinct time series for each of the two metrics are presented. Fig. 9(a), (c), and (e) depict the variations of glottal area across time, indicating the phonation cycle through the opening and closing movements of the vocal cords. However, while the glottal area data reveals the vocal cord activity, it does not specify the movement of each vocal cord individually. By computing the AGA movement time series for both vocal cords, as shown in Fig. 9(b), (d), and (f), MASL provides clinicians with more detailed and actionable metrics for inspecting UVP.\nFrom Table VII, by comparing the variance of left and right AGA metrics, we are able to distinguish the left or right paralysis for a UVP patient. We also conduct ablation studies to show effectiveness of different modules in our proposed system. Table VII illustrates that incorporating quadratic fitting improves the classification results. For both left and right vocal cord paralysis and the weighted average, all the metrics have improved by a large margin. Moreover, adding diffusion refinement can further improve all the metrics, with a weighted"}, {"title": "VII. CONCLUSION", "content": "In this paper, we introduced the Multimodal Analyzing System for Laryngoscope (MASL), which integrates both the audio and video processing techniques to enhance the diagnosis of laryngeal disorders. MASL is able to produce high quality laryngoscopic video highlights for better diagnosis by clinicians. It incorporates KWS that ensures the complete phonation cycle of the patients, and video processing modules that ensure the occurrence of the vocal cords. Moreover, MASL is able to extract the stroboscopic slices from the raw laryngoscopic video, saving clinicians' time finding the stroboscopic slices.\nTo produce objective metrics for analysis, MASL employs a two-stage segmentation method with a U-Net followed by the diffusion-based refinement to accurately delineate the glottal area, significantly reducing false positives produced by U-Net. With the segmented masks, MASL adopts quadratic fitting and extracts AGA movements for both left and right vocal cords, producing a new modality for VP. With multi-modality, MASL improves the VP performance. Simply by comparing the variance between the AGA movements for left and right vocal cords, MASL yields 92% of Fl-score on the real-world dataset. The Experimental results demonstrate MASL's robustness and effectiveness, offering clinicians a powerful tool to improve diagnostic accuracy and efficiency."}]}