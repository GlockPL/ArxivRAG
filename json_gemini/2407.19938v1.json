{"title": "Robust Conformal Volume Estimation in 3D Medical Images", "authors": ["Benjamin Lambert", "Florence Forbes", "Senan Doyle", "Michel Dojat"], "abstract": "Volumetry is one of the principal downstream applications of 3D medical image segmentation, for example, to detect abnormal tissue growth or for surgery planning. Conformal Prediction is a promising framework for uncertainty quantification, providing calibrated predictive intervals associated with automatic volume measurements. However, this methodology is based on the hypothesis that calibration and test samples are exchangeable, an assumption that is in practice often violated in medical image applications. A weighted formulation of Conformal Prediction can be framed to mitigate this issue, but its empirical investigation in the medical domain is still lacking. A potential reason is that it relies on the estimation of the density ratio between the calibration and test distributions, which is likely to be intractable in scenarios involving high-dimensional data. To circumvent this, we propose an efficient approach for density ratio estimation relying on the compressed latent representations generated by the segmentation model. Our experiments demonstrate the efficiency of our approach to reduce the coverage error in the presence of covariate shifts, in both synthetic and real-world settings. Our implementation is available at https://github.com/benolmbrt/wcp_miccai.", "sections": [{"title": "1 Introduction", "content": "An important downstream application of medical image segmentation is the extraction of volume measurements for lesions or organs. Lesion volumetry plays a pivotal role in various medical scenarios, including predicting the outcome after stroke [12], grading brain tumors [5], or monitoring the progression of Multiple Sclerosis [20]. Brain volumetry can also be useful to monitor atrophy [9], and analyzing the volume of organs is useful for aging studies [27]. However, automated segmentations can be error-prone, which inevitably leads to imprecise volumetric measurements. A potential solution would be to associate predictive intervals (PIs) with the estimations to take into account this uncertainty.\nConformal prediction (CP) [19,23] is an uncertainty paradigm allowing to associate PIs with regressed scores (here, volumes). The most popular variant"}, {"title": "2 Conformal Prediction for volumetry in medical images", "content": "of CP, Split CP [23], relies on a set-aside calibration dataset (generally a subset of the training dataset) that is used to calibrate the intervals so that they match the target coverage level on fresh test data. However, it is based on the exchangeability hypothesis, following which calibration and test data are drawn independently from the same distribution. In general, this is not the case for medical image processing applications, where domain shifts are extremely common, due to variations in the data acquisition protocol or the presence of pathologies unseen during training [29]. When calibration and test data points are not exchangeable, the accuracy of the conformal procedure collapses drastically [4,26], which hinders the relevancy of conformalized PIs in medical applications.\nAs a potential solution, Weighted Conformal Prediction (WCP) has been proposed to account for shifts between calibration and test distributions [4,26]. It is based on the reweighting of calibration samples according to the estimated density ratio dPtest/dPtrain. As a result, calibration samples close to the test samples are attributed with higher importance in the conformal procedure. A flourishing literature can be found for density ratio estimation, with popular approaches including the training of a classifier to distinguish between training and test distributions [6,2], moment [14] or ratio matching [15]. More recently, Deep Learning (DL) approaches are also investigated to estimate density ratios [10,22]. However, we note that applications of WCP to medical image segmentation are still lacking, which may be due to the difficulty of estimating the density ratio for high-dimensional imaging data.\nIn this work, we propose to investigate the use of WCP to tackle covariate shifts in medical image segmentation tasks, with the ultimate goal of computing calibrated PIs for lesion volumes. As a contribution, we propose an efficient way of computing the density ratio in high-dimensional medical images, by relying on latent representations generated by the segmentation model."}, {"title": "2.1 Problem definition", "content": "We consider a 3D segmentation problem with N classes where our objective is to estimate the true volumes Y \u2208 RN-1 of each foreground class based on the predicted segmentation. Within this framework, for an estimation X of the volume, we define a predictive interval \u0393\u03b1(X) as a range of values that are constructed to contain the true volume Y with a user-defined degree of confidence 1-a (e.g 90% or 95%). More formally, given a set of estimated volumes X\u2081 ... Xn and their corresponding ground truth volumes Y\u2081 . . . Yn, \u0393\u03b1(\u00b7) should be learned such that it satisfies:\n\\begin{equation}\n1 - \\alpha < P(Y_{\\text{test}} \\in \\Gamma_{\\alpha}(X_{\\text{test}})) \\leq 1-\\alpha+\\frac{1}{n+1}\n\\end{equation}"}, {"title": "2.2 Predictive Interval computation using a multi-head segmentation architecture", "content": "In practice, a PI associated with a volume Xi is composed of a lower bound li, and an upper bound ui. For a practical estimation of these three quantities (Xi, li and ui), [18] proposed to train a multi-head segmentation network that predicts 3 output masks: a restrictive one (low recall, high precision) to estimate the lower bound, a permissive one (high recall, low precision) to estimate the upper bound, and a balanced one for the estimation of the mean (Figure 1). The key element is to perform training using the Tversky loss \u03a4\u03b1,\u03b2 [24] allowing to control the penalties applied to false positives (FP) and negatives (FN) contained in each mask through the loss parameters a and \u03b2, respectively. Writing Plower, Pmean and Pupper the outputs of each head and y the ground-truth segmentation, the loss is defined:\n\\begin{equation}\nL = T_{1-\\gamma,\\gamma} (P_{\\text{lower}},y) + T_{0.5,0.5}(P_{\\text{mean}},y) + T_{\\gamma,1-y}(P_{\\text{upper}},y)\n\\end{equation}\nwhere y is a hyperparameter set to 0.2 controlling the penalties applied to FP and FN during the training of the lower and upper bound heads.\nTo ensure that the computed PIs will achieve the user-defined level of coverage on test data, the conformal calibration of intervals can be performed [1]. It operates by first defining a score function si = max(li \u2013 Yi, Yi \u2013 ui). This score is a way to estimate the accuracy of the interval [li, ui] for the true quantity Yi, with larger scores indicating larger discrepancy. The scores are computed on a"}, {"title": "2.3 Weighted Conformal Prediction to tackle covariate shift", "content": "WCP has been proposed to take into account the non-exchangeability of calibration and test data [1,4,26]. The core concept of WCP is to reweight the calibration dataset to more accurately match the test one. This is achieved by estimating the density ratio w = dPtest/dPtrain for each calibration and test sample. In practice, writing X1,..., Xn the n calibration samples and x the fresh test point, importance weights are computed as:\n\\begin{equation}\np^{\\alpha}(x) = \\frac{w(X_i)}{\\sum_{i=1}^{N} w(X_i) + w(x)}\n\\end{equation}\nEssentially, the weight is large when the calibration sample Xi is likely under the test distribution. Then, the corrective value \u011d can be reframed as the 1 - a quantile of the reweighted distribution [1]:\n\\begin{equation}\nq(x) = \\inf\\{s_j: \\sum_{j=1}^{n} p(x)1\\{s_i < s_j\\} \\geq 1-\\alpha\\}\n\\end{equation}\nNote that when all weights are equal to 1/n, the standard CP procedure is recovered. A convenient way to estimate this ratio is to use an auxiliary classifier that only requires that unlabeled samples from the test distribution are available during the calibration step [26]. The idea is to train a probabilistic classification model to classify samples between the training and test distributions. That is, writing X1, ..., Xn and Xn+1, ..., Xn+m the training and test data points, one can form a classification dataset composed of the pairs {Xi, Ci} where Ci = 0 for i = 1, ..., n and Ci = 1 for i = n + 1, ..., n + m. Writing p(x) = P(C = 1|X = x) the probability predicted by a classifier model trained on the {Xi, Ci} dataset that the input sample x belongs to the test distribution, the weight function can be expressed as [25]:\n\\begin{equation}\nw(x) = \\frac{p(x)}{1-p(x)}\n\\end{equation}"}, {"title": "2.4 Efficient density ratio estimation using latent representations", "content": "As training the auxiliary classifier directly from the input images is too costly, more efficient approaches have to be investigated. One idea would be to use a compressed representation of the input image that still preserves important structural information. A lead in this direction is the use of low-dimensional latent representations generated by the segmentation model during the inference process, which has been proven to be a highly efficient summary allowing the detection of out-of-distribution images [7,13,28]. Therefore, using compressed latent representations in place of the high-dimensional 3D images seems promising as our end goal is to address covariate shifts. To test this framework, we collect the activations of the penultimate convolution layer. The feature maps have a shape of K \u00d7 H \u00d7 W \u00d7 D, where H, W, and D are the spatial dimensions of the 3D image and K the number of kernels in the layer. This feature map is reduced to a compressed vector z of dimension K by performing an averaging over the spatial dimensions (see Figure 1). This approach allows training the auxiliary classifier on compressed representations of the input MRIs, which can be performed efficiently during the WCP procedure."}, {"title": "3 Experiments", "content": ""}, {"title": "3.1 Synthetic dataset with controlled covariate shift", "content": "To prove the relevancy of the proposed approach, we first rely on a synthetic setting allowing us to control covariate shift precisely. The task that we propose here is the segmentation of spheres inside cubic volumes of shape 32 x 32 x 32, with the end goal of computing a PI for the volume of each sphere. The covariate of interest here is the signal-to-noise ratio (SNR) between the background of the image and the foreground spheres. A total of 4000 synthetic images are generated. We then split this dataset into an in-distribution (ID) split (3000 images) containing images with high SNRs, and a shifted test dataset (1000"}, {"title": "3.2 Real-world covariate shift in brain tumor segmentation tasks", "content": "To test the framework on real-world medical image data, we address multi-class tumor segmentation in brain MRI. Our dataset consists of glioblastoma and meningioma subjects gathered from the open-source BraTS 2023 datasets [17,21]. Each subject has four MRI sequences: T1-weighted, T2-weighted, FLAIR, and T1 with contrast enhancement. Ground truth masks include necrosis, edematous, and gadolinium-enhancing tumor classes. For covariate shift analysis, we divide subjects into an ID dataset (320 glioblastoma subjects, 748 meningioma subjects, 30%-70% repartition) and a shifted test dataset (873 glioblastoma subjects, 196 meningioma subjects, 82% - 18% repartition). The covariate shift thus corresponds to the difference in frequencies of each subtype of tumor in the ID and shifted datasets. The ID dataset is further divided into training (568), calibration (250), and ID test (250) subsets."}, {"title": "3.3 Experimental Protocol and Metrics", "content": "We use MONAI's [8] Dynamic U-Net [11] as segmentation backbone, modified to have three output heads. The penultimate convolution layer contains 64 kernels, meaning that the extracted latent representations will also have a dimension of 64. The models are trained using Equation 2 and the ADAM optimizer [16] with a learning rate of 2 \u00d7 10-4. After training, PIs are calibrated on the calibration dataset, with a target coverage of 95% for the synthetic task, and 90% for brain tumors as the segmentation is more challenging. Three variants of CP are further compared:"}, {"title": "4 Results and Discussion", "content": "Tables 1 and 2 present the performance of each CP variant on the synthetic and brain tumor datasets, respectively. In the absence of covariate shifts (ID datasets), W-Oracle and W-Latent closely mimic Standard CP, achieving target coverages with great accuracy (95% for synthetic data, 90% for brain tumors). However, in Shift datasets, Standard CP exhibits miscoverage, with empirical coverages lower than the target level, revealing its inability to handle non-exchangeable data points. W-Oracle and W-Latent alleviate this issue, with W-Oracle recovering the exact target coverages on the synthetic task and the necrosis and edematous brain tumor classes. W-Latent also reduces the coverage gap, although it doesn't exactly recover the target coverages. It can be noticed that this increase robustness is linked with an increase in the average interval width to achieve the target coverage on shifted test data.\nA deeper dive into the functioning of WCP is presented in Figure 3. It presents the calibration weights' behavior with and without covariate shifts. When there are no shifts, all weights are close to the unit, mimicking the standard CP procedure. When a covariate shift is observed, higher weights are assigned to calibration samples resembling test samples. For the synthetic task, higher weights are attributed to calibration weights with low SNRs, which are similar to the shifted test samples. For tumor segmentation, higher weights are attributed to glioblastoma subjects, which indeed represent the majority of the shifted test subjects. W-Oracle and W-Latent provide similar trends, although W-Latent is more noisy than the Oracle version.\nIn conclusion, our WCP framework is effective in tackling covariate shifts in medical image analysis, by addressing covariate shifts either directly or through"}, {"title": "8", "content": "latent representations, ensuring the robustness of predictive intervals. However, one limitation of the presented WCP framework is that it can only account for moderate covariate shifts. Otherwise, if the covariate shift is too important between calibration and test samples, the weights will likely diverge (see Equation 6 when p(x) converges to 1) which would undermine the accuracy of the WCP procedure."}]}