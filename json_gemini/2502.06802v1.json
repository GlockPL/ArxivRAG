{"title": "Solving the Content Gap in Roblox Game Recommendations: LLM-Based Profile Generation and Reranking", "authors": ["Chen Wang", "Xiaokai Wei", "Yexi Jiang", "Frank Ong", "Kevin Gao", "Xiao Yu", "Zheng Hui", "Se-eun Yoon", "Philip Yu", "Michelle Gong"], "abstract": "With the vast and dynamic user-generated content on Roblox, creating effective game recommendations requires a deep understanding of game content. However, due to the inconsistent and sparse nature of game text features-such as titles and descriptions-traditional recommendation models struggle to interpret and utilize this information effectively. Recent advancements in large language models (LLMs) offer new possibilities for enhancing recommendation systems by extracting and analyzing in-game text data. This paper addresses two primary challenges: (1) generating high-quality, structured text features for games without extensive human annotation, and (2) validating the quality of these features to ensure they improve recommendation relevance. To tackle these, we propose an approach centered on extracting in-game text and leveraging LLMs to infer essential attributes, such as genre and gameplay objectives, from raw player interactions. Additionally, we introduce an LLM-based re-ranking mechanism to verify the effectiveness of the generated text features, thereby enhancing personalization and user satisfaction. Beyond recommendations, our approach enables applications such as user engagement-based integrity detection, which has already been deployed in production. Our method provides a scalable, content-driven framework for improving recommendation quality on Roblox, demonstrating the potential of in-game text understanding to adapt recommendations to the platform's unique, user-generated ecosystem.", "sections": [{"title": "1 INTRODUCTION", "content": "Roblox is a popular online platform where users create and play games designed by other users, resulting in a vast and diverse collection of interactive experiences. To enhance user engagement, Roblox relies on a multi-stage recommendation system that ranks games based on deep neural network (DNN)-driven models, leveraging sparse ID features (such as User ID, Game ID) and dense features derived from user behavior and game statistics. However, these recommendations often fail to capture individual user preferences fully, as they lack content-based signals-particularly game-related text features such as genre and descriptions.\nRecent advances in large language models (LLMs) offer new opportunities to improve recommendation systems by leveraging game text data. LLMs can deepen the understanding of game content, enabling not only enhanced personalization but also supporting essential tasks such as game content validation, scam and fraud detection, and age-appropriate recommendations. As illustrated in Figure 1, in-game text understanding enables a broad range of applications, including explainable ranking, toxicity detection, and conversational recommendations, fostering user trust by providing transparent and engaging interactions.\nRe-ranking techniques refine recommendation lists to better align with user preferences, balancing factors like accuracy, diversity, and personalization [3, 6]. Recent advancements leverage LLMs, including Chain-of-Thought (CoT) reasoning [6] and instruction-tuning frameworks like RecRanker [9], which use enriched prompts to enhance personalization. Transformer-based models further improve quality by modeling item relationships and user preferences holistically [7]. While effective in structured environments, these methods are underexplored in dynamic, unstructured platforms like Roblox. Our work extends these approaches, adapting LLMs to handle Roblox's noisy game text data, enabling personalized re-ranking in a user-generated ecosystem.\nUnlike platforms like Steam [4, 10, 14], where game text is structured and professionally crafted, Roblox faces unique challenges. With accessible tools like Roblox Studio, even young users can create games, resulting in inconsistent and unstructured game text, such as titles and descriptions. This variability, coupled with the rapid influx of new games, complicates the use of text-based features for recommendations. While LLMs excel with structured text, they struggle with Roblox's unrefined descriptions. Generating high-quality, reliable text features without extensive human annotation is essential for delivering personalized recommendations.\nThis paper addresses two critical challenges in improving game recommendations on Roblox, where the platform's user-generated content results in inconsistent and sparse game text features, such as titles and descriptions. This scenario creates a \u201cchicken-and-egg\" problem: effective game recommendations require high-quality text features, but without structured or professionally curated descriptions, LLMs struggle to interpret game content-particularly for new or rapidly changing games.\nThe first challenge is to develop a method for generating high-quality, structured text features for Roblox games without extensive human annotation, which is infeasible given Roblox's scale and rapid content turnover. The second challenge is to establish a framework to validate the quality of these generated text features to ensure that they enhance recommendation accuracy. Addressing these challenges is essential for building a scalable, content-driven recommendation system that adapts to Roblox's unique dynamics.\nTo address the first challenge-generating high-quality game text features-we propose a method centered on extracting and understanding raw in-game text. Our approach leverages the fact that developers often guide players with in-game instructions to prevent drop-off. Using LLMs equipped with strong global knowledge, we analyze this in-game text to infer attributes such as game genre, content themes, and player objectives, ultimately constructing a high-quality, structured game profile. To tackle the second challenge-validating the quality of these generated profiles-we introduce an LLM-based re-ranking mechanism. This model integrates the generated text features to validate ranking performance and personalize game recommendations. By re-ranking based on text feature quality, we ensure that our generated profiles effectively improve both recommendation relevance and user satisfaction.\nIn summary, the key contributions of this paper are as follows:\n\u2022 In-Game Text Extraction and Understanding: We propose a novel approach for generating high-quality game profiles on Roblox by extracting and interpreting raw in-game text, using LLMs to infer game attributes such as genre, content, and play style without human annotation.\n\u2022 LLM-Based Re-Ranking for Quality Verification: To validate the effectiveness of generated game profiles, we introduce an LLM-based re-ranking model that incorporates text features into the recommendation system, enhancing ranking personalization and relevance.\n\u2022 Scalable Framework for Content-Driven Recommendations: By addressing the challenges of text feature generation and quality validation, this work lays the foundation for scalable, content-based recommendation improvements on Roblox, adaptable to the platform's dynamic, user-generated ecosystem."}, {"title": "2 RELATED WORKS", "content": "Content-based recommendation systems enhance personalization by leveraging textual and content-derived features. Early methods relied on structured metadata, such as product descriptions and user reviews [1, 5], while recent advances in deep learning extract rich representations from unstructured text [2, 8]. In gaming, studies have explored large-scale game recommendations [14], user preferences in online games [11], and action recommendations in text-based games [12]. However, these approaches often assume clean, structured input data, which is unavailable on user-generated platforms like Roblox, where text is frequently noisy and inconsistent. Our work addresses this gap by extracting raw in-game text and generating structured profiles to capture genre, objectives, and gameplay mechanics, enabling effective recommendations in noisy environments."}, {"title": "3 PROPOSED METHOD", "content": "In this section, we present a two-part pipeline designed to integrate content-driven insights into Roblox's recommendation system. The pipeline comprises (1) Game Profile Generation, which involves extracting and structuring in-game text into detailed game profiles, and (2) LLM-Based Reranker, where these profiles are leveraged to re-rank existing recommendations and validate their effectiveness in enhancing relevance and personalization. Algorithm 1 summarizes the overall process of our proposed method."}, {"title": "3.1 Game Profile Generation", "content": "The first component of our method is Game Profile Generation, which focuses on creating high-quality, structured profiles from raw in-game text. Given the diversity of games on Roblox and the inconsistency in developer-provided descriptions, we approach this by analyzing text that appears naturally within gameplay, as developers often guide players through instructions and cues during play. Formally, let G denote the set of games. For each game $g \\in G$, we extract the in-game text $T_g$. The main steps in this process are as follows:\n3.1.1 In-Game Text Extraction. We extract all in-game text elements encountered by players, including instructions, background descriptions, button prompts, and other guidance, denoted as $T_g$ for each game g. This text provides valuable insights into the game's genre, objectives, themes, mechanics, and language. To enable holistic analysis, we aggregate all in-game text into a single file per game, serving as input for the LLM. If $T_g$ exceeds the LLM's token limit, we apply random sampling to reduce its length while preserving diverse content. Instead of manually filtering noisy text, we use carefully designed prompts to instruct the LLM to focus on relevant game aspects and ignore irrelevant content. This approach ensures extraction of key information, such as gameplay objectives, genre, and mechanics, without extensive preprocessing. Figure 2 illustrates various types of in-game text that inform our understanding, including gameplay instructions, background context, action prompts, and language cues. This process generates structured game profiles to support enhanced recommendations\n3.1.2 Game Profile Generation via LLMs. After preparing the in-game text file $T_g$ for each game g, we employ a specifically crafted prompt for the LLM to generate a structured game profile $P_g$, focusing on essential attributes while filtering out irrelevant information. The prompt directs the LLM to produce a concise summary that highlights the game's main theme, storyline, primary objectives,"}, {"title": "3.2 LLM-Based Reranker", "content": "Building on the generated game profiles, the LLM-Based Reranker assesses their effectiveness by re-evaluating the initial recommendations through a content-driven, personalized process. Inspired by the Chain-of-Thought (CoT) reasoning strategy [13], the reranker uses carefully crafted prompts to generate a reranking strategy tailored to user preferences. By incorporating content-rich game profiles, it demonstrates how in-game text understanding enhances recommendation quality. The reranker focuses on the top 30 recommendations from the initial ranking list $R_u$, which contains up to 250 games in Roblox's \"Recommended For You\" sort. This focus is driven by several key factors: (1) User interaction data indicates that players predominantly engage with the top 30 games, making this subset the most relevant for optimizing user experience. (2) Improving the top 30 rankings has the highest potential impact on user satisfaction, as it directly influences the most visible and frequently accessed recommendations. (3) Reranking a smaller subset reduces computational overhead, enabling a balance between effectiveness and efficiency in the reranking process. The LLM-Based Reranker operates in three sequential steps, designed to adapt recommendations to user preferences. These include user profile generation, personalized strategy creation, and the application of this strategy to the top 30 games. Detailed prompts used for these steps are provided in the Appendix B, and Appendix C.\n3.2.1 User Profile Generation. The first step in the reranking process is to generate a user profile $P_u$ that encapsulates individual preferences based on the user's recent activity. Using the generated game profiles as contextual data, we analyze the user's play history from the past seven days, denoted as $H_u$. Each game ID in this history is converted into its corresponding game profile $P_g$, producing a sequence of structured game attributes that reflect the user's recent interactions and preferences. To ensure the validity of in-game text understanding, this process relies solely on play history and excludes any direct game IDs, which lack descriptive features. We use a carefully crafted LLM prompt to analyze this sequence of game profiles and generate a comprehensive user profile $P_u$. This prompt instructs the LLM to summarize the user's preferences across multiple dimensions, such as game genres, themes, mechanics, and gameplay styles. For example, it considers the types of games the user played most frequently, the diversity in their preferences, and any dominant patterns in their gaming behavior.\n3.2.2 Personalized Reranking Strategy Generation. Once the user profile $P_u$ is created, the next step is to generate a personalized reranking strategy $S_u$ tailored to the user's preferences. This strategy is crafted by feeding the user profile into the LLM with a specially designed prompt that instructs the model to identify and prioritize attributes most relevant to the user. The prompt, inspired by the CoT reasoning approach [13], systematically guides the LLM to consider key aspects such as preferred game genres, gameplay mechanics, unique features, and thematic elements. The reranking strategy $S_u$ acts as a blueprint for the subsequent reranking process. For example, if a user's profile indicates a strong preference for adventure games with exploration mechanics, $S_u$ will prioritize these attributes when re-evaluating the recommendations. The strategy explicitly avoids referencing specific game IDs, as they lack meaningful descriptive features, and instead focuses on actionable insights derived from the game profiles.\n3.2.3 Reranking the Top 30 List. The final step applies the personalized strategy $S_u$ to the top 30 games in the initial recommendation list, denoted as $R_{30}^u$. By focusing on this concise subset, the reranker targets the most impactful portion of the ranking, where user engagement is typically highest. The LLM uses the guidelines from $S_u$ to re-evaluate and reorder these recommendations based on their alignment with the user profile $P_u$. This process leverages the structured attributes in each game profile to determine relevance and prioritization. For instance, games matching the user's preferred genres or featuring gameplay mechanics identified in $S_u$ are ranked higher. The refined recommendation list, $R_u'$, represents a personalized, content-driven ordering designed to maximize user engagement and satisfaction. By concentrating on the top 30 games, this approach balances computational efficiency with the practical needs of a user-focused recommendation system."}, {"title": "4 EXPERIMENTS", "content": "4.1 Experimental Setup\n4.1.1 Objective. The primary objective of this experiment is to verify the effectiveness of the game profiles generated through in-game text understanding. To evaluate this, we designed an LLM-Based Reranker that utilizes only the generated game profiles and the user's recent play history from the past seven days. By reranking recommendations based on these profiles, we aim to determine whether incorporating content-driven information enhances recommendation quality. Additionally, we engaged human annotators"}, {"title": "4.2 Data Collection", "content": "For this study, we extracted real training data from the Roblox platform, dividing it into nine sub-datasets to account for variations in user behavior and list length. We created these sub-datasets along two dimensions: the length of user play history and the length of the ranking list. Specifically, we used the (0-30, 30-70, 70-100) percentiles to segment user play history lengths, capturing different levels of user engagement and diversity in game interactions. Additionally, we limited the ranking list to (top10, top20, top30) games for reranking in certain scenarios. Overall, the dataset comprises 2,700 unique users and 18,941 unique games, providing a diverse sample for assessing the performance of the LLM-Based Reranker and game profiles. Table 1 presents key statistics for each sub-dataset, including user and game counts as well as playtime details, providing an overview of the dataset's characteristics and user engagement patterns.\n4.2.1 Environment. All experiments were conducted using GPT-40 as our LLM model, selected for its ability to handle complex language understanding tasks and deliver accurate content-based interpretations. This model was used for both game profile generation and reranking, ensuring consistency across all steps of the recommendation pipeline."}, {"title": "4.3 Baseline and Comparative Models", "content": "4.3.1 Baseline Model. The baseline model used in this study is the existing Roblox ranking model. This model ranks games primarily based on ID-based features and user behavior data, without incorporating any content-derived attributes, such as game title, description, or thematic elements. As a traditional recommendation system, it relies on collaborative filtering methods, utilizing user and game IDs alongside sparse behavior data like play frequency. This model serves as a foundational comparison point, as it does not leverage in-game text understanding or content-driven game profiles.\n4.3.2 Comparative Models. To evaluate the incremental impact of content-driven profiles and personalized reranking strategies, we include several comparative models: (1) Game Title-Based Reranking: This model reranks the recommendation list solely based on the game title. By using only this minimal text feature, we can evaluate the basic effect of content-based reranking when limited to title information, giving a partial view of the game's identity. (2) Game Title and Description-Based Reranking: This model expands on the previous approach by incorporating both game title and description for reranking. With additional descriptive context, this model provides a better understanding of the game content but still lacks the depth of the structured game profiles generated through in-game text understanding. (3) LLM-Based Reranker without Personalized Strategy: In this model, the LLM-Based Reranker uses the full, generated game profiles (including elements such as genre, mechanics, and thematic details) but does not apply a user-specific personalized strategy. Instead, it ranks games based on a generalized relevance derived from game profiles alone. This model helps us assess the quality and effectiveness of the generated game profiles without any individual user customization. (4) LLM-Based Reranker with Personalized Strategy (Proposed Model): This is the full implementation of our proposed LLM-Based Reranker, which not only leverages content-rich game profiles but also incorporates a personalized reranking strategy based on recent user play history. By generating a custom reranking guideline for each user, this model aims to maximize recommendation relevance by aligning it closely with each user's unique preferences and past interactions.\n4.3.3 Purpose of Comparison. The primary purpose of this comparative analysis is to evaluate the quality and effectiveness of the generated game profiles and their impact on recommendation performance. By comparing these models, we aim to measure how well each approach reflects user interests and enhances recommendation relevance. Additionally, this analysis explores the potential for applying the LLM-Based Reranker for more personalized ranking in the future. By isolating the effects of content-driven profiles and personalized strategies, we gain insight into the value of in-game text understanding and its scalability for adaptive, user-specific recommendations on Roblox."}, {"title": "4.4 Evaluation Metrics", "content": "4.4.1 NDCG Engagement. To evaluate the effectiveness of the LLM-Based Reranker, we use NDCG Engagement, a variation of the traditional Normalized Discounted Cumulative Gain (NDCG) metric that measures the relevance of recommendations based on user engagement. Unlike standard NDCG, where each relevant item has a binary relevance score of 1, NDCG Engagement assigns relevance scores according to the user's playtime within the 7 days following their initial interaction with the recommended game. This approach allows us to capture deeper insights into how well the recommendations align with user interests, as increased playtime indicates a higher level of engagement and satisfaction. The formula for NDCG"}, {"title": "4.5 Results and Analysis", "content": "4.5.1 Quantitative Results. In this section, we analyze the NDCG Engagement scores for different reranking models across various percentile ranges and NDCG cutoffs (NDCG@10, NDCG@20, and NDCG@30), as shown in Table 2. To ensure reliability, all experiments were run five times, and the average results are reported. The quantitative analysis highlights the performance of the Baseline model, Title-based Reranking, Title+Desc Reranking, LLM-Based Reranker without Personalized Strategy, and the Proposed LLM-Based Reranker with Personalized Strategy (proposed model).\n\u2022 Overall Improvement of the Proposed Model: The Proposed LLM-Based Reranker with Personalized Strategy consistently demonstrates superior performance compared to the Baseline model across most percentile ranges and NDCG cutoffs. This is evident in the Improvement (%) column, where the Proposed model achieves notable gains, especially in lower percentile ranges. The overall average improvement for the Proposed model is highest in NDCG@10 (4.90%), followed by NDCG@20 (2.01%), and NDCG@30 (1.51%). These results suggest that the personalized reranking strategy is particularly effective at optimizing the top ranks, which are critical in enhancing user engagement. The substantial improvement at the higher positions (NDCG@10) emphasizes the value of personalization in capturing user interest early in the recommendation list.\n\u2022 Performance of Title-Based and Title+Desc Models: Both the Title-based Reranking and Title+Desc Reranking models show a decrease in performance compared to the Baseline. While the Title+Desc model leverages both game title and description text, these features often contain noise in Roblox. Game titles and descriptions may be inconsistent or unrelated to the actual game content, as they are often created by a wide range of developers with varying levels of professionalism. Furthermore, the LLM may lack sufficient global knowledge about many Roblox games, particularly new and less popular ones, limiting its ability to interpret these text features accurately. This underlines the importance of reliable content-based features and demonstrates\nthe limitations of using unstructured, noisy text in the absence of comprehensive domain knowledge.\n\u2022 LLM-Based Reranker without Personalized Strategy: The LLM-Based Reranker without Personalized Strategy performs comparably to the Baseline model but does not outperform the Proposed model. This outcome highlights the crucial role of personalization in enhancing recommendation relevance. While the content-based reranker benefits from in-game text understanding, the absence of user-specific personalization limits its effectiveness. These findings indicate that merely incorporating content-based features is insufficient; personalized adaptation to user preferences based on in-game text understanding is essential for optimal performance.\n\u2022 Challenges with the 30-70 Percentile Range: Across the 30-70 percentile range, we observe a performance decrease for most text-based models. This drop can be attributed to the diverse and less focused nature of user play histories in this range. Unlike users in the lower percentile range, who often have narrow interests, or high-engagement users, who exhibit consistent preferences, mid-engagement users tend to explore a broader variety of games without clear patterns. For example, a user might play simulation, obby, adventure, tycoon, and competition games over the past week, but their preferences may not consistently favor one genre, such as simulation, over another, like obby. Without a discernible trend in user preferences, the reranker struggles to capture specific interests accurately, especially when relying solely on in-game text features without leveraging statistical insights or behavioral patterns. This highlights the need for methods capable of resolving ambiguities in diverse play histories and creating a clearer representation of user interests for mid-engagement users.\n4.5.2 User Engagement Analysis. Figure 3 illustrates the average time users spent on games at each ranking position for both the Baseline model and the LLM-based reranker. Higher values represent greater user engagement, suggesting increased interest in games at those positions. The LLM-based reranker, indicated by the dashed red line, generally places games with higher user engagement towards the top of the ranking list, particularly in the highest-ranked positions, as evidenced by the elevated average time spent. This demonstrates the LLM-based reranker's effectiveness in surfacing games that align with user preferences, as compared to the Baseline model.\n4.5.3 Ablation Study: Impact of Different LLM Models. To evaluate the impact of different LLM models on the performance of our proposed pipeline, we conducted an ablation study using several LLMs: Meta-Llama-3.1-8B-Instruct, Meta-Llama-3.1-70B-Instruct, GPT-40-mini, and GPT-4-turbo. We compared each model's performance against GPT-40, which serves as the baseline, using NDCG@30 as the evaluation metric. The results reveal significant variability in the effectiveness of the LLMs. Meta-Llama-3.1-8B-Instruct was unable to follow the instructions in our prompt and, therefore, could not produce usable results. Among the remaining models, all exhibited performance degradation compared to GPT-40. Specifically, Meta-Llama-3.1-70B-Instruct, GPT-40-mini, and GPT-4-turbo showed relative decreases of -3.889%, -0.832%, and -2.269% in NDCG@30, respectively."}, {"title": "4.5.4 Case Study: Effectiveness of LLM-Based Personalized Ranking Strategy", "content": "To better understand the impact of in-game text understanding and personalized reranking, we conducted a case study on the effectiveness of the LLM-based reranker in following a personalized ranking strategy. This study demonstrates the reranker's ability to prioritize games based on specific user preferences, highlighting its potential for delivering highly relevant recommendations. Below is an example of the ranking strategy generated by the LLM:"}, {"title": "5 CONCLUSION", "content": "5.1 Summary\nIn this paper, we proposed a novel approach to improving game recommendations on Roblox by leveraging in-game text understanding and large language models (LLMs). Our method consists of two stages: Game Profile Generation, which extracts and structures raw in-game text into meaningful profiles, and an LLM-Based Reranker, which validates the effectiveness of these profiles through personalized reranking. It successfully demonstrates the value of in-game text understanding in enhancing recommendation relevance and aligning results more closely with user interests. Experimental results show that incorporating content-driven insights can improve recommendation quality, providing a strong foundation for future improvements in game recommendation systems.\n5.2 Limitations\nDespite its effectiveness, our method faces several limitations. First, the game features and taxonomies generated by LLMs remain somewhat vague and lack fine-grained distinctions. For example, while Roblox hosts a variety of games broadly categorized as simulation, adventure, or obby, user preferences often require more granular profiling to accurately capture nuanced interests. Second, the reranker, in its current form, relies solely on the LLM's general knowledge and lacks the ability to incorporate dataset-specific popularity and statistical features. This limitation arises because the LLM is not fine-tuned on the Roblox dataset, preventing it from fully leveraging platform-specific trends and user behavior patterns. Lastly, while the dynamic nature of Roblox introduces challenges in profiling new games, this can largely be addressed by profiling games only after they surpass a certain popularity or usage threshold, minimizing computational overhead. Together, these challenges highlight the need for improvements in granularity, dataset-specific adaptation, and profiling strategies.\n5.3 Future Work\nTo address these limitations, future work should focus on several key areas. First, enhancing the game profile generation process to produce finer-grained and more detailed representations of user preferences and game attributes will be critical for improving personalization. Second, fine-tuning LLMs on Roblox-specific data is an essential next step to better capture platform-specific statistics and popularity trends. This fine-tuning will enable the reranker to incorporate dataset-specific insights, such as the relative popularity of games and temporal user behavior patterns, alongside general content-based understanding. Additionally, integrating multimodal data, such as game visuals, audio cues, and user interaction logs, will provide a richer context for recommendations. Future developments should also explore personalized sort generation. By leveraging the personalized reranking strategy, it would be possible to create user-specific ranking lists with tailored sort names that align with individual preferences. For example, users with a strong interest in adventure games could receive a customized \"Top Adventures for You\" sort, enhancing engagement through more relatable and intuitive categorizations."}, {"title": "A LLM PROMPT FOR GAME PROFILE GENERATION", "content": "Given a Roblox game, we have access only to the in\n-game text features.\nThese features are provided in the following list\nformat: %s.\nThe game's language is: %s. If the language is\nspecified as \"NONE,\" please\nanalyze the text to determine the game's language.\nContext:\n1. In-game text features are the text elements\ndisplayed to users while they play the game. (\nWHAT)\n2. These text features provide crucial information\nto help users understand and navigate the\ngame. (WHY)\n3. Understanding these text features is essential\nfor users to play the game effectively. (\nPURPOSE)\n4. The in-game text features can be noisy and may\ncontain irrelevant information. Please focus\nonly on the relevant information and omit any\nirrelevant details. (NOTE)\nTask:\n1. Generate a summary for the game. This summary\nis vital for the recommender\nsystem to better understand the game.\n2. The summary should be concise, informative, and\na few sentences long. It\nwill help in understanding user preferences and\nrecommending games accordingly.\n3. The summary MUST be in JSON format, directly\nreadable by json.loads(). The\nJSON should have the following structure, where\neach key represents an\nattribute of the game and the value is the\ncorresponding attribute's value:\n{\n\"game_about\": \"Provide a concise and\ninformative description of the Roblox\ngame. Include the main theme or storyline,\nprimary objectives, core gameplay\nmechanics, unique features, and target\naudience. This should give a clear\noverview of what the game is about and what\nplayers can expect.\",\n\"game_genre\": \"Specify the genre of the Roblox\ngame. Examples include obby\n(obstacle course), tycoon, role-playing,\nsimulator, adventure, etc. This helps\ncategorize the game and gives an idea of the\ntype of gameplay involved.\",\n\"suitable_for\": \"Indicate the target audience\nfor the Roblox game. This\ncould be based on age group (e.g., kids, teens\n,\nall ages) or other demographic\nfactors (e.g., casual players, competitive\nplayers). This helps in\nunderstanding who the game is designed for.\",\n\"features\": \"List the key features of the\nRoblox game. These could include\nmultiplayer modes, character customization, in\n-game purchases, special\nabilities, unique controls, etc. This\nhighlights what makes the game\ninteresting and engaging.\",\n\"includes\": \"Mention any additional content or\nelements included in the\nRoblox game. This could be special events,\nseasonal updates, exclusive items,\netc. This provides information on the extra\ncontent available to players.\",\n\"game_language\": \"Specify the language of the\nRoblox game. If the language\nis 'NONE', analyze the in-game text to\ndetermine the language. This helps in"}, {"title": "B LLM PROMPT FOR USER PROFILE AND RANKING STRATEGY GENERATION", "content": "Given the user play history in the past 7 days,\nplease write a personalized ranking strategy\nfor the user for future ranking usage, you can\nconsider below attributes but not limit of\nthem:\n1. What type of games the user has played in the\npast 7 days?\n2. What type of games the user played most\nfrequently?\n3. Analyze the user's preference based on the game\ngenres.\n4. In the ranking strategy, we do not need to\nmention the game ID that user has played,\nsince game ID doesnot reflect any game\nfeatures.\nBelow is the user play history in the past 7 days,\neach game is represnted by a unique id with\nthe game profile information.\n{user_play_history_str}"}, {"title": "C LLM PROMPT FOR GAME RERANKING", "content": "Given the user's personalized ranking strategy,\nplease rank the following games based on the\nuser's preference.\nYou can use the following game profile information\nto rank the games.\nThe output format MUST be top ranking_length}\ngame_id list in the order of the ranking\nWITHOUT any other information.\nHere is the user's personalized ranking strategy:\n{user_profile}\nHere is the game profile information for the games\nto be ranked:\n<Candidate Game Info Start >\n{ranking_results_str}\n<Candidate Game Info End>"}]}