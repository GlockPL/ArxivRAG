{"title": "NAVIX: Scaling MiniGrid Environments with JAX", "authors": ["Eduardo Pignatelli", "Robert Tjarko Lange", "Pablo Samuel Castro", "Jarek Liesen", "Chris Lu", "Laura Toni"], "abstract": "As Deep Reinforcement Learning (Deep RL) research moves towards solving large-scale worlds, efficient environment simulations become crucial for rapid experimentation. However, most existing environments struggle to scale to high throughput, setting back meaningful progress. Interactions are typically computed on the CPU, limiting training speed and throughput, due to slower computation and communication overhead when distributing the task across multiple machines. Ultimately, Deep RL training is CPU-bound, and developing batched, fast, and scalable environments has become a frontier for progress. Among the most used Reinforcement Learning (RL) environments, MiniGrid is at the foundation of several studies on exploration, curriculum learning, representation learning, diversity, meta-learning, credit assignment, and language-conditioned RL, and still suffers from the limitations described above. In this work, we introduce NAVIX, a re-implementation of MiniGrid in JAX. NAVIX achieves over 200000\u00d7 speed improvements in batch mode, supporting up to 2048 agents in parallel on a single Nvidia A100 80 GB. This reduces experiment times from one week to 15 minutes, promoting faster design iterations and more scalable RL model development.", "sections": [{"title": "1 Introduction", "content": "Deep RL is notoriously sample inefficient [Kaiser et al., 2019, Wang et al., 2021, Johnson et al., 2016, K\u00fcttler et al., 2020]. Depending on the complexity of the environment dynamics, the observation space, and the action space, agents often require between 107 to 109 interactions or even more for training up to a good enough policy. Therefore, as Deep RL moves towards tackling more complex environments, leveraging efficient environment implementations is an essential ingredient of rapid experimentation and fast design iterations.\nHowever, while the efficiency and scalability of solutions for agents have improved massively in recent years [Schulman et al., 2017, Espeholt et al., 2018, Kapturowski et al., 2018], especially due to the scalability of the current deep learning frameworks [Abadi et al., 2016, Paszke et al., 2019, Ansel et al., 2024, Bradbury et al., 2018, Sabne, 2020], environments have not kept pace. They are mostly based on CPU, cannot adapt to different types of devices, and scaling often requires complex distributed systems, introducing design complexity and communication overhead. Overall, deep RL experiments are CPU-bound, limiting both speed and throughput of RL training.\nRecently, a set of GPU-based environments [Freeman et al., 2021, Lange, 2022, Weng et al., 2022, Koyamada et al., 2023, Rutherford et al., 2023a, Nikulin et al., 2023, Matthews et al., 2024, Bonnet et al., 2024, Lu et al., 2023, Liesen et al., 2024a] and frameworks [Lu et al., 2022, Liesen et al., 2024b, Toledo, 2024, Nishimori, 2024, Jiang et al., 2023] has sparked raising interest, proposing JAX-based, batched implementations of common RL environments that can significantly increase the speed and throughput of canonical Deep RL algorithms. This enables large-scale parallelism, allowing the training of thousands of agents in parallel on a single accelerator, significantly outperforming traditional CPU-based environments, and fostering meta-RL applications.\nIn this work, we build on this trend and focus on the MiniGrid suite of environments [Chevalier-Boisvert et al., 2024], due to its central role in the Deep RL literature. MiniGrid is fundamental to many studies. For instance, Zhang et al. [2020], Zha et al. [2021], Mavor-Parker et al. [2022] used it to test new exploration strategies; Jiang et al. [2021] for curriculum learning; Zhao et al. [2021] for planning; Paischer et al. [2022] for representation learning, Flet-Berliac et al. [2021], Guan et al. [2022] for diversity. Parisi et al. [2021] employed MiniGrid to design meta and transfer learning strategies, and Mu et al. [2022] to study language grounding.\nHowever, despite its ubiquity in the Deep RL literature, MiniGrid faces the limitations of CPU-bound environments. We bridge this gap and propose NAVIX, a reimplementation of Minigrid in JAX that leverages JAX's intermediate language representation to migrate the computation to different accelerators, such as GPUs, and TPUs.\nOur results show that NAVIX is over 10\u00d7 faster than the original Minigrid implementation, in common Deep RL settings (see Section 4.1), and increases the throughput by over 106\u00d7, turning 1-week experiments into 15 minutes ones. We show the scaling ability of NAVIX by training over 2048 PPO agents in parallel (see Section 4.2), each using their own subset of environments, all on a single Nvidia A100 80 GB.\nThe main contributions of this work are the following:\n1. A fully JAX-based implementation of environment configurations that reproduces exactly the original Minigrid Markov Decision Processes (MDPs) and Partially-observable MDPs (POMDPS).\n2. A description of the design philosophy, the design pattern and principles, the organisation, and the components of NAVIX, which, together with the online documentation\u00b2, form an instruction manual to use and extend NAVIX.\n3. A set of RL algorithm baselines for all environments in Section 4.3."}, {"title": "2 Related work", "content": "JAX-based environments. The number of JAX-based reimplementations of common environments is in a bullish trend. Freeman et al. [2021] provide a fully differentiable physics engine for robotics,\nincluding MJX, a reimplementation of MuJoCo [Todorov et al., 2012]. Lange [2022] reimplements several gym [Brockman et al., 2016] environments, including classic control, Bsuite [Osband et al., 2020], and MinAtar [Young and Tian, 2019],\nKoyamada et al. [2023] reimplement many board games, including backgammon, chess, shogi, and go. Lu et al. [2023] provides JAX implementations of POPGym [Morad et al., 2023], which contains partially-observed RL environments. Matthews et al. [2024] reimplement Crafter [Hafner, 2021]. Bonnet et al. [2024] provides JAX implementations of combinatorial problems frequently encountered in industry, including bin packing, capacitated vehicle routing problem, PacMan, Sokoban, Snake, 2048, Sudoku, and many others. Rutherford et al. [2023b] reimplement a set of multi-agent environments, including a MiniGrid-inspired implementation of the Overcooked benchmark.\nYet, none of these works proposes a reimplementation of Minigrid. Weng et al. [2022] is the only one providing a single environment of the suite, Empty, but it is only one of the many, most commonly used environments of the suite, and arguably the simplest one.\nBatched MiniGrid-like environments. Two works stand out for they aim to partially reimplement MiniGrid. Jiang et al. [2023] present AMaze, a fully batched implementation of a partially observable maze environment, with MiniGrid-like sprites and observations. However, like Weng et al. [2022], the work does not reimplement the full MiniGrid suite. Nikulin et al. [2023] proposes XLand-MiniGrid, a suite of grid-world environments for meta RL. Like [Jiang et al., 2023], XLand-MiniGrid reproduces Minigrid-like observations but focuses on designing a set of composable rules that can be used to generate a wide range of environments, rather than reimplementing the original Minigrid suite.\nTo conclude, MiniGrid is a fundamental tool for Deep RL experiments, at the base of a high number of studies, as we highlighted in Section 1. It is easy to use, easy to extend, and provides a wide range of environments of scalable complexity that are easy to inspect for a clearer understanding of an algorithm dynamics, pitfalls, and strengths.\nNevertheless, none of the works above provides a full, batched reimplementation of Minigrid in JAX that mirrors the original suite in terms of environments, observations, state transitions, and rewards. Instead, we propose a full JAX-based reimplementation of the MiniGrid suite that can be used as a drop-in replacement for the original environments."}, {"title": "3 NAVIX: design philosophy and principles", "content": "In this section we describe: (i) the design philosophy and pattern of NAVIX in Section 3.1, and (ii) the design principles at its foundation in Sections 3.2.1 and 3.2.2.\nIn particular, in Section 3.2.2, we highlight why a JAX port of MiniGrid is not trivial. Among others, the obstacles to transform a stateless program, where a function is allowed to change elements that are not an input of the function, to a stateful one, where the outputs of functions depend solely on the inputs; and the restrictions in the use of for loops and control flow primitives, such as if statements.\u00b3"}, {"title": "3.1 Design pattern", "content": "NAVIX is broadly inspired by the Entity-Component-System Model (ECSM), a design pattern widely used in video game development. In an ECSM, entities \u2013 the objects on the grid in our case are \u2013 composed of components \u2013 the properties of the object. Each property holds data about the entity, which can then be used to process the game state. For example, an entity Player is composed of components Positionable, Holder, Directional, each of which injects properties into the entity: the Positionable component injects the Position property, holding the coordinates of the entity (e.g., a player, a door, a key) on the grid, the Holder component injects the Pocket property, holding the id of the entity that the agent holds, and so on. A full list of components and their properties is provided in Table 1. This compositional layout allows to easily generate the wide range of combinations of tasks that MiniGrid offers, and to easily extend the suite with new environments.\nEntities are then processed by systems, which are functions that operate on the collective state of all entities and components. For example, the decision system may update the state of the entities"}, {"title": "3.2 Design principles", "content": "On this background, two principles are at the foundation of NAVIX, and the key aspects that characterise it: (i) NAVIX aims to exactly match MiniGrid (Section 3.2.1), working as a drop-in replacement for the original environments, and; (ii) every environment is fully jittabile and differentiable (Section 3.2.2), to exploit the full set of features that JAX offers."}, {"title": "3.2.1 NAVIX matches MiniGrid", "content": "NAVIX matches the original MiniGrid suite in terms of environments, observations, state transitions, rewards, and actions. We include the most commonly used environments of the suite (see Table 8), and provide a set of baselines for the implemented environments in see Section 4 and Table 8, Appendix E.\nFormally, a NAVIX environment is a tuple $M = (h,w,T, O, A, R, d, \\mathcal{O}, \\mathcal{R}, \\gamma, \\mathcal{P})$. Here, h and w are the height and width of the grid, T is the number of timesteps before timeout; O is the observation space, A is the action space, R is the reward space; \u03b3 is the discount factor. $\\mathcal{O}$ is the observation function, $\\mathcal{R}$ is the reward function, d is the termination function, and $\\mathcal{P}$ is the transition function.\nBy default, one key difference between NAVIX and MiniGrid is that the latter uses a non-Markovian reward function. In fact, MiniGrid dispenses a reward of 0 everywhere, except at task completion, where it is inversely proportional to the number of steps taken by the agent to reach the goal:\n$r_t = \\mathcal{R}(s_t, a, s_{t+1}) \u2013 0.9 * (t + 1)/T,$\nHere $\\mathcal{R}$ is the reward function, $s_t$ is the state at time t, a is the action taken at time t, $s_{t+1}$ is the state at time t + 1, and T is the number of timesteps before timeout. Notice the dependency on the number of steps t, which makes the reward non-Markovian.\nThe use of a non-Markovian reward function is not a mild assumption as most RL algorithms assume Markov rewards. This might call into question the validity of the historical results obtained with MiniGrid, and the generalisation of the results to other environments. For this reason, we depart from the original MiniGrid reward function and use a Markovian reward function, instead, which is 0 everywhere, and 1 at task completion."}, {"title": "3.2.2 Stateful and fully jittable", "content": "While we aim to match MiniGrid in terms of environments, observations, state transitions, rewards, and actions, the API of NAVIX is different, as it must align with JAX requirements for the environment to be fully jittable. In fact, NAVIX environments can be compiled into XLA and run on any JAX-supported accelerator, including GPUs and TPUs. This includes both simply jitting the step function, and jitting the entire training sequence [Lu et al., 2022], assuming that the agent is also implemented in JAX. XLA compilation increases the throughput of experiments massively, allowing for the training of thousands of agents in parallel on a single accelerator, compared to a few that are possible with traditional CPU-based environments. We show the scalability of NAVIX in Section 4.\nFor environments to be fully jittable, the computation must be stateful. For this reason, we need to define an environment state-object: the timestep. The timestep is a tuple $(t, o_t, a_t, r_{t+1}, \\gamma_{t+1}, s_t, i_{t+1})$, where t is the current time \u2013 the number of steps elapsed from the last reset \u2013 $o_t$ is the observation at time t, at is the action taken after $o_t$, $r_{t+1}$ is the reward received after at, $\\gamma_{t+1}$ is the termination signal after at, st is the true state of the environment at time t, and $i_{t+1}$ is the info dictionary, useful to store accumulations, such as returns.\nThis structure is necessary to guarantee the same return schema for both the step and the reset methods, and allows the environment to autoreset, and avoid conditional statements in the agent code, which would prevent the environment from being fully jittable."}, {"title": "4 Experiments", "content": "This section aims to show the advantages of NAVIX compared to the original MiniGrid implementation, and provides the community with a set of baselines for all environments. It does the former by comparing the two suites, for all environments, both in terms of speed improvements and throughput. For the latter, we train a set of baselines for all environments, and provide a scoreboard that stores the results for all environments. All experiments are run on a single Nvidia A100 80Gb, and Intel(R) Xeon(R) Silver 4310 CPU @ 2.10GHz and 128Gb of RAM."}, {"title": "4.1 Speed", "content": "We first benchmark the raw speed improvements of NAVIX compared to the original Minigrid implementation, in the most common settings. For each NAVIX environment and its MiniGrid equivalent, we run 1K steps with 8 parallel environments, and measure the wall time of both. Notice that this is the mere speed of the environment, and does not include the agent training.\nWe show results in Figure 3 and observe that NAVIX is over 45\u00d7 faster than the original MiniGrid implementation on average. These improvements are due to both the migration of the computation to the GPU via XLA, which optimises the computation graph for the specific accelerator, and the"}, {"title": "4.2 Throughput", "content": "While NAVIX provides speed improvements compared to the original MiniGrid implementation, the real advantage comes from the ability to perform highly parallel training runs on a single accelerator. In this experiment, we test how the computation scales with the number of environments.\nWe first test the limits of NAVIX by measuring the computation while varying the number of environments that run in parallel. MiniGrid uses gymnasium, which parallelises the computation with Python's multiprocessing library. NAVIX, instead, uses JAX's native vmap, which directly vectorises the computation. We confront the results with the original MiniGrid implementation, using the MiniGrid-Empty-8x8-v0 environment.\nResults in Figure 5 show that the original MiniGrid implementation cannot scale beyond 16 environments on 128GB of RAM, for which it takes around 1s to complete 1K unrolls. On the contrary, NAVIX can run up to 221 (over 2M) environments in parallel on the same hardware, with a wall time"}, {"title": "4.3 Baselines", "content": "We provide additional baselines using the implementations of PPO [Schulman et al., 2017], Double DQN (DDQN) [Hasselt et al., 2016], and Soft Actor Critic (SAC) [Haarnoja et al., 2018] in Rejax [Liesen et al., 2024b]. We optimize hyperparameters (HP) for each algorithm and environment combination using 32 iterations of random search. Each HP configuration is evaluated with 16 different initial seeds. The HP configuration with the highest average final return is selected. The specific hyperparameters we searched for are shown in Table 9.\nWe run the baselines for 10M steps, across 32 seeds, with the tuned hyperparameters for the environments shown in Figure 7. All algorithms use networks with two hidden layers of 64 units. Instead of alternating between a single environment step and network update, DQN and SAC instead"}, {"title": "5 Conclusions", "content": "We introduced NAVIX, a reimplementation of the Minigrid environment suite in JAX that leverages JAX's intermediate language representation to migrate the computation to different accelerators, such as GPUs and TPUs. We described the design philosophy, the design pattern, the organisation, and the components of NAVIX, highlighting the connections to the ECSM design pattern, and the correspondence between the structure of its functions and the mathematical formalism of RL.\nWe presented the environment interface, the list of available environments, and the scoreboard, storing state-of-the-art results that new algorithms can refer to avoid running also baselines, which are prone to errors and manipulations. We showed the speed improvements of NAVIX compared to the original Minigrid implementation, and the scalability of NAVIX with respect to the number of agents that can be trained in parallel, or the number of environments that can be run in parallel.\nOverall, NAVIX is over 1000x faster than the original Minigrid implementation, turning 1-week experiments into 15-minute ones. With the current pace of the research in RL, the ability to run fast experiments is crucial to keep up with the state-of-the-art, and to develop new, more efficient algorithms. We hope that NAVIX will be a valuable tool for the RL community, and that it will foster the development of new, more efficient algorithms, and the exploration of new research directions."}, {"title": "A Details on NAVIX systems", "content": "Systems are functions that operate on the collective state of all entities, defining the rules of the interactions between them. In designing NAVIX, we aimed to maintain a bijective relationship between the systems and their respective mathematical formalism in RL. This makes it easier to translate the mathematical formalism into code, and vice versa, connecting the implementation to the theory. NAVIX includes the following systems: 1. Intervention: a function that updates the state of the entities according to the actions taken by the agents. 2. Transition: a function that updates the state of the entities according to the MDP state transitions. 3. Observation: a function that generates the observations that the agents receive. 4. Reward: a function that computes the rewards that the agents receive. 5. Termination: a function that determines if the episode is terminated. We now describe the systems formally.\nThe intervention is a function $I : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathcal{S}$ that updates the state of the entities according to the actions taken by the agents. This corresponds to the canonical decision in an MDP.\nThe transition is a function $\\mu : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathcal{S}$ that updates the state of the entities according to the MDP state transitions. This corresponds to the canonical state transition kernel in an MDP.\nThe observation is a function $\\mathcal{O} : \\mathcal{S} \\rightarrow \\mathcal{O}$ that generates the observations that the agents receive. NAVIX includes multiple observation functions, each generating a different type of observation, for example, a first-person view, a top-down view, or a third-person view, both in symbolic and pixel format. We provide both full and partial observations, allowing to cast a NAVIX environment both as an MDP or as a POMDP, depending on the needs of the algorithm. This follows the design of the original MiniGrid suite.\nThe reward is a function $\\mathcal{R} : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$ that computes the rewards that the agents receive. Likewise, the termination is a function $\\gamma : \\mathcal{S} \\rightarrow \\{0,1\\}$ that determines if the episode is terminated. We include both the reward and the termination functions necessary to reproduce all MiniGrid environments. Both these systems rely on the concept of events, representing a goal to achieve. An event is itself an entity signalling that a particular state of the environment has been reached. For example, it can indicate that the agent has reached a particular cell, has picked up a particular object, or that the agent performed a certain action in a particular state.\nWe provide a summary of the implemented systems in NAVIX in Tables 4, 5, and 6 for the observation, reward, and termination systems, respectively."}, {"title": "B Reusable patterns", "content": "Here we provide some useful patterns that users can reuse as-they-are or modify to suit their needs. In particular, we show how to jit the full interaction loop of a NAVIX environment in Code 2, and how to run multiple seeds in parallel in Code 3. Further examples, including how to jit a whole training loop with a JAX-based agent, and how to automate hyperparameter search, are available in the NAVIX documentation at https://epignatelli.com/navix/examples/getting_started.html."}, {"title": "C Customising NAVIX environments", "content": "NAVIX is designed to be highly customisable, allowing users to create new environments by combining existing entities and systems. In this section, we provide examples of how to customise NAVIX environments by using different systems.\nFor example, to create a new environment where the agent has to reach a goal while avoiding lava, we can combine the Goal and Lava entities with the Reward system:"}, {"title": "D Extending NAVIX environments", "content": "NAVIX is designed to be easily extensible. Users can create new entities, components, systems, and full environments by implementing the necessary functions. In this section, we provide templates to extend NAVIX environments. In particular, Code 7 shows how to create a custom environment, Code 8 shows how to create a custom component, Code 9 shows how to create a custom entity, and Code 10 shows how to create custom systems."}]}