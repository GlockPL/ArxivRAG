{"title": "CROP: Context-wise Robust Static Human-Sensing Personalization", "authors": ["Sawinder Kaur", "Avery Gump", "Jingyu Xin", "Yi Xiao", "Harshit Sharma", "Nina R Benway", "Jonathan L Preston", "Asif Salekin"], "abstract": "The advancement in deep learning and internet-of-things have led to diverse human sensing applications. However, distinct patterns in human sensing, influenced by various factors or contexts, challenge generic neural network model's performance due to natural distribution shifts. To address this, personalization tailors models to individual users. Yet most personalization studies overlook intra-user heterogeneity across contexts in sensory data, limiting intra-user generalizability. This limitation is especially critical in clinical applications, where limited data availability hampers both generalizability and personalization. Notably, intra-user sensing attributes are expected to change due to external factors such as treatment progression, further complicating the challenges. This work introduces CROP, a novel static personalization approach using an off-the-shelf pre-trained model and pruning to optimize personalization and generalization. CRoP shows superior personalization effectiveness and intra-user robustness across four human-sensing datasets, including two from real-world health domains, highlighting its practical and social impact. Additionally, to support CROP's generalization ability and design choices, we provide empirical justification through gradient inner product analysis, ablation studies, and comparisons against state-of-the-art baselines.", "sections": [{"title": "Introduction", "content": "AI in human sensing applications\u2014like activity recognition, fall detection, and health tracking\u2014revolutionizes daily life, especially in personal health management [Wang et al., 2023]. However, unique user patterns and natural distribution shifts [Gong et al., 2023] caused by behaviors, physical traits, environment, and device placements [Ustev et al., 2013, Stisen et al., 2015] lead to the underperformance of generic AI models in practical use. To tackle this, various domain adaptation techniques have been explored, with personalization widely used to adapt a generic model to the target user's specific domain or natural distribution [Lamichhane et al., 2023, Iaboni et al., 2022, Meegahapola et al., 2023, Ahamed and Farid, 2018, Ren et al., 2022, Sempionatto et al., 2021, Boukhechba et al., 2020]. In literature, personalization occurs either during the enrollment phase (static) [Duan et al., 2023, Liu et al., 2022, Burns et al., 2022] or continuously throughout application use [Daniels et al., 2023, Liu et al., 2024, Wu et al., 2024, Wang et al., 2022a].\nStatic personalization customizes the model with limited individual data collected at enrollment, requiring minimal computation and user engagement, making it highly practical for human-sensing applications. However, existing such studies often overlook intra-user variability due to factors like changes in magnetic field [Robert-Lachaine et al., 2017], sensor position [Park et al., 2014], terrain [Kowalsky et al., 2021], or the health symptoms P\u00e4eske et al. [2023], leading to poor intra-user generalizability for contexts not present during personalization. For instance, a smartphone activity recognition model personalized with handheld data may perform poorly when the phone is in a pocket.\nStatic personalization is particularly crucial for clinical datasets, which are often characterized by data scarcity, leading to reduced robustness of lab-validated models for prospectively collected users Berisha et al. [2021]. It enhances model accuracy for clinical users whose traits are underrepresented in the global model's training data. In contrast, continuous supervised personalization is generally infeasible in many health domains since ground truths must be validated by clinicians, making it impractical in continuous settings, especially in remote or mobile health applications.\nNevertheless, the distribution of clinical data is expected to shift, even within the same individual. For instance, in clinical speech technologies, changes in data distribution over time may occur due to the progression of neurodegenerative diseases, relevant for disease monitoring apps Stegmann et al. [2020], or through desired learning mechanisms resulting from the use of technology, as seen in automated speech therapy apps Benway and Preston [2023]. Similarly, in stress monitoring via wearables, the distribution of psychophysiological data changes as the same individuals encounter different types of stressors Nagaraj et al. [2023]. This research defines 'context' as the intra-user data distribution formed by varying external factors.\nThis research gap is worsened since static personalization typically relies on a small sample set from the target user, covering limited contexts\u2014particularly in clinical settings or applications with data scarcity Berisha et al. [2021], Benway and Preston [2023]. Commercial human sensing technologies like Google Assistant, Amazon Alexa, and Apple's Siri also personalize speech recognition models using limited phrases during enrollment [Team, 2017, Awobajo, 2023, Phelan, 2019]. Similarly, the Apple Watch uses initial calibration for enhanced running activity tracking [Apple, 2023, Potuck, 2021]. This limited context during personalization is problematic, as shown in this paper's Motivation Section, where we demonstrate that static personalization may improve performance in training contexts but can also significantly degrade it in other unseen contexts for the same user.\nTherefore, given the importance of static personalization in human sensing, this paper addresses its intra-user generalizability gap. As shown in Figure 1, this paper endeavors to personalize an off-the-shelf generic model for a specific user using limited data from limited contexts. The primary objective is to ensure that the personalized model thus obtained exhibits robust generalization capabilities across unseen contexts. Crucially, unseen context"}, {"title": "Problem Statement", "content": "Given a generic model $M_G$, the objective is to tailor a personalized model $M_P^{pa}$ specifically for a user $U_i$ utilizing the data $D_a$ associated with available context $C_a$, here $\\theta$ represents the parameters of the model. The primary goal is to ensure that the personalized model $M_P^{pa}$ performs reasonably well on $U_i$'s data $D_u$ derived from an unseen context $C_u$. Notably, there is no overlap between the data belonging to the two contexts, that is $D_a \\cap D_u = \\phi$.\nIn other words, if $M_O^{pa}$ represents a conventionally-finetuned model trained for a user $U_i$ on data $D_a$, then, the models trained using CROP, $M_P^{pa}$, must on avg. perform better on both available $C_a$ and unseen context $C_u$ than $M_O^{pa}$. More formally, learning objective can be defined as:\n$M_P^{pa} = \\underset{\\theta}{argmin} \\sum_{d \\in D_a} l(M_G^\\theta, d) + \\alpha l||M_P^\\theta||_1$,\nsuch that $D_a \\cap D_u = \\phi$ and\n$\\sum_{d \\in {D_u, D_a}} l(M_P^{pa}, d) < \\sum_{d \\in {D_u, D_a}} l(M_O^{pa}, d)$,\nthat is, the loss incurred by the resulting personalized model $M_P^{pa}$ on avg. across all contexts' data is less than the loss incurred by conventionally-finetuned model $M_O^{pa}$. Here, $l$ represents the standard cross-entropy loss.\nIt is important to emphasize that the above-mentioned optimization problem restricts the usage of data to the available context $C_a$ and has no knowledge of data from the unseen context $C_u$. Hence, for $d \\in D_u$ (unseen context data), the information $l(M_P^{pa}, d)$, and $l(M_O^{pa}, d)$ is absent during the training process."}, {"title": "Approach", "content": "As previously discussed, the generic model's parameters contain generalizable information across all contexts. Addressing the problem statement requires retaining this information to the greatest extent while enabling fine-tuning for the target user. Furthermore, our investigation revealed"}, {"title": "CROP Approach", "content": "Algorithm 1 describes the presented approach which takes as input: the generic model $M_G$, user $U_i$'s data $D_a$ for available context $C_a$, initial value for coefficient of regularization $\\alpha$ and tolerance for pruning $\\tau$; and generates the target personalized model $M_P^{pa}$. Here, $\\alpha$ and $\\tau$ are hyperparameter whose values can be tuned for the given data and model.\nThe approach initiates by finetuning the generic model $M_G$ on data $D_a$, concurrently applying $l_1$ regularization to penalize model parameters (line 2). This regularization encourages sparsity by specifically targeting the magnitude of redundant parameters [Mayank, 2023]. This step is followed by the pruning of redundant weights using the \u2018ToleratedPrune\u2019 module (line 3). The pruned weights are then replaced by the corresponding weights from the generic model $M_G$ (line 4) to restore generalizability; this hybrid model is referred to as the 'Mixed Model.' This step leads to the modification of the activated paths in the personalized model, resulting in changes in the model inferences. However, since the newly activated paths are determined by weights retained from two models and not learned from data patterns, there is a consequent loss of accuracy, as shown and discussed in Appendix E. To mitigate such a loss, as a final step, the Mixed Model undergoes fine-tuning once again on the data from the available context $D_a$ (line 5). The detailed explanation of each of these steps is as follows:\nPersonalized Finetuning with Penalty (Algorithm 1 \u2013 Step 2): The approach uses data $D_a$ to finetune the generic model $M_G$. As shown in the motivation section, such conventional finetuning enhances the model's accuracy within the available context $C_a$. Nevertheless, its performance in unfamiliar contexts may get suboptimal. Notably, during the model's fine-tuning process, we apply $l_1$ regularization to penalize the model weights, forcing the magnitudes of redundant parameters to be close to zero [Mayank, 2023]. The regularization coefficient $\\alpha$ is a trainable parameter optimized during training to minimize the overall loss. As a result, the parameters with high magnitudes carry most of the information regarding the data patterns in $D$, offering two key benefits:\n1. Minimal loss in $C_a$ accuracy: A high fraction of parameters have close to zero magnitudes, and their removal results in minimal information loss for context $C_a$; thus, the adverse impact of pruning in context $C_a$ is minimized.\n2. Maximal generalization: The inclusion of regularization aids ToleratedPrune (discussed below) module in efficiently pruning a higher number of parameters, which are then replaced with weights from the generic model. This restores information from the generic model, contributing to enhanced accuracy in unseen contexts.\nToleratedPrune Module (Algorithm 1 \u2013 Step 3): Algorithm 2 outlines the ToleratedPrune module, taking a model $M_{\\theta}$, pruning tolerance $\\tau$, and the dataset $D$ as inputs. It initiates with a modest pruning amount of $k$ and incrementally increases this amount by $k'$ until the model's accuracy exhibits a drop of $\\tau$ percent on $D$. Here, $k$ and $k'$ are hyperparameters within the range of (0,1). The module returns $M_{\\theta\\downarrow}$, representing the pruned state of the model before the last pruning iteration. This state is such that further pruning would result in a higher accuracy loss on dataset D than the tolerable amount $\\tau$. This module performs pruning leveraging the conventional magnitude-based unstructured pruning [Zhu and Gupta, 2017].\nThus, step 3 in Algorithm 1 generates a pruned personalized model state $M_P^{pa}|M_{\\theta\\downarrow}$ whose prediction accuracy on"}, {"title": "Related Work", "content": "A few static personalization approaches [Burns et al., 2022, Duan et al., 2023, Liu et al., 2022] aimed for the additional goal of out-of-distribution robustness. However, these methods require access to the generic model\u2014either to make specific design choices [Burns et al., 2022], which prevents them from utilizing off-the-shelf models, or to incorporate knowledge about the target user's data distribution during the generic model's training phase [Duan et al., 2023, Liu et al., 2022], raising privacy concerns, particularly in sensitive clinical applications. These requirements do not align with the research objectives of this paper, making them unsuitable as baselines.\nA different set of approaches that do consider the privacy concern is referred to as source-free domain adaptation [Liang et al., 2020]. Liang et al. [2020] (SHOT) proposed the transfer of hypothesis from source by freezing the parameter weights for the classifier layers and only allowing feature extraction to be finetuned to the new domain. The approach is applicable to unsupervised domain adaptation scenarios and employs self-supervised pseudolabeling to align the target domain's representations to the source hypothesis. However, these approaches do not address the constraint of limited-context data during finetuning. We adapted SHOT in this paper's problem setting as one of the baselines.\nContinuous personalization approaches Wang et al. [2022a], Liang et al. [2020], Wu et al. [2024], Daniels et al. [2023], Fini et al. [2022], Tang et al. [2024a,b], Mallya and Lazebnik [2018], Mallya et al. [2018], Wang et al. [2022b] can improve intra-user generalizability by continually fine-tuning the model as new data arrives. Some of these approaches Fini et al. [2022], Tang et al. [2024a,b] require specialized training of the generic model, limiting the use of off-the-shelf pre-trained models. Others like PackNet Mallya and Lazebnik [2018] and Piggyback Mallya et al. [2018] propose supervised methods that require continued steam of labeled data, limiting their application in health-care scenarios. Additionally, Continual Test Time Domain Adaptation (CoTTA) Wang et al. [2022b] proposes unsupervised learning methods and allows the use of off-the-shelf models. However, all continuous learning approaches require repeated computation overhead to adjust the model outcome to new data [Prabhu et al., 2023], which can be infeasible in real-time applications, more so for scalable platforms like wearables Schmidt et al. [2018], which is prominent for health sensing such as stress or fall detection. Nevertheless, since the problems addressed by Packnet, Piggyback, and CoTTA are the closest to the problem addressed in this study, we considered these approaches as baselines. Appendix C provides further details of the related work."}, {"title": "Motivation", "content": "When learning patterns from human sensing data in a limited context, conventional fine-tuning approaches can overwrite generic knowledge that is not relevant to that specific context but applicable to others, leading to a performance drop in those unrepresented contexts. To illustrate this, we conducted a preliminary study comparing the performance of generic and conventionally-finetuned [Hong et al., 2016] personalized human-gesture-recognition models using the LeNet architecture [Zhang et al., 2022] trained on the WIDAR dataset. Data preprocessing details are discussed in the Experiments section."}, {"title": "Empirical Justification for CRoP", "content": "This section empirically discusses how each step of CROP (Algorithm 1) facilitates intra-user generalizability signifying similarity in model's behavior towards available (available during personalization finetuning) and unseen contexts.\nShi et al. [2021] introduced the use of gradient inner product (GIP) to estimate the similarity between a model's behavior across different domains. If $G_i$ and $G_j$ represent the gradient incurred by the model for Domains $D_i$ and $D_j$, then the sign of the product $G_i * G_j$ represents whether the model treats two domains similarly or not. For instance, $G_i * G_j > 0$ signifies that the gradient for both domains has the same direction. We used GIP to quantify generalization. A higher GIP value for a personalized model across available and unseen contexts indicates more similar behavior toward both domains. GIP is measured as: $||\\sum_i G_i||^2 - \\sum_j ||G_i||^2$.\nFigure 3 shows that fine-tuning the generic model (Algorithm 1 \u2013 Step 2) on Context 1, optimizes the model for this context, leading to a highly negative GIP, indicating a greater discrepancy between two contexts. Since model pruning results in generalization [Jin et al., 2022], an increase in GIP value can be observed in the pruned model (Step 3).\nOn further analysis, we found that the model complementary to the pruned model (that is, the parameters that were removed) also contributed towards inter-context behavior discrepancy (negative GIP value). However, the same parameters in the generic model (that are replaced in Step 4) formed a more generalizable set of weights, i.e., GIP > 0. Thus, the model mixing step (Step 4) introduces further generalizability (GIP > 0) in the personalized model."}, {"title": "Ablation Study", "content": "This section presents evaluations showing the effectiveness of the design choices of CROP, focusing on the WIDAR dataset in Scenario 1. Similar patterns were observed in other scenarios and datasets."}, {"title": "One shot Magnitude based pruning as the pruning mechanism", "content": "A variety of pruning mechanism have been proposed in the literature: Magnitude-Based Pruning (MP) [Luo et al., 2017, Zhu and Gupta, 2017], Gradient-Based Pruning (GP) [Liu et al., 2020], pruning top magnitude weights instead of lower ones (MP-T) [Bartoldson et al., 2020], iterative pruning (MP-I)[Paganini and Forde, 2020], and more [Hoefler et al., 2021]. Among these, we found that one-shot magnitude based pruning serves the best purpose for the application discussed in this work. Detailed discussion is provided in Appendix F.1."}, {"title": "Limitations and Future Direction", "content": "Some limitations and future research are discussed below:\n1. The paper performed a limited evaluation on pruning paradigms through ablation studies as it was not the primary focus of the study. Section on ablation study justifies CROP's design choice but does not establish any particular paradigm's superiority in unseen contexts.\n2. The approach relies on using a pre-trained off-the-shelf model as an input, the quality of this model can impact the performance of the final personalized models.\n3. We restrict our study to the models benchmarked and deployed for datasets used in this work without accounting for model variability.\n4. The metrics $A_P$ and $A_G$ are computed for each individual separately as personalized models customized for one user are not applicable to other users in real-world scenarios. Consequently, we focus our evaluations on intra-user generalizability, excluding discussion for inter-user or inter-dataset generalizability."}, {"title": "Broader Impact", "content": "This paper addresses a critical research gap, enhancing the practical utility of human-sensing solutions in real-world applications, particularly in automated healthcare. Next-generation healthcare systems, which employ neural networks for tasks ranging from daily activity detection [Ustev et al., 2013, Stisen et al., 2015] to safety-critical conditions like atrial fibrillation [Comstock, 2017], benefit from personalization due to the heterogeneity in health sensing data [Ji et al., 2021, Sempionatto et al., 2021].\nCROP offers several advantages:\n1. Eliminating the Generic Model Training: CROP leverages off-the-shelf pre-trained models, eliminating the need for training generic models. It is especially valuable in clinical settings where privacy concerns restrict data sharing for training purposes Malin et al. [2018]. This increases the feasibility of deploying personalized models in healthcare.\n2. No privacy concerns: CROP operates on local devices, eliminating the need for transferring potentially sensitive information to a central server. To demonstrate scalability on local devices, the resource consumption for CROP personalization on 4 devices is shown in Appendix G.\n3. Flexibility to use any model architectures: As model pruning has proven applicable to various model architectures, CROP is not restricted to any model architecture constraints, ensuring wide-ranging applicability."}, {"title": "Conclusion", "content": "This study introduces CROP, a novel static personalization approach generating context-wise robust models from limited context data. Using pruning to balance personalization and generalization, empirical analysis on four human-sensing datasets shows CROP models exhibit an average increase of 35.23% in personalization compared to generic models and 7.78% in generalization compared to conventionally-finetuned personalized models. CROP utilizes off-the-shelf models, reducing training effort and addressing privacy concerns. With practical benefits and quantitative performance enhancements, CROP facilitates reliable real-world deployment for AI-based human-sensing applications like healthcare."}, {"title": "Code", "content": "The code is provided in supplementary material arranged into dataset-specific folders. Each folder contains the pre-trained generic model, all the required modules, and the instructions to run the code. The seed values used for the evaluations are also provided in the shell files. The data partitioned into personalized and context-wise sets will be released upon publication."}, {"title": "Compute Resources", "content": "All the computations have been performed on NVIDIA Quadro RTX 5000."}, {"title": "Detailed Results", "content": "The personalized models obtained using CROP exhibit higher classification accuracy than the generic models on the available context's data $D_a$, showcasing the benefits of personalization. To demonstrate the existence of such improvement, Tables 6a- 6h and Table 9a compare the performance of generic model $M_G^\\theta$ and personalized models obtained using CROP $M_P^{pa}$.\nWIDAR: Tables 6a and 6b show that there is an average improvement of 25.25 and 11.88 percent points among three users for the available context $C_a$ for Scenario 1 and Scenario 2, respectively. However, this benefit comes at the cost of a reduction in accuracy for the unseen context. There is an average reduction of 16.69 and 5.97 percent points for Scenario 1 and Scenario 2, respectively, for the unseen context $C_u$. Notably, the loss of accuracy in the unseen context is much lower as compared to the conventionally-finetune model as discussed in the Motivation Section.\nExtraSensory: Similar patterns could be observed for the Extrasensory dataset. Tables 6c and 6d show that there is an average increment of 16.40 and 18.37 percent points for the available context for Scenario 1 and Scenario 2, respectively. Interestingly, the performance of the personalized model for Scenario 1 on unseen context $C_u$ was not adversely impacted. This is attributed to the fact that the inertial sensing patterns of Bag and Pocket phone carrying modes capture the user's body movement, whereas the phone-in-hand movement patterns can be distinct. In Scenario 1, $C_a$ comprises pocket and $C_u$ comprises bag, meaning both available and unseen contexts encompass similar inertial patterns, leading to advantageous performance even in the unseen context. This evaluation illustrates minimal intra-user generalizability loss on unseen contexts when both available and unseen contexts share similar user traits. However, in Scenario 2, where only the hand belongs to the unseen context $C_u$, there is an average loss of 5.02 percentage points on the unseen context.\nStress Sensing: The physiological features used in this dataset vary significantly from one user to other. Thus, Tables 6e-6h show that the generic models do not perform well on personalized data. Personalized finetuning enables the model to learn person-specific patterns, allowing the model's performance to improve not only in the available context but also in the unseen context. This results in average personalization benefit ($A_P$) of 67.81 and 85.25 for Scenario 1 and Scenario 2, respectively. It is important to note that for each Scenario, only one model is trained for the available context and tested for two different unseen contexts. Moreover, double context change (Tables 6g and 6h) shows lower personalization benefit as compared to single context change (Tables 6e and 6f).\nPERCEPT-R: In this dataset, the heterogeneity of features among individuals is reflected through the difference in prediction accuracy of the generic model. It can be observed in Table 9a that for some individuals, the generic model exhibits over 90% accuracy on the available context data, while for others, the generic model's accuracy drops to around 60%. This results in significant variability over gains in available and unseen contexts. Overall, CROP yields an average personalization gain of 5.09%.\nOn average over all the datasets, a personalization benefit ($A_P$) of 35.23 percent points are seen as compared to the generic models across the four datasets under both scenarios.\nThese evaluations establish that the personalized models obtained using CROP demonstrate improved performance over the available context data than the generic models and exhibit personalization."}, {"title": "Comparison with Personalized Models", "content": "The personalized models obtained using CROP ($M_P^{pa}$) are expected to have higher accuracy on unseen context $C_u$ than the conventionally-finetune personalized models ($M_O^{pa}$) as discussed in the Motivation Section. This section assesses whether the results align with these expectations.\nWIDAR: Tables 8a and 8b demonstrate that the personalized models $M_P^{pa}$ exhibit an average increment of 8.01 and 2.85 percent points in the unseen context for Scenario 1 and Scenario 2, respectively. However, an average loss of 1.57 and an average gain of 1.44 percent points in $C_a$ accuracy could be observed for Scenario 1 and Scenario 2, respectively.\nExtrasensory: Similar patterns could be observed for the ExtraSensory dataset where the average accuracy on the unseen context improved by 4.97 and 12.61 percentage points for Scenario 1 and Scenario 2 as shown in Tables 6c and 8d, respectively. As expected, there is a loss of 5.43 and 4.44 percent points in the available contexts for Scenario 1 and Scenario 2, respectively.\nStress Sensing: As observed in Tables 6e-6h, personalized finetuning improves models performance on unseen context as well, we can claim that there is some person-specific traits which are common in available and unseen context. While comparing our final models with conventionally-finetuned models (Tables 8e-8h), performance boost in both available and unseen context could be observed. This can be attributed to the generalization improvement benefits of model pruning [Jin et al., 2022]. This results in average generalization benefit ($A_G$) of 13.81 and 13.08 for Scenario 1 and Scenario 2, respectively, for single context change. Similar personalization benefits could be seen for double context change.\nPERCEPT-R: As observed in Table 9b, the variability in generalization benefits among different individuals is less pronounced as compared to personalization benefits. On average, CROP introduces a generalization benefit of 2.57%.\nOn average over all the datasets, a generalization benefit ($A_G$) of 7.78% percent points are seen over the conventionally-finetuned personalized models across all datasets under both scenarios."}, {"title": "Individual Analysis for PERCEPT-R dataset", "content": "The heterogeneity of data in PERCEPT-R dataset resulted in variability in $A_P$ among various participants. In order to investigate that further, we conducted fisher information matrix (FIM) analysis of the generic model$M_G$, conventionally-finetuned model $M_O^{pa}$ and the final models obtained using CROP $M_P^{pa}$ for the data belonging to available $C_a$ and unseen contexts $C_u$. These results can be found in Table 7.\nPrevious works have explored the fisher information matrix (FIM) as a means to investigate the curvature properties of the loss landscape Zandonati et al. [2022], Martens [2020] and its relationship to model generalizability Jastrzebski et al. [2021]. The fisher information trace serves as a metric to study the loss landscape curvature sensitivity Jastrzebski et al. [2021], Zandonati et al. [2022] i.e., a larger trace coincides with a sharper loss landscape minima signifying higher sensitivity and poorer generalization performance Jastrzebski et al. [2021], Kim et al. [2022]."}, {"title": "Pruning mechanism", "content": "Magnitude Based Pruning (MP) vs Gradient Based Pruning\nLiu et al. [2020] proposed a gradient-based pruning (GP) that associates the importance of a kernel or a node with l\u2081 norm of gradients and prunes the ones with the least importance. However, the approach has been shown to"}, {"title": "One shot vs Iterative approach", "content": "The presented approach uses a one-shot approach where the initially finetuned model undergoes one pass of pruning, mixing and finetuning. To implement the iterative approach, we allow the initially finetuned model to undergo multiple passes of pruning, mixing, and finetuning. Figure 8(a) and 8(b) show that the iterative approach does not result in significant improvements in model performance, but the computation cost involved in carrying out the iterative approach is quite high."}, {"title": "Regularization Mechanisms", "content": "Weight penalty-based regularization mechanisms are based on the computation of certain norms over the weights of the model. Some common norms used are lo, 11, 12 and polarization. Figure 9 shows that among all these possible choices of regularization, l\u2081 performs the best among both the scenarios and hence is used as the weight penalty method in the presented approach."}, {"title": "Full finetune vs. partial finetune", "content": "In order to keep the zeroed-out parameters as zero, conventional pruning approaches finetune only the weights that are retained during the pruning phase. However, in the presented approach, the zeroed-out weights are replaced by corresponding weights from the generic model. Thus, there are no zero parameters. So, the presented approach finetunes all the parameters. For the sake of completeness, Figure 10 shows the comparison of the results for finetuning the full model versus the partial model(weights retained during pruning). It can be observed that there is no significant difference between the two approaches."}, {"title": "Run-time analysis", "content": "CROP is a static personalization approach that requires one-time on-device training of the generic model during personalization. Table 13 shows the amount of resources required in terms of time, memory, and power for two health-related datasets: PERCEPT-R and Stress-Sensing. According to our evaluation, except for the scalable device, Jetson Nano, the training time is a couple of seconds, and resource constraints are minimal. But on scalable devices like Jetson Nano, computation time takes minutes. However, it is feasible since it is a one-time computation. In contrast, the continuous learning approaches Wang et al. [2022a], Liang et al. [2020], Wu et al. [2024], Daniels et al. [2023], Fini et al. [2022], Tang et al. [2024a,b], Mallya and Lazebnik [2018], Mallya et al. [2018], Wang et al. [2022b] require repeated adapta-"}, {"title": "Top prune vs. lower prune", "content": "Bartoldson et al. [2020] recommends that generalization can be achieved by pruning the top weights of the model and retraining them. This pushes the model towards flatter landscapes, which helps generalize. Although the resulting models are better than the conventionally personalized models, but, Figures 7(a) and 7(b) show that models obtained by pruning top 1% weights perform significantly worse than models obtained using CROP for all users in both scenarios. This can be attributed to the fact that pruning top weights leads to a loss of user-specific information learned by the personalized model during step 2. Although replacing these with parameters with generic weights does result in a little improvement in unseen context $C_u$, it results in significant degradation for the available context $C_a$ as compared to the conventionally-finetuned model. Since these new weights were important for context $C_a$, the final finetuning stage, which retrains the model with the available context $C_a$, overwrites this generic information. Thus, nullifying the effect of the approach."}]}