{"title": "Controllable Protein Sequence Generation with LLM Preference Optimization", "authors": ["Xiangyu Liu", "Yi Liu", "Silei Chen", "Wei Hu"], "abstract": "Designing proteins with specific attributes offers an important solution to address biomedical challenges. Pre-trained protein large language models (LLMs) have shown promising results on protein sequence generation. However, to control sequence generation for specific attributes, existing work still exhibits poor functionality and structural stability. In this paper, we propose a novel controllable protein design method called CtrlProt. We finetune a protein LLM with a new multi-listwise preference optimization strategy to improve generation quality and support multi-attribute controllable generation. Experiments demonstrate that CtrlProt can meet functionality and structural stability requirements effectively, achieving state-of-the-art performance in both single-attribute and multi-attribute protein sequence generation.", "sections": [{"title": "Introduction", "content": "The objective of protein design (Jumper et al. 2021; Zheng et al. 2023) is to create proteins with specific biochemical functions. Designing and exploring novel proteins offers a highly promising approach to addressing challenges in various fields, e.g., drug discovery (\u015aled\u017a and Caflisch 2018; Cao et al. 2022), vaccine and enzyme design (Correia et al. 2014; Richter et al. 2011). Generating high-quality proteins that not only possess the desired functions but also exhibit structural stability has become increasingly important. Recently, several works based on deep models have achieved notable success in protein sequence generation (Ferruz, Schmidt, and H\u00f6cker 2022; Nijkamp et al. 2023) using protein large language models (LLMs). They leverage the similarities between protein sequences and natural language to generate biologically relevant and functional protein sequences with high accuracy.\nDirectly finetuning protein language models is an effective way to constrain their outputs to meet the specific function attributes (Luo et al. 2023; Nathansen et al. 2024). While some works (Wang et al. 2023; Fang et al. 2024) align protein space with natural language space to achieve controllable outputs, yielding promising results in protein sequence understanding (Zhuo et al. 2024), finetuning on downstream tasks remains essential to ensure the generation of specific functional proteins (Lv et al. 2024).\nFinetuning LLMs for controllable protein generation has two major challenges. First, existing works based on finetuning do not explicitly constrain structural stability and functionality during training. As a result, the finetuned model may still generate proteins that are functionally irrelevant or incapable of folding into stable structures. Second, the effectiveness of finetuning depends heavily on the quality and quantity of the dataset. We analyze the protein number of each term in Gene Ontology (GO) annotations\u00b9, which provides descriptions of a protein's molecular functions, biological processes, and cellular components. As shown in Fig. 1(a), most terms have data quantities concentrated below 5,000, and there are very few terms with a sufficient number of proteins. Furthermore, we randomly sample 100 terms and count the number of proteins with shared attributes between any two terms. The results in Fig. 1(b) show that fewer proteins meet both attributes, as indicated by the lighter color in the intersection of the heatmap. So, it is harder to construct sufficiently large data for finetuning when aiming to generate proteins with multiple attributes.\nTo address the issues mentioned above, we propose a preference optimization-based method to enhance the quality of controllable protein sequence generation. We design two metrics to assess the protein's structural stability and functionality. For functionality, given the critical relationship be-"}, {"title": "Related Work", "content": "Protein language models are widely used in protein sequence analysis, function prediction, and protein design. Encoder-based protein language models like ESM (Rives et al. 2021; Lin et al. 2023a), ProtBERT (Elnaggar et al. 2021) and ProtT5 (Elnaggar et al. 2021) are trained on large-scale protein sequence data and capable of capturing the semantic information and latent structural features within protein sequences. Decoder-based models, such as ProtGPT2 (Ferruz, Schmidt, and H\u00f6cker 2022) and Pro-Gen2 (Nijkamp et al. 2023), have shown promising results in protein sequence generation. Recent works also explore using diffusion models for generating protein sequences (Alamdari et al. 2023; Zongying et al. 2024).\nThe most straightforward method to controllable generation using protein language models is to finetune the model on downstream data (Luo et al. 2023; Nijkamp et al. 2023; Nathansen et al. 2024). It allows to generate proteins with specific functions. Some works aim to guide the model's generation using natural language instructions. For example, recent studies (Fang et al. 2024; Wang et al. 2024, 2023) construct instruction-tuning datasets based on functional labels or descriptions to align models with natural language, achieving promising results in protein understanding but limited effectiveness and incomplete evaluation on sequence generation. ProLLaMA (Lv et al. 2024) incorporates continual learning of protein sequences before instruction-tuning, yielding better results on generation, but it still requires finetuning on downstream data to improve performance. However, finetuning does not explicitly focus on structural stability and functionality, which affects the quality of the generated sequences. To address it, we propose a preference optimization method to further enhance generation quality."}, {"title": "Preference Optimization", "content": "The reinforcement learning from human feedback (RLHF) framework has significantly improved model performance on downstream tasks by aligning with human preferences. Some researchers have shifted towards alternative methods to reduce the complexity of the RLHF framework. Direct preference optimization (DPO) (Rafailov et al. 2023) directly aligns the behavior of language models without using a reward model. ORPO (Hong, Lee, and Thorne 2024) and SimPO (Meng, Xia, and Chen 2024) further simplify the process by eliminating the reference model. Some works also focus on modeling sequence data. For example, PRO (Song et al. 2024) proposes a list maximum likelihood estimation loss for response lists. Lipo-X (Liu et al. 2024) introduces a training weight to represent the ranking difference. However, they are unsuitable when generating a large number of protein sequences, which is costly for PRO and may lead to an overemphasis on top sequences for Lipo-\u03bb. Additionally, some works (Meng, Xia, and Chen 2024; Park et al. 2024; Qin, Feng, and Yang 2024) explore the use of regularization terms as a margin to increase the reward difference between preferred and non-preferred data, thereby enhancing training effectiveness."}, {"title": "Preliminaries", "content": "RLHF. In the RLHF process (Ziegler et al. 2019; Liu et al. 2020), an LLM is first finetuned with instructions, resulting in model $t_{ref}$. The output of $t_{ref}$ is sampled to obtain responses $y_1, y_2 \\sim t_{ref}$. Humans then annotate and rank the quality of $y_1$ and $y_2$, and create the preference optimization dataset of $D = \\{(x^{(i)}, y_w^{(i)}, y_l^{(i)})\\}_{i=1}^N$, where $y_w$ and $y_l$ denote the preferred and rejected responses, respectively. Based on the Bradley-Terry model (Bradley and Terry 1952), the reward model $r_{\\phi}(x, y)$ is used to model the preference distribution."}, {"title": "Methods", "content": "In this section, we provide a detailed description of our proposed method CtrlProt. The framework is shown in Fig. 2. The goal of CtrlProt is to enhance the structural stability and functionality of LLM controllable sequence generation with preference optimization. Our method is divided into three main steps. First, we finetune an LLM on a protein sequence dataset with specific attributes. Then, we generate and evaluate candidate sequences with functionality and stability metrics to build preference optimization datasets. Finally, we perform multi-listwise preference optimization to improve the performance of the LLM.\nSupervised Finetuning\nSuppose that we have several protein sequences related to the attribute A. We use prefix-tuning (Li and Liang 2021) for supervised finetuning on the attribute A. Let $y = (a_1,..., a_l)$ be a protein sequence with l amino acids. The generation probability of y can be formulated as $p(x) = \\prod_{i=1}^l P(a_i | a_{<i})$ and we optimize the LLM by minimizing the negative log-likelihood as follows:\n$L_{sft} = -\\sum_{i=1}^k log p_{\\theta} (a_i | a_{<i}, P_A).$ (5)\nwhere PA denotes the prefix related to the attribute A.\nAlthough prefix-tuning can effectively leverage pre-trained knowledge and finetune data to generate sequences related to the attribute A, we observe from the experimental results that solely finetuning on attribute sequences may still result in protein sequences with poor structural stability and functionality. This indicates that finetuning alone is insufficient to fully leverage the model's understanding of protein structural semantics acquired during pre-training. Therefore, we formulate this issue of improving the quality of the generated proteins as a preference optimization problem on structural stability and functionality.\nDPO Data Construction\nWe generate abundant candidate sequences from the prefix-tuned model $\\pi_{ref}(\\cdot|P_A)$. To fully evaluate the quality of sequence $y_i \\sim \\pi_{ref}(\\cdot|P_A)$, we design two dimensions of evaluation metrics for stability and functionality."}, {"title": "Stability.", "content": "The Rosetta energy function (Alford et al. 2017) is a widely used metric for assessing conformational energy which reflects the structural stability of the protein (Nijkamp et al. 2023; Ferruz, Schmidt, and H\u00f6cker 2022). It includes interactions and force fields like van der Waals forces, charge interactions, hydrogen bonds, and virtual side-chain conformations (Alford et al. 2017). Typically, protein structures that achieve lower scores are more likely to approximate stable structures. To compare the stability between proteins with different lengths, we normalize the raw Rosetta scores by lengths and use the per-residue Rosetta score (Kim, Seffernick, and Lindert 2018) $e_i$ of $y_i$ to calculate the structural stability score $\\gamma_i$:\n$\\gamma_i = 1 - \\frac{e_i - e_{min}}{e_{max} - e_{min}},$ (6)\nwhere $e_{max}$ and $e_{min}$ are the maximum and minimum Rosetta energy scores of all sequences, respectively.\nFunctionality. The structural similarity between protein sequences indicates their functionality relevance (Hamamsy et al. 2024). We use a pre-trained encoder to obtain representations from the protein structures and measure the functionality relevance between $y_i$ and the sequences in the training set based on the similarity of their structural representations. The functionality score $\\tau_A$ is:\n$\\tau_A = \\frac{1}{M} \\sum_{j=1}^M cos\\left(Encoder(y_i), Encoder(y_{train})\\right),$ (7)\nwhere $cos(\\cdot)$ measures cosine similarity and $y_{train}$ is from the training set containing M sequences. We use Protein-MPNN (Dauparas et al. 2022) as the structural encoder.\nBased on the two scores $\\gamma_i$ and $\\tau_A$, we can build the preference optimization dataset $D_A$ for the attribute A:\n$D_A = \\{(\\mathbf{y}_w, \\mathbf{y}_l, \\gamma_w, \\gamma_l, \\tau_w, \\tau_l) | \\gamma_w > \\gamma_l, \\tau_w > \\tau_l\\},$ (8)\nwhere $\\mathbf{y}_w, \\mathbf{y}_l \\sim \\pi_{ref}(\\cdot|P_A)$ and both scores of $\\mathbf{y}_w$ are greater than $\\mathbf{y}_l$ in $D_A$."}, {"title": "Multi-listwise DPO", "content": "Since DA is obtained by extensive sampling and simultaneous evaluation of functionality and structural stability, our preference optimization method should also consider both multidimensionality and sequentiality.\nSequentiality. All sequences are evaluated based on determined scores, which can be compared and ranked. It is natural to incorporate ranking information into DPO, making the process more attentive to pairs with significant differences while reducing the intensity of optimization for pairs with minor differences (Song et al. 2024). Here we use $G(y_i, \\gamma_i)$ to calculate the weighted stability score of $y_i$:\n$G(y_i, \\gamma_i) = F(i)(2\\gamma_i - 1),$ (9)\nwhere $F(\\cdot)$ denotes the cumulative distribution function of the distribution of $\\gamma$, which provides the rank of $\\gamma_i$ within the entire dataset. Even when scores are relatively concentrated, $F(i)$ can still help capture and utilize the information of the subtle differences between samples. We use the beta distribution to fit the score and calculate $F(i)$."}, {"title": "Multidimensionality.", "content": "We aim to simultaneously consider the functionality and structural stability of protein sequences during the optimization process. Thus, the difference between chosen and rejected sequences should be considered by $\\tau$ and $\\gamma_i$, respectively. We obtain the quality score $p(y_i)$ of the sequence $y_i$ by\n$p(y_i) = G(y_i, \\gamma_i) + G(\\gamma_i, \\tau_i),$ (10)\nwhere $G(y_i, \\tau_i)$ is generated in the same way as Eq. (9).\nPreference optimization. We rewrite the optimization objective based on Eq. (2) with the quality score $p(\\cdot)$:\n$\\underset{\\pi_{\\theta}}{\\text{max}} \\mathbb{E}_{x\\sim D, y \\sim \\pi_{\\theta}(y|x)} [r^*(x, y) + p(y)] - \\beta D_{KL} [\\pi_{\\theta}(y|x) || \\pi_{ref}(y | x)],$ (11)\nwhere we use $r^*(x, y) + p(y)$ equivalently replaces the originally hypothesized latent reward function.\nFollowing the same derivation in DPO, we get the mapping from reward functions to optimal policies:\n$r^*(x, y) = \\beta \\log \\frac{\\pi_{\\theta}(y|x)}{\\pi_{ref}(y|x)} + \\beta \\log Z(x) - \\rho(y).$ (12)\nFinally, substituting $r^*(x, y)$ into Eq. (1), we eliminate the partition function Z(x) and obtain the final loss:\n$L_{MLPO} (\\pi_{\\theta}; \\pi_{ref}) = -\\mathbb{E}_{x \\mathbf{y}_w, \\mathbf{y}_l \\sim D} [\\log \\sigma (\\beta (\\log \\frac{\\pi_{\\theta}(\\mathbf{y}_l | x)}{\\pi_{ref}(\\mathbf{y}_l | x)} - \\log \\frac{\\pi_{\\theta}(\\mathbf{y}_w | x)}{\\pi_{ref}(\\mathbf{y}_w | x)} - \\alpha(\\rho(\\mathbf{y}_w) - \\rho(\\mathbf{y}_l))))],$ (13)\nwhere we add $\\alpha$ to adjust the intensity. $\\alpha (\\rho(\\mathbf{y}_w) - \\rho(\\mathbf{y}_l))$ denotes the difference between preference optimization pairs and as a regularization term, influences the training. Specifically, when $\\mathbf{y}_w$ and $\\mathbf{y}_l$ have a significant difference, the gradient on the pairs during training will increase. Conversely, when $\\mathbf{y}_w$ and $\\mathbf{y}_l$ are similar in functionality and stability, the training gradient will decrease (Park et al. 2024). This can also be understood from a margin perspective (Amini, Vieira, and Cotterell 2024) that pairs with larger differences between $\\mathbf{y}_w$ and $\\mathbf{y}_l$ will be penalized more heavily.\nGeneralization to Multi-attribute Generation\nAssuming we have sufficient protein sequences with multiple attributes, it can be treated as a generation task similar to the single-attribute scenario and optimized using the aforementioned method. However, in most cases, the amount of protein data with multi-attribute labels is limited, making it difficult to directly optimize with a multi-attribute dataset. Therefore, we aim to extend the above method and leverage several single-attribute datasets to enhance the effectiveness of multi-attribute generation.\nSuppose that we need to generate protein sequences with K attributes. After prefix-tuning, we can get the prefix for single attribute $P_1, P_2, ..., P_K$. For multi-attribute generation, a naive method is to directly concatenate all the prefixes and use $P_{multi} = [P_1; P_2; ...; P_K]$ to generate sequences with all K attributes (Luo et al. 2023)."}, {"title": "Experiments and Results", "content": "Experiment Setup\nDataset construction. We extract protein sequences with Gene Ontology (GO) terms from the UniProtKB database\u00b2 and corresponding structures from the AlphaFold protein structure database\u00b3. We choose six terms as attributes from three different aspects: molecular function ontology (MFO): metal ion binding and RNA binding; biological process ontology (BPO): phosphorylation and translation; cellular component ontology (CCO): cytoplasm and nucleus. Each attribute contains 10k protein sequences for training.\nEvaluation metrics. To evaluate the quality of sequences comprehensively, we use CLS-score, TM-score, and RMSD to assess their functionality and pLDDT for structural stability. For each attribute, we extract 100k sequences from UniProtKB as the evaluation set, excluding the training set to ensure no data leakage. We construct a database for each attribute on the evaluation set for alignment with Foldseek (Van Kempen et al. 2024). CLS-score: We finetune a classifier based on the ESM-2 (Lin et al. 2023a) model on the evaluation set, using the classification probabilities as the classifier score (CLS-score). TM-score and RMSD: Following previous works (Lv et al. 2024; Zongying et al. 2024), we use Foldseek (Van Kempen et al. 2024) to assess structural similarity with Template Modeling score (TM-score) (Zhang and Skolnick 2004) and Root Mean Square Distance (RMSD) (Betancourt and Skolnick 2001). Higher CLS-score or TM-score, or a lower RMSD, indicate greater structural similarity between the generated sequences and the evaluation set. pLDDT: The predicted Local Distance Difference Test (pLDDT) is used to assess the confidence of protein structure predictions. A higher pLDDT indicates higher prediction confidence and greater structural stability.\nBaselines. We compare CtrlProt to six competitive baselines: The encoder-based methods including ESM-1b (Rives et al. 2021) and ESM-2 (Lin et al. 2023a). The diffusion-based model is EvoDiff (Alamdari et al. 2023). We also compare with the state-of-the-art decoder-based methods. PrefixProt (Luo et al. 2023) uses prefix-tuning to finetune Prot-GPT2 (Ferruz, Schmidt, and H\u00f6cker 2022). ProGen2 (Nijkamp et al. 2023) is pre-trained on a large corpus of protein sequences, and we finetune it using the same prefix-tuning setting. ProLLaMa (Lv et al. 2024) leverages the alignment of natural language to generate corresponding protein sequences. All baselines are finetuned on the training set.\nSettings. For prefix-tuning, we finetune ProtGPT2 with following settings: batch size (16), learning rate (1e-4), prefix token number (100). For preference optimization, we use 5k pairs on each attribute and set the learning rate (5e-5), \u03b2 (0.1), and a (0.05). The maximum generation length is 400. We use ProteinMPNN as the structural encoder and ESM-Fold (Lin et al. 2023b) for structure prediction, both with default parameters. The Rosetta score is calculated using the weight configuration of ref2015 (Park et al. 2016). All training and generation are conducted on a single A800 GPU."}, {"title": "Single-attribute Generation Results", "content": "To assess our method CtrlProt's performance in controllable sequence generation, for each attribute, we generate 500 sequences using our method and baselines. We compare the generated results with natural proteins from the evaluation set using Foldseek. The results, as shown in Table 1, indicate that our method outperforms the baselines on six single-attribute controllable generation datasets. Notably, CtrlProt exhibits a significant advantage in pLDDT and TM-score, suggesting that our generated sequences have greater structural stability, and are structurally more similar to natural proteins with the same attributes, which implies similar functionality. This verifies that CtrlProt effectively improves the quality of the generated proteins.\nOverall, the performance of decoder-based models after finetuning is superior to encoder-based models, highlighting their significant advantage in sequence generation tasks. ESM-1b, ESM-2, and EvoDiff generate sequences by predicting masked tokens. It is challenging to generate protein sequences from scratch. Therefore, we provide 10% of amino acids to assist in the generation process. It is worth noting that, while ProLLaMA has a larger number of parameters, its performance is not particularly remarkable. This may be because its original instruction tuning limits the sequence length to 256. Although we do not impose such a restriction, it is still affected, leading to decreased performance in generating longer protein sequences."}, {"title": "Ablation and Alternative Results", "content": "We perform ablation and alternative studies to further analyze the effectiveness of CtrlProt. As presented in Table 2, we list the average results across six single-attribute datasets. In the ablation study, we separately remove the functionality metric \u03c4 and the structural stability metric \u03b3, and both result in a decline in CtrlProt's performance. Specifically, removing \u03b3 leads to a more significant drop in pLDDT, while removing \u03c4 causes larger decreases in CLS-score, TM-score, and RMSD. Therefore, \u03c4 and \u03b3 play a crucial role in enhancing functionality and structural stability, respectively.\nIn the alternative study, we focus on comparing several preference optimization methods: DPO, ORPO, and Lipo-\u03bb. Overall, our multi-listwise preference optimization outperforms all other methods. Compared to DPO and ORPO, CtrlProt introduces the difference of the quality scores as a regularization term, allowing our method to pay more attention to the pairs with greater quality differences. This leads to superior results in protein sequence data. On the other hand, Lipo-\u03bb, which also employs a listwise method, performs less effectively. This is because Lipo-\u03bb uses the difference of the reciprocal of ranking in its lambda weight. When it comes to ranking and comparing in a large number of protein sequences, it overly emphasizes a few top-ranked proteins and reduces the distinction among lower-ranked proteins. CtrlProt incorporates the CDF of the quality score p to introduce ranking information and avoids this issue."}, {"title": "Diversity Analysis", "content": "To further verify that CtrlProt can generate high-quality proteins without overfitting to the training set or experiencing mode collapse (Shumailov et al. 2024), we analyze and compare its diversity with other baselines. Following previous works (Kirk et al. 2024; Li et al. 2016), we use n-gram to calculate the similarity ratio between two sequences:\n$Sim(y_i, y_j) = \\frac{| Set(y_i) \\cap Set(y_j) |}{|Set(y_i)|},$ (15)\nwhere Set() is the set of all 3-gram items. We report the inter-output similarity for the similarity among all generated sequences, and the training set similarity, which shows similarity between generated sequences and the training set.\nThe results, as shown in Table 3, indicate that CtrlProt gains the lowest similarity to the training set, close to natural proteins, which suggests that CtrlProt does not achieve optimal performance through overfitting. Although the inter-output similarity of CtrlProt slightly increases compared to PrefixProt, it remains within a relatively optimal range. It indicates that no mode collapse occurs during preference optimization, meaning CtrlProt does not resort to generating only a few patterns of proteins to obtain optimal results."}, {"title": "Case Study", "content": "In Fig. 3, we present proteins generated by CtrlProt (shown in blue) and the most similar natural proteins (shown in yellow). We use Sequence Identity (Seq.Id.) from foldseek to reflect the sequence similarity. The significant overlap in 3D structures and high TM-scores confirm structural similarity between the generated and natural proteins. We observe that these natural proteins also satisfy the corresponding attributes, indicating functional similarity between the generated and natural proteins. The lower Seq.Id. indicates lower amino acid sequence similarity, which means CtrlProt can generate desired attribute proteins with novel sequences."}, {"title": "Multi-attribute Generation Results", "content": "CtrlProt can be extended to multi-attribute generation. To validate the effectiveness, we construct six attribute combinations. Due to the lack of studies on multi-attribute generation, we follow PrefixProt by comparing with two straightforward methods: Average, where the prefixes are averaged and merged, and Concat, where the prefixes are concatenated for generation. We adopt the same metrics used in single-attribute generation. The results, as shown in Fig. 4, demonstrate that CtrlProt achieves higher CLS-score and TM-score, as well as lower RMSD, indicating a significant improvement in functionality. The higher pLDDT suggests stronger structural stability in all attribute combinations.\nIn Fig. 5, we present four cases of two attribute combinations. The overlap in structures and high TM-scores shows that the generated proteins exhibit structural similarity to natural proteins with related attributes while maintaining low sequence similarity, based on low Seq.Id."}, {"title": "Conclusion", "content": "In this paper, we present CtrlProt, a preference optimization-based method that improves the quality of controllable protein sequence generation. We propose multi-listwise preference optimization on functionality and structural stability metrics, targeting pairs with notable quality differences. Experiments show that CtrlProt excels in single-attribute and multi-attribute generation and can generate diverse proteins. However, achieving more precise and programmable generation for certain attribute combinations remains a challenge. We hope to continue exploring this in future work."}]}