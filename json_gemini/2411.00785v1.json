{"title": "IGOR: Image-GOal Representations", "authors": ["Xiaoyu Chen", "Junliang Guo", "Tianyu He", "Chuheng Zhang", "Pushi Zhang", "Derek Yang", "Li Zhao", "Jiang Bian"], "abstract": "We introduce Image-GOal Representations (IGOR), aiming to learn a unified, semantically consistent action space across human and various robots. Through this unified latent action space, IGOR enables knowledge transfer among large-scale robot and human activity data. We achieve this by compressing visual changes between an initial image and its goal state into latent actions. IGOR allows us to generate latent action labels for internet-scale video data. This unified latent action space enables the training of foundation policy and world models across a wide variety of tasks performed by both robots and humans. We demonstrate that: (1) IGOR learns a semantically consistent action space for both human and robots, characterizing various possible motions of objects representing the physical interaction knowledge; (2) IGOR can \u201cmigrate\u201d the movements of the object in the one video to other videos, even across human and robots, by jointly using the latent action model and world model; (3) IGOR can learn to align latent actions with natural language through the foundation policy model, and integrate latent actions with a low-level policy model to achieve effective robot control. We believe IGOR opens new possibilities for human-to-robot knowledge transfer and control.", "sections": [{"title": "1 Introduction", "content": "Learning foundation models for embodied AI has been notably constrained by a lack of interaction data. Unlike text or video data, which are abundantly available, interaction data is much scarcer. Research efforts have been devoted to creating large-scale interaction datasets, such as Open-X-Embodiment (Collaboration et al., 2023) and DROID (Khazatsky et al., 2024). Based on multi-task interaction data, a series of generalist agents (or foundation policy models) have been proposed, such as RT-1 (Brohan et al., 2022), Robocat (Bousmalis et al., 2023), RT-2 (Brohan et al., 2023), Octo (Team et al., 2024), and OpenVLA (Kim et al., 2024). However, the volume of interaction data remains several orders of magnitude smaller than that of internet text or video data. Given that the success of foundation models relies on scaling up datasets and extracting knowledge from such large-scale datasets, it is essential to design methods for building embodied AI foundation models that can effectively utilize internet-scale video data.\nInternet-scale video data contains abundant sequential records of human activities and perfect demonstrations of how human perform various tasks by interacting with the real world. When the human brain extracts information from videos, instead of doing it frame by frame, it modularizes the differences between frames into a single word such as \u201cmove\u201d, \u201copen\u201d, \u201cclose\u201d. We refer to these highly compressed, modularized actions as latent actions that are shared across different tasks. The question to ask here is, is it possible to recover latent actions from video datasets with humans and robots performing various real embodied AI tasks? While recent works such as Genie (Bruce et al., 2024) and LAPO (Schmidt & Jiang, 2023) made attempts in recovering such latent actions from videos, they primarily focus on 2D platformer games where each latent action $a_t$ corresponds to a specific control button. The action space is highly designed to fit a specific scenario and incomparable to the complex human and robot action space in various embodied AI tasks. To take a step further, the question would be, can we learn a unified, semantically consistent latent action space, allowing the transfer of knowledge across different tasks, and embodiments including human and various robots?\nIn this paper, we propose Image-GOal Representations (IGOR), which learns a unified and semantically consistent latent action space shared across different tasks and embodiments, enabling the knowledge transfer among internet-scale video data. We propose a latent action model designed to capture robot and human actions across various embodied AI tasks. IGOR compresses the visual changes between an image and its goal state into latent actions, which are also embeddings of sub-tasks defined by reaching the goal from the initial image. IGOR is trained by minimizing the reconstruction loss of the goal state, which is predicted based on the image and the latent action. The core insight behind IGOR is that if compressed sufficiently, the image-goal pairs with similar visual changes will have similar embeddings.\nWith the latent action model, we can transform internet-scale human video data into interaction data labeled with latent actions, which largely expands the data available to building embodied AI foundation models. This unified latent action space allows us to train foundation policy and world models on nearly arbitrary tasks performed by robots and humans. Specifically, we train a foundation policy model on large-scale video data with text labels. This model uses text to describe tasks and makes decisions, generating the next latent action to perform. Additionally, we train a foundation world model on the same dataset, learning to simulate the outcome of executing the foundation policy model. Image-Goal Representations can be viewed as atomic control units in visual space. They function both as latent actions for a foundational policy model to predict in visual trajectory planning and as sub-tasks for a robot-specific low-level policy to execute."}, {"title": "2 Methodology", "content": "2.1 Latent Action Model\nThe primary objective of the latent action model is to label latent actions from unlabeled open-domain videos in an unsupervised manner. Given a sequence of video frames $o_{1:t+1}$, the goal is to derive the latent action $a_t$, which captures the key information describing only the changes that occur at time step $t$, removing other redundant information. In contrast to prior works (Schmidt & Jiang, 2023; Bruce et al., 2024), which primarily focus on 2D platformer games where each latent action $a_t$ corresponds to a specific control button, we aim to develop a more generalizable model. Our model is designed to handle the significantly greater complexity of open-world scenarios, where latent actions may not correspond to any specific underlying actions. This presents several additional challenges.\nFirst, rather than focusing solely on absolute position of pixel changes, the latent action model must learn to capture semantic movements that remain consistent across varying scenarios. Moreover, due to the temporal redundancy, actions are often sparse given long contexts, which can lead the model to infer $o_{t+1}$ directly from the history, bypassing the need for a more informative latent action $a_t$.\nTo address these issues, we propose a novel model architecture. Our latent action model consists of a pair of Inverse Dynamics Model (IDM) and Forward Dynamics Model (FDM). IDM $I$ is trained to predict the latent action $a_t$ based on the full sequence of observations $o_{1:t+1}$. Instead of using the raw observations, we first apply random cropping $c_1$ to the inputs: $a_t = I (c_1[o_{1:t+1}])$. For the architecture of $I$, we first extract features for each frame through Vision Transformer (ViT) (Dosovitskiy et al., 2021) and then adopt a Spatio-Temporal transformer (ST-transformer) (Bruce et al., 2024; Xu et al., 2021) with a temporal causal mask as the backbone. Learnable readout tokens are then used to extract and compress the visual changes into $N$ tokens. To further compress the information stored in latent action, we apply vector quantization to each token, restricting them to a discrete codebook of size $|C|$. Finally, we derive the latent action $a_t \\in \\mathbb{R}^{N \\times D}$ where $D$ is the dimension of each code. We refer to $a_t$ as the latent action embedding, or sub-task embedding, as they describe the information that takes the observation of to the next observation $o_{t+1}$.\nFor the FDM $F$, we propose using a single-frame Vision Transformer to reconstruct $o_{t+1}$, in contrast to previous works (Schmidt & Jiang, 2023; Bruce et al., 2024), which reconstruct the next frame given the entire context $o_{1:t}$. This approach mitigates the case where the model might predict the next frame directly from the context, bypassing the latent action. By conditioning on a single frame, it encourages more information to flow into the latent action $a_t$. For reconstruction, we apply another random cropping $c_2$, and the next frame is predicted as $\\hat{o}_{t+1} = F (c_2[o_t], a_t)$. By using different croppings $c_1$ and $c_2$, the model is encouraged to learn a more semantically invariant latent action across different trajectories. The models are trained jointly with the reconstruction loss $||c_2[o_{t+1}] - \\hat{o}_{t+1}||_2$ and the commitment loss in vector quantization.\n2.2 Foundation World Model\nOur foundation world model is a continuous-time Rectified Flow (Liu et al., 2023b; Esser et al., 2024) that learns to predict the future frames $o_{t+1:T}$ conditioned on the history observation frame $o_{1:t}$, and future latent actions $a_{t:T-1}$. To achieve this goal, there are two key challenges: 1) Generating the photo-realistic frame that describes the states precisely; 2) Controlling the generated frames by the latent actions.\nAccordingly, we start our foundation world model with the pre-trained Open-Sora (Zheng et al., 2024). It consists of two components: a 3D Variational AutoEncoder (VAE) that encodes the raw observation into latent space with 8 \\times 8 times downsampling in spatial dimension and 4\\times times downsampling in temporal dimension; a Spatial-Temporal Rectified Flow Transformer (ST-RFT) that generates the latent from the text conditions. To enable the control from the observation and action, we make two modifications to the original Open-Sora: 1) We replace the original text input of the pre-trained model with our latent actions $a_{1:T}$ obtained from LAM. Zero-padding is applied for the last action. For each frame, we map the latent actions into a single token and feed it to the ST-RFT via the cross-attention mechanism; 2) We also make the generation conditioned on the output of FDM $\\hat{o}_{t+1:T}$, which provides a coarse-grained prediction according to the input latent action. For the conditioning of $\\hat{o}_{t+1:T}$, we encode it to the latent space with the same 3D VAE and directly add it to the noisy input element-wise.\nFormally, Rectified Flow (Liu et al., 2023b; Albergo & Vanden-Eijnden, 2023; Esser et al., 2024) aims at"}, {"title": "2.3 Foundation Policy Model and Low-level Policy Model", "content": "The training of the policy model consists of two stages. In the first pretraining stage, taken as input the raw observation frames $o_{1:t}$ and a textual description $s$ for the task, the foundation policy model predicts latent actions $a_t = I([o_{1:t+1}])$ labeled by the IDM in the latent action model at each step. The training dataset of this stage is the same as that used for the latent action model, i.e., with large-scale and diverse sources of videos. In the second finetuning stage, we add an extra prediction component on the foundation policy model to predict real continuous robot actions, with taking the raw observations as well as the latent actions predicted by the first stage model as input. In this stage, only the prediction component (i.e., the low-level policy model) is optimized on small-scale and task-specific downstream datasets, while other components are frozen.\nSpecifically, similar to the latent action model, the backbone of foundation policy model is also a ST-transformer equipped with a ViT image encoder, with a feed-forward layer as the final prediction layer. The textual description $s$ is encoded to a latent representation by a pre-trained text encoder, which is concatenated with the observation representation encoded by the ViT encoder as the joint input to the model.\nWe use the L2 distance between the predicted hidden output and the latent action as the loss function. Given a trajectory consists of $t$ observations $o_{1:t}$, the training objective can be written as:\n$L_{policy} = ||P([s;o_{1:t}]) - a_t||^2,$\nwhere $P(\\cdot)$ denotes the policy model.\nIn the second stage, we train the low-level policy model to predict the real continuous actions within each latent action, where the image-goal latent actions can be seen as representations for sub-tasks defined by reaching a goal from an initial image. The low-level policy model is also an ST-transformer with a prediction layer. The input consists of the textual representation $s$, the observation $o_{1:t}$ and latent actions predicted by the foundation policy model $P([s;o_{1:t}])$, which are concatenated together at the patch level as one part. The latent action $P([s; o_{1:t}])$ predicted by the foundation policy model also serves as sub-task embedding for the low-level policy model. We denote that each latent action corresponds to $\\tau$ real robot actions, and the latent action $a_t$ corresponds to real robot action $u_t^1:t$. Denote the low-level policy model as $P_f(\\cdot)$, we train the second stage model also by L2 distance:\n$L_{ft} = ||P_f([s; P([s; 0_{1:t}]); 0_{1:t}]) - u_t^1:t||^2,$\nwhere only the parameters of the low-level policy are optimized."}, {"title": "3 Experiments", "content": "3.1 Dataset\nIn the pretraining stage, we construct a large-scale dataset comprising diverse domains, including robotic data from various embodiments and a substantial amount of human activity videos."}, {"title": "3.3 Qualitative Results on Latent Actions", "content": "We present qualitative results on latent actions learned from robotics and human activity dataset. Specifically, we answer the following questions on learned latent actions:\n\u2022 Do similar latent actions reflect similar visual changes?\n\u2022 Can latent actions encode semantically consistent changes across different tasks, and embodiments including human and robots? If so, are we able to migrate movements in videos across embodiments and tasks via latent action?\n\u2022 Does the policy foundation model properly follow language instructions for task solving?\n3.3.1 Visualization of Image-Goal Pairs with Similar Latent Actions\nWe investigate whether similar learned latent actions reflect similar visual changes on robotics manipulation dataset. We use RT-1 dataset, which was excluded from the latent action model training and serves as out-of-domain samples for evaluation. We randomly select image-goal pairs from RT-1 dataset, and present the image-goal pairs with smallest euclidean distance in latent action embedding in RT-1 dataset in Figure 3. We observe that pairs with similar embeddings indeed have similar visual changes, and also similar sub-tasks in semantic, for example, \"open the gripper\u201d, \u201cmove left\u201d, and \u201cclose the gripper\". Furthermore, each sub-task appears in different raw language tasks, suggesting the latent actions are reused, thereby facilitating generalization in model learning.\n3.3.2 Controllability of Latent Actions\nWe demonstrate that latent actions are able to control the changes in objects on different real world scenes, and the effects of latent actions generalize across tasks and embodiments. Specially, the generalizability of latent actions enable IGOR to successfully migrate human movement videos into robot movements provided the initial image, despite they largely differ in embodiments.\nObject Controllability Among Multiple Objects. We evaluate the controllability of the latent actions on object movements among multiple objects on the same image. In Figure 4, we generate subsequent"}, {"title": "3.4 Quantitative Results", "content": "3.4.1 Evaluation on the Google Robot Tasks in SIMPLER\nWe evaluate our IGOR-based training framework on the Google robot tasks in the SIMPLER simulator under a low-data regime, utilizing only 1% of the data from the large RT-1 dataset for the low-level policy learning stage.\nEvaluation Setups. We test different model's ability to control the Google Robot following language tasks with RGB images as observations, where all robots are controlled with low-level end-effector control actions, after finetuning on the same amount of data from RT-1 dataset. We evaluate the success rate on three tasks: \u201cPick Coke Can\u201d, \u201cMove Near\u201d, and \u201cOpen / Close Drawer\u201d.\n3.4.2 Predictiveness of Latent Actions on Robot Actions\nWe analyze whether our learned latent actions are predictive of real robot actions. On RT-1 dataset, we randomly sample a number of $M = 15,000$ pairs of images, and compute their latent action embeddings. For each pair of image, we find $N$ nearest neighbours of image pairs in RT-1 dataset with the closest latent action embedding, and compute the standard deviation of real robot actions among $N$ neighbours on each action dimension, normalized by the standard deviation of robot actions over each dimension over the whole RT-1 dataset. By varying $N$, we assess whether closer latent actions correspond to more similar downstream actions.\nThe results are shown in Figure 6(b). The fact that smaller $N$ leading to lower normalized standard deviation, and all normalized standard deviation being below 1.0, show that the latent actions are predictive of real robot actions including robot movements, rotations and gripper actions. It is also shown that the latent actions are more predictive of the robot movement than rotations and gripper actions, suggesting that the IGOR learned action space reflects more information in robot movements than robot arm rotations and gripping."}, {"title": "3.5 Ablation Studies", "content": "We provide additional ablation studies on the pretraining dataset of latent action model, showing that using a mixture of robotics and human activity dataset benefits the generalization of latent action model. Detailed ablation studies results are provided in Appendix C."}, {"title": "4 Related Work", "content": "Foundation Agents for Robots Open-ended task-agnostic training and high-capacity neural network architectures have been recognized as key to the success of foundation models. In this context, a series of generalist agents have been proposed as the foundation policy models for robots (Brohan et al., 2022; Bousmalis et al., 2023; Brohan et al., 2023; Team et al., 2024; Kim et al., 2024). RT-1 (Brohan et al., 2022) contributes a large-scale multi-task dataset and a robotic transformer architecture,facilitating and assessing generalization across multiple tasks. RoboCat builds on Gato (Reed et al., 2022), further"}, {"title": "5 Conclusions, Limitations, and Future Work", "content": "In this paper, we propose IGOR, a novel training framework, taking the first step towards learning a unified action space for humans and robots in various embodied AI tasks.\nQualitatively, we demonstrate that:\n\u2022 IGOR learns similar representations for image pairs with similar visual changes.\n\u2022 The learned latent action has control over the next state given the current image.\n\u2022 The foundation world model acquires knowledge about objects and their potential movements.\n\u2022 The foundation policy model learns to follow instructions across different states.\nQuantitatively, we show that:"}, {"title": "A Dataset", "content": "We present the datasets used for pre-training in Table 1. In total, these datasets comprise approximately 0.8 million robot trajectories and 2.0 million filtered human activity video clips. The robot data ratios are from (Team et al., 2024).\nData Filtering We observed that video quality significantly affects the action model, particularly for human activities video. Excessive shakiness in videos can introduce visual changes between consecutive frames that are unrelated to the agent's actions.\nWe calculate the camera motion over the videos, and filter approximately 40% percent of open-world video data. For the remaining data, we further stabilize the videos. Although we retain only 60% percent of open-world video data, we find that the action model improves dramatically.\nFrame Interval A noticeable amount of visual changes is crucial for our latent action model. If we select two frames that are too close in time, the agent may barely move, resulting in visual changes that are not significant enough for inferring meaningful actions. Conversely, if the frames are too far apart, the changes might be too large to model accurately. To address this issue, we tune the sampling interval. For robot data, we choose frames that are three intervals apart, using $s_t$ and $s_{t+3}$ as the image-goal pair. For real world videos, we control the sampling. For real world data, we control the sample interval to be within [0.1s, 0.5s]. For the action and policy model, the context frames follow the same interval, ensuring that each pair of consecutive frames maintains this consistent spacing."}, {"title": "B Training Details", "content": "B.1 Latent Action Model Training\nThe latent action model uses an ST-transformer equipped with a frozen DINO-v2 pretrained ViT image encoder. The latent action model uses a patch size of 14, and a codebook with $N = 4$ tokens and size $|C| = 32$, each with an embedding size of $D = 128$. We train the latent action model with batch size $B = 512$, training iterations of 140K steps, and learning rate of 1.5e \u2013 4 with Adam optimizer.\nB.2 Foundation World Model Training\nWe start on the top of the OpenSora (Zheng et al., 2024) model with newly initialized projection layers. The foundation world model with batch size $B = 12$, training iterations of 48K, and learning rate of 1e - 4 with Adam optimizer.\nB.3 Foundation Policy Model and Low-Level Policy Model\nThe latent action model uses an ST-transformer equipped with a frozen DINO-v2 pretrained ViT image encoder, following the latent action model's image encoder. The foundation policy model consists of 12 layers of spatial and temporal attentions, each with 12 attention heads and hidden dimension as 768 and a patch size of 14. We use frozen CLIP features for text instructions. We pretrain the foundation policy model with batch size $B = 128$, training iterations of 124K, and learning rate of 1e \u2013 4 with Adam optimizer.\nFor the low-level policy model, we add extra layers on top of the foundation policy model. We use a sub-task length of $\\tau = 3$ for finetuning the low-level policy model on RT-1 dataset. We finetune the low-level policy model with batch size $B = 128$, training iterations of 32K, and learning rate of 1e \u2013 4 with Adam optimizer."}, {"title": "C Additional Ablation Results", "content": "C.1 Dataset ablation for Latent Action Model\nWe compare two different settings for the pre-training dataset: only use the robotic dataset (robot data), and use both robotic and human activity dataset (mixed data). We evaluate the validation loss on the latent action model on RT-1 dataset, which is held out from the pretraining dataset and serves for OOD evaluation. Validation loss of the latent action model assesses the extent to which the IDM and FDM can jointly generate latent actions and recover goal states from these latent actions conditioned on states on the unseen dataset. We find that the OOD validation loss is greatly reduced by adding human activity dataset. This may be due to the diversity of human videos, which comprise real daily life environments with lots of diverse backgrounds and objects. These results demonstrate that it is promising to leverage human data for improving robot tasks under the IGOR framework."}, {"title": "D Additional Highlight of Contributions", "content": "We would like to especially acknowledge Pushi Zhang's contributions, including his involvement from the very beginning in shaping the initial ideas for the IGOR project, his valuable support in experiment analysis and debugging, his dedicated efforts during the intense final stages of paper writing and refinement, and his crucial work in building the project webpage. We also would like to thank Kaixin Wang's valuable suggestions on paper title and great effort for preparing the well-designed paper template."}]}