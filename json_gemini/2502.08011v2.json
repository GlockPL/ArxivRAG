{"title": "Training-Free Safe Denoisers for Safe Use of Diffusion Models", "authors": ["Mingyu Kim", "Dongjun Kim", "Amman Yusuf", "Stefano Ermon", "Mi Jung Park"], "abstract": "There is growing concern over the safety of powerful diffusion models (DMs), as they are often misused to produce inappropriate, not-safe-for-work (NSFW) content or generate copyrighted material or data of individuals who wish to be forgotten. Many existing methods tackle these issues by heavily relying on text-based negative prompts or extensively retraining DMs to eliminate certain features or samples. In this paper, we take a radically different approach, directly modifying the sampling trajectory by leveraging a negation set (e.g., unsafe images, copyrighted data, or datapoints needed to be excluded) to avoid specific regions of data distribution, without needing to retrain or fine-tune DMs. We formally derive the relationship between the expected denoised samples that are safe and those that are not safe, leading to our safe denoiser which ensures its final samples are away from the area to be negated. Inspired by the derivation, we develop a practical algorithm that successfully produces high-quality samples while avoiding negation areas of the data distribution in text-conditional, class-conditional, and unconditional image generation scenarios. These results hint at the great potential of our training-free safe denoiser for using DMs more safely.", "sections": [{"title": "1. Introduction", "content": "Diffusion models (DMs) have emerged as a powerful class of generative models, consistently surpassing previous approaches on a variety of tasks, including text-to-image generation (Rombach et al., 2022), audio synthesis (Kong et al., 2021), video synthesis (Bar-tal et al., 2024), and protein design (Watson et al., 2023). A significant factor contributing to their success is the flexible and controllable sampling with guidance (Dhariwal & Nichol, 2021; Ho & Salimans, 2021). In particular, text-based guidance (Saharia et al., 2022) has played a key role in the success of modern text-to-image models (Rombach et al., 2022; Podell et al., 2024).\nDespite remarkable advancements, there is growing concern about the safety of content generated by these models. The first concern is regarding not-safe-for-work (NSFW) content generation. To tackle the concern, negative prompts (Gandikota et al., 2023a; Ban et al., 2024) have predominantly been used to guide models away from toxic text descriptions. Fine-tuning methods aimed at un-learning undesirable features (Gandikota et al., 2023a;b; Gong et al., 2024; Kim et al., 2024) have shown promise, too. However, their effectiveness is limited by adversarial attacks or jailbreaks that can circumvent safeguards (Zhang et al., 2024b; Yang et al., 2024).\nOther safety concerns include DMs' generation of copyrighted content and data of individuals who wish to be excluded (machine unlearning). These two concerns are closely related to DMs's exceptional ability to memorize training data (Carlini et al., 2023). While differentially private training (Dockhorn et al., 2023; Liu et al., 2024) could mitigate the danger of memorization, there is an inevitable performance drop due to the added noise to the training process.\nIn this work, we propose directly modifying the sampling trajectories of DMs such that the sampling trajectories adhere to theoretically safe distributions. The modification follows, what-we-call, safe denoiser, which is derived from the relationship (in Theorem. 3.2) between the expected denoised samples that are safe and those that are not safe. The final samples from the safe denoiser are theoretically guaranteed to be safe and away from the area to be negated. Based on this derivation, we develop a practical algorithm (in Algorithm 1) that approximates the theoretically safe denoiser to generate safe images or combined with existing negative prompting methods to enhance the safety of text-to-image generation.\nIn our experiments, we demonstrate that our safe denoiser achieves state-of-the-art performance in terms of its safe generation, in the tasks of concept erasing (a popular benchmark for avoiding NSFW images in text-to-image"}, {"title": "2. Preliminary", "content": "DMs generate samples through iterative decoding starting from random noise to data. This iterative process is a reverse of the forward data destruction process, given by $x_t = \\alpha_t x + \\sigma_t \\epsilon$, where x follows the data distribution $P_{data}(x)$ and $\\epsilon$ follows the noise prior distribution $\\mathcal{N}(0, I)$, which results in the perturbation kernel to be $q_t(x_t|x) = \\mathcal{N}(x_t; \\alpha_t x, \\sigma_t^2 I)$. The specific choice of the coefficients $\\alpha_t$ and $\\sigma_t$ determine the variant of DMs. Depending on these parameters, the model may be referred to as Denoising Diffusion Probabilistic Models (DDPM) (Ho et al., 2020), Elucidating Diffusion Models (EDM) (Karras et al., 2022), or flow matching (Lipman et al., 2022).\nRegardless of whether the model is trained with noise-prediction (Ho et al., 2020), data-prediction (Karras et al., 2022), or velocity-prediction (Salimans & Ho, 2022; Lipman et al., 2022), these approaches are fundamentally equivalent (Kingma et al., 2021; Kim et al., 2021). This paper adopts the data-prediction framework due to its most intuitive interpretation. In data-prediction, the model approximates the denoiser function, defined by\n$\\mathbf{E}_{data}[x | x_t] := \\int x \\frac{P_{data}(x) q_t(x_t | x)}{P_{data, t}(x_t)} dx = \\frac{1}{Z} \\int x P_{data}(x) q_t(x_t | x) dx$\n$\\frac{1}{Z} \\int x \\mathcal{N}(x_t; \\alpha_t x, \\sigma_t^2 I) dx \\approx \\frac{1}{\\alpha_t^2 + \\sigma_t^2} (x_t + \\sigma_t^2 \\epsilon_{\\theta}) = \\frac{\\alpha_t}{\\alpha_t^2 + \\sigma_t^2} (x_t - \\sigma_t \\theta)$\nwhere $P_{data,t}(x_t)$ is a marginal distribution of the noisy data distribution at time t, and $\\epsilon_{\\theta}$ and $\\theta$ are score-prediction and noise-prediction, respectively.\nDMs can be guided to produce samples (Dhariwal &\nNichol, 2021; Kim et al., 2022) that adhere more closely to a desired condition denoted by c. A common approach in modern DMs is classifier-free guidance (CFG) (Ho &\nSalimans, 2021). The model is trained to learn both the un-conditional denoiser $\\mathbf{E}_{data}[x | x_t]$ and the conitional denoiser"}, {"title": "3. Methodology", "content": "The negative prompt $c_-$ or the SLD prompt $c_{us}$ consist of a limited set of pre-selected words by humans, and therefore may not encompass all images intended to be negated. Consequently, instead of ensuring safety solely based on text prompt, we introduce a methodology that guarantees safety based on images, which operates orthogonally to existing text-based safety approaches. Furthermore, while text-based negative guidance can enhance safety, its application lacks a theoretical foundation, thereby offering no guarantees regarding the distribution of the samples. To address these issues, we propose constructing a sampling trajectory that adheres to the safe distribution by using a safe denoiser defined below.\n$\\mathbf{E}_{data}[x | x_t, c]$\nThe CFG modifies the sampling trajectory by\n$\\mathbf{E}_{data}[x | x_t] + \\lambda (\\mathbf{E}_{data}[x | x_t, c] - \\mathbf{E}_{data}[x | x_t])$\nallowing stronger alignment of the sample with the prompt c via the scale \u03bb. The purpose of the additional term is to guide the unconditioinal denoiser in the sharpening direction toward a desired condition c.\nNegative prompting (Liu et al., 2022) reverses the CFG gradient direction for an undesired prompt denoted by $c_{-}$. Formally, one replaces the standard CFG update with\n$\\mathbf{E}_{data}[x | x_t] + \\lambda (\\mathbf{E}_{data}[x | x_t, c_+] - \\mathbf{E}_{data}[x | x_t, c_-])$\nwhere $c_+$ denotes a positive condition and $c_\u2013$ represents a negative context, such as low quality, watermark, logo, etc.\nRecently, Schramowski et al. (2023) introduced Safe Latent Diffusion (SLD), a new type of guidance, given by\n$\\mathbf{E}_{data}[x | x_t] + \\lambda (\\mathbf{E}_{data}[x | x_t, c_+] - \\mathbf{E}_{data}[x | x_t]) - \\mu(c_+, c_{us}; \\gamma, \\lambda) (\\mathbf{E}_{data}[x | x_t, c_{us}] - \\mathbf{E}_{data}[x | x_t])$\nwhere $c_{us}$ represents a predefined set of unsafe prompts suggested by authors. Hypothetically, if we assume \u03bc was set to be \u03bb, the SLD guidance simplifies to\n$\\mathbf{E}_{data}[x | x_t] + \\lambda (\\mathbf{E}_{data}[x | x_t, c_+] - \\mathbf{E}_{data}[x | x_t, c_{us}])$\nInstead of directly using $c_{us}$ as $c_-$, SLD introduces an adaptive weight $\\mu(c_+, c_{us}; \\gamma, \\lambda)$ proportional to the\ndenoiser difference norm, defined as $||\\mathbf{E}_{data}[x | x_t, c_+] - \\mathbf{E}_{data}[x | x_t, c_{us}]||$. The magnitude of this norm serves as an indicator of the proximity of the sampling trajectory to the unsafe region. Specifically, a larger norm suggests that the trajectory is likely to be safe, whereas a smaller norm indicates potential unsafety."}, {"title": "3.1. Safe Denoiser", "content": "To define the safe denoiser, we first define an indicator function, $1_{safe}(x)$ taking the value of 1 if x is safe and 0 if not. Similarly, we define an indicator function, $1_{unsafe}(x)$ taking the value of 1 if x is unsafe and 0 if not. Hence, for each sample x, we have a constant function, taking the value of 1, defined by $1(x) = 1_{safe}(x) + 1_{unsafe}(x)$. Then, we define the following concepts.\nDefinition 3.1. The unnormalized safe distribution $p_{safe}(x)$\nis $1_{safe}(x)p_{data}(x)$. The safe denoiser is defined by\n$\\mathbf{E}_{safe} [x | x_t] = \\int x \\frac{p_{safe}(x) q_t(x_t | x)}{p_{safe, t}(x_t)} dx$\nwhere $p_{safe,t}(x_t)$ is the marginal distribution of the noisy safe data at t. Analogously, the unnormalized unsafe distribution $p_{unsafe}(x)$ is $1_{unsafe}(x)p_{data}(x)$. The unsafe denoiser is\n$\\mathbf{E}_{unsafe} [x | x_t] = \\int x \\frac{p_{unsafe}(x) q_t(x_t | x)}{p_{unsafe, t}(x_t)} dx$\nwhere $p_{unsafe,t}(x_t)$ is the marginal distribution of the noisy unsafe data at t.\nOur interest is to obtain $\\mathbf{E}_{safe} [x | x_t]$ given the data denoiser $\\mathbf{E}_{data} [x | x_t]$ defined in Eq. (1). The theorem below describes the relationship between our safe denoiser and the usual data denoiser.\nTheorem 3.2. Suppose that $\\mathbf{E}_{data} [x | x_t]$, $\\mathbf{E}_{safe} [x | x_t]$, and $\\mathbf{E}_{unsafe} [x | x_t]$ are the data denoiser, the safe denoiser, and the unsafe denoiser. Then,\n$\\mathbf{E}_{safe} [x | x_t] = \\mathbf{E}_{data} [x | x_t] + \\beta^*(x_t) (\\mathbf{E}_{data} [x | x_t] - \\mathbf{E}_{unsafe} [x | x_t])$\nfor a weight is defined by\n$\\beta^*(x_t) = \\frac{Z_{unsafe} p_{unsafe, t}(x_t)}{Z_{safe} p_{safe, t}(x_t)}$\nwhere $Z_{safe} := \\int 1_{safe}(x)p_{data}(x) dx$ and $Z_{unsafe} := \\int 1_{unsafe}(x)p_{data}(x) dx$ are normalizing constants of safe and unsafe distributions, respectively.\nThe theorem above suggests that a safe denoiser can be constructed similarly to CFG. In our case, the denoiser is"}, {"title": "3.2. Practial Considerations", "content": "For computing Eq. (4), we need to compute three terms: the data denoiser $\\mathbf{E}_{data}[x | x_t]$, the unsafe denoiser $\\mathbf{E}_{unsafe} [x | x_t]$ and the weight $\\beta^*(x_t)$. We approximate $\\mathbf{E}_{data}[x | x_t]$ by utilizing a pre-trained diffusion model. Consequently, the task reduces to deriving $\\mathbf{E}_{unsafe} [x | x_t]$ and the weight. This section delineates the approach to compute these quantities.\nset of unsafe data points denoted by $x^{(1)}, ..., x^{(N)}$,\n$\\mathbf{E}_{unsafe} [x | x_t] = \\sum_{n=1}^N x^{(n)} \\frac{q_t(x_t | x^{(n)})}{\\sum_{m=1}^N q_t(x_t | x^{(m)})}$\nEach numerator and denominator terms of Eq. (6) approximates the numerator and denominator terms of Eq. (3), respectively. It shows that an unsafe denoiser can be expressed as a weighted sum of the unsafe dataset. Here, the weights $\\{\\frac{q_t(x_t | x^{(m)})}{\\sum_{m=1}^N q_t(x_t | x^{(m)})}\\}_{m=1}^N$ form a sum-to-one normalized vector across the unsafe data points, so the unsafe denoiser is approximated as a weighted unsafe data point.\nEstimate of the weight. Next, we turn our attention to the computation of $\\beta^*(x_t)$ in Eq. (5). Direct calculation is intractable due to the denominator $Z_{safe} \\int p_{safe}(x)q_t(x_t|x)$, which is computationally infeasible\u00b9 to evaluate at every sampling steps. To address this challenge, we approximate $\\beta^*$ as\n$\\beta^*(x_t) \\approx \\eta \\cdot \\beta(x_t)$,\nwith a constant \u03b7 and a function $\\beta(x_t)$ defined by\n$\\beta(x_t) = \\int p_{unsafe}(x)q_t(x_t|x) dx = \\frac{1}{N} \\sum_{n=1}^N q_t(x_t | x^{(n)})$\nwhere the last line is an unbiased estimate of \u03b2. We treat \u03b7 as a controllable hyperparmeter, with which we replace the computation of the remaining terms in Eq. (5). This approximation is reasonable insofar as the numerator alone captures the overall trend of $\\beta^*(x_t)$: as $x_t$ becomes more likely to be unsafe, both $\\beta^*(x_t)$ and the numerator increase correspondingly. This approximation of the weight significantly reduces computational complexity. Additionally, we observe that applying the safe denoiser at the final stage of sampling (i.e., when t is small) hurts the sample quality, since the signal from unsafe denoiser-a weighted sum of unsafe data points-acts as a structural noise for detailed denoising. From this observation, we propose to apply the safe denoiser only at the beginning of sampling process.\nPutting things together. With these approximations mentioned above, we arrive at the final safe denoiser:\n$x_{o|t} = \\mathbf{E}_{data}[x | x_t] + \\eta \\beta(x_t) (\\mathbf{E}_{data}[x | x_t] - \\mathbf{E}_{unsafe} [x | x_t])$\nIt requies computing $q_t (x_t|x)$ over all safe data $x \\sim p_{safe} (x)$, where safe data includes the entire training dataset excluding few unsafe data. Modern text-to-image models like Stable Diffusion (Rombach et al., 2022) are trained with billions of training data (Schuhmann et al., 2022), and is infeasible to iterate the en-tire data at inference time."}, {"title": "3.3. Extending Safe Denoiser to Text-to-Image", "content": "Our approach can be combined with existing text-based guidance methods to enhance their performance:\n$x_{0|t} = \\mathbf{E}_{data}[x | x_t] + \\beta^*(x_t) (\\mathbf{E}_{data}[x | x_t] - \\mathbf{E}_{unsafe} [x | x_t]) + \\lambda (\\mathbf{E}_{data}[x | x_t, c_+] - \\mathbf{E}_{data} [x | x_t]) - \\mu(c_+, c_s; \\gamma, \\lambda) (\\mathbf{E}_{data}[x | x_t, c_s] - \\mathbf{E}_{data}[x | x_t])$\nUsing this denoiser allows us to negate data samples based on the information from the images (from our safe denoiser) and the information based on the prompts (from both CFG and SLD). Note this Eq. (8) includes only the additional term for the safe denoiser compared to Eq. (2). In implementation, as described in Sec. 3.2, we approximate the second term of Eq. (8) by Eq. (7). In diffusion sampling, we utilize this safe $x_{0|t}$ in either DDPM (Ho et al., 2020) or DDIM (Song et al., 2020), see Algorithm 1 for details.\nWhen our safe denoiser is combined with the text-based guidance methods, we introduce a new set of hyperparameters $\u03b2_t$, such that we set $\u03b2(x_t)$ to zero if this value falls below a predefined threshold $\u03b2_t$. This condition indicates that if a sample $x_t$ is sufficiently safe, modifying the trajectory is no longer necessary. This thresholding improves"}, {"title": "4. Related Work", "content": "Earlier work on machine unlearning in generative modelling focused on object unlearning in classification (forgetting images from a selected class), unconditional image generation (forgetting harmful images) or concept erasing (forgetting harmful concepts). Most of the work belonging to this category required retraining the entire generative models or some part of them, rather than modifying the sampling trajectory or input prompts (Heng & Soh, 2023; Li et al., 2024; Tiwary et al., 2025; Zhang et al., 2024a; Gandikota et al., 2023b; Lu et al., 2024; Gong et al., 2024; Lu et al., 2024). In more recent work, training-free and text-based methods have also emerged as computationally efficient alternatives (Schramowski et al., 2023; Yoon et al., 2024; Ban et al., 2024; Armandpour et al., 2023). However, most of these approaches lack a theoretical ground, unlike our work.\nDespite these advances, generative models remain susceptible to adversarial prompts, malicious manipulations of learnable parameters, textual cues, or even random noise (Pham et al., 2023; Chin et al., 2024; Zhang et al., 2024b; Tsai et al., 2024). These findings highlight using a single defense such as concept erasing as a standalone solution may be insufficient to ensure safe content generation. We see this as an opportunity for our method to be combined with powerful text-based defense mechanisms to enhance their performance.\nThe most closely related work is Sparse Repellency (SR) by Kirchhof et al. (2024), a training-free technique that modifies the denoising trajectory to avoid unsafe images $\\{x^{(n)}\\}_1^N$. Their denoiser follows\n$\\mathbf{E}_{data} [x | x_t] + \\sum_{i=1}^N \\frac{ReLU \\left(r - ||\\mathbf{E}_{data}[x | x_t] - x^{(n)}|| \\right)}{\\sum_{i=1}^N ReLU \\left(r - ||\\mathbf{E}_{data}[x | x_t] - x^{(n)}|| \\right)} (\\mathbf{E}_{data}[x | x_t] - x^{(n)})$. The Rectified Linear Unit (ReLU) function ensures that the diffusion trajectory is penalized when the denoiser falls within the neighborhood of radius r around unsafe data, and remains unmodified otherwise. Given a single unsafe image, $\\frac{ReLU \\left(r - ||\\mathbf{E}_{data}[x | x_t] - x^{(n)}|| \\right)}{\\sum_{i=1}^N ReLU \\left(r - ||\\mathbf{E}_{data}[x | x_t] - x^{(n)}|| \\right)} (\\mathbf{E}_{data}[x | x_t] - x^{(n)})$ resembles the second term in Eq. (4) if the ReLU value is comparable to our $\\beta^*$. From this, our method can be viewed as a generalization of the SR. However, unlike our method, their guidance does not guarantee sampling from a safe distribution.\nLastly, the work by Biggs et al. (2024) shares a similar theoretical analysis as ours. They propose to merge the weights of DMs separately trained on independent subsets of data, resulting in a sampling distribution that extends beyond the framework outlined in our Theorem. 3.2. However, unlike their method, we do not require additional training of DMs and our analysis defines the safe and unsafe denoisers and their explicit relationship between those."}, {"title": "5. Experiments", "content": "In this section, we present the experimental results of our method, Safe Denoiser. Section 5.1 details the outcomes of our text-to-image generation experiments, while the subsequent section explores both unconditional and conditional image generation."}, {"title": "5.1. Text-to-Image Generation", "content": "In this section, we conduct an in-depth analysis of the improvements achieved by additionally applying our safe denoiser in text-to-image models. As a baseline, we utilize Stable Diffusion (SD) (Rombach et al., 2022) v1.4. For our experiments, we employ the DDPM sampler.\nTo assess the model safety, we evaluate Attack Success Rate (ASR) and Toxic Rate (TR) (Yoon et al., 2024). We measure ASR by the proportion of generated images that exceeds 0.6 nude class probability, measured by NudeNet\u00b3. The TR is computed by the average of nude class probability, measured also by NudeNet. We select 515 unsafe images as the unsafe dataset of $\\{x^{(1)}, . . ., x^{(N)} \\}$ from I2P (Schramowski et al., 2023) that exceeds 0.6 nude class probability.\nTo evaluate the image quality, we calculate Fr\u00e9chet Inception Distance (FID) (Heusel et al., 2017) and CLIP (Radford et al., 2021). We use a pytorch package (Seitzer, 2020) to compute the FID by comparing 10K reference images selected from the COCO-2014 (Lin et al., 2014) validation split and 10K generated images from the prompts identically selected from the same COCO dataset. Also, we evaluate the CLIP score using ViT-B-32\u2074 with the same dataset."}, {"title": "6. Conclusion", "content": "We have addressed significant safety challenges in DMs, particularly concerning the generation of NSFW content and the inadvertent reproduction of sensitive data. We introduce the safe denoiser, a novel approach that modifies the sampling trajectories of DMs to adhere to theoretically safe distributions, thereby ensuring the generation of appropriate and authorized content. Experimental results demonstrate that the safe denoiser achieves state-of-the-art performance in tasks such as concept erasing, class removal, and unconditional image generation. Ultimately, this work provides a robust and scalable solution for mitigating safety risks in generative AI, paving a way for their responsible and ethical applications."}, {"title": "Impact Statements", "content": "This paper presents a work whose goal is to build a reliable and trustworthy Generative AI. There are many potential societal consequences of our work, particularly in addressing ethical risks associated with generative models. Our research is focused on preventing the generation of NSFW content, including nudity and violence, and mitigating the risk of models memorizing and reproducing private information, such as human face, from training datasets. We believe the presented work contributes to the responsible use of generative AI, reinforcing ethical safeguards and promoting AI systems that align with societal values and human rights."}, {"title": "A. Proof", "content": "Theorem 3.2. Suppose that $\\mathbf{E}_{data} [x | x_t]$, $\\mathbf{E}_{safe} [x | x_t]$, and $\\mathbf{E}_{unsafe} [x | x_t]$ are the data denoiser, the safe denoiser, and the unsafe denoiser. Then,\n$\\mathbf{E}_{safe} [x | x_t] = \\mathbf{E}_{data}[x | x_t] + \\beta^*(x_t) (\\mathbf{E}_{data}[x | x_t] - \\mathbf{E}_{unsafe} [x | x_t])$\nfor a weight is defined by\n$\\beta^* (x_t) = \\frac{Z_{unsafe} p_{unsafe,t}(x_t)}{Z_{safe}p_{safe,t}(x_t)}$\nwhere $Z_{safe} := \\int 1_{safe} (x)p_{data}(x) dx$ and $Z_{unsafe} := \\int 1_{unsafe} (x)p_{data}(x) dx$ are normalizing constants of safe and unsafe distributions, respectively.\nProof. Using the relationships,\n$p_{safe} (x) = \\frac{1}{Z_{safe}} 1_{safe} (x)p_{world}(x)$ and $p_{unsafe} (x) = \\frac{1}{Z_{unsafe}} 1_{unsafe} (x) p_{world}(x)$,\nwe derive the safe denoiser by\n$\\mathbf{E}_{safe} [x | x_t] = \\frac{\\int x p_{safe}(x) q_t(x_t | x) dx}{p_{safe,t} (x_t)} = \\frac{\\int x 1_{safe} (x)p_{data}(x) q_t(x_t | x) dx}{Z_{safe} p_{safe,t} (x_t)} = \\frac{\\int x (1(x) \u2013 (1(x) \u2013 1_{safe}(x)))p_{data}(x) q_t(x_t | x) dx}{Z_{safe} p_{safe,t} (x_t)} = \\frac{\\int x (1(x) \u2013 1_{unsafe} (x))p_{data}(x) q_t(x_t | x) dx}{Z_{safe} p_{safe,t} (x_t)} = \\frac{\\int x p_{data}(x) q_t(x_t | x) dx \u2013 \\int x 1_{unsafe} (x)p_{data}(x) q_t(x_t | x) dx}{Z_{safe} p_{safe,t} (x_t)} = \\frac{\\frac{p_{data,t}(x_t)}{p_{data,t}(x_t)} \\int x p_{data}(x) q_t(x_t | x) dx \u2013 \\frac{Z_{unsafe} p_{unsafe,t}(x_t)}{Z_{unsafe} p_{unsafe,t}(x_t)} \\int x p_{unsafe} (x) q_t(x_t | x) dx}{Z_{safe} p_{safe,t} (x_t)} = \\frac{p_{data,t}(x_t) \\mathbf{E}_{data} [x | x_t] \u2013 \\frac{Z_{unsafe} p_{unsafe,t}(x_t)}{Z_{safe}p_{safe,t}(x_t)} p_{data,t}(x_t) \\mathbf{E}_{unsafe} [x | x_t]}{Z_{safe}p_{safe,t}(x_t)} = \\mathbf{E}_{data} [x | x_t] - \\frac{Z_{unsafe} p_{unsafe,t} (x_t)}{Z_{safe}p_{safe,t}(x_t)} \\mathbf{E}_{unsafe} [x | x_t]$\nNow,\n$1 = \\frac{Z_{safe}p_{safe,t}(x_t)}{Z_{safe} f p_{safe} (x)q_t(x_t|x) dx} = \\frac{Z_{safe}p_{safe,t}(x_t)}{Z_{safe} \\int p_{safe} (x)q_t(x_t|x) dx + Z_{unsafe} \\int p_{unsafe} (x)q_t(x_t|x) dx} = \\frac{Z_{safe}p_{safe,t}(x_t)}{\\int (Z_{safe}p_{safe} (x) + Z_{unsafe}p_{unsafe} (x))q_t(x_t|x) dx} = \\frac{Z_{safe}p_{safe,t}(x_t)}{\\int (1_{safe}(x)p_{data}(x) + 1_{unsafe}(x)p_{data}(x))q_t(x_t|x) dx} = \\frac{Z_{safe}p_{safe,t}(x_t)}{\\int p_{data}(x)q_t(x_t|x) dx} = \\frac{Z_{safe}p_{safe,t}(x_t)}{p_{data,t} (x_t)}$\nwhich completes the proof."}, {"title": "B. Experimental Details and Additional Results", "content": "B.1. Implementation Details\nText-to-Image Generation As outlined in the manuscript, we conduct the Text-to-Image experiment using SDv1.4, following the same model as the baselines for generating images from text, as referenced in (Schramowski et al., 2023; Wu et al., 2024; Gong et al., 2024; Yoon et al., 2024). To ensure consistency, we adopt the generation procedure described in each baseline. Preliminary observing the sensitivity of nudity-related content, we employ the DDPM scheduler (Ho et al., 2020). For a fair comparison, we maintain the same number of inference steps, specifically 50, aligning with the official implementations of both SLD and SAFREE, which also use 50 inference steps.\nRegarding the Safe Denoiser, the proposed model computes the transition kernel with an RBF kernel. The RBF kernel function is defined as follows:\n$K(x,x') = exp\\left(-\\frac{||x - x'||^2}{2\\sigma^2}\\right)$\nFor the bandwidth parameter \u03c3, we set a value of 1.0 for SLD and 3.15 for SAFREE. Additionally, in case of SAFREE, we apply a scaling factor \u03b7 = 0.33, whereas for SLD, we use \u03b7 = 0.03 to regulate the strength of the repellency in Eq. (8). For reference images, we utilize a total of 515 images sourced from the I2P dataset (Schramowski et al., 2023), which were generated using SDv1.4. As stated in the manuscript, these reference images meet the criterion of having a nude class probability above 0.6, as determined by Nudenet. Sample images are shown below."}, {"title": "B.2. Additional Results", "content": "We present additional qualitative results across three experimental scenarios: (1) Text-to-Image Generation for preventing nudity, (2) Sexual Debiasing in unconditional generation for facial images, and (3) Class-Conditional Generation, where reference images serve as constraints not to generate. To systematically demonstrate the effectiveness of our approach, we present the results in sequence, beginning with text-to-image generation followed by unconditional generation and concluding with conditional generation."}]}