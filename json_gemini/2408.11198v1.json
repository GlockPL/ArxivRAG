{"title": "EPIC: Cost-effective Search-based Prompt Engineering of LLMs for Code Generation", "authors": ["Hamed Taherkhani", "Melika Sepindband", "Hung Viet Pham", "Song Wang", "Hadi Hemmati"], "abstract": "Large Language Models (LLMs) have seen increasing use in various software development tasks, especially in code generation. The most advanced recent methods attempt to incorporate feedback from code execution into prompts to help guide LLMs in generating correct code, in an iterative process. While effective, these methods could be costly and time-consuming due to numerous interactions with the LLM and the extensive token usage. To address this issue, we propose an alternative approach named Evolutionary Prompt Engineering for Code (EPiC), which leverages a lightweight evolutionary algorithm to evolve the original prompts toward better ones that produce high-quality code, with minimal interactions with LLM. Our evaluation against state-of-the-art (SOTA) LLM-based code generation models shows that EPiC outperforms all the baselines in terms of cost-effectiveness.", "sections": [{"title": "1 INTRODUCTION", "content": "LLMs have been used in many software development activities such as software testing, design, requirement engineering, code generation, maintenance, deployment, etc. [30], [32]. Among these activities, code generation using LLMs has demonstrated significant potential.\nIn LLM-based code generation, various prompt engineering techniques, including zero-shot [5], in-context learning [33], [34], RAG [35], and task-specific methods [36], [37], have been shown to outperform fine-tuned smaller models. The most advanced prompt engineering methods for code generation employ various agent-based approaches [28]. SOTA methods such as Reflexion [20], Language Agent Tree Search (LATS) [21], AgentCoder [22], LDB [23], and MetaGPT [29] are either planning-based or multi-collaborative agents. While effective, these methods can be costly and time-consuming due to numerous interactions with LLMs which results in extensive token usage, making them less attractive in practical settings. For instance, LATS requires on average 3 minutes to generate the implementation of a function with an average of only 6 lines of code, on the MBPP dataset.\nTo tackle this limitation, in this paper, we explicitly focus on the cost of prompt engineering for code and propose a cost-effective approach that leverages a lightweight evolutionary search to optimize prompts for code generation. Our evolutionary-based prompt engineering tool for code generation, namely, EPiC, optimizes prompts for code generation by assessing the generated code against a fitness function, i.e., the pass rate of test cases. Our approach consists of two phases: Initial Evaluation (IE) and Evolutionary Prompt Engineering (EPE). The first phase involves primary code generation using an initial prompt and its evaluation using a set of test cases. If a correct solution is not generated, the process moves to the second (EPE) phase, where an initial population of prompts is generated using the initial prompt. Using each prompt in the initial population, we generate its corresponding code by the LLM under study. We then evaluate the generated code against test cases to calculate its fitness score. Candidate prompts are selected out of the population, using a weighted random selection approach. These candidates are then mutated to form the next generation of prompts. The mutation is carried out using two approaches: one utilizes an LLM guided by a prompt that specifies how to perform the mutation, and the other employs vector embeddings of words to find and replace similar words."}, {"title": "2 BACKGROUND AND RELATED WORK", "content": "In this section, we briefly explain the background of LLMs for code generation and prompt engineering, then report the most related work in the context of prompt engineering of LLM for code.\nModels for code generation can be divided into two categories, RNN-based models and transformer-based models [1]. RNN-based models are older models that are outperformed by transformers. Transformer-based code generation models are transformers pre-trained on code-related datasets. For instance, CodeBERT [2] is a transformer model based on BERT [3], specifically pre-trained on the Code-SearchNet dataset [4]. Another example is CodeGPT-2, which builds upon GPT-2 [5] and is pre-trained on the same dataset. Lu et al. [6] evaluated the performance of both GPT-2 and CodeGPT-2 in the code generation task and later, Perez et al. [7], fine-tuned GPT-2 using their dataset for code generation.\nNewer versions of LLMs, such as GPT-4, have demonstrated improved performance compared to their predecessors across diverse tasks, including code generation. For instance, Codex [8] is a language model developed by OpenAI based on the GPT architecture, similar to GPT-3. However, it's specifically designed for code generation and understanding. Codex is trained on a diverse range of publicly available code repositories and strongly focuses on understanding and generating programming code. Another model, CodeLlama [9], is a family of large language models for code based on Llama 2 [10] that is available in 7B, 13B, and 34B parameters. DeepSeek Coder is another state-of-the-art LLM that has been used in software tasks such as Code Generation. This LLM has various sizes, ranging from 1B to 33B. Magicoder [11] is another model that has been fine-tuned on both CodeLlama-7b-Python and deepseek-coder-6.7b.\nIn this research, we employed two different types of LLMs: GPT-40, as an example of closed-source large models, and MagicCoder as an example of open-source smaller models. We selected GPT-40 due to its high performance among the closed-source LLMs. MagicCoder was chosen as the most cost-effective small-size open-source model, at the time of designing our experiments."}, {"title": "2.2 Prompt Engineering for LLMs", "content": "A prompt is an input or query provided to a model to generate a response or perform a task. Prompt engineering refers to the process of designing and refining prompts to achieve desired outcomes when using LLMs 1.\nThere are multiple categories of prompt engineering, including approaches without training, reasoning and logic, reducing hallucination, and evolutionary-based methods [38]. Zero-shot [5] and few-shot [33] prompting fall under the category of approaches without training. Techniques such as chain-of-thought (CoT) prompting [12], Automatic Chain-of-Thought (Auto-CoT) [39], Self-Consistency [40], and knowledge prompting [13] exemplify reasoning and logic-based methods. To reduce hallucination for prompt engineering, techniques such as Retrieval Augmented Generation (RAG) [35], ReAct Prompting [41], and Chain-of-Knowledge (CoK) Prompting [42] are employed. Evolutionary algorithms are utilized to optimize prompts, as demonstrated by EvoPrompt [16] and PromptBreeder [25]. There are other solutions such as Automated Prompt Engineering [14] and Prompt Optimization with Textual Gradients (ProTeGi) [15].\nZero-shot prompting, as described in [5], eliminates the necessity for extensive training data by employing accurately designed prompts to direct the model toward executing new tasks. The model receives a task description without labeled training data and employs its pre-existing knowledge to generate predictions based on the prompt. Few-shot prompting [33] uses a limited number of input-output examples to help models understand tasks, unlike zero-shot prompting which provides no examples. However, this method requires additional tokens, which can be impractical for longer texts. The selection and composition of examples can also significantly influence model behavior and introduce biases.\nCoT [12] improves the reasoning abilities of large language models by incorporating intermediate reasoning steps within prompts. This technique breaks down complex tasks into smaller sub-tasks, mimicking human problem-solving. It significantly enhances performance in arithmetic,"}, {"title": "2.3 Prompt Engineering of LLMs for Code Generation", "content": "Zelikman et al. [17] introduced Parsel, a framework enabling automatic implementation and validation of complex algorithms with code LLMs. Using Parsel, they break down algorithmic tasks into structured descriptions written in natural language, then explore various combinations of function implementations using tests.\nDyLAN [19] is a framework designed to boost the performance of Large Language Model (LLM) agents by assembling them into a dynamic team that adapts to different tasks. Unlike traditional methods with fixed agent sets, DyLAN's architecture adjusts dynamically to the task query, enhancing its versatility. It employs features like inference-time agent selection and early stopping to improve efficiency and effectiveness. Furthermore, DyLAN incorporates an automatic agent team optimization algorithm based on an unsupervised metric called Agent Importance Score, which selects the most effective agents for each task. Empirical results show DyLAN's success in tasks like reasoning and code generation, achieving significant improvements over single LLM executions. Reflexion [20] is a reinforcement-based framework for this problem where language agents learn from linguistic feedback rather than weight updates. Agents reflect on task feedback verbally and maintain their own reflective text in memory, which helps them make better decisions in subsequent trials. Reflexion is adaptable to different types and sources of feedback signals and shows significant improvements over baseline agents across various tasks. LATS (Language Agent Tree Search) [21] is another search-based framework that combines LLMs' abilities in planning, acting, and reasoning. LATS draws inspiration from the Monte Carlo tree search and repurposes LLMs' strengths as agents, value functions, and optimizers. Crucially, LATS incorporates an environment for external feedback to enable more deliberate and adaptive problem-solving beyond existing techniques' limitations. AgentCoder [22] utilizes a multi-agent framework with specialized agents: the programmer agent, the test designer agent, and the test executor agent. The programmer agent focuses on code generation and refinement based on feedback from the test executor agent, while the test designer agent generates test cases for the code. The test executor agent runs the code with the test cases and provides feedback to the programmer. This collaborative system aims to improve code generation efficiency, out-performing single-agent models and previous strategies.\nZhong et al. [23] introduced Large Language Model Debugger (LDB), a framework that segments programs into basic blocks and tracks intermediate variable values during runtime. LDB enables LLMs to focus on simpler code units, verify their correctness block by block, and pinpoint errors effectively.\nIn contrast to existing methods and to the best of our knowledge, EPiC is the first search-based prompt engineering method for code generation. It employs a lightweight process to identify the optimal solution in a cost-efficient manner. To achieve this, EPiC utilizes a local embedding function to implement mutation operators on text, to reduce the cost of iterative prompt engineering for code generation. It also guides the search over iterations using the fitness function in Section 4.4, which helpsfindg the most effective prompts."}, {"title": "3 EVOLUTIONARY PROMPT ENGINEERING FOR CODE", "content": "This section provides a detailed elaboration of EPiC. As depicted in Figure 2, our approach comprises two phases, i.e., the Initial Evaluation (IE) phase and the Evolutionary Prompt Engineering (EPE) phase. The process begins with a user providing an initial prompt that describes the intended functionality of the code to be generated. In the IE phase, we evaluate the prompt by generating the code based on the original prompt using an LLM. This evaluation determines whether the prompt is sufficient to generate the correct implementation or if it requires a further process in the EPE phase. EPiC's algorithm is presented in Algorithm 1, which will be elaborated upon in the subsequent sections. Lines 2 to 6 in Algorithm 1 correspond to the IE phase of EPiC and lines 7 to 30 are the implementaion of the EPE phase."}, {"title": "3.1 Initial Evaluation", "content": "In Step 1 of the IE phase, the initial prompts for code and test case generation are provided to the LLM (the choice of LLM to be used in our case will be discussed in the Section 4) to generate the corresponding code and test cases. Subsequently, in Step 2, the generated code and test cases are sent to the Testcase Evaluation module. evaluatePrompt in Line 4 of Algorithm 1 is responsible for generating the code and evaluating the test cases. If any test case fails on the code we continue with the EPE phase. If all the test cases pass, we stop and report the generated code as the final answer.\nNote that test cases can be provided in various ways. One approach is to use developer-provided test cases for evaluation, while another is to generate test cases using LLMs. The challenge with developer-provided test cases is that typically users do not have test cases before implementation, making the approach impractical for a code generation task. On the other hand, the advantage of using LLM-generated test cases is that it makes the process fully automated. This method is currently used in most SOTA code generation tools [20], [21], [22], [18], and [29], as part of their internal evaluation. Consequently, we also opted to use LLMs for test case generation to ensure a fully automated approach, assuming no developer-provided test cases. Specifically, we employed OpenAI's GPT-40 model to generate test cases with few-shot examples. To ensure the functional correctness of these test cases, we validated them by parsing their Abstract Syntax Trees (AST). A test case is considered valid if it successfully parses into a syntactically"}, {"title": "3.2 Evolutionary Prompt Engineering", "content": "In the EPE phase, the first step (Step (3) involves populating the first generation of individuals, where each individual is a distinct prompt to generate the same code. In Step (3), we generate multiple prompts by modifying the initial prompt using an LLM agent. Generating this prompt population forms an important part of our evolutionary algorithm. A more detailed description of this process will be provided in Section 3.2.1. This step corresponds to line 8 of Algorithm 1.\nIn Step (4) of EPE which corresponds to lines 12 to 20 in Algorithm 1, we generate code for each prompt and evaluate each generated code sample using the same test cases. We define a fitness function based on the ratio of test cases passed. If any of the prompts achieves the maximum fitness score, the process stops. If not, the process continues until all prompts are validated and given a fitness score, which will guide the selection process. The fitness function is basically the passing rate of each prompt. The mathematical formula is explained in Section 4.4.\nThis fitness function is used to select the candidate prompts for mutation in Step 5. This is implemented in chooseCandidate function in Algorithm 1 (Lines 22 and 26), where a weighted random selection algorithm selects candidates randomly, with the probability of selection being proportional to their respective fitness score (Section 4.4). This selection occurs with substitution, allowing the same prompt to be chosen multiple times. In Step 5, we select N-1 candidates for mutation and one prompt for \u201celitism\u201d.\nFollowing prompt selection, in Step (6), EPiC randomly mutates the selected prompt candidates to better explore the search space of potential prompts in the next generation. The prompt mutation which is detailed in Algorithm 2, is elaborated in Section 3.2.2. The next generation is formed by adding the elite prompt to the pool of mutated prompts. Lines 21 to 27 in Algorithm 1 corresponds to Steps (5) and 6.\nThe process of EPE is repeated iteratively until a solution achieves maximum fitness score or until predefined stopping criteria, i.e., reaching the maximum number of iterations or observing no improvement in the fitness scores, are met. In such cases, the best-generated code, determined by its fitness, is chosen and returned (line 29 of Algorithm 1).\nIn the next two subsections, we will explain more details about Step (3) (Initial Population Builder), and Step (6 (Prompt Mutation.)"}, {"title": "3.2.1 Initial Population Builder", "content": "This subsection provides a detailed explanation of the Step (3) in Figure 2. Human-written prompts are often very brief and lack the necessary information or context. This results in insufficient descriptions and a failure to adhere to a well-defined format [43]. Thus manual prompting typically results in several back-and-forth interactions with LLM until all necessary details are given [28]. Providing the initial"}, {"title": "3.2.2 Prompt Mutation", "content": "This subsection provides a detailed explanation of the Step (6) in Figure 2. We employed two kinds of prompt mutation approaches. In the first approach, EPiC uses an LLM to facilitate the mutation process (LLM_as_mutator). This approach is inspired by a framework named EvoPrompt [16]. In LLM_as_mutator, we provide LLMs with predefined instructions on how to implement the prompt mutation.\nIn the second approach, we employ Natural Language Processing (NLP) techniques using the NLTK and Gensim libraries to alter the prompts by substituting words with their synonyms (sim_words_as_mutator). This method is our contribution as a local lightweight prompt mutation process. The main motivator is to reduce the cost of the iterative part of the EA. This prompt mutation algorithm (sim_words_as_mutator) is described in Algorithm 2. We will compare the performance of both approaches in the following sections. We use the following prompt for mutation using LLMs:\nYou are a mutation tool. This is a Python function and its description. Please mutate the description by enhancing its clarity and comprehension for sophisticated language models.\nPlease put the changed description between #Explanation and #End. Use at most 600 words.\nIn the sim_words_as_mutator algorithm (Algorithm 2), we first extract the description part of the prompt. For each word in the sentence, if the word is not a stop word or proper noun, it is lemmatized, and the get_related_words() function (line 10 in Algorithm 2) is called to get its related words. To obtain the related words for a given word, we use WordNet and gensim's GloVevectors (fasttext - wiki news subwords - 300) to calculate the cosine similarity between the word and other related words. We rank the related words to create a list of the most similar words to the original word, and based on the specified num_t or sim_t, we filter out the least related words and keep words that pass the given threshold. Then, we randomly substitute the word with one of the related words based on a mutation_probability."}, {"title": "4 EXPERIMENT DESIGN", "content": "We define the following research questions to explore the performance of EPIC:\n\u2022 RQ1: How cost-effective are the SOTA LLM-based code generation approaches? Motivation: Most of the SOTA LLM-based code generation studies do not include a comprehensive approach to evaluate cost-effectiveness. In this RQ, we set out to evaluate these tools from the cost-effectiveness perspective.\n\u2022 RQ2: How effective is EPiC in code generation? Motivation: In this RQ, we explore the performance of EPiC against SOTA LLM-based code generation tools"}, {"title": "4.2 Datasets and Models", "content": "We used two datasets for our experiments: HumanEval [8] and MBPP [24].\nThe HumanEval is a collection of 164 programming problems designed to evaluate the functional correctness of code generated by Al models. Each problem includes a function signature, a detailed description of the task (docstring), the function body, and multiple unit tests to verify the solution. One major obstacle in making fair judgments on the performance of the related works was that some papers did not use the HumanEval dataset as is. For instance, there are cases where the samples are removed or changed. To have a fair comparison among all methods we transformed the original dataset into each tool's required format (if needed) to re-run their code using the original dataset. We will explain this in more detail as we discuss the configuration setup for assessing each related work.\nThe Mostly Basic Programming Problems (MBPP) comprises 974 short Python programming tasks designed to be solvable by entry-level programmers. The tasks were crowd-sourced from individuals with basic Python knowledge, who provided a problem statement, a self-contained Python function solving the problem, and three test cases to check for correctness. A subset of 427 tasks was manually inspected and edited for consistency and accuracy, referred to as mbpp-sanitized. In this paper, we used mbpp-sanitized for our experiments. While experimenting with the SOTA, we discovered that they have used different subsets of the MBPP dataset. For this reason, their reported results are not comparable. Consequently, we chose to use the mbpp-sanitized subset in all our experiments.\nFor RQ1 and RQ2.1, we used both datasets. For RQ2.2 and RQ3, we used only HumanEval. We will explain this in more detail in the corresponding RQs.\nWe employed two LLMs in our study: OpenAI's GPT-402 and Magicoder-S-DS-6.7B [11]. The latter model was"}, {"title": "4.3 Evaluation Metrics", "content": "To evaluate the quality of the generated code by LLMs, we utilized the pass@k metric. This metric, introduced in [8], is advantageous over metrics such as CodeBLEU and ROUGE because it evaluates code behavior rather than text semantics, which are the focus of the latter metrics. The formula for pass@k is presented in Equation 1. In this equation, E denotes the expected value over the problems, n is the total number of generated samples per task (code samples) and c is the number of correct samples. For a single problem, this metric estimates the probability that at least one of top k samples is correct. When applied to multiple problems, it evaluates the expectation across all problems.\npass@k := E 1  \\binom{n-c}{k} \\binom{n}{k} (1)\nWhen comparing EPiC with baselines, we also report Mann-Whitney U-test results, which is a non-parametric significant test, to consider the randomness introduced by our algorithm. The tests compare the distribution of 10 pass@1 results for EPiC (using 10 random seeds) with the pass@1 of each baseline, separately. Note that here we only rerun EPIC with 10 random seeds to consider its internal randomness introduced in the algorithm and do not rerun the baselines as they do not have internal randomness. In other words, we do not study the potential randomness of LLMs results and their effect on each baseline, as this is not reported in the original papers and would go beyond the scope of this paper."}, {"title": "4.4 Objective Function", "content": "We use X and Z to denote the prompt space and output responses respectively. Let S : X \u2192 Z be the LLM that takes an input x \u2208 X and outputs a response z \u2208 Z. The output response is the code implementation of the given prompt. Let's say we have n number of test cases. Consider T as a set of test case functions. A test case t\u1d62 \u2208 T is a function that maps any output response z to either 0 or 1. It can be formally defined as t\u2081 : Z \u2192 {0,1}. The fitness function which is denoted as F : X,T \u2192 [0,1], maps any input prompt to a value between 0 and 1. The fitness function is defined as follows:\nF(x,T) =  \\frac{\\sum_{i=1}^{n}t_i(S(x))}{n} (2)\nThe goal of the EPiC framework is to optimize the fitness function F based on the provided test cases T by identifying the optimal input x within the prompt space X."}, {"title": "4.5 Baselines", "content": "To evaluate the performance of EPiC, we first identified the most relevant papers in code generation that utilized a common benchmark for comparison, i.e., papers that used"}, {"title": "5 EXPERIMENT RESULTS", "content": "We evaluated LATS, LDB, and Reflexion on the entire HumanEval and Sanitized MBPP datasets. To monitor the monetary costs, we used the OpenAI dashboard. This cost is calculated based on the number of tokens exchanged with the OpenAI API. As the financial cost is calculated based on the number of exchanged tokens, we chose the dollar cost reported in the OpenAI dashboard as the metric to measure the cost of an approach. The results of this experiment are presented in Table 2, for HumanEval and MBPP.\nThe first observation is that there is a notable discrepancy between our achieved pass@1 results and those reported in the original papers of the baselines. One possible reason is that we used GPT-40 for our experiments, whereas the original studies used other OpenAI models. Another plausible reason could be inconsistencies between the original dataset and the ones they used, as we utilized the original dataset while they used a modified version.\nReflexion incurred the lowest cost in both datasets but also demonstrated the lowest performance, with an overall accuracy of %75 and a cost of \u00a21.1 per instance. Conversely, LATS exhibited the highest time and dollar costs in both datasets, with a total cost of $96.71 in total and a cost of"}, {"title": "5.1 RQ1: How cost-effective are the SOTA LLM-based code generation tools?", "content": "$16.3 per instance. LDB achieved the best performance on HumanEval with an accuracy of %92 but ranked second on MBPP overall. LATS achieved the best performance overall but had the highest cost, considerably higher than both Reflexion and LDB. On the other hand, LDB provided better performance than Reflexion without significantly increasing the cost.\nA noteworthy observation is that the cost (measured by dollar amount) and effectiveness (measured by pass@1) of these code generation tools do not always increase proportionally. This suggests that achieving higher performance does not necessarily require a proportional increase in cost. For instance, comparing Reflexion and LDB on MBPP shows that obtaining a %71 pass@1 costs $4.9, while increasing the performance to %73 requires $13.8. This indicates that a %2 improvement in performance requires around %180 increase in cost. On the other hand, comparing LDB and LATS on MBPP indicates that a further %3 increase in performance requires more than %500 increase in cost. This observation aligns with the understanding that some programming problems are inherently more challenging and require more time for both humans and LLMs to solve correctly. However, it is crucial to develop an approach that achieves higher performance without a substantial increase in cost, ensuring cost-effectiveness. This emphasizes that future studies in this field must always report their cost as part of the evaluation as well.\nAnswer to RQ1: The SOTA code generation tools vary in cost-effectiveness, and the relationship between cost and effectiveness is not always linear. For instance, in MBPP dataset, LATS demonstrates the best performance (%76 pass@1) but incurs almost 6 times more cost compared to the second best baseline, LDB, with only %3 less pass@1. On the other hand, Reflexion shows slightly lower performance (%71 pass@1), but with a very low cost ($4.9). Therefore, we recommend a cost analysis always be a part of the evaluation when studying LLMs for code generation."}, {"title": "5.2 RQ2: How effective is EPiC in code generation?", "content": "In addressing RQ2.1, we accounted for the inherent randomness in our algorithm by testing EPiC with different seeds. The seed was utilized in Python's random function,"}, {"title": "5.2.1 RQ2.1: How cost-effective is EPiC compared to the SOTA?", "content": "We evaluated EPiC on HumanEval and MBPP, each 10 times using 10 unique random seeds. The statistical results for this experiment are presented in Table 3. The results on HumanEval indicate that the median pass@1 is %93.5. Specifically, in 5 out of 10 runs, we achieved %93, in 1 run we achieved %94, and in 4 out of 10 runs, we achieved %95. These results demonstrate that EPiC consistently outperforms the baselines in all runs. The variance of the results is 0.89, indicating the robustness of EPiC. This demonstrates that the algorithm's performance remains stable despite inherent randomness. The comparison between EPiC (on median pass@1) and the SOTA on Humaneval is presented in Figure 5.\nIn terms of cost, EPiC demonstrated a much lower cost than LATS, and almost the same cost as LDB, although they ranked after Reflexion. Nonetheless, EPIC displayed considerably better performance with a slightly higher cost compared to Reflexion. This cost is significantly lower than that of LATS and is comparable to LDB. As shown in Figure 5, EPiC achieves the best performance with a marginally higher cost than the lowest-cost baseline, making it the most cost-effective approach among the SOTA. We also conducted a one-sample Wilcoxon test over the 10 runs of EPiC. Our analysis of results on the HumanEval dataset revealed that the pass@1 differences reported for EPiC compared to Reflexion, LATS, and LDB are statistically significant, with a p-value of 0.0025.\nSimilarly, on the MBPP dataset, EPiC's median pass@1 is %79, which makes it outperform all other baselines. Among 10 runs, we achieved %78 in 3 runs out of 10, %79 in 5 runs of 10, and %80 in 2 runs. The results on this dataset are presented in Figure 6. In terms of cost, EPiC ranked third, following Reflexion and LDB. The cost was much lower than LATS and close to LDB. EPiC achieved an %8 better pass@1 compared to Reflexion with a slightly higher cost. Similar to the previous experiment, EPiC achieved the best performance with a minimal extra cost over the lowest-cost baseline. The variance of the EPIC results on the MBPP dataset is 0.55, indicating a low level of variability (variance<1). This finding is consistent with results from the HumanEval dataset, further underscoring the robustness of the EPiC. Our statistical analysis of the MBPP dataset demonstrates that the differences between EPiC's pass@1 results over the 10 runs and the pass@1 results of the baselines are again statistically significant, with a p-value of 0.0025 for all three baselines.\nIn conclusion, the robustness of the EPiC is highlighted by its consistent performance on the HumanEval and MBPP datasets. On HumanEval, EPiC achieved a %93.5 pass@1 rate with a low variance of 0.89, indicating stable outcomes across different runs. Similarly, on MBPP, the model's pass@1 was %79 with a variance of 0.55, showcasing its reliability."}, {"title": "5.2.2 RQ2.2: How effective is EPiC on smaller open-source LLMs compared to larger closed-source models?", "content": "In this RQ, we replaced EPiC's LLM component, i.e., GPT40, with a smaller open-source model, i.e., Magic-Coder [11]. All other settings remain the same as in RQ2.1. We chose MagicCoder because it has the highest performance, in the literature, among open-source LLMs at the time of designing these experiments. Note that given the low variance of the results in RQ2.1, over the 10 runs, we only ran this experiment once. Due to extensive cost, we also limit this experiment to the HumanEval dataset.\nWe evaluated the performance of magiccoder under zero-shot settings (baseline) and compared it with EPiC's magiccoder version, where EPiC utilizes magiccoder as its base LLM. Table 4 presents the results for this research question. The zero-shot approach with magiccoder achieved a pass@1 rate of %63, whereas EPiC using magiccoder achieved a pass@1_rate of %72. This indicates a %9 improvement over the base model. magiccoder has 7 billion parameters, making it significantly less complex than OpenAI's model. However, this experiment demonstrates that"}, {"title": "5.3 RQ3: What are the impacts of different EA design choices on the cost-effectiveness of EPiC?", "content": "In this RQ, we measure the impact of different design choices (hyper-parameters) of EPiC on HumanEval. It is important to emphasize that the goal of this question is to analyze the sensitivity of the results over each hyper-parameter's value. Thus we can independently analyze each hyper-parameter. In other words, we do NOT aim to optimize the hyper-parameters here, since that fine-tuning would require much more extensive study (e.g., a grid search over all combinations).\nIn addition, note that given the low variance of the results in RQ2.1, over the 10 runs, we only run the RQ3 experiment once, and due to its extensive cost (multiple runs per hyper-parameter), we also limit this RQ to the HumanEval dataset.\nThe first row in Table 5 represents the base configuration selected for this RQ. We set mutation_tool to sim_word_as_mutator and population_size to 5 to make this configuration less costly. A value of 0.25 for mutation_prob was chosen because a low value for mutation probability (as low as 0.1) is commonly used in the literature [44]. However in our algorithm, since we don't have a crossover or other evolutionary operators, we start at a bit higher rate to give the base configuration more chance to explore the search space. We will however try higher and lower mutation_prob in RQ3.2. fixed_number(10) was selected for the similarity word selection method as a robust option. We again explore this in more detail in RQ3.3."}, {"title": "5.3.1 RQ3.1: How does the complexity of the mutation generator impact the cost-effectiveness of EPiC?", "content": "We compared two mutation generator methods: (LLM_as_mutator and sim_words_as_mutator). LLM_as_mutator utilizes an LLM in the mutation process, whereas sim_words_as_mutator employs locally calculated similarity matrices for mutation. The first method is more advanced but more costly too, while the second approach, significantly simpler, incurs minimal cost.\nThe results for this RQ are presented in rows 1 and 2 of Table 5. We observed a %94 success rate (pass@1) when using LLM_as_mutator vs %93 when using sim_word_as_mutator. However, our results indicate that the local mutation tool performs at a significantly lower cost. The monetary cost of using sim_word_as_mutator is zero, whereas LLM_as_mutator incurs an additional cost of $3.9 on HumanEval, in total. This finding suggests that a simple mutation tool is more cost-effective than an LLM-based tool, although this may not hold for all datasets. Generally, LLM_as_mutator involves higher costs and is expected to produce better prompts. The selection of the prompt used in the mutation process also influences the results. The prompt may be an instruction to simply alter the words in the text or it can do more complex modifications, such as elaborating on or summarizing the original text. As explained in Section 3.2.2, we used a prompt to elaborate on the original text for the mutation process. We leave the study on the choice of prompt for mutation to future research, as it falls outside the scope of this paper."}, {"title": "5.3.2 RQ3.2: How do the mutation probability and similar word selection method impact the performance of EPiC?", "content": "The mutation probability defines the likelihood of a word being substituted with a similar word. We set the mutation_probability to 0.1, 0.25, 0.4, and 0.8 to investigate the impact of this factor on the results. The similar word selection method can be based on either a threshold or a fixed number or the best_choice. In the threshold-based method, we select words whose similarity scores with the target word exceed the defined threshold. This ensures a minimum level of similarity for any chosen replacement. In the fixed number method"}]}