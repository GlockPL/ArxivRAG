{"title": "EPIC: Cost-effective Search-based Prompt Engineering of LLMs for Code Generation", "authors": ["Hamed Taherkhani", "Melika Sepindband", "Hung Viet Pham", "Song Wang", "Hadi Hemmati"], "abstract": "Large Language Models (LLMs) have seen increasing use in various software development tasks, especially in code generation. The most advanced recent methods attempt to incorporate feedback from code execution into prompts to help guide LLMs in generating correct code, in an iterative process. While effective, these methods could be costly and time-consuming due to numerous interactions with the LLM and the extensive token usage. To address this issue, we propose an alternative approach named Evolutionary Prompt Engineering for Code (EPiC), which leverages a lightweight evolutionary algorithm to evolve the original prompts toward better ones that produce high-quality code, with minimal interactions with LLM. Our evaluation against state-of-the-art (SOTA) LLM-based code generation models shows that EPiC outperforms all the baselines in terms of cost-effectiveness.", "sections": [{"title": "1 INTRODUCTION", "content": "LLMS have been used in many software development activities such as software testing, design, requirement engineering, code generation, maintenance, deployment, etc. [30], [32]. Among these activities, code generation using LLMs has demonstrated significant potential.\nIn LLM-based code generation, various prompt engi- neering techniques, including zero-shot [5], in-context learn- ing [33], [34], RAG [35], and task-specific methods [36], [37], have been shown to outperform fine-tuned smaller models. The most advanced prompt engineering meth- ods for code generation employ various agent-based ap- proaches [28]. SOTA methods such as Reflexion [20], Lan- guage Agent Tree Search (LATS) [21], AgentCoder [22], LDB [23], and MetaGPT [29] are either planning-based or multi-collaborative agents. While effective, these methods can be costly and time-consuming due to numerous interactions with LLMs which results in extensive token usage, making them less attractive in practical settings. For instance, LATS requires on average 3 minutes to generate the implementation of a function with an average of only 6 lines of code, on the MBPP dataset.\nTo tackle this limitation, in this paper, we explicitly focus on the cost of prompt engineering for code and propose a cost-effective approach that leverages a lightweight evolu- tionary search to optimize prompts for code generation. Our evolutionary-based prompt engineering tool for code gener- ation, namely, EPiC, optimizes prompts for code generation by assessing the generated code against a fitness function, i.e., the pass rate of test cases. Our approach consists of two phases: Initial Evaluation (IE) and Evolutionary Prompt"}, {"title": "2 BACKGROUND AND RELATED WORK", "content": "In this section, we briefly explain the background of LLMs for code generation and prompt engineering, then report the most related work in the context of prompt engineering of LLM for code.\nModels for code generation can be divided into two cate- gories, RNN-based models and transformer-based models [1]. RNN-based models are older models that are outper- formed by transformers. Transformer-based code genera- tion models are transformers pre-trained on code-related datasets. For instance, CodeBERT [2] is a transformer model based on BERT [3], specifically pre-trained on the Code- SearchNet dataset [4]. Another example is CodeGPT-2, which builds upon GPT-2 [5] and is pre-trained on the same dataset. Lu et al. [6] evaluated the performance of both GPT- 2 and CodeGPT-2 in the code generation task and later, Perez et al. [7], fine-tuned GPT-2 using their dataset for code generation.\nNewer versions of LLMs, such as GPT-4, have demon- strated improved performance compared to their predeces- sors across diverse tasks, including code generation. For instance, Codex [8] is a language model developed by OpenAI based on the GPT architecture, similar to GPT- 3. However, it's specifically designed for code generation and understanding. Codex is trained on a diverse range of publicly available code repositories and strongly focuses on understanding and generating programming code. Another model, CodeLlama [9], is a family of large language models for code based on Llama 2 [10] that is available in 7B, 13B, and 34B parameters. DeepSeek Coder is another state-of-the-art LLM that has been used in software tasks such as Code Generation. This LLM has various sizes, ranging from 1B to 33B. Magicoder [11] is another model that has been fine-tuned on both CodeLlama-7b-Python and deepseek- coder-6.7b.\nIn this research, we employed two different types of LLMs: GPT-40, as an example of closed-source large models, and MagicCoder as an example of open-source smaller models. We selected GPT-40 due to its high performance among the closed-source LLMs. MagicCoder was chosen as the most cost-effective small-size open-source model, at the time of designing our experiments."}, {"title": "2.2 Prompt Engineering for LLMs", "content": "A prompt is an input or query provided to a model to generate a response or perform a task. Prompt engineering refers to the process of designing and refining prompts to achieve desired outcomes when using LLMs 1.\nThere are multiple categories of prompt engineering, including approaches without training, reasoning and logic, reducing hallucination, and evolutionary-based methods [38]. Zero-shot [5] and few-shot [33] prompting fall under the category of approaches without training. Techniques such as chain-of-thought (CoT) prompting [12], Automatic Chain-of-Thought (Auto-CoT) [39], Self-Consistency [40], and knowledge prompting [13] exemplify reasoning and logic-based methods. To reduce hallucination for prompt engineering, techniques such as Retrieval Augmented Gen- eration (RAG) [35], ReAct Prompting [41], and Chain-of- Knowledge (CoK) Prompting [42] are employed. Evolution- ary algorithms are utilized to optimize prompts, as demon- strated by EvoPrompt [16] and PromptBreeder [25]. There are other solutions such as Automated Prompt Engineer- ing [14] and Prompt Optimization with Textual Gradients (ProTeGi) [15].\nZero-shot prompting, as described in [5], eliminates the necessity for extensive training data by employing accu- rately designed prompts to direct the model toward ex- ecuting new tasks. The model receives a task description without labeled training data and employs its pre-existing knowledge to generate predictions based on the prompt. Few-shot prompting [33] uses a limited number of input-output examples to help models understand tasks, unlike zero-shot prompting which provides no examples. How- ever, this method requires additional tokens, which can be impractical for longer texts. The selection and composition of examples can also significantly influence model behavior and introduce biases.\nCoT [12] improves the reasoning abilities of large lan- guage models by incorporating intermediate reasoning steps within prompts. This technique breaks down complex tasks into smaller sub-tasks, mimicking human problem- solving. It significantly enhances performance in arithmetic, commonsense, and symbolic reasoning, especially with larger models. Auto-CoT [39] is an approach designed to enhance the reasoning capabilities of LLMs by automating the generation of intermediate reasoning steps. It automates the creation of reasoning chains and demonstrations using diversity-based sampling to generate multiple reasoning paths.\nAutomated prompt engineering approaches use an agent or a similar automated engine to interact with LLM and typically get some feedback to improve the prompt. Zhou et al. [14] proposed APE, a framework designed for the automatic generation and selection of instructions. In this framework, they used an LLM to generate instruction can- didates and then search for candidate solutions to maximize a selected score function, treating prompt engineering as an optimization problem. Pryzan et al. [15] proposed Prompt Optimization with Textual Gradients (ProTeGi), which im- proves prompts for LLMs using a non-parametric approach inspired by numerical gradient descent. It uses natural language gradients from training data to critique and edit prompts, employing a process guided by beam search and bandit selection. This approach improved the efficiency of previous prompt editing techniques across various NLP tasks, including the novel challenge of LLM jailbreak de- tection.\nEvolutionary algorithms have been used in the past for prompt engineering of LLMs in generic NLP tasks. For example, EvoPrompt [16], a framework designed to auto- mate the prompt engineering process for LLMs. EvoPrompt begins with an initial population of prompts and iteratively generates new ones using evolutionary operators such as mutation and crossover, which are performed by LLMs. Another similar approach is PromptBreeder [25], which employs task prompts to condition the LLM and mutation prompts to modify task prompts. The interplay between task and mutation prompts drives iterative improvement in PromptBreeder. Similar approaches for prompt enhance- ment using LLMs through EAs are also proposed in [26] and [27].\nIn contrast to existing evolutionary prompt engineering techniques, EPiC is particularly tailored for prompt en- gineering in coding tasks, with a fitness function defined based on the pass rate of test cases. In addition, it focuses on cost-efficiency throughout the process. To achieve this, it minimizes the calls to external LLMs and implements the mutation operator using local lightweight word embed- dings libraries."}, {"title": "2.3 Prompt Engineering of LLMs for Code Generation", "content": "Zelikman et al. [17] introduced Parsel, a framework en- abling automatic implementation and validation of com- plex algorithms with code LLMs. Using Parsel, they break down algorithmic tasks into structured descriptions written in natural language, then explore various combinations of function implementations using tests.\nDyLAN [19] is a framework designed to boost the performance of Large Language Model (LLM) agents by assembling them into a dynamic team that adapts to dif- ferent tasks. Unlike traditional methods with fixed agent sets, DyLAN's architecture adjusts dynamically to the task query, enhancing its versatility. It employs features like inference-time agent selection and early stopping to im- prove efficiency and effectiveness. Furthermore, DyLAN incorporates an automatic agent team optimization algo- rithm based on an unsupervised metric called Agent Im- portance Score, which selects the most effective agents for each task. Empirical results show DyLAN's success in tasks like reasoning and code generation, achieving significant improvements over single LLM executions. Reflexion [20] is a reinforcement-based framework for this problem where language agents learn from linguistic feedback rather than weight updates. Agents reflect on task feedback verbally and maintain their own reflective text in memory, which helps them make better decisions in subsequent trials. Reflexion is adaptable to different types and sources of feedback signals and shows significant improvements over baseline agents across various tasks. LATS (Language Agent Tree Search) [21] is another search-based framework that combines LLMs' abilities in planning, acting, and reason- ing. LATS draws inspiration from the Monte Carlo tree search and repurposes LLMs' strengths as agents, value functions, and optimizers. Crucially, LATS incorporates an environment for external feedback to enable more deliberate and adaptive problem-solving beyond existing techniques' limitations. AgentCoder [22] utilizes a multi-agent frame- work with specialized agents: the programmer agent, the test designer agent, and the test executor agent. The pro- grammer agent focuses on code generation and refinement based on feedback from the test executor agent, while the test designer agent generates test cases for the code. The test executor agent runs the code with the test cases and provides feedback to the programmer. This collaborative system aims to improve code generation efficiency, out- performing single-agent models and previous strategies.\nZhong et al. [23] introduced Large Language Model De- bugger (LDB), a framework that segments programs into basic blocks and tracks intermediate variable values during runtime. LDB enables LLMs to focus on simpler code units, verify their correctness block by block, and pinpoint errors effectively.\nIn contrast to existing methods and to the best of our knowledge, EPiC is the first search-based prompt engineer- ing method for code generation. It employs a lightweight process to identify the optimal solution in a cost-efficient manner. To achieve this, EPiC utilizes a local embedding function to implement mutation operators on text, to reduce the cost of iterative prompt engineering for code generation. It also guides the search over iterations using the fitness function in Section 4.4, which helpsfindg the most effective prompts."}, {"title": "3 EVOLUTIONARY PROMPT ENGINEERING FOR CODE", "content": "This section provides a detailed elaboration of EPiC. As depicted in Figure 2, our approach comprises two phases, i.e., the Initial Evaluation (IE) phase and the Evolutionary Prompt Engineering (EPE) phase. The process begins with a user providing an initial prompt that describes the intended functionality of the code to be generated. In the IE phase, we evaluate the prompt by generating the code based on the original prompt using an LLM. This evaluation determines whether the prompt is sufficient to generate the correct implementation or if it requires a further process in the EPE phase. EPiC's algorithm is presented in Algorithm 1, which will be elaborated upon in the subsequent sections. Lines 2 to 6 in Algorithm 1 correspond to the IE phase of EPiC and lines 7 to 30 are the implementaion of the EPE phase."}, {"title": "3.1 Initial Evaluation", "content": "In Step 1 of the IE phase, the initial prompts for code and test case generation are provided to the LLM (the choice of LLM to be used in our case will be discussed in the Section 4) to generate the corresponding code and test cases. Subsequently, in Step 2, the generated code and test cases are sent to the Testcase Evaluation module. evaluatePrompt in Line 4 of Algorithm 1 is responsible for generating the code and evaluating the test cases. If any test case fails on the code we continue with the EPE phase. If all the test cases pass, we stop and report the generated code as the final answer.\nNote that test cases can be provided in various ways. One approach is to use developer-provided test cases for evaluation, while another is to generate test cases using LLMs. The challenge with developer-provided test cases is that typically users do not have test cases before im- plementation, making the approach impractical for a code generation task. On the other hand, the advantage of using LLM-generated test cases is that it makes the process fully automated. This method is currently used in most SOTA code generation tools [20], [21], [22], [18], and [29], as part of their internal evaluation. Consequently, we also opted to use LLMs for test case generation to ensure a fully automated approach, assuming no developer-provided test cases. Specifically, we employed OpenAI's GPT-40 model to generate test cases with few-shot examples. To ensure the functional correctness of these test cases, we validated them by parsing their Abstract Syntax Trees (AST). A test case is considered valid if it successfully parses into a syntactically"}, {"title": "3.2 Evolutionary Prompt Engineering", "content": "In the EPE phase, the first step (Step 3) involves popu- lating the first generation of individuals, where each indi- vidual is a distinct prompt to generate the same code. In Step (3), we generate multiple prompts by modifying the initial prompt using an LLM agent. Generating this prompt population forms an important part of our evolutionary algorithm. A more detailed description of this process will be provided in Section 3.2.1. This step corresponds to line 8 of Algorithm 1.\nIn Step (4) of EPE which corresponds to lines 12 to 20 in Algorithm 1, we generate code for each prompt and evaluate each generated code sample using the same test cases. We define a fitness function based on the ratio of test cases passed. If any of the prompts achieves the maximum fitness score, the process stops. If not, the process continues until all prompts are validated and given a fitness score, which will guide the selection process. The fitness function is basically the passing rate of each prompt. The mathematical formula is explained in Section 4.4.\nThis fitness function is used to select the candidate prompts for mutation in Step 5. This is implemented in chooseCandidate function in Algorithm 1 (Lines 22 and 26), where a weighted random selection algorithm selects candidates randomly, with the probability of selection being proportional to their respective fitness score (Section 4.4). This selection occurs with substitution, allowing the same prompt to be chosen multiple times. In Step 5, we select N-1 candidates for mutation and one prompt for \u201celitism\u201d.\nFollowing prompt selection, in Step 6, EPiC randomly mutates the selected prompt candidates to better explore the search space of potential prompts in the next generation. The prompt mutation which is detailed in Algorithm 2, is elaborated in Section 3.2.2. The next generation is formed by adding the elite prompt to the pool of mutated prompts. Lines 21 to 27 in Algorithm 1 corresponds to Steps 5 and 6\nThe process of EPE is repeated iteratively until a solu- tion achieves maximum fitness score or until predefined stopping criteria, i.e., reaching the maximum number of iterations or observing no improvement in the fitness scores, are met. In such cases, the best-generated code, determined by its fitness, is chosen and returned (line 29 of Algorithm 1).\nIn the next two subsections, we will explain more details about Step 3 (Initial Population Builder), and Step 6 (Prompt Mutation.)"}, {"title": "3.2.1 Initial Population Builder", "content": "This subsection provides a detailed explanation of the Step 3 in Figure 2. Human-written prompts are often very brief and lack the necessary information or context. This results in insufficient descriptions and a failure to adhere to a well-defined format [43]. Thus manual prompting typically results in several back-and-forth interactions with LLM until all necessary details are given [28]. Providing the initial prompt in a structured and elaborate format will guide the LLM to generate better results with fewer interactions [31].\nIn our evolutionary algorithm (Algorithm 1), we require an initial population of prompts. To create it, we utilize OpenAI's GPT-40 to generate multiple prompts based on the initial given prompt. The prompt we use is as follows:\nPlease rewrite the function description based on these instructions: 1- Add input and output types of the function to the description. 2- Elaborate the description so that it is understandable for large language models. 3- Keep the original test cases and add three test cases to the description to cover the edge cases. Do not separate the generated test cases and the original ones. 4- Keep the structure of the function and add the description as a comment in the function. Use at most 500 words.\nWe employ a high-temperature setting (0.9) for the LLMto stimulate creativity in prompt generation. Elaborate prompts, which include explicit input-output types and test cases, more effectively direct the language model to produce the desired behavior compared to the original prompts."}, {"title": "3.2.2 Prompt Mutation", "content": "This subsection provides a detailed explanation of the Step (6) in Figure 2. We employed two kinds of prompt mutation approaches. In the first approach, EPiC uses an LLM to facilitate the mutation process (LLM_as_mutator). This ap- proach is inspired by a framework named EvoPrompt [16]. In LLM_as_mutator, we provide LLMs with predefined instructions on how to implement the prompt mutation.\nIn the second approach, we employ Natural Language Processing (NLP) techniques using the NLTK and Gensim libraries to alter the prompts by substituting words with their synonyms (sim_words_as_mutator). This method is our contribution as a local lightweight prompt mutation process. The main motivator is to reduce the cost of the iterative part of the EA. This prompt mutation algorithm (sim_words_as_mutator) is described in Algorithm 2. We will compare the performance of both approaches in the fol- lowing sections. We use the following prompt for mutation using LLMs:\nYou are a mutation tool. This is a Python function and its descrip- tion. Please mutate the description by enhancing its clarity and comprehension for sophisticated language models. Please put the changed description between #Explanation and #End. Use at most 600 words.\nIn the sim_words_as_mutator algorithm (Algorithm 2), we first extract the description part of the prompt. For each word in the sentence, if the word is not a stop word or proper noun, it is lemmatized, and the get_related_words() function (line 10 in Algorithm 2) is called to get its related words. To obtain the related words for a given word, we use WordNet and gensim's GloVevectors (fasttext - wiki news subwords - 300) to calculate the cosine similarity between the word and other related words. We rank the related words to create a list of the most similar words to the original word, and based on the specified num_t or sim_t, we filter out the least related words and keep words that pass the given threshold. Then, we randomly substitute the word with one of the related words based on a mutation_probability."}, {"title": "4 EXPERIMENT DESIGN", "content": "We define the following research questions to explore the performance of EPiC:\n\u2022 RQ1: How cost-effective are the SOTA LLM-based code generation approaches? Motivation: Most of the SOTA LLM-based code generation studies do not in- clude a comprehensive approach to evaluate cost- effectiveness. In this RQ, we set out to evaluate these tools from the cost-effectiveness perspective.\n\u2022 RQ2: How effective is EPiC in code generation? Motivation: In this RQ, we explore the performance of EPiC against SOTA LLM-based code generation tools in terms of both cost and effectiveness. We experiment with a large closed-source LLM (RQ2.1) and a smaller open-source model(RQ2.2)\nRQ2.1: How cost-effective is EPiC compared to the SOTA?\nRQ2.2: How effective is EPiC on smaller open- source LLMs compared to larger closed-source mod- els?\n\u2022 RQ3:What are the impacts of different EA design choices on the cost-effectiveness of EPiC? Motivation: The goal of this RQ is to explore the impact of different hyperparameters in our approach. We define three sub- RQs to address the impact of the mutation generator (RQ3.1), mutation probability and similar word selec- tion (RQ3.2), and population size (RQ3.3), as follows:\nRQ3.1: How does the complexity of the mutation generator impact the cost-effectiveness of EPiC?\nRQ3.2: How does the mutation probability and similar word selection impact the performance of EPiC?\nRQ3.3: How does population size impact the cost- effectiveness of EPiC?"}, {"title": "4.2 Datasets and Models", "content": "We used two datasets for our experiments: HumanEval [8] and MBPP [24].\nThe HumanEval is a collection of 164 programming prob- lems designed to evaluate the functional correctness of code generated by Al models. Each problem includes a function signature, a detailed description of the task (docstring), the function body, and multiple unit tests to verify the solution. One major obstacle in making fair judgments on the per- formance of the related works was that some papers did not use the HumanEval dataset as is. For instance, there are cases where the samples are removed or changed. To have a fair comparison among all methods we transformed the original dataset into each tool's required format (if needed) to re-run their code using the original dataset. We will explain this in more detail as we discuss the configuration setup for assessing each related work.\nThe Mostly Basic Programming Problems (MBPP) com- prises 974 short Python programming tasks designed to be solvable by entry-level programmers. The tasks were crowd-sourced from individuals with basic Python knowl- edge, who provided a problem statement, a self-contained Python function solving the problem, and three test cases to check for correctness. A subset of 427 tasks was manually inspected and edited for consistency and accuracy, referred to as mbpp-sanitized. In this paper, we used mbpp-sanitized for our experiments. While experimenting with the SOTA, we discovered that they have used different subsets of the MBPP dataset. For this reason, their reported results are not comparable. Consequently, we chose to use the mbpp- sanitized subset in all our experiments.\nFor RQ1 and RQ2.1, we used both datasets. For RQ2.2 and RQ3, we used only HumanEval. We will explain this in more detail in the corresponding RQs.\nWe employed two LLMs in our study: OpenAI's GPT- 402 and Magicoder-S-DS-6.7B [11]. The latter model was"}, {"title": "4.3 Evaluation Metrics", "content": "To evaluate the quality of the generated code by LLMs, we utilized the pass@k metric. This metric, introduced in [8], is advantageous over metrics such as CodeBLEU and ROUGE because it evaluates code behavior rather than text semantics, which are the focus of the latter metrics. The formula for pass@k is presented in Equation 1. In this equation, E denotes the expected value over the problems, n is the total number of generated samples per task (code samples) and c is the number of correct samples. For a single problem, this metric estimates the probability that at least one of top k samples is correct. When applied to multiple problems, it evaluates the expectation across all problems.\npass@k := E 1 Problems ( (n-c) k ) / (n k) ) (1)\nWhen comparing EPiC with baselines, we also report Mann-Whitney U-test results, which is a non-parametric significant test, to consider the randomness introduced by our algorithm. The tests compare the distribution of 10 pass@1 results for EPiC (using 10 random seeds) with the pass@1 of each baseline, separately. Note that here we only rerun EPIC with 10 random seeds to consider its internal randomness introduced in the algorithm and do not rerun the baselines as they do not have internal randomness. In other words, we do not study the potential randomness of LLMs results and their effect on each baseline, as this is not reported in the original papers and would go beyond the scope of this paper."}, {"title": "4.4 Objective Function", "content": "We use X and Z to denote the prompt space and output responses respectively. Let S : X \u2192 Z be the LLM that takes an input x \u2208 X and outputs a response z \u2208 Z. The output response is the code implementation of the given prompt. Let's say we have n number of test cases. Consider T as a set of test case functions. A test case t\u2081 \u2208 T is a function that maps any output response z to either 0 or 1. It can be formally defined as t\u2081 : Z \u2192 {0,1}. The fitness function which is denoted as F : X,T \u2192 [0,1], maps any input prompt to a value between 0 and 1. The fitness function is defined as follows:\nF(x,T) = ( \u03a3i=1^n ti(S(x)) ) / n (2)\nThe goal of the EPiC framework is to optimize the fitness function F based on the provided test cases T by identifying the optimal input x within the prompt space X."}, {"title": "4.5 Baselines", "content": "To evaluate the performance of EPiC, we first identified the most relevant papers in code generation that utilized a common benchmark for comparison, i.e., papers that used HumanEval or MBPP, either exclusively or as part of their benchmarks. These two benchmarks were chosen due to their popularity in the field. Further narrowing down the selection, we applied three additional criteria: (1) whether their results were competitive (more than %90 pass@1 on HumanEval), (2) whether their GitHub repository is avail- able online, and (3) whether their cost is not disproportionately high. The third criterion was specifically defined to exclude SOTA tools with excessively high costs, ensuring that the selected tools remained competitive in terms of cost-effectiveness. For comparison, we used the pass@1 metric. As a result, four papers are selected as our baselines, i.e., Reflexion[20], LATS[21], LDB[23], and AgentCoder[22] (details are in Sec- tion 2). For each baseline, we ran the experiments with the recommended configuration provided in its GitHub repository or provided in its paper. After initial evaluation, AgentCoder[22] was excluded from further analysis as we found out that it's 6 times more expensive than the second most expensive alternative. The baselines' configuration details used in our experiments are as follows:\nReflexion: We ran Reflexion with max_iters set to 2, strategy set to \"reflexion\", pass_at_k set to 1, and model set to \u201cgpt4o\u201d. Note that, we have adapted Reflexion's code base to support the \"gpt4o\" model and tracking of time cost as these features were unavailable in Reflexion's original version. During our experiments, we found inconsistencies between the HumanEval dataset on their GitHub and the original dataset. There were missing instances and modified or removed test cases compared to the original dataset. We reported this issue in their repository\u00b3. To ensure a valid assessment of their work, we used the original HumanEval dataset.\nLATS: We ran the experiments with max_iters set to eight, number_of_tests set to two, strategy set to ", "py": "pass@k set to 1, and model set to \u201cgpt4o\u201d. LATS used the same code base as Reflexion, result- ing in the same issues with the HumanEval dataset. Similar to Reflexion, there were missing instances and modified or removed test cases compared to the original dataset. We also implemented the", "gpt4o": "odel and tracking of time cost features in its codebase. Note that, we have also identified and fixed an issue in LATS's code related to counting the number of successful instances. In its codebase, success, which is used to calculate accuracy (pass@k), was incremented when the generated code passed the generated test cases. However, success should be defined as passing the original test cases, not the generated ones. We corrected this issue in their code (and reported it to the authors) before running the experiments. While they have now resolved this issue in their codebase yet, the results in their paper had not been updated at the time of writing this paper\u2074. Like Reflexion, they also used LLM-generated test cases for internal evaluations.\nLDB: We set pass@k to 1, n_proc to 1, max_iters to 10, strategy to \u201cldb"}, {"gpt4o": "Consistent with previous works, we implemented the"}, {"gpt4o": "odel and tracking of time cost features as they were unavailable in the original version. The authors used seed programs in their code to enhance their results. The seed programs served as the initial implementation of the code. We re- moved these seed files to ensure fairness, as other baselines do not use any seeds (an initial given output code) to boost their results. Upon reviewing their dataset, we found no mismatches with the original dataset. However, we iden- tified inconsistencies between the actual test cases in the function descriptions and the test cases they used as their visible test cases in the codebase, in HumanEval dataset. It appears that some test cases were manually modified. This poses an issue for a fair comparison. In instances where we identified such cases, we adjusted them to ensure no manual intervention. As a result, we replaced the original extracted test cases with the visible test cases to ensure a fair comparison before conducting our experiments and we reported this issue in their GitHub repository to be fixed5.\nFor the MBPP experiments, LATS and Reflexion em- ployed the same code generation approach to produce test cases, using these for internal evaluations. In contrast, LDB utilized the test cases within the MBPP dataset for the internal evaluation. We determined that using the original test cases for internal evaluation is unfair to other baselines and impractical, as these test cases are meant solely for final evaluation, not for use during the process (in practice, test cases are not typically available before the code is devel- oped). Consequently, we modified their test cases to use the generated test cases produced by the method employed in LATS and Reflexion, so we can have a practical approach implemented across the baselines and a fair comparison."}, {"title": "4.6 Experimental Setup", "content": "For all the experiments, we selected gpt-40 as the base LLM model, because it is OpenAI's latest model, offering superior speed and cost-efficiency while maintaining equal intelligence compared to GPT-4. We conducted experiments on the HumanEval and MBPP datasets, utilizing the base- line methods in RQ1 and EPIC in RQ2.\nOur default configuration setups with EPiC are pre- sented in Table 1. The default configuration is chosen based on the experiments in Section 5.3. Fine-tuning EPiC's hyper-parameters properly (e.g., using a grid search over all combinations of input space in the hyper-parameters) would be extremely expensive. However, RQ3's sensitiv- ity analysis already provides an acceptable (not optimal) configuration to set as our default. We used the default configuration for RQ2.1 and changed it in the subsequent RQs, which we will explain later. In the default config- uration, we utilized OpenAI's GPT-40 as the code gen- erator LLM agent. We evaluated our approach on both"}, {"title": "5 EXPERIMENT RESULTS", "content": "We evaluated LATS, LDB, and Reflexion on the entire HumanEval and Sanitized MBPP datasets. To monitor the monetary costs, we used the OpenAI dashboard. This cost is calculated based on the number of tokens exchanged with the OpenAI API. As the financial cost is calculated based on the number of exchanged tokens, we chose the dollar cost reported in the OpenAI dashboard as the metric to measure the cost of an approach. The results of this experiment are presented in Table 2, for HumanEval and MBPP.\nThe first observation is that there is a notable discrepancy between our achieved pass@1 results and those reported in the original papers of the baselines. One possible reason is that we used GPT-40 for our experiments, whereas the orig- inal studies used other OpenAI models. Another plausible reason could be inconsistencies between the original dataset and the ones they used, as we utilized the original dataset while they used a modified version.\nReflexion incurred the lowest cost in both datasets but also demonstrated the lowest performance, with an overall accuracy of %75 and a cost of \u00a21.1 per instance. Conversely, LATS exhibited the highest time and dollar costs in both datasets, with a total cost of $96.71 in total and a cost of"}, {"title": "5.2 RQ2: How effective is EPiC in code generation?", "content": "In addressing RQ2.1, we accounted for the inherent ran- domness in our algorithm by testing EPiC with different seeds. The seed was utilized in Python's random function,"}, {"title": "5.2.1 RQ2.1: How cost-effective is EPiC compared to the SOTA?", "content": "We evaluated EPiC on HumanEval and MBPP, each 10 times using 10 unique random seeds. The statistical results for this experiment are presented in Table 3. The results on HumanEval indicate that the median pass@1 is %93.5. Specifically, in 5 out of 10 runs, we achieved %93, in 1 run we achieved %94, and in 4 out of 10 runs, we achieved %95. These results demonstrate that EPiC consistently outper- forms the baselines in all runs. The variance of the results is 0.89, indicating the robustness of EPiC. This demonstrates that the algorithm's performance remains stable despite inherent randomness. The comparison between EPiC (on median pass@1) and the SOTA on Humaneval is presented in Figure 5.\nIn terms of cost, EPiC demonstrated a much lower cost than LATS"}]}