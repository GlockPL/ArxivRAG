{"title": "YINYANG-ALIGN: Benchmarking Contradictory Objectives and Proposing Multi-Objective Optimization based DPO for Text-to-Image Alignment", "authors": ["Amitava Das", "Yaswanth Narsupalli", "Gurpreet Singh", "Vinija Jain", "Vasu Sharma", "Suranjana Trivedy", "Aman Chadha", "Amit Sheth"], "abstract": "Precise alignment in Text-to-Image (T2I) systems is crucial to ensure that generated visuals not only accurately encapsulate user intents but also conform to stringent ethical and aesthetic benchmarks. Incidents like the Google Gemini fiasco, where misaligned outputs triggered significant public backlash, underscore the critical need for robust alignment mechanisms. In contrast, Large Language Models (LLMs) have achieved notable success in alignment. Building on these advancements, researchers are eager to apply similar alignment techniques, such as Direct Preference Optimization (DPO), to T2I systems to enhance image generation fidelity and reliability.\nWe present YinYangAlign, an advanced benchmarking framework that systematically quantifies the alignment fidelity of T2I systems, addressing six fundamental and inherently contradictory design objectives. Each pair represents fundamental tensions in image generation, such as balancing adherence to user prompts with creative modifications or maintaining diversity alongside visual coherence. YinYangAlign includes detailed axiom datasets featuring human prompts, aligned (chosen) responses, misaligned (rejected) AI-generated outputs, and explanations of the underlying contradictions.\nIn addition to presenting this benchmark, we introduce Contradictory Alignment Optimization (CAO), a novel extension of DPO. The CAO framework incorporates a per-axiom loss design to explicitly model and address competing objectives. Then it optimizes these objectives using multi-objective optimization techniques, including synergy-driven global preferences, axiom-specific regularization, and the novel synergy Jacobian for effectively balancing contradictory goals. By utilizing tools such", "sections": [{"title": "Why and How T2I Models Must Be Aligned?", "content": "The alignment of T2I models is essential to ensure that generated visuals faithfully represent user intentions while adhering to ethical and aesthetic standards. This necessity is underscored by projections from EUROPOL, which estimate that by the end of 2026, approximately 90% of web content will be generated by AI (EUROPOL, 2023). The widespread use of AI-generated content underscores the need for robust alignment mechanisms to prevent misleading, biased, or unethical visuals. The recent announcement by social media"}, {"title": "YinYangAlign: Six Contradictory Alignment Objectives", "content": "Current research and benchmarking in T2I alignment primarily focus on isolated objectives (Guo et al., 2022), such as fidelity to prompts (Ramesh et al., 2021), aesthetic quality (Rombach et al., 2022), or bias mitigation (Zhao et al., 2023), often treating these goals independently. However, there is a clear gap in benchmarks that evaluate how T2I systems balance multiple, often contradictory objectives. The lack of multi-objective benchmarks restricts the ability to holistically assess and improve T2I alignment, ultimately affecting their reliability and effectiveness in practical scenarios.\nSelection of Six Contradictory Objectives:\nYinYangAlign introduces six carefully selected pairs of contradictory objectives that capture the fundamental tensions in T2I image generation. These pairs are chosen for their relevance and significance in real-world applications. Fig. 1 introduces the core trade-offs central to the YinYangAlign framework, each representing a critical conflict that T2I systems must navigate to balance user expectations and ethical considerations. The trade-offs include: Faithfulness to Prompt vs. Artistic Freedom, which involves adhering to user instructions while minimizing creative deviations; Emotional Impact vs. Neutrality, requiring a balance between evoking emotions and maintaining objective representation; and Visual Realism vs. Artistic Freedom, focusing on achieving photorealistic outputs without compromising artistic liberties. Additionally, Originality vs. Referentiality addresses the challenge of fostering stylistic innovation while avoiding reliance on established artistic styles to ensure uniqueness. Verifiability vs. Artistic Freedom emphasizes balancing factual accuracy with creative liberties to minimize misinformation. Finally, Cultural Sensitivity vs. Artistic Freedom underscores the need to respect cultural representations while ensuring that creative freedoms do not lead to misrepresentation or insensitivity. Fig. 2 provides illustrative examples of these alignment axioms.\nYinYangAlign serves as a holistic benchmark for evaluating alignment performance, ensuring that T2I models are not only accurate and reliable but also adaptable, ethical, and capable of meeting complex user demands and societal expectations."}, {"title": "YinYangAlign: Dataset and Annotation", "content": "The development of YinYangAlign employs a carefully designed annotation pipeline to enable a comprehensive evaluation of T2I systems. To overcome the inherent stochasticity of T2I models and the subjective complexities of visual alignment, we propose a hybrid annotation pipeline. This pipeline leverages advanced vision-language models (VLMs) for automated identification of misalignments, augmented by meticulous human validation to ensure scalability and reliability. This hybrid strategy ensures scalability while maintaining high annotation reliability, resulting in a robust and reliable benchmark. The subsequent sections outline the models, data sources, and annotation methodology utilized in the creation of YinYangAlign.\nT21 Models Utilized: For our data creation, we utilize state-of-the-art T21 models such as Stable Diffusion XL (Podell et al., 2023), and Midjourney 6 (Midjourney, 2024).\nPrompt Sources: To construct the YinYang dataset, we strategically selected diverse datasets to cover the six contradictory alignment axioms. For the first three axioms\u2014Faithfulness to Prompt vs. Artistic Freedom, Emotional Impact vs. Neutrality, and Visual Realism vs. Artistic Freedom\u2014we uti-"}, {"title": "Annotation Pipeline", "content": "Annotation process involves the following steps:\nMultiple Outputs per Prompt: To account for the stochastic nature of T2I systems, we generate 10 distinct outputs for each prompt to capture the variability in the generated visuals. Fig. 3 illustrates an example of how the same prompt can produce diverse images due to this inherent randomness.\nAutomated Annotation Using VLMs: We employ two VLMs: GPT-4o (OpenAI, 2023) and LLaVA (Liu et al., 2023), to annotate the generated images. The annotation is guided by the following prompt. See more examples of prompts in Appendix B.\nConsensus Filtering: To improve annotation reliability, we utilized LLaVA-Critic (Xiong et al., 2024) and Prometheus-Vision (Lee et al., 2024), for independently scoring of each generated image. Since these models are fine-tuned on pointwise and/or pairwise ranking data, they are specialized for grading tasks. This fine-tuning enables them to consistently and effectively assess the quality and relevance of generated content. Images were selected for human verification only when both VLMs produced consistent annotations, specifically when the scores for a given axiom were \u2265 3. This approach ensured higher confidence in the automated annotation process before proceeding to manual review.\nHuman Verification: Ten human annotators evaluated approximately 50,000 images flagged by the VLMs. To measure inter-annotator agreement, a subset of 5,000 images was annotated by all 10 annotators, achieving a kappa score of 0.83, demonstrating high consistency and reliability in the annotation process. During the manual review, approximately 10,000 images were discarded due to quality issues, resulting in the final YinYangAlign benchmark consisting of 40,000 high-quality images. Fig. 4 presents the kappa scores comparing agreement levels between human annotators and machine (VLM) evaluations across six contradictory alignment axioms, highlighting areas of alignment and divergence. The entire annotation process, including model-based tagging and human verification, spanned 11 weeks. cf Appendix B."}, {"title": "Contradictory Alignment Optimization (CAO)", "content": "The YinYangAlign framework, models the challenge of balancing inherently contradictory objectives. For example, prioritizing Faithfulness to Prompt can limit Artistic Freedom, while emphasizing Emotional Impact may erode Neutrality. To address these tensions, we introduce Contradictory Alignment Optimization (CAO), which employs a per-axiom loss design to explicitly model competing goals. CAO employs a dynamic weighting mechanism to prioritize sub-objectives within each axiom, facilitating granular control over trade-offs and enabling adaptive optimization across diverse alignment paradigms. Additionally, CAO integrates Pareto optimality principles with the Bradley-Terry preference framework, introducing a novel global synergy mechanism that unifies all contradictory objectives into a cohesive optimization strategy. This unique combination of multi-objective synergy defines the core innovation of CAO, distinguishing it from existing T2I alignment methods."}, {"title": "Axiom-Wise Loss Expansion and Synergy", "content": "Local Axiom-Wise Loss : Below, we illustrate how each axiom's loss is defined, before showing how these losses connect into a global synergy framework. For each axiom a, CAO defines a loss function $f_a(I)$ that blends two competing sub-objectives, $L_p(I)$ and $L_q(I)$, via a mixing parameter $\u03b1_a$:\n$f_a(I) = \u03b1_a L_p(I) + (1 \u2212 \u03b1_a) L_q(I)$.\nFor example, $L_p(I)$ might emphasize faithfulness to prompt, while $L_q(I)$ favors artistic freedom, or any other pair of conflicting objectives. Varying $\u03b1_a$ adjusts the per-axiom balance according to domain or policy needs.\nThe resulting loss surfaces and their corresponding sweet spots, where competing objectives are in harmony, are visualized in Fig. 5.\nMulti-Objective Aggregator and Pareto Frontiers: Although $f_a(I)$ provides local control over each axiom a, reconciling multiple axioms at once requires a global view. We thus define a multi-objective synergy function:\n$S(I) = \\sum_{a=1}^{A} w_a f_a(I)$,\nwhere the ${w_a}$ are global coefficients reflecting the relative priority of each axiom. By varying these synergy weights, we trace out a Pareto frontier (Miettinen, 1999; Yang et al., 2021; Lin et al., 2023) in the T2I objective space, clarifying how small concessions in one axiom can yield major gains in another.\nInterpretation and Importance. In multi-objective optimization, the Pareto frontier is the set of all solutions where improving any one objective strictly worsens at least one other (?Zhou et al., 2022). By tuning ${w_a}$, we systematically explore these tradeoffs, finding, for example, that a slight drop in visual realism could allow for notably higher stylistic freedom. Such multi-objective approaches have been central in multi-task learning (Ma et al., 2020; Navon et al., 2022; Yu et al., 2020) and modular/decomposed learning (Liebenwein et al., 2021; Lin et al., 2022), ensuring transparent control over each tension point (e.g., verifiability vs. creativity) and easy adaptation to new constraints. cf Appendix C."}, {"title": "Connecting Synergy to Pairwise Preference", "content": "To fully implement both local axiom-wise guidance and global synergy-based tradeoffs, we integrate the synergy function into the DPO framework. Concretely, each $f_a(I)$ enters a Bradley-Terry style preference:\n$P_{ij}^{a} = \\frac{exp(f_a(I_i))}{exp(f_a(I_i)) + exp(f_a(I_j))}$,\nensuring local interpretability for each axiom. Meanwhile, a combined preference over $S(I)$ expresses the global tradeoff:\n$P_{ij}^{S} = \\frac{exp(S(I_i))}{exp(S(I_i)) + exp(S(I_j))}$.\nA hyperparameter $\u03bb$ then balances how much this global synergy affects the final optimization vs. how much weight is given to local per-axiom preferences:\n$L_{CAO} = \u03bb\\sum_{(i,j)} -log(P_{ij}^{S}) + \\sum_{a=1}^{A} -log(P_{ij}^{a})$."}, {"title": "Unified CAO Loss", "content": "We can consolidate the local and global preferences into a single loss function. One straightfor-"}, {"title": "Putting It All Together: Final CAO Formulation", "content": "Bringing together the synergy function, local Bradley-Terry preferences, and axiom-specific regularization leads to the final CAO objective:\n$L_{CAO} = \u03bb\\sum_{(i,j)} -log(P_{ij}^{S}) + \\sum_{a=1}^{A} -log(P_{ij}^{a}) + T_a R_a$.\nRole of the Synergy Jacobian (Js): The Synergy Jacobian Js is a vital component in managing gradient interactions across multiple axioms during training. While the regularization parameter \u03bb balances local and global objectives, Js quantifies how updates to model parameters for one axiom impact the alignment of others. Mathematically, Js is defined as:\n$J_S = \\frac{\\partial S(I)}{\\partial \\theta}$,\nwhere S(I) represents the synergy aggregator that measures overall alignment, I denotes the input, and \u03b8 are the model parameters. This Jacobian provides a structured view of the interdependencies among axioms, capturing how conflicting objectives influence each other (Navon et al., 2022; Yu et al., 2020).\nIntuition and Practical Role: During training, gradients for individual axioms often conflict, resulting in updates that disproportionately favor one objective at the expense of others. The Synergy Jacobian addresses this issue by scaling or adjusting gradients based on their interactions with the synergy aggregator S(I). Specifically:\nGradients that align well with improving overall synergy are preserved to maintain their positive contribution.\nGradients that disproportionately benefit a single axiom while adversely affecting others are scaled back to ensure balance across objectives.\nThe parameter update during training can be expressed as:\n$\\Delta \\theta = \\eta \\cdot \\nabla L - \\alpha \\cdot J_s$,\nwhere \u2207L is the standard gradient of the loss, \u03b7 is the learning rate, and \u03b1 is a scaling factor controlling the influence of the Synergy Jacobian. This"}, {"title": "Axiom-Specific Loss Function Design", "content": "We now expand each of the axiom-wise losses introduced previously: $L_{artistic}$, $L_{faith}$, $L_{emotion}$, $L_{neutral}$, $L_{originality}$, $L_{referentiality}$, $L_{verifiability}$, $L_{cultural}$. $L_{artistic}$. Note that $L_{artistic}$ appears in four of the six axioms, but the core design of the artistic loss remains consistent across all such instances. cf Appendix L.\nArtistic Freedom: $L_{artistic}$\nThe Artistic Freedom Score (AFS) measures how much creative enhancement a generated image Igen receives, relative to a baseline Ibase. It comprises three components:\n1. Style Difference: Gauges stylistic deviation using VGG-based Gram features (Gatys et al., 2016; Johnson et al., 2016), a widely adopted approach in neural style transfer for capturing higher-order correlations that define an image's aesthetic characteristics:\n$StyleDiff = ||S(I_{gen}) - S(I_{base})||_2$.\nHere, S(\u00b7) represents a pretrained style-extraction network.\n2. Content Abstraction: Evaluates how abstractly Igen interprets the textual prompt P. Formally,\n$ContentAbs = 1 \u2212 cos(E(P), E(I_{gen}))$,\nwhere E(\u00b7) is a multimodal embedding model (e.g., CLIP) (Radford et al., 2021). Higher ContentAbs indicates stronger abstraction away from literal prompt details. This concept of content abstraction draws inspiration from prior cross-modal research (Zhang et al., 2021; Mou et al., 2022), which highlights how multimodal embeddings can bridge prompt semantics and visual representations (Lei et al., 2023; Gupta et al., 2023).\n3. Content Difference: Measures deviation from the baseline image:\n$ContentDiff = 1 - cos(E(I_{gen}), E(I_{base}))$.\nThis term ensures the generated image does not diverge excessively from Ibase, acting as a mild regularizer for subject fidelity.\nWe define:\n$AFS = \u03b1 StyleDiff + \u03b2 ContentAbs + \u03b3 ContentDiff$.\nBy default, we set \u03b1 = 0.5, \u03b2 = 0.3, and \u03b3 = 0.2 based on empirical tuning. Omitting ContentDiff may boost artistic freedom but risks straying too far from baseline subject matter, reflecting the inherent tension between creativity and fidelity.\nCalculating the AFS for the images in Fig. 3 using the first image as the reference yields: Chosen 1 and Chosen 2 with moderate AFS scores of 0.80 and 0.82, indicating minimal artistic deviation. In contrast, the Rejected images score higher, with Rejected 1, Rejected 2, and Rejected 3 achieving 0.99, 1.06, and 0.87 respectively, reflecting greater abstraction and stylistic deviation. AFS ranges are defined as Low (0.0-0.5), Moderate (0.5-1.0), and High (1.0-2.0), capturing the balance between prompt adherence and artistic creativity."}, {"title": "Faithfulness to Prompt: Lfaith", "content": "Faithfulness to the prompt is a cornerstone of T2I alignment, ensuring that generated images adhere to the semantic and visual details specified by the user. To evaluate faithfulness, we leverage a semantic alignment metric based on the Sinkhorn-VAE Wasserstein Distance, a robust measure of distributional similarity that has gained traction in generative modeling for its interpretability and effectiveness (Arjovsky et al., 2017; Tolstikhin et al., 2018). The Faithfulness Loss is formulated as:\n$L_{faith} = -W(P(Z_{prompt}), Q(Z_{image}))$,\nwhere:\n$P(Z_{prompt})$ and $Q(Z_{image})$ are the latent distributions of the textual prompt and the generated image, respectively, extracted using a Variational Autoencoder (VAE).\nW denotes the Sinkhorn-regularized Wasserstein Distance, which facilitates computational efficiency and stability (Cuturi, 2013)."}, {"title": "Emotional Impact Score (EIS): Lemotion", "content": "EIS quantifies the emotional intensity of generated images using emotion detection models (e.g., DeepEmotion (Abidin and Shaarani, 2018)), pretrained on datasets labeled with emotions such as happiness, sadness, anger, or fear. Higher ERS values indicate stronger emotional tones.\n$ERS = \\frac{1}{M} \\sum_{i=1}^{M} EmotionIntensity(img_i)$"}, {"title": "Originality vs. Referentiality: Loriginality & Lreferentiality", "content": "To evaluate the originality of a generated image Igen, we propose leveraging CLIP Retrieval to dynamically identify reference styles and compute stylistic divergence. This method builds on the capabilities of pretrained CLIP models to represent both semantic and visual features effectively (Radford et al., 2021; Carlier et al., 2023).\nThe originality loss, Loriginality, is computed as the average cosine dissimilarity between the embedding of the generated image and the embeddings of the top-K reference images retrieved from a large-scale style database:\n$L_{originality foriginality_referentiality} (I) = \\frac{1}{K} \\sum_{k=1}^{K} cos (E_{CLIP}(I_{gen}), E_{CLIP}(S_{retr,k}))]$\nwhere:"}, {"title": "Cultural Sensitivity: Lcultural", "content": "Evaluating Cultural Sensitivity in T2I systems is challenging due to the lack of pre-trained cultural classifiers and the vast diversity of cultural contexts. We propose a novel metric called Simulated Cultural Context Matching (SCCM), which dynamically generates cultural sub-prompts using LLMs and evaluates their alignment with T2I-generated images. Dynamic Cultural Context Matching (SCCM) involves the following steps:\n1. Prompt Embedding: For each dynamically generated cultural sub-prompt Pi, embeddings are extracted using a multimodal model (e.g., CLIP). Let {E(P1), E(P2),..., E(Pk)} represent the embeddings of k sub-prompts.\n2. Image Embedding: The T2I-generated image I is embedded using the same model, yielding E(I).\n3. Prompt-Image Similarity: For each sub-prompt Pi and the generated image I, calculate the semantic similarity using cosine similarity:\n$sim(E(P_i), E(I)) = \\frac{E(P_i) \\cdot E(I)}{||E(P_i)|| ||E(I)||}$", "sections": []}, {"title": "Discussion and Limitations", "content": "The development of YinYangAlign introduces a novel paradigm for balancing contradictory axioms in Text-to-Image (T2I) systems, offering both theoretical contributions and practical implications. However, as with any sophisticated framework, its deployment and efficacy raise important points of discussion and reveal inherent limitations. This section critically examines the strengths and potential areas for improvement in YinYangAlign, situating it within the broader landscape of T2I alignment research.\nWe begin by reflecting on the broader implications of our methodology, including its adaptability to diverse tasks and its capacity to integrate user preferences dynamically. We then address the limitations that stem from reliance on predefined axioms, the scalability of the framework across domains, and the challenges associated with data diversity and representation. These reflections aim to provide a balanced perspective, guiding future refinements and encouraging dialogue within the research community to advance T2I alignment technologies further.", "sections": []}, {"title": "Mapping User Preferences to Multi-Objective Optimization Weights", "content": "YinYangAlign introduces a flexible and user-centric framework (cf. Fig. 12 for controls and Fig. 13 and Fig. 14 for the effect of varied controls on the output) for aligning text-to-image (T2I) models with potentially contradictory axioms. A core strength of this framework lies in its adaptability: given sufficient annotated data, end-users/developer can specify their desired balance between competing objectives, such as Faithfulness to Prompt versus Artistic Freedom or Cultural Sensitivity versus Creative Expression. This customization is facilitated by the Contradictory Alignment Optimization (CAO) mechanism, which translates user-defined preferences into weights for multi-objective optimization.\nBy leveraging the sliders, users directly influence the blending of contradictory axioms, enabling a tailored optimization process that reflects individual or application-specific requirements. For instance, a use case focused on creative content generation may prioritize Artistic Freedom, while another requiring factual accuracy and cultural sensitivity may emphasize Verifiability and Cultural Sensitivity. The CAO framework dynamically adapts to these preferences, ensuring that the optimization process aligns with user-defined priorities.\nThis section details how user-selected scales, representing preferences for contradictory axioms, are normalized and integrated into the multi-objective optimization process. The mathematical foundation of this mapping ensures clarity, reproducibility, and seamless adaptability for various use cases. Below, we describe the key steps involved in translating user preferences into actionable weights for CAO's optimization pipeline."}, {"title": "Normalize Slider Values", "content": "Each slider value vi is normalized to compute the weight ai for the i-th axiom. The normalization ensures the weights sum to 1:\n$\u03b1_i = \\frac{V_i}{\\sum_{j=1}^{N} v_j}$, for i = 1, . . ., N,\nwhere:\n* vi: Value of the i-th slider (e.g., v1 = 67 for Faithfulness to Prompt).\n* N: Total number of axioms (e.g., N = 6)."}, {"title": "Frequently Asked Questions (FAQs)", "content": "How does YinYangAlign differ from existing T21 benchmarks?\nExisting benchmarks typically focus on isolated objectives, such as fidelity to prompts or aesthetic quality. YinYangAlign is unique in evaluating how T2I systems navigate trade-offs between multiple conflicting objectives, providing a more holistic assessment.\nWhat is the role of Contradictory Alignment Optimization (CAO)?\nCAO is a framework introduced in the paper that harmonizes competing objectives through a synergy-driven multi-objective loss function. It integrates local axiom-specific preferences with global trade-offs to achieve balanced optimization across all alignment goals.\nWhat are the key components of the CAO framework?\nThe key components include:\n1. Local per-axiom preferences to handle individual trade-offs.\n2. A global synergy mechanism for unified alignment.\n3. A regularization term to prevent overfitting to any single objective.\nHow does YinYangAlign handle annotation challenges?\nYinYangAlign combines automated annotations using Vision-Language Models (VLMs) like GPT-4o and LLaVA with rigorous human verification. A consensus filtering mechanism ensures reliability, with a high inter-annotator agreement score (kappa = 0.83).\nWhat insights were gained from the empirical evaluation of DPO and CAO?\nThe study revealed that optimizing a single axiom using Directed Preference Optimization (DPO) often disrupts other objectives. For instance, improving Artistic Freedom by 40% caused declines in Cultural Sensitivity (-30%) and Verifiability (-35%). In contrast, CAO demonstrated controlled trade-offs, achieving more balanced alignment across all objectives.\nWhat are the metrics used to evaluate alignment in YinYangAlign?\nMetrics include changes in alignment scores across the six objectives, regularization terms to measure trade-offs, and statistical measures like the Pareto frontier to visualize multi-objective optimization.\nWhy is the Pareto frontier significant in the CAO framework?\nThe Pareto frontier illustrates the trade-offs between different objectives, showing how improvements in one area (e.g., faithfulness) may require concessions in another (e.g., artistic freedom). CAO leverages this concept to optimize multiple objectives simultaneously.\nWhat specific challenges does YinYangAlign address in the alignment of Text-to-Image (T2I) systems?\nYinYangAlign addresses the fundamental challenge of balancing multiple contradictory alignment objectives that are inherent to T2I systems. These include tensions such as adhering to user prompts (Faithfulness to Prompt) while allowing creative expression (Artistic Freedom) and maintaining cultural sensitivity without stifling artistic innovation. These challenges have been inadequately addressed by existing benchmarks, which often focus on singular objectives without considering their interplay.\nWhat are the six contradictory alignment objectives, and why were they chosen for YinYangAlign?\nThe six contradictory objectives are:\n1. Faithfulness to Prompt vs. Artistic Freedom: Ensures adherence to user instructions while allowing creative reinterpretation."}]}