{"title": "M\u00b3: A Modular World Model over Streams of Tokens", "authors": ["Lior Cohen", "Kaixin Wang", "Bingyi Kang", "Uri Gadot", "Shie Mannor"], "abstract": "Token-based world models emerged as a promising modular framework, modeling dynamics over token streams while optimizing tokenization separately. While successful in visual environments with discrete actions (e.g., Atari games), their broader applicability remains uncertain. In this paper, we introduce M\u00b3, a modular world model that extends this framework, enabling flexible combinations of observation and action modalities through independent modality-specific components. M\u00b3 integrates several improvements from existing literature to enhance agent performance. Through extensive empirical evaluation across diverse benchmarks, M\u00b3 achieves state-of-the-art sample efficiency for planning-free world models. Notably, among these methods, it is the first to reach a human-level median score on Atari 100K, with superhuman performance on 13 games. Our code and model weights are publicly available at https://github.com/leor-c/M3.", "sections": [{"title": "1. Introduction", "content": "Modeling the dynamics of the world has been a long-standing and compelling topic in reinforcement learning, with foundational ideas proposed over 30 years ago (Sutton, 1991; Schmidhuber, 1991; 2010). It has been leveraged to enhance planning performance (Schrittwieser et al., 2019), produce fictitious data for policy training (Hafner et al., 2023), or simulate interactive experiences (DeepMind, 2024; Decart et al., 2024; Agarwal et al., 2025).\nIn this study, we focus on token-based world models (TB-WMs) (Micheli et al., 2023; 2024; Cohen et al., 2024), sample-efficient reinforcement learning (RL) methods that learn the dynamics entirely within a learned token space. TBWMs offer a clear modularity between the representation and the dynamics by separating their optimization. Such modularity provides a clean and efficient way to unify dif-"}, {"title": "2. Method", "content": "Notations We consider the Partially Observable Markov Decision Process (POMDP) setting. However, since in practice the agent has no knowledge about the hidden state space, consider the following state-agnostic formulation. Let \u03a9, A be the sets of observations and actions, respectively. At every step t, the agent observes $o_t \\in \\Omega$ and picks an action $a_t \\in A$. From the agent's perspective, the environment evolves according to $O_{t+1}, r_t, d_t \\sim p(o_{t+1}, r_r, d_t|o_{<t}, a_{<t})$, where rt, dt are the observed reward and termination signals, respectively. The process repeats until a positive termination signal $d_t \\in {0,1}$ is obtained. The agent's objective is to maximize its expected return $E[\\Sigma_t \\gamma^t r_{t+1}]$ where $\u03b3 \\in [0, 1]$ is a discount factor.\nFor multi-modal observations, let $o_t = {o^{(i)}}^K_{i=1}$ where $\u03ba$ is the set of environment modalities and $o^{(i)}_t$ denotes the features of modality $\u03ba_i$."}, {"title": "Overview", "content": "M\u00b3 builds on REM (Cohen et al., 2024). The agent comprises a representation module V, a world model M, a controller C, and a replay buffer. To facilitate a modular design, following REM, each module is optimized separately. The training process of the agent involves a repeated cycle of four steps: data collection, representation learning (V), world model learning (M), and control learning in imagination (C)."}, {"title": "2.1. The Representation Module V", "content": "V is responsible for encoding and decoding raw observations and actions. It is a modular tokenization system with encoder-decoder pairs for different input modalities. Encoders produce fixed-length token sequences, creating a common interface that enables combining tokens from various sources into a unified representation. After embedding, these token sequences are concatenated into a single representation, as described in Section 2.2. Note that encoder-decoder pairs need not be learning-based methods. Although learned pairs are optimized independently. This design enables V to deal with any combination of input modalities, provided the respective encoder-decoder pairs.\nTokenization V transforms raw observations o to sets of fixed-length integer token sequences z = {$z^{(i)}$}^K_{i=1} by applying the encoder of each modality $z^{(i)} = enc_i(o^{(i)})$. Actions a are tokenized using the encoder-decoder pair of the related modality to produce $z^a$. The respective decoders reconstruct observations from their tokens: $\\hat{o}^{(i)} = dec_i(z^{(i)})$.\nM\u00b3 natively supports four modalities: images, continuous vectors, categorical variables, and image-like multi-channel grids of categorical variables, referred to as \"2D categoricals\". More formally, 2D categoricals are elements of $([k_1] \\times [k_2] \\times ... \\times [k_C])^{m\\times n}$ where $k_1,...,k_C$ are per channel vocabulary sizes, C is the number of channels, m, n are spatial dimensions, and $[k] = {1, ..., k}$.\nFollowing REM, we use a VQ-VAE (Esser et al., 2021; van den Oord et al., 2017) for image observations. For the tokenization of continuous vectors, each feature is quantized to produce a token, as in (Reed et al., 2022). Unbounded vectors are first transformed using the symlog function (Hafner et al., 2023), defined as $symlog(x) = sign(x) ln(1+ |x|)$, which compresses the magnitude of large absolute values. Lastly, while no special tokenization is required for categorical inputs, 2D categoricals are flattened along the spatial dimensions to form a sequence of categorical vectors. The embedding of each token vector is obtained by averaging the embeddings of its entries."}, {"title": "2.2. The World Model M", "content": "The purpose of M is to learn a model of the environment's dynamics. Concretely, given trajectory segments"}, {"title": "3. Experiments", "content": "To evaluate sample efficiency, we used benchmarks that measure performance within a fixed, limited environment interaction budget. The selected benchmarks also addressed two key research questions: (1) whether M\u00b3 performs effectively in continuous environments and (2) whether it handles multi-modal observations successfully."}, {"title": "3.1. Experimental Setup", "content": "Benchmarks: We evaluate M\u00b3 on three sample-efficiency benchmarks of different observation and action modalities: Atari 100K (Kaiser et al., 2020), DeepMind Control Suite (DMC) Proprioception 500K (Tunyasuvunakool et al., 2020), and Craftax-1M (Matthews et al., 2024).\nAtari 100K has become the gold standard in the literature for evaluating sample-efficient deep RL agents. The benchmark comprises a subset of 26 games. Within each game, agents must learn from visual image signal under a tightly restricted budget of 100K interactions, corresponding to roughly two hours of human gameplay.\nThe DeepMind Control Suite (DMC) is a set of continuous control tasks involving multiple agent embodiments ranging from simple single-joint models to complex humanoids. Here, we follow the subset of proprioception tasks used for the evaluation of DreamerV3 (Hafner et al., 2023), where observations and actions are continuous vectors. At each task, the agent's interaction budget is limited to 500K steps.\nCraftax is a 2D open-world survival game benchmark inspired by Minecraft, designed to evaluate RL agents' capabilities in planning, memory, and exploration. The partially-observable environment features procedurally generated worlds where agents must gather and craft resources while surviving against hostile creatures. Observations consist of a 9\u00d711 tile egocentric map, where each tile consists of 4 symbols, and 48 state features corresponding to state information such as inventory and health. Here, we consider the sample-efficiency oriented Craftax-1M variant which only allows an interaction budget of one million steps.\nBaselines On Atari-100K, we compare M\u00b3 against DreamerV3 (Hafner et al., 2023) and several methods restricted to image observations: TWM (Robine et al., 2023), STORM (Zhang et al., 2024), DIAMOND (Alonso et al., 2024), and REM (Cohen et al., 2024). On DMC, we compare exclusively with DreamerV3, currently the only planning-free world model method with published results on the 500K proprioception benchmark. On Craftax-1M, we compare against TWM (Dedieu et al., 2025), a concurrent work that proposes a Transformer based world model with a focus on the Craftax benchmark, and the baselines reported in the Craftax paper: Random Network Distillation (RND)"}, {"title": "3.2. Results", "content": "M\u00b3 achieves state-of-the-art performance across all three benchmarks (Figure 1). On Atari 100k, M\u00b3 outperforms all baselines across all key metrics (Figure 4). Notably, M\u00b3 is the first planning-free world model to reach a humen-level IQM and median scores. In addition, it achieves superhuman performance on 13 out of 26 games, surpassing all baselines. These results highlight M\u00b3's effectiveness in sample-efficient learning and its robustness across diverse tasks in the Atari 100k benchmark.\nIs M\u00b3 effective in continuous environments? Figure 4 provides compelling evidence that token-based architectures can indeed excel in continuous domains: M\u00b3 achieves performance comparable to DreamerV3 across most tasks, and notably outperforms it on Cartpole Swingup Sparse and both Hopper tasks.\nIs M\u00b3 effective in environments with multi-modal observations? Since modularity is at the core of M\u00b3, we"}, {"title": "3.3. Ablation Studies", "content": "We ablate the intrinsic rewards, prioritized replay, and classification-based predictions to demonstrate their individual contributions to M\u00b3's performance. In each ablation, M\u00b3 is modified so that only the component of interest is disabled. Due to limited computational resources, we consider a subset of 8 tasks for each of the Atari 100K and DMC benchmarks. Concretely, for Atari 100K, we used \"Assault\", \"Breakout\", \"Chopper Command\", \"Crazy Climber\", \"James bond\", \"Kangaroo\", \"Seaquest\", and \"Up'n Down\", in which significant improvements were observed. For DMC, we chose a subset that includes different embodiments: \"acrobot swingup\", \"cartpole swingup sparse\", \"cheetah run\", \"finger turn hard\", \"hopper stand\", \"pendulum swingup\", \"reacher hard\", and \"walker run\u201d."}, {"title": "4. Related Work", "content": "Offline Multi-Modal Methods Large-scale token-based sequence models for modeling and generating multi-modal trajectories of agent experience were proposed in (Lu et al., 2023; 2024; Reed et al., 2022; Schubert et al., 2023). In Gato (Reed et al., 2022) and TDM (Schubert et al., 2023), multi-modal inputs are first tokenized through predefined transformations, while in Unified IO (Lu et al., 2023; 2024) pretrained models are also used. The produced tokens are"}, {"title": "5. Limitations and Future Work", "content": "Here, we briefly highlight several limitations of this work. First, although the feature quantization approach for tokenizing continuous vectors showed promise, it leads to excessive sequence lengths. We believe that more efficient solutions can be found for dealing with continuous inputs. Second, since rich multi-modal RL benchmarks are scarce, future work could wrap multiple single-modality environments into a unified multi-modal environment, which can be used to evaluate agents under different modality combinations as well as the agent's ability to perform multiple independent tasks concurrently."}, {"title": "6. Conclusions", "content": "In this paper, we proposed a modular world model, M\u00b3, for sample-efficient RL. M\u00b3 extends token-based world model agents via a modular framework to support a wide range of environments and modalities. In addition, it combines multiple advances from existing literature to enhance performance. Through extensive empirical evaluation, M\u00b3 exhibited state-of-the-art performance for planning-free world models on a diverse set of benchmarks, involving visual, continuous, and structured symbolic modalities. Notably, on the well-established Atari-100K benchmark, M\u00b3 outperformed all baselines across all metrics."}, {"title": "A. Models and Hyperparameters", "content": "A.1. Hyperparameters\nWe detail shared hyperparameters in Table 2, training hyperparameters in Table 3, world model hyperparameters in Table 4, and controller hyperparameters in Table 5. Environment hyperparameters are detailed in Table 6 (Atari-100K) and Table 7 (DMC).\nTuning Due to limited computational resources, M\u00b3 was not properly tuned. We believe that proper tuning could further improve M\u00b3's performance."}, {"title": "A.2. The Representation Module V", "content": "A.2.1. IMAGE OBSERVATIONS\nImage observations are tokenized using a vector-quantized variational auto-encoder (VQ-VAE) (van den Oord et al., 2017; Esser et al., 2021). A VQ-VAE comprises a convolutional neural network (CNN) encoder, an embedding table $E \\in R^{n\\times d}$, and a CNN decoder. Here, the size of the embedding table n determines the vocabulary size.\nThe encoder's output $h \\in R^{W\\times H\\times d}$ is a grid of W \u00d7 H multi-channel vectors of dimension d that encode high-level learned features. Each such vector is mapped to a discrete token by finding the closest embedding in E:\n$z = arg \\min_i ||h \u2013 E(i)||\nwhere E(i) is the i-th row of E. To reconstruct the original image, the decoder first maps z to their embeddings using E. During training, the straight-through estimator (Bengio et al., 2013) is used for backpropagating the learning signal from the decoder to the encoder: $h = h + sg(E_z \u2013 h)$. The architecture of the encoder and decoder models is presented in Table 8. The optimization objective is given by\n$L(enc, dec, E) = ||o \u2013 dec(z) ||^2 + || sg(enc(o)) \u2013 E(z) ||^2 + || sg(E(z) \u2013 enc(0)||^2 + L_{perceptual} (o, dec(z))$\nwhere $L_{perceptual}$ is a perceptual loss (Johnson et al., 2016; Larsen et al., 2016), proposed in (Micheli et al., 2023).\nCrucially, the learned embedding table E is used for embedding the (image) tokens across all stages of the algorithm."}, {"title": "A.2.2. CONTINUOUS VECTORS", "content": "The quantization of each feature uses 125 values (vocabulary size) in the range [-6, 6], where 63 values are uniformly distributed in $[-\\ln(1 + \\pi), \\ln(1+ \\pi)]$ and the rest are uniformly distributed in the remaining intervals."}, {"title": "A.3. The World Model M", "content": "Embedding Details Each token in $z^{(i)}$ of each modality is mapped to a d-dimensional embedding vector $X^{(i)}$ using the embedding (look-up) table $E^{(i)}$ of modality $\u03ba_i$. The embedding vector that corresponds to token z is simply the z-th row in the embedding table. Formally, $x^{(i)}_t = E^{(i)}(z_t)$, $l = \\frac{C}{1}$. In the special case of 2D categorical inputs, $x^{(i)}_{t,j} = \\frac{1}{C} \\Sigma^{C}_{n=1} E^{(i)}_{n}(z_{t,j,n})$ where C is the number of channels and i is the index of the 2D categorical modality in \u043a.\nTo concatenate the embeddings, we use the following order among the modalities: images, continuous vectors, categorical variables, and 2D categoricals.\nPrediction Heads Each prediction head in M is a multi-layer perceptron (MLP) with a single hidden layer of dimension 2d where d is the embedding dimension.\nEpistemic Uncertainty Estimation Working with discrete distributions enables efficient entropy computation and ensures that the ensemble disagreement term $\\hat{r}_t$ is bounded by $\\frac{T}{2} \\Sigma_{z \\in Z} log(vocab\\_size(z))$."}, {"title": "A.3.1. OPTIMIZATION", "content": "For each training example in the form of a trajectory segment in token representation $\u03c4 = z_1,a_1,..., z_H,a_H$, the optimization objective is given by\n$L_M(\u03b8, \u03c6, \u03c4) = \\Sigma^{H}_{t=1} L_{obs}(\u03b8, z_t, p_\u03b8(z_t|Y)) + L_{reward}(\u03b8, r_t, r_t) \u2013 log(p_\u03b8(d_t)) + \\Sigma^{N_{ens}}_{i=1} L_{obs}(\u03c6, z_t, p_{\u03b8_i}(z_t|sg(Y)))$ where\n$L_{obs}(\u03b8, z_t, p_\u03b8(z_t|Y)) = \\frac{1}{K} \\Sigma^K_{i=1} log p_\u03b8(z_{t,i}|Y_t)$\nis the average of the cross-entropy losses of the individual tokens, and $L_{reward}(\u03b8, r_t, r_t)$ is the LHL-Gauss loss with the respective parameters of the reward head. Here, yi is the vector of Y that corresponds to zi, the i-th token of zt."}, {"title": "A.3.2. RETENTIVE NETWORKS", "content": "Retentive Networks (RetNet) (Sun et al., 2023) are sequence model architectures with a Transformer-like structure (Vaswani et al., 2017). However, RetNet replaces the self-attention mechanism with a linear-attention (Katharopoulos et al., 2020) based Retention mechanism. At a high level, given an input sequence $X \\in R^{|X|\\times d}$ of d-dimensional vectors, the Retention operator outputs\n$Retention(X) = (QK^T \\odot D)V$\nwhere Q, K, V are the queries, keys, and values, respectively, and D is a causal mask and decay matrix. Notably, the softmax operation is discarded in Retention and other linear attention methods. As a linear attention method, the computation can also be carried in a recurrent form:\n$Retention(x_t, S_{t-1}) = S_{tq_t}$\n$S_t = \\eta S_{t-1} + v_t k_t \\in R^{d \\times d}$ where $\u03b7$ is a decay factor, $S_t$ is a recurrent state, and $S_0 = 0$. In addition, a hybrid form of recurrent and parallel forward computation known as the chunkwise mode allows to balance the quadratic cost of the parallel form and the sequential cost of the recurrent form by processing the input as a sequence of chunks. We refer the reader to (Sun et al., 2023) for the full details about this architecture.\nIn our implementation, since inputs are complete observation-action block sequences $X_1, . . ., X_t$, we configure the decay factors of the multi-scale retention operator in block units:\n$\u03b7 = 1 - 2^{-linspace(log(K_{n_{min}}),log(K_{n_{max}}),N_h)}$\nwhere linspace(a, b, c) is a sequence of c values evenly distributed between a and b, Nh is the number of retention heads, and $\u03b7_{min}$, $\u03b7_{max}$ are hyperparameters that control the memory decay in observation-action block units."}, {"title": "A.3.3. PARALLEL OBSERVATION PREDICTION (POP)", "content": "POP (Cohen et al., 2024) is a mechanism for parallel generation of non-causal subsequences such as observations in token representation. It's purpose is to improve generation efficiency by alleviating the sequential bottleneck caused by generating observations a single token at a time (as done in language models). However, to achieve this goal, POP also includes a mechanism for maintaining training efficiency. Specifically, POP extends the chunkwise forward mode of RetNet to maintain efficient training of the sequence model.\nTo generate multiple tokens into the future at once, POP introduces a set of prediction tokens $u = u_1,..., u_K$ and embeddings $X^u \\in R^{K\\times d}$ where K is the number of tokens in an observation. Each token in u corresponds to an observation token in z. These tokens, and their respective learned embeddings, serve as a learned prior.\nLet $X_1,..., X_T$ be a sequence of T observation-action (embeddings) blocks. Given $S_{t-1}$ summarizing all key-value outer products of elements of $X_{<t-1}$, the outputs $Y_u$ from which the next observation tokens are predicted are given by:\n$(\u00b7, Y^u) = f_\u03b8(S_{t-1}, X^u)$\nImportantly, the recurrent state is never updated based on the prediction tokens u (or their embeddings). The next observation tokens zt are sampled from $p_\u03b8(z_t|Y^u)$. Then, the next action is generated by the controller, and the next observation-action block $X_t$ can be processed to predict the next observation $z_{t+1}$.\nTo maintain efficient training, a two step computation is carried at each RetNet layer. First, all recurrent states $S_t$ for all $1 \\le t \\le T$ are calculated in parallel. Although there is an auto-regressive relationship between time steps, the linear structure of S allows to calculate the compute-intensive part of each state in parallel and incorporate past information efficiently afterwards. In the second step, all outputs $Y^u_t$ for all $1 < t < T$ are computed in parallel, using the appropriate states $S_{t-1}$ and $X^u$ in batch computation. Note that this computation involves delicate positional information handling. We refer the reader to (Cohen et al., 2024) for full details of this computation."}, {"title": "A.4. The Controller C", "content": "Critic The value prediction uses 128 bins in the range $b = (-15, ..., 15)$.\nContinuous Action Spaces The policy network outputs m 51 logits corresponding to m quantization values uniformly distributed in [-1,1] for each individual action in the action vector."}, {"title": "A.4.1. INPUT ENCODING", "content": "The controller C operates in the latent token space. Token trajectories $\u03c4 = z_1, a_1, . . ., z_H, a_H$ are processed sequentially by the LSTM model. At each time step t, the network gets $z_t$ as input, outputs $\u03c0(a_t|\u03c4_{<t\u22121}, z_t)$ and $V^u(a_t|\u03c4_{<t\u22121}, z_t)$, samples an action at and then process the sampled action as another sequence element.\nThe processing of actions involve embedding the action into a latent vector which is then provided as input to the LSTM. Embedding of continuous action tokens is performed by first reconstructing the continuous action vector and then computing the embedding using a linear projection. Discrete tokens are embedded using a dedicated embedding table.\nTo embed observation tokens z, the tokens of each modality are processed by a modality-specific encoder. The outputs of the encoders are concatenated and further processed by a MLP $g_\u03c8$ that combines the information into a single vector latent representation.\nThe image encoder is a convolutional neural network (CNN). Its architecture is given in Table 9.\nCategorical variables are embedded using a learned embedding table. For 2D categoricals, shared per-channel embedding tables map tokens to embedding vectors, which are averaged to obtain a single embedding for each multi-channel token vector. For both types of categorical inputs we use 64 dimensional embeddings. The embeddings are concatenated and processed by $g_\u03c8$."}, {"title": "A.4.2. OPTIMIZATION", "content": "\u03bb-returns are computed for each generated trajectory segment $\u03c4 = (z_1, a_1, r_1, d_1, z_2, a_2, r_2, d_2, ..., z_H, a_H, r_H, d_H)$:\n$G_t = \\begin{cases} r_t + (1 \u2013 d_t)((1 \u2013 \u03bb)V_t + \u03bbG_{t+1}) & t < H \\\\ r_t & t = H \\\\ \\end{cases}$\nwhere $V_t = V^\u03c0(\\hat{\u03c4}_{<t})$. These \u03bb-returns are used as targets for critic learning. For policy learning, a REINFORCE (Sutton et al., 1999) objective is used, with a normalized $V^\u03c0$ baseline for variance reduction:\n$\\mathcal{L}_\u03c0(\u03c8) = E[\\Sigma^{H}_{t=1} sg( \\frac{G_t \u2013 V_t}{max(1, c)} ) log \u03c0(a_t| \\hat{\u03c4}_{\u2264t\u22121}, z_t) + w_{ent} H(\u03c0_\u03b8(\\hat{\u03c4}_{\u2264t\u22121}, z_t))]$\nwhere c is an estimate of the effective return scale similar to DreamerV3 (Hafner et al., 2023) and $w_{ent}$ is a hyperparameter that controls the entropy regularization weight. c is calculated as the difference between the running average estimators of the 97.5 and 2.5 return percentiles, based on a window of return estimates obtained in the last 500 batches (imagination)."}, {"title": "B. Implementation Details", "content": "Code We open-source our code and trained model weights. Our code is written in Pytorch (Paszke et al., 2019). All code was developed by Lior Cohen. Experiments were run by Lior Cohen, Bingyi Kang, and Uri Gadot.\nHardware All Atari and DMC experiments were performed on V100 GPUs, while for Craftax a single RTX 4090 was used.\nRun Times Experiments on Atari require approximately 12 hours on an RTX 4090 GPU and around 29 hours on a V100 GPU. For DMC, the runtime is about 40 hours on a V100 GPU. Craftax runs take roughly 94 hours, equivalent to 3.9 days.\nCraftax The official environment provides the categorical variables in one-hot encoding format. Our implementation translates these variables to integer values which can be interpreted as tokens.\nSetup in Atari Freeway For the Freeway environment, we adopted a modified sampling strategy where a temperature of 0.01 is used instead of the standard value of 1, following (Micheli et al., 2023; Cohen et al., 2024). This adjustment helps directing the agent toward rewarding paths. Note that alternative approaches in the literature tackle the exploration challenge through different mechanisms, including epsilon-greedy exploration schedules and deterministic action selection via argmax policies (Micheli et al., 2023)."}]}