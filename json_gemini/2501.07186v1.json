{"title": "Generalizable Graph Neural Networks for Robust Power Grid Topology Control", "authors": ["Matthijs de Jong", "Jan Viebahn", "Yuliya Shapovalova"], "abstract": "The energy transition necessitates new congestion management methods. One such method is controlling the grid topology with machine learning (ML). This approach has gained popularity following the Learning to Run a Power Network (L2RPN) competitions. Graph neural networks (GNNs) are a class of ML models that reflect graph structure in their computation, which makes them suitable for power grid modeling. Various GNN approaches for topology control have thus been proposed. We propose the first GNN model for grid topology control that uses only GNN layers. Additionally, we identify the busbar information asymmetry problem that the popular homogeneous graph representation suffers from, and propose a heterogeneous graph representation to resolve it. We train both homogeneous and heterogeneous GNNS and fully connected neural networks (FCNN) baselines on an imitation learning task. We evaluate the models according to their classification accuracy and grid operation ability. We find that the heterogeneous GNNs perform best on in-distribution networks, followed by the FCNNs, and lastly, the homogeneous GNNs. We also find that both GNN types generalize better to out-of-distribution networks than FCNNs.", "sections": [{"title": "1. Introduction", "content": "The energy transition is crucial for ensuring society's sustainability and future security. Transmission system operators (TSOs) play a crucial role in this transition. They are consequently facing new operational challenges [9]. Grid congestion is a problem that TSOs are already encountering but which the energy transition will exacerbate [42]. If not mitigated, grid congestion could make the future availability of energy unreliable.\nThere are several actions that TSOs can take to alleviate grid congestion [17]. Presently, redispatching and curtailment are the primary methods. However, these actions are often costly, disruptive, or not applicable. Topology control presents an opportunity for remedying grid congestion without these problems [41]. Congestion can be relieved by exploiting the flexibility in the network topology. However, this technique mostly remained unutilized, as the space of potential topologies is combinatorially large.\nDue to the recent congestion management challenges, there has been a new interest in topology control. This interest led to the development of the GridOptions tool for remedial action recommendation by Viebahn et al. [41]. This tool validates the practical value of topology control. Even simple topological strategies could reduce line congestion by ten to twenty percent. The recommended strategies improved upon strategies without topological actions or those considered by grid operators. However, while the results are promising, further developments are necessary for broader applicability. The tool was only applied to a small subset of the Dutch power grid, featuring nine substations. Additionally, the inference time of the tool is relatively long: one hour. The next challenges involve scaling this approach and reducing inference time.\nMachine learning (ML) presents a solution to these challenges [42]. ML models can learn which topologies are suitable in which situations and predict these topologies much quicker than traditional computational methods. To facilitate research in this area, the Learning to Run a Power Network (L2RPN) competitions were hosted [28, 27]. Furthermore, the Grid2Op Python library was developed.\u00b9 This library greatly simplified the development and evaluation of ML methods for grid topology control. These developments kick-started research into ML methods for grid topology control."}, {"title": "2. Related Work", "content": "The L2RPN competitions piqued the academic interest in grid topology control with machine learning. Prior research into topology control used linear programming [10, 49], but such methods are too rigid and/or slow to be practical. The first L2RPN competition premiered in 2019. It focused on operating scenarios of the EEE14-bus network within a specific time frame [28]. The following L2RPN competition was featured at WCCI in 2020 and featured the larger IEEE118-bus network [29]. The next competition at NeurIPS 2020 [27] introduced a robustness track with unplanned outages and an adaptability track with out-of-distribution injection profiles. Later L2RPN competitions focused on trust and sending an alarm signal [26], and the inclusion of batteries [37]."}, {"title": "3. Power Grid Setup", "content": "In this study, we experiment on the IEEE14-bus system, displayed in Figure 1. We simulate this system with the rte_case14_realistic environment from the Grid2Op library\u00b3. The power grid includes fourteen substations, five generators, 11 loads, and 20 power lines. One generator is solar, one nuclear, one wind-based, and two are thermal. The grid is divided into two sides. The high-voltage transmission side contains substations 0 to 4. The low-voltage distribution side contains substations 5 to 13. Lines 15 to 19 model the transformers connecting the two sides of the grid. We adjust the thermal limits to the values specified by Subramanian et al. [39]. This makes the transmission and distribution differences more pronounced and realistic.\nWe also investigate network variations with single lines disabled. We refer to these as N-1 networks. We use the N-1 networks to investigate the ability of agents to operate with line outages and to test the generalization of ML models to out-of-distribution networks."}, {"title": "3.2. Operational Period", "content": "The environment features one thousand scenarios, each consisting of 8064 five-minute timesteps. One scenario thus models 28 days. Each scenario features different injection profiles. We split the scenarios into individual days. This daily period better reflects operational periods and emulates beneficial topology reversal [22]. A game-over occurs when the power grid fails to transport sufficient power from generators to loads\u2074."}, {"title": "3.3. Action Space", "content": "The substations have precisely two busbars in the environment. The topology actions switch objects, i.e., generators, loads, and line endpoints, between these busbars. The topology vector specifies the busbar attachments of objects. Each index in the vector corresponds to an object. At an index, a value of 1 indicates attachment to the first busbar, 2 indicates attachment to the second busbar, and -1 indicates object disconnection."}, {"title": "3.4. Regimes", "content": "We consider three environmental regimes, which represent difficulty levels for grid operation. The full-network regime involves the whole network. Lines are statically disabled in the planned-outage regime. The unplanned-outage regime introduces an opponent that disables lines spontaneously. We use the opponent specified by Manczak et al. [24]. The opponent disables a randomly selected line for four hours twice a day. A cooldown period ensures that outages are separated by at least an hour."}, {"title": "4. Methods", "content": ""}, {"title": "4.1. Datasets", "content": "We use the data generated with the two expert agents from our previous study [15]. The first agent is the greedy agent, which decreases the line loading greedily. The second agent is the N-1 agent, which pursues topologies that are N-1 redundant. We found that the N-1 agent performed superiorly to the Greedy agent in settings with and without outages. Both agents have an activity threshold, below which do-nothing actions are taken automatically. State-action pairs with such do-nothing actions or from unsuccessful days are excluded. Do-nothing actions above this threshold are included.\nWe create two datasets. First, a in-distribution (ID) dataset containing networks on which the models are trained and evaluated. This dataset was used in our previous study [15]. This dataset combines the data from the greedy agent and the N-1 agent. The data from the N-1 agent on the full network is used. The data from the greedy agent on a subset of the N-1 networks is used. These are the N-1 networks with lines 0, 2, 4, 5, 6, and 12 disabled, which the greedy agent can operate well [15].\nSecond, an out-of-distribution (OOD) dataset, which is used to investigate generalization. This dataset comprises data from the greedy agent on the N-1 networks with lines 1 and 3 disabled.\nOn both datasets, we split the datapoints into 70/10/20 train/validation/test sets based on their scenario. The sizes of the partitions are listed in Table 3. In this paper, ID or OOD networks refer to the networks in either the ID or OOD datasets. ID or OOD outages refer to the outages present in either ID or OOD networks."}, {"title": "4.2. Datapoints", "content": "Each datapoint includes the features per object, the topology vector, and the expert action. The features for each object type are listed in Table 4. The object features are normalized. If a line is disabled, its endpoint features are zero-imputed, and the corresponding topology vector values are -1. Actions are converted from a set-action format into a switch-action format. Thus, actions are represented by a vector with a length of the topology vector. Each index indicates whether the corresponding object is switched between busbars (a value of 1) or not (a value of 0). This presents a multi-label binary classification task."}, {"title": "4.3. FCNN", "content": "The fully connected neural network (FCNN) consists of an input layer, multiple hidden layers, and an output layer. The input vector is obtained by flattening the input features into a vector and concatenating the topology vector. The hidden layers use the ReLU activation function. The output layer uses the sigmoid activation function to constrain the output to the (0, 1) range. The output vector has the length of the topology vector, so one value $p_u \\in (0,1)$ is predicted per node u."}, {"title": "4.4. Homogeneous GNN", "content": "Applying a graph neural network to the power grid requires a graph representation of the power grid. We represent the grid objects as nodes. We connect nodes if the corresponding objects are attached to the same busbar or are corresponding line endpoints. We call this graph representation homogeneous as it does not consider edge types. We call the associated GNN the HomGNN. Figures 2a and 2b display an example grid and its homogeneous representation.\nDifferent types of grid objects have different features (see Table 4). We use multiple two-layer perceptrons to embed the varying object features into a common embedding:\n$h_{u,0} = \\begin{cases}\nMLP_{gen} (x_u) & \\text{node u represents a generator} \\\\\nMLP_{load}(x_u) & \\text{node u represents a load} \\\\\nMLP_{line} (x_u) & \\text{node u represents a line endpoint,}\n\\end{cases}$\nwhere $h_{u,0}$ and $x_u$ are, respectively, the initial node embedding and the features of node u. The embeddings of subsequent layers are computed with the message passing rule [36]:\n$h_{u,k+1} = \\sigma (W_{self,k}h_{u,k} + W_{neighbor,k} \\sum_{v \\in N(u)} h_{v,k} + b_k),$ \nwhere $h_{u,k}$ is the embedding for node u in layer k, $\\sigma$ is the activation function, $W_{self,k}$ and $W_{neighbor,k}$ are the self and neighbor weights in layer k, $N(u)$ is the neighborhood of node u, and $b_k$ is the bias term. Figure 2d displays an example of homogeneous message passing. The activation function $\\sigma$ is the ReLU function in all but the final layer. The final layer uses the sigmoid activation function and outputs one value. Thus, the model predicts one value $p_u \\in (0,1)$ per node u."}, {"title": "4.5. Busbar Information Asymmetry", "content": "The aforementioned graph neural network formulation has a problem. Each node's output specifies whether to switch the corresponding object between busbars. However, messages from objects on the current busbar are passed, while messages from the objects on the other busbar are not. We call this the busbar information asymmetry. This is intuitively problematic as a switching decision requires information about the other busbar. This also decreases the model's expressiveness. Objects at the same substation but on different busbars and objects at entirely different substations cannot be distinguished. Moreover, the absence of inter-busbar connections also decreases the graph's connectivity. This can result in longer paths that may require deeper GNNs."}, {"title": "4.6. Heterogeneous GNN", "content": "Our solution is to represent the grid with a heterogeneous graph representation. The associated GNN is called the HetGNN. This heterogeneous representation features three edge types: one for objects on the same busbar, one for objects on the other busbar (but at the same substation), and one for corresponding line endpoints. This is displayed in Figure 2c. The message passing rule is adapted to consider a weight and neighborhood aggregation per edge type:\n$h_{u,k+1} = \\sigma (W_{self,k}h_{u,k} + W_{same,k} \\sum_{v \\in N_{same}(u)} h_{v,k} +\nW_{other,k} \\sum_{v \\in N_{other}(u)}h_{v,k} + W_{line,k}h_{line,k} + b_k),$"}, {"title": "4.7. Optimization & Postprocessing", "content": "The weights are initialized using a normal distribution, with the standard deviation set as a tuned hyperparameter. We use the Adam optimizer to minimize a label-weighted binary cross-entropy loss, defined as:\n$L =mean(-w(y log(p) + (1 - y) log(1 \u2013 p))),$\nwhere w, y, and p denote the label weight, target, and prediction vectors, respectively. We introduce label weights to prevent the prediction vectors from collapsing to zeros. This occurs as the majority of target values are zero. A lower label weight 0 < \u03b1 < 1 is assigned to labels that do not correspond to objects at either the target or predicted substation:\n$w_i =\\begin{cases}\n1 & \\text{object i corresponds to the target substation}\n\\\\\n1 & \\text{object i corresponds to the predicted substation} \\forall w_i \\in w.\n\\\\\n\\alpha & \\text{otherwise}\n\\end{cases}$\nThere is no target substation if the target action is a do-nothing action. The predicted action is classified as a do-nothing action, without a predicted substation, if all predictions $p_i \\in p$ do not exceed 0.5. Otherwise, the predicted substation is the substation where the predictions $p_i$ at that substation $s \\in S$ maximize\n$\\Sigma_{p_i \\in p_s} max(p_i - 0.5,0)$.\nEach value in the prediction vector represents whether to switch the corresponding object. However, not every prediction vector corresponds to an action in the filtered action space (see Sec. 3.3). We apply a postprocessing step that replaces the model's prediction p with the nearest action. This postprocessing step is applied during validation, testing, and inference but not during training."}, {"title": "4.8. Hyperparameter Tuning & Training", "content": "The hyperparameters were tuned with two iterations of hyperparameter sweeps. This procedure was repeated for the three model types. The first sweep narrowed the hyperparameter ranges. It covered wide parameter ranges and trained for a few epochs with strict early stopping. The second sweep covered narrower ranges and trained with more epochs and less strict early stopping. All sweeps use random search with Hyperband early termination (distinct from early stopping). Hyperband early termination terminated unpromising runs early. Table 5 describes all final hyperparameter values, the second sweep ranges, and additional clarification where necessary.\nFive models were trained per model type, each with different weight initializations. Each training run lasted for 100 epochs unless stopped early. Runs were terminated early if the highest validation accuracy did not increase in 20 evaluations. The validation accuracy was calculated every 50,000 iterations. The training curves are displayed in Figure 3. Finally, we trained five heterogeneous GNNs on the OOD data to contrast the models' generalization performance. These are referred to as OOD-GNNs."}, {"title": "4.9. Imitation Learning Agents", "content": "The ML models are applied to the environment. They can be applied directly or combined with simulation. The naive agent executes a model's predicted action directly. We observe that this agent occasionally fails by predicting a singular erroneous action. The verify agent addresses this by verifying predicted actions with simulation. The predicted action is simulated. A do-nothing action is selected if the simulated line loadings are increased beyond the thermal limit. We also consider hybrid agents. The verify+greedy agent normally functions as the verify agent, but switches to the greedy agent if a line breaches the thermal limit. The verify+N-1 agent functions similarly but with the N-1 agent. Each agent also uses the activity threshold used by the expert agents [15]."}, {"title": "5. Results", "content": ""}, {"title": "5.1. Supervised Learning", "content": "Table 6 lists the accuracies of the different models on the various data partitions and the ID/OOD network groups. Figure 4 shows the test accuracies of the models on both datasets. On the ID dataset (rows 1 to 3), the accuracies remain limited to approximately 80%. On each split, the HetGNN achieves the highest accuracy, followed by the FCNN and, lastly, the HomGNN. Each model type shows a tiny drop in accuracy between the train, validation, and test sets, indicating slight overfitting. Row 4 displays that accuracy drops slightly without the postprocessing step described in Sec. 4.7.\nThe default topology appears frequently in the ID dataset, comprising 46% of the test set. In this topology, all objects are attached to the same busbar. This avoids the busbar asymmetry problem and consequently negates the theoretical advantage of the HetGNN over the HomGNN. As shown in row 5, the accuracies on the standard topology are relatively similar. As shown in row 6, the accuracies vary considerably between topologies with split busbars.\nThe GNNs obtain far higher accuracies on the OOD dataset than the FCNNs (rows 7 to 9). The HetGNNs also obtain higher accuracies than the HomGNNs. However, the performance of either the HomGNN or the HetGNN on the OOD dataset is still substantially lower than models trained on OOD data (column 'OOD-GNN')."}, {"title": "5.2. Error Analysis", "content": "We repeated the error analysis previously performed on the FCNN [15] to the two types of GNN models. We again find that the class imbalance and class overlap play a role in the limited accuracy. Figure 5 shows that both GNN types predict infrequent classes disproportionately infrequently. We investigated pairs of classes that were often confused, i.e., pairs of classes that were often mistaken for one another in prediction. Figure 6 shows that the errors of frequently confused classes are in overlapping regions.\nInspection of the nearest neighbors supports the notion that the class overlap contributes to low accuracy. We applied one of the HetGNNs to a subset of 2,500 validation data points from the full network. The accuracy over the data points whose nearest neighbor is in the same class is 92.97%, but only 44.44% for those whose nearest neighbor is in another class. Other models showed similar results."}, {"title": "5.3. Graph Smoothness", "content": "The tendency of graph neural networks to suffer from oversmoothing is well-documented [3, 23]. We compute the mean average distance (MAD) values to measure graph smoothness [3]. MAD values quantify the similarity of node embeddings. Small MAD values indicate that node embeddings are similar and, thus, that the graph is smooth.\nThe MAD values are computed between neighbor nodes over the first 1000 validation datapoints. Figure 7 shows the MAD values between the various layers. As visible, the MAD values in the middle GNN layers are consistently higher in the HetGNN than in the HomGNN. This suggests that the heterogeneous representation leads to a less smooth local embedding. This finding is consistent among the models."}, {"title": "5.4. Graph Diameter", "content": "We mentioned that the heterogeneous graph representation can lead to shorter paths. We measure this by the diameter, i.e., the shortest path length between the two most distant nodes. We calculate the diameter by finding the lowest exponent of the adjacency matrix that produces a matrix without zero entries [7]. The diameters of the 25 most common topologies are shown in Figure 8. As visible, the heterogeneous graph representation infrequently leads to a shorter diameter."}, {"title": "5.5. Simulation Performance", "content": "Table 7 shows the days completed by the various agents in the different settings. The 'Full Network\u2019column describes the setting without outages. The 'ID Outages' describes a setting with an opponent that causes random outages present in the ID dataset (see Sec. 4.1). The 'OOD Outages' describes a setting with an opponent that causes random outages present in the OOD dataset.\nThe agents can be ordered by performance: Do-Nothing < Naive < Verify < Greedy Hybrid < N-1 Hybrid. The naive agents (rows 4 to 6) and verify agents (rows 8 to 10) come close to the performance of the N-1 expert agents (row 3). The hybrid agents (rows 12 to 14, and 16 to 18) can match the N-1 expert agents.\nOn the setting with no or ID outages (columns 'Full Network' and 'ID Outages') the HetGNN network consistently performs best, followed by the FCNN and, finally, the HomGNN (rows 4 to 6, and 8 to 10). Additional simulation reduces or nullifies the differences (rows 12 to 14, and 16 to 18).\nOn the setting with OOD outages (column 'OOD Outages'), the GNNs outperform the FCNN (rows 4 to 6, and 8 to 10). However, additional simulation also decreases this effect (rows 12 to 14, and 16 to 18). Furthermore, the performance of the models trained on the ID dataset remains substantially lower than models trained on OOD dataset (rows 7 and 11), even with additional simulation (rows 15 and 19)."}, {"title": "5.6. Efficiency", "content": "Finally, we consider the inference speed of the different agents. The inference speed was measured on an Apple M1 CPU with minimal background processes on the first fifty validation scenarios. Table 8 lists the durations per model type and agent. Figure 9 plots these values against the agent's performance on the full-network topology. The GNNs, particularly the Het-GNNs, are considerably slower than the FCNNs. However, these differences are small relative to the simulation times of the expert agents, which are orders of magnitude higher. Most importantly, all GNN agents have a favorable combination of speed and performance."}, {"title": "6. Discussion", "content": "The heterogeneous GNN consistently performs best, followed by the FCNN, and lastly, the homogeneous GNN. This is true for both the accuracy and grid operation. The superiority of the HetGNN to the FCNN indicates that GNNs can exploit the graph structure better. The inferiority of the HomGNN can be attributed to the busbar asymmetry problem. However, smoother graphs and longer path lengths might also play a role. The HetGNN's coarser graphs can be attributed to the more diverse edge representation. The GNNs take considerably longer to evaluate than the FCNNs, although this difference is very tiny compared to the expert agents.\nThe GNNs achieve far higher accuracy than the FCNNs on the OOD networks. This is also reflected in the simulation performance, although the effect is smaller. Although GNNs show a superior ability to generalize to OOD networks, they still perform considerably worse than GNNs trained on these networks.\nThere seemed to be a limit to the accuracy of the trained models. This can be attributed to class imbalance and class overlap. We hypothesize that the class overlap originates from the Grid2Op simulate function. It is possible that identical network states have diverging forecasts and consequently diverging actions. The limited accuracy does not stop the ML models from successfully completing many of the grid operation days. The naive ML models come close to the performance of the expert agents. The hybrid agents can match them.\nHybrid agents can outperform expert agents in specific settings, which is surprising. We hypothesize that this results from omitting the state-action pairs from failed runs. The model bias towards frequent actions might also result in more robust topologies.\nAn interesting effect is that model differences are more pronounced in accuracy than simulation performance. Similarly, the limited accuracy does not stop the models from operating the grid well. We have two explanations for this. Firstly, a misprediction does not necessarily lead to a game-over. This is probably particularly true in regions with class overlap. Secondly, difficult days led to the generation of many relatively uncommon datapoints. Failure to predict many such datapoints would consequently affect the completion of only few days."}, {"title": "7. Future Work", "content": "Our findings indicate the promise of fully graphical neural networks for topology control methods. However, further research is necessary to establish this. Future work should compare fully GNN models with models that only use GNNs for feature extraction. Similarly, the heterogeneous graph representation should be introduced into other approaches. Moreso, it would be good to apply these methods in a reinforcement learning framework.\nFurther efforts are likely also necessary to scale this approach to larger grids. Fully GNN models might require very deep GNNs to model large graphs. Very deep GNNs are associated with additional challenges [46]. Future research could focus on methods to downsample grid graph representations, e.g., through trainable graph pooling [12]. Alternatively, it might be beneficial to combine GNN layers and other layers to capture long-range patterns.\nThe imitation learning approach used here can also be developed further. Our approach suffers from distribution shift, a compounding divergence between the behavior of the ML and the expert agent [1]. More advanced IL frameworks, such as DAgger, address this [34]. Investigating other expert agents and developing an improved dataset would also be valuable.\nFinally, the generalization capabilities of graph neural networks should be studied more extensively. Future studies should investigate how factors such as the diversity of training data and differences between the ID and OOD data affect generalization."}, {"title": "8. Statements", "content": ""}, {"title": "8.1. CRediT authorship contribution statement", "content": "Matthijs de Jong: Methodology, Software, Formal Analysis, Investigation, Data Curation, Writing - Original Draft, Visualization. Jan Viebahn: Conceptualization, Methodology, Resources, Writing - Review & Editing, Supervision, Project Administration. Yuliya Shapovalova: Methodology, Resources, Writing - Review & Editing, Supervision."}, {"title": "8.2. Declaration of competing interest", "content": "The authors declare the following financial interests/personal relationships which may be considered as potential competing interest: Jan Viebahn and Matthijs de Jong reports financial support was provided by TenneT TSO BV. Yuliya Shapovalova declares that she has no such interests."}]}