{"title": "Enhancing Privacy in Federated Learning: Secure Aggregation for Real-World Healthcare Applications", "authors": ["Riccardo Taiello", "Sergen Cansiz", "Marc Vesin", "Francesco Cremonesi", "Lucia Innocenti", "Melek \u00d6nen", "Marco Lorenzi"], "abstract": "Deploying federated learning (FL) in real-world scenarios, particularly in healthcare, poses challenges in communication and security. In particular, with respect to the federated aggregation procedure, researchers have been focusing on the study of secure aggregation (SA) schemes to provide privacy guarantees over the model's parameters transmitted by the clients. Nevertheless, the practical availability of SA in currently available FL frameworks is currently limited, due to computational and communication bottlenecks. To fill this gap, this study explores the implementation of SA within the open-source Fed-BioMed framework. We implement and compare two SA protocols, Joye-Libert (JL) and Low Overhead Masking (LOM), by providing extensive benchmarks in a panel of healthcare data analysis problems. Our theoretical and experimental evaluations on four datasets demonstrate that SA protocols effectively protect privacy while maintaining task accuracy. Computational overhead during training is less than 1% on a CPU and less than 50% on a GPU for large models, with protection phases tacking less than 10 seconds. Incorporating SA into Fed-BioMed impacts task accuracy by no more than 2% compared to non-SA scenarios. Overall this study demonstrates the feasibility of SA in real-world healthcare applications and contributes in reducing the gap towards the adoption of privacy-preserving technologies in sensitive applications.", "sections": [{"title": "1 Introduction", "content": "Federated Learning (FL) is a distributed machine learning paradigm that enables multiple clients to collaboratively train a global model without sharing their local datasets. While researchers have largely focused in developing FL theories and methods in a variety of applications, the deployment of FL in real-world scenarios is still challenging, particularly in terms of communication protocols, security, and customization bottlenecks.\nA critical requirement for real-world applications of FL concerns the protection of the model's parameters shared by the clients during model aggregation. To this end, privacy-preserving methodologies such as Secure Aggregation (SA) [17] are currently under study, to guarantee that aggregated data shared among participants do not reveal individual contributions. Contrarily to other privacy-enhancing technologies like Differential Privacy (DP) [11], the privacy guarantees of SA rely on the security proofs of established cryptographic primitives [13,15].\nOn the practical side, while DP requires only minor adjustments to the federated aggregation process through the injection of noise to the model's parameters, implementing SA in production is more complex as it requires changes to the standard operational flow of the FL framework by incorporating new communication phases. As a result, the adoption of SA in currently available FL software frameworks is lagging behind. Existing SA solutions primarily target settings with a large number of clients, where hardware limitations can lead to protocol execution failures. Some preliminary solutions have been proposed in the framework FLOWER [4], which however introduce a non-negligible overhead. The approach provided by NVFLARE is simpler but suffers from a weak security model [21]. Finally SA in OPENFL [20] requires dedicated hardware solutions. Overall, the applications of these SA protocols in the cross-silo healthcare setting is suboptimal, due to the limited number of clients, and their general availability as compared to the cross-device setting.\nTo address these limitations, in this work we explore the implementation of SA schemes optimally customized for cross-silo healthcare applications. In particular, we study the two suitable categories of SA based on masking and additively homomorphic encryption [17]. We identify respectively LOW OVERHEAD MASKING [15] and JOYE-LIBERT [13] as the most relevant solutions for our application. These protocols are designed to protect individual updates from being exposed during the aggregation process.\nThis work is based on theoretical and experimental evaluation of these SA protocols within the Fed-BioMed framework [9]. In particular, we conducted a comprehensive comparison on four distinct medical datasets including medical images and tabular data: Fed-IXI [24], Fed-Heart [24], REPLACE-BG [1], and FedProstate [12]. We measured the computational resources required for training, encryption, and overall execution time. When training was performed on a CPU, we achieved a total computation overhead of less than 1%, while on a GPU, for larger machine learning models (> 5M parameters), the overhead was less than 50%, with a protection phase that took less than 10 seconds. Furthermore, we analyzed the impact of SA on task accuracy, demonstrating that incorporating SA into Fed-BioMed affects accuracy by no more than 2% compared to non-SA scenarios. Overall this study demonstrates the feasibility of SA in real-world healthcare applications and contributes in reducing the gap towards the adoption of privacy-preserving technologies in sensitive applications."}, {"title": "2 Background", "content": "Federated Learning. As introduced by McMahan et al. [18], FL consists of a distributed machine learning paradigm where a group of clients, denoted as $\\mathcal{U}$, collaboratively trains a global model with parameters $\\theta \\in \\mathbb{R}^d$, under the guidance of a FL server. One of the first and popular methods used to train a FL model is the FedAvg scheme [18]. With FedAvg, at each FL round denoted by $t$, each client $u \\in \\mathcal{U}$ trains the model $\\theta_{u, t}$ on the private local data $\\mathcal{D}_u$, for example through Stochastic Gradient Descent (SGD) [6]. Upon completion of the local training, each client forwards its updated model $\\theta_{u, t}$ to the server and the local dataset size $w_u = |\\mathcal{D}_u|$. When the server receives the updated models from all participating clients, it proceeds to the weighted aggregation step:\n\n$\\theta_{t+1} \\leftarrow \\frac{\\sum_{u \\in \\mathcal{U}^{(t)}} w_u \\theta_{u, t}}{\\sum_{u \\in \\mathcal{U}^{(t)}} w_u}$\n\nThis iterative process continues until the global model $\\theta$ reaches some desired level of accuracy. The presence of a large number of FL clients significantly impacts the communication overhead. To mitigate this, instead of involving all clients in the training, at each FL round, the server selects a subset of clients (client selection [18]), denoted as $\\mathcal{U}^{(t)} \\subseteq \\mathcal{U}$, with $|\\mathcal{U}^{(t)}| = n$, and collects their parameters only for aggregation.\nSecure Aggregation. SA [17] typically involves multiple users and a single aggregator. Each user possesses a private input, and the role of the aggregator is to calculate the sum of these inputs. A property of SA is that the aggregator learns nothing more than the aggregated sum, thereby preserving the privacy of individual user inputs.\nSA has found significant applications in Federated Learning (FL), where it is used to securely aggregate the updated model parameters received from FL clients (aligned with the user's concept in SA) during each FL round, by instantiating an FL server (aggregator in the context of SA). The adoption of SA is motivated by the potential threats posed by adversaries having access to the client's updated model $\\theta_u$, which may infer information about its private dataset $\\mathcal{D}_u$ [19,23]. Hence, the local models should remain confidential even against the FL server. SA in FL was first developed by Bonawitz et al. [5]. The protocol considered in that study faced two different challenges:\n\u2022 Threat models defining the potential risks and behaviors that the security protocol is designed to protect against. The primary threat scenarios in SA include the honest-but-curious model where parties (server and clients) follow the protocol without tampering with the data but may attempt to infer additional information.\n\u2022 Client dropouts, caused by factors such as connectivity issues or voluntary withdrawal, are common in real-world federated learning environments. Dropouts can significantly impact the computation and number of communication rounds of the SA protocol, as they often require the participation of all"}, {"title": "3 Related Works", "content": "In real-world deployments, only a few FL frameworks implement some form of SA: OPENFL [20], NVFLARE[21], and FLOWER [4].\nFLOWER implements SECAGG+ [2], a masking-based protocol that ensures security in the honest-but-curious model. This protocol requires four communication rounds and uses Shamir's Secret Sharing to recover missing masks in case of client dropout, ensuring the server can complete the aggregation. Compared to the SA schemes here introduced in Fed-BioMed, Flower's approach is more costly in terms of communications, albeit accommodating for client dropout.\nNVFLARE introduces an SA method that leverages the CKKS asymmetric homomorphic encryption scheme [8]. This threat model is considered weaker than typical state-of-the-art protocols because it requires clients to share a common secret key and assumes clients are honest. Clients protect their inputs using a public key, while the server, operating under the honest-but-curious model, aggregates these inputs and returns the aggregate to each client for decryption using the same secret key. This approach requires one communication round and allows client dropout.\nOPENFL's use of Trusted Execution Environments (TEEs) represents a further step in sandboxing and securing local computations, but requires specific hardware which may not be available in typical FL studies involving hospitals."}, {"title": "4 Methods", "content": "In this section, we detail the implementation in Fed-BioMed of the two SA protocols, JOYE-LIBERT (JL) and LOW OVERHEAD MASKING (LOM). From this point on, we adopt the terminology of Fed-BioMed, where a client is referred to as a node.\nA general overview of SA is depicted in Figure 1, and a more detailed scheme is provided in Supplementary Figure 4. An SA protocol comprises two phases: setup and online. The setup phase, illustrated in Figure 1.1, is executed among all participating nodes in $\\mathcal{U}$ before the FL training. This step ensures that all parties have the appropriate cryptographic material necessary to run the specific SA protocol.\nThe online phase, Figure 1.2 is repeated during each FL round $t$ and consists of two steps: (i) protect and (ii) aggregate. In the protection step, each node protects its private local model using specific SA primitives and then sends the protected model to the server. In the aggregation step, the server receives the protected local models, computes the aggregate, and then decrypts it. To ensure the correct functioning of the cryptographic primitives, the locally-trained model vector of each node must be quantized beforehand.\nTo perform FedAvg with SA, we first convert the node's local parameters $\\Theta_{u, t} \\in \\mathbb{R}^d$ into integers $\\mathbb{Z}_{2^L}$, where $L$ represents the maximum number of bits of the plaintext. This conversion is achieved by applying uniform quantization, defined as: $Q(\\theta_{u, t}) = \\lfloor 2^L \\cdot \\frac{(\\theta_{u, t} - \\theta_{min})}{(\\theta_{max} - \\theta_{min})}\\rfloor$. Here, $\\lfloor .\\rfloor$ denotes the standard rounding function. To ensure that real values are within a desired range, we apply a clipping function, $clip(x, \\theta_{min}, \\theta_{max}) = min(max(x, \\theta_{min}), \\theta_{max})$, where $\\theta_{min}$ and $\\theta_{max}$ are the lower and upper bounds, respectively.\nTo apply weighted averaging over the integers, we assume that $w_u \\in \\mathbb{Z}_{2^{W_u}}$, where $W_u$ is the number of bits to represent the node's dataset size, and we define $W = max(\\lbrace W_u \\rbrace_{v \\in \\mathcal{u}})$.\nThe weighted local model is computed as $x_{u, t} = Q(\\theta_{u, t}) \\cdot w_u$, resulting in $x_{u, t} \\in \\mathbb{Z}_{2^{L+W}}$. To avoid overflow, we define $M = L+W+log_2(n)$ as the maximum number of bits for sum computation. The aggregate $X_{t+1} = (\\sum_{u \\in \\mathcal{u}^{(t)}} x_{u, t}) \\in \\mathbb{Z}_{2^M}$ is then divided by $s = \\sum_{u \\in \\mathcal{u}^{(t)}} w_u$ and dequantized using the following formula: $\\Theta_{t+1} = Q^{-1}(X_{t+1}) = \\frac{X_{t+1}}{s} \\cdot \\frac{(\\theta_{max}-\\theta_{min})}{2^L} + \\theta_{min}$.\nIn this context, we assume that quantization has been performed and omit the details of the dequantization process in the protocol explanation.\nJoye-Libert\nIn Supplementary Figure 4a, we illustrate the Joye-Libert (JL) implementation in Fed-BioMed. During the setup phase, the participating nodes $\\mathcal{U}$ generate their private keys $sk_u$, and the server creates its server key $sko$ - which is the sum of the node keys - using Shamir Secret Sharing (SS) [22].\nDuring the online phase, the protection and aggregation are applied as described in JL (Section 4 [13]). In the protection step, each node uses a private secret key $sk_u$ at FL round $t$ with a one-time mask derived from $sk_u$ and $t$, to obtain a protected local model through modular exponentiation over a large moudulus $N$. Using the server key $sko$, the server can recover the aggregate of the nodes' private local models in clear.\nOur JL implementation works with vectors; the protection and aggregation algorithms are applied element-wise. We use the element's index $i$ to generate a unique FL round (need to guarantee a one-time mask) for each element in the vector. For instance, to protect $x_{u, t}$, we execute protect and aggregate over the FL round $t$ and input $x_{u, t}[i]$, where $x_{u, t}[i]$ represents the $i$-th element of the vector $x_{u, t}$.\nThe computation and communication of the protected local model is optimized by using vector encoding [16].\nLow Overhead Masking\nThe second implementation, Low Overhead Masking (LOM) [15], which supports client selection, is depicted in Supplementary Figure 4b. During the setup phase, all participating nodes $\\mathcal{U}$ establish a pairwise secret $s_{u,v}$, such that $s_{u,v} = s_{v,u}$, with all nodes through the Diffie-Hellman Key Agreement (KA) [10], which will be used in the protect step.\nIn the online phase, during the protection step, a selected node $u \\in \\mathcal{U}^{(t)}$ runs the protect algorithm (Section 3.4 [15]). This algorithm protects the local model with a one-time mask derived through a Pseudo-Random Function (PRF) which uses the pairwise secret with the selected nodes $\\mathcal{U}^{(t)}$ and the current FL round $t$, and sends the protected local model to the server. The server then sums the protected local models and collects the final aggregate $X_{t+1}$."}, {"title": "5 Evaluation", "content": "In this Section we provide our theoretical and experimental evaluation of the two implemented SA protocols.\nComplexity Analysis: JL's node computation is $O(d)$, independent of the number of selected nodes, but requires modular exponentiation, and node communication for vector encoding is $O(d \\cdot 2^M)$ [16]. The server's computation is $O(n + d)$, involving $n$ multiplications and $d$ exponentiation [13].\nLOM's node computation is $O(nd)$, dependent on the number of selected nodes, using faster modular addition and PRF evaluation. The server's computation involves $nd$ modular additions, and node communication is $O(d \\cdot M)$ [15].\nExperimental evaluation: The experimental evaluation consists of tracking the computation time between JL and the LOM. We carried out the experiments by considering varying FL hyper-parameters represented by the number of total nodes $n_{tot}$, the number of selected nodes $n$, the number of FL rounds $T$, the number of local SGD steps $e$, the batch size $b$ and the learning rate $\\eta$. For SA, the hyper-parameterswe explored were the number of bits input $L$, the number of bits weight $W$. Moreover, we fixed the aggregation number of bits $M = 32$ and the clipping range min and max. Finally, we report the hardware used to train ML model. We report all this information for each experiments in Table 1.\nWe use four medical datasets to evaluate the task accuracy of our SA implementations over the aggregated global model at each FL round, using a dedicated tasks-specific test set, and tracking the required computational resources for the nodes."}, {"title": "6 Conclusion and Future Works", "content": "We have demonstrated that SA can be effectively implemented within the Fed-BioMed framework to enhance privacy in federated learning. Our evaluations using four medical datasets show that both Joye-Libert and Low Overhead Masking protocols protect privacy while maintaining task accuracy. The computational overhead is minimal, making SA a viable option for real-world deployments. As part of future work, we plan to replace MP-SPDZ with a direct implementation of additive secret sharing within Fed-BioMed. We also aim to replace JL with a quantum-resistant SA [7] using the SHELL C++ library 7."}]}