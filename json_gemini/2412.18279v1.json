{"title": "Improving Multi-Step Reasoning Abilities of Large Language Models with Direct Advantage Policy Optimization", "authors": ["Jiacai Liu", "Chaojie Wang", "Chris Yuhao Liu", "Liang Zeng", "Rui Yan", "Yiwen Sun", "Yang Liu", "Yahui Zhou"], "abstract": "The role of reinforcement learning (RL) in enhancing the reasoning of large language models (LLMs) is becoming increasingly significant. Despite the success of RL in many scenarios, there are still many challenges in improving the reasoning of LLMs. One challenge is the sparse reward, which makes optimization difficult for RL and necessitates a large amount of data samples. Another challenge stems from the inherent instability of RL, particularly when using Actor-Critic (AC) methods to derive optimal policies, which often leads to unstable training processes. To address these issues, we introduce Direct Advantage Policy Optimization (DAPO), an novel step-level offline RL algorithm. Unlike standard alignment that rely solely outcome rewards to optimize policies (such as DPO), DAPO employs a critic function to predict the reasoning accuracy at each step, thereby generating dense signals to refine the generation strategy. Additionally, the Actor and Critic components in DAPO are trained independently, avoiding the co-training instability observed in standard AC algorithms like PPO. We train DAPO on mathematical and code query datasets and then evaluate its performance on multiple benchmarks. Our results show that DAPO can effectively enhance the mathematical and code capabilities on both SFT models and RL models, demonstrating the effectiveness of DAPO.", "sections": [{"title": "1 Introduction", "content": "In the rapidly evolving landscape of artificial intelligence, large language models (LLMs) have emerged as a cornerstone of natural language processing (NLP) and beyond. These models, trained on vast corpora of text data, have demonstrated an unprecedented ability to understand [17, 3], generate and reasoning such as solving mathematical problems [52, 40, 53, 46] and code generations [22, 9]. When the token generation process of a LLM is modeled as a Markov Decision Process (MDP), it can be naturally optimized and aligned with human preference using reinforcement learning (RL) methods, known as Reinforcement Learning from Human Feedback (RLHF). The pipline of RLHF generally consists of two stages: 1) Train a reward model (often under Bradley-Terry [7] model) to label the responses generated by the LLM. 2) Sample a lot of responses and use RL methods to optimize LLM's generation policy by these responses and the associated rewards.\nDespite the success of RLHF in various fields [12, 35, 36, 47, 39, 6], it still encounters challenges and difficulties in the field of reasoning. One of the key challenges is the sparsity of rewards [37, 49]. When use LLMs for mathematical problem-solving and code generation, rewards are only assigned to the terminal tokens. This implies that the generation of intermediate tokens receives no direct reward, and the optimization direction relies solely on the backpropagation of the reward from the terminal token. Consequently, given the vast generation space, the successful training of LLMs requires extensive sampling to address exploration"}, {"title": "1.1 Related Work", "content": "RLHF Generally speaking, RLHF solves the problem formulated as\n$$\\max _{\\pi \\in \\Pi} \\mathbb{E}_{x \\sim D, \\atop \\mathbb{E}_{t \\in \\mathbb{N}: a_{t} \\sim \\pi(\\cdot|s_{t})}[\\sum_{t=0}^{T-1} r(s_{t}, a_{t}) - \\beta \\cdot K L(\\pi(\\cdot|s_{t}), \\pi_{r e f}(\\cdot|s_{t})) | s_{0}=s]$$\nwhere D is the distribution of training prompts x, $a_t$ is the action (or the tokens generated) at time t, T is the decision horizon, r(\u00b7) is the ground truth reward function, $\\pi_\\theta$ is the parameterized generation policy of LLM, $\\pi_{ref}$ is the initial reference policy, \u03b2 is the KL coefficient. We present a more formal formulation of RLHF problem in Section 2. Notice that the definition on what is a single action can be very flexible, thus different definition of a action yields different algorithm designs and theoretical results. Response-level RLHF regards a full response as a single action, and the RLHF problems (1.1) reduces to a bandit problem [10], i.e.,\n$$\\max _{\\theta} \\mathbb{E}_{x \\sim D} [\\mathbb{E}_{y \\sim \\pi_{\\theta}(\\cdot|x)}[r(x, y)] - K L(\\pi_{\\theta}(\\cdot|x), \\pi_{r e f}(\\cdot|x))],$$\nwhere y is the response of x outputted by the LLM. A great advantage of considering the RLHF problem in the bandit setting is that the optimal policy $\\pi_\\theta^*$ of (1.2) has the analytic solution given by\n$$\\forall x, y: \\log \\frac{\\pi_{\\theta^{*}}(y | x)}{\\pi_{r e f}(y | x)}=\\frac{1}{\\beta} \\left\\{r(x, y)-\\log \\left{\\mathbb{E}_{y^{\\prime} \\sim \\pi_{r e f}(\\cdot | x)}[\\exp \\left(\\frac{1}{\\beta} r(x, y^{\\prime})\\right)]\\right}\\right\\} :=\\mathcal{V}_{r e f}(x) .$$"}, {"title": "2 Preliminaries", "content": "In this section, we introduce the mathematical formulation of step-level RLHF problem and some important theoretical results.\nRLHF on step-level MDP Compared with standard RLHF, which regard outputting a full response as a single action, step-level RLHF treats outputing a full reasoning step as a single action. More Specifically, the fundamental model of step-level RLHF can be represented by a tuple M = (S, A, r, f, \u03bc), where S is the state space and a state s \u2208 S is any reasoning steps outputted by the LLM so far, A is the action space and an action a \u2208 A is any valid singular reasoning step, r(s, a) is the step-wise reward, f is the deterministic transition model between steps (i.e., \u2200t \u2208 N, $s_{t+1}$ = f($s_t$,$a_t$)), \u00b5 is the distribution of the initial state $s_0$. Under this setting, the objective of RLHF is to find a policy that maximize the cumulative reward in the trust region of reference policy $\\pi_{ref}$,\n$$\\pi^{*} \\in \\underset{\\pi \\in \\Pi}{\\operatorname{argmax}} \\mathbb{E}_{\\forall t \\in \\mathbb{N}: a_{t} \\sim \\pi(\\cdot|s_{t})}[\\sum_{t=0}^{T-1} r(s_{t}, a_{t}) - \\beta \\cdot K L(\\pi(\\cdot|s_{t}) \\|\\pi_{r e f}(\\cdot|s_{t})) | s_{0} \\sim \\mu],$$,\ngiven the KL coefficient \u03b2, and the set of all admissible policies \u03a0. Here T is the random decision horizon (i.e., LLM outputs a full response after T reasoning steps and $s_T$ is a terminate state that either contains the [EOS] token or is truncated due to the length limit). For easy of notation, we define $P^\\pi$ as the probability function of the trajectory ($s_0$, $a_0$, ..., $a_{T\u22121}$) induced by the policy \u03c0 and the expectation $E^\\pi[.]$ as\n$$\\mathbb{E}^{\\pi}[g(s, a)] := \\mathbb{E}_{s_{t}, a_{t} \\sim \\mathbb{P}^{\\pi}}[\\sum_{t=0}^{T-1} g(s_{t}, a_{t})| s \\sim p]$$\nfor any real-valued function g : S \u00d7 A \u2192 R and p\u2208\u0394(S).\nAssumption 2.1. We assume there exists a constant $H_1$ < \u221e such that for any policy \u03c0\u2208 \u03a0 and initial state $s_0$ \u2208 S, the random decision horizon T satisfies T < $H_1$ almost surely. We also assume that there exists a constant $H_2$ < \u221e such that for any singular reasoning step a \u2208 A, the token length of a is no greater than $H_2$ so that the state space S and action space A are both finite.\nAssumption 2.2 (Traversal initial distribution). We assume for any state s \u2208 S, \u03bc(s) > 0\nRemark 2.1. Assumption 2.2 is made just for the simplicity of theoretical analysis and is commonly appeared in the previous works on the convergence analysis of policy optimization like [1, 33, 29, 30, 28, 32].\nValue functions Given any state x \u2208 S, we define the KL-constrained state value function as\n$$V_{\\pi}^{\\beta}(s) := \\mathbb{E}_{\\forall t \\in \\mathbb{N}: a_{t} \\sim \\pi(\\cdot|s_{t})}[\\sum_{t=0}^{T-1} (r(s_{t}, a_{t}) - \\beta K L(\\pi(\\cdot|s_{t}), \\pi_{r e f}(\\cdot|s_{t})))| s_{0}=s]$$\nand the KL-constrained state-action value function as\n$$Q_{\\pi}^{\\beta}(s, a) := r(s, a) + V_{\\pi}^{\\beta}(f(s, a)) .$$"}, {"title": "3 Direct Advantage Policy Optimization", "content": "In this section, we present the details of DAPO. We only consider the case that the groud-truth reward is only assigned in the terminal state since it's the most common situation in the LLMs setting. We first derive the objective of policy optimization in Section 3.1 assuming that one has access to the true value function. Then in Section 3.2, we present how to train a critic as the approximation of the true value function. Finally, we give our theoretical analysis of DAPO in Section 3.3."}, {"title": "3.1 Policy Objective", "content": "We first derive the policy optimization objective in one single iteration provided with the reference policy $\\pi_{ref}$. In the perspective of policy optimization, the most important quantity may be the advantage function. Recall Lemma 2.2. The performance difference for any policy \u03c0\u2208 I w.r.t ref is measured as\n$$V_{\\pi}^{\\beta}(\\mu) - V_{r e f}^{\\beta}(\\mu) := \\mathbb{E}_{\\mu} [I_{s}(\\pi, \\pi_{r e f})],$$\nwhere the term $I_s(\\pi, \\pi_{ref})$ := $T_{\\pi}^{\\beta}V_{ref}(s) - V_{ref}^{\\beta}(s)$ can be regarded as the one-step policy improvement at the state s. A direct computation yields that\n$$I_{s}(\\pi, \\pi_{r e f}) = \\mathbb{E}_{a \\sim \\pi(\\cdot|s)}[r(s, a) + V_{r e f}^{\\beta}(f(s, a)) - \\beta \\log \\frac{\\pi(a | s)}{\\pi_{r e f}(a | s)} - V_{r e f}^{\\beta}(s)]$$\n$$= \\mathbb{E}_{a \\sim \\pi(\\cdot|s)}[A_{r e f}^{\\beta}(s, a) - \\beta \\log \\frac{\\pi(a | s)}{\\pi_{r e f}(a | s)}] \\tag{a}$$\n$$= \\mathbb{E}_{a \\sim \\pi(\\cdot|s)}[A_{r e f}^{\\beta}(s, a)] - \\beta \\mathbb{E}_{a \\sim \\pi(\\cdot|s)}[\\log \\frac{\\pi(a | s)}{\\pi_{r e f}(a | s)}] \\tag{b}$$$,\nwhere (a),(b) leverages the definition of $A_{ref}^{\\beta}$ and $A_{r e f}^{\\beta}$ in (2.4). A natural idea that ensures $V_{\\pi}^{\\beta}(\\mu)$ \u2265 $V_{ref}^{\\beta}(\\mu)$ \u2265 0 might be to increase $I_s(\\pi, \\pi_{ref})$ on as many states s as possible. For ease of understanding, we first focus on how to increase $I_s(\\pi, \\pi_{ref})$ at an arbitrary state s \u2208 S. Considering parameterized policy $\\pi_\\theta$, the most directly and easily method may be to do multi gradient ascent steps w.r.t $I_s(\\pi_\\theta, \\pi_{ref})$, i.e.,\n$$\\forall k \\in \\mathbb{N}^{+}: \\theta_{k+1} \\leftarrow \\theta_{k} + \\eta \\cdot \\nabla_{\\theta} I_{s}(\\pi_{\\theta_{k}}, \\pi_{r e f}).$$\nthe parameter after K \u2208 $\\mathbb{N}^{+}$ gradient ascent steps is actually\n$$\\theta_{K} = \\theta_{0} - \\eta \\sum_{k=0}^{K-1} \\nabla_{\\theta} I_{s}(\\pi_{\\theta_{k}}, \\pi_{r e f}).$$\nFollowing lemma gives an equivalent form of $\\nabla_{\\theta} I_{s}(\\pi_{\\theta_{k}}, \\pi_{r e f})$.\nLemma 3.1. If we define the surrogate function\n$$H^{\\beta}(\\pi_{\\theta}, \\pi_{r e f}) := \\frac{1}{2} \\mathbb{E}_{a \\sim \\pi_{\\theta_{k}}(\\cdot|s)}[\\frac{1}{\\beta} A_{r e f}^{\\beta}(s, a) - \\log \\frac{\\pi_{\\theta}(a | s)}{\\pi_{r e f}(a | s)}]^{2},$$\nthen $\\nabla_{\\theta} I_{s}(\\pi_{\\theta_{k}}, \\pi_{r e f}) = -\\beta \\nabla_{\\theta} H^{\\beta}(\\pi_{\\theta}, \\pi_{r e f})$.\nBy Lemma 3.1, the update (3.3) is equivalent to\n$$\\theta_{K} = \\theta_{0} - \\eta \\beta \\sum_{k=0}^{K-1} \\nabla_{\\theta} H^{\\beta}(\\pi_{\\theta}, \\pi_{r e f}).$$\nWe now to turn to the offline policy optimization problem setting for the sake of implementation simplicity and data efficiency. A natural question is that can we implement (3.4) approximately in an offline dataset"}, {"title": "3.2 Critic Objective", "content": "Generally speaking, the critic optimization problem for any target policy \u03c0 can be formulated as\n$$\\underset{\\Phi}{\\arg \\min } \\mathbb{E}_{s \\sim \\mathcal{D}_{s}}[\\mathcal{L}(V_{\\Phi}(s), V^{\\pi}(s))],$$"}, {"title": "3.3 Theoretical Analysis", "content": "In this section, we give some theoretical analysis of DAPO in exact setting and the optimization problem in (3.6) becomes\n$$\\text { Exact DAPO: } \\min _{\\theta} \\mathbb{E}_{s \\sim v_{s}, a \\sim v_{A}(\\cdot|s)}\\left[\\frac{1}{2} [\\frac{A_{r e f}^{\\beta}(s, a)}{\\beta} - \\log \\frac{\\pi(a | s)}{\\pi_{r e f}(a | s)}]^{2}\\right].$$\nHere $v_s$ is the distribution of training states, $v_A(.|s)$ the distribution of training actions at each state s. We assume both $v_s$ and $v_A(a|s)$ are exploratory, i.e.,\n$$\\forall s \\in \\mathcal{S}, a \\in \\mathcal{A}: v_{s}(s)>0, v_{A}(a | s)>0.$$\nNotice that the state space S and action space A are finite in step-level MDP. Thus the assumption that the training distribution is exploratory isn't strict in exact setting. For example, the assumption can be satisfied if $v_A$ is a softmax policy and $v_s$ is the state visitation distribution induced by $v_A$. We first present monotonic improvement property when one has access to the advantage function oracle $A_{ref}$."}]}