{"title": "NOTA: Multimodal Music Notation Understanding for Visual Large Language Model", "authors": ["Mingni Tang", "Jiajia Li", "Lu Yang", "Zhiqiang Zhang", "Jinghao Tian", "Zuchao Li", "Lefei Zhang", "Ping Wang"], "abstract": "Symbolic music is represented in two distinct forms: two-dimensional, visually intuitive score images, and one-dimensional, standardized text annotation sequences. While large language models have shown extraordinary potential in music, current research has primarily focused on unimodal symbol sequence text. Existing general-domain visual language models still lack the ability of music notation understanding. Recognizing this gap, we propose NOTA, the first large-scale comprehensive multimodal music notation dataset. It consists of 1,019,237 records, from 3 regions of the world, and contains 3 tasks. Based on the dataset, we trained NotaGPT, a music notation visual large language model. Specifically, we involve a pre-alignment training phase for cross-modal alignment between the musical notes depicted in music score images and their textual representation in ABC notation. Subsequent training phases focus on foundational music information extraction, followed by training on music notation analysis. Experimental results demonstrate that our NotaGPT-7B achieves significant improvement on music understanding, showcasing the effectiveness of NOTA and the training pipeline.", "sections": [{"title": "Introduction", "content": "Music is expressed primarily in two forms: auditory music and symbolic music. Symbolic music can be represented in two-dimensional space through scores that display notes, rhythms, and dynamics, thereby guiding performers on how to play the music. It can also be expressed through lines of text sequences, effectively linearizing the complexity of music for ease of computer processing and programmatic manipulation. The evolution of Natural Language Processing (NLP) and multimodal interactions has provided valuable insights into the understanding and generation of music. With the advent of universal dialogue Multimodal Large Language Models (MLLMs) such as GPT-4(OpenAI, 2023), specialized models designed for various professional domains (Dey et al., 2024; Baez and Saggion, 2023), including music (e.g., MU-LLaMA (Liu et al., 2024)), have begun to proliferate. However, these works have only focused on the single modality of text, and in order to interact with multiple modalities, some MLLMs have been recently introduced. Nevertheless, these MLLM models mainly focus on the task of multimodal information extraction in the general domain, and rarely involve multimodal information extraction. Most existing datasets focus on specific symbols or audio (like ABC notation (Allwright, 2003), MIDI (Ryu et al., 2024), WAV (Sturm, 2013), and lyrics (\u00c7ano and Morisio, 2017)) and do not emphasize the visual modality, limiting their ability to enable MLLMs to understand music notation. Visual representations serve as a tangible record of music. These images not only encapsulate the score's information but also visually delineate its intricate structures (Tian et al., 2024a; Li et al., 2023b).\nTo address the above limitations, we introduce NOTA, the first and largest comprehensive dataset designed to train and evaluate multimodal models in music notation understanding. Spanning three distinct global regions, NOTA encompasses over 1 million records of music scores. And it is structured around 3 pivotal tasks: music information extraction, cross-modal alignment test, and music notation analysis. These tasks cover various aspects of music, including music theory, composition, genres, musical ontological elements, and humanistic connotations. Our dataset is divided into two main parts: the training dataset and the test dataset. On the one hand, it provides training materials for researchers in the community to train their own multimodal music models. On the other hand, it enables the evaluation of existing multimodal models' ability to understand music.\nBased on this dataset, we trained a 7B model, NotaGPT, capable of understanding music notation across multiple modalities, including visual modalities. This training process comprises a pre-alignment training focused on cross-modal alignment between the visual symbols in the music scores and their textual symbolic counterparts. This is followed by fine-tuning that aim at foundational music information extraction, and music notation analysis.\nUtilizing NOTA, we conducted comprehensive experiments on 17 mainstream multimodal large language models. Specifically, we input music score images and background information about the pieces, asking them to output basic information such as note lengths and key signatures or to perform analyses of the musical style and rhythm. Even the best-performing model, Gemini, achieved a music information extraction rate of only 33.34%. In contrast, our 7B model, trained on our dataset, achieved 67.84%. The experimental results demonstrate the limitations in model performance caused by the lack of multimodal music datasets and highlight the effectiveness of our NOTA dataset and our training pipeline.\nOur contribution can be summarized as follows: We introduced NOTA, the first and largest comprehensive multimodal music notation understanding dataset. This dataset encompasses 1,019,237 records from 3 distinct global regions and is dedicated to 3 tasks, addressing the resource limitation available for multimodal music notation understanding."}, {"title": "Related Work", "content": "In the fields of NLP and multimodal interactions, traditional evaluation metrics predominantly focus on assessing specific capabilities of a model within singular task types(Goyal et al., 2017). For example, the GLUE (General Language Understanding Evaluation) (Sarlin et al., 2020) benchmark is a collection of diverse natural language understanding tasks designed to evaluate and advance the performance of models on a wide range of language comprehension challenges. These criteria either provide more dimensions of assessment (Guha et al., 2024; Sun et al., 2024)and advanced capabilities or employ sophisticated evaluation mechanisms (Wang et al., 2023b; Valmeekam et al., 2024). For instance, the C-Eval (Huang et al., 2024b) benchmark addresses the gap in Chinese language data.\nThe evolution of evaluation benchmarks in NLP and multimodal fields has consequently influenced the benchmarks used in music evaluation. Presently, music evaluation metrics generally concentrate on distinct musical capabilities, such as music generation (Agostinelli et al., 2023; Melechovsky et al., 2023), music information retrieval (Kong et al., 2020; Zhao and Guo, 2021) and music understanding (Li et al., 2024b). Some initiatives, such as ChatMusician (Yuan et al., 2024), attempt to unify tasks in music generation and comprehension, yet suffer from limited data volumes. Despite the rapid development of multimodal generative models, there is still a lack of data and benchmarks that can effectively evaluate the models' capabilities in understanding visual modality of music score images."}, {"title": "Generative Models for Music Understanding and Generation", "content": "With the advent of generative dialogue LLMs such as ChatGPT(OpenAI, 2022), alongside a series of universal dialogue MLLMs, specialized models designed for various professional domains (Li et al., 2024a; Sun et al., 2022), including music (e.g., MU-LLaMA (Liu et al., 2024)), have begun to proliferate. As these MLLMs continue to evolve, music understanding capabilities have also been enhanced. For instance, current models like MusicAgent (Yu et al., 2023) and MusicLM (Agostinelli et al., 2023) have made remarkable progress in music comprehension and generation abilities.\nGenerative models for music understanding and generation can be broadly categorized into two modalities: audio music (Huang et al.; Copet et al., 2024) and symbolic music (Tian et al., 2024b; Lu et al., 2023). The former predominantly incorporates audio modalities into large language models (Huang et al., 2024a) or employs diffusion models (e.g., JEN-1 (Li et al., 2023a) and MeLoDy (Lam et al., 2024)) to process the audio components of music; the latter typically converts symbolic music information into sequences for integration into large language models (Yuan et al., 2024; Geerlings and Merono-Penuela, 2020).\nThe efficacy of these models hinges on precise instruction fine-tuning and cross-modal alignment (Geerlings and Merono-Penuela, 2020), utilizing specific musical datasets. Nevertheless, current generative music LLMs lack the ability to understand images of music scores in the visual modality."}, {"title": "Multimodal information extraction", "content": "Multimodal information extraction first searches for alignment in the two modalities connects them together, and then performs information extraction. It can be divided into two main categories: visual entity extraction and visual event extraction. In MORE (He et al., 2023), the objective is to predict relations between objects and entities based on both textual and image inputs. Visual event extraction can be further divided into situation recognition (Yatskar et al., 2016) and grounded situation recognition (Pratt et al., 2020). With the development of MLLMs, information extraction datasets for different tasks have also evolved (Wan et al., 2021; Yuan et al., 2023). However, there is still a lack of multimodal information extraction models and datasets specifically for the music domain."}, {"title": "NOTA Dataset", "content": "Our dataset is collected around three tasks: cross-modal alignment, music information extraction, and music notation analysis. We choose to use ABC notation to represent music scores. ABC notation encodes music into two parts: header and body. The first header is the reference number and the other headers are title T, time signature M, default note length L, key K, etc. The body mainly includes notes, bar lines, and so on.\nMusic Information Extraction In this task, we collect a total of 1,185,761 data entries. Music information extraction is divided into 6 subtasks: extracting ABC notation from corresponding images, and extracting specific information from the ABC notation, including T (tune title), K (key), L (unit note length), M (meter), and C (composer). We obtained 193,484 data entries from the ABC notation website, the vast majority of which are directly downloaded, and a small portion are scraped. After data cleaning, we only keep the ABC files that could generate the correct music score (we remove the original ABC file's comments, lyrics, and sequence numbers (X:)). We then transform ABC files into MusicXML files and use MuseScore4 to generate music score images from the MusicXML files. Afterward, we divide each data entry into 6 data entries corresponding to 6 subtasks, resulting in 1,160,904 data entries.\nIn order to test whether MLLMs have a special tendency towards certain regions, we additionally collect nearly 4000.krn files from the internet, subsequently use the humdrum toolkit to convert them into ABC files, then filter and convert them into MusicXML files, generate music score from MusicXML files, and finally divide them into 6 extraction subtasks, obtaining a total of 24857 data entries with three regional labels: <China>, <Europe>, and <America>.\nEach data sample includes the ABC notation information to extract, the corresponding music score images, the prompt used for extracting, and the gold answer. Data examples are in Figure 3.\nCross-modal Alignment In this task, we obtain 29,116 data entries. We highlight portions of the music score images, expecting that MLLMs can understand and extract the corresponding ABC notation content. Each music score image has 2 to 4 highlighted sections. For a music score image $X_v$ and its associated content $X_c$, we sample a question $X_q$, which asks to extract the specific content of the image. With $(X_v, X_c, X_q)$, we create a single-turn instruction-following example:\nHuman: < ImageHere > $X_q$ $X_v$ < STOP >\nAssistant: $X_c$ <STOP > (1)\nMusic Notation Analysis This task includes analysis of score structure and musical styles. In terms of score structure analysis, it involves systematic analysis of various musical elements such as structure, melody, harmony, tonality, rhythm, tempo, dynamics, texture, etc. We integrate authoritative works on domestic and international music notation analysis. We obtain 250 questions on score structure analysis and 600 questions on musical style notation analysis. These questions cover the analysis of classic works from different countries (Germany, France, Italy, the UK, the United States, and so on) and different historical periods (from the Baroque period to the 20th century), involving various musical genres such as sonatas, symphonies, waltzes, and operas. Each data entry contains title, composer, the corresponding image, a description, and an analysis or structural breakdown.\nOur dataset is divided into a train dataset and a test dataset. The train dataset has 998,976 samples, and the test dataset has 20,961 samples. More details are provided in Figure 1."}, {"title": "NotaGPT Training", "content": "We apply Mistral-7B (Jiang et al., 2023) as the base large language model and CLIP (Radford et al., 2021) as the vision encoder. Using the same network architecture as LLaVA (Liu et al., 2023a,b), the text model and the visual coder are connected through a linear projection layer. The model is first pre-trained with generalized domain multimodal datasets, which enables the model to understand images. Our music understanding training is mainly in three stages: cross-modal alignment, music information extraction, and music notation analysis, as shown in Figure 2."}, {"title": "Experiments", "content": "We comprehensively assess 17 MLLMs, including API-based models and open-source models. The API-based models contain GPT-4V (OpenAI, 2023), and Gemini (Team et al., 2023). The open-source models contain LLaVA (Liu et al., 2023a,b) series, VisualGLM (Du et al., 2022), Qwen-VL (Bai et al., 2023) series, and Yi-VL (Young et al., 2024) series.\nTraining Details For pre-training, we utilized the alignment section 3 data conducting training 10 epoch with a learning rate of 2e-4. For supervised fine-tune training, we employed the train data in section 3, training 3 epochs with a learning rate of 2e-5 and a batch size of 32. All experiments are conducted on 8\u00d780GB NVIDIA A100 SXM GPUs.\nEvaluation Details The temperature parameter was set to 0 to ensure deterministic output. For each model, we performed 3 separate evaluations using the GPT-4 API. The final score is determined by averaging the results from these 3 assessments."}, {"title": "Evaluation Metrics", "content": "Closed-set tasks. (1) For tasks such as music information extraction, performance is assessed using the weighted extraction rate. They are questions with definitive answers such as music titles and note lengths. Given a response sequence R and an answer sequence A across a dataset of n queries, the overall success of the extractions can be defined as:\nExtraction Rate = $ \\frac{\\sum_{i=1}^{n} \\delta ([A_i \\subset R_i], 1)}{n} $ (2)\nwhere $\\delta(x,y)$ is the Kronecker delta function, which equals 1 if x = y and 0 otherwise. The condition $[A_i \\subset R_i]$ evaluates to 1 if the answer sequence $A_i$ is contained within the response sequence $R_i$, and 0 otherwise.\n(2) Regarding the task of converting images to ABC notation text, we utilize the Levenshtein Distance (Yujian and Bo, 2007) as evaluation metric. It refers to the minimum number of single-character operations required to transform model responses into answer sequence. Let D be a matrix of size $(|R| + 1) \\times (|A| + 1)$, where D[i][j] denotes the minimum edit distance between the first i characters of R and the first j characters of A. The subsequent values of D are computed using the recurrence relation:\nD[i][j] = min\n\t{\\begin{cases}\nD[i - 1][j] + 1 \\qquad (delet)\nD[i][j - 1]+1 \\qquad (insert)\nD[i - 1][j - 1] + + cost \\qquad (substitute)\n\\end{cases}\n(3)\nwhere cost is 0 if the characters R[i - 1] and A[j - 1] are the same, and 1 otherwise.\nOpen-set tasks. For notation analysis tasks with open-ended answers, we used 2 type assessment: (1)Calculating using metrics. Our metrics are divided into two categories: semantic similarity and word matching. For semantic similarity, we use LSA, which measures the semantic similarity of text by computing the cosine similarity between vectors. For word matching, we use ROUGE-1, ROUGE-L, and METEOR, which respectively calculate the number of unigram matches, longest common subsequence matches, and synonym matches.\n(2)Scoring using LLM as an evaluator. As existing studies (Zheng et al., 2023) demonstrated, strong LLMs can be good evaluators. We compare the analysis generated by NotaGPT-7B with the analysis generated by other models, and have GPT-4 (text model) evaluate the analysis from both models. The evaluation considers both the music itself and the music's background. The evaluation of the music itself includes aspects such as musical language (melody, tonality, rhythm, musical terminology, etc.), technique application, and composition style. The evaluation of the music's background includes considerations of the social, historical, and cultural context, including the composer's milieu, the background of the composition, and the ideology of the creation."}, {"title": "Results", "content": "Our experiment revolves around proving the effectiveness of NOTA in promoting music understanding. In order to enable the model to ultimately achieve music understanding, we have broken down the experiment into three sub-experiments: music information extraction, cross-modal alignment and music notation analysis. Music information extraction only extracts the basic elements from the score image, such as author information, title, T, K, L, M and C. Score image recognition builds upon the basic element extraction, further extracting the music score in ABC notation form. Music analysis then, based on the extracted music score, conducts understanding and analysis, including score structure analysis and musical style analysis.\nGeneral comparison The evaluation results are presented in Table 1. We report the average extraction rate, with 23.53% of the models showing an effective precision lower than 10%. Additionally, 58.82% of the models have an accuracy approximately between 10% to 30%, and only 17.64% of the models achieve an accuracy exceeding 30%. Overall, NotaGPT-7B demonstrated the best performance among all the models evaluated, achieving an extracte rate of 67.84. These findings highlight the challenges of the NOTA test dataset.\nFigure 4 illustrates the comparative performance of NotaGPT-7B and Gemini in several subcategories of an information extraction task. NotaGPT-7B significantly outperforms Gemini in the tasks of Author, K, L, and M, demonstrating the effectiveness of the training data. NotaGPT-7B does not perform very well on the title extraction task, and after analyzing it, we found that it is because it mistakenly extracts author information as title information.\nAfter training with the NOTA dataset, models of size 7B achieved substantial improvements in the categories K, L, and M, where performance was originally poor. These enhancements allowed them to surpass models of the same size and even those of larger sizes.\nTable 2 presents the evaluation results. Overall, while high precision in music information extraction benefits cross-modal tasks, the relationship isn't simply linear. NotaGPT-7B consistently performs well, showcasing its strength in both extracting and aligning musical information. In contrast, while GPT-4V and Gemini-pro-vision score similarly in extraction tasks (around 33.34), they differ greatly in alignment accuracy, with Levenshtein distances of 655.45 and 354.30, respectively, suggesting that factors like model structure and optimization strategies also influence performance.\nSince the model's analysis and the standard answer cannot be completely identical, we evaluate the strength of the model's analysis capability of the recognized music score from semantic similarity and word matching.\nFrom the results in Table 3, in terms of the LSA metric, the performance of NotaGPT-7B is stronger than most models, including some models with"}, {"title": "Conclusion", "content": "In this study, we introduce NOTA, a large-scale music understanding dataset encompassing 3 tasks with over 1.1 million data entries. Based on the NOTA train dataset, we trained NotaGPT-7B, which demonstrates robust music notation understanding capability. We further assess 17 multimodal models' capabilities in music understanding. The results show the constraints that are caused by the lack of multimodal music datasets, emphasizing the significance of the NOTA dataset."}, {"title": "Limitations", "content": "Although NOTA makes substantial advancement in developing effective music understanding datasets, we are aware of typical limitations in MLLMs, including hallucinations and shallow reasoning. Our future efforts will focus on improving the fidelity and dependability of these models."}, {"title": "Social Impact", "content": "The Nota-Eval dataset contains music from multiple regions and diverse cultural backgrounds. Not understanding the cultural context of the music may lead to misinterpretation of the music data, such as misreading the meaning and emotional expression of the music, as well as misjudging the characteristics and styles of the music."}, {"title": "Region-Level Evaluation", "content": "Table 5 presents the overall information extraction results for five information extraction tasks across 3 different regions using various models on our NOTA dataset. The experimental results indicate that the GPT-4V model significantly outperforms other models in music information extraction across different regions. For the five information extraction tasks in the regions of China and Europe, different models showed better performance compared to the America region. Additionally, there are noticeable differences in the information extraction capabilities of different models across the three regions. This suggests that different models have distinct preferences for understanding music from different regions, which may be related to the distribution of training data in these multimodal models."}, {"title": "Detailed Evaluation Metrics for Open-Set Tasks", "content": "Latent Semantic Analysis (LSA) is a technique in natural language processing and information retrieval that analyzes relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms. LSA assumes that words that are close in meaning will appear in similar pieces of text. The core idea involves constructing a term-document matrix, which is then decomposed using singular value decomposition (SVD). The semantic similarity between texts is often measured using the cosine similarity between their vector representations. Let A be the term-document matrix, then LSA involves the following computation:\nA\u2248 Uk2kV VT\nwhere:\n\u2022 Uk represents the first k columns of U,\n\u2022 Ek is the top k \u00d7 k submatrix of \u03a3,\n\u2022 VT is the first k rows of VT.\nROUGE-1 is a metric used to evaluate automatic summarization and machine translation software, focusing specifically on the overlap of unigrams (single words) between the system-generated summary or translation and a set of reference summaries. The ROUGE-1 score is calculated by counting the number of unigrams in the generated text that match the unigrams in the reference text and then normalizing this number by the total number of unigrams in the reference text, providing a measure of recall. ROUGE-N is a metric for evaluating text summarization and machine translation quality by measuring the overlap of N-grams between system-generated summaries and reference summaries. Specifically, ROUGE-1 is a variant of ROUGE-N where N equals 1, meaning it calculates the overlap using unigrams (individual words). ROUGE-1 focuses on assessing the recall of single words, providing a basic measure of content overlap and is widely used due to its simplicity and effectiveness in capturing essential content accuracy. ROUGE-N can be represented as:\nRouge-N = $ \\frac{\\sum_{S \\in ReferenceSummaries} \\sum_{gram_n \\in S} Count_{match}(gram_n)}{\\sum_{S \\in ReferenceSummaries} \\sum_{gram_n} Count(gram_n)} $\nROUGE-L measures the longest common subsequence (LCS) between a system-generated summary or translation and a set of reference texts. It is particularly useful for evaluating the fluency and the order of the text in summaries and translations. The LCS does not require consecutive matches but is a sequence where each word is in the same order in both texts. The score is computed by dividing the length of the LCS by the total length of the reference sequence, providing insights into the overall"}, {"title": "Author's statement and data license", "content": "We undertake to assume all legal liability that may arise from the use of the dataset, in particular in relation to data infringement. This includes, but is not limited to, copyright infringement, privacy breaches or any other legal issues of any kind. With respect to the licensing of the data, we confirm"}, {"title": "The Role of Humans in Data Collection", "content": "In the first two tasks, data was collected from electronic websites. The cleaning primarily involved dealing with some improperly formatted images and texts, as well as music pieces that had lost both author and title information, retaining only the melody.\nIn the final task, individuals were responsible for manually typing texts from over a dozen book publications. This included integrating authoritative works from both domestic and international sources in the fields of music appreciation, musical works analysis, and form analysis. Notable works included Yang Minwang's \"New Compilation of World Famous Music Appreciation,\" Wu Zuqiang's \"Form and Works Analysis,\" the \"Norton Introduction to Music History\" series, and Roger Kamien's \"Music: An Appreciation,\" among dozens of seminal studies on Western musical works."}]}