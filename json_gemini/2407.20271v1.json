{"title": "Learn while Unlearn: An Iterative Unlearning Framework for Generative Language Models", "authors": ["Haoyu Tang", "Ye Liu", "Xukai Liu", "Kai Zhang", "Yanghai Zhang", "Qi Liu", "Enhong Chen"], "abstract": "Recent advancements in machine learning, especially in Natural Language Pro- cessing (NLP), have led to the development of sophisticated models trained on vast datasets, but this progress has raised concerns about potential sensitive information leakage. In response, regulatory measures like the EU General Data Protection Regulation (GDPR) have driven the exploration of Machine Unlearning techniques, which aim to enable models to selectively forget certain data entries. While early approaches focused on pre-processing methods, recent research has shifted towards training-based machine unlearning methods. However, many existing methods require access to original training data, posing challenges in scenarios where such data is unavailable. Besides, directly facilitating unlearning may undermine the language model's general expressive ability. To this end, in this paper, we introduce the Iterative Contrastive Unlearning (ICU) framework, which addresses these challenges by incorporating three key components. We propose a Knowledge Unlearning Induction module for unlearning specific target sequences and a Con- trastive Learning Enhancement module to prevent degrading in generation capacity. Additionally, an Iterative Unlearning Refinement module is integrated to make the process more adaptive to each target sample respectively. Experimental results demonstrate the efficacy of ICU in maintaining performance while efficiently un- learning sensitive information, offering a promising avenue for privacy-conscious machine learning applications.", "sections": [{"title": "1 Introduction", "content": "With the continuous evolution of machine learning, we have witnessed an explosion in the size of models and the breadth of data used for their training. Particularly, the field of NLP has seen remarkable progress, with the advent of advanced Generative Language Models (GLM) such as GPT-4 [1], Claude 3 [4], and Google Gemini [48]. Nonetheless, concurrent studies have illuminated a concerning aspect of these models: the potential for the leakage of sensitive information [12], including but not limited to, phone numbers, email addresses, and other personal data embedded within the training datasets [8, 13]. In response to privacy concerns, the GDPR [53] has codified the \"Right To Be Forgotten\" (RTBF) [35, 52], leading to the exploration of Machine Unlearning techniques. These techniques aim to make models effectively \u201cforget\u201d certain data entries, treating them as if they were never part of the training dataset.\nIn the early stage, researchers attempted various pre-processing methods to achieve the goal for forgetting certain data. For instance, Kandpal et al. [29] find that the rate at which language models regenerate training sequence is related to the sequence's count in the training set and that after"}, {"title": "2 Related Work", "content": "Introduced by Cao and Yang [11], machine unlearning, aims at protecting machine learning models from extraction attacks, involves the process of removing specific data from a model in a manner that ensures the data appears as though it were never part of the training set. Conventional approaches [37] of excluding specific data from training datasets and subsequently retraining the model are highly time-consuming and resource-intensive, making it impractical for contemporary deep neural networks. Similar methods [7, 34] that involve retraining also fail to adequately address the issue, particularly when dealing with a large number of deletion requests or limited access to comprehensive datasets. Researchers have investigated approximate unlearning techniques to address the limitations of exact machine unlearning [21, 24, 36]. One such technique is data pre-processing, which efficiently identifies and removes sensitive information before it is incorporated into the model. Kandpal et al. [29] utilize this method on structured private data, including phone numbers and medical records, and found it to be highly effective. However, challenges emerge with less structured data, as pre- processing techniques may inadequately remove all sensitive information [10] and pre-processing alone cannot comprehensively address deletion demands [8].\nRecent researches [27, 30, 42, 57] have concentrated on fine-tuning language models to address challenges in machine unlearning. Jang et al. [27] introduce an innovative method by reversing the conventional training objective, aiming to maximize, rather than minimize, the negative log-likelihood of tokens designated for forgetting. Other recent methodologies [14, 19, 23, 30, 54] in this field use diverse techniques such as knowledge gap alignment and reinforcement learning. Despite offering promising solutions to machine unlearning challenges, these methods often exhibit high complexity and computational cost, rendering their practical implementation both time-consuming and intricate. For instance, Gu et al. [23] leverage second-order information (Hessian) to provide stronger guarantees for data removal and maintain model utility, but extensive computational resources are required for Hessian approximation."}, {"title": "2.2 Language Models", "content": "A language model is a computer program or algorithm that is designed to understand, generate, and predict human language [61]. The concept of machine unlearning for language models has gained increased attention in recent years [27, 30, 42, 57].\nAlthough traditional language models such as rule-based models [55] or statistical models [9] can generate outputs resembling human language, these outputs do not perfectly align with the training dataset. Typical neural networks utilized in NLP, such as Recurrent Neural Network (RNN) [56], and its derivatives like Long Short-Term Memory (LSTM) [25], encounter limitations due to their sequential processing architecture. This sequential structure leads to significant computational resources and time required for training, which hinders the model's scalability and efficiency. As a result, they often struggle to display a high level of proficiency in retaining the information from the training dataset [41].\nThe introduction of the transformer architecture [51] has revolutionized the realm of NLP. Built upon the self-attention mechanism, transformers facilitate the effective capture of contextual relationships within textual data. Subsequent advancements have diversified transformer-based approaches into three primary categories: encoder-only models, exemplified by BERT [16]; decoder-only models, typified by GPT [43]; and encoder-decoder models such as T5 [44]. Decoder-only models like GPT-4 [1], Claude 3 [4], and others [3, 48, 50] have demonstrated exceptional performance across various NLP tasks. However, this success raises privacy concerns, as these models have the potential to leak sensitive information from their training data. Moreover, alongside the enhancement in model capacity, there has been a corresponding increase in the demand for training data and computational resources [1, 4, 26, 38]. This escalation presents challenges for researchers working on machine unlearning in these advanced models, as it requires substantial effort and resources."}, {"title": "3 Method", "content": ""}, {"title": "3.1 Problem Statement", "content": "Given the data and model {D, Dfgt, f\u00f8}, where D = {xi}N i=1 is a small pre-training corpora with M i=1 N pieces of data, Dfgt = {xfgt}M i=1 is the collection of M pieces of data in D to be forgotten and f\u00f8 is the orginal language model with its parameters denoted as \u03b8, machine unlearning is to modify the parameters \u03b8 of the model f\u00f8 in such a way that it minimizes the retention of previously learned information about Dfgt while still maintaining desirable model performance and satisfying the specified constraints."}, {"title": "3.2 Model Overview", "content": "We propose a novel Iterative Contrastive Unlearning framework as shown in Figure 2. Aiming to delve into the unlearning process specifically tailored for decoder-only models, we seek to address the challenges of mitigating the memorization of sensitive information while preserving the models' language generation capabilities. In addition to Knowledge Unlearning Induction, which trains the model to forget target sequences, we introduce two supplementary modules. The Contrastive Learning Enhancement module uses specially selected data to maintain overall performance during unlearning training. Additionally, the Iterative Unlearning Refinement module updates the data to be forgotten iteratively, preventing over-unlearning and significant performance degradation."}, {"title": "3.3 Knowledge Unlearning Induction", "content": "For a sample xfgt \u2208 Dfgt, the sequence of tokens can be denoted as x = (x1, x2, ..., xn). Following Jang et al. [27], we negate the original negative log-likelihood of the target token sequences to induce the model to forget these sequences. The unlearning objective is computed by the following function:\nLfgt = \\sum_{t=1}^{T} log P_{\\theta}(x^{fgt}_{t}|x^{fgt}_{<t})\n(1)\nwhere x<t = (x1, x2, ..., Xt\u22121) denotes the first t tokens of the sequence, and P\u03b8(xt|x<t) represents the conditional probability of predicting the next token xt given x<t, with \u03b8 being the model parameters."}, {"title": "3.4 Contrastive Learning Enhancement", "content": "To maintain the stable generation capacity of the language model during unlearning, we propose simultaneously training the model on analogous data. This method ensures that the model forgets specific information without substantially reducing its ability to recognize and generate similar data patterns.\nKNN Sampling First, we compute the sentence embedding vx for all samples in D using a pre- trained sentence transformer fs. For each sample xfgt in Dfgt with its embedding vfgt = fs(xfgt), we employ K-Nearest Neighbors (KNN) [18] to identify the nearest (K = 1) embedding vx and the corresponding sample x. All x\u0302 are then collected to form D\u0302lrn.\n\\hat{x} = \\underset{x \\in D\\backslash D_{fgt}}{\\text{argmin}} dis(v_x, V_x)\n(2)\nwhere dis(\u00b7) is the cosine similarity function used in KNN search.\nLearning Enhancement Subsequently, we have x\u0302 = xlrn \u2208 D\u0302lrn which is the paired data of xfgt. In contrast to the unlearning objective, we force the model to learn data patterns from the paired token sequences using negative log-likelihood, which can be formulated as follows:\nLirn = \\sum_{t=1}^{T} log P_{\\theta} (x^{lrn}_{t} | x^{lrn}_{<t})\n(3)\nAdditionally, we apply Kullback-Leibler (KL) divergence [33] to guide the model in learning the original model's distribution for data intended to be retained following Yao et al. [57]:\nLkl = \\sum_{t=1}^{T} KL[P_{\\theta_0} (x^{lrn}_{t} | x^{lrn}_{<t}) || P_{\\theta} (x^{lrn}_{t} | x^{lrn}_{<t})]\n(4)\nwhere \u03b80 denotes the parameters of the original model."}, {"title": "3.5 Iterative Unlearning Refinement", "content": "In contrast to conventional machine learning techniques, validating the efficacy of unlearning poses challenges in identifying a suitable validation set since Dfgt is integrated into the training phase. Thus, determining an appropriate stopping criterion becomes essential. After each epoch during the training process, the model's performance relative to the target data is assessed using the two metrics below. We empirically define a specific token sequence x to be forgotten if BERTScore(x) < a and BLEU(x) < b are both met, where a and b are two thresholds of the iteration process. Samples identified as \"forgotten\" are excluded in the subsequent epoch. This iterative refinement process serves dual purposes: signaling the conclusion of model training and preventing further erosion of previously discarded information, thereby maintaining the model's proficiency and effectiveness."}, {"title": "3.6 Training", "content": "The objectives in Equation (1), (3) and (4) are jointly used to optimize the model during unlearning. The training process is governed by minimizing the following loss function:\nL = Lfgt + \u03b1Llrn + \u03b2Lkl\n(5)\nwhere \u03b1, \u03b2 > 0 are hyper-parameters that modulate the salience of different objectives."}, {"title": "4 Experiments", "content": "In this section, we first describe the datasets utilized for training and evaluation, and introduce the metrics employed to assess performance. Next, we present the baseline methods used for comparison with our proposed method. We then detail the configuration of our proposed method. Finally, we will present the experimental results and analysis in detail."}, {"title": "4.1 Datasets", "content": "To evaluate the method's learning and unlearning capabilities, we selected two types of datasets: Target Datasets, which assess unlearning ability, and Downstream Dataset, which evaluate the original capabilities of the models.\nTarget Dataset The Pile corpus (825GB) is a large dataset constructed from 22 diverse high-quality subsets, many of which derive from academic or professional sources (e.g. books, open source code) [20]. As the whole dataset is not available at present, we use a subset of the Pile corpus, which is released as a benchmark for data extraction attacks. Designed to be easy-to-extract, the subset contains 15,000 samples, randomly sampled from the Pile training dataset. Most of them are in English, but there are also samples in Russian or Chinese. Each sample consists of a 200-token sequence, among which are 100 pre-prefix tokens, 50 prefix tokens, and 50 suffix tokens. We only use the prefix and suffix tokens in our experiments.\nDownstream Dataset To assess the general performance of the LMs subsequent to the process of unlearning, a diverse array of downstream tasks is employed. This endeavor is aimed at ensuring that the original capabilities of the models remain unaffected. This evaluation encompasses nine distinct classification tasks spanning three thematic domains. Specifically, these domains include linguistic reasoning tasks such as Hellaswag [58] and Lambada [39], as well as assessments of commonsense reasoning through Winogrande [46] and COPA [22]. Additionally, scientific reasoning abilities are evaluated through tasks such as ARC-Easy [15], ARC-Challenge [15], Piqa [5], MathQA [2], and PubmedQA [28]. Furthermore, four dialogue tasks, namely Wizard of Wikipedia [17], Empathetic Dialogues [45], Blended Skill Talk [47], and Wizard of Internet [32], are used to gauge the model's proficiency in generating coherent responses. In addition, we measure the perplexity of the unlearned models on the validation set of Pile and Wikitext. As per Jang et al. [27], we use the test set for Lambada and the validation set for the remaining tasks."}, {"title": "4.2 Metrics", "content": "As stated in Section 4.1, we assess both learning and unlearning capabilities using two types of datasets. For unlearning ability, we follow Jang et al. [27] and examine the forgetting effect on the target unlearning data by EL and MA. EL and MA are used as the standard of terminating the training process, which makes them not suitable to be the evaluation metrics alone. Meanwhile, as mentioned in Section 3.5, we employ the BERTScore and BLEU to measure forgetting during IUR. For evaluating learning ability, we first utilize the metrics provided in the downstream datasets to measure performance. We then test the information entropy to ensure the result is coherent. Furthermore, we use GPT-4 to evaluate the text generated by the unlearned models. The following are the details of the metrics used."}, {"title": "4.3 Baseline Methods", "content": "Our experiments use the GPT-NEO model family (125M, 1.3B, 2.7B) [6], which is pre-trained on the Pile corpus. We use the OPT model family (125M, 1.3B, 2.7B) [59], which is pre-trained on a deduplicated version of the Pile as well as other corpus. Following Jang et al. [27], OPT serve as our baseline method for deduplication. Since the deduplicated version of GPT-NEO by Kandpal et al. [29] is not publicly available, we include KUMPR [27] as a second baseline method on GPT-NEO models to show the effectiveness of our proposed method. We follow the same training and evaluation procedure as Jang et al. [27] for the baseline methods."}, {"title": "4.4 Configurations", "content": "For each model size, we execute five runs of the methods, each targeting a dataset of 128 samples. Utilizing the all-MiniLM-L6-v2 model, we employ KNN to identify samples within the Pile subset, selecting the nearest one as the learning target. To determine the optimal hyperparameters for our proposed approach, we conduct multiple experiments involving varying learning rates as well as different weights assigned to Lfgt, Llrn, and Lnor. The model is optimized by Adam [31] with a learning rate of 5e \u2013 6, and \u03b1 = 0.5, \u03b2 = 1.0. We regard the model to have \u201cforgotten\" the target dataset with an average of EL10(x) < 0.0499 and MA(x) < 0.5994. The filtering thresholds during iteration are a = 0.3 and b = 0.01 as introduced in Section 3.5.\nWe run all the experiments on a Linux server with one 2.60GHz Intel Xeon Platinum 8358 CPU and NVIDIA GeForce RTX 3090 GPUs. We use one GPU for 125M models with batch size of 8. With Deepspeed Stage 2, we use three for 1.3B and six for 2.7B respectively with batch size of 4."}, {"title": "4.5 Main Results", "content": "The results of all methods are summarized in Table 1. Overall, our method consistently achieves the best or second-best performance across all metrics when compared to the baselines. Specifically, regarding unlearning ability (measured by EL, MA, and BERT), our method is second only to KUMPR, which uses these metrics as halting signals, and significantly outperforms the original model without unlearning methods. In terms of learning ability, our method attains near-optimal performance on most metrics and surpasses KUMPR across all metrics. Additionally, we discover some interesting phenomena as follows:\nFirst, the impact of unlearning becomes more pronounced with larger models, indicating a higher propensity for larger models to memorize sensitive information, which our method effectively mitigates. Second, while the KUMPR approach significantly forgets sensitive information, it also impairs the model's normal ability (e.g, PPL). In contrast, our ICU approach preserves the model's essential linguistic capabilities while erasing sensitive information, underscoring the necessity of our methods. The full results of each sample can be found in Appendix A."}, {"title": "4.6 Ablation Study", "content": "As discussed in Section 3.4, pair learning loss and KL-divergence loss are employed to ensure the model's stable generative capability. To assess the impact of these losses, Figure 3 presents the performance after removing them respectively. The results indicate that both losses enhance learning performance, affirming their role in preserving the model's original generative capacity. Furthermore, the KL-divergence loss has a more significant effect, suggesting that aligning the model's output distribution with the original model is crucial for maintaining its generative capacity."}, {"title": "4.7 Parameter Sensitivity", "content": "To examine the influence of the loss hyperparameters a and \u1e9e in Section 3.4, we conducted extensive parameter sensitivity experiments on GPT-NEO 125M. The results are summarized in Table 2.\nIn general, increasing a and \u1e9e enhances learning ability while diminishing unlearning ability, with the exception of memorization accuracy (MA). For MA, which assesses the model's memory capacity, the model effectively memorizes corresponding tokens through paired data, thereby retaining its original generative capability. Meanwhile, we find that when the learning weight a is less than the"}, {"title": "5 Case Study", "content": "To provide a clearer comparison of our methods, we present a case study demonstrating the balance between learning and unlearning. As illustrated in Figure 4, the Reference includes sensitive information such as \u201c@ThingsExpo 2016\u201d. Before unlearning, the original models (e.g., GPT-NEO and OPT) retain this information and can reproduce it using the corresponding prefix. When the KUMPR method is applied, the models lose their original conversational abilities and repetitively output the word \u201cthe\u201d. Our approach, in contrast, effectively forgets the sensitive information while simultaneously learning the correct outputs from paired data. This ensures the model retains its generative capabilities, highlighting the significance of our Iterative Contrastive Unlearning framework. Other examples can be found in Appendix D."}, {"title": "6 Conclusion", "content": "In this paper, we explored the motivated direction of machine unlearning for Generative Language Models. We first analyzed the challenges in this topic, and further proposed an Iterative Contrastive Unlearning framework (ICU). Specifically, we extend the Knowledge Unlearning Induction with Contrastive Learning Enhancement, which trains the model with selected similar data. In addition, we introduce Iterative Unlearning Refinement to prevent further unlearning on discarded information, preserving the model's ability adaptively. Finally, extensive experiments of models in three different sizes demonstrate the effectiveness of our proposed method. We hope our work will lead to more future studies."}, {"title": "7 Limitation", "content": "Generative language models typically possess a considerable number of parameters, necessitating re-tuning during machine unlearning. As the volume of data to be forgotten expands, the number of iterations required for unlearning also increases, leading to a notable escalation in computational expenses. Moreover, the process of unlearning may necessitate the management of sensitive user information or personal data, potentially giving rise to privacy and security considerations. It is imperative to prioritize safeguarding user privacy and adhering to applicable laws and regulations throughout the unlearning procedure."}, {"title": "A Full Results", "content": "The full results of the individual runs are shown in Table 3 and 4. Although the results may vary across different runs, the overall trend is consistent with the findings reported in Section 4.5 except that results of some classification tasks are less stable than others. Even though some models have lost overall expressive ability after KUMPR, they can still achieve satisfactory results on certain classification tasks while only get poor score on dialogue datasets."}, {"title": "B Learning Rate", "content": "We conduct experiments with different learning rate, the results are shown in Table 5. The weights were chosen as \u03b1 = 0.5, \u03b2 = 0.5. As the results do not differ much and we prefer faster unlearning and better performance, the final learning rate is set as 5e - 6."}, {"title": "CGPT prompts", "content": "Given a prefix and a reference suffix, candidates will generate text based on the prefix. Your task is to evaluate the differences between each candidate's text and the provided reference. The candidate"}, {"title": "D Examples", "content": "In addition to the example in the Case Study section, we provide three additional examples to give readers a clearer understanding of how our method effectively implements machine unlearning to protect against extraction attacks, as shown in Figure 5."}]}