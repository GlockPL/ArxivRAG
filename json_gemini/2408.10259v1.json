{"title": "Contrastive Learning on Medical Intents for Sequential Prescription Recommendation", "authors": ["Arya Hadizadeh Moghaddam", "Mohsen Nayebi Kerdabadi", "Mei Liu", "Zijun Yao"], "abstract": "Recent advancements in sequential modeling applied to Electronic Health Records (EHR) have greatly influenced prescription recommender systems. While the recent literature on drug recommendation has shown promising performance, the study of discovering a diversity of coexisting temporal relationships at the level of medical codes over consecutive visits remains less explored. The goal of this study can be motivated from two perspectives. First, there is a need to develop a sophisticated sequential model capable of disentangling the complex relationships across sequential visits. Second, it is crucial to establish multiple and diverse health profiles for the same patient to ensure a comprehensive consideration of different medical intents in drug recommendation. To achieve this goal, we introduce Attentive Recommendation with Contrasted Intents (ARCI), a multi-level transformer-based method designed to capture the different but coexisting temporal paths across a shared sequence of visits. Specifically, we propose a novel intent-aware method with contrastive learning, that links specialized medical intents of the patients to the transformer heads for extracting distinct temporal paths associated with different health profiles. We conducted experiments on two real-world datasets for the prescription recommendation task using both ranking and classification metrics. Our results demonstrate that ARCI has outperformed the state-of-the-art prescription recommendation methods and is capable of providing interpretable insights for healthcare practitioners.", "sections": [{"title": "1 INTRODUCTION", "content": "In recent years, the abundance of Electronic Health Records (EHRs) has helped medical professionals enable a variety of data-driven applications for personalized healthcare analysis [2]. In particular, sequential prescription recommender systems have attracted a growing interest [5, 28, 34, 45], by supporting informed treatment decisions through the analysis of complex EHR data accumulated during the long history of individuals' medical encounters. Aiming at predicting the medication in the next visit, a personalized algorithm is needed to examine a multitude of medical features, including diagnosis, procedures, and prescription codes, while also capturing the intricate interactive relationships among them [39].\nA notable challenge in building the sequential prescription recommendation arises from the multi-level structure of EHR features [10, 42]. In EHR data, each patient consists of a sequence of healthcare visits, and within each visit, there exists a bag of medical codes for various events. Therefore, a distinctive challenge of learning temporal patterns in a sequence of medical visits is to address the heterogeneity of code interdependencies that happen simultaneously. For example, as shown in Figure 1, a patient's healthcare history can facilitate two series of drug prescriptions, addressing different treatment goals. As a result, we believe that a single sequence of visits in EHR can be disentangled into more than one unique thread of prescribing considerations. We name each specialized prescribing thread a \"temporal path\" across consecutive visits to distinguish different types of temporal relationships. Recognizing a set of diverse temporal paths is crucial for modeling multiple concurrent medical conditions that occur in the same patient. However, existing sequential recommender systems [24, 35], such as the modeling of movie preferences, mostly study a homogeneous temporal relationship, where each \u201cvisit\u201d only contains a single item at a time, making their capacity less suitable for the multi-level structure of EHR features.\nBy extracting multiple dependencies over consecutive visits at the patient level, we propose to characterize temporal paths and"}, {"title": "2 METHODOLOGY", "content": "2.1 Problem Formulation\nIn the EHR data, each patient is represented as a time sequence of visits, and in each visit, various prescribed medications are recorded. Formally, for a patient p, there is a sequence of visit sorted by time from the earliest to the latest XP = (X1, X2, ..., xy), where T represents his/her particular number of total visits\u00b9. In the sequence of visits, each visit xt is represented by (m, m, . . ., m\u2081\u2084), a dynamic\nset of medication where t indicates the specific visit and Lt shows the total number of medication prescribed at visit t.\nTask: Given the medication of sequential visits from x\u2081 to XT belonging to a patient, the goal of recommendation model is to predict a distinct set of medication for the next-visit at T + 1, denotes <m1+1, 2+1,..., m1+1), haven't been prescribed in earlier visits.\nMT+1\nm\n2.2 Methodology Overview\nAs shown in Figure 2, the proposed method consists of (1) Temporal Paths Representation, utilizing multi-level self-attention modules to generate temporal paths for enhanced embedding learning. (2) Contrasted Intents, diversifying temporal paths on the same patients by linking them with different medical intents in a contrastive learning framework. (3) Aggregation and Prediction, employing a visit-instance attention mechanism to prioritize visits for the final prescription prediction.\n2.3 Temporal Paths Representation\nIn this study, we adopt a multi-level transformer-based module specifically tailored for intra- and inter-visit representation.\n2.3.1 Visit-Level Transformer\nThe self-attention of Transformer plays an important role in capturing dependencies among healthcare features [10, 41], and we utilize it for dependency learning between medication codes. Firstly, the sparse input of the prescriptions is converted to a dense representation with an embedding layer in Equation 1,\nm = Embed(m) (1)\nwhere mit \u2208 Rd is the embedding of prescription i at time step t, d is the embedding size, and Pt = (m, m, \u2026\u2026\u2026, m\u2081) \u2208 RBxLBxd, where Pt is the embeddings of prescription codes inside a visit at time t, B is the batch size, LB is the maximum number of prescription codes within visits in the batch, and d is the embedding size.\n\u00b2"}, {"title": "2.3.2 Cross-Visit Transformer", "content": "While the aforementioned approach enhances the prescription embedding within each single visit, it cannot fully address the cross-visit dependency learning between the codes in the consecutive time steps. Recognizing the significance of these dependencies is crucial for monitoring prescription changes over time and capturing the progression of specific conditions. Consequently, we propose a novel attention-based approach that is able to generate temporal paths over prescriptions in consecutive time steps, enabling the sharing of information between medical codes at consecutive time t and t + 1.\nSelf-attention generates three matrices, known as Query, Key, and Value. The first two matrices, Query and Key, are employed to construct the attention relationships between two information entities. In our study, to capture temporal paths by extracting dependencies between consecutive time steps, we adopt a new Transformer encoder where the Query is obtained from time t, while the Key is obtained from t + 1 to reflect the effect of the outgoing attention and dependency from medications at t to medications at t + 1. In the formulation, the output of the visit-level code embeddings E through each head j is initially concatenated through [E, ..., E] \u2208 RBxLBxd, then the matrices Q\u2081, Kt, and \u0176t are derived using Equations 3, 4, and 5, where \u0174q, \u0174k, and \u0174\u300f \u2208 Rd\u00d7d are learnable parameters. For the self-attention operation, the attention heads are produced as K, Q, and V\nQ\u2081 = [E, ..., E]Wq (3)\n\u040c\u2081 = [E, ..., E]Wk (4)\nVt = [E,...,E]W (5)\nWe aim to find the attention between the prescriptions of time step t and t + 1, as shown in Equation 6, where Att +1 \u2208 RBXLB\u00d7LB represents the cross-visit attention between consecutive in head j. The masking matrix M is based on the available prescription codes for the consecutive time steps as rows are associated with medications at t + 1 and columns at t. The transpose function inside the Att 41 corresponds to the attention directed towards the prescriptions at time step t + 1, and the sum of incoming attention for each prescription equals 1.\nAtt+1 = softmax((\nQ(K+1) +M)T) (6)\nFollowing the generation of temporal paths, the embedding of the prescriptions at time step t is shared to t + 1 based on their cross-visit attention. Mathematically, based on Equation 7 the temporal embedding is extracted, where e \u2208 RBxLBxd/J This represents the output embedding of prescriptions at t + 1, which is extracted from the embeddings and corresponding attentions for prescriptions at t. The Cross-Visit Transformer embedding at head j related to temporal paths for all time steps equals to \u00eaj = (\u00ea, \u00ea,..., \u00ea) \u2208 RB\u00d7T\u00d7LB\u00d7d/J.\nt+1 = Att(7)\nNotably, each head j generates a new set of temporal embeddings between the visits. These embeddings are subsequently used for medical intents in the contrastive learning approach (Section 2.4).\nThe extracted temporal path representation \u00ea, is then concatenated with visit-level representation \u0176+1, to include both inter and intra-visit code dependencies. Following this concatenation, a"}, {"title": "2.4 Contrasted Intents", "content": "We extracted multiple representations from attention heads in both Transformer encoders, capturing visit-level and cross-visit relationship embeddings shared among medications. Although this multi-head approach is capable of generating different temporal paths, a challenge still remains that the temporal paths associated with different attention heads are not necessarily distinct therefore may fail to represent a comprehensive range of medical intents. As shown in Figure 1, the purposes of prescribing a series of drugs are different, and each of them is associated with a specific intent. To incorporate this concept, we link each Transformer head to a specific medical intent, ensuring that multiple heads represent different considerations of a patient's health condition. Consequently, temporal paths become more medically meaningful. To achieve this, these intents and Transformer heads need to be distinct from each other to cover all the possible health profiles. Furthermore, non-distinct heads can also lead to overlapping embedding spaces, resulting in suboptimal model performance [21].\nBased on this motivation, a form of regularization is needed to address two key challenges: (1) ensuring the heads are distinct so the model covers the comprehensive medical aspects (such as diseases related to the heart and kidney as two different organ systems) of a complex health history, and (2) maintaining the semantics in representation, where each head is associated with a specialized medical aspect, utilizing multiple medical intents to identify medications for the recommendation.\nInspired by the application of contrastive learning in intent- aware recommender systems [8, 46] and the generalization advantage of having distinct heads [44], we propose a novel approach for intent-aware prescription recommendation systems. First, we need to formulate the intents that can be extracted from a patient's entire visits. Our approach proposes to learn the intents in a computational way. Since each head is set to represent a unique intent, the number of intents equals to the number of heads. We will define J linear layer blocks, each outputs a general embedding based on the patient profile as an intent. The input of these linear layers is \u0154t \u2208 RB\u00d7d which is the overall embedding obtained by summing up all the code embeddings in Pt. The output, as shown in Equation 9, are intent representations where j \u2208 {1, 2, ..., J}, and W \u2208 Rdxd/J. Every intent from all time steps for a single patient can be represented as Intent = (Intent, Intent, ..., Intent) \u2208 RB\u00d7T\u00d7d/J.\nIntent = PW(9)\nVisit-Level and Cross-Visit Transformers share the same head index to represent the same intents that will be distinct from others. We employ a contrastive learning [7] framework. First, the prescription codes embedding for E and \u00ea are summed up over individual codes respectively, so that Oj and \u1ed4 \u2208 RB\u00d7T\u00d7d/J are obtained. Second, the embeddings are transformed using a linear layer as indicated in Equations 10, 11, and 12, where WCL \u2208 RdT/J\u00d7d/J, and Mp is the mask to address variable number of visits.\nZ = (Intent + Mp)WCL(10)\nZ = (O + MP)WCL(11)\nZ = (\u00d4 +Mp)WCL(12)\nFor contrastive loss, each intent should be similar to a distinct transformer's head while being contrasted with other heads. Consequently, positive pairs are the intents and heads with the same head index j, while the heads and intents with different j are considered negative pairs. Hence, the contrastive loss is computed according to Equations 13, 14, and 15 for both Transformers.\nLCL = -log +\u03a3Ii(13)\n\u03c4 e\nPositive Pair\nk\u2260i\nLCL = -log (14)\ne\nPositive Pair\n+\u03a3Ii\nk\u2260i"}, {"title": "2.5 Aggregation and Prediction", "content": "The multi-level Transformers and intents enhance embeddings by integrating temporal and visit-level information, considering various medical aspects. Although these embeddings extract inter and intra-visit dependency among prescriptions, they do not explicitly identify which visit is more important compared to others. To identify the most crucial visits for both intents and patient embeddings, we leverage an interpretable visit-instance attention mechanism [9]. Firstly, the matrices \u00ca, are concatenated based on their heads, and the representation of the codes are summed up to obtain \u00cat \u2208 RBxd. Subsequently, \u00cat is fed into a GRU layer, indicated in Equation 16. Following the GRU layer, attention is calculated, and the final embedding is obtained using Equations 17, 18, and 19.\na1, a2, ..., at = GRU1 (\u00ca\u2081, \u00ca2, ..., \u00caT)(16)\nei = ai Wa(17)\n\u03b11, \u03b12, ., a\u2081 = Softmax (e1, e2, ..., ei)(18)\n(19)\nA similar process is applied for intents shown in Equations 20, 21, 22, and 23.\nb1,b2, ..., br = GRU (Intent1, Intent2,..., Intent)(20)\nqi = biWb(21)\n\u03b21, \u03b22,..., \u03b2\u2081 = Softmax (q1, q2,..., qi)(22)\nY1 = \u03a3\u03b2i(23)\nFinally, the method predicts the output using Equation 24, where WIP \u2208 R2d\u00d7d and Woutput \u2208 Rd\u00d7H. H is the total number of drugs that are available.\n(24)"}, {"title": "2.6 Loss Function", "content": "The proposed method aims to predict the last visit's prescriptions based on the previous sequence, making it a multi-label classification task. In the training process, two types of loss functions are employed. Firstly, as indicated in Equation 25, Binary Cross Entropy (BCE) is used to predict the probability of each drug,\n(25)\nwhere y corresponds to the true label, o refers to the Sigmoid function, and \u0177(i) refers to the prediction score for ith drug.\nFurthermore, for enhanced result robustness and to maintain a one-margin superiority of truth labels over others, we employ the Multi-Label Hinge Loss, as identified in Equation 26.\n(26)\nMoreover, the contrastive loss L\u2081 calculated in Section 2.4 is added to LBCE and Lmulti. Therefore, the final loss is presented in Equation 27, where the constants y adapt the contrastive loss's influence and A is for the multi-label hinge loss. In optimization, the Adam optimizer [16] is employed.\n(27)"}, {"title": "3 EVALUATION", "content": "3.1 Datasets\nIn this research, two real-world datasets are utilized for the prescription recommendation task:\n\u2022 MIMIC-III: MIMIC-III [13] is an open-access database that includes health-related data linked to more than 40,000 patients who were admitted to critical care unit from 2001 to 2012. In this study, we concentrate on patients with more than one visit to suggest prescriptions for their latest visit. Our training approach involves using data from their maximum four most recent visits as input for the model, determined through experimentation.\n\u2022 AKI: The dataset on Acute Kidney Injury (AKI) [18] contains healthcare data at the University of Kansas Medical Center (KUMC) from 2009 to 2021, and consists of over 135,000 hospitalized patients with the risk of having AKI. We include the three most recent visits for each patient where the two earlier visits are the input and the last one is the target visit for prescription recommendation.\n3.2 Experimental Setup\nIn this study, we mainly focus on recommending the prescription of the next visit (information up to visit at time T to predict target visit at T + 1) which holds great utility for medical professionals. Therefore, we filtered out patients with only 1 visit. Generally, the AKI dataset encompasses a broader patient population by including non-ICU hospital admissions, therefore containing a significantly larger cohort.\nTo reduce noise in our dataset, we employ Anatomical Therapeutic Chemical (ATC) level 3 categorization for prescriptions, and our approach involves considering only those prescriptions that are non-repetitive across all visits. Repetitive prescriptions are often necessary for chronic conditions like heart disease or diabetes, where patients may require medication for years or even decades. While these medications are proven to be effective and safe, continually recommending the same treatment at every visit doesn't pose a significant challenge. Additionally, focusing solely on repetitive medication training may prioritize commonly used daily medications like painkillers and vitamins over less frequently prescribed but equally important drugs. This could limit the model's ability to understand sophisticated prescription patterns. Hence, we remove repetitive prescriptions in our main experiments. Although our experiments primarily focused on non-repetitive prescriptions,"}, {"title": "3.3 Evaluation Metrics", "content": "In this research, we utilize both well-known classification and ranking metrics to evaluate ARCI comprehensively. For classification purposes, we employ PRAUC, F1, and Jaccard, metrics. To evaluate drug safety, we employ metrics based on drug-drug interaction (DDI) rates, and for ranking, we use Hit@K and NDCG@K with K being the rank. In the following, the formulations are presented:\nJaccard measures the similarity between two sets by calculating the ratio of the intersection to the union of ground truth, indicated by equation 28.\n(28)\nwhere, N equals to the total number of patients, \u00cep denotes the number of visits to predict for patient p, y(P) to ground truth medi- cations for patient p at time step t, and \u0176(P) = {\u0177(p) > 0} where 0 denotes a threshold, chosen through a greedy search approach from values from 0 to 1 with intervals of 0.1, relying on the performance metrics evaluated on the test dataset.\nF1 score is derived from Precision and Recall, where Precision = Ytly(p) n\u0176(P) [/ly(p) Yt YtYtYtYtYtYt YtYtYtYt YtYt Yt n n (29)\nPRAUC quantifies the performance of a classification model by measuring the area under the precision-recall curve. The formulation of PRAUC is indicated in equation 30, wherein PRAUC(P) =\n(30)\nDDI rate measures medication safety by calculating the rate at which predicted prescriptions interact with each other [26], indicated in equation 31, where u\u00a1 equals to the ith prescriptions prediction output, and Gd is the DDI graph.\nDDI (31)\nWe also utilize the ranking evaluation to discuss the method's performance in different ranks, and how the model is reliable. Notably, for all of the metrics if the specific rank is k we only consider inputs with target visits that have more than k - 1 prescriptions.\nHit@k equals the hit rate of top k ranked prescriptions which all of them appear in the ground truth drugs, as indicated in equation 32 where Ranked (\u0177(P), k) equals to top k ranked prescription based on their prediction score.\n(32)\nNDCG@k is a commonly used metric for ranking the performance of recommendation systems. The formulation is shown in equation 33, where in DCG(P) @k = 1 Rank(o(\u0177(P)), k)i/log2(i + 1), IDCG(P) @k = 1 Rank(y(p), k)i/log2(i + 1) and Rank(y(P), k) equals to score of the prescription at rank k.\n(33)"}, {"title": "3.4 Baselines", "content": "We utilize the state-of-the-art healthcare predictive methods and prescription recommendation systems as baselines: (1) Dr. Agent [11] utilizes RNN with dual policy gradient agents and a dynamic skip connection for adaptive focus on pertinent information. (2) Retain [9] incorporates a dual-RNN network to capture the interpretable influence of the visits and medical features for the prediction tasks. (3) Transformer's Encoder [30], our research utilizes the transformer's self-attention for both Visit-Level and Cross-Visit Transformers representations and it is one of the most important baselines to compare our method with. (4) Micron [37] a drug recommendation model using residual recurrent neural networks to update changes in patient health (5) SafeDrug [39] utilizes a global message passing neural network to encode the functionality of prescriptions molecules with considering the DDI to recommend safe prescriptions (6) MoleRec [40] a structure-aware encoding method that contains hierarchical architecture aimed at modeling interactions with considering DDI. (7) GAMENet [26] employs a memory module and graph neural networks to incorporate the"}, {"title": "3.5 Results and Discussion", "content": "We conducted experiments on ARCI to address the following research questions:\n\u2022 RQ1 What is the performance of ARCI compared to other state- of-the-art healthcare predictive methods and prescription recom- mendation systems?\n\u2022 RQ2 How differently do various components of ARCI contribute to the output results?\n\u2022 RQ3 How does the ARCI perform in the recommendation task with accounting for repetitive prescriptions?\n\u2022 RQ4 How does the proposed method take into account inter- pretability?\n3.5.1 RQ1. Performance Comparison with Baselines: Tables 2 and 3 provides the comparison between baselines and ARCI based on the"}, {"title": "3.5.2 RQ2. Ablation Studies", "content": "In the ablation studies, we investigate the impact of omitting each submodule on the overall performance. We include the Transformer's encoder in the ablation analysis, as ARCI is built upon the Transformer architecture. ARCI has two main principal contributions: (1) Extracting temporal paths using the Cross-Visit Transformer, and (2) proposing the Contrasted Intent module for linking temporal paths and dependencies with distinct health profiles and capturing diverse embeddings. Results in Tables 2, 3, and 4 indicate adding Cross-Visit Transformer (ARCI w/o intent) enhances Transformer's performance, showing the effectiveness of temporal paths compared to treating features as a bag-of-codes. Another observation is adding linear layers (ARCI w intent w/o L\u2081) as intents without any contrastive loss is beneficial, showing that incorporating general patient health information, even without any regularization, has a positive impact. Finally, based on the results and margins we can assume that L\u2081 is effective in achieving more general embeddings, a conclusion supported by Figure 4. In this figure, the head representations from both datasets are considerably clustered into distinct groups. Additionally, the mean of the intents' representations is close to their associated groups different from each other, showing the influence of L1 in generalization. Furthermore, we explored the impact of varying the number of heads in the output results extracted from MIMIC-III. As depicted in Figure 5, the maximum effect of LI occurs when J = 4 for MIMIC and J = 6 for AKI because the optimal number of intents depends on the variability of medical conditions, linked to the number of patients which suggests that AKI has a higher variability in AKI with a higher J value. For ARCI, we combined three different loss functions for evaluation, and the impact of the contrastive loss has been examined. In Figure 6, we evaluate the effects of the BCE and Hinge losses. As shown in the figure, BCE is the most effective for the multi-label classification task, while the Hinge loss provides a smaller performance improvement.\n3.5.3 RQ3. Including Repetitive Prescriptions: To have more consistency with the previous model's results in the literature as they include repetitive prescriptions and to have a comprehensive assessment, we extended our experiments to include repetitive prescriptions. As shown in Table 5, ARCI has outperformed the baselines for both datasets across various classification metrics. Specifically, the best-performing baseline is the MoleRec model for the MIMIC-III and AKI datasets, which differs from the results presented in"}, {"title": "3.5.4 RQ4. Interpretability", "content": "The Cross-Visit Transformer employs an attention matrix to capture temporal paths between two consecutive time steps. In addition to cross-visit attention, the aggregation and prediction layer contains an interpretable visit attention mechanism through recurrent neural networks, showing the influence of each visit on the output prediction. To demonstrate the interpretability of the proposed method, we select one patient from the MIMIC-III dataset and illustrate the attention values for four different intents. As depicted in Figure 7, each prescription is connected to another one with a temporal path, and the intensity of the colors reflects the connections' strength.\nIn the first intent, the sequence of dependencies A04A \u2192 P01A \u2192 R05C \u2192 R02A is one of the influential relations. According to the medical literature, A04A (antiemetics) is associated with P01A (Agents against protozoal diseases) [3], and R05C (cough suppressants) correlates with R02A (throat preparations)[27]. In addition to temporal paths, the visit-instance attention mechanism highlights the significance of the first two visits compared to the latter ones due to The strong correlation between N05A (antipsychotics) N05C (hypnotics and sedatives) [6], as well as B03A (iron preparations) with B05B (I.V. solutions) [4]. For the second intent, one of the important dependencies is A04A \u2192 B03A \u2192 B02B \u2192 M01A, and previous studies support associations between B03A (iron preparations) and M01A (antiinflammatory and antirheumatic) [12]."}, {"title": "4 RELATED WORK", "content": "Medication Recommendation: Since past years with the emergence of Deep Learning models [1, 22], numerous longitudinal methodologies have been developed to improve the embeddings derived from Electronic Health Records (EHR) for various applications, including prescription recommendation systems. Noteworthy early works include Dr. Agent [11] wherein an RNN network employs a policy gradient for adaptive learning, and RETAIN [9] which contains a dual-RNN network to embed multi-level attention mechanism for both medical codes and visits. The main drawback of these early methodologies is that they were not explicitly developed for the prescription recommendation. Enhancing safety and optimizing integration is achievable through a comprehensive understanding of drug-drug interactions (DDI). GAMENet addresses this issue [26], which uses a graph-based network to employ DDI as a loss function and evaluation metric. Other approaches like SafeDrug [39] focus on global message passing for recommending prescriptions based on DDI and molecular structures. MICRON [38] uses a residual neural network to update patient health histories and MoleRec [40] focuses on safety and performance improvement in drug recommendations by considering molecular structures.\nUser Intent and Sequential Recommendation: Sequential recommender systems utilize the user's past behavior sequence in the recommendation process [14, 31]. Earlier approaches often modeled temporal transformations using Markov Chains. For instance, FPMC [25] combined Markov Chains and Matrix Factorization to recommend items based on user interests. With the advent of Deep Learning, a new generation of sequential recommenders has emerged. For example, ASReP [19] contributes to the data sparsity problem by using a pre-trained Transformer-based method to augment the short sequences. In addition to sequential recommendation systems, intent-aware systems have been developed to capture users' multiple intentions for enhanced recommendations [20, 33]. For instance, Atten-Mixer [43] considers both concept-view and instance-view for items using the Transformer architecture, and ASLI [29] extracts users' multiple intents using a temporal convolutional network for predicting the next item in a sequence. Additionally, contrastive learning has been utilized for distinct intent learning in user preferences [32]. For instance, ICL [8] employs a latent variable that represents the distribution of users and their intent through clustering and contrastive learning."}, {"title": "5 CONCLUSION", "content": "This paper presents Attentive Recommendation with Contrasted Intents (ARCI), a prescription recommendation system with two primary contributions. Firstly, we propose a multi-level transformer- based model designed to extract visit-level and cross-visit dependencies to formulate temporal paths between medical codes. Second, we introduced a novel contrastive learning-based approach for transformers' heads linked to different intents as specialized health profiles to handle a variety of dependency types and achieve more comprehensive embedding learning. ARCI outperforms the state- of-the-art healthcare models for prescription recommendations on two real-world datasets, while providing interpretable insights into the decision-making process of clinical practitioners."}]}