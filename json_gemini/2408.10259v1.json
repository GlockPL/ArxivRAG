{"title": "Contrastive Learning on Medical Intents for Sequential Prescription Recommendation", "authors": ["Arya Hadizadeh Moghaddam", "Mei Liu", "Mohsen Nayebi Kerdabadi", "Zijun Yao"], "abstract": "Recent advancements in sequential modeling applied to Electronic Health Records (EHR) have greatly influenced prescription recommender systems. While the recent literature on drug recommendation has shown promising performance, the study of discovering a diversity of coexisting temporal relationships at the level of medical codes over consecutive visits remains less explored. The goal of this study can be motivated from two perspectives. First, there is a need to develop a sophisticated sequential model capable of disentangling the complex relationships across sequential visits. Second, it is crucial to establish multiple and diverse health profiles for the same patient to ensure a comprehensive consideration of different medical intents in drug recommendation. To achieve this goal, we introduce Attentive Recommendation with Contrasted Intents (ARCI), a multi-level transformer-based method designed to capture the different but coexisting temporal paths across a shared sequence of visits. Specifically, we propose a novel intent-aware method with contrastive learning, that links specialized medical intents of the patients to the transformer heads for extracting distinct temporal paths associated with different health profiles. We conducted experiments on two real-world datasets for the prescription recommendation task using both ranking and classification metrics. Our results demonstrate that ARCI has outperformed the state-of-the-art prescription recommendation methods and is capable of providing interpretable insights for healthcare practitioners.", "sections": [{"title": "1 INTRODUCTION", "content": "In recent years, the abundance of Electronic Health Records (EHRs) has helped medical professionals enable a variety of data-driven applications for personalized healthcare analysis [2]. In particular, sequential prescription recommender systems have attracted a growing interest [5, 28, 34, 45], by supporting informed treatment decisions through the analysis of complex EHR data accumulated during the long history of individuals' medical encounters. Aiming at predicting the medication in the next visit, a personalized algorithm is needed to examine a multitude of medical features, including diagnosis, procedures, and prescription codes, while also capturing the intricate interactive relationships among them [39].\nA notable challenge in building the sequential prescription recommendation arises from the multi-level structure of EHR features [10, 42]. In EHR data, each patient consists of a sequence of healthcare visits, and within each visit, there exists a bag of medical codes for various events. Therefore, a distinctive challenge of learning temporal patterns in a sequence of medical visits is to address the heterogeneity of code interdependencies that happen simultaneously. For example, as shown in Figure 1, a patient's healthcare history can facilitate two series of drug prescriptions, addressing different treatment goals. As a result, we believe that a single sequence of visits in EHR can be disentangled into more than one unique thread of prescribing considerations. We name each specialized prescribing thread a \"temporal path\" across consecutive visits to distinguish different types of temporal relationships. Recognizing a set of diverse temporal paths is crucial for modeling multiple concurrent medical conditions that occur in the same patient. However, existing sequential recommender systems [24, 35], such as the modeling of movie preferences, mostly study a homogeneous temporal relationship, where each \u201cvisit\u201d only contains a single item at a time, making their capacity less suitable for the multi-level structure of EHR features.\nBy extracting multiple dependencies over consecutive visits at the patient level, we propose to characterize temporal paths and link them to a set of distinct medical profiles (e.g., \"influenza\" and \"ADHD\" in Figure 1). This specific design of having multiple distinct profiles in the same patient echoes a concept presented in the recent literature of recommender systems, known as \"Intent-Aware\" recommendation, which is capable of capturing diverse profiles of user preference simultaneously [8, 17, 43]. For example in e-commerce recommendation, a successful intent-aware algorithm can capture a user's interest in both technology and car equipment, and by analyzing both specific interests, the system generates overall recommendations that are aligned with the user's preferences in both areas. The concept of intent in recommender systems can also be applied to EHR systems for more generalized temporal relationship learning. However, it remains a less explored research direction in prescription recommendation. A significant challenge in using intents for prescription recommendations is to ensure that each intent is able to represent a separate temporal path, and it can be clearly distinguished from other intents to represent a specialized medical profile. To achieve this goal, developing a contrastive learning [7, 15] framework on such medical intents constitutes a feasible approach. Contrastive learning is generally achieved by training an objective of contrasting positive pairs (similar samples) and negative pairs (dissimilar samples) to learn a discriminative representation. In our case, by viewing every medical intent as an anchor, the framework will encourage the recommendation models to bring the extracted temporal paths closer to the corresponding intents, meanwhile, to push each medical intent apart from the other intents in the embedding space. In this way, every intent will be maximally diverse from each other, and the specialized temporal patterns will be extracted.\nTo this end, in this paper, we introduce Attentive Recommendation with Contrasted Intents (ARCI), a personalized multi-level transformer-based approach designed to model simultaneous temporal paths over consecutive visits, and extract distinct medical intents for facilitating prescription recommendation with the following contributions:\n\u2022 First, we introduce a multi-level transformer-based approach designed to extract multiple temporal paths associated with distinct medical intents, which captures both inter-visit and intra-visit medication dependencies for visit-level and cross-visit EHR representation.\n\u2022 Second, we propose a novel contrastive learning approach to extract diverse medical intents for regularizing the multiple temporal paths of the same patients. Each intent is linked to a dedicated transformer attention-head offering two advantages: (1) each intent and corresponding temporal path represent a specialized type of patient profile; (2) all the intents will be distinct from each other and work collectively to achieve a comprehensive patient representation.\n\u2022 Last, we validate ARCI on MIMIC-III [13] and Acute Kidney Injury (AKI) [18] datasets with comprehensive ranking and classification metrics on the quality of recommendations. The experimental results illustrate that the proposed method outperforms state-of-the-art recommendation approaches. Furthermore, we assess the interpretability of the proposed method to provide meaningful clinical insights for practitioners."}, {"title": "2 METHODOLOGY", "content": "In the EHR data, each patient is represented as a time sequence of visits, and in each visit, various prescribed medications are recorded. Formally, for a patient p, there is a sequence of visit sorted by time from the earliest to the latest \\(X^p = (X_1, X_2, ..., X_T)\\), where T represents his/her particular number of total visits\u00b9. In the sequence of visits, each visit \\(x_t\\) is represented by \\((m_1^t, m_2^t, ..., m_{L_t}^t)\\), a dynamic set of medication where t indicates the specific visit and Lt shows the total number of medication prescribed at visit t.\nTask: Given the medication of sequential visits from \\(x_1\\) to \\(x_T\\) belonging to a patient, the goal of recommendation model is to predict a distinct set of medication for the next-visit at T + 1, denotes \\(<m_1^{T+1}, m_2^{T+1}, ..., m_{M_{T+1}}^{T+1}>\\), haven't been prescribed in earlier visits."}, {"title": "2.2 Methodology Overview", "content": "As shown in Figure 2, the proposed method consists of (1) Temporal Paths Representation, utilizing multi-level self-attention modules to generate temporal paths for enhanced embedding learning. (2) Contrasted Intents, diversifying temporal paths on the same patients by linking them with different medical intents in a contrastive learning framework. (3) Aggregation and Prediction, employing a visit-instance attention mechanism to prioritize visits for the final prescription prediction."}, {"title": "2.3 Temporal Paths Representation", "content": "In this study, we adopt a multi-level transformer-based module specifically tailored for intra- and inter-visit representation."}, {"title": "2.3.1 Visit-Level Transformer", "content": "The self-attention of Transformer plays an important role in capturing dependencies among healthcare features [10, 41], and we utilize it for dependency learning between medication codes. Firstly, the sparse input of the prescriptions is converted to a dense representation with an embedding layer in Equation 1,\n\\(m_i^t = Embed(m_i)\\)\nwhere \\(m_i^t \\in \\mathbb{R}^d\\) is the embedding of prescription i at time step t, d is the embedding size, and \\(P_t = (m_1^t, m_2^t, \\dots, m_{L_B}^t) \\in \\mathbb{R}^{B \\times L_B \\times d}\\), where Pt is the embeddings of prescription codes inside a visit at time t, B is the batch size, LB is the maximum number of prescription codes within visits in the batch, and d is the embedding size.\nNext, the embeddings are fed into the self-attention encoder [30], which focuses on extracting dependencies between prescription codes within the same visits as shown in Equation 2, where \\(E_t \\in \\mathbb{R}^{B \\times L_B \\times d/J}\\) represents the visit-level embedding at time step t and in j-th head where J equals to total number of heads.\n\\(E_t^j = SelfAtt_j(P_t)\\)\nAdditionally, the sequence of the embeddings for a single patient for a specific head is \\(E^j = (E_1^j, E_2^j, ..., E_T^j) \\in \\mathbb{R}^{B \\times T \\times L_B \\times d/J}\\), where T is the total number of time steps."}, {"title": "2.3.2 Cross-Visit Transformer", "content": "While the aforementioned approach enhances the prescription embedding within each single visit, it cannot fully address the cross-visit dependency learning between the codes in the consecutive time steps. Recognizing the significance of these dependencies is crucial for monitoring prescription changes over time and capturing the progression of specific conditions. Consequently, we propose a novel attention-based approach that is able to generate temporal paths over prescriptions in consecutive time steps, enabling the sharing of information between medical codes at consecutive time t and t + 1.\nSelf-attention generates three matrices, known as Query, Key, and Value. The first two matrices, Query and Key, are employed to construct the attention relationships between two information entities. In our study, to capture temporal paths by extracting dependencies between consecutive time steps, we adopt a new Transformer encoder where the Query is obtained from time t, while the Key is obtained from t + 1 to reflect the effect of the outgoing attention and dependency from medications at t to medications at t + 1. In the formulation, the output of the visit-level code embeddings \\(E_t^j\\) through each head j is initially concatenated through \\([E_t^j, \\dots, E_t^j] \\in \\mathbb{R}^{B \\times L_B \\times d}\\), then the matrices \\(Q_t\\), \\(K_t\\), and \\(V_t\\) are derived using Equations 3, 4, and 5, where \\(W_q, W_k\\), and \\(W_v \\in \\mathbb{R}^{d \\times d}\\) are learnable parameters. For the self-attention operation, the attention heads are produced as \\(K_t^j, Q_t^j\\), and \\(V_t^j\\)\n\\(Q_t = [E_t^j, \\dots, E_t^j] W_q\\)\n\\(K_t = [E_t^j, \\dots, E_t^j] W_k\\)\n\\(V_t = [E_t^j, \\dots, E_t^j] W_v\\)\nWe aim to find the attention between the prescriptions of time step t and t + 1, as shown in Equation 6, where \\(Att_{t+1}^j \\in \\mathbb{R}^{B \\times L_B \\times L_B}\\) represents the cross-visit attention between consecutive in head j. The masking matrix M is based on the available prescription codes for the consecutive time steps as rows are associated with medications at t + 1 and columns at t. The transpose function inside the \\(Att_{t+1}^j\\) corresponds to the attention directed towards the prescriptions at time step t + 1, and the sum of incoming attention for each prescription equals 1.\n\\(Att_{t+1}^j = softmax((\\frac{Q_t (K_{t+1}^j)^T}{\\sqrt{d/J}} + M)^T)\\)\nFollowing the generation of temporal paths, the embedding of the prescriptions at time step t is shared to t + 1 based on their cross-visit attention. Mathematically, based on Equation 7 the temporal embedding is extracted, where \\(e_{t+1}^j \\in \\mathbb{R}^{B \\times L_B \\times d/J}\\). This represents the output embedding of prescriptions at t + 1, which is extracted from the embeddings and corresponding attentions for prescriptions at t. The Cross-Visit Transformer embedding at head j related to temporal paths for all time steps equals to \\(\\hat{e}^j = (\\hat{e}_1^j, \\hat{e}_2^j, \\dots, \\hat{e}_T^j) \\in \\mathbb{R}^{B \\times T \\times L_B \\times d/J}\\).\n\\(e_{t+1}^j = Att_{t+1}^j E_t^j\\)\nNotably, each head j generates a new set of temporal embeddings between the visits. These embeddings are subsequently used for medical intents in the contrastive learning approach (Section 2.4).\nThe extracted temporal path representation \\(\\hat{e}_{t+1}^j\\) is then concatenated with visit-level representation \\(E_{t+1}^j\\), to include both inter and intra-visit code dependencies. Following this concatenation, a linear layer is applied to derive the cross-visit embedding at t + 1, as indicated in Equation 8, where \\(W_T \\in \\mathbb{R}^{(2d/J) \\times (d/J)}\\).\n\\(E_{t+1}^j = [E_{t+1}^j \\| \\hat{e}_{t+1}^j] W_T\\)\nFor further clarification, as shown in Figure 3, the attention matrix depicted in the figure is based on Equation 6, where the Query is from time step t and the Key from time step t+1, originating from the Cross-Visit Transformer architecture (Equations 3, 4, and 5). Subsequently, as illustrated in the central part of the figure, the temporal embedding for each prescription at time step t + 1 is extracted (Equation 7), and these embeddings are then concatenated with the Value matrix from the Visit-Level Transformer output (denoted by the green box) to obtain the final embedding for prescriptions at t + 1. (Equation 8). The attention matrix and output of the Cross-Visit Transformer represent the temporal paths between inter-visit prescriptions based on consecutive visits.\nThe embedding from all time steps for a batch of patients can be represented as \\(\\hat{E}^j = (\\hat{E}_1^j, \\hat{E}_2^j, \\dots, \\hat{E}_T^j) \\in \\mathbb{R}^{B \\times T \\times L_B \\times d/J}\\). For t = 1, where incoming attention is absent, the Visit-Level Transformer output is utilized, and \\(\\hat{E}_1^j = E_1^j\\). Notably, with this approach, the Cross-Visit Transformer at a given time t takes into account the impact of prescription codes from t = 1 to t."}, {"title": "2.4 Contrasted Intents", "content": "We extracted multiple representations from attention heads in both Transformer encoders, capturing visit-level and cross-visit relationship embeddings shared among medications. Although this multi-head approach is capable of generating different temporal paths, a challenge still remains that the temporal paths associated with different attention heads are not necessarily distinct therefore may fail to represent a comprehensive range of medical intents. As shown in Figure 1, the purposes of prescribing a series of drugs are different, and each of them is associated with a specific intent. To incorporate this concept, we link each Transformer head to a specific medical intent, ensuring that multiple heads represent different considerations of a patient's health condition. Consequently, temporal paths become more medically meaningful. To achieve this, these intents and Transformer heads need to be distinct from each other to cover all the possible health profiles. Furthermore, non-distinct heads can also lead to overlapping embedding spaces, resulting in suboptimal model performance [21].\nBased on this motivation, a form of regularization is needed to address two key challenges: (1) ensuring the heads are distinct so the model covers the comprehensive medical aspects (such as diseases related to the heart and kidney as two different organ systems) of a complex health history, and (2) maintaining the semantics in representation, where each head is associated with a specialized medical aspect, utilizing multiple medical intents to identify medications for the recommendation.\nInspired by the application of contrastive learning in intent-aware recommender systems [8, 46] and the generalization advantage of having distinct heads [44], we propose a novel approach for intent-aware prescription recommendation systems. First, we need to formulate the intents that can be extracted from a patient's entire visits. Our approach proposes to learn the intents in a computational way. Since each head is set to represent a unique intent, the number of intents equals to the number of heads. We will define J linear layer blocks, each outputs a general embedding based on the patient profile as an intent. The input of these linear layers is \\(R_t \\in \\mathbb{R}^{B \\times d}\\) which is the overall embedding obtained by summing up all the code embeddings in Pt. The output, as shown in Equation 9, are intent representations where \\(j \\in \\{1, 2, ..., J\\}\\), and \\(W \\in \\mathbb{R}^{d \\times d/J}\\). Every intent from all time steps for a single patient can be represented as \\(Intent^j = (Intent_1^j, Intent_2^j, \\dots, Intent_T^j) \\in \\mathbb{R}^{B \\times T \\times d/J}\\).\n\\(Intent_t^j = R_t W_j\\)\nVisit-Level and Cross-Visit Transformers share the same head index to represent the same intents that will be distinct from others. We employ a contrastive learning [7] framework. First, the prescription codes embedding for \\(E_t^j\\) and \\(\\hat{e}_t^j\\) are summed up over individual codes respectively, so that \\(O_t^j\\) and \\(\\hat{O}_t^j \\in \\mathbb{R}^{B \\times T \\times d/J}\\) are obtained. Second, the embeddings are transformed using a linear layer as indicated in Equations 10, 11, and 12, where \\(W_{CL} \\in \\mathbb{R}^{d \\times T/J \\times d/J}\\), and \\(M_p\\) is the mask to address variable number of visits.\n\\(Z_t^j = (Intent_t^j + M_p) W_{CL}\\)\n\\(\\acute{Z}_t^j = (O_t^j + M_P) W_{CL}\\)\n\\(\\grave{Z}_t^j = (\\hat{O}_t^j + M_p) W_{CL}\\)\nFor contrastive loss, each intent should be similar to a distinct transformer's head while being contrasted with other heads. Consequently, positive pairs are the intents and heads with the same head index j, while the heads and intents with different j are considered negative pairs. Hence, the contrastive loss is computed according to Equations 13, 14, and 15 for both Transformers.\n\\(L_{CL}^j = -log \\frac{e^{s(Z_t^j, \\acute{Z}_t^j)/\\tau}}{\\sum_{k \\neq i} e^{s(\\acute{Z}_t^j, \\acute{Z}_t^k)/\\tau} + I_i}\\)\n\\(L_{CL}^j = -log \\frac{e^{s(Z_t^j, \\grave{Z}_t^j)/\\tau}}{\\sum_{k \\neq i} e^{s(\\acute{Z}_t^j, \\grave{Z}_t^k)/\\tau} + I_i}\\)\n\\(L_I = \\sum_{i=1}^{j=J} L_{CL}^j + L_{CL}^j\\)"}, {"title": "2.5 Aggregation and Prediction", "content": "The multi-level Transformers and intents enhance embeddings by integrating temporal and visit-level information, considering various medical aspects. Although these embeddings extract inter and intra-visit dependency among prescriptions, they do not explicitly identify which visit is more important compared to others. To identify the most crucial visits for both intents and patient embeddings, we leverage an interpretable visit-instance attention mechanism [9]. Firstly, the matrices \\(\\hat{E}_t^j\\) are concatenated based on their heads, and the representation of the codes are summed up to obtain \\(\\hat{E}_t \\in \\mathbb{R}^{B \\times d}\\). Subsequently, \\(\\hat{E}_t\\) is fed into a GRU layer, indicated in Equation 16. Following the GRU layer, attention is calculated, and the final embedding is obtained using Equations 17, 18, and 19.\n\\(a_1, a_2, ..., a_T = GRU_1(\\hat{E}_1, \\hat{E}_2, ..., \\hat{E}_T)\\)\n\\(e_i = a_i W_a\\)\n\\(\\alpha_1, \\alpha_2, ., a_T = Softmax(e_1, e_2, ..., e_i)\\)\n\\(Y_p = \\sum_{i=1}^T \\alpha_i \\hat{E}_i\\)\nA similar process is applied for intents shown in Equations 20, 21, 22, and 23.\n\\(b_1, b_2, ..., b_T = GRU(Intent_1, Intent_2, ..., Intent_J)\\)\n\\(q_i = b_i W_b\\)\n\\(\\beta_1, \\beta_2, ..., \\beta_i = Softmax (q_1, q_2, ..., q_i)\\)\n\\(Y_I = \\sum_{i=1}^i \\beta_i Intent_i\\)\nFinally, the method predicts the output using Equation 24, where \\(W_{IP} \\in \\mathbb{R}^{2d \\times d}\\) and \\(W_{output} \\in \\mathbb{R}^{d \\times H}\\). H is the total number of drugs that are available.\n\\(\\hat{y} = ([Y_p, Y_I]W_{IP})W_{output}\\)"}, {"title": "2.6 Loss Function", "content": "The proposed method aims to predict the last visit's prescriptions based on the previous sequence, making it a multi-label classification task. In the training process, two types of loss functions are employed. Firstly, as indicated in Equation 25, Binary Cross Entropy (BCE) is used to predict the probability of each drug,\n\\(L_{BCE} = -\\sum_{i=1}^{H} y^{(i)} log\\sigma(\\hat{y}^{(i)}) + (1 - y^{(i)}) log (1 - \\sigma(\\hat{y}^{(i)}))\\)\nwhere y corresponds to the true label, \\(\\sigma\\) refers to the Sigmoid function, and \\(\\hat{y}^{(i)}\\) refers to the prediction score for ith drug. Furthermore, for enhanced result robustness and to maintain a one-margin superiority of truth labels over others, we employ the Multi-Label Hinge Loss, as identified in Equation 26.\n\\(L_{multi} = \\sum_{i,j:y(i)=1,y(j)=0} max (0, 1 - ((\\hat{y}^{(i)} - (\\hat{y}^{(j)}))))\\)\nMoreover, the contrastive loss \\(L_I\\) calculated in Section 2.4 is added to \\(L_{BCE}\\) and \\(L_{multi}\\). Therefore, the final loss is presented in Equation 27, where the constants \\(\\gamma\\) adapt the contrastive loss's influence and \\(\\lambda\\) is for the multi-label hinge loss. In optimization, the Adam optimizer [16] is employed.\n\\(L = L_{BCE} + \\gamma L_I + \\lambda L_{multi}\\)"}, {"title": "3 EVALUATION", "content": "In this research, two real-world datasets are utilized for the prescription recommendation task:\n\u2022 MIMIC-III: MIMIC-III [13] is an open-access database that includes health-related data linked to more than 40,000 patients who were admitted to critical care unit from 2001 to 2012. In this study, we concentrate on patients with more than one visit to suggest prescriptions for their latest visit. Our training approach involves using data from their maximum four most recent visits as input for the model, determined through experimentation.\n\u2022 AKI: The dataset on Acute Kidney Injury (AKI) [18] contains healthcare data at the University of Kansas Medical Center (KUMC) from 2009 to 2021, and consists of over 135,000 hospitalized patients with the risk of having AKI. We include the three most recent visits for each patient where the two earlier visits are the input and the last one is the target visit for prescription recommendation."}, {"title": "3.2 Experimental Setup", "content": "In this study, we mainly focus on recommending the prescription of the next visit (information up to visit at time T to predict target visit at T + 1) which holds great utility for medical professionals. Therefore, we filtered out patients with only 1 visit. Generally, the AKI dataset encompasses a broader patient population by including non-ICU hospital admissions, therefore containing a significantly larger cohort.\nTo reduce noise in our dataset, we employ Anatomical Therapeutic Chemical (ATC) level 3 categorization for prescriptions, and our approach involves considering only those prescriptions that are non-repetitive across all visits. Repetitive prescriptions are often necessary for chronic conditions like heart disease or diabetes, where patients may require medication for years or even decades. While these medications are proven to be effective and safe, continually recommending the same treatment at every visit doesn't pose a significant challenge. Additionally, focusing solely on repetitive medication training may prioritize commonly used daily medications like painkillers and vitamins over less frequently prescribed but equally important drugs. This could limit the model's ability to understand sophisticated prescription patterns. Hence, we remove repetitive prescriptions in our main experiments. Although our experiments primarily focused on non-repetitive prescriptions, we also conducted experiments with repetitive drugs for clarity and consistency with previous studies reports (Section 3.4.3).\nFor ARCI and baselines, we use a greedy search approach to find the best hyperparameters for a comprehensive evaluation. We randomly split the data into 80% for training and 20% for testing, and we repeat this approach 7 times and report the means and confidence intervals of results. The optimal configuration for the ARCI on MIMIC-III entails J equals to 4, \\(\\tau\\) to 0.05, batch size to 128, \\(\\lambda\\) to 0.05, \\(\\gamma\\) to 0.2, and d to 256. For AKI, J set to 6, batch size to 128, \\(\\tau\\) to 0.05, \\(\\lambda\\) to 0.05, \\(\\gamma\\) to 0.1, and d to 256.\nThe statistics of the datasets are shown in Table 1. In this project, the programming language is Python, and we utilize PyTorch [23] for the method architecture. Furthermore, ARCI is compatible with PyHealth [36] framework, and we employ its baseline implementations. We release the project source code\u00b2 on GitHub."}, {"title": "3.3 Evaluation Metrics", "content": "In this research, we utilize both well-known classification and ranking metrics to evaluate ARCI comprehensively. For classification purposes, we employ PRAUC, F1, and Jaccard, metrics. To evaluate drug safety, we employ metrics based on drug-drug interaction (DDI) rates, and for ranking, we use Hit@K and NDCG@K with K being the rank. In the following, the formulations are presented:\nJaccard measures the similarity between two sets by calculating the ratio of the intersection to the union of ground truth, indicated by equation 28.\n\\(Jaccard = \\frac{1}{N} \\sum_{p=1}^{N} \\sum_{t=1}^{T_p} \\frac{|y_t^{(p)} \\cap \\acute{y}_t^{(p)}|}{|y_t^{(p)} \\cup \\acute{y}_t^{(p)}|}\\)\nwhere, N equals to the total number of patients, \\(T_p\\) denotes the number of visits to predict for patient p, \\(y_t^{(p)}\\) to ground truth medications for patient p at time step t, and \\(\\acute{y}_t^{(p)} = {\\acute{y}^{(p)} > 0}\\) where \\(0\\) denotes a threshold, chosen through a greedy search approach from values from 0 to 1 with intervals of 0.1, relying on the performance metrics evaluated on the test dataset.\nF1 score is derived from Precision and Recall, where \\(Precision = \\frac{|y_t^{(p)} \\cap \\acute{y}_t^{(p)}|}{|\\acute{y}_t^{(p)}|}\\) and \\(Recall = \\frac{|y_t^{(p)} \\cap \\acute{y}_t^{(p)}|}{|y_t^{(p)}|}\\). The final score is obtained as indicated in equation 29.\n\\(F1 = \\frac{1}{\\sum_{p=1}^{N} T_p} \\sum_{t=1}^{T_p} \\frac{2 \\times Precision_t^{(p)} \\times Recall_t^{(p)}}{Precision_t^{(p)} + Recall_t^{(p)}}\\)\nPRAUC quantifies the performance of a classification model by measuring the area under the precision-recall curve. The formulation of PRAUC is indicated in equation 30, wherein \\(PRAUC_t^{(p)} = \\sum_{l=1}^{H} Precision^{(l)}(p) ( \\sum_{l=1}^{H} Precision^{(l)}(p) Recall^{(l)}(p) \\) and \\(\\Delta Recall^{(l)} (p) = Recall^{(l)}(p) - Recall^{(l-1)} (p)\\), where l is the rank of the sequence of the re- trieved prescription.\n\\(PRAUC = \\frac{1}{N} \\sum_{p=1}^{N} \\sum_{t=1}^{T_p} PRAUC_t^{(p)}\\)\nDDI rate measures medication safety by calculating the rate at which predicted prescriptions interact with each other [26], indicated in equation 31, where \\(u_i\\) equals to the ith prescriptions prediction output, and \\(G_d\\) is the DDI graph.\n\\(DDI = \\frac{\\sum_{p=1}^{N} \\sum_{t=1}^{T_p} \\sum_{u_i \\in \\acute{y}_t^{(p)}} \\sum_{u_j \\in \\acute{y}_t^{(p)}} I\\{(u_i, u_j) \\in G_d\\}}{\\sum_{p=1}^{N} \\sum_{t=1}^{T_p} (\\sum_{u_i \\in \\acute{y}_t^{(p)}} \\sum_{u_j \\in \\acute{y}_t^{(p)}} I\\{(u_i, u_j) \\in G_d\\})}\\)\nWe also utilize the ranking evaluation to discuss the method's performance in different ranks, and how the model is reliable. Notably, for all of the metrics if the specific rank is k we only consider inputs with target visits that have more than k - 1 prescriptions. Hit@k equals the hit rate of top k ranked prescriptions which all of them appear in the ground truth drugs, as indicated in equation 32 where \\(Ranked(\\acute{y}^{(p)}, k)\\) equals to top k ranked prescription based on their prediction score.\n\\(Hit@k = \\frac{1}{N} \\sum_{p=1}^{N} \\sum_{t=1}^{T_p} \\omega_t^{(p)}; \\omega_t^{(p)} = \\begin{cases} 1: & Ranked(\\acute{y}^{(p)}, k) \\subseteq y_t^{(p)} \\\\ 0: & otherwise \\end{cases}\\)\nNDCG@k is a commonly used metric for ranking the performance of recommendation systems. The formulation is shown in equation 33, where in \\(DCG_t^{(p)}@k = \\sum_{i=1}^{Rank(\\acute{y}^{(p)}, k)} i/log_2(i + 1)\\), \\(IDCG_t^{(p)}@k = \\sum_{i=1}^{Rank(y^{(p)}, k)} i/log_2(i + 1)\\) and \\(Rank(y^{(p)}, k)\\) equals to score of the prescription at rank k.\n\\(NDCG@K = \\frac{1}{\\sum_{p=1}^{N} \\sum_{t=1}^{T_p}} \\frac{DCG_t^{(p)}@k}{IDCG_t^{(p)}@k}\\)"}, {"title": "3.4 Baselines", "content": "We utilize the state-of-the-art healthcare predictive methods and prescription recommendation systems as baselines: (1) Dr. Agent [11] utilizes RNN with dual policy gradient agents and a dynamic skip connection for adaptive focus on pertinent information. (2) Retain [9] incorporates a dual-RNN network to capture the interpretable influence of the visits and medical features for the prediction tasks. (3) Transformer's Encoder [30], our research utilizes the transformer's self-attention for both Visit-Level and Cross-Visit Transformers representations and it is one of the most important baselines to compare our method with. (4) Micron [37] a drug recommendation model using residual recurrent neural networks to update changes in patient health (5) SafeDrug [39] utilizes a global message passing neural network to encode the functionality of prescriptions molecules with considering the DDI to recommend safe prescriptions (6) MoleRec [40] a structure-aware encoding method that contains hierarchical architecture aimed at modeling interactions with considering DDI. (7) GAMENet [26] employs a memory module and graph neural networks to incorporate the"}, {"title": "3.5 Results and Discussion", "content": "We conducted experiments on ARCI to address the following research questions:\n\u2022 RQ1 What is the performance of ARCI compared to other state-of-the-art healthcare predictive methods and prescription recommendation systems?\n\u2022 RQ2 How differently do various components of ARCI contribute to the output results?\n\u2022 RQ3 How does the ARCI perform in the recommendation task with accounting for repetitive prescriptions?\n\u2022 RQ4 How does the proposed method take into account interpretability?"}, {"title": "3.5.1 RQ1. Performance Comparison with Baselines", "content": "Tables 2 and 3 provides the comparison between baselines and ARCI based on the ranking metrics for MIMIC-III and AKI datasets. The results highlight superior performance of ARCI across all metrics, particularly demonstrating an advantage over one of the best-performing"}]}