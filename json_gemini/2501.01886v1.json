{"title": "Evaluating Scenario-based Decision-making for Interactive Autonomous Driving Using Rational Criteria: A Survey", "authors": ["Zhen Tian", "Zhihao Lin", "Dezong Zhao", "Wenjing Zhao", "David Flynn", "Shuja Ansari", "Chongfeng Wei"], "abstract": "Autonomous vehicles (AVs) can significantly promote the advances in road transport mobility in terms of safety, reliability, and decarbonization. However, ensuring safety and efficiency in interactive during within dynamic and diverse environments is still a primary barrier to large-scale AV adoption. In recent years, deep reinforcement learning (DRL) has emerged as an advanced AI-based approach, enabling AVs to learn decision-making strategies adaptively from data and interactions. DRL strategies are better suited than traditional rule-based methods for handling complex, dynamic, and unpredictable driving environments due to their adaptivity. However, varying driving scenarios present distinct challenges, such as avoiding obstacles on highways and reaching specific exits at intersections, requiring different scenario-specific decision-making algorithms. Many DRL algorithms have been proposed in interactive decision-making. However, a rationale review of these DRL algorithms across various scenarios is lacking. Therefore, a comprehensive evaluation is essential to assess these algorithms from multiple perspectives, including those of vehicle users and vehicle manufacturers. This survey reviews the application of DRL algorithms in autonomous driving across typical scenarios, summarizing road features and recent advancements. The scenarios include highways, on-ramp merging, roundabouts, and unsignalized intersections. Furthermore, DRL-based algorithms are evaluated based on five rationale criteria: driving safety, driving efficiency, training efficiency, unselfishness, and interpretability (DDTUI). Each criterion of DDTUI is specifically analyzed in relation to the reviewed algorithms. Finally, the challenges for future DRL-based decision-making algorithms are summarized.", "sections": [{"title": "I. INTRODUCTION", "content": "AUTONOMOUS vehicles (AVs) face significant chal- lenges in making reliable decisions when interacting with human-driven vehicles (HDVs). This challenge is primarily due to the difficulty of accurately predicting the intentions of HDVs. Road traffic crashes cause significant fatalities and serious injuries, reflecting the global issue of millions of lives lost annually [1]. Since 2021, over 900 Tesla crashes involving driver-assistance systems have been reported [2]. Despite unresolved safety issues, the number of AVs is projected to surpass 50 million by 2024 [3]. These statistics underscore the critical need for improving safety in autonomous driving. With a safe decision-making system, AVs have the potential to significantly decrease road crashes caused by human er- rors such as fatigue, distraction, and delayed reactions [4]. Moreover, AVs are capable of making optimal decisions faster than human drivers, thereby enhancing traffic efficiency [5]. There are several typical driving scenarios, such as highways, roundabouts, on-ramping merging, and unsignalized intersec- tions, each characterized by distinct road features and scenario- specific requirements. Autonomous driving in such scenarios is depicted in Fig. 1. For example, on-ramp merging involves completing lane changes well in advance of any obstructed roadway, while navigating a roundabout requires seamlessly exiting at the intended point. Achieving these scenario-based requirements relies heavily on precise and timely operational decision-making in real time. Operational decision support for AV driving includes perception, planning, and control modules. The perception module consists of onboard sensors that continuously perceive the surrounding environment. The perceived data is processed through perception algorithms, such as YOLO methods [6], [7]. The planning module handles driving tasks based on scenario recognition. Subsequently, the motion planner generates discrete decisions and converts them into feasible trajectories. These feasible trajectories are then transmitted to the control module to generate control commands, which are sent to the vehicle's actuators. The actuators, including the steering wheel and pedals, receive and execute the control commands to drive the vehicle. The interactions between AVs and HDVs are complex and therefore continuous decision-making is required, such as lane changes or braking [8]. The model-based, simple guidance, and learning-based methods are commonly used in interactive driving with HDVs. There are mainly four types of model-based approaches. The first model-based approach aims to predict the intentions or trajectories of HDVs, but heavily relies on rule-based classification. For example, [9] predicts the trajectories of HDVs within a fixed time window. However, the time required for a lane-changing maneuver may exceed this fixed time win- dow. The second model-based approach is to make decisions using robust control methods, such as the min-max model predictive control [10]. However, robust control methods make excessively cautious decisions based on a worst-case scenario assumption [11]. These methods are not suitable for most real traffic environments because worst-case scenarios are rare in real-world settings. Furthermore, decisions made for"}, {"title": "II. ROAD FEATURES AND DRIVING TASKS", "content": "This section provides the road features and driving tasks for AVs in the scenarios of highways, on-ramping merging, roundabouts, and unsignalized intersections.\nRoad Features and Driving Tasks: Highways are fun- damental components of road networks, designed to enable vehicle movement over long distances with minimal interrup- tion. The design of highways focuses on safety, efficiency, and environmental impact. Safety features include wide lanes and clear signage to reduce collision risks.High efficiency is achieved by optimizing lane layouts to keep vehicles driving smoothly and reduce bottlenecks. The impact of highways on natural landscapes is reduced through careful route planning. The Interstate Highway System in the United States is a vast network of highways designed to support long-distance travel and economic connectivity across states [49]. Similarly, Germany's Autobahn, known for its sections without speed limits, exemplifies the balance between high-speed travel and safety on highways [50]. An Example of a Highway: Fig. 4(a) presents a scenario involving a three-lane highway. The AV drives in main lane 3 and interacts with HDVs in all three lanes. There are no disturbances or uncertainties other than the surrounding HDVs. Therefore, the issue of driving safety primarily relates to collisions with surrounding HDVs. In the car-following phase, the AV can follow the HDV ahead by adjusting its acceleration. However, cautious following can lead to a loss of driving efficiency. To maintain high driving efficiency, the AV may change lanes when the space ahead is limited. However, collisions with HDVs in the target lane could occur during the lane-changing. Therefore, the driving task on highways can be summarized as balancing collision avoidance with surrounding\nOn-ramping Merging\nRoad Features and Driving Tasks: Ramps, including on-ramps or off-ramps, are essential components of highway systems. Due to the symmetry between on-ramping and off-ramping processes, this survey considers only on-ramping merging. Ramps enable the smooth and safe transition of vehicles between different roadways, typically connecting sur- face streets with highways. Ramps provide access to highways without disrupting traffic flow on the main highway lanes.\nRamp design focuses on safety, efficiency, and space utiliza- tion. Safety is crucial, as ramps must accommodate vehicles accelerating or decelerating while merging onto or diverging from the highway. On-ramps enhance traffic flow by reducing disruptions to mainline traffic and providing sufficient space for safe merging. Additionally, urban space constraints often require innovative ramp designs, such as cloverleaf inter- changes, to connect multiple roadways effectively. Comparison with Highways: Highways and ramps serve different functions, which are summarized below.\n\u2022 Functionality: Highways are designed for high-speed, long-distance travel with minimal interruption, while ramps are the transition between different road types.\n\u2022 Design: Highways are characterized by long, straight stretches with multiple lanes, designed to maintain high speeds and efficient traffic flow. In contrast, ramps often involve curves and elevation changes, designed to accom- modate vehicles as they speed up or slow down.\n\u2022 Speed: Highways support higher speeds, with vehicles typically traveling at constant high speeds over long distances. Ramps involve acceleration or deceleration, requiring careful design to manage the speed differential between the ramp lane and the main lane.\nFor example, the Cloverleaf Interchange is a common de- sign that efficiently manages space while connecting highways with multiple surface streets [51]. Another example is the High Occupancy Vehicle (HOV) lane ramps, which are designed to control the flow of carpool vehicles onto highways, providing direct and less congested access points [52].\nConsider a three-lane ramp scenario in Fig. 4(b), which includes two main lanes and one ramp lane. The AV interacts with both dynamic and static objects. The dynamic objects are surrounding HDVs, each with unique driving intentions, speeds, and acceleration patterns. The static object represents an obstruction within the ramp lane, rendering the lane impass- able and blocking access. As a result, the AV must change into the main lane before the ramp ends, considering the HDVs and the feasibility in lane-changing.\nWaiting for enough space to change lanes and driving slowly to avoid blocked roads lead to safer driving. However, this cautious driving can significantly reduce driving efficiency and lower road capacity on the ramp. Consequently, it is challenging to navigate the ramp, avoid collisions with both surrounding HDVs and the blocked road ahead, while still maintaining a high driving speed.\nRoundabouts\nRoad Features and Driving Tasks: Roundabouts are de- signed to improve traffic flow and enhance safety by reducing the likelihood of severe accidents. One example of a typical roundabout is Folon's obelisk in Pietrasanta in Italy, which features a central island and circular roads around it [53]. Another example is the Place Charles de Gaulle in Paris, France, where twelve major avenues converge around the Arc de Triomphe [54].\nAn Example of a Roundabout: An example of a round- about is presented in Fig. 4(c). The AV starts from the EB4 port and has three possible exit choices: O1, O2, and 03. When the target exit is O1, the AV can simply follow the outer lane. For the target exit O2, there are two possible routes. One route is staying in the outer lane, which is generally safer. The other route is merging into the inner lane and exiting near O2. This second route is more efficient, as the inner lane offers a shorter curve length for the same round angle. However, rear vehicles driving in the inner lane bring potential collision risks. For the target exit O3, the AV must find the right moment to merge into the inner lane. After traveling in the inner lane for a period, the AV is expected to change lanes near the exit. The main challenge is to safely interact with other HDVs when approaching each of these three exits.\nUnsignalized Intersections\nRoad Features and Driving Tasks: Unsignalized inter- sections are critical components of road networks where two or roads meet or cross. They are designed to manage traffic flow from different directions, enabling vehicles to navigate safely through crossing points. Unsignalized intersection can control and organize traffic movements, reduce congestion, and enhance safety for all vehicles. One example is the Diverging Diamond Interchange (DDI) [55].\nAn Example of an Unsignalized Intersection: Fig. 4(d) shows a three-lane unsignalized intersection designed for moderate to heavy traffic flow. The intersection accommodates vehicles from all four directions, with dedicated lanes for specific traffic movements. Each approach to the intersection includes three lanes, and the areas surrounding the intersection are grassland. At the center, where all four roads meet, there is an ample space for vehicles to make turns from any direction. This central area is essential for preventing bottlenecks and ensuring smooth traffic flow."}, {"title": "III. RATIONALE OF THE EVALUATION FACTORS", "content": "In the context of adapting decision-making algorithms to real-world driving, five key evaluation factors have been selected: driving safety and efficiency, training efficiency, unselfishness, and interpretability. As depicted in Fig. 5, driving safety and efficiency form the foundation of any autonomous driving system. Training efficiency enables faster convergence of algorithms. Unselfishness enhances interaction with surrounding traffic, promoting cooperation with HDVs. Meanwhile, interpretability fosters public trust and addresses algorithmic errors, ensuring that decision-making is transpar- ent and understandable. The detailed rationale behind selecting these factors is discussed below.\nDriving Safety\nDriving safety is a fundamental requirement for autonomous vehicles. Frequent collisions cause substantial economic losses and pose severe safety risks [56], [57]. Therefore, driving safety is primarily evaluated based on the frequency of colli- sions with other vehicles [58], [59]. Minimizing collisions is a direct measure of the vehicle's compliance to safety standards. Collision avoidance commonly relies on flexible reactions to hazardous areas. Once a hazard is detected, the system assesses the risk by analyzing the relative speed, distance, and trajectory of surrounding objects [60]. Additionally, some autonomous driving systems evaluate possible decisions to avoid collisions while maintaining high efficiency [61], [62]. Furthermore, other autonomous driving systems use rule-based commands to adjust the AV's behavior when unsafe conditions emerge [63]. For instance, AVs will be asked to stop when they encounter an interaction and spot-lines simultaneously [63].\nDriving Efficiency\nDriving efficiency refers to an AV's ability to maintain a high average speed while adapting to varying traffic condi- tions. However, the implications of driving efficiency extend far beyond speed, affecting road capacity, user experience, and energy consumption.\nOn road capacity, efficient driving allows vehicles to travel at optimal speeds, minimizing delays and reducing traffic congestion. For example, HDVs tend to drive faster on fa- miliar roads, contributing to higher road capacity and traf- fic flow [64]\u2013[66]. Similarly, AVs promote smoother traffic flow when they operate efficiently. Therefore, efficient driving allows more vehicles to travel smoothly without congestion. On user experience, an efficient journey means shorter travel time and a smoother ride, significantly improving overall satisfaction [67]\u2013[69]. Besides, improving driving efficiency is crucial for reducing the energy consumption [70], [71].\nTraining Efficiency\nTraining efficiency of algorithms directly impact the time and resources required to bring a fully functional AV system to reality. One primary benefit of improved training efficiency is the reduced training time. The acceleration allows developers to focus more on system fine-tuning and extensive testing. Sev- eral studies have reduced training time by adding extra training mechanisms or adjusting the structures of networks [35], [72]\u2013 [74]. Another important benefit is the reduction in device wear and tear. Fast and efficient training reduces the required computational resources. By improving training efficiency, the workload of computing equipment is minimized, resulting in less frequent maintenance and replacement.\nUnselfishness\nIn the context of autonomous driving, unselfishness refers to an AV's ability to consider and accommodate the intentions of other HDVs on the road. Unselfishness evaluates how well an AV can cooperate with surrounding vehicles by predicting their intentions and adjusting its behavior accordingly. Human drivers often prioritize factors such as safety, efficiency, and comfort, and these intentions vary widely depending on the specific situations.\nAccurately classifying these driving intentions is essen- tial for effective interactions with surrounding HDVs. Exist- ing methods for recognizing driving intentions and enabling interaction-aware driving have been reviewed in [75]. These methods categorize driving intentions across various scenarios,"}, {"title": "IV. DEEP REINFORCEMENT LEARNING-BASED DECISION-MAKING ON HIGHWAYS", "content": "Many works consider only one of five key factors. A Double Deep Q-Network (DDQN) is integrated with handcrafted safety and dynamically-learned safety modules in [95]. The handcrafted safety module relies on heuristic safety rules derived from common driving practices, ensuring a $d_s$ with other vehicles. The dynamically-learned safety module uses driving data to learn safety patterns. By integrating both the handcrafted and dynamically-learned safety modules, the driving safety is improved.\nMoreover, deep deterministic policy gradients (DDPG) have been used to improve driving efficiency by overtaking sur- rounding vehicles in [96]. The overtaking-oriented training is achieved by adding a high reward for overtaking maneuvers. The reward function for overtaking is formulated as [96]:\n$R_{overtaking} = R_{lane\\_keeping} + 100 \\times (n - R_{acePos})$\nwhere $R_{lane\\_keeping}$ is the reward for lane-keeping, n is the total number of vehicles in a given episode, and $R_{ace Pos}$ reflects the number of vehicles in front of the AV. Therefore, the larger the $R_{acePos}$, the smaller the $R_{overtaking}$. Although safety rewards are applied, the collision rate increases with the frequency of overtaking.\nFurthermore, non-linear model predictive control (NMPC) has been integrated with DDQN to maintain safe highway driv- ing in [97]. NMPC inherently incorporates vehicle dynamics as constraints into its optimization, ensuring that the control inputs from the DRL agent remain within safe and feasible bounds [97]:\n$\\begin{aligned} &\\min _{x(t), u(t)} \\int_{0}^{T} e^{T}(t) Q e(t)+r d^{2}(t)+r u^{2}(t) d t \\\\ & \\text { s.t. } \\dot{x}(t)=f(x(t), u(t)),\\\\ &e_{y_{m i n}} \\leq e_{y}(t) \\leq e_{y_{m a x}},\\\\ &e_{\\psi_{m i n}} \\leq e_{\\psi}(t) \\leq e_{\\psi_{m a x}},\\\\ &\\delta_{m i n} \\leq \\delta(t) \\leq \\delta_{m a x},\\\\ &U_{m i n} \\leq u_{1}(t) \\leq U_{m a x} \\end{aligned}$\nwhere T is the prediction horizon, e(t) is the error vector to be regulated to zero, and Q = diag (q1, q2) is a diagonal matrix of tracking weights. The control effort weight is denoted by r. The steering angle is represented by $\\delta(t)$, and the control input is u(t). The state vector is x(t), and f represents the system dynamics. $e_{y}(t)$ and $e_{\\psi}(t)$ are the lateral position error and heading angle error, respectively. The variables $e_{y_{m i n}}$, $e_{y_{m a x}}$, $e_{\\psi_{m i n}}$, $e_{\\psi_{m a x}}$, $\\delta_{m i n}$, $\\delta_{m a x}$, $U_{m i n}$, and $U_{m a x}$ are the minimum and maximum admissible values for the lateral position error, heading angle error, steering angle, and control input, respectively. NMPC improves the interpretability of safe control by providing a clear mathematical formulation that integrates the system's constraints with the agent's decision-making [98]\u2013[100].\nAdditionally, a policy gradient (PG) method has been used with hard constraints to ensure safe highway driving in [101]. These hard constraints prevent the AV from approaching risky boundaries, such as track edges. For example, the AV's longi- tudinal and lateral positions are restricted from approaching the track boundaries. Cooperative lane-changing has been achieved in [102], enhancing the unselfishness. Interpretability has been improved by combining DRL with imitation learning (IL) in [103]. IL uses expert demonstrations to make the learning more interpretable. Training efficiency in highway"}, {"title": "B. Dual-factor Methods for Highway Driving", "content": "Additionally, two of the five considered factors are inte- grated in some recent studies. The Intelligent Driver Model (IDM) [105] has been incorporated into the DDQN for high- way driving in [106]. The IDM prevents collisions during car-following and therefore, the integration of DDQN with IDM enhances both the driving safety and interpretability in highway driving. The IDM is formulated as [106]:\n$U_{IDM} = U_{max} \\left[1-\\left(\\frac{v}{v_{e}}\\right)^{\\delta}-\\left(\\frac{g^{*}}{g}\\right)^{2}\\right]$\nwhere $U_{max}$ is the maximum acceleration of the AV, $v_e$ is the expected velocity, and g is the gap between the AV and the HDV. The desired gap $g^*$ between the AV and the front HDV is formulated as [106]:\n$g^{*} = d_s + V_{AV}T_e + \\frac{v_{AV} \\Delta v}{2 \\sqrt{U_{max} b}}$\nwhere $T_e$ is the expected time gap, $A_v$ is the velocity difference between the AV and the front vehicle (FV), and b is the comfortable deceleration.\nThe reward function of DDQN has been adapted to improve driving safety and efficiency in [107]. Specifically, a penalty is applied when the vehicle goes off-road or the time-to-collision (TTC) falls below a threshold [108] . The reward for driving efficiency is formulated as [107]:\n$R = \\frac{v_o}{U_{max}}$\nwhere $U_{max}$ is the maximum velocity, and $v_o$ is the current velocity. This reward function helps maintain a relatively high driving velocity, thus increasing driving efficiency. Moreover, driving safety and altruism have been achieved using a level- k game-based DQN in [109]. The level-k game models the reasoning interaction between AVs and HDVs, promoting unselfish decision-making. A crash penalty is implemented in the DQN to prevent frequent collisions between AVs and HDVs. Additionally, unselfishness and training efficiency have been considered in [110]. Unselfishness is achieved through a cooperative multi-goal credit function-based policy gradient (PG). This adapted PG accounts for the goals of all vehicles, optimizing overall performance during training. Training ef- ficiency is improved by a multi-agent reinforcement learning (MARL) curriculum, which reduces the number of trainable parameters and lowers computational costs.\nUnselfishness and driving efficiency on highways are achieved in [111]. Unselfishness is promoted through MARL by considering each vehicle's state. Driving efficiency is en- hanced by a reward function that selects actions to increase the average velocity of all vehicles. Driving safety and driving ef- ficiency have been achieved using multi-objective approximate policy iteration (MO-API) in [112]. Driving safety is ensured by monitoring collisions, while driving efficiency has been assessed by comparing the $v_o$ with the $v_e$. In [113], driving"}, {"title": "C. Three-factor Methods for Highway Driving", "content": "Furthermore, three of the five considered factors are com- bined in a few recent papers. Driving safety, interpretability, and driving efficiency have been improved in [115]. Driving safety and interpretability are enhanced by using a collision penalty and the IDM. Driving efficiency is ensured by a reward based on the velocity difference between $U_{max}$ and $v_o$. The reward at time step t is formulated as [115]:\n$R_{t}=-\\text { Collision }-0.1 \\times(\\text { vmax }-v o)-0.4 \\times(L-1)^{2}$\nwhere Collision, vmax, and vo are the occurrence of a collision, maximum velocity, and current velocity at time t, respectively. L represents the relative position the target lane, where L = 1 indicates that the vehicle has successfully reached the target lane. A collision results in a negative reward, and a larger difference between vo and vmax also leads to a negative reward. Additionally, if the vehicle does not drive in the target lane, a penalty is applied.\nA multi reward-based DQN has been proposed to achieve safe, efficient, and unselfish driving in [116]. Three rewards are combined: speed reward, limited lane-changing reward, and overtaking reward. The speed reward is a normalized reward based on the current speed relative to the minimum and maximum speed limits [116]:\n$R v=\\frac{(v o-U \\min ) \\ast \\gamma v}{U_{max}-U \\min }$\nwhere $R_{v}$ represents the reward for speed, encouraging higher speeds within safe limits. Umin is the minimum speed of the agent vehicle, and r, is the base reward for speed. The limited lane-changing reward function is designed to minimize the number of lane changes, promoting safer driving and reducing the disturbance to surrounding vehicles:\n$R_{l}=\\begin{cases}-\\gamma_{l}, & \\text { if the agent vehicle changes lanes; } \\\\ 0, & \\text { otherwise. } \\end{cases}$\nwhere $-\\gamma_{l}$ is the penalty value for a lane change. The overtaking reward function encourages the agent vehicle to overtake more vehicles, improving driving efficiency:\n$R_{o}=\\begin{cases}\\gamma_{o}, & \\text { if the agent vehicle overtakes another vehicle; } \\\\ 0, & \\text { otherwise. } \\end{cases}$\nwhere $r_o$ is the reward value for overtaking.\nInterpretability, driving safety, and driving efficiency have been achieved in [117]. Safety and efficiency are enhanced by penalizing frequent lane changes and tracking the desired velocity $v_d$, respectively. Interpretability is achieved through"}, {"title": "D. Four-factor Methods for Highway Driving", "content": "Moreover, four of the five considered factors have been included in some studies. Driving safety, driving efficiency, training efficiency, and interpretability have been considered in [121]. Driving safety and driving efficiency are achieved by a reward function that maintains a ds from the leading vehicle while tracking the va. Interpretability is ensured through safety-based driving rules [121]:\n$\\begin{aligned} &\\min \\left\\{\\begin{array}{l}\\inf {t: t>0} \\frac{2\\left(v-v_{\\text {ftarget }}\\right)}{a_{d m a x}}, \\\\ \\inf \\left\\{t: t>0\\right} \\frac{2\\left(v_{\\text {btarget }}-v\\right)}{a_{d \\max }}\\end{array}\\right\\} \\geq t_{f m i n}, t_{b m i n} \\\\\\ &d_{\\text {target }} \\geq \\min \\left{\\begin{array}{l}\\left|\\left(v-v_{f}\\right) t_{f}-v_{f} \\frac{t_{o}^{2}}{2}\\right|, \\left|\\left(v_{b}-v\\right) t_{o}+v_{o} \\frac{t_{o}^{2}}{2}\\right|\\\\ min \\left{\\left|x_{A V}-x_{f a r g e t }\\right|, \\left|x_{A V}-x_{b \\text {target }}\\right|\\right}, a_{\\text {darget }} \\geq a_{\\text {darget min }} \\end{array}\\right\\} \nwhere tfmin and tbmin are the minimum safe time intervals between the AV and the vehicles in front and behind in the target lane, respectively. v is the speed of the AV; vftarget and vbtarget are the speeds of the front and behind vehicles in the target lane, respectively. admax is the maximum deceleration. dtarget... is the minimum distance between the AV and the FV in the target lane, and Adtarget is the actual distance between the AV and the nearest vehicle in the target lane. XAV, X ftarget, and Xbtarget represent the horizontal coordinates of the AV, the front target vehicle, and the vehicle behind in the target lane, respectively. By implementing these safety rules, the decision- making of the AV becomes more transparent and interpretable."}, {"title": "V. DEEP REINFORCEMENT LEARNING-BASED DECISION-MAKING IN ON-RAMPING MERGING", "content": "Driving efficiency has been considered using Q-learning in [130]. The remaining time of AV on the ramp lane is reduced by optimizing the reward function, thus promoting fast lane-changing to the main lane. The reward function is formulated as [130]:\n$r_{t}=\\mu \\bar{u}_{t}+\\omega \\bar{q}_{t}, \\mu>0, \\omega<0$\nwhere $r_t$ represents the reward after taking action at; Ut denotes the average speed in the merging area during time step t; qt indicates the average queue length at the on-ramp during time steps t and t + 1; \u03bc is a positive weight assigned to the speed reward, and w is a negative weight for the queue length reward. These rewards help balance the trade- off between enhancing vehicle mobility on the mainline and reducing delays at the on-ramp. Driving efficiency has also been improved in [131] by reducing the total travel time reward (RTTT), represented by the summation of the total number of vehicles at each time step. Driving safety has been achieved through a safety factor in [132]. The safety factor is a negative reward when the relative distances between AV and HDV are small. Driving safety has been achieved in [133], by giving rewards for each state having a ds and penalties for collisions.\nDual-factor Methods for On-ramping Merging\nDriving efficiency and unselfishness have been considered in [134]. Driving efficiency is achieved by using the average velocity of AVs as part of the reward, and unselfishness is achieved using MARL to maximize general profits. Inter- pretability and driving efficiency have been addressed in [135]."}, {"title": "VI. DEEP REINFORCEMENT LEARNING-BASED DECISION-MAKING AT ROUNDABOUTS", "content": "Driving efficiency in roundabout driving has been im- proved using soft actor-critic (SAC) with higher peak rewards in [146]. Training efficiency has been achieved through action repeat and asynchronous advantage in [147]. Action repeat improves efficiency by allowing the agent to repeat the same action for several time steps, decreasing the frequency of making new decisions. Asynchronous advantage enables each"}, {"title": "VII. DEEP REINFORCEMENT LEARNING-BASED DECISION-MAKING AT UNSIGNALIZED INTERSECTIONS", "content": "Traffic efficiency has been improved by using the difference between vo and va as a reward in [160]. Additionally, a penalty is applied when the velocity drops below a threshold, further boosting traffic efficiency. In [161], driving efficiency has been achieved by applying a constant penalty as long as the AV has not reached the target exits."}, {"title": "VIII. CONCLUSION AND DISCUSSION", "content": "This survey presents a comprehensive overview of the current state of the art in DRL-based decision-making for autonomous vehicles. By discussing recent research efforts in this field, this survey highlights the diverse algorithms developed to address decision-making tasks across various sce- narios, including highways, on-ramping merging, roundabouts, and unsignalized intersections. Our analysis goes beyond simply presenting these algorithms by uncovering valuable insights, identifying key gaps in the current research, and high- lighting emerging trends in DRL-based decision making for autonomous driving. While driving efficiency and safety are"}]}