{"title": "Pureformer-VC: Non-parallel One-Shot Voice Conversion with Pure Transformer Blocks and Triplet Discriminative Training", "authors": ["Wenhan Yao", "Zedong Xing", "Xiarun Chen", "Jia Liu", "Yongqiang He", "Weiping Wen"], "abstract": "One-shot voice conversion(VC) aims to change the timbre of any source speech to match that of the unseen target speaker with only one speech sample. Existing style transfer-based VC methods relied on speech representation disentanglement and suffered from accurately and independently encoding each speech component and recomposing back to converted speech effectively. To tackle this, we proposed Pureformer-VC, which utilizes Conformer blocks to build a disentangled encoder, and Zipformer blocks to build a style transfer decoder as the generator. In the decoder, we used effective styleformer blocks to integrate speaker characteristics into the generated speech effectively. The models used the generative VAE loss for encoding components and triplet loss for unsupervised discriminative training. We applied the styleformer method to Zipformer's shared weights for style transfer. The experimental results show that the proposed model achieves comparable subjective scores and exhibits improvements in objective metrics compared to existing methods in a one-shot voice conversion scenario.", "sections": [{"title": "I. INTRODUCTION", "content": "Voice conversion(VC) aims to convert the speaker's timbre in speech to the target speaker's while keeping the content unchanged. It is usually text-independent because the model only learns the sound characteristics within target utterances.\nResearchers have recently focused on many-to-many and any-to-one voice conversion, where models learn from non-parallel data. Motivated by the concept of image style transfer from the field of computer vision, various approaches leveraging generative adversarial networks (GANs) have been implemented for GANs-based voice conversion [1]-[6]. Among these approaches, speaker encoder assist the generator in learning the transformation relationships between different domains with the important style transfer module, such as AdaIN and WadaIN [7], [8]. These methods apply normalization parameterized affine to the hidden variances or the weights of convolution layers. Although the generated results are acceptable, GAN training is challenging due to convergence difficulties and sensitivity to the imbalance of the dataset.\nFrom the perspective and assumption that speech can be decomposed into multiple components(e.g. timbre, pitch, content, and rhythm) [9], the disentanglement-based VC is worth considering. This allows neural networks to learn representations of each speech component separately by multiple encoders and a decoder [9], [10]. During training, different encoders are fed with the homologous spectrogram respectively and obtain independent speech component representations. The decoder is responsible for reassembling the various components into speech for reconstruction training. However, the present methods involve force decomposition in Speech-Split [9], [10], instance normalization with INVC [11], and information bottleneck in AutoVC [12], which can not ensure perfect disentanglement and reconstruction.\nBased on the previous narrative, we propose an effective VC framework called Pureformer-VC. We declare that a successful disentangled voice conversion based on an encoder-decoder framework should depend on three factors: (1) A reasonable design of the encoder and decoder. (2) An optimization objective with representational discriminability, and (3) A well-functioning style transfer module within the decoder to fuse speech components and recover the speech. In this paper, we propose the Pureformer-VC framework to satisfy the three factors, which contains the content encoder, speaker encoder, decoder, and vocoder. Based on the excellent success of sequence modeling with effective transformer blocks, we propose constructing the content encoder with Conformer blocks containing IN operation and decoder with Zipformer blocks [13], considering their excellent performance in speech"}, {"title": "II. RELATED WORK", "content": "Style transfer learning teaches the voice conversion models to fuse different speech representations. Accordingly, the style transfer function accepts the source speaker-independent representations and one target speaker-dependent representation. Chou et al. [11] first found that instance normalization can filter out the speaker information and preserve the source content in INVC. Then, the IN function was widely used in the GANs-based VC. [4], [5]. In addition, the WadaIN method applies affine operations to the convolutional kernel and convolves the source data, thereby modifying the style of the source data in the WadaIN-VC [6]. However, these models are CNN-based. To utilize the self-attention mechanism in Transformer, the Attention-AdaIN-VC [17] inserted the styleformer block into the CNN blocks and gain a better voice conversion effect. In general, we continue using the styleformer transfer mechanism(STM) function in this paper."}, {"title": "III. METHODOLOGY", "content": "The overall architecture of Pureformer-VC is illustrated in Figure 1(a). Pureformer-VC includes the content encoder, decoder, speaker encoder, and vocoder. We used the pre-trained Hifi-GAN generator as vocoder. [18].\nWe constructed the decoder with 4 Zipformer blocks with STM as shown in Figure 1(b). The decoder reintegrates the content and timbre representations of the speech. Therefore, we apply the STM to the weights in self-attention of Zip-former blocks. During the model initialization phase, the STM firstly initializes some attention weights. With affine operations later, the weights are imbued with the style characteristics of the split speaker embedding vector S1, S2. Then, we adopt weight normalization(WN) [19] on the parameters to get better convergence performance. The WN accepts the weight and normalizes it at the outcoming dimension. By the WN operation, we scale the output of each weight back to unit standard deviation. The WN helps the model to accelerate convergence in training after the attention calculation with stylized weights."}, {"title": "C. Content Encoder with VAE Training", "content": "The content encoder parameterizes and approximates the variational distribution of $q_\\theta(z|x)$. We constructed each Con-former block with instance normalization and an AveragePooling1D layer to squeeze the time dimension to half times. There are 4 continuous blocks, and finally output the posterior variances of content representation \u03bc, \u03c3. It turns out to optimize the ELBO of logp(x):\n$\\mathcal{L}_{elbo} = E_{q_\\theta(z|x)}[logP_\\phi(x|z)] - KL(q_\\theta(z|x)||p(z)) $\nwhere \u03b8 denotes the encoder network and \u03c6 represents the decoder. The above first term is the reconstruction loss, while the second one is the KullbackLeibler divergence between the approximate posterior and the prior. Thus, the VAE training loss can be summarized as [20]:\n$\\mathcal{L}_{vae} = E [||x - \\hat{x}_{dec}||] + \\frac{1}{2} * E[\\mu^2 + \\sigma^2 - log(\\sigma^2) - 1] $"}, {"title": "D. Speaker Encoder with AAM-Softmax Loss", "content": "The speaker encoder can extract the timbre representations from Mel-spectrograms. We use the backbone of multiple Conformer blocks without instance normalization from MFA-Conformer [21] to extract the hidden feature. Then, the AAM-softmax layer, which is a parameterized loss function for finding cluster centers, is used to enhance the embeddings."}, {"title": "E. Triplet loss and Data Sample Strategy", "content": "In the previous disentanglement-based VC, the source Mel-spectrogram and the target one(accepted by the speaker encoder) were the same in the training stage but differed in the inference stage. This weakened the model's generalization capability. To tackle this problem, we employ the triplet loss [16] as Figure 2 shows, which is an unsupervised learning technique with discriminative training and enable the speaker encoder to learn the differences in timbre among various voices.\nIn the training stage, we sample three utterance segments of equal length from the dataset as anchor sample $x_{anc}$, positive sample $x_{pos}$, and negative sample $x_{neg}$. As shown in Figure 2, the anchor sample and positive sample have the same timbre, while the negative sample comes from another speaker differing from the anchor. Thus, let the $n_m$ denote the L2 normalization. We can use the speaker encoder outputs of the three samples to calculate a triplet loss:\n$C_{anc, pos, neg} = E_s(x_{anc}), E_s (x_{pos}), E_s(x_{neg})$"}, {"title": "F. Training Objective", "content": "The training objective of the Pureformer-VC model includes VAE loss, AAM-softmax loss, and triplet loss. The total VAE loss contains two outputs $\\hat{Y_1}$, $\\hat{Y_2}$ as Figure 2 shows, and can be denote as:\n$\\mathcal{L}_{t-vae} = \\lambda_1(L_{rec}(x_{anc}, x_{pos}) + L_{rec}(x_{anc}, x_{neg}))+\\lambda_2(L_{KL}(x_{anc}, x_{pos}) + L_{KL}(x_{anc}, x_{pos}))$\nThe AAM-softmax loss can be calculated by the three true labels of samples C and the speaker encoder's predictions:\n$\\mathcal{L}_{t-aam} = \\sum_{C_i \\in C} L_{aam} (C_i, X_i)$\nFinally, the triplet loss helps the speaker encoder to distinguish the embeddings. The total training objective is as follows:\n$\\mathcal{L}_{total} = \\mathcal{L}_{t-vae} + \\lambda_3\\mathcal{L}_{t-aam} + \\lambda_4\\mathcal{L}_{tri}$"}, {"title": "G. Vocoder", "content": "The vocoder has the same structure as the HiFi-GAN generator. In our study, the vocoder was pre-trained in the same dataset as the voice conversion training."}, {"title": "IV. EXPERIMENTS AND RESULTS", "content": "Datasets. To evaluate the effectiveness of Pureformer-VC, we conducted a comparative experiment and an ablation study on VCTK corpus [22]. The VCTK corpus includes 109 speakers, each reading about 400 utterances. During training, ten speakers are randomly selected as unseen speakers for one-shot VC. The Mel-spectrogram extraction is as the same as HiFi-GAN work. In the sampling stage, it will randomly sample an utterance from a speaker and sample two utterances from another speaker to form a training subset $x_{anc}, x_{pos}, x_{neg}$.\nTraining Setup. In the training stage, the batch size is 16. The learning rate is constant at 2 \u00d7 10-4. The Pureformer-VC is trained by Adam optimizer [25] with \u03b2\u2081 = 0.9, \u03b22 = 0.99, \u20ac = 1 \u00d7 10-6. The \u039b\u2081 is set to 10 and 12 ranges from 1 \u00d7 10 \u2212 4 to 1. The \u039b3, \u039b4 are set to 1. The d is 0.3.\nBaseline Setup. We compared Pureformer-VC with recent VC frameworks, such as AdaIN-VC [11], AutoVC [12],"}, {"title": "V. CONCLUSION", "content": "In this paper, we introduced a novel approach to voice conversion leveraging a pure transformer network constructed as a VAE encoder-decoder framework, which is called Pureformer-VC. Within the decoder, we integrated a styleformer module, which has enhanced the model's capacity for style transfer. Furthermore, we enhanced the effectiveness of the speaker encoder by incorporating the triplet loss and AAMSoftmax loss. These additions have significantly improved the model's ability to capture and represent the nuances of different speaking voices, leading to more accurate and robust voice conversion. In conclusion, the Pureformer-VC model, bolstered by the strategic use of specialized loss functions and style adaptation mechanisms, presents a substantial advancement in the field of voice conversion."}]}