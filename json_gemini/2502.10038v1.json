{"title": "POI-Enhancer: An LLM-based Semantic Enhancement Framework for POI Representation Learning", "authors": ["Jiawei Cheng", "Jingyuan Wang", "Yichuan Zhang", "Jiahao Ji", "Yuanshao Zhu", "Zhibo Zhang", "Xiangyu Zhao"], "abstract": "POI representation learning plays a crucial role in handling tasks related to user mobility data. Recent studies have shown that enriching POI representations with multimodal information can significantly enhance their task performance. Previously, the textual information incorporated into POI representations typically involved only POI categories or check-in content, leading to relatively weak textual features in existing methods. In contrast, large language models (LLMs) trained on extensive text data have been found to possess rich textual knowledge. However leveraging such knowledge to enhance POI representation learning presents two key challenges: first, how to extract POI-related knowledge from LLMs effectively, and second, how to integrate the extracted information to enhance POI representations. To address these challenges, we propose POI-Enhancer, a portable framework that leverages LLMs to improve POI representations produced by classic POI learning models. We first design three specialized prompts to extract semantic information from LLMs efficiently. Then, the Dual Feature Alignment module enhances the quality of the extracted information, while the Semantic Feature Fusion module preserves its integrity. The Cross Attention Fusion module then fully adaptively integrates such high-quality information into POI representations and Multi-View Contrastive Learning further injects human-understandable semantic information into these representations. Extensive experiments on three real-world datasets demonstrate the effectiveness of our framework, showing significant improvements across all baseline representations.", "sections": [{"title": "1 Introduction", "content": "With the advancement of smart city technology (Ji et al. 2022b; Wang et al. 2021b; Wang, Wang, and Wu 2018) and the widespread adoption of smart devices, the volume of location-based mobile data, such as POI (Points of Interest) check-in data and user trajectory data, has surged (Ding et al. 2018; Zhu et al. 2023). Predicting user destinations (Zhao et al. 2020), forecasting visit flow (Song et al. 2020), and similar tasks (Han et al. 2024) have become key research focuses. In tackling these difficulties, POI representation learning, which can be trained via self-supervised methods and utilized across various tasks like traffic forecasting (Liu et al. 2024b; Ji et al. 2022a; Wang et al. 2022) and trajectory predction (Jiang et al. 2023b; Wu et al. 2019; Wang et al. 2018), stands as a particularly meaningful and promising direction.\nTo enhance the diversity of information within POI representation vectors and achieve superior performance in complex downstream tasks, researchers are exploring the integration of various information beyond basic geographic data. For example, they incorporated user preference data (Dai et al. 2022) and visual information (Balsebre et al. 2023) into POI representations. Although related textual information, such as POI categories (e.g. restaurants and hotels) and check-in content on social media like Twitter, provides some insights into the social functions and other aspects of POIs, the semantic richness and depth of these data are limited. When compared to the vast amount of descriptive information available on the internet regarding POIs, these data sources fall short in both content richness and coverage. In recent years, large language models (LLMs) trained on extensive volumes of internet data have been applied across numerous fields, demonstrating remarkable capabilities, particularly in the domain of spatial-temporal data (Li et al. 2024). Although LLMs have proven beneficial in addressing challenges in this area, leveraging LLMs to enhance POI representation presents two specific challenges.\nThe first challenge lies in effectively extracting the geographical knowledge within LLMs. A common idea (Golkar et al. 2023) is to provide LLMs with prompts related to geographic information and then obtain text output. However, LLMs have limitations in handling numerical input, and for representation learning, we need vectors that are versatile across tasks, which makes this method not suitable. Some studies (Chen, Wang, and Xu 2023; Liu et al. 2024a) have also experimented with feeding extracted spatial-temporal features to a partially or fully frozen LLM, using the LLM as the backbone to solve specific problems. But these works are typically tailored to a single spatial-"}, {"title": "2 Preliminaries", "content": "Definition 1 (Point of Interest (POI)). A POI is a specific geographic location with some basic attributes $p = (id, pn, c, lon, lat)$, where id indicates index, pn means"}, {"title": "3 Methodology", "content": "This section provides a comprehensive demonstration of the technical details of POI-Enhancer framework and Fig. 1 presents the overall architecture. In Fig. 1, part (a) is the Prompt Generation and Feature Extraction phase, where specialized prompts are generated and used to extract relevant semantic information from the LLM. The second phase, Embedding Enhancement, corresponds to part (b), where the extracted information is further refined and integrated with the POI representations to be enhanced. Finally, part (c) represents Multi-View Contrastive Learning, where we designed three sampling strategies to select positive and negative samples for contrastive learning. Besides, to assist LLMs in more accurately capturing POI-related knowledge, we additionally processed and derived three kinds of extra attributes mentioned above. A detailed description of this procedure can be found in the Supplementary Material."}, {"title": "Prompt Generation and Feature Extraction", "content": "Generate prompt Due to the LLM's low sensitivity to numbers, we need to bundle basic attributes like latitude, longitude, and name with extra attributes when inputting them, to help the LLM accurately target the desired POI. Besides, simply stacking various features into a prompt can make it difficult for the LLM to focus on key points and effectively extract information. Hence, the proposed prompt pattern consists of three parts: (1) Role-Playing, (2) POI Information, and (3) the Question. The POI Information part encompasses basic information and extra information, corresponding to the basic and extra attributes, respectively. Firstly, the design purpose of role-playing at the beginning of the prompt is to allow the LLM to fully unleash its knowledge, enabling the LLM to embody a role familiar with geographical knowledge. An attribute header is added in front of the POI information to help the LLM accurately capture the information of input attributes. Next, we generate multiple sentences based on combinations of the basic attributes and three extra attributes. Lastly, inspired by the (Gurnee and Tegmark 2024), we design the question at the end of each prompt about the content to trigger the relevant knowledge. Consequently, we generate three types of prompts for each POI $p_i$:POI Visit Pattern Prompt, POI Address Prompt, and POI Surrounding Prompt, denoted as $T^V_i$, $T^A_i$, $T^S_i$. An Example of the prompt we generated is shown in Fig. 5, which is in Supplementary Material.\nExtract from LLM In POI-Enhancer, we input the prompts into the LLM and take the final hidden layer state from the LLM as the semantic feature. It is worth noting that the LLM serves as a frozen encoder when training. So, for a POI $p_i$, the feature extraction process can be denoted as:\n$E^V_{p_i} = H(T^V_i), E^A_{p_i} = H(T^A_i), E^S_{p_i} = H(T^S_i),$ (1)\nwhere $E^V_{p_i}$, $E^A_{p_i}$, $E^S_{p_i} \\in \\mathbb{R}^D$ are the corresponding semantic feature of three kinds of prompts, $H$ is the process of extracting the last hidden state from the LLM, and $D$ is the dimension size of the hidden state vector."}, {"title": "Embedding Enhancement", "content": "Dual Feature Alignment leverages the intricate connections between address and visit patterns, as well as between address and surrounding environment to obtain higher-quality semantic features. Semantic Feature Fusion uses attention score-weighted merging to ensure the quality of the features when fusing the semantic features into a single semantic vector. Afterward, Cross Attention Enhancement, based on the cross-attention method, employs the semantic vector obtained earlier to fully integrate and enhance the POI representations, resulting in the final output vector.\nDual Feature Alignment A POI's address, a key factor of geography information, is closely linked to its visit patterns and surrounding environment. For example, as shown in Tab. 1, the New York Stock Exchange is on Wall Street, a"}, {"title": "4 Experiments", "content": "Experiment Setup\nDatasets We conducted experiments on three check-in datasets provided by (Yang et al. 2014): Foursquare-NY, Foursq-SG, and Foursquare-TKY, sampled from New York, Singapore, and Tokyo, respectively. We remove all POIs with less than 5 check-ins in the dataset and check-in sequences with less than 10 POIs. The statistics of the processed dataset are in Supplementary Material. Then we shuffled the dataset and split it into a ratio of 2:1:7 for the test set, validation set, and training set. It should be noted that the training set for the POI Recommendation task will also be used as the dataset for sampling in contrastive learning.\nBaselines We introduced six baselines in our experiment including Skip-Gram (Tomas et al. 2013), POI2Vec (Feng et al. 2017), Geo-Teaser (Zhao et al. 2017), TALE (Wan et al. 2021), Hier (Shimizu, Yabe, and Tsubouchi 2020), and CTLE (Lin et al. 2021). The details of the baselines are in the Supplementary Material. LLM-based baselines are also included like Llama2 (Touvron et al. 2023), ChatGLM2 (GLM et al. 2024), and GPT-2 (Radford et al. 2019).\nDownstream Tasks & Metrics To evaluate POI-Enhancer and make a comprehensive comparison, we set up three downstream tasks based on LibCity (Wang et al. 2021a).\n\u2022 POI Recommendation, Based on a user's historical check-in sequence, the POI Recommendation task aims to predict the next POI the user would visit.\n\u2022 Check-in Sequence Classification, Given an arbitrary check-in sequence, this task requires the downstream model to detect which user this sequence belongs to.\n\u2022 POI Visitor Flow Prediction, POI visitor flow prediction requires the downstream model to forecast the future volume of visitor flow based on historical visitor data.\nIn the POI Recommendation task, we use Hit@k as the evaluation metric (value equals 1 if the ground truth is among the top k in the recommendation list, otherwise 0, k = 1, 5). The Check-in Sequence Classification task is evaluated using Accuracy (ACC) and Macro-F1 while the Visitor Flow Prediction task is assessed with Mean Absolute Error (MAE) and Root Mean Square Error (RMSE).\nImplementation In our framework, we use the Llama-2-7B as the LLM backbone. The complete implementation details are provided in the Supplementary Material."}, {"title": "Overall Result Analysis", "content": "The result of downstream tasks is presented in Tab. 2, demonstrating that POI-Enhancer significantly improved the performance of all baselines across all datasets. Skip-Gram and POI2Vec incorporate spatial information differently: Skip-Gram uses co-occurrence frequencies, while POI2Vec employs a geographic binary tree, both ignoring temporal features. Geo-Teaser includes spatial and temporal data with coarse granularity, while TALE, Hier, and CTLE integrate finer-grained spatiotemporal data. However, all six methods overlook POI semantic knowledge. Our framework addresses this gap, significantly enhancing performance.\nIn three tasks, POI-Enhancer shows the most significant improvement in the Check-in Sequence Classification. This could be because the textual knowledge provided by POI-Enhancer is more beneficial for handling classification tasks. For the first task, POI2Vec achieves the greatest improvement on the New York dataset, with both metrics increasing by over 20%. This is due to its focus on capturing check-in sequence patterns while neglecting other modalities. Our framework compensates for these limitations by enriching textual knowledge. As for the second task, our findings indicate that Skip-Gram shows the weakest improvement, which is because it focuses on modeling representations from user trajectories and limits the potential for improvement. In the"}, {"title": "Further Analysis on POI-Enhancer", "content": "Ablation Experiment In this subsection, we conduct comprehensive experiments with four variant settings to evaluate the effectiveness of the components we design:\n\u2022 POI-Enhancer/P We remove the special prompt design including the role-playing, the attribute headers, and the question. \u2022 POI-Enhancer/D We removed the Dual Feature Alignment and Semantic Embedding Fusion. Instead, we generated a single prompt, which accumulates the content of the previous three kinds of prompts while maintaining the same format. The features extracted from this prompt by the LLM will be directly input into the Cross Attention Fusion.\n\u2022 POI-Enhancer/F We remove Cross Attention Fusion and concatenate the $E_{POI}$ and $E_{LLM}$ as the final vector instead.\n\u2022 POI-Enhancer/C We only consider the spatial perspective. Specifically, given a POI, we define a square area centered around it to collect positive samples, with the parameters consistent with Geography Contrastive Learning.\nWe tested them on three downstream tasks using the New York dataset, with Hit@1, ACC, and MAE as evaluation metrics. As shown in the Fig. 2, POI-Enhancer outperforms all variant settings and we can draw the following conclusions: (1) The specialized prompts can enhance the framework's performance because they stimulate the LLM to extract spatial-temporal knowledge more efficiently. (2) The"}, {"title": "Parameters Analysis", "content": "In this subsection, we study the effect of different $L_1$ and $L_2$ parameter settings in our framework. Specifically, we focus on enhancing the Hier model using the New York dataset, with POI recommendation as the downstream task. When evaluating the impact of one parameter, we keep the other parameters fixed at their optimal values. As shown in the Fig. 3, we can observe that for both $L_1$ and $L_2$, the performance initially improves with the increasing number of layers, reaches optimal performance, and then deteriorates. So, in other experiments, we set $L_1$ to 4 and $L_2$ to 2. On the one hand, this indicates that when $L_1$ is too low, our alignment method fails to fully utilize the relational information between features, while an excessively high number of $L_1$ layers tends to cause over-fitting. On the other hand, this suggests that when $L_2$ is below the optimal value, our fusion method fails to effectively incorporate the knowledge from LLM into the original representations."}, {"title": "Quality Analysis", "content": "To further evaluate the quality of the enhanced vectors produced by POI-Enhancer, we conducted clustering tasks using the K-means algorithm on three datasets. We applied this algorithm to all types of representation vectors, both before and after enhancement. The number of clusters was set to match the number of POI categories in each dataset. We then assessed the clustering performance using the Normalized Mutual Information (NMI) metric. The results depicted in the Fig. 4 demonstrate the effectiveness of our framework, as all evaluation metrics for the representation vectors across the three datasets have shown significant improvement. This indicates that: (1) We successfully extracted high-quality textual features, and the rich textual information helps similar representation vectors to cluster more closely together. (2) We effectively integrated textual information into the initial representations, further enhancing the quality of the original vectors. (3) The Multi-View Contrastive Learning approach encouraged vectors of the same class to be closer together while pushing vectors of different classes further apart."}, {"title": "5 Related Work", "content": "LLMs in Spatial-temporal Tasks Considerable efforts have been dedicated to using LLMs to improve the performance of spatial-temporal tasks (Yu et al. 2024). For instance, GeoGPT (Zhang et al. 2023a) introduced an LLM-based tool capable of automating the processing of geographic data, but it does not delve into extracting detailed information about locations. GEOLLM (Manvi et al. 2023) designed prompts that include coordinates, address, and surrounding buildings, but it can only address simple questions in a Q&A format and are unable to handle complex tasks like POI recommendation. Besides, they fail to fully extract the semantic information of POIs. Some researchers have used LLMs as backbones to tackle complex real-world tasks. For example, GATGPT (Chen, Wang, and Xu 2023) input spatial-temporal features into a frozen LLM to predict traffic speeds, while ST-LLM (Liu et al. 2024a) used a partially frozen LLM to forecast traffic flow. However, these methods are designed for specific individual problems and cannot be applied across multiple tasks. To solve these limitations, we designed three types of special prompts to extract the semantic information of POIs from LLMs effectively.\nPOI Representation with Semantic Information POI representation aims to turn each POI into a vector that can be utilized in various downstream tasks like traffic forecasting tasks (Zhang et al. 2023b; Jiang et al. 2023a; Ji et al. 2023) and trajectory tasks (Zhu et al. 2024; Jiang et al. 2023c,d). Most existing methods like (Dai et al. 2022), leverage textual features typically using one-hot code to encode POI categories and then concatenate them with the embedding vectors. For data types like check-in content, (Xie et al. 2016) model the similarity between POIs by constructing a POI-Word relationship graph, while (Chang et al. 2018) draws inspiration from Word2Vec method, simultaneously training word vectors and POI vectors. However, these methods often fall short in preserving semantic information and achieving a more comprehensive integration during the fusion process. To address this issue, we designed three modules in the Embedding Enhancement to improve the preservation and integration of semantic information in the POI embedding."}, {"title": "6 Conclusion and Future Work", "content": "We propose a framework called POI-Enhancer, which enhances all POI representation methods by leveraging the LLM. To introduce textual information into POI embeddings, we designed three special prompts to extract features from the LLM. To use the links between address features and other features, we introduced Dual Feature Alignment and Semantic Feature Fusion, which help obtain and preserve high-quality textual features. To better integrate the extracted knowledge into POI representations, we further developed the Cross Attention Fusion. Lastly, to enhance the representation capabilities of the vectors, we proposed Multi-View Contrastive Learning, using three strategies to sample positive and negative examples. The experiment results demonstrate that our framework significantly improves the performance of POI representation vectors across various downstream tasks in three real-world datasets."}, {"title": "Technical Appendix", "content": "POI Attributes Preprocess\nVisit Pattern of POI We conduct a statistical analysis of the check-in data to identify the visit patterns of a POI, which consists of the weekly visit pattern and daily visit pattern. On the one hand, we first divide the week into weekdays (Monday to Friday) and weekends (Saturday and Sunday). Whether a POI is visited more during the weekdays or weekends will determine its weekly visit pattern. On the other hand, We part every day into seven time periods: early morning (6 to 9 AM), morning (9 to 11 AM), noon (11 AM to 1 PM), afternoon (1 PM to 5 PM), evening (5 to 7 PM), night (7 to 12 PM), and midnight (0 to 6 AM). Afterward, we categorize each POI's check-in records into the corresponding time slot. Finally, the time slot with the highest number of check-in records will represent the daily visit pattern. Take the POI from Tab. 1 as an example: its weekly visit pattern is weekdays, and its daily visit pattern is 6 to 9 AM.\nAddress of POI Address features are essential for a POI. Typically, a POI's address includes the street name, house number, and postal code. By utilizing geographic revere search API provided by Nominatim \u00b9, we can input latitude and longitude coordinates to retrieve the corresponding address details. For each POI in the dataset, we leverage this API to query its address information as well as the basic attribute Name.\nSurrounding of POI For any given POI, we survey the category of other POIs nearby and consider this as the surrounding attributes. Specifically, we search for all POIs within a square area centered on a certain POI and the square's side length is set to 0.5 km. Then, we count the number of categories within these POIs and sort them in descending order. Finally, we select the top three categories as its surrounding attribute. For example, the top three categories near the POI in Tab. 1 are office, building, and road, which are its surrounding attributes.\nMulti-View Contrastive Learning\nSequence-Time Contrastive Learning A formal constraint is that given a check-in record sequence R and a check-in record r = (u,p, t), for any r' = (u,p',t') in the R, we have the following two requirements for r':\n\u2022 $|index(r') \u2013 index(r)| < \u03bb$, where index is a function indicating the position in R, and \u03bb is a threshold.\n\u2022 $date(r') = date(r)$, where date is a function that extracts the date from a timestamp.\nThe p' in r' which meets the above requirements is considered a positive sample.\nGeography Contrastive Learning Given a POI p, the positive samples p' have to meet the following two criteria:\n\u2022 Area(p) is defined as $Square(p, \u03c4)$, where Square is a function that generates the corresponding square region with a side length of \u03c4. The point p' lies within the Area(p)."}]}