{"title": "Autoregressive + Chain of Thought (CoT) ~ Recurrent:\nRecurrence's Role in Language Models and a Revist of Recurrent Transformer", "authors": ["Xiang Zhang", "Muhammad Abdul-Mageed", "Laks V.S. Lakshmanan"], "abstract": "The Transformer architecture excels in a variety of language\nmodeling tasks, outperforming traditional neural architectures\nsuch as RNN and LSTM. This is partially due to its elimination\nof recurrent connections, which allows for parallel training\nand a smoother flow of gradients. However, this move away\nfrom recurrent structures places the Transformer model at the\nlower end of Chomsky's computational hierarchy, imposing\nlimitations on its computational abilities. Consequently, even\nadvanced Transformer-based models face considerable diffi-\nculties in tasks like counting, string reversal, bracket pairing,\nand multiplication. These tasks, though seemingly elemen-\ntary, require a level of computational complexity that exceeds\nthe capabilities of the Transformer architecture. Concurrently,\nthe emergence of \"Chain of Thought\" (CoT) prompting has\nenabled Transformer-based language models to tackle tasks\nthat were previously impossible or poorly executed. Despite\nsome previous research primarily interpreting CoT from a\npsychological perspective, a comprehensive understanding\nof why CoT proves so effective in the reasoning process re-\nmains elusive. In this work, we thoroughly investigate the\ninfluence of recurrent structures in language models on their\nreasoning abilities, shedding light on how the CoT approach\ncan mimic recurrent computation and act as a bridge between\nautoregression and recurrence. It is this approximated recur-\nrence that notably improves the model's performance and\ncomputational capacity. Moreover, we revisit recent recurrent-\nbased Transformer model designs, focusing on their computa-\ntional abilities through our proposed concept of \"recurrence-\ncompleteness\" and identify key theoretical limitations in mod-\nels like Linear Transformer and RWKV. Through this, we\naim to provide insight into the neural model architectures and\nprompt better model design.", "sections": [{"title": "1 Introduction", "content": "The emergence of large language models (LLMs) (Achiam\net al. 2023; Touvron et al. 2023; Jiang et al. 2023), fea-\nturing billions to trillions of parameters, marks significant\nprogress in diverse language-related tasks (Chang et al. 2024;\nThirunavukarasu et al. 2023; Zhang et al. 2023; Wu et al.\n2023; Beltagy, Lo, and Cohan 2019). However, growing\nconcerns have arisen over the limitations (Dziri et al. 2024;\nValmeekam et al. 2022; Ullman 2023) of current LLMs, par-\nticularly their difficulties with basic tasks such as multiplica-\ntion or counting. While much of the debate centers on training"}, {"title": "2 Definition and Concept", "content": "Our study places significant emphasis on the concept of re-\ncurrence within neural networks. However, the concepts of\nrecurrence and autoregression have not been clearly defined\nand contrasted with in previous literature. In this section, we\nprovide explicit definitions and distinctions between recur-\nrence and autoregression in the context of neural networks to\nestablish a foundation for our analysis."}, {"title": "2.1 Recurrence and Autoregression", "content": "A neural network can generate two types of outputs for a\ngiven input: (1) h, representing the neural network's hidden\nstate encoded as a vector, and (2) o, the token (label), a single\nvalue derived from the hidden state vector h. The use of these\noutputs shapes the network's modeling capabilities, resulting\nin either autoregressive or recurrent architectures.\nThe notion of recurrence is derived from computation the-\nory, where a model maps input data to corresponding output\nvalues, represented as $f : X \\rightarrow H$. In line with computability\ntheory conventions, we restrict X and H to countable sets,\nwhere all elements in X can be enumerated in a specific order\nas $(X_1, X_2, X_3,\u2026)$. We denote the mapping of an element\n$x_t$ at position t as $h_t = f(x_t)$. Function f is said to be (k\nterms) recurrent under function $g : H^k \\rightarrow H$, if the output\nof f on $x_t$ can be generated by applying g to the k preceding\noutputs of f as follows:\n$h_t = f(x_t) = g(h_{t-1}, h_{t-2}, h_{t-3},..., h_{t-k})$ (1)\nNote that function g is usually much simpler than f.\nTake the Fibonacci sequence, $f(n) = \\frac{1}{\\sqrt{5}}(\\frac{1+\\sqrt{5}}{2})^n - (\\frac{1-\\sqrt{5}}{2})^n$, as an\nexample. It is (2 terms) recurrent under a simpler function\n$g_{add}(a, b) = a + b$, where the nth result can be derived from\nits two predecessors:\n$h_n = g_{add}(h_{n-1}, h_{n-2}) = h_{n-1} + h_{n-2}$ (2)\nThis captures the core principle of recurrent modeling: the\ncurrent computational result, $h_t$, can be derived solely using\nprevious outcomes. This is possible because the preceding\nterms of h encapsulate all essential computational informa-\ntion required for subsequent calculations and can thus be\nreused to resume the computation for obtaining the next h.\nBy employing the simple function g, the model utilizes the\nknowledge in the previous h terms and avoids the need to\ndirectly apply the complex function f to the entire input $x_t$\nand restart the entire computation anew."}, {"title": "2.2 Recurrence and Automata Theory", "content": "Classic computational analysis is based on concecpt to Au-\ntomata, the understanding of which is essential for applying\ncomputability theory to neural networks. A state machine,\nor Finite Automata (FA), is defined by a set of states and\ntransitions governed by transition rules (Figure 2). Formally,\nFA is represented as a tuple $M = (Q, \u03a3, \u03b4, q\u03bf, F)$, where\n$Q = (q_1, q_2,\u2026, q_n)$ is a finite set of states, \u2211 is the input\nalphabet, $\u03b4: 2 \u00d7 \u03a3 \u2192 Q$ is the transition function, qo is the\ninitial state, and F denotes the accepting states.\nA FA generates two outputs upon processing an input $x_t$:\n(1) $h_t$, which is some resulting state $q_i$ from the set of all\nstates Q; and (2) $o_t$, a binary indicator, True or False,\ndepending on whether the state $h_t = q_i$ is an accepting or\nrejecting state. Similarly, state $q_i$ encapsulates all the infor-\nmation pertinent to the current state of the state machine,\nserving as the central hub for processing reasoning and cu-\nmulative memory. In contrast, $o_t$ contains only partial binary\ninformation derived from h up to time t.\nA state machine is inherently recurrent, as the transition\nfunction & acts as the recurrent function g in Equation 1 on\neach output h of such machine. Specifically, 8 takes one\nprevious computational state, i.e., $h_{t\u22121} = q_j$, and generates\nthe current state, denoted as $h_t = q_i = d(h_{t\u22121}, \u03c3\u03c4)$. Thus,\nfor any input string $x_t = 0_1\u00b7\u00b7\u00b7 5_t$, the final state $h_t = q_i$ is\nrecurrently derived by applying 8 to the previous state $h_{t-i} =\n q_j$ for each input token, i.e., $h_{t-i} = q_j = \u03b4(h_{t\u2212\u22121}, \u03c3\u03c4)$.\nFor instance, when processing the string '1234' on the state\nmachine depicted in Figure 2.b, the final state $h_4 = q_{acc}$ is\nderived by applying the transition function 8 recurrently to\nthe computational result $h_3 = q_{rej}$ of the preceding string\n'123'. And h3 is obtained in the same way from its preceding\nstring \"12\", and so on. Such recurrence is key to the power\nof a state machine, as computation can be continued for\nas many times as there are symbols in the input string $x_t$,\nusing the same function $g(\u00b7) = \u03b4(\u00b7)$ and previous state h.\nHowever, if we change the transitional function 8 by having\nit take $ot \u2208 {True, False}$as input, computation might\nnot progress correctly as True or False does not provide\nenough information for certain tasks to proceed to the next\nstate when next token comes in."}, {"title": "2.3 Time and Depth Complexity", "content": "To illustrate the distinct roles of recurrence and autoregres-\nsion within a given neural model, we apply two complexity\nmetrics in the reasoning process: time complexity and depth\ncomplexity. Time complexity measures the total computa-\ntional operations executed to process an input of length n\nutilizing the said model. In contrast, depth complexity mea-\nsures the number of sequential steps, after considering all\nparallel processing that a model performs, to process input x.\nDepth complexity highlights the longest chain of dependent\nsteps rather than the cumulative count of computational steps.\nBoth complexities are quantified using the Big O notation.\nDifferent models exhibit varying complexities during the\nprocessing of inputs, based on their design. Nevertheless,\neach task has an inherent minimum complexity (lower bound)\nnecessary for solving it. Models falling below this threshold\nare incapable of solving the task. For instance, multiplying\ntwo n-bit numbers requires a minimum of $O(n log n)$ (Af-\nshani et al. 2019) time complexity, representing the total\nnumber of floating point operations needed for an input of\nlength n and at least $O(log n)$ depth complexity due to the\npossibility of parallelizing the multiplication of individual\ndigits. The only sequential dependency arises in the subse-\nquent addition of digits, which requires log n sequential steps\nif each pair of additions is performed simultaneously. An-\nother example pertains to modeling a chess game with n\ninput moves, which requires O(n) depth complexity, as each\nboard state calculation depends on both the current move and\nthe previous state, and such dependency does not admit any\nparallelization. Models which exhibit lesser depth complexi-\nties for given input of length n, like Transformers, are thus\nill-suited for, i.e., incapable of tasks mentioned above, as we\nwill show."}, {"title": "2.4 Memorization in Neural Network", "content": "In practical applications, the effective depth c of matrix mul-\ntiplication WX is not exactly 1. This is due to large matrix\nmultiplications combined with nonlinear functions, which\ncan approximate complex functions and \u201cmemorize\u201d map-\nping results of computations requiring multiple sequential\nsteps. For instance, results from multiplying large numbers\ncan be memorized during training and retrieved via WX\nin a single parallel computation, circumventing the typical\ndependencies. Hence, the constant c is proportional to the\nmatrix size d, denoted as:\n$c\\times d = O(1)$ (6)\nThe size of matrix d in a neural network is influenced by\nthe dimensionality of W and the precision of its floating-\npoint numbers. Increased dimensionality and precision allow\nfor greater information storage. If precision were infinite,\nboth the time and depth complexities of WX could theoret-\nically become infinite, transforming the matrix into a vast\nlookup table through pure memorization.\nHowever, with finite precision, merely storing the mapping\nresults for specific tasks \u2013 such as the outcomes of certain\nnumber multiplications \u2013 does not truly \"solve\" the task, as\nthere exist larger input instances that exceed the matrix's\nmemorization capacity. While memorization eliminates the\nnecessity for recurrent computation and depth iteration for\nthe memorized task instances, it often falls short in effectively\nsolving tasks and demands exponentially more space."}, {"title": "3 CoT + Autoregressive ~ Recurrent", "content": "In this section, we show how recurrence enhances the com-\nputational capabilities of neural networks. A network with\ninfinite precision ($d \u2192 \u221e$) could theoretically handle com-\nputations of infinite depth complexity by serving as a com-\nprehensive lookup table for all feasible mappings. However,\nthis ideal scenario is impractical. Motivated by this, our focus\nshifts to the practical setting of networks with finite precision.\nFurthermore, we explore the concept of chain-of-thought\n(CoT) (Wei et al. 2022) prompting as an approximation of\nrecurrence within the domain of LLMs. Our analysis is situ-\nated within the realm of computability, delineating the upper\nbound of a model's computational capacity dictated by its ar-\nchitecture. We note that the impact of optimization techniques"}, {"title": "3.1 Role of Recurrence in Computability", "content": "To show the role of recurrence in neural models, we examine\nthe computational complexity exhibited by recurrent models\n(e.g., RNNs) as opposed to non-recurrent models (e.g., MLPS\nand Transformers).\nA Multi-Layer Perceptron (MLP) (Popescu et al. 2009)\nis not recurrent, as it processes input of a fixed size and\ntraverses through layers sequentially in a single iteration. An\nMLP consists of m layers, with each layer parameterized\nby matrix W(i), and performs matrix multiplication on the\noutput from the previous layer h(i-1):\n$h^{(i)} = \u03c3(W^{(i)}h^{(i-1)})$ (7)\nwhere o denotes a non-linear function applied independently\nto each value of the resultant vector. This formulation di-\nverges from the recursive definition in Equation 1, as each\nlayer represents a distinct function parameterized by differ-\nent $W^{(i)}$ utilized only once in the forward pass rather than\nrecurring upon itself.\nAs previously demonstrated, each WX operation in an\nMLP entails a depth complexity of O(1), so an MLP with\nm layers has a cumulative depth complexity of $O(1) \u00d7 m =$\nO(m) as layers are computed sequentially one after another.\nHowever, this complexity simplifies to O(1) because m re-\nmains constant for a given MLP, irrespective of the input\nlength n. This inherent limitation hinders MLPs from ef-\nfectively addressing tasks like complex computations (e.g.,\nmultiplying large numbers) or string manipulations, requiring\ngrowing depth.\nIn contrast, an RNN (Medsker, Jain et al. 2001) (with m\nlayers) modifies an MLP by integrating recurrent connec-\ntions over the MLP itself. Specifically, the output from the\nlast MLP layer, h(m), loops back as input for subsequent com-\nputations within the same RNN. Given an input sequence of\nn elements, denoted as $x_n = (x_1, x_2, , x_n)$, computations\noccur sequentially on each $x_i$, expressed at time t as:\n$h^{(m)} = (W_1h^{(m)} + W_2x_t)$\nThis can be simplified to:\n$h^{(m)} = g_0(h^{(m)}, x_t)$ (9)\nHere, ge signifies the function encapsulated by the RNN's\nMLP. This computation aligns with the definition of recur-\nrence in Equation 1 where the same model function ge itera-\ntively operates upon itself. Given that each application of a\ngiven MLP represents a depth of O(m) = O(1), sequential\napplication extends the depth complexity to O(n), with n\nindicating the input length.\nThe Transformer (Vaswani et al. 2017), despite its prowess\nin language modeling, does not exhibit recurrent structure\nand has a limited depth. For an input sequence of length\nn, the Transformer employs an attention mechanism which\ncomputes key (k), query (q), and value (v) vectors for each\ninput token xi before they attend to each other for information"}, {"title": "3.2 Role of Autoregressive in Computability", "content": "As demonstrated previously, autoregression is not a substitute\nfor recurrence in the computational process. Unlike a recur-\nrent process, where the computed state h is re-entered into\nthe model as input, an autoregressive model condenses the en-\ntire computational state h into a single token o and augments\nthe input with o. For example, consider simulating a chess\ngame with a sequence of n actions $x_n = (X_1,X_2,\u2026\u2026,X_n)$.\nThe computational state h must encode the board informa-\ntion at each step to avoid having to resort to memorization.\nAn autoregressive model does not pass this calculated hid-\nden state h into the next calculation. Instead, the next chess\nmove o is derived from ht, and this token o is reintroduced\ninto the model, resulting in a new input augmented sequence\n$X_{n+1} = (X_1, X_2, \u2026\u2026\u2026, X_n, o_1)$. That is, the autoregressive pro-\ncess extends the input sequence by appending the newly\nderived token to the original input. However, this does not\nenhance depth complexity since it does not alter the model\nstructure but only the input. Because the tokens o generally\ndo not encode enough computational information, reasoning\nfor the next move $o_2$ must start from scratch, unlike leverag-\ning ht from the previous step in a recurrent process.\nTherefore, autoregression preserves the original model's\ndepth complexity while increasing the time complexity as\nmore computations are performed on the extended input."}, {"title": "3.3 Role of CoT in Computability", "content": "Large language models (LLMs) are autoregressive models\nthat utilize condensed outputs, $o_t$, derived from hidden states\n$h_t$ for subsequent computations. Natural language is a pow-\nerful medium for encoding various kinds of information.\nSpecifically, the Chain of Thought (CoT) approach employs\na sequence of natural language tokens, $(o_1, o_2, o_3, ..., o_k)$,\nto form sentences that encode intermediate computational\ninformation from the hidden state h. This behavior is repre-\nsented as $h_t^{(m)} \u2192 o_{1:k}$, where \u2192 \" denotes discretizing\nand encoding the computation state information into string\nformat.\"\nIn subsequent computations, instead of solely using\nthe task-related input $x_n = (x_1,...,x_n)$, the encoded\nCoT strings are appended to form a new input $X_{n+k} =$\n$(x_1,..., x_n, o_1, o_2,\u00b7\u00b7\u00b7, o_k)$. When this input is fed to the\nmodel, $o_{1:k}$, which encodes $h_t^{(m)}$, is converted back to the\nhidden vector, denoted as $o_{1:k} \u2192h_{n+k}$. Since this string\nencodes the computational state represented by $h_t^{(m)}$, con-\nverting it back to the hidden state allows the model to directly\nutilize it to continue the computation from the recovered\n$h_{n+k}^{(m)}$, rather than reasoning from the beginning. The entire\nCoT process can be represented as:\n$h_{n+k}^{(m)} \u2192 o_{1:k} \u2192 h_{n+k}^{(1)}$ (14)\nThus, the autoregressive process in CoT simulates the miss-\ning recurrent connection by iteratively encoding the compu-\ntation state into strings and decoding the strings back to the\ncomputation state. Assuming CoT performs T(n) steps of\nthe above conversion for a given task instance $I_n$, the time\ncomplexity is enhanced to $O(n + T(n))$ through the autore-\ngressive process during CoT, with a depth complexity of\n$O(T(n))$ attained through simulated recurrence from such\nstring-vector conversions.\nFor example, in a chess-playing scenario, the computa-\ntional state h, which encodes the board information, is con-\nverted to descriptive strings detailing the current chessboard\nconfiguration in the CoT process. Contrast that with directly\noutputting the next action as done by a non-CoT-based in-\nference. The board description $o_{1:k}$ can then be used by the\nCoT-based model to resume the computation, bypassing the\nneed to compute from scratch by using only previous actions\nas input. An illustration of this process and how CoT approx-\nimates recurrent connections like RNN is shown in Figure\n3."}, {"title": "3.4 Role of CoT Variants in Computability", "content": "As the naive CoT simply uses the prompt \"Think Step by\nStep\" and guides the model to output the reasoning state ht\ninto a sequence of natural language tokens ($o_1, o_2,\u2026\u2026\u2026, o_k)$,\nit might not effectively convert all useful computational in-\nformation from h to $o_{1:k}$. Therefore, different CoT variants\nhave been proposed, and we discuss how these different CoT\nmethods affect the reasoning process and the model's com-\nputability.\nTree of Thought (ToT). Instead of outputting a single rea-\nsoning sequence $o_{1:k}$, ToT encourages the model to output\nmultiple possible reasoning paths simultaneously. We denote\nthe i-th reasoning path as $o_{1:k}^{(i)}$. Each path describes a differ-\nent possible CoT reasoning logic to solve the problem. Then,\nwe evaluate each one of them before expanding the reasoning\non the most promising top L paths independently. Similarly,\nall reasoning paths are obtained through $h_t^{(m)}$ and this can\nbe represented as:\n$h_t^{(m)} \u2192(o_{1:k}^{(1)},o_{1:k}^{(2)}, ... ,o_{1:k}^{(L)}) \u2192(h_{n+k_1}^{(1)}, h_{n+k_1}^{(2)},..., h_{n+k_L}^{(L)} )$ (15)\nAs we can see, even though there might be L different reason-\ning paths, each path performs reasoning steps independently\nand conforms to our previous analysis of CoT. Each path ex-\ntracts different reasoning (computational) information from\nh and then discretizes the hidden computation into strings\nbefore converting these strings back to h. During this process,\neach path approximates recurrence on its own. Assuming the\nlongest reasoning path in ToT performs T(n) steps of CoT,\nthe depth complexity of ToT will be n + T(n), the same as\nCoT. Therefore, ToT does not increase the depth complexity\nbeyond that of CoT but improves the conversion of h \u2192 o\nby encoding multiple possible reasoning solutions.\nSince h cannot be directly passed to the next step as in a\nrecurrent model, ToT explicitly extracts all possible solutions\nencoded in h and further expands on them. For complex\ntasks, this can be helpful as some require searching rather\nthan simple one-directional reasoning, and a single reasoning"}, {"title": "3.5 Autoregressive + CoT ~ Recurrent\nHolds Only in Language Models", "content": "An implicit prerequisite for mimicking recurrence using\nChain of Thought is that the tokens $o_{1:k}$ must be expres-\nsive and universal enough to encode all types of information,\nincluding reasoning states, state memories, and intermediate\ncomputational results. Natural language is posited to be pow-\nerful enough to encode all sorts of information using natural\nlanguage tokens. From chess boards and programs to data\nstructures and computational graphs, strings can effectively\nencode them all in meaningful values.\nHowever, this does not hold true for certain non-natural\nlanguage-based large models. For instance, protein language\nmodels that use 20 amino acids as tokens (Lv et al. 2024) can-\nnot effectively convert hidden representations h into mean-\ningful representations with amino acid tokens, as these to-\nkens can only encode limited, rather than universal, infor-\nmation. Similarly, a pretrained chess model cannot perform\nautoregressive-based recurrent reasoning because it only has\ntokens representing chess moves, lacking the ability to con-\nvert h into descriptions of the chessboard."}, {"title": "4 Experiments", "content": "While we have highlighted the critical role of recurrence and\nthe mechanisms of Chain of Thought (CoT), quantifying the\nChomsky hierarchy-aligned computability of CoT-enhanced\nLLMs remains challenging. This involves analyzing memory"}, {"title": "4.1 Experiment Design", "content": "Model Choice. The goal of our work and experiments is\nnot to evaluate and compare the performance of different\nLLMs. Instead, our aim is to demonstrate the role of CoT in\napproximating recurrence and show the improved computa-\ntional power of incorporating recurrence. Specifically, our\nwork focuses on the upper limit of the model's computational\npower based on architectural designs. Other factors such as\noptimization, training efficacy, and tokenizer choices are be-\nyond the scope of this investigation. Therefore, we choose\nthe best-performing model available to us, GPT-4 (Achiam\net al. 2023), to cater to this purpose. Detailed model usage\nand prompt examples are shown in the Appendix.\nTasks Choice. We follow previous work on the empiri-\ncal analysis of the expressiveness of neural expert mod-\nels (Del\u00e9tang et al. 2023) and adopt their task settings. Tasks\nare divided into three computational levels: Regular (R),\nwhich requires machines equivalent to or more powerful than\na DFA; Context-Free (CF), solvable by Pushdown Automaton\n(PDA); and Context-Sensitive (CS), requiring linear-bounded\nAutomaton (LBA).\nTask input format can significantly influence LLM per-\nformance. For example, LLMs often mistakenly infer \"9.9\n< 9.11\" or count characters incorrectly due to suboptimal\nsplitting during text tokenizing. To minimize these effects,\nwe redesigned the tasks. Task instances like \"aababababa\" for\nstring reversing are replaced with list reversing, e.g., of [\"ap-\nple\", \"monkey\", \"apple\", \u00b7\u00b7\u00b7 ], as word like \"apple\" remains\na single token in modern tokenizers. We also limit task length\nto avoid issues with long context access and cross-session\nproblems in prompting. Detailed task designs, length sam-\npling, and example inputs/outputs are given in the Appendix."}, {"title": "4.2 Results", "content": "The experiment results for LLM without and with CoT with\nare appended to the expert model's performance from pre-\nvious work (Del\u00e9tang et al. 2023) in Table 2. As we can\nsee, all recurrence-augmented models can solve tasks in\nthe regular (R) category. This includes true recurrence mod-\nels such as RNN, Stack-RNN (Joulin and Mikolov 2015),\nTape-RNN (Del\u00e9tang et al. 2023), and LSTM, as well as\napproximated-recurrence using CoT-based LLMs. Specifi-\ncally, the accuracy for R tasks is nearly 100% for all such\nmodels. In comparison, non-recurrent models, whether ex-\npert (trained for a specific task) or general-purpose LLM,\nstruggle with R tasks. The accuracies on R tasks for Trans-\nformer expert models are far from ideal (20-60% accuracy)\ncompared to RNN (100%), with Transformer-based LLMS\n(without CoT) performing even worse.\nThis further solidifies the complexity analysis shown in\nTable 1. Specifically, all recurrent-based models, including\nCoT, possess a depth complexity greater than DFA, which is"}, {"title": "5 Recurrent Transformer", "content": "Recurrence is crucial in the reasoning process, sparking\nextensive research into integrating recurrent features into\nTransformer architectures. This section explores various de-\nsigns for embedding recurrence into Transformer models\nand proposes two categories: Recurrence-Complete (RC) and\nRecurrence-Incomplete (RI). Figure 4 provides an overview\nof all discussed models."}, {"title": "5.1 Recurrence-Complete (RC) Models", "content": "A model is said to be recurrence-complete if it can repre-\nsent any recurrent function as specified in Equation 1. We\nfirst illustrate how recurrence-completeness is achieved using\nthe simplest recurrent network, RNN, and then extend this\nanalysis to Transformer-based RC models.\nAs demonstrated in Equation 9, RNNs model the recurrent\nfunction\u00b9 $h_t = g_0(h_{t\u22121})$ by recursively taking the previ-\nous output h as the model's input (Figure 4(b) and Figure 5,\nleft). Given that the function go, parameterized by the RNN\nnetwork, incorporates both linear and nonlinear activation\nfunctions, by the Universal Approximation Theorem (Cy-\nbenko 1989), for any given (one term) recurrent function g',\nwe have \u2200\u20ac > 0, \u22030 : |g\u2032(h) -ge(h)| < \u20ac. In other words, the\nmodel-encoded function ge can infinitesimally approximate\nor simulate any function g' such that $h_t = g'(h_{t\u22121})$, to an\narbitrary degree of precision. Therefore, RNNs possess the\ncapability to simulate any one-term recurrent function."}, {"title": "5.2 RC Transformers", "content": "Standard Recurrent Transformer. The Standard Recurrent\nTransformer (Yang et al. 2022) integrates recurrent connec-\ntions of h with the original attention mechanism, as depicted\nin Figure 4c. At each time step t, the computation of the first\nlayer's key (k), query (q), and value (v) incorporates not\nonly the current input xt but also the output hidden vector\nfrom the previous time step h(m):\n$k_t^{(1)}, q_t^{(1)}, v_t^{(1)} = W_{k,q,v}(x_t + h_t^{(m)})$ (16)\nThe subsequent layers retain the standard attention mech-\nanism in the standard Transformer. Since each input xt is\nenhanced by the previous h(m), the output of the final h at\nthe current time step t is a function of both x1:t due to the"}, {"title": "5.3 Recurrent-Incomplete (RI) Models", "content": "Some Transformer variants, though described as \"recurrent,\u201d\ndo not fully model the general recurrence function as delin-\neated in Equation 1. These models leverage the recurrence\nconcept to streamline complex attention computations by it-\nerating over intermediate results. This modification avoids\nthe need for recalculating attention from time step 1 to t\nat each iteration, significantly reducing the time complexity\nof the attention mechanism during inference and improving\nefficiency. However, this approach neither enhances depth\ncomplexity nor achieves genuine recurrence modeling.\nSpecifically, such models recurrently update previous atten-\ntion aggregations and store them for the next attention com-\nputation. However, the recurrent variable is updated through\na fixed \"shifting operation\" rather than a learned function\nby the model itself (Figure 5), thus only mimicking linear\nrecurrent relations."}, {"title": "5.4 RI Transformer", "content": "RWKV. As opposed to the standard attention mechanism", "layer": "n$k_t^{(i)"}, "v_t^{(i)} = W_{k,v} h_{t}^{(i-1)}$ (20)\n$h_t^{(i)} = RWKVLinearAttn(k_t^{(i)}, v_t^{(i)})$ (21)\n$= \\frac{\\sum_{j=1}^{t}e^{-(t-1-j)w+k_j^{(i)}}v_j^{(i)} + e^{u+k_t^{(i)}}}{\\sum_{j=1}^{t}e^{-(t-1-j)w+k_j^{(i)}} + e^{u+k_t^{(i)}}}$ (22)\nwhere w and u are constant vectors.\nSimilar to the standard attention function, directly apply-\ning RWKVLinearAttn using vector values k and v is com-\nplex due to its dependence on v, k values from steps 1 to t.\nHowever, since RWKVLinearAttn removes the non-linear\nrelations between pairs of k and q in the standard atten-\ntion, he can now be reformulated recursively using only the\nintermediate results from the (t \u2013 1)-th step, significantly\nstreamlining the function. At each time step t, RWKV stores\ntwo intermediate values: $a_t^{(i)} = \\sum_{j=1}^{t-1} e^{-(t-1-j)w+k_j^{(i)}}$\nand $b_t^{(i)} = \\sum_{j=1}^{t-1} e^{-(t-1-j)w+k_j^{(i)}}$, enabling the calculation\nof $h_t^{(i)}$ using solely $a_{t-1}^{(i)}$ and $b_{t-1}^{(i)}$ from the previous time\nstep as follows:\n$h_t^{(i)} = \\frac{a_{t-1}^{(i)} + e^{u+k_t^{(i)}}v_t}{b_{t-1} + e^{u+k_t^{(i)}}}$ (23)\nThis way, ht is no longer dependent on values from all time\nsteps 1 to t as in Equation 22, but only on values from steps\nt-1 and t. Values at and bt are stored and updated at each\ntime step for each layer i as follows:\n$a_t^{(i)} = za_{t-1}^{(1} + g'(x_t)$ (24)\n$b_t^{(i)} = zb_{t-1}^{(1} + g''(x_t)$ (25)\nwhere z is a constant value $e^{-w}$, referred to as the posi-\ntional shift. The functions go and g\" are represented by the\nith network layer, using the network's weights W for their\ncomputations: $g'(x_t) = e^{k_tv_t}$ and $g''(x_t) = e^{k_t}$. Here, the\ncalculations for a and b in Equations 24 and 25 are indeed re-\ncurrent,"]}