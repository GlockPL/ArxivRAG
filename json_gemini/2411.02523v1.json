{"title": "Evaluating the Impact of Lab Test Results on Large Language Models Generated Differential Diagnoses from Clinical Case Vignettes", "authors": ["Balu Bhasuran", "Qiao Jin", "Yuzhang Xie", "Carl Yang", "Karim Hanna", "Jennifer Costa", "Cindy Shavor", "Zhiyong Lu", "Zhe He"], "abstract": "Differential diagnosis is crucial for medicine as it helps healthcare providers systematically distinguish between conditions that share similar symptoms. This study assesses the impact of lab test results on differential diagnoses (DDx) made by large language models (LLMs). Clinical vignettes from 50 case reports from PubMed Central were created incorporating patient demographics, symptoms, and lab results. Five LLMs\u2014GPT-4, GPT-3.5, Llama-2-70b, Claude-2, and Mixtral-8x7B were tested to generate Top 10, Top 5, and Top 1 DDx with and without lab data. A comprehensive evaluation involving GPT-4, a knowledge graph, and clinicians was conducted. GPT-4 performed best, achieving 55% accuracy for Top 1 diagnoses and 60% for Top 10 with lab data, with lenient accuracy up to 80%. Lab results significantly improved accuracy, with GPT-4 and Mixtral excelling, though exact match rates were low. Lab tests, including liver function, metabolic/toxicology panels, and serology/immune tests, were generally interpreted correctly by LLMs for differential diagnosis.", "sections": [{"title": "Introduction", "content": "Accurate diagnosis is critical for the effective management of patients' conditions, as it directly influences treatment decisions and overall patient outcomes\u00b9. A correct diagnosis ensures that patients receive timely and appropriate interventions, which not only improves outcomes but also reduces morbidity and mortality. Moreover, a correct diagnosis enables healthcare providers to select the most effective therapies, minimizing the risks associated with unnecessary or inappropriate treatments. By reducing diagnostic errors, accurate diagnosis streamlines patient care, eliminating the need for excessive or repeated testing, and ultimately lowering healthcare costs through reduced hospital stays and unnecessary procedures. Furthermore, patient safety is enhanced when accurate diagnoses mitigate the risks of complications and adverse drug reactions.2,3.\nIn contrast, differential diagnosis (DDx) plays a crucial role in enhancing clinical decision- making by systematically evaluating and ruling in/out potential conditions. The prediction of differential diagnoses provides several advantages, such as improving the reliability of clinical data, supporting the discovery of new treatments, and fostering a better understanding of diseases. Differential diagnosis also serves as an essential learning tool for medical students and healthcare professionals, enhancing their diagnostic skills. Its application spans a wide range of diseases including Alzheimer's disease\u2074, multiple sclerosis, inflammatory bowel disease colitis, epilepsy7, strokes, and others.\nClinicians often rely on their expertise and various case presentations to achieve diagnostic excellence. Differential diagnosis has long been a necessary step in clinical settings, prompting the development of earlier systems such as differential diagnosis generators and symptom checkers10. High-performing deep learning models have also been created for generating DDx in various specialties, including radiology\u00b9\u00b9, ophthalmology12 and dermatology13. However, these systems face significant issues: they often require structured data, lack the ability to provide valid reasoning for differential diagnoses, and do not have any interactive capabilities. The recent emergence of large language models (LLMs) such as OpenAI's Generative Pretrained Transformers (GPT-4)14 offers significant potential for developing tools that aid in generating accurate DDx. Previous studies have shown that LLMs can generate DDx with satisfactory performance."}, {"title": "Results", "content": "We evaluated the impact of laboratory test results on improving the accuracy of differential diagnosis using five large language models: GPT -414, GPT -3.523, Llama-224, Claude225, and Mixtral26. Clinical case reports for this assessment were obtained from the PMC-Patients dataset. From 50 selected case reports, we manually generated clinical vignettes that included details such as patient age, gender, symptoms, laboratory test results, and other relevant information, allowing the models to generate differential diagnosis responses. The clinical vignettes used for this study is described in detail in the Methods section (see Methods: Clinical Vignettes). A specific prompt was designed to instruct the models to consider all relevant details and provide differential diagnoses, including Top 1, Top 5, and Top 10 DDx lists. Model predictions were reviewed by clinicians and automatically evaluated using a knowledge graph and GPT-4, utilizing exact match, relevance, and incorrect predictions. The diagnostic accuracy was evaluated using accuracy and lenient accuracy metrics for Top 1, Top 5, and Top 10 DDx, derived from clinical vignettes with and without laboratory test data. Accuracy is calculated by assigning weights of 1.0 for exact matches, 0.5 (or 0.75 for lenient accuracy) for relevant matches, and 0.0 for incorrect matches, with the sum divided by the total of 50 diagnoses evaluated. For the evaluation metrics used, see Methods: Evaluation of Differential Diagnosis Lists. An overview of the study pipeline is presented in Fig. 1."}, {"title": "Stage 1: Evaluation of 300 Differential Diagnoses by Clinicians, Knowledge Graph, and GPT-4", "content": "This evaluation aimed to determine whether automated evaluations by BKG and GPT4 aligned with clinician evaluations and was carried out through four different scenarios: GPT-4 vs. Clinicians, GPT-4 vs. BKG, Clinicians vs. BKG, and Clinicians vs. the combined GPT-4+KG (BKG-GPT). Table 3 provides a detailed comparison of the agreement and disagreement percentages across four different evaluation scenarios involving five LLMs.\nIn the first scenario (GPT-4 vs. Clinicians), predictions from LLMs were evaluated by comparing their outputs with both GPT-4 and clinician evaluations. The results highlight varying degrees of alignment. Claude-2 achieved an alignment percentage of 75% with GPT-4 and clinicians, showing a relatively high alignment with a variance percentage of 25%. GPT-3.5 had a slightly lower alignment percentage of 71.67%, with a variance of 28.33%. GPT-4, when compared with clinicians, demonstrated a 73.33% alignment percentage and a variance of 26.67%. LLaMa- 2 had a lower alignment percentage of 66.67% and a variance of 33.33%, indicating more divergence from clinician evaluations. Mixtral's alignment percentage was 73.33%, identical to GPT-4, with a variance of 26.67%. The average alignment percentage for GPT-4 and clinicians across these LLMs was 72%, with a variance of 28%.\nIn the second scenario (GPT-4 vs. BKG), the predictions of the LLMs were compared between GPT-4 and the BKG predictions. The alignment percentages in this context varied. Claude-2 showed a moderate alignment percentage of 65%, with a variance percentage of 35%. GPT-3.5 performed the best in this scenario, achieving the highest alignment percentage of 86.67% and a variance of 13.33%. GPT-4 also showed strong alignment with BKG evaluations, with an alignment percentage of 78.33% and a variance of 21.67%. LLaMa-2 had the lowest alignment percentage at 56.67%, indicating more substantial divergence from BKG evaluations, with a variance percentage of 43.33%. Mixtral had an alignment percentage of 68.33% and a variance of 31.67%. The average alignment percentage for GPT-4 and KG was 71%, with a variance of 29%.\nIn the third scenario (Clinicians vs. BKG), Claude-2 achieved an alignment percentage of 80%, with a variance percentage of 20%, showing strong alignment between clinician and BKG evaluations. GPT-3.5 had a slightly higher alignment percentage at 81.67%, with a variance of 18.33%. GPT-4 performed the best in this scenario, with the highest alignment percentage of 91.67% and a variance of 8.33%, indicating very close alignment between GPT-4's predictions and BKG evaluations. LLaMa-2 showed a lower alignment percentage of 73.33%, with a variance of 26.67%. Mixtral had a strong performance with an alignment percentage of 85% and a variance percentage of 15%. The average alignment percentage between clinicians and BKG across the LLMs was 82.33%, with a variance of 17.66%. In all scenarios, \"agreement\" refers to the match between the LLMs' predictions and the evaluations from GPT-4, clinicians, and BKG, while \u201cdisagreement\u201d reflects mismatches in prediction output evaluations.\nIn the final scenario (Clinicians vs BKG-GPT) a combined mean score was then used to classify predictions into one of three categories: Exact Match, Relevant, or Incorrect. An Exact Match was defined as a mean score of 0.75 or 1, indicating high alignment with the correct diagnosis. A Relevant prediction, with a score of 0.25 or 0.5, indicated partial accuracy and some useful information. An Incorrect prediction, scored at 0, indicated a failure to provide an accurate diagnosis by both models. The results demonstrated that this approach yielded the highest accuracies across all models. Claude-2 achieved 83.33% accuracy, while GPT-3.5 reached 86.67%. GPT-4 had the highest accuracy overall at 93.33%, showcasing exceptional alignment with clinician judgments. LLaMa-2-70b had an accuracy of 80%, showing significant alignment but lower than GPT-3.5 and GPT-4. Mixtral-8x7B performed similarly to GPT-3.5 with 86.67% accuracy. Overall, this scenario highlighted strong performance across models, particularly GPT- 4, in aligning with clinicians' evaluations. The average match count was 86, with an average mismatch count of 14 across the models.\nThis analysis reveals that GPT-4, particularly when combined with BKG, consistently shows the highest alignment percentages with clinician evaluations, achieving the strongest performance across most scenarios. GPT-3.5 also performs well, especially in the BKG-related evaluations. LLaMa-2 consistently shows lower alignment percentages, indicating less alignment with both clinician and BKG evaluations. Mixtral generally performs well, particularly in the combined GPT-4 and BKG evaluation context. This detailed assessment underscores the utility of integrating predictions from LLMs like GPT-4 with BKG evaluations to improve alignment in medical decision-making contexts."}, {"title": "Stage 2: Evaluation of 1500 Differential Diagnoses by Biomedical Knowledge Graph+GPT- 4", "content": "Based on the inference from the previous step, we evaluated all 1500 predictions from five LLMs using a combination of Biomedical Knowledge Graphs and GPT-4 (BKG-GPT). Table 4 provides a detailed performance comparison of LLMs in generating differential diagnoses with and without laboratory data. The table provides detailed insight into the performance of five language models (Llama-2, Claude-2, Mixtral, GPT-3.5, and GPT-4) across three scenarios: Top 1, Top 5, and Top 10 differential diagnoses. Metrics include Exact Match, Relevant diagnoses, Incorrect diagnoses, Accuracy, and Lenient Accuracy. Table 2 presents the results for both scenarios with and without lab data.\nGPT-4 generally performed the best across multiple scenarios, particularly in terms of lenient accuracy. It achieved the highest lenient accuracy for the Top 1 (74.5%), Top 5 (78.5%), and Top 10 (80%) differential diagnoses with lab data. These results highlight GPT-4's strong ability to generate relevant diagnoses across the differential lists. Mixtral also demonstrated strong performance, particularly in the Top 5 and Top 10 scenarios with lab data. For the Top 5 diagnoses, Mixtral achieved an accuracy of 60% and a lenient accuracy of 80%. In the Top 10 diagnoses, it maintained an accuracy of 58% with a lenient accuracy of 79%, showing consistently high performance across larger diagnosis lists. When comparing the top two performing LLMs, GPT- 3.5 showed notable success in generating Top 5 differential diagnoses with lab data, where it achieved the highest lenient accuracy of 77% and a solid accuracy of 54%. This reflects a very low error rate in providing relevant diagnoses within the top 5 predictions, demonstrating its capability in handling key clinical scenarios. GPT-3.5 was also the only model to achieve more than 50% accuracy across all three DDx scenarios. Claude-2 and LLaMa-2 displayed comparable performances overall, though Claude-2 had a slight edge over LLaMa-2 in several metrics. For instance, Claude-2 had a higher exact match rate in the Top 1 differential diagnosis (5 exact matches vs. LLaMa-2's 3) with lab data. Additionally, Claude-2 achieved higher accuracy and lenient accuracy in the Top 5 and Top 10 scenarios. Specifically, Claude-2's Top 5 diagnoses had an accuracy of 58% and a lenient accuracy of 79%, while in the Top 10, it maintained an accuracy of 58% and a lenient accuracy of 79%.\nFor Top 1 DDx with lab data, GPT-4 had the highest exact match rate, achieving 8 exact matches, while Mixtral followed with 7, and Claude-2 with 5. LLaMa-2 predicted 3 exact matches, and GPT-3.5 lagged with 2. Without lab data, GPT-4 achieved 1 exact match, while Claude-2, Mixtral, and GPT-3.5 did not achieve any exact matches. LLaMa-2 also did not predict any exact matches in this scenario. For Top 5 DDx with lab data, Mixtral performed the best, achieving 10 exact matches, while Claude-2 followed closely with 8. GPT-4 achieved 7 exact matches, while GPT-3.5 and LLaMa-2 both predicted 4 exact matches. Without lab data, GPT-4, Claude-2, and Mixtral achieved 2 exact matches each, while LLaMa-2 and GPT-3.5 had 2 and 1 exact matches, respectively. Finally, for Top 10 DDx with lab data, GPT-4 had the highest exact match rate, achieving 10 exact matches. Claude-2 and Mixtral both followed with 8 exact matches, while LLaMa-2 and GPT-3.5 achieved 4 and 3 exact matches, respectively. Without lab data, Claude-2 led with 4 exact matches, GPT-4 followed with 2, and Mixtral and GPT-3.5 both had 2 and 3 exact matches, respectively. LLaMa-2 predicted 3 exact matches in this scenario. The ability to achieve a high exact match rate in differential diagnosis is crucial, as it directly reflects the model's capacity to provide the most accurate and relevant diagnosis, significantly enhancing clinical decision- making efficiency and reliability.\nIn summary, GPT-4 consistently showed the highest performance in generating differential diagnoses across various scenarios, particularly excelling in lenient accuracy. Mixtral followed closely, especially in the Top 5 and Top 10 lists, while GPT-3.5 stood out for its high lenient accuracy in the Top 5 diagnoses. Claude-2 slightly outperformed LLaMa-2, particularly in exact match rates and lenient accuracy across different differential diagnosis lists."}, {"title": "Error Analysis", "content": "The LLM models showed varying error rates (incorrect diagnoses). For Top 1 differential diagnosis with lab data, Llama-2 had the lowest error rate (1 incorrect diagnosis), while GPT-3.5 had the highest (8 incorrect diagnoses). The error rates were zero for Top 5 and 10 DDx among all LLMs with lab data showing the predictions have some meaningful connection with the final diagnosis. Without lab data, the error rates increased across all models. For instance, in the Top 1 scenario without lab data, GPT-4 and GPT-3.5 had 8 incorrect diagnoses, while Mixtral had 7 incorrect diagnoses. Exact match rates were relatively low across all models, indicating the difficulty of achieving an exact diagnosis match. For example, in Top 1 differential diagnosis with lab data, GPT-4 had the highest exact match rate (8), whereas others had lower exact match rates. Lenient accuracy rates were significantly higher than exact match rates, reflecting the models' ability to provide relevant but not exact diagnoses. This indicates that the models are better at providing useful differential diagnoses rather than pinpointing the exact one every time.\nThe sub-set of 300 differential diagnosis comparisons between the predictions made by LLMs and their evaluations by GPT-4, clinicians, and the BKG reveals nuanced insights into the accuracy and alignment of these models with clinical standards. The evaluation of LLM GPT-4 reveals that its predictions predominantly align with both clinician comments and BKG evaluations as \u201cRelevant\u201d or \u201cExact Match,\u201d demonstrating its reliability in producing clinically accurate information. In the case report (PMID 19162360, final diagnosis: diabetic nephropathy with near-nephrotic range proteinuria), GPT-4's prediction (diabetic nephropathy) was \u201cRelevant,\u201d and this assessment was supported by both clinician comments and BKG, indicating close alignment with clinical standards. For LLM GPT-3.5, a similar pattern of alignment is observed. Most of its \u201cRelevant\u201d predictions were consistently categorized as such by both GPT- 4 and clinicians, as seen in the example of PMID 23415437 (Final diagnosis: Coronary heart disease (CHD) caused by ApoA-INashua mutation VS GPT-3.5 diagnosis: Hypoalphalipoproteinemia (low HDL-C) secondary to a novel heterozygous A-1 in-frame insertion mutation). This indicates that GPT-3.5 frequently provides relevant diagnoses that align well with clinical evaluations. However, occasional discrepancies arise, highlighting the importance of integrating these LLM predictions with systematic data sources like BKG to enhance diagnostic accuracy.\nFor LLM Claude, the analysis shows a high degree of consistency between GPT-4's evaluations and clinician comments, particularly in scenarios where predictions were categorized as \u201cExact Match.\u201d For instance, in the case reports with PMID 31497118 (Final diagnosis: Drug- induced liver injury (DILI) from levetiracetam (LEV) vs Claude diagnosis: Drug-induced liver injury caused by levetiracetam(LEV)) and PMID 31497445 (Final diagnosis: Vancomycin-induced DRESS syndrome vs Claude diagnosis: Vancomycin-induced drug reaction with eosinophilia and systemic symptoms(DRESS) syndrome), Claude's predictions were marked as \u201cExact Match\u201d by both GPT-4 and clinicians, indicating strong alignment . This suggests that Claude's predictions align well with clinical judgments when they are accurate. Additionally, Claude LLM generated incorrect predictions, in the case of PMID 31380008 (Final diagnosis: AMAN subtype of Guillain- Barr\u00e9 syndrome vs Claude: Acute Hepatitis A Infection), Claude's prediction was deemed \u201cIncorrect\u201d by both GPT-4 and clinicians, further demonstrating consistency in identifying less accurate predictions. When comparing the evaluations across GPT-4, clinicians, and BKG for LLM Claude, most predictions categorized as \u201cRelevant\u201d were similarly evaluated as \u201cRelevant\" by all three evaluators, reinforcing the model's ability to produce clinically meaningful information. However, there are instances where \u201cExact Match\u201d outcomes were achieved, demonstrating that while Claude often aligns with clinician judgment, exact matches are rare but significant when they occur. The evaluations of LLaMA2 and Mixtral present more variability. For example, while both models show strong alignment with \u201cRelevant\u201d predictions, they tend to diverge more frequently when aiming for \u201cExact Match\u201d outcomes, suggesting less consistency in achieving the highest levels of diagnostic accuracy compared to Claude and GPT-3.5. Overall, the analysis underscores the importance of integrating LLM-generated predictions with systematic data sources like BKG and clinician evaluations to enhance diagnostic accuracy. While exact matches are rare across all models, the consistent identification of \"Relevant\" predictions highlights the utility of these models in providing clinically valuable insights that align well with clinician judgment and data-driven assessments.\nFor a subset of LLM predictions, we asked LLM to generate the reasoning behind each prediction and asked clinicians to evaluate it. Clinicians assessed whether the provided reasoning was logical and whether it supported the accuracy of the diagnosis. For the \u201cGuillain-Barr Syndrome following acute hepatitis E diagnosis\u201d, it correctly predicted, \u201cAcute hepatitis E complicated by hepatic encephalopathy and Guillain-Barr syndrome (GBS)\u201d. The model provided the reason for the prediction as: \u201cInitial and subsequent lab tests showed elevated liver enzymes ( alanine aminotransferase (ALT) and aspartate aminotransferase (AST)), and very high bilirubin levels, which are indicative of acute liver injury. The serology tests confirmed the presence of HEV antibodies (both IgM and IgG). The prolonged presence of IgM anti-HEV indicates a sustained immune response to the infection. The development of lower limb weakness progressing to severe muscle weakness represents a rare but recognized neurological manifestation associated with HEV infection. The rapid progression and the timing of these symptoms during the course of the liver disease further link them to the HEV infection.\u201d The reasoning was evaluated by the clinician as", "Aspergillus fumigatus infection with liver failure": "and the model predicted \u201cAcute liver failure complicated by invasive pulmonary aspergillosis", "Ondansetron (OSDN)-induced hepatotoxicity\", the model predicted \u201cGestational Hyperemesis compounded by a Drug-induced Hepatotoxicity and a Pregnancy-related Psychotic Disorder": "The reasoning provided was, \u201cThe patient's symptoms of persistent nausea, vomiting, and significant weight loss over four weeks are indicative of hyperemesis gravidarum, especially given their occurrence during pregnancy. The improvement in these symptoms post-delivery further supports this diagnosis, as hyperemesis gravidarum typically resolves after delivery. Drug-induced Hepatotoxicity: The marked elevation in liver enzymes (AST and ALT), which escalated during treatment and began to normalize post-delivery, suggests a drug-induced liver injury. Olanzapine, initially used, is known for potential hepatotoxic effects, which could explain the initial liver enzyme elevations\u201d. This was a wrong prediction and the clinician confirmed it with a comment \u201cthe LLM missed the resolution of symptoms and labs with the stopping of the medicine\u201d. For a correct prediction of \u201cThallium intoxication", "Acute thallium poisoning\" the Mixtral model provided a reason \u201cThe patient's plasma and 24-hour urine tests for heavy metals show a significantly elevated level of thallium in the urine (540 \u03bcg/g Cr) compared to the normal range (0.4-10 \u03bcg/g Cr) and a normal level of thallium in the plasma (12 \u00b5g/dL) which is consistent with the diagnosis of thallium poisoning. Thallium is a highly toxic heavy metal that can cause a wide range of symptoms, including gastrointestinal disturbances, neurologic symptoms, and liver damage": "The clinician commented that, \u201cThis is sound decision making, arriving at the right diagnosis. The details of labs made this clear\u201d.\nGenerally, the models were able to interpret the lab test results along with symptoms and medications administered. The models generated solid interpretations like, 'The lab test results show elevated total bilirubin, direct bilirubin, aspartate transaminase, alanine transaminase, prothrombin time, international normalized ratio, lactate dehydrogenase, and C-reactive protein, which can be indicative of liver damage and inflammation, which can be seen in heavy metal poisoning.' and the clinician commented as 'This is sound decision making, arriving at the right diagnosis. The details of the labs made this clear."}, {"title": "Case incidence", "content": "We conducted a search for the incidence of diseases corresponding to the final diagnosis in the 50 cases by reviewing the literature on PubMed (https://pubmed.ncbi.nlm.nih.gov/). The search results provided in Table 6 revealed a wide range of disease incidence, with the majority of cases being reported in fewer than 100 articles. Specifically, 22 diagnoses had 1-10 articles, and 13 diagnoses had 11-100 articles, highlighting the rarity of these conditions. Additionally, there were 10 diagnoses with 101-1000 articles, indicating these diseases are relatively uncommon. The search results show that 70% of the diagnoses (35 out of 50) have fewer than 100 articles in PubMed, emphasizing their rarity. In contrast, 20% of the diagnoses (10 out of 50) fall within the 101 to 1000 report range, suggesting these are somewhat more prevalent but still uncommon. Meanwhile, 8% of the diagnoses (4 out of 50) have more than 10000 articles, showing these are more frequently occurring or well-documented conditions in the literature. This highlights the rarity of the majority of the diagnoses, as 70% of them are reported in fewer than 100 articles. Since these are such rare conditions, LLMs must possess specific knowledge of these diseases to make accurate diagnosis predictions."}, {"title": "Discussion", "content": "This study evaluated the impact of lab test results on the accuracy of differential diagnoses using five LLMs with published clinical case reports. The results showed that including lab data improved both accuracy and lenient accuracy, with GPT-4 achieving the highest performance in generating relevant differential diagnoses, even if the exact match was not always achieved. The Mixtral-8x7B model also performed well, particularly with lab data, highlighting the advanced capabilities of these LLMs in processing complex clinical information. A detailed analysis of the 300 selected predictions evaluated using GPT-4, BKG and clinicians, along with their various combinations, revealed that GPT-4's predictions align significantly better with clinicians when relevant predictions are considered as correct. This improvement underscores the practical utility of GPT-4's predictions, even if they are not exact matches. Furthermore, the combination of GPT- 4 and BKG evaluations achieved the highest accuracy, indicating that integrating LLM-generated predictions with systematic data enhances the relevance and clinical utility of diagnostic predictions.\nThe high performance of GPT-4 (lenient accuracy: 74% - 80%) indicates a strong ability to provide relevant differential diagnoses even if the exact match is not achieved. The consistent performance (lenient accuracy: 71% - 80%) of Mixtral suggests that it is reliable in providing a broader set of relevant differential diagnoses. It is also worth mentioning that GPT-4 is the highest- performing LLM in predicting the exact diagnosis of 8 and 10 cases in the Top 1, and 10 DDx list and Mixtral predicted 10 exact cases in the Top 5 DDx respectively. GPT-4 excelled and achieved the best performance by predicting most of the relevant DDx list. Accuracy and lenient accuracy were generally higher when lab data was included, highlighting the importance of lab data in improving diagnostic accuracy. Incorporating lab data significantly enhances model performance, with GPT-4's accuracy increasing from 43% to 55% in the Top 1 scenario, representing a 12% improvement and 7 additional exact case diagnoses. GPT-4 stands out for its balanced performance across all scenarios, suggesting its robustness and reliability in clinical decision support. Mixtral's consistent performance in providing relevant diagnoses makes it a reliable option for scenarios where exact matches are less critical. Claude-2 and Llama-2, while slightly behind GPT-4 and Mixtral, still show competent performance, particularly when lab data is available.\nClinicians generally observed that model performance varied based on case complexity and lab requirements. One clinician commented that in the simpler case, with a final diagnosis of \"diabetic nephropathy with near-nephrotic range proteinuria\" (PMID: 19162360) most models missed the near-nephrotic proteinuria, and the relevance of differentials decreased as the number of diagnoses expanded (DDx5 and DDx10). In the moderately complex case, with a final diagnosis of \"coronary heart disease (CHD) caused by ApoA-I Nashua mutation\u201d (PMID: 23415437) models often failed to connect the genetic mutation causing the lipid disorder to coronary artery disease. In the most complex case, diagnosed as \"Stage IV classical Hodgkin's lymphoma\" (PMID: 23975921) involving multiple specialists and extensive diagnostic procedures, model predictions were less accurate. Overall, as case complexity increased and more specialized labs were needed, model predictions became less precise and struggled to link related diagnoses comprehensively. Additionally, models often redundantly included diagnoses already confirmed in the case study, a limitation that could be addressed with more advanced techniques in LLMs. One clinician also pointed out that the Top 5 and Top 10 DDx made by LLMs got more irrelevant compared by what a human clinician would make.\nThe superior performance of the GPT-4 and Mixtral-8x7B model, across different scenarios, underscores the advanced capabilities of the latest LLMs in processing and integrating complex clinical data for diagnosis. However, the observed performance dropped when lab test results were excluded and for complex diseases with lab tests raises important considerations for implementing LLMs in clinical settings. The slight lag in the performance of Mixtral-8x7B compared to GPT-4, for instance, offers a starting point for further research such as Retrieval Augmented Generation (RAG) for medical applications."}, {"title": "Conclusions", "content": "Through the evaluation of five LLMs (GPT-4, GPT-3.5, Llama-2, Claude2, and Mixtral-8x7B) on the clinical case reports from PMC-Patients dataset, the study reports that the accuracy of differential diagnoses improves substantially when lab test results are included, underscoring their critical role in accurate medical diagnosis. The inclusion of lab test results significantly enhances the accuracy and lenient accuracy of differential diagnosis predictions made by large language models, especially in improving the exact match predictions. Lab data, such as liver function tests, toxicology/metabolic panels, and serology/immune tests, were generally interpreted correctly, enhancing the models' ability to generate relevant diagnoses. The study also found that the combination of Biomedical Knowledge Graphs and GPT-4 (BKG-GPT) can perform automatic assessments with a level of accuracy comparable to that of clinicians. Our study demonstrates that models such as GPT-4 and Mixtral-8x7B excel in providing relevant differential diagnoses when lab data is considered, with GPT-4 achieving the highest lenient accuracy across various scenarios. Although exact match rates remain relatively low, the high performance in lenient accuracy suggests that these models are adept at generating plausible diagnoses, thus offering valuable support in clinical decision-making. The findings underscore the critical role of lab data in improving diagnostic precision and the advanced capabilities of current LLMs in integrating complex clinical information."}, {"title": "Methods", "content": "We assessed the impact of laboratory test results on enhancing the accuracy of differential diagnosis using five large language models: GPT-414, GPT-3.523, Llama2-70B24, Claude-225, and Mixtral 8x7B-Chat26. Clinical case reports were sourced from the PMC-Patients dataset for this evaluation. The term \u201cdifferential diagnosis\" refers to a list of potential conditions or diseases that may be causing a patient's symptoms and signs. Clinicians consider the patient's clinical history, physical examination findings, and investigation results, collectively known as clinical vignettes, to aid in the diagnostic process. For this study, clinical vignettes were manually generated from 50 selected case reports, including details such as the patient's age, gender, symptoms, laboratory test results, and other relevant information, to enable the language models to formulate differential diagnostic responses. Initially, clinical reports including laboratory test results and age-specific cases, were selected to generate clinical vignettes. These vignettes were manually extracted from the clinical case presentation sections, encompassing details such as age, sex, symptoms, full case report, and lab tests. A differential diagnosis (DDX) prompt was then created, instructing the large language models to consider all pertinent details and formulate comprehensive and accurate differential diagnoses. These predictions were categorized into top 10, top 5, and top 1 differential diagnoses. The accuracy of the model's predictions was evaluated using metrics such as exact match, relevance, and incorrect predictions, with accuracy further divided into exact and lenient categories."}, {"title": "Ethical Considerations", "content": "Since this study employed case vignettes derived from publicly available published case reports, approval from the ethics committee and the requirement for individual consent were not necessary."}, {"title": "Clinical Vignettes", "content": "We utilized PMC-Patients22, a novel benchmark dataset that includes patient summaries and relationships derived from PubMed Central articles, to collect 50 clinical case reports. PMC- Patients encompasses 167,000 patient summaries with 3.1 million patient-article relevance annotations and 293,000 patient-patient similarity annotations, making it the largest resource for ReCDS and one of the largest patient collections available. Case reports were manually selected to cover a wide range of diseases such as Endocrine/Metabolic, Cardiovascular, Hematologic/Oncologic, Infectious Diseases Neurological Disorders, etc., ensuring equal representation of genders and various age groups. Figure 2 illustrates the distribution of diseases across various clinical categories. Following the selection of 50 case reports, four undergraduate premedical students were recruited to manually extract details such as age, sex, symptoms, lab tests, full case report, and final diagnosis to generate the clinical vignettes.\nFor example, consider the case report titled \u201cAcute cytomegalovirus hepatitis in an immunocompetent host\u201d (PMID: 24275336)27. From this case report we extracted the following data, Age: '52', Gender: 'Female', Lab test: 'serum aspartate aminotransferase of 739 U/L (normal value 15-37 U/L)....', Case Report: 'A 52-year-old Hispanic woman with a medical history of hypoparathyroidism....' Final diagnosis: 'Acute cytomegalovirus hepatitis'. All the case reports are indexed in PubMed and published in peer-reviewed clinical journals. The final diagnosis for each case was established through standard diagnostic processes and subsequently documented in these case reports."}, {"title": "Differential Diagnosis Lists generated by LLMs", "content": "We utilized several large language models for our study: GPT-4 and GPT-3.5 (OpenAI, LLC), Llama-2-70b-chat (Meta LLC), Claude 2 (Anthropic, LLC), and Mixtral 8x7B Mixture-of-Experts (Mistral AI, LLC). None of these models were specifically trained or reinforced for medical diagnoses. We accessed the GPT models through the OpenAI GUI (https://chatgpt.com/), while the Llama-2, Claude-2, and Mixtral 8x7B models were accessed via the open-source web interface POE (https://poe.com/). To ensure no influence from previous interactions, each model was prompted with a fresh chat interface. The initial prompt used was: \u201cImagine you are a Medical Professional tasked with providing one (1) comprehensive and accurate diagnosis for a patient presenting with the following case report. Please consider the patient's Age, Gender, Symptoms, Lab tests, and the full Case Report and any pertinent details to formulate your response.\u201d This prompt was followed by the clinical vignette as described earlier. To generate five and ten differential diagnoses (DDx), the prompts were adjusted to request \u201cfive (5) comprehensive and accurate differential diagnoses\u201d and \u201cten (10) comprehensive and accurate differential diagnoses,\u201d respectively. For evaluating the role of lab tests, DDx were generated both including and excluding the laboratory test results, starting with the prompt excluding lab test results. The final prompt was refined using prompt engineering techniques and by evaluating various prompts to encourage the LLMs to generate comprehensive lists of DDx. This optimized prompt template consistently yielded reliable and inclusive differential diagnoses across all the LLMs."}, {"title": "Evaluation of Differential Diagnosis Lists", "content": "The current study design generates 1,500 differential diagnosis (DDx) sets, comprising 50 case reports evaluated by five large language models (LLMs) across six conditions. The six different conditions are Top 1, Top 5, and Top 10, with each considered both with and without lab test results. To comparatively evaluate the LLMs' ability to"}]}