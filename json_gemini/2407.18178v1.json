{"title": "PianoMime: Learning a Generalist, Dexterous Piano Player from Internet Demonstrations", "authors": ["Cheng Qian", "Julen Urain", "Kevin Zakka", "Jan Peters"], "abstract": "In this work, we introduce PianoMime, a framework for training a piano-playing agent using internet demonstrations. The internet is a promising source of large-scale demonstrations for training our robot agents. In particular, for the case of piano-playing, Youtube is full of videos of professional pianists playing a wide myriad of songs. In our work, we leverage these demonstrations to learn a generalist piano-playing agent capable of playing any arbitrary song. Our framework is divided into three parts: a data preparation phase to extract the informative features from the Youtube videos, a policy learning phase to train song-specific expert policies from the demonstrations and a policy distillation phase to distil the policies into a single generalist agent. We explore different policy designs to represent the agent and evaluate the influence of the amount of training data on the generalization capability of the agent to novel songs not available in the dataset. We show that we are able to learn a policy with up to 56% F1 score on unseen songs. Project website: https://pianomime.github.io/", "sections": [{"title": "Introduction", "content": "The Internet is a promising source of large-scale data for training generalist robot agents. If properly exploited, it is full of demonstrations (video, text, audio) of humans solving an infinite amount of tasks [1, 2, 3] that could inform our robot agents on how to behave. However, learning from these databases is challenging for several reasons. First, unlike teleoperation demonstrations, video data does not specify the actions applied by the robot, usually requiring the use of reinforcement learning to induce the robot actions [4, 2, 5]. Second, videos typically show a human performing the task, while the learned policy is deployed on a robot. This often requires to re-target the human motion to the robot body [5, 6, 7]. Finally, as pointed in [2], if we aim to learn a generalist agent, we must select a task for which large-scale databases are available and that allows an unlimited variety of open-ended goals.\nFrom opening doors [6] to rope manipulation [8] or pick and place tasks [9, 10], previous works have successfully taught robot manipulation skills through observations. However, these approaches have been limited to low dexterity in the robots or to a small variety of goals.\nIn this work, we focus on the task of learning a generalist piano player from Internet demonstrations. Piano-playing is a highly dexterous open-ended task [11]. Given two multi-fingered robot hands and a desired song, the goal of a piano-playing agent is to press the correct keys and only the correct keys at the proper timing. Moreover, the task can be conditioned on arbitrary songs, allowing for a large, and high-dimensional goal conditioning.\nAdditionally, the Internet is full of videos of professional piano players performing a wide myriad of songs. Interestingly, these piano players often record themselves from a top-view allowing an easy observation of the demonstrations. Additionally, they usually share the MIDI files of the song they play, facilitating the extraction of relevant information."}, {"title": "Related Work", "content": "Robotic Piano Playing Several studies have investigated the development of robots capable of playing the piano. In [14], multiple-targets Inverse Kinematics (IK) and offline trajectory planning are utilized to position the fingers above the intended keys. In [15], a Reinforcement Learning (RL) agent is trained to control a single Allegro hand to play the piano using tactile sensor feedback. However, the piano pieces used in these studies are relatively simple. Subsequently, in [11], an RL agent is trained to control two Shadow Hands to play complex piano pieces by designing a reward function comprising a fingering reward, a task reward, and an energy reward. In contrast with pre-vious approaches, our approach exploits Youtube piano-playing videos, enabling faster training and more accurate and human-like robot behavior.\nMotion Retargeting and Reinforcement Learning Our work shares similarities with motion re-targeting [16], specifically with those works that combine motion retargeting with RL to learn con-trol policies [17, 18, 5, 19, 6]. Given a mocap demonstration, it has been common to exploit the"}, {"title": "Method", "content": "The PianoMime framework is composed of three phases: data preparation, policy learning, and pol-icy distillation.\nIn the data preparation phase, given the raw video demonstration, we extract the informative sig-nals needed to train the policies. Specifically, we extract fingertip trajectories and a MIDI file that informs the piano state at every time instant.\nIn the policy learning phase, we train song-specific policies via RL. This step is essential for gener-ating the robot actions that are missing in the demonstrations. The policy is trained with two reward functions: a style reward and a task reward. The style reward aims to match the robot's finger move-ments with those of the human in the demonstrations to preserve the human style, while the task reward encourages the robot to press the correct keys at the proper timing.\nIn the policy distillation phase, we train a single behavioral cloning policy to mimic all the song-specific policies. The goal of this phase is to train a single generalist policy capable of playing any song. We explore different policy designs and the representation learning of goals to improve the generalization capability of the policy."}, {"title": "Data preparation: From raw data to human and piano state trajectories", "content": "We generate the training dataset by web scraping. We download YouTube videos of professional piano artists playing various songs. We particularly choose YouTube channels that also upload MIDI files of the played songs. The MIDI files represent trajectories of the piano state (pressed/unpressed keys) throughout the song. We use the video to extract the motion of human pianists and the MIDI file to inform about the goal state of piano during the execution of the song.\nWe select the fingertip position as the essential signal to mimic with the robot hand. While several dexterous tasks might require the use of the palm (e.g. grasping a bottle), we consider mimicking the fingertip motion to be sufficient for the task of piano playing. This will also reduce the constraints applied to the robot, allowing it to adapt its embodiment more freely.\nTo extract the fingertip motion from videos, we use MediaPipe [20], an open-source framework for perception. Given a frame from the demonstration videos, MediaPipe outputs the skeleton of the hand. We find that the classical top-view recording in piano-playing YouTube videos is highly beneficial for obtaining an accurate estimate of the fingertip positions.\nNotice that given the videos are RGB, we lack depth signal. Therefore, we predict the 3D fingertip positions based on the piano state. The detailed procedure is explained in Appendix A."}, {"title": "Policy learning: generating robot actions from observations", "content": "Through the data preparation phase, we extract two trajectories: a human fingertip trajectory $T_\\alpha$ and a piano state trajectory $T_\\gamma$. The human fingertip trajectory $T_\\alpha : (x_1,...,x_T)$ is a T-step trajectory of two hands' 3D fingertip positions $x \\in \\mathbb{R}^{3\\times 10}$ (10 fingers). The piano state trajectory $T_\\gamma : (\\delta_1,..., \\delta_T)$ is a T-step trajectory of piano states $\\delta \\in \\mathbb{B}^{88}$, represented with an 88-dimensional binary variable representing which keys should be pressed.\nGiven the ROBOPIANIST [11] environment, our goal is to learn a goal-conditioned policy $\\pi_\\theta$ that plays the song defined by $\\gamma$ while matching the fingertip motion given by $T_\\alpha$. Notice that satisfying both objectives jointly might be impossible. Tracking perfectly the fingertip trajectory $T_\\alpha$ might not necessarily lead to playing the song correctly. Although both trajectories are collected from the same source, errors in hand tracking and embodiment mismatches might lead to deviations, resulting in poor song performance. Thus, we propose using $T_\\alpha$ as a style guiding behavior."}, {"title": "Policy distillation: learning a generalist piano-playing agent", "content": "Through the policy learning phase, we train song-specific expert policies from which we roll out state and action trajectories $\\mathcal{T}^s : (s_0,..., s_T)$ and $\\mathcal{T}^q : (q_0, ..., q_T)$. Then, we generate a dataset $\\mathcal{D} : (\\mathcal{T}_{\\alpha}, \\mathcal{T}_{\\gamma}, \\mathcal{T}^s, \\mathcal{T}^q )_{i=1}^N$ with $N$ being the number of learned songs. Given the dataset $\\mathcal{D}$, we apply Be-havioral Cloning (BC) to learn a single generalist piano-playing agent $\\pi_\\theta(q_{t:t+L}, x_{t:t+L}|s_t, \\gamma_{t:t+L})$ that outputs configuration-space actions $q_{t:t+L}$ and fingertip motion $x_{t:t+L}$ conditioned on the cur-rent state $s_t$ and the future desired piano state $\\gamma_{t:t+L}$.\nWe explore different strategies to represent and learn the behavioral cloning policy and improve its generalization capabilities. In particular, we explore (1) representation learning approaches to induce spatially informative features, (2) a hierarchical policy structure for sample-efficient training, and (3) expressive generative models [22, 23, 24] to capture the multimodality of the data. Also, inspired by current behavioral cloning approaches [22, 25], we train policies that output sequences of actions rather than single-step actions and execute them in chunks.\nRepresentation Learning. We pre-train an observation encoder over the piano state $\\gamma$ to learn spatially consistent latent features. We hypothesize that two piano states that are spatially close"}, {"title": "Experimental Results", "content": "We split the experimental evaluation into three parts. In the first part, we explore the performance of our proposed framework in learning song-specific policies via RL. In the second part, we perform ablation studies on policy designs for learning a generalist piano-playing agent by distilling the pre-viously learned policies via BC. Finally, in the third part, we explore the influence of the amount of training data on the performance of the test environments.\nDataset and Evaluation Metrics All experiments are conducted on our collected dataset, which contains the notes and the corresponding demonstration videos and fingertip trajectories of 60 piano songs from a Youtube channel PianoX 1. To standardize the length of each task, each song is di-vided into several clips, each with a duration of 30 seconds (The dataset contains totally 431 clips, 258K state-action pairs). Furthermore, we choose 12 unseen clips to investigate the generalization capability of the generalist policy. We use the same evaluation metrics from RoboPianist [11], i.e., precision, recall, and F1 score.\nSimulation Environment Our experiment setup utilizes RoboPIANIST simulation environment [11], conducted in Mujoco physics simulator [26]. The agent predicts target joint angles at 20Hz and the targets are converted to torques using PD controllers running at 500Hz. We use the same physical setting as [11] with two modifications: 1) The z-axis sliding joints fixed on both forearms are enabled to allow more versatile hand movement. Therefore, the action space of our agent is 47 dimensional (45 dimensional in [11]). 2) We increase the proportional gain of the PD controller for the x-axis sliding joints to enable faster horizontal movement, which we find essential for some fast-paced piano songs."}, {"title": "Evaluation on learning song-specific policies from demonstrations", "content": "In this section, we evaluate the song-specific policy learning and aim to answer the following ques-tions: (1) Does integrating human demonstrations with RL help in achieving better performance? (2) What elements of the learning algorithm are the most influential in achieving good performance?\nWe use Proximal Point Optimization (PPO) [27], because we find that it performs the best compared to other RL algorithms. We compare our model against two baselines:\nRobopianist [11] We use the RL method introduced in [11]. We maintain the same reward functions"}, {"title": "Evaluation of model design strategies for policy distillation", "content": "This section focuses on the evaluation of policy distillation for playing different songs. We evaluate the influence of different policy design strategies on the agent's performance. We aim to assess (1) the impact of integrating a pre-trained observation encoder to induce spatially consistent features, (2) the impact of a hierarchical design of the policy, and (3) the performance of different generative models on piano-playing data."}, {"title": "Evaluations on the impact of the data in the generalization", "content": "In this section, we investigate the impact of scaling training data on the generalization capabilities of the agent. We evaluate three policy designs (One-stage Diff, Two-stage Diff, and Two-stage Diff-res). We train them using various proportions of the dataset, and evaluate their performance on the test dataset (see Figure 5 Top). Note that One-stage Diff uses the same dataset as the low-level policy of Two-stage Diff.\nResults. We observe that both Two-stage Diff and Two-stage Diff-res show consistent performance improvement with increasing used data. This trend implies that the two-stage policies have not yet reached their performance saturation with the given data and could potentially continue to benefit from additional training data in future works."}, {"title": "Evaluation on imbalance training datasets", "content": "We further employ different combinations of the high-level and low-level policies of Two-stage Diff trained with different proportions of the dataset and assess their performance. In addition, we introduce an oracle high-level policy, which outputs the ground-truth fingertip position from human demonstration videos. The results (see Figure 5 Bottom) demonstrate that the overall performance of policy is significantly influenced by the quality of the high-level policy. Low-level policies paired with Oracle high-level policies consistently outperform the ones paired with other high-level policies. Besides, we observe early performance convergence with increasing training data when paired with a low-quality high-level policy. Specifically, with the HL 1% policy and HL 50%, performance almost converged with around 10% and 50% low-level data, respectively."}, {"title": "Limitations", "content": "Inference Speed One of the limitations is the inference speed. The models operate with an infer-ence frequency of approximately 15Hz on an RTX 4090 machine, which is lower than the standard real-time demand on hardware. Future works can employ faster diffusion models, e.g., DDIM [29], to speed up the inference.\nOut-of-distribution Data Most of the songs in our collected dataset are of modern style. When evaluating the model on the dataset from [11], which mainly contains classical songs, the perfor-mance degrades. This discrepancy implies the model's limited generalization across songs of differ-ent styles. Future work can collect more diverse training data to improve this aspect.\nAcoustic Experience Although the policy achieves up to 56% F1-score on unseen songs, we found that higher accuracy is still necessary to make the song acoustically appealing and recognizable. Future work should focus on improving this accuracy to enhance the overall acoustic experience."}, {"title": "Conclusion", "content": "In this work, we present PianoMime, a framework for training a generalist robotic pianist using in-ternet video sources. We start by training song-specific policies with residual RL, enabling the robot to master individual songs by mimicking human pianists. Subsequently, we train a single behavioral cloning policy that mimics these song-specific policies to play unseen songs. The policy leverages three key techniques: goal representation learning, policy hierarchy, and expressive generative mod-els. The resulting policy demonstrates an impressive generalization capability, achieving an average F1-score of 70% on unseen songs. This also highlights that leveraging internet data can be highly useful for training generalist robotic agents."}, {"title": "Retargeting: From human hand to robot hand", "content": "To retarget from the human hand to robot hand, we follow a structured process.\nStep 1: Homography Matrix Computation Given a top-view piano demonstration video, we firstly choose n different feature points on the piano. These points could be center points of specific keys, edges, or other identifiable parts of the keys that are easily recognizable (See Figure 6). Due to the uniform design of pianos, these points represent the same physical positions in both the video and Mujoco. Given the chosen points, we follow the Eight-point Algorithm to compute the Homography Matrix H that transforms the pixel coordinate in videos to the x-y coordinate in Mujoco (z-axis is the vertical axis).\nStep 2: Transformation of Fingertip Trajectory We then obtain the human fingertip tra-jectory with MediaPipe [20]. We collect the fingertips positions every 0.05 seconds. Then we transform the human fingertip trajectory within pixel coordinate into the Mujoco x-y 2D coordinate using the computed homography matrix H.\nStep 3: Heuristic Adjustment for Physical Alignment We found that the transformed fin-gertip trajectory might not physically align with the notes, which means there might be no detected fingertip that physically locates at the keys to be pressed or the detected fingertip might locate at the border of the key (normally human presses the middle point of the horizontal axis of the key). This misalignment could be due to the inaccuracy of the hand-tracking algorithm and the homography matrix. Therefore, we perform a simple heuristic adjustment on the trajectory to improve the physical alignment. Specifically, at each timestep of the video, we check whether there is any fingertip that physically locates at the key to be pressed. If there is, we adjust its y-axis value to the middle point of the corresponding key. Otherwise, we search within a small range, specifically the neighboring two keys, to find the nearest fingertip. If no fingertip is found in the range or the found fingertip has been assigned to another key to be pressed, we then leave it. Otherwise, we adjust its y-axis value to the center of the corresponding key to ensure proper physical alignment.\nStep 4: Z-axis Value Assignment Lastly, we assign the z-axis value for the fingertips. For the fingertips that press keys, we set their z-axis values to 0. For other fingertips, we set their z-axis value to $2 \\cdot h_{key}$, where $h_{key}$ is the height of the keys in Mujoco."}, {"title": "Implementation of Inverse Kinematics Solver", "content": "The implementation of the IK solver is based on the approach of [21]. The solver addresses multiple tasks simultaneously by formulating an optimization problem and find the optimal joint velocities that minimize the objective function. The optimization problem is given by:\n$\\min \\sum_i w_i|| J_i q - \\dot{K_i v_i} ||^2,$\n(1)"}, {"title": "Detailed MDP Formulation of Song-specific Policy", "content": "Table 3: The detailed reward function to train the song-specific policy. The Key Press reward is the same as in [11], where ks and kg represent the current and the goal states of the key respectively, and g is a function that transforms the distances to rewards in the [0, 1] range. Paf and prf represent the fingertip positions of human demonstrator and robot respectively."}, {"title": "Training Details of Diffusion Model", "content": "All the diffusion models utilized in this work, including One-stage Diff, the high-level and low-level policies of Two-stage Diff, Two-stage Diff-res and Two-stage Diff w/o SDF, share"}, {"title": "Policy Distillation Experiment", "content": "Two-stage Diff w/o SDF We directly use the binary representation of goal instead of the SDF embedding representation to condition the high-level and low-level policies.\nTwo-stage Diff-res We employ an IK solver to compute the target joints given the fingertip positions predicted by the high-level policy. The low-level policy predicts the residual terms of IK solver instead of the robot actions.\nTwo-stage BeT We train both high-level and low-level policies with Behavior Transformer [23] instead of DDPM. The hyperparameter of Bet is listed in Table 8.\nOne-stage Diff We train a single diffusion model to predict the robot actions given the SDF embedding representation of goals and the proprioception state.\nMulti-task RL We create a multi-task environment where for each episode a random song is sampled from the dataset. Consequently, we use Soft-Actor-Critic (SAC) [33] to train a single agent within the environment. Both the actor and critic networks are MLPs, each with 3 hidden layers, and each hidden layer contains 256 neurons. The reward function is the same as that in [11].\nBC-MSE We train a feedforward network to predict the robot action of next timestep con-ditioned on the binary representation of goal and proprioception state with MSE loss. The feedforward network is a MLP with 3 hidden layers, each with 1024 neurons."}]}