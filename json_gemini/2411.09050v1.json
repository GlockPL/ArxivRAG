{"title": "The Systems Engineering Approach in Times of Large Language Models", "authors": ["Christian Cabrera", "Jennifer Schooling", "Viviana Bastidas", "Neil D. Lawrence"], "abstract": "Using Large Language Models (LLMs) to address critical societal problems requires adopting this novel technology into socio-technical systems. However, the complexity of such systems and the nature of LLMs challenge such a vision. It is unlikely that the solution to such challenges will come from the Artificial Intelligence (AI) community itself. Instead, the Systems Engineering approach is better equipped to facilitate the adoption of LLMs by prioritising the problems and their context before any other aspects. This paper introduces the challenges LLMs generate and surveys systems research efforts for engineering AI-based systems. We reveal how the systems engineering principles have supported addressing similar issues to the ones LLMs pose and discuss our findings to provide future directions for adopting LLMs.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs) leverage neural network architectures trained on large amounts of data to learn underlying language patterns. LLMs generate content in formats humans understand based on these architectures (Feuerriegel et al., 2024). Such ability creates novel human-machine interfaces (Cabrera et al., 2024) for adopting AI at different levels of our society. Generative AI technologies promise new applications for addressing critical problems in diverse domains.\n\nThe complexity of socio-technical systems and the LLMs' nature challenge the realisation of this vision. Social problems have critical requirements that demand reliable systems. LLMs rely on probabilistic models that make systems' components based on these technologies non-deterministic, data-driven, and prone to hallucinations (D'Antonoli et al., 2024) impacting the alignment and reliability of the systems. LLMs operate as black-boxes (Feuerriegel et al., 2024), which impact systems' accountability and interpretability as designers and users do not control and understand the systems (i.e., intellectual debt). Social problems usually appear in resource-constrained environments. Building LLMs is an expensive process that causes significant environmental concerns because it generates an immense carbon footprint (Schwartz et al., 2020).\n\nThe outlined challenges require interdisciplinary research to align societal problems, systems, and AI technical advances. The systems engineering approach is equipped with principles to facilitate this alignment by prioritising the problems and their context before considering the technologies for their resolution. We envisage an ecosystem where systems engineering and LLMs mutually benefit instead of the naive belief that benefits come from the LLMs to the domains in one direction. This paper surveys how researchers have used the system engineering approach to design AI-based systems since 2017 (i.e., when current LLM technologies emerged) as a first step to building such an ecosystem. The main research question we want to answer is how does current research use the systems engineering approach to address challenges similar to the ones LLMs impose on socio-technical systems?"}, {"title": "2. Related Work", "content": "Researchers have revised the system engineering concept in light of advances in AI before. Sommerville (2019) identifies a gap between systems engineering and AI communities, given the different process models these communities use. The systems engineering process starts with requirements and ends with a valid system. Al follows an exploratory process that formulates, develops and evaluates ideas. Wade et al. (2020) review the systems engineering approach for AI inspired by the Very Large Scale Integration (VSLI) revolution. The authors make a parallel between the challenges the transistor technology faced before its adoption and the current challenges that the adoption of Al faces. The common denominator in both cases is that the technologies rapidly surpassed the ability to engineer them. Llinas et al. (2021) review the role of Systems Engineering for AI-based systems through time and formulate future research areas. This review focuses on expert systems and machine learning (ML) technologies, which are predecessors of novel generative AI. Jake Vanderlinde and Mashford (2022) review the new failure modes that Al introduces into systems and their potential challenges for systems engineering. They claim that traditional systems engineering methods are insufficient to design robust systems. Pfrommer et al. (2022) introduces the concept of AI Systems Engineering as a new branch of Systems Engineering that addresses the systematic development and operation of AI-based solutions as parts of systems that master complex tasks. Related research coincides with the importance of the systems approach for AI-based systems. Some advocate for more systems engineering practices in AI projects, while others claim the systems engineering approach should evolve to handle new technologies. We nurture these discussions by surveying applications of the systems engineering approach from the perspective of the latest AI shift."}, {"title": "3. Socio-technical Systems and LLMs Challenges", "content": "This section summarises the most common issues the applications of LLMs generate."}, {"title": "3.1. Systems Alignment and Reliability", "content": "Socio-technical systems address problems that affect the interests of multiple stakeholders. Engineers define system requirements according to such problems. Alignment issues emerge when systems do not accomplish their requirements (Bastidas et al., 2022).\n\nAlignment issues can emerge when systems integrate LLMs-based components. LLMs are probabilistic and inject uncertainty or even errors (i.e., hallucinations (Feuerriegel et al., 2024)) in their outputs. Uncertain, unexpected, or incorrect outputs can go against the satisfaction of systems requirements (i.e., misalignment). In addition, the quality of the LLMs outputs depends on the quality of the data they are trained on (Cabrera et al., 2023). If the data is biased and contains unfair relationships, LLMs will produce biased and unfair outputs (D'Antonoli et al., 2024). Such outputs can then propagate through systems, impacting their performance and how systems satisfy their requirements. These limitations negatively impact system reliability, which in turn causes a lack of trust. End users can perceive systems as unhelpful or harmful, even when systems are well-aligned with their requirements because of such unreliability."}, {"title": "3.2. Interpretability and Accountability", "content": "Socio-technical systems comprise separate but interdependent social and technical subsystems (Baxter & Sommerville, 2010). Defining and understanding subsystems' scope, responsibilities, and impact is crucial in designing and deploying systems. Interpretability and accountability issues emerge when there is no clear definition of the responsibilities of systems components and users have difficulty understanding their complexity and behaviour.\n\nComplex and large deep neural network architectures are behind LLMs. These architectures have billions of parameters, and the reasons behind their outputs are often hard to understand. A whole research area explores how to improve the explainability of machine learning models (Zhao et al., 2024). LLMs bring the explainability issue to the systems level as they operate as black-box components (Cabrera et al., 2023). Integrating LLMs as black boxes in larger systems negatively impacts their interpretability as these generate intellectual debt. Engineers, practitioners, and end-users know that ML-based components can solve challenging problems but do not always know why these components generate their outputs (Zittrain, 2022). This lack of understanding also affects systems' accountability and trustworthiness because components' functionalities and boundaries are unclear, which makes it difficult to detect the sources of misbehaviour."}, {"title": "3.3. Maintainability and Sustainability", "content": "Socio-technical systems rely on optimising social and technical subsystems (Baxter & Sommerville, 2010). This optimisation must consider constraints over natural, human, and economic resources. Engineers must create self-maintaining and sustainable solutions as the planet is in an environmental crisis.\n\nIntegrating LLMs into systems requires developing LLMs trained on specific datasets (D'Antonoli et al., 2024). However, building, fine-tuning, and testing LLMs cause significant environmental concerns as they generate an immense carbon footprint (Feuerriegel et al., 2024). These issues threaten the sustainability of socio-technical systems against the optimal usage of resources. es. The intellectual debt (Zittrain, 2022) also challenges self-maintaining socio-technical systems based on LLMs. Self-maintaining systems require an autonomous understanding of the system to identify intervention points and anticipate cascade effects."}, {"title": "3.4. Security and Privacy", "content": "Socio-technical systems create environments where data flows between external entities, internal actors, subsystems, and their components (Baxter & Sommerville, 2010). This data might be sensible depending on the application domain (e.g., health care, urban planning, etc). Engineers must prioritise privacy and security requirements when designing socio-technical systems that manipulate sensible data.\n\nLLMs inject new security threats into critical socio-technical systems. These models create risks of leaking confidential and sensitive information from the model's training data. Recent research shows that adversarial attacks or prompt engineering strategies can extract personal information from LLMs (D'Antonoli et al., 2024). The complexity, lack of transparency, and limited accountability that LLMs add to the systems challenge the mitigation strategies developed by different communities to address security risks."}, {"title": "4. Survey Methodology", "content": "We survey papers that apply systems engineering approaches to address the described challenges in the context of AI-based systems. Our survey uses a tool that semi-automates the paper selection process (Kitchenham & Brereton, 2013). We use the following query as an input to search for the relevant papers: (\"systems engineering\" OR \"systems thinking\u201d OR \"dependable systems\" OR \"engineering AI\u201d) AND (\"generative ai\" OR \"large language model\" OR \"llm\" OR \"artificial intelligence\" OR \"ai\" OR \"ml\" OR \"deep learning\"). The parameters file for this search process is here2. We searched for papers in IEEEXplore, Springer Nature, Scopus, Semantic Scholar, CORE, and ArXiv. The tool retrieved a total of 3,504 papers. We apply a semantic filter comparing the semantic embedding of the titles and abstracts of these papers with a description of"}, {"title": "5. Systems Engineering in Times of LLMS", "content": "Figure 1 presents how the papers applied the systems engineering principles. Haberfellner et al. (2019) classify the principles into two categories. The first category corresponds to Systems Thinking and groups the principles of Systems Views, Agility Systems, and System Dynamics. The second category corresponds to the Systems Engineering Process Model and includes the Top-Down, Variant Creation, and Problem-Solving Cycle principles. Our survey shows that current research prioritises Alignment and Reliability to leverage AI features while assuring that systems behave as expected. The principles that guide most approaches are the Systems Views, Top-Down, and Problem-Solving Cycle. The rest of this section details our findings."}, {"title": "5.1. Systems Alignment and Reliability", "content": "Around 87% of papers (i.e., 21 out of 24) address alignment and reliability issues. Figure 2 shows how papers apply the principles. Most works apply the System Views and Top-Down principles followed by the Problem-Solving Cycle principle. The System Views principle defines systems from different perspectives and facilitates the design of reliable systems that align with their intents. Fujii et al. (2020) propose quality guidelines for systems that rely on ML components with limited accuracy. These guidelines establish checkpoints along the system lifecycle, including data integrity, model robustness, system quality, process agility, and customer expectation viewpoints. Salwei and Carayon (2022) propose a model to represent systems that apply AI in healthcare. This model includes people, organisations, tasks, AI technologies, other technologies, and physical environments. The authors use this model to analyse if AI generates positive or negative outcomes. For example, the model can support observing the practitioners' workflow to identify bottlenecks and assess AI tools' usability. Wang et al. (2022) propose to model the healthcare systems using three perspectives to support a transparent understanding of the interactions and connections between governance institutions, patients' perceptions, and complex healthcare systems. The digital and governance perspectives improve patients' perceptions when systems prioritise patients' needs and regulation and education policies are in place.\n\nThe top-down principle decomposes the problem and the solution to guarantee that the latter addresses the former. Meyer and Gruhn (2019) uses this principle to align RL solutions with high-level requirements. The misalignment between both sides emerges from the fact that ML model goals (e.g., accuracy) are distant from the requirements of socio-technical systems. The component-based software development model bridges this gap. Lavazza and Morasca (2021) propose a notation to represent uncertainty in AI-Intensive Systems (AIIS). This notation decomposes the system into infrastructure, ML components, and traditional software, enabling an alignment analysis. Langford and Cheng (2022) introduce a framework to manage learning-enabled components (LECs) for safety-critical tasks. The framework decomposes the uncertainty problem and creates microservices to manage LECs. An adversarial detection microservice identifies when LECs are likely to misbehave to avoid their usage. Hedin and Curtis (2023) propose to support the human component of AI-based systems. They improve systems reliability by creating a better-informed public through AI benchmarks. The paper builds the benchmarks by adopting top-down strategies from high-level requirements to systems components.\n\nCombining principles supports solutions that consider more aspects of the systems. Mattioli et al. (2023) introduce the Confiance.ai framework for verifying and validating industrial systems. The framework formalises ten viewpoints in a metamodel that captures concepts around the system's specification, lifecycle, technical implementation, associated processes, people, governance, and risk and quality management. Adedjouma et al. (2022) uses these models for an assurance use case. Requirements are decomposed and mapped to the system's components that realise them. Such a mapping enables evidence-based reliability specification. Nabavi and Browne (2022) propose the Five Ps as an analytical and planning tool that models AI-based systems from the Problem and solution viewpoints and decomposes the system from requirements to components into Purpose, Pathway, Process and Parameter zones. The tool identifies the points in the zones where interventions can generate more value for Responsible Al requirements (i.e., fairness and trustworthiness.).\n\nLaato et al. (2022) propose to close the gap between governance and ML-based systems by considering different viewpoints to define governance concepts and requirements. These concepts are mapped to the development process (i.e., the Problem-Solving Cycle principle) to establish a governance model for system design, development, and operation. The system design dimension focuses on the system requirements, data needs, and alignment. Zeller et al. (2024) introduce a workflow for continuous deployment and safety assurance of ML-based systems. This process includes testing and verification steps along the systems' lifecycle, considering three viewpoints to provide solutions that operate according to safety functional requirements. Solomonides et al. (2021) present governance principles and a systems lifecycle for adopting AI into healthcare systems. The authors propose principles from the medical practice (i.e., the Belmont principles), healthcare organisations, and technology perspectives. A process model applies these principles in the inception, development, deployment,"}, {"title": "5.2. Interpretability and Accountability", "content": "Figure 3 shows how the selected papers approach interpretability and accountability challenges (i.e., above 30% of works). Six of these papers apply the Systems View principle, and none of them use the Agility Systems and the Variant Creation ones. Analysing problems and systems from different perspectives defines the roles of systems\u2019 actors as a base for accountability processes. Mokander et al. \u00a8 (2023) propose to audit large language models and their applications using three views: governance audits"}, {"title": "5.3. Maintainability and Sustainability", "content": "Figure 4 shows how the works use the principles to address maintainability and sustainability issues. We found that three papers support effective resource management by using cost-benefit analysis. ACDANS (Hershey, 2021) relies on a workflow that uses a Modelling & Simulation (M&S) tool that permits the evaluation of different solutions from a cost-benefit perspective. The MLTRL framework (Lavin et al., 2022) supports systems maintainability and sustainability in two ways. First, defining requirements and constant checkpoints reduce the risk of wasting resources. Second, evaluating alternatives enables designers to make informed decisions when selecting a solution. Folds and McDermott (2019) propose using digital twins models for systems analysis at the design and operation stages. The digital twin models rely on Mission Function Task (MFT) analysis, which creates different design scenarios for a system. The application of the MFT analysis in the digital twin model enables dynamic modelling, simulation, and experimentation of systems, which can include, among other variables, their resource consumption.\n\nFive papers propose approaches for systems self-maintenance. The microservices architectures proposed by Langford and Cheng (2022) implement the MAPE-K loop for systems self-adaptation tasks at runtime. These tasks include a flexible updating of learning models (i.e., the Agility Systems principle). The formal method proposed by Cody et al. (2023) allows temporal analysis between systems. Changes in the core and peripheral structures represent the evolution of systems over time to support maintenance decision-making processes from designers or the system itself. One of the two architectural patterns proposed by Dzambic et al. (2022) explores alternatives for integrating AI into autonomous architectures for safety-critical systems. These alternatives explore architectural configurations in the Cloud and at the Edge infrastructures. The system operation phase of the ML governance model introduced by Laato et al. (2022) focuses on systems\u2019 testing and maintenance. The goal is to support failure identification and recovery when systems are deployed and running. Similarly, the governance principles defined by Solomonides et al. (2021) consider systems\u2019 constant monitoring and maintenance, including updating tasks that enable"}, {"title": "5.4. Security and Privacy", "content": "Eight of the twenty-four selected papers propose approaches to address security and privacy issues (Figure 5). The most used principles are the System Views and the Problem-Solving Cycle. None of the papers apply the Agility Systems principle. Cai (2020) proposes a checklist for analysing \u0391\u0399 systems safety from different perspectives. The paper proposes decomposing autonomous systems into parts to apply the safety checklist. These parts represent potential vulnerabilities of the system, including system components, structures, assumptions, properties, and interfaces. The safety checklist assesses systems sensors, data collection tasks, default design assumptions, systems observability and controllability capabilities, the complexity of the systems, and their interactions with humans. Similarly, Fujii et al. (2020) establish checkpoints along the whole Al system lifecycle to represent different analysis viewpoints. The data integrity, system quality, and customer expectation viewpoints represent security and privacy requirements for assuring the quality of ML-based systems. M\u00f6kander et al. (2023) defines that security and privacy issues caused by LLMs can emerge from technology providers or downstream applications. The authors structure the audit process into three layers to assign responsibilities and identify and prevent security and privacy risks. The causal graph proposed by Wang et al. (2022) facilitates modelling relationships between patients, physicians, AI components, and governance institutions in complex socio-technical systems. This graph is a formal representation that enables multiple analyses (e.g., identifying security and privacy vulnerabilities). Solomonides et al. (2021) propose principles for adopting AI into healthcare systems. These principles include data management tasks, auditing mechanisms, systems decommissioning, and external standards to assure patients' data privacy.\n\nFor example, the systems decommissioning principles deal with the closure, curation, and maintenance of records of Al systems. Institutions must preserve data for medical and legal reasons (e.g., hospitals must keep data records from systems in the paediatrics domain for 21 years). Likewise, the development dimension of the ML governance model proposed by Laato et al. (2022) maps the data privacy and validation requirements to the systems development process. This mapping makes particular emphasis on the ethical use of sensible data. The MLTR framework (Lavin et al., 2022) includes data intervention tests and monitoring at integration and deployment levels. These tests assess possible security and privacy issues and solution alternatives. The Modelling & Simulation tool of ACDANS (Hershey, 2021) simulates and evaluates different systems' scenarios. These scenarios can include security vulnerabilities and their possible solutions."}, {"title": "6. Open Research Directions", "content": "Our survey shows how current research uses systems approaches to mitigate the challenges that emerge from adopting AI. We discuss open research challenges based on these findings."}, {"title": "6.1. Inclusive Requirements Definition", "content": "The Systems Views and Top Down principles enable considering different perspectives and analysis levels when defining requirements according to our survey. However, these principles work well when the stakeholders are inside the same organisation and have a common language. Socio-technical systems involve heterogeneous subsystems that impact large populations of the public. Defining comprehensive requirements for such systems is an open challenge because these can impact multiple stakeholders who do not communicate with each other. Systems engineering methodologies must consider public engagement activities to get an inclusive problem definition. Educational policies are crucial as the public can misunderstand the implications of technologies. This misunderstanding creates both fake hopes and fears, which negatively impacts the LLMs' adoption. A promising direction to address these challenges is designing AI systems driven by public value (Bastidas & Schooling, 2024). In addition, new AI-based interfaces can empower people when defining ML-based systems (Robinson et al., 2024)."}, {"title": "6.2. Operationalising High-Level Concepts", "content": "The surveyed papers mostly rely on the Top Down principle to translate requirements into artefacts that satisfy them using systems\u2019 design metamodels, architectures, and methodologies. However, socio-technical systems requirements are hard to operationalise. There is a gap between high-level concepts like sustainability, robustness, and truthfulness and their technical implementations (e.g., LLMs\u2019 fairness metrics). Reductionist techniques aim to close this gap. However, systems based on such techniques usually fail to capture the real-world complexity. AI-based socio-technical systems require novel approaches to manage the interaction between multiple actors, subsystems, and data sources during the system\u2019s lifecycle. The System Dynamics and Agility Systems principles require more attention as they can provide dynamic and flexible tools (Folds & McDermott, 2019; Yu et al., 2024) and architectures (Cabrera et al., 2023) for systems operation (Cabrera et al., 2024)."}, {"title": "6.3. Changing organisations\u2019 culture", "content": "Most surveyed papers support safety-critical systems. The Problem-Solving Cycle principle defines development methodologies for such systems, which prioritise the documentation, research, planning, and prototyping stages before moving to implementation. However, current organisations profess an agile culture where working software is the measure of progress (Srivastava et al., 2017). Including AI-based components that designers do not fully understand lowers the bar to qualify a system as safety-critical. Organisations need a shift towards methodologies that balance or even prioritise planning phases against production phases. Current methods for designing critical systems provide good directions (Lavin et al., 2022). Automating the systems implementation stages (e.g., code generation) frees resources and leverages such methodological shifts (Robinson et al., 2024)."}, {"title": "6.4. Dynamic Technological Landscape", "content": "The technological ecosystem around LLMs is changing quickly. New learning models and tools challenge the completeness and relevance of systems guidelines, metamodels, methodologies, architectural patterns, and formal methods that assume or require complete knowledge about the system actors, enablers, benefits, and risks. The most applied principles (i.e., Systems Views, Top Down, and Problem-Solving Cycle) are static and rely on prior knowledge. We must research how to combine these with the dynamic and flexible ones (i.e., Agility Systems, System Dynamics, and Variant Creation). Neurosymbolic approaches and ML for systems engineering are promising directions in this regard (Decardi-Nelson et al., 2024)."}, {"title": "7. Conclusions", "content": "This paper surveys systems research efforts for engineering AI-based systems. We show the systems engineering approach offers a good starting point for addressing the LLMs\u2019 challenges. Most works prioritise addressing the systems Alignment and Reliability issues. The systems engineering principles that guide most of the approaches are the Systems Views, the Top-Down, and the Problem-Solving Cycle. The Agility Systems, System Dynamics, and Variant Creation principles require more attention. We plan to use the outputs of this survey to provide systems engineering principles and guidance for designing and developing AI-based systems integrating LLMs. These principles will drive our work towards building an ecosystem of architectural patterns, design artefacts, and software platforms to develop and deploy sustainable AI-based systems."}]}