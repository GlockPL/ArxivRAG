{"title": "Prompt Your Brain: Scaffold Prompt Tuning for Efficient Adaptation of fMRI Pre-trained Model", "authors": ["Zijian Dong", "Yilei Wu", "Zijiao Chen", "Yichi Zhang", "Yueming Jin", "Juan Helen Zhou"], "abstract": "We introduce Scaffold Prompt Tuning (ScaPT), a novel prompt-based framework for adapting large-scale functional magnetic resonance imaging (fMRI) pre-trained models to downstream tasks, with high parameter efficiency and improved performance compared to fine-tuning and baselines for prompt tuning. The full fine-tuning updates all pre-trained parameters, which may distort the learned feature space and lead to overfitting with limited training data which is common in fMRI fields. In contrast, we design a hierarchical prompt structure that transfers the knowledge learned from high-resource tasks to low-resource ones. This structure, equipped with a Deeply-conditioned Input-Prompt (DIP) mapping module, allows for efficient adaptation by updating only 2% of the trainable parameters. The framework enhances semantic interpretability through attention mechanisms between inputs and prompts, and it clusters prompts in the latent space in alignment with prior knowledge. Experiments on public resting state fMRI datasets reveal ScaPT outperforms fine-tuning and multitask-based prompt tuning in neurodegenerative diseases diagnosis/prognosis and personality trait prediction, even with fewer than 20 participants. It highlights ScaPT's efficiency in adapting pre-trained fMRI models to low-resource tasks.", "sections": [{"title": "1 Introduction", "content": "In the realm of neuroimaging, the emergence of large-scale, self-supervised pre-trained models/foundation models for fMRI, has demonstrated a significant"}, {"title": "2 Method", "content": "Problem Setup. Given an fMRI pre-trained model $f(.)$ with parameters $\\theta$,\nand a high-resource fMRI dataset $D = \\{(x_i, y_i)\\}$, where each data $x_i$ is paired\nwith $y_i = [y_{(1,i)}, y_{(2,i)}, \\ldots, y_{(n,i)}, \\ldots, y_{(N, i)}]$ corresponding to $N$ high-resource tasks\n$T = \\{T_1, ...,T_n, \\ldots,T_N\\}$, our goal is to learn a new low-resource task $T_{target}$ by\nefficiently updating parameters $\\phi$ given the target task dataset $D' = \\{(x',y')\\}$\n$(|D| > |D'|)$. The number of updating parameters $\\phi$ is much smaller than that\nof trainable parameters $\\theta$ in the pre-trained model $f(.)$.\n\nOverview. Prompts in ScaPT are trainable embeddings that direct the model's\nresponses without changing its architecture. Shown in Figure 1, ScaPT operates"}, {"title": "Deeply-conditioned Input-Prompt (DIP) Mapping.", "content": "Previous research\nindicates that soft prompts might not match input embedding spaces [9], mak-\ning direct attention between input and prompts unreliable. A proposed solution\ninvolves a network for projecting inputs into prompt spaces [1], but it falls short\nin multitasking scenarios by projecting inputs uniformly, thus failing to cap-\nture task-specific information. It also increases training parameters due to the\naddition of a separate network and relies on linear projections, which may not\neffectively represent the complex relationships between inputs and prompts.\nTo address the above issues, we propose Deeply-conditioned Input-Prompt\n(DIP) mapping M through reusing $f(.)$ (Figure 1). We introduce learnable deep\nconditioning (DC) tokens $C = \\{c_1, ..., c_n, c_N|c_n \\in R^{e\\times m}\\}$ that are prepended\nto the input, to guide $f(.)$ to map the input to an appropriate prompt space\nconditioned on the given task. The prediction output from $f(\\cdot)$ is then fed into a\nlinear projection layer and a Layer Norm LN(\u00b7) to avoid gradient explosion [1].\nTo deeply guide the conditional mapping from input to prompt spaces, we inject\nDC to every layer of $f(\\cdot)$. Formally, the input-prompt mapping is defined as:\n\n$H_{(n,i)} = f([c_n; X_i]); \\quad \\hat{H}_{(n,i)} = LN(NonLinear(WH_{(n,i)}))$"}, {"title": "Source Training (ST).", "content": "A healthy cohort encompasses a large number of par-\nticipants, with various phenotypes associated (high-resource tasks). In the first\nstage - Source Training (ST), we aim to train a set of phenotype prompts (PheP)\nthat encapsulate information on different behavior-relevant brain phenotypes. It\nwill serve as a source of knowledge for downstream tasks.\nTo better capture the relationship between input fMRI and prompts, and\nboost the capacity of $Phep$ to match the complexity of input, we model each\nparticipant-wise PheP as an interpolation of a set of modular prompts (MOP),"}, {"title": "Target Training (TT).", "content": "Prompts for a new task could be blended with pre-\ntrained prompts to incorporate gained knowledge [1]. In the second stage - Target\nTraining (TT), we first initialize a target prompt $P_{target}$ tailored for a target\ntask. To capitalize on the insights embedded in $P_n$, we learn a vertex prompt\n$P^*$, by interpolating $P_n$ and $P_{target}$ given attention computed by M (Figure 1).\nSimilar to ST, the goal of TT is to maximize the likelihood of predicting the\ncorrect target task label $y'$, given the concatenation of $P^*$ and input $X'$:\n$\\underset{P_{target}, M}{max} \\quad po(y'|[P^*; X']); \\quad P^* = P_{target} + \\sum_{n=1}^{N} \\beta_n \\cdot P_n$\nwhere $P^*$ is the interpolation of $P_{target}$ and $P_n$. $\\beta_n$ is the attention score\nbetween $X'$ and the $P_n$ computed by M."}, {"title": "3 Experiments", "content": "Datasets. In ST, resting state fMRI data from 656 participants of the Lifespan\nHuman Connectome Project Aging (HCP-A) [3,6] were analyzed to predict 38\nphenotypes (see details in the supplementary material), alongside sex and age.\nThe brain-behavior phenotypes established at this stage include three domains:\ncognition, personality, and social emotion. We hypothesized that ScaPT would\nperform well in tasks relevant to these domains. In TT, ScaPT was assessed on"}, {"title": "Training Details.", "content": "We adopt the state-of-the-art fMRI language model [16],\nwith causal sequence modeling structure, for our downstream adaptation. It was\npre-trained using 11,980 runs of 1,726 individuals across 34 datasets. The pre-\ntrained model contains 4 GPT-2 layers [12], with 12 attention heads in each self-\nattention module. To utilize the pre-trained model, the input must be parcellated\nby Dictionaries of Functional Modes (DiFuMo) [4]. The preprocessed input to\nthe model is $X \\in R^{l\\times m}$ obtained by DiFuMo, where $l$ is the input sequence\nlength and $m = 1024$ networks. The hidden dimension in the model is $h = 768$.\nWe utilized $K = 5$ modular prompts. Prompt/DC length is $d = e = 5$. Refer to\nsupplementary for more hyper-parameters and experimental settings."}, {"title": "Main Results.", "content": "In a scenario with limited training data, we evaluated ScaPT\nagainst fine-tuning and three multitask-based prompt-tuning approaches: SPOT\n[17], MP2 [15], and ATTEMPT [1] (Table 1 and 2). For fine-tuning, $f(.)$ un-\nderwent direct fine-tuning using TT datasets (ADNI/UKB). Meanwhile, for the\nmultitask-based methods, prompts were initially trained on ST datasets (HCP-\nA), which were then served as the prompt initialization in the TT stage.\nScaPT demonstrated superior performance over both fine-tuning and other\nprompt tuning methods across various sizes of training datasets, scaling well\nwith the number of training data. This underscores ScaPT's effectiveness in\ntransferring knowledge from high-resource tasks to those with scarce resources."}, {"title": "Prompt Interpretation.", "content": "In the ST stage, we created 40 phenotype prompts\nfor 38 phenotype predictions, alongside age and sex determinations. Remarkably,\nthese prompts naturally formed into three clusters - Personality, Social Emotion,\nand Cognition - without prior supervision, indicating they effectively capture\ndifferent pillars of brain-behavior associations (Figure 2-1).\nDuring the TT stage, the attention scores between input and phenotype\nprompts (Figure 2-2) aid in interpreting the target task. Attention score vec-\ntors were averaged across inputs, and then attributes within each group were\naveraged (without the one for $P_{target}$). These five values were normalized for\nanalysis. Aligning well with the literature, ScaPT shows a focus on \"cognition\"\nor \"age\" in neurodegenerative disease diagnosis/prognosis task, while it focuses\non \"personality\" for neuroticism score prediction."}, {"title": "Ablation Study and Parameter-efficiency.", "content": "We evaluated ScaPT against\nits ablations (Figure 2-3), including ScaPT w/o PheP (using MoP directly for\n$P^*$ formulation without high-resource task training), ScaPT w/o MoP (learn-\ning $P_n$ without prompt width expansion), and ScaPT w/o DIP (utilizing AT-\nTEMPT's subnetwork for input-prompt mapping). The absence of $Phep$ led\nto a significant performance drop, underscoring the importance of high-resource\ntask knowledge in boosting low-resource task performance. ScaPT outperformed\nits counterparts lacking MoP, demonstrating MoP's role in enhancing expressive\ncapacity by widening prompts. Additionally, ScaPT's DIP module surpassed AT-\nTEMPT's subnetwork in mapping inputs to prompts, effectively capturing com-\nplex input-prompt relationships using $f(\\cdot)$. In Figure 2-4, we compared ScaPT's\nperformance with other models relative to their trainable parameters. ScaPT\nsignificantly outperformed fine-tuning, MP2, and ATTEMPT, despite updating\nonly 2% of total parameters. Although SPOT had the fewest trainable parame-\nters, its performance lagged, likely due to its limited feature capture capability."}, {"title": "4 Conclusion", "content": "We introduce Scaffold Prompt Tuning (ScaPT), the first prompt-based adapta-tion framework for large-scale fMRI pre-trained models, compatible with very limited training data. ScaPT features a hierarchical prompt structure that facil-itates knowledge transfer from high-resource tasks to those with fewer resources. Moreover, we develop a Deeply-conditioned Input-Prompt (DIP) mapping to capture the complex relationship between the input and prompt spaces. Our experiments demonstrate ScaPT's exceptional parameter efficiency and its su-perior performance in neurodegenerative disease diagnosis or prognosis, as well as personality trait prediction from resting-state fMRI data. In addition, our at-tention mechanism offers semantic interpretation for target tasks. Future studies could expand ScaPT's reach to longitudinal data, and explore its possibility for prompting vision models."}]}