{"title": "Prompt Your Brain: Scaffold Prompt Tuning for Efficient Adaptation of fMRI Pre-trained Model", "authors": ["Zijian Dong", "Yilei Wu", "Zijiao Chen", "Yichi Zhang", "Yueming Jin", "Juan Helen Zhou"], "abstract": "We introduce Scaffold Prompt Tuning (ScaPT), a novel prompt-based framework for adapting large-scale functional magnetic resonance imaging (fMRI) pre-trained models to downstream tasks, with high parameter efficiency and improved performance compared to fine-tuning and baselines for prompt tuning. The full fine-tuning updates all pre-trained parameters, which may distort the learned feature space and lead to overfitting with limited training data which is common in fMRI fields. In contrast, we design a hierarchical prompt structure that transfers the knowledge learned from high-resource tasks to low-resource ones. This structure, equipped with a Deeply-conditioned Input-Prompt (DIP) mapping module, allows for efficient adaptation by updating only 2% of the trainable parameters. The framework enhances semantic interpretability through attention mechanisms between inputs and prompts, and it clusters prompts in the latent space in alignment with prior knowledge. Experiments on public resting state fMRI datasets reveal ScaPT outperforms fine-tuning and multitask-based prompt tuning in neurodegenerative diseases diagnosis/prognosis and personality trait prediction, even with fewer than 20 participants. It highlights ScaPT's efficiency in adapting pre-trained fMRI models to low-resource tasks.", "sections": [{"title": "1 Introduction", "content": "In the realm of neuroimaging, the emergence of large-scale, self-supervised pre-trained models/foundation models for fMRI, has demonstrated a significant"}, {"title": "2 Method", "content": "Problem Setup. Given an fMRI pre-trained model f(.) with parameters 0, and a high-resource fMRI dataset D = {(xi, yi)}, where each data x\u1d62 is paired with yi = [Y(1,i), Y(2,i), \u2026\u2026\u2026, Y(n,i), \u2026\u2026\u2026, Y(N, \u2170)] corresponding to N high-resource tasks T = {T1, ..., Tn, \u2026\u2026, TN}, our goal is to learn a new low-resource task Ttarget by efficiently updating parameters \u00f3 given the target task dataset D' = {(x',y')} (|D| > |D'|). The number of updating parameters o is much smaller than that of trainable parameters o in the pre-trained model f(.).\nOverview. Prompts in ScaPT are trainable embeddings that direct the model's responses without changing its architecture. Shown in Figure 1, ScaPT operates"}, {"title": "Deeply-conditioned Input-Prompt (DIP) Mapping.", "content": "Previous research indicates that soft prompts might not match input embedding spaces [9], making direct attention between input and prompts unreliable. A proposed solution involves a network for projecting inputs into prompt spaces [1], but it falls short in multitasking scenarios by projecting inputs uniformly, thus failing to capture task-specific information. It also increases training parameters due to the addition of a separate network and relies on linear projections, which may not effectively represent the complex relationships between inputs and prompts.\nTo address the above issues, we propose Deeply-conditioned Input-Prompt (DIP) mapping M through reusing f(.) (Figure 1). We introduce learnable deep conditioning (DC) tokens C = {c1, ..., cn, cN|cn \u2208 Rexm} that are prepended to the input, to guide f(.) to map the input to an appropriate prompt space conditioned on the given task. The prediction output from f(\u00b7) is then fed into a linear projection layer and a Layer Norm LN(\u00b7) to avoid gradient explosion [1]. To deeply guide the conditional mapping from input to prompt spaces, we inject DC to every layer of f(\u00b7). Formally, the input-prompt mapping is defined as:\nH(n,i) = f([cn; Xi]); \u0124(n,i) = LN(NonLinear(WTH(n,i))  (1)\nwhere Xi \u2208 Rlxm is the \u201ctext-like\" representation generated from fMRI following [16], describing the signal of each of m brain networks for each time point l. X\u2081 prepended by DC [cn; Xi] \u2208 R(1+e)\u00d7m is input to the frozen f(.). H(n,i) \u2208 Rh is the conditional output from f(\u00b7), with h representing the hidden dimension. W \u2208 Rh\u00d7h is the projection parameter to be updated during training, and \u0124(n,i) \u2208 Rh is the projected input."}, {"title": "Source Training (ST).", "content": "A healthy cohort encompasses a large number of participants, with various phenotypes associated (high-resource tasks). In the first stage - Source Training (ST), we aim to train a set of phenotype prompts (PheP) that encapsulate information on different behavior-relevant brain phenotypes. It will serve as a source of knowledge for downstream tasks.\nTo better capture the relationship between input fMRI and prompts, and boost the capacity of Phep to match the complexity of input, we model each participant-wise PheP as an interpolation of a set of modular prompts (MOP),"}, {"title": "Target Training (TT).", "content": "Prompts for a new task could be blended with pre-trained prompts to incorporate gained knowledge [1]. In the second stage - Target Training (TT), we first initialize a target prompt Ptarget tailored for a target task. To capitalize on the insights embedded in Pn, we learn a vertex prompt P*, by interpolating Pn and Ptarget given attention computed by M (Figure 1). Similar to ST, the goal of TT is to maximize the likelihood of predicting the correct target task label y', given the concatenation of P* and input X':\nmax po (y'][P*; X']); P* = Ptarget + \u03a3\u03b2n. Pn (4)\nPtarget, M\nn=1\nwhere P* is the interpolation of Ptarget and Pn. \u1e9en is the attention score between X' and the Pn computed by M."}, {"title": "3 Experiments", "content": "Datasets. In ST, resting state fMRI data from 656 participants of the Lifespan Human Connectome Project Aging (HCP-A) [3,6] were analyzed to predict 38 phenotypes (see details in the supplementary material), alongside sex and age. The brain-behavior phenotypes established at this stage include three domains: cognition, personality, and social emotion. We hypothesized that ScaPT would perform well in tasks relevant to these domains. In TT, ScaPT was assessed on"}, {"title": "4 Conclusion", "content": "We introduce Scaffold Prompt Tuning (ScaPT), the first prompt-based adaptation framework for large-scale fMRI pre-trained models, compatible with very limited training data. ScaPT features a hierarchical prompt structure that facilitates knowledge transfer from high-resource tasks to those with fewer resources. Moreover, we develop a Deeply-conditioned Input-Prompt (DIP) mapping to capture the complex relationship between the input and prompt spaces. Our experiments demonstrate ScaPT's exceptional parameter efficiency and its superior performance in neurodegenerative disease diagnosis or prognosis, as well as personality trait prediction from resting-state fMRI data. In addition, our attention mechanism offers semantic interpretation for target tasks. Future studies could expand ScaPT's reach to longitudinal data, and explore its possibility for prompting vision models."}]}