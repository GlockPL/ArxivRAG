{"title": "Parseval Regularization for Continual Reinforcement Learning", "authors": ["Wesley Chung", "Lynn Cherif", "David Meger", "Doina Precup"], "abstract": "Loss of plasticity, trainability loss, and primacy bias have been identified as issues arising when training deep neural networks on sequences of tasks all referring to the increased difficulty in training on new tasks. We propose to use Parseval regularization, which maintains orthogonality of weight matrices, to preserve useful optimization properties and improve training in a continual reinforcement learning setting. We show that it provides significant benefits to RL agents on a suite of gridworld, CARL and MetaWorld tasks. We conduct comprehensive ablations to identify the source of its benefits and investigate the effect of certain metrics associated to network trainability including weight matrix rank, weight norms and policy entropy.", "sections": [{"title": "1 Introduction", "content": "Continual reinforcement learning (RL) [30], a setting where a single agent has to learn in a complex environment with potentially changing tasks and dynamics, has remained a challenge for current agents. The core difficulty stems from training deep neural networks on sequences of tasks. Although the network may be able to learn the first task easily, after several tasks, progress may be impeded-a phenomenon termed plasticity loss [12, 44, 38, 37, 57, 1]. Other works have found that neural networks may be strongly influenced by data in the early phases of learning, leading to weaker performance on later tasks [5, 43, 2, 29, 58]. Taken together, these works have demonstrated the difficulty of learning in the presence of nonstationarity, an integral aspect of reinforcement learning problems.\nOptimization has historically been a barrier towards training neural networks, even on single tasks [25, 46]. The development of suitable parameter initialization schemes was a key ingredient to successful training, such as Xavier and Kaiming initializations [16, 22]. By prescribing a suitable variance for random Gaussian weights, these initialization strategies allow gradients to propagate throughout the network and avoid the vanishing and exploding gradient problems.\nThis line of work led to the development of orthogonal initialization [53], a technique designed to ensure better gradient propagation by making the singular values of the weight matrices all equal to 1 and thus maintaining the singular values of the Jacobian of the output with respect to the input to also be 1-a property known as dynamical isometry. Orthogonal initialization was used to allow training of a vanilla convolutional network with 10 thousand layers without the use of normalization layers [61]. Additionally, this technique has found success in RL settings with more shallow networks, being the default setting for PPO agents and improving upon Gaussian initialization strategies [28, 27]. Overall, this research direction has showcased the importance of encouraging model parameters to lie in regions amenable to optimization."}, {"title": "2 Preliminaries", "content": "The problems we consider are defined as sequences of Markov Decision Processes (MDP). An MDP is defined by its state space, action space, transition function, reward function and a discount factor. The agent aims to learn a policy \\( \\pi \\) that maximizes the expected return \\( \\mathbb{E}[\\sum_{t=0}^{\\infty} \\gamma^t R_t] \\). In this work, we will focus on sequences of tasks where the reward function changes after a certain number of timesteps while the transition dynamics stay the same. The changes are not signaled to the agent and thus there is nonstationarity in the environment that is difficult to model. This represents a simplified"}, {"title": "3 Parseval Regularization", "content": "Orthogonal initialization was initially proposed in the context of deep linear neural networks. It was shown that if orthonormal\u00b9 (orthogonal with unit norm row vectors) weight matrices are used, one would have depth-independent training times [53]. This result was expanded to show that nonlinear networks with tanh activations could also achieve better convergence times as a function of depth [47, 26]. These theoretical results were validated by successfully training networks with thousands of layers and orthogonal initialization was also effectively employed with shallow networks in a deep RL setting [54, 27.\nOrthogonal weight matrices are useful because they can ensure that the singular values of the weight matrices are all equal and, if these are equal to 1, it would mean that the layer forms an isometry- preserving distances, magnitudes and angles of its inputs and outputs (when the dimensions are matching) [48]. Since the Jacobian of a linear layer with weights \\( W \\) is simply \\( W^T \\), which is also orthogonal, the error gradients passed through in the backwards pass also maintain their structure as well, without exploding or vanishing (ignoring the activation function). This intuitively can lead to more favourable optimization.\nParseval regularization is implemented simply by adding a term to the usual objective. For each weight matrix \\( W \\) of a dense layer, we add the following regularizer:\n\n\\( L_{Parseval}(W) = \\lambda ||WW^T - sI||_F^2 \\)\n\nwhere \\( \\lambda > 0 \\) controls the regularization strength, \\( s > 0 \\) is a scaling factor, \\( I \\) is the identity matrix of appropriate dimension and \\( || \\cdot ||_F \\) denotes the Frobenius norm. This regularizing loss encourages the rows of \\( W \\) to be orthogonal to each other and also have a squared \\( l2 \\)-norm equal to \\( s \\). If these constraints are satisfied, all the singular values of \\( W \\) will be equal to \\( \\sqrt{s} \\). We directly add this regularization term to both the policy and value networks to every layer except the last. That is, the final objective is:\n\n\\( L(\\theta) = L_p(\\theta) + \\lambda_v L_v(\\theta) + \\sum_{k=1}^{(#layers)-1} L_{Parseval}(W_k) \\)\n\nwhere \\( \\theta \\) denotes all the parameter and \\( W_k \\) denotes the weight matrix of layer \\( k \\). \\( L_p \\) and \\( L_v \\) are the policy and value losses, with \\( \\lambda \\) and \\( \\lambda_v \\) being weighting coefficients.\nThe additional computational cost of this regularizer is of order \\( O(d^3) \\) for a dense layer of width \\( d \\) (with \\( d \\) inputs). This is similar to the cost of one forward pass when the size of the minibatch is close to \\( d \\). In practice, we have found that the net runtime increase ranges from 1.8% to 11.4% over the vanilla agent. Empirical runtimes and a more detailed analysis can be found in appendix C.7."}, {"title": "3.1 Network capacity and Lipschitz continuity", "content": "By restricting the weights to be orthogonal, the network may be overly constrained. In particular, if the weight matrices of all the layers are orthogonal and the activation function is Lipschitz, then the function given by the neural network is also Lipschitz. A Lipschitz function \\( f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m \\) satisfies: for all \\( x, y \\in \\mathbb{R}^n \\) \\( ||f(x) - f(y)|| \\le L||x - y|| \\) for some constant \\( L > 0 \\) and norm \\( || . || \\). This is a fairly strong constraint, meaning that the function values cannot vary too quickly as the inputs change, which may be overly limiting the neural network's capacity to express complex functions.\nThus, to relax this Lipschitz condition, we test a few additions. Throughout all the experiments, we do not regularize the final layer to preserve some expressiveness. However, the network"}, {"title": "4 Experiments", "content": "Environments and baseline algorithms\nWe utilize a sequence of tasks with changes between each to force the agent to learn something new. How to choose these changes is an important design decision. Some common tasks in the continual learning literature such as Permuted MNIST [17] (shuffling the inputs at each change) or changing Atari games [1] have drastic changes between tasks, which may not reflect realistic variations that would face any practical agent. Due to these large changes, it often leaves little room for the agent to reuse previously learned knowledge.\nWe focus on producing different tasks by either changing the task (reward functions) while the transition dynamics generally remain the same, or by changing context variables such as wind or stiffness affecting the simulation properties while keeping the reward function fixed. These two types of nonstationarity can be natural. Agents may need to learn many different tasks over time in a single environment. e.g. a robot arm can both learn to push objects and, later, to grasp them. Reinforcement learning from human feedback (RLHF) is another instance where the reward function may change due to changing preferences although the environment otherwise stays the same. From another perspective, the agent may face different environmental conditions and would want to adapt to changes in them.\nWe run experiments in four sets of environments:\nThe first is a navigation task in a 15-by-15 Gridworld where the agent has to reach a varying goal location. The goal is fixed for multiple episodes and is then changed every 40 thousand steps. Each episode is limited to 100 timesteps. For the gridworld, the agent is evaluated every 5000 steps by running 10 episodes. The success rate is measured as the fraction of the episodes where the agent successfully reaches the goal (within 100 timesteps). We run 50 seeds. For training, the agent receives a reward equal to the length of the shortest path to the goal state (divided by 10 for scaling purposes). This is a dense, informative reward signal so the difficulty from exploration is minimized. As such, we can attribute any performance issues to the optimization aspects of training a deep RL agent.\nThen, we consider two environments from the CARL suite [9]: LunarLander and DMCQuadruped. To generate a sequence of tasks, we choose certain context variables (e.g. gravity, wind, joint stiffness) and vary them for each task. The same sequence of context variables are presented to all the agents. For LunarLander, the agent is trained for 10 million steps with task changes every 500 thousand steps. For DMCQuadruped, the task changes every 1.5 million steps up to 12 million steps. For both of these environments, we generate 20 different sequences of tasks and run each sequence for 3 seeds.\nAs a final benchmark, we use environments from the MetaWorld suite [62], where the majority of our experiments will be conducted. First, we run an RPO agent on all the environments and identify those where the agent can achieve a high success rate after 1M steps of training. This preliminary selection process results in a set of 19 environments (see Appendix C.4). These roughly match the tasks with high success rates indicated in [62] (appendix B).\nFrom this set of candidate environments, we produce sequences of 10 tasks by sampling environments. In total, we produce 20 sequences of tasks, where each task corresponds to one Metaworld environment. We use a stratified sampling approach to ensure that each of the 19 environments are present the same number of times (or a difference of one) in all the sequences. Moreover, each sequence of tasks does not contain the same environment twice. These choices promote a diversity of task orderings and task choices. Overall, we obtain 20 sequences of 10 tasks each. We call this benchmark Metaworld20-10. Each agent is run on every sequence with 3 seeds each.\nBase agent. We use the RPO agent [50] (a variant of PPO) for continuous actions or PPO for discrete actions, based on the implementation from CleanRL [28]. Some adjustments to the hyperparameters were made as specified in the appendix (Sec. C.5), with one set of hyperparameters being used across all Metaworld tasks and one set for each of the other environments. For the learning curves, we plot the interquartile mean along with a 90% confidence interval shaded region. Each curve is also smoothed with a window of 5. Hyperparameters were tuned around the values provided in the CleanRL implementation. For all the algorithms, small sweeps were conducted on relevant hyperparameters and the best setting was chosen.\nPerformance profiles. Due to the high variability in performance due to the nonstationarity of tasks, we favor looking at the entire distribution of the agent's performance. To do so, we plot performance profiles [3]. These plots correspond to 1 minus the empirical CDF of the measured evaluation statistic"}, {"title": "4.1 Utility of Parseval regularization", "content": "We first test the base RPO agent, the proposed addition of Parseval regularization as well as the baseline algorithms on the primary continual RL setting. Metaworld results are presented in Fig. 1 and other environments in Fig. 4.\nOverall, we see that the addition of Parseval regularization can greatly improve the performance of the base agent on the four set of tasks. While the alternative algorithms can also improve the baseline, Parseval regularization makes the largest difference. This is most apparent in the gridworld where it lets the agent make progress on almost every task while the agent often gets stuck at 0 success rate with the other algorithms (as indicated by the large drop near zero on the x-axis).\nFor the next sections, we focus our attention on the Metaworld benchmark and investigate various questions in detail, including testing varying the network architecture, ablation studies and analysing parameter properties throughout training."}, {"title": "4.2 Variations of architecture and algorithm", "content": "We check if the benefits of Parseval regularization carry over to different neural network architectures."}, {"title": "Activation functions", "content": "The tanh activation function is the default choice for PPO [28] and we have already seen from the baseline that adding Parseval regularization improves the agent's performance considerably. We validate that Parseval regularization benefits other choices of activation functions such as ReLU [41], mish [40], concatenated ReLU (CReLU) [1, 55] and MaxMin [4].\nReLU is a classic choice of activation function still widely in use. Mish, a smoother version of ReLU, and similar functions (e.g. GELU, Swish) [40, 24, 14, 51] have been shown to outperform the standard ReLU activation in deep learning tasks. Concatenated ReLU (CReLU) was used in the context of reinforcement to deal with the problem of dead units [1]. MaxMin was proposed in the design of Lipschitz neural networks [4] and, by maintaining the norm of backpropagated gradients, it may also be suitable in this continual RL setting."}, {"title": "Neural network widths", "content": "We investigate what happens when the width of the network is changed. To adapt Parseval regularization to different network widths, we ensure that the regularization strength is scaled appropriately by the square of the width. If the width is doubled, the regularization strength is divided by four. Results are summarized in Fig. 5. Again, Parseval regularization can benefit networks of reduced or increased widths."}, {"title": "4.3 Ablation studies", "content": "Parseval regularization can be split into two different components. It acts on the rows of the weight matrices and 1) encourages the angles between them to be 0 and 2) encourages the norms to be constant. We test these components separately to better identify the source of Parseval regularization's benefits.\nAngles between vectors We investigate whether a variant of Parseval regularization that only regularizes the angles is also helpful. This can assess the benefits of diversity in the weight vector directions. Specifically, if \\( W \\) denotes the weight matrix to be regularized, we first normalize the rows to have norm 1 to get \\( \\hat{W} = W/||W||_{row} \\) where division is row-wise and \\( ||W||_{row} \\) computes the \\( l2 \\) norm of each row. Then, we apply Parseval regularization on \\( \\hat{W} \\). This has the net effect of disregarding the norm of the row weight vectors while still regularizing the inner products towards zero."}, {"title": "Norm of the weights.", "content": "Parseval regularization regularizes the row weight vectors towards the initial scale, set to \\( \\sqrt{2} \\) by default. The norm of the weights is a common metric linked to plasticity loss, with growing norms potentially indicating training difficulties [12, 38]. In this view, regularizing the norms of the weight matrix rows could be beneficial in a similar fashion to weight decay.\nRevisiting Fig. 3, setting the number of subgroups to 64 (the width of the network) corresponds to applying only regularization to the weight norms, without regularizing angles. There is a small benefit over the baseline, but not nearly as much as full Parseval regularization.\nFindings. Both ablations improve the performance over the baseline but do not match full Parseval regularization, although using regularization on the angles makes a larger difference. From this, we can conclude that both components are critical to the success of Parseval regularization."}, {"title": "4.4 Analysis of training", "content": "To verify the impact of Parseval regularization on network properties throughout training, we inspect two measures of diversity: the stable rank of weight matrices and the correlation between weight vectors of neurons.\nStable rank. We first check the stable rank of the matrices, defined as \\( srank(A) = \\frac{\\Sigma_i \\sigma_i^2}{\\max_i \\sigma_i^2} \\), where \\( A \\) is an \\( n \\times n \\) matrix and \\( \\sigma_i (i = 1, ..., n) \\) are its singular values. This soft version of the rank will be equal to \\( n \\) if all the singular values are equal and is at most the standard rank. It is less sensitive to small singular values.\nJustification: Having a larger rank would indicate that the weight vectors span a greater portion of the input space. A similar notion of rank has previously been found to be correlated to the performance of reinforcement learning agents [32, 37, 12] in certain settings. In particular, excessively low rank values can be linked to poor performance. Since Parseval regularization encourages matrices to be orthogonal and thus have all equal singular values, we would expect its addition to make matrices closer to full rank, potentially boosting performance.\nFindings. We find that Parseval regularization can increase the stable significantly, leading the matrices to maintain almost full rank, while the baselines all experience a quick reduction in rank during training before stabilizing at a small value (often less than 10). See plots in Appendix B.1 for details.\nNeuron weight similarity. We also verify another measure of diversity, corresponding to the average cosine similarity of the row vectors of a weight matrix. Symbolically, for a given weight matrix \\( W \\) with \\( n \\) rows, it is given by: \\( \\frac{1}{n(n-1)} \\sum_{i \\neq j} \\frac{W_i \\cdot W_j}{||W_i|| ||W_j||} \\) where \\( w_i, i \\in (1, ..., n) \\) denotes the i-th row of \\( W \\).\nJustification: Weight similarity measures how different the directions of the weight vectors are from each other. So, the lower the weight correlation, the greater the diversity amongst the neurons. With orthogonal initialization, we expect direction correlation to be zero at the start of training (or near-zero if the weight matrix has more rows than columns so there are more vectors than dimensions in the space). This measure was also used by [10] under the name forward correlation.\nOne may expect that a smaller neuron weight correlation would be favourable since there will be greater diversity in the \u201cactive\" regions of the activation function and cover the space of pre-activations more fully. This could be especially important when there is nonstationarity in the data (as in RL), so state inputs or intermediate activations may lie in previously uncovered regions of the space. Maintaining some diversity pre-emptively could yield faster learning when change occurs. There is"}, {"title": "5 Related works", "content": "Previous works have tackled the problem of loss of plasticity from different angles. Many algorithms in this line of work focus on injecting new randomness into the weights to restore some of the initial randomness present in the usual Gaussian weights. Aside from Shrink-and-Perturb discussed previously, other algorithms algorithms focus on identifying useful neurons and resetting the weights of those deemed as unnecessary [12, 57]. These algorithms use a notion of usefulness that is dependent on using ReLU activations, which is not directly applicable to other nonlinearities such as the tanh activation. Another approach is to reset the weights entirely for certain layers [43, 13] to fresh weights. While this can solve trainability issues, it also removes any possibility of using learned representations from previous tasks to speed up future learning. Moreover, the frequency of the resets can be an important hyperparameter. Methods like Parseval regularization that act on the network architecture (parameters, activations or normalization layers), naturally avoid having to specify the timescale at which change occurs. Expanding the network by adding more neurons to the network during training can also be used to improve trainability [44]. This comes with increasing compute and memory costs as the agent interacts with the environment, making it less appealing for continual learning settings where, in principle, an infinite number of task changes may occur.\nIn the continual learning community, many works have tackled how to learn efficiently from sequences of tasks, although they have mainly focused on the problem of catastrophic forgetting [15, 35, 31, 60], that is, how to remember previous solutions to previous tasks without overriding them when learning on new tasks. Many of these investigations have focused on supervised learning in the past. In this paper, we focus mainly on improving the plasticity of neural networks rather than the stability, which may be a priority in RL settings where the agent is focused on improving its current policy rather than remembering all past policies.\nParseval regularization was originally proposed in the context of maintaining Lipchitz constraints to improve adversarial robustness [11, 4] with little focus on the optimization aspects. Many other techniques have been proposed to maintain orthogonal weight matrices, including regularization [18], parameter scaling [49] and specialized parameterizations [4, 56]. In this work, we have focused on the simplest method-directly regularizing the weight matrices to be orthogonal\u2014although future investigations may find some benefit in using more advanced techniques.\nOrthogonal initialization [53] and later developments in initialization schemes such as ReZero [7] and Fixup [64] have used similar design principles such as maintaining an input-output Jacobian near an identity matrix to maintain gradient magnitudes across layers and avoiding exploding and vanishing gradients. These later initializations are tailored to Resnet-based architectures featuring skip connections [23] and have focused on the optimization properties at initialization whereas, in this paper, we emphasize the entire training process."}, {"title": "6 Discussion and Limitations", "content": "We take an optimization viewpoint on the continual RL problem and suggest improving performance by considering the optimization properties of network layers. Starting from the classic idea of using proper weight initializations, Parseval regularization aims to keep the weights in regions of the"}, {"title": "A.3 The role of entropy", "content": "In many environments, a well-performing agent seems to correlate well with a high level of entropy in the policy. For example, see Fig. 9 (right).\nEntropy regularization has been found to be theoretically beneficial for policy gradient algorithms by improving the curvature properties of the policy optimization objective. In practice, the baseline PPO agent does not use entropy regularization by default as it did not produce any benefit in continuous control tasks [27]. We revisit this finding and verify the use of entropy regularization in conjunction with PPO. Entropy regularization is added to the usual policy optimization loss as follows: \\( L(\\pi_{\\theta}) = L_{policy}(\\pi_{\\theta}) - \\lambda_{ent}H(\\pi_{\\theta}) \\) where \\( H(\\pi) = \\int \\pi(x) \\ln{\\pi(x)} dx \\) is the entropy of the policy \\( \\pi \\), \\( \\lambda_{ent} > 0 \\) is a weighting term."}, {"title": "A.4 Input-output Jacobian", "content": "The input-output Jacobian is defined as the matrix of derivatives of outputs with respect to the inputs. It has been as useful tool to derive initializations to train very deep neural networks through the concept of dynamical isometry, the property that input-output Jacobian has singular values equal to 1 [53, 47]. This property allows gradients to propagate backwards while avoiding the vanishing and exploding gradient problems [20]. [19] more directly studied the entries of the input-output Jacobian and found that the variance among the (square of the) entries was also an important quantity, with a low variance being crucial to favourable optimization properties.\nBalduzzi et al. [8] found that the input-output Jacobian could also be used to study the optimization benefits of Resnets [23]. They identify the shattered gradients problem which is indicated by the input-output Jacobian being sensitive to perturbations in the input.\nWe examine the input-output Jacobian matrix at various points in training to glean insight into the trainability of the network. Specifically, we save a batch of test states for an environment obtained by the policy during training. We compute the Jacobian matrix of the outputs with respect to the inputs of the network and inspect its entries on these test states. We are interested in inspecting the"}]}