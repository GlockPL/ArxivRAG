{"title": "Event Detection via Probability Density Function Regression", "authors": ["Clark Peng", "Tolga Din\u00e7er"], "abstract": "In the domain of time series analysis, particularly in event detection tasks, current methodologies predominantly rely on segmentation-based approaches, which predict the class label for each individual timesteps and use the changepoints of these labels to detect events. However, these approaches may not effectively detect the precise onset and offset of events within the data and suffer from class imbalance problems. This study introduces a generalized regression-based approach to reframe the time-interval-defined event detection problem. Inspired by heatmap regression techniques from computer vision, our approach aims to predict probability densities at event locations rather than class labels across the entire time series. The primary aim of this approach is to improve the accuracy of event detection methods, particularly for long-duration events where identifying the onset and offset is more critical than classifying individual event states. We demonstrate that regression-based approaches outperform segmentation-based methods across various state-of-the-art baseline networks and datasets, offering a more effective solution for specific event detection tasks.", "sections": [{"title": "Introduction", "content": ""}, {"title": "Background and Problem Statement", "content": "In the past decade, neural networks have become increasingly popular for addressing multivariate time series problems, including both event detection (ED) and change-point detection (CPD). Event detection refers to identifying significant occurrences or changes that last a meaningful duration of time within a series, while change-point detection involves identifying singular points where the statistical properties of the series change suddenly. For the purposes of this study, we refer to the time-interval-based event detection task as event detection and the opposite task as change-point detection, in which events last a negligible or no duration of time. Applications of event detection are numerous, such as sleep staging, sleep detection and seizure detection, as well as bow-shock event detection [4,14,15]. The model architectures most recently employed for these time series problems include sequence-to-sequence (seq2seq) models based on recurrent neural networks such as gated recurrent units (GRUs), long-short-term memory units (LSTMs), and/or one-dimensional convolutional neural networks (1D CNNs), often featuring a UNet-like (hourglass) topology [14, 15, 22].\nTraditionally, event detection strategies have predominantly employed segmentation-based methodologies. These approaches focus on predicting the class of each individual time step rather than the precise onset and offset of specific events [16,18]. However, segmentation may not always be the most effective approach, particularly when the goal is to detect specific event points within the time series.\nThis study introduces a generalized approach to reframe the event detection problem, inspired by heatmap regression solutions for two-dimensional keypoint detection in computer vision [12,24]. We redefine the event detection problem as a regression problem, utilizing deep learning models to predict probability density functions (pdfs) at each event's onset and offset locations rather than class labels across the entire time series.\nAs it is a regression method it is also robust to class imbalance problems exhibited by segmentation techniques [2,4]. Additionally, like the gaussian regression targets used in two-dimensional keypoint detection, our approach addresses labeling ambiguities by approximating a probability distribution instead of relying on hard ground-truth labels [12,24]. This is particularly important in time series data, where high-resolution sampling often contrasts with coarser event annotations, or when the labels contain human error. This solution for event detection can also be generalized to supervised change-point detection, which is an easier task than event detection, since it does not consider event durations.\nWe have also applied this regression method in the Child Mind Institute's (CMI) sleep detection competition on Kaggle in September 2023, utilizing it to detect sleep onset and wake events in children using wrist-worn"}, {"title": "Previous Work", "content": "Recent research in event detection has introduced regression-based methodologies [4]. These methods use a function defined by the Intersection over Union (IoU) metric applied to overlapping partitions at event locations to set their regression targets. Although designed to optimize the detection of event onsets and offsets, these approaches often generate fixed target distributions focused on predicting event midpoints, assuming uniform event durations. This assumption complicates the accurate identification of event boundaries, as events typically vary in duration. Furthermore, the reliance on overlapping partitions results in redundant memory usage and restricts the model's ability to leverage global context, limiting it to local features around specific timesteps.\nProbability density methods have been developed for sleep-detection problems by fitting Gaussian probability densities over event locations [17]. These methods involve an indirect process: first, decision trees predict the distance from each step to the event location, and then another tree predicts the event prediction's confidence. These decision trees are only trained on local partitions of data around each timestep. These predictions are aggregated by summing Gaussians across all timesteps, fitting a probability density to each event. This approach aids in event detection by identifying the centers of these densities via peak-detection. However, the requirement for extensive data processing and tabular modeling methods limits their applicability to modern time series frameworks and models.\nIn the following, we expand on these previous works by presenting our approach to applying a customizable probability density function (pdf) objective to two ED datasets, removing the need for overlapping partitions, and showing how this method can easily be applied to state-of-the-art seq2seq model architectures, which can incorporate global context, unlike previous work. In Section 3, we explain our approach, apply it to various state of the art deep learning models, and evaluate them on CMI's sleep detection dataset. In addition, we compare them against models trained on segmentation objectives to assess which method performs the best. Section 4 summarizes our main findings, and 5 discusses limitations and applications before we conclude in Section 6."}, {"title": "Regression Methodology", "content": "Our method involves convolving a pdf over an event series to produce a series with peaks at each event location. Since we are aiming to predict both the onset and offset of events, this necessitates two output target series. Our approach eliminates the need for partitioning by directly convolving a pdf rather than calculating an IoU over partitions."}, {"title": "Experiments and Evaluation", "content": ""}, {"title": "Data Description", "content": "To test event detection algorithms, we use two real publicly available real-world datasets in order to measure how our technique performs in different conditions. From the human activity recognition domain, Child Mind Institute's (CMI) Sleep Detection Dataset contains 2-dimensional accelerometer signals collected from a hand-worn wristband at a rate of 0.2 Hz from 250+ subjects [1]. The timestep when the subject begins sleeping and when the subject wakes up are considered event onsets and offsets.\nThe other event-detection dataset is from the brain activity domain. The CHB-MIT Scalp EEG Database contains 22-dimensional electrode signals collected at a rate of 256 Hz from only 22 subjects [21]. The signals used the International 10-20 system of EEG electrode positions and nomenclature [10]. Features were selected to utilize a bipolar montage for EEG recordings, where the voltage of one electrode is subtracted from another (e.g., FP1-F7). Only the subset of signals where seizures were detected were used.\nIncluding these datasets with varying sampling rates, signal lengths, and dimensionality provides a robust foundation for testing our regression framework. A more comprehensive description of the datasets are available in Appendix \u0412."}, {"title": "Evaluation", "content": "We will be using the Event Detection AP (EDAP) metric from the original CMI Sleep Detection competition. Event Detection AP is a soft ED metric, meaning that it can capture the degree to which a detection represents a particular event, incorporating the concept of temporal tolerance for inaccuracy in the evaluation [20].\nThis is different from hard metrics that only reward exact matches for specific thresholds [20]. As such, it is similar to IoU-threshold average precision metrics commonly used in object detection, but with IoU thresholds replaced by time tolerance values [18]. The advantage of this metric is that it naturally matches with the concave probability density functions that we propose.\nThe metric is evaluated using the following procedure:\n\u2022 Assignment: Predicted events are matched with ground-truth events.\n\u2022 Scoring: Each group of predictions is scored against its corresponding group of ground-truth events via thresholded average precision (AP).\n\u2022 Reduction: The thresholded AP scores are averaged to produce a single overall score.\n\u2022 The metric thresholds for the Sleep Detection Dataset, as provided by CMI, include the following values, in minutes:\n[1, 3, 5, 7.5, 10, 12.5, 15, 20, 25, 30]\n\u2022 The metric thresholds for the Seizure Detection Dataset were inferred based on previous findings, which state that 50% of seizures were detected within 3 seconds, 71% within 5 seconds, and 91% within 10 seconds [21]. With this in mind, we arbitrarily define the thresholds-in seconds-to be:\n[1, 2, 5, 10, 20, 60]"}, {"title": "Objective Distributions", "content": "Each model will be trained on the segmentation objective and 3 regression objectives created by different pdfs:"}, {"title": "Segmentation", "content": "We use the traditional way to train seq2seq models for ED problems using cross entropy loss with the binary segmentation task."}, {"title": "Regression", "content": "Three different probability density functions (pdfs) were utilized. These objectives serve to tailor the model's performance to suit various event detection tasks. By adjusting the spread of the pdfs, we can emphasize either overall event detection accuracy or precise event localization, thereby catering to different requirements in event detection problems, as indicated by the different EDAP thresholds.\n\u2022 Hard regression, similar to the binary classification problem with a regression objective. Defined as 1 at the event onsets and offsets and 0 everywhere else.\n\u2022 Gaussian regression, inspired by keypoint detection methods [12,24]. Guided by metric thresholds, we arbitrarily set \u03c3 = 50 ( 4 minutes) on the Sleep Dataset and \u03c3 = 256 (1 second) on the Seizure Dataset.\n\u2022 EDAP Regression, tailored to optimize the Event Detection AP metric specific to each dataset. This metric employs thresholds at intervals of 1, 3, 5, 7.5, 10, 12.5, 15, 20, 25, and 30 minutes for the Sleep Dataset and intervals of 1, 2, 5, 10, 20, and 60 seconds for the Seizure Dataset. As the event approaches the designated time thresholds, the regression target aligns with the metric, increasing accordingly.\nWe need to normalize each of these pdfs before using them to train the model. This ensures that the model converges consistently across various target distributions by aligning the initial expected loss from the model for each pdf. As such, we normalize the target by dividing the target sequence with the following scaling factor \u03b3:\n$\\gamma = \\frac{1}{w}\\sum_{i=0}^{w/2} \\frac{1}{d} \\approx \\frac{1}{w^2} \\int_{-w/2}^{w/2}f(x)dx \\qquad (1)$\nIn (2), w denotes the width of the regression target, and d is a constant that denotes the length of day in timesteps, in which we expect an event every d steps.\nThus, assuming the model initializes with normally distributed values, the model should initially output an average around zero for all steps in the time series. With this assumption in (2), applying the MSE loss function L to each initial prediction yields a loss of approximately 1 using (3):\n$\\hat{y} =  \\frac{[0 \\quad 0 \\quad ... \\quad 0]}{d}  \\qquad (2)$\n$L(\\hat{y}, y) = \\frac{1}{d} \\sum_{i=1}^{d} (\\hat{y_i}-y_i)^2 \\approx 1 \\qquad (3)$\nTransforming the target in this way ensures that all regression targets scale up to generate similar loss values, thereby ensuring training stability across different pdfs."}, {"title": "Preprocessing", "content": "Before feeding the training data into the models for deep learning, we preprocess it to reduce dimensionality and memory usage. This involves downsampling the time series by an order of magnitude, wherein aggregated features such as mean, standard deviation, maximum, and minimum are computed along each time segment for continuous features, which are then appended to the feature list. For categorical features, we take the last value in the downsampled time series."}, {"title": "Modeling and Training Parameters", "content": "We employed three custom-designed models alongside one state-of-the-art architecture. Our custom models include a bidirectional GRU model and a simplified fully convolutional encoder-decoder network, inspired by the U-Net architecture seen in U-Time [14,19]. Additionally, we adapted the U-Time model by incorporating an attention layer at the bottleneck to enhance feature representation and capture long-range dependencies [14].\nFor comparison, we incorporated one state-of-the-art model, PrecTime, which combines convolutional and recurrent layers to leverage both spatial and temporal features for precise event detection [6]."}, {"title": "Postprocessing", "content": "Figure 4 illustrates examples of various post-processing pipelines applied to raw regression and segmentation model outputs."}, {"title": "Regression Postprocessing", "content": "Our post processing pipeline for regression follows three simple steps with adjustable hyperparameters applied on a raw regression prediction:\n\u2022 Gaussian-smoothing step: Smooth all predictions along the time series with a gaussian filter, with hyper-parameter \u03c3 denoting the standard deviation of the gaussian kernel.\n\u2022 Finding Events: Apply a simple peak-finding function, for example, scipy.find_peaks, to find candidate peaks in the predictions. This function is similar to the peak detection algorithm detailed in [17].\n\u2022 Scoring Each Event: The EDAP metric requires each prediction to have a score assigned to it in order to rank the predictions and determine the optimal thresholds. Thus, we assign the peak probability of the probability density of an event to that event."}, {"title": "Segmentation Postprocessing", "content": "In order to compare our regression methods against segmentation models, we also use a separate post processing step for segmentation models to determine events. This involves setting a threshold that denotes a state change"}, {"title": "Regression Post-Processing Procedure", "content": "Inputs time series {Y_{on}(t)}_{t=0}^{T} and {Y_{off}(t)}_{t=0}^{T}, threshold \u03bc, smooth coef \u03c3\nT = [L/D] - sequence length\n1: procedure FINDEVENTS(\u0176, \u03bc, \u03c3)\n2: Initialize S an empty list  \u25b7 onsets\n3: Initialize E an empty list  \u25b7 offsets\n4: Initialize {\\hat{Y_{on}}(t)}_{t=0}^{T} \u2190 {gaussian\\_smooth(\\hat{Y_{on}}(t), \u03c3)}_{t=0}^{T}\n5: Initialize {\\hat{Y_{off}}(t)}_{t=0}^{T} \u2190 {gaussian\\_smooth(\\hat{Y_{off}}(t), \u03c3)}_{t=0}^{T}  \u25b7 perform gaussian smoothing\n6: S' \u2190 peak\\_detect({\\hat{Y_{on}}(t)}_{t=0}^{T})\n7: E' \u2190 peak\\_detect({\\hat{Y_{off}}(t)}_{t=0}^{T})\n8: for t \u2208 S' do\n9: S.append(t, \\hat{Y_{on}}(t))\n10: for t \u2208 E' do\n11: E.append(t, \\hat{Y_{off}}(t))\nOutputs S, E event locations with relative probability scores\nand then finding time steps where the probability for the event exceeds the threshold. We locate events by locating where the probabilities for a class exceed the given threshold. However, probabilities can go up for more than one timestep when an event is detected. It is possible that for one single event, the probabilities go up and down many times, even though they are usually expected to increase continuously afterwards. In other words, multiple changes in the output of the model can represent one single event. As such, the following steps are used to reduce the likelihood of this happening:\n\u2022 Gaussian-smoothing step: Smooth all probability predictions along the time series with a gaussian filter, with the hyperparameter \u03c3 denoting the standard deviation of the gaussian.\n\u2022 Finding Events: Use a threshold \u03bc to denote class thresholds. Events occur where the event class's predicted probabilities exceed \u03bc.\n\u2022 Scoring Each Event:\n$H(t) = \\begin{cases} 0, & t \\le -a, \\\\ -1/a, & -a \\le t \\le 0, \\\\ 1/a, & 0 \\le t \\le a, \\\\ 0, & a \\le t \\end{cases} \\qquad (4)$\n$I(t) = (\\hat{Y} * H)(t) \\qquad (5)$\nThe EDAP metric requires each prediction to have a score assigned to it in order to rank the predictions and determine the optimal thresholds. This is addressed by convolving the probability predictions along the time dimension using the following piecewise sliding window function in Equation (4) to obtain a pseudo-probability function I(t) mimicking to the probability function obtained using pdf regression, with a denoting the window size:\nA pseudo probability density distribution is generated by the procedure depicted in equation 5. The absolute value of this distribution is maximized at points where class probabilities change suddenly and remain changed. Consequently, the probability density generated by this function allow us to score each candidate event, giving more weight to events with more dramatic changes in class probabilities.\nThe above segmentation post-processing procedure will be referred to as Method 1.\nAlternatively, events can be directly detected performing a regression-based peak-finding procedure on the pseudo probability function generated by I. This peak-based event detection method will be referred to as Method 2. Both methods were tested and compared."}, {"title": "Traditional Segmentation Post-Processing Procedure (Method 1)", "content": "Inputs time series {\\hat{Y}(t)}_{t=0}^{T}, threshold \u03bc, smooth coef \u03c3\nT = [L/D] - sequence length\n1: procedure FINDEVENTS(\u0176, \u03bc, \u03c3)\n2: Initialize S an empty list  \u25b7 onsets\n3: Initialize E an empty list  \u25b7 offsets\n4: Initialize {\\hat{Y'}(t)}_{t=0}^{T} \u2190 {gaussian\\_smooth(\\hat{Y}(t), \u03c3)}_{t=0}^{T}  \u25b7 perform gaussian smoothing\n5: Initialize {I(t)}_{t=0}^{T} \u2190 {(\\hat{Y'} * H)(t)}_{t=0}^{T}  \u25b7 get pseudo probability density\n6: for t \u2208 1... T do\n7: if \\hat{Y'}[t \u2013 1] < \u03bc and \\hat{Y'}[t] > \u03bc then\n8: S.append(t, |I[t]|)\n9: else if \\hat{Y'}[t \u2013 1] > \u03bc and \\hat{Y'}[t] < \u03bc then\n10: E.append(t, |I[t]|)\nOutputs S, E event locations with relative probability scores"}, {"title": "Alternative Segmentation Post-Processing Procedure (Method 2)", "content": "Inputs time series {\\hat{Y}(t)}_{t=0}^{T}, threshold \u03bc, smooth coef \u03c3\nT = [L/D] - sequence length\n1: procedure FINDEVENTS(\u0176, \u03bc, \u03c3)\n2: Initialize S an empty list  \u25b7 onsets\n3: Initialize E an empty list  \u25b7 offsets\n4: Initialize {\\hat{Y'}(t)}_{t=0}^{T} \u2190 {gaussian\\_smooth(\\hat{Y}(t), \u03c3)}_{t=0}^{T}  \u25b7 perform gaussian smoothing\n5: Initialize {I(t)}_{t=0}^{T} \u2190 {(\\hat{Y'} * H)(t)}_{t=0}^{T}  \u25b7 get pseudo probability density\n6: S' \u2190 peak\\_detect({\\hat{I'}(t)}_{t=0}^{T})\n7: E' \u2190 peak\\_detect({-\\hat{I'}(t)}_{t=0}^{T})\n8: for t \u2208 S' do\n9: S.append(t, |I[t]|)\n10: for t \u2208 E' do\n11: E.append(t, |I[t]|)\nOutputs S, E event locations with relative probability scores"}, {"title": "Results", "content": ""}, {"title": "Experimental Setup", "content": ""}, {"title": "Training Parameters", "content": "The models were implemented using the Pytorch v2.0.0 framework [3]. The training parameters are shown in Table 1 and 2. The network training was performed using the Adam optimizer and a cosine learning rate scheduler with no restarts [9,11]."}, {"title": "Evaluation Parameters", "content": "Our post-processing step for pdf regression, as shown in Section 3.6.1, involves Gaussian-smoothing using a smoothing parameter \u03c3. Similarly, our process for segmentation uses the tunable smoothing parameter \u03c3 and an additional tunable event threshold \u03bc.\n$\\mu \\in \\{0, \\frac{1}{10}, \\frac{2}{10}, \\frac{3}{10}, ..., 1\\} \\qquad (6)$\n$\\sigma \\in \\{None, 1, 10, 100, 1000\\}$\nIn our experiments, we compared our model's evaluation by deploying default hyperparameters \u03bc = 0.5 and no gaussian smoothing step. The optimal values of these hyperparameters are estimated using grid search with the parameter grid shown in Figure 6 to maximize the EDAP metric."}, {"title": "Experimental Results", "content": "Regression vs Segmentation Models trained on pdf regression consistently achieved comparable or improved performance than the models trained on segmentation tasks (Table 3 and 4). The top-performing regression models also match or exceed the precision of segmentation models across all time tolerance thresholds, demon-strating that pdf regression is superior to the segmentation approaches on these two datasets (Table 5 and 6)\nImpact of peak-finding approaches There is a significant difference between framing the event detection challenge as a peak detection task versus a segmentation task. This is evident in the superior performance of regression-based methods compared to segmentation methods on the CMI dataset. The best regression approach achieved a tuned EDAP of 0.621, while the best segmentation method achieved 0.508 (Table 3). Even within segmentation models, employing peak-finding techniques instead of solely identifying class change locations can be beneficial. Segmentation Method 2, which uses a sliding window function to identify peaks from class probabilities, outperformed Method 1 in 75% of optimized models on the CMI Dataset, achieving a maximum EDAP of 0.508 compared to 0.477 for Method 1. However, Method 1 outperformed Method 2 on the CHB-MIT"}, {"title": "Discussion", "content": "We have demonstrated that the proposed pdf regression framework outperforms segmentation-based approaches in the given event detection problems. Additionally, our method can be applied to any seq2seq segmentation neural-network model by changing only the output layer, meaning that existing segmentation models can easily be converted to support our objectives.\nLimitations We may find more effective methods for extracting events from segmentation probabilities, po-tentially altering these experiments' results. This limitation could be tackled in future research. Moreover, our selection of datasets and evaluation metrics is limited, warranting further exploration to validate the suitability of pdf regression for various event detection problems. This could involve exploring different evaluation metrics or datasets.\nCross-Method Ensembling Our method supports the straightforward ensembling of predictions across multi-ple probability density functions and model architectures by blending output probability distributions. Addition-ally, the segmentation post-processing method Method 1 converts segmentation outputs into pseudo probability distributions that can be blended with regression outputs. This ensemble approach could combine the strengths of both segmentation and regression methods, leading to more precise event detection.\nOnline detection Our method could potentially apply to real-time event detection tasks using live data streams [2]. This could involve using a unidirectional GRU for regression on our target variable and utilizing its predictions to detect events in real time.\nMore PDFs Segmentation-based event-detection tasks often necessitate complex loss functions to optimize different task-specific objectives, such as early event detection, in which events should be detected as soon as they happen [16]. Our method could potentially simplify this process significantly. By adjusting a pdf function to be asymmetrical and left-skewed, prioritizing pre-event detection over post-event becomes feasible. This approach could potentially yield comparable results to customized loss functions, without the need for the fine-tuning and design efforts needed to design custom loss functions.\nAdaptive PDFs: In diffusion models, noise levels start high and are gradually reduced throughout the training process, which allows the model to generate higher-quality samples as it converges [8]. This gradual reduction of noise helps the model progressively refine its outputs. Similarly, in event detection, a comparable approach can be used where the variance of the probability distributions is systematically decreased over time. This reduction in variance enables the model to focus on finer temporal details as it converges. This method was effectively utilized in the CMI Sleep Detection Competition, where decaying target PDFs with reduced variability were employed to enhance the model's temporal accuracy as training progressed [5]. Future work could investigate how scheduling affects model performance.\nProblem Types We've only tested our method on a subset of event detection involving long term time-interval based events, but it's evident that our method should also be applicable to change-point detection. For CPD, this would involve treating each event point as only an onset and ignoring offsets, requiring only one output dimension instead of two, and performing pdf regression on this objective.\nFor preliminary experimental data on the point-based event detection case, the reader is referred to the additional materials presented in Appendix A."}, {"title": "Conclusion", "content": "This study presents a generalized regression-based approach to event detection, providing an alternative to traditional segmentation-based methods and existing regression-based approaches. We introduce techniques to normalize regression targets across unique pdfs and convert both regression and segmentation predictions into precise event locations. Specifically, we employ peak detection for regression predictions and various strategies for segmentation outputs, including peak identification and traditional event-point determination using thresholds.\nThis approach to event detection surpasses traditional segmentation methods in accuracy and effectiveness. By leveraging regression techniques, we provide a versatile and powerful tool for identifying events in time series data. Unlike existing regression approaches, our method supports the integration of state-of-the-art neural network models, enhancing its robustness and applicability."}, {"title": "Additional Experiments", "content": "We additionally evaluated our approach on two CPD datasets provided by [4]: the bow shock event detec-tion dataset and fraud-detection dataset. Table 7 and 9 lists the experimental training parameters for each dataset respectively, while Table 8 and Table 10 presents the validation results after training with 5 fold cross validation.\nIt is worth mentioning that the validation scheme used in our experiments are not the same used in [4]. Thus, results may not be entirely comparable."}, {"title": "Detailed Data Descriptions", "content": ""}, {"title": "CMI Sleep Detection Dataset", "content": "We conducted experiments on Child Mind Institute's (CMI) sleep detection dataset. The data is provided by CMI along with the Healthy Brain Network, a landmark mental health study based in New York City [1].\nThe data contains over 250 publicly available recordings of wrist-worn accelerometer data, collected using wear-able accelerometers on children spanning multiple days, where each time step corresponds to the accelerometer's reading over 5 seconds. The two types of events are onset and wakeup, which correspond to the onset of sleep and the onset of awakeness, respectively.\nThis dataset additionally contains the following specifications about sleep periods:\n\u2022 Each sleep period must be at least 30 minutes long.\n\u2022 A single sleep period can be interrupted by activity, provided that these interruptions do not exceed 30 consecutive minutes.\n\u2022 Sleep windows are only detected if the watch is worn for the entire duration of the monitoring period (details provided below).\n\u2022 Only the longest sleep window during the night is recorded.\n\u2022 If no valid sleep window is found, no sleep onset or wake-up event is recorded for that night.\n\u2022 Sleep events do not need to cross the dayline, and there is no strict limit on the number of sleep events per day. However, only one sleep window should be assigned per night.\n\u2022 The number of nights recorded for a series is approximately equal to the number of 24-hour periods in that series.\nThere are two input variables that are used to train the models:"}, {"title": "CHB-MIT Scalp EEG Seizure Detection Database", "content": "We conducted our experiments on a subset of the CHB-MIT Scalp EEG Database [7]. The original data provided by the Children's Hospital Boston and consists of EEG recordings from pediatric subjects with intractable seizures [21].\nThe entire database includes recordings from 22 subjects with a total of 23 cases. Subjects in this dataset were given anti-seizure medication. Each case originally contained between 9 and 42 continuous waveforms, collected from a single subject. These recordings have been segmented into one-hour waveforms, though some cases include longer segments (up to four hours). We only included the continuous waveforms with seizure events.\nThe EEG signals were sampled at 256 samples per second with 16-bit resolution. All signals used the Interna-tional 10-20 system of EEG electrode positions and nomenclature [10]. Features were selected to include the same 22 EEG signals, utilizing a bipolar montage for EEG recordings, where the voltage of one electrode is subtracted from another (e.g., FP1-F7). This method reduces common noise and improves the signal-to-noise ratio, enhancing the detection of localized brain activity by focusing on the potential difference between adjacent electrodes.\nElectrode were placed according to the International 10-20 system. Figure 5 highlights the bipolar electrode chains used in this study; each chain, for example, the Left Temporal Chain, subtracts neighboring measurements to produce signals like FP1-F7, F7-T7, and others. These pairs are critical for obtaining high-quality EEG signals by focusing on the differential voltages between adjacent electrodes."}]}