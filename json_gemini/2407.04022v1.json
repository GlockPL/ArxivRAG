{"title": "Learning Non-Linear Invariants for Unsupervised Out-of-Distribution Detection", "authors": ["Lars Doorenbos", "Raphael Sznitman", "Pablo M\u00e1rquez-Neila"], "abstract": "The inability of deep learning models to handle data drawn from unseen distributions has sparked much interest in unsupervised out-of-distribution (U-OOD) detection, as it is crucial for reliable deep learning models. Despite considerable attention, theoretically-motivated approaches are few and far between, with most methods building on top of some form of heuristic. Recently, U-OOD was formalized in the context of data invariants, allowing a clearer understanding of how to characterize U-OOD, and methods leveraging affine invariants have attained state-of-the-art results on large-scale benchmarks. Nevertheless, the restriction to affine invariants hinders the expressiveness of the approach. In this work, we broaden the affine invariants formulation to a more general case and propose a framework consisting of a normalizing flow-like architecture capable of learning non-linear invariants. Our novel approach achieves state-of-the-art results on an extensive U-OOD benchmark, and we demonstrate its further applicability to tabular data. Finally, we show our method has the same desirable properties as those based on affine invariants.", "sections": [{"title": "1 Introduction", "content": "Deep learning (DL) models can perform remarkably in controlled settings, where samples evaluated come from the same distribution as those seen during training. Unsurprisingly, real-world scenarios rarely allow for such controlled settings, and a mismatch between train and test distributions is often a reality instead. Additionally, evaluating out-of-distribution (OOD) samples comes with few guarantees, and model performance is typically poorer than expected. More insidiously, no obvious in-built way exists to identify when the evaluated sample differs from the training distribution. Jointly, these shortcomings limit the use of DL models in real-world settings, as their reliability cannot be taken for granted.\nConsequently, OOD samples need to be detected beforehand to ensure that unreliable model predictions for those samples can be dealt with appropriately. This problem has become known as OOD detection [15] and shares goals with related fields such as anomaly detection, novelty detection, outlier detection, one-class classification, and open-set recognition [43]. Here, we consider generalized OOD [53], where any distributional shift from the in-distribution should be identified.\nOOD detection can be divided into supervised and unsupervised OOD (U-OOD). Supervised OOD methods can access the labels of a downstream task or explicit OOD samples. In contrast, U-OOD methods operate solely on unlabeled training samples. The lack of training labels or OOD samples is an important reason why U-OOD is so challenging, as determining what should be consid- ered OOD is not always clear. Unlike the supervised case, one cannot rely on marking every sample that does not belong to one of the classes as OOD. \u03a4\u03bf address this, [8] proposed characterizing datasets with multiple data invariants. Specifically, data points that do not have the expected value for any of these invariants are deemed OOD. With this characterization, it is possible to assess what datasets can be used to evaluate U-OOD detectors by considering whether a potential dataset satisfies all the invariants in the training data. Formally, the data invariants characterization of U-OOD aims to define a set of functions over the training features with a (near-)constant value. The union of these functions is used at inference time to spot U-OOD samples by testing whether the invari- ants hold for a given new sample. When restricting invariants to affine functions, the problem can be cast in terms of principal component analysis (PCA) and achieves state-of-the-art results on a large-scale benchmark [8].\nHowever, it seems improbable that affine functions are sufficient to character- ize all invariants present in training datasets. Examples of their limitations are easily found, as exemplified in Fig. 1. Despite the potential benefits of non-linear invariants for U-OOD detection, their actual advantages are still unexplored. In this work, we propose to find non-linear invariants by modeling them with a volume preserving network, a bijective function inspired by normalizing flows that deforms the input space while preserving the volume almost everywhere by design. Since the network cannot perform a projection, any invariant dimen- sion at the network's output when processing the training data must necessarily be an invariant of the training data. We extensively evaluate our approach and demonstrate that non-linear invariants outperform previous U-OOD detection methods. Moreover, we show how our method extends to different modalities by its application to tabular data and its benefit over affine invariants.\nIn summary, our main contributions are (1) a generalization of the invariant- based characterization of U-OOD that allows for the inclusion of non-linearities, (2) a novel embodiment of this framework that can learn non-linear invariants, and (3) an extensive evaluation of our method and other state-of-the-art methods on two benchmarks, the large image benchmark from [8] and a novel tabular benchmark."}, {"title": "2 Related work", "content": "While developing new supervised OOD detection methods is an active area of research (e.g. [10, 15, 18, 19, 23, 25, 26, 32, 48]), their reliance on labeled datasets and trained classifiers limit their applicability. For the remainder of this section, we focus on unsupervised approaches.\nGenerative models have played an important role in U-OOD. In theory, gen- erative models make for excellent U-OOD detectors because of their capabil- ity to estimate complex data distributions. However, in practice, they fail even in straightforward cases [5, 31, 34, 46]. Various explanations and remedies for this have been proposed, based on, for instance, input complexity [46], back- ground information [41,52], architectural limitations [22], ensembles [5], or typ- icality [33, 35, 36]. Most recently, approaches based on diffusion models have gained popularity [27, 38, 47, 51], although they also require heuristics to func- tion, as using the estimated data likelihood is often insufficient.\nAlternatively, representation learning-based methods have been proposed for U-OOD. Here, a model is trained using a self-supervised approach, and a test sample is scored using the model's output probabilities [2, 16], or by a simple anomaly detector operating on the features of the model [4, 45, 49]. Initially, the self-supervised training task consisted of transformation prediction [2,16], while more recent methods use contrastive learning [4, 45, 49].\nRather than training a model with a self-supervised training task, state-of- the-art methods use a network pre-trained on a general dataset, such as Ima- geNet, to provide a strong foundation for the U-OOD task. Using these features directly already provides high performance [1,29,37,42], while other works adapt these features to a target domain using an OOD-specific loss function [31,39,40]. Our work follows the first line of work, relying on the features of a frozen pre- trained model. We use ResNet architectures to run competing baselines and facilitate comparisons with earlier works. However, our method is in no way restricted to this architectural choice.\nArchitecturally, our approach is closely linked to normalizing flows (NF) [21]. More precisely, our method resembles an NF where we only have volume-preserving operations and lack the generative objective. These choices set us apart from other NF-based OOD works [3,22,44,46] and are validated by our experiments. A closely related method from this field is the Denoising Normalizing Flow (DNF)"}, {"title": "3 Method", "content": "Given a training set {x\u1d62}\u1d62=1, with corresponding feature vectors f(x\u1d62) = f\u1d62 \u2208 \u211d\u1d30, we define an invariant following [8] as a non-constant function, g : \u211d\u1d30 \u2192 \u211d, such that g(f\u1d62) = 0, \u2200i. That is, g is an invariant if it computes a constant value (i.e., g(f\u1d62) = 0) for the training set elements but may compute different constant values for other elements. For convenience, we will stack the invariants in a single vector function g : \u211d\u1d30 \u2192 \u211d\u1d37 with g = (g\u2081,..., g\u2096). Our goal is to find a function g of invariants that satisfies\n\ng(f\u1d62) = 0 \u2200i,\n\ndet(J\u2089(f\u1d62)\u30fbJ\u2089\u1d40(f\u1d62)) \u2260 0 \u2200i,\n\nwhere J\u2089(f\u1d62) is the Jacobian of g evaluated at f\u1d62. The second condition ensures that no component of g is trivially constant and that there are no redundant invariants by making the Jacobian J full rank. The 0 level-set of g that satis- fies these conditions defines an implicit manifold on the feature space \u211d\u1d30. A test feature vector f will be considered OOD if it does not lie on the manifold (i.e., g(f) \u2260 0).\nHowever, noisy real-world data rarely lies on an exact manifold, and solving Eq. (1) for a reasonably regularized g is unfeasible even for a small number of invariants K in practice. Instead, as proposed in [8], we relax these conditions and find a set of soft invariants (i.e., functions that are approximately constant for all the training set elements). These are found by optimizing a soft version of Eq. (1),\n\nmin ||g(f\u1d62)||\u00b2\ns.t. det(J\u2089(f\u1d62)\u30fbJ\u2089\u1d40(f\u1d62)) \u2260 0 \u2200i.\n\nOnce the function g is found, test feature vectors are evaluated by measuring how much they violate each invariant compared to the elements of the training set. Specifically, a test vector f is scored by computing the ratios between the test squared error and the average training squared error,\n\ns(f) =\n\u03a3\u2096=\u2081\u1d37 g\u2096(f)\u00b2\ne\u2096\n,\n\nwhere e\u2096 is the mean squared error of the soft invariant g\u2096 on the training set,\n\ne\u2096 =\n1\nN\n\u03a3\u1d62 g\u2096(f\u1d62)\u00b2.\n\nIntuitively, strong invariants with low e\u2096 values will strongly influence the final score, while weak invariants with large e\u2096 values will effectively be ignored.\nThe work [8] simplified the problem by modelling invariants as affine func- tions g(f) = Af+b, which allowed for tractable solutions of Eq. (3). Specifically, it was shown that finding A and b could be done by applying PCA to the train- ing features and that Eq. (5) was equivalent to the square of the Mahalanobis distance."}, {"title": "3.1 Non-linear invariants", "content": "In this work, we relax the assumption of affine invariants and allow for a broader family of invariants by modeling the function g with a deep neural network g. Specifically, we impose the constraint of Eq. (4) in the neural network design by choosing an architecture that ensures full-rank Jacobians. Inspired by nor- malizing flows [21], we design a volume preserving network (VPN) as a bijective function : \u211d\u1d30 \u2192 \u211d\u1d30 composed of bijective operations with unimodular Ja- cobians. A volume-preserving approach prevents the network from learning a projection to a (near-)constant value, which would artificially create invariants. Preserving the volume forces the network to learn actual invariants instead of shortcuts.\nIn particular, we design our VPN by alternating rotation and coupling layers. Rotation layers are linear layers with orthogonal transformations and a bias vec- tor. We parameterize an orthogonal layer of n dimensions with a (2)-dimensional vector v and an n-dimensional bias vector b. The layer transforms an input vec- tor x as,\n\nr(x) = e^(v)\u00d7 \u2022 x + b,\n\nwhere (v)\u00d7 is the skew symmetric matrix with the elements of v, and e is the matrix exponential. The Jacobian of an orthogonal layer is the orthogonal ma- trix e^(v)\u00d7 and has, therefore, determinant 1. Coupling layers [6] use some of the components of the input vector to compute a transformation that will be applied to the remaining components,\n\n(x\u2090, x\u266d) = split(x),\ny = join(x\u2090 + t(x\u266d), x\u266d),\n\nwhere x and y are the input and output of the coupling layer, respectively, and t is a multi-layer perceptron (MLP) computing a translation. Unlike [6, 7], no scale factor is applied to keep the Jacobian unimodular. Both orthogonal and coupling layers are easily inverted. In particular, the inverse of an orthogonal layer is,\n\nr\u207b\u00b9(y) = e^([-v]\u00d7) \u2022 (y \u2013 b),\n\nand for the coupling layer,\n\n(y\u2090, y\u266d) = split(y),\nx = join(y\u2090 - t(y\u266d), y\u266d).\n\nThe composition of alternating rotation and coupling layers ensures that the complete VPN \u011d is an invertible function with unimodular Jacobian and is, therefore, volume-preserving almost everywhere. The invariant function g : \u211d\u1d30 \u2192 \u211d\u1d37 is defined by the first K outputs of the VPN, g = \u011d\u2081:\u2096. Its Jacobian J, corresponding to the first K rows of the Jacobian of \u011d, is also full rank, thus satisfying the constraint of Eq. (4) by design. Eq. (3) can now be solved efficiently by simply minimizing the forward loss,\n\nLfwd(f) = ||\u011d\u2081:\u2096(f) ||\u00b2.\n\nIn addition, we leverage the bijectivity of g to define a backward loss mini- mizing the reconstruction error between a training feature vector f and its re- construction,\n\nLbwd(f) = ||g\u207b\u00b9 (P\u2096.\u011d(f)) \u2013 f||\u00b2,\n\nwhere P\u2096 is a diagonal linear operator projecting the first K dimensions to 0, which zeroes the invariants. Although optimizing the forward loss implicitly min- imizes the backward loss, we found that explicitly introducing the backward loss improved the stability of the training and the performance in our experiments. Nonetheless, the backward loss by itself also encodes invariants: by reconstruct- ing the data from a representation where K dimensions are zeroed out with a volume-preserving network, all variance must be in the non-invariant dimensions for a good reconstruction, and the K zeroed dimensions will encode invariants. The final training loss is the sum of the forward and backward losses. A schematic of our approach can be found in Fig. 2\nTo illustrate our approach, we use the 2-dimensional toy example depicted in Figure 3. The data shown in Figure 3(a) has no affine invariant (i.e., there exists no affine g\u2096 for which (1/N) \u03a3\u1d62 g\u2096(x\u1d62)\u00b2 is close to 0). However, it does have a soft non-linear invariant, namely, the distance of the samples to the origin. We therefore set K = 1.\nAfter training, we pass the data through the network to obtain an invariant representation shown in Figure 3(b). The network has learned an almost constant dimension for the training data, the non-linear invariant, and the variability is encoded in the other dimension. On the other hand, the OOD samples are not invariant along this dimension and score higher than in-distribution samples when compared with Eq. (5).\nFigure 3(c) shows the result of reconstructing the data with the composition g\u207b\u00b9 \u25e6 P\u2096 \u25e6 g from Eq. (12). After zeroing the invariant with P\u2096, the recon- structed data lies in a one-dimensional manifold that minimizes the distance to the original data and reduces the backward loss while removing noise in the radial direction. Therefore, the invariant measures deviations from this manifold."}, {"title": "3.2 Multi-scale invariants", "content": "As in [8], we use a pre-trained CNN to compute feature descriptors at multiple scales. The CNN is applied to each input image x to generate a collection of feature vectors {f\u2097(x)} =1 by performing global average pooling on the activation maps at each layer l. During training, the training feature vectors {f\u2097(x\u1d62)} =1 at layer l are used to train a set of L invariant functions {g(\u2097)} =1 through the procedure described in the previous section. Each function g(\u2097) is trained with a different number of invariants K\u2097, which are hyperparameters of our method. At inference time, the test images x are evaluated by computing layer-wise scores s\u2097(f\u2097(x)) following Eq. (5),\n\ns\u2097(f) =\n\u03a3\u2096=\u2081\u1d37\u2097 g\u2096(\u2097)(f)\u00b2\ne\u2096(\u2097)\n,\n\nwhich are aggregated to compute the final invariant score,\n\nSinv(x) =\n\u03a3\nl=1\nLs\u2097(f\u2097(x))."}, {"title": "3.3 Scoring samples", "content": "We empirically found our invariant score of Eq. (14) to be complementary to a standard 2-NN score [1] and observed that combining the two scores leads to a further boost in performance. To compute the 2-NN score, we first define the 2-NN distance of a test sample at a layer l as\n\ndist-2nn\u2097(f) =\n1\n2\n\u03a3 || f - f\u2099||\u00b2,\nf\u2099\u2208N\u00b2(\u2097)(f)\n\nwhere N\u00b2(\u2097) (f) are the 2 nearest neighbours of f in the training set at layer l. As with the layer-wise invariant score, the 2-NN distances are normalized by the average 2-NN distances of the training set,\n\ns-2nn\u2097(f) = K\u2097\ndist-2nn(f)\n\u03a3\u1d62 dist-2nn(f\u2097(x\u1d62))'\n\nwhere the factor K\u2097 compensates for the difference in magnitude with respect to the invariant score s\u2097. In the denominator, the 2-NN distances are calculated for the training set elements to themselves, making each feature vector f\u2097(x\u1d62) its own first neighbor. To avoid this, we exclude the element f\u2097(x\u1d62) from the training set when computing dist-2nn(f\u2097(x\u1d62)). The 2NN score is computed as,\n\nS2nn(x) =\n\u03a3\nl=1\nLs-2nn\u2097 (f\u2097(x)),\n\nand the final score is the sum of the invariant and the 2NN scores,\n\nSfinal(x) = Sinv(x) + S2nn(x).\n\nWe will analyze the contribution of each of these terms to the detection perfor- mance in the ablation study of the results section."}, {"title": "4 Experiments", "content": "We use the U-OOD evaluation benchmark introduced in [8] and propose a new benchmark with shallow datasets for additional experiments. Both benchmarks are described below.\nGeneral U-OOD. The U-OOD benchmark introduced in [8] consists of 73 experiments spread over five tasks, each containing varying criteria for the in and out distributions. Three of the tasks have an unimodal training dataset: uni- class, containing 30 one-class classification experiments on the low-resolution CIFAR10 and CIFAR100 datasets; uni-ano, which consists of 15 experiments on the high-resolution MVTec images where the number of training images is limited; and uni-med, which has 7 experiments on different medical imaging modalities. The remaining two tasks use entirely different datasets as OOD. These are shift-low-res, containing the CIFAR10:SVHN experiment on which many OOD-detectors fail, and shift-high-res, comprising 20 experiments with the DomainNet dataset.\nShallow U-OOD. Collection of experiments on shallow anomaly detection datasets with tabular data where deep neural network features from images are unavailable. This benchmark aims to show the generality of our approach to other data modalities. We use six tabular datasets from [11]. These datasets were conceived for unsupervised anomaly detection and contain inliers and outliers intertwined within the data. To adapt the datasets to our OOD detection prob- lem, we pre-processed them by separating all the outliers and an equal number of inliers from each dataset and reserving them for the testing split. The remaining inliers were utilized as training data. The datasets included in the benchmark are thyroid, breast cancer, speech, pen global, shuttle and KDD99. Further details are provided in the appendix."}, {"title": "4.2 Baselines", "content": "For the General U-OOD benchmark, we compare our method NL-Invs against nine state-of-the-art methods. Six methods, DN2 [1], CFlow [12], DDV [31], DIF [37], MSCL [40], and MahaAD [42] that use the same ResNet-101 back- bone initialized with ImageNet pre-trained features, and three normalizing flow methods, Glow [21], IC [46], and HierAD [44].\nFor the Shallow U-OOD benchmark, we compare NL-Invs to the baselines MahaAD (Mahalanobis distance), DN2 (kNN), and DIF (Isolation Forest). The remaining baselines are bound to deep learning methods that cannot work with non-image or tabular data and are thus excluded from the comparison."}, {"title": "4.3 Implementation details", "content": "Our VPN architecture includes four rotation and coupling layers before the final rotation layer (N = 4 in Fig. 2). Each coupling layer comprises an MLP with four linear layers of equal size as its input, interspersed with ReLU activations.\nNL-Invs requires setting the number of invariants per layer K\u2097, as described in Sect. 3.2. Considering these values as independent hyperparameters would exponentially increase the search space and evaluation time. Instead, we set each K\u2097 to the largest number of principal components of the data at layer l that jointly explain less than p% of the variance, where p is a hyperparameter shared by all layers.\nWe utilized a ResNet-101 for the multi-scale feature extraction of Sect 3.2. We extract features from L = 3 feature maps at the end of the last ResNet blocks. Following [40], we normalize the feature vectors of the final layer to the unit norm for improved performance. In all our experiments, we train for 25 epochs with p set to 5 and a batch size of 64. We use the Adam optimizer [20] with a learning rate of 10\u207b\u00b3 linearly decaying to 10\u207b\u2074 over the epochs."}, {"title": "5 Results", "content": "This section describes the results obtained on the two benchmarks, followed by a multi-faceted analysis of the behavior of our method.\nGeneral U-OOD. The performances of NL-Invs and the other methods are shown in Tab. 1. Most methods behave inconsistently across the benchmark, with different methods scoring high for each task. For instance, CFlow is the best scoring method on uni-ano by a large margin. However, its high performance does not translate to the other experiments, where it is consistently among the lowest-scoring methods. DN2 struggles on shift-low-resbut scores consistently well on the other tasks. DIF achieves decent performance overall but reaches the second-best score on shift-high-res. MSCL and MahaAD are generally good, with MahaAD being superior to MSCL on all cases except uni-ano. However, NL-Invs is consistently among the best-performing methods, reaching the high- est mean score of the benchmark and the best score on uni-med, shift-low-res and shift-high-res. Moreover, it outperforms the normalizing flow methods CFlow, HierAD and IC by large margins.\nSome recent works claim that models pre-trained on ImageNet are not a good foundation for U-OOD detectors because they lead to catastrophic failures on seemingly extremely simple cases (e.g., CIFAR10:SVHN of task shift-low- res [14, 54]), and argue that U-OOD models should be trained from scratch instead. While we also observe catastrophic failure for DN2 and CFlow, we find that NL-Invs is able to reach high AUC without any modification to the underlying neural network. In addition, MahaAD, MSCL and to a lesser extent DIF still reach high scores on shift-low-res. The presumed failure of models based on pre-trained features for certain tasks might thus be related to other factors, such as incorrect processing of features or inappropriate hyperparameters, rather than an intrinsic inability.\nShallow U-OOD. Tab. 2 summarizes our results. Here, MahaAD is the worst performing method, matching NL-Invs's perfect score on breast can- cer and KDD99 but struggling on the other datasets. DIF achieves good per- formance except on speech, although it does not reach a perfect score on any dataset. DN2 performs very well, but NL-Invs is again the best method over- all.\nOverall, there is a clear benefit of NL-Invs over MahaAD on tabular datasets: our non-linear invariants approach improves upon the affine invariants approach by 10.6 percentage points of AUC on average across the six experi- ments. This large difference compared to the results on General U-OOD in Tab. 1 suggests that invariants in the deep features extracted from a neural network are linear to some extent."}, {"title": "5.1 Ablation study", "content": "We ablate our design choices in Tab. 3. The previous best method, MahaAD, uses linear invariants and reaches a score of 86.5 AUC on the General U- OOD benchmark. We find that our generalization of this formulation, which allows for learning non-linear invariants, reaches a new state-of-the-art of 87.2 AUC. Part of this improvement is by means of the backward loss. Furthermore, incorporating S2NN raises the performance even further to 87.9 AUC."}, {"title": "5.2 Other architectures", "content": "To show the applicability of NL-Invs to other architectures and model sizes, we show results on uni-class with varying models, including ConvNeXT and a vision transformer, in Tab. 4. All models use the same hyperparameters, and we extract the features from L = 3 feature maps at the last blocks for all models. In general, models with better performance on ImageNet lead to better U-OOD performance, with ConvNeXT reaching the best results."}, {"title": "5.3 Hyperparameter sensitivity", "content": "NL-Invs has one main hyperparameter, p. We show in Tab. 5 that NL-Invs is robust to the choice of p, with its performance changing by as little as 0.3 AUC on General U-OOD across a wide range of values."}, {"title": "5.4 Assessing invariants", "content": "We conduct an additional experiment on CIFAR10 following [8] to assess how NL-Invs incorporates the intuitive idea of invariants in practice. To this end, we compare how U-OOD methods handle different types of OOD datasets as the number of classes in the training set increases.\nWhen the training dataset contains only one class, samples belonging to dif- ferent classes should be considered outliers, as the class is an invariant. As the number of classes in the training set increases, samples belonging to classes not present in the training dataset should no longer be considered outliers, as the class identity is no longer an invariant. This behavior is shown in Fig. 4(left), where all methods perform as anticipated. Conversely, test samples that ex- hibit visual dissimilarity from the training set should always be considered out- liers, irrespective of the number of classes in the training set. As depicted in Fig. 4(right), our experimental findings indicate that invariant-based methods, namely MahaAD and especially NL-Invs, exhibit the expected behavior when test samples come from a different domain, where most of the test samples remain outliers despite the increase in training set classes. In contrast, the next-best per- forming method, MSCL, experiences a stronger decrease in performance."}, {"title": "5.5 Loss landscape analysis", "content": "The true U-OOD objective function is impossible to optimize due to the in- tractability of sampling the entire OOD space. Therefore, all U-OOD methods optimize a proxy loss function to approximate this underlying objective. This, in turn, leads to many U-OOD methods having no apparent correlation between training loss and OOD performance [40].\nData invariants offer a theoretically sound concept of U-OOD, whereby low training loss regions should correspond to high U-OOD performance and vice versa. To verify this empirically, we utilized [24]'s methodology to visualize train- ing loss and U-OOD AUC along two arbitrary directions in the weight space of the VPN. Our results, displayed in Fig. 5 for car: rest, confirm this proposition."}, {"title": "6 Conclusion", "content": "This work introduces a new U-OOD method that learns data invariants within a training set. Our framework, called NL-Invs, is the first volume-preserving approach to OOD detection. NL-Invs learns non-linear invariants over a set of training features and generalizes previous invariant-based formulations of U- OOD, reaching state-of-the-art performance when compared against competitive methods on a large-scale benchmark. Additionally, we validate our model on different tabular datasets, showing its generalizability and advantage over affine invariants.\nFinally, we confirm the results of [8] and observe that the performance of several U-OOD methods is highly sensitive, with the majority of techniques dis- playing inconsistent scores across various tasks. Nevertheless, invariant-based approaches maintain a prominent position in terms of consistency, with NL- Invs outperforming all other methods by achieving the highest overall perfor- mance and ranking as the top-performing technique on three out of five tasks on the General U-OOD benchmark, in addition to obtaining the best score on tabular data. All in all, U-OOD remains challenging due to its many inconsisten- cies. We believe that with proper evaluation set-ups and theoretically motivated approaches, such as those based on data invariants, significant progress can be made toward the reliable use of deep learning models in everyday settings."}, {"title": "A Supplementary Material", "content": "We provide further details for our Shallow U-OOD benchmark, which consists of the following six datasets from [11]:\nthyroid. Training: 6'666 samples consisting of 21 measurements of healthy thy- roids. Testing: 250 samples from healthy thyroids as inliers and 250 outliers from hyper-functioning and subnormal-functioning thyroids as outliers.\nbreast cancer. Training: 357 samples with 30 measurements taken from med- ical images from healthy patients. Testing: 10 inliers from healthy patients and 10 outliers from cancer instances.\nspeech. Training: 3'625 samples of 400-dimensional features extracted from record- ings of people speaking with an American accent. Testing: 61 American- accent recordings as inliers and 61 outliers from speakers with non-American accents.\npen global. Training: 719 samples of the digit '8' represented as a vector of 16 dimensions. Testing: 90 samples of '8' as inliers and 90 samples of other digits as outliers.\nshuttle. Training: 45'586 samples describing a space shuttle's radiator posi- tions with 9-dimensional vectors. Testing: 878 inliers from normal situations, 878 outliers taken from abnormal situations.\nKDD99. Dataset of simulated traffic in a computer network, where attacks are seen as anomalies and normal traffic as inliers. Training: 619'046 samples of 38 dimensions. Testing: 1'052 inliers from normal traffic and 1'052 outliers."}, {"title": "A.2 Additional results", "content": ""}]}