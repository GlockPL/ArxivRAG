{"title": "Learning Non-Linear Invariants for Unsupervised\nOut-of-Distribution Detection", "authors": ["Lars Doorenbos", "Raphael Sznitman", "Pablo M\u00e1rquez-Neila"], "abstract": "The inability of deep learning models to handle data drawn\nfrom unseen distributions has sparked much interest in unsupervised\nout-of-distribution (U-OOD) detection, as it is crucial for reliable deep\nlearning models. Despite considerable attention, theoretically-motivated\napproaches are few and far between, with most methods building on\ntop of some form of heuristic. Recently, U-OOD was formalized in the\ncontext of data invariants, allowing a clearer understanding of how to\ncharacterize U-OOD, and methods leveraging affine invariants have at-\ntained state-of-the-art results on large-scale benchmarks. Nevertheless,\nthe restriction to affine invariants hinders the expressiveness of the ap-\nproach. In this work, we broaden the affine invariants formulation to a\nmore general case and propose a framework consisting of a normaliz-\ning flow-like architecture capable of learning non-linear invariants. Our\nnovel approach achieves state-of-the-art results on an extensive U-OOD\nbenchmark, and we demonstrate its further applicability to tabular data.\nFinally, we show our method has the same desirable properties as those\nbased on affine invariants.", "sections": [{"title": "1 Introduction", "content": "Deep learning (DL) models can perform remarkably in controlled settings, where\nsamples evaluated come from the same distribution as those seen during training.\nUnsurprisingly, real-world scenarios rarely allow for such controlled settings, and\na mismatch between train and test distributions is often a reality instead. Addi-\ntionally, evaluating out-of-distribution (OOD) samples comes with few guaran-\ntees, and model performance is typically poorer than expected. More insidiously,\nno obvious in-built way exists to identify when the evaluated sample differs from\nthe training distribution. Jointly, these shortcomings limit the use of DL models\nin real-world settings, as their reliability cannot be taken for granted.\nConsequently, OOD samples need to be detected beforehand to ensure that\nunreliable model predictions for those samples can be dealt with appropriately.\nThis problem has become known as OOD detection [15] and shares goals with\nrelated fields such as anomaly detection, novelty detection, outlier detection,"}, {"title": "2 Related work", "content": "While developing new supervised OOD detection methods is an active area of\nresearch (e.g. [10, 15, 18, 19, 23, 25, 26, 32, 48]), their reliance on labeled datasets\nand trained classifiers limit their applicability. For the remainder of this section,\nwe focus on unsupervised approaches.\nGenerative models have played an important role in U-OOD. In theory, gen-\nerative models make for excellent U-OOD detectors because of their capabil-\nity to estimate complex data distributions. However, in practice, they fail even\nin straightforward cases [5, 31, 34, 46]. Various explanations and remedies for\nthis have been proposed, based on, for instance, input complexity [46], back-\nground information [41,52], architectural limitations [22], ensembles [5], or typ-\nicality [33, 35, 36]. Most recently, approaches based on diffusion models have\ngained popularity [27, 38, 47, 51], although they also require heuristics to func-\ntion, as using the estimated data likelihood is often insufficient.\nAlternatively, representation learning-based methods have been proposed for\nU-OOD. Here, a model is trained using a self-supervised approach, and a test\nsample is scored using the model's output probabilities [2, 16], or by a simple\nanomaly detector operating on the features of the model [4, 45, 49]. Initially, the\nself-supervised training task consisted of transformation prediction [2,16], while\nmore recent methods use contrastive learning [4, 45, 49].\nRather than training a model with a self-supervised training task, state-of-\nthe-art methods use a network pre-trained on a general dataset, such as Ima-\ngeNet, to provide a strong foundation for the U-OOD task. Using these features\ndirectly already provides high performance [1,29,37,42], while other works adapt\nthese features to a target domain using an OOD-specific loss function [31,39,40].\nOur work follows the first line of work, relying on the features of a frozen pre-\ntrained model. We use ResNet architectures to run competing baselines and\nfacilitate comparisons with earlier works. However, our method is in no way\nrestricted to this architectural choice.\nArchitecturally, our approach is closely linked to normalizing flows (NF) [21].\nMore precisely, our method resembles an NF where we only have volume-preserving\noperations and lack the generative objective. These choices set us apart from\nother NF-based OOD works [3,22,44,46] and are validated by our experiments. A\nclosely related method from this field is the Denoising Normalizing Flow (DNF)"}, {"title": "3 Method", "content": "Given a training set {xi}\u2081\u1d3a, with corresponding feature vectors f(xi) = fi \u2208 \u211d\u1d30,\nwe define an invariant following [8] as a non-constant function, g : \u211d\u1d30 \u2192 \u211d,\nsuch that g(fi) = 0, \u2200i. That is, g is an invariant if it computes a constant\nvalue (i.e., g(fi) = 0) for the training set elements but may compute different\nconstant values for other elements. For convenience, we will stack the invariants\nin a single vector function g : \u211d\u1d30 \u2192 \u211d\u1d37 with g = (g\u2081,..., g\u1d37). Our goal is to\nfind a function g of invariants that satisfies\n\ng(fi) = 0 \u2200i, (1)\ndet(J\u2089(fi)\u22c5J\u2089\u1d40(fi)) \u2260 0 \u2200i, (2)\n\nwhere J\u2089(fi) is the Jacobian of g evaluated at fi. The second condition ensures\nthat no component of g is trivially constant and that there are no redundant\ninvariants by making the Jacobian J full rank. The 0 level-set of g that satis-\nfies these conditions defines an implicit manifold on the feature space \u211d\u1d30. A\ntest feature vector f will be considered OOD if it does not lie on the manifold\n(i.e., g(f) \u2260 0).\nHowever, noisy real-world data rarely lies on an exact manifold, and solving\nEq. (1) for a reasonably regularized g is unfeasible even for a small number of\ninvariants K in practice. Instead, as proposed in [8], we relax these conditions\nand find a set of soft invariants (i.e., functions that are approximately constant\nfor all the training set elements). These are found by optimizing a soft version\nof Eq. (1),\n\nmin ||g(fi)||\u00b2\ng\ns.t. det(J\u2089(fi)\u22c5J\u2089\u1d40(fi)) \u2260 0 \u2200i. (3)\n(4)\n\nOnce the function g is found, test feature vectors are evaluated by measuring\nhow much they violate each invariant compared to the elements of the training\nset. Specifically, a test vector f is scored by computing the ratios between the"}, {"title": "3.1 Non-linear invariants", "content": "In this work, we relax the assumption of affine invariants and allow for a broader\nfamily of invariants by modeling the function g with a deep neural network g.\nSpecifically, we impose the constraint of Eq. (4) in the neural network design\nby choosing an architecture that ensures full-rank Jacobians. Inspired by nor-\nmalizing flows [21], we design a volume preserving network (VPN) as a bijective\nfunction \u011d : \u211d\u1d30 \u2192 \u211d\u1d30 composed of bijective operations with unimodular Ja-\ncobians. A volume-preserving approach prevents the network from learning a\nprojection to a (near-)constant value, which would artificially create invariants.\nPreserving the volume forces the network to learn actual invariants instead of\nshortcuts.\nIn particular, we design our VPN by alternating rotation and coupling layers.\nRotation layers are linear layers with orthogonal transformations and a bias vec-\ntor. We parameterize an orthogonal layer of n dimensions with a (\u207f/\u2082)-dimensional"}, {"title": "3.2 Multi-scale invariants", "content": "As in [8], we use a pre-trained CNN to compute feature descriptors at multiple\nscales. The CNN is applied to each input image x to generate a collection of\nfeature vectors {f\u2097(x)}\u2097\u208c\u2081\u1d38 by performing global average pooling on the activation\nmaps at each layer l. During training, the training feature vectors {f\u2097(xi)}\u2081\u1d3a\nat layer l are used to train a set of L invariant functions {g(\u2097)}\u2097\u208c\u2081\u1d38 through the\nprocedure described in the previous section. Each function g(\u2097) is trained with\na different number of invariants K\u2097, which are hyperparameters of our method.\nAt inference time, the test images x are evaluated by computing layer-wise\nscores s\u2097(f\u2097(x)) following Eq. (5),\n\ns\u2097(f) = \u2211\u2096\u208c\u2081\u1d37\u2097 g\u2096(\u2097)(f)\u00b2 / e\u2096(\u2097),\n\nwhich are aggregated to compute the final invariant score,\n\nS\u1d62\u2099\u1d65(x) = \u2211\u2097\u208c\u2081\u1d38 s\u2097(f\u2097(x)). (14)"}, {"title": "3.3 Scoring samples", "content": "We empirically found our invariant score of Eq. (14) to be complementary to a\nstandard 2-NN score [1] and observed that combining the two scores leads to a\nfurther boost in performance. To compute the 2-NN score, we first define the\n2-NN distance of a test sample at a layer l as\n\ndist-2nn\u2097(f) = 1/2 \u2211f\u2099\u2208N\u00b2(\u2097)(f) ||f - fn||\u00b2,\n(15)\n\nwhere N\u00b2(\u2097)(f) are the 2 nearest neighbours of f in the training set at layer l. As\nwith the layer-wise invariant score, the 2-NN distances are normalized by the\naverage 2-NN distances of the training set,\n\ns-2nn\u2097(f) = K\u2097 dist-2nn(f) / \u2211\u2081\u1d3a dist-2nn(f\u2097(xi)),\n\nwhere the factor K\u2097 compensates for the difference in magnitude with respect\nto the invariant score s\u2097. In the denominator, the 2-NN distances are calculated\nfor the training set elements to themselves, making each feature vector f\u2097(xi) its\nown first neighbor. To avoid this, we exclude the element f\u2097(xi) from the training\nset when computing dist-2nn(f\u2097(xi)). The 2NN score is computed as,\n\nS\u2082nn(x) = \u2211\u2097\u208c\u2081\u1d38 s\u208b\u2082nn\u2097 (f\u2097(x)), (17)"}, {"title": "4 Experiments", "content": "We will analyze the contribution of each of these terms to the detection perfor-\nmance in the ablation study of the results section.\n\n4.1 Benchmarks\nWe use the U-OOD evaluation benchmark introduced in [8] and propose a new\nbenchmark with shallow datasets for additional experiments. Both benchmarks\nare described below.\nGeneral U-OOD. The U-OOD benchmark introduced in [8] consists of 73\nexperiments spread over five tasks, each containing varying criteria for the in and\nout distributions. Three of the tasks have an unimodal training dataset: uni-\nclass, containing 30 one-class classification experiments on the low-resolution\nCIFAR10 and CIFAR100 datasets; uni-ano, which consists of 15 experiments\non the high-resolution MVTec images where the number of training images is\nlimited; and uni-med, which has 7 experiments on different medical imaging\nmodalities. The remaining two tasks use entirely different datasets as OOD.\nThese are shift-low-res, containing the CIFAR10:SVHN experiment on which\nmany OOD-detectors fail, and shift-high-res, comprising 20 experiments with\nthe DomainNet dataset.\nShallow U-OOD. Collection of experiments on shallow anomaly detection\ndatasets with tabular data where deep neural network features from images\nare unavailable. This benchmark aims to show the generality of our approach to\nother data modalities. We use six tabular datasets from [11]. These datasets were\nconceived for unsupervised anomaly detection and contain inliers and outliers\nintertwined within the data. To adapt the datasets to our OOD detection prob-\nlem, we pre-processed them by separating all the outliers and an equal number of\ninliers from each dataset and reserving them for the testing split. The remaining\ninliers were utilized as training data. The datasets included in the benchmark\nare thyroid, breast cancer, speech, pen global, shuttle and KDD99. Further details\nare provided in the appendix."}, {"title": "4.2 Baselines", "content": "For the General U-OOD benchmark, we compare our method NL-Invs against\nnine state-of-the-art methods. Six methods, DN2 [1], CFlow [12], DDV [31],\nDIF [37], MSCL [40], and MahaAD [42] that use the same ResNet-101 back-\nbone initialized with ImageNet pre-trained features, and three normalizing flow\nmethods, Glow [21], IC [46], and HierAD [44].\nFor the Shallow U-OOD benchmark, we compare NL-Invs to the baselines\nMahaAD (Mahalanobis distance), DN2 (kNN), and DIF (Isolation Forest).\nThe remaining baselines are bound to deep learning methods that cannot work\nwith non-image or tabular data and are thus excluded from the comparison."}, {"title": "4.3 Implementation details", "content": "Our VPN architecture includes four rotation and coupling layers before the final\nrotation layer (N = 4 in Fig. 2). Each coupling layer comprises an MLP with\nfour linear layers of equal size as its input, interspersed with ReLU activations.\nNL-Invs requires setting the number of invariants per layer K\u2097, as described\nin Sect. 3.2. Considering these values as independent hyperparameters would\nexponentially increase the search space and evaluation time. Instead, we set\neach K\u2097 to the largest number of principal components of the data at layer l\nthat jointly explain less than p% of the variance, where p is a hyperparameter\nshared by all layers.\nWe utilized a ResNet-101 for the multi-scale feature extraction of Sect 3.2.\nWe extract features from L = 3 feature maps at the end of the last ResNet\nblocks. Following [40], we normalize the feature vectors of the final layer to the\nunit norm for improved performance. In all our experiments, we train for 25\nepochs with p set to 5 and a batch size of 64. We use the Adam optimizer [20]\nwith a learning rate of 10\u207b\u00b3 linearly decaying to 10\u207b\u2074 over the epochs."}, {"title": "5 Results", "content": "This section describes the results obtained on the two benchmarks, followed by\na multi-faceted analysis of the behavior of our method.\nGeneral U-OOD. The performances of NL-Invs and the other methods\nare shown in Tab. 1. Most methods behave inconsistently across the benchmark,\nwith different methods scoring high for each task. For instance, CFlow is the\nbest scoring method on uni-ano by a large margin. However, its high performance"}, {"title": "5.1 Ablation study", "content": "We ablate our design choices in Tab. 3. The previous best method, MahaAD,\nuses linear invariants and reaches a score of 86.5 AUC on the General U-\nOOD benchmark. We find that our generalization of this formulation, which\nallows for learning non-linear invariants, reaches a new state-of-the-art of 87.2\nAUC. Part of this improvement is by means of the backward loss. Furthermore,\nincorporating S\u2082nn raises the performance even further to 87.9 AUC."}, {"title": "5.2 Other architectures", "content": "To show the applicability of NL-Invs to other architectures and model sizes,\nwe show results on uni-class with varying models, including ConvNeXT and a\nvision transformer, in Tab. 4. All models use the same hyperparameters, and we\nextract the features from L = 3 feature maps at the last blocks for all models.\nIn general, models with better performance on ImageNet lead to better U-OOD\nperformance, with ConvNeXT reaching the best results."}, {"title": "5.3 Hyperparameter sensitivity", "content": "NL-Invs has one main hyperparameter, p. We show in Tab. 5 that NL-Invs is\nrobust to the choice of p, with its performance changing by as little as 0.3 AUC\non General U-OOD across a wide range of values."}, {"title": "5.4 Assessing invariants", "content": "We conduct an additional experiment on CIFAR10 following [8] to assess how\nNL-Invs incorporates the intuitive idea of invariants in practice. To this end,\nwe compare how U-OOD methods handle different types of OOD datasets as the\nnumber of classes in the training set increases.\nWhen the training dataset contains only one class, samples belonging to dif-\nferent classes should be considered outliers, as the class is an invariant. As the\nnumber of classes in the training set increases, samples belonging to classes not\npresent in the training dataset should no longer be considered outliers, as the\nclass identity is no longer an invariant. This behavior is shown in Fig. 4(left),\nwhere all methods perform as anticipated. Conversely, test samples that ex-\nhibit visual dissimilarity from the training set should always be considered out-\nliers, irrespective of the number of classes in the training set. As depicted in\nFig. 4(right), our experimental findings indicate that invariant-based methods,\nnamely MahaAD and especially NL-Invs, exhibit the expected behavior when\ntest samples come from a different domain, where most of the test samples remain\noutliers despite the increase in training set classes. In contrast, the next-best per-\nforming method, MSCL, experiences a stronger decrease in performance."}, {"title": "5.5 Loss landscape analysis", "content": "The true U-OOD objective function is impossible to optimize due to the in-\ntractability of sampling the entire OOD space. Therefore, all U-OOD methods\noptimize a proxy loss function to approximate this underlying objective. This,\nin turn, leads to many U-OOD methods having no apparent correlation between\ntraining loss and OOD performance [40].\nData invariants offer a theoretically sound concept of U-OOD, whereby low\ntraining loss regions should correspond to high U-OOD performance and vice"}, {"title": "6 Conclusion", "content": "This work introduces a new U-OOD method that learns data invariants within\na training set. Our framework, called NL-Invs, is the first volume-preserving\napproach to OOD detection. NL-Invs learns non-linear invariants over a set\nof training features and generalizes previous invariant-based formulations of U-\nOOD, reaching state-of-the-art performance when compared against competitive\nmethods on a large-scale benchmark. Additionally, we validate our model on\ndifferent tabular datasets, showing its generalizability and advantage over affine\ninvariants.\nFinally, we confirm the results of [8] and observe that the performance of\nseveral U-OOD methods is highly sensitive, with the majority of techniques dis-\nplaying inconsistent scores across various tasks. Nevertheless, invariant-based"}, {"title": "A Supplementary Material", "content": "A.1 Benchmark details\nWe provide further details for our Shallow U-OOD benchmark, which consists\nof the following six datasets from [11]:\nthyroid. Training: 6'666 samples consisting of 21 measurements of healthy thy-\nroids. Testing: 250 samples from healthy thyroids as inliers and 250 outliers\nfrom hyper-functioning and subnormal-functioning thyroids as outliers.\nbreast cancer. Training: 357 samples with 30 measurements taken from med-\nical images from healthy patients. Testing: 10 inliers from healthy patients\nand 10 outliers from cancer instances.\nspeech. Training: 3'625 samples of 400-dimensional features extracted from record-\nings of people speaking with an American accent. Testing: 61 American-\naccent recordings as inliers and 61 outliers from speakers with non-American\naccents.\npen global. Training: 719 samples of the digit '8' represented as a vector of\n16 dimensions. Testing: 90 samples of '8' as inliers and 90 samples of other\ndigits as outliers.\nshuttle. Training: 45'586 samples describing a space shuttle's radiator posi-\ntions with 9-dimensional vectors. Testing: 878 inliers from normal situations,\n878 outliers taken from abnormal situations.\nKDD99. Dataset of simulated traffic in a computer network, where attacks are\nseen as anomalies and normal traffic as inliers. Training: 619'046 samples of\n38 dimensions. Testing: 1'052 inliers from normal traffic and 1'052 outliers."}]}