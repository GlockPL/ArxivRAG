{"title": "OmniRL: In-Context Reinforcement Learning by Large-Scale Meta-Training in Randomized Worlds", "authors": ["Fan Wang", "Pengtao Shao", "Yiming Zhang", "Bo Yu", "Shaoshan Liu", "Ning Ding", "Yang Cao", "Yu Kang", "Haifeng Wang"], "abstract": "We introduce OmniRL, a highly generalizable in-context reinforcement learning (ICRL) model that is meta-trained on hundreds of thousands of diverse tasks. These tasks are procedurally generated by randomizing state transitions and rewards within Markov Decision Processes. To facilitate this extensive meta-training, we propose two key innovations: (1) An efficient data synthesis pipeline for ICRL, which leverages the interaction histories of diverse behavior policies; and (2) A novel modeling framework that integrates both imitation learning and reinforcement learning (RL) within the context, by incorporating prior knowledge. For the first time, we demonstrate that in-context learning (ICL) alone, without any gradient-based fine-tuning, can successfully tackle unseen Gymnasium tasks through imitation learning, online RL, or offline RL. Additionally, we show that achieving generalized ICRL capabilities-unlike task identification-oriented few-shot learning-critically depends on long trajectories generated by variant tasks and diverse behavior policies. By emphasizing the potential of ICL and departing from pre-training focused on acquiring specific skills, we further underscore the significance of meta-training aimed at cultivating the ability of ICL itself.", "sections": [{"title": "1 Introduction", "content": "Large-scale pre-training has achieved tremendous success, especially in processing natural languages, images, and videos (Radford et al., 2021; Driess et al., 2023; Touvron et al., 2023; Achiam et al., 2023). They have demonstrated the ability to address unseen tasks through in-context learning (ICL) (Brown et al., 2020), a paradigm that leverages contextual information to enhance performance. Unlike in-weights learning (IWL), which relies on gradient-based updates to model weights, ICL enables models to acquire new skills in a few-shot manner, enhancing their adaptability to novel environments. With commonalities with model-based meta-learning approaches (Duan et al., 2016; Santoro et al., 2016), ICL can accommodate traditional learning paradigms within its framework, including supervised learning (Santoro et al., 2016; Garg et al., 2022), imitation learning (Reed et al., 2022; Fu et al.; Vosylius & Johns), and reinforcement learning (Laskin et al., 2022; Grigsby et al.; Zisman et al., 2024; Lee et al., 2024). This significantly alleviates the need for laborious human-designed objective functions and optimization strategies, which are typically required in IWL. Further, gradient-based IWL has been criticized for its inefficiency in continually adapting to new tasks (Dohare et al., 2024). In contrast, ICL has demonstrated plasticity that resembles the adaptability of the human brain (Lior et al., 2024).\nHowever, current meta-learning and in-context learning frameworks exhibit several limitations. Firstly, they often focus on few-shot adaptation in relatively small-scale tasks, language formations, or constrained domains (Chen et al., 2021b; Min et al., 2021; Coda-Forno et al., 2023). As a result, the efficacy of in-context learning (ICL) in adapting to complex novel tasks\u2014such as those requiring substantial data volume and reinforcement learning-remains uncertain. Consequently, adaptation in such scenarios primarily relies on gradient-based IWL rather than ICL. Secondly, the mechanisms underlying the \u201cemergence\u201d of ICL capabilities and their limitations are not fully understood (Wei et al., 2022). For instance, excessive pre-training on specific datasets can enhance IWL while potentially hindering ICL capabilities beyond a certain point (Singh et al., 2024). Therefore, we are interested in the question: Is it possible to meta-train generalized ICL abilities that are stable and agnostic to the underlying data distribution?\nTo this end, this paper introduces OmniRL, a model capable of adapting novel tasks at scale through ICRL and other ICL paradigms. To train OmniRL, we first propose an efficient simulator capable of generating a vast array of tasks modeled through Markov Decision Processes (MDPs). These tasks, which we dub AnyMDP, feature diverse state transitions and reward structures within discrete state and action spaces. Albeit lacking high fidelity to real-world problems, this powerful task generation scheme enables us to explore large-scale meta-training involving billions of time steps generated from millions of distinct tasks.\nFurthermore, unlike traditional ICRL models, OmniRL enables the agent to leverage both posterior feedback (such as rewards) and prior knowledge for in-context adaptation. Additionally, we introduce a data synthesis strategy that emphasizes both the diversity of trajectories and computational efficiency. These innovations facilitate large-scale imitation-only meta-training, while enabling effective in-context adaptation through imitation learning, online RL, or offline RL.\nOmniRL not only outperforms existing ICRL frameworks but also demonstrates the ability to generalize to unseen Gymnasium environments (Towers et al., 2024), including Cliff, Lake, Pendulum, and even multi-agent games. Furthermore, we conducted a quantitative analysis of the impact of the number of meta-training tasks on the acquisition of ICRL abilities at scale. Our findings reveal that the volume of training tasks is crucial in balancing between \"task identification\"-oriented few-shot learning and generalized in-context learning (ICL) (Kirsch et al., 2022). Specifically, agents focused on \"task identification\" excel at solving familiar tasks with fewer samples but often lack generalization capabilities. In contrast, generalized ICL agents (Kirsch et al., 2022; 2023; Wang et al., 2024) can solve both seen and unseen tasks, albeit with the trade-off of requiring a larger volume of data in context. Our results indicate that addressing longer trajectories is essential for achieving robust generalization.\nOur contributions are summarized as follows: 1We introduce AnyMDP, a scalable collection of tasks and environments modeled as Markov Decision Processes (MDPs) with randomized state transitions and rewards. This framework enables the meta-training process to scale up to hundreds of thousands of tasks. We propose an ICRL framework for the large-scale meta-training of OmniRL featuring an efficient data synthesis pipeline and a new model framework. 3 We demonstrate that the volume of tasks and the modeling of long trajectories are crucial for the emergence and the generalizability of ICRL and ICL abilities."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Meta-Learning for In-Context Learning", "content": "Meta-learning, also known as learning to learn (Thrun & Pratt, 1998), pertains to a category of approaches that prioritize the acquisition of generalizable adaptation skills across a spectrum of tasks. It encompasses a broad array of methodologies, including the optimization of gradient-based methods (Finn et al., 2017) and model-based meta-learning (Santoro et al., 2016; Duan et al., 2016). As the typical setting of model-based meta-RL (Duan et al., 2016; Mishra et al., 2018) or ICRL (Laskin et al., 2022; Lee et al., 2024; Grigsby et al.), states, actions, and rewards are typically arranged as a trajectory to compose the inner loop for task adaptation, while the pre-training and meta-training are recognized as the outer loop. Common choices for the outer-loop optimizer include reinforcement learning (Duan et al., 2016; Mishra et al., 2018; Grigsby et al.), evolutionary strategies (Najarro & Risi, 2020; Wang et al., 2022), and imitation learning (Lee et al., 2024; Laskin et al., 2022). Imitation learning of ICRL is also related to the reinforcement learning coach (RLCoach), which uses pre-trained RL agents to generate demonstrations. RLCoach is not only used for ICRL but is also widely employed to accelerate single-task reinforcement learning (Zhang et al., 2021) and multi-task learning (Reed et al., 2022)."}, {"title": "2.2 In-Context Learning from Pre-training", "content": "The rise of Large Language Models (LLMs) blurs the boundary between pre-training and meta-training, as pre-training with huge datasets incentivizes ICL in a manner similar to meta-learning (Brown et al., 2020; Chen et al., 2021b; Coda-Forno et al., 2023). For clarity, we use \"pre-training\" to describe training that primarily targets the acquisition of diverse skills, typically followed by a subsequent gradient-based tuning stage. \"Meta-training\" refers to training aimed at acquiring the ability to learn, which does not necessarily require subsequent gradient-based tuning. The correlation between the ability of ICL and the distribution of pre-training data has been thoroughly investigated (Chan et al., 2022a; Singh et al., 2024) recently, indicating that ICL is related to \"burstiness,\" which refers to patterns that exhibit a distributional gap between specific trajectories and the pre-training dataset. Additionally, the level of \"burstiness\" may affect the trade-off between ICL and IWL, with non-bursty trajectories stimulating more IWL and less ICL. Analyses and experiments have been conducted to show that computation-based ICL can exhibit a richer array of behaviors than gradient-based IWL (Chan et al., 2022b; Von Oswald et al., 2023; Xie et al.), particularly in terms of plasticity and continual learning (Lior et al., 2024). Specifically, depending on the pre-training dataset, ICL can perform either associative generalization or rule-based generalization (Chan et al., 2022b). In many cases, ICL primarily serves as a task identifier (Wies et al., 2024), where skills are memorized through IWL, and ICL simply invokes the correct one. This issue may be prevalent in many meta-learning benchmarks emphasizing few-shot learning, since those methods typically operate within a restricted domain and require re-training across domains. It further motivates the need for generalized in-context learning (Kirsch et al., 2022; 2023; Wang et al., 2024), where the acquirement of skill is dominated by ICL instead of IWL.\nRelations with long chain-of-thought (CoT). Recently we have also observed a trend toward increasing the reasoning length in LLMs (OpenAI, 2024; DeepSeek-AI, 2024). However, they are quite distinct from the proposed generalized ICL: LLM reasoning emphasizes the ability of system 2 which represents rule-based and analytical thinking, while generalized ICL emphasizes the in-context improvement of system 1 which represents rapid and intuitive decision-making (Wason & Evans, 1974; Kahneman, 2011). Nonetheless, our research also underscores the importance of exploring long-sequence causal models beyond the Transformer architecture."}, {"title": "2.3 Benchmarking In-Context Reinforcement Learning", "content": "Meta-learning typically requires a set of related yet diverse tasks. One commonly used technique is to randomize a subset of domain parameters to create these variant tasks. These benchmarks can be broadly categorized as follows:\n1 Randomizing the rewards or targets while keeping the transitions fixed. This includes multi-armed bandits (Mishra et al., 2018; Laskin et al., 2022), varying goals in a fixed grid world or maze (Lee et al., 2024), different walking speeds in locomotion tasks (Finn et al., 2017; Mishra et al., 2018), and diverse tasks in object manipulation (Yu et al., 2020). This approach is also closely related to multi-objective reinforcement learning (Alegre et al., 2022). 2 Modifying the transitions while keeping the targets unchanged. Examples include tasks with crippled joints or varying joint sizes in locomotion (Najarro & Risi, 2020; Pedersen & Risi, 2021), procedurally generated grid worlds (Wang et al., 2022; Nikulin et al., 2023) and games (Cobbe et al., 2020). 3 Randomizing the observations and labels without altering the underlying transitions and rewards. Examples include hiding parts of observations (Morad et al., 2023), randomizing labels (Kirsch et al., 2022), and actions (Sinii et al.). The above approaches typically create a group of tasks that start from a \"seed\" task. These methods are also related to domain randomization (Peng et al., 2018; Arndt et al., 2020), which has proven effective in reducing sim-to-real gaps by improving both in- and out-of-distribution generalization(Peng et al., 2022)."}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 AnyMDP:Randomized Worlds", "content": "To effectively generate a large number of tasks to enhance generalized ICL, we adopt a different approach from previous benchmarks that typically apply domain randomization to a \"seed task.\u201d Instead, we choose an extreme approach that sacrifices fidelity to the real world in favor of maximizing task diversity. We primarily consider fully observable Markov Decision Processes in discrete state and action spaces. Let $n_s$ and $n_a$ denote the sizes of the state and action spaces, respectively. Any task that can be modeled through MDPs in discrete state and action spaces is represented as follows:\n$\\tau_{(n_s, n_a)} = \\{T_\\tau, R_\\tau, \\Sigma_\\tau\\} \\in \\mathcal{T}_{(n_s, n_a)},$ (1)\nwith $T_\\tau, R_\\tau, \\Sigma_\\tau \\in \\mathbb{R}^{n_s \\times n_a \\times n_s}$. The transitions follow $p(s'|s,a) = T_\\tau[s, a, s']$, and the rewards are decided by $r(s, a, s') \\sim \\mathcal{N}(R_\\tau[s, a, s'], \\Sigma_\\tau[s, a, s'])$. By sampling $T_\\tau$, $R_\\tau$, and $\\Sigma$ randomly, we can theoretically cover any possible MDPs. However, in practice, naively sampling transition and reward matrices ends up with a trivial task with a high probability. Therefore, we devised a method for generating challenging tasks efficiently. In this method, the representations of states and actions are first sampled from a high-dimensional continuous space, and then the distributions of transitions and rewards are recalculated based on these representations, as described in Appendix A.1.\nAnyMDP degenerates to classical bandit benchmarks (Duan et al., 2016; Mishra et al., 2018) by setting $n_s = 1$. With $n_s > 1$, it demands reasoning over diverse trajectories and delayed rewards, which poses a greater challenge over ICL. Moreover, with access to ground truth transitions and rewards, it facilitates low-cost access to an oracle solution through value iteration (Bellman, 1958), eliminating the necessity to execute costly RL optimization. A visualization of these procedurally generated tasks can be found in Figure 1."}, {"title": "3.2 Modeling and Training Framework", "content": "Problem Setting: In ICRL, the agent adapts to novel tasks by incorporating its interaction history into its context, denoted as: $h_{t-1} = [s_1, a_1, r_1, ..., s_{t-1}, a_{t-1}, r_{t-1}]$. We use the character s, a, and r to denote state, action, and reward, respectively. The policy neural network, parameterized by $\\theta$, is denoted as $\\pi_\\theta(a_t|s_t, h_{t-1})$. Here, $h_{t-1}$ serves as the task-specific training data for the inner loop, where the agent is expected to continually improve its performance as t increases due to the accumulation of task-specific data. To optimize $\\theta$, the outer loop involves meta-training, which is performed on a training task set $\\mathcal{T}_{tra}$. The policy is then validated using a testing task set $\\mathcal{T}_{tst}$.\nIncluding the prior knowledge of policy in context: Reinforcement learning (RL) relies predominantly on posterior information for learning, specifically feedback or reward. However, it may overlook crucial prior knowledge that could enhance the learning process. For example, consider a scenario where an expert provides the agent with a demonstrated trajectory, the agent would be unable to fully trust the demonstrating policy without comparing its feedback to other trajectories and sufficiently exploring the entire state-action space. Therefore, we introduce an additional feature p to denote the prior information associated with each action. In practice, it may be used to incorporate tags, commands, or prompts for the upcoming action. The agent is required to consider both the prior information and the posterior information in the history simultaneously, which may benefit in two aspects. 1. It may be used as a tag to avoid confusion in interpreting the trajectory (h), which could originate from diverse policies including exploration, myopic, and non-myopic exploitation (Chen et al., 2021a). 2. It may be used to denote the trustworthiness of the previous action. For instance, if the actions are generated by an oracle or expert, the agent may be more inclined to directly trust them, without relying solely on feedback or leaning towards exploration. For now, we set $p_i$ to be the class ID marking the policy from which the action $a_i$ is generated, with an additional ID \u201cUNK\u201d reserved as the default. Then, the interaction history is extended to $h_{t-1} = [s_1, p_1, a_1, r_1, s_2, p_2, \u2026, s_{t-1}, a_{t-1}, r_{t-1}]$. The policy neural network is thereby denoted by $\\pi_\\theta(a_t|h_{t-1}, p_t, s_t)$.\nData synthesis for imitation-only meta-training: Imitation learning has been demonstrated to effectively elicit ICRL with lower cost and better scalability. Nonetheless, directly imitating the trajectory of an expert (behavior cloning) is less effective due to the accumulation of compound er-In this work, we focus solely on fully observable Markov Decision Processes (MDPs); extending our approach to Partially Observable Markov Decision Processes (POMDPs) is conceptually straightforward within this framework, but it necessitates significant effort in datasets, which we plan to address in future research."}, {"title": "", "content": "rors in MDPs (Ross & Bagnell, 2010). Inspired by data aggregation (Ross et al., 2011), we define two key policies in our framework that are independent of each other. The behavior policy (superscript (b)) refers to the policy that is actually executed to generate the trajectory h. Meanwhile, the reference policy (superscript (r)) serves as the target policy to be imitated, but it is not executed directly. This yields the following target:\nMinimize :$\\mathcal{L}_1 = -log\\pi_\\theta(a^{(r)}_t|h_{t-1}, s_t, p_t^{(r)}),$ (2)\nwith\n$\\pi_\\theta = \\text{Softmax}(z_t)$,\n$z_t = \\text{Causal}_\\theta(h_{t-1}, p_t^{(r)}, s_t),$ (3)\nwith $h_{t-1}= [s_0, p_0^{(b)}, a_0^{(b)}, r_0, s_1^{(b)}, ..., r_{t-1}]$. Equation (2) can be used to represent various ICRL techniques by varying behavior and reference policies, including algorithm distillation (AD) (Laskin et al., 2022), noise distillation (AD\u03f5) (Zisman et al., 2024), and decision pre-training Transformers (DPT) (Lee et al., 2024) (see Appendix A.2 for details). Following DPT, we mainly use the oracle policy for data collection. In this process, the reference policy $p^{(r)}$ can be omitted from the trajectory, while $p^{(b)}$ is retained. However, we introduce even more diversity into behavior policies to enhance the completeness of the data. Specifically, we include methods such as Q-learning, model-based reinforcement learning, multi-y oracle policies (Grigsby et al.), and noise distillation. A summary of the data synthesis pipeline is in Algorithm 6.\nChunk-wise meta-training. By independently sampling from the behavior policy to generate trajectories and from the reference policy to generate labels, we can further reformulate Equation Equation (2) into an efficient chunk-wise form for training, as illustrated in Figure 2. It is reformulated as:\nMinimize: $\\mathcal{L} = \\sum_{z_1, z_2, ..., z_t} \\sum_{t} \\mathcal{W}_t \\mathcal{L}_t$\\\n$= \\text{Causal}_\\theta(p_0^{(r)}, s_0, p_0^{(b)}, a_0^{(b)}, r_0, ..., s_1, ..., s_2, ..., s_t).$ (4)\nExtending ICL to complex tasks at scale requires efficient modeling of very long contexts. While the Transformer (Vaswani, 2017) is regarded as state-of-the-art for short horizons, it is challenging to apply the Transformer to trajectories longer than 10K. Thus, we employ sliding window attention (Beltagy et al., 2020) on top of Transformers with rotary position embeddings (Su et al., 2024), but in a segment-wise setting, which is more akin to Transformer-XL (Dai et al., 2018). The theoretical limit of the valid context length is given by $(\\text{Nlayers} + 1) \\times \\text{Lsegment}$. We further investigate more efficient linear attention layers including Mamba (Gu & Dao, 2023; Lv et al., 2024; Huang et al., 2024; Ota, 2024), RWKV6 (Peng et al., 2023), and Gated Slot Attention (GSA) (Zhang et al.). These structures offer promising inference capabilities with a computational cost of \u0398(T). Alternatively, inference relies on memory states with a fixed size, rather than a growing context, thus transforming in-context learning into in-memory learning. We select GSA for the subsequent experiments based on the conclusion of some preliminary experiments. Further more, to facilitate training with long trajectories scaling up to over 1 million, we implemented a segment-wise back propagation pipeline. In the training phase, the full sequence is first divided into multiple segments. The forward pass is calculated across the entire sequence, while the backward pass is calculated within each segment, with the gradient eliminated across segments. This enables us to train on arbitrarily long sequences with limited computation resources (see Algorithm 5 in appendices)."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Experimental Setup", "content": "Meta-Training. We conduct all our experiments by performing meta-training on data synthesized from AnyMDP tasks exclusively. We sample tasks totaling $|\\mathcal{T}(n_s \\in [16, 128], n_a = 5)| = 584\\text{K}$, from which we synthesize up to $|\\mathcal{D}(\\mathcal{T}(n_s \\in [16, 128], n_a = 5))| = 584\\text{K}$ trajectories (we use $\\mathcal{D}(\\mathcal{T})$ to denote the dataset generated based on $\\mathcal{T}$), with trajectory lengths $\\mathcal{T} \\in [8\\text{K}, 1024\\text{K}]$. We find it beneficial to follow a curriculum learning procedure, commencing with $n_s = 16$ and gradually scaling up $n_s$, the details of which are presented in Appendix A.3. To conserve computational resources, we primarily utilize Stage 1 for comparisons. Only the most promising settings proceed to Stages 2 and 3. We sample multiple groups of validation datasets $\\mathcal{D}(\\mathcal{T}_{tst}(n_s \\in \\{16, 32, 64, 128\\}, n_a = 5)$ from both seen and unseen tasks, with each group containing 256 trajectories. Unless otherwise specified, we default to selecting the model that achieves the best average performance on the validation set. Our experimental results demonstrate the superior performance of GSA over Transformer in both computational efficiency and long-term sequence modeling. Unless otherwise specified, we report the results of GSA.\nValidation and Evaluation. We use the term \u201cvalidation\u201d to refer to the process of evaluating the loss on the validation dataset, which represents the offline evaluation of the model. In contrast, \u201cevaluation\u201d refers to the online performance of the model, assessed by deploying the agent and allowing it to interact with the environment. Furthermore, the evaluation is divided into three categories: 1. Online-RL: The agent starts with an empty trajectory, denoted as $h_0 = \\emptyset$. 2. Offline-RL: The agent starts with a trajectory of a certain length derived from imperfect demonstrations, denoted as $h_0 = h^{\\pi}$. 3. Imitation Learning: The agent starts with an oracle demonstration, denoted as $h_0 = h^{(\\exp)}$. For all three categories, the subsequent interactions are continually added to the trajectory within the evaluation process. Therefore, the models differ only in their initial KV-Cache (Transformers) or memory (GSA). The evaluation assesses the agents\u2019 abilities in two key areas: first, their capacity to exploit existing information, and second, their ability to explore and exploit continually based on that.\nIn addition to AnyMDP, we select gymnasium tasks (Towers et al., 2024) for evaluation, including Lake, Cliff, Pendulum, and Switch2 (a multi-agent game), with ICL only and no parameter tuning. For Pendulum with continuous observation space, we manually discretize the observation space into 60 states using grid discretization (12-class position \u00d7 5-class velocity).\nBaselines. We mainly compare with AD, AD\u03f5, and DPT, all of which use imitation learning for meta-training (See Table 1). Although we believe that an online RL2 would further enhance performance, it is associated with a significantly higher cost of meta-training and is therefore not included in the comparison. For gymnasium tasks, ICRL is also compared to Q-Learning implemented in stable"}, {"title": "", "content": "baseline3 (Raffin et al., 2021). To investigate the impact of prior knowledge and the use of diversified reference policies through multi-y oracles, we also include OmniRL(w/o a priori) and OmniRL(multi-y) (Grigsby et al.) into the comparison."}, {"title": "4.2 Comparison with baselines", "content": "Including the prior knowledge in trajectory and diversifying behavior policies benefits ICL: Figure 3 summarizes the performance of different methods on $\\mathcal{T}_{tst}(16, 5)$, including AD, AD\u03f5, DPT, OmniRL (w/o a priori), and OmniRL. The performance score is normalized by the expected score per episode of the oracle policy (100%) and the uniformly random policy (0%). OmniRL (w/o a priori) lags behind OmniRL with a noticeable gap in all three groups, demonstrating the importance of introducing the prior information. Comparing DPT with OmniRL (w/o a priori) reveals that increasing the diversity of behavior policies in the training data offers certain advantages in online RL. However, in offline RL and imitation learning, excluding prior information of the behavior policies introduces additional challenges. All methods surprisingly exhibit some extent of imitation learning techniques, but OmniRL is the only method that performs well on all of online-RL, offline-RL, and imitation learning within 200 episodes.\nDiversifying reference policies shows no advantage: Additional results also indicate that there are no benefits derived from the diversity of the reference policy. Additionally, we found it unnecessary to incorporate exploration strategies, just like AD and AD\u03f5, into the reference policy, as these significantly reduce performance. It is consistent with the theoretical analyses (Lee et al., 2024) proving that imitating the oracle potentially leads to the exploration strategy of \"posterior sampling.\" Investigating the entropy of the decision-making process also reveals that the OmniRL agent tends to automatically explore more when insufficient information is provided in the context (Appendix B.4)."}, {"title": "4.3 Impact of Task Diversity", "content": "To investigate the effect of task diversity on meta-training, we generate an equal number of trajectories $(\\mathcal{D}(\\mathcal{T}_{tra}(16,5))| = 128\\text{K})$ based on different volumes of tasks with $|\\mathcal{T}_{tra}| \\in \\{100, 1\\text{K}, 10\\text{K}, 128\\text{K}\\}$. Note that different trajectories can be generated from a single task, attributed to the diverse behavior policies, randomness in decision sampling, and randomness in transition sampling. Then, another $|\\mathcal{D}(\\mathcal{T}_{tst}(16,5))| = 256$ trajectories are sampled from both seen tasks (where the task overlaps with the training set) and unseen tasks (newly generated tasks not in any of the training set) for evaluation. We examine how the loss function $\\mathcal{L}_t$ changes with the number of meta-training iterations (outer-loop steps) and steps in context t (inner-loop steps) simultaneously; the results are shown in Figure 4.\nThe following observations are remarkable: 1. Fewer tasks ($\\mathcal{T}_{tra} < 1\\text{K}$) in meta-training lead to \u201ctask identification,\u201d where the model primarily relies on in-weight learning (IWL) to capture the input-output mapping and employs in-context learning (ICL) for identification only. This is manifested by very fast adaptation in seen tasks and an inability to generalize to unseen tasks. 2. In the group with $|\\mathcal{T}_{tra}| = 1\\text{K}$, ICL is observed in unseen tasks at around 10K iterations, but degenerates quickly as meta-training continues. This reaffirms the \u201ctransiency\u201d of ICL as mentioned in Singh et al. (2024). As we increase the number of tasks, this \u201ctransiency\u201d diminishes, which somewhat resembles \u201cover-fitting\u201d in classical machine learning. However, the two phenomena are fundamentally different. Over-fitting in classical machine learning can typically be mitigated by increasing the amount of data. In contrast, the \u201ctransiency\u201d observed in ICL is alleviated by increasing the volume of tasks. 3. When tasks are sufficiently diverse, it leads to \u201cgeneralized ICL,\u201d where in-context adaptation takes longer but generalizes better to unseen tasks.\nGiven that Figure 4 is on relatively simple task sets $\\mathcal{T}(16, 5)$, and more complex tasks may require significantly more task diversity for convergence, a reasonable assumption is that much of the existing meta-learning benchmarks fall into the category of task identification. This potentially facilitates few-shot learning for in-distribution generalization but is less capable of generalizing to out-of-domain tasks. Therefore, we advocate for meta-training on a scalable collection of tasks rather than on a restricted domain."}, {"title": "4.4 Scaling up State Spaces", "content": "We follow a curriculum learning approach with approximately three stages to further scale up the meta-training, thereby accommodating tasks with larger state spaces and longer trajectories (Appendix A.3). Until the final stage (stage-3), we use 16B steps of interaction overall, and scale the state space up to 128 and steps within each trajectory to up to 1024K, which requires the actual context length of 4096K for OmniRL. By validating the Stage-3 model in $\\mathcal{T}(16,5)$, we observe a further improvement over the stage-1 OmniRL, as shown in Figure 1.\nWe also validate the learned model on $\\mathcal{D}(\\mathcal{T}_{tst}(16,5))$, $\\mathcal{D}(\\mathcal{T}_{tst}(32,5))$, $\\mathcal{D}(\\mathcal{T}_{tst}(64,5))$, and $\\mathcal{D}(\\mathcal{T}_{tst}(128,5))$ and show the step-wise loss in figure 5. On a semi-logarithmic axis, the position-wise validation loss exhibits a nearly perfect linear relationship with the context length before the \"saturation\" of in-context improvements. This \u201csaturation\u201d might be induced by the upper limit of the environment itself or the limitations of the sequence modeling capabilities. The context length at which performance saturates is referred to as the ICL horizon. Figure 5a demonstrates that higher task complexity leads to a longer ICL horizon. In Figure 5b, we further show the online-RL evaluation of the stage-3 model on state spaces ranging from $\\mathcal{T}_{tst}(16,5)$ to $\\mathcal{T}_{tst}(128, 5)$. These results demonstrate strong consistency with the validation results on the static dataset (Figure 5a).\nFigures 5c and 5d further validated the superiority of GSA in long-sequence modeling. When validating on $\\mathcal{D}(\\mathcal{T}(64,5))$, although Transformer-XL performs better within 2K steps, the GSA surpasses Transformer-XL by a large margin beyond 20K steps. It is also worth noticing that on $\\mathcal{D}(\\mathcal{T}(16,5))$ and the other benchmarks (such as NLP benchmarks) (Zhang et al.), the superiority of GSA is not that obvious. It indicates that AnyMDP offers benchmarks more scalable in terms of context length."}, {"title": "4.5 Generalizing to Gymnasium Tasks", "content": "Aiming at studying the model\u2019s generalization capabilities toward diverse environments, OmniRL is further evaluated in the OpenAI Gymnasium with grid world and classic control problems NOT included in the training dataset, including lake4 \u00d7 4 (with both slippery and non-slippery dynamics), cliff, pendulum (with variant environment configurations of g = {1.0, 5.0, 9.8}). The results are shown in Figure 6 and Figure 9. OmniRL (Stage-3) demonstrates strong performance across most environments, including online RL, offline RL, and imitation learning. However, it underperforms in the pendulum environment with g = 9.8, particularly during offline RL evaluations. We hypothesize that random exploration in the pendulum environment is insufficient for achieving success by chance. We also found that proper reward shaping is important for OmniRL to work, which is described in Figure 7.\nOmniRL can generalize to multi-agent system: OmniRL can be applied to the two-agent game of Switch2 (Koul, 2019) without any fine-tuning by incorporating the state of the other agent into its observation. Although both agents begin with identical models, they eventually exhibit distinct action patterns to effectively cooperate. This divergence in behavior arises from their ability to adapt in-context. Nonetheless, we observe some instability in continual learning when starting from imitation learning to reinforcement learning. Initially, the agent closely follows the teacher\u2019s demonstrations for the first few episodes. However, its performance deteriorates as it begins to learn from its interaction history. Then its performance improves once more as it presumably switches back to \"RL mode.\" This issue in continual learning deserves further investigation."}, {"title": "5 Conclusions and Discussions", "content": "We propose a scalable task set for benchmarking and"}]}