{"title": "Scaling and evaluating sparse autoencoders", "authors": ["Leo Gao", "Tom Dupr\u00e9 la Tour", "Henk Tillman", "Gabriel Goh", "Rajan Troll", "Alec Radford", "Ilya Sutskever", "Jan Leike", "Jeffrey Wu", "OpenAI"], "abstract": "Sparse autoencoders provide a promising unsupervised approach for extracting interpretable features from a language model by reconstructing activations from a sparse bottleneck layer. Since language models learn many concepts, autoencoders need to be very large to recover all relevant features. However, studying the properties of autoencoder scaling is difficult due to the need to balance reconstruction and sparsity objectives and the presence of dead latents. We propose using k-sparse autoencoders [Makhzani and Frey, 2013] to directly control sparsity, simplifying tuning and improving the reconstruction-sparsity frontier. Additionally, we find modifications that result in few dead latents, even at the largest scales we tried. Using these techniques, we find clean scaling laws with respect to autoencoder size and sparsity. We also introduce several new metrics for evaluating feature quality based on the recovery of hypothesized features, the explainability of activation patterns, and the sparsity of downstream effects. These metrics all generally improve with autoencoder size. To demonstrate the scalability of our approach, we train a 16 million latent autoencoder on GPT-4 activations for 40 billion tokens. We release code and autoencoders for open-source models, as well as a visualizer.", "sections": [{"title": "1 Introduction", "content": "Sparse autoencoders (SAEs) have shown great promise for finding features [Cunningham et al., 2023, Bricken et al., 2023, Templeton et al., 2024, Goh, 2016] and circuits [Marks et al., 2024] in language models. Unfortunately, they are difficult to train due to their extreme sparsity, so prior work has primarily focused on training relatively small sparse autoencoders on small language models.\nWe develop a state-of-the-art methodology to reliably train extremely wide and sparse autoencoders with very few dead latents on the activations of any language model. We systematically study the scaling laws with respect to sparsity, autoencoder size, and language model size. To demonstrate that our methodology can scale reliably, we train a 16 million latent autoencoder on GPT-4 [OpenAI, 2023] residual stream activations.\nBecause improving reconstruction and sparsity is not the ultimate objective of sparse autoencoders, we also explore better methods for quantifying autoencoder quality. We study quantities corresponding"}, {"title": "2 Methods", "content": ""}, {"title": "2.1 Setup", "content": "Inputs: We train autoencoders on the residual streams of both GPT-2 small [Radford et al., 2019] and models from a series of increasing sized models sharing GPT-4 architecture and training setup, including GPT-4 itself [OpenAI, 2023]4. We choose a layer near the end of the network, which should contain many features without being specialized for next-token predictions (see Section F.1 for more discussion). Specifically, we use a layer of the way into the network for GPT-4 series models, and we use layer 8 (1 of the way) for GPT-2 small. We use a context length of 64 tokens for all experiments. We subtract the mean over the dmodel dimension and normalize to all inputs to unit norm, prior to passing to the autoencoder (or computing reconstruction errors).\nEvaluation: After training, we evaluate autoencoders on sparsity Lo, and reconstruction mean-squared error (MSE). We report a normalized version of all MSE numbers, where we divide by a baseline reconstruction error of always predicting the mean activations.\nHyperparameters: To simplify analysis, we do not consider learning rate warmup or decay unless otherwise noted. We sweep learning rates at small scales and extrapolate the trend of optimal learning rates for large scale. See Appendix A for other optimization details."}, {"title": "2.2 Baseline: ReLU autoencoders", "content": "For an input vector x \u2208 Rd from the residual stream, and n latent dimensions, we use baseline ReLU autoencoders from [Bricken et al., 2023]. The encoder and decoder are defined by:\nz = ReLU(Wenc(x - bpre) + benc)   (1)\nx = Wdecz + bpre"}, {"title": "2.3 TopK activation function", "content": "We use a k-sparse autoencoder [Makhzani and Frey, 2013], which directly controls the number of active latents by using an activation function (TopK) that only keeps the k largest latents, zeroing the rest. The encoder is thus defined as:\nz = TopK(Wenc(x - bpre))   (2)\nand the decoder is unchanged. The training loss is simply L = ||x \u2212 2||2.\nUsing k-sparse autoencoders has a number of benefits:\n\u2022 It removes the need for the L1 penalty. L1 is an imperfect approximation of Lo, and it introduces a bias of shrinking all positive activations toward zero (Section 5.1).\n\u2022 It enables setting the Lo directly, as opposed to tuning an L\u2081 coefficient \u5165, enabling simpler model comparison and rapid iteration. It can also be used in combination with arbitrary activation functions.5\n\u2022 It empirically outperforms baseline ReLU autoencoders on the sparsity-reconstruction frontier (Figure 2a), and this gap increases with scale (Figure 2b).\n\u2022 It increases monosemanticity of random activating examples by effectively clamping small activations to zero (Section 4.3)."}, {"title": "2.4 Preventing dead latents", "content": "Dead latents pose another significant difficulty in autoencoder training. In larger autoencoders, an increasingly large proportion of latents stop activating entirely at some point in training. For example, Templeton et al. [2024] train a 34 million latent autoencoder with only 12 million alive latents, and in our ablations we find up to 90% dead latents7 when no mitigations are applied (Figure 15). This results in substantially worse MSE and makes training computationally wasteful. We find two important ingredients for preventing dead latents: we initialize the encoder to the transpose of the decoder, and we use an auxiliary loss that models reconstruction error using the top-kaux dead latents (see Section A.2 for more details). Using these techniques, even in our largest (16 million latent) autoencoder only 7% of latents are dead."}, {"title": "3 Scaling laws", "content": "Due to the broad capabilities of frontier models such as GPT-4, we hypothesize that faithfully representing model state will require large numbers of sparse features. We consider two primary approaches to choose autoencoder size and token budget:"}, {"title": "3.1 Number of latents", "content": ""}, {"title": "3.1.1 Training to compute-MSE frontier (L(C'))", "content": "Firstly, following Lindsey et al. [2024], we train autoencoders to the optimal MSE given the available compute, disregarding convergence. This method was introduced for pre-training language models [Kaplan et al., 2020, Hoffmann et al., 2022]. We find that MSE follows a power law L(C) of compute, though the smallest models are off trend (Figure 1).\nHowever, latents are the important artifact of training (not reconstruction predictions), whereas for language models we typically care only about token predictions. Comparing MSE across different n is thus not a fair comparison the latents have a looser information bottleneck with larger n, so lower MSE is more easily achieved. Thus, this approach is arguably unprincipled for autoencoder training."}, {"title": "3.1.2 Training to convergence (L(N))", "content": "We also look at training autoencoders to convergence (within some \u20ac). This gives a bound on the best possible reconstruction achievable by our training method if we disregard compute efficiency. In practice, we would ideally train to some intermediate token budget between L(N) and L(C).\nWe find that the largest learning rate that converges scales with 1/\u221an (Figure 3). We also find that the optimal learning rate for L(N) is about four times smaller than the optimal learning rate for L(C).\nWe find that the number of tokens to convergence increases as approximately \u0398(n0.6) for GPT-2 small and \u0398(n0.65) for GPT-4 (Figure 11). This must break at some point \u2013 if token budget continues to increase sublinearly, the number of tokens each latent receives gradient signal on would approach zero.8"}, {"title": "3.1.3 Irreducible loss", "content": "Scaling laws sometimes include an irreducible loss term e, such that y = ax + e [Henighan et al., 2020]. We find that including an irreducible loss term substantially improves the quality of our fits for both L(N) and L(C)."}, {"title": "3.1.4 Jointly fitting sparsity (L(N, K))", "content": "We find that MSE follows a joint scaling law along the number of latents n and the sparsity level k (Figure 1b). Because reconstruction becomes trivial as k approaches dmodel, this scaling law only holds for the small k regime. Our joint scaling law fit on GPT-4 autoencoders is:\nL(n,k) = exp(\u03b1 + \u03b2k log(k) + \u03b2\u03b7 log(n) + \u03b3log(k) log(n)) + exp(\u03da + \u03b7 log(k))  (3)\nwith a = -0.50, \u03b2\u03ba = 0.26, \u03b2\u03b7 = \u22120.017, \u03b3 = \u22120.042, \u03da = \u22121.32, and \u03b7 = \u22120.085. We can see that y is negative, which means that the scaling law L(N) gets steeper as k increases. \u03b7 is negative too, which means that the irreducible loss decreases with k."}, {"title": "3.2 Subject model size Ls (N)", "content": "Since language models are likely to keep growing in size, we would also like to understand how sparse autoencoders scale as the subject models get larger. We find that if we hold k constant, larger subject models require larger autoencoders to achieve the same MSE, and the exponent is worse (Figure 4)."}, {"title": "4 Evaluation", "content": "We demonstrated in Section 3 that our larger autoencoders scale well in terms of MSE and sparsity (see also a comparison of activation functions in Section 5.2). However, the end goal of autoencoders is not to improve the sparsity-reconstruction frontier (which degenerates in the limit), but rather to find features useful for applications, such as mechanistic interpretability. Therefore, we measure autoencoder quality with the following metrics:\n1. Downstream loss: How good is the language model loss if the residual stream latent is replaced with the autoencoder reconstruction of that latent? (Section 4.1)\n2. Probe loss: Do autoencoders recover features that we believe they might have? (Section 4.2)"}, {"title": "4.1 Downstream loss", "content": "An autoencoder with non-zero reconstruction error may not succeed at modeling the features most relevant for behavior [Braun et al., 2024]. To measure whether we model features relevant to language modeling, we follow prior work [Bills et al., 2023, Cunningham et al., 2023, Bricken et al., 2023, Braun et al., 2024] and consider downstream Kullback-Leibler (KL) divergence and cross-entropy loss. 10 In both cases, we test an autoencoder by replacing the residual stream by the reconstructed value during the forward pass, and seeing how it affects downstream predictions. We find that k-sparse autoencoders improve more on downstream loss than on MSE over prior methods (Figure 5a). We also find that MSE has a clean power law relationship with both KL divergence, and difference of cross entropy loss (Figure 5b), when keeping sparsity Lo fixed and only varying autoencoder size. Note that while this trend is clean for our trained autoencoders, we can observe instances where it breaks such as when modulating k at test time (see Section 5.3).\nOne additional issue is that raw loss numbers alone are difficult to interpret\u2014we would like to know how good it is in an absolute sense. Prior work [Bricken et al., 2023, Rajamanoharan et al., 2024] use the loss of ablating activations to zero as a baseline and report the fraction of loss recovered from that baseline. However, because ablating the residual stream to zero causes very high downstream loss, this means that even very poorly explaining the behavior can result in high scores.11\nInstead, we believe a more natural metric is to consider the relative amount of pretraining compute needed to train a language model of comparable downstream loss. For example, when our 16 million latent autoencoder is substituted into GPT-4, we get a language modeling loss corresponding to 10% of the pretraining compute of GPT-4."}, {"title": "4.2 Recovering known features with 1d probes", "content": "If we expect that a specific feature (e.g sentiment, language identification) should be discovered by a high quality autoencoder, then one metric of autoencoder quality is to check whether these features are present. Based on this intuition, we curated a set of 61 binary classification datasets (details in Table 1). For each task, we train a 1d logistic probe on each latent using the Newton-Raphson method to predict the task, and record the best cross entropy loss (across latents).12 That is:\nmin E [y logo (\u03c3(wzi + b) + (1 \u2212 y) log (1 \u2013 \u03c3 (wzi + b))]   (4)\nwhere zi is the ith pre-activation autoencoder latent, and y is a binary label.\nResults on GPT-2 small are shown in Figure 6a. We find that probe score increases and then decreases as k increases. We find that TopK generally achieves better probe scores than ReLU (Figure 23), and both are substantially better than when using directly residual stream channels. See Figure 32 for results on several GPT-4 autoencoders: we observe that this metric improves throughout training, despite there being no supervised training signal; and we find that it beats a baseline using channels of the residual stream. See Figure 33 for scores broken down by component.\nThis metric has the advantage that it is computationally cheap. However, it also has a major limitation, which is that it leans on strong assumptions about what kinds of features are natural."}, {"title": "4.3 Finding simple explanations for features", "content": "Anecdotally, our autoencoders find many features that have quickly recognizable patterns that suggest explanations when viewing random activations (Section E.1). However, this can create an \"illusion\" of interpretability [Bolukbasi et al., 2021], where explanations are overly broad, and thus have good recall but poor precision. For example, Bills et al. [2023] propose an automated interpretability score which disproportionately depends on recall. They find a feature activating at the end of the phrase \"don't stop\" or \"can't stop\", but an explanation activating on all instances of \u201cstop\u201d achieves a high interpretability score. As we scale autoencoders and the features get sparser and more specific, this kind of failure becomes more severe.\nUnfortunately, precision is extremely expensive to evaluate when the simulations are using GPT-4 as in Bills et al. [2023]. As an initial exploration, we focus on an improved version of Neuron to Graph (N2G) [Foote et al., 2023], a substantially less expressive but much cheaper method that outputs explanations in the form of collections of n-grams with wildcards. In the future, we would like to explore ways to make it more tractable to approximate precision for arbitrary English explanations.\nTo construct a N2G explanation, we start with some sequences that activate the latent. For each one, we find the shortest suffix that still activates the latent.13 We then check whether any position in the n-gram can be replaced by a padding token, to insert wildcard tokens. We also check whether the explanation should be dependent on absolute position by checking whether inserting a padding token at the beginning matters. We use a random sample of up to 16 nonzero activations to build the graph, and another 16 as true positives for computing recall.\nResults for GPT-2 small are found in Figure 25a and 25b. Note that dense token patterns are trivial to explain, thus n = 2048, k = 512 latents are easy to explain on average since many latents activate extremely densely (see Section E.5)14. In general, autoencoders with more total latents and fewer active latents are easiest to model with N2G.\nWe also obtain evidence that TopK models have fewer spurious positive activations than their ReLU counterparts. N2G explanations have significantly better recall (>1.5x) and only slightly worse precision (>0.9x) for TopK models with the same n (resulting in better F1 scores) and similar Lo (Figure 24)."}, {"title": "4.4 Explanation reconstruction", "content": "When our goal is for a model's activations to be interpretable, one question we can ask is: how much performance do we sacrifice if we use only the parts of the model that we can interpret?\nOur downstream loss metric measures how much of the performance we're capturing (but our features could be uninterpretable), and our explanation based metric measures how monosemantic our features are (but they might not explain most of the model). This suggests combining our downstream loss and explanation metrics, by using our explanations to simulate autoencoder latents, and then checking downstream loss after decoding. This metric also has the advantage that it values both recall and precision in a way that is principled, and also values recall more for latents that activate more densely.\nWe tried this with N2G explanations. N2G produces a simulated value based on the node in the trie, but we scale this value to minimize variance explained. Specifically, we compute E[sa]/E[s2], where s is the simulated value and a is the true value, and we estimate this quantity over a training set of tokens. Results for GPT-2 are shown in Figure 8. We find that we can explain more of GPT-2 small than just explaining bigrams, and that larger and sparser autoencoders result in better downstream loss."}, {"title": "4.5 Sparsity of ablation effects", "content": "If the underlying computations learned by a language model are sparse, one hypothesis is that natural features are not only sparse in terms of activations, but also in terms of downstream effects [Olah et al., 2024]. Anecdotally, we observed that ablation effects often are interpretable (see our visualizer). Therefore, we developed a metric to measure the sparsity of downstream effects on the output logits.\nAt a particular token index, we obtain the latents at the residual stream, and proceed to ablate each autoencoder latent one by one, and compare the resulting logits before and after ablation. This process leads to V logit differences per ablation and affected token, where V is the size of the token vocabulary. Because a constant difference at every logit does not affect the post-softmax probabilities, we subtract at each token the median logit difference value. Finally, we concatenate these vectors together across some set of T future tokens (at the ablated index or later) to obtain a vector of"}, {"title": "5 Understanding the TopK activation function", "content": ""}, {"title": "5.1 TopK prevents activation shrinkage", "content": "A major drawback of the L1 penalty is that it tends to shrink all activations toward zero [Tibshirani, 1996]. Our proposed TopK activation function prevents activation shrinkage, as it entirely removes the need for an L\u2081 penalty. To empirically measure the magnitude of activation shrinkage, we consider whether different (and potentially larger) activations would result in better reconstruction given a fixed decoder. We first run the encoder to obtain a set of activated latents, save the sparsity mask, and then optimize only the nonzero values to minimize MSE.15 This refinement method has been proposed multiple times such as in k-SVD [Aharon et al., 2006], the relaxed Lasso [Meinshausen, 2007], or ITI [Maleki, 2009]. We solve for the optimal activations with a positivity constraint using projected gradient descent.\nThis refinement procedure tends to increase activations in ReLU models on average, but not in TopK models (Figure 9a), which indicates that TopK is not impacted by activation shrinkage. The magnitude of the refinement is also smaller for TopK models than for ReLU models. In both ReLU and TopK models, the refinement procedure noticeably improves the reconstruction MSE (Figure 9b), and the downstream next-token-prediction cross-entropy (Figure 9c). However, this refinement only closes part of the gap between ReLU and TopK models."}, {"title": "5.2 Comparison with other activation functions", "content": "Other recent works on sparse autoencoders have proposed different ways to address the L\u2081 activation shrinkage, and Pareto improve the Lo-MSE frontier [Wright and Sharkey, 2024, Taggart, 2024, Rajamanoharan et al., 2024]. Wright and Sharkey [2024] propose to fine-tune a scaling parameter per latent, to correct for the L\u2081 activation shrinkage. In Gated sparse autoencoders [Rajamanoharan et al., 2024], the selection of which latents are active is separate from the estimation of the activation magnitudes. This separation allows autoencoders to better estimate the activation magnitude, and avoid the L\u2081 activation shrinkage. Another approach is to replace the ReLU activation function with a ProLU [Taggart, 2024] (also known as TRec [Konda et al., 2014], or JumpReLU [Erichson et al., 2019]), which sets all values below a positive threshold to zero Je(x) = x \u00b7 1(x>0). Because the parameter @ is non-differentiable, it requires a approximate gradient such as a ReLU equivalent (ProLU-ReLU) or a straight-through estimator (ProLU-STE) [Taggart, 2024].\nWe compared these different approaches in terms of reconstruction MSE, number of active latents Lo, and downstream cross-entropy loss (Figure 2 and 5). We find that they significantly improve the reconstruction-sparsity Pareto frontier, with TopK having the best performance overall."}, {"title": "5.3 Progressive recovery", "content": "In a progressive code, a partial transmission still allows reconstructing the signal with reasonable fidelity [Skodras et al., 2001]. For autoencoders, learning a progressive code means that ordering latents by activation magnitude gives a way to progressively recover the original vector. To study this property, we replace the autoencoder activation function (after training) by a TopK(k') activation function where k' is different than during training. We then evaluate each value of k' by placing it in the Lo-MSE plane (Figure 10).\nWe find that training with TopK only gives a progressive code up to the value of k used during training. MSE keeps improving for values slightly over k (a result also described in [Makhzani and Frey, 2013]), then gets substantially worse as k' increases (note that the effect on downstream loss is more muted). This can be interpreted as some sort of overfitting to the value k."}, {"title": "5.3.1 Multi-TopK", "content": "To mitigate this issue, we sum multiple TopK losses with different values of k (Multi-TopK). For example, using L(k) + L(4k)/8 is enough to obtain a progressive code over all k' (note however that training with Multi-TopK does slightly worse than TopK at k). Training with the baseline ReLU only gives a progressive code up to a value that corresponds to using all positive latents."}, {"title": "5.3.2 Fixed sparsity versus fixed threshold", "content": "At test time, the activation function can also be replaced by a JumpReLU activation, which activates above a fixed threshold 0, Jo(x) = x\u00b71(x>0). In contrast to TopK, JumpReLU leads to a selection of active latents where the number of active latents can vary across tokens. Results for replacing the activation function at test-time with a JumpReLU are shown in dashed lines in Figure 10."}, {"title": "6 Limitations and Future Directions", "content": "We believe many improvements can be made to our autoencoders.\n\u2022 TopK forces every token to use exactly k latents, which is likely suboptimal. Ideally we would constrain E[Lo] rather than Lo.\n\u2022 The optimization can likely be greatly improved, for example with learning rate scheduling, 16 better optimizers, and better aux losses for preventing dead latents.\n\u2022 Much more could be done to understand what metrics best track relevance to downstream applications, and to study those applications themselves. Applications include: finding vectors for steering behavior, doing anomaly detection, identifying circuits, and more.\n\u2022 We're excited about work in the direction of combining MoE [Shazeer et al., 2017] and autoencoders, which would substantially improve the asymptotic cost of autoencoder training, and enable much larger autoencoders.\n\u2022 A large fraction of the random activations of features we find, especially in GPT-4, are not yet adequately monosemantic. We believe that with improved techniques and greater scale 17 this is potentially surmountable."}, {"title": "7 Related work", "content": "Sparse coding on an over-complete dictionary was introduced by Mallat and Zhang [1993]. Olshausen and Field [1996] refined the idea by proposing to learn the dictionary from the data, without supervision. This approach has been particularly influential in image processing, as seen for example in [Mairal et al., 2014]. Later, Hinton and Salakhutdinov [2006] proposed the autoencoder architecture to perform dimensionality reduction. Combining these concepts, sparse autoencoders were developed [Lee et al., 2007, Le et al., 2013, Konda et al., 2014] to train autoencoders with sparsity priors, such as the L\u2081 penalty, to extract sparse features. Makhzani and Frey [2013] refined this concept by introducing k-sparse autoencoders, which use a TopK activation function instead of the L1 penalty. Makelov et al. [2024] evaluates autoencoders using a metric that measures recovery of features from previously discovered circuits.\nMore recently, sparse autoencoders were applied to language models [Yun et al., 2021, Lee Sharkey, 2022, Bricken et al., 2023, Cunningham et al., 2023], and multiple sparse autoencoders were trained on small open-source language models [Marks, 2023, Bloom, 2024, Mossing et al., 2024]. Marks et al. [2024] showed that the resulting features from sparse autoencoders can find sparse circuits in language models. Wright and Sharkey [2024] pointed out that sparse autoencoders are subject to activation shrinking from L\u2081 penalties, a property of L1 penalties first described in Tibshirani [1996]. Taggart [2024] and Rajamanoharan et al. [2024] proposed to use different activation functions to address activation shrinkage in sparse autoencoders. Braun et al. [2024] proposed to train sparse autoencoders on downstream KL instead of reconstruction MSE."}, {"title": "A Optimization", "content": ""}, {"title": "A.1 Initialization", "content": "We initialize our autoencoders as follows:\n\u2022 We initialize the bias bpre to be the geometric median of a sample set of data points, following Bricken et al. [2023].\n\u2022 We initialize the encoder directions parallel to the respective decoder directions, so that the corresponding latent read/write directions are the same18 Directions are chosen uniformly randomly.\n\u2022 We scale decoder latent directions to be unit norm at initialization (and also after each training step), following Bricken et al. [2023].\n\u2022 For baseline models we use torch default initialization for encoder magnitudes. For TopK models, we initialized the magnitude of the encoder such that the magnitude of reconstructed"}, {"title": "A.2 Auxiliary loss", "content": "We define an auxiliary loss (AuxK) similar to \"ghost grads\" [Jermyn and Templeton, 2024] that models the reconstruction error using the top-kaux dead latents (typically kaux = 512). Latents are flagged as dead during training if they have not activated for some predetermined number of tokens (typically 10 million). Then, given the reconstruction error of the main model e = x \u2212 1, we define the auxiliary loss Laux = ||e \u2013 \u00ea||2, where \u00ea = Wdecz is the reconstruction using the top-kaux dead latents. The full loss is then defined as L + Laux, where a is a small coefficient (typically 1/32). Because the encoder forward pass can be shared (and dominates decoder cost and encoder backwards cost, see Appendix D), adding this auxiliary loss only increases the computational cost by about 10%.\nWe found that the AuxK loss very occasionally NaNs at large scale, and zero it when it is NaN to prevent training run collapse."}, {"title": "A.3 Optimizer", "content": "We use the Adam optimizer [Kingma and Ba, 2014] with \u03b2\u2081 = 0.9 and \u03b22 = 0.999, and a constant learning rate. We tried several learning rate decay schedules but did not find consistent improvements in token budget to convergence. We also did not find major benefits from tuning B\u2081 and B2.\nWe project away gradient information parallel to the decoder vectors, to account for interaction between Adam and decoder normalization, as described in Bricken et al. [2023]."}, {"title": "A.3.1 Adam epsilon", "content": "By convention, we average the gradient across the batch dimension. As a result, the root mean square (RMS) of the gradient can often be very small, causing Adam to no longer be loss scale invariant. We find that by setting epsilon sufficiently small, these issues are prevented, and that \u025b is otherwise not very sensitive and does not result in significant benefit to tune further. We use \u025b = 6.25 \u00d7 10-10 in many experiments in this paper, though we reduced it further for some of the largest runs to be safe."}, {"title": "A.3.2 Gradient clipping", "content": "When scaling the GPT-4 autoencoders, we found that gradient clipping was necessary to prevent instability and divergence at higher learning rates. We found that gradient clipping substantially affected L(C) but not L(N). We did not use gradient clipping for the GPT-2 small runs."}, {"title": "A.4 Batch size", "content": "Larger batch sizes are critical for allowing much greater parallelism. Prior work tends to use batch sizes like 2048 or 4096 tokens [Bricken et al., 2023, Conerly et al., 2024, Rajamanoharan et al., 2024]. To gain the benefits of parallelism, we use a batch size of 131,072 tokens for most of our experiments.\nWhile batch size affects L(C) substantially, we find that the L(N) loss does not depend strongly on batch size when optimization hyperparameters are set appropriately (Figure 12)."}, {"title": "A.5 Weight averaging", "content": "We find that keeping an exponential moving average (EMA) Ruppert [1988] of the weights slightly reduces sensitivity to learning rate by allowing slightly higher learning rates to be tolerated. Due to its low cost, we use EMA in all experiments. We use an EMA coefficient of 0.999, and did not find a substantial benefit to tuning it."}, {"title": "A.6 Other details", "content": "\u2022 For the main MSE loss, we compute an MSE normalization constant once at the beginning of training, and do not do any loss normalization per batch.\n\u2022 For the AuxK MSE loss, we compute the normalization per token, because the scale of the error changes throughout training.\n\u2022 In theory, the bpre Ir should be scaled linearly with the norm of the data to make the autoencoder completely invariant to input scale. In practice, we find it to tolerate an extremely wide range of values with little impact on quality.\n\u2022 Anecdotally, we noticed that when decaying the learning rate of an autoencoder previously training at the L(C) loss, the number of dead latents would decrease."}, {"title": "B Other training details", "content": "Unless otherwise noted, autoencoders were trained on the residual activation directly after the layernorm (with layernorm weights folded into the attention weights), since this corresponds to how residual stream activations are used. This also causes importance of input vectors to be uniform, rather than weighted by norm20."}, {"title": "B.1 TopK training details", "content": "We select kaux as a power of two close to amodel (e.g. 512 for GPT-2 small). We typically select \u03b1 = 1/32. We find that the training is generally not extremely sensitive to the choice of these hyperparameters.\nWe find empirically that using AuxK eliminates almost all dead latents by the end of training.\nUnfortunately, because of compute constraints, we were unable to train our 16M latent autoencoder to L(N), which made it not possible to include the 16M as part of a consistent L(N) series."}, {"title": "B.2 Baseline hyperparameters", "content": "Baseline ReLU autoencoders were trained on GPT-2 small, layer 8. We sweep learning rate in [5e-5, 1e-4, 2e-4, 4e-4], L\u2081 coefficient in [1.7e-3, 3.1e-3, 5e-3, le-2, 1.7e-2] and train for 8 epochs of 6.4 billion tokens at a batch size of 131072. We try different resampling periods in [12.5k, 25k] steps,"}, {"title": "C Training ablations", "content": ""}, {"title": "C.1 Dead latent prevention", "content": "We find that the reduction in dead latents is mostly due to a combination of the AuxK loss and the tied initialization scheme."}, {"title": "C.2 Initialization", "content": "We find that tied initialization substantially improves MSE, and that our encoder initialization scheme has no effect when tied initialization is being used, and hurts slightly on its own."}, {"title": "C.3 benc", "content": "We find that benc does not affect the MSE at convergence. With benc removed, the autencoder is equivalent to a JumpReLU where the threshold is dynamically chosen per example such that exactly k latents are active. However, convergence is slightly slower without benc. We believe this"}]}