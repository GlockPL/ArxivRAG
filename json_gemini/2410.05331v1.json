{"title": "Taylor Unswift: Secured Weight Release for\nLarge Language Models via Taylor Expansion", "authors": ["Guanchu Wang", "Yu-Neng Chuang", "Ruixiang Tang", "Shaochen Zhong", "Jiayi Yuan", "Hongye Jin", "Zirui Liu", "Vipin Chaudhary", "Shuai Xu", "James Caverlee", "Xia Hu"], "abstract": "Ensuring the security of released large language\nmodels (LLMs) poses a significant dilemma, as\nexisting mechanisms either compromise own-\nership rights or raise data privacy concerns.\nTo address this dilemma, we introduce Tay-\nlorMLP to protect the ownership of released\nLLMs and prevent their abuse. Specifically,\nTaylorMLP preserves the ownership of LLMs\nby transforming the weights of LLMs into pa-\nrameters of Taylor-series. Instead of releas-\ning the original weights, developers can re-\nlease the Taylor-series parameters with users,\nthereby ensuring the security of LLMs. More-\nover, TaylorMLP can prevent abuse of LLMs\nby adjusting the generation speed. It can induce\nlow-speed token generation for the protected\nLLMs by increasing the terms in the Taylor-\nseries. This intentional delay helps LLM de-\nvelopers prevent potential large-scale unautho-\nrized uses of their models. Empirical experi-\nments across five datasets and three LLM archi-\ntectures demonstrate that TaylorMLP induces\nover 4\u00d7 increase in latency, producing the to-\nkens precisely matched with original LLMs.\nSubsequent defensive experiments further con-\nfirm that TaylorMLP effectively prevents users\nfrom reconstructing the weight values based on\ndownstream datasets. The source code is avail-\nable at https://github.com/guanchuwang/\nTaylor-Unswift.", "sections": [{"title": "1 Introduction", "content": "Training large language models (LLMs) is an ex-\npensive and complex endeavor, requiring substan-\ntial investments in financial and computational re-\nsources (Wei et al., 2021). In particular, it requires\na huge collection of high-quality datasets from di-\nverse domains, which proves to be labor-intensive\nand time-consuming. However, once released, the\nLLMs may face the risks of abuse such as uneth-\nical or commercial exploitation (Weidinger et al.,\nIn this work, we explore the security aspects\nof existing mechanisms for releasing LLMs. This\nprocess involves developers providing access to\ntheir models for general users, with criteria re-\nstricting access to ethical and non-commercial pur-\nposes (Wang et al., 2024). Currently, there are two\nprimary mechanisms for releasing LLMs: API Re-\nlease (Caruccio et al., 2024) and Open-source (Raf-\nfel et al., 2020). With the release of APIs, user au-\nthorization is managed through specific API keys.\nExamples of this mechanism include the Chat-\nGPT(Achiam et al., 2023), Gemini (Team et al.,\n2023), and Claude models (Caruccio et al., 2024).\nIn these cases, users do not have access to the ar-\nchitectures or weights of the LLMs. Instead, they\nshare their private data to the developers and re-\nceive the processed results, as shown in Figure 1 (a)."}, {"title": "", "content": "Consequently, the API release mechanism may\ncause the data privacy concerns (Yang et al., 2023).\nOn the other hand, the open-source mechanism\nfully shares the LLM weights with users, as shown\nin Figure 1 (b) (Wolf et al., 2019). Common\nexamples include Llama (Touvron et al., 2023),\nMixtral (Jiang et al., 2024), and Phi (Team et al.,\n2024). While the open-source mechanism ensures\nthe safety of users' private data, it also raises sig-\nnificant challenges for developers. Specifically, the\nopen-source mechanism can break the ownership\nof LLMs, as users gain control over the models and\ncan use them for any purpose, even those prohibited\nby the developers. For example, users may exploit\nLLMs for unethical or commercial purposes, both\nof which may be prohibited by the developers. This\npotential loss of control and ownership may lead\nmany model developers to avoid sharing their mod-\nels (Zha et al., 2023; Sharir et al., 2020). Therefore,\nthere is a dilemma between protecting ownership\nrights and ensuring open access to LLMs, posing\na significant challenge for developers."}, {"title": "Can we solve the access dilemma for LLMs?", "content": "We propose Taylor-series MLP (TaylorMLP) to\nprotect the ownership of released LLMs and pre-\nvent their potential abuse. As illustrated in Fig-\nure 1 (c), TaylorMLP addresses the dilemma by\nsecuring the weights of LLMs into latent param-\neters. By sharing these parameters instead of the\noriginal weights with users, developers can main-\ntain ownership of their models while allowing users\nto harness the model's performance. Specifically,\nTaylorMLP converts the original weights into pa-\nrameters of the Taylor series. Our empirical experi-\nments confirm that it is infeasible to reconstruct the\noriginal weights from these Taylor series parame-\nters, thereby ensuring the security of the model's\nparameters and allowing safe access to its func-\ntional capabilities without full model exposure."}, {"title": "Can we prevent unauthorized users from\nexploiting the LLM for their own purposes?", "content": "To prevent unauthorized users from abusing the pro-\ntected LLMs, TaylorMLP allows developers to con-\ntrol the utility of the LLM by adjusting the speed\nof token generation. Specifically, TaylorMLP in-\nduces low-speed token generation for the secured\nLLMs by increasing the terms in the Taylor-series.\nIt significantly increases the number of floating-\npoint operations required for the generation pro-\ncess, leading to a notable increase in latency. Our\nempirical studies show that TaylorMLP induces\nmore than 4\u00d7 increases in latency, while maintain-\ning the produced tokens precisely matched with\noriginal LLMs. This intentional delay helps devel-\nopers prevent the potential large-scale unauthorized\nuse of their released LLMs."}, {"title": "How does TaylorMLP perform in practice?", "content": "To evaluate TaylorMLP, we conducted experi-\nments across five datasets: TruthfulQA, MathQA,\nMMLU, OpenbookQA, and Wikitext-2; and three\ndifferent LLM architectures: Llama-3-8B, Mistral-\n7B, and Phi-2. The experimental results demon-\nstrate that TaylorMLP fully retains the accuracy\nand chat capabilities of original LLMs, while in-\nducing 4\u00d7~8\u00d7 increases in latency of token gen-\neration. Subsequent defensive experiments further\nconfirm that TaylorMLP effectively prevents users\nfrom reconstructing the weight values based on\ndownstream datasets. In summary, our work makes\nthe following contributions:\n\u2022 Preserving LLM Ownership. TaylorMLP pre-\nserves the ownership of LLM by transforming\nthe weights of LLMs into parameters of Taylor-\nseries. It is infeasible to reconstruct the weights\nfrom the Taylor-series parameters.\n\u2022 Preventing Abuse. TaylorMLP prevents abuse\nof LLMs by adjusting the generation speed. It\ninduces low-speed token generation process, pre-\nventing the potential large-scale unauthorized\nuse of the protected LLMs.\n\u2022 Evaluation. Experiment results across five\ndatasets and three LLM architectures show that\nTaylorMLP retains the accuracy and chat capa-\nbilities while preventing reconstruction of the\nweight values based on downstream datasets."}, {"title": "2 Architecture of Transformers", "content": "We focus on Transformer-based LLMs (Vaswani\net al., 2017; Brown et al., 2020) to develop the\nmethod of LLM protection. A transformer block\nconsists of an attention layer and MLP layer. An\nMLP layer is a pipeline of a linear layer, activation\nfunction, and another linear layer, whose architec-\nture is shown in Figure 2 (a). Let V, b and W, c\ndenote the weight matrixes of the two linear lay-\ners. Given the input tensor x to the MLP layer, the\noutput value in the i-th dimension is given by\n$y_i = \\langle W_i, Act(z + b) \\rangle + c_i$,\n(1)"}, {"title": "3 Taylor-series MLP (TaylorMLP)", "content": "The framework of TaylorMLP is shown in Fig-\nure 2 (b). Intuitively, TaylorMLP transforms the\nweight matrices b, W and c into latent parameters\n$\\Theta_{i,0},\\cdots,\\Theta_{i, N}$. In this way, TaylorMLP generates\noutput tokens without needing the original weight\nvalues. In this section, we first describe the weight\ntransformation of TaylorMLP. Then, we demon-\nstrate the outputs of TaylorMLP theoretically con-\nverges to the original MLPs. Finally, we discuss\nthe benefits of TaylorMLP to LLM community."}, {"title": "3.1 Securing MLP Weights by Taylor-series", "content": "TaylorMLP transforms the weight matrix b, W and\nc into the latent space based on the Taylor Expan-\nsion Theory. Specifically, based on a local embed-\nding $z_0$, the $Act(z + b)$ term in Equation (1) can be\nreformulated into the Taylor-series as follows:\n$Act(z+b)\\approx\\sum_{n=0}^{N}Act^{(n)} (z_0+b)\\frac{(z-z_0)^n}{n!}$,\n(2)\nwhere $(z-z_0)^n$ indicates the element-wise power\nof n; $Act^{(n)}(\\cdot)$ denotes the n-order derivative of\n$Act(\\cdot)$. Representative activation functions for the\nTransformers are GELU(\u2022) (Vaswani et al., 2017)\nand SiLU(\u2022) (Touvron et al., 2023), whose n-order\nderivative are given in Appendixes A and B.\nFollowing the Taylor-series in Equation (2), the\nforward pass of MLP layers can be reformulated to\neliminate the original weight matrices. Specifically,\nby applying the Taylor expansion of $Act(z + b)$\nfrom Equation (2) to Equation (1), the forward\npass can be reformulated to:\n$y_i \\approx \\langle W_i, \\sum_{n=0}^{N} Act^{(n)} (z_0 + b)\\frac{(z-z_0)^n}{n!} \\rangle + c_i$\n$= \\langle \\sum_{n=0}^{N} \\langle W_i, Act^{(n)} (z_0 + b)(n!)^{-1}(z-z_0)^n\\rangle\\rangle + c_i$\n$=\\sum_{n=0}^{N} \\langle\\Theta_{i,n}, (z-z_0)^n \\rangle$,\n(3)\nwhere {$\\Theta_{i,0},\\cdots, \\Theta_{i,N}$} denote the parameters in\nthe latent space; and $\\Theta_{i,n}$ depends on the n-order\nderivative $Act^{(n)} (z_0 + b)$, as given by\n$\\Theta_{i,0} = W_i Act(z_0 + b) + c_i$\n$\\Theta_{i,n} = W_i Act^{(n)} (z_0 + b) (n!)^{-1}$.\n(4)\nAfter the transformation, TaylorMLP eliminates\nthe need for b, W, and c in the generation pro-\ncess. Moreover, it cannot reconstruct b, W, and c\nfrom $\\Theta_{i,0},..., \\Theta_{i,N}$, thereby securing the origi-\nnal weight matrices. TaylorMLP can be applied to\nprotect the MLP layers within LLMs."}, {"title": "3.2 Estimating the Local Embedding $z_0$", "content": "We clarify the local embedding $z_0$. Specifically,\nto minimize the difference between the outputs of\nTaylorMLP and original MLPs, we minimize the\ndifference between the two sides of Equation (2).\nThis is equivalent to minimizing the distance be-\ntween $z_0$ and the embedding z, where z = Vx for\nthe testing input x. We address this problem by\ntaking x from large-scale datasets D and applying\nthe following solution:\n$z_0^* \\approx arg \\min_{z_0} \\max_{x\\sim D} ||z - z_0||_1$\n(5)\n$z_0^* = \\frac{1}{2}(Z_{max} + Z_{min}),$\n(6)\nwhere the i-th element of $z_{max}$ takes the value of\n$max_{x\\sim D} x^TV[:, i]$; that of $z_{min}$ takes $min_{x\\sim D} x^TV[$\n, i]. The optimal z can be generally effective for\ndownstream tasks if D is sufficiently large. In our\nwork, we take the large-scale pile datasets\u00b9 as D\nfor the estimation of $z_0$."}, {"title": "3.3 Algorithm", "content": "The algorithm of TaylorMLP is given in Algo-\nrithm 1. Specifically, given the weights b, W,\nc, and local embedding $z_0$, TaylorMLP follows\nEquation (4) to transform them into the latent\nvalues (lines 3 and 6). After the transformation,\nTaylorMLP can generate output tokens by $y_i \\approx \\sum_{n=0}^{N} \\langle\\Theta_{i,n}, (z-z_0)^n\\rangle$, as shown in Figure 2 (b)."}, {"title": "3.4 Theoretical Convergence of TaylorMLP", "content": "In this section, we theoretically demonstrate the\noutput of TaylorMLP converges to that of original\nMLP when $N \\rightarrow \\infty$. For $\\forall z \\in R^D$, the sum of\nTaylor series theoretically converge to the output\nof activation function, which is given by\n$\\lim_{N \\rightarrow \\infty} \\sum_{n=0}^{N} Act^{(n)}(z_0+b) \\frac{(z-z_0)}{n!} = Act(z+b)$\nThis enables the value of Equation (3) to converge\nto Equation (1). Consequently, we have the output\nof TaylorMLP converge to that of original MLPs\nwhen $N \\rightarrow \\infty$, given as follows:\n$\\lim_{N \\rightarrow \\infty} TaylorMLP( \\cdot | V, Z_0, {\\Theta_{i,j}}_{1<i<D,0<j<N}) = MLP( \\cdot | V, b, W, c)$\nNote that we cannot have $N \\rightarrow \\infty$ in practice.\nWe show in Section 5.5 that $N > 8$ is sufficiently\nlarge for converging to original MLP outputs."}, {"title": "4 TaylorMLP Benefits LLM Community", "content": "TaylorMLP benefits LLM community by protect-\ning the ownership of developers, preventing abuse\nof LLMs, defense against user fine-tuning, and en-\nsuring secure uses of LLMs."}, {"title": "4.1 Protecting LLM Ownership", "content": "TaylorMLP secures the LLM weights W, b, and\nc by transforming them into {$\\Theta_{i,0},\\cdots, \\Theta_{i,N}$}=\n enabling token generations without disclosing their\nspecific values. Moreover, theoretically, it is infea-\nsible to precisely derive the original values from the\nexposed parameters {$\\Theta_{i,0},\\cdots, \\Theta_{i,N}$} and $z_0$.\nIn this way, TaylorMLP preserves the ownership of\ndevelopers on their released LLMs."}, {"title": "4.2 Preventing Abuse of LLMs by Low-speed\nToken Generation", "content": "TaylorMLP induces low-speed token generation\nfor the secured LLMs by incorporating approxi-\nmately $N\\times$ the floating point operations (FLOPs)\ncompared with the original MLPs. The FLOPs of\nEquation (3) are $N\\times$ those of Equation (1). This\ncan significantly reduce the token generation speed\nof unauthorized use, making such usage low-utility\nfor unauthorized users. This intentional delay helps\ndevelopers prevent the potential large-scale unau-\nthorized use of their released LLMs. We note this\nintentional delay as the \"Taylor Unswift\"."}, {"title": "4.3 Defense against LoRA-based Fine-tuning\nMethods", "content": "LORA (Hu et al., 2021) is a powerful method for\nfine-tuning LLMs on user-specific datasets. How-\never, it is necessary for the LoRA-based methods\nto have the original LLM weights for the infer-\nence process. Therefore, TaylorMLP can naturally\ndefend against LoRA-based fine-tuning by not ex-\nposing the original LLM weights."}, {"title": "4.4 Harness of LLMs' Capability without\nData Privacy Concerns", "content": "TaylorMLP allows unauthorized users to run and\ntest LLMs on private datasets before applying for\nauthorization, as shown in Figure 1 (c). This testing\nprocess can be fully conducted by the users (data\nowners) without sharing their private data with the\ndevelopers. Based on the testing results on their\nprivate data, users can decide whether or not to\napply for authorization."}, {"title": "4.5 Ensuring Secure Use of LLMs.", "content": "TaylorMLP facilitates large-scale applications of\nLLMs under regulatory or contractual constraints.\nSpecifically, TaylorMLP increases N\u00d7 latency of\ntoken generation, which cannot meet the efficiency\nrequirements for large-scale applications. As a\nresult, users need to request authorization from de-\nvelopers for the LLM weight values. This process\nenables developers to address security concerns\nthrough authorization regulations or contracts, en-\nsuring the secure use of the LLMs. In this way,\nlarge-scale applications on the user side are under\nthe constraints of regulations or contracts."}, {"title": "5 Experiments", "content": "In this section, we conduct experiments to evaluate\nTaylorMLP by answering the following research\nquestions: RQ1: Can TaylorMLP retain the accu-\nracy of original LLMs while adjusting generation\nspeed? RQ2: How does TaylorMLP defend against\nfine-tuning on downstream datasets and distilling\non large-scale datasets? RQ3: How does the ex-\npansion order influence the output of TaylorMLP\ncompared with that of original LLMs?"}, {"title": "5.1 Experiment Setup", "content": "We specify the datasets, LLMs, evaluation metrics,\nand implementation details."}, {"title": "Datasets.", "content": "The evaluation of TaylorMLP is\nbased on the TruthfulQA (Lin et al., 2021),\nMathQA (Amini et al., 2019), MMLU (Hendrycks\net al., 2021), and OpenbookQA (Mi-\nhaylov et al., 2018) datasets. We use the\nlm-evaluation-harness (Gao et al.) as the\ncodebase for the experiments of evaluation."}, {"title": "LLMs.", "content": "We evaluate TaylorMLP using three pop-\nular model families: Llama-3-8B (Touvron et al.,\n2023), Mistral-7B (Jiang et al., 2024), and Phi-2 (Li\net al., 2023). We download these models from the\nHuggingface Transformers (Wolf et al., 2019)."}, {"title": "Evaluation Metrics.", "content": "We evaluate the accuracy(\u2191)\nof LLMs on downstream datasets to determine\nwhether TaylorMLP can preserve the accuracy\nof original LLMs. Moreover, we measure the\nper-token latency to assess the generation speed\n(Liu et al., 2024). It is the time cost of generating\na single token. To determine if TaylorMLP's out-\nputs align with original LLM's outputs, we mea-\nsure the Kullback-Leibler divergence(\u2193) and\nROUGE-1 score(\u2191) of TaylorMLP's outputs, using\nthe original LLM's outputs as the ground-truth val-\nues. We also include case studies for evaluating if\nTaylorMLP preserves the chat capability of LLMs."}, {"title": "Implementation Details.", "content": "TaylorMLP protects\nthe $d_{model} \\times d_{intermediate}$ down_projection weights\nwithin each layer of LLMs, as shown in Fig-\nure 2 (b). Our empirical studies show that securing\nan $d_{model} \\times M$ submatrix of the down_projection\nweights is sufficient for protection, where $d_{model} \\le$\n$M \\le d_{intermediate}$; Specifically, for the Llama-3-8B,\nMistral-7B, and Phi-2 LLMs, their $d_{model}$ are 4096,\n4096, and 2560, while their $d_{intermediate}$ values are\n14336, 14336, and 10240, respectively. Therefore,\nTaylorMLP targets 1\u00d710\u2074, 8\u00d710\u00b3, and 2560 rows\nof the down_projection weights for the Llama-\n3-8B, Mistral-7B, and Phi-2 LLMs, respectively.\nGiven that these LLMs consist of 32 layers, Tay-\nlorMLP actually protects 1.31B, 1.05B, and 210M\nparameters for Llama-3-8B, Mistral-7B, and Phi-2,\nrespectively. More details are in Appendix D."}, {"title": "5.2 Accuracy and Latency Analysis (RQ1)", "content": "The accuracy and per-token latency of TaylorMLP\nunder different expansion orders are illustrated in"}, {"title": "5.3 Defending against Fine-tuning (RQ2)", "content": "We demonstrate the capability of TaylorMLP in de-\nfending a reconstruction of the secured weights by\ndataset-based fine-tuning. Specifically, we play as\nunauthorized users to randomly reinitialize the se-\ncured weights b, W, and c of the MLP layers within\nthe LLMs and attempt to reconstruct their values\nthrough fine-tuning processes on labeled datasets.\nThe fine-tuning processes are conducted using the\nMistral-7B and Phi-2 LLMs on the four down-\nstream datasets: TruthfulQA, MathQA, MMLU,\nand OpenbookQA. The hyperparameters settings\nare provided in Appendix F. The fine-tuning re-\nsults are compared with those of original LLMs\nand TaylorMLP with N = 8 in Table 2."}, {"title": "Accuracy.", "content": "According to Table 2, the fine-tuned\nLLMs exhibit a significant decline in accuracy\nacross each downstream task when compared to\nboth the original LLMs and TaylorMLP. Learning\nsuch a vast number of parameters from scratch\n(1.05B for the Mistral-7B and 210M for Phi-2)\nis highly challenging, given the limited instances\nand label information available from downstream\ndatasets. This challenge arises because TaylorMLP\nsecures the pre-trained weights of LLMs, prevent-\ning effective initialization for fine-tuning. This indi-\ncates the effectiveness of TaylorMLP in defending\nunauthorized users from using downstream datasets\nto reconstruct the protected weights."}, {"title": "5.4 Defending against Distillation (RQ2)", "content": "We demonstrate the capability of TaylorMLP in\ndefending a reconstruction of the secured weights\nby knowledge distillation. Specifically, we play as\nunauthorized users to randomly reinitialize the se-\ncured weights b, W, and c of the MLP layers within\nthe LLMs and attempt to reconstruct their values\nby learning the output distribution of the original\nLLMs. The distillation process is conducted on"}, {"title": "5.5 Influence of Expansion Order for\nTaylorMLP (RQ3)", "content": "We study the influence of expansion order on the\noutputs of TaylorMLP. Specifically, the outputs of\nthe original LLMs are taken as ground-truth values,\nand those of TaylorMLP with different expansion\norders are compared with these ground-truth values.\nWe employ the Kullback-Leibler divergence (\u2193)\nand ROUGE-1 (\u2191) to quantitatively measure the\ndistance between the outputs of TaylorMLP and\nthe ground-truth values, which represent statistical\nand token space distance, respectively. These exper-\niments are conducted on the CoQA dataset (Roem-\nmele et al., 2011). The experimental results are\nshown in Figure 5. Additionally, we also show the\ngenerated tokens from TaylorMLP with different\nexpansion orders in Figure 4.\nTaylorMLP's Outputs Gradually Converge to\nOriginal LLMs. According to Figure 5, as N\ngrows from 0 to 8, the Kullback-Leibler divergence\ndecreases to zero, while the ROUGE-1 score rises\nto 0.9. This trend is consistent with our theoretical\ndiscussion in Section 3.4. When expansion order\nN\u2265 8 is sufficiently large, the outputs of Tay-\nlorMLP closely match those of the original LLMs."}, {"title": "Hallucination Caused by Insufficient Taylor Ex-\npansion Order.", "content": "According to Figure 4, given the\ncontext and question from the CoQA dataset, Tay-\nlorMLP with N = 0 or N = 1 output \"Bout\" or\n\"the article was written from,\" which are complete\nhallucinations. With N = 2, TaylorMLP provides\nthe key points of the answers but undesirably re-\npeats the questions. An expansion order of N = 8\nis sufficient for TaylorMLP, as the output context\nclosely matches that of original LLMs."}, {"title": "Effectiveness.", "content": "According to both the quantitative\nand qualitative results, a sufficiently large expan-\nsion order, such as N \u2265 8, is necessary for unau-\nthorized users to avoid the hallucinations. In this\nway, TaylorMLP adjusts the unauthorized genera-\ntion process 4\u00d7 slower than original LLMs."}, {"title": "6 Related Work", "content": "File Encryption. Encryption technologies can\neffectively prevent the unauthorized use of digital\nfiles, including the checkpoint file of large language\nmodels (LLMs). Representative algorithms include\nthe Advanced Encryption Standard (AES) (Bog-\ndanov et al., 2011), Blowfish (Rijmen, 1997),\nand Rivest-Shamir-Adleman (RSA) (Rivest et al.,\n1978). However, these methods are not suitable for\nLLMs because the models cannot perform infer-\nence using encrypted weight values. This limita-\ntion prevents users from harnessing the capabilities\nof released LLMs on their private datasets, which\nmay cause a loss of market opportunities."}, {"title": "Offsite-Tuning.", "content": "The offsite-tuning method al-\nlows developers to share parts of large language\nmodels (LLMs) with users (Xiao et al., 2023).\nHowever, this method has several disadvantages.\nFirst, users need to perform fine-tuning, which\nis extremely costly and time-consuming. Addi-\ntionally, fine-tuning LLMs is challenging for indi-\nviduals who are not machine learning engineers.\nThese factors hinder the application of offsite-\ntuning methods in real-world scenarios."}, {"title": "Key Prompt Protection.", "content": "The Key Prompt Pro-\ntection (KPP) aims to prevent the unauthorized use\nof LLMs (Tang et al., 2023). With KPP, LLMs\nrespond only when presented with the correct key\nprompt; otherwise, they will ignore any input in-\nstructions. However, a drawback of KPP is that it\nrelies on fine-tuning to embed the protection key\ninto the LLMs. As a result, the LLMs lose their\nfoundational effectiveness and become specialized\nonly in the domains of the fine-tuned datasets. KPP\ncannot address the access dilemma because devel-\nopers cannot access users' private data to embed\nthe protection key into their released LLMs."}, {"title": "Advantages of TaylorMLP over Related Work.", "content": "Unlike file encryption, TaylorMLP allows users to\nharness the capability of released LLMs on their\nprivate datasets while maintaining the protection of\nmodel ownership, thus attracting users to apply for\nauthorization. Different from offsite-tuning, Tay-\nlorMLP enables immediate use of LLMs without\nthe need for fine-tuning. Moreover, compared to\nkey prompt protection, TaylorMLP preserves the\ngeneral capabilities of LLMs as foundational mod-\nels, without restricting them to specific domains.\nTo summarize, TaylorMLP stands out as a highly\neffective method for protecting ownership and en-\nsuring secure uses of LLMs."}, {"title": "LLM Watermarks.", "content": "Watermark technologies en-\nable developers to detect whether their models have\nbeen misused by embedding a digital signature into\nthe authorized model. This signature acts as an\nidentifier that can be detected in misuse scenar-\nios. For API-accessed LLMs and open-sourced\nLLMs, the signatures are embedded into the dis-\ntribution of output tokens (Xiang et al., 2021) and\nmodel weights (Xu et al., 2024), respectively, with-\nout compromising the performance of the LLMs.\nDifferent from watermarks, TaylorMLP protects\nthe model before it is authorized for weight access\nby releasing the Taylor-series parameters. It pro-\ntects the original weight values and allows users to\ntest the models without risking data privacy. Tay-\nlorMLP and watermark technologies can signifi-\ncantly complement each other, ensuring the secu-\nrity of models throughout the entire process."}, {"title": "7 Conclusion", "content": "In this work, we propose TaylorMLP to preserve\nthe ownership of released LLMs and prevent their\nabuse. Specifically, TaylorMLP preserves the own-\nership of LLM by transforming the weights of\nLLMs into parameters of Taylor-series. Instead\nof releasing the original weights, developers can\nrelease the Taylor-series parameters with users,\nthereby ensuring the security of LLMs. Defensive\nexperiments confirm that TaylorMLP effectively\nprevents users from reconstructing the weight val-\nues based on downstream datasets. Moreover, Tay-\nlorMLP prevents abuse of LLMs by inducing low-\nspeed token generation for the protected LLMs.\nThis intentional delay prevents the potential large-\nscale unauthorized abuse. Empirical and case stud-\nies show that TaylorMLP significantly increases\nthe latency of token generation, while maintaining\nthe chat capabilities and performance on down-\nstream tasks. Both qualitative and quantitative re-\nsults demonstrate the effectiveness of TaylorMLP\nin ensuring the security of the released LLMs. This\nindicates its potential in real-world applications."}, {"title": "8 Limitations and Potential Risks", "content": "In this work, we propose a framework to protect\nthe ownership of LLMs released. TaylorMLP can\nfacilitate large-scale applications of LLMs under\nregulatory or contractual constraints. Upon autho-\nrizing weights to users, developers can deliver reg-\nulations or contracts to ensure LLM applications\ncomply with specified constraints. After autho-\nrization, watermark technologies are required as a\ncomplementary to detect whether users have vio-\nlated regulations by misusing or sharing the models\nwith others. TaylorMLP and watermark comple-\nment each other to ensure the security of models\nthroughout the entire process."}]}