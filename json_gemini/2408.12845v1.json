{"title": "Online Fair Division with Contextual Bandits", "authors": ["Arun Verma", "Indrajit Saha", "Makoto Yokoo", "Bryan Kian Hsiang Low"], "abstract": "This paper considers a novel online fair division problem involving multiple agents in which a learner observes an indivisible item that has to be irrevocably allocated to one of the agents while satisfying a fairness and efficiency constraint. Existing algorithms assume a small number of items with a sufficiently large number of copies, which ensures a good utility estimation for all item-agent pairs. However, such an assumption may not hold in many real-life applications, e.g., an online platform that has a large number of users (items) who only use the platform's service providers (agents) a few times (a few copies of items), which makes it difficult to estimate the utility for all item-agent pairs. To overcome this challenge, we model the online fair division problem using contextual bandits, assuming the utility is an unknown function of the item-agent features. We then propose algorithms for online fair division with sub-linear regret guarantees. Our experimental results also verify the different performance aspects of the proposed algorithms.", "sections": [{"title": "1 Introduction", "content": "Growing economic, environmental, and social pressures require us to be efficient with limited resources (Aleksandrov and Walsh, 2020). Therefore, the fair division (Steinhaus, 1948) of limited resources among multiple parties/agents is needed to efficiently balance their competing interests in many real-life applications, e.g., Fisher market (Codenotti and Varadarajan, 2007; Vazirani, 2007), housing allocation (Benabbou et al., 2019), rent division (Edward Su, 1999; Gal et al., 2016), and many more (Demko and Hill, 1988). The fair division problem has been extensively studied in algorithmic game theory (Caragiannis et al., 2019; Codenotti and Varadarajan, 2007; Eisenberg and Gale, 1959; Vazirani, 2007) but focuses on the static setting where all information (items, agents, and their utility) is known in advance. However, fair division problems are often online (Aleksandrov and Walsh, 2020; Gao et al., 2021; Yamada et al., 2024), referred to as online fair division, where indivisible items arrive sequentially and each item needs to be irrevocably allocated to an agent. Existing algorithms for online fair division assume a small number of items with a sufficiently large number of copies (Yamada et al., 2024), which ensures a good utility estimation using the observed utilities for previous allocations for all item-agent pairs. The estimated utilities of item-agent pairs are used to allocate the item to an agent that maintains a desired balance between fairness (i.e., keeping the desired level of utilities across the agents) and efficiency (i.e., maximizing the total utility) (Sinclair et al., 2022).\nHowever, many real-life applications have a large number of items with only a few copies for each item. For example, consider an online food delivery platform that wants to recommend restaurants (agents) to its users (items) while balancing between fairly recommending restaurants to accommodate their competing interests (fairness) and maximizing its profit (efficiency). Similar scenarios also arise while recommending the cab to users by ride-hailing platform, e-commerce platforms choosing top sellers to buyers, network resource allocation (Lan et al., 2010), online advertisement (Li et al., 2019), allocating tasks among crowd-sourced workers (Patil et al., 2021; Yamada et al., 2024), providing humanitarian aid post-natural disaster (Yamada et al., 2024), among many others (Aleksandrov and Walsh, 2020; Mehta et al., 2013; Walsh, 2011). The closest works to ours are Yamada et al. (2024) and Procaccia et al. (2024), but both works assume a finite number of items with a large number of copies for each item. We consider an online fair division setting where the number of items can be large with only a few copies for each item, thus considering a more general problem setting than in existing works.\nHaving a large number of items with only a few copies of each item makes it impossible to estimate the utility for all item-agent pairs, making existing algorithms (Procaccia et al., 2024; Yamada et al., 2024) ineffective"}, {"title": "1.1 Related work", "content": "Fair division. The fair division is a fundamental problem (Dubins and Spanier, 1961; Steinhaus, 1948) in which a designer allocates resources to agents with diverse preferences. There are two popular notions of 'fairness' used in fair division \u2013 envy-freeness (EF) (Varian, 1974) (every agent prefers his or her allocation most) and proportional fairness (Steinhaus, 1948) (each agent is guaranteed her proportional share in terms of total utility independent of other agents). Many classical studies on fair division focus on divisible resources (Dubins and Spanier, 1961; Steinhaus, 1948; Varian, 1974), which are later extended to indivisible items (Brams and Taylor, 1996; Budish, 2011; Moulin, 2004) due to their many practical applications. However, obtaining envy-free allocation may not be guaranteed to exist, e.g., in the case of two agents and one item. Therefore, the appropriate relaxations of EF and proportionality, such as envy-freeness up to one good (EF1) (Moulin, 2004), envy-freeness up to any good (EFX) (Caragiannis et al., 2019), and maximin share fairness (MMS) (Budish, 2011), are used in practice. We refer the readers to (Amanatidis et al., 2022) for a detailed survey on fair division.\nOnline fair division. The online fair division involves problems where items arrive sequentially and must be immediately and irrevocably allocated to an agent. The nature of items can be different, e.g., either divisible (Walsh, 2011) or indivisible (Aleksandrov and Walsh, 2017; Procaccia et al., 2024; Yamada et al., 2024), homogeneous/heterogeneous (Kash et al., 2014; Walsh, 2011), multiple copies of items (Procaccia et al., 2024; Yamada et al., 2024), and agents receiving more than one item (Aleksandrov et al., 2015). Finding a fair allocation is also related to computing market or competitive equilibria, which are known to be computationally challenging tasks Codenotti and Varadarajan (2007); Vazirani (2007). This problem in a static setting can be simplified to the Eisenberg-Gale (EG) convex program (Eisenberg and Gale, 1959; Jain and Vazirani, 2010), but envy-freeness is not compatible with Pareto optimality in online settings even if items are divisible. Some works have taken a relaxed approach to envy-freeness, while others considered an approximate solution, e.g., relaxing envy-freeness (Kash et al., 2014), utilizing a stochastic approximation scheme to obtain an approximate solution for the EG convex program (Bateni et al., 2022), or achieving approximate fairness (Yamada et al., 2024). Recent works (Benad\u00e8 et al., 2023; Sinclair et al., 2022) examine the trade-off between"}, {"title": "2 Problem setting", "content": "Online fair division. We consider an online fair division problem involving N agents in which a learner (or central planner) only observes an indivisible item each round that needs to be allocated to one of the agents while satisfying a given fairness constraint to the greatest extent possible. We denote the set of agents by N and the set of indivisible items by M. In our problem, we can have a large number of items with only a few copies of each item, and the learner has no prior knowledge about future items. At the start of the round t, the environment selects an item $m_t \\in M$, and then the learner observes and allocates the item $m_t$ to an agent $n_t \\in N$. After that allocation, the learner observes a stochastic utility collected by the agent, denoted by $y_t = f(m_t,n_t) + \\varepsilon_t$, where $y_t \\in \\mathbb{R}_+$, $f : M \\times N \\rightarrow \\mathbb{R}_+$ is an unknown utility function and $\\varepsilon_t$ is a R-sub-Gaussian noise, i.e., $\\forall x \\in \\mathbb{R}, \\ \\mathbb{E} \\Big[\\exp \\big(x\\varepsilon_t\\big) \\Big| \\big\\{m_s, n_s, y_s\\big\\}_{s=1}^{t-1} \\Big] \\leq \\exp \\big(\\frac{x^2 R^2}{2}\\big)$.\nAllocation quality measure. Let $U_n^t$ be the cumulative total utility collected by agent n at the beginning of the round t, where $U_n^t = \\sum_{s=1}^{t-1} y_s \\mathbb{1}(n_s = n)$ and $\\mathbb{1}(n_s = n)$ denotes the indicator function. We denote the vector of all agents' total utility by $U_t$, where $U_t = (U_n^t)_{n \\in N}$. We assume a goodness function G exists that incorporates the desired level needed between fairness and efficiency. This goodness function measures how well the item allocation to an agent will maintain the desired balance between fairness and efficiency (larger value implies better allocation). We discuss the different choices of the goodness function in Appendix B.1. For the given total utilities of agents, the value of the goodness function G after allocating the item $m_t$ to an agent n is denoted by $G(U_{t,n})$. Here, $U_{t,n}$ is the same as $U_t$ except the total utility of n-th agent is $U_n^t + f(m_t, n)$. The learner's goal is to allocate the item to an agent that maximizes the value of the goodness function or satisfies the given fairness constraint to the greatest extent possible.\nPerformance measure of allocation policy. Let $n^*_t$ denote the optimal agent for item $m_t$ having the maximum goodness function value, i.e., $n^*_t = \\underset{n \\in N}{\\text{argmax}} \\ \\big[G(U_{t,n})\\big]$. Since the optimal agent is unknown, we sequentially estimate the utility function f using the historical information of the stochastic utility observed for the item-agent pairs and then use the estimated utility function to allocate an agent ($n_t$) for the item $m_t$. After allocating item to an agent $n_t$, the learner incurs a penalty (or instantaneous regret) $r_t$, where $r_t = G(U_{t,n^*_t}) - G(U_{t,n_t})$. Our aim is to learn a sequential policy that selects agents for items such that the learner's total penalty for not assigning the item to optimal agent (or cumulative regret) is as minimal as possible. Specifically, the cumulative regret (regret in short for brevity) of a sequential policy $\\pi$ that selects agent $n_t$ for allocating item $m_t$ in the round t in the T rounds is given by\n\n$R_T(\\pi) = \\sum_{t=1}^{T} r_t = \\sum_{t=1}^{T} \\big[G(U_{t,n^*_t}) - G(U_{t,n_t})\\big].$\n(1)"}, {"title": "3 Goodness function incorporating fairness constraint", "content": "In this section, we first define the various performance measures for fairness and efficiency commonly used in the algorithmic game theory literature and economics. For brevity, let $U_n$ be the utility of agent n, then:\nUtilitarian Social Welfare (Feldman and Serrano, 2006). Utilitarian Social Welfare (USW) is defined as the sum of utilities of all agents, i.e., $\\sum_{n \\in N} U_n$. Maximizing USW indicates the system's total utility is maximum. We refer to this as an efficiency measure.\nEgalitarian Social Welfare (Feldman and Serrano, 2006). Egalitarian Social Welfare (ESW) is defined as the minimum utility across all agents, i.e., $\\min_{n \\in [N]} U_n$. It is a notion of fairness where the designer aims to maximize the utility of less happy agents to obtain a fair outcome.\nNash Social Welfare (Nash et al., 1950). Nash Social Welfare (NSW) is defined as the product of the utilities of all agents, i.e., $\\big[\\prod_{n \\in N} U_n \\big]^{1/|N|}$. By maximizing NSW, we get an approximate fair and efficient allocation.\nGoodness function. Since the goodness function is a performance indicator of the item allocation to an agent, it is used to allocate the items to agents in a way that maintains the desired balance between fairness and efficiency. It is a well-known fact that there is a trade-off between these two performance measures. For instance, when a learner aims to maximize social welfare, the item is allocated to an agent with the highest utility. Such an allocation scheme achieves efficiency but sacrifices fairness. Intuitively, one way to obtain fairness is to apply a smaller weight factor to agents with higher cumulative utility and a larger weight factor to those with lower cumulative utility. Therefore, we must consider an appropriate goodness function G such that optimizing G corresponds to a) maximizing efficiency, i.e., maximizing the individual agent's cumulative utility, and b) reducing the utility disparity among the agents, which ensures fairness. Our regret, as defined in Eq. (1), is formulated for a general goodness function G. However, we first consider the goodness function to be the weighted Gini social-evaluation function (Weymark, 1981), which measures the weighted fairer distribution of utility while balancing the trade-off between fairness and efficiency. To define this function, we introduce a positive and non-increasing weight vector $w_{\\eta} = (w_1,..., w_{|N|})$, where $0 < w_n < 1$, and ${\\rm I}_n$ that is a sorting operator and arranges the cumulative utilities in increasing order. The weighted Gini social-evaluation function is then given as follows:\n\n$G(U_{t,n_t}) = \\sum_{n \\in N} W_\\eta \\Phi_n (U_{t,n_t}),$\n(2)\nWe now consider the following cases that coincide with the well-known social welfare functions.\n1. If $w_1 = 1$ and $w_n = 0$ for $n \\geq 2$, then it is evident from Eq. (2) that the weighted Gini social-evaluation function coincides with ESW, i.e., maximizing the minimum utility among agents.\n2. If $w_n = 1$ for all $n \\in N$, then the weighted Gini social-evaluation function aligns with USW, i.e., maximizing the goodness function leads towards efficiency.\n3. If $0 < w_n < 1$ for all $n \\in N$, then by appropriately choosing the weights, we can effectively control the trade-off between efficiency and fairness.\nRemark 2. In the above first two cases, we observe that larger variability in the weights leads to fairness, and smaller variability leads to efficiency. We carry forward this idea and aim to control the trade-off with a single parameter instead of $|N|$parameters. We set the weight $w_n = p^{n-1}$, for all $n \\in N$ where, $0 < p \\leq 1$ is a control parameter. We experimentally investigate this behavior in detail in Section 5.\nWe primarily focus on the weighted Gini social-evaluation function because it allows us to interpolate between fairness (ESW) and efficiency (USW) by adjusting a single parameter p as discussed in Remark 2. We also explore other goodness functions, such as the NSW and log-NSW; further details are provided in Appendix B.1."}, {"title": "4 Online fair division with contextual bandits", "content": "The online fair division setting that we consider can have a large number of items with only a few copies (even only one); hence, it is impossible to get a good utility estimate for all item-agent pairs. To overcome this challenge, we assume a correlation structure exists between the utility and item-agent features, which can be realized via parameterization of the expected utility such that the utility depends on the unknown function of item-agent features. Such assumption is common in the contextual bandit literature (Agrawal and Goyal, 2013; Chu et al., 2011; Krause and Ong, 2011; Li et al., 2010; Zhang et al., 2021; Zhou et al., 2020), where the reward (utility) is assumed be an unknown function of context-arm (item-agent) features. A utility function estimation is the main component of our problem for achieving good performance. To bring out the key ideas and results, we restrict our setting to linear utility functions and then extend our results to non-linear utility functions in Section 4.2."}, {"title": "4.1 Linear utility function", "content": "For brevity, we denote the set of all item-agent feature vectors in the round t by $M_t \\subset \\mathbb{R}^d$ ($d \\geq 1$) and the item-agent features for item $m_t$ and an agent n by $m_{t,n}$. After allocating the item $m_t$ to an agent $n_t$, the learner observes stochastic utility $Y_{n_t}^t = m_{t,n_t} \\theta^* + \\varepsilon_t$, where $\\theta^* \\in \\mathbb{R}^d$ is the unknown parameter and $\\varepsilon_t$ is R-sub-Gaussian. Let $M_t = \\lambda I_d + \\sum_{s=1}^{t-1} m_{s,n_s} m_{s,n_s}^\\top$, where $\\lambda > 0$ is the regularization parameter that ensures $M_t$ is a positive definite matrix and $I_d$ is the d x d identity matrix. The weighted $\\ell_2$-norm of vector $m_{t,n}$ with respect to matrix $M_t$ is denoted by $||m_{t,n}||_{M^{-1}_{t-1}}$.\nAt the beginning of the round t, the estimate of the unknown parameter $\\theta^*$ is given by $\\hat{\\theta}_t = M_t^{-1} \\sum_{s=1}^{t-1} m_{s,n_s} y_{n_s}$. After having this utility function estimator, the learner has to decide which agent to allocate the given item. Since the utility is only observed for the selected item-agent pair, the learner needs to deal with the exploration-exploitation trade-off (Auer et al., 2002; Lattimore and Szepesv\u00e1ri, 2020), i.e., choosing the best agent based on the current utility estimate (exploitation) or trying a new agent that may lead to a better utility estimator in the future (exploration). The upper confidence bound (UCB) (Li et al., 2010; Chu et al., 2011; Zhou et al., 2020) and Thompson sampling (TS) (Krause and Ong, 2011; Agrawal and Goyal, 2013; Zhang et al., 2021) are widely-used techniques for dealing with the exploration-exploitation trade-off.\nUCB-based algorithm. We first propose an UCB-based algorithm named OFD-UCB that works as follows: for the first N rounds, the learner allocates items to agents in a round-robin fashion to ensure each agent has positive utility, i.e., $U_n^t > 0, \\forall n \\in N$. In the round t, the environment reveals an item $m_t$ to the learner. Before selecting the agent for that item, the learner updates the utility function estimate $(\\hat{\\theta}_t)$ using available historical information before the round t (i.e., $\\{m_{s,n_s}, y_{n_s}\\}_{s=1}^{t-1}$). Then, the utility UCB value of allocating the item $m_t$ to an agent n, denoted by $u_{m_t,n}^{UCB}$, is computed as follows:\n\n$u_{m_t,n}^{UCB} = m_{t,n}^\\top \\hat{\\theta}_t + a_t ||m_{t,n}||_{M^{-1}_{t-1}},$\n(3)\nwhere $m_{t,n}^\\top \\hat{\\theta}_t$ is the estimated utility for allocating item $m_t$ to agent n and $a_t ||m_{t,n}||_{M^{-1}_{t-1}}$ is the confidence bonus in which $a_t = R \\sqrt{d \\log \\big( \\frac{1+(tL^2/\\lambda)}{\\delta} \\big)} + \\lambda^{1/2} S$ is a slowly increasing function in t and the value of $||m_{t,n}||_{M^{-1}_{t-1}}$ goes to zero as t increases."}, {"title": "4.2 Non-linear utility function", "content": "We now consider the setting in which the utility function can be non-linear. As shown in Section 4.1, the linear contextual bandit algorithm can be used as a sub-routine to get the optimistic utility estimate in our online fair allocation. We then use these estimates to compute the value of the goodness function for allocating the received item to the agent that gives the best-desired balance between fairness and efficiency. We generalize this observation for the online fair division problem with a non-linear utility function by using a suitable non-linear contextual bandit algorithm and introduce the notion of Online Fair Division (OFD) Compatible contextual bandit algorithm.\nDefinition 1 (OFD Compatible Contextual Bandit Algorithm). Let $\\Theta_t$ denote the observations of item-agent pairs at the beginning of round t and $m_{t,n} \\in M$. Then, any contextual bandit algorithm A is OFD Compatible if its estimated function $\\hat{f}^A_t$, with probability 1 \u2013 $\\delta$, satisfies:\n\n$|\\hat{f}^A_t(m_{t,n}) - f(m_{t,n})| \\leq h(m_{t,n}, \\Theta_t)$.\nMany contextual bandit algorithms like Lin-UCB (Chu et al., 2011), UCB-GLM (Li et al., 2017), IGP-UCB (Chowdhury and Gopalan, 2017), GP-TS (Chowdhury and Gopalan, 2017), Neural-CUB (Zhou et al., 2020), and Neural-TS (Zhang et al., 2021) are OFD compatible. The value of $h(m_{t,n}, \\Theta_t)$ gives the upper bound on the goodness function of the estimated utility with respect to the true utility function. The value of function depends on the problem and the choice of contextual bandit algorithm A and its associated hyperparameters,"}, {"title": "4.3 Regret bound for other goodness functions", "content": "The regret upper bounds are given in Theorem 1 and Theorem 2 hold for a specific goodness function (defined in Eq. (2)). We have also derived the regret upper bounds for goodness functions that satisfy the following properties.\nDefinition 2 (Locally monotonically non-decreasing and Lipschitz continuous properties). A goodness function G is\n(i) locally monotonically non-decreasing for any utility $u > 0$ if $G \\big(U_{t,n_1},..., U_{t,n_i},...,U_{t,n_N}\\big) \\leq G \\big(U_{t,n_1},..., U_{t,n_i} + u,..., U_{t,n_N}\\big) \\ \\forall i$ and,\n(ii) locally Lipschitz continuous if, for a constant $c_i > 0$ corresponding to the i-th element, $|G \\big(U_{t,n_1},..., U_{t,n_i},...,U_{t,n_N}\\big) - G \\big(U_{t,n_1},..., U_{t,n'_i},...,U_{t,n_N}\\big) | \\leq c_i|U_{t,n_i} - U_{t,n'_i} | \\ \\forall i$.\nNext, we provide regret upper bound for goodness functions that are locally monotonically non-decreasing and locally Lipschitz continuous, e.g., NSW and log-NSW.\nTheorem 3. Let A be an OFD compatible contextual bandit algorithm with $|\\hat{f}^A_t(m_{t,n}) - f(m_{t,n})| \\leq h(m_{t,n}, \\Theta_t)$ and the goodness function G be is locally monotonically non-decreasing and Lipschitz continuous with $C_{\\text{max}} = \\text{max}_{n \\in N} \\ c_n$. If assumptions used in A holds, then, with a probability of at least 1 \u2013 $\\delta$, the regret of corresponding online fair division algorithm OFD-A in T rounds is\n\n$R_T \\big(\\text{OFD-A}, G\\big) \\leq 2 C_{\\text{max}} \\sqrt{T \\sum_{t=1}^{T} [h(m_{t,n_t}, \\Theta_t)]^2}.$"}, {"title": "5 Experiments", "content": "Our goal is to corroborate our theoretical results and empirically demonstrate the performance of our proposed algorithms in different online fair allocation problems. We repeat all our experiments 20 times and show"}, {"title": "6 Conclusion", "content": "This paper considers a novel online fair division problem involving multiple agents in which a learner must irrevocably allocate an indivisible item to one of the agents while satisfying a given fairness and efficiency constraint. Since the number of items can be very large in our setting, with only a few copies of each item in many real-life applications, it is difficult to estimate the utility for all item-agent pairs. To address this challenge, we use contextual bandit to get optimistic utility estimates for each item-agent pair. We then proposed algorithms for online fair division that use these optimistic utility estimates for item allocation to agents and showed that they enjoy the sub-linear regret guarantees. Our experimental results further validate the performance of the proposed algorithms. For future research, one can explore other fairness constraints, such as envy-freeness and proportionality. Another promising direction is investigating fairness across multiple types of items in the online fair division setting with unknown utility functions."}, {"title": "A Regret Analysis.", "content": "A.1 Proof of Theorem 1\nWe first state the following result that we will use in the prove Theorem 1.\nLemma 1 (Theorem 2 of Abbasi-Yadkori et al. (2011)). Let $\\delta \\in (0,1), \\lambda > 0, \\hat{\\theta}_t = M_t^{-1} \\sum_{s=1}^{t-1} m_{s,n_s} y_{n_s}, ||\\theta^*||_2 \\leq S$, and $||m_{t,n}||_2 < L \\ \\forall t \\geq 1, n \\in N$. Then, with probability 1 \u2013 \u0431,\n\n$||\\hat{\\theta}_t - \\theta^*||_{M_t} \\leq \\Big(R \\sqrt{d \\log \\Big( \\frac{1+(tL^2/\\lambda)}{\\delta} \\Big)} + \\lambda^{1/2} S\\Big) = a_t$.\nTheorem 1. Let \u03b4 \u2208 (0, 1), \u03bb > 0, noise in utility be the R-sub-Gaussian, and the goodness function be same as defined in Eq. (2) with wmax = maxn\u2208N Wn. Then, with a probability of at least 1 \u2013 8, the regret in T > 0 rounds is\n\n$R_T (\\text{OFD-UCB}) \\leq 2 a_\\tau w_{\\text{max}} \\sqrt{2dT \\log(\\lambda + TL/d)},$\nwhere $a_t = R \\sqrt{d \\log \\big(\\frac{1+(TL^2/\\lambda)}{\\delta} \\big)} + \\lambda^{1/2} S$, $||\\theta^*||_2 \\leq S$, and $||m_{t,n}||_2 \\leq L \\ \\forall t \\geq 1, n \\in N$.\nProof. As $h(m_{t,n_t}, \\Theta_t) = \\Big(R \\sqrt{d \\log \\big( \\frac{1+(tL^2/\\lambda)}{\\delta} \\big)} + \\lambda^{1/2} S\\Big) ||m_{t,n_t}||_{M^{-1}_{t-1}}$ when linear contextual bandit algorithm is used. After using Theorem 2, we have\n\n$R_T(\\text{OFD-UCB}) \\leq 2w_{\\text{max}} \\sqrt{T \\sum_{t=1}^{T} [h(m_{t,n_t}, \\Theta_t)]^2}$\n\n$= 2w_{\\text{max}} \\sqrt{T \\sum_{t=1}^{T} \\Big[R \\sqrt{d \\log \\big( \\frac{1+(tL^2/\\lambda)}{\\delta} \\big)} + \\lambda^{1/2} S\\Big]^2 ||m_{t,n_t}||_{M^{-1}_{t-1}}^2}$\n\n$= 2w_{\\text{max}} \\sqrt{T \\sum_{t=1}^{T} \\Big[R \\sqrt{d \\log \\big( \\frac{1+(tL^2/\\lambda)}{\\delta} \\big)} \\Big]^2 ||m_{t,n_t}||_{M^{-1}_{t-1}}^2 + \\sum_{t=1}^{T} [\\lambda^{1/2} S]^2 ||m_{t,n_t}||_{M^{-1}_{t-1}}^2}$\n\n$= 2w_{\\text{max}} \\sqrt{T R^2 d \\log \\big( \\frac{1+(tL^2/\\lambda)}{\\delta} \\big) \\sum_{t=1}^{T} ||m_{t,n_t}||_{M^{-1}_{t-1}}^2 + T [\\lambda S^2] \\sum_{t=1}^{T} ||m_{t,n_t}||_{M^{-1}_{t-1}}^2}$\n\n$= 2w_{\\text{max}} \\sqrt{T R^2 d \\log \\big( \\frac{1+(TL^2/\\lambda)}{\\delta} \\big) \\sum_{t=1}^{T} ||m_{t,n_t}||_{M^{-1}_{t-1}}^2 + \\lambda S^2 \\sum_{t=1}^{T} ||m_{t,n_t}||_{M^{-1}_{t-1}}^2}$\n\n$\\leq 2w_{\\text{max}} \\sqrt{T \\Big[R^2 d \\log \\big( \\frac{1+(TL^2/\\lambda)}{\\delta} \\big) + \\lambda S^2 \\Big] \\sum_{t=1}^{T} ||m_{t,n_t}||_{M^{-1}_{t-1}}^2}$\n\n$= 2w_{\\text{max}} \\sqrt{T \\sum_{t=1}^{T} ||m_{t,n_t}||_{M^{-1}_{t-1}}^2}$$\n\n$\\leq 2w_{\\text{max}} \\sqrt{2T \\log \\frac{\\text{det}(M_T)}{\\text{det}(\\lambda I_d)}}$\n\n$\\leq 2a_{\\tau}w_{\\text{max}} \\sqrt{2dT \\log(\\lambda + TL/d)}$\n\n$\\Longrightarrow R_T (\\text{OFD-UCB}) \\leq 2 a_{\\tau}w_{\\text{max}} \\sqrt{2dT \\log(\\lambda + TL/d)}.$\nThe last two inequalities follow from Lemma 11 and Lemma 10 of Abbasi-Yadkori et al. (2011), respectively. With this, the proof of Theorem 1 is complete."}, {"title": "A.2 Proof of Theorem 2", "content": "We first state the following result that we will use to prove our regret upper bounds for Theorem 2.\nLemma 2 (Lemma 1 of Weymark (1981)). Let $w = (w_1,w_2, ..., w_n)$ and $u = (u_1,u_2,..., u_n)$ are opposite ordered. If $U = \\{u^1,u^2, . . ., u^p \\}$ is the set of all permutations of u, then\n\n$\\sum_{n=1}^{N} w_n u_n \\leq \\sum_{n=1}^{N} w_n u_n^j$, for $j = 1, 2, ..., p$.\nLemma 3 (Lemma 1 of Sim et al. (2021)). Consider a non-decreasing order of weights $W_1 > ... > W_N$ and let $G(U) = \\sum_{n \\in N} W_n \\Phi_n (U)$, where U is an N-dimensional utility vector and $\\Phi_n$ returns the n-th smallest element of U. For any two utility vectors $U^a$ and $U^b$, if there exists i such that $U^a_i > U^b_i$ and$\\forall n \\in N \\backslash \\{i\\}, U^a_n = U^b_n$, then $G(U^a) > G(U^b)$.\nThe regret analysis of our proposed algorithms depends on bounding the instantaneous regret for each action. The following result gives an upper bound on the instantaneous regret when using an contextual algorithm A.\nLemma 4. Let G be the goodness function defined in Eq. (2) and A be an OFD compatible contextual bandit algorithm with $|\\hat{f}(m_{t,n}) - f(m_{t,n})| \\leq h(m_{t,n}, \\Theta_t)$. Then, the instantaneous regret incurred by UCB-A for selecting agent nt for item mt is\n\n$r_t = G(U_{t,n_t^*}) - G(U_{t,n_t}) \\leq 2w_{n_t}h(m_{t,n_t}, \\Theta_t)$.\nProof. Recall the definition of the goodness function, i.e., $G(U_{t"}]}