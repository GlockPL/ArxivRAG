{"title": "LLM-PCGC: Large Language Model-based Point Cloud Geometry Compression", "authors": ["Yuqi Ye", "Wei Gao"], "abstract": "The key to effective point cloud compression is to obtain a robust context model consistent with complex 3D data structures. Recently, the advancement of large language models (LLMs) has highlighted their capabilities not only as powerful generators for in-context learning and generation but also as effective compressors. These dual attributes of LLMs make them particularly well-suited to meet the demands of data compression. Therefore, this paper explores the potential of using LLM for compression tasks, focusing on lossless point cloud geometry compression (PCGC) experiments. However, applying LLM directly to PCGC tasks presents some significant challenges, i.e., LLM does not understand the structure of the point cloud well, and it is a difficult task to fill the gap between text and point cloud through text description, especially for large complicated and small shapeless point clouds. To address these problems, we introduce a novel architecture, namely the Large Language Model-based Point Cloud Geometry Compression (LLM-PCGC) method, using LLM to compress point cloud geometry information without any text description or aligning operation. By utilizing different adaptation techniques for cross-modality representation alignment and semantic consistency, including clustering, K-tree, token mapping invariance, and Low Rank Adaptation (LoRA), the proposed method can translate LLM to a compressor/generator for point cloud. To the best of our knowledge, this is the first structure to employ LLM as a compressor for point cloud data. Experiments demonstrate that the LLM-PCGC outperforms the other existing methods significantly, by achieving -40.213% bit rate reduction compared to the reference software of MPEG Geometry-based Point Cloud Compression (G-PCC) standard, and by achieving -2.267% bit rate reduction compared to the state-of-the-art learning-based method.", "sections": [{"title": "Introduction", "content": "Point cloud is a critical and valuable data structure for autonomous driving and virtual reality. Recently, with the development of deep neural networks, more and more learning-based architectures for the lossless PCGC task (Que, Lu, and Xu 2021; Fu et al. 2022; Wang et al. 2022) are proposed, which have demonstrated remarkable performance in the task of lossless PCGC. For these methods, they can be divided into two main categories, i.e., voxel-based and tree-based. Whether voxel-based or tree-based methods, the key to compression performance is the establishment of a strong and robust context model. However, the context capabilities of previous methods remain significantly restricted due to the limitations in data volume and model size, as discussed in the scaling law for large language models (LLMs) (Kaplan et al. 2020). This inspires us to directly replace the original context model with LLM, which has large-scale context and generation capabilities.\nThe emerging viewpoint illustrates that the essence of LLMs fundamentally lies in their ability to compress information (Del\u00e9tang et al. 2024; Li et al. 2024; Valmeekam et al. 2023; Yu et al. 2024). However, prior research works only generally discuss the compact feature representation in LLM from the perspective of compression, but ignore the potential of data compression with LLM. Although in (Del\u00e9tang et al. 2024), the discussion centers on the lossless compression capabilities of a text-only trained LLM across different modalities of 1D and 2D data, including text, image, and speech. Two limitations emerge from the analysis: 1) This work neglects the compression problem for 3D point clouds. Different from the simple 1D and 2D data, 3D structural data requires a more elaborated and powerful context model. Therefore, as a more complex data type, point cloud owns unique 3D structural characteristics, leading to new challenges for compression task. 2) The exploration of the LLM's data compression capability is restricted to in-context learning, without any additional parameter training. This shows the inherent data compression potential of LLM model, while the performance improvement is still unknown after tailore training for the compression task. In this paper, we propose a completely new architecture, namely the Large Language Model-based Point Cloud Geometry Compression (LLM-PCGC) method, which can better adapt to the lossless PCGC compression task.\nConverting text-based LLM to LLM-PCGC is a cross-modal problem. Since LLM is a model based on text, the current multi-modal large language model (MM-LLM) in order to process multi-modal data, the unified approach is to map other modal tokens to the text space and then generate the modal data through text description (Yin et al. 2023; Hong et al. 2023; Xu et al. 2023). On the one hand, for coding tasks, we do not really need text data, and there is no text data to pair with multimodality. On the other hand, we utilize LLMs for their potent generative and contextual understanding capabilities, yet for encoding tasks, the text-based features are extraneous. Hence, we seek to discard the text-specific aspects while maintaining the essential generative and contextual functions. Inspired by (Mirchandani et al. 2023), we are the first to fine-tune the pre-trained LLM to achieve cross-modality via token mapping invariance.\nThrough the above methods, we propose large language model-based point cloud geometry compression (LLM-PCGC). As shown in Fig. 1, a comparison is made with existing end-to-end deep learning training methods. Our approach, LLM-PCGC, fine-tunes a pre-trained text generator LLM with point cloud data, achieving cross-modality and serving as a point cloud compressor. In the encoding phase, the procedure begins with the clustering of the input 3D point clouds. Subsequently, each cluster is processed in parallel through a series of steps. First, the coordinates are normalized by subtracting an offset, and a K-tree structure is organized to systematize the point cloud data. Then, the hierarchical tree structure is flattened and divided into segments. Subsequently, a codebook is utilized to translate the point cloud tokens into text tokens to construct an analogous linguistic sentence. Finally, a trained LoRA architecture is employed with a frozen LLM to predict the probability distribution of the next token, which is integrated with an arithmetic encoder to complete the encoding process. The decoding phase mirrors the aforementioned steps in reverse order, thereby reconstructing the original point cloud geometry from the encoded data.\nOur contributions are summarized as follows:\n\u2022 We propose a novel architecture, namely LLM-PCGC, which is the first to apply LLM as a compressor to point cloud compression within the \u201cGenerator is compressor\" framework. To the best of our knowledge, LLM-PCGC is also the first to transform LLM to a large model that can understand point cloud structure without any text information assistance.\n\u2022 We propose utilize different adaptation techniques for cross-modality representation alignment and semantic consistency, including clustering, K-tree, token mapping invariance, and LoRA, the proposed method can translate LLM to a compressor/generator for point cloud. The approach of token mapping invariance can be transferred to other modalities, offering a new paradigm for multi-modal and cross-modal applications of LLMs.\n\u2022 Experiments demonstrate that the LLM-PCGC outperforms the other existing methods significantly, by achieving -40.213% bit rate reduction compared to the G-PCC, and by achieving -2.267% bit rate reduction compared to the state-of-the-art learning-based method. As the first LLM-based point compression method, the proposed LLM-PCGC method achieves superior performances."}, {"title": "Framework Overview", "content": "Encoding Pipline\nThe LLM-PCGC encoding pipeline is depicted in Fig. 2. The encoding phase initiates with the clustering of input 3D point clouds, where each cluster undergoes parallel processing. This process involves several key steps, including normalization of coordinates through offset subtraction, organization of data using a K-tree structure, flattening and chunking of the hierarchical tree structure, and translation of the point cloud's patch tokens into text tokens with a codebook. Special tokens, such as <bos_id> to denote the beginning token id, and <eos_id> to denote the end token id, are incorporated to construct analogous linguistic sentences. The encoding process culminates with the employment of a trained LORA architecture in conjunction with a frozen LLM to predict the next token's probability distribution, which is then encoded using an arithmetic encoder.\nDecoding Pipline\nThe LLM-PCGC decoding pipeline is depicted in Fig. 3. In the decoding phase, the parallel-received binary files are segmented to identify the corresponding offset, <chunk_index>, and the main bitstream for each bitstream. These identifiers are converted from binary to decimal values, facilitating the processing of the main bitstream through a trainable LoRA and a frozen LLM model to obtain a probability distribution for the next token, which is decoded using an arithmetic coder. The codebook then translates text tokens back into point cloud patch tokens, which are aligned and merged based on their common offset and chunk indexes. An algorithm is applied to reconstruct trees and coordinates. This algorithm ingeniously restores coordinates by counting the number of one, due to the lack of ancestral information. Finally, the cluster point clouds are adjusted by their respective offsets, reconstructing the full point cloud to its original form."}, {"title": "Experiments", "content": "Training and Testing Setting\nData Processing Given the current methods which autoregressive methods like OctAttention (Fu et al. 2022), VoxelDNN (Nguyen et al. 2021a), MSVoxelDNN (Nguyen et al. 2021b), and NNOC (Kaya and Tabus 2021) utilize Microsoft Voxelized Upper Bodies (MVUB) (Loop et al. 2016) and 8i Voxelized Full Bodies (MPEG 8i) (d'Eon et al. 2017) datasets for training, and other approaches like SparsePCGC (Wang et al. 2022) are trained on ShapeNet (Chang et al. 2015), we aim to ensure fair comparison by training two sets of LLM-PCGC parameters on similar datasets.\nIn our experimental comparison with SparsePCGC, we group ModelNet40 (Wu et al. 2015) point cloud data into 12 clusters, then organize 3D point cloud clusters in a K-Tree structure with K=12 for training. For testing, we follow the common test condition (CTC) (Schwarz et al. 2018), which recommend to evaluate two public datasets, i.e., MPEG 8i and Owlii (Xu, Lu, and Wen 2017).\nIn relation to autoregressive methods such as OctAttention, VoxelDNN, MSVoxelDNN, and NNOC, we align with the norm by employing widely-used sequences for training. Specifically, we use the point cloud sequences of Andrew 10, David10, and Sarah10 from the MVUB, as well as the point cloud sequences of Longdress10 and Soldier10 from MPEG 8i for training. We do a similar clustering process with the cluster number of 240 and K-Tree structure with K=12 for the chosen data. For testing, we select two point clouds from MPEG 8i, Thaidancer and Boxer, which both are downsampled from 12-bit to 10-bit resolution.\nBase Model and LoRA Setting Based on LLaMA (Touvron et al. 2023), an open-source LLM that competes in performance with GPT-3 (Brown et al. 2020), and taking into consideration the hardware resources available, we choose the smallest model, LLaMA2-7B, as our foundational model for this experiment. As delineated in Table 2, we provide"}, {"title": "Conclusion", "content": "In this paper, we propose the LLM-PCGC method, which is the first to employ LLM as compressor for the point cloud compression task within the \"Generator is compressor\" framework. We utilize different adaptation techniques, i.e., clustering, K-tree, token mapping invariance, and LoRA, to achieve efficient cross-modality representation alignment and semantic consistency. Without any text data, a text generator can be translated to a point cloud compressor. Experimental results show that the proposed LLM-PCGC method achieves superior compression performance over G-PCC and the state-of-the-art learning-based method, demonstrating the potential of LLMs in data compression. Although as the first attempt to develop a LLM-based point compression method, the proposed LLM-PCGC method achieves superior performances, future research efforts can be made for optimizing the issues on the excessive memory consumption and the long inference time of LLMs."}]}