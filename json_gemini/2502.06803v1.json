{"title": "EMOTION RECOGNITION AND GENERATION: A COMPREHENSIVE REVIEW OF FACE, SPEECH, AND TEXT MODALITIES", "authors": ["Rebecca Mobbs", "Dimitrios Makris", "Vasileios Argyriou"], "abstract": "Emotion recognition and generation have emerged as crucial topics in Artificial Intelligence research, playing a significant role in enhancing human-computer interaction within healthcare, customer service, and other fields. Although several reviews have been conducted on emotion recognition and generation as separate entities, many of these works are either fragmented or limited to specific methodologies, lacking a comprehensive overview of recent developments and trends across different modalities. In this survey, we provide a holistic review aimed at researchers beginning their exploration in emotion recognition and generation. We introduce the fundamental principles underlying emotion recognition and generation across facial, vocal, and textual modalities. This work categorises recent state-of-the-art research into distinct technical approaches and explains the theoretical foundations and motivations behind these methodologies, offering a clearer understanding of their application. Moreover, we discuss evaluation metrics, comparative analyses, and current limitations, shedding light on the challenges faced by researchers in the field. Finally, we propose future research directions to address these challenges and encourage further exploration into developing robust, effective, and ethically responsible emotion recognition and generation systems.", "sections": [{"title": "1 Introduction", "content": "Emotions are central to human communication, shaping interactions through body language, facial expressions, vocal intonations, and textual cues [1]. Psychological research suggests recognition of emotions is innate in humans, with newborns able to replicate facial expressions and vocal tones as early as two days old [2]. Understanding emotions aids in teamwork and cooperation, a concept recognised by Darwin's theories on survival mechanisms [3]. This significance has led to the development of emotion models like Ekman and Friesen's Facial Action Coding System (FACS), which categorises emotions such as anger, disgust, fear, happiness, sadness, surprise, and contempt [4, 5], forming the basis for many contemporary emotion recognition systems."}, {"title": "1.1 Applications", "content": "Emotion recognition systems are used across various fields. In customer service, they are utilised to discern customers' emotions and evaluate the effectiveness of sales assistants' communication strategies through assessment of transcripts [7]. Similarly, at self-service checkouts, FER is used to gauge customer satisfaction based on their facial cues [6]. In healthcare, these systems assist in tracking the progression of Alzheimer's disease [8], facilitating therapy sessions [9], and supporting individuals with Asperger's Syndrome in recognising emotions [24]. They are also used in robotics to interpret human emotions during interactions with machines [10], and in educational settings to evaluate students' engagement and learning [11]. Other applications include lie detection [13] and monitoring driver fatigue levels [12].\nEmotion recognition systems can also serve as foundational tools for training models capable of generating realistic emotional content [22]. These models can be used to create visual virtual assistants and avatars for virtual calls [25]. As reliance on chatbots for social interactions and advice increases [26], there is a growing opportunity for the development of talking head chatbots. Such chatbots would use speech or text input-whether from a customer service representative, therapist, or a text generation model to produce animated faces with lifelike emotions in real-time. These animated avatars could integrate with AI models such as Character.AI [27], ChatGPT [28], Llama [29], or Gemini [30] to function as therapeutic or customer service bots. This technology has the potential to provide users with a highly immersive and personalised experience, enhancing or even replacing current customer service chatbots."}, {"title": "2 Preprocessing for ER and EG Systems", "content": "Preprocessing is an important stage in deep learning pipelines, particularly when handling data obtained from uncontrolled or 'in-the-wild' environments, such as facial and speech data extracted from movies or textual data from social media. Such data often exhibit significant variability compared to controlled laboratory settings, with variations in background, lighting, noise, and other artefacts. To address these challenges, preprocessing typically involves standard steps like data normalisation, noise reduction, and feature extraction to ensure data consistency and optimise model performance. Below, we explore the specific preprocessing techniques used for processing face, speech, and textual data."}, {"title": "2.1 Preprocessing for Face Systems", "content": "Preprocessing for facial emotion recognition systems aims to enhance image quality, standardise data, and extract critical features for accurate model predictions. The initial step involves resizing and cropping facial images to create uniform input dimensions, ensuring consistency across the dataset. By eliminating background elements and focusing on the region of interest, these techniques enable models to concentrate on key facial features. Normalisation, through scaling pixel values to a common range (e.g., 0 to 1 or -1 to 1), ensures uniform pixel intensity across different samples, thereby enhancing the model's capacity to learn relevant patterns. Common methods such as mean subtraction [31] and standard deviation normalisation [32] are frequently used. Noise reduction techniques, like Gaussian blurring [33] and median filtering [34], are used to minimise the impact of noise introduced during image acquisition or transmission.\nTechniques such as histogram equalisation [35] improve contrast by redistributing pixel intensities, enhancing visibility in images captured under challenging conditions. Data augmentation, involving transformations like rotation, scaling, and flipping, increases training data diversity and mitigates overfitting [36]. Furthermore, advanced algorithms such as Haar cascades [37] and deep learning-based facial landmark detection methods [38] are applied to extract and align facial regions, standardising poses and reducing variability. Feature extraction models, such as VGG [39], ResNet [40], and MobileNet [41], are widely used for extracting high-level features. Colour space transformations and quality control measures help streamline data preparation, ensuring only high-quality data is fed into the models [33, 42]."}, {"title": "2.2 Preprocessing for Speech Systems", "content": "The primary goals of preprocessing in speech systems are noise reduction, normalisation, segmentation, and feature extraction from raw audio signals. Noise reduction methods like spectral subtraction [43], Wiener filtering [44], and adaptive filtering [45] are used to eliminate background noise which can degrade speech signal quality. Normalisation adjusts amplitude and dynamic range to maintain consistency across recordings [46]. Speech segmentation techniques, such as endpoint detection [47] and silence removal [48], isolate speech segments within continuous audio streams, enabling more targeted analysis.\nFeature extraction captures the salient characteristics of speech, using Mel-Frequency Cepstral Coefficients (MFCCs) [49], which represent spectral properties in a compact form, and Linear Predictive Coding (LPC) [50], which models the spectral envelope. Other methods like pitch estimation [51] and anti-aliasing filtering [52] help preserve signal integrity. Techniques such as de-reverberation [53] and pre-emphasis [54] further refine the signal quality. For segmentation, windowing techniques like frame blocking divide speech signals into shorter frames, facilitating computational efficiency [55]. Mean and variance normalisation standardises feature scales, improving model robustness to variability in input data [56]."}, {"title": "2.3 Preprocessing for Text Systems", "content": "Text preprocessing begins with tokenisation, which breaks down text into smaller units, such as words or characters. This is followed by lowercasing, which standardises the text by treating uppercase and lowercase versions of words identically, thereby reducing vocabulary size and simplifying the learning process [57]. Punctuation and special character removal further eliminate noise which could interfere with learning. Stopwords such as \u201cand\u201d or \u201cthe\u201d-are often removed, as they carry little semantic value [58]. Stemming and lemmatisation techniques group words with similar meanings, helping models understand linguistic variations [59, 60].\nNumerical values are encoded or replaced with placeholders to maintain the semantic integrity of the text [61]. Out-of-vocabulary words are managed through tokenisation or character-level representations [62], while padding and truncation ensure uniform sequence lengths, which is crucial for text classification [63]. Pretrained word embeddings, such as Word2Vec [64], can be used to initialise the embedding layers of deep learning models or be fine-tuned during training. Encoding methods like one-hot or integer encoding convert textual data into numerical representations, while pretrained tokenisers accelerate this conversion [65]. Text augmentation techniques, such as synonym replacement and paraphrasing, diversify training data and reduce overfitting, improving generalisation [66]."}, {"title": "3 Datasets for Face, Text, and Speech ER and EG Systems", "content": "High-quality, diverse datasets are essential for training emotion recognition and generation models. These datasets provide labelled examples from facial expressions, speech, and text, enabling models to learn emotional cues in varied contexts. Some datasets are captured in controlled environments, while others are collected in the wild, offering more complex real-world variations. This section highlights the most widely used datasets across facial, speech, and text systems, focusing on those with comprehensive emotional labelling and diversity (see 1)."}, {"title": "4 Emotion Recognition for Faces, Speech, and Text", "content": "This section will discuss deep learning methodologies for emotion recognition for faces, speech, and text. We will discuss the strengths and limitations of current literature. Most emotion recognition systems use the 8 primary emotions anger, disgust, fear, happiness, sadness, surprise, contempt, and neutral [5]. Unlike traditional methodologies where feature extraction and classification are treated as distinct stages [67], deep learning frameworks for emotion detection enable end-to-end pipelines. A key component in classification is the use of a loss layer, which regulates the back-propagation error, for estimating prediction probabilities for each sample. For example, in CNNs the softmax loss function is typically used to minimise the difference between the predicted class probabilities and the ground-truth. Some models simultaneously predict both discrete emotions and continuous affect dimensions, such as arousal, valence, and strength of emotion [23] (see Fig.1). This aims to minimise data mislabelling and improve overall prediction accuracy."}, {"title": "4.1 Facial Expression Recognition", "content": "FER systems begin with facial feature detection, whereby the face is identified and isolated. Methods such as the Viola-Jones algorithm, Histogram of Oriented Gradients (HOG), and Convolutional Neural Networks (CNNs) are used. Facial landmark detection identifies key points on the face, then feature extraction focuses on geometric features and appearance features. Traditional machine learning algorithms and deep learning models, especially CNNs, classify these features into emotional categories. CNNs are effective as they automatically learn and extract hierarchical features from raw pixel data [68]. The following section will discuss state-of-the-art research in FER with an emphasis on novelty, recurring themes, strengths, and limitations of current research.\nFER systems are classified into two categories: static image and dynamic sequence. While static methods encode spatial information from individual images, dynamic techniques use temporal relationships across frames within sequences [17]. Historically, FER heavily relied on handcrafted features or shallow learning techniques such as Decision Trees [69], K-Nearest Neighbors (K-NN) [70], and Support Vector Machines (SVM) [71]. However, with the rise in emotion detection competitions such as FER2013 [72], EMOCA [73], and ABAW 2023 [74] a shift towards the use of deep learning techniques occurred. This has coincided with improvements in processing capabilities and network architectures, enabling the widespread adoption of deep learning methodologies.\nModels using pretrained Contrastive Language-Image Pretrained (CLIP) [75] achieve remarkable results in FER. Using the joint embedding space of text and images, CLIP models can understand contextual information across modalities. By training on large datasets containing images paired with descriptions of emotions, CLIP learns to associate visual patterns with their emotional description. One such model which uses CLIP is DFER-CLIP [76]. This method combines both modalities, using a temporal model atop the CLIP image encoder. Temporal facial features are captured while using descriptions of facial behaviour instead of class names for the text encoder. It uses learnable prompts as context for descriptors of each facial expression class, enabling automatic learning of relevant context information during training. The model's pipeline involves extracting features from facial images or frames, and predicting facial expression descriptions. Furthermore, DFER-CLIP automates the generation of textual descriptors by prompting a language model with queries about useful visual features for each expression, culminating in comprehensive descriptions for classification.\nAttention is a key topic in FER with approaches such as self-attention, patch attention, and cross attention being utilised. EmoFan (see Fig.1) uses attention mechanisms on facial landmarks and facial heat maps and achieves SOTA results. [77] uses patch attention and a pretrained ResNet-18 to extract the facial feature maps to overcome issues caused by occlusion for improved performance. [78] uses a similar approach by making use of window-based cross-attention mechanisms in conjunction with landmark detection, and multi-scale feature extraction. In comparison, [79] uses self-attention and a transformer to identify facial expressions in images or videos where the face is difficult to see. [73] addresses a shortfall in labelled datasets by incorporating an emotion recognition model into the 3D face reconstruction framework DECA[80] This enables improved emotion reconstruction and classification, along with the use of their Emotion Consistency Loss."}, {"title": "4.2 Speech Emotion Recognition", "content": "Recognising emotions in speech involves a multidisciplinary approach, integrating linguistics, psychology, and computer science [82]. Acoustic feature analysis, focusing on prosody and voice quality, plays a key role. Prosodic features, such as pitch, intensity, and speech rate, effectively indicate emotions. For example, happiness or excitement use higher pitch and greater variability, while sad voices use lower pitch and slower speech. Voice quality, including elements such as breathiness and tension, can also signal different emotions. Word choices and sentence structures, provide additional clues. Short, abrupt sentences can indicate anger, while longer, complex sentences might suggest calmness. Contextual analysis, considering the situational context and dialog history, is vital, as the same utterance can convey different emotions depending on the context [83].\nTransformer based model ESCM [84], achieved state-of-the-art results in SER by adjusting emotions and semantics based on context. They achieve this by using Graph Convolutional Network (GCN) to find correlations between words in spoken coversations. In contrast, [81] introduces a novel approach to speech emotion recognition by integrating attention mechanisms into Long Short Term Memory (LSTM) models. By prioritising relevant information across both time and feature dimensions, the attention-based LSTM architecture improves performance in SER. The use of frame-level features provide a comprehensive representation of emotional content, contributing to the model's accuracy. [79] use Large Language Models (LLMs) and weakly-supervised learning to label the emotions in speech data, which contributes to the effectiveness of their SER model.\nFurther innovations in time-frequency analysis have also improved SER. For instance, the fast Continuous Wavelet Transform (fCWT) enables high-resolution analysis of non-stationary speech signals, balancing temporal and spectral features. When combined with Deep Convolutional Neural Networks (DCNNs), this approach enhances the extraction of paralinguistic information, offering robust real-time performance while overcoming limitations of traditional methods like the Short-Term Fourier Transform (STFT) [85]."}, {"title": "4.3 Text Sentiment Recognition", "content": "TER focuses on the identification and classification of emotions expressed in textual data using Natural Language Processing models (NLP). NLP models enable machines to understand, interpret, and generate text [87]. Bidirectional Encoder Representations from Transformers (BERT) [88] are used in most modern NLP models [89]. These models are useful for TER due to their ability to capture contextual data and decipher emotions in text, enabling SOTA performance. Campagnano et al. [90] combines BERT encodings with bidirectional LSTM layers to achieve robust emotion classification, particularly in semantic role labelling tasks. [91] use a modified BERT-based architecture to classify emotions for individual sentences and entire texts. [92] use a BERT model trained on data from 100 languages as well as X (formerly Twitter), to detect emotions on social media platforms. In contrast, [86] use LSTM and a CNN based model for TER. The use of CNN-LSTM channels extracts both local and global contextual information from input text, working for diverse text inputs. [91, 92] address multilingual emotion recognition, developing models and datasets capable of working across languages. As seen in this analysis there is a distinct lack of recent research into TER, highlighting the need for updated studies to address current challenges and advancements in the field."}, {"title": "5 Emotion Generation for Faces, Speech, and Text", "content": "This section will discuss generated content for faces - which will focus on animated face generation, speech - taking the nuances of audio from one speaker and converting to another voice, and text - the generation of realistic text. Emotion recognition models are sometimes used for training [93], and evaluating [94] these models to generate accurate emotional content. Emotion recognition datasets are also utilised for emotion generation models [95]. A recent challenge with creating emotionally realistic generated content comes from negativity in public's perception due to media hype surrounding stealing of identities [96], deepfakes [97], and the rapid rate in which models are being released [75]. This consideration has the capacity to hinder research in these fields due to restrictions on the availability of models for researchers [98], due to the fear they will fall into the wrong hands. This section will discuss SOTA methods for these modalities, and will discuss the strengths and limitations of current research."}, {"title": "5.1 Facial Expression Generation", "content": "FEG and face manipulation techniques have been around for years, present on mobile phone apps such as Instagram [100], SnapChat [101], and AI photo editors such as FaceApp [102] and others. The release of visually appealing talking-head models such as VASA [103] and EMO-Live [99], have further bolstered public interest in this research area. Talking-head animation refers to models which take as input an image of a person, and generates new frames using audio [99], video [104], or text [22] to guide the facial expressions. The manipulation of facial expressions through prompts is another new area of research [105, 106, 107]. FEG models often focus on prioritising the manipulation of the mouth, eyes, or poses [108, 109, 110, 111], while others focus on overall realism [99, 103, 112]. With mouth movements now achieving realism pretrained SOTA models such as Wav2Lip [109] are incorporated into larger models to guide the lip movements, while the model focuses on poses and facial expressions [112]. [99, 104] use 3D face modelling techniques and reconstruction methods to capture detailed facial geometry. This allows for accurate expression synthesis and emotion manipulation. Similarly, other methods use 3D registration and mesh-based representations to achieve realistic Face Expression generation [113, 114].\nGenerative Adversarial Networks (GAN) are used for generating animations [105, 115] due to their ability to create realistic synthetic content. GANs are trained by generating content through a generator network, then using a discriminator network to predict if the generated content is real or not. For Face Expression generation, GANs are combined with other models to generate realistic facial expressions in talking-head animation generation [116, 117, 118, 106, 113]. For example, [106] employ LSTM networks and a GAN for speech-driven animation. [116] use a GAN to guide the generation process of emotional animations, and preserve the identity of the target face. [117] focuses on facial expression manipulation using a modified U-Net structure with GANs and achieves precise emotion manipulation. [119] use GANs and attention mechanisms as the backbone of their text to talking-head generation framework. Meanwhile, [120] and [105] utilise GANs in their methodologies for efficient emotional manipulation. Additionally, [113] use a GAN for personalised facial expression manipulation. [99] use Diffusion models for generative power and extensive control over the generation of animations. Diffusion models iteratively refine a noisy image into a high-quality sample. This refinement allows for the generation of highly realistic facial expressions, while maintaining control over intensity, duration, and subtle movements. By conditioning the diffusion process on desired expression labels or latent codes, these models produce specific facial expressions with remarkable realism. As diffusion models capture uncertainty during generation, this enables the synthesis of realistic variations.\nAttention's ability to focus on important facial regions and generate realistic facial expressions has enabled them to become a key part of face generation architectures. In [107], attention mechanisms ensure the generated facial animations accurately capture the speaker's gestures and facial expressions. [99] integrates attention mechanisms into the pipeline to improve the quality and synchronisation of talking portrait videos, attention mechanisms are utilised to refine motion dynamics and speed adjustments. This method achieves realistic talking portrait videos which closely align with the input audio content. [119] use attention gate and self-attention mechanisms in their text-based talking-head generation framework. By incorporating these mechanisms their model manipulates Action Unit-related embeddings, leading for accurate and expressive facial animations synchronised with input text. CLIP with its multimodal capabilities is useful for facial animation generation tasks. By inputting textual prompts to describing desired emotional states along with images associated with those emotions, CLIP can generate images reflecting the specified emotions. This allows the model to learn associations between text and images which improves its ability to generate content with realistic emotions. TalkCLIP by[121] generates realistic talking head videos of a target speaker with specific speaking styles. Their model utilises CLIP embeddings and an adaptor network to map text descriptions, to speaking style codes.\nFurthermore, researchers have explored the ability to control the generation of emotions on the faces through various inputs such as speech, video, facial reenactment, and text. Speech data is the most common input medium whereby an animated face video is generated using the emotions in the speech [106, 105, 99]. Video is used as an input in architectures where the face is changed to a target face using facial reenactment methods [104], or the emotions are manipulated via facial reenactment from a static image [116]. However, the synchronisation of speech and facial animations rely on robust phoneme processing within the architectures [119]. Using text as a input is a relatively unexplored field which enables the generation of Face Expression generation based on the emotion content of textual dialogue [114]. Other researchers have explored methods to directly control the emotions on the output videos using CLIP text prompts [105, 121]."}, {"title": "5.2 Speech Emotion Generation", "content": "One element of SEG, known as voice conversion, speech-in-speech synthesis, or speech reenactment, involves the transformation of speech signals to modify the vocal characteristics of one speaker to resemble another or to produce entirely synthetic voices. These methods form the basis of SEG, whereby the emotions in a target voice can be changed through prompts [122], or by the emotions in a target voice through using a emotional reference voice [123].\nRecent advancements in AI have led to the development of synthetic voices that are almost indistinguishable from human speech. Achieving realism in generated speech involves capturing natural intonation, rhythm, and emotion. Advanced systems, such as those by ElevenLabs [124], use SOTA deep learning techniques to produce high-quality, realistic speech. These systems generate voices that sound authentic and carry unique characteristics associated with individual speakers. This section reviews recent advancements in SEG methodologies.\nSEG models use phonetic content, including emotional cues, from a source voice to synthesize audio in a target voice while retaining desired stylistic characteristics [123]. A common approach in SEG involves using language models like BERT [88] for extracting contextualized representations of linguistic content, thereby enabling precise alignment between source and target voices. BERT embeddings contribute to the controllability and realism of SEG systems, facilitating accurate transformations in speech style and characteristics, which allows synthetic speech to be tailored to specific emotions [125, 123, 126, 127]. Traditional approaches often rely on text-based conditioning using transcripts [128]; however, recent methods, such as that by [127], employ discrete representations for phonetic content. This enables the capture of non-textual cues, such as laughter, and supports diverse linguistic applications. Additionally, [126] propose an architecture that integrates source and target encoders with a decoder, preserving critical linguistic and speaker features throughout the conversion process to ensure the synthesized speech remains natural and true to the source.\nSEG also benefits from adversarial training techniques inspired by GANs [129]. In these frameworks, a discriminator differentiates between target voice samples and synthesized speech, prompting the model to generate speech that convincingly reconstructs the source content while mimicking the target speaker's characteristics.The DDDM-VC model [130] introduces a novel approach for SEG, enhancing controllability by decoupling and independently processing attributes such as content, pitch, and timbre. Through attribute-specific denoising, DDDM-VC achieves high-precision voice style transformations, while the inclusion of prior mixup techniques strengthens robustness in voice adaptation, especially in zero-shot scenarios. This disentangled structure enables DDDM-VC to maintain speaker fidelity and naturalness in synthesized voices across a variety of speaker styles. Similarly, PromptVC by [122] uses a latent diffusion model for voice style conversion using natural language prompts. This enables precise control over the attributes in the generated speech. Another method uses Contrastive Predictive Coding (CPC) features to enhance the quality of synthesised speech [123], which is a self-supervised learning technique for predicting future utterances in latent space. Similarly, [131] preserves time-synchronisation and fundamental frequency information to maintain the naturalness of converted speech. Finally, two-stage training schemes are frequently used to align hidden representations between source and target speech. The initial stage focuses on reconstructing single utterances to establish alignment, followed by a second stage where multiple utterances refine the conversion process [132]. This progressive refinement enhances the model's adaptability, improving performance in scenarios with significant divergence between source and target speech characteristics."}, {"title": "5.3 Text Sentiment Generation", "content": "TSG models work from a user interface by taking input text, and generating a response . TSG have the ability to alter the emotional content in existing text. Large Language Models (LLMs) such as ChatGPT [28], Llama [29], Gemini [133] can create text with emotions and personality which can pass for human writing. Ensuring accurate grammar and syntax, a diverse and contextually appropriate vocabulary, and consistency in style, tone, and information are all important for TSG. Additionally, typographical errors, realistic mistakes, smooth transitions between ideas and a deep understanding of context also contribute to the text's realism.\nUntil recently Recurrent Neural Networks (RNNs) [134] were used extensively in text generation due to their ability to handle sequential data by maintaining an internal memory. However, traditional RNNs suffer from the vanishing gradient problem, which impedes long-range dependencies. They also struggled to work on long sentences [135]. Researchers attempted to combat this by running the RNNs both forward and backward over the textual data [136], which did not rectify the problem. These limitations led to the development of Long Short-Term Memory (LSTM) networks, a variant of RNNs. LSTMs employ architectures with gated mechanisms, including input, output, and forget gates, enabling them to learn and retain long-term dependencies in sequential data [135]. This feature makes LSTMs particularly ideal for tasks requiring memory over extended sequences, such as text generation. Another architecture used for text generation are Sequence-to-Sequence (Seq2Seq) models [137], which consist of an encoder and a decoder. Seq2Seq models have shown proficiency in generating coherent and contextually relevant text, making them valuable for emotional text generation tasks. Generative Adversarial Networks (GANs) [138], used mostly in computer vision, have also emerged as useful for text generation tasks. The generator produces synthetic text data, while the discriminator evaluates the authenticity of the generated text. Used in conjunction with the above algorithms, attention mechanisms enable models to focus on relevant parts of the input text sequence when generating a response. Attention mechanisms allow models to weigh the importance of each word in the input sequence dynamically as they generate each word in the output sequence [139]. For example, in the Seq2Seq model, attention mechanisms help align the encoder hidden states with the decoder hidden states at each time point, ensuring the model attends to the most relevant parts of the input sequence when generating each word in the output sequence [139].\nTo address these challenges researchers are exploring various approaches. One approach involves fine-tuning pretrained language models such as ChatGPT [28] for emotion-specific tasks [140]. This approach uses datasets annotated with emotional labels to train the model to associate linguistic patterns with emotional states. During fine-tuning, adjustments are made to the model's parameters through additional training iterations on emotional text datasets. Developing models with an understanding of contextual cues is essential for accurate emotional text generation. This involves considering factors such as the broader narrative, speaker intent, and audience context to generate realistic text."}, {"title": "5.4 Generative Models with Emotion Control", "content": "This section will examine methodologies for implementing emotion control within FEG, SEG, and TSG. Emotion control, in this context, pertains to the systematic generation of content-spanning animations, speech, and textual outputs-characterised by realistic and contextually appropriate emotional expressions. These emotions are elicited or guided through specific prompts or control mechanisms, ensuring that the generated outputs align with intended affective states. The discussion will encompass techniques used to encode, manipulate, and render emotions, as well as the underlying computational models that enable nuanced emotional dynamics across various modalities."}, {"title": "5.4.1 Audio Driven Face Expression Generation", "content": "shows audio driven Face Expression generation by [99]. This method for Face Expression generation takes a reference image as input which is put through a frames encoder. Next, a feature extraction network, called ReferenceNet extracts detailed features from the reference image and after the first iteration, the motion frames, to preserve the identity from the reference image. The architecture then progresses to the diffusion stage where a pretrained audio encoder processes the input voice audio clip, extracting voice features which influence the facial movements and expressions. The Backbone Network, using reference-attention and audio-attention mechanisms, denoises the input data and generating realistic video frames. This comprehensive network architecture ensures the generated video frames sync with the provided audio content. Speed layers fine-tune temporal modules and control head motion across clips, improving consistency and stability in the generated videos."}, {"title": "5.4.2 Text Driven Face Expression Generation", "content": "The text-based talking-head generation framework by [114] uses neural networks tailored to different aspects of generating Face Expression animations from textual inputs. Gmou, dedicated to animating mouth movements from phonemes, uses a structure based on CNNs for efficient parallel computation and is trained using a combination of L1 loss and Least Squares Generative Adversarial Network (LSGAN) loss. Similarly, Gupp and Ghed utilise encoder-decoder network structures to synthesize upper face parameters and head pose, respectively, from input words, training with analogous loss functions to ensure realistic outputs. The Style-Preserving Landmark Generator, Gldmk, uses a multi-linear 3D Morphable Model (3DMM) and a fully-connected network to ensure consistency and accuracy in facial expressions, incorporating a unique mapping technique to preserve speaker-specific styles."}, {"title": "5.4.3 Video Driven Face Expression Generation", "content": "NED by [104] allows manipulation of Face Expressions in in-the-wild videos while preserving natural speech-related mouth motion. The Face Analysis module incorporates preprocessing steps such as face landmark detection, segmentation, and resizing, alongside 3D Morphable Models (3DMMs) for accurate estimation of 3D face geometry. The Expressions Translator, a GAN, utilises a recurrent network with LSTM units to convert sequences of facial expressions into desired emotions, while maintaining the original mouth motion. A encoder extracts emotion-related style vectors from the input sequences, while the Mapping Network generates style vectors associated with target emotions. A neural face renderer generates realistic frames, incorporating techniques such as multi-band blending for seamless integration of generated faces into the original backgrounds. This ensures the manipulated facial expressions seamlessly blend into real-world scenarios. During testing, N-length sliding windows are applied frame by frame, with the sequences processed through the Expressions Translator. The conditional style vector is either generated by the Mapping Network or extracted from a reference video, allowing for flexible manipulation of emotions in facial videos."}, {"title": "5.4.4 Emotion Prompted Face Expression Generation", "content": "EAT by [105] takes in an image of a target face, speech, and an emotion prompt such as happy, sad, or angry, to generate animated videos. The model first trains the CLIP model on emotion labelled datasets to learn audio-visual correlations. This pre-training phase uses enhanced latent representations and a transformer model. Enhanced latent representations capture intricate facial expressions, incorporating identity-specific canonical keypoints, rotation, translation, and expression deformation components. The transformer model predicts synchronised expression deformations from audio inputs and predicts head pose features, and latent source image representations. Next, three primary modules-Deep Emotional Prompts, Emotional Deformation Network (EDN), and Emotional Adaptation Module (EAM)\u2014play integral roles in the emotional adaptation. Deep Emotional Prompts inject emotion-guided expression generation into the model, using latent codes sampled from a Gaussian distribution to provide crucial emotional guidance. EDN complements this by predicting emotion-related expression deformations. EAM further refines the visual quality of generated videos by generating emotion-conditioned features. The architecture also accommodates zero-shot expression editing, which allows text-guided manipulation of talking-head videos without the need for extensive emotional training data. Using the CLIP model, the system aligns generated expressions with textual descriptions, offering users control over the emotional content of the videos."}, {"title": "5.4.5 Speech Emotion Generation Model", "content": "The architecture in [123] comprises three main components: source encoder, target encoder, and a decoder. The source encoder uses Wav2Vec 2.0 [141], a pretrained feature extractor, to capture speech representations from the source utterance. The target encoder processes log mel-spectrograms of utterances from the target speaker, and the decoder consists of transformer layers using both self-attention and cross-attention. A linear projection layer contributes to the final prediction of the desired output voice, following a non-autoregressive approach. The model is trained using a two-stage approach. In the first stage, single utterances from both the source and target speakers are used to reconstruct the log mel-spectrogram of the utterance. In the second stage, multiple utterances, typically 10, from the target speaker are concatenated and fed into the target encoder. Simultaneously, a single utterance from the source speaker is fed into the source encoder."}, {"title": "5.4.6 Text Sentiment Generation Model", "content": "A model [140] built upon ChatGPT2 [142], has been trained to generate text with specific emotions. The ChatGPT2 model is fine-tuned with text samples annotated with affective labels or sentiment scores. The Plug and Play Language Model (PPLM) framework is integrated into the ChatGPT2 architecture to enable attribute-controlled text generation. PPLM incorporates perturbation and optimisation mechanisms during training, enhancing the model's ability to generate text with specific affective attributes. The model's loss functions include terms which encourage the generation of text with desired emotional attributes and intensity levels. Users specify the desired emotional tone or topic, and the intensity of the emotion desired. The model uses specified attributes and intensity levels to control the content and tone during the text generation process."}, {"title": "5.4.7 Text Sentiment Generation Chatbot", "content": "The Empathetic Semantic Correlation Model (ESCM) by [84] generates empathetic responses in dialogues by understanding emotions and semantics. It includes three components: a context encoder, a dynamic correlation encoding module, and an emotion and response predicting module. The dynamic correlation encoding module features dynamic emotion-semantic vectors and a correlation Graph Convolutional Network, adjusting emotions and semantics based on contextual cues. The emotion and response predicting module uses context semantics and correlations to predict emotions and generate empathetic responses. During training, ESCM optimises parameters using multiple loss functions and supervised learning on annotated datasets. In use, ESCM processes dialogue context, adjusts to contextual cues, and continuously learns to provide accurate, empathetic responses."}, {"title": "6 Evaluation", "content": "This section provides an overview of the metrics used to evaluate ER and EG models across facial, speech, and textual modalities. It explores various evaluation techniques to determine their effectiveness in measuring model performance and accuracy. Furthermore, the comparative analysis within this section examines state-of-the-art methods to identify the"}]}