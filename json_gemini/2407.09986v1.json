{"title": "Curriculum Is More Influential Than Haptic Information During Reinforcement Learning of Object Manipulation Against Gravity", "authors": ["Pegah Ojaghi", "Romina Mir", "Ali Marjaninejad", "Andrew Erwin", "Michael Wehner", "Francisco J Valero-Cuevas"], "abstract": "Learning to lift and rotate objects with the fingertips is necessary for au-\ntonomous in-hand dexterous manipulation. In our study, we explore the im-\npact of various factors on successful learning strategies for this task. Specif-\nically, we investigate the role of curriculum learning and haptic feedback in\nenabling the learning of dexterous manipulation. Using model-free Reinforce-\nment Learning, we compare different curricula and two haptic information\nmodalities (No-tactile vs. 3D-force sensing) for lifting and rotating a ball against\ngravity with a three-fingered simulated robotic hand with no visual input.\nNote that our best results were obtained when we used a novel curriculum-\nbased learning rate scheduler, which adjusts the linearly-decaying learning\nrate when the reward is changed as it accelerates convergence to higher re-\nwards. Our findings demonstrate that the choice of curriculum greatly biases\nthe acquisition of different features of dexterous manipulation. Surprisingly,\nsuccessful learning can be achieved even in the absence of tactile feedback,\nchallenging conventional assumptions about the necessity of haptic informa-\ntion for dexterous manipulation tasks. We demonstrate the generalizability of\nour results to balls of different weights and sizes, underscoring the robustness\nof our learning approach. This work, therefore, emphasizes the importance\nof the choice curriculum and challenges long-held notions about the need for\ntactile information to autonomously learn in-hand dexterous manipulation.", "sections": [{"title": "Introduction", "content": "Dexterous manipulation is a triumph of biology (1\u20138). However, the autonomous learning of\nsuch behavior continues to remain out of reach for robots (4, 9\u201312). Robots have excelled at\ngrasping (reaching for and statically coupling an object to the hand by applying forces with the\nfingertips, fingers, and palm (4, 6, 13, 14)) for decades (e.g., (15\u201323)), but grasp is not dexterous\nmanipulation (4). Dexterous in-hand manipulation (i.e., dynamically holding and reorienting\nan object with the fingertips (4, 18, 24, 25)) is critical for interaction with, and use of, objects in\nunstructured human environments.\nTo achieve this kind of manipulation with multi-fingered robotic hands, the robotics commu-\nnity has developed sophisticated control theoretical approaches \u00b9 (e.g., (4,7,26\u201333)). These con-\ntrol theoretical approaches, however, tend to require accurate models and state estimation, have\nnarrow stability margins, and have difficulty compensating for friction, interpreting intermit-\ntent/deformable contact, and coordinating between multiple fingers. As an alternative approach,\nbiorobotic, neuromechanics, and artificial intelligence communities have introduced a variety\nof bio-inspired and data-driven machine-learning approaches (for reviews see (4, 11, 14, 34, 35))\nin simulation and hardware.\nOne particularly promising approach is the sub-field of Reinforcement Learning (RL), which\nhas provided several successful examples (12,23,36\u201339). RL empowers robots to iteratively en-\nhance their manipulation skills through trial and error (without of a need of an accurate model of\nthe task or the environment), resulting in gradual improvements within complex environments.\nHowever, manipulation RL studies to date are usually highly computationally intensive\u2014and\nhave relied on vision\u2014which limits their applicability (28, 35, 40\u201349). Lastly, most studies\nhave been limited to the upward-facing hand configuration, relying on the palm as a resting\nplatform for the object being manipulated which makes it an inherently more stable task to han-\ndle than a down-facing hand configuration (9). Adding the downward-facing hand configuration\nbroadens the scope of solutions, delivering valuable insight to the robot manipulation commu-\nnity (9, 12,50). However, it introduces additional challenges as this orientation requires the hand\nto counteract gravity at all times (51), and errors can lead to instabilities and failure by dropping\nthe object. Here we use an RL based on the Proximal Policy Optimization (PPO) (52) algorithm\nto autonomously learn manipulation with a downward-facing hand without direct vision. We"}, {"title": "Results", "content": "find that the choice of curriculum biases learning manipulation toward one or another combi-\nnation of skills (i.e., lifting the ball and/or rotating it) more profoundly than the availability of\ntactile information.\nSurprisingly, the absence of tactile information did not necessarily prevent or significantly\ndegrade learning relative to the influence of curriculum. These results reveal fundamental and\npreviously underappreciated aspects of curricula as a powerful tool for autonomous learning of\nmulti-objective tasks. For example, curricula commencing with both lift and rotation exhibit\ninitial superior performance compared to those building up from simpler blocks, such as focus-\ning solely on lift or rotation. Focusing on a single skill thereafter, however, can be additionally\nbeneficial. Beyond assessing the impact of curricula on autonomous manipulation, our study\nyielded the significant revelation that, contrary to long-held notions, the absence of tactile in-\nformation (and direct vision) does not inherently impede or degrade the learning process. In\nfact, there seems to be a functional interaction with a curriculum where available sensing capa-\nbilities bias the learning process toward combinations of dexterous manipulation skills that can\nleverage the available tactile information.\nThe goal of this project was to utilize curriculum-based RL to learn in-hand manipulation of\nan object against gravity in a data-efficient way\u2014even while not using visual information. We\ndemonstrate how the choice of curriculum is more influential than tactile information when\nlearning to lift and rotate a ball (weighing 50 g with 35 mm radius) with a three-finger robotic\nhand in simulation (Fig. 1). To do so, we systematically explored two tactile conditions: No-\ntactile (no force perception at all at the fingertip) vs. 3D-force (a 3D force vector in the direction\nof force at the fingertip) during five distinct curricula (details in Methods). We defined each\ncurriculum as implementing a learning policy that rewards various combinations and sequences\nof lift (L) and rotation (R) of a ball, which can switch at the halfway point (Methods, Table 1)."}, {"title": "Discussion", "content": "and sizes (Fig. 5). We find that the order of reward (curriculum) greatly affects the progression\nof learning and the final performance, 3D tactile information was not consistently better than\nNo-tactile information, and a similar trend was observed across all configurations (see the video\nfile in Supplementary Information).\nEach combination of curriculum and tactile information (Methods, Table 1) leads to a distinct\nevolution of learning and final performance. This effect of curriculum affects both the progres-\nsion of learning (path) and final performance (endpoint), and can be visualized as traversing a\ndevelopmental process (as \u2018Waddington Landscapes' in biology, Fig. 2, see Discussion).\nCurricula, as expected, diverge in their ability to lift and rotate the ball. In fact, they had the\nprofound effect of biasing toward one or another combination of skills (L or R) and also adapt to\nthe available sensory input, much like experience-dependent developmental paths from an initial\npluripotent state (Fig. 2C). As we describe in detail in the Discussion section, we explicitly\nexplored different initial rewards with similar final rewards (C1 [L|L+R] vs. C2 [R|L+R] ),\nand vice versa (C4 [L+R|R] vs. C5 [L+R|L] ). In all cases, the system was able to respond\nto the change in reward (albeit with variable success). Note the evolution of skills for each\ncurriculum tended to saturate quickly within the first 250 episodes of the first and second phases\nof learning. They tended to asymptote between the 250 and 1,000, and between the 1,250 and\n2,000 episodes, respectively. Nevertheless, the final endpoints for each curriculum differed\nsignificantly, showing that curricula are more than simply a means to learn multi-objective\ntasks, but can actually produce different learning paths and endpoints\u2014which can be exploited\nby the user to achieve different capabilities with the same na\u00efve system (Fig. 2).\nCounterintuitively, starting with a multi-objective reward can be as effective, if not more\neffective, than starting with simpler rewards. For example, rewarding both lift and rotation\nduring the first 1,000 episodes (C3 [L+R|L+R], C4 [L+R|R], and C5 [L+R|L] ) improves\nrotating the ball at the end of learning (episode 2,000) better than when only rewarding rotation\n(C2 [R|L+R]) at the start.\nMost surprisingly, the absence of tactile information did not preclude learning. Moreover, learn-\ning with No-tactile information was comparable to the 3D-force information (Fig. 2). The\npresence or absence of 3D-force information did, however, change the learning paths and end-\npoints of each curriculum (Fig. 2, 3)\u2014but the effect was not uniform. For example, 3D-force\ninformation did produced more lifting than No-tactile in C1 [L|L+R] at the end of learning. But\nthis was reversed in C3 [L+R|L+R] ; and tactile information did not affect C4 [L+R|R] or C5\n[L+R|L] much (Fig. 2). This nuanced effect of tactile information at the end of learning is also\nseen in Fig. 3, and on average during learning in Fig. 4. This interaction was also seen while\nlearning with different objects (see details in the Generalizability section and Fig. 5).\nFurther nuance of the effect of tactile information can be seen in the different paths of\nlearning and in the response to switching of rewards between the first and second learning\nphases (i.e., after episode 1,000). Note C3 [L+R|L+R] rewards both skills during the entirety\nof both phases, but tends to be most effective at lifting in the No-tactile condition compared to\n3D-force condition, Fig. 2. Nevertheless, when switching the reward to only lift C5 [L+R|L] or\nonly rotation C4 [L+R|R] at the end of the first learning phase, the 3D-force case makes up for\nlost ground and has endpoints similar to those for the No-tactile condition. This effect seems to\nbe reversed for C1 [L|L+R] and C2 [R|L+R] where only lift or rotation were rewarded at first.\nIn these cases, the 3D-force condition produced greater lift and rotation during both learning\nphases.\nWhat did we learn about learning?\nWe provide proof-of-principle that it is possible to learn the hard problem of dynamic dexterous\nmanipulation. Putting our work in context is critical and best done by pointing to its place in the"}, {"title": "Methods", "content": "updated taxonomy of hand function put forth by MacKenzie, Iberall, Brand, Curkosky, Dollar,\nand others (2, 18,54\u201357). In particular we have addressed the problem of dynamic manipulation\nwith three fingers while the ball is at risk of being dropped at any moment (see \u2018Compari-\nson to State-of-the-Art' section). This definition emphasizes that 'grasp' and 'pick-and-place\nmanipulation' are conceptually and mechanically distinct from 'dynamic manipulation' as ad-\ndressed here, even though they are at times used interchangeably in the literature (58). Such\ndynamic manipulation is, in fact, an enviable ability that is also difficult for biology to achieve\nas it develops in humans late in childhood, degrades in healthy aging, and is quickly lost in\neven mild/initial forms of neurological conditions such as peripheral neuropathies, stroke, and\nParkinson's disease (e.g., (58, 59)). In our work, the fingertips induce dynamic translation and\nrotation of the ball while making and breaking contact. As such, the hand function we achieved\nmerits the description of dynamic dexterous manipulation.\nCurriculum learning can be seen as a developmental process from a pluripotent state. We use\nthe analogy of the Waddington Landscape (Fig. 2C) for curriculum learning of manipulation\nbecause of its similarity to epigenetic transformation from a pluripotent state in biological de-\nvelopment (53, 60). Curriculum learning, in fact, produces a developmental trajectory from a\nnaive (i.e., pluripotent) state based on resources (tactile information) and experience (curricu-\nlum) (Fig. 2A&B). Each curriculum affects both the progression of the learning (path) and\nits final performance (endpoint), and can thus be thought of as traversing a Waddington Land-\nscape. Importantly, the evolution of skills for each curriculum was (unlike cell differentiation)\nnot strictly irreversible, but remained adaptable. Specifically, the change of reward after the\nfirst learning phase did not preclude the system from emphasizing the improvement of the new\nskill. This is visually represented by 90\u00b0 shifts in the paths (see C1 [L|L+R] and C2 [R|L+R]\nin Fig. 2). In some cases, the response to a switch in reward even reversed a learned skill for\nthe first 250 episodes in the second phase of learning, and only then increased the new skill (see\nC4 [L+R|R] and C5 [L+R|L] in Fig. 2). In one case, C3 [L+R|L+R], there was no change in\nreward after the end of the first learning phase, and the system was saturated already. In others,\nthe system did respond like an \u2018irreversible' system that learned little of the new skill, of at all,\nwhen the reward function was switched (e.g., C2 [R|L+R] in the 3D-tactile case in Fig. 2). See\nthe next Discussion Section.\nManipulation can be achieved without tactile information or vision. Tactile information has\nlong been thought as necessary for human\u2014and by extension robotic\u2014manipulation (4, 61).\nThis idea was reinforced by the work of Johansson and Westling (62, 63) demonstrating that\nnumbing the fingerpads with temporary anesthetic greatly impairs fine manipulation. Our re-\nsults in Fig. 2 provide a counter-example to this longstanding notion. Interestingly, we found\nthat our system was able to learn even in the absence of tactile information (the No-tactile con-\ndition in Figs. 2A and 3). In fact, having 3D-tactile information not always produced better\nperformance (cf. C3 [L+R|L+R] in Fig. 2A&B).\nHow is it possible to learn to manipulate without vision or tactile information? The answer,\nwe believe, comes from the nature of reinforcement itself. As described in Fig. 1B, PPO\u2014as a\nreinforcement learning algorithm\u2014condition its actions (next-step finger joint angles, angular\nvelocities, palm position, and velocity) based on the system state, ultimately optimizing for\nincreased reward. In the No-tactile case, the hand's state comprises finger joint angles, angular\nvelocities, palm position, and palm velocity-which seem to suffice to learn the task. Therefore,\nlift and rotation of the ball was a product of guided hand kinematics that properly affect ball\ndynamics to increase the reward in the No-tactile case, such as in our previous work to learn\nlocomotor movements without the need to sense the ground (39).\nAs such, a main contribution of our work is to provide an existence proof that an agent\nusing reinforcement learning is able to learn a sophisticated manipulation behavior even in the\nabsence of tactile information. Note that direct vision was not necessary either, as in other prior\nwork (39). Our important result about dynamic manipulation provides impetus to revise our\nthinking about, and use of, tactile and visual information to allow freer thinking for engineers\n(and bio-roboticists) creating the next generation of dexterous hands and robots.\nThe presence or absence of tactile information did, however, alter the progression of learning.\nFigures 2A&B (and the Supplementary Information Fig. S1) show that the type of sensory infor-\nmation did affect learning\u2014but the general features of the path and endpoint of each curriculum\nremained similar in both tactile conditions. Importantly, the effect of tactile information was not\nsystematic. The 3D-tactile cases were not consistently or necessarily better than the No-tactile\ncases, or vice versa. Thus, curriculum is a dominant factor compared to tactile information.\nFrom the computational perspective, one could have expected that when learning with a\nfixed number of episodes, 3D-tactile information would perform systematically worse because\nof the computational demands associated with extending the length of the hand state vector sh\nby 9 elements (3 forces per finger) for the same PPO algorithm architecture which now has to\ntune more weights (Fig. 1). But, 3D-tactile cases at times outperformed the No-tactile cases\n(e.g., C1 [L|L+R] in Fig. 1A&B), which strongly suggests that our comparisons across curricula\nand tactile conditions are not the result of an imbalance in computational demands for a fixed\nnumber of learning episodes (1,000 per learning phase for a total of 2,000). This is additionally\nsupported by the fact that 3D-tactile cases also saturated their learning by the 250th episode\n(like the No-tactile cases).\nAlso, it is important to note that this study does not undermine the effectiveness of tactile\ninformation in many everyday tasks. It merely provides a proof-of-principle that it is possible\nto learn a specific task (i.e., the manipulation task of interest in this paper) without using tactile\ninformation; and with performance comparable to when tactile information is available. It\nis clear that many tasks exist for which sensory signals would either be crucial to perform,\nor would greatly enhance, either the learning speed for the task, the final performance, error\ncorrection, and/or their robustness and repeatability. These are beyond the scope of our work.\nOur system exhibits some important features of lifelong learning. As defined in (11), our system\nshows transfer and adaptation because it reuses knowledge to improve performance and rapidly\nadapts to novel skills as in C1 [L|L+R] C2 [R|L+R], C4 [L+R|R], and C5 [L+R|L] (in Figs.\n23, and 4). Similarly, our system did not suffer from catastrophic forgetting as it was able\nto retain varying amounts of previously learned knowledge on a case-by-case basis (Fig. 2 and\nSupplementary Information, Fig. S1). For example C4 [L+R|R] and C5 [L+R|L] did not entirely\nforget to lift or rotate when they were no longer rewarded, respectively.\nCurriculum learning does not necessarily have to advance gradually from single-objective to\nmulti-objective rewards. In many applications such as locomotion, investigators have found\nthat curriculum learning is indispensable to advance gradually from single-objective (i.e., \u2018sim-\npler') to multi-objective (i.e., 'more complex') rewards (64). This has led to curriculum learning\nbecoming the standard approach in the field. From the traditional definitions of Vanilla or Pro-\ngressive curriculum learning (65, 66), one might assume that first learning to lift the ball (a form\nof grasp) is 'easier' than rotating it, which involves a dynamic behavior (4, 27) and a curriculum\nstrategy in which rotation is learned only after lift is going to be a significantly more successful\none. However, rewarding lift and rotation from the start does not hinder learning, as demon-\nstrated by C3 [L+R|L+R] . In fact, it allowed transfer and adaptation for C4 [L+R|R] and C5\n[L+R|L] to subsequently refine the single skill rewarded during the second phase of learning\u2014\nalbeit at the expense of some reduction of the non-rewarded skill. However, it is noteworthy\nthat curricula that rewarded only one skill from the start (C1 [L|L+R] and C2 [R|L+R] ) were\nnot able to learn the second skill as efficiently during the second learning phase (rotation and\nlift, respectively).\nAnother aspect of lifelong learning involves the saturation of capacity causing learning to\nslow down (67, 68). Capacity saturation arises due to the fixed representational capacity of\nparametric models, including the PPO algorithm (68). We see this in our implementation of\nPPO\u2014which increasingly fails to absorb additional knowledge from successive episodes. This\nis most evident in C3 [L+R|L+R] for the entire second phase of learning, as shown in Fig. 2.\nA learning model with more free parameters would theoretically be able to absorb additional\nknowledge from successive episodes.\nThe curriculum-based learning rate scheduler enhances the efficiency of learning which accel-\nerates convergence to higher reward\nWe sought to align the implementation of learning rates in PPO with the nature of curricu-\nlum learning. To do this, we defined our curriculum-based learning rate scheduler to adjust\nthe linearly-decaying learning rate when the reward changed (Fig. 6). We find this improved\nlearning and allowed a more fair comparison across curricula as it reduced heuristic tuning ef-\nforts. This curriculum-based learning rate scheduler offers an effective approach tailored to\ncurriculum learning for autonomous systems by modifying the learning rate only when chang-\ning task complexities and rewards. Empowering curriculum learning to adapt learning rates in\na way compatible with changing rewards enables autonomous systems to learn complex and\ndynamic environments more systematically, autonomously, and effectively. Thus, integrating\ncurriculum-based learning and reward scheduling into a 'curriculum-based learning rate sched-\nuler' for autonomous systems is vital to enhance their learning capabilities and performance in\nmanipulation tasks.\nLastly, we demonstrate our results generalize to balls of different weights and size. As shown\nin the Supplementary Information in Figs. S1-S7, our results were consistent across the four\nobjects we studied (i.e., of two masses, 50 and 5 g, and two sizes, 35 and 30 mm in radius, Fig.\n1A). Namely, our system was successful at learning to manipulate, but in a way that curriculum\nhad a greater impact than tactile information. There were minor differences across the endpoint\nperformance for each object (note the difference is the scales of the axis). But the learning paths\nfor each curriculum, and the effect of switching the reward, remained consistent (Fig. S1). This\ncan also be seen in the detailed depiction of the distribution of rewards as learning progressed,\nFig. 5). This further shows that the effect of ball size or weight (like that of tactile information\nas mentioned above) was not substantial nor systematic.\nIt is critical to note that, as we have stated in the past (4), grasp or pick-and-place tasks are not\ndexterous manipulation in the rigorous sense of grasp taxonomies. Even reorienting a cube rest-\ning on an upward-facing palm (9, 16, 50, 69\u201372) is not prehensile manipulation. Likewise, Prior\nwork has used extensive vision with the upright palm to hold an object being reoriented (12,73),\nother than one demonstration of learning under increasing force of gravity (74), we know of no\npublished demonstration of dynamic manipulation task against full gravity utilizing curriculum\nlearning either directly in simulation or hardware. We employ a novel curriculum-based learn-\ning rate scheduler for PPO, which significantly enhances the success performance across all\nscenarios. We now discuss how our novel approach to manipulation compares and contrasts\nwith other studies in robotics and RL. The state-of-the-art of autonomous learning for in-hand\nmanipulation is limited. Although important advances have been made using computationally\nintensive approaches in simulation and hardware (e.g., (12, 28, 35, 41\u201343)), these tend to be im-\npractical for autonomous learning at the edge. Augmenting RL for manipulation with imitation\nlearning has shown some successes (12, 36\u201338), but collecting task-specific expert demonstra-\ntions from humans are often limited to specific objects or tasks, might not always be practical,\nrequire specialized equipment and can be time-consuming.\nIn contrast, we used a model-free data-driven approach because precise prior knowledge\nof the system, objects and the environment is not always available, especially in unstructured\nenvironments. Although some other studies also use model-free RL methods for rotating objects\nwith simulated fingers or a robotic hand (9, 75\u201377), we have overcome some of their drawbacks.\nIn (9, 76), the orientation of an object was controlled while resting on an upward-facing palm.\nThus, it did not have to be held against gravity as it was not at risk of being dropped at any time.\nSome of these limitations were addressed by Chen et al. (77) in simulation by manipulating\nthe object with the palm facing downward like we did, but gravity was introduced slowly as\npart of the curriculum. Moreover, to successfully manipulate the object the authors found it\nimportant to 'initialize the object in a stable configuration'\u2014which we did not need.\nThe way our work went beyond the state-of-the-art, therefore, is by demonstrating for the\nfirst time a method with the ability to autonomously learn to manipulate an object against gravity\nwhile revealing the role of curriculum learning and tactile information in in-hand manipulation.\nThe impact of learning rate scheduling on stochastic optimizer performance has been exten-\nsively investigated in recent research (78, 79). In our study, we specifically explore the effects\nof a constant and linear piecewise learning rate for PPO on the success of our architecture. Af-\nter careful consideration, we have decided to proceed with the piecewise learning rate. This\nadaptive approach adjusts rates dynamically throughout training, speeding up the process with\nhigher initial rates and ensuring stable convergence with lower rates later on.\nLastly, our work underlines the importance of curricula in manipulation and shows how the\nright choice of a curriculum can enhance performance and robustness across multiple tasks by\nexhibiting some important features of lifelong learning.\nWhile our work pushes the field of autonomous manipulation forward, it naturally has some\nlimitations. First, our work is done in simulation. But, as with many other studies looking\nto bridge the sim2real divide (39, 80), we used realistic physical constraints within our state-\nof-art physics engine (MuJoCo) that handles dynamic contacts and impacts well. This is a\nfoundation that will enable future implementations in hardware. As to the geometry of our\nhand, it is common for useful robotic hands to have three fingers (28, 75). Curriculum learning\nhas multiple varieties (66) that can adapt as learning progresses such as Self-Paced curriculum\nlearning. In our case, our learning phases were of fixed duration even though the system tended\nto plateau. Thus, it could benefit from future implementations that adapt reward changes to\nminimize training time. Lastly, our manipulation tasks serve as a foundation for\u2014but do not\nyet address-traditional use cases for activities of everyday life."}, {"title": "Simulation environment", "content": "Our choices regarding PPO, curriculum design, hand and object structure, reward function,\nand other parameters were specifically tailored to address the scientific questions of interest\nwithin the scope of this paper and to establish a proof of concept. It's important to empha-\nsize that our selections were not intended as universally applicable solutions. That is to say, to\naddress a different need, a similar pipeline to this paper can be utilized but different tasks, envi-\nronments, or robotic structures might need to be used. Also, different learning blocks (different\nthan the RL technique or the adaptive curriculum-based learning rate scheduler function used\nin this paper) can be used that might serve best for another specific task or purpose.\nIn this section, we first describe the simulation environment and the task used in this study.\nThen, we elaborate on the learning policy that enabled autonomous manipulation.\nThe manipulation and machine learning communities have used the advanced physics simula-\ntion environment MuJoCo (81) for tasks involving autonomous manipulation. MuJoCo allowed\nus to implement reinforcement learning algorithms on a robotic hand in a realistic environment\nthat includes contact dynamics (including penetration) and gravitational acceleration (81, 82).\nTo demonstrate the adaptability and robustness of our proposed methodologies, we assessed\nthe performance using four different objects. Our evaluations encompassed systematic explo-\nration, considering two different weight combinations (50 g and 5 g), as well as varying ball\nradii (35 mm and 30 mm). The work presented herein focuses on a ball of 50 g with a radius of\n35 mm with the other configurations presented in the section Generalizability in Supplementary\nmaterial."}, {"title": "Robotic Hand Design", "content": "We simulated a bio-inspired, three-fingered robotic hand with a palm and three identical servo-\ndriven fingers: two adjacent fingers, analogous to the 'index' and 'middle' fingers, and one\nopposing them, analogous to the 'thumb'. In contrast to our prior efforts (83), where we show-\ncased the reach-to-manipulate capability with a downward-facing orientation using distinct cur-\nricula, we modified the hand design. Each finger consisted of two joints that could rotate about\nthe y-axis (q1 and q2 in (Fig. 1A)), similar to the flexion or extension seen in human fingers. The\nsize of the palm and length of each 'phalanx' was based on an average human hand (75, 84).\nAn additional servo motor was included at the base of the hand, which provides translational\nmotion in the vertical direction (zh).\nThis work incorporated tactile information and RL, sometimes referred to as touch-augmented\nRL, as we covered the internal side (i.e., the \u2018pads' of the fingertips) of the distal phalanx of\neach finger with tactile sensors. Contact regions were configured near the tips of each finger\n(Tactile Area, Fig. 1). Objects contacting the finger outside of these tactile areas (sites, in\nMuJoCo) are not perceived as tactile information by the learning algorithm (76, 81).\nWe used MuJoCo's built-in features to record the 3D-force sensor on the fingertips of all\nthree fingers. The 3D-force sensor sites provide a 3D array of 3 orthogonal forces (one normal\nand two tangential to the sensor site for each sensor) of scalar values representing the 3D-force\nvector. Moreover, we have considered an additional case: No-tactile. In the No-tactile case, the\nstate vector for the tactile information sh,f is null (we do not consider the tactile information\nin learning). As shown in Fig. 1A, the possible contact tactile information at each fingertip is\nindicated by sh, f = [ft,1, ft,2, fn] and it depends on tactile sensing available at fingertips. See\nSupplementary Table S3 for more details on the tactile information."}, {"title": "Task Description", "content": "The robotic hand attempts to manipulate a 50 g, 35 mm radius ball, which starts each episode\non the ground with the palm of the robotic hand at a height of 200 mm above the ground. The\nball height zo is defined at the center of the ball, and we specified a desired height for the ball zd\nto be 25 mm above zb. In other words, the desired height za is 60 mm above the ground.\nThrough simulation constraints, the ball is limited to 2 translational DOFs (moving vertically z\nand horizontally x) and 1 rotational DOF (rotation about the dy direction; see (Fig. 1)). We\nincluded viscous damping in the translational and rotational DOFs of the ball to stabilize the\nsimulation and prevent numerical instabilities for the simulation of the rigid fingers.\nWe further limited the ball's movement in the x direction by adding stiffness to the ball.\nThe details of the simulation parameters, including the robotic hand and the ball) are shown in\nSupplementary Table S1.\nThe system's state vector includes the hand state vector (sh), consisting of fourteen kinematic\ndegrees of freedom (DOFs), along with the position and velocities of the hand's palm (sp) (2\nDOFs), and the position and velocity of the ball (s\u266d) (6 DOFs). This 20-dimensional vector\nencapsulates joint angles (q1 to 96) and their derivatives, as well as the vertical height of the\nhand (zh) and its derivative, collectively describing the dynamic state of the system.\nAdditionally, the ball state vector comprises vertical (z) and horizontal (x) translation, and\nits rotation about the y-axis (0\u2084). No other translations or rotations are permitted (see Sup-\nplementary Table S2). The height of the hand, zh, is actuated for the hand to reach for and\nmanipulate the state of the ball (sb) by rotating (0y) and lifting (25) it to a desired height (za).\nIt's important to note that not all state variables are utilized in our reinforcement learning\npolicy (observation state). Specifically, the observation state omits details about the ball's veloc-\nity and position, as explained in the following subsection. Furthermore, it's worth mentioning"}, {"title": "Autonomous Learning Approach", "content": "that the action space aligns with the observation state. When a 3D-force is introduced, the state\nof the system dynamically changes, augmenting the hand state with an additional 9 data points.\nTo autonomously learn in-hand manipulation of a ball against gravity through utilizing tactile\ninformation, we used a model-free RL algorithm to learn the policy. We used Proximal Policy\nOptimization (PPO) as our main algorithm as it presented a balance between the ease of imple-\nmentation, sample complexity, and ease of adjustment, trying to update at each step to minimize\nthe cost function while assuring that the new policies are not too far from last policies (52, 85).\nPPO has also been adopted as one of the default methods of OpenAI owing to its excellent\nperformance (86, 87)."}, {"title": "Reward Function", "content": "The reward engineering concept (a subset of RL) focuses on finding the most appropriate reward\nto maximize successful learning via reward shaping (88). Reward shaping involves carefully\ndesigning reward functions that provide the agent with rewards for progress toward the goal.\nIn our work", "Lift": "Our desired height (center of the\nball above the ground) is zd = 25 mm, shown in (Fig. 1). In our algorithm, the goal is reached\nwhen the agent supports the ball against gravity within a desired height range of [21, 29"}]}