{"title": "Double Successive Over-Relaxation Q-Learning with an Extension to Deep Reinforcement Learning", "authors": ["Shreyas S R"], "abstract": "Q-learning is a widely used algorithm in reinforcement learning (RL), but its convergence can be slow, especially when the discount factor is close to one. Successive Over-Relaxation (SOR) Q-learning, which introduces a relaxation factor to speed up convergence, addresses this issue but has two major limitations: In the tabular setting, the relaxation parameter depends on transition probability, making it not entirely model-free, and it suffers from overestimation bias. To overcome these limitations, we propose a sample-based, model-free double SOR Q-learning algorithm. Theoretically and empirically, this algorithm is shown to be less biased than SOR Q-learning. Further, in the tabular setting, the convergence analysis under boundedness assumptions on iterates is discussed. The proposed algorithm is extended to large-scale problems using deep RL. Finally, the tabular version of the proposed algorithm is compared using roulette and grid world environments, while the deep RL version is tested on a maximization bias example and OpenAI Gym environments.", "sections": [{"title": "I. INTRODUCTION", "content": "Reinforcement learning (RL) is a commonly employed approach for decision-making in a variety of domains, wherein agents learn from their interactions with the environment. The primary goal of RL is to find an optimal policy, which involves solving Bellman's optimality equation. Depending on the amount of information available, various algorithms are proposed in both dynamic programming and RL to solve Bellman's optimality equation. When the complete information of the Markov Decision Process (MDP) is available, techniques from dynamic programming, such as value iteration and policy iteration, are utilized to find the optimal value function, thereby, an optimal policy [1]. In cases where the MDP is not fully known (model-free), RL techniques are utilized [2]. Q-learning, introduced by Watkins in 1992, is a fundamental algorithm in RL used to find an optimal policy in model-free environments [3]. Despite its wide application in several areas (see, [4]\u2013[7]), Q-learning suffers from slow convergence and over-estimation bias. The drawbacks of Q-learning are well-documented in the literature, and considerable efforts are being made to overcome them (for instance, see [8]\u2013[12]).\nIn 1973, a successive over-relaxation (SOR) technique was proposed by D. Reetz to fasten the convergence of value iteration [13]. This technique involves constructing a modified Bellman equation with a successive over-relaxation parameter. Recently, this technique has been incorporated into Q-learning, resulting in a new algorithm called SOR Q-learning (SORQL) [10]. The underlying deterministic mapping of the SORQL algorithm has been shown to have a contraction factor less than that of the standard Q-Bellman operator [10]. This was achieved by selecting a suitable relaxation factor that depends on the transition probability of a self-loop of an MDP, making the algorithm partially model-free. Furthermore, the theory and our empirical results on benchmark examples have revealed that SOR Q-learning, similar to Q-learning, suffers from overestimation bias. To address these issues, we propose double SOR Q-learning (DSORQL), which combines the idea of double Q-learning [8] with SOR. The advantages of the proposed algorithm are that it is entirely model-free and has been shown, both theoretically and empirically, to have less bias than SORQL. In addition, this work provides the convergence analysis of the proposed algorithm using stochastic approximation (SA) techniques. Recently, the concept of SOR is applied to the well-known speedy Q-learning algorithm [11], leading to the development of a new algorithm called generalized speedy Q-learning, which is discussed in [14]. Further, in [15], the SORQL algorithm has been suitably modified to handle two-player zero-sum games, and consequently, a generalized minimax Q-learning algorithm was proposed. It is also worth noting that a deep RL version of the SOR Q-learning algorithm, similar to the deep Q-network (DQN) [16], is proposed in [17]. Additionally, the application of this deep RL variant of the SOR Q-learning algorithm is discussed for auto-scaling cloud resources [17].\nThe convergence of several RL algorithms depends on the theory developed in SA [18]\u2013[21]. Many of the iterative schemes in the literature assume the boundedness and proceed to show the convergence. For instance, in [15], the convergence of the generalized minimax Q-learning algorithm was shown to converge under the boundedness assumption. In this manuscript, we take the same approach to show the convergence of the proposed algorithms in the tabular setting. Motivated from the work in [16], and [17], wherein the double Q-learning and SOR Q-learning in the tabular version extended to large-scale problems using the function approximation. In this manuscript, we extend the tabular version of the proposed algorithm and present a double successive over-relaxation deep Q-network (DSORDQN). The proposed algorithm is tested on OpenAI gym's CartPole [22], LunarLander [22] environments, and a maximization example where the state space is in the order of $10^9$ [23]. The experiments corroborate the theoretical findings."}, {"title": "II. PRELIMINARIES", "content": "MDP provides a mathematical representation of the underlying control problem and is defined as follows: Let S be the set of finite states and A be the finite set of actions. The transition probability for reaching the state j when the agent takes action a in state i is denoted by $P_{ij}^{a}$. Similarly, let $r_{ij}^a$ be a real number denoting the reward the agent receives when an action a is chosen at state i, and the agent transitions to state j. Finally, the discount factor is \u03b3, where 0 < \u03b3 < 1. A policy \u03c0 is a mapping from S to \u2206(A), where \u2206(A) denotes the set of all probability distributions over A. The solution of the Markov decision problem is an optimal policy \u03c0*. The problem of finding an optimal policy \u03c0* reduces to finding the fixed point of the following operator U : $R^{|S\\times A|}$ \u2192 $R^{|S\\times A|}$ defined as\n$UQ(i, a) = r(i, a) + \\gamma \\sum_{j=1}^{|S|}P_{ij}^{a}MQ(j)$,\nwhere\n$r(i, a) = \\sum_{j=1}^{|S|}P_{ij}^{a}r_{ij}^{a}$, and $MQ(j) = \\max_{b \\in A} Q(j, b)$.\nTo simplify the equations, we defined the max-operator MQ(i) as $\\max_{a\\in A} Q(i, a)$ for $i \\in S$. Also, $R_{max} = \\max_{(s,a,s') }r_{ss'}^{a}$, $\\forall (s, a, s') \\in S \\times A \\times S$. These notations will be used throughout the manuscript as necessary. Note that U is a contraction operator with \u03b3 as the contraction factor, and $Q^*$ serves as the optimal action-value function of the operator U. Moreover, one can obtain an optimal policy \u03c0* from the $Q^*$, using the relation $\u03c0^*(i) \\in arg \\max_{a\\in A} Q^*(i)$, $\\forall i \\in S$. Q-learning algorithm is a SA version of Bellman's operator U. Given a sample {$i, a, j, r_a$}, current estimate $Q_n(i,a)$, and a suitable step-size $\u03b2_n(i, a)$, the update rule of Q-learning is as follows\n$Q_{n+1}(i, a) = Q_n(i, a) +\u03b2_n(i, a) (r_{ia}+MQ_n(j)-Q_n(i, a))$.\nUnder suitable assumptions, the above iterative scheme is shown to converge to the fixed point of U with probability one (w.p.1) [3], [18], [19]. Recently, in [10], a new Q-learning algorithm known as successive over-relaxation Q-learning (SORQL) is proposed to fasten the convergence of Q-learning. More specifically, a modified Bellman's operator is obtained using the successive over-relaxation technique. In other words, instead of finding the fixed point of U, the fixed point of the modified Bellman's operator $U_w$ : $R^{|S\\times A|}$ \u2192 $R^{|S\\times A|}$ is evaluated and is defined as\n$(U_wQ)(i, a)$\n$= w \\Big(r(i,a) + \\gamma \\sum_{j=1}^{|S|}P_{ij}^{a}MQ(j) \\Big) + (1 - w)MQ(i)$,\nwhere $0 < w < w^*$, and $w^* = \\min_{i,a}\\frac{1-\\gamma \\rho}{\\rho}$,  $\\rho = \\sum_{j=1}^{|S|}P_{ij}^{a}$.\nIn [10], it was shown that $U_w$ is $1 - w + w\\gamma$-contractive. Moreover, from Lemma 4, in [10] for $1 < w < w^*$ the contraction factor of the operator $U_w$ satisfy $1 - w + \\gamma w < \\gamma$. Further, it was shown that\n$MQ_w^*(i) = MQ^*(i), \\forall i \\in S$,\nwhere $Q^*$ and $Q_w^*$ are the fixed points of U and $U_w$, respectively. Under suitable assumptions, it was proved that the SORQL algorithm given by the update rule\n$Q_{n+1}(i, a)$\n$= (1 - \u03b2_n(i, a))Q_n(i, a) + \u03b2_n(i, a) \\Big[w \\Big(r_{ia} + \\gamma \\sum_{j=1}^{|S|}P_{ij}^{a}MQ_n(j)\\Big)$\n$+ (1 - w)MQ_n(i)\\Big]$,\nwhere $0 < w < w^*$, and $0 < \u03b2_n(i,a) \u2264 1$ converges to the fixed point of $U_w$ w.p.1 [10]. Note that the successive relaxation factor in the above equation depends on the transition probability, which makes it not entirely model-free. Owing to this and the natural concerns regarding the over-estimation of SORQL, This manuscript proposes a model-free variant of the SORQL algorithm and a model-free double SOR Q-learning algorithm. To prove the convergence of the proposed algorithms in the tabular setting, the following result from [21] will be useful, and we conclude this section by stating it here as lemma."}, {"title": "III. PROPOSED ALGORITHM", "content": "In this section, we first discuss the tabular version of the proposed algorithms, followed by a discussion of the deep RL version.\n\nThis subsection presents the model-free variants of the algorithm SOR Q-learning and double SOR Q-learning. As previously mentioned, the successive relaxation factor w is dependent on $P_{ij}^{a}$, which is generally unknown. To remove this dependency on the transition probability of a self-loop, the following scheme is proposed: For any i,j\u2208 S and a \u2208 A, let $Y_n[i][j][a]$ denote the number of times the states i, j and action a are visited till nth iteration. Further, we assume $Y_0[i][j][a] = 0, \u2200i, j, a \u2208 S \u00d7 S \u00d7 A$, and define\n$\\hat{P_{ij}^{a}}=\\frac{Y_n[i][j][a]}{V_n}$ $\\forall \\hat{P_{ij}^{a}} \\geq 1$.\nUsing the strong law of large numbers $\\hat{P_{ij}^{a}}$ \u2192 $P_{ij}^{a}$, \u2200 i, j, a as n \u2192 \u221e, w.p.1. Now for $w_0 \u2208 [\\frac{1-\\gamma \\rho}{\\rho}, 1]$, and a(n) satisfying step-size condition (2) in Lemma 1, we consider the following iterative scheme\n$w_{n+1} = w_n + \\alpha(n)\\Big[\\frac{1}{1 - \\gamma \\min \\hat{P_{ij}^{a}}} - w_n \\Big]$ $n \\geq 1$.\nWe rewrite the above iterative scheme as follows\n$w_{n+1} = w_n + \\alpha(n)(f(w_n) + e_n)$ $n \\geq 1$,\nwhere\n$f(w_n) = \\frac{1}{1 - \\gamma \\min \\hat{P_{ij}^{a}}} - w_n$, $e_n = \\frac{1}{1 - \\gamma \\min P_{ij}^{a}} - \\frac{1}{1 - \\gamma \\min \\hat{P_{ij}^{a}}}$\nSince $\\hat{P_{ij}^{a}}$ \u2192 $P_{ij}^{a}$ w.p.1, this implies $e_n$ \u2192 0 as n \u2192 \u221e w.p.1. Note that for any $w_1, w_2 \u2208 R$\n$|f(w_1) - f(w_2)| \u2264 |w_1 - w_2|$.\nThe iterate {$w_n$} in (4) track the the ODE, $ \\dot{w} = w^* - w$ [ [20], Section 2.2]. Let $f_{\\infty}(w) = \\lim_{r\\to\\infty}f(rw)$. The function $f_{\\infty}(w)$ exist and is equal to -w. Further, the origin and $w^*$ is the unique globally asymptotically stable equilibrium for the ODE $\\dot{w} = f_{\\infty}(w) = -w$ and $\\dot{w} = w^* - w$, respectively. As a result of the above observations and [20], one can obtain the following theorem\nTheorem 1. The iterative scheme defined in (4), satisfy $sup_n ||w_n|| < \\infty$ w.p.1. Further, $w_n$ \u2192 $w^*$ w.p.1 as n \u2192 \u221e.\nProof. The proof is a consequence of Theorem 7 in Chapter 3, and Theorem 2 - Corollary 4 in Chapter 2 [20].\nNote 1. Throughout this manuscript, the sequence $W_n$ is assumed to be updated using (4).\nRemark 1. The model-free SORQL (MF-SORQL) algorithm is nothing but Algorithm 1 in [10], with w replaced by $w_n$.\nRemark 2. If the sequence {$W_n$}n>0 in Algorithm 1 is a constant sequence, with w as the constant and 0 < w < w*, we refer to this algorithm as double SOR Q-learning (DSORQL). Therefore, given the samples {$i,a,j,r_a$} the update rule for the DSORQL is as follows:\nWith probability 0.5 update $Q^A$:\n$Q_{n+1}^A(i, a) = (1 \u2013 \u03b2_n)Q_n^A(i, a)$\n$+ \u03b2_n [w(r_{ia}+\\gamma Q_n^B (j, b^*)) + (1 \u2212 w)(Q_n^B(i, c^*))]$\nelse update $Q^B$:\n$Q_{n+1}^B(i, a) = (1 \u2013 \u03b2_n)Q_n^B(i, a)$\n$+ \u03b2_n [w(r_{ia} + \\gamma Q_n^A (j, d^*)) + (1 \u2212 w) (Q_n^A(i, e^*))],$"}, {"title": "IV. THEORETICAL ANALYSIS", "content": "At first, we discuss the convergence of model-free SOR Q-learning (Remark 1) under the following assumption: (A1):\n$||Q_n|| \u2264 B < \u221e$, $\u2200 n \u2265 0$.\nTheorem 2. Suppose (A1) holds. Given an MDP defined as in Section II, and $w_n$ as in (4). Let every state-action pair be sampled indefinitely. Then, for sample {$i, a, j,r_a$} the update rule of model-free SOR Q-learning algorithm given by:\n$Q_{n+1}(i, a) = (1 \u2013 \u03b2_n(i, a))Q_n(i, a)$\n$+ B_n(i, a) \\Big[w_n \\Big(r_{ia} + \\gamma \\sum_{j=1}^{|S|}P_{ij}^{a}MQ_n(j)\\Big)$\n$+(1 \u2013 w_n) (MQ_n(i))\\Big]$\nconverges w.p.1 to the fixed point of $U_{w^*}$, where $\\sum_{n}\u03b2_n(i, a) = \\infty$, $\\sum_{n} \u03b2_n^2(i, a) < \\infty$, and $0 \u2264 \u03b2_n(i, a) \u2264 1$.\nProof. The correspondence to Lemma 1 follows from associating X with the set of state-action pairs (i, a), $\u03b2_n(x)$ with $\u03b2_n(i, a)$, and $J_n(i, a)$ with $Q_n(i, a) \u2013 Q^* (i, a)$, where $Q^{**}$ is the fixed point of $U_{w^*}$. Let the filtration for this process be defined by, $G_n = {Q_0, i_j, a_j, \u03b2_j, w_j,\\forall j \u2264 n, n \u2265 0}$. Note that $G_n \u2286 G_{n+1}$, $\u2200 n \u2265 0$. The iteration of model-free SORQL is rewritten as follows:\n$\u03a6_{n+1}(i_n, a_n) = (1 - \u03b2_n(i_n, a_n)) \u03a6_n (i_n, a_n)$\n$+ B_n(i_n, a_n) \\Big[w_n \\Big(r_{i_na_n} + \\gamma \\sum_{j=1}^{|S|}P_{i_nj}^{a_n}MQ_n(i_{n+1})\\Big)$\n$+(1-w_n) (MQ_n(i_n)) - Q^* (i_n, a_n)\\Big].$\nWithout loss of generality, let $J_n(i,a) = w_n\\Big(r_{ia} +\\gamma \\sum_{j=1}^{|S|}P_{ij}^{a}MQ_n(j)\\Big) + (1 \u2212w_n)(MQ_n(i)) \u2212 Q^*(i, a)$. Also $\u03b2_0, \u03a6_0$ is $G_0$ measurable and $\u03b2_n, \u03a6_n, J_{n-1}$ are $G_n$ measurable for n \u2265 1. Define $J^R = w^* \\Big(r_{ia} +\\gamma \\sum_{j=1}^{|S|}P_{ij}^{a}MQ_n(j)\\Big) + (1 \u2212 w^*)(MQ_n(i))$. We now consider,\n$|E[J_n(i, a) | G_n]|$\n$= |E [J_n(i, a) + J^R \u2013 J^R + Q^{**} (i, a) \u2013 Q^* (i, a) | G_n]|$\n$\\leq\\Big|\\sum_{j=1}^{|S|} P_{ij}^{a} (w^* (r_{ia} + \\gamma MQ_n(j))$\n$+(1 \u2013 w^*)MQ_n(i)) \u2013 Q^* (i, a)\\Big|$\n$+ |W_n - w^* | | E [r_{ia} + \\gamma \\sum_{j=1}^{|S|}P_{ij}^{a}MQ_n(j) + MQ_n(i) | G_n] |$\n$\\leq|U_{w^*}Q_n(i, a) \u2013 U_{w^*}Q^* (i, a)|$\n$+|W_n - w^*|(R_{max}+2||Q_n||)$\n$<(1 \u2013 w^* + w^*\\gamma) || \u03a6_n|| + \u03b4_n,$\nwhere $\u03b4_n = |W_n - w^*|(R_{max}+2||Q_n||)$. From Theorem 1 and (A1), $\u03b4_n$ \u2192 0 as n \u2192 \u221e. Therefore, condition (3) of Lemma 1 holds. Now we verify the condition (4) of Lemma 1,\nVar$[J_n(i, a)|G_n]$\n$= E [(J_n(i,a) \u2013 E[J_n(i,a)|G_n])^2 |G_n]$\n$\\leq E \\Big[(w_n(r_{ia} + \\gamma \\sum_{j=1}^{|S|}P_{ij}^{a}MQ_n(j)) + (1 - w_n)(MQ_n(i))^2 | G_n\\Big]$\n$\\leq 3 (\\kappa R_{max} + \\kappa \\gamma^2||Q_n||^2 + \\kappa_1||Q_n||^2)$\n$\\leq 3 (\\kappa R_{max} + 2(\\kappa \\gamma^2 + \\kappa_1)(||\u03a6_n||^2 + ||Q^{**}||^2))$\n$\\leq 3 (\\kappa R_{max} + 2(\\kappa \\gamma^2 + \\kappa_1)(||\u03a6_n||^2 + 2(\\kappa \\gamma^2 + \\kappa_1)||Q^{w^*}||^2)$\n$\\leq (3\\kappa R_{max} +6(\\kappa \\gamma^2 + \\kappa_1)||Q^{w^*}||^2 + 6(\\kappa \\gamma^2 + \\kappa_1)||\u03a6_n||^2)$\n$\\leq C(1 + ||\u03a6_n||^2) \u2264 C'(1 + ||\u03a6_n||)^2,$\nwhere $C = \\max{3\\kappa R_{max}+6(\\kappa \\gamma^2+\\kappa_1)||Q^{w^*}||^2,6(\\kappa \\gamma^2+\\kappa_1)}$, $w < \\kappa$ and $(1 \u2013 W_n)^2 < \\kappa_1$. Therefore, all the conditions of Lemma 1 holds and hence $\u03a6_n$ \u2192 0 w.p.1, therefore, $Q_n \u2192 Q^*$ w.p.1.\nBefore going to discuss the convergence of the double SOR Q-learning algorithm mentioned in Remark 2. A characteri-zation result for the dynamics of $V_{n+1}^{BA}(i,a) := Q_{n+1}^B(i,a) - Q_{n+1}^A(i, a)$ will be presented.\nLemma 2. Consider the updates $Q^A$ and $Q^B$ as in Remark 2. Then $E[V_{n+1}^{BA}(i, a)|G_n]$ converges to zero w.p.1 as n \u2192 \u221e.\nProof. Suppose $V_{n+1}^{BA}(i, a) := Q_{n+1}^B(i, a) \u2013 Q_{n+1}^A(i, a)$. Then Remark 2 indicates that at each iteration, either $Q^A$ or $Q^B$ is updated with equal probability. If $Q^A$ is getting updated at $(n + 1)th$ iteration, then we have\n$V_{n+1}^{BA}(i, a)$\n$= Q_n^B (i, a) \u2013 ((1 \u2013 \u03b2_n)Q_n^A(i, a) + \u03b2_n \\Big(w(r_{ia} + \\gamma Q_n^B (j, b^*))$\n$+ (1 - w)Q_n^B(i, c^*)\\Big))$\n$= (1 \u2013 \u03b2_n)V_n^{BA}(i, a) + \u03b2_n \\Big(Q_n^B (i, a) \u2013 w(r_{ia} + \\gamma Q_n^B (j, b^*))$\n$\u2013 (1 \u2013 w) Q_n^B(i, c^*)\\Big)$.\nSimilarly if $Q^B$ is getting updated at $(n + 1)th$ iteration, then\n$V_{n+1}^{BA}(i, a) = (1 - \u03b2_n)Q_n^B (i, a) + \u03b2_n \\Big(w(r_{ia} + \\gamma Q_n^A (j, d^*))$\n$+ (1 - w) Q_n^A(i, e^*)\\Big) - Q_n^A(i, a)$\n$= (1 \u2013 \u03b2_n)V_n^{BA}(i,a) + \u03b2_n \\Big(w(r_{ia} + \\gamma Q_n^A (j, d^*))$\n$+ (1 - w) Q_n^A(i, e^*)\\Big) \u2013 Q_n^A(i, a)$.\nCombining (i) and (ii), we have\n$V_{n+1}^{BA}(i, a) = (1 \u2013 \u03b2_n)V_n^{BA}(i, a) + \u03b2_n J_n^A(i, a)$,\n$V_{n+1}^{BA}(i, a) = (1 \u2013 \u03b2_n)V_n^{BA}(i, a) + \u03b2_n J_n^B(i, a)$,\nwhere\n$J_n^A (i, a) = Q_n^B (i, a)-w(r_{ia}+\\gamma Q_n^B (j, b^*))-(1-w) Q_n^B(i, c^*)$,\n$J_n^B (i, a) = w(r_{ia}+\\gamma Q_n^A (j, d^*))+(1-w) Q_n^A(i, e^*))-Q_n^A(i, a)$.\nTherefore,\n$E[V_{n+1}^{BA}(i, a) |G_n]$\n$=(1-\\beta_n)E[V_n^{BA}(i,a)|G_n] + P_j E[w\\gamma(Q_n^A(j,d^*)$\n$- Q_n^B (j, b^*)) + (1 \u2212 w)(Q_n^A (i, e*) \u2013 Q_n^B(i, c^*))|G_n].$\nLet $\\frac{\\gamma}{2}= C_n$. Then, the above equation is reduced to\n$E[V_{n+1}^{BA}(i, a) |G_m] = (1-C_n)E[V_n^{BA}(i,a)|G_n] + C_n E[V_n^{BA}(i, a)$,"}, {"title": "V. NUMERICAL EXPERIMENTS", "content": "In this section, the tabular and deep RL versions of the proposed algorithms are compared on benchmark examples.\nThe code for all the numerical experiments in this manuscript is publicly available and can be accessed in the author's GitHub repository [25].\nThis subsection provides numerical comparisons between Q-learning (QL), double Q-learning (DQL), model-free SOR Q-learning (MF-SORQL), and model-free double SOR Q-learning (MF-DSORQL). More specifically, we compare the above-mentioned algorithms on benchmark examples used in [8], namely roulette and grid world. Note that the step size for all the algorithms is the same, and all the other parameters used in the experiments are similar to [8].\nIn roulette, a player can choose from 170 different betting actions, such as betting on a specific number or on the color black or red. Each bet has a payout designed so that the expected return is approximately $0.947 per dollar wagered, resulting in an expected loss of $-0.053 per play. This is modeled as an MDP with one state and 171 actions, where one action is not placing a bet, ending the episode. If the player chooses not to bet, the payout is $0. Assuming the player bets $1 each time, the optimal strategy is to refrain from gambling, resulting in an expected profit of $0. In this example, the choice of successive relaxation factor w is known as we have only one state. More specifically, 0 < w < $w^* = \\frac{1-\\gamma \\min_{i,a} P_{ij}^{a}}{P}$ = $\\frac{1}{1-1\\gamma}$ = 115. Therefore, both MF-DSORQL and MF-SORQL are the same as that of DSORQL and SORQL, respectively. The experiment is conducted with \u03b3 = 0.95, and the polynomial step-size mentioned in [8]. Fig. 1,2,3, and 4, presents the behavior of the proposed algorithms for various choices of SOR parameters and compares them with QL and DQL. Each experiment consists of 100000 trails, and the graph is plotted by taking the average over 10 independent experiments. It is evident from the plot that the performance of DSORQL is consistent for various choices of w, whereas the performance of the SORQL algorithm improves as w gets closer to w*. Further, the over-estimation bias of the SORQL algorithm is visible when w = 1.3 (Fig. 1) and w = 5 (Fig. 2). It is also interesting to see that, in Fig. 4., the over-estimation bias is not visible as this is a special case wherein w = w* = 1, and since the next state j will be i, Equation (6) reduces to the following:\n$Q_{n+1}(i, a) = (1 \u2212 \u03b2_n(i, a))Q_n(s, a) + \u03b2_n(i, a) \\frac{r_{ia}}{(\\frac{1-1}{r})}$.\nTherefore, in this particular case, the maximum operator gets canceled, and this is the reason for the behavior in Fig. 4. Note that in this classic example, the performance of the proposed DSORQL and DQL are almost the same.\nConsider a 3 \u00d7 3 grid world MDP. The agent can choose from four actions along four directions. The agent starts in the fixed starting state from one corner, with the goal state in the diagonally opposite corner. If the agent's actions lead off the grid, it will stay in the same state. For every non-terminal"}, {"title": "VI. CONCLUSION", "content": "Motivated by the recent success of SOR methods in Q-learning and the issues concerning its over-estimation bias, this manuscript proposes a double successive over-relaxation Q-learning. Both theoretically and empirically, the proposed algorithm demonstrates a lower bias compared to SOR Q-learning. In the tabular setting, convergence is proven under suitable assumptions using SA techniques. A deep RL version of the method is also presented. Experiments on standard examples, including roulette, grid-world, and popular gym environments, showcase the algorithm's performance. It is worth mentioning that, similar to the boundedness of iterates observed in Q-learning [27], investigating the boundedness of the proposed algorithms in the tabular setting presents an interesting theoretical question for future research."}]}