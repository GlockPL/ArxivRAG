{"title": "Optimus-1: Hybrid Multimodal Memory Empowered Agents Excel in Long-Horizon Tasks", "authors": ["Zaijing Li", "Yuquan Xie", "Rui Shao", "Gongwei Chen", "Dongmei Jiang", "Liqiang Nie"], "abstract": "Building a general-purpose agent is a long-standing vision in the field of artificial intelligence. Existing agents have made remarkable progress in many domains, yet they still struggle to complete long-horizon tasks in an open world. We attribute this to the lack of necessary world knowledge and multimodal experience that can guide agents through a variety of long-horizon tasks. In this paper, we propose a Hybrid Multimodal Memory module to address the above challenges. It 1) transforms knowledge into Hierarchical Directed Knowledge Graph that allows agents to explicitly represent and learn world knowledge, and 2) summarises historical information into Abstracted Multimodal Experience Pool that provide agents with rich references for in-context learning. On top of the Hybrid Multimodal Memory module, a multimodal agent, Optimus-1, is constructed with dedicated Knowledge-guided Planner and Experience-Driven Reflector, contributing to a better planning and reflection in the face of long-horizon tasks in Minecraft. Extensive experimental results show that Optimus-1 significantly outperforms all existing agents on challenging long-horizon task benchmarks, and exhibits near human-level performance on many tasks. In addition, we introduce various Multimodal Large Language Models (MLLMs) as the backbone of Optimus-1. Experimental results show that Optimus-1 exhibits strong generalization with the help of the Hybrid Multimodal Memory module, outperforming the GPT-4V baseline on many tasks.", "sections": [{"title": "1 Introduction", "content": "Optimus Prime faces complex tasks alongside humans in Transformers to protect the peace of the planet. Creating an agent like Optimus that can perceive, plan, reflect, and complete long-horizon tasks in an open world has been a longstanding aspiration in the field of artificial intelligence [31, 15, 12]. Early research [1, 6, 17] developed simple agents by constructing policy networks. Recent works [34, 39, 37] have utilized Large Language Models (LLMs) as action planners for agents, generating executable sub-goal sequences for low-level action controllers. Leveraging the powerful instruction-following and logical reasoning capabilities of (Multimodal) LLMs, LLM-based agents have achieved remarkable success across multiple domains [13, 8, 9, 42]. Nevertheless, the ability of these agents to complete long-horizon tasks still falls significantly short of human-level performance.\nAccording to relevant studies [19, 29, 33], the human ability to complete long-horizon tasks in an open world relies on long-term memory storage, which is divided into knowledge and experience. The storage and utilization of knowledge and experience play a crucial role in guiding human behavior and enabling humans to adapt flexibly to their environments in order to accomplish long-horizon tasks. Inspired by this theory, we summarize the challenges faced by current agents as follows:\nInsufficient Exploration of Structured Knowledge: Structured knowledge, encompassing open world rules, object relationships, and interaction methods with the environment, is essential for agents to complete complex tasks [25, 31]. However, existing agents [1, 17, 6] only learn dispersed knowledge from video data and are unable to efficiently represent and learn this structured knowledge, rendering them incapable of performing complex tasks.\nLack of Multimodal Experience: Humans derive successful strategies and lessons from information on historical experience [7, 23], which assists them in tackling current complex tasks. In a similar manner, agents can benefit from in-context learning with experience demonstrations [30, 41]. However, existing agents [34, 38, 24] only consider unimodal information, which prevents them from learning from multimodal experience as humans do.\nTo address the aforementioned challenges, we propose Hybrid Multimodal Memory module that consists of Hierarchical Directed Knowledge Graph (HDKG) and Abstracted Multimodal Experience Pool (AMEP). For HDKG, we map the logical relationships between objects into a directed graph structure, thereby transforming knowledge into high-level semantic representations. HDKG efficiently provides the agent with the necessary knowledge for task execution, without requiring any parameter updates. For AMEP, we dynamically summarize and store the multimodal information (e.g., environment, agent state, task plan, video frames, etc.) from the agent's task execution process, ensuring that historical information contains both a global overview and local details. Different from the method of directly storing successful cases as experience [39], AMEP considers both successful and failed cases as references. This innovative approach of incorporating failure cases into in-context learning significantly enhances the performance of the agent.\nOn top of the Hybrid Multimodal Memory module, we construct a multimodal composable agent, Optimus-1. As shown in Figure 1, Optimus-1 consists of Knowledge-Guided Planner, Experience-Driven Reflector, and Action Controller. To enhance the ability of agents to cope with complex environments and long-horizon tasks, Knowledge-Guided Planner incorporates visual observation into the planning phase, leveraging HDKG to capture the knowledge needed. This allows the agent to efficiently transform tasks into executable sub-goals. Action Controller takes the sub-goal and the current observation as inputs and generates low-level actions, interacting with the game environment to update the agent's state. In open-world complex environments, agents are prone to be erroneous"}, {"title": "2 Optimus-1", "content": "To build an agent that can perform long-horizon tasks in an open world like a human, we develop a multimodal composable agent, Optimus-1, in Minecraft. Optimus-1 consists of Hybrid Multimodal Memory, Knowledge-Guided Planner, Experience-Driven Reflector, and Action Controller."}, {"title": "2.1 Hybrid Multimodal Memory", "content": "In order to endow agent with a long-term memory storage mechanism [19, 33], we propose the Hybrid Multimodal Memory module, which consists of Abstracted Multimodal Experience Pool (AMEP) and Hierarchical Directed Knowledge Graph (HDKG)."}, {"title": "2.1.1 Abstracted Multimodal Experience Pool", "content": "Relevant studies [4, 20, 16, 14] highlight the importance of historical information for agents completing long-horizon tasks. Minedojo [6] and Voyager [34] employed unimodal storage of historical information. Jarvis-1 [39] used a multimodal experience mechanism, storing task planning and visual information without summarization, leading to storage and retrieval challenges. To address this, we propose AMEP, which aims to summarize all multimodal information during the agent's execution of the task. It can maintain long sequential data integrity, and improve storage and retrieval efficiency.\nSpecifically, as depicted in Figure 2, to conduct the static visual information abstraction, the video stream captured by Optimus-1 during task execution is first input to a video buffer, filtering the stream at a fixed frequency of 1 frame per second. Based on the filtered video frames, to further perform a dynamic visual information abstraction, these frames are then fed into an image buffer with a window size of 16, where the image similarity is dynamically computed and final abstracted frames are adaptively updated. To align such abstracted visual information with the corresponding textual sub-goal, we then utilize MineCLIP [6], a pre-trained video-text alignment model, to calculate their multimodal correlation. When this correlation exceeds a threshold, the corresponding image buffer and textual sub-goal are saved as multimodal experience into a pool. Finally, we further incorporate environment information, agent initial state, and plan generated by Knowledge-Guided Planner, into such a pool, which forms the AMEP. In this way, we consider the multimodal information of each sub-goal, and summarise it to finally compose the multimodal experience of the given task."}, {"title": "2.1.2 Hierarchical Directed Knowledge Graph", "content": "World knowledge is essential for an agent's ability to perform long-horizon complex tasks. Instead of implicit learning through fine-tuning [24, 45], we propose HDKG, enabling agents to learn explicitly by retrieving knowledge graph.\nMinecraft's mining and crafting represent a complex knowledge network crucial for effective task planning. For instance, crafting a diamond sword requires two diamonds and one wooden stick \u2713, while mining diamonds requires an iron pickaxe, which involving further materials and steps. To enable a more structured knowledge representation, as shown in the Figure 2, we transform this knowledge into a graph $D(V, E)$, where nodes set $V$ represent objects, and directed edges set $E$ point to nodes that can be crafted by this object. An edge $e \\in E$ in the $D$ can be represented as $e = (u, v)$, where $u, v \\in V$. The directed graph efficiently stores and updates knowledge. For a given object $x$, retrieving the corresponding node allows extraction of a sub-graph $D_j(V_j, E_j) \\in D$, where nodes set $V_j$ and edges set $E_j$ can be formulated as:\n$V_j = \\{v \\in V \\mid x\\}$, $E_j = \\{e = (u, v) \\in V \\mid u \\in V_j \\cup v \\in 1 V_j \\}$,                                                                      (1)\nThen by topological sorting, we can get all the materials and their relationships needed to complete the task. This knowledge is provided to the Knowledge-Guided Planner as a way to generate a more reasonable sequence of sub-goals. With HDKG, we can significantly enhance the world knowledge of the agent in a train-free manner."}, {"title": "2.2 Optimus-1: Framework", "content": "Relevant studies indicate that the human brain is essential for planning and reflection, while the cerebellum controls low-level actions, both crucial for complex tasks [27, 28]. Inspired by this, we divide the structure of Optimus-1 into Knowledge-Guided Planner, Experience-Driven Reflector, and Action Controller. In a given game environment with a long-horizon task, the Knowledge-Guided Planner senses the environment, retrieves knowledge from HDKG, and decomposes the task into executable sub-goals. The action controller then sequentially executes these sub-goals. During execution, the Experience-Driven Reflector is activated periodically, leveraging historical experience from AMEP to assess whether Optimus-1 can complete the current sub-goal. If not, it instructs the Knowledge-Guided Planner to revise its plan. Through iterative interaction with the environment, Optimus-1 ultimately completes the task.\nKnowledge-Guided Planner. Open-world environments vary greatly, affecting task execution. Previous approaches [38] using LLMs for task planning failed to consider the environment, leading to the failure of tasks. For example, an agent in a cave aims to catch fish. It lacks visual information to plan conditions on the current situation, such as \"leave the cave and find a river\". Therefore, we integrate environmental information into the planning stage. Unlike Jarvis-1 [39] and MP5 [24], which convert observation to textual descriptions, Optimus-1 directly employs observation as visual conditions to generate environment-related plans, i.e., sub-goal sequences. This results in more comprehensive and reasonable planning. More importantly, Knowledge-Guided Planner retrieves the knowledge needed to complete the task from HDKG, allowing task planning to be done once, rather than generating the next step in each iteration. Given the task $t$, observation $o$, the sub-goals sequence $q_1, q_2, q_3, ..., q_n$ can be formulated as:\n$q_1, q_2, q_3, \u2026, q_n = p_o(o, t, p_{\\eta}(t))$,                                                                                                                                                           (2)\nwhere $n$ is the number of sub-goals, $p_{\\eta}$ denotes sub-graph retrieved from HDKG, $p_o$ denotes MLLM. In this paper, we employ OpenAI's GPT-4V as Knowledge-Guided Planner and Experience-Driven Reflector. We also evaluate other alternatives of GPT-4V, such as open-source models like Deepseek-VL [18] and InternLM-XComposer2-VL [5] in Section 3.4."}, {"title": "3 Experiments", "content": "3.1 Experiments Setting\nEnvironment. To ensure realistic gameplay like human players, we employ MineRL [10] with Minecraft 1.16.5 as our simulation environment. The agent operates at a fixed speed of 20 frames per second and only interacts with the environment via low-level action control signals of the mouse and keyboard. For more information about the detailed descriptions of the observation and action spaces, please refer to the Appendix C.\nBenchmark. We constructed a benchmark of 67 tasks to evaluate the Optimus-1's ability to complete long-horizon tasks. As illustrated in Table 5, we divide the 67 Minecraft tasks into 7 groups according to recommended categories in Minecraft. Please refer to Appendix E for more details."}, {"title": "3.3 Ablation Study", "content": "We conduct extensive ablation experiments on 18 tasks, experiment setting can be found in Table 6. As shown in Table 2, we first remove Knowledge-Driven Planner and Experience-Driven Reflector, the performance of Optimus-1 on all task groups drops dramatically. It demonstrates the necessity of Knowledge-Guided Planner and Experience-Driven Reflector modules for performing long-horizon tasks. As for Hybrid Multimodal Memory, we remove HDKG from Optimus-1. Without the help of world knowledge, the performance of Optimus-1 decreased by an average of 20% across all task groups. We then removed AMEP, this resulted in the performance of Optimus-1 decreased by an average of 12%. Finally, we performed ablation experiments on the way of retrieving cases from AMEP. As shown in Table 3, without retrieving cases from AMEP, the success rate shows an average of 10% decrease across all groups. It reveals that this reflection mechanism, which considers both success and failure cases, has a significant impact on the performance of Optimus-1. To illustrate the role of the reflection mechanism, we have shown some cases in Figure 4."}, {"title": "3.4 Generalization Ability", "content": "In this section, we explore an interesting issue: whether generic MLLMs can effectively perform various long-horizon complex tasks in Minecraft using Hybrid Multimodal Memory. As shown in Figure 5, We employ Deepseek-VL [18] and InternLM-XComposer2-VL [5] as Knowledge-Guided Planner and Experience-Driven Reflector. The experimental results show that the original MLLM has low performance on long-horizon tasks due to the lack of knowledge and experience of Minecraft. With the assistance of Hybrid Multimodal Memory, the performance of MLLMs has improved by 2 to 6 times across various task groups, outperforming the GPT-4V baseline. This encouraging result demonstrates the generalization of the proposed Hybrid Multimodal Memory."}, {"title": "3.5 Self-Evolution via Hybrid Multimodal Memory", "content": "As shown in Section 2.3, we randomly initialize the Hybrid Multimodal Memory of Optimus-1, then update it multiple times by using the \"free exploration-teacher guidance\" learning method. We set the epoch to 4, and the number of learning tasks to 160. At each period, Optimus-1 performs free exploration on 150 tasks and teacher guidance learning on the remaining 10 tasks, we then evaluate Optimus-1's learning ability on the task groups same as ablation study. Experimental results are shown in Figure 5. It reveals that Optimus-1 keeps getting stronger through the continuous expansion of memory during the learning process of multiple periods. Moreover, it demonstrates that MLLM with Hybrid Multimodal Memory can incarnate an expert agent in a self-evolution manner [32]."}, {"title": "4 Related Work", "content": "Agents in Minecraft. We summarise the differences of existing Minecraft agents in the Appendix E.3. Earlier work [21, 43, 2, 3] introduced policy models for agents to perform simple tasks in Minecraft. MineCLIP [6] used text-video data to train a contrastive video-language model as a reward model for policy, while VPT [1] pre-trained on unlabelled videos but lacked instruction as input. Building on VPT and MineCLIP, STEVE-1 [17] added text input to generate low-level action sequences from human instructions and images. However, these agents struggle with complex tasks due to limitations in instruction comprehension and planning. Recent work [37, 34, 46] incorporated LLMs as planning and reflection modules, but lacked visual information integration for adaptive planning. MP5 [24], MineDreamer [45], and Jarvis-1 [39] enhanced situation-aware planning by obtaining textual descriptions of visual information, yet lacked detailed visual data. Optimus-1 addresses these issues by directly using observation as situation-aware conditions in the planning phase, enabling more rational, visually informed planning. Additionally, unlike other agents requiring multiple queries for task refinement, Optimus-1 generates a complete and effective plan in one step with the help of HDKG. This makes Optimus-1 planning more efficient.\nMemory in Agents. In the agent-environment interaction process, memory is key to achieving experience accumulation, environment exploration, and knowledge abstraction [44]. There are two forms to represent memory content in LLM-based agents: textual form [16, 14, 22] and parametric form [4, 20, 35]. In textual form, the information is explicitly retained and recalled by natural languages. In parametric form, the memory information is encoded into parameters and implicitly influences the agent's actions. Recent work [36, 40, 11] has explored the long-term visual information storage and summarisation in MLLM. Our proposed hybrid multimodal memory module is plug-and-play and can provide world knowledge and multimodal experience for Optimus-1 efficiently."}, {"title": "5 Conclusion", "content": "In this paper, we propose Hybrid Multimodal Memory module, which consists of two parts: HDKG and AMEP. HDKG provides the necessary world knowledge for the planning phase of the agent, and AMEP provides the refined historical experience for the reflection phase of the agent. On top of the Hybrid Multimodal Memory, we construct the multimodal composable agent, Optimus-1, in Minecraft. Extensive experimental results show that Optimus-1 outperforms all existing agents on long-horizon tasks. Moreover, we validate that general-purpose MLLMs, based on Hybrid Multimodal Memory and without additional parameter updates, can exceed the powerful GPT-4V baseline. The extensive experimental results show that Optimus-1 makes a major step toward a general agent with a human-like level of performance."}, {"title": "A Broader Impact", "content": "With the increasing capability level of Multimodal Large Language Models (MLLM) comes many potential benefits and also risks. On the positive side, we anticipate that the techniques that used to create Optimus-1 could be applied to the creation of helpful agents in robotics, video games, and the web. This plug-and-play architecture that we have created can be quickly adapted to different MLLMs, and the proposed methods also provide a viable solution for other application areas in the agent domain. However, on the negative side, it is imperative to acknowledge the inherent stochastic nature of MLLMs in text generation. If not addressed carefully, this could lead to devastating consequences for society. Prior to deploying MLLMs in conjunction with the Hybrid Multimodal Memory methodology, a comprehensive assessment of their potential risks must be undertaken. We hope that while the stakes are low, works such as ours can improve access to safety research on instruction-following models in multimodal agents domains."}, {"title": "B Limitation and Future Work", "content": "In the framework of Optimus-1, we are dedicated to leverage proposed Hierarchical Directed Knowledge Graph and Abstracted Multimodal Experience Pool can be used to enhance the agent's ability to plan and reflect. For Action Controller, we directly introduce STEVE-1 [17] as a generator of low-level actions. However, limited by STEVE-1's ability to follow instructions and execute complex actions, Optimus-1 is weak in completing challenging tasks such as \"beat ender dragon\" and \"build a house\".\nIn order to further train STEVE-1's low-level action generation ability, high-quality video-action data is indispensable. With the help of our highly scalable and adaptable architecture, we can obtain such high quality video data during inference process of Optimus-1. In our future work, we will strengthen the ability of Action Controller through further training."}, {"title": "C Minecraft", "content": "Minecraft is an extremely popular sandbox video game developed by Mojang Studios *. It allows players to explore a blockly, procedurally generated 3D world with infinite terrain, discover and extract raw materials, craft tools and items, and build structures or earthworks(shown in Figure 6). In this game, AI agents need to face situations that are highly similar to the real world, making judgments and decisions to deal with various environments and problems. Therefore, Minecraft is a very suitable environment to be used as an AI testing simulator."}, {"title": "C.1 Basic Rules", "content": "Biomes. The Minecraft world is divided into different areas called \"biomes\". Different biomes contain different blocks and plants and change how the land is shaped. There are 79 biomes in Minecraft 1.16.5, including ocean, plains, forest, desert, etc. Diverse environments have high requirements for the generalization of agents.\nTime. Time passes within this world, and a game day lasts for 20 real-world minutes. Nighttime is much more dangerous than daytime: the game starts at dawn, and agents have 10 minutes of game time before nightfall. Hostile or neutral mobs spawn when night falls, and most of these mobs are dangerous, trying to attack agents. How to survive in such a dangerous world is an open problem for Minecraft agents research.\nItem. In Minecraft 1.16.5, there are 975 items can be obtained, such as wooden pickaxe, iron sword. Item can be obtained by crafting or destroying blocks or attacking entities. For example, agent can attack cows to obtain leather and beef. Agent also can use 1 stick / and 2 diamonds\nto craft diamond sword.\nGameplay progress. Progression primarily involves discovering and utilizing various materials and resources, each of which unlocks new capabilities and options. For instance, crafting a wooden pickaxe enables the player to mine stone, which can be used to create a stone pickaxe"}, {"title": "C.2 Observation and Action Spaces", "content": "Observation. Our observation space is completely consistent with human players. The agent only receives an RGB image with dimensions of 640 \u00d7 360 during the gameplay process, including the hotbar, health indicators, food saturation, and animations of the player's hands. It is worth helping the agent see more clearly in extremely dark environments, we have added a night vision effect for the agent, which increases the brightness of the environment during the night."}, {"title": "C.3 Long-horizon Tasks", "content": "Long-horizon Tasks are complex tasks that require world knowledge to solve and consist of multiple indispensable subtask sequences. In Minecraft, technology has six levels, including wood, stone\u2764, iron, golden, diamond, and netherite. Wooden tools can mine stone-level blocks, but can't mine iron-level and upper-level blocks. Stone tools can mine iron-level blocks, but can't mine diamond-level and upper-level blocks. Iron-level tools can mine diamond-level blocks, but can't mine netherite-level blocks. Diamond-level tools can mine any level blocks.\nFor example, the agent now wants to complete the task \"Craft iron sword\". The agent needs to craft wood-level tools to mine stone, and craft stone-level tools to mine iron ore. In order to craft tools, the agent needs a crafting table. To smelt iron ore into iron ingot, the agent needs a furnace. Moreover, craft crafting table needs 4 planks, and craft furnace needs 8 cobblestone. In summary, the agent needs to obtain many raw materials, wood-level and stone-level tools, 1 crafting table, 1 furnace, and most importantly, 2 iron ingots. The process of this task is shown in Figure 7."}, {"title": "D Theory", "content": "In this section, we briefly introduce the relevant theory of cognitive science. For more details, please refer to the original articles.\nOur ability to understand and predict the world around us depends on our long-term memory stores, which have historically been divided into two distinct systems [19, 29, 33]. The semantic memory system provides a conceptual framework for describing the similar meanings of words and objects as they are encountered in different contexts (e.g., a bee is a flying insect with yellow and black stripes that produces honey), whereas the episodic memory system records our personal experiences characterized by the co-occurrence of words and objects at different times and places (e.g., being stung by a bee while eating honey at a picnic last weekend). These information stores and the interactions between them play a crucial role in guiding our behaviour and giving us the flexibility to adapt to the various demands of our environment."}, {"title": "E Benchmark Suite", "content": "E.1 Benchmark\nWe constructed a benchmark of 67 tasks to evaluate Optimus-1's ability to complete long-horizon tasks in Minecraft. According to recommended categories in Minecraft, we have classified these tasks into 7 groups: Wood, Stone, Iron, Gold, Diamond, Redstone, Armor. The statistics for benchmark are shown in Table 5. Due to the varying complexity of these tasks, we adopt different maximum gameplay steps (Max. Steps) for each task. The maximum steps are determined by the average steps that human players need to complete the task. Due to the randomness of Minecraft, the world and initial spawn point of the agent could vary a lot. In our benchmark setting, We initialize the agent with an empty inventory, which makes it necessary for the agent to complete a series of sub-goals (mining materials, crafting tools) in order to perform any tasks. This makes every task challenging, even for human players.\nNote that Diamonds are a very rare item that only spawns in levels 2 to 16 and have a 0.0846% chance of spawning in Minecraft 1.16.5. Diamonds are usually found near level 9, or in man-made or natural mines no higher than level 16. In order to reduce the huge impact that diamond generation probability has on agent's likelihood of completing a task, we have adjusted the diamond generation probability to 20%, spawns in levels 2 to 16. This setting applies to human players as well.\nIn the ablation study, we select the subset of our benchmark as the test set (shown in Table 6). The environment setting is the same as the benchmark."}, {"title": "F Implementation Details", "content": "F.1 Hybrid Multimodal Memory\nF.1.1 Abstracted Multimodal Experience Pool\nRelevant studies [4, 20, 16, 14] have demonstrated the importance of memory for agents to complete long-horizon tasks. To implement the memory mechanism, Minedojo [6] and Voyager [34] only considered unimodal storage of historical information. Jarvis-1 [39] considered a multimodal memory mechanism to store task planning and visual information as experience, but it stores all historical information without summarisation. This approach stores all visual images, which poses a huge challenge in storage size and retrieval efficiency. To solve the problem, we propose the Abstracted Multimodal Experience Pool structure, which summarizes all historical information during the agent's execution of the task, which maintains the integrity of long sequential information and greatly improves the storage and retrieval efficiency of the experience.\nAs shown in Figure 2, we first input the visual image stream captured by Optimus-1 during the execution of the task to the video buffer, which filters the image stream at a fixed frequency, which makes the length of the image stream substantially shorter. Empirically, we set the frequency of filtering to 1 second/frame, meaning that the video buffer takes one frame per second from the original image stream to compose the filtered image stream. We found that above this frequency makes the visual information redundant (too much similarity between images), and below this frequency does not preserve enough complete visual information. We then feed the filtered frames into an image buffer with a window size of 16. We dynamically compute the similarity between images in the image buffer, when a new image comes in, we compute the similarity between the new image and the most recent image, and then we remove the image with the highest similarity in order to keep the image buffer's window size to 16. We then introduce MineCLIP [6], a pre-trained model of video-text alignment with a structure similar to CLIP [26], as our visual summariser. For a given sub-goal, it calculates the correlation between the visual content within the current memory bank and the sub-goal, and when this correlation exceeds a pre-set threshold, the frames within the memory bank are saved as the visual memories corresponding to that sub-goal. Finally, we store the visual memories with the sub goal's textual description into the Abstracted Multimodal Experience Pool. In addition, we incorporate the environment information, agent initial state, plan from Knowledge-Guided Planner, etc. into the experience memory of the given task. In this way, we consider the history information of each sub-goal and summaries and summarise it to finally compose the multimodal experience of the given task.\nNote that we also store these visual memories as failure cases when the feedback from the reflection phase is REPLAN. Therefore, when Optimus-1 executes a long-horizon task, it can retrieve past successes and failures as references and update memory after the task is finished. In the reflection phase, Optimus-1 retrieve the most relevant cases from Abstracted Multimodal Experience Pool, which contains the three scenarios COMPLETE, CONTINUE, and REPLAN, to help the agent better assess which state the current situation belongs to. This approach of considering both successful and failed cases for in-context learning is inspired by related research [7, 23], and its effectiveness is validated in Section 3.3.\nF.1.2 Hierarchical Directed Knowledge Graph\nWorld knowledge is the foundation and key to an agent's ability to perform long-horizon complex tasks. Unlike allowing agents to learn world knowledge implicitly through fine-tuning [24, 45], we propose a method for constructing and transforming hierarchical world knowledge into graph structure that allows agents to learn knowledge explicitly in a more efficient manner.\nMining and crafting objects in Minecraft is a complex and large knowledge network, and has a significant impact on the ability of the agent to generate effective task planning. As shown in the Figure 2, crafting a diamond sword requires two diamonds and a wooden stick /, while mining diamonds requires an iron pickaxe , which in turn requires additional raw materials and crafting steps. We transform this mine and craft knowledge into a graph structure, where the nodes of the graph are objects, and the nodes point to objects that can be crafted or completed by that object. With directed graph, we show that connections between objects are established, and that this knowledge can be stored and updated efficiently. For a given object, we only need to retrieve the corresponding node to extract the corresponding subgraph from the knowledge graph. Then by topological sorting, we can get the antecedents and required materials for the object, and this information is provided to the Knowledge-Guided Planner as a way to generate a more reasonable sequence of sub-goals. With Hierarchical Directed Knowledge Graph, we can significantly enhance the world knowledge of the agent in a train-free manner, as shown in the experimental results in Section 3.3."}, {"title": "F.2 Hybrid Multimodal Memory Driven Optimus-1", "content": "In order to implement the proposed Hybrid Multimodal Memory and to progressively increase the capacity of Optimus-1 in a self-evolution manner, we propose a non-parametric learning method named \"free exploration-teacher guidance\". In the free exploration phase, we randomly initialize Optimus-1's equipment and tasks, and ask it to explore freely in a randomly set environment, and acquire world knowledge through the environmental feedback, e.g., \"A stone sword can be crafted by using a wooden stick and two cobblestone\", and then finally store it in Hierarchical Directed Knowledge Graph. At the same time, Optimus-1 stores successful and failed cases in the Abstracted Multimodal Experience Pool, which enables Optimus-1 to accumulate reference experiences in the reflection phase. In the free exploration phase, we initialize multiple Optimus-1 at the same time, and they share the same Hierarchical Directed Knowledge Graph and Abstracted Multimodal Experience Pool, thus the memory is filled up efficiently. After free exploration, Optimus-1 have basic hierarchical knowledge and multimodal experience. In the teacher guidance phase, Optimus-1 need to learn a small number of long-horizon tasks based on extra knowledge given by teacher. For example, during the free exploration phase, Optimus-1 mastered crafting stick and mining diamond , but did not know that \"a diamond sword is obtained by a stick / and two diamonds\". This will serve as extra knowledge to guide Optimus-1 in his attempts to complete the task of \"craft diamond sword\". During the teacher guidance phase, Optimus-1's memory is further expanded and it gains the experience of executing complete long-horizon tasks.\nUnlike fine-tuning, we incrementally enhance the capabilities of Optimus-1 in Minecraft without any parameters updating, in a self-evolution manner. Specifically, we initialize Optimus-1's Hybrid Multimodal Memory to be empty, and then let Optimus-1 iterate on \"free exploration-teacher guidance\" learning and unseen task inference. As the number of iterations grows, Optimus-1 expands its Hybrid Multimodal Memory capacity through \"free exploration-teacher guidance\", allowing it to progressively master short-term to long-term tasks. The experimental results in section 3.5 demonstrate that this non-parametric learning approach empowers Optimus-1 to incrementally enhance long-horizon task performance."}, {"title": "F.3 Backbone of Optimus-1", "content": "Optimus-1 consists of Knowledge-Guided Planner, Experience-Driven Reflector, and Action Controller. In this paper, we employ OpenAI's GPT-4V (gpt-4-turbo)\u00a7 as Knowledge-Guided Planner and Experience-Driven Reflector, and STEVE-1 [17] as Action Controller. We also employ open-source models like Deepseek-VL [18] and InternLM-XComposer2-VL [5] as Knowledge-Guided Planner and Experience-Driven Reflector.\nAll experiments were implemented on 4x NVIDIA A100 GPUs. We employ multiple Optimus-1 to perform different tasks at the same time, and this parallelized inference greatly improves our experimental efficiency. Throughout the experiment, we spent about $5,000 to access the GPT-4V API. However, we also offer more cost-effective solutions. As shown in Figure 5, if we employ Deepseek-VL [18] or InternLM-XComposer2-VL [5] as Optimus-1's backbone, we can get comparable performance for nearly free!"}, {"title": "F.4 Prompt for Optimus-1", "content": "We show the prompt templates for Experience-Driven Reflector and Action Controller as follows."}, {"title": "H Case Study", "content": "This section introduces several cases to comprehensively demonstrate Optimus-1's capabilities.\nFigures 9, 10, and 11 demonstrate the superiority of our reflection mechanism, which dynamically adjusts the plan based on the current game progress.\n* Figure 9 illustrates Optimus-1's replanning ability. When Optimus-1 realizes it cannot complete a task (such as a craft failure shown in the figure), it will replan the current task and continue execution.\n* Figures 10 and 11 showcase Optimus-1's ability to make judgments based on visual signals. When Optimus-1 determines that it has completed a task (such as \"kill a cow in Figure 10), it will finish the current task and move on to the next one. If Optimus-1 discovers that it has not yet completed the task and the task has not failed(as shown in Figure 11), it will continue executing the task.\nFigures 12 and 13 illustrate the advantages of planning with knowledge. With the Hierarchical Directed Knowledge Graph, we can generate a high-quality plan in one step and dynamically adjust the plan based on current visual signals.\n* Figure 12 demonstrates the importance of"}]}