{"title": "Optimus-1 : Hybrid Multimodal Memory Empowered Agents Excel in Long-Horizon Tasks", "authors": ["Zaijing Li", "Yuquan Xie", "Rui Shao", "Gongwei Chen", "Dongmei Jiang", "Liqiang Nie"], "abstract": "Building a general-purpose agent is a long-standing vision in the field of artificial intelligence. Existing agents have made remarkable progress in many domains, yet they still struggle to complete long-horizon tasks in an open world. We attribute this to the lack of necessary world knowledge and multimodal experience that can guide agents through a variety of long-horizon tasks. In this paper, we propose a Hybrid Multimodal Memory module to address the above challenges. It 1) transforms knowledge into Hierarchical Directed Knowledge Graph that allows agents to explicitly represent and learn world knowledge, and 2) summarises historical information into Abstracted Multimodal Experience Pool that provide agents with rich references for in-context learning. On top of the Hybrid Multimodal Memory module, a multimodal agent, Optimus-1, is constructed with dedicated", "sections": [{"title": "1 Introduction", "content": "Optimus Prime faces complex tasks alongside humans in Transformers to protect the peace of the planet. Creating an agent like Optimus that can perceive, plan, reflect, and complete long-horizon tasks in an open world has been a longstanding aspiration in the field of artificial intelligence [31, 15, 12]. Early research [1, 6, 17] developed simple agents by constructing policy networks. Recent works [34, 39, 37] have utilized Large Language Models (LLMs) as action planners for agents, generating executable sub-goal sequences for low-level action controllers. Leveraging the powerful instruction-following and logical reasoning capabilities of (Multimodal) LLMs, LLM-based agents have achieved remarkable success across multiple domains [13, 8, 9, 42]. Nevertheless, the ability of these agents to complete long-horizon tasks still falls significantly short of human-level performance.\nAccording to relevant studies [19, 29, 33], the human ability to complete long-horizon tasks in an open world relies on long-term memory storage, which is divided into knowledge and experience. The storage and utilization of knowledge and experience play a crucial role in guiding human behavior and enabling humans to adapt flexibly to their environments in order to accomplish long-horizon tasks. Inspired by this theory, we summarize the challenges faced by current agents as follows:\nInsufficient Exploration of Structured Knowledge: Structured knowledge, encompassing open world rules, object relationships, and interaction methods with the environment, is essential for agents to complete complex tasks [25, 31]. However, existing agents [1, 17, 6] only learn dispersed knowledge from video data and are unable to efficiently represent and learn this structured knowledge, rendering them incapable of performing complex tasks.\nLack of Multimodal Experience: Humans derive successful strategies and lessons from information on historical experience [7, 23], which assists them in tackling current complex tasks. In a similar manner, agents can benefit from in-context learning with experience demonstrations [30, 41]. However, existing agents [34, 38, 24] only consider unimodal information, which prevents them from learning from multimodal experience as humans do.\nTo address the aforementioned challenges, we propose Hybrid Multimodal Memory module that consists of Hierarchical Directed Knowledge Graph (HDKG) and Abstracted Multimodal Experience Pool (AMEP). For HDKG, we map the logical relationships between objects into a directed graph structure, thereby transforming knowledge into high-level semantic representations. HDKG efficiently provides the agent with the necessary knowledge for task execution, without requiring any parameter updates. For AMEP, we dynamically summarize and store the multimodal information (e.g., environment, agent state, task plan, video frames, etc.) from the agent's task execution process, ensuring that historical information contains both a global overview and local details. Different from the method of directly storing successful cases as experience [39], AMEP considers both successful and failed cases as references. This innovative approach of incorporating failure cases into in-context learning significantly enhances the performance of the agent.\nOn top of the Hybrid Multimodal Memory module, we construct a multimodal composable agent, Optimus-1. As shown in Figure 1, Optimus-1 consists of Knowledge-Guided Planner, Experience-Driven Reflector, and Action Controller. To enhance the ability of agents to cope with complex environments and long-horizon tasks, Knowledge-Guided Planner incorporates visual observation into the planning phase, leveraging HDKG to capture the knowledge needed. This allows the agent to efficiently transform tasks into executable sub-goals. Action Controller takes the sub-goal and the current observation as inputs and generates low-level actions, interacting with the game environment to update the agent's state. In open-world complex environments, agents are prone to be erroneous"}, {"title": "2 Optimus-1", "content": "To build an agent that can perform long-horizon tasks in an open world like a human, we develop a multimodal composable agent, Optimus-1, in Minecraft. Optimus-1 consists of Hybrid Multimodal Memory, Knowledge-Guided Planner, Experience-Driven Reflector, and Action Controller."}, {"title": "2.1 Hybrid Multimodal Memory", "content": "In order to endow agent with a long-term memory storage mechanism [19, 33], we propose the Hybrid Multimodal Memory module, which consists of Abstracted Multimodal Experience Pool (AMEP) and Hierarchical Directed Knowledge Graph (HDKG)."}, {"title": "2.1.1 Abstracted Multimodal Experience Pool", "content": "Relevant studies [4, 20, 16, 14] highlight the importance of historical information for agents completing long-horizon tasks. Minedojo [6] and Voyager [34] employed unimodal storage of historical information. Jarvis-1 [39] used a multimodal experience mechanism, storing task planning and visual information without summarization, leading to storage and retrieval challenges. To address this, we propose AMEP, which aims to summarize all multimodal information during the agent's execution of the task. It can maintain long sequential data integrity, and improve storage and retrieval efficiency.\nSpecifically, as depicted in Figure 2, to conduct the static visual information abstraction, the video stream captured by Optimus-1 during task execution is first input to a video buffer, filtering the stream at a fixed frequency of 1 frame per second. Based on the filtered video frames, to further perform a dynamic visual information abstraction, these frames are then fed into an image buffer with a window size of 16, where the image similarity is dynamically computed and final abstracted frames are adaptively updated. To align such abstracted visual information with the corresponding textual sub-goal, we then utilize MineCLIP [6], a pre-trained video-text alignment model, to calculate their multimodal correlation. When this correlation exceeds a threshold, the corresponding image buffer and textual sub-goal are saved as multimodal experience into a pool. Finally, we further incorporate environment information, agent initial state, and plan generated by Knowledge-Guided Planner, into such a pool, which forms the AMEP. In this way, we consider the multimodal information of each sub-goal, and summarise it to finally compose the multimodal experience of the given task."}, {"title": "2.1.2 Hierarchical Directed Knowledge Graph", "content": "World knowledge is essential for an agent's ability to perform long-horizon complex tasks. Instead of implicit learning through fine-tuning [24, 45], we propose HDKG, enabling agents to learn explicitly by retrieving knowledge graph.\nMinecraft's mining and crafting represent a complex knowledge network crucial for effective task planning. For instance, crafting a diamond sword requires two diamonds and one wooden stick \u2713, while mining diamonds requires an iron pickaxe, which involving further materials and steps. To enable a more structured knowledge representation, as shown in the Figure 2, we transform this knowledge into a graph D(V, E), where nodes set V represent objects, and directed edges set & point to nodes that can be crafted by this object. An edge e \u2208 E in the D can be represented as e = (u, v), where u, v \u2208 V. The directed graph efficiently stores and updates knowledge. For a given object x, retrieving the corresponding node allows extraction of a sub-graph Dj(Vj, Ej) \u2208 D, where nodes set V; and edges set E; can be formulated as:\nVj = {v \u2208 V | x}, Ej = {e = (u, v) \u2208 V | u \u2208 V; U v \u2208 1 V; },\nThen by topological sorting, we can get all the materials and their relationships needed to complete the task. This knowledge is provided to the Knowledge-Guided Planner as a way to generate a more reasonable sequence of sub-goals. With HDKG, we can significantly enhance the world knowledge of the agent in a train-free manner."}, {"title": "2.2 Optimus-1: Framework", "content": "Relevant studies indicate that the human brain is essential for planning and reflection, while the cerebellum controls low-level actions, both crucial for complex tasks [27, 28]. Inspired by this, we"}, {"title": "Action Controller", "content": "It takes the sub-goal and the current observation as inputs and then generates low-level actions, which are control signals for the mouse and keyboard. Thus, it can interact with the game environment to update the agent's state and the observation. The formulation is as follows:\n\u03b1\u03ba = \u03a1\u03c0(0, 9\u03af),\nwhere ak denotes low-level action at time k, p\u016b denotes action controller. Unlike generating code [34, 24, 37], generating control actions for the mouse and keyboard [1, 17, 39, 3] more closely resembles human behavior. In this paper, we employ STEVE-1 [17] as our Action Controller."}, {"title": "Experience-Driven Reflector", "content": "The sub-goals generated by Knowledge-Guided Planner are interdependent. The failure of any sub-goal halts the execution of subsequent ones, leading to overall task failure. Therefore, a reflection module is essential to identify and rectify errors promptly. During task execution, the Experience-Driven Reflector activates at regular intervals, retrieving historical experience from AMEP, and then analyzing the current state of Optimus-1. The reflection results of Optimus-1 are categorized as COMPLETE, CONTINUE, or REPLAN. COMPLETE indicates successful execution, prompting the action controller to proceed to the next sub-goal. CONTINUE signifies ongoing execution without additional feedback. REPLAN denotes failure, requiring the Knowledge-Guided Planner to revise the plan. The reflection r generated by Experience-Driven Reflector can be formulated as:\nr = po(0, gi, Pe(t)),\nwhere pe denotes multimodal experience retrieved from AMEP. Experimental results in Section 3.3 demonstrate that the Experience-Driven Reflector significantly enhances the success rate of long-horizon tasks."}, {"title": "2.3 Non-parametric Learning of Hybrid Multimodal Memory", "content": "To implement the Hybrid Multimodal Memory and enhance Optimus-1's capacity, we propose a non-parametric learning method named \"free exploration-teacher guidance\". In the free exploration phase, Optimus-1's equipment and tasks are randomly initialized, and it explores random environments, acquiring world knowledge through environmental feedback. For example, it learns that \u201ca stone sword can be crafted with a wooden stick / and two cobblestones\", storing this in the HDKG. Additionally, successful and failed cases are stored in the AMEP, providing reference experience for the reflection phase. We initialize multiple Optimus-1, and they share the same HDKG and AMEP. Thus the memory is filled up efficiently. After free exploration, Optimus-1 has basic world knowledge and multimodal experience. In the teacher guidance phase, Optimus-1 needs to learn a small number of long-horizon tasks based on extra knowledge. For example, it learns \u201ca diamond sword is obtained by a stick and two diamonds\" from the teacher, then perform the task \"craft diamond sword\". During the teacher guidance phase, Optimus-1's memory is further expanded and it gains the experience of executing complete long-horizon tasks.\nUnlike fine-tuning, this method enhances Optimus-1 incrementally without updating parameters, in a self-evolution manner. Starting with an empty Hybrid Multimodal Memory, Optimus-1 iterates between \"free exploration-teacher guidance\" learning and unseen task inference. With each iteration, its memory capacity grows, enabling mastery of tasks from easy to hard."}, {"title": "3 Experiments", "content": "To ensure realistic gameplay like human players, we employ MineRL [10] with Minecraft 1.16.5 as our simulation environment. The agent operates at a fixed speed of 20 frames per second and only interacts with the environment via low-level action control signals of the mouse and keyboard. For more information about the detailed descriptions of the observation and action spaces, please refer to the Appendix C."}, {"title": "3.1 Experiments Setting", "content": "Environment. To ensure realistic gameplay like human players, we employ MineRL [10] with Minecraft 1.16.5 as our simulation environment. The agent operates at a fixed speed of 20 frames per second and only interacts with the environment via low-level action control signals of the mouse and keyboard. For more information about the detailed descriptions of the observation and action spaces, please refer to the Appendix C.\nBenchmark. We constructed a benchmark of 67 tasks to evaluate the Optimus-1's ability to complete long-horizon tasks. As illustrated in Table 5, we divide the 67 Minecraft tasks into 7 groups according to recommended categories in Minecraft. Please refer to Appendix E for more details."}, {"title": "Evaluation Metrics", "content": "The agent always starts in survival mode, with an empty inventory. We conducted at least 30 times for each task using different world seeds and reported the average success rate to ensure fair and thorough evaluation. Additionally, we add the average steps and average time of completing the task as evaluation metrics."}, {"title": "3.2 Experimental Results", "content": "The overall experimental results on benchmark are shown in Table 1, see the accuracy for each task in Appendix G. Optimus-1 has a success rate near 100% on the Wood Group. Compared with Jarvis-1, Optimus-1 has 29.28% and 53.40% improvement on the Diamond Group and Redstone Group, respectively. Optimus-1 achieves the best performance and the shortest elapsed time among all task groups. It reveals the effectiveness and efficiency of our proposed Optimus-1 framework. Moreover, compared with all baselines, Optimus-1 performance was closer (average 5.37% improvement) to human levels on long-horizon task groups."}, {"title": "3.3 Ablation Study", "content": "We conduct extensive ablation experiments on 18 tasks, experiment setting can be found in Table 6. As shown in Table 2, we first remove Knowledge-Driven Planner and Experience-Driven Reflector, the performance of Optimus-1 on all task groups drops dramatically. It demonstrates the necessity of Knowledge-Guided Planner and Experience-Driven Reflector modules for performing long-horizon tasks. As for Hybrid Multimodal Memory, we remove HDKG from Optimus-1. Without the help of world knowledge, the performance of Optimus-1 decreased by an average of 20% across all task groups. We then removed AMEP, this resulted in the performance of Optimus-1 decreased by an average of 12%. Finally, we performed ablation experiments on the way of retrieving cases from AMEP. As shown in Table 3, without retrieving cases from AMEP, the success rate shows an average of 10% decrease across all groups. It reveals that this reflection mechanism, which considers both success and failure cases, has a significant impact on the performance of Optimus-1. To illustrate the role of the reflection mechanism, we have shown some cases in Figure 4."}, {"title": "3.4 Generalization Ability", "content": "In this section, we explore an interesting issue: whether generic MLLMs can effectively perform various long-horizon complex tasks in Minecraft using Hybrid Multimodal Memory. As shown in Figure 5, We employ Deepseek-VL [18] and InternLM-XComposer2-VL [5] as Knowledge-Guided Planner and Experience-Driven Reflector. The experimental results show that the original MLLM has low performance on long-horizon tasks due to the lack of knowledge and experience of Minecraft. With the assistance of Hybrid Multimodal Memory, the performance of MLLMs has improved by 2 to 6 times across various task groups, outperforming the GPT-4V baseline. This encouraging result demonstrates the generalization of the proposed Hybrid Multimodal Memory."}, {"title": "3.5 Self-Evolution via Hybrid Multimodal Memory", "content": "As shown in Section 2.3, we randomly initialize the Hybrid Multimodal Memory of Optimus-1, then update it multiple times by using the \"free exploration-teacher guidance\" learning method. We set the epoch to 4, and the number of learning tasks to 160. At each period, Optimus-1 performs free exploration on 150 tasks and teacher guidance learning on the remaining 10 tasks, we then evaluate Optimus-1's learning ability on the task groups same as ablation study. Experimental results are shown in Figure 5. It reveals that Optimus-1 keeps getting stronger through the continuous expansion of memory during the learning process of multiple periods. Moreover, it demonstrates that MLLM with Hybrid Multimodal Memory can incarnate an expert agent in a self-evolution manner [32]."}, {"title": "4 Related Work", "content": "Agents in Minecraft. We summarise the differences of existing Minecraft agents in the Appendix E.3. Earlier work [21, 43, 2, 3] introduced policy models for agents to perform simple tasks in Minecraft. MineCLIP [6] used text-video data to train a contrastive video-language model as a reward model for policy, while VPT [1] pre-trained on unlabelled videos but lacked instruction as input. Building on VPT and MineCLIP, STEVE-1 [17] added text input to generate low-level action sequences from human instructions and images. However, these agents struggle with complex tasks due to limitations in instruction comprehension and planning. Recent work [37, 34, 46] incorporated LLMs as planning and reflection modules, but lacked visual information integration for adaptive planning. MP5 [24], MineDreamer [45], and Jarvis-1 [39] enhanced situation-aware planning by obtaining textual descriptions of visual information, yet lacked detailed visual data. Optimus-1 addresses these issues by directly using observation as situation-aware conditions in the planning phase, enabling more rational, visually informed planning. Additionally, unlike other agents requiring multiple queries for task refinement, Optimus-1 generates a complete and effective plan in one step with the help of HDKG. This makes Optimus-1 planning more efficient.\nMemory in Agents. In the agent-environment interaction process, memory is key to achieving experience accumulation, environment exploration, and knowledge abstraction [44]. There are two forms to represent memory content in LLM-based agents: textual form [16, 14, 22] and parametric form [4, 20, 35]. In textual form, the information is explicitly retained and recalled by natural languages. In parametric form, the memory information is encoded into parameters and implicitly influences the agent's actions. Recent work [36, 40, 11] has explored the long-term visual information storage and summarisation in MLLM. Our proposed hybrid multimodal memory module is plug-and-play and can provide world knowledge and multimodal experience for Optimus-1 efficiently."}, {"title": "5 Conclusion", "content": "In this paper, we propose Hybrid Multimodal Memory module, which consists of two parts: HDKG and AMEP. HDKG provides the necessary world knowledge for the planning phase of the agent, and AMEP provides the refined historical experience for the reflection phase of the agent. On top of the Hybrid Multimodal Memory, we construct the multimodal composable agent, Optimus-1, in Minecraft. Extensive experimental results show that Optimus-1 outperforms all existing agents on long-horizon tasks. Moreover, we validate that general-purpose MLLMs, based on Hybrid Multimodal Memory and without additional parameter updates, can exceed the powerful GPT-4V baseline. The extensive experimental results show that Optimus-1 makes a major step toward a general agent with a human-like level of performance."}, {"title": "A Broader Impact", "content": "With the increasing capability level of Multimodal Large Language Models (MLLM) comes many potential benefits and also risks. On the positive side, we anticipate that the techniques that used to create Optimus-1 could be applied to the creation of helpful agents in robotics, video games, and the web. This plug-and-play architecture that we have created can be quickly adapted to different MLLMs, and the proposed methods also provide a viable solution for other application areas in the agent domain. However, on the negative side, it is imperative to acknowledge the inherent stochastic nature of MLLMs in text generation. If not addressed carefully, this could lead to devastating consequences for society. Prior to deploying MLLMs in conjunction with the Hybrid Multimodal Memory methodology, a comprehensive assessment of their potential risks must be undertaken. We hope that while the stakes are low, works such as ours can improve access to safety research on instruction-following models in multimodal agents domains."}, {"title": "B Limitation and Future Work", "content": "In the framework of Optimus-1, we are dedicated to leverage proposed Hierarchical Directed Knowledge Graph and Abstracted Multimodal Experience Pool can be used to enhance the agent's ability to plan and reflect. For Action Controller, we directly introduce STEVE-1 [17] as a generator of low-level actions. However, limited by STEVE-1's ability to follow instructions and execute complex actions, Optimus-1 is weak in completing challenging tasks such as \"beat ender dragon\" and \"build a house\".\nIn order to further train STEVE-1's low-level action generation ability, high-quality video-action data is indispensable. With the help of our highly scalable and adaptable architecture, we can obtain such high quality video data during inference process of Optimus-1. In our future work, we will strengthen the ability of Action Controller through further training."}, {"title": "C Minecraft", "content": "Minecraft is an extremely popular sandbox video game developed by Mojang Studios *. It allows players to explore a blockly, procedurally generated 3D world with infinite terrain, discover and extract raw materials, craft tools and items, and build structures or earthworks(shown in Figure 6). In this game, AI agents need to face situations that are highly similar to the real world, making judgments and decisions to deal with various environments and problems. Therefore, Minecraft is a very suitable environment to be used as an AI testing simulator."}, {"title": "C.1 Basic Rules", "content": "Biomes. The Minecraft world is divided into different areas called \"biomes\". Different biomes contain different blocks and plants and change how the land is shaped. There are 79 biomes in Minecraft 1.16.5, including ocean, plains, forest, desert, etc. Diverse environments have high requirements for the generalization of agents.\nTime. Time passes within this world, and a game day lasts for 20 real-world minutes. Nighttime is much more dangerous than daytime: the game starts at dawn, and agents have 10 minutes of game time before nightfall. Hostile or neutral mobs spawn when night falls, and most of these mobs are dangerous, trying to attack agents. How to survive in such a dangerous world is an open problem for Minecraft agents research.\nItem. In Minecraft 1.16.5, there are 975 items can be obtained, such as wooden pickaxe, iron sword. Item can be obtained by crafting or destroying blocks or attacking entities. For example, agent can attack cows to obtain leather and beef. Agent also can use 1 stick / and 2 diamonds to craft diamond sword.\nGameplay progress. Progression primarily involves discovering and utilizing various materials and resources, each of which unlocks new capabilities and options. For instance, crafting a wooden pickaxe enables the player to mine stone, which can be used to create a stone pickaxe."}, {"title": "C.2 Observation and Action Spaces", "content": "Observation. Our observation space is completely consistent with human players. The agent only receives an RGB image with dimensions of 640 \u00d7 360 during the gameplay process, including the hotbar, health indicators, food saturation, and animations of the player's hands. It is worth helping the agent see more clearly in extremely dark environments, we have added a night vision effect for the agent, which increases the brightness of the environment during the night."}, {"title": "Action Spaces", "content": "Our action space is almost similar to human players, except for craft and smelt actions. It consists of two parts: the mouse and the keyboard. The keypresses are responsible for controlling the movement of agents, such as jumping, forward, back, etc. The mouse movements are responsible for controlling the perspective of agents and the cursor movements when the GUI is opened. The left and right buttons of the mouse are responsible for attacking and using or placing items. In Minecraft, precise mouse movements are important when completing complex tasks that need open inventory or crafting table. In order to achieve both the same action space with MineDojo [6], we abstract the craft and the smelt action into action space. The detailed action space is described in Table 4."}, {"title": "C.3 Long-horizon Tasks", "content": "Long-horizon Tasks are complex tasks that require world knowledge to solve and consist of multiple indispensable subtask sequences. In Minecraft, technology has six levels, including wood, stone \u2764, iron, golden, diamond, and netherite. Wooden tools can mine stone-level blocks, but can't mine iron-level and upper-level blocks. Stone tools can mine iron-level blocks, but can't mine diamond-level and upper-level blocks. Iron-level tools can mine diamond-level blocks, but can't mine netherite-level blocks. Diamond-level tools can mine any level blocks.\nFor example, the agent now wants to complete the task \"Craft iron sword\". The agent needs to craft wood-level tools to mine stone, and craft stone-level tools to mine iron ore. In order to craft tools, the agent needs a crafting table. To smelt iron ore into iron ingot, the agent needs a furnace. Moreover, craft crafting table needs 4 planks, and craft furnace needs 8 cobblestone. In summary, the agent needs to obtain many raw materials, wood-level and stone-level tools, 1 crafting table, 1 furnace, and most importantly, 2 iron ingots. The process of this task is shown in Figure 7."}, {"title": "D Theory", "content": "In this section, we briefly introduce the relevant theory of cognitive science. For more details, please refer to the original articles.\nOur ability to understand and predict the world around us depends on our long-term memory stores, which have historically been divided into two distinct systems [19, 29, 33]. The semantic memory system provides a conceptual framework for describing the similar meanings of words and objects as they are encountered in different contexts (e.g., a bee is a flying insect with yellow and black stripes that produces honey), whereas the episodic memory system records our personal experiences characterized by the co-occurrence of words and objects at different times and places (e.g., being stung by a bee while eating honey at a picnic last weekend). These information stores and the interactions between them play a crucial role in guiding our behaviour and giving us the flexibility to adapt to the various demands of our environment."}, {"title": "E.1 Benchmark", "content": "We constructed a benchmark of 67 tasks to evaluate Optimus-1's ability to complete long-horizon tasks in Minecraft. According to recommended categories in Minecraft, we have classified these tasks into 7 groups: Wood, Stone, Iron, Gold, Diamond, Redstone, Armor. The statistics for benchmark are shown in Table 5. Due to the varying complexity of these tasks, we adopt different maximum gameplay steps (Max. Steps) for each task. The maximum steps are determined by the average steps that human players need to complete the task. Due to the randomness of Minecraft, the world and initial spawn point of the agent could vary a lot. In our benchmark setting, We initialize the agent with an empty inventory, which makes it necessary for the agent to complete a series of"}, {"title": "E.2 Human-level Baseline", "content": "To better demonstrate agent's performance level in Minecraft, we hired 10 volunteers to play the game as a human-level baseline. The volunteers played the game with the same environment and settings, and every volunteer asked to perform the each task on the benchmark 10 times. Ultimately, we used the average scores of 10 volunteers as the human-level baseline. The results of the human-level baseline are shown in Table 1."}, {"title": "E.3 Minecraft Agents", "content": "In this section, we summarise the differences between existing Minecraft agents. As shown in the Table 7, earlier work [1, 6, 17, 3] constructed Transformer-based policy network as agent. Recent work [34, 38, 39, 24] introduces the Multimodal Large Language Model, which empowers the agent to complete long-horizon tasks by exploiting the powerful language comprehension and planning capabilities of LLM. However, these agents lack knowledge and experience, and their performance in Minecraft is still vastly gapped from the human level. In this paper, we introduce Hybrid Multimodal Memory, which empowers Optimus-1 with hierarchical knowledge and multimodal experience. This makes Optimus-1 significantly outperform all existing agents on challenging long-horizon tasks benchmark, and exhibits near human-level performance on many tasks."}, {"title": "F Implementation Details", "content": "F.1 Hybrid Multimodal Memory\nF.1.1 Abstracted Multimodal Experience Pool\nRelevant studies [4, 20, 16, 14] have demonstrated the importance of memory for agents to complete long-horizon tasks. To implement the memory mechanism, Minedojo [6] and Voyager [34] only considered unimodal storage of historical information. Jarvis-1 [39] considered a multimodal memory mechanism to store task planning and visual information as experience, but it stores all historical information without summarisation. This approach stores all visual images, which poses a huge challenge in storage size and retrieval efficiency. To solve the problem, we propose the Abstracted Multimodal Experience Pool structure, which summarizes all historical information during the agent's execution of the task, which maintains the integrity of long sequential information and greatly improves the storage and retrieval efficiency of the experience.\nAs shown in Figure 2, we first input the visual image stream captured by Optimus-1 during the execution of the task to the video buffer, which filters the image stream at a fixed frequency, which makes the length of the image stream substantially shorter. Empirically, we set the frequency of filtering to 1 second/frame, meaning that the video buffer takes one frame per second from the original image stream to compose the filtered image stream. We found that above this frequency makes the visual information redundant (too much similarity between images), and below this frequency does not preserve enough complete visual information. We then feed the filtered frames into an image buffer with a window size of 16. We dynamically compute the similarity between images in the image buffer, when a new image comes in, we compute the similarity between the new image and the most recent image, and then we remove the image with the highest similarity in order to keep the image buffer's window size to 16. We then introduce MineCLIP [6], a pre-trained model of video-text alignment with a structure similar to CLIP [26], as our visual summariser. For a given sub-goal, it calculates the correlation between the visual content within the current memory bank and the sub-goal, and when this correlation exceeds a pre-set threshold, the frames within the memory bank are saved as the visual memories corresponding to that sub-goal. Finally, we store the visual memories with"}, {"title": "F.1.2 Hierarchical Directed Knowledge Graph", "content": "World knowledge is the foundation and key to an agent's ability to perform long-horizon complex tasks. Unlike allowing agents to learn world knowledge implicitly through fine-tuning [24, 45], we propose a method for constructing and transforming hierarchical world knowledge into graph structure that allows agents to learn knowledge explicitly in a more efficient manner.\nMining and crafting objects in Minecraft is a complex and large knowledge network, and has a significant impact on the ability of the agent to generate effective task planning. As shown in the Figure 2, crafting a diamond sword requires two diamonds and a wooden stick /, while mining diamonds requires an iron pickaxe, which in turn requires additional raw materials and crafting steps. We transform this mine and craft knowledge into a graph structure, where the nodes of the graph are objects, and the nodes point to objects that can be crafted or completed by that object. With directed graph, we show that connections between objects are established, and that this knowledge can be stored and updated efficiently. For a given object, we only need to retrieve the corresponding node to extract the corresponding subgraph from the knowledge graph. Then by topological sorting, we can get the antecedents and required materials for the object, and this information is provided to the Knowledge-Guided Planner as a way to generate a more reasonable sequence of sub-goals. With Hierarchical Directed Knowledge Graph, we can significantly enhance the world knowledge of the agent in a train-free manner, as shown in the experimental results in Section 3.3."}, {"title": "F.2 Hybrid Multimodal Memory Driven Optimus-1", "content": "In order to implement the proposed Hybrid Multimodal Memory and to progressively increase the capacity of Optimus-1 in a self-evolution manner, we propose a non-parametric learning method named \"free exploration-teacher guidance\". In the free exploration phase, we randomly initialize Optimus-1's equipment and tasks, and ask it to explore freely in a randomly set environment, and acquire world knowledge through the environmental feedback, e.g., \"A stone sword can be crafted by using a wooden stick and two cobblestone\", and then finally store it in Hierarchical Directed Knowledge Graph. At the same time, Optimus-1 stores successful and failed cases in the Abstracted Multimodal Experience Pool, which enables Optimus-1 to accumulate reference experiences in the reflection phase. In the free exploration phase, we initialize multiple Optimus-1 at the same time, and they share the same Hierarchical Directed Knowledge Graph and Abstracted Multimodal Experience Pool, thus the memory is filled up efficiently. After free exploration, Optimus-1 have basic hierarchical knowledge and multimodal experience. In the teacher guidance phase, Optimus-1 need to learn a small number of long-horizon tasks based on extra knowledge given by teacher. For example, during the free exploration phase, Optimus-1 mastered crafting stick / and mining diamond, but did not know that \"a diamond sword is obtained by a stick / and two diamonds\". This will serve as extra knowledge to guide Optimus-1 in his attempts to complete the task of \"craft diamond sword\". During the teacher guidance phase, Optimus-1's memory is further expanded and it gains the experience of executing complete long-horizon tasks.\nUnlike fine-tuning, we incrementally enhance the capabilities of Optimus-1 in Minecraft without any parameters updating, in a self-evolution manner. Specifically, we initialize Optimus-1's Hybrid Multimodal Memory to be empty, and then let Optimus-1 iterate on \"free exploration-teacher guidance\" learning and unseen task inference. As the number of iterations grows, Optimus-1"}, {"title": "F.3 Backbone of Optimus-1", "content": "Optimus-1 consists of Knowledge-Guided Planner, Experience-Driven Reflector, and Action Controller. In this paper, we employ OpenAI's GPT-4V (gpt-4-turbo) \u00a7 as Knowledge-Guided Planner and Experience-Driven Reflector, and STEVE-1 [17", "18": "and InternLM-XComposer2-VL ["}]}