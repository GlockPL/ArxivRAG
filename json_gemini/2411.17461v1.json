{"title": "SoK: Decentralized AI (DeAI)", "authors": ["Zhipeng Wang", "Rui Sun", "Elizabeth Luis", "Vatsal Shah", "Xihan Xiong", "Jiahao Sun", "Davide Crapi", "William Knottenbelt"], "abstract": "The centralization of Artificial Intelligence (AI) poses significant challenges, including single points of failure, inherent biases, data privacy concerns, and scalability issues. These problems are especially prevalent in closed-source large language models (LLMs), where user data is collected and used without transparency. To mitigate these issues, blockchain-based decentralized AI (DeAI) has emerged as a promising solution. DeAI combines the strengths of both blockchain and AI technologies to enhance the transparency, security, decentralization, and trustworthiness of AI systems. However, a comprehensive understanding of state-of-the-art DeAI development, particularly for active industry solutions, is still lacking.\nIn this work, we present a Systematization of Knowledge (SoK) for blockchain-based DeAI solutions. We propose a taxonomy to classify existing DeAI protocols based on the model lifecycle. Based on this taxonomy, we provide a structured way to clarify the landscape of DeAI protocols and identify their similarities and differences. We analyze the functionalities of blockchain in DeAI, investigating how blockchain features contribute to enhancing the security, transparency, and trustworthiness of AI processes, while also ensuring fair incentives for AI data and model contributors. In addition, we identify key insights and research gaps in developing DeAI protocols, highlighting several critical avenues for future research. The full list of papers and platforms covered in this SoK is available at https://github.com/FLock-io/awesome-decentralized-ai.", "sections": [{"title": "I. INTRODUCTION", "content": "The rapid advancement of Artificial Intelligence (AI) has revolutionized various industries such as healthcare, finance, and transportation. However, alongside these benefits, concerns have emerged regarding its centralization\u2014where a single entity holds control over training, decision-making processes, and data storage for AI models. This centralization presents several challenges. Firstly, it can lead to a single point of failure, making AI systems more vulnerable to technical disruptions and cyberattacks [1]. Secondly, it risks undermining the diversity of perspectives in AI outputs, as the controlling entities can impose their own biases and limit the range of viewpoints represented. These biases often include those related to gender, ethnicity, socioeconomic status, and geographic representation. This issue has been highlighted in criticisms of large-scale Al models for reflecting narrow and controlled perspectives [2], [3]. Additionally, centralized control raises significant data protection concerns, particularly with closed-source Large Language Models (LLMs) where user interactions may be monitored and user data incorporated into corporate-controlled models without explicit consent [4]\u2013[6]. This highlights the need for fairer contribution incentives and more accurate valuation of user-contributed data [7]. Moreover, as data volumes and task complexities increase, the limited processing power of centralized systems becomes a bottleneck, restricting scalability [8]. In addition, the concentration of resources for model training among a limited number of entities constrains innovation and restricts broader advancements in the AI field [9]\u2013[11].\nTo address the above-mentioned issues, blockchain-based decentralized AI (DeAI) has emerged as a promising solution that leverages the strengths of both blockchain and AI [12]. Blockchain technology offers transparency, security, and decentralization, which can significantly enhance the trustworthiness of AI systems [13]. By integrating blockchain with AI, it is possible to create a more robust and decentralized ecosystem that addresses key challenges such as data privacy, model integrity, and equitable access to AI resources [14]. Blockchain-based DeAI has widely been explored in academia [15]\u2013[19] and implemented in industry [20]\u2013[26].\nThe integration of AI with blockchain technology offers both significant opportunities and challenges. On the positive side, blockchain can enhance data security and provide immutable audit trails for AI models, achieving greater trust among participants [27]. DeAI platforms can leverage distributed computing resources to reduce dependence on centralized servers and mitigate single points of failure [28]. However, issues such as scalability and computational overhead remain substantial challenges [29]. The intensive computational requirements of AI models, particularly deep learning algorithms, can overwhelm blockchain networks, causing latency and performance bottlenecks [30]. Moreover, balancing data privacy with the need for transparency in a decentralized setting presents a complex challenge [31].\nAlthough DeAI has been widely adopted in industry, particularly in active solutions [20]\u2013[26], the academic community still lacks a systematic analysis of its technical architecture, strengths, and limitations. Therefore, in this work, we provide a Systematization of Knowledge (SoK) for blockchain-based DeAI and aim to answer the following research questions:\nRQ1: What is the general framework of existing DeAI?\nRQ2: How can blockchain help an AI protocol to be more decentralized and address the challenges of centralized AI?\nRQ3: What insights can be drawn from existing DeAI solutions, and what research gaps remain to further improve blockchain-based DeAI protocols?\nBy systematically reviewing existing DeAI solutions in practice, we discuss the benefits such as improved security and decentralized data management, and the challenges like scalability and data privacy. We analyze existing DeAI platforms and protocols, evaluate their approaches to decentralization and security, and identify potential vulnerabilities.\nContributions. Our contributions are summarized as follows:\nWe provide a taxonomy to summarize the existing DeAI protocols, categorizing them based on the lifecycle of an AI model. This taxonomy provides a structured way to clarify the landscape of DeAI protocols and identifies their similarities and differences (see Table I).\nWe analyze the functionalities of blockchain in DeAI protocols, focusing on how key blockchain features such as immutability and decentralization contribute to enhancing the security, transparency, and trustworthiness of AI processes. We also examine how blockchain can enable fair incentives for data and model contributors, thereby fostering collaborative AI development in a decentralized ecosystem.\nWe highlight the insights and research gaps for building blockchain-based DeAI protocols, outlining several critical areas for future exploration."}, {"title": "II. BACKGROUND", "content": "A. Artificial Intelligence\nAI focuses on creating systems capable of performing tasks that typically require human intelligence, such as learning, reasoning, problem-solving, perception, and language understanding [32], [33]. Modern AI development is primarily driven by machine learning (ML), which enables computers to learn from data and make decisions through the use of sophisticated algorithms [34]. Rapid advancements in AI have been accelerated by increased computational power, the availability of large datasets, and breakthroughs in algorithms [35], leading to significant progress in fields such as computer vision [36], natural language processing [37], and game playing [38].\nHowever, most current AI systems are centralized, where the creation, training, and deployment of AI models are controlled by a single or a few entities. As shown in Figure 1(B), organizations (e.g., companies) collect personal data from multiple sources (e.g., clients, mobile app users) and aggregate it into a central repository, allowing their pre-designed ML models to learn from this unified dataset. While this approach can be effective for building powerful algorithms, centralized AI faces several challenges, including system and network resource bottlenecks, data availability, privacy concerns [39], value bias [40], governance issues [41], and ethical concerns [42].\nDistributed Machine Learning (DML) is an approach designed to overcome the computational limitations of single-machine learning systems, particularly when handling large models and datasets. As modern AI models grow in size and complexity, a single computational unit often lacks the memory or processing power to efficiently manage the workload. DML solves this by distributing the computational tasks across multiple computational units (e.g., CPUs, GPUs, TPUs) or machines, enabling parallel execution [43]. DML can be broadly categorized into two main strategies: data parallelism and model parallelism. In data parallelism, as illustrated in Figure 1(C), the entire model is replicated across multiple GPUs, with each GPU processing a different subset of the data in parallel. After local computations, the results are aggregated to update the global model. In contrast, model parallelism splits the model itself across multiple machines, with each machine responsible for computing different parts of the model. While this method is useful for extremely large models that cannot fit on a single device, it requires all machines to access the entire dataset, which increases data privacy risks compared to data parallelism.\nFederated Learning (FL) is a distributed and collaborative machine learning approach designed to address the privacy concerns and limitations of traditional centralized AI systems by utilizing decentralized computational resources. In conventional AI, one straightforward solution to protect user privacy is to allow each client to perform standalone (on-device) learning without transmitting any information externally, as shown in Figure 1(A). However, this leads to the data silo problem, where isolated data across devices severely limits model performance and generalization capabilities.\nTo overcome the data silo issue while preserving data privacy, FL enables model training to occur locally on individual devices (e.g., smartphones or edge devices) or within organizations (e.g., companies or institutions) without sharing raw data. Each FL client computes model updates based on its local data, and only the model parameters (gradients) are shared with a central server for aggregation [44], as illustrated in Figure 1(D). This approach allows collaborative model training across distributed nodes while ensuring sensitive data remains on the local devices, achieving data privacy.\nHowever, centralized FL still faces significant challenges, including fault tolerance, privacy vulnerabilities, and communication overheads that strain bandwidth usage. To address these limitations, decentralized FL has gained attention, leveraging the ring-allreduce [45] paradigm. In this approach, participating devices are arranged in a logical ring topology to collaboratively train a global model without relying on a central server, as shown in Figure 1(E). Each device trains a local model on its private data and then shares its model updates with its immediate neighbor in the ring. The updates are passed around the ring in multiple iterations, with each device receiving, forwarding, and aggregating updates from other devices along the ring.\nWhile decentralized FL mitigates some of the challenges of centralized approaches, issues like privacy concerns, governance, and trust between participating nodes still remain. Ensuring that each participant adheres to the agreed protocols without central oversight, maintaining data integrity across distributed systems, and verifying the accuracy of local model updates in a transparent and secure manner are ongoing challenges. Additionally, safeguarding the privacy of individual data while ensuring robust collaboration between untrusted parties continues to be a significant concern in decentralized environments."}, {"title": "B. Blockchain and Smart Contracts", "content": "Blockchain is a decentralized, distributed ledger technology that records transactions across a network of computers, ensuring security, transparency, and immutability [46]. Transactions are grouped into blocks, which are linked together in a chain through cryptographic hash functions. Once a block is added, the data becomes immutable, which is guaranteed by the consensus mechanism from the underlying network.\nSmart Contract are self-executing programs stored on a blockchain that automatically enforce and execute the terms of an agreement when predefined conditions are met. These contracts eliminate the need for intermediaries by allowing transactions and agreements to occur directly between parties. The terms of the contract are written in code, and once deployed, the contract is immutable and transparent, ensuring trust and security. Smart contracts are widely used in various applications, such as Decentralized Finance (DeFi) [47], [48] and supply chain management [49]."}, {"title": "C. Blockchain-based A\u0399", "content": "Blockchain technology offers innovative solutions to address the limitations of centralized AI [50] and provide many advantages. For instance, blockchain enables secure and privacy-preserving data sharing for AI model training. As shown in Figure 1(F), blockchain can support decentralized Al models that reduce centralized control and distribute computational resources more efficiently. Blockchain's immutable and auditable ledger can also enhance trust in AI systems [28], [51], [52]. However, challenges such as scalability issues, performance bottlenecks, the balance between privacy and transparency, and the complexity of implementation persist [31], [53], [54]. Numerous ongoing academic and industry works [19]\u2013[21], [55] aim to leverage the strengths of blockchain to build more robust and DeAI systems."}, {"title": "III. DEAI FRAMEWORK", "content": "To enable practical DeAI applications in real-world scenarios, we propose a framework to ensure that AI processes are traceable and decentralized throughout their lifecycle.\nA. DeAI Model Lifecycle\nAs shown in Figure 2, the lifecycle of a DeAI application consists of five phases: task proposing, pre-training, on-training, post-training, and a feedback loop that may return to task proposal for model refining or fine-tuning.\n1 Task proposing involves algorithm preparation, where algorithms are designed to ensure privacy, scalability, and communication efficiency within decentralized environments. Additionally, code verification is conducted to verify that the submitted code complies with DeAI standards.\n\u25cf Pre-training is the stage where data and compute resources necessary for the training process are set up. This step includes data preparation and compute power preparation. During data preparation, the data is collected, cleaned, preprocessed, and partitioned to ensure compatibility with the training process while maintaining privacy and security. For compute power preparation, the required computational resources, such as GPUs, CPUs, or distributed systems, are allocated and configured to enable efficient and reliable training.\n\u2462 On-training focuses on how information and updates are managed safely and efficiently during the training process, ensuring proper communication and parameter sharing among the participating nodes. This step contains model training and model validation. During model training, participating nodes collaboratively update model parameters by processing local or public datasets and maintaining synchronization. During model validation, the trained models are evaluated on a shared validation dataset or through cross-validation to ensure their accuracy and generalization capability across participants.\n\u2463 Post-training is the phase where the focus shifts from model development to deployment and utilization. In this stage, the trained model is shared and integrated into applications. It ensures the model is ready for real-world use and continuous improvement. This step includes model inference, Al agents and model marketplaces. During model inference, the trained model is employed to generate predictions or perform tasks on new data, ensuring accuracy and efficiency in diverse applications. For Al agents, the model is embedded into autonomous systems that interact with users or environments to execute tasks intelligently. In model marketplaces, the trained model can be shared, traded, or monetized, fostering collaboration and accessibility for broader use cases."}, {"title": "B. Blockchain Functionalities in DeAI", "content": "The functionalities of blockchain in each category within DeAI are diverse and offer significant advantages in enhancing the security, transparency, and efficiency of DeAI processes. These functionalities can include:\nIncentive Mechanism: A blockchain-based incentive mechanism can incentivize participants to contribute to the DeAI ecosystem. For example, data providers, model trainers, and validators can be rewarded with tokens or cryptocurrency for their contributions. This mechanism encourages active participation and ensures that contributors are fairly compensated, fostering a collaborative and sustainable DeAI community.\nDecentralized Permission Control: Blockchain enables a decentralized and trustless environment where permission control is managed through smart contracts or consensus mechanisms. This ensures that access to data, models, and computational resources is securely controlled without relying on a central authority. In a collaborative Al environment, decentralized permission control can prevent unauthorized access and maintain the integrity of shared resources.\nData Storage: Blockchain can provide a secure, immutable, and decentralized data storage solution for AI. By storing datasets or model parameters on a blockchain or an associated decentralized storage network, the integrity and availability of the data are ensured. This is particularly important for maintaining transparency and trust in Al applications, as all data modifications and access can be traced and audited.\nPublic Reference: Blockchain can serve as a public reference for Al models and datasets, ensuring that they are accessible and verifiable by anyone. This public ledger can store cryptographic hashes or metadata of models and datasets, providing proof of their existence and authenticity at a specific point in time. This transparency is crucial for building trust in ML systems, as stakeholders"}, {"title": "IV. PRE-TRAINING", "content": "A. Data Preparation\nData preparation involves processes such data collection, cleaning, normalization, transformation, and feature selection. These steps are crucial for effective AI model training and can directly affect model performance, especially accuracy [99]. Well-cleaned data can also reduce dimensionality, mitigate the overfitting problem, and enhance model interpretability [100].\n1) Weaknesses of Centralized Data Preparation: The effectiveness of LLMs substantially depends on access to vast amounts of high-quality, diverse, and well-labeled data. While computational power and model architectures have seen significant advancements, the availability of such data has not kept pace [101], [102]. Recent advancements in LLMs have significantly increased the demand for large volumes of high-quality public data for training purposes. OpenAI's GPT-2 was trained on approximately 40GB of text data [103], while its successor, GPT-3, utilized an estimated 1.2TB of data [104]. Similarly, Google's T5 required about 750GB of cleaned text [105], and their PaLM model consumed an estimated 3.1TB of data [106]. Meta's LLaMA pushed these boundaries further by training on 1.4 trillion tokens, amounting to an estimated 5.6TB of data [107]. Moreover, recent analyses estimate that the total amount of high-quality, publicly available text data suitable for training LLMs is approximately 6TB [108], indicating that we are nearing the limit of available data for future models. The escalating data requirements underscore a critical challenge for centralized AI: the finite pool of publicly available high-quality data is nearing exhaustion [109], which hampers the potential for further scaling of LLMs and restricts model performance enhancements. Thus, being reliant solely on publicly available, centralized data presents limitations in several key areas. First, the pool of openly accessible data is finite, leading to potential saturation where new insights from existing data sources diminish, thus restricting model performance. Second, centralized data collection often lacks representation across domains, languages, and regions, which can result in biased and less effective models for diverse applications. Furthermore, the increasing concerns around data privacy and security hinder access to private, domain-specific datasets, which are crucial for developing specialized and high-performance models.\n2) Decentralized Solutions for Data Preparation: Decentralized data preparation offers a compelling solution by enabling access to diverse and geographically distributed data sources without requiring centralized storage. For instance, through blockchain or federated learning frameworks [17], data providers can contribute data securely, preserving privacy and ensuring data ownership. However, different from centralized systems, where a single entity oversees data quality, a decentralized approach lacks a central authority to manage data submissions. This raises the following challenges:\u2460 Malicious participants may contribute harmful or low-quality data; \u2461 Free-riding actors may exploit the system for rewards without actual contributions; \u2462 The private data of honest participants might be leaked in a decentralized system.\nIn the following, we investigate existing methods for overcoming these challenges in decentralized data preparation, as well as the relevant protocols [22], [56]\u2013[66].\na) Incentive Mechanisms for Data Contributions: To incentivize participants to contribute high-quality data, existing decentralized data preparation platforms typically leverage blockchain to build reward mechanisms.\nDataset Tokenization. Existing data preparation platforms, such as Ocean Protocol [57] and Vana [22] support decentralization by providing a marketplace where data assets are tokenized. Such a solution allows data providers to publish datasets as datatokens, which are tokens that represent access to the underlying data. Data consumers can purchase these datatokens to access or use the data, creating a market-driven approach to data sharing. Ocean Protocol also uses staking and curation to promote high-quality data contributions. Users can stake tokens on data assets they believe are valuable, and in return, they earn a portion of the transaction fees when others access that data. This mechanism incentivizes users to identify"}, {"title": "b) Data Privacy Protection:", "content": "Privacy enhancing tech-nologies, such as public encryption and zero-knowledge proofs (ZKPs), are typically used to protect the privacy of data contributors in decentralized data preparation platforms. For instance, Vana [22], [110] incorporates ZKPs, such as Groth16 [111], with blockchain to verify the authenticity and integrity of data without revealing its full content. Specifically, when data contributors or custodians submit data, a zero-knowledge proof is generated and recorded on the blockchain. This on-chain verification allows validators to confirm that the data meets certain criteria without accessing the actual data, ensuring the data privacy is maintained. Ocean Protocol [57] employs encryption and blockchain-based access control mechanisms to protect data privacy. When a user wishes to access a dataset, they must acquire the corresponding datatoken, effectively buying a token that serves as a key to unlock the data. Blockchain's smart contracts enforce these access controls, ensuring that only authorized users can retrieve and work with the data."}, {"title": "c) Data Verification and Authentication:", "content": "As Al systems increasingly depend on large datasets, ensuring that this data is authentic and unaltered is critical for maintaining the reliability of AI models. Blockchain's cryptographic mechanisms enable the detection of any data tampering, making it a valuable tool for safeguarding data integrity. By leveraging blockchain's inherent characteristics, such as immutability, transparency, and decentralization, data can be securely traced and verified without reliance on a central authority. For example, Numbers Protocol [58] establishes a transparent, immutable ledger that records the history and metadata of each data piece or digital asset. Storing this information on the blockchain ensures that all transactions and modifications are permanently recorded and cannot be altered retroactively. Additionally, ownership and licensing information can be embedded directly into the content's digital footprint through smart contracts. These on-chain smart contracts can automatically manage data rights and permissions, ensuring that creators receive proper credit and compensation for their work."}, {"title": "\u25c7 Insight 1. Decentralized data preparation requires:", "content": "ensuring the authenticity and integrity of data and digital content; \u2461 establishing well-defined, robust, and sustainable incentive mechanisms to encourage broader data contributions; and \u2462 implementing systems that protect data privacy to foster trust among data contributors."}, {"title": "3) Discussion:", "content": "Despite the existing attempts [22], [56]\u2013[58] to build decentralized data preparation systems, addressing the trade-offs between rewards, privacy, and authenticity in these solutions highlights several pressing research gaps. First, optimizing incentive structures that offer fair rewards without risking inflation or reward dilution remains a challenge, as current mechanisms such as Liquidity Pools [22] and Revenue Rights Certificates [56] may vary widely in effectiveness and scalability. Second, safeguarding privacy in a decentralized setting requires advanced cryptographic techniques, such as ZKPs and Trusted Execution Environmentss, but these methods may face limitations in terms of computational load and compatibility across diverse data types and platforms. Additionally, ensuring data authenticity through decentralized consensus mechanisms, such as Proof of Contribution in Vana [22], may present scalability issues, particularly as data volumes increase and the need for real-time validation grows. Additionally, once data consumers have paid the fees and gained access to the data, they may forward the data to other consumers without sharing any rewards with the original data providers. Further research is needed to address these gaps by developing more efficient reward models, scalable privacy solutions, and lightweight yet robust consensus algorithms that together balance data contributor incentives, privacy preservation, and data authenticity in decentralized ecosystems."}, {"title": "O Gap 1. How to develop efficient and scalable privacy", "content": "solutions, and lightweight yet robust consensus mechanisms that collectively balance data contributor incentives, privacy, and authenticity in decentralized ecosystems?"}, {"title": "B. Decentralized Compute", "content": "Compute resource plays a pivotal role in the development of Al systems and directly impacts model training and inference performance. Large AI models rely on vast datasets and intricate architectures, especially in deep neural networks (DNNs) that can contain millions or even billions of parameters. During the training phase, models learn patterns from data by optimizing these parameters through iterative processes, which involve substantial matrix operations and gradient descent algorithms [102]. For instance, in the case of backpropagation training algorithm [112], each iteration requires the adjustment of a large number of weights. The computational cost scales exponentially with the number of parameters, making traditional CPUs inefficient for such tasks."}, {"title": "2) Blockchain-Enabled Decentralization of Compute Ac-", "content": "cess: Given the significant barriers in accessing affordable and scalable compute resources for large-scale AI training, blockchain technology offers a transformative solution. The properties of blockchains, including transparency, tokenized incentives, and decentralization, provide innovative ways to address the challenges associated with compute in AI training [25], [67]\u2013[76], [79].\nPermissionless Access to Compute. Blockchain's trustless nature allows compute users and providers to interact without the need for intermediaries, such as cloud service providers. This enables a permissionless environment where anyone with underutilized compute power can contribute, and AI developers can access those resources directly. For instance, Lilypad [76] leverages blockchain to create a distributed network where users can run containerized workloads on idle compute nodes without relying on centralized providers. This opens up access to high-performance compute for AI training, as anyone with idle hardware can participate, contributing to the overall decentralization of compute access.\nIncentive Mechanisms for Efficient Resource Utilization. Blockchain-based compute networks [25], [67] introduce tokenized economies, where compute contributors are rewarded with tokens for their participation in a fair way. This design allows compute providers to monetize their idle resources and makes high-performance compute more affordable. For example, IO.NET [67] implements a decentralized marketplace where GPU users can pay for compute resources with tokens. GPU owners are incentivized to offer their idle GPUs, while users benefit from a competitive, market-driven pricing system that balances supply and demand. Similarly, Akash [25] also implements a tokenized system where compute providers are rewarded based on a Proof-of-Stake consensus mechanism. In Akash, providers stake tokens to contribute compute resources, and rewards are distributed based on network usage and demand. Akash also implements a reverse auction model which enables AI developers to access compute at market-driven prices, ensuring a cost-efficient resource allocation.\nDecentralized Compute Resource Scalability. Blockchain enables scalable networks that aggregate compute resources from a global pool of contributors. This allows decentralized compute networks to handle large-scale AI workloads while maintaining flexibility and performance. Lilypad's [76] distributed architecture allows it to scale horizontally by adding more compute nodes as demand increases. This enables AI developers to access compute resources on demand, with tasks dynamically allocated to available nodes, ensuring scalability for compute-intensive workloads such as large language models. Similarly, Render Network [69] leverages its P2P architecture to dynamically allocate GPU resources for AI computation tasks. As more nodes join the network, Render can scale its compute capacity, ensuring that workloads are distributed efficiently across the network.\nCompute Task Verification. Blockchain's immutability and transparency ensure that every task and transaction is recorded on a decentralized ledger, enabling verifiable compute pro-"}, {"title": "\u25c7 Insight 2. Decentralized computing systems require:\u2460", "content": "permissionless access to computing resources; \u2461 incentive mechanisms to promote efficient utilization of these resources; scalable solutions for managing decentralized computing resources; and \u2463 mechanisms to ensure the security, privacy, and integrity of computing resources."}, {"title": "3) Discussion:", "content": "Staking mechanisms contribute significantly to the security and reliability of decentralized compute by aligning participants' incentives with network stability. Beyond its role in securing the network, staking also serves as a signal of demand in decentralized systems, acting as a proxy for market value and influencing rewards distribution.\nAlternatives for Staking. It is important to note that staking is not the only way to achieve these goals. For instance, Lilypad [76], instead of using PoS, opts for a Proof of Work (POW) consensus mechanism. Specifically, nodes must be online for a minimum of four hours a day continuously in order to be eligible for rewards. To verify the online time of individual nodes, Lilypad uses PoW to ensure compliance. This approach provides an alternative means of ensuring network reliability and incentivizing participation without relying on staking.\nTokenomics Inflation and Deflationary in Decentralized Compute. Different models are employed in tokenomics for decentralized computing. For example, Akash [25] uses an inflationary model, while IO.NET [67] adopts a disinflationary approach. Specifically, IO.NET reduces emitted rewards each month after the first year, whereas Akash's inflationary model starts at an inflation rate of 100% that halves every two years. In theory, an inflationary model can effectively incentivize compute providers during the early stages of network development, whereas a disinflationary model may help sustain the network's long-term economic health. An open question is to analyze the empirical impacts of these tokenomic models on decentralized compute networks, particularly given the fluctuating costs of compute resources such as GPUs."}, {"title": "O Gap 2. What are the empirical impacts of inflationary", "content": "versus deflationary tokenomics models on decentralized compute networks?"}, {"title": "V. ON-TRAINING", "content": "A. Model Training in Traditional AI\nIn traditional centralized AI platforms, the process of training an AI model is typically managed by a single entity that controls both the data and the computational resources. Once a task for a target model is defined and the necessary training data is prepared, the following steps are undertaken:\nModel Training: The AI model is trained using computational resources provided by centralized data centers. These resources, such as GPUs or TPUs, are controlled and managed by a central authority. The central authority oversees the entire training process, which includes adjusting hyperparameters, monitoring model performance, handling data preprocessing, and ensuring the computational infrastructure is functioning optimally. For instance, Google and Facebook train their AI models on proprietary datasets using their own data centers [8].\nModel Evaluation: After the initial training phase, the model is evaluated using a validation dataset to assess its performance, to determine whether the model generalizes well to unseen data. The evaluation process often involves calculating metrics such as accuracy, precision, recall, or F1-score, depending on the task. If the performance is not satisfactory, the model may be retrained with adjusted hyperparameters or with additional data augmentation techniques. All these activities occur within the centralized infrastructure, with data, model updates, and results stored and managed centrally. For example, in the development of image recognition models using ImageNet [119], organizations download the dataset, train their models on local or cloud-based centralized servers, and evaluate performance internally before deploying the models.\nB. Challenges in Traditional Centralized AI Training\nDespite the widespread adoption of centralized AI training platforms, several challenges raise concerns:"}, {"title": "\u2022", "content": "Data Privacy and Control: Centralized platforms require all training data to be uploaded to a central server, raising concerns about data privacy and security, especially when dealing with sensitive or proprietary information. For instance, hospitals may be reluctant to share patient data with external entities due to privacy laws such as the Health Insurance Portability and Accountability Act (HIPAA) [120], hindering collaborative AI development in healthcare."}, {"title": "\u2022", "content": "Resource Centralization: The reliance on centralized computational infrastructure means that only entities with significant resources can afford to train large-scale models. Training state-of-the-art models often requires extensive computational power and storage capacity, creating barriers for smaller companies or research institutions. For example, training models such as GPT-3 requires vast resources that are typically beyond the reach of most organizations [121]."}, {"title": "\u2022", "content": "Single Point of Failure: Centralized systems might be vulnerable to outages and attacks. If the central server experiences downtime or is compromised, the entire training process can be disrupted. Moreover, central servers can become bottlenecks, limiting scalability when more users or data are added to the system."}, {"title": "\u2022", "content": "Trust and Transparency: Users must trust the central authority to handle data responsibly and to train models without introducing bias or errors. However, without transparency into the training process, it is difficult to verify that the models are being developed ethically and effectively. Instances of data misuse or lack of transparency can erode trust in AI systems [1]."}, {"title": "C. How Blockchain Makes AI Training Decentralized", "content": "Decentralized training platforms, such as Bittensor [77], FLock.io [21], Numerai [78], and Commune AI [79], leverage blockchain to distribute AI training tasks across a P2P network of participants, addressing key challenges in traditional centralized AI platforms in the following ways:\nTrustless and Transparent Training: Blockchain enables a trustless environment where participants do not need to rely on a central authority. All interactions, such as task assignments, model updates, and reward distributions, are recorded on an immutable ledger, ensuring transparency. For instance, in Bittensor, the rules governing the training process are encoded and the contributions of participants are recorded on the blockchain. This transparency builds trust among participants and allows for the verification of the training process. Similarly, Numerai provides a decentralized data science competition where participants worldwide can download training data, develop models locally, and submit predictions to the network.\nDecentralized Model Validation: After nodes complete their training tasks, the model updates are validated in a decentralized manner. Validators within the blockchain network are responsible for verifying the correctness of these updates. In Numerai, models are trained independently by participants, and their predictions are aggregated to form a meta-model that the platform uses for trading in financial markets. The performance of individual models is evaluated based on their contribution to the overall performance, ensuring that malicious or low-quality models do not negatively impact the system.\nConsensus and Incentive Mechanisms: Participants are rewarded based on their performance on the platforms, with the reward amounts determined by predefined consensus mechanisms. For instance, the Bittensor Yuma Consensus [20] allocates rewards to miners and validators according to their contributions. Similarly, in Flock.io [21], participants stake tokens to join as training nodes or validators, receiving rewards proportional to their performance and adherence to expected outcomes. Such incentive mechanisms encourage participants to develop high-quality models."}, {"title": "D. Key Functionalities Blockchain in DeAI Training", "content": "Blockchain technology introduces several key functionalities that enhance decentralized AI training:\nPublic Task Coordination and Distribution: Blockchain enables the coordinated and fair distribution of AI training tasks across the network. For example, in both Bittensor [20] and Flock.io [21], tasks are automatically assigned to training nodes based on decentralized protocols. The blockchain ensures that tasks are fairly allocated and that all nodes can view task assignments and completions, providing transparency that prevents any single node from monopolizing tasks or manipulating the process.\nFair Incentivization Mechanisms: Blockchain provides fair incentivization mechanisms by using decentralized, transparent protocols that allocate rewards based on objective contributions. Consensus mechanisms on blockchain platforms ensure that rewards are distributed equitably among participants, preventing any single entity from manipulating the reward process or monopolizing tasks. This transparency and decentralization motivate participants to contribute high-quality work, as they are rewarded fairly for measurable performance.\nImmutable Audit Trails: Every interaction in the blockchain is recorded immutably, creating a transparent audit trail. This is crucial for accountability and trust among participants. All transactions, including data sharing, model updates, and reward distributions, are recorded on the blockchain, allowing anyone to verify the history and integrity of the system.\nDecentralized Governance: Blockchain enables decentralized governance for DeAI through token-based voting mechanisms. DeAI stakeholders participate in governance decisions, influencing the system's direction and development [20], [79]. This decentralized control ensures that the system evolves according to the collective interests of its participants, rather than being directed by a central authority."}, {"title": "\u25c7 Insight 3. Decentralized training systems require:", "content": "verified task creation; \u2461 trustless and transparent Training; decentralized model validation; \u2463 fair incentive mechanisms; and decentralized governance."}, {"title": "E. Discussion", "content": "In implementing DeAI training", "Training": "One core challenge in DeAI training is verifying that participants genuinely contribute to model training", "122": ".", "proof-of-learning": "echanism [123", "125": "could be implemented. Proof-of-learning would require participants to demonstrate that their model has been genuinely trained on the provided dataset within the decentralized network. Techniques such as periodic accuracy or loss validation and comparison with expected learning curves could help in verifying that a model has been authentically trained. Additionally", "126": [128], "Security": "Finally, designing a fair and secure incentive mechanism is essential to maintaining participant motivation while safeguarding the network from misuse. As participants are rewarded for their contributions, care"}]}