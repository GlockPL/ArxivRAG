{"title": "LEVERAGING PROMPTS IN LLMS TO OVERCOME IMBALANCES\nIN COMPLEX EDUCATIONAL TEXT DATA", "authors": ["Jeanne McClure", "Machi Shimmei", "Noboru Matsuda", "Shiyan Jiang"], "abstract": "Background: The study addresses the challenge of imbalances in educational datasets, which is\nprominent in the education sector due to the varied cognitive engagement levels among students in\ntheir open responses. Traditional machine learning (ML) models often struggle with the complexity\nand nuanced nature of this data, leading to inadequate analyses, especially for minority data represen-\ntations [Karimah and Hasegawa, 2022, Radwan and Cataltepe, 2017, Yun et al., 2011]. Understanding\nstudents' cognitive engagement is vital as it reflects their mental investment in learning activities,\nwhich is closely linked to academic success [Fredricks et al., 2004, Blumenfeld et al., 2006, Corno\nand Mandinach, 1983, Pintrich, 2000, Schunk et al., 2014].\nObjective: The objective of this paper is to investigate the efficacy of Large Language Models (LLMs)\nenhanced with assertions in tackling the complexities of imbalanced educational datasets, with a\nspecial focus on the precise classification of cognitive engagement levels from student texts. This\nexploration is underpinned by two critical research questions. The first seeks to evaluate how LLMs\nequipped with Prompt Engineering fare in comparison to conventional ML algorithms when dealing\nwith the inherent challenges of imbalanced educational data. The second question delves into the\nspecific contributions of integrating assertions into LLMs, examining how such augmentations can\nimprove the models' effectiveness in handling the nuanced difficulties presented by imbalanced\ntextual educational datasets. Through this inquiry, the study aims to shed light on the potential of\nLLMs and assertions in enhancing the accuracy and reliability of cognitive engagement classification,\nthereby addressing a significant gap in educational data analysis.\nMethods: The study employed an 'Iterative - ICL PE Design Process' to compare traditional ML\nmodels against LLMs augmented with assertions (N=135). A sensitivity analysis on a subset (n=27)\nexamined variance in model performance concerning classification metrics and cognitive engagement\nlevels. This process involved the utilization of assertion-based prompt engineering, comparing the\nperformance of traditional ML models to LLMs with assertions in classifying cognitive engagement\nfrom student texts in an educational setting [Shahriar et al., 2023, Brown et al., 2020, Wei et al.,\n2022a].\nFindings: LLMs with assertions significantly outperformed traditional ML models, especially in\nrecognizing cognitive engagement levels with minority representation, showing up to a 32% increase\nin F1-score. Incorporating targeted assertions into the LLM on the subset enhanced its performance\nby 11.94%, primarily addressing errors from limitations in understanding context and resolving\nlexical ambiguities in student responses.\nImplications: The study demonstrates the superior capability of LLMs, particularly when augmented\nwith assertions, in addressing the nuanced challenges of imbalanced educational datasets. This\nadvancement not only improves the accuracy of classifying cognitive engagement levels but also\nopens new avenues for data-driven educational research and practice. The findings suggest a potential\nparadigm shift towards employing advanced LLM techniques in educational settings to achieve a\nmore nuanced and accurate analysis of student engagement, thereby enhancing learning outcomes.\nFuture research should further explore the capabilities of LLMs across broader educational contexts", "sections": [{"title": "1 Introduction", "content": "Understanding students' cognitive engagement (CE) at both the school and task levels is crucial, as it offers deep insights\ninto their commitment to learning [Fredricks et al., 2004]. This form of engagement, characterized by a student's\ndeliberate and intentional approach to schoolwork and their willingness to invest the necessary effort in comprehending\ncomplex concepts and mastering challenging skills, serves as a key indicator of academic success [Fredricks et al.,\n2004, Blumenfeld et al., 2006]. CE encompasses the psychological investment and effort driven by student motivation\nand strategies, alongside their dedication to learning [Corno and Mandinach, 1983, Fredricks et al., 2004, Pintrich,\n2000, Schunk et al., 2014].\nWhile analyzing students' CE is crucial for enhancing learning experiences, a significant challenge arises from\nimbalanced datasets [Radwan and Cataltepe, 2017]. These datasets often feature unevenly distributed categories and are\ntypically small, not fitting the 'big data' criteria usually required for effective Machine Learning (ML) training. This\nsize limitation, along with the disproportionate representation of majority and minority data, further complicates the\ntraining process in traditional analyses [Yun et al., 2011]. Traditional ML methods, commonly employed to classify CE,\noften struggle to adequately address these imbalances, raising concerns about the accuracy and reliability of their results.\nThis issue presents a major hurdle in accurately assessing and interpreting CE, as the uneven representation of data\ncan lead to skewed insights and potentially overlook critical aspects of student engagement [Karimah and Hasegawa,\n2022]. This imbalance in datasets not only complicates the analysis but also raises concerns about the reliability and\ngeneralizability of the findings in diverse educational settings [Radwan and Cataltepe, 2017].\nThe exploration of LLMs provides a promising solution to the limitations of traditional ML approaches. Recent studies,\nincluding [Wu, 2021], have highlighted the potential of prompt engineering in reducing the need for extensive training of\ncase labeling which is imperative for imbalance data. LLMs employ techniques like In-context Learning (ICL) [Brown\net al., 2020] and Chain-of-Thought (COT) prompting [Wei et al., 2022b], enabling more nuanced and context-aware\nresponses. ICL trains models using examples in specific contexts, improving with scaled model and corpus sizes, as seen\nin N-shot prompting [Brown et al., 2020]. This is illustrated by Brown et al. [2020]'s few-shot learning, where LLMs\nprocess input-output pairs in-context, leading to better test-time predictions. Similarly, COT, by Wei et al. [2022b],\ninvolves logical, step-by-step natural language reasoning. Furthering this, Shahriar et al. [2023] developed Assertion\nEnhanced Few-Shot Learning, incorporating domain-specific assertions in prompts to enhance accuracy and reduce\nerrors. These innovations significantly boost LLMs' task-specific efficiency, surpassing traditional methods.\nWhile LLMs have shown potential in educational research, their application has predominantly been refined to solve\nlogical reasoning or arithmetic problems [Lee et al., 2024], with limited exploration in addressing imbalanced datasets\nof education. Our study breaks new ground by applying LLMs with Prompt Engineering (PE) to this specific challenge.\nWe hypothesize that LLMs, renowned for their nuanced language understanding, will surpass traditional ML algorithms\nin classifying cognitive engagement levels from student texts. Our exploration is guided by two research questions:\nRQ1 addresses the comparative efficacy of LLMs against traditional ML algorithms, and RQ2 investigates the role of\nassertions in overcoming contextual and lexical challenges within imbalanced datasets. Specifically:\n1.  How do the results obtained from LLMs with PE compare to traditional Machine Learning algorithms in\nhandling imbalanced educational data?\n2.  In what ways does the integration of assertions enhance the efficacy of models when addressing the challenges\nassociated with imbalanced textual educational datasets?\nThis paper examines how AEFL mitigates issues in imbalanced educational data analysis, revealing how these\ntechnologies can effectively address the challenges posed by uneven dataset distributions. By applying this cutting-\nedge technique, we uncover new possibilities for analyzing and interpreting complex educational data. Our findings\ndemonstrate the advantage of AEFL in educational settings, especially where traditional ML methods fall short, opening\nnew avenues for data-driven educational research and practice.\nThe rest of the paper is set up as follows: Section 2 delves into the background, highlighting the emergence of LLMs\nas a promising solution in education. Section 3 outlines our methodology, including the Iterative - ICL PE Design\nProcess, and the experimental setup. The results and discussions are presented in Section 4, where we compare the\nperformance of LLMs augmented with assertions against traditional ML models and discuss the impact of assertions on\nmodel efficacy and limitations. Finally, Section 5 concludes with our findings and future directions."}, {"title": "2 Background", "content": "The exploration of CE within educational research has significantly evolved, transitioning from a simplistic focus on\nstudent participation to a complex understanding of mental investment in learning activities. This shift is paramount\nfor fully capturing the essence of engagement, as initially highlighted by Craik and Lockhart [1972] through their\ndistinction between shallow and deep processing. Subsequent work by Appleton et al. [2006] and Fredricks et al. [2004]\nexpanded the concept to encompass behavioral, emotional, and cognitive dimensions, underscoring engagement's\nmultifaceted nature across various educational contexts. A pivotal insight from this exploration is the strong positive\ncorrelation between student learning and cognitive engagement, evidenced by Chi and Wylie [2014], which underscores\nthe significant educational outcomes associated with deep cognitive processes.\nCE distinguishes itself within the broader spectrum of educational engagement by focusing on the intensity of students'\nmental investment in learning. This stands in contrast to behavioral engagement's emphasis on participation and\nemotional engagement's concern with feelings towards learning Blumenfeld et al. [2006]. Such a distinction is crucial\nfor educators and researchers dedicated to enhancing learning outcomes through targeted interventions.\nCentral to understanding and enhancing CE are theoretical frameworks and models like Bloom's taxonomy, Corno and\nMandinach's model, and the ICAP model, as well as Wang et al.'s framework for connectivist learning contexts. These\nmodels provide comprehensive insights into the various dimensions and components of cognitive engagement, aiding\nresearchers in designing effective studies, developing targeted interventions, and evaluating educational outcomes\n[Anderson and Krathwohl, 2001, Bloom et al., 1956, Corno and Mandinach, 1983, Chi and Wylie, 2014, Chase et al.,\n2019, Hsiao et al., 2022, Wang et al., 2016].\nMeasuring CE, however, presents inherent challenges due to its complex and internal nature. As a latent construct,\nCE's assessment relies on inferences from behavioral indicators or through self-report measures [Chi and Wylie, 2014,\nFredricks et al., 2004, McCoach et al., 2013]. Traditional methods, including self-report questionnaires, surveys, and\nobservational techniques, often inadequately capture the nuanced cognitive processes involved in learning. A variety\nof measures have been employed in past studies to gauge CE, such as self-reported scales, classroom observations,\ninterviews, teacher ratings, experience sampling, eyetracking, physiological sensors, trace analysis, and content analysis\n[Greene et al., 2004, Smiley and Anderson, 2011, Lee and Anderson, 1993, Helme and Clarke, 2001, Wigfield et al.,\n2008, Xie et al., 2019, D'Mello et al., 2017, Bernacki et al., 2012, Ireland and Henderson, 2014]. Nonetheless, the\ncomplexity of accurately assessing CE through these measures necessitates innovative approaches that more precisely\nreflect students' cognitive investment in their educational activities [Fredricks et al., 2004].\nIn educational research, traditional ML methods have extensively analyzed student data patterns but face limitations\nwhen addressing nuanced aspects like cognitive engagement. The problem is exacerbated by imbalanced datasets,\nleading to skewed insights and overlooking crucial engagement aspects, thus affecting the findings' accuracy, reliability,\nand generalizability across diverse educational contexts [Lee and Kinzie, 2012, Fredricks et al., 2004]. This issue with\nimbalanced datasets, characterized by unevenly distributed categories and small sample sizes, highlights the need for\nspecialized techniques to improve model performance and accuracy, ensuring a comprehensive understanding of CE\nacross educational contexts [Chawla, 2010, Fern\u00e1ndez et al., 2018, Kulkarni et al., 2020, Japkowicz and Stephen, 2002,\nBruce et al., 2020, Lema\u00c3\u017dtre et al., 2017].\nThe advent of LLMs presents a promising solution to the issues posed by imbalanced datasets in educational research.\nRecent breakthroughs in LLMs, particularly with ICL, COT and AEFL prompting techniques, have demonstrated their\npotential to generate nuanced, context-aware responses beyond the capabilities of traditional ML methods [Brown et al.,\n2020, Wei et al., 2022b, Shahriar et al., 2023]. For example, Savelka et al. [2023] showcased how GPT-3.5 & 4 could\neffectively classify student help requests in programming courses, illustrating the superior ability of LLMs to handle\nnuanced educational data. Zeng et al. [2023] delved into the cognitive and reasoning abilities of LLMs, highlighting\nthe necessity for task-specific tuning to address complex reasoning challenges. Cui et al. [2023] introduced the\nDivide-Conquer-Reasoning (DCR) framework to enhance the consistency and reliability of LLM-generated texts, vital\nfor creating educational content. These examples reveal the capacity of LLMs to offer more accurate classification and\nanalysis of CE, surpassing traditional ML methods in dealing with the intricacies of educational datasets. Additionally,\nLee et al. [2024] explored LLMs' use with CoT prompting to improve automatic scoring systems in science education,\nfurther indicating LLMs' potential to enhance the quality and reliability of educational content analysis.\nBy harnessing the intrinsic capacity of LLMs to interpret and utilize language within specific contexts, researchers can\nnavigate the challenges posed by imbalanced datasets, facilitating a deeper understanding of student CE."}, {"title": "3 Methodology", "content": "This study performs a secondary analysis on a dataset originally gathered to assess CE from student responses in a\nHigh School English Language Arts course's AI curriculum. The StoryQ curriculum [Chao et al., 2022], spanned\nthree weeks with daily 45-minute classes, incorporated Machine Learning Practices through open-ended questions in\neight modules but our analysis only evaluated three: \u201cSentiment Analysis,\u201d \u201cFeatures and Models,\" and \"All Words.\u201d\nThe initial study's diverse participant group of 28 students included 17 females, 7 males, and 4 non-specified gender\nindividuals, spanning various grades and racial backgrounds. The racial composition was 43% Black/African American,\n17% Hispanic/Latinx, 18% White/Caucasian, with others choosing not to disclose. Students' CE was evaluated\nusing a modified Interactive-Constructive-Active- Passive (ICAP) framework by Chi and Wylie [2014], focusing on\nConstructive, Active, and Passive levels. Their open-ended responses (N = 840) were analyzed using the CE coding\nscheme, yielding a Cohen's kappa inter-rater reliability of 0.84."}, {"title": "3.2 Prompt Engineering Design", "content": "Our prompt development process, grounded in the ICL Prompt Engineering Design begins with drafting\nan initial few-shot ICL format prompt. This prompt, inputting student responses and outputting CE classifications,\nundergoes validation testing on a subset (n=27). If benchmarks are met, it progresses to full dataset testing; otherwise, we\ndiagnose misclassifications, realigning LLM outputs with our coding standards through domain-specific CE knowledge\nintegration. Adjustments may involve refining COT processes, FewSHOT learning, or embedding conceptual knowledge\nassertions. After subset retesting and validation, the optimized prompt is applied to the full dataset (n=135), with\niterative refinement ensuring optimal performance. See Appendix B for additional LLM-specific prompt details.\nOur engineering approach encompasses three components: General COT, FewShot with Reasoning Sequence, and\nassertions Prompting. General COT, embeds sequential instructions with \u201cthink time\" to initiate the model's reasoning\non given tasks [Fulford and Ng, 2023]. Our General COT prompt follows a seven-step sequence to guide the LLM's\ntask reasoning. Initially, the model attentively reads the provided \u00abQuestion, Response\u00bb (Step 1), laying\nthe foundation for accurate comprehension and subsequent cognitive engagement analysis. Step 2 involves feeding the\nmodel CE domain-specific definitions for Passive, Active, and Constructive levels, requiring it to discern the appropriate\nengagement level based on the initial input. Progressing to Step 3, the model assesses the rationale behind the assigned\ncognitive engagement label, ensuring it reflects the response's depth and nature. In Step 4, the LLM reevaluates the\nresponse to prevent misclassification and assesses if a different CE level is more aligned. Steps 5 and 6 prompt the\nmodel to consider ways to enhance the CE level, crucial in the validation and diagnostic phases, particularly when\nintegrating assertions. The final step (Step 7) circles back to the initial input, where the LLM reexamines the cognitive\nengagement level to verify the accuracy and consistency of its prediction. This structured approach is key in sharpening\nthe model's evaluative and analytical capabilities.\nFewShot with Reasoning, guided by gold standard examples [Wang et al., 2023, Shahriar et al., 2023], includes a\nfour-element structure: \u00abQuestion, Response, Label, and Reasoning\u00bb. This method enhances LLM's task-specific"}, {"title": "3.3 Experiment Design", "content": "To analyze traditional ML methods (SVM, RF, DT, and ADABoost), we divided our data into training (n=432) and\ntesting sets (n=135), applying default hyperparameters from the Scikit-Learn package (Pedregosa et al., 2011). See\nAppendix A for hyperparameters. The dataset comprised two majority classes and one minority class (see Table 2.\nDuring data preprocessing, we executed text cleaning steps: removing non-alphanumeric/special characters (except\nperiods), new lines, isolated \"n\" characters, excess spaces, double quotes, and backslashes; converting to lowercase;\neliminating stop words; and correcting spelling errors. We transformed the tokenized text using TF-IDF vectorization\nfor ML algorithm suitability. These traditional ML methods served as benchmarks for comparing with LLM prompt\nresults.\nIn analyzing LLM, we employed GPT-4 through the Colab Python OpenAI API, setting hyperparameters to temperature\n= 0 and top p= 0.01 for optimal automatic scoring [Wang et al., 2023]. The data preprocessing mirrored the traditional\nML approach but without tokenization or vectorization. We maintained the integrity of student sentences, ensuring\ncapitalized start and appropriate punctuation, mainly periods. The final prompt See Appendix B underwent testing with\nthe same dataset (n=135) used in traditional ML.\nIn our final experiment, we adopted a subset-based iterative modification approach (n=27) as per the ICL Prompt Design\nProcess 3.2. This involved a sensitivity analysis for precise influence measurement of assertions on LLM performance.\nEach iteration entailed scrutinizing misclassified data, focusing on informal language nuances in text inputs. This\nqualitative analysis was pivotal for understanding the impact on model accuracy and response. This systematic approach"}, {"title": "3.4 Analysis", "content": "In our multiclass dataset analysis, we utilize Precision, Recall, and F1 Score to evaluate the performance of LLMs\nwith assertions versus traditional ML models. These metrics are integral for assessing model efficacy in a multiclass\nenvironment. Precision gauges the model's accuracy in predicting each class, indicating the reliability of its positive\npredictions. Recall measures the model's capacity to correctly identify all instances of each class, vital for ensuring\ncomprehensive representation in a multiclass context. The F1 Score, as the harmonic mean of Precision and Recall,\noffers a balanced evaluation of the model's overall performance, particularly important in our study to address potential\nclass imbalance. Following Pennebaker et al. [2015], we emphasize both precision and recall to minimize false positives\nand negatives, crucial in multiclass datasets. Additionally, we assess the percentage change in F1 score performance to\nquantify the impact of assertions, using the following formula:\nPercent Increase = ( F1 score of LLM - F1 score of traditional ML / F1 score of traditional ML ) \u00d7 100%\nTo further this analysis we examined F1 scores. To differentiate between models, we developed a custom metric,\ninspired by Cohen's D [Cohen, 2013]. However, unlike the traditional Cohen's D, which uses standardized effect sizes\n(small at 0.2, medium at 0.5, large at 0.8) based on pooled standard deviation, our metric directly compares raw F1\nscore differences. This modification suits our data, where standard deviation calculations aren't feasible due to single\nobservations per model. We categorized differences in F1 scores as small (up to 10 points), medium (10 to 30 points),\nand large (over 30 points). We defined a function for calculating pairwise differences in scores mi, mj M represent any\ntwo models, and si, sj are their respective scores. The function:\nf(mi, mj) = si s; is defined as the difference between si and sj.\nIt computes the difference in performance scores between each pair of models. For each combination of models (mi,\nmj), the score of model mj is subtracted from that of model mi. This function calculates the performance difference\nbetween each model pair. We then generate a matrix showcasing these differences, allowing for a thorough pairwise\ncomparison of model performances.\nTo answer RQ 2 and evaluate the ways that the integration of assertions enhance the efficacy of models when addressing\nthe challenges associated with imbalanced textual educational datasets we chose to test on a subset (N=27, P = 10, A\n= 10, C= 7) as is common in the research to \"increase the depth of our analysis, reduce run-time, and decrease cost\"\n[Rodriguez et al., 2023, p. 2]. We chose a sensitivity analysis [Akinwande et al., 2023] to critically assess the impact or\ninfluence of the assertions. We did this qualitatively by adding two steps (Step 5 & 6 of General COT into the \u00abGeneral\nCOT\u00bb and interpreting for the \u00abmodel outcome\u00bb for recurring themes. Our examination extended to a comparative\nanalysis of the experiments, employing class-wise analysis to measure each experiment against a baseline prompt that\ndid not incorporate assertions."}, {"title": "3.5 RQ1: How do the results obtained from LLMs with Prompt Engineering compare to traditional Machine\nLearning algorithms in handling imbalanced educational data?", "content": "The summary results indicated a varied performance across classes. In the Passive class, the LLM significantly\noutperformed traditional models, showing a 14.9% increase over SVM, 6.25% over RF, 18.0% over DT, and a notable\n23.2% increase over AdaBoost. Conversely, in the Active class, traditional models (SVM, RF, and DT) surpassed LLM\nby 11.1%, while AdaBoost and LLM performances were comparable. The most striking contrast was observed in the\nConstructive class, where traditional models (SVM, RF, DT, and AdaBoost) failed to effectively identify instances. In\ncontrast, the LLM demonstrated a remarkable improvement with an F1 score of 32, showcasing its superior capability\nin recognizing elements of the minority class.\nThese results suggest that while traditional machine learning models like SVM, RF, DT, and AdaBoost may perform\ncomparably or better in majority classes, the LLM exhibits superior capability in dealing with minority class instances,\nparticularly in complex classification tasks like the Constructive class in our dataset"}, {"title": "3.5.2 Relative Performance", "content": "We see similar results in our custom metric inspired by Cohen's D due to the unique nature of our data, where standard\ndeviation calculations were not applicable, and produced interesting results. The LLM with assertions\nfor the passive class demonstrated noteworthy advantages over traditional models in various comparisons which\nresonate with the work of researchers [Shahriar et al., 2023], who demonstrated the enhanced effectiveness of LLMs in\neducational settings."}, {"title": "3.6 RQ2: In what ways does the integration of assertions enhance the efficacy of models when addressing the\nchallenges associated with imbalanced textual educational datasets?", "content": "Our analysis aimed to augment Active class metrics and foster a more equitable model across cognitive classes.\nThroughout the course of ten experiments, including the baseline, the implementation of assertions, particularly those\ndelineated in \u00ab\u00abGeneral COT\u00bb (Steps 5 & 6, see Appendix B), was pivotal in surfacing two primary themes post the\ninitial experiment: textual ambiguity and contextual comprehension challenges.\nFor text ambiguity, the baseline experiment revealed the model's propensity to misconstrue the depth of student\nengagement. Instances where contributions appeared analytical but merely constituted a superficial application of\nknown concepts underscored this issue. By systematically applying the assertions detailed in the Methodology, we\nobserved significant improvements in model performance, particularly within the Active and Constructive classes.\nWith regard to Unusual language, the model's interpretation of speculative language (e.g., \"I think,\" \"possibly,\" \"I\nbelieve\") as indicative of reflective or analytical thought. Such expressions, particularly when conveying opinions that\nsuperficially suggested deeper analysis, were erroneously classified as constructive engagement.\nInitially, our approach to integrating assertions was exploratory but became more systematic by the third experiment.\nFor example, between experiments two through four, certain responses intended as \"Constructive\" were incorrectly\nclassified as \"Active\":\nMisclassified Example 1:\nQuestion: Why do you think the model learned a large negative weight for this feature? Student\nresponse: \"I think the model learned a negative weight for this feature because the model categorized\nthe reviews as negative and categorized the surprisingly negative features as negative too since that\nwas the whole sentiment of the review.\"\nMisclassified Example 2:\nQuestion: Why do you think the model learned a large positive weight for this feature? Student\nresponse: \"I feel like it had to do with the words and how much they were used whenever there was a\npositive review it would contain more than one good word to go along with it\"\nBy incorporating the assertion \u00abDo label the statement as Constructive when they form a hypothesis about why the\nmodel learned a weight for a certain feature\u00bb, these responses were accurately predicted as constructive, enhancing\nthe Constructive class with precision and recall metrics-specifically, a recall increase of 6.33% and an F1-score\nimprovement of 4.30%."}, {"title": "4 Limitations", "content": "While our study sheds light on the potential of LLMs and AEFL in addressing imbalanced datasets, it also highlights the\nneed for caution in interpreting these findings without consideration of the broader methodological and technological\nlandscape. Firstly, our reliance on specific LLM techniques and AEFL might not capture the full spectrum of potential\nsolutions available within the rapidly evolving field of machine learning. The specific parameters and configurations\nemployed in our LLM applications [Shahriar et al., 2023, Wei et al., 2022b, Zeng et al., 2023], while effective in\nthis context, might not be universally applicable or optimal across different datasets or learning tasks. While our\nstudy provides valuable insights, it echoes the concerns raised by Radwan and Cataltepe [2017] and Yun et al. [2011]\nregarding the challenges of imbalanced datasets in education and the limitations of traditional ML approaches.\nFurthermore, our study's focus on a AI High School ELA course dataset [Zeng et al., 2023], while providing a rich\nsource of cognitive engagement data, also presents a limitation in terms of diversity and representativeness. The\nlinguistic and cognitive patterns inherent in this specific educational setting may not fully encapsulate the variety\nof cognitive engagement manifestations across different age groups, subjects, or educational methodologies. This\nlimitation underscores the importance of extending research efforts to encompass a wider range of educational contexts,\nto ensure the findings' applicability and robustness, as indicated by Fredricks et al. [2004] and Blumenfeld et al. [2006]."}, {"title": "5 Conclusion and Future Studies", "content": "Our study makes significant contributions to the evolving landscape of cognitive engagement (CE) research, building\nupon the foundational work of seminal researchers like Craik and Lockhart [1972], Appleton et al. [2006], and Fredricks\net al. [2004]. We leveraged the capabilities of Large Language Models (LLMs) and Assertion Enhanced Few-Shot\nLearning (AEFL), marking a notable advancement in the domain of CE. This approach pays homage to the pioneering\nefforts that have shaped our understanding of CE while extending these concepts through the integration of cutting-edge\nLLM technologies.\nBy adeptly navigating the challenges posed by imbalanced datasets and accurately classifying cognitive engagement\nlevels, this study underscores the potential of LLMs to refine our measurement and analysis of CE, setting a new\nbenchmark for educational research. The integration of AEFL enhances contextual comprehension, improving model\naccuracy and balance, as highlighted by Shahriar et al. [2023]. Experiment 6.1 further illustrates the value of tailored\nassertions in reducing misclassifications linked to textual ambiguities, offering novel insights into AEFL's effectiveness\nin managing class-imbalanced data."}, {"title": "A Appendix A", "content": "(0"}, {"title": "B Appendix B", "content": "A. Few Shot with reasoning + General COT (step by step) < Black, Green, Red, Blue>\nB. Few Shot with reasoning + General COT (step by step) + Assertion (do and don't)<\n----Prompt Starts Here---\nYour task is to identify the label of the statement delimited by triple backticks\nRead the instructions below:\nStep 1: Read the question and statement attentively to understand the context and the nature of the statement provided.\nStep 2: Determine the initial cognitive engagement level of the statement using the definitions of the provided cognitive\nengagement labels - passive, active, and constructive.\n1.  Passive engagement: a statement is classified as \"Passive\" when the individual is only receiving information without\ninteracting with it or adding anything to it. Passive engagement typically involves listening, reading, or receiving\ninformation without actively processing, manipulating, or reflecting upon it.\n2.  Active engagement: a statement is classified as \"Active\" when the response involves applying knowledge, analyzing\ninformation, or manipulating information but not generating new ideas or concepts.\n3.  Constructive engagement: a statement is classified as \"Constructive\" if it reflects reasoning, justification, or thoughtful\nconsideration based on prior knowledge.\nStep 3: Assess why it corresponds to the label you placed it in. Consider the extent to which it demonstrates recall of\nbasic information (passive), application of learned knowledge to slightly different contexts (active), or a deeper level of\nanalysis and synthesis of various concepts (constructive).\nStep 4: Critically evaluate whether the statement could potentially belong to other labels. Examine the nuances of the\nstatement to see if there are elements that might indicate a higher or lower level of cognitive engagement."}]}