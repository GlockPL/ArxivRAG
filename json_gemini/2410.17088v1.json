{"title": "SCIENCE OUT OF ITS IVORY TOWER: IMPROVING ACCESSIBILITY WITH REINFORCEMENT LEARNING", "authors": ["Haining Wang", "Jason Clark", "Hannah McKelvey", "Leila Sterman", "Zheng Gao", "Zuoyu Tian", "Sandra K\u00fcbler", "Xiaozhong Liu"], "abstract": "A vast amount of scholarly work is published daily, yet much of it remains inaccessible to the\ngeneral public due to dense jargon and complex language. To address this challenge in science\ncommunication, we introduce a reinforcement learning framework that fine-tunes a language model\nto rewrite scholarly abstracts into more comprehensible versions. Guided by a carefully balanced\ncombination of word- and sentence-level accessibility rewards, our language model effectively\nsubstitutes technical terms with more accessible alternatives, a task which models supervised fine-\ntuned or guided by conventional readability measures struggle to accomplish. Our best model adjusts\nthe readability level of scholarly abstracts by approximately six U.S. grade levels in other words,\nfrom a postgraduate to a high school level. This translates to roughly a 90% relative boost over the\nsupervised fine-tuning baseline, all while maintaining factual accuracy and high-quality language. An\nin-depth analysis of our approach shows that balanced rewards lead to systematic modifications in the\nbase model, likely contributing to smoother optimization and superior performance. We envision this\nwork as a step toward bridging the gap between scholarly research and the general public, particularly\nyounger readers and those without a college degree.", "sections": [{"title": "1 Introduction", "content": ""}, {"title": "1.1 Accessible Language in Science Communication", "content": "At first glance, the daily publication of tens of thousands of scientific papers-many freely accessible through open\nscience and open access initiatives\u2014suggests few barriers to knowledge dissemination. However, two key facts\nchallenge this perception and show that significant barriers remain. A recent survey by the U.S. Department of\nEducation found that more than half of U.S. adults aged 16 to 74 (54%, or 130 million people) read at or below a\nsixth-grade level (Rothwell, 2020). Meanwhile, an analysis of the readability of biomedical research abstracts published\nfrom 1881 to 2015 found that scientific writing has become increasingly hard to read over time (Plav\u00e9n-Sigray et al.,"}, {"title": "1.2 Challenges to Effective Simplification", "content": "Fine-tuning a language model using pairs of abstracts and their accessible versions is the de facto method for automating\nthe rewriting of scholarly abstracts into more accessible versions (Xu et al., 2015; Goldsack et al., 2022; Joseph et al.,\n2023). Accordingly, we introduced the Scientific Abstract-Significance Statement (SASS) corpus (Wang and Clark,\n2024), a dataset composed of paired abstracts and significance statements from diverse disciplines, with the latter\ntargeting \"an undergraduate-educated scientist outside their field of specialty\u201d (Berenbaum, 2021; Pool et al., 2021).\nAlthough the simplified abstracts generated from language models fine-tuned on the SASS corpus are approximately\nthree grade levels more readable than the original abstracts, as measured by U.S. grade-based readability scores (Wang\nand Clark, 2024, Sec. 6), the documents are still not sufficiently accessible; even the best models produce college-level\ntexts. Additionally, because the vocabulary used in significance statements is often just as complex as that found in\nthe abstracts themselves (Wang and Clark, 2024, Sec. 3), the readability improvements are primarily due to shorter\nsentences, and technical terms remain inadequately addressed.\nAlternatively, the optimization of a language model can be guided by a chosen objective in an actor-critic manner\n(Ramamurthy et al., 2023). It is intuitive to choose an established document readability measure, such as the Automated\nReadability Index (ARI; see Section 5.2.2), to assess the overall readability of the outputs generated by the language\nmodel.\u00b9 However, we found that the optimization of language models guided by ARI is highly unstable, often resulting\nin the production of seemingly more accessible versions that still contain many technical terms. Inspired by Riddell\nand Igarashi (2021), we decomposed the measurement of document readability into two distinct measures: one at the\nsentence level and one at the word level. We then prioritized word-level accessibility in the optimization to encourage\nthe model to use more accessible words instead of simply shortening sentences."}, {"title": "1.3 Contribution", "content": "Our work aims to serve as a bridge between scholarly works and the general public, particularly benefiting younger\nreaders and those without a college degree.\n1.  We address the common challenges in science communication by rewriting scholarly abstracts at a high school\nreading level using a language model.\n2.  We identify the challenges language models face in properly addressing jargon and propose Reinforcement\nLearning from Accessibility Measures (RLAM) as a means to improve the models' use of accessible terms\nin their rewrites. RLAM-trained language models can significantly reduce the reading level of a scholarly"}, {"title": "2 Literature Review", "content": ""}, {"title": "2.1 The Common Challenges of Science Communication", "content": "Science communication research uses several models to describe strategies for public understanding of and engagement\nwith science, including the deficit model, the dialogue model, and the participatory model (Trench, 2008; Hetland,\n2014; Sokolovska et al., 2019). The deficit model assumes that the public lacks scientific knowledge and that science\ncommunication should focus on simplifying complex scholarship through one-way information transmission. The\ndialogue model encourages two-way communication, where scientists and the public engage in conversations that\nfoster mutual understanding (Trench, 2008; Joly and Kaufmann, 2008). The participatory model involves the public as\nactive participants in the scientific process, recognizing their contributions as equally valuable to those of scientists\n(Brossard and Lewenstein, 2009). In practice, the dialogue model suggests that researchers should engage with the\npublic through mediums such as social media (Davies, 2008; Hara et al., 2019; Knox and Hara, 2021), by crafting more\ndigestible manuscripts in research (Maurer et al., 2021), and by developing plain language guidelines for practitioners\n(Grene et al., 2017). Communicators are encouraged to scaffold content by providing the audience with relevant, easily\nunderstandable material that either gradually increases in complexity or includes simplified explanations, examples, and\nreferences to better support the learning process (Wolfe, 2008; Landrum et al., 2014). These models often coexist and\noverlap in practice (Brossard and Lewenstein, 2009; Metcalfe, 2022), with practitioners employing multiple strategies\nto increase the readability of complex information.\nIn regular practice, two notable challenges persist for researchers when attempting to effectively communicate their\nfindings to lay audiences. First, placing the burden of effective communication on scholars, who are trained in conducting\nresearch and not in effective communication, can be overwhelming. Second, translating scientific findings that heavily\nutilize specialized jargon often involves sacrificing some specificity in favor of accessibility. Balancing the precision\nrequired by the scientific community with the clarity needed for public understanding remains a significant challenge.\nHowever, in order to uphold the fundamental principles of open science and the goal of science communication, the\npublic's understanding of scholarship should be considered a requisite part of the scholarly process (Burns et al., 2003;\nFecher and Friesike, 2014)."}, {"title": "2.2 Text Simplification", "content": "The natural language processing community addresses document accessibility through automated text simplification, a\nprocess that rewrites complex documents using simpler grammar and vocabulary while retaining the original meaning\n(Chandrasekar et al., 1996; Alva-Manchego et al., 2020; Al-Thanyyan and Azmi, 2021). In the early days of text\nsimplification, knowledge-based approaches were widely used (De Belder and Moens, 2010; Zhao et al., 2018; Hijazi\net al., 2022). These methods relied on predefined sets of linguistic rules to transform complex sentences into simpler\nforms. For example, such systems would apply syntactic transformations, such as breaking down compound sentences\ninto simpler ones, and lexical substitutions, where difficult words were replaced with more frequent, simpler synonyms.\nWhile effective in specific cases, rule-based systems were limited by the rigidity of their rules, often struggling with the\nvariability of natural language.\nWith the rise of machine learning, approaches that learn simplification patterns from large corpora of text have become\nprominent. The most common method currently is fine-tuning a pre-trained language model with parallel corpora\ncontaining paired documents of different readability levels (Xu et al., 2015; Goldsack et al., 2022; Joseph et al., 2023).\nThese parallel corpora allow the model to \u201ctranslate\u201d complex texts into simpler ones, typically using cross-entropy\nloss to align the predicted simplified version with the target text. This shift to data-driven methods allows for greater\nflexibility and more nuanced simplifications, as the models learn from a broader range of linguistic patterns.\nThe effectiveness that supervised fine-tuning can achieve depends on the quality of the parallel corpus, specifically\nwhether the simplified documents are sufficiently accessible. As an example, Devaraj et al. (2021) achieved approx-\nimately a one-grade reading level improvement, comparable to the readability of the plain-language samples, by"}, {"title": "3 Scientific Abstract-Significance Statement (SASS) Corpus", "content": "We used the Scientific Abstract-Significance Statement (SASS) corpus in our experiments. This corpus is composed of\n3,430 abstract-significance statement pairs derived from PNAS and divided into training (3,030 samples), validation\n(200 samples), and test sets (200 samples) (Wang and Clark, 2024). It covers a wide range of disciplines, ensuring\ndiverse representation across various fields, as shown in Figure 1. Corpus statistics are shown in Table 1; refer to\nSection 5.2 for a detailed description of the measures.\nWe observed that significance statements are semantically coherent with their corresponding abstracts. The corpus\nstatistics indicate that significance statements are more readable than abstracts, as shown by lower mean values in the\nAutomated Readability Index (ARI) and Flesch-Kincaid readability test (F-K). This suggests that the SASS corpus can\nbe useful in simplifying scholarly abstracts across diverse disciplines.\nWe also observed that word accessibility (i.e., log frequency per 1 billion tokens found in English Wikipedia) and\naverage word length suggest that significance statements can be less accessible at the word level than are their\ncorresponding abstracts. Although the log ratio of words found in the VOA1500 vocabulary is slightly lower than in the\ncorresponding abstracts, these 1,500 words are very basic and include a high proportion of function words. Considering\nthat significance statements use approximately 1.5 fewer words on average, the increased use of VOA words may be a\nconsequence of the higher use of function words to maintain grammaticality."}, {"title": "4 Reinforcement Learning from Accessibility Measures", "content": ""}, {"title": "4.1 Language Modeling via Proximal Policy Optimization", "content": "At the core of our approach is language modeling with Proximal Policy Optimization (PPO) (Schulman et al., 2017)\nguided by two accessibility measures. A causal language model trained on large corpora can generate the next token\nbased on the current sequence, which is useful in the context of reinforcement learning for developing a policy model\nthat determines the most appropriate next token to maximize the expected return in terms of document readability.\nThe process begins with an input sequence $s_0 = (a_0, a_1, ..., a_i)$, where each $a_i$ is from a set of tokens W, and $s_0$\nrepresents an abstract formatted in a simple template.\u00b2 The language model \u03c0\u0473 then generates $a_0, a_1,..., a_{T-1} \\sim$"}, {"title": "4.2 Reward Function", "content": "The reward function evaluates the overall quality of the output (ST). After the initial failures of testing a traditional\nreadability measure (i.e., ARI) as the criterion, we decided to use a balance of two accessibility measures: average\nsentence length in words and word accessibility, adopted from Riddell and Igarashi (2021), to guide the optimization.\nWord Accessibility Reward A word's accessibility is approximated by how frequently it appears in a large reference\ncorpus. We chose the English Wikipedia corpus due to its domain similarity and applied a Moses tokenizer, yielding\na vocabulary of 14.6 million types from a total of 3.6 billion tokens. If a token is among the most common 100,000\ntypes, we report its frequency per billion tokens as its accessibility measure. Otherwise, we estimate its frequency\nusing ridge regression with an l2-norm coefficient equal to 1.0. This model allows us to make serviceable estimates\nof the frequency of arbitrary tokens, including tokens that do not appear in the reference corpus. This model takes\nas input the token's length in Unicode code points, its byte unigrams, byte bigrams, and byte trigrams. The model\nestimates the token's log frequency per 1 billion tokens. We used the natural logarithm of frequencies per billion tokens\nas the measure of word accessibility.\u00b3 For example, the accessibility score for \u201cbig\u201d is 11.8, while \u201ccolossal\u201d scores 7.3.\nDespite being comparable in meaning, the model's production of the latter will receive fewer rewards. Coefficient Bwa\nis to control the scale of the credit given for word accessibility.\nSentence Length Reward Sentence length is also determined by a Moses tokenizer, which preserves hyphenation\nand splits contractions. We negate the value of sentence length for intuitive calculation of the rewards for optimization."}, {"title": "5 Experiment Setup", "content": ""}, {"title": "5.1 Training", "content": "We initialized the policy models \u03c0\u03b8 by adopting the Gemma-2B checkpoint reported by Wang and Clark (2024) (\u03c0\u03b8SFT).\nThe original Gemma-2B was trained on three trillion tokens, consisting of publicly available data as well as proprietary\ndatasets comprising \u201cprimarily English data from web documents\u201d (Mesnard et al., 2024). The specific checkpoint we\nadopted was fine-tuned using the SASS corpus in a straightforward manner. It was chosen for its strong performance in\nsimplification quality, its faithfulness, and its relatively compact size."}, {"title": "5.2 Evaluation", "content": "We evaluated the simplified texts generated by the language models trained with the reinforcement learning framework\nusing 200 abstracts from the SASS corpus test set for simplification. Though advanced decoding methods might further\nrefine the quality of the outputs, we used multinomial sampling with the temperature set to zero to intentionally produce\nthe most deterministic outputs. This approach helped us better understand the modeling of accessible language and\nmade it easier for us to identify potential quirks. We assessed the quality of the generated simplified abstracts both\nquantitatively and qualitatively. Quantitatively, we measured the generated texts based on their semantic retention and\naccessibility using established relevance measures as well as readability and accessibility metrics."}, {"title": "5.2.1 Semantic Retention", "content": "BERTScore calculates the cosine similarity between each token in the candidate sentence and each token in the reference\nsentence using contextual embeddings from a pre-trained language model; its results align well with human judgment\non semantic similarity evaluation (Zhang et al., 2019). It is not directly influenced by lexical overlap, making it more\nsuitable for evaluating simplification systems than are metrics that rely on matching words, such as BLEU (Papineni\net al., 2002). For our evaluation, we used embeddings from the 18th layer of a BERT-large-uncased model and reported\nthe F1 score. This choice is based on prior findings indicating that the 18th layer yields a strong Pearson correlation\n(0.72) on the WMT16 To-English benchmark (Zhang et al., 2019)."}, {"title": "5.2.2 Simplification & Accessibility", "content": "Accessibility can be measured with respect to the overall simplification quality (SARI); readability (ARI and Flesch-\nKincaid); and other straightforward document complexity measures, including average sentence length, word accessibil-\nity (i.e., log frequency per 1 billion tokens found in English Wikipedia), the log ratio of its proportion of VOA Special\nEnglish words (1,517 types in total), and average word length.\nSARI (System output Against References and against the Input sentence) is specifically designed to evaluate text\nsimplification (Xu et al., 2016). It aims to measure how well a simplified text retains the original meaning while\nimproving readability. SARI provides a balanced measure of how well a text simplification system performs by focusing\non the necessary operations of adding, deleting, and retaining words.\nARI and Flesch-Kincaid readability tests assign a numerical score to text that reflects the U.S. grade level required\nfor comprehension. Lower scores (1-13) indicate content suitable for kindergarten through twelfth grade, with each\nscore corresponding to a subsequent grade level. Scores in the range of 14-18 suggest college-level readability, ranging\nfrom first- to senior-year content appropriateness. Higher scores (19 and above) are associated with advanced college\neducation. Both measures use average sentence length. Flesch-Kincaid uses syllables per word, while ARI uses\ncharacters per word for its linear combination with sentence length."}, {"title": "5.2.3 Language Quality, Faithfulness, & Completeness", "content": "We manually examined 5% of all generated samples, corresponding to a randomly chosen subset of the test set from\nthe SASS corpus. Each generation is annotated with respect to language quality, faithfulness, and completeness using\na rubric of Good, Acceptable, and Poor. We focused on fluency and grammaticality and hand-picked both good and\nproblematic examples when evaluating language quality. For the evaluation of faithfulness, we conduct close readings\nto assess the extent to which a simplified abstract remains factually faithful to the original narrative. If uncertainty arises,\nwe consult the corresponding manuscript, as our abstract simplification system must avoid producing misinformation.\nCompleteness is also a key consideration, as it is essential to include the main findings and implications of the research\nfor the general public, since this is the primary goal of scientific dissemination."}, {"title": "5.2.4 Token Distribution Shift Analysis", "content": "To understand the impact of optimization guided by different rewards, we analyzed the shifts in token distribution in\ngenerations from reinforcement learning models compared to those generated by the supervised fine-tuning model from\nwhich they were initiated. Following the approach of Lin et al. (2023), we first generated a response using a model\ntrained with reinforcement learning for a given query. We then used the supervised fine-tuning model to predict the\nmost probable token at each position based on the context up to that point.\nToken positions were categorized into three groups based on how their ranks shifted between the supervised fine-tuning\nmodel and the reinforcement learning-optimized models. The first group, unshifted positions, includes tokens that\nmaintained their top rank in both models. The second group, marginal positions, consists of tokens that dropped slightly\nin rank, appearing as the second or third choice in the supervised fine-tuning model. Finally, shifted positions are those\nwhere a token's rank fell outside the top three choices in the supervised fine-tuning model. This categorization allows\nus to identify which tokens are affected, thereby understanding the impact of reinforcement learning guided by different\nrewards on the model's decision-making process."}, {"title": "6 Findings & Discussion", "content": ""}, {"title": "6.1 Quantitative Assessment", "content": "Table 2 summarizes the performance of Gemma-2B, tuned in different ways, when evaluated on the test set of the\nSASS corpus. The first scenario is the supervised fine-tuned baseline (SFT), which performs next-token prediction on\nthe SASS corpus training set. The second and third scenarios are reinforcement learning through PPO guided by ARI\n(RLARI) or accessibility measures (RLAM). We assessed the generation quality by considering both semantic retention\nand simplification, specifically using BERT score (BS), SARI, ARI, Flesch-Kincaid readability test (F-K), the log ratio\nof words in the VOA1500 vocabulary (VOA), sentence length (SL), word accessibility (WA), and word length (WL). A\none-tailed paired t-test was conducted for each metric to compare observations between the reinforcement learning and\nsupervised fine-tuning baselines, assuming improvement in document readability. Bonferroni correction was applied to\neach set of tests to maintain a family-wise significance level of 0.05.\nWe observe that reinforcement learning models trained with different rewards exhibit a notable reduction in reading\nlevel, bringing abstracts down to high school levels. The model directly guided by ARI (RLARI) achieves an ARI\nof 12.6, while the most performant model guided by accessibility measures (RLAM, BSL = 4.0 and \u1e9ewa = 0.2)\nreaches 12.5, both aligning with the readability level expected for individuals who have completed K-12 education\n(approximately ARI 13). However, RLARI and RLAM models achieve these readability improvements in different\nways. For RLAM models, better readability is achieved mostly through improved token-level accessibility. RLAM\nmodels show an increase in word accessibility from 0.5 to 0.8 compared to the supervised baseline. This increase in the\nnatural logarithm suggests that words generated by RLAM models are, on average, 1.6 to 2.2 times more frequent in the\nEnglish Wikipedia corpus than those generated by the SFT model. In comparison, RLARI's 0.1-unit increase in word\naccessibility, although observed, does not result in a statistically significant change in word frequency compared to the\nSFT model. Similarly, the log ratio of the VOA1500 vocabulary in the RLAM models shows a significant improvement,\nwith log ratios ranging from -0.02 to 0.08. This implies that for every 100 non-VOA1500 (or more complex) words\ngenerated, RLAM models can produce approximately 98 to 106 VOA1500 basic words. In contrast, the SFT and\nRLARI models exhibit VOA log ratios of -0.26 and -0.17, respectively, indicating that for every 100 non-VOA1500"}, {"title": "6.2 Qualitative Analysis", "content": "We annotated 5% of the generated simplified abstracts from reinforcement learning models guided by ARI (RLARI) and\naccessibility measures (RLAM, with \u1e9ewa = 4.0 and varying BSL values) with respect to language quality, faithfulness,\nand completeness, as shown in Figure 2. The test abstracts included three from biological sciences and one each\nfrom chemistry; mathematics; evolutionary biology; environmental sciences; ecology; economic sciences; and earth,\natmospheric, and planetary sciences.\nRegarding overall quality, we found that reinforcement learning-trained models generally produced high-quality\nlanguage. Compared to the SFT generations, RLAM outputs are often shorter and more semantically complete, due to\nthe imposed token budget for newly generated content (241 tokens, the length of the longest significance statement in\nthe training set). The main shortcoming is the presence of small trailing phrases that often deflate readability scores,"}, {"title": "6.3 Token Distribution Shift Analysis for Reinforcement Learning Models", "content": "We annotate each token in the generations according to the three categories defined in our token distribution shift\nanalysis. These categories are: unshifted positions (where the supervised fine-tuning model's top prediction agrees with\nthat of the reinforcement learning model), marginal positions (where the reinforcement learning model's top choice is\nthe second or third choice of the supervised fine-tuning model), and shifted positions (where the reinforcement learning\nmodel's top prediction falls outside the top three choices of the supervised fine-tuning model). This categorization\nallows us to closely examine how reinforcement learning with different rewards influences the behavior of the supervised\nfine-tuning model from which they were initiated.\nAs shown in Table 3, we quantify the distribution of token types across models and their respective proportions relative\nto the total token count for each model. The table suggests that the RLARI model modifies the SFT baseline less\nfrequently: 0.5% of tokens are shifted, ca. 6.0% are marginally shifted, while approximately 94.0% remain unaltered.\nIn contrast, the RLAM models exhibit a higher proportion of both shifted (ranging from 1.0% to 1.4%) and marginal\n(8.0% to 10.1%) tokens: approximately twice as many shifted tokens as the RLARI model, and a noticeably increased\nnumber of marginal tokens. We hypothesize that the differences primarily originate from how the sentence length\nreward is incorporated into the optimization process. Compared to the coefficient of sentence length in the ARI formula,\nwe assign much smaller weights to sentence length in the rewards guiding RLAM models. Since the supervised"}, {"title": "7 Conclusion", "content": "To improve the accessibility of scientific literature to the general public, we implemented reinforcement learning\ntechniques to guide language models, extending beyond the traditional cross-entropy objective. Our study demonstrates\nthat carefully balancing accessibility measures at the word and sentence levels can effectively guide Gemma-2B in\nsimplifying scholarly abstracts, outperforming the supervised fine-tuning baseline by a large margin. This approach\nachieves these improvements without compromising language quality or faithfulness and mitigates the supervised\nfine-tuning model's tendency to overemphasize research implications. The best model trained using our method\nsuccessfully adjusts the readability level of scholarly abstracts by approximately six U.S. grade levels\u2014in other words,\nfrom a postgraduate to a high school level. Compared to the supervised fine-tuning model, the words generated by the\nmodel trained via our approach are proven to be more common (1.6 to 2.2 times more frequent), easier (with more VOA\nbasic words), and shorter (by 0.3-0.4 characters). This improvement addresses a key limitation of existing corpora,\nin which the target distribution (i.e., significance statements) often does not adequately prioritize the accessibility\nof word choice. We also investigated the token distribution shift to better understand how reinforcement learning\nwith different rewards influences the behavior of the SFT model from which it was initiated. Our analysis revealed\nsystematic modifications in model outputs when using carefully balanced word- and sentence-level rewards, compared\nto traditional readability measures, which inevitably involve more aggressive rewards for reducing sentence length.\nWe considered the limitations of using average sentence length as a reward, as it can be a rather coarse measurement\nof syntactic complexity and perhaps also of cognitive comprehensibility. We observed that it is not uncommon for\nrestrictive attributive clauses and other syntactic structures to appear in the generations from RLAM-trained models\nthat could theoretically be split into separate sentences. However, increasing the coefficient for the sentence length\nreward exacerbated optimization instability, as was observed in the RLARI models. In future work, more direct yet\neasily obtainable measures, such as dependency length (Futrell et al., 2015), could be a promising direction to explore\nfor generating shorter sentences.\nAnother potential direction is to generate lay summaries from the full text, not just the abstract. With current large\nlanguage models capable of handling contexts with several thousand tokens, potentially enhanced by rotary positional\nencoding (Su et al., 2024), this approach is feasible. This method also has the added benefit of reducing hallucinations by\nproviding more comprehensive information from the entire manuscript. Also, reinforcement learning from accessibility\nmeasures is not limited to scientific literature; it can also be an effective solution for other simplification scenarios\nwhere a gold-standard corpus is unavailable. We hope this work contributes to bridging the gap between scholarship\nand a broader audience, advancing the understanding and development of better simplification systems, and ultimately\nfostering a more informed and engaged society."}, {"title": "B Adaptive KL Controller", "content": "Following Ziegler et al. (2019, Sec. 2.2), we dynamically adjust BKL to target a specific KL divergence value, KLtarget,\nusing a log-space proportional controller.\nThe update rule is:\n$BKL_{t+1} = BKL_t (1 + K_B \\cdot clip (\\frac{KL(\\pi_{\\theta}, \\pi_{\\theta_{SFT}}) - KL_{target}}{KL_{target}}, -0.2, 0.2))$\nwhere KB is the proportional gain, set to 0.01."}], "equations": ["\\pi_{\\theta}(a_0, a_1,..., a_{T-1}) = \\prod_{t=0}^{T-1} \\pi_{\\theta}(a_t | s_t)", "J(\\pi_{\\theta}) = E_{\\tau \\sim \\pi_{\\theta}} [\\sum_{t=0}^{T-1} (r(s_t, a_t) - \\beta_{KL}KL(\\pi_{\\theta}(a_t |  s_t) || \\pi_{\\theta_{SFT}} (a_t | s_t)))]", "d_t = \\sum_{t'=t}^{T} (\\gamma \\lambda)^{t'-t} (r_\\tau + \\gamma V_{\\rho}(s_{t'+1}) \u2013 V_{\\rho}(s_{t'}))", "\\theta \\leftarrow arg max_{\\theta} \\frac{1}{n} \\sum_{n=1}^{n} \\sum_{t=1}^{T}  min (clip (\\frac{\\pi_{\\theta_{online}} (a_t | s_t)}{\\pi_{\\theta_{offline}} (a_t | s_t)}, 1 \u2013 \\epsilon, 1 + \\epsilon)  (\\hat{A}_t - \\beta_{KL} KL_t)", "\\rho \\leftarrow arg min_{\\rho} \\frac{1}{n} \\sum_{n=1}^{n} \\sum_{t=1}^{T} (V_{\\rho}(s_t) \u2013 \\hat{V}_{targ} (s_t))^2", "BKL_{t+1} = BKL_t (1 + K_B \\cdot clip (\\frac{KL(\\pi_{\\theta}, \\pi_{\\theta_{SFT}}) - KL_{target}}{KL_{target}}, -0.2, 0.2))"]}