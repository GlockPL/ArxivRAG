{"title": "Towards a Learning Theory of Representation Alignment", "authors": ["Francesco Insulla", "Shuo Huang", "Lorenzo Rosasco"], "abstract": "It has recently been argued that AI models' representations are becoming aligned as their scale and performance increase. Empirical analyses have been designed to support this idea and conjecture the possible alignment of different representations toward a shared statistical model of reality. In this paper, we propose a learning-theoretic perspective to representation alignment. First, we review and connect different notions of alignment based on metric, probabilistic, and spectral ideas. Then, we focus on stitching, a particular approach to understanding the interplay between different representations in the context of a task. Our main contribution here is relating properties of stitching to the kernel alignment of the underlying representation. Our results can be seen as a first step toward casting representation alignment as a learning-theoretic problem.", "sections": [{"title": "Introduction", "content": "In recent years, as AI systems have grown in scale and performance, attention has moved towards universal models that share architecture across modalities. Examples of such systems include CLIP (Radford et al., 2021), VinVL (Zhang et al., 2021), FLAVA (Singh et al., 2022), OpenAI's GPT-4 (OpenAI, 2023), and Google's Gemini (Google, 2023). These models are trained on diverse datasets containing both images and text and yield embeddings that can be used for downstream tasks in either modality or for tasks that require both modalities. The emergence of this new class of multimodal models poses interesting questions regarding alignment and the trade-offs between unimodal and multimodal modeling. While multimodal models may provide access to greater scale through dataset size and computational efficiency,\nhow well do features learned from different modalities correspond to each other? How do we mathematically quantify and evaluate this alignment and feature learning across modalities?\nRegarding alignment, Huh et al. (2024) observed that as the scale and performance of deep networks increase, the models' representations tend to align. They further conjectured that the limiting representations accurately describe reality - known as Platonic representation hypothesis. Their analysis also suggests that alignment correlates with performance, implying that improving the alignment of learned features across different modalities could enhance a model's generalization ability. However, alignment across modalities has yet to be evaluated in a more interpretable manner, and theoretical guarantees of alignment under realistic assumptions are still lacking."}, {"title": "Preliminaries", "content": "Empirical results demonstrate that well-aligned features significantly enhance task performance. However, there is a pressing need for more rigorous mathematical tools to formalize and quantify these concepts in uni/multi-modal settings. In this section, we provide a mathematical formalization of uni/ multi-modal learning, introducing key notations to facilitate a deeper understanding of the underlying processes.\nSetup Without loss of generality, we focus on the case of two modalities, as illustrated in Figure 1, which outlines the corresponding process. For $q = 1,2$, let $(X_q, \\mu_q)$ and $(Z_q, \\lambda_q)$ be probability spaces, and let F\u2081 be the space of functions $f_q : X_q \\rightarrow Z_q = \\mathbb{R}^{d_q}$. We regard $X_q$ as the space of objects (or data), F\u2081 as the space of representation (or embedding) maps, and $Z_q$ as the space of representations. We relate $\\mu_q$ and $\\lambda_q$ by assuming $\\lambda_q = (f_q)_{\\#}\\mu_q$.  We also assume that $\\mu_1$ and $\\mu_2$ are the marginals of a joint probability space $(X,\\mu)$ with $X = X_1 \\times X_2$, $\\mu_q = (\\pi_q)_{\\#}\\mu$, where $\\pi_q : X \\rightarrow X_q$ is the projection map. Moreover, let $(Y_q, \\nu_q)$ be the task-based output spaces and define $G_q = \\{g_q : Z_q \\rightarrow Y_q\\}$ with $\\nu_q = (g_q)_{\\#}\\lambda_q$. Each overall model is generated by $H_q := \\{h_q : X_q \\rightarrow Y_q | h_q = g_q \\circ f_q\\}$.\nReality Consider a space of abstract objects, called the reality space and denoted by \u039e, which generates the observed data in various modalities through maps $m_q : \u039e \\rightarrow X_q$. These maps may be bijective, lossy, or stochastic. Reality can be modeled as a probability space $(\u039e, \\xi)$. Alternatively, one may define reality as the joint distribution over modalities by setting $m_q = \\pi_q$."}, {"title": "Frameworks for Representation Alignment", "content": "In this section, we describe various definitions of representation alignment from different communities and demonstrate the relationship among them. We begin with a detailed presentation of empirical and population Kernel Alignment and its statistical properties. We then cover other notions of alignment coming from metrics, independence testing, and probability measures, as well as their spectral interpretations. We draw connections to kernel alignment which emerges as a central object."}, {"title": "Kernel alignment (KA)", "content": "Based on the work of Cristianini et al. (2001), who introduced the definition of kernel alignment using empirical kernel matrices, we propose different perspectives to understand kernel alignment in both empirical and population settings and derive its statistical properties accordingly.\nA reproducing positive definite kernel $K : X \\times X \\rightarrow \\mathbb{R}$ captures the notion of similarity between objects by inducing an inner product in the associated reproducing kernel Hilbert space (RKHS) $\\mathcal{H}$. Specifically, $K(x, x') = \\langle f(x), f(x') \\rangle$ for any representation (feature) map $f \\in \\mathcal{H}$, and $x, x' \\in X$. For the multi-modal case, we define $K_q(x, x') := K_q(\\pi_q(x), \\pi_q(x')) = K_q(x_q, x_q')$, where $K_q$ is the reproducing kernel associated with $\\mathcal{H}_q$. In other words, $K_q$ acts on $x = (x_1, x_2)$ by first applying the projection $\\pi_q(x) = x_q$. In the following, the subscript $x_q$ denotes the $q$-th modality, and the superscript $x^i$ indicates the $i$-th sample."}, {"title": "Empirical KA", "content": "From Cristianini et al. (2001), we adopt the following formulation for kernel alignment for kernel matrix $K_{q,n} \\in \\mathbb{R}^{n \\times n}$ with samples $\\{x^i\\}_{i=1}^n$ drawing according to the probability measure $\\mu$\n$\\mathcal{A}(K_{1,n}, K_{2,n}) = \\frac{\\langle K_{1,n}, K_{2,n} \\rangle_F}{\\sqrt{\\langle K_{1,n}, K_{1,n} \\rangle_F \\langle K_{2,n}, K_{2,n} \\rangle_F}}$\nwhere $\\langle K_{1,n}, K_{2,n} \\rangle_F = \\sum_{i,j=1}^n K_{1,n}(x^i, x^j)K_{2,n}(x^i, x^j)$. One modification is to first demean the kernel by applying a matrix $H = I_n - \\frac{1}{n}1_n1_n^T$ on the left and right of each $K_{q,n}$ with $I \\in \\mathbb{R}^{n \\times n}$ being the identity matrix and $1_n$ being the ones vectors. This results in Centered Kernel Alignment (CKA).\nRepresentation interpretation of KA Denote the empirical cross-covariance matrix between the representation maps $f_1$ and $f_2$ as $\\Sigma_{1,2} = \\mathbb{E}_{m} [f_1(x)f_2(x)^T] = \\frac{1}{n} \\sum_{i=1}^n f_1(x^i)f_2(x^i)^T \\in \\mathbb{R}^{d_1 \\times d_2}$. Then the empirical KA will become\n$\\mathcal{A}(K_{1,n}, K_{2,n}) = \\frac{\\|\\Sigma_{1,2}\\|_F}{\\|\\Sigma_{1,1}\\|_F \\|\\Sigma_{2,2}\\|_F} \\qquad(1)$"}, {"title": "RKHS operator interpretation of KA", "content": "Inspired by the equation 1, we construct a consistent definition of Kernel Alignment using tools of RKHS, where it suffices to consider output in one dimension\u00b2. Consider RKHS $\\mathcal{H}_q$ containing functions $h_q : X_q \\rightarrow \\mathbb{R}$ with kernel $K_q$. Given evaluation (sampling) operators $S_q : \\mathcal{H}_q \\rightarrow \\mathbb{R}^n$ defined by $(S_qh_q)_i = h_q(x_i) = \\langle h_q, K_{q x_i} \\rangle$. It is not hard to check that the adjoint operator $S_q^* : \\mathbb{R}^n \\rightarrow \\mathcal{H}_q$ can be written as $S_q^* (w^1, ..., w^n) = \\sum_{i=1}^n w^iK_q(x_i, \\cdot)$ and the empirical kernels can be written as $K_{q,n}/n = S_q^*S_q$ (Smale & Zhou, 2004; De Vito et al., 2005). Then the empirical KA may be written as\n$\\mathcal{A}(K_{1,n}, K_{2,n}) = \\frac{\\langle S_1^*S_1, S_2^*S_2 \\rangle_F}{\\|S_1^*S_1\\|_F \\|S_2^*S_2\\|_F} = \\frac{\\|S_1^*S_2\\|_F}{\\|S_1\\|_F \\|S_2\\|_F}$\nwhere $S_1^*S_2 = \\frac{1}{n} \\sum_i K_{1 x_i, \\cdot} \\otimes K_{2 x_i, \\cdot}$ and it coincides with the literature about learning theory with RKHS."}, {"title": "Population version of KA", "content": "For the population setting (infinite data limit of the evaluation operator) in $L^2$, the restriction operator $S_q : \\mathcal{H}_q \\rightarrow L^2(X_q,\\mu)$ is defined by $S_qh_q(x) = \\langle h_q, K_q(x,\\cdot) \\rangle_{\\mathcal{K}}$, and its adjoint $S_q^* : L^2(X_q,\\mu) \\rightarrow \\mathcal{H}_q$ is given by $S_q^*g = \\int_{x}g(x)K_q(x,\\cdot)dx$. Then the integral operator $L_{K_q} = S_qS_q^* : L^2(X_q, \\mu) \\rightarrow L^2(X_q, \\mu)$ is given by $L_{K_q}g(x) = \\int_{x'}K_q(x, x')g(x')d\\mu(x')$ and the operator $\\Xi_q = S_q^*S_q : \\mathcal{H}_q \\rightarrow \\mathcal{H}_q$ can be written as $\\Xi_q = \\int_{x}K_q(x, \\cdot) \\otimes K_q(x, \\cdot)d\\mu(x)$ (De Vito et al., 2005; Rosasco et al., 2010). Similarly, the population KA between two kernels $K_1, K_2$ can be defined by\n$\\mathcal{A}(K_1, K_2) = \\frac{Tr (L_{K_1} L_{K_2})}{\\sqrt{Tr(L_{K_1}^2) Tr (L_{K_2}^2)}}$\nwhere the summation in $\\langle K_{1,n}, K_{2,n} \\rangle_F$ becomes the integration as\n$Tr (L_{K_1} L_{K_2}) = \\int d\\mu(x_1, x_2)d\\mu(x_1', x_2') K_1(x_1, x_1') K_2(x_2, x_2')$.\nIf $K_q(x, x') = \\langle f_q(x), f_q(x') \\rangle$, then $S_q^*S_q$ is a projection onto the span of coordinates of $f_q$. The population version of CKA is KA with $S_q$ replaced with $\\mathcal{H}S_q$.\nSpectral Interpretation of KA Furthermore, the understanding of kernel alignment (KA) can be deepened via the spectral decomposition of the associated integral operator. The mercer kernel $K$ can be decomposed as $K = \\sum_i \\eta_i \\phi_i \\otimes \\phi_i$, where $\\eta_i$ are the eigenvalues and $\\phi_i$ are the eigenfunctions of the integral operator $L_K$ (Cucker & Smale, 2002; Sch\u00f6lkopf, 2002). Defining the features as $f_i = \\sqrt{\\eta_i}\\phi_i$ and expressing the target function as $h = \\sum_i w_i f_i$, we obtain\n$\\mathcal{A}(K, hh) = \\frac{\\sum_i \\eta_i w_i^2}{\\sqrt{\\sum_i \\eta_i \\sqrt{\\sum_i \\eta_i w_i^4}}}$\nSimilarly, given two kernels $K_q = \\sum_i \\eta_{q,i} \\phi_{q,i} \\otimes \\phi_{q,i}$ with $f_{q,i} = \\sqrt{\\eta_{q,i}}\\phi_{q,i}$, we have\n$\\mathcal{A}(K_1, K_2) = \\frac{\\sum_{i,j} \\langle f_{1,i}, f_{2,j} \\rangle^2}{\\sqrt{\\sum_i \\eta_i^2 \\sum_j \\eta_j^2}} = \\frac{\\sum_{i,j} \\eta_{1,i}\\eta_{2,j} \\langle \\phi_{1,i}, \\phi_{2,j} \\rangle^2}{\\sqrt{\\sum_i \\eta_i^2 \\sum_j \\eta_j^2}}$\nLetting $[C_{1,2}]_{i,j} = \\langle \\phi_{1,i}, \\phi_{2,j} \\rangle$ and defining $\\hat{\\eta}_i = \\eta_i / \\|\\eta\\|$, we can equivalently write\n$\\mathcal{A}(K_1, K_2) = Tr[C_{1,2} diag(\\hat{\\eta}_2) C_{1,2}^* diag(\\hat{\\eta}_1)] = \\langle \\hat{\\eta}_1, (C_{1,2} \\odot C_{1,2}) \\hat{\\eta}_2 \\rangle = \\langle \\hat{\\eta}_1, C_{1,2} \\odot C_{1,2}\\rangle$\nwith $\\odot$ as the Hadamard product. This formulation provides insight into kernel alignment by relating it to the similarity between the eigenfunctions of the two integral operators. In particular, if $\\eta_1$ and $\\eta_2$ are constant, then $\\mathcal{A}(K_1, K_2) \\propto \\|C_{1,2}\\|^2$; and if $C_{1,2} = I$, then $\\mathcal{A}(K_1, K_2) = \\langle \\hat{\\eta}_1, \\hat{\\eta}_2 \\rangle$."}, {"title": "Statistical properties of KA.", "content": "Having introduced both the empirical and population versions of KA, we now explore its statistical properties. Cristianini et al. (2006) shows that empirical KA concentrates to its expectation by McDiarmid's inequality and gives an $O(1/ \\sqrt{n})$ bound. For completeness, we state the following lemma summarizing this statistical property and the proof is provided in Appendix 6.3.\nLemma 1. Let $K_1, K_2$ be two kernels for different representations and $K_{1,n}, K_{2,n} \\in \\mathbb{R}^{n \\times n}$ be kernel matrices generated by $n$ samples, then with probability at least $1 \u2013 \\delta$, we have\n$\\mathcal{A}(K_{1,n}, K_{2,n}) \u2013 \\mathcal{A}(K_1, K_2) \\leq \\sqrt{(\\frac{32}{n})log(2/\\delta)}$."}, {"title": "Alignment from distance alignment", "content": "Distance alignment (DA) Given distances $d_q : X_q \\times X_q \\rightarrow \\mathbb{R}$, then we can compare the difference of two spaces by\n$D(d_1, d_2) = \\int (d_1(x, x') \u2013 d_2(x, x'))^2 d\\mu(x)d\\mu(x')$.\nEquivalence between KA and DA Suppose $d^2_q = 2(1-K_q)$ (Igel et al., 2007) and $K_q(x_q, x_q) = 1$, which emerge naturally from assuming $K_q(x, x') = \\langle f_q(x), f_q(x') \\rangle$, $\\|f_q(x)\\| = 1$, and $d^2(x_q, x_q') = \\|f_q(x_q) \u2013 f_q(x_q')\\|^2$, (i.e., $K_q$ represents a mapping onto a ball). Also assume $\\|K_q\\| = C. Then, $D(d_1, d_2) = 8C(1 \u2013 \\mathcal{A}(K_1, K_2))$, hence the two paradigms are equivalent."}, {"title": "Alignment from independence testing", "content": "Independence testing is a statistical method used to assess the degree of dependence between variables. It often involves examining the covariance and correlations between random variables and can also be applied to quantify kernel-based independence. In this section, we outline several approaches from independence testing within the alignment framework and investigate their connections to the kernel alignment method discussed earlier.\nHilbert-Schmidt Independence Criterion (HSIC) The cross-covariance operator for two functions (Baker, 1973) is given by $C_{1,2}[h_1, h_2] = \\mathbb{E}_{X_1,X_2}[(h_1(x_1)-\\mathbb{E}_{X_1}(h_1(x_1))(h_2(x_2)-\\mathbb{E}_{X_2}(h_2(x_2))]$ for $h_1 \\in \\mathcal{H}_1, h_2 \\in \\mathcal{H}_2$. From Gretton et al. (2005a)\n$HSIC(\\mu, \\mathcal{H}_1, \\mathcal{H}_2) = \\|C_{1,2}\\|_{\\mathcal{HS}}$,\nwhere $\\mu$ is the joint distribution of $X_1$ and $X_2$. We can also note that\n$HSIC(\\mu, \\mathcal{H}_1, \\mathcal{H}_2) = \\|\\mathbb{E} [K_{x_1} \\otimes K_{x_2}] \\|^2 = \\|\\Sigma_{1,2}\\|_{\\mathcal{HS}}^2$.\nHence HSIC is effectively and unnormalized version of CKA, or, more explicitly,\n$CKA(K_1, K_2) = \\frac{HSIC(\\mathcal{H}_1, \\mathcal{H}_2)}{\\sqrt{HSIC(\\mathcal{H}_1, \\mathcal{H}_1)HSIC(\\mathcal{H}_2, \\mathcal{H}_2)}}$"}, {"title": "Alignment from measure alignment", "content": "There are several methods for comparing measures on the same space. One can then quantify independence by comparing a joint measure with the product of its marginals. This principle allows us to interpret HSIC as test for independence given two function classes.\nMMD to HSIC Following Gretton et al. (2012), we start by introducing the so-called Maximum Mean Discrepancy (MMD). Let $\\mathcal{H}$ be a class of functions $h : X \\rightarrow \\mathbb{R}$ and let $u_q$ be different measures on $X$. Then, letting $x_q \\sim \\mu_q$,\n$MMD(\\mu_1, \\mu_2; \\mathcal{H}) = sup_{h\\in\\mathcal{H}} \\mathbb{E} [h(x_1) \u2013 h(x_2)]$.\nLet $\\mathcal{H}$ be an RKHS and restrict to a ball of radius 1, then\n$MMD(\\mu_1, \\mu_2; \\mathcal{H})^2 = \\|\\mathbb{E} [K_{x_1} \u2013 K_{x_2}] \\|^2_{\\mathcal{H}} = \\mathbb{E} [K(x_1, x_1) + K(x_2, x_2) \u2013 2K(x_1, x_2)]$.\nNow we construct a measure of independence by applying MMD on $\\mu$ versus $\\mu_1 \\otimes \\mu_2$ where $\\mathcal{H}$ is replaced with $\\mathcal{H}_1 \\times \\mathcal{H}_2$ and get HSIC\n$MMD(\\mu, \\mu_1 \\otimes \\mu_2; \\mathcal{H}_1 \\mathcal{H}_2)^2 = HSIC(\\mu, \\mathcal{H}_1, \\mathcal{H}_2) = \\|\\Sigma_{1,2}\\|^2 = \\sum_i \\rho_i^2$\nwhere $\\{\\rho_i\\}$ is the spectrum of $\\Sigma_{1,2}^2$.\nWe can also use tests of independence that don't explicitly depend on a function class, such as mutual information, by letting u be a Gaussian Process measure on two functions in their respective RKHS with covariance defined by their kernels.\nKL Divergence to Mutual Information Given KL divergence\n$D_{KL}(\u00b5||\u03bd) = \\int d\u00b5(x) log(\\frac{d\u00b5}{d\u03bd}(x))$"}, {"title": "Stitching: Task Aware Representation Alignment", "content": "Building on our understanding of kernel alignment\u2014a fundamental metric for evaluating the alignment of representations detailed in the previous section\u2014we now explore stitching, a task-aware concept of alignment. Stitching involves combining layers or components from various models to create a new model which can be used to understand of how different parts contribute to overall performance or to compare the learned features for a task. In this section, we mathematically formulate this process and provide some intuition by demonstrating that the generalization error after stitching can be bounded by kernel alignment using spectral arguments."}, {"title": "Stitching error between models", "content": "In the following, we focus on stitching between two modalities. Figure 1 provides a detailed illustration of the functions, spaces, and compositions in question. Denote the function space for task learning as $\\mathcal{H}_q := \\{h_q : X_q \\rightarrow Y_q|h_q = g_q \\circ f_q, g_q \\in \\mathcal{G}_q, f_q \\in \\mathcal{F}_q\\}$ with $q = 1,2$. Here $\\mathcal{F}_q : X_q\\rightarrow Z_q$ and $\\mathcal{G}_q : Z_q \\rightarrow Y_q$. Denote $\\mathcal{S}_{1,2} := \\{s_{1,2} : Z_1 \\rightarrow Z_2\\}$ as the stitching map from $Z_1$ to $Z_2$ and $\\mathcal{S}_{2,1} := \\{s_{2,1} : Z_2 \\rightarrow Z_1\\}$ reversely. Define the risk concerning the least squares loss as\n$R_q(h_q) = \\mathbb{E} [\\|h_q(x) \u2013 y\\|^2] = \\int_{X_q \\times Y_q} \\|h_q(x) \u2013 y\\|^2dp_q(x, y), h_q \\in \\mathcal{H}_q$.\nHere, $p_q(x, y)$ is the joint distribution of $X_q$ and $Y$, and we use the notation $\\| \\cdot \\|$ to represent $\\| \\cdot \\|_{Y_q}$, associated with space $Y_q$ for simplicity, i.e. absolute value for $Y_q = \\mathbb{R}$, $l_2$ norm for $Y_q = \\mathbb{R}^d$ and $L_2$ norm for $Y_q$ being the function space. For $h_q \\in \\mathcal{H}_q$, denote any minimizer of $R(h_q)$ among $\\mathcal{H}_q$ as $h_q^*$, that is,\n$R_q(H_q) := R_q(h_q^*) = \\min_{h \\in H_q} R_q(h), q = 1,2$.\nMoreover, denote the function spaces generated after stitching from $Z_1$ to $Z_2$ as\n$\\mathcal{H}_{1,2} = \\{h_{1,2} = g_2 \\circ s_{1,2} \\circ f_1 : s_{1,2} \\in \\mathcal{S}_{1,2}\\}$\nand conversely as $\\mathcal{H}_{2,1}$.\nLenc & Vedaldi (2015) proposed to describe the similarity between two representations by measuring how usable a representation $f_1$ is when stitching with $g_2$ through a function $s_{1,2}: Z_1 \\rightarrow Z_2$ or oppositely through $s_{2,1} \\in \\mathcal{S}_{2,1}$. To quantify the similarity, we provide a detailed definition of the stitching error.\nStitching error Define the stitching error as\n$R_{1,2}^{stitch} (s_{1,2}) := R_2(g_2 \\circ s_{1,2} \\circ f_1) = R_2(h_{1,2})$\nand the minimum as\n$R_{1,2}^{stitch} (S_{1,2}) := \\min_{s_{1,2}\\in S_{1,2}} R_2(h_{1,2}) = R_2(\\mathcal{H}_{1,2})$.\nTo quantify the difference in the use of stitching, we define the excess stitching risk as\n$R_{1,2}^{stitch} (S_{1,2}) - R_2(\\mathcal{H}_2)$.\nNote that $R_{1,2}^{stitch}(S_{1,2}) \u2013 R_2(\\mathcal{H}_2)$ quantifies a difference in use of representation (fix $g_2$, compare $s_{1,2} \\circ f_1$ vs $f_2$), while if $Y_1 = Y_2$ then $R_{1,2}^{stitch}(S_{1,2}) \u2013 R_1(\\mathcal{H}_1)$ quantifies difference between $g_2 \\circ s_{1,2}$ and $g_1$ (fix $f_1$)."}, {"title": "Stitching error bounds with kernel alignment", "content": "In this section, we focus on a simplified setting where $s_{1,2} : Z_1 \\rightarrow Z_2$ is a linear stitching, that is, $s_{1,2}(z_1) = S_{1,2}z_1$ with $S_{1,2} \\in \\mathbb{R}^{d_2 \\times d_1}, z_q \\in \\mathbb{R}^{d_q}$. Additionally, we assume $Y_1 = \\mathbb{R}^d, Y_2 = \\mathbb{R}^d$. In this section, we quantify the stitching error and excess stitching risk using kernel alignment and provide a lower bound for the stitching error when stitching forward.\nThe following lemma shows that when $\\mathcal{G}_q$ are linear, stitching error only measures the difference in risk of $\\mathcal{H}_1$ versus $\\mathcal{H}_2$.\nLemma 2. Suppose dim$(Y_1) =$ dim$(Y_2) = d$ and $R_1 = R_2$. Let $g_q \\in \\mathcal{G}_q$ be linear with $g_q(z_q) = W_qz_q$ and $W_q \\in \\mathbb{R}^{d \\times d_q}$. Let $s_{1,2} : Z_1 \\rightarrow Z_2$ be linear with $s_{1,2}(z_1) = S_{1,2}z_1$ and $S_{1,2} \\in \\mathbb{R}^{d_2 \\times d_1}$. Then $R^{stitch}_{1,2}(S_{1,2}) = R_1(\\mathcal{H}_1)$.\nRemark 2. The lemma applies when $\\mathcal{H}_1$ represents a neural network with $\\mathcal{G}_q$ as the output linear layer, as well as when $\\mathcal{H}_1$ is an RKHS with a Mercer kernel and $\\mathcal{G}_q$ is the linear map of representations 3.\nThe next theorem shows the case when $\\mathcal{G}_q$ are nonlinear with the $\\kappa$-Lipschitz property, $\\|g(z)-g(z')\\| \\leq \\kappa\\|z-z'\\|$. One intermediate example is the stitching between the middle layers of neural networks.\nTheorem 1. Suppose $g_2$ is $\\kappa_2$-Lipschitz. Again let $s_{1,2}$ be linear, identified with matrix $S_{1,2}$. With the spectral interpretations of $\\Sigma_{1,2} = \\mathbb{E}[f_1f_2^T] = diag(\\eta_1)^{1/2}C_{1,2}diag(\\eta_2)^{1/2}$ and $\\tilde{\\Lambda}_2 = \\|I\\|_{\\eta_2} \u2013 \\|C_{1,2}\\|^2_{\\eta_2}$ as Paragraph 3.1.2, we have\n$R^{stitch}_{1,2}(S_{1,2}) \\leq R_2(\\mathcal{H}_2) + \\kappa_2^2\\tilde{\\Lambda}_2 + 2\\kappa_2(\\tilde{\\Lambda}_2R_2(\\mathcal{H}_2))^{1/2}. \\qquad (2)$"}, {"title": "Conclusion", "content": "In this paper, we review and unify several representation alignment metrics, including kernel alignment, distance alignment, and independence testing, demonstrating their equivalence and interrelationships. Additionally, we formalize the concept of stitching, a technique used in uni/multi-modal settings to quantify alignment in relation to a given task. Furthermore, we establish bounds on stitching error across different modalities and derive stitching error bounds based on misalignment, along with their generalizations and implications."}]}