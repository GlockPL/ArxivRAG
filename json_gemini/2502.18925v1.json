{"title": "BeamVQ: Beam Search with Vector Quantization to Mitigate Data Scarcity in Physical Spatiotemporal Forecasting", "authors": ["Weiyan Wang", "Xingjian Shi", "Ruiqi Shu", "Yuan Gao", "Rui Ray Chen", "Kun Wang", "Fan Xu", "Jinbao Xue", "Shuaipeng Li", "Yangyu Tao", "Di Wang", "Hao Wu", "Xiaomeng Huang"], "abstract": "In practice, physical spatiotemporal forecasting can suffer from data scarcity, because collecting large-scale data is non-trivial, especially for extreme events. Hence, we propose BeamVQ, a novel probabilistic framework to realize iterative self-training with new self-ensemble strategies, achieving better physical consistency and generalization on extreme events. Following any base forecasting model, we can encode its deterministic outputs into a latent space and retrieve multiple codebook entries to generate probabilistic outputs. Then BeamVQ extends the beam search from discrete spaces to the continuous state spaces in this field. We can further employ domain-specific metrics (e.g., Critical Success Index for extreme events) to filter out the top-k candidates and develop the new self-ensemble strategy by combining the high-quality candidates. The self-ensemble can not only improve the inference quality and robustness but also iteratively augment the training datasets during continuous self-training. Consequently, BeamVQ realizes the exploration of rare but critical phenomena beyond the original dataset. Comprehensive experiments on different benchmarks and backbones show that BeamVQ consistently reduces forecasting MSE (up to 39%), enhancing extreme events detection and proving its effectiveness in handling data scarcity.", "sections": [{"title": "1. Introduction", "content": "In physical spatiotemporal forecasting (e.g., meteorological forecasting (Bi et al., 2023; Lam et al., 2022), fluid simulation (Wu et al., 2024b; Wu et al.), and various multi-physics system models (Li et al., 2020; Wu et al., 2024c)), researchers typically need to capture physical patterns and predict extreme events, such as heavy rainfall due to severe convective weather (Ravuri et al., 2021; Doswell III, 2001), marine heatwave (Fr\u00f6licher et al., 2018), and intense turbulence (Moisy & Jim\u00e9nez, 2004)). However, they suffer from the fundamental problem of data scarcity to ensure physical consistency and accurately predict extreme events. Collecting large-scale and high-resolution physical data can be expensive and even infeasible. Consequently, limited training data can prevent data driven models (Sun et al., 2020; Zhu et al., 2019) like physics-informed neural networks (Raissi et al., 2019) from generalizing well, even though they have adopted physical laws as prior knowledge. Furthermore, extreme events occur infrequently in nature, making their labeled data quite sparse and imbalanced throughout the entire data set. Therefore, data-driven methods usually fail to capture these low-probability phenomena."}, {"title": "2. Method", "content": "Problem Definition. We investigate spatiotemporal prediction tasks spanning meteorological forecasting (Bi et al., 2023), computational fluid dynamics (Wu et al., 2024b), and PDE-based systems (Wu et al., 2024c). The observational data is structured as a 4D tensor X \u2208 RT\u00d7C\u00d7H\u00d7W, where T denotes temporal steps, C represents physical variables (temperature, pressure, velocity fields), and (H, W) specify spatial resolution. Our objective is to learn a parametric mapping fe : Xt \u2192 Yt+1 that predicts subsequent system states from historical sequences Xt = {X1, ..., Xt}. The parameters \u0398 are optimized through maximum likelihood estimation:\n$\\Theta^* = \\arg \\max_\\Theta \\sum_{i=1}^T \\log P(Y_{t+1}|X; \\Theta)$ (1)\nwhere P(Yt+1|X; \u0398) defines the predictive distribution. The optimized model enables multi-step forecasting via iterative rollout \u0176t+k = fe({Xt, \u0176t+1, ..., \u0176t+k\u22121}), crucial for applications requiring temporal extrapolation in climate modeling (Bi et al., 2023), combustion dynamics (Anonymous, 2024), and fluid simulations (Wu et al.).\nArchitecture Overview. Our framework comprises three core stages of progressive refinement, as shown in Figure 2. Initially, we train a base spatiotemporal predictor fe that processes historical observations Xt \u2208 R1\u00d7C\u00d7H\u00d7W (single-step training) to generate next-step predictions \u0176t+1 = fe(Xt). Subsequently, we develop a Top-K VQ-VAE ho through codebook-based pretraining, where the encoder e\u03c6 maps \u0176t+1 to latent code z, quantized via K-nearest codebook vectors {q(k)}Kk=1 \u2282 Rdz, followed by decoder do reconstruction to yield diverse outputs {\u1ef8(k)t+1}Kk=1. During joint optimization, we employ a non-differentiable metric M (e.g., Critical Success Index (Gao et al., 2022)) to select the optimal reconstruction \u1ef8k*t+1, then minimize ||\u0176k*t+1 \u2212 Yt+1||2 to refine fe, while augmenting training data with ensemble averages of top-K' candidates. For multi-step inference, beam search (Steinbiss et al., 1994) maintains K trajectory candidates per step, progressively selecting optimal sequences through metric-guided pruning."}, {"title": "2.1. Stage 1: Pre-training the Base Prediction Model", "content": "We first develop a foundational predictor fe that learns deterministic spatiotemporal dynamics from observational data. The model ingests input tensors Xt \u2208 R1\u00d7C\u00d7H\u00d7W (single-step temporal context during training) and generates predictions \u0176t+1 = fe(Xt) through parametric mapping fe : RC\u00d7H\u00d7W \u2192 RC\u00d7H\u00d7W. Architectural implementations are task-adaptive: Fourier Neural Operators (FNO) (Li et al., 2020) for spectral systems governed by PDEs, Vision Transformers (Dosovitskiy, 2020) for global dependency modeling, or ConvLSTM (Shi et al., 2015) networks for local spatiotemporal correlations. The parameters \u0398 are learned by minimizing the reconstruction error:\n$\\mathcal{L}_{base} = \\mathbb{E}_{(X_t, Y_{t+1})} ||\\hat{Y}_{t+1} - Y_{t+1}||_2^2$ (2)\nwhere the expectation is over training pairs (Xt, Yt+1). Optimization employs gradient-based methods (Adam) (Kingma & Ba, 2014) with learning rate annealing, ensuring stable convergence. This stage establishes a strong deterministic prior that captures dominant physical patterns - for instance, FNO architectures learn Green's functions in Fourier space for fluid dynamics, while transformer variants attend to long-range atmospheric interactions. The trained f\u2217e provides initial point estimates for subsequent uncertainty-aware refinement."}, {"title": "2.2. Stage 2: Top-K VQ-VAE Pre-training", "content": "We construct a variational quantization framework ho = (e\u03c6, C, do) to learn diverse reconstructions from deterministic predictions. Given the base model output \u0176t+1, the encoder e\u03c6 projects it to latent code:\nz = e\u03c6(\u0176t+1) \u2208 Rdz (3)\nA codebook C = {cn}Nn=1 \u2282 Rdz with N entries enables Top-K vector quantization:\nq(k) = \\arg\\min_{c \\in C} ||z - c||_2^2 \\ \\text{ for } 1 \\le k \\le K (4)\nThe decoder do reconstructs K variants through parallel decoding:\n$\\tilde{Y}_{t+1}^{(k)} = d_{\\phi}(q^{(k)}), \\ \\text{ 1$\\le$k$\\le$K}$ (5)\nThe training objective combines three components:\n$\\mathcal{L}_{VQ} = \\underbrace{||\\tilde{Y}_1 - Y_{t+1}||^2}_{Reconstruction} + \\underbrace{||sg[z] - q^{(1)} ||^2}_{Codebook\\ Learning} + \\underbrace{\\beta ||z - sg[q^{(1)}]||^2}_{Commitment}$ (6)\nwhere sg[\u00b7] denotes stop-gradient operator and \u03b2 balances latent commitment. This design ensures: (1) Accurate primary mode reconstruction via \u1ef81 optimization; (2) Codebook diversity preservation through Top-K retrieval; (3) Stable encoder-codebook alignment via commitment loss.\nWe conducted several experiments to verify the effect of selecting different K. And we use an optimization to explain how to choose K to achieve the best performance.\nTheorem 1. The best selection of K is determined by the numerical solution of the following optimization problem\n$\\arg \\min_{\\pi} h(\\pi, T) := \\pi^T A_\\pi \\pi,$ (7)\n$\\sum_{i=1}^N \\pi_i T_i \\leq \\alpha,$ (8)\nsubject to\n$\\sum_{i=1}^N \\pi_i = 1,$\n$0 \\leq \\pi_i \\leq m^{-1}, 1 \\leq i \\leq N.$"}, {"title": "2.3. Stage 3: Joint Optimization", "content": "We develop a dual-phase optimization framework to refine the base predictor fe using the frozen Top-K VQ-VAE ho. The process iterates between:\n$\\hat{Y}_{t+1} = f_e(X_t), \\ \\{\\tilde{Y}_{t+1}^{(k)}\\}_{k=1}^K = h_\\phi(\\hat{Y}_{t+1})$ (9)\nwhere ho remains fixed with \u03a6 = \u03a6\u2217 from Stage 2. A domain-specific metric M (e.g., Critical Success Index) evaluates each reconstruction:\ns(k) = M(\u1ef8(k)t+1, Yt+1), k \u2208 [1, K] (10)\nOptimization Cycle is as follows:"}, {"title": "1. Optimal Guidance", "content": "Select the highest-scoring variant\nk\u2217 = arg maxks(k), Lguide = ||\u1ef8(k\u2217)t+1 - Yt+1||2 (11)"}, {"title": "2. Ensemble Distillation", "content": "Aggregate top-K' candidates\nYt+1 = K' 1 \u2211k\u2208ktop\u1ef8(k)t+1 (12)\nwhere ktop indexes the K' highest s(k)."}, {"title": "3. Parameter Update", "content": "\u0398 \u2190 \u0398 \u2212 \u03b7\u2207\u0398(Lguide + \u03bb||\u0176t+1 \u2212 Yt+1||2) (13)\nThe frozen VQ-VAE acts as an uncertainty-aware teacher: Lguide aligns predictions with metric-optimal reconstructions. Ensemble distillation Yt+1 mitigates exposure bias through data augmentation. Hyperparameter \u03bb balances direct supervision and distributional smoothing"}, {"title": "2.4. Inference Stage with Beam Search", "content": "We propose a novel beam search protocol that synergizes the base predictor fe with the diversity-generating VQ-VAE h\u03c6. The algorithm maintains B candidate trajectories to balance exploration (via codebook variations) and exploitation (through metric-guided selection), crucial for chaotic systems where small deviations amplify exponentially. The procedure (Algorithm 1) operates in three phases:\nInitialization: Generate K initial variants from Xt using the VQ-VAE's decoding diversity\nIterative Rollout: At each step n, expand B candidates into B \u00d7 K possibilities using the codebook\nTrajectory Selection: Retain top-B paths based on accumulated scores sn(b,k) = \u2211mt+1 an\u2212mS(\u1ef8m(b,k))\nThe term \u03b1n\u2212t (\u03b1 \u2208 (0, 1]) in the scoring function prioritizes recent accuracy, crucial for non-stationary processes. This implements:\n$\\mathcal{S}_n^{(b,k)} = \\sum_{m=t+1}^n \\alpha^{n-m} S(\\tilde{Y}_m^{(b,k)})$ (14)\nDynamic Beam Pruning: The selection operator arg maxs solves a knapsack-like optimization to maximize total score while maintaining beam width B. This is equivalent to:\nBn = maximize S s.t. |S| \u2264 B (15)\nThe whole algorithm of the proposed BeamVQ is summarized in Algorithm 2."}, {"title": "3. Experiment", "content": "In this section, we verify the effectiveness of our method by evaluating 5 benchmarks and 10 backbone models. The experiments aim to answer the following research questions:\nRQ1. Can BeamVQ enhance the performance of the baselines? RQ2. How does BeamVQ perform under data-scarce conditions? RQ3. Can BeamVQ have better physical alignment? RQ4. Can BeamVQ produce long-term forecasting? Appendix D also has additional results."}, {"title": "D. Additional Experiments", "content": "D.1. Long-term forecasting experiment expansion\nIn the long-term forecasting experiments, we compare the performance of different backbone models on the SWE benchmark, evaluating the relative L2 error for three variables (U, V, and H). Our setup inputs 5 frames and predicts 50 frames. For the SimVP-v2 model, using BeamVQ reduces the relative L2 error for SWE (u) from 0.0187 to 0.0154, SWE (v) from 0.0387 to 0.0342, and SWE (h) from 0.0443 to 0.0397. We visualize SWE (h) in 3D as shown in Figure 6 [I]. For the ConvLSTM model, applying BeamVQ reduces the relative L2 error for SWE (u) from 0.0487 to 0.0321, SWE (v) from 0.0673 to 0.0351, and SWE (h) from 0.0762 to 0.0432. For the FNO model, using BeamVQ reduces the relative L2 error for SWE (u) from 0.0571 to 0.0502, SWE (v) from 0.0832 to 0.0653, and SWE (h) from 0.0981 to 0.0911. Overall, BeamVQ significantly improves the long-term forecasting accuracy of different backbone models."}, {"title": "D.2. Experiment Statistical Significance", "content": "To measure the statistical significance of our main experiment results, we choose three backbones to train on two datasets to run 5 times. Table 5 records the average and standard deviation of the test MSE loss. The results prove that our method is statistically significant to outperform the baselines because our confidence interval is always upper than the confidence interval of the baselines. Due to limited computation resources, we do not cover all ten backbones and five datasets, but we believe these results have shown that our method has consistent advantages."}, {"title": "A. Metric", "content": "Mean Squared Error (MSE) Mean Squared Error (MSE) is a common statistical metric used to assess the difference between predicted and actual values. The formula is:\n$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y_i})^2$ (16)\nwhere n is the number of samples, yi is the actual value, and \u0177i is the predicted value.\nRelative L2 Error Relative L2 error measures the relative difference between predicted and actual values, commonly used in time series prediction. The formula is:\n$\\text{Relative L2 Error} = \\frac{|| \\text{Ypred} - \\text{Ytrue} ||_2}{|| \\text{Ytrue} ||_2}$ (17)\nwhere Ypred is the predicted value and Ytrue is the actual value.\nStructural Similarity Index Measure (SSIM) The Structural Similarity Index (SSIM) measures the similarity between two images in terms of luminance, contrast, and structure. The formula is:\n$\\text{SSIM}(x, y) = \\frac{(2 \\mu_x \\mu_y + C_1) (2 \\sigma_{xy} + C_2)}{(\\mu_x^2 + \\mu_y^2 + C_1) (\\sigma_x^2 + \\sigma_y^2 + C_2)}$ (18)\nwhere \u03bcx and \u03bcy are the mean values, \u03c3x and \u03c3y are the standard deviations, \u03c3xy is the covariance."}, {"title": "B. Related Work", "content": "Numerical Methods and Ensemble Forecasting: they are the traditional methods to realize physical spatial-temporal forecasting (Jouvet et al., 2009; Rogallo & Moin, 1984; Orszag & Israeli, 1974; Griebel et al., 1998), which employ discrete approximation techniques to solve sets of equations derived from physical laws. Although these physics-driven methods ensure compliance with fundamental principles such as conservation laws (Karpatne et al., 2017; Karnopp et al., 2012; Pukrushpan et al., 2004), they require highly trained professionals for development (Lam et al., 2022), incur high computational costs (Pathak et al., 2022), are less effective when the underlying physics is not fully known (Takamoto et al., 2022), and cannot easily improve as more observational data become available (Lam et al., 2022). Moreover, traditional numerical methods usually perturb initial observation inputs with different random noises, which can alleviate the problem of observation errors. Then Ensemble Forecasting (Leutbecher & Palmer, 2008; Karlbauer et al., 2024) can average the outputs of different noisy inputs to improve the robustness.\nData-Driven Methods: Recently, data-driven deep learning starts to revolutionize the space of space-time forecasting for complex physical systems (Gao et al., 2022; Wu et al., 2024a; Li et al., 2020; Tan et al., 2022; Shi et al., 2015; Pathak et al., 2022; Wu et al., 2023; Bi et al., 2023; Lam et al., 2022; Zhang et al., 2023). Rather than relying on differential equations governed by physical laws, the data-driven approach constructs model by optimizing statistical metrics such as Mean Squared Error (MSE), using large-scale datasets. These methods (Wang et al., 2022b; Shi et al., 2015; Wang et al., 2018; Tan et al., 2022; Gao et al., 2022; Wu et al., 2024a) are orders of magnitude faster, and excel in capturing the intricate patterns and distributions present in high-dimensional nonlinear systems (Pathak et al., 2022). Despite their success, purely data-driven methods fall short in generating physically plausible predictions, leading to unreliable outputs that violate critical constraints (Bi et al., 2023; Pathak et al., 2022; Wu et al., 2024c).\nPrevious works have tried to combine physics-driven methods and data-driven methods to get the best of both worlds. Some methods try to embed physical constraints in the neural network (Long et al., 2018; Greydanus et al., 2019; Cranmer et al., 2020; Guen & Thome, 2020). For example, PhyDNet (Guen & Thome, 2020) adds a physics-inspired PhyCell in the recurrent network. However, such methods require explicit formulation of the physical rules along with specialized designs for network architectures or training algorithms. As a result, they lack flexibility and cannot easily adapt to different backbone architectures. Another type of methods (Raissi et al., 2019; Li et al., 2021; Hansen et al., 2023), best exemplified by the Physics-Informed Neural Network (PINN) (Raissi et al., 2019), leverages physical equations as additional regularizers in neural network training (Hansen et al., 2023). Physics-Informed Neural Operator (PINO) (Li et al., 2021) extends the data-driven Fourier Neural Operator (FNO) to be physics-informed by adding"}, {"title": "C. Detailed Mathematical Proof", "content": "Proof of Theorem 1\nNow we have N augmented data and we need to select the best from them. We consider both the quality and the diversity of these data and get the sampling strategy from an optimization problem.\nWe model the sampling strategy as a multinomial distribution supported on all the augmented data S = {Xi}Ni=1, which means that the sampling strategy \u03c0 = (\u03c01, ..., \u03c0N) is the corresponding probabilities of selecting X1, ..., XN, then we can model the expectation of the similarity as:\n$\\mathbb{E}_{Y_i, Y_j \\in \\mathcal{C}}\\{g(x, x') | \\mathcal{S}\\}$\n$= \\int \\int g(x, x') \\pi(x) Pr_\\mathcal{S}(Y_i \\in \\mathcal{C} | x = x) \\pi(x') Pr_\\mathcal{S}(Y_j \\in \\mathcal{C} | x = x') dx dx'$\n$= \\sum_{i,j=1}^N g(X_i, X_j) \\pi_i \\pi_j Pr_\\mathcal{S}(Y_i \\in \\mathcal{C} | x = X_i) Pr_\\mathcal{S}(Y_j \\in \\mathcal{C} | x = X_j),$\nwhere the set C denotes the criterion of selection we are using, the function g can be chosen as any similarity metric function and x means a random variable.\nThe core to solving the above optimization problem is to use predictive inference to approximate the conditional probability of {Yx \u2208 C} given x = Xi. Let \u03bc(x) := E(Y | X = x) be the oracle associated with (X, Y). Denote \u03b8i = I{Yi \u2208 C}. As the augmented data X1, ..., XN are independently identically distributed, \u03b81, ..., \u03b8N can be regarded as independent Bernoulli(q) variables with q = Pr(Yi \u2208 C). The probability distribution of the predicted result Wj for j = 1, ..., N is\n$Pr(W_j | \\theta_j) = (1 - \\theta_j) f_0 + \\theta_j f_1,$\nwhere f0 and f1 are the conditional distributions of Wj on Yj \u2208 C or not.\nDenote $\\mathcal{T}(w) = \\frac{q f_1(w)}{(1 - q) f_0(w)},$ we can rewrite the expectation of the similarity as\n$\\mathbb{E}_{Y_i, Y_j \\in \\mathcal{C}}\\{g(x, x') | \\mathcal{S}\\} = \\sum_{i,j=1}^N g(X_i, X_j) \\pi_i \\pi_j (1 - \\mathcal{T}_i) (1 - \\mathcal{T}_j) = \\pi^T A_\\mathcal{T} \\pi,$\nNext, we use the expectation to control the quality of the data.\n$\\mathbb{E}\\{I(Y_i \\in \\mathcal{C}) | \\mathcal{S}\\} = \\sum_{i=1}^N Pr(Y_i \\in \\mathcal{C} | X_i) \\pi_i = \\sum_{i=1}^N \\pi_i \\mathcal{T}_i \\leq \\alpha,$\nIn all, the optimization problem can be modeled as\n$\\arg \\min_{\\pi} h(\\pi, \\mathcal{T}) := \\pi^T A_\\mathcal{T} \\pi, \\sum_{i=1}^N \\pi_i \\mathcal{T}_i \\leq \\alpha,$ (19)\n$\\sum_{i=1}^N \\pi_i = 1,$\n$0 \\leq \\pi_i \\leq m^{-1}, 1 \\leq i \\leq N.$ (20)\nsubject to\nwhere m is used to control the maximum selection.\nThe best selection of K is determined by the strategy \u03c0 which serves as the solution to the above optimization problem."}]}