{"title": "Timeline and Boundary Guided Diffusion Network for Video Shadow Detection", "authors": ["Haipeng Zhou", "Honqiu Wang", "Tian Ye", "Zhaohu Xing", "Jun Ma", "Qiong Wang", "Lei Zhu", "Ping Li"], "abstract": "Video Shadow Detection (VSD) aims to detect the shadow masks with frame sequence. Existing works suffer from inefficient temporal learning. Moreover, few works address the VSD problem by considering the characteristic (i.e., boundary) of shadow. Motivated by this, we propose a Timeline and Boundary Guided Diffusion (TBGDiff) network for VSD where we take account of the past-future temporal guidance and boundary information jointly. In detail, we design a Dual Scale Aggregation (DSA) module for better temporal understanding by rethinking the affinity of the long-term and short-term frames for the clipped video. Next, we introduce Shadow Boundary Aware Attention (SBAA) to utilize the edge contexts for capturing the characteristics of shadows. Moreover, we are the first to introduce the Diffusion model for VSD in which we explore a Space-Time Encoded Embedding (STEE) to inject the temporal guidance for Diffusion to conduct shadow detection. Benefiting from these designs, our model can not only capture the temporal information but also the shadow property. Extensive experiments show that the performance of our approach overtakes the state-of-the-art methods, verifying the effectiveness of our components. We release the codes, weights, and results at Github.", "sections": [{"title": "1 INTRODUCTION", "content": "Shadow detection is increasingly important in vision analysis and applications. Vision tasks can suffer from the shadows, including incorrect segmentation [52], inaccurate object detection [15, 25], and flawed tracking [10, 37]. Hence, shadow detection has become a highly focused area of research. Recently, one can witness significant progress in single Image Shadow Detection (ISD) [11, 25, 26, 79, 80], whereas in the dynamic scenario, Video Shadow Detection (VSD) is much more challenging.\nTemporal correspondence information matters in video shadow detection. For example, the SC-Cor [16] focuses on the relationship between shadow and optical-flow. It relies on a contrastive loss to explore the temporal correspondence for adjacent frames. However, like other Unsupervised Video Object Segmentation (UVOS) [41, 43, 50, 56, 63, 73] methods, it still depends on additional clues (i.e., optical-flow) and lack of semantic correspondence. We point out that the adjacent frames usually change slightly, leading the model to focus on the consistent area while distracting on the deformation region, which is more crucial for shadow detection.\nIn addition, few works notice the characteristic of shadow to propose a task-specific model. We are attracted to recent shadow removal works [19, 38], which use the boundary information to guide the restoration model to remove shadows. This indicates the boundary can provide potential clues to identify the shadows. Meanwhile, the contexts of boundary often contain higher uncertainty [59, 76, 78] making it difficult for the model to perform accurate segmentation. These observations motivate us to explore"}, {"title": "2 RELATED WORKS", "content": "2.1 Video Object Segmentation\nDifferent from Semi-Supervised Video Object Segmentation (SSVOS) [12, 13, 29, 30, 39, 54] which provides the first frame mask to initialization during the testing stage, the Video Shadow Detection (VSD) follows the paradigm of Unsupervised Video Object Segmentation (UVOS) [28, 36, 41, 47, 50, 63] where we detect the shadow without the ground truth in the initial frame. In VOS, it usually deploys auxiliary encoder to extract temporal information, like optical flow [41, 43, 50, 63], vanilla encoder [36, 55, 57], or salient map [28, 47]. Recent studies [12, 13, 39, 54, 75] focus on the affinity between the past and current frames such that one can build a memory-bank, which aims to utilize the sequential information and past prior for video understanding. Not only in the past, but we also reconsider the information of future frames, i.e., in a timeline sequence. We rethink the affinity in dual temporal scales to consider timeline aggregation instead of a sequential way.\n2.2 Shadow Detection\nPrevious shadow detection works [11, 20, 25, 58, 79, 80] focus on the image shadow detection (ISD). For example, Zhu et al. [80] design a bidirectional FPN [31] to extract local and global contexts for detecting shadow. With respect to the dynamic scenarios, i.e., video shadow detection, it encounters huger challenges on account of the complexity of the real-world. Chen et al. [10] collect the first video shadow detection dataset named ViSha, and introduce TVSD-Net to apply collaborative training in different videos to learn the shadow context. Similarly, STICT [35] deploys Teacher-Student model [23] to achieve video consistency learning. Considering the temporal correspondence, SC-Cor [16] enables the network to focus on the anchor pixel of shadow via contrastive learning. In order to balance temporal learning and contrastive learning, in Scotch&SODA [32] the authors apply trajectory Transformer to conduct VSD and set a new record on ViSha dataset. Differently, motivated by the shadow removal works [19, 38, 68] which utilizes the boundary information to guide the image restoration, we first design a specific shadow boundary-aware attention to detect the characteristic of shadows.\n2.3 Diffusion Model\nDiffusion model has shown remarkable promise in visual generation [4, 24, 42, 44, 48], and it enlightens other tasks like object detection [6], segmentation [2, 7, 9, 27] classification [21, 71]. For segmentation tasks, the denoise process is not suitable [8, 27] since the discrete signals undermine the segmentation. Bit Diffusion [7] introduces a simple and generic way to embed and analog the discrete mask into continuous signals, and it also presents a simple concatenation strategy to process VOS. While there is still room to improve the temporal guidance for Diffusion to tackle VOS. Moreover, the efficiency should be considered as well. Though several works [4, 48, 49] dedicate to accelerate the inference, the conventional Diffusion models [1, 4, 24, 40, 44, 48, 74, 77] use heavy U-Net for noise estimation. Huge parameters make it hard to go on the downstream works. Instead, we further explore the feasibility and temporal understanding of the Diffusion model for VSD."}, {"title": "3 METHODOLOGY", "content": "3.1 Overview\nGiven 2i + 1 frames, our TBGDiff can simultaneously detect the shadow for all clipped sequences. In brevity, we illustrate the workflow of the T-th frame xy in Fig. 1. First, we take an encoder to extract the temporal-agnostic features for all the frames, then the features are sent to the Dual Scale Aggregation (DSA) module to implement temporal aggregation. With an Auxiliary Head, these aggregated features are used to produce pseudo masks and boundary masks. Next, we input the T-th aggregated feature FDSA, pseudo mask \u00ffy, and boundary by to Shadow Boundary-Aware Attention (SBAA) to further explore the characteristic of shadows. To utilize the timeline temporal information for Diffusion, we use a guidance encoder to yield Space-Time Encoded Embedding (STEE) via encoding the past and future pairs (pseudo masks and images). Finally, we adopt bit analog strategy [7] to embed noise and conduct the denoise process to predict the final shadow masks.\n3.2 Dual Scale Aggregation\nWhen it comes to video-related works, the matching-based methods [13, 18, 39, 61, 62, 70, 72, 73, 75] usually adopt affinity to read and visit the space-time correspondences. However, the vanilla affinity"}, {"title": null, "content": "will introduce the concern: it intends to give more weight to the adjacent frames because the contexts of close-range sequences change smoothly and slightly, and the interval frames gain less attention due to time-shift. We argue that both of the temporal scales of features should be considered, and to alleviate the temporal bias we propose a Dual Scale Aggregation (DSA) module where we rethink the affinity considering short-term and long-term scenarios jointly.\nThe core of affinity is to compute the similarity between the query feature and the memory feature to retrieve the temporal and spatial feature. Given the query feature \\(Q \\in \\mathbb{R}^{C_Q\\times HW}\\), the memory key feature \\(K \\in \\mathbb{R}^{C_K\\times NHW}\\), and memory value feature \\(V \\in \\mathbb{R}^{C_V\\times NHW}\\) (the H and W are the spatial dimensions, the C{Q,K,V} denote the channels, and N represents the memory length), the affinity \\(M \\in \\mathbb{R}^{NHW\\times HW}\\) is computed as:\n\\[M_{(a,b)} (Q, K) = \\frac{exp(f(Q_{(a)}, K_{(b)}))}{ \\Sigma_{x} exp(f(Q_{(x)}, K_{(b)}))},\\]\nwhere f() is L2 similarity function [13]. Such that, we can readout the aggregated feature \\(F \\in \\mathbb{R}^{C_V\\times HW}\\) via the matrix multiplication:\n\\[F = VM(Q, K).\\]\nSuch a vanilla affinity can be deployed for short-term aggregation. With the current frame feature Fr as Q, and the concatenated"}, {"title": null, "content": "adjacent features {\\(F_{T-1}, F_{T+1}\\)} as the Ks and Vs (see Fig. 1), we readout the short-term aggregated feature via the vanilla affinity:\n\\[F^s = VM^s(Q, K^s).\\]\nConsidering the long-term visiting, Eq. 1 indicates that similar areas among different frames occupy higher weight. This leads the model to distract from the deformation region. To encourage the network to focus on it, we propose residual affinity to enhance the weight of deformation areas. Similarly, we have the query of Fr, the key and value \\(K^l = V^l = \\{F_l\\}\\) where \\(l \\in [T - i, ..., T - 2, T + 2, ..., T + i]\\), and the long-term aggregated feature can be obtained by a residual operation with the affinity matrix:\n\\[F^l = V^lM^l(Q, K^l) = V^l (M^{self} (Q,Q') - M^l(Q, K')),\\]\nwhere \\(M^{self}\\) is a self-affinity anchored the consistent areas and the Q' is the broadcasting version of Q to match the size. An explanation is that the close-range frames usually contain consistent area leading to higher similarity, while the discrepancy area receives less attention. Adopting a subtraction operation on the affinity matrix, the residual area will reveal the difference region which is crucial to track the shadow deformation. With \\(F^s\\) and \\(F^l\\), our DSA can produce the dual scales aggregated features for T-th frame via a simple convolutional residual block:\n\\[F^{DSA} = ResBlock(F_T, F^s, F^l).\\]\n3.3 Shadow Boundary-Aware Attention\nConsidering the property of shadows, previous shadow removal works [19, 38, 68] suggest that the marginal contexts of shadows indicate crucial clues to identify the shadow and non-shadow regions. Motivated by this, we design a Shadow Boundary-Aware Attention (SBAA) for specializing in detecting the shadows."}, {"title": null, "content": "First, we introduce an Auxiliary Head in which we input the aggregated features FDSA to produce the pseudo mask \u00fd and boundary mask b. Benefiting from this design, the pseudo mask can produce the semantic guidance for our Diffusion (see Sec. 3.4), and the boundary mask helps the model capture the characteristics of shadows. Here, we use the auxiliary loss to supervise the production via:\n\\[\\mathcal{L}_{aux} = L_{bce}(b, \\hat{b}) + L_{bce}(y, \\hat{y}),\\]\nwhere b and y are the ground truths of the boundary and shadow masks, respectively. We adopt Binary Cross Entropy (Lbce) to compute loss. Note that all the frames are conducted.\nThen move to a specific frame at T, given the aggregated feature \\(F^{DSA} \\in \\mathbb{R}^{C\\times H\\times W}\\) from DSA, the predicted boundary br, and the pseudo mask \u00fdy, we regard by as the position embedding such that the boundary-aware embedded tokens can be obtained by:\n\\[F^l = [f_T^l + E; f_T^{-E}; ...; f_T^{fE}] + E_{bp}, n = H \\times W, f_T^l \\in F^{DSA},\\]\nwhere E is a patch embedding projection [17], and we flatten the boundary mask to obtain Ebp serving as the boundary-aware position embedding. To highlight the shadow region, we propose the SBAA which can be described as:\n\\[q = F_T W_q, k = (F^l \\cdot \\hat{y}_T) W_k, v = (F^l \\cdot \\hat{y}_T) W_v,\\]\n\\[SBAA = Softmax(\\frac{qk^T}{\\sqrt{C}}),\\]\nwhere Wq, Wk, and W are the learnable matrices, and C is the number of channel to scale. We implement broadcasting mechanism to extend the channels of \u00ffr to match the size of Fr, and y\u012b can serve as the weights of probability to emphasize the shadow-relevant areas. A visual depiction of our SBAA can be found in Fig. 2.\nBy doing so, the boundary-aware query is able to better retrieve the shadow regions. And based on the attention, we can deploy feed-forward network (FFN) [51] (i.e., the MLP linear layer) on it to produce the output feature which is used in our Diffusion process to give the final prediction.\n3.4 Space-Time Encoded Embedding Guidance\nRecently, Diffusion Models have indicated powerful ability in segmentation tasks [2, 7, 27, 65]. When it comes to VOS, Pix2Seq [7] adopts a straightforward way that just concatenates the past predicated masks into Diffusion as guidance to conduct VOS. While, the potential of conditional Diffusion in VOS has yet to be further explored. Motivated by this, we try to seek more effective temporal guidance for Diffusion models and deploy it in VSD.\nBriefly, Diffusion model [24, 48] contains a forward process q and a reverse process p. The q aims to gradually add Gaussian noise to corrupt the distribution of a mask y\u00ba making it close to a normal distribution, which can be illustrated as:\n\\[q(y_t|y^0) = \\sqrt{\\bar{\\alpha}_t}y^0 + \\sqrt{(1 - \\bar{\\alpha}_t)}\\epsilon, \\epsilon \\sim \\mathcal{N}(0, 1).\\]\nThe t denotes the timestep, and \u0101t is a noise scheduler which could be adjusted by a cosine or linear style. And the reverse process pe is parameterized by a network \\({\\theta}(y_t, g, x)\\), which is used to predict y0 from yt step by step based on the condition guidance g and image x. This Markov process can be written as:\n\\[P_{\\theta}(y_{0:t}|g, x) = p(y_t) \\prod_{t=1}^{T} P_{\\theta}(y_{t-1}|y^t, g, x).\\]"}, {"title": null, "content": "Instead of predicting noise like conventional Diffusion models, we predict the mask since the robust representation and bit analog strategy [8, 27] controlled by a scale weight enable the model to directly decode the mask rather than relying on the heavy U-Net. The detail of the Diffusion's operation are provided in Supplementary Material, and the ablation studies of the hyperparameters (scale weight, noise scheduler, and sampling steps) are given later. Here, we mainly discuss the importance of the guidance g.\nIn the scheme of Diffusion, the conditional guidance is usually accessible (e.g., vectors of text or images). Considering the VOS, Pix2Seq [7] introduces a loop to predict the masks frame by frame such that the obtained predictions can serve as a temporal guidance to promote the following segmentation. It simply downsamples the masks and concatenates them with the latent features, and we denote it as Past Concatenate Embedding (PCE, see Fig. 3(a)). While, existing Latent Diffusion models [4, 44] have proved that in latent space the Diffusion can perform better, which suggests us rethink the embedding ways. Intuitively, we propose other two methods to embed the guidance: Past Encoded Embedding (PEE, see Fig. 3(b)) and Space-Time Encoded Embedding (STEE, see Fig. 3(c)). We concatenate the masks with corresponding frames to form pairs, then a light-weight guidance encoder is deployed to embed them. Compared to PCE, the encoded guidances from PEE and STEE are much more robust to visit the temporal information.\nHere, we point out that the unidirectional embeddings (PCE & PEE) are not the best solution. They bring the following concerns: (1) The PCE and PEE access the guidance sequentially, leading to lower efficiency. Since the embedding is conducted frame by frame, they are restricted to real-time online shadow detection."}, {"title": null, "content": "Moreover, the uncertainty can accumulate as well. (2) The future prediction is agnostic, resulting in limited temporal guidance usage. All the space-time clipped frames should be considered to provide a temporal context. To address the aforementioned issues, we devise STEE to use all the space-time information in an efficient way. Instead of using the predicted masks, we take use of the pseudo masks produced by an Auxiliary Head (see Fig. 1 and Sec. 3.3). Because all the pseudo masks are available, we can execute the guidance encoding in a parallel manner rather than wait for the last prediction. As a result, our Diffusion can visit all the timeline temporal information leading to better performance.\n3.5 Objective Loss\nAs mentioned in the last section, we directly predict the masks rather than the noise. Hence, the objective loss is similar to the segmentation task. Here, we adopt the Binary Cross Entropy [66, 67] and lovasz-hinge loss [3] to restrict training. Considering the auxiliary loss, the total term of our loss function is computed as:\n\\[\\mathcal{L}_{seg} = L_{bce} + L_{hinge} + L_{aux}.\\]"}, {"title": "4 EXPERIMENT", "content": "We use Video Shadow dataset (ViSha) [10] to conduct our experiments and make comparisons with state-of-the-art methods. The ViSha dataset contains 50 videos for training and 70 videos for testing. Following previous studies [10, 16, 32, 35, 60], we deploy Mean Absolute Error (MAE), Intersection over Union (IoU), F-measure score (F\u1e9e), and Balance Error Rate (BER) as evaluation metrics for quantitative comparisons. Regarding the BER, we also compute the S-BER score at the shadow regions and the N-BER score at non-shadow regions, respectively.\n4.1 Implement details\nWe utilize AdamW optimizer [33] with a learning rate of 3e-5 to train our model. The batch size is 4, and the clipped sequence is 5 frames. Four A6000 GPUs are used to conduct our experiments, and a fixed random seed ensures the reproduction. For a fair comparison, following Scotch&SODA [32], we use MiT-B3 [64] as our feature extraction backbone, and all experiments are conducted with a resolution of 512\u00d7512. Note that the boundary masks are obtained by utilizing the canny operator on the shadow masks. More setup details are provided in Supplementary Material.\n4.2 Comparisons with State-of-the-art Methods\n4.2.1 Compared Methods. We compare our network against 20 cutting-edge methods, including Image Object Segmentation (IOS) [5, 27, 31, 64], Image Shadow Detection (ISD) [11, 14, 25, 69, 79, 80], Video Object Segmentation (VOS) [7, 13, 36, 39, 53], and Video Shadow Detection (VSD) [10, 16, 32, 35, 60]. Note that the Semi-Supervised video segmentation methods like STM [39], STCN [13], and ShadowSAM [60] are not given the label of the first frame during testing, we reproduce them by predicting the initial frame for fair comparisons in line with previous VSD methods and ours.\n4.2.2 Quantitative Comparisons. We report the quantitative results of our TBGDiff and compared methods in Tab.7. Among the 20 compared methods, Scotch&SODA [32] has the best MAE score of 0.029, the best IoU score of 0.640, the best BER score of 9.07, and the best S-BER score of 16.26, while SILT [69] ranks the first place in terms of F\u1e9e (0.796) and N-BER (1.29). Our method outperforms"}, {"title": "4.3 Ablation Studies", "content": "We first conduct an ablation study to evaluate the effectiveness of our two modules (i.e., SBBA, and DSA) on the Diffusion model. To do so, we build a \"Baseline\" by removing both SBBA and DSA modules from our network. Then \"M1\" and \"M2\" are reconstructed by adding the SBBA module and the DSA module into \"Baseline\"."}, {"title": "5 CONCLUSION", "content": "In this work, we propose a Timeline and Boundary Guided Diffusion (TBGDiff) network which is the first work to use Diffusion model for video shadow detection task. The main idea of our TBGDiff is to extract temporal guidance for Diffusion and to utilize the boundary information to capture the characteristics of shadows. In detail, we propose a Dual Scale Aggregation (DSA) module to aggregate the temporal signals by rethinking the discrepancy of the affinity among short-term and long-term. We also devise an Auxiliary Head to yield boundary masks and pseudo masks, which can be used for extracting the boundary context of shadows by Shadow Boundary-Aware Attention (SBAA) and producing timeline temporal guidance via Space-Time Encoded Embedding (STEE) for Diffusion, respectively. Experimental results show that the developed designs are effective and our approach can outperform state-of-the-art methods."}, {"title": "A APPENDIX", "content": "This is a text supplementary material for \"Timeline and Boundary Guided Diffusion Network for Video Shadow Detection\". The outline in this text material is:\n\u2022 Sec B More Details of Our Approach.\n\u2022 Sec C More Implement Details.\n\u2022 Sec D More Experiments.\nB MORE DETAILS OF OUR APPROACH"}, {"title": null, "content": "B.1 Workflow of our pipeline\nHere, we give more details about the workflow of our method. To begin with, our TBGDiff follows the previous video shadow detection paradigm [10, 16, 32] in which we detect all the shadow masks by the given frame sequence. That is to say, our workflow is based on clipped video level and each batch will contain a video clip. Empirically, we have 5 frames for each sequence as reported in our manuscript.\nWe encode all the frames by encoder E such that the DSA can aggregate all the timeline features. Then the aggregated features are used to produce pseudo masks and boundary masks. For a specific frame, it could be individually and parallelly decoded by the Diffusion. For each frame, we use the timeline (past and feature) guidance to guide the Diffusion model.\nIn terms of the Diffusion model, we detail it with training and inference stage. For training process, we use bit analog [7] to serialize the discrete masks and embed them into the latent features which are controlled by a scale weight. Then we use Guidance Encoder (GE) to yield space-time guidance by inputting the pseudo masks and timeline frames. Last, the decoder predicts the mask of the current frame with the guidance. With respect to the sampling stage, the logic is similar to the training stage. To accelerate the inference, we use DDIM [48] strategy to sample. We display the pseudo codes of our TBGDiff including the training (Algorithm 1) and inference (Algorithm 2) stages and only one frame is shown for simplicity. We can implement the process parallelly for faster speed.\nB.2 Details of the DSA\nWe illustrate the short-term and long-term frames in Fig. 7. We use the interval frames as the long-range frames to conduct residual affinity and use adjacent frames to apply vanilla affinity. Dual scale temporal frames are in consideration. For the first and last one, we copy itself as the adjacent frame. See the computation of the affinity in our manuscript.\nB.3 Details of the Guidance Injection\nThe frames of the guidance from past and future are various. Hence, we concatenate the timeline guidance (gf and gp) and current feature, and send them to a residual block to fuse the space-time guidance. Fig. 8 shows the details.\nC MORE IMPLEMENT DETAILS\nHere, we list more setup details about our experiments. We take use of mixed-precision strategy to accelerate the training and test- ing, and the total of the training epoch is set as 20. To ensure the reproduction, we utilize a fixed random seed 42 to conduct all the experiments. Following [32], basic data augmentation techniques are adopted, e.g., random flip horizontally and vertically. The code is supported by Pytorch.\nAs indicated in the manuscript, we adopt hierarchical transformer backbone MiT-B3 [64] to obtain the multi-scale features. For DSA, we only conduct the top-level features (i.e., R512\u00d7\u00d7\u865f)for aggregating the temporal semantics, since applying it for all of the levels is redundant and extremely time-consuming due to matrix multiplication [13]. For the Guidance Encoder (GE), we only utilize a light-weight backbone MiT-B1 [64] to encode the timeline frames and pseudo masks jointly."}, {"title": "D MORE EXPERIMENTS", "content": "D.1 Input Different Number of Frames\nWe add the ablation studies for the number of input frames, where the results are shown in Tab. 7. Apparently, when inputting 5 frames our model can produce the best results.\nD.2 More Visual Comparisons\nWe provide more visual comparisons with state-of-the-art methods. For the video comparison demos, please refer to our Supplementary Video or visit our homepage https://haipengzhou856.github. io/paper_page/TBGDiff/TBGDiff.html to check the comparisons. We present 3 complete cases to compare the video shadow detection result, and each video contains 100 frames in 10 FPS speed."}]}