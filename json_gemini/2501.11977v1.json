{"title": "LEVERAGING GRAPH STRUCTURES AND LARGE LANGUAGE MODELS FOR END-TO-END SYNTHETIC TASK-ORIENTED DIALOGUES", "authors": ["Maya Medjad", "Hugo Imbert", "Bruno Yun", "Rapha\u00ebl Szymocha", "Fr\u00e9d\u00e9ric Armetta"], "abstract": "Training task-oriented dialogue systems is both costly and time-consuming, due to the need for high-quality datasets encompassing diverse intents. Traditional methods depend on extensive human annotation, while recent advancements leverage large language models (LLMs) to generate synthetic data. However, these approaches often require custom prompts or code, limiting accessibility for non-technical users. We introduce GraphTOD, an end-to-end framework that simplifies the generation of task-oriented dialogues. Users can create dialogues by specifying transition graphs in JSON format. Our evaluation demonstrates that GraphTOD generates high-quality dialogues across various domains, significantly lowering the cost and complexity of dataset creation.", "sections": [{"title": "1 Introduction", "content": "Task-Oriented Dialogue Systems (TODS) are increasingly used in domains like customer support, personal assistants, and enterprise solutions to help users achieve specific objectives through natural language conversations [1, 2, 3, 4].\nTraditional TODS rely on machine learning models trained on predefined schemas [5, 2], but they struggle with complex/nuanced dialogues, especially when domain-specific data is scarce. In contrast, LLM-based TODS enable more human-like and engaging responses [6, 7, 8, 1]. However, these systems are prone to hallucinations [9, 10], needing fine-tuning for specific use cases [11, 12].\nFine-tuning LLM-based TODS requires large amounts of training data, with diverse and high-quality structures, which are costly and time-consuming to collect [13]. A diverse dataset of realistic dialogues is essential to allow these systems"}, {"title": "2 The GraphTOD generation pipeline", "content": "GraphTOD is based on two agents (system and user) which simulate dialogue utterances by navigating an action transition graph (see Figure 1). We formalize each of those elements.\nAn action transition graph is a tuple G = (V, Ac, E, t, s, f), where V is a set of nodes, Ac is a set of actions, s, f \u2208 V are the initial and final states, E \u2286 (V \\ {f}) \u00d7 Ac is a set of available actions at each non-final node, and t : (V \\ {f}) \u00d7 Ac \u2192 V is the transition function. The action transition graph serves as the link between the two agents and can be specified quickly in JSON format.\nGiven an action transition graph G, a subset of actions F \u2282 Ac, called function calls, are associated with APIs to obtain external knowledge. For a node v \u2208 V, the possible actions at v are denoted Act = {a \u2208 Ac | (v, a) \u2208 E}. Our pipeline makes use of five carefully crafted prompt templates denoted by pi, 1 \u2264 i \u2264 5. These prompt templates take as input a set of parameters and return a formatted string for an LLM. A dialogue history is H = (u\u2081, u\u2082, u\u2083, ..., un), where u\u2082j and u\u2082j+1 represent the user's and system's utterances, respectively, at time j \u2265 0. We denote an LLM as a (possibly non-deterministic) function l that outputs l(x) for input x.\nThe system agent is defined as As = (G, F, Ps, K, u\u2081), where G is an action transition graph, F is a set of function calls, Ps = {p1, . . ., p4 } is a set of system prompt templates, K is an agent knowledge database initialized at (\u00d8, and u\u2081 is a starting utterance. Considering a dialogue history H = (u1, u2, ..., u2j+1), a user utterance u2j+2, and the current node v \u2208 V, the system agent performs a two-steps reasoning. First, p1 (H, u2j+2, K, Act) is used to make the LLM detect the user's intention. Second, depending on the detected intention, potential APIs are triggered to collect knowledge, and the system utterance is generated to either state that the intent was not recognized, end the conversation, or continue the conversation (using the corresponding prompt templates p2, p3, or p4)."}, {"title": "3 Evaluation", "content": "We generated 150 conversations on four domain scenarios (Recipe, Hotel, RentCar, Doctor) using our pipeline and OpenAI's GPT-4. We evaluate the dialogues using three metrics (naturalness, coherence, understandability) from the pre-trained T5-based UniEval metrics [17] as classic NLP metrics (e.g., BLEU [18] or ROUGE [19]) are not sufficient to portray the difference between the advanced generation models. As shown in Table 1, GraphTOD performs consistently well overall and reports similar performances to human-in-the-loop approaches based on LLMs such as LAPS [20]."}, {"title": "4 Conclusion", "content": "GraphTOD is an end-to-end LLM-based pipeline designed to generate task-oriented conversations efficiently. By using an action transition graph in JSON format, GraphTOD simulates high-quality dialogues between two agents across various domains, thanks to a generalized prompting approach. GraphTOD also includes the automatic generation of user-agent preferences from the input graph and LLM-powered intent detection, resulting in a fully automated and fault-tolerant pipeline."}]}