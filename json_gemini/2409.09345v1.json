{"title": "Enhancing Decision-Making for LLM Agents via Step-Level Q-Value Models", "authors": ["Yuanzhao Zhai", "Tingkai Yang", "Kele Xu", "Dawei Feng*", "Cheng Yang", "Ding Bo", "Huaimin Wang"], "abstract": "Agents significantly enhance the capabilities of standalone Large Language Models (LLMs) by perceiving environments, making decisions, and executing actions. However, LLM agents still face challenges in tasks that require multiple decision-making steps. Estimating the value of actions in specific tasks is difficult when intermediate actions are neither appropriately rewarded nor penalized. In this paper, we propose leveraging a task-relevant Q-value model to guide action selection. Specifically, we first collect decision-making trajectories annotated with step-level Q values via Monte Carlo Tree Search (MCTS) and construct preference data. We then use another LLM to fit these preferences through step-level Direct Policy Optimization (DPO), which serves as the Q-value model. During inference, at each decision-making step, LLM agents select the action with the highest Q value before interacting with the environment. We apply our method to various open-source and API-based LLM agents, demonstrating that Q-value models significantly improve their performance. Notably, the performance of the agent built with Phi-3-mini-4k-instruct improved by 103% on WebShop and 75% on Hot-PotQA when enhanced with Q-value models, even surpassing GPT-40-mini. Additionally, Q-value models offer several advantages, such as generalization to different LLM agents and seamless integration with existing prompting strategies.", "sections": [{"title": "Introduction", "content": "Autonomous agents powered by large language models (LLMs) can operate across a wide range of domains, including web navigation (Yao et al. 2022; Zhou et al. 2024b), interactive question answering (Yang et al. 2018), and tool usage (Ma et al. 2024). By utilizing feedback or observations from environments, LLM agents can reason and plan using prompting strategies to accomplish specific tasks (Yao et al. 2023). The resulting text-based outputs and action plans can then be employed to make API calls and execute operations within these environments.\nDespite these advancements, even agents powered by some of the most effective LLMs, such as GPT-4, struggle with complex multi-step decision-making tasks (Achiam et al. 2023). Beyond intermediate environmental feedback, additional task-specific knowledge is necessary to further enhance decision-making. Allowing LLM agents to engage in multiple trial-and-error processes during inference, strategies such as carefully designed reflection (Shinn et al. 2023) or tree-based search (Zhou et al. 2024a; Koh et al. 2024) can help agents iteratively refine their actions. However, this assumption is not always feasible in realistic applications. Recently, fine-tuning open-source LLM backbones with agent trajectories has emerged as an alternative. While this approach enables LLMs to acquire more task-specific knowledge, it can also degrade their general performance (Chen et al. 2024b). Furthermore, state-of-the-art API-based LLMs, which are more effective for building agents, are not accessible for fine-tuning.\nAs the number of decision-making steps increases, compounding errors and uncertainties can accumulate (Xi et al. 2024a), exacerbating the problem. Since actions are sampled from a distribution of text, the greedy action may not always be the optimal choice in the environment. As shown in Figure 2, suboptimal actions in intermediate steps can lead to task failure. A common and effective approach to enhancing LLMs during inference is Best-of-N sampling (Yang et al. 2024). However, while LLM agents can sample multiple candidate actions before interacting with the environment, they often lack a clear understanding of the action values associated with task completion, as environmental rewards are typically sparse, with only a terminal scalar indicating success (Xi et al. 2024b).\nTo overcome these limitations, we propose leveraging a Q-value model to guide action selection at each decision-making step. Q-value functions, widely adopted by traditional Reinforcement Learning (RL) agents (Konda and Tsitsiklis 1999; Mnih et al. 2015), are trained to estimate the value of specific actions. When applying the Q-value approach to LLM agents, the challenges lie in how to collect training data and how to train Q-value models effectively. As illustrated in Figure 1, we integrate LLM agents with Monte Carlo Tree Search (MCTS) to iteratively explore trajectories, using its look-ahead capability to decompose sparse outcome rewards into step-level Q values. We then construct preference data based on the annotated Q-values. To train the Q-value model, we propose a step-level version of direct policy optimization (DPO) (Rafailov et al. 2023) using an additional LLM. During inference, LLM agents can sample multiple candidate actions and select the one with the highest Q value to interact with the environment in a single trial.\nWe conduct experiments across diverse domains, including web navigation and interactive question answering. The results demonstrate that Q-value models can clearly distinguish actions that lead to success or failure, enhancing decision-making for LLM Agents via select effective actions at each step. Additionally, task-dependent Q-value models are generalizable across different LLM agents, allowing us to utilize inexpensive LLM agents to collect training data while enhancing the decision-making of more advanced LLM agents in a plug-and-play manner. Furthermore, our method complements the design of effective prompting strategies, and integrating it with these strategies can further improve performance. In summary, our main contributions are as follows:\n\u2022 We leverage Q values to enhance the decision-making for LLM agents by guiding action selection at each step.\n\u2022 We utilize the MCTS algorithm to collect decision-making trajectories and annotate them with step-level Q values.\n\u2022 We construct preference data for training and propose step-level DPO to train Q-value models.\n\u2022 Experiments across two domains demonstrate the effectiveness, generalization across LLM agents, and compatibility with existing methods of our Q-value models."}, {"title": "Related Work", "content": "With the advancement of LLMs, LLM agents that interact with the world to perform a wide variety of tasks have become a major focus of research (Wang et al. 2024). The LLM backbone of these agents can be classified into open-source and API-based categories. Open-source LLM agents offer greater flexibility, while API-based LLMs (e.g., GPT-4) are typically more effective as agents (Chen et al. 2024b). In numerous real-world scenarios, agents must execute multi-step actions to tackle complex tasks and incorporate valuable feedback to improve decision-making.\nPrompting Strategies. Numerous prompting strategies (Wang et al. 2022; Xie et al. 2023; Madaan et al. 2023) have been proposed to enhance the reasoning and planning abilities of LLM agents. In the context of enhancing decision-making, ReAct (Yao et al. 2023) is widely used to integrate chain-of-thought (CoT) (Wei et al. 2022) reasoning with intermediate environment observations and agent actions. Reflection involves prompting an LLM to review and critique past interactions to improve current outputs. Reflexion (Shinn et al. 2023) provides agents with dynamic memory and self-reflection modules, enhancing decision-making through multiple trial-and-error iterations. However, due to the limited input context window of LLMs, these methods struggle to accumulate extensive task experience.\nTree-based Search for LLMs. Tree-based search approaches, such as depth-first search (DFS), breadth-first search (BFS), and Monte Carlo Tree Search (MCTS)(Browne et al. 2012), maintain a favorable exploration-exploitation trade-off in many planning algorithms (LaValle 1998). Equipping LLMs with tree-based search methods shows great potential in enhancing reasoning abilities (Hao et al. 2023; Feng et al. 2023; Chen et al. 2024a; Luo et al. 2024). More recently, tree-based search has been integrated with LLM agents to improve planning performance. Zhou et al. (2024a) integrate agents with MCTS, along with LLM-powered value functions and other prompt mechanisms such as reflection. Koh et al. (2024) utilize best-first tree search to enhance LLM agents in realistic web environments. However, constructing a tree during inference not only introduces significant token consumption but also requires environmental reversion assumptions, limiting its practical application.\nFine-tuning LLMs as Agent. Fine-tuning based methods further train open-source LLM backbones as effective alternatives to API-based LLM agents. Most fine-tuning based methods (Chen et al. 2023; Zeng et al. 2023; Chen et al. 2024b) concentrate on imitating curated expert trajectories, which is expensive and sub-optimal due to compounding errors and limited exploration data. In order to get rid of the reliance on expert trajectories, recent works (Christianos et al. 2023; Xi et al. 2024b; Song et al. 2024; Zhai et al. 2024) collect trajectories with outcome rewards to fine-tune LLM using reject sampling fine-tuning (RFT) (Yuan et al. 2023), RL or its variants. Notably, Song et al. (2024) proposes to utilize both successful and failure trajectories to fine-tune LLMs as agents via direct policy optimization (DPO) (Rafailov et al. 2023). Fine-tuning LLMs with agent data on a specific tasks may deteriorate the general performance (Chen et al. 2024b) Additionally, these works can not apply to API-based LLMs, which are demonstrated to be more effective in constructing agents than most open-source LLMs.\nCompared to the various approaches summarized in Table 1, equipping LLM agents with step-level Q-value models offers several notable advantages. Our method can be applied to both open-source and API-based LLM agents without requiring training of the LLM backbones. Additionally, decision-making ability is enhanced by Q-values with a single trial, without needing assumptions about environmental reversion during inference. Our method does not increase context length and allows for accumulation of task experience in Q-value models, which can generalize across different agents and instructions within the task."}, {"title": "Task Formulation", "content": "The agent task with environment feedback can be formalized as a partially observable Markov decision process (POMDP) $(U, S, A, O,T, r)$ with instruction space U, state space S, action space A, observation space O, state transition function $T:S\\times A\\rightarrow S$, and reward function r.\nGiven a task instruction u in the environment, the LLM agent generates an action $\\alpha_{0} \\sim \\pi(\\cdot|u)$ based on its policy $\\pi$. The state then transitions to $s_{1} \\in S$, and the agent receives observation $o_{1} \\in O$. The agent continues to interact with the environment until the task is completed or the maximum number of steps is reached. At time step t, given the history and current observation, the agent generates the subsequent action $a_{t+1} \\sim \\pi(\\cdot|u, a_{0}, o_{0}, ..., a_{t}, o_{t})$. Then the multi-step decision-making task can be formulated as:\n$\\pi(\\tau|u) = \\prod_{t=1}^{T} \\pi(a_{t}|u, \\tau_{t-1})$\n(1)\nwhere we denote $\\tau$ as the whole trajectory, T as the total interaction steps. $\\tau_{t-1} = (a_{0}, o_{0}, ..., h_{t-1}, a_{t-1}, O_{t-1})$ denotes the interactive history up to t \u2013 1. The environment only provide the outcome reward $r(u, \\tau) \\in [0,1]$. The objective of LLM agents is to maximize rewards from the environment:\n$\\max_{\\pi} E_{u \\sim D,\\tau \\sim \\pi(\\cdot|u)} [r(u, \\tau)]$,\n(2)\nwhere D represents the dataset containing task instructions."}, {"title": "Proposed Method", "content": "We can build a decision tree where each node in the tree denotes an state and edge is an action. Each node stores a set of statistics:\n$\\left\\{V(s_{t}), N(s_{t})\\right\\}$,\n(3)\nwhere $V(s_{t})$ represents the value function, which measures the expected reward from the sub-tree of st. $N(s_{t})$ denotes the number of visits to a node st.\nStep-level Q Values Estimation with MCTS\nThe MCTS process starts from a root node s0 and progresses through four iterative stages: selection, expansion, evaluation and backpropagation, as shown in Figure 3(a).\nSelection. The objective of the first operation, selection, is to identify the most suitable trajectories for the next expansion step. We select the trajectory from the root node to a current leaf node. At each depth, we select the children with the highest Upper Confidence bounds applied to Trees (UCT) (Kocsis and Szepesv\u00e1ri 2006) value to balance exploration and exploitation:\n$UCT(s_{t}) = V(s_{t}) + \\eta \\sqrt{\\frac{1n N(p(s_{t}))}{N(s_{t})}}$\n(4)\nwhere $\\eta$ is the exploration weight, and $p(s_{t})$ denotes the parent node of st."}, {"title": "Expansion", "content": "The second operation expands the tree by sampling n actions from $\\pi$, as outlined in the previous section. Unlike traditional agents, such as those used in Go, which operate in a finite action space, LLM agents have an infinite action space. LLMs can generate an unlimited number of distinct actions (sequences of tokens), though some of these may be invalid. To ensure diversity, we sample multiple candidate actions using a high temperature. The environment processes each action and provides corresponding feedback as an observation, resulting in n new child nodes being added to the tree."}, {"title": "Evaluation", "content": "Since the tree depths for LLM agent tasks are typically much shallower than those for Go games, expansions quickly reach terminal nodes. Unlike AlphaGo (Silver et al. 2016), which learns a value network to evaluate the value of state nodes, we evaluate the expanded nodes using a rollout algorithm. Specifically, starting from the expanded nodes, the LLM agent interacts with the environment until termination or the maximum rollout depth is reached. If the explored node is terminal, the environment's provided outcome reward is returned; otherwise, a fixed negative reward is assigned to the explored node at the maximum depth."}, {"title": "Backpropagation", "content": "This operation updates the tree statistics based on the outcome rewards or fixed negative rewards assigned during the evaluation stage. For each node in the trajectory T, N(s) is incremented by 1, and the values are updated from the end node st to the root node s0 using the following formula:\n$V(s_{t}) \\leftarrow \\frac{V(s_{t-1})(N(s_{t-1}) - 1) + r(s)}{N(s_{t})}$\n(5)\nThe updated values are utilized in the UCT Equation 4 to guide the selection of the next node.\nAfter multiple iterations of selection, expansion, evaluation, and backpropagation, we obtain the final tree, which stores the expanded nodes and their corresponding state values. Early stopping is triggered once the maximum reward of 1 is obtained. The Q-value of non-terminal nodes can be calculated as follows:\n$Q(s_{t}, a_{t}) = r(s_{t}, a_{t}) + V(s_{t+1}) = V(s_{t+1})$,\n(6)\nassuming the transition function is deterministic. Otherwise, $Q(s_{t}, a_{t})$ can be considered a Monte Carlo estimate of the true Q-value."}, {"title": "Training Q-Value Models", "content": "Due to the limitations of MCTS iterations, $Q(s_{t}, a_{t})$ may not accurately fit the true Q-value. However, it is easier to distinguish between win and lose actions based on Q-values among multiple candidate actions. Therefore, we employ a preference learning algorithm called Direct Policy Optimization (DPO), leveraging its effectiveness in learning implicit value models (Zhong et al. 2024; Rafailov et al. 2023). As mentioned earlier, directly fine-tuning LLM backbones has several drawbacks. Instead, we train an additional LLM, $\\pi_{\\theta}$, parameterized by $\\theta$, to learn Q-values. Given that evaluation tasks are simpler than generation tasks (Pang et al. 2024), $\\pi_{\\theta}$ can be smaller than the LLM backbones $\\pi$ of the agent.\nUnder the Bradley-Terry model (Bradley and Terry 1952), DPO propose a preference learning loss to optimize the objective in Equation 2 while keeping the KL distance between the training model and the initial model.\n$L_{trajectory} (\\pi_{\\theta}; \\pi_{ref}) = -E_{(u,\\tau^{w},\\tau^{l})\\sim D} \\left[log \\sigma \\left(\\beta \\left(log \\frac{\\pi_{\\theta}(\\tau^{w}|u)}{\\pi_{ref}(\\tau^{w}|u)} - log \\frac{\\pi_{\\theta}(\\tau^{l}|u)}{\\pi_{ref}(\\tau^{l}|u)}\\right)\\right)\\right]$\n(7)\nwhere $\\sigma$ is the sigmoid function, $\\beta$ is a weighting parameter of KL regularization, and $\\pi_{ref}$ is the reference model, which is usually served by supervised fine-tuning LLMs before preference learning. Besides task instructions u, the dataset D contains win trajectories $\\tau^{w}$ and lose trajectories $\\tau^{l}$. Without process supervision, LLM agents cannot be fine-tuned at the step level. This limitation hinders performance in multi-step decision-making tasks, as will be demonstrated in the experimental section. To address this issue, we construct more fine-grained preference data and propose a step-level version of DPO."}, {"title": "Preference data construction", "content": "We aim to construct step-level preference data based on $Q(s_{t}, a_{t})$ estimated using Equation 6. To achieve this, we need to identify win and lose actions for the shared decision-making trajectory segment. We first locate the terminal node with the highest reward in the final tree and then extract the corresponding trajectories from the terminal node to the root node. At each depth, we select a partial segment of the trajectory $T_{t}$ as the shared part. Win actions, $a^{w}$, are taken from the selected trajectory at the next step, while lose actions, $a^{l}$, are chosen from candidate actions with the lowest $Q(s_{t}, a_{t})$, as illustrated in Figure 3(b). This approach focuses preference learning on distinguishing between $a^{w}$ and $a^{l}$, providing detailed insights into which actions might lead to failure in the overall decision-making process, as indicated by the Q-value."}, {"title": "Step-level preference learning", "content": "Given the preference pairs $\\left\\{u, T_{t}, a^{w}, a^{l}\\right\\}$, the objective of training step-level Q-value models can be formulated as:\n$L_{step} (\\pi_{\\theta}; \\pi_{ref}) = -E_{(u,T_{t},a^{w},a^{l})\\sim D} \\left[log \\sigma \\left(\\beta \\left(log \\frac{\\pi_{\\theta}(a^{w}|u, T_{t})}{\\pi_{ref}(a^{w}|u, T_{t})} - log \\frac{\\pi_{\\theta}(a^{l}|u, T_{t})}{\\pi_{ref}(a^{l}|u, T_{t})}\\right)\\right)\\right]$\n(8)\nwhere D contains step-level preference data from t = 0 to t = T. The normalized logits of the DPO model effectively learn implicit value models (Rafailov et al. 2023, 2024). In our scenario, DPO fits the estimated Q-value $Q(s_{t}, a_{t})$ and can generalize to new states and actions. With the well-trained $\\pi_{\\theta}$, the Q-value can be calculated as:\n$Q(u, T_{t}, a_{t}) = \\beta log \\pi_{\\theta}(a^{w}|u, T_{t}) - \\beta \\pi_{ref}(a^{w}|u, T_{t})$.\n(9)\nFor brevity, we refer to $Q(u, T_{t}, a_{t})$ as the Q-value model, which consists of the trained model $\\pi_{\\theta}$ and its reference model $\\pi_{ref}$ for normalization.\nAt inference time, the LLM agent uses the Q-value model to generate the action with the highest Q-value to interact with the environment. This is formulated as:\n$a_{t} = \\underset{a}{arg \\ max} [Q(u, T_{t}, a)]$\n(10)\nIn practice, due to the infinite action space, we sample n candidate actions, similar to the expansion stage of MCTS, and select the action with the highest Q-value to interact with the environment."}, {"title": "Experiments", "content": "Experimental Settings\nTo validate the versatility of our method, we apply Q-value models to various LLM backbones, including popular open-source LLMs such as the Phi-3-mini-4k-instruct model with 3.8B parameters and Llama-3.1-8B-Instruct, as well as API-based LLMs like GPT-40-mini and GPT-4-turbo. The Q-value models are based on Phi-1.51, which has 1.3B parameters. For efficiency, unless otherwise stated, the LLM agents used for collecting step-level preference data are primarily based on the Phi-3-mini-4k-instruct model. The maximum context length is set to 4096.\nWe evaluate our method on two tasks across different domains: WebShop (Yao et al. 2022) and HotPotQA (Yang et al. 2018). We include 3-shot in-context examples in the instruction prompt for both tasks. The maximum number of decision-making steps is set to 10 for WebShop and 7 for HotPotQA. For HotPotQA, we randomly select 1000 questions for training, 100 for validation, and 100 for testing. For WebShop, we follow the data split described in Song et al. (2024), which consists of 1824 instructions for training, 100 questions for validation, and 100 questions for testing. All experiments are conducted on a single NVIDIA A40 48G GPU, except when implementing fine-tuning-based methods, which require two NVIDIA A100 80G GPUs. Detailed information on the environment and hyperparameters can be found in Appendix A.\nBaselines. We mainly compare our method with various fine-tuning based methods because both approaches involve accumulating task experience through training LLMs and do not require multiple trials during inference. Rejection Sampling Fine-Tuning (RFT) (Yuan et al. 2023) uses demonstrated trajectories to train LLM backbones. AgentEval is similar to RFT but assigns weights to trajectories based on their rewards. ETO employs DPO to enhance LLM agents, using both win trajectories $\\tau^{w}$ and lose trajectories $\\tau^{l}$, which are sampled from self-explored trajectories and distinguished by outcome rewards from the environment. Best-of-N (BON) samples n trajectories using vanilla LLM agents and selects the one with the highest reward. Note that BoN serves as a strong baseline because it requires multiple query outcome rewards from the environment. The number of candidate actions is set to n = 5, unless otherwise specified, for both our method and BoN. For a fair comparison, training data for all methods are collected using MCTS.\nResults\nWe report the results on two tasks in Table 2. As shown, our main findings are as follows:\nQ-value models can significantly enhance decision-making. Well-trained Q-value models double the performance of LLM agents based on Phi-3-mini-4k-instruct on the WebShop task and improve performance by 75% on the HotPotQA task. The enhanced LLM agent outperform the lightweight GPT-40-mini on both tasks and even surpass the more advanced GPT-4-turbo on the WebShop task. There are two reasons to explain why Q-value models bring more performance gains on WebShop. First, the WebShop task involves more decision-making steps than HotPotQA, allowing Q-value models to substantially reduce accumulation errors. Second, unlike the WebShop task, which provides more granular rewards ranging from 0 to 1, HotPotQA offers binary rewards of 0 or 1. This binary reward structure makes it more challenging to construct finely distinguished preference data, which we will explore in the next section."}, {"title": "Training Q-value models is more efficient and effective than fine-tuning LLM backbones", "content": "RFT, which utilizes demonstrated trajectories for supervised fine-tuning of LLMs, improves performance on both tasks. AgentEval, which incorporates more reward information, enhances performance in the WebShop task but not in the HotPotQA task. This is because the HotPotQA environment only provides binary rewards, effectively reducing AgentEval\u2019s performance to that of RFT. ETO, which incorporates more losing trajectories for learning, achieves the best performance among fine-tuning-based methods. This underscores the importance of including imperfect trajectories in training.\nFine-tuning LLM backbones requires high-performance computing resources, particularly as LLM size and context length increase. Therefore, our comparison with fine-tuning-based methods primarily uses Phi-3-mini-4k-instruct with 3.8B parameters. In contrast, our Q-value models are based on the more lightweight Phi-1.5 with 1.3B parameters. Nevertheless, our method is more effective than all the fine-tuning-based methods mentioned above and outperforms BoN in both tasks. We note that BoN, which has the same computational overhead with our method but the additional outcome reward from the environment, is a strong baseline, and our method outperforms BoN with on both tasks."}, {"title": "Q-value models are generalizable across different LLM backbones", "content": "The Q-value models accumulate task experience, and we expect them to generalize across different LLM agents within the same task. To verify this, we first train Q-value models using preference data sampled from Phi-3-mini-4k-instruct. We then apply these Q-value models directly to stronger open-source LLMs, such as Llama-3.1-8B-instruct, and API-based LLMs, including GPT-40-mini and GPT-4-turbo. We observe that the decision-making abilities are consistently improved, although the performance gains are not as substantial as when the Q-value models are applied to the LLM agents that generated the training data. This is because the states and actions sampled by other LLM agents can be considered Out-Of-Distribution (OOD) relative to the step-level preference data collected by Phi-3-mini-4k-instruct, which was used to train the Q-value models.\nNevertheless, these positive results suggest that trial-and-error experience from a less powerful and more cost-effective LLM agent can benefit stronger or API-based, more expensive LLM agents."}, {"title": "Evaluations of Q-value Models", "content": "We further investigate the accuracy of Q-value models in assessing the preference relationships of collected step-level data. As shown in Figure 4(a), preference relationships within the training sets are learned effectively in both tasks. However, when evaluating on the in-distribution (IND) test set, accuracy decreases to 83% on WebShop and 67% on HotPotQA. The performance gap on HotPotQA is attributed to its binary outcome reward and the early stopping of MCTS when the reward of 1 is obtained. Additionally, generalizing to the OOD test set, where preference data is collected by other LLM agents, results in a slight performance degradation on both tasks. Nevertheless, this level of preference accuracy is sufficient to enhance the performance of downstream tasks, consistent with recent studies on learning reward models (Lambert et al. 2024).\nTo further evaluate the effectiveness of Q-value models, we select 200 actions from successful and failed trajectories, respectively, and visualize their Q-values in Figure 4(b). The Q-value distribution for actions in failed trajectories is skewed to the left, while the distribution for successful actions shows less skewness, with most of the probability density leaning to the right. This pattern may arise because failures often result from choosing detrimental actions (Koh et al. 2024), suggesting that our Q-value models are capable of effective credit assignment."}, {"title": "Ablation Studies", "content": "Advantage of Step-Level Preference Data. Recent studies (Rafailov et al. 2024; Zhong et al. 2024) indicate that the trajectory-level DPO objective, as described in Equation 7, also holds potential for credit assignment. To evaluate this, we establish an additional baseline by comparing our proposed step-level Q-value model with a Q-value model trained using trajectory-level preference data $(u, \\tau^{w}, \\tau^{l})$. Our results, as shown in Table 3, suggest that while Q-value models trained with trajectory-level data can enhance LLM agents, their performance improves gradually as more candidate actions are sampled at each step. However, models trained with our step-level preference data consistently outperform this baseline across various numbers of candidate actions. This superior performance can be attributed to the more granular information provided by planning steps, as represented by the node values in the Monte Carlo tree.\nHow much preference data is needed for training? To train a Q-value model, step-level preference data must be constructed using task instructions. We investigate how different amounts of training data impact downstream performance. As shown in Figure 5(a), we evaluate several checkpoints from one epoch of training the Q-value model on the HotPotQA task, which represents varying quantities of training samples. We observe that fewer than 400 step-level preference data points can significantly enhance performance, achievable with approximately 250 task instructions in our setting. This demonstrates the sample efficiency of our approach for training Q-value models.\nAblation of MCTS Iterations. More preference data can be collected by increasing the number of MCTS iterations, though this also increases computational overhead. In our previous experiments, we set the MCTS iteration to m = 30 by default. We perform an ablation study on the number of MCTS iterations to assess its impact on data collection. As shown in Figure 5(b), the number of successful trajectories available for constructing step-level preference data increases with the maximum number of MCTS iterations. Nearly all MCTS processes terminate early, before the 50th iteration, due to achieving the maximum reward or depth, rendering additional iterations redundant. Furthermore, the number of step-level preference data points increases more rapidly than the number of successful trajectories with additional MCTS iterations. This is because trajectories explored with a larger number of MCTS iterations typically involve more decision-making steps, thus providing more step-level preference data."}, {"title": "Integration with different prompting strategies", "content": "In our work, we use a ReAct-style prompt to enable LLMs to function as agents. We further enhance LLM agents with a more sophisticated prompting strategy, \u201cReAct + Reflection\u201d. As shown in Table 4, this improves the performance of GPT-40-mini from 0.31 to 0.39. We also apply the prompting strategy to the LLM agent based on Phi-3-mini-4k-instruct. However, the performance decreased from 0.20 to 0.15. This may be-cause that Phi-3-mini-4k-instruct with 3.8B parameters can not adequately understand the reflection prompts.\nWe use the same experimental settings as described in Table 2 to train Q-value models, but with different prompting strategies and by sampling trajectories using GPT-40-mini instead of Phi-3-mini-4k-instruct. The results indicate that methods incorporating both reflection and Q-value models achieve the highest average reward of 0.48, suggesting that our proposed method complements the design of more effective prompting strategies. Additionally, combining the results from Table 2 and Table 4, we observe that the Q-value model trained on preference data collected by GPT-40-mini outperforms the model trained on data sampled by Phi-3-mini-4k-instruct, with average rewards of 0.48 and 0.46, respectively. This finding is consistent with our observation that the preference accuracy on the OOD test set exceeds the preference accuracy on the IND test set, as shown in Figure 4(a)."}, {"title": "Conclusion and Limitations", "content": "In this paper, we propose leveraging Q-values to guide action selection at each decision-making step. We collect training data using MCTS and train Q-value models through step-level direct policy optimization. Results from two distinct tasks demonstrate that our method is more efficient and effective compared to fine-tuning LLM backbones. Furthermore, the trained Q-value models are plug-and-play, easily applicable to both open-source and API-based LLM agents, and generalize well across them. We believe our method introduces a novel and flexible paradigm for enhancing the decision-making capabilities of LLM agents.\nWhile collecting training data introduces O(kn) sample complexity, the feasibility of sampling with lightweight open-source LLM agents makes this manageable. Our method does not increase context length, but it does introduce n-fold token consumption for sampling multiple candidate actions during inference. This trade-off is acceptable and can be further optimized through caching technologies. Due to computational resource constraints, the Q-value models are limited to 1.3B parameters. Exploring the use of more powerful LLMs could enhance the effectiveness of Q-value models, which we plan to address in future work."}, {"title": "A.1 Environment Details", "content": "WebShop. WebShop tasks the agent with solving a shopping task by browsing websites with detailed product descriptions and specifications. The available action APIs include search[QUERY] for using the search bar and click[BUTTON] for clicking buttons on web pages. Clickable buttons include product titles, options, buy, back to search, and previous/next page, among others. When the agent selects the \"Buy Now\" action, the environment provides an outcome reward ranging from 0 to 1 based on the matching heuristics of the product\u2019s attributes and price.\nHotPotQA. HotPotQA is a question-answering task that requires retrieval across Wikipedia passages. Following the setup of (Yao et al. 2023), LLM agents are equipped with API calls for searching (Search[INFORMATION]) and retrieving (Lookup[INFORMATION]). Upon receiving an answer, the environment provides a binary outcome reward of 0 or 1 based on its correctness according to the ground truth."}, {"title": "A.2 Hyper-parameters", "content": "The hyper-parameters for collecting step-level preference data via MCTS and training Q-value models are summarized in Table 5."}, {"title": "B Case Study on WebShop", "content": "In this section, we present a case study to further analyze the action selection guided by Q-"}]}