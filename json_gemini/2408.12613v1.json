{"title": "Deceptive uses of Artificial Intelligence in elections strengthen support for AI ban", "authors": ["Andreas Jungherr", "Adrian Rauchfleisch", "Alexander Wuttke"], "abstract": "All over the world, political parties, politicians, and campaigns explore how Artificial Intelligence (AI) can help them win elections. However, the effects of these activities are unknown. We propose a framework for assessing Al's impact on elections by considering its application in various campaigning tasks. The electoral uses of AI vary widely, carrying different levels of concern and need for regulatory oversight. To account for this diversity, we group AI-enabled campaigning uses into three categories - campaign operations, voter outreach, and deception. Using this framework, we provide the first systematic evidence from a preregistered representative survey and two preregistered experiments (n=7,635) on how Americans think about AI in elections and the effects of specific campaigning choices. We provide three significant findings. 1) the public distinguishes between different AI uses in elections, seeing AI uses predominantly negative but objecting most strongly to deceptive uses; 2) deceptive AI practices can have adverse effects on relevant attitudes and strengthen public support for stopping AI development; 3) Although deceptive electoral uses of AI are intensely disliked, they do not result in substantial favorability penalties for the parties involved. There is a misalignment of incentives for deceptive practices and their externalities. We cannot count on public opinion to provide strong enough incentives for parties to forgo tactical advantages from AI-enabled deception. There is a need for regulatory oversight and systematic outside monitoring of electoral uses of AI. Still, regulators should account for the diversity of AI uses and not completely disincentivize their electoral use.", "sections": [{"title": "People dislike AI use in elections but differentiate between uses", "content": "We asked a representative sample of Americans (n=1,199) for their opinions on specific uses of AI and checked for associations with underlying views on the risks and benefits of AI generally (see Supplementary Information for details). In our preregistered study, we provided respondents with fifteen short descriptions of various campaigning tasks for which parties and candidates use AI. These tasks fall into three broad categories:\n\u2022 Support of campaign operations, including automated idea and content generation, automated interactions through chatbots, or the automated segmentation of donor and walk lists.\n\u2022 Improving voter outreach, including the AI-enabled identification of people likely to be susceptible to volunteer approaches, AI-enabled optimization of messages to increase their persuasive appeal either on mass or targeted to individuals, or automated generation and targeted roll-out of personalized ads in digital communication environments.\n\u2022 Deception, including undeclared uses of AI to generate false or misleading audio or video content misrepresenting a candidate's actions to make them look better or an opponent worse, impersonating a candidate's likeness in video or audio formats and having them communicate misleading messages, or automated and interactive astroturfing by bots enabled through large language models in digital communication spaces or email communication with journalists or members of the public.\nWe identified five specific example tasks for each category and asked respondents how they felt about them.\nThe ridgeline plots show that people dislike all kinds of AI uses, but they specifically dislike deceptive uses. In general, people tend to perceive AI uses in elections with a greater sense of norm violation and worry than the impression that they could increase voter involvement. Compared to other AI uses, deceptive uses of AI carried a greater sense of norm violation, were more worrisome, and were seen as less likely to increase voter involvement than AI uses for operations and voter outreach. The plots clearly show that people look at different uses of AI differently.\nWe ran regression models (n = 1,199) explaining worry, norm violation, and perceived opportunities for a rise in voter involvement. Responses ranged from 1 (low) to 7 (high). The models show that controlling for other factors, people dislike any electoral use of AI and see somewhat low potential in AI use to increase voter involvement. Uses categorized as deceptive were seen more negatively than other uses.\nPeople's attitudes toward AI use in elections are connected with underlying attitudes toward Al's general benefits and risks for society. Those who see benefits in AI have more positive views on AI use in elections. This suggests that these attitudes align with deeper assessments of AI's role in society and indicates that experiences with electoral uses of AI might also"}, {"title": "AI-enabled deception increases support for a stop to AI devel- opment", "content": "In a preregistered follow-up study (n=1,985), we identified the causal effects of learning about different types of AI uses in elections (see Supplementary Information for details). We divided respondents into three treatment groups and one control group (n = 497). Deception Treatment (n = 497) contained information about campaigns' uses of AI for deception. Operations Treatment (n = 494) contained information about campaigns' uses of AI for improving campaign operations. Outreach Treatment (n = 497) contained information about campaigns' uses of AI for voter outreach. Since Study 1 showed that deceptive uses of AI stand out in people's perception consistently, we preregistered the deception treatment as the reference group.\nPeople who learned about campaigns' deceptive uses of AI were likelier to express worry and a sense of norm violation than respondents in all other experimental conditions. Some outcomes remained unaffected, such as the perceived fairness of the election. Political parties were also rated similarly across experimental conditions, suggesting limited effects of AI use on party competition.\nYet, we observe various negative side-effects of AI-enabled deception. Learning about deceptive uses leads to a sense of personal control loss. But, even more fundamentally, learning about deceptive uses of AI generally impacts people's attitudes toward AI. When asking people for their support or opposition toward a complete stop to AI development and use, we find"}, {"title": "Parties face no favorability penalty for deceptive AI use", "content": "We also tested whether parties face a penalty for deceptive AI uses attributed to them. Given the strong evidence for motivated group-serving cognitions among partisans in other contexts, we can expect heterogeneous effects across party lines.\nThis preregistered study is based on three samples containing only self-identified partisans for (1) Democrats (n = 1,489), (2) Republicans (n = 1,485), and (3) Independents (n = 1,477) (see Supplementary Information for details). Respondents were split into two treatment groups and one control group, serving as the reference category. Treatments contained information about deceptive uses of AI by candidates from the Democratic Party (Democrat Deception) or the Republican Party (Republican Deception).\nWe do not find evidence of meaningful effects on party-related attitudes. While Democrats and Republicans expressed a greater sense of norm violation when learning of their parties' alleged deceptive use of AI, neither group sanctioned their party for using AI. Compared to the control groups, neither Democrats nor Republicans significantly lowered their favorability assessment of their party when learning of alleged deceptive uses of AI. Partisans of both parties disapprove of AI-enabled deception but do not punish the party they support for this violation. We also see that independents are not adjusting their favorability ratings of parties allegedly using AI deceptively (see Supplementary Information for an equivalence test identifying no substantial differences in favorability ratings).\nAgain, information about deceptive AI use in elections increases support for an AI ban. This effect is not meaningfully explained by the group-serving cognitions identified above. The bar charts show different base levels of support for an AI ban among Democrats and Republicans, with 39% of Republican respondents supporting the ban without any information about deceptive uses. In comparison, only 28% of Democrat respondents do so. Democrats significantly increased their support for the ban when informed about deceptive uses by either party.\nThese findings underscore that deceptive uses of AI in elections come with negative externalities. People disapprove of deceptive uses, but parties face no favorability penalty for alleged deceptive uses, either because of motivated group-serving cognitions or entrenched attitudes in the current political climate in the US. While the perpetrators of deceptive uses of AI might thus face no attitudinal penalties, their actions impact the public demand for a stricter and potentially downright hostile regulatory environment for the development and use of AI."}, {"title": "Discussion", "content": "This article provides evidence of how people think about AI uses in elections and the effects of different uses on public opinion. People have distinct attitudes on different types of use, reacting"}, {"title": "Materials and Methods", "content": "See Supporting Information for a detailed description of all materials and methods used in this study, preregistrations, replication materials, and full regression tables for the reported models. The Institutional Review Board at National Taiwan University approved our research."}]}