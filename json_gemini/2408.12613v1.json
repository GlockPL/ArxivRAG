{"title": "Deceptive uses of Artificial Intelligence in elections strengthen support for AI ban", "authors": ["Andreas Jungherr", "Adrian Rauchfleisch", "Alexander Wuttke"], "abstract": "All over the world, political parties, politicians, and campaigns explore how Artificial Intelligence (AI) can help them win elections. However, the effects of these activities are unknown. We propose a framework for assessing Al's impact on elections by considering its application in various campaigning tasks. The electoral uses of AI vary widely, carrying different levels of concern and need for regulatory oversight. To account for this diversity, we group AI-enabled campaigning uses into three categories - campaign operations, voter outreach, and deception. Using this framework, we provide the first systematic evidence from a preregistered representative survey and two preregistered experiments (n=7,635) on how Americans think about AI in elections and the effects of specific campaigning choices. We provide three significant findings. 1) the public distinguishes between different AI uses in elections, seeing AI uses predominantly negative but objecting most strongly to deceptive uses; 2) deceptive AI practices can have adverse effects on relevant attitudes and strengthen public support for stopping AI development; 3) Although deceptive electoral uses of AI are intensely disliked, they do not result in substantial favorability penalties for the parties involved. There is a misalignment of incentives for deceptive practices and their externalities. We cannot count on public opinion to provide strong enough incentives for parties to forgo tactical advantages from AI-enabled deception. There is a need for regulatory oversight and systematic outside monitoring of electoral uses of AI. Still, regulators should account for the diversity of AI uses and not completely disincentivize their electoral use.", "sections": [{"title": "Introduction", "content": "All over the world, political parties, politicians, and campaigns explore how Artificial Intelligence (AI) can help them win elections\u00b9. However, the effects of these activities are unknown. Some studies have started documenting the direct effects of AI-enabled communicative interventions - such as AI-driven persuasion 2,3 or deepfakes 4,5. However, the impact of AI use in elections goes further.\nElections are times of high public attention on campaigns and their tools of communication. Many campaigns have become key exemplars for the perceived power of new technology for communication, coordination, and organizing. For example, the campaigns by Howard Dean and Barack Obama have become exemplars for the supposedly empowering effects of digital technology and contributed to largely positive and aspirational narratives about the empowering role of digital media for society 7,8. Conversely, narratives about the supposed role of Cambridge Analytica in the Brexit and Trump campaigns are regularly used to illustrate the perceived dangers of data-driven profiling and surveillance and have contributed to a demand for heightened regulatory oversight and control of digital media companies 9,10. The coming US presidential election is shaping up to become a focusing event for the public perception of AI."}, {"title": "Results", "content": "Our study identifies a misalignment of incentives for deceptive practices and their externalities. We cannot count on public opinion to provide strong enough incentives for parties to forgo tactical advantages from AI-enabled deception. At the same time, deceptive practices carry significant"}, {"title": "People dislike AI use in elections but differentiate between uses", "content": "We asked a representative sample of Americans (n=1,199) for their opinions on specific uses of AI and checked for associations with underlying views on the risks and benefits of AI generally (see Supplementary Information for details). In our preregistered study, we provided respondents with fifteen short descriptions of various campaigning tasks for which parties and candidates use AI. These tasks fall into three broad categories:\n\u2022 Support of campaign operations, including automated idea and content generation, automated interactions through chatbots, or the automated segmentation of donor and walk lists.\n\u2022 Improving voter outreach, including the AI-enabled identification of people likely to be susceptible to volunteer approaches, AI-enabled optimization of messages to increase their persuasive appeal either on mass or targeted to individuals, or automated generation and targeted roll-out of personalized ads in digital communication environments.\n\u2022 Deception, including undeclared uses of AI to generate false or misleading audio or video content misrepresenting a candidate's actions to make them look better or an opponent worse, impersonating a candidate's likeness in video or audio formats and having them communicate misleading messages, or automated and interactive astroturfing by bots enabled through large language models in digital communication spaces or email communication with journalists or members of the public.\nWe identified five specific example tasks for each category and asked respondents how they felt about them.\n shows the distribution of responses for each of the fifteen uses grouped by category. The ridgeline plots show that people dislike all kinds of AI uses, but they specifically dislike deceptive uses. In general, people tend to perceive AI uses in elections with a greater sense of norm violation 12-14 and worry than the impression that they could increase voter involvement. Compared to other AI uses, deceptive uses of AI carried a greater sense of norm violation, were more worrisome, and were seen as less likely to increase voter involvement than AI uses for operations and voter outreach. The plots clearly show that people look at different uses of AI differently.\nWe ran regression models (n = 1,199) explaining worry, norm violation, and perceived opportunities for a rise in voter involvement. Responses ranged from 1 (low) to 7 (high). The models show that controlling for other factors, people dislike any electoral use of AI and see somewhat low potential in AI use to increase voter involvement. Uses categorized as deceptive were seen more negatively than other uses (see , first row).\nPeople's attitudes toward AI use in elections are connected with underlying attitudes toward Al's general benefits and risks for society (see , second row). Those who see benefits in AI have more positive views on AI use in elections. This suggests that these attitudes align with deeper assessments of Al's role in society and indicates that experiences with electoral uses of AI might also"}, {"title": "AI-enabled deception increases support for a stop to AI development", "content": "In a preregistered follow-up study (n=1,985), we identified the causal effects of learning about different types of AI uses in elections (see Supplementary Information for details). We divided respondents into three treatment groups and one control group (n = 497). Deception Treatment (n = 497) contained information about campaigns' uses of AI for deception. Operations Treatment (n = 494) contained information about campaigns' uses of AI for improving campaign operations. Outreach Treatment (n = 497) contained information about campaigns' uses of AI for voter outreach. Since Study 1 showed that deceptive uses of AI stand out in people's perception consistently, we preregistered the deception treatment as the reference group.\nPeople who learned about campaigns' deceptive uses of AI were likelier to express worry and a sense of norm violation than respondents in all other experimental conditions (see , first row). Some outcomes remained unaffected, such as the perceived fairness of the election. Political parties were also rated similarly across experimental conditions, suggesting limited effects of AI use on party competition.\nYet, we observe various negative side-effects of AI-enabled deception. Learning about deceptive uses leads to a sense of personal control loss. But, even more fundamentally, learning about deceptive uses of AI generally impacts people's attitudes toward AI (see , second row). When asking people for their support or opposition toward a complete stop to AI development and use, we find"}, {"title": "Parties face no favorability penalty for deceptive AI use", "content": "We also tested whether parties face a penalty for deceptive AI uses attributed to them. Given the strong evidence for motivated group-serving cognitions among partisans in other contexts15\u201317, , we can expect heterogeneous effects across party lines.\nThis preregistered study is based on three samples containing only self-identified partisans for (1) Democrats (n = 1,489), (2) Republicans (n = 1,485), and (3) Independents (n = 1,477) (see Supplementary Information for details). Respondents were split into two treatment groups and one control group, serving as the reference category. Treatments contained information about deceptive uses of AI by candidates from the Democratic Party (Democrat Deception) or the Republican Party (Republican Deception).\nWe do not find evidence of meaningful effects on party-related attitudes (see , first row). While Democrats and Republicans expressed a greater sense of norm violation when learning of their parties' alleged deceptive use of AI, neither group sanctioned their party for using AI. Compared to the control groups, neither Democrats nor Republicans significantly lowered their favorability assessment of their party when learning of alleged deceptive uses of AI. Partisans of both parties disapprove of AI-enabled deception but do not punish the party they support for this violation. We also see that independents are not adjusting their favorability ratings of parties allegedly using AI deceptively (see Supplementary Information for an equivalence test identifying no substantial differences in favorability ratings).\nAgain, information about deceptive AI use in elections increases support for an AI ban (see ). This effect is not meaningfully explained by the group-serving cognitions identified above. The bar charts show different base levels of support for an AI ban among Democrats and Republicans, with 39% of Republican respondents supporting the ban without any information about deceptive uses. In comparison, only 28% of Democrat respondents do so. Democrats significantly increased their support for the ban when informed about deceptive uses by either party.\nThese findings underscore that deceptive uses of AI in elections come with negative externalities. People disapprove of deceptive uses, but parties face no favorability penalty for alleged deceptive uses, either because of motivated group-serving cognitions or entrenched attitudes in the current political climate in the US. While the perpetrators of deceptive uses of AI might thus face no attitudinal penalties, their actions impact the public demand for a stricter and potentially downright hostile regulatory environment for the development and use of AI."}, {"title": "Discussion", "content": "This article provides evidence of how people think about AI uses in elections and the effects of different uses on public opinion. People have distinct attitudes on different types of use, reacting"}, {"title": "Conclusion", "content": "negative externalities by increasing public demand for a restrictive regulatory environment for all AI development and use, potentially leading societies to forgo nascent AI-driven opportunities in other societal fields. Consequently, there is a need for regulatory oversight and systematic outside monitoring of electoral uses of AI. Still, we see the public differentiate between different electoral uses of AI. Correspondingly, regulators should not completely disincentivize electoral AI uses. AI can contribute to various campaigning tasks, often allowing parties to allocate resources more efficiently, concentrate their outreach efforts, and be more responsive to voters. This can strengthen democracy. Any proposal for the governance and regulation of AI in elections must account for the diversity of uses and potential impacts."}, {"title": "Materials and Methods", "content": "See Supporting Information for a detailed description of all materials and methods used in this study, preregistrations, replication materials, and full regression tables for the reported models. The Institutional Review Board at National Taiwan University approved our research."}, {"title": "Appendices", "content": ""}, {"title": "Study 1", "content": "In Study 1, we queried people for their opinions on specific uses of AI in elections and their views on the benefits and risks of AI in other areas. We ran a preregistered survey (n=1,199) among members of an online panel that the market and public opinion research company Ipsos provided.\nWe used quotas on age, gender, region, and education to realize a sample representative of the US electorate. As Table 1 shows, the sampling was largely successful. The average interview length was 15 minutes. The survey was fielded between April 4 and April 17, 2024. The fieldwork was conducted in compliance with the standards ISO 9001:2015 and ISO 20252:2019, as were all study- related processes. Before running the survey, we registered our research design, analysis plan, and hypotheses about outcomes. We did not deviate from the registered procedure (Preregistration: https://osf.io/3nrb4/?view_only=1d82e100d6084edd81d9c4af46f31a30).\nAs specified in the preregistration, we used three attention checks to identify and exclude inattentive respondents. The first was an open-ended question, the second was hidden in an item grid, and the third was a simple single-choice question. The three checks were distributed throughout the entire survey. Ipsos excluded respondents if they failed two out of three attention checks. In the final dataset provided by Ipsos, only one additional respondent remained with two failed attention checks and was"}, {"title": "Other items", "content": "We provided respondents with short descriptions of various campaigning tasks for which parties and candidates use AI. These tasks fall into three broad categories:\n\u2022 campaign operations;\n\u2022 voter outreach;\n\u2022 deception.\nFor each category we identified five example tasks that practitioners and journalists have documented and discussed. See Table 3."}, {"title": "Study 2", "content": "In our second study, we test the causal effects of learning about different types of AI use in elections. We ran a preregistered survey experiment with members of an online panel provided by Prolific (n=1,985). We used quotas to realize a sample resembling the US electorate. As Table 9 shows, the sampling was somewhat successful. The survey was fielded between June 19 and June 21, 2024.\nWe divided respondents into three treatment and one control group (C, n = 497). Treatment 1 (T1, n = 497) contained information about campaigns' uses of AI for deception. Treatment 2 (T2, n = 494)"}, {"title": "Descriptions of the AI", "content": ""}, {"title": "Treatment 1: Deception", "content": "Candidates from all parties, including Republicans and Democrats, and candidates from various third parties use AI in their campaigns.\nThey use AI technology to produce videos depicting fictional scenarios involving their opposing candidates. Picture a scenario where a video portrays an opposing candidate making controversial statements or engaging in questionable conduct - all generated using A\u0399.\nThese resulting videos are frequently captivating and occasionally gain substantial traction, especially among demographics typically tricky to engage with for political parties. However, it's essential to note that these videos are pure fiction and do not reflect actual events or actions."}, {"title": "Treatment 2: Campaign Operations", "content": "Candidates from all parties, including Republicans and Democrats, and candidates from various third parties use AI in their campaigns.\nFor example, they use AI to automatically generate emails, speeches, and policy documents. By leveraging AI, campaigns can conserve valuable resources through the automation of repetitive tasks and help with the allocation of funds and volunteer hours. This enhanced efficiency aids campaigns in pursuing their objectives effectively and is particularly beneficial for financially constrained campaigns."}, {"title": "Treatment 3: Voter Outreach", "content": "Candidates from all parties, including Republicans and Democrats, and candidates from various third parties use AI in their campaigns.\nThey use AI technology to create customized voter outreach strategies. By meticulously analyzing consumer data, online activities, and voting histories, AI has the capacity to create and distribute personalized campaign messages that align with each voter's specific interests. For instance, a tech-savvy urban dweller might receive information about the party's innovation initiatives, while a young parent could receive insights on education reform.\nCampaigns rely on AI-enabled outreach to distinguish themselves in a sea of generic political communications and to effectively connect with voters on subjects that resonate with them."}, {"title": "Study 3", "content": "In Study 3, we test whether parties face a penalty for deceptive AI uses attributed to them and whether partisans' group-protective cognitions lead to heterogeneous effects of being informed about deceptive uses.\nFor this preregistered study (Preregistration: https://osf.io/vugp8/?view_only=5e4387422dc94458 bb355e6e2e5fba3d, we recruited three samples containing only respondents identifying as partisans for (1) Democrats (n=1,489), (2) Republicans (n=1,485), or as (3) Independent (n=1,477). Prolific prescreened partisans. No attempt to be representative was made. The survey was fielded between June 25 and June 30, 2024."}, {"title": "Detailed Responses to Specific Uses of AI in Elections (Study 1)", "content": "Table 20 shows the shares of responses agreeing with the statements This use of AI worries me a lot (Worry), This AI use is not how political campaigns should act (Norm Violation), and This use of AI"}, {"title": "Content Analysis Open Answer Fields (Study 1)", "content": "We were also interested in the risks people associate with specific campaigning uses of AI and whether these risks correspond systematically with the usage categories identified by use (i.e., campaign operations, voter outreach, and deception). After the short description of specific campaign tasks AI was used for (see Table 3), we posted the question: \u201cWhat risks for society do you see with this use of AI in political campaigns?\". Respondents were provided with an open answer field, where they could answer without having specific risks prompted by us.\nOur preregistered expectations (see preregistration https://osf.io/3nrb4/?view_only=1d82e100d608 4edd81d9c4af46f31a30) were supported by the analysis. We expected that people were significantly more likely to mention risks associated with deception to campaign tasks within our deception category than to those in the categories campaign operations and voter outreach (see Table 15 H4a,b). Correspondingly, we expected that people were significantly more likely to mention risks associated with reduced voter agency to campaign tasks within our voter outreach category than to those in the categories campaign operations and deception (see Table 15 H5a,b).\nTo classify the responses, we used two preregistered prompts with the OpenAI model \"gpt-40-mini- 2024-07-18\" (temperature=0) (see https://openai.com/index/gpt-40-mini-advancing-cost-efficient- intelligence/). The workings of both prompts were validated by manual coding. Both prompts worked well as the manual validation with 50 randomly sampled answers for each variable indicated - deception (Cohen's Kappa=0.87; Accuracy=0.94) and reduced agency (Cohen's Kappa=1; Accuracy=1).\""}, {"title": "Using prompt questions", "content": "We used the following preregistered prompts:\ni Prompt risk 1: Deception (associated with AI-enabled deception)\nAnalyze the following response to an open survey question and determine if it explicitly mentions deceptive uses of AI in politics.\nDeception includes all acts and statements that mislead, hide the truth, or promote a belief, concept, or idea that is not true. Examples include but are not limited to the use of deep fakes to generate faked text, video, or audio content. It also includes the automated generation of social media posts pretending to be from humans. Another form of deception are automated interactions with journalists, political elites, or voters in text, audio, or video pretending to come from humans. Deception does also include the purposeful generation and distribution of misinformation, disinformation, and lies.\nReply with 1 if it does, and with 2 if it does not. Reply only with a number.\nHere is the response: [survey response was added here]\ni Prompt risk 2: Reduced agency (associated with AI-enabled voter outreach)\nAnalyze the following response to an open survey question and determine if it explicitly mentions uses of AI in politics that reduced the agency of voters.\nHere, reduced agency refers to a situation where individuals' ability to make informed and autonomous choices in the political sphere is constrained or limited. Examples for reduced agency include, but are not limited to, presenting people selected true information that supports the campaign's goals. Another example is profiling people based on their behavior on- as well as offline and then adapting communicative approaches and content to better persuade or influence them to support a campaign, donate money, or turn up to vote and in general to use these profiles to undermine voters' critical reasoning. Reduced agency does not include cases where a campaign actively deceives people or lies to them.\nReply with 1 if it does, and with 2 if it does not. Reply only with a number.\nHere is the response: [survey response was added here]"}, {"title": "Equivalence Test, Effects on Party Favorability (Study 3)", "content": "We also used an equivalence test for the party favorability variables \u201cto test whether an observed effect is surprisingly small, assuming that a meaningful effect exists in the population\"32. For all tests, we used Cohen's D of 0.216 from the preregistration as the smallest effect size of interest for the upper and lower bounds of the test (\u0394L = -0.216, AU = 0.216). We used Welch's t-tests for the equivalence test. All the nonsignificant results for the favorability scores show a significant equivalence test (two one-sided tests). Thus, we can assume the effect of deceptive use of AI does not substantially affect party favorability in all three samples. In Table 23, we report the test for the bound with the smaller t statistic and, thus, the higher p-value32\""}]}