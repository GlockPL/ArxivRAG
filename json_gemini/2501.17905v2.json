{"title": "DRESS: Data-driven Regularized Structured Streamlining for Large Language Models", "authors": ["Mingkuan Feng", "Jinyang Wu", "Shuai Zhang", "Pengpeng Shao", "Ruihan Jin", "Zhengqi Wen", "Jianhua Tao", "Feihu Che"], "abstract": "Large language models (LLMs) have achieved significant progress across various domains, but their increasing scale leads to high computational and memory costs. Recent studies show that LLMs exhibit sparsity, which can be exploited for pruning. However, existing pruning methods typically follow a prune-then-finetune paradigm. Since the pruned components still contain valuable information, their direct removal often leads to irreversible performance degradation, necessitating expensive fine-tuning to recover performance. To address this, we propose a new paradigm: first apply regularization, then prune, and finally fine-tune. Based on this paradigm, we introduce DRESS, a simple and effective Data-driven Regularized Structured Streamlining method for LLMs. By leveraging a small amount of data to regularize the components before pruning, DRESS transfers the important information to the remaining parts of the model in advance. Compared to direct pruning, this can reduce the information loss caused by parameter removal, thereby enhancing its language modeling capabilities. Experimental results demonstrate DRESS achieves comparable performance to conventional methods even without fine-tuning, drastically alleviating computational costs. Moreover, DRESS significantly outperforms existing pruning methods even under extreme pruning ratios, significantly reducing latency and increasing throughput.", "sections": [{"title": "1. Introduction", "content": "Large language models (LLMs) have achieved significant advancements across a wide range of tasks and domains, demonstrating their robust capabilities (Zhang et al., 2022; Achiam et al., 2023; Touvron et al., 2023; Wu et al., 2024). However, as the model size increases, the growing number of parameters leads to significant computational and memory requirements, which significantly hinder the practical deployment of LLMs. Consequently, it is urgent to develop methods that can reduce model size while maintaining performance.\nTo address these challenges, several methods have been proposed, including pruning (Frantar & Alistarh, 2023; Sun et al., 2023; Ma et al., 2023; An et al., 2024), quantization (Frantar et al., 2022; Xiao et al., 2023), knowledge distillation (Shridhar et al., 2022; Hsieh et al., 2023), and low-rank decomposition (Saha et al., 2023). In this work, we mainly focus on pruning-an efficient and highly generalizable approach that can be seamlessly integrated with other model compression strategies. Pruning techniques are generally classified into two primary categories: unstructured pruning (Frantar & Alistarh, 2023; Sun et al., 2023) and structured pruning (Ma et al., 2023; An et al., 2024). Compared to unstructured pruning, structured pruning offers the flexibility to do recovery fine-tuning (RFT) for specific downstream tasks without relying on specialized hardware (Zhu et al., 2024). Moreover, the model obtained through structured pruning typically achieves much faster inference speed due to the regular data patterns.\nDespite these advancements, existing structured pruning methods still have some limitations. They all follow the paradigm of first selecting channels or layers to prune based on a designed metric, and then performing RFT (Chavan et al., 2024). However, they neglect the fact that important information can also exist in the pruned parts (Dettmers et al., 2022; Xiao et al., 2023; Yin et al., 2023), and directly removing them leads to an irreversible decline in model performance. Thus, achieving satisfactory performance with pruned models often requires extensive data for RFT (Ma et al., 2023), increasing computational costs. Additionally, high pruning ratios frequently cause performance collapse, limiting their effectiveness in reducing latency and improving throughput.\nTo tackle these limitations, in this paper, we propose a new"}, {"title": "2. Related Works", "content": ""}, {"title": "2.1. Pruning Methods", "content": "The elimination of redundant weights, referred to as pruning, has been an effective approach for reducing the complexity of deep neural networks over the past several decades (LeCun et al., 1989; Hassibi et al., 1993; Han et al., 2015; Wen et al., 2016; Louizos et al., 2017). Pruning methods can be broadly categorized into two types: unstructured pruning (Kurtic et al., 2022; Zhang et al., 2024; Xu et al., 2024) and structured pruning (Xia et al., 2023b; Gao et al., 2024b; Yang et al., 2024).\nUnstructured pruning methods remove individual weights based on their importance. For instance, Magnitude (Han et al., 2015; Zhou et al., 2021) assumes larger weights are more important, Wanda (Sun et al., 2023) uses the product of layer weights and activations as the metric, and SparseGPT (Frantar & Alistarh, 2023) relies on the weight squared divided by the inverse Hessian matrix. The main advantage is flexibility, enabling the removal of unimportant weights while retaining critical ones, allowing high sparsity in LLMs. However, its limitations include the requirement for specialized hardware to accelerate inference (Xia et al., 2023a), the necessity for high sparsity to achieve substantial acceleration (Wang, 2020), and the inability to perform RFT on downstream tasks.\nStructured pruning methods, including channel-wise pruning (An et al., 2024) and layer-wise pruning (Men et al., 2024), can achieve significant inference speedup with a low sparsity. Channel-wise pruning methods typically design a metric to evaluate the importance of channels in the parameter matrix of LLMs and then prune the less important channels. For instance, SliceGPT (Ashkboos et al., 2024) adopts Principal Component Analysis (PCA), using the eigenvalues of the covariance matrix of activation values as a metric to preserve channels corresponding to larger eigenvalues. LLM Surgeon (van der Ouderaa et al., 2023) periodically updates model weights and structures, pruning more aggressively in the initial layers and less so in the intermediate layers. Layer-wise pruning methods, such as SLEB (Song et al., 2024), iteratively prune entire transformer layers by assessing their importance. However, these methods have a key limitation: even less important channels or layers may contain valuable information, and pruning them directly often leads to significant performance degradation. To address this, we investigate the possibility of 'shifting' important information from the pruned parts to the retained parts, which could substantially improve model performance."}, {"title": "2.2. Regularization", "content": "Regularization is widely employed in machine learning (Hoerl & Kennard, 1970; Poggio et al., 1987; Balestriero et al., 2022), such as feature selection (Tibshirani, 1996) and preventing model overfitting (Santos & Papa, 2022). The l\u2081-norm typically induces sparsity by driving certain coefficients to zero, while l2-norm encourages smoother solutions (Boyd & Vandenberghe, 2004). Both of them can significantly alter the distribution pattern of the data (Han et al., 2015; Tao et al., 2023). Motivated by this observation, we propose a data-driven regularization approach. By applying a small amount of data, the regularization transfers important information from the pruned parameter space to the retained parts of the model, thereby enhancing the performance of the pruned model."}, {"title": "3. Methodology", "content": "In this section, we provide a detailed introduction to DRESS. The overall DRESS algorithm is summarized in Algorithm 1, our method consists of four main steps:\n\u2022 Data Selection: Select a small amount of data for pre-pruning regularization and post-pruning RFT.\n\u2022 Regularization: Applying regularization to the selected channels of weight matrices during forward and backward propagations.\n\u2022 Pruing: Prune the regularized portions.\n\u2022 RFT: Perform RFT on the pruned model using the subset of data selected in the first step."}, {"title": "3.1. Data Selection", "content": "The required data is selected from widely used calibration datasets, including Alpaca (Taori et al., 2023), WikiText-2 (Merity et al., 2016), PTB (Marcus et al., 1993), and C4 (Raffel et al., 2020). For instance, approximately 1,000 samples are randomly selected from the WikiText-2 training dataset for pre-pruning regularization and post-pruning RFT. In Section 4.5, we analyze the impact of using different datasets."}, {"title": "3.2. Regularization", "content": "To provide a clearer understanding of this section, we first introduce the relevant notations. Let p denote the pruning ratio, which indicates that p% of the model's parameters need to be pruned. The model dimension of the LLMs is represented by d, and the number of layers is denoted by l. Additionally, let $W_{emb} \\in \\mathbb{R}^{b \\times n \\times d}$ represents the data feature map, and $W_{pos} \\in \\mathbb{R}^{b \\times n \\times d}$ denotes the positional feature map, where b is the batch size and n is the number of tokens. The model weights are represented by $W \\in \\mathbb{R}^{d_1 \\times d_2}$. Specifically, $W_o \\in \\mathbb{R}^{d \\times d_1}, W_{ix} \\in \\mathbb{R}^{d \\times d_1}, W_i \\in \\mathbb{R}^{d \\times d_1}$, and $W_i\\in \\mathbb{R}^{d_1 \\times d}$ represent the parameter matrices in the ith layer Attention block. Similarly, $W_{up} \\in \\mathbb{R}^{d \\times d_2}$ and $W_{down} \\in \\mathbb{R}^{d_2 \\times d}$ represent the parameter matrices in the ith layer FFN block. Additionally, $w_N \\in \\mathbb{R}^{d}$ denotes the LayerNorm vector in the ith layer, and $W_{lm} \\in \\mathbb{R}^{d \\times b \\times n}$ represents the language modeling matrix. Finally, $I \\in \\mathbb{R}^{d \\times d}$ represents the identity matrix, let $R \\in \\mathbb{R}^{d \\times d}$ represent a pseudo-index selection matrix, which is a diagonal matrix"}, {"title": "Proposition 3.1.", "content": "(Dependency between the Attention Block and FFN Block. Proof in Appendix A.1). If regularization is applied to certain columns of $W_{down}$ in the (i - 1)th layer FFN block, then regularization must also be applied to the corresponding rows of $W^o, W^k, and W^v$ in the ith layer Attention block. The same dependency applies to $W^v$ and $W^{i-1}_{up}$."}, {"title": "3.3. Pruning", "content": "After completing the regularization, we prune the rows and columns directly using the pseudo-index selection matrix R. For clarity, we define $S = I \u2013 R$ and the pruning for the ith layer attention block is then performed as:\n$W_o' = SW_o, W_K' = SW_k$\n$W_ = SW_v, W = W^\nWhile for the FFN block, it follows:\nW = SW^\n$W_{up} = SW_{up}, W_{down}' = W_{down} S$\nThe remaining parts are pruned as below:\n$W_{emb} = W_{emb}S, W_{pos} = W_{pos} S$\nw_N' = SW_N, W_{Im} = SW_{Im}$\nThe entire pruning process is straightforward and simple."}, {"title": "3.4. Optional RFT", "content": "After pruning, we perform RFT on the model using a small subset of the data selected in Section 3.1, leveraging LORA (Hu et al., 2021). This step is optional, as experimental results in section 4.7 show that DRESS without RFT still maintains competitive performance in terms of both perplexity and zero-shot accuracy."}, {"title": "4. Experiments", "content": "This section introduces experimental setup (4.1) and analyzes the effectiveness of DRESS from the following aspects: performance comparison (4.2), acceleration (4.3), robustness under different pruning ratios (4.4), dependency on different datasets (4.5), low overhead (4.6), and ablation study (4.7)."}, {"title": "4.1. Experimental Setup", "content": "Implementation: DRESS and baseline methods are implemented on the PyTorch framework (Paszke et al., 2019) utilizing the Hugging Face Transformers library (Wolf, 2019). All experiments are conducted on 80GB NVIDIA A100 GPUs. For fairness, we use llm-eval-harness (Gao et al., 2024a) to evaluate the pruned models. More details are shown in Appendix B.\nDatasets: To evaluate the performance of DRESS, we conducted experiments on generation and zero-shot tasks. For generation task, following prior work(Ashkboos et al., 2024), we evaluate the model's perplexity on WikiText-2. For zero-shot task evaluation, the benchmarks consists of PIQA (Bisk et al., 2020), WinoGrande (Sakaguchi et al., 2021), HellaSwag (Zellers et al., 2019), ARC-e and ARC-c (Clark et al., 2018). To demonstrate that DRESS is not specifically dependent on the regularization data, we use a randomly selected subset of data from Alpaca (Taori et al., 2023), WikiText-2 (Merity et al., 2016), PTB (Marcus et al., 1993), and C4 (Raffel et al., 2020) for calibration before pruning and for RFT after pruning.\nModels: The models pruned using DReSS include the LLaMA models (LLaMA2-7B, LLaMA2-13B) (Touvron et al., 2023), OPT models (OPT-2.7B, OPT-6.7B, OPT-13B) (Zhang et al., 2022), and Phi-2 (Javaheripi et al., 2023).\nBaselines: We evaluate DRESS against competitive structured pruning methods: LLM Surgeon (van der Ouderaa et al., 2023), SliceGPT (Ashkboos et al., 2024), and SLEB (Song et al., 2024). Among them, LLM Surgeon and SliceGPT are channel-wise, SLEB is layer-wise. To ensure a fair comparison, in the subsequent experiments, we ensure that the data used for regularization in DRESS is consistent with the calibration data used by LLM Surgeon, SliceGPT, and SLEB before pruning, and that the data"}, {"title": "4.3. Acceleration Effectiveness", "content": "Language processing in LLMs comprises two primary stages: prompt processing, which is compute-bound, and token generation, which is memory-bound. We separately analyze the speedup achieved in each stage. Table 2 presents the throughput and latency results for OPT-13B and LLaMA2-13B, evaluated using a single NVIDIA A100 GPU. Following the methodology of previous work (Song et al., 2024), the token generation test scenario involves generating sentences of 128 tokens with a batch size of 64, whereas for prompt processing, latency is measured by processing an input sequence of 2048 tokens.\nAt a pruning ratio of 50% on OPT-13B, DRESS delivers a 35% improvement in throughput and a 30% reduction in latency compared to the dense model. These results highlight the superior efficiency of DRESS in accelerating model inference."}, {"title": "4.4. Robustness to Varying Pruning Ratios", "content": "Keeping all other settings consistent with Section 4.2, we extend the pruning ratio from 20% to 60%. The perplexity of LLaMA2-7B on WikiText-2 under different methods are shown in Figure 4. DRESS significantly outperforms other methods across various pruning ratios. At a pruning ratio of 20%, DRESS performs similarly to the dense model. When the pruning ratio is up to 60%, SLEB collapses, while DRESS maintains relatively low perplexity compared to other pruning methods. This demonstrates that DRESS maintains robust performance even under extreme pruning ratios, enabling structured pruning to achieve high sparsity levels and unlocking significant potential for model acceleration. A more detailed exploration of pruning ratios using DRESS is provided in the Appendix E."}, {"title": "4.5. Dependency on Datasets", "content": "Since DRESS relies on data-driven regularization, we investigate its potential dataset dependence. We evaluated perplexity for four methods on WikiText-2, using calibration and RFT data selected from Alpaca, WikiText-2, PTB, and C4. To ensure fairness, we randomly selected 1,000 samples from each dataset, with other settings consistent with Section 4.2. As shown in Figure 5, DRESS consistently outperforms the other methods across datasets, demonstrating its robustness and scalability. More details are shown in Appendix F."}, {"title": "4.6. Minimal Overhead", "content": "We evaluated the perplexity of each method on LLaMA2-7B using varying amounts of data, keeping all other conditions"}, {"title": "4.7. Ablation Study", "content": "Effectiveness of Regularization: As shown in Table 4, both random pruning and full parameter regularization significantly increase perplexity on WikiText-2 and reduce average accuracy at a 25% pruning ratio. This highlights the importance of applying regularization exclusively to the pruned components. Additionally, RFT after pruning has minimal impact on model performance, indicating that the RFT process is optional."}, {"title": "5. Conclusion", "content": "In this paper, we introduce a novel pruning paradigm: first applying regularization to the pruned components, followed by pruning and then RFT. Unlike traditional paradigms that first prune based on importance and then apply RFT, DRESS transfers critical information from the pruned parameter space to the remaining components during regularization, effectively mitigating the irreversible performance degradation caused by information loss. DRESS demonstrates superior performance in both generation and zero-shot tasks, significantly outperforming existing pruning methods. For instance, DRESS surpasses the powerful LLM Surgeon by 20% in perplexity on LLaMA2-7B. On OPT-13B, with a pruning ratio of 25%, the average accuracy drops by only 1%, while achieving a 1.41\u00d7 speedup compared to the dense model. Moreover, DRESS requires only 25% of the data to achieve comparable performance to previous methods, substantially reducing both data and computational costs while minimizing the reliance on RFT."}, {"title": "Impact Statement", "content": "Pruning plays a crucial role in model compression. DRESS is an efficient and low-overhead pruning method, which enables smaller models to achieve competitive performance, especially in resource-constrained scenarios and it also provides some insights for the development of LLMs."}, {"title": "Proposition 3.2.", "content": "(If the loss function includes the l\u2081-norm, it can also be solved using backpropagation. Proof in Appendix A.2). The following unconstrained optimization problem is equivalent to the constrained optimization problem, where $||x||_1$ denotes the l\u2081-norm.\n$\\underset{x,y}{min} ||x||_1  \\Leftrightarrow  \\underset{x,y}{min} 1^T y$\ns.t. $- y \\leq x \\leq y,$\n$y \\geq 0.$\nWe formalize the minimization of the overall loss as an optimization problem. When l2-norm is used, the problem can be efficiently solved using backpropagation (BP) algorithm (Rumelhart et al., 1986). In contrast, when the l1-norm is employed, as the regularization losses introduced by various components in LLMs are similar, we consider only $C_{HEN}$ as an example. According to Proposition 3.2, the optimization problem in Equation 6 can be equivalently transformed into a constrained formulation in Equation 7, enabling its solution by BP algorithm.\n$\\underset{W}{min} L(W, X) + \\lambda (||RW_{up}||_1 + ||W_{down} R||_1)$ \n$\\underset{W,Y_1, Y_2}{min} L(W, X) + \\lambda 1^T Y_1 + \\lambda 1^T Y_2$\ns.t. $- Y_1 \\leq RW_{up} \\leq Y_1,$\n$- Y_2 \\leq W_{down} R \\leq Y_2,$\n$Y_1 \\geq 0, Y_2 \\geq 0.$\nDuring the forward and backward propagation processes, due to the application of regularization on certain specific channels in LLMs, the values within these rows or columns are significantly reduced. Intuitively, this reduction could potentially transfer important information and decrease the impact of these parameters on the model's performance."}, {"title": "A. Proofs of the Propositions", "content": ""}, {"title": "A.1. Proof of Proposition 3.1", "content": "Let us first assume that $A \\in \\mathbb{R}^{m \\times n}$ and $B \\in \\mathbb{R}^{n \\times k}$. We can express $A$ as $[a_1 \\quad a_2 \\quad ... \\quad a_n]$, where $a_i \\in \\mathbb{R}^{m \\times 1}$, and B\nas $\\begin{bmatrix}b_1^T \\\\ ... \\\\ b_n^T\\end{bmatrix}$, where $b_i \\in \\mathbb{R}^{k \\times 1}$. Then, we obtain $C \\in \\mathbb{R}^{m \\times k}$ as shown in Equation 11. It is important to note that the final summation in Equation 11 refers to the summation of n rank-1 matrices, not a scalar summation.\n$C = AB = [a_1 \\quad a_2 \\quad ... \\quad a_n]\\begin{bmatrix}b_1^T \\\\ ... \\\\ b_n^T\\end{bmatrix} = \\sum_{i=1}^{n} a_i b_i^T$\nAccording to Equation 11, when some columns of A are zero, the rows of B that correspond to the outer product with the zero columns of A can take any values, as the product of zero and any value is zero. These rows can therefore be set to zero.\nApplying the above conclusion to the parameters of a transformer layer, if certain columns of $W_{down}^{i-1}$ in the FFN layer of the (i - 1)th transformer layer are pruned, then, since $W_o, W_i^k, and W_i^v$ in the ith transformer layer are immediately multiplied by the result of $W_{down}^{i-1}$, the corresponding rows in $W_o, W_i^k, and W_i^v$ can be set to zero. Additionally, if certain columns of $W_o$ in the ith transformer layer are pruned, the $W_{up}$ in the FFN block will immediately multiply with it, so the corresponding rows in $W_{up}^{i}$ should also be pruned. Thus, we have completed the proof of Proposition 3.1."}, {"title": "A.2. Proof of Proposition 3.2", "content": "Step 1: Expressing l\u2081-norm Using Elements.\nThe objective function in the unconstrained problem is the l\u2081-norm of the vector x, which is defined as:\n$||x||_1 = \\sum_{i=1}^{n} |x_i|$\nThis function aims to minimize the sum of the absolute values of the components of x.\nStep 2: Reformulating the Constrained Problem\nThe constrained optimization problem introduces an auxiliary variable y, where for each element i:\n$x_i -y_i \\leq x_i \\leq y_i$\nThis implies that yi \u2265 |xi|, meaning each element of y serves as an upper bound for the absolute value of the corresponding element in x. Consequently, minimizing |x|1 is equivalent to minimizing the sum of the elements in y. Thus, the objective function is defined as:\n$1^T y$\nThus, minimizing $1^T y$ is equivalent to minimizing the sum of the absolute values of x, which is the l\u2081-norm of x.\nThis transformation allows the optimization problem to be solved without directly involving the absolute value function, resulting in an equivalent constrained optimization problem that can be addressed via backpropagation. Thus, the proof of the Proposition 3.2 is complete."}, {"title": "B. Detailed Implementation", "content": "In this part, we first introduce several hyperparameter settings, with the detailed results shown in Table 7. In our experiments, we employ FP16 precision for all evaluated models, including Phi-2, OPT-2.7B, OPT-6.7B, OPT-13B, LLaMA2-7B, and LLaMA2-13B. For all RFT configurations, we set the LoRA rank r to 32, the scaling factor \u03b1 to 10, and the sequence length to 2048. All other hyperparameters follow the default settings provided in the Hugging Face PEFT package (Mangrulkar et al., 2022). Due to limited computational resources, we set the batch size to 64. In future work, we will further explore the impact of a broader range of batch sizes on the performance of the pruning method. These choices ensure a balance between model adaptability and computational efficiency, leveraging parameter-efficient fine-tuning while maintaining stable optimization dynamics.\nTo ensure a fair comparison between DRESS and other methods, we maintain consistency in the data used across all approaches. Specifically, the data used by DRESS for regularization, by LLM Surgeon for periodic updates of model weights and structures, by SliceGPT for selecting channel importance, and by SLEB for identifying crucial transformer layers are identical. Furthermore, we ensure that the data employed during the RFT process is consistent across all methods, thereby enabling a controlled and equitable evaluation framework. Following previous works (Ashkboos et al., 2024; Song et al., 2024), for the comparison unstructured pruning methods like Mangniude, Wanda, and SparseGPT, we ensure that the data used to compute the importance of individual weights is the same as the data used by DRESS for regularization."}, {"title": "C. Results of Comparison with Unstructured Pruning Methods", "content": "We compare DRESS at a 25% pruning ratio with unstructured pruning methods employing 2:4 sparsity (where two out of every four consecutive parameters are set to zero) and present the experimental results in Table 8. For unstructured pruning methods, only calibration data is required. To ensure consistency, all methods utilize a total of 1,000 samples, randomly selected from WikiText-2. The models used in these experiments consists of the OPT and LLaMA model families.\nCompare to structured pruning methods, the primary advantage of unstructured pruning lies in its flexibility, enabling the removal of specific weights while preserving critical information in the remaining parameters of LLMs. Based on the results in Table 8, it can be seen that DRESS with a pruning ratio of 25% significantly outperforms existing unstructured pruning methods with a sparsity of 2:4 on generative and zero-shot tasks, demonstrating the powerful performance of DRESS. The reason DRESS outperforms unstructured pruning in comparisons lies in the regularization process before pruning. This process transfers important information hidden in the parameter space to the remaining parts of the model, preventing irreversible performance degradation caused by direct pruning and the resulting loss of information."}, {"title": "D. Changes in the Parts Without Regularization", "content": "As illustrated in Figure 6, the magnitude of the unregularized parameters exhibits an increase after the application of regularization, suggesting a redistribution of model capacity. This phenomenon indicates that during regularization, critical information, initially encoded in the regularized portion of the model, is partially transferred to the unregularized portion. In contrast, Figure 3 shows a reduction in the magnitude of the regularized parameters after regularization, implying that the imposed constraints effectively suppress the corresponding parameter values, enforcing sparsity or compression in that region.\nCollectively, these observations suggest that the regularization process facilitates an implicit redistribution of information across different parameter subsets. Specifically, the regularization term promotes a shift of important model characteristics from the constrained (regularized) portion to the unconstrained (unregularized) portion, thereby preserving essential model knowledge despite the imposed sparsity constraints. Based on this insight, we posit that pruning the regularized portion post-regularization could mitigate information loss, as the core knowledge has already been migrated to the unregularized segment. This pruning strategy effectively reduces parameter redundancy while retaining the model's language modeling capacity, thereby achieving a more compact and efficient representation without compromising performance."}, {"title": "E. Performance of DReSS under Different Pruning Ratios and Datasets", "content": ""}, {"title": "E.1. The Perplexity of DRESS under Different Pruning Ratios and Datasets", "content": "In Section 4.2, we utilize 1,000 samples randomly selected from the WikiText-2 dataset to guide the regularization process. Subsequently, we evaluate multiple large language models (LLMs) by measuring changes in perplexity across various generative task datasets, including WikiText-2, Alpaca, PTB, and C4, under pruning rates of 10%, 20%, 30%, 40%, 50%, and 60%. The detailed results, presented in Table 9, indicate that DRESS exhibits greater robustness as model scale increases, suggesting that the proposed method effectively mitigates performance degradation in larger architectures. This highlights the scalability of DRESS and its potential to maintain model efficiency under varying levels of sparsity."}, {"title": "E.2. The Accuracy of DRESS under Different Pruning Ratios on Zero-shot Tasks", "content": "To systematically evaluate the performance of DRESS on zero-shot tasks under varying pruning rates, we adopt the experimental setup outlined in Section 4.2, where 1,000 samples are randomly selected from the WikiText-2 dataset to guide the regularization process. We assess the accuracy of different model configurations at pruning rates of 10%, 20%, 30%, 40%, 50%, and 60% across a diverse set of benchmark datasets, including PIQA, WinoGrande, HellaSwag, ARC-\u0435, and ARC-c. The results, summarized in Table 10, provide insights into the impact of sparsity on zero-shot generalization. Notably, the analysis reveals that DRESS maintains competitive performance even at higher pruning rates, demonstrating its effectiveness in preserving reasoning and commonsense understanding across different tasks."}, {"title": "F. More Details of the Dependency on Calibration Dataset", "content": "When using Alpaca, WikiText-2, C4, and PTB as calibration and RFT data, the perplexity of various methods on Alpaca, C4, and PTB is shown as follows:"}]}