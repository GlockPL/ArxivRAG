{"title": "Autonomous Microscopy Experiments through Large Language Model Agents", "authors": ["Indrajeet Mandal", "Jitendra Soni", "Mohd Zaki", "Morten M. Smedskjaer", "Katrin Wondraczek", "Lothar Wondraczek", "Nitya Nand Gosvami", "N. M. Anoop Krishnan"], "abstract": "The emergence of large language models (LLMs) has accelerated the development of self-driving laboratories (SDLs) for materials research. Despite their transformative potential, current SDL implementations rely on rigid, predefined protocols that limit their adaptability to dynamic experimental scenarios across different labs. A significant challenge persists in measuring how effectively Al agents can replicate the adaptive decision-making and experimental intuition of expert scientists. Here, we introduce AILA (Artificially Intelligent Lab Assistant), a framework that automates atomic force microscopy (AFM) through LLM-driven agents. Using AFM as an experimental testbed, we develop AFMBench-a comprehensive evaluation suite that challenges AI agents based on language models like GPT-4o and GPT-3.5 to perform tasks spanning the scientific workflow: from experimental design to results analysis. Our systematic assessment shows that state-of-the-art language models struggle even with basic tasks such as documentation retrieval, leading to a significant decline in performance in multi-agent coordination scenarios. Further, we observe that LLMs exhibit a tendency to not adhere to instructions or even divagate to additional tasks beyond the original request, raising serious concerns regarding safety alignment aspects of Al agents for SDLs. Finally, we demonstrate the application of AILA on increasingly complex experiments open-ended experiments: automated AFM calibration, high-resolution feature detection, and mechanical property measurement. Our findings emphasize the necessity for stringent", "sections": [{"title": "1. Introduction", "content": "Scientific experimentation demands exceptional domain expertise, from exploration or hypothesis-driven experimental design to precision execution and rigorous data analysis. This complexity creates bottlenecks in scientific discovery, particularly as experimental techniques grow increasingly sophisticated. The advent of large language models (LLMs) has propelled the development of self-driving laboratories (SDLs) that integrate diverse information sources for automated planning\u00b9 and experimentation. AI-agents2,3 and SDLs have already achieved several feats in materials or molecular discovery4\u20136, chemistry research, and inorganic materials synthesis. The promise of SDLs toward achieving sustainable development has resulted in enormous efforts to harness their potential in high-throughput experimentation and discovery. Efforts to streamline SDLs have resulted in orchestration architectures such as ChemOS10. Additionally, it has been demonstrated that the capability of SDLs can be enhanced by a human-in-the-loop framework that handles disambiguation, thereby enabling better planning and execution\u00b9\u00b9. While early demonstrations of LLM-based lab assistants showed promise in chemistry and materials science1-3, their operational reliability remains largely uncharacterized beyond specific applications or repetitive use cases with predetermined protocols15\u201319.\nCurrent research predominantly addresses well-documented or predefined protocols and single-objective tasks, failing to capture the intricate interplay between experimental planning, multi-tool coordination, and result interpretation or online intervention10. While recent investigations incorporating planning elements have demonstrated success in achieving specific experimental objectives, they have not systematically evaluated SDL reliability across the broader spectrum of laboratory automation tasks15,16. Although several studies have benchmarked LLMS 17,18,20\u201323 and vision language models (VLMs)13\u201316 through question-answer protocols to assess their potential as materials research co-pilots, a crucial knowledge gap persists: understanding how these AI systems handle novel experimental scenarios and their fundamental limitations.\nTo address this challenge, we here introduce AILA (Artificially Intelligent Lab Assistant), an LLM-powered framework augmented with specialized tools. We selected scanning probe microscopy20, specifically atomic force microscopy (AFM), as our experimental testbed, given its inherent complexity and broad applicability in materials research. There have been several efforts to automate microscopy techniques using AI and human-in-the-loop approaches due to their extensive applications in materials characterization28\u201330,30,31,31\u201335. These efforts focus exclusively on advancing specific operational aspects, such as analysing moving objects or optimizing illumination conditions, with an emphasis on improving individual steps within the broader experimental protocol30,32,35. AFM operation demands expertise across multiple domains-from probe calibration to parameter optimization and data interpretation\u2014making"}, {"title": "2. Results", "content": "Using AFM as the model system, we probe AILA's capabilities through AFMBench on five critical aspects of scientific automation: experimental workflow design, multi-tool coordination, decision-making, execution of open-ended experiments, and data analysis. Our systematic evaluation reveals key failure modes and areas requiring enhancement. We demonstrate AILA's practical utility through three increasingly complex case studies: automated microscope calibration, high-resolution graphene step-edge imaging, and load-dependent roughness analysis on highly oriented pyrolytic graphite (HOPG)."}, {"title": "2.1 AILA framework", "content": "AILA's architecture prioritizes modularity, enabling seamless integration with diverse experimental and analytical platforms. At its core lies an LLM-powered planner-the framework's cognitive centre\u2014which orchestrates user interactions and coordinates specialized agents (Fig. 1a). For AFM operations, AILA deploys two agents: the AFM Handler Agent (AFM-HA) for experimental control and the Data Handler Agent (DHA) for analysis. The AFM-HA interfaces with a document retrieval system comprising AFM software documentation and a code execution engine that translates Python commands into experimental actions. A Python-based API establishes the hardware-software interface, enabling direct control of the AFM system through vendor-specific protocols (Fig. 1b). The DHA manages image optimization and analysis through dedicated tools: an Image Optimizer that fine-tunes PID parameters for high-fidelity imaging and an Image Analyzer that extracts targeted features from experimental data. For queries beyond agent capabilities, the planner generates alternative approaches or recommended actions. The technical specifications and implementation details of each module are explained in the Methods section.\nTo demonstrate AILA's operational workflow, we present a multi-step experiment: acquiring an AFM image of HOPG and extracting its friction and roughness parameters (Fig. 1c). This open-ended task exemplifies real-world complexity, offering multiple solution pathways. Upon receiving the query, AILA dissects it into sequential objectives: image acquisition via AFM-HA followed by DHA-led analysis. AFM-HA retrieves relevant documentation, generates executable code, and captures the image. Following successful acquisition, AILA transitions control to DHA, which directs the Image Analyzer to compute the specified parameters. This orchestrated sequence exemplifies AILA's core strengths: the ability to parse complex natural language queries, develop strategic workflows, and coordinate multiple agents toward achieving experimental objectives."}, {"title": "2.2 AFMBench: tasks for evaluating the AILA framework", "content": "AFMBench comprises 100 expertly curated experimental tasks (see S3.1 in Supplementary Information for a few examples of tasks; all the tasks are available in the GitHub repo) manually designed to rigorously evaluate autonomous AFM operations across multiple dimensions of complexity. Unlike conventional LLM benchmarks or simulation-based evaluations, each AFMBench task demands physical execution on AFM hardware, introducing real-world temporal constraints and experimental variability. Analysis of the task architecture reveals distinct patterns in resource utilization and operational complexity. In Figure 2A, tool coordination requirements highlight a systematic preference for sophisticated workflows, with"}, {"title": "2.3 Performance of AI Agents", "content": "Systematic evaluation of AILA using two advanced closed source language models\u2014GPT-40 and GPT-3.5-turbo-0125\u2014unveils distinctive execution patterns and operational efficacies. GPT-40 exhibits exceptional proficiency in documentation-centric operations, achieving a 92% success rate, complemented by robust execution in analysis (71%) and computational tasks (70%). The model's strength lies in its ability to navigate interconnected workflows: 80% success in merged documentation-analysis procedures and 60% in documentation-computation sequences. These metrics highlight GPT-4o's capacity to replicate the integrative reasoning characteristic of expert microscopists."}, {"title": "2.4 Error Analysis Reveals Model-Specific Limitations", "content": "Detailed examination of failure cases revealed distinctive error patterns between the two language models, offering insights into their operational limitations. GPT-4o exhibits a total error rate of 20%, with errors distributed across three primary categories: code generation (60%), agent/tool selection (25%), and instruction adherence (15%). The predominance of code generation errors suggests challenges in translating conceptual understanding into executable commands despite the model's strong performance in task comprehension.\nGPT-3.5-turbo-0125 demonstrates a markedly higher total error rate of 45%, with errors concentrated in two categories: code generation (66.7%) and agent/tool selection (33.3%). Notably, the model shows no fundamental query interpretation errors, indicating robust natural language processing capabilities. However, the elevated frequency of code generation errors, coupled with significant agent/tool selection failures, points to underlying deficiencies in translating comprehension into actionable experimental protocols.\nA critical finding emerged regarding GPT4o's instruction adherence. In one of the three recorded errors, GPT-4o exceeded its designated operational limits, performing actions that were not authorized by the provided guidelines. For instance, it carried out potentially risky tip movements while it was only instructed to scan the surface (see S2.3 in the Supplementary Information). In another case, GPT-40 was instructed to capture an image and calculate surface friction. While the image was captured correctly, the system failed to analyse it and switches the AFM to the lateral force mode instead of following clear instructions for a specific task (see S2.3 in the Supplementary Information). Instead of staying within the scope of the task, it performed additional actions. Although sometimes the final result may have been correct, the failure to follow instructions highlights concerns about Al-agent behaviour and raises safety risks in automated lab environments. Similar to the observation of \u201challucination\" in LLMs36, these results present a unique challenge\u2014SDLs tend to take arbitrary actions potentially based on memory rather than following the instructions, referred to hereafter as \u201csleepwalking\u201d. These issues are especially critical in sensitive experimental settings, where strict protocol adherence is essential to ensure both equipment safety and the validity of results.\nThis error distribution illuminates critical areas for framework enhancement. While GPT-40's balanced error profile suggests the need for targeted improvements across multiple domains,\""}, {"title": "2.5 Safety Alignment in SDLS", "content": "To understand the safety challenges37 of AI agents, we evaluate the effectiveness of implementing a safety framework in AILA. First, we establish restricted access protocols for critical AFM functions, coupled with ethical system prompts (see S2.1 in Supplementary Information) that constrain code generation to predefined documentation38. Second, we develop strict operational boundaries that permit dynamic code generation solely for image analysis while preventing external software installation or system modifications. Evaluation of the improved protocol demonstrates the effectiveness of these safeguards-AILA appropriately failed when prompted to install external Python libraries. (see S3.3 in Supplementary Information for complete validation logs). These findings underscore the critical importance of robust safety protocols in SDLs, emphasizing the necessity of comprehensive benchmarking and operational guardrails."}, {"title": "2.6 Pushing the Limits of Autonomous Experimentation", "content": "Finally, to demonstrate AILA's capabilities in real-world scenarios, we demonstrate three increasingly complex experimental tasks that typically require expert intervention: automated AFM calibration, high-resolution feature detection, and mechanical property measurement."}, {"title": "2.6.1 AFM Parameter Optimization", "content": "AFM imaging requires precise calibration of Proportional-Integral-Derivative (PID) gain values, which traditionally demand expert intervention due to the continuous nature of these parameters. This dependency on skilled operators presents a significant barrier to broader AFM adoption. We demonstrate AILA's capability to autonomously optimize these parameters by minimizing the forward-backward scan differential on standard calibration grids. To this end, after loading the calibration sample, AILA was prompted to optimize the imaging parameters (see S4 in Supplementary Information for the complete prompt and output log). A total of 45 images are generated, with 3 images produced in each of the 15 generations. Figure 5A presents experimental AFM data acquired by AILA for the 1st and 15th generation of variable PID configurations, with corresponding line scan analyses that quantify trace-retrace symmetry. Initial scans with suboptimal parameters (P: 93-208, I: 1747-6623, D: 10-39) exhibit poor SSIM scores (0.392-0.768), manifesting as visible distortions in topographic data. Note that a higher SSIM value, closer to 1, indicates a perfect match, while a value of 0 represents no similarity. Through iterative optimization, AILA achieves superior scan quality (SSIM > 0.81) with optimized parameters (P: 246-249, I: 8676-8957, D: 17-30; see Figure 5A).\nThe genetic algorithm's convergence efficiency is demonstrated in Figure 5C, where optimal PID configurations are achieved within 15 generations. Both maximum and mean SSIM values show rapid improvement, stabilizing above 0.8, indicating robust parameter optimization. Figure 5B validates the optimized parameters (P:249, I:8957, D:26) across a larger scan area, maintaining high-quality imaging across multiple grid features."}, {"title": "2.6.2 High-Resolution Step-Edge Detection", "content": "Surface characterization through AFM is challenged by noise sources such as thermal drift, mechanical vibrations, and electronic interference39\u201341, which can obscure subtle topographic features like graphene step edges. In this study, we leverage the advanced analytical capabilities of AILA to address these challenges using highly ordered pyrolytic graphite (HOPG) as a model system. AILA autonomously determines the necessity for baseline correction based on feature size, recognizing that baseline artifacts predominantly affect smaller features. For instance, in the raw image (Figure 6A), the graphene step edge remains indiscernible due to"}, {"title": "2.6.3 Load-dependent roughness measurement", "content": "We conduct a comprehensive load-dependent roughness analysis of HOPG. The experiment requires iterative adjustments of AFM parameters, including setting a range of setpoints, capturing images, and analyzing the corresponding friction data. Manually performing this"}, {"title": "3. Discussion", "content": "AILA's evaluation of AFMBench reveals quantifiable distinctions between LLM capabilities in experimental automation. The performance gap between GPT-40 and GPT-3.5-turbo-0125 in cross-domain operations (80% versus 0% success in hybrid tasks) quantifies a key requirement for autonomous laboratories: integrated experimental reasoning. Single-domain competence, while necessary, proves insufficient for replicating expert-level experimental workflows. This disparity in cross-domain integration particularly manifests in complex scenarios, requiring simultaneous optimization of imaging parameters and data analysis-tasks routinely performed by experienced microscopists.\nError pattern analysis identifies a persistent challenge: protocol-to-execution translation. Code generation errors dominate failure modes (60-67%) across both architectures, establishing a primary bottleneck in experimental automation. This systematic limitation persists even in advanced models, suggesting architectural constraints beyond mere computational capability. The distribution of error types\u2014particularly the presence of instruction adherence errors (15%) in GPT-40 versus their absence in GPT-3.5-turbo-0125-reveals nuanced differences in model behavior that warrant further investigation regarding the safety and alignment issues associated with the LLMs. It is worth noting that AI-agent based on GPT-40, a model known for its reasoning capabilities, exhibit tendencies to perform actions beyond what is instructed, termed as \"sleepwalking\u201d. Note that depending on the nature of the LLM, \u201csleepwalking\" could result in potentially harmful outcomes in SDLs when dealing with highly dangerous chemicals or conditions such as high temperature pressure to name a few.\nAILA's modular design establishes quantifiable metrics for autonomous system development. The successful automation of AFM operations validates this architecture for complex"}, {"title": "4. Outlook", "content": "Altogether, AILA demonstrates quantifiable advances in experimental automation through systematic benchmarking. The framework's comprehensive performance metrics in AFM operations establish standards for autonomous laboratory evaluation, while AFMBench introduces reproducible protocols for systematic assessment across experimental domains. Successful execution of complex tasks-from automated image optimization to nanomechanical measurements\u2014validates the framework's capabilities for sophisticated materials characterization. However, the observed tendencies of LLM agents to exceed operational boundaries and perform \u201csleepwalking\u201d while carrying out the experiments raise significant safety concerns. This behavior, reported for the first time to the best of that authors' knowledge, emphasize critical areas for development in instruction alignment and operational safety.\nThis work's implications transcend materials characterization. The empirically validated principles-modular architecture (69% multi-tool efficiency), strategic agent deployment (83:17 ratio), and cross-domain integration (80% hybrid task success)\u2014establish design parameters for autonomous laboratories across disciplines. Applications span pharmaceutical screening, environmental monitoring, and process optimization. For instance, the documented success in parameter optimization could translate directly to automated high-throughput drug screening or catalyst discovery platforms. While current limitations in code generation (60% error rate) and tool coordination (31% efficiency) define immediate development targets, these metrics provide clear objectives for advancing autonomous scientific platforms. The path forward requires focused development in three key areas: enhanced cross-domain reasoning capabilities, robust code generation protocols, and sophisticated multi-agent coordination mechanisms. Success in these domains would enable truly autonomous scientific platforms capable of accelerating discovery across the scientific landscape."}, {"title": "Methodology", "content": "AILA is constructed utilizing the LangChain software framework, incorporating components such as prompts, LLMs, memory, agents, and tools. AILA uses two categories of prompts: system prompts (see S2.1 in Supplementary Information for the system prompts) and user prompts. System prompts define ethical rules for AILA's interactions and describe the responsibilities assigned to each agent, whereas user prompts are variable inputs provided by end-users. AILA's backbone consists of LLMs, namely GPT-40 and GPT-3.5-turbo-0125, which process user input as strings and provide string-based outputs. These LLMs are stateless, indicating that they do not save conversational context. Here, all interactions and agent states are stored in a Python dictionary and can be accessed by other agents. AILA consists of two specialized agents: the AFM Handler Agent and the Data Handler Agent, both equipped with unique tools to do specific tasks. These agents possess individual prompts, LLMs, and tools; however, they utilize a shared memory to store and access states, facilitating smooth interaction. The system prompts within the agents offer instructions for tool utilization and ethical guidelines, whereas the outputs from other tools or agents serve as user prompts. The framework utilizes LangGraph, a library that allows the construction of an effective multi-agent workflow, integrating all agents and tools seamlessly.\nThe architecture for AILA's decision-making process is carefully designed to ensure precise information routing. AILA can dynamically select among three primary options: AFM Handler, Data Handler, or FINISH. When AILA identifies the appropriate agent to handle a query, it routes the information to the selected option. In cases where AILA determines that none of the available agents can sufficiently address the question, it generates a well-structured response and selects the FINISH option to conclude the session effectively. The agents within this system are equipped with three distinct operational choices: utilizing their respective tools, transferring information to the next agent, or terminating the session. A system prompt has been integrated to streamline these decisions. Agents append the prefix NEED HELP to their response when transferring information to another agent. Alternatively, if they believe the query has been adequately addressed, they use the prefix FINAL ANSWER to signal the session's conclusion. By analyzing the output for these keywords, the system seamlessly routes the response to the designated agent or tool or finalizes the session. This structured approach enables efficient multi-agent collaboration, ensuring clarity, accuracy, and optimal performance across tasks while maintaining a robust and adaptive framework."}, {"title": "AFM Handler Agent", "content": "Atomic Force Microscopy demands precise sequential execution of multiple experimental stages. Image acquisition requires optimization across three critical parameters: imaging conditions, probe selection, and operational mode configuration (tapping/contact). The experimental sequence encompasses surface approach protocols, scanning procedures, and standardized data acquisition\u2014with procedural deviations potentially resulting in equipment damage or data corruption. Our implementation utilizes the DriveAFM instrument (Nanosurf), which is accessed through a Python-based API architecture and designed for universal compatibility with API-enabled AFM systems. To facilitate AFM imaging experiments, we"}, {"title": "Document Retrieval tool", "content": "The documentation for the instrument offers detailed instructions on how to handle and calibrate it. However, providing full access to the documentation to an LLM entails risks, such as inadvertent alterations to factory settings or calibration data, which could potentially result in damage or malfunction of the instrument. To address this concern, we manually extracted the essential information from the AFM documentation necessary for conducting experiments while safeguarding the instrument's integrity. We consolidated all the crucial codes for regulating each parameter of the instrument into a comprehensive Python script. Since Python code relies heavily on precise indentation and line structure, we utilized the Recursive Character Text Splitter from the LangChain library, specifically designed for Python, to divide the script into manageable chunks. The chunk size was set to a maximum of 1000 characters without overlap, adhering to the token limit for embedding models. Each code chunk comprises three sections: the first includes the necessary Python libraries, the second contains the code required to load the application, and the third section features unique Python code specific to the given task. The first two sections are consistent across all chunks (see S2.2 in the Supplementary Information file for more details). These chunks were then combined to generate a document, embedded using OpenAI's text-embedding-3-large model. This model, with the capability of producing embeddings of size up to 3072 dimensions, delivers exceptional performance compared to other OpenAI embedding models, especially in multi-language retrieval benchmarks like MIRACL42. To store the embeddings, we opted for Chroma, an open-source vector database known for its reliability and efficiency in managing large-scale embedding data. We use a vector store retriever to retrieve the data from the vector store."}, {"title": "Code Executor tool", "content": "A code executor tool has been developed to execute Python scripts generated by the AFM Handler Agent to control the AFM software. This tool is intended to run Python code, provided as a text string, directly on the local system to allow for smooth integration with the workflow of the AFM Handler Agent. The utility executes the code and sends back a success message or a detailed description of the error that occurred. If there is an error, the error message is returned to the AFM Handler agent so it can correct the error and retry executing. Otherwise, if the script runs without errors, it is considered the final result. This iterative process ensures precise control of the AFM system while systematically addressing any issues in the script."}, {"title": "Data Handler Agent", "content": "Surface tracking optimization in AFM requires precise calibration of three fundamental parameters: Proportional (P), Integral (I), and Derivative (D) gains. Optimal calibration manifests as convergence between trace and retrace signals, indicating stable scanning conditions. The Data Handler agent interfaces with specialized optimization and analysis"}, {"title": "Image Optimization Tool", "content": "The feedback system in an Atomic Force Microscope (AFM) plays a crucial role in maintaining control over the interaction between the cantilever tip and the sample surface. During scanning, variations in surface features alter the interaction forces between the tip and the sample, leading to deflections in the cantilever. These deflections are detected by a photodetector. To ensure that these deflections stay within a specified range, the feedback mechanism continuously adjusts the height of either the tip or the sample stage in real time. This process is managed by a PID (Proportional-Integral-Derivative) controller, which regulates the position of the z-piezo actuator. By moving the cantilever probe up or down, the controller maintains a steady interaction force or adheres to a predefined setpoint, depending on the chosen mode of operation.\nFine-tuning the P, I, and D gain values of the controller is vital for achieving accurate control of the setpoint in AFM imaging. The integral gain is especially important for enhancing image clarity by mitigating drift and reducing steady-state errors. Once the integral gain is optimized, adjusting the proportional gain can provide further refinement. The derivative gain, on the other hand, is particularly beneficial for imaging samples with pronounced edge features. If the gains are set too low, the PID loop may fail to maintain the setpoint effectively, while excessively high gain values can introduce electrical noise into the image due to amplified feedback or overcompensation for deviations. Properly optimized PID parameters ensure that the feedback loop remains stable and responsive, enabling the AFM to accurately track surface topography, even at higher scanning speeds. This balance is especially critical when imaging delicate, irregular, or soft materials, as it preserves the integrity of tip-sample interactions.\nA genetic algorithm (GA) was employed for PID gain optimization. The GA parameters included a fixed population size of three and a total of 15 generations, enabling efficient tuning of the gains. Although these parameters can be manually adjusted, but excessive image scanning may degrade the AFM tip. The optimized gains ensure effective feedback control, producing comparable forward and backward images. This can be achieved by calculating the mean squared error (MSE) between forward and backward z-axis images for various PID gain settings. However, this method is sensitive to drift during scanning, and this method also depends on previously acquired images. To address this, the structural similarity index (SSIM) was adopted as the fitness function in the genetic algorithm, providing a robust measure of image similarity between the z-axis forward and backward image independent of prior image data.\nThis metric offers advantages over traditional Mean Square Error (MSE) approaches by (i) addressing tip degradation challenges in contact-mode AFM by minimizing required scan cycles and enabling optimization using low-resolution images, (ii) maintaining accuracy under drift conditions, (iii) incorporating structural, brightness, and contrast variations in"}, {"title": "Baseline correction", "content": "The adaptive baseline correction employed in the step-edge detection of graphene is given by\n$B(x,y) = \\sum_{i,j} A_{ij} x^i y^j$\nWhere, $B(x, y)$ is the baseline function, $a_{ij}$ are the polynomial coefficients, i and j are the polynomial degrees (0 \u2264 i,j \u2264 n) with n being the maximum polynomial degree."}, {"title": "Image Analysis tool", "content": "AFM instrument stores the image data as a *.nid file in the local system. This *.nid file contains deflection, friction force, and z-axis images for both backward and forward scans. To further process any image from the file, exact data must be extracted from the file. To conduct this, we have used the NSFopen python library in the Image Analysis tool, which takes the query from the data handler agent regarding the specific image data and its location and returns the image data in an array to the data handler tool. To conduct further processing of the images, any Python script generated by the data handler tool can be executed in the Image Analysis tool, and the result can be returned to the data handler agent. Note that there is no database available to guide the LLM model in generating the Python script. It can generate the Python script by itself. There is a total of 6 input parameters for this tool:\n(1) path (str): Directory path to search for the latest file (default: None).\n(2) filename (str): Specific image file to display (default: None).\n(3) dynamic_code (str): Python code for processing image data (default: None).\n(4) calculate_friction (bool): Option to compute average friction (default: False).\n(5) calculate_mean_roughness (bool): Option to compute mean roughness (default: False).\n(6) calculate_rms_roughness (bool): Option to compute RMS roughness (default: False).\nReturns: A dictionary with the status, image data, or error details.\nAverage friction was calculated using the following formula:"}, {"title": "", "content": "$F_{ave} = \\frac{1}{2} (f_{ij} - b_{ij})$\nWhere $f_{ij}$ and $b_{ij}$ are the element at position $(i,j)$ in the array of the forward and backward friction image data. We have used the formula in this tool to calculate the mean roughness and RMS roughness values\n$R_{mean} = \\frac{1}{M.N} \\sum_{i=1}^M \\sum_{j=1}^N |z_{ij} - \\bar{z}|$\n$R_{rms} = \\sqrt{\\frac{1}{M.N} \\sum_{i=1}^M \\sum_{j=1}^N (z_{ij} - \\bar{z})^2}$\nwhere $z_{ij}$ is the element at position $(i, j)$ in the array, $\\bar{z}$ is the mean of all elements in the array, M is the number of rows in the array, N is the number of columns in the array of the z-axis forward image data."}, {"title": "AFMBench", "content": "To evaluate the performance of the AILA, we have manually created a set of 100 questions, carefully categorized into three distinct groups. The first classification is based on whether a question requires one or multiple tools/agents to be solved. The second category assesses the complexity of the questions, distinguishing between basic and advanced levels. Lastly, the questions are grouped by their requirements, such as documentation analysis or calculations. The complexity of each question is determined by the number of agents involved and the steps required to achieve the solution. For instance, modifying a parameter in an AFM system typically requires documentation review and the use of a single agent, categorizing it as a basic task. Conversely, capturing an AFM image and analyzing its surface roughness involves multiple agents, documentation analysis, and calculations, making it an advanced task. A comprehensive JSON file has been created, encapsulating detailed metadata about each question, including its respective category, for streamlined analysis and evaluation. This file serves as a structured resource for further investigations and testing."}]}