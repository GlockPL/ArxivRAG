{"title": "GAIM: ATTACKING GRAPH NEURAL NETWORKS VIA ADVERSARIAL INFLUENCE MAXIMIZATION", "authors": ["Xiaodong Yang", "Xiaoting Li", "Huiyuan Chen", "Yiwei Cai"], "abstract": "Recent studies show that well-devised perturbations on graph structures or node features can mislead trained Graph Neural Network (GNN) models. However, these methods often overlook practical assumptions, over-rely on heuristics, or separate vital attack components. In response, we present GAIM, an integrated adversarial attack method conducted on a node feature basis while considering the strict black-box setting. Specifically, we define an adversarial influence function to theoretically assess the adversarial impact of node perturbations, thereby reframing the GNN attack problem into the adversarial influence maximization problem. In our approach, we unify the selection of the target node and the construction of feature perturbations into a single optimization problem, ensuring a unique and consistent feature perturbation for each target node. We leverage a surrogate model to transform this problem into a solvable linear programming task, streamlining the optimization process. Moreover, we extend our method to accommodate label-oriented attacks, broadening its applicability. Thorough evaluations on five benchmark datasets across three popular models underscore the effectiveness of our method in both untargeted and label-oriented targeted attacks. Through comprehensive analysis and ablation studies, we demonstrate the practical value and efficacy inherent to our design choices.", "sections": [{"title": "1 Introduction", "content": "In recent years, Graph Neural Networks (GNNs) have emerged as increasingly powerful tools for graph understanding and mining [1, 2, 3, 4, 5]. GNNs capitalize on the connectivity structure of graphs as a filter for aggregating neighborhood information, enabling them to extract high-level node features [6]. As a consequence, GNNs have significantly advanced the state-of-the-art for various downstream tasks, including node classification and link prediction over graphs. However, despite their success, GNNs are not exempt from the inherent learning-security challenges found in standard machine learning models, such as a lack of adversarial robustness [7, 8]. A number of recent studies have illuminated the susceptibility of GNNs to adversarial attacks [9, 10, 11, 12, 13, 14, 15, 16, 17]. These attacks, armed with carefully crafted perturbations on graph structures and/or node features, can effectively deceive trained models.\nDespite the considerable efforts, existing works have been subject to certain limitations. Such constraints include impractical assumptions about an attacker's knowledge of target models or their ability to manipulate data. Even the so-called restricted black-box attacks [18, 19, 9, 20, 21, 15], where the attackers' access to a target model's parameters or outputs is limited, often permit access to model predictions or internal representations such as node embedding. Additionally, graph modifications should align with practical restrictions rather than assuming attackers can target any node at will. For example, nodes representing celebrities in social networks are often challenging to access or modify for significant attack impact. Moreover, most feature-level graph attacks exhibit limitations in the form of heuristic-based designs or the isolation of node selection and feature perturbation during the optimization process"}, {"title": "2 Related Work", "content": "Graph Neural Networks (GNNs) are known to be vulnerable to the quality of the input graphs due to its recursive message passing schema [13, 24, 25, 26]. The increasing incidence of adversarial attacks on GNNs is of significant concern as they pose a serious threat to the security and integrity of these networks. Over recent years, numerous studies [9, 10, 11, 12] have provided evidence of the vulnerability of GNNs to adversarial attacks. For example, Nettack considers both test-time and training-time attacks on node classification models [12]. These attacks, often achieved through minor perturbations to the graph structures or node features, can lead to severe misclassifications by the models. In light of these findings, the existing body of work can be classified based on several factors: the machine learning tasks, the intent of the attack, the phase in which the attack occurs, the nature of the attack, and the attacker's knowledge of the model during the attack process. Detailed analyses and comprehensive overviews of this literature are presented in numerous review papers [25, 27].\nThe attack methodologies can be divided into two primary categories: (1) Graph Poisoning, which focuses on altering the original graph by introducing malicious modifications during the training phase to compromise the integrity and"}, {"title": "3 Preliminary", "content": ""}, {"title": "3.1 Graph Neural Networks", "content": "Many real-world data are naturally represented as graphs, such as social networks, gene interaction networks and citation relations among documents. Coping with those frequent applied tasks on graph data, GNNs provide powerful techniques for graph mining and understanding [2, 3, 4]. Specifically, the GNNs use a well-designed message-passing scheme to perform nodes information aggregation via graph connectivity and extract high-level embeddings using message-passing schema.\nWithout loss of generality, we denote an undirected graph as G = (V, E, X), where V (|V| = N) is the set of nodes, E is the set of edges specifying node connections, and X \u2208 R^{N\u00d7D}, where D is the size of raw features. A \u2208 {0,1}^{N\u00d7N} is the adjacency matrix.\nA GNN model f : R^{N\u00d7D} \u2192 R^{N\u00d7K} is to map each node in graph to a label in C = {c_1, c_2, . . ., c_K }. The computation of its lth layer can be formulated as,\nH^{(t)} = ReLU(\\tilde{A}H^{(l-1)}W^{(l)}),\nwhere \\tilde{A} = \\tilde{D}^{-1/2}\\tilde{A}\\tilde{D}^{-1/2} is a symmetrically normalized adjacency matrix and \\tilde{A} is the adjacency matrix A added with self-loops and \\tilde{D}_{ii} = \u2211_j \\tilde{A}_{i,j}. With H^0 = X, the model with two layers can be formulated as,\nf_\\theta(X, A) = \\tilde{A}\u00b7 ReLU(\\tilde{A}XW^{(1)})W^{(2)},\nwhere \\theta denotes the set of model parameters with \u03b8 = {W^{(1)}, W^{(2)}}, f_\u03b8(X, A) represents the output logits of the graph input. We denote f_\u03b8(X, A) \u2208 R^K as the output logit of the node j and f_\u03b8(X, A)_c \u2208 R as the logit value corresponding to the label c."}, {"title": "3.2 GNN Adversarial Attacks", "content": "The attack aims to identify representative nodes and their impactful features, such that conducting perturbation on them will result in the maximal performance degradation of models. The process includes the selection of nodes and features.\nSimilarly to works [15, 16], in the selection of nodes to perturb, the attack selects a small set of nodes A \u2282 V under two constraints:\n|A| \u2264 B_n & \u2200\u03c5 \u2208 A, deg(v) \u2264 T\nwhere B_n is the node budget, deg denotes the degree of a node, and T is predefined threshold. The purpose is to limit the target scope to ordinary nodes with small degrees and avoid the significant nodes being manipulated, which makes the attack practical.\nFor the feature perturbation, the attacker picks out a small portion of impactful features of the selected nodes with a feature budget B_f, and then conducts perturbation \u2208_i on the node's feature X_i \u2208 R^{1\u00d7D}, as follows:\nP_i = {\u2208_i | lb \u2264 X_i + \u2208_i \u2264 ub & ||\u2208_i||_0 \u2264 B_f},\nwhere ub and lb denotes the upper bounds and lower bounds of features, respectively. It is noteworthy that \\hat{\u2208}_{ij} \u2208 P_i will be used to represent the optimal feature perturbation derived by our method on the node i for misclassifying its neighboring node j."}, {"title": "4 Methodology", "content": "This section introduces our attack method, GAIM, which aims to identify the nodes with the highest adversarial influence by optimizing feature modifications. We achieve this by generating feature perturbations within the adversarial feature domain and calculating an adversarial influence score for each node. This score quantifies the node's ability to transmit misleading messages to other nodes. In our setting, a node's adversarial influence is defined as its capacity to induce prediction changes of its neighboring nodes under budgets. However, estimating this influence directly on a target GNN model can be challenging due to the inherent nonlinearity and the non-accessibility of the target model in restricted black-box attacks. As an alternative approach, we approximate the nodes' adversarial influence using a simplified surrogate model to mitigate complexity and non-accessibility. Leveraging the well-established concept of adversarial transferability between models, we utilize a specifically designed adversarial influence function to measure the impact of nodes on other nodes within the graph. By employing this framework, we can effectively identify the most influential nodes for attacks.\nWe illustrate the framework of our method in Figure 1. The yellow circles in the figure represent candidate nodes whose degree is below a specified threshold. In Step 1, we train a surrogate model on the target graph. In Step 2, we compute the adversarial influences of each candidate node on the predictions of its neighboring nodes by modifying its features. To accomplish this, for each candidate node, we will identify a set of affected nodes denoted as M, which are nodes whose predictions can be altered by applying perturbations to the candidate node within a predefined budget.\nThe computation of these perturbations involves solving a series of optimization problems intersected with step 3. During this phase, we derive the optimal feature perturbation, represented as \\hat{\u2208}_{ij} \u2208 P_i on the candidate node i for its neighboring node j. The cumulation of these perturbations for a given candidate is denoted as a set \u2206X. Subsequently, we rank the efficacy of each feature perturbation based on its contribution to the attack, retaining only the top-B f impactful ones. Following this, from all perturbations tailored for the candidate's neighbors, we identify the most prevalent perturbation patterns, forming the final perturbation for the candidate node. In selecting the target nodes, we apply this finalized feature perturbation, compute candidate influence scores, and select the top-Bn nodes as our primary attack targets."}, {"title": "4.1 Attacks & Adversarial Influence", "content": "Harnessing the advantages of our highly integrated framework, our method enables the execution of three distinct attacks. The first one is the untargeted attacks [15, 16], which selects a group of target nodes to degrade the overall accuracy of a model. Complementing this approach, we also have developed two novel label-oriented attacks that have not been extensively studied before in the graph learning. Moreover, we emphasize the significance of perturbation consistency, an essential but often neglected aspect in untargeted attacks on graphs.\nAs described in Equation (3), the essence of these attacks is to identify a group of the most influential target nodes to maximize the attack effect. As introduced above, the adversarial influence of one node is associated with its M that represent its neighboring nodes whose classification can be affected by perturbations on the node. Take a node i for"}, {"title": "4.1.1 Untargeted Attack", "content": "Untargeted attack methods such as [15, 16] do not require the misclassified label \u03bd_i(X + \\hat{\u2208}_{ij}, A) for each j \u2208 N_i to be identical. That seems reasonable as the primary concern of adversarial influence lies in the capability to modify other nodes' predictions, without necessarily specifying the misclassified target label. However, a different scenario arises when crafting a node's feature perturbation by merging multiple impactful feature perturbations. As each of these features may yield to different misclassified labels, impulsively aggregating them could undermine the final perturbation's effectiveness in the attack. To ensure the attack's effectiveness, it is crucial to maintain what we refer to as perturbation consistency. In accordance with this principle, all perturbed features should lead to the same misclassified direction, implying that \u03bd_i(X + \\hat{\u2208}_{ij}, A) for each j \u2208 N_i is the same.\nConsidering the perturbation consistency, the set M_{ic} with the label c \u2208 C as the misclassified label is defined as\nM_{ic} = M_i \u2229 {j \u2208 N_i | \u03bd_i(X + \\hat{\u2208}_{ij}, A) = c},\nwhere c is not necessarily the same for each candidate node i \u2208 V. The adversarial influence score of the node i in our untargeted attack is computed by\nS_i = max_{c \u2208 C} |M_{ic}|."}, {"title": "4.1.2 Two Types of Label-oriented Targeted Attacks", "content": "The adversarial influence score as described in Equation (9) is employed in untargeted attacks with the objective of selecting target nodes to undermine the overall classification accuracy of models on graph nodes. In addition to this attack, we can expand this definition to encompass two types of label-oriented attacks. The first type involves modifying target nodes to diminish the classification accuracy for a specified label, denoted as c_t. In order to achieve this, the adversarial influence of a node must be assessed based on its capability to misclassify neighboring nodes that possess the target label. The node set M_i is accordingly modified to\nType I: M_{ict} = M_i \u2229 {j \u2208 N_i | \u03bd_i(X, A) = c_t},\nIt is noteworthy that unlike c in Equation 8, the label ct remains fixed for all candidate nodes throughout the attack. Essentially, this attack aims to misclassify nodes who are already predicted as \u03bd_i(X, A) = c_t.\nThe other type of attack entails the selection of target nodes with the explicit objective of maximizing the misclassifica- tion rate towards a specific target label c_t. The goal is to induce the nodes to be erroneously classified as the target label. The M_i is updated to,\nType II: M_{ict}^\\prime = M_i \u2229 {j \u2208 N_i | \u03bd_i(X + \\hat{\u2208}_{ij}, A) = c_t}.\nTo compute the adversarial influence score of node i for these attacks, for each j \u2208 N_i, we need to verify whether there exists valid perturbation \\hat{\u2208}_{ij} \u2208 P_i that affects its prediction, as formally described in the following problem.\nGiven a GNN model f_\u03b8, a graph G, a target node i and one of its neighboring node j. Let \u0109 \u2208 C be the node j's predicted label with clean graph data. The worst-case margin between logit values of the labels c (c \u2208 C and c \u2260 \u0109) and \u0109 achievable under the perturbation domain P_i can be formulated by,\np(c, \u0109) := max_{\u2208_{ij} \u2208 P_i} f_\u03b8(X + \\hat{\u2208}_{ij}, A)_c - f_\u03b8(X + \\hat{\u2208}_{ij}, A)_\u0109,"}, {"title": "4.2 Adversarial Influence on A Surrogate Model", "content": "A SGC model is a linearized GCN model with the activation function ReLU in Equation (1) being removed. Therefore, Equation 1 and 2 for the SGC model with L layers are reformulated into:\nH^{(l)} = \\tilde{A}H^{(l\u22121)}W^{(l)}, f_\u03b8(X, A) = AXW,\nwhere A = \\tilde{A}^L and W = \u03a0_{l=1}^L W^{(l)} The output logits of a neighboring node j with the perturbation \\hat{\u2208}_{ij} on the node i can be described as in Equation (13), where a_{kj} is an entry of A that represents the propagation weight between the nodes k and j, N_j = N_i \u222a {j}, and [W]_c represents the column of W corresponding to the label c.\nf_\u03b8(X + \\hat{\u2208}_{ij}, A)_c = a_{ij}\u2208_{ij} + \u2211_{k\u2208N_j} a_{kj}X_k [W]_c\nTherefore, by substituting the f_\u03b8(X + \\hat{\u2208}_{ij}, A) in Equation (12) with the expression in Equation (13), we effectively convert the original nonlinear optimization problem that aims to change the predicted label of the node j from \u0109 to c into an LP problem:\np(c, \u0109) := max_{\u2208_{ij} \u2208 P_i} a_{ij}\u2208_{ij} + \u2211_{k\u2208N_j} a_{kj}X_k([W]_c - [W]_\u0109)"}, {"title": "4.2.1 Solving of The LP Problem", "content": "In Equation (4), the feasible region P_i is a conjunction of lb < X_i + \u2208_i < ub and ||\u2208_i||_0 < B_f. The domain lb \u2264 X_i + \u2208_i \u2264 ub is essentially a box constraint. Let xi,d denote the dth entry of X_i, then, we have the range of \\hat{\u2208}_{ij,d} to be,\nlb_d - X_{i,d}\u2264 \\hat{\u2208}_{ij,d}\u2264 ub_d - X_{i,d}.\nand the domain ||\u2208_i||_0 \u2264 B_f specifies the maximal number of features to perturb. We first expand the objective function into\np(c, \u0109) := max_{\u2208_{ij} \u2208 P_i}\u2211_{d=1}^D a_{ij}\u2208_{ij,d} + \u2211_{k\u2208N_j}\u2211_{d=1}^D a_{kj}X_{k,d}(W_{dc} \u2212 W_{d\u0109});\nwhere \u03c9_d = a_{ij} (W_{dc} \u2212 W_{d\u0109}) and \u03c9_{dc} is the entry of W. We can notice that \u03c9_d is the coefficient, \\hat{\u2208}_{ij,d} is the decision variable, and the item after the addition sign is a constant. When \u03c9_d is positive and negative, we have\n\u03c9_d(lb_d \u2212 x_{i,d}) \u2264 \u03c9_d\\hat{\u2208}_{ij,d} \u2264 \u03c9_d(ub_d \u2212 x_{i,d}),\n\u03c9_d(ub_d \u2212 x_{i,d}) \u2264 \u03c9_d\\hat{\u2208}_{ij,d} \u2264 \u03c9_d(lb_d \u2212 x_{i,d}),"}, {"title": "4.2.2 Construction of Final Perturbation", "content": "The aforementioned computation of the optimal perturbation on the node i for each of its neighboring nodes yield a set of perturbation denoted as \u2206X_i in Equation (7). How to craft the final perturbation for i that maximizes its overall adversarial influence is critical. Empirically, we observe that in our attack the selected features and their perturbations of each \\hat{\u2208}_{ij} in \u2206X_i share a lot of similarity. Therefore, we utilize the most common features and their perturbations in \u2206X_i to form the final perturbation. This crafting process is efficient and its effectiveness is also validated in our experiments."}, {"title": "4.2.3 The Black-box GNN Attack Procedure", "content": "In a nutshell, we operate GAIM for maximizing adversarial influence on nodes by combining target node selection with optimized feature perturbations in a black-box setup. Initially, candidate nodes are identified by filtering out those with degrees exceeding the threshold (Equation (3)). For each candidate, we compute influence scores and feature perturbations using the method from Section 4. Notably, a neighboring node's use in one candidate's influence score computation precludes its reuse for others, maximizing overall impact as in Equation (5). Subsequently, we select the most impactful candidate nodes with perturbations to deploy the modified graph for testing."}, {"title": "5 Experimental Results and Analysis", "content": "This section presents a comprehensive evaluation of our proposed attack strategy (GAIM) against several typical GNN models, and compare its effectiveness with the baseline methods. Additionally, we conduct the parameter analysis to study their impacts on the attack performance, and ablation study to investigate the importance of different components in our design."}, {"title": "5.1 Experimental Setup", "content": "Datasets and GNN models. In this study, we use node classification tasks to assess the attack capability of GAIM on five benchmark datasets: Cora, Citeseer, Pubmed [33], one online image network Flickr [34] and Reddit [35]. A summary of the basic properties of these datasets is provided in Table 1. In all of our experiments, we randomly split each dataset at the ratio of 3:1:1 for training, validation, and testing. For each attack setting, we run 20 trials and gather the average as our results. We evaluate our attack strategy on several commonly-used GNN models, including: (1) GCN [2], (2) JK-NetMaxpool [31], and (3) GAT [32]. We set the number of layers to 2 for all models and set the number of heads to 8 for GAT. The complete experimental result is available at our supplementary material.\nBaselines We compare our strategy with two distinct groups of methods for fair comparison. The first group contains the popular heuristic-based metrics commonly used to measure the informativeness of nodes in few-shot learning or Active Learning problems [36, 37]. These methods are typically employed as selection criteria to identify representative nodes in graph learning, based on the intuition that they have a greater impact on other nodes in the message-passing scheme.\nIn this study, we use three well-known baselines from this group, namely Degree, PageRank, and Betweenness. Additionally, we include Random selection as a trivial baseline. In the second group, we adopt the methods in the related works [16, 15] to compare with. In these methods, RWCS and GC-RWCS [16] derive adversarial attacks by approximately maximizing the cross-entropy classification loss using heuristics, while InfMax-Unif and InfMax-Norm [15] model the problem of maximizing model misclassification rate as an influence maximization problem on a variant of linear threshold model. For this group of baselines, we replicated the parameter setup as described in the original works.\nNotably, all the baseline strategies solely focus on target node selections and neglect the perturbation construction on node features. This emphasizes the uniqueness of our design. In constructing the perturbation vector in these baseline methods, we adopt the approach outlined in the paper [15]. They train 20 GCN models as proxy models and compute the average gradients of the classification loss with respect to the node features. Then, the features with the top gradients"}, {"title": "5.2 Method Evaluation", "content": "In this part, we evaluate the performance of our method under different settings. In practical adversarial scenarios, ensuring the imperceptibility of the attack is a crucial design requirement [8]. To achieve feasibility and practicality, we impose three essential constraints on our attack strategy. (1) We set the maximum number of target nodes to be only 1% of the total graph size for the datasets Cora, Citeseer and Pubmed, and 0.1% for the larger datasets Flickr and Reddit. (2)We perform perturbations exclusively on nodes with low degrees., as they are typically more accessible in real-world scenarios. In contrast, high-degree nodes are unlikely to have their properties altered, rendering them unrealistic targets. We restrict our selection of candidate nodes to be the set after removing top 10% and 30% of nodes with the highest degree; (3) The feature modification rate of the target nodes is set as 2% for Cora, Citeseer and Pubmed, and 5% for Flickr and Reddit. These controlled rates ensure that the alterations to the nodes' features remain subtle and inconspicuous while still exerting significant influence. The features of Flickr and Reddit are actually embedding vectors which don't have specific ranges. Therefore, we set their upper bound and lower bound with the global maximum and minimum value over all features in the data, respectively."}, {"title": "5.2.1 Comparison results under untargeted attack settings.", "content": "In this section, we evaluate our proposed attack method against baseline techniques in a general untargeted attack setting. The objective was to validate the effectiveness of our approach, and to this end, we collected attack results for three models across five datasets. The evaluation results are summarized in Table 2 and 3, which presents the results with removing top 10% nodes with highest degree. For the results pertaining to the removal of 30% of nodes are detailed in our supplementary material. In the table, the label None denotes the regular GNN with no attacks applied. As the results illustrate, our attack strategy significantly diminishes the classification accuracy of diverse GNN models across all the experimented datasets. We managed to induce an average reduction in accuracy of 21.0% on the Cora dataset, 21.7% on Citeseer, 13.0% on Pubmed, 8.7% on Flickr, and a striking 39.2% on Reddit. We think the substantial performance degradation on Reddit could be attributed to the wide range of feature boundaries, allowing for large perturbations.\nOur method consistently outperformed most of the baseline methods for all attacking settings. Particularly, our approach exhibited superior performance compared to others when applied to the JKNetMax model and GCN model. It is important to note that baseline methods (RWCS, GC-RWCS, InfMax-Unif, InfMax-Norm) face challenges when scaling to large graph datasets and are restricted in their applicability to black-box scenarios. In contrast, our proposed method is not encumbered by such constraints and can be proficiently deployed across various scenarios. Remarkably, As shown in Table 3, our method outperforms the best-performing baseline across the three models with two large datasets. This solid empirical performance demonstrates the efficacy of our attack methodology and affirms its potential to hinder the reliability of GNN models across a spectrum of datasets."}, {"title": "5.2.2 Evaluation results of Label-oriented attacks.", "content": "In this section, we present the evaluation results for two types of label-oriented attack settings. We conduct the experiments on JKNetMax, GCN, and GAT models across all datasets. The first type involves degrading the model's performance specifically on the specified label. Hyperparameters keep the same as before. During testing, we measure both the accuracy of the attacking label and the overall performance of the model across all labels, recording the outcomes for analysis. We display the top three labels which are mostly impacted by attack in Figure 2 Type-I attack. The results show that our method significantly reduces the classification accuracy of the targeted labels. Remarkably, our technique manages to degrade the targeted classification of these three models by an average of 69.4% for Cora, 35.9% for Citeseer, 41.6% for Pubmed, 20.4% for Flickr and and 92.6% for Reddit. The substantial drops in Reddit is mainly due to the large range of feature values which enables strong attacks. Overall, these results validate our approach's capability of strategically compromising the model's predictions for specific labels, highlighting its utility for label-oriented attacks\nThe second type of label-oriented attack seeks to induce the model into misclassifying nodes as the targeted label. We compute the misclassification rate for each label during testing, representing the proportion of nodes successfully"}, {"title": "5.3 Parameter Analysis", "content": "In this section, we investigate the impact of the two hyperparameters, namely the node budget B_n and the feature budget B_f, on the performance of our method. Notely, our approach does not require additional hyperparameters. The node budget represents the percentage of target nodes related to the graph size, while the feature budget is the percentage of the maximum allowed modified features in each node.\nTo ensure a comprehensive analysis and fair comparison, we divide the investigation into two parts based on the sizes of different datasets. When we study the impact of different node budget, we vary the node budget Bn over the values {1%, 2%, 3%, 4%, 5%} for smaller graph datasets (Cora, Citeseer, Pubmed), with the feature budget B_f remaining constant at 2%. For the larger graph datasets (Flickr and Reddit), Bn is varied across {0.1%, 0.2%, 0.3%, 0.4%, 0.5%}, with the B_f fixed at 5%. As we analyze the influence of different feature budgets B_f, we adjust Bf to span"}, {"title": "5.4 Analysis of Computational Complexity", "content": "In our method, the influence computation of one node is an LP problem. If its time complexity is denoted as O(lp), the overall complexity of our algorithm will be O(MBK \u00b7 lp), where M is the number of nodes, K is the number of labels, and B is the averaged number of one's neighboring nodes. The inclusion of K is because of the perturbation consistency in Equation 9. It is noteworthy that our LP problem only includes decision variables with independent range constraints. This results in a significantly reduced complexity of O(lp). The solving process is shown in Section 4.2.1.\nThe baseline methods RWCS, GC-RWCS, InfMax-Unif and InfMax-Norm are mainly based on Random Walk method. Their complexity is O(ML) which means the number of elementary matrix-multiplication operation. Here, M is the number of nodes and L stands for L-step random walk. It is challenging to intuitively compare our time complexity with theirs. In practice, the complexity of these methods causes scalability issues when applied to large datasets like Flickr and Reddit 3. This implicitly indicates that our method has a lower complexity compared to these related works. Therefore, our method is more scalable."}, {"title": "5.5 Ablation Study", "content": "In this section, we undertake an ablation study to investigate the individual contributions of our different key components to the overall attack performance. This helps validate the rationale behind our design. We develop two distinct reassembly mechanisms to further substantiate the importance of two key parts of our method. To do this, we arrange three types of workflows: (1) GAIMglobal_perturb.: Instead of our customized perturbation for each node, we establish a global feature perturbation for all target nodes, analogous to the baselines. (2) GAIMinconsistency: In this version, we do not insist on adhering to the perturbation consistency when evaluating the adversarial influence score of a candidate node. Contrarily, the misclassified labels of neighboring nodes, represented by 4(X + ei, A), can be diverse and random. (3) GAIMoriginal: This represents the complete design of our method. We employ the three citation graph as the representative examples for evaluating these different settings. All three models JKNetMax, GCN and GAT are considered. The results are in Table 4.\nWe can see that these two pivotal components within our attack play crucial roles. Employing a global perturbation instead of our customized perturbation exposes weakness in the attack, resulting in a drop of success attack rate by 7.41%. This discrepancy stands as a primary factor contributing to our method's performance over other state-of-the-art works. Similarly, the absence of our perturbation consistency mechanism in the alternative method also reveals weakness in its attack, resulting in a success attack rate 4.12% lower than our original method. This validates our claim that impulsively merging perturbations \u0109ij, which cause different misclassifications of node i's neighboring nodes, can compromise the effectiveness of the final perturbation on the node i. It highlights the importance of perturbation consistency in our attack. Overall, we can conclude that both our perturbation customization and consistency contribute to our attack's outstanding efficacy."}, {"title": "6 Conclusion", "content": "In this study, we propose GAIM, a practical graph node-level attack designed for the restricted black-box setting. By strategically selecting target nodes and perturbing their features to maximize adversarial influence, our integrated approach achieves significant impact while targeting only a small number of low-degree nodes within the graph. Additionally, our method easily extends to two types of label-oriented attacks, showcasing superior performance in general untargeted and label-oriented attack scenarios. The adaptability and effectiveness of GAIM highlight its potential to disrupt the accuracy and integrity of graph neural network models under various adversarial conditions. Its practical nature, coupled with its ability to excel in different attack scenarios, positions GAIM as a promising tool for advancing research in graph adversarial attacks and enhancing the security of graph-based machine learning systems. As future work, we aim to derive extensions of our strategy to other network architectures like Transformers."}]}