{"title": "GAIM: ATTACKING GRAPH NEURAL NETWORKS VIA ADVERSARIAL INFLUENCE MAXIMIZATION", "authors": ["Xiaodong Yang", "Xiaoting Li", "Huiyuan Chen", "Yiwei Cai"], "abstract": "Recent studies show that well-devised perturbations on graph structures or node features can mislead trained Graph Neural Network (GNN) models. However, these methods often overlook practical assumptions, over-rely on heuristics, or separate vital attack components. In response, we present GAIM, an integrated adversarial attack method conducted on a node feature basis while considering the strict black-box setting. Specifically, we define an adversarial influence function to theoretically assess the adversarial impact of node perturbations, thereby reframing the GNN attack problem into the adversarial influence maximization problem. In our approach, we unify the selection of the target node and the construction of feature perturbations into a single optimization problem, ensuring a unique and consistent feature perturbation for each target node. We leverage a surrogate model to transform this problem into a solvable linear programming task, streamlining the optimization process. Moreover, we extend our method to accommodate label-oriented attacks, broadening its applicability. Thorough evaluations on five benchmark datasets across three popular models underscore the effectiveness of our method in both untargeted and label-oriented targeted attacks. Through comprehensive analysis and ablation studies, we demonstrate the practical value and efficacy inherent to our design choices.", "sections": [{"title": "1 Introduction", "content": "In recent years, Graph Neural Networks (GNNs) have emerged as increasingly powerful tools for graph understanding and mining [1, 2, 3, 4, 5]. GNNs capitalize on the connectivity structure of graphs as a filter for aggregating neighborhood information, enabling them to extract high-level node features [6]. As a consequence, GNNs have significantly advanced the state-of-the-art for various downstream tasks, including node classification and link prediction over graphs. However, despite their success, GNNs are not exempt from the inherent learning-security challenges found in standard machine learning models, such as a lack of adversarial robustness [7, 8]. A number of recent studies have illuminated the susceptibility of GNNs to adversarial attacks [9, 10, 11, 12, 13, 14, 15, 16, 17]. These attacks, armed with carefully crafted perturbations on graph structures and/or node features, can effectively deceive trained models.\nDespite the considerable efforts, existing works have been subject to certain limitations. Such constraints include impractical assumptions about an attacker's knowledge of target models or their ability to manipulate data. Even the so-called restricted black-box attacks [18, 19, 9, 20, 21, 15], where the attackers' access to a target model's parameters or outputs is limited, often permit access to model predictions or internal representations such as node embedding. Additionally, graph modifications should align with practical restrictions rather than assuming attackers can target any node at will. For example, nodes representing celebrities in social networks are often challenging to access or modify for significant attack impact. Moreover, most feature-level graph attacks exhibit limitations in the form of heuristic-based designs or the isolation of node selection and feature perturbation during the optimization process"}, {"title": "2 Related Work", "content": "Graph Neural Networks (GNNs) are known to be vulnerable to the quality of the input graphs due to its recursive message passing schema [13, 24, 25, 26]. The increasing incidence of adversarial attacks on GNNs is of significant concern as they pose a serious threat to the security and integrity of these networks. Over recent years, numerous studies [9, 10, 11, 12] have provided evidence of the vulnerability of GNNs to adversarial attacks. For example, Nettack considers both test-time and training-time attacks on node classification models [12]. These attacks, often achieved through minor perturbations to the graph structures or node features, can lead to severe misclassifications by the models. In light of these findings, the existing body of work can be classified based on several factors: the machine learning tasks, the intent of the attack, the phase in which the attack occurs, the nature of the attack, and the attacker's knowledge of the model during the attack process. Detailed analyses and comprehensive overviews of this literature are presented in numerous review papers [25, 27].\nThe attack methodologies can be divided into two primary categories: (1) Graph Poisoning, which focuses on altering the original graph by introducing malicious modifications during the training phase to compromise the integrity and"}, {"title": "3 Preliminary", "content": ""}, {"title": "3.1 Graph Neural Networks", "content": "Many real-world data are naturally represented as graphs, such as social networks, gene interaction networks and citation relations among documents. Coping with those frequent applied tasks on graph data, GNNs provide powerful techniques for graph mining and understanding [2, 3, 4]. Specifically, the GNNs use a well-designed message-passing scheme to perform nodes information aggregation via graph connectivity and extract high-level embeddings using message-passing schema.\nWithout loss of generality, we denote an undirected graph as $G = (V, E, X)$, where $V (|V| = N)$ is the set of nodes, $E$ is the set of edges specifying node connections, and $X \\in \\mathbb{R}^{N\\times D}$, where $D$ is the size of raw features. $A \\in \\{0, 1\\}^{N\\times N}$ is the adjacency matrix.\nA GNN model $f : \\mathbb{R}^{N\\times D} \\rightarrow \\mathbb{R}^{N\\times K}$ is to map each node in graph to a label in $C = \\{C_1, C_2, . . ., C_K \\}$. The computation of its $l$th layer can be formulated as,\n$H^{(l)} = ReLU(\\hat{A}H^{(l-1)}W^{(l)}),$\nwhere $\\hat{A} = \\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}$ is a symmetrically normalized adjacency matrix and $\\tilde{A}$ is the adjacency matrix $A$ added with self-loops and $\\tilde{D}_{ii} = \\sum_j \\tilde{A}_{i,j}$. With $H^0 = X$, the model with two layers can be formulated as,\n$f_\\theta(X, A) = \\hat{A}\\cdot ReLU(\\hat{A}XW^{(1)})W^{(2)},$\nwhere $\\theta$ denotes the set of model parameters with $\\theta = \\{W^{(1)}, W^{(2)}\\}$, $f_\\theta(X, A)$ represents the output logits of the graph input. We denote $f^\\theta_j(X, A) \\in \\mathbb{R}^{K}$ as the output logit of the node $j$ and $f^\\theta_j(X, A)_c \\in \\mathbb{R}$ as the logit value corresponding to the label $c$."}, {"title": "3.2 GNN Adversarial Attacks", "content": "The attack aims to identify representative nodes and their impactful features, such that conducting perturbation on them will result in the maximal performance degradation of models. The process includes the selection of nodes and features.\nSimilarly to works [15, 16], in the selection of nodes to perturb, the attack selects a small set of nodes $A \\subset V$ under two constraints:\n$|A| \\leq B_n \\& \\forall v \\in A, deg(v) \\leq T$\nwhere $B_n$ is the node budget, $deg$ denotes the degree of a node, and $T$ is predefined threshold. The purpose is to limit the target scope to ordinary nodes with small degrees and avoid the significant nodes being manipulated, which makes the attack practical.\nFor the feature perturbation, the attacker picks out a small portion of impactful features of the selected nodes with a feature budget $B_f$, and then conducts perturbation $\\epsilon_i$ on the node's feature $X_i \\in \\mathbb{R}^{1\\times D}$, as follows:\n$\\mathcal{P}_i = \\{ \\epsilon_i | lb \\leq X_i + \\epsilon_i \\leq ub \\& ||\\epsilon_i||_0 \\leq B_f \\},$\nwhere $ub$ and $lb$ denotes the upper bounds and lower bounds of features, respectively. It is noteworthy that $\\hat{\\epsilon}_{ij} \\in \\mathcal{P}_i$ will be used to represent the optimal feature perturbation derived by our method on the node $i$ for misclassifying its neighboring node $j$."}, {"title": "4 Methodology", "content": "This section introduces our attack method, GAIM, which aims to identify the nodes with the highest adversarial influence by optimizing feature modifications. We achieve this by generating feature perturbations within the adversarial feature domain and calculating an adversarial influence score for each node. This score quantifies the node's ability to transmit misleading messages to other nodes. In our setting, a node's adversarial influence is defined as its capacity to induce prediction changes of its neighboring nodes under budgets. However, estimating this influence directly on a target GNN model can be challenging due to the inherent nonlinearity and the non-accessibility of the target model in restricted black-box attacks. As an alternative approach, we approximate the nodes' adversarial influence using a simplified surrogate model to mitigate complexity and non-accessibility. Leveraging the well-established concept of adversarial transferability between models, we utilize a specifically designed adversarial influence function to measure the impact of nodes on other nodes within the graph. By employing this framework, we can effectively identify the most influential nodes for attacks."}, {"title": "4.1 Attacks & Adversarial Influence", "content": "Harnessing the advantages of our highly integrated framework, our method enables the execution of three distinct attacks. The first one is the untargeted attacks [15, 16], which selects a group of target nodes to degrade the overall accuracy of a model. Complementing this approach, we also have developed two novel label-oriented attacks that have not been extensively studied before in the graph learning. Moreover, we emphasize the significance of perturbation consistency, an essential but often neglected aspect in untargeted attacks on graphs.\nAs described in Equation (3), the essence of these attacks is to identify a group of the most influential target nodes to maximize the attack effect. As introduced above, the adversarial influence of one node is associated with its $M$ that represent its neighboring nodes whose classification can be affected by perturbations on the node. Take a node $i$ for"}, {"title": "4.1.1 Untargeted Attack", "content": "Untargeted attack methods such as [15, 16] do not require the misclassified label $\\hat{y}^\\theta_j(X + \\hat{\\epsilon}_{ij}, A)$ for each $j \\in N_i$ to be identical. That seems reasonable as the primary concern of adversarial influence lies in the capability to modify other nodes' predictions, without necessarily specifying the misclassified target label. However, a different scenario arises when crafting a node's feature perturbation by merging multiple impactful feature perturbations. As each of these features may yield to different misclassified labels, impulsively aggregating them could undermine the final perturbation's effectiveness in the attack. To ensure the attack's effectiveness, it is crucial to maintain what we refer to as perturbation consistency. In accordance with this principle, all perturbed features should lead to the same misclassified direction, implying that $\\hat{y}^\\theta_j(X + \\hat{\\epsilon}_{ij}, A)$ for each $j \\in N_i$ is the same.\nConsidering the perturbation consistency, the set $M_{ic}$ with the label $c \\in \\mathcal{C}$ as the misclassified label is defined as\n$M_{ic} = M_i \\cap \\{j \\in N_i | \\hat{y}^\\theta_j(X + \\hat{\\epsilon}_{ij}, A) = c\\},$\nwhere $c$ is not necessarily the same for each candidate node $i \\in V$. The adversarial influence score of the node $i$ in our untargeted attack is computed by\n$S_i = \\max_{c\\in C} |M_{ic}|.$"}, {"title": "4.1.2 Two Types of Label-oriented Targeted Attacks", "content": "The adversarial influence score as described in Equation (9) is employed in untargeted attacks with the objective of selecting target nodes to undermine the overall classification accuracy of models on graph nodes. In addition to this attack, we can expand this definition to encompass two types of label-oriented attacks. The first type involves modifying target nodes to diminish the classification accuracy for a specified label, denoted as $c_t$. In order to achieve this, the adversarial influence of a node must be assessed based on its capability to misclassify neighboring nodes that possess the target label. The node set $M_i$ is accordingly modified to\nType I: $M_{ict} = M_i \\cap \\{j \\in N_i | y^\\theta_j(X, A) = c_t\\},$\nIt is noteworthy that unlike $c$ in Equation 8, the label $c_t$ remains fixed for all candidate nodes throughout the attack. Essentially, this attack aims to misclassify nodes who are already predicted as $\\hat{y}^\\theta_j(X, A) = c_t$.\nThe other type of attack entails the selection of target nodes with the explicit objective of maximizing the misclassifica- tion rate towards a specific target label $c_t$. The goal is to induce the nodes to be erroneously classified as the target label. The $M_i$ is updated to,\nType II: $M^c_{ict} = M_i \\cap \\{j \\in N_i | \\hat{y}^\\theta_j(X + \\hat{\\epsilon}_{ij}, A) = c_t\\}.$\nTo compute the adversarial influence score of node $i$ for these attacks, for each $j \\in N_i$, we need to verify whether there exists valid perturbation $\\hat{\\epsilon}_{ij} \\in \\mathcal{P}_i$ that affects its prediction, as formally described in the following problem.\nGiven a GNN model $f_\\theta$, a graph $G$, a target node $i$ and one of its neighboring node $j$. Let $\\hat{c} \\in \\mathcal{C}$ be the node $j$'s predicted label with clean graph data. The worst-case margin between logit values of the labels $c$ ($c \\in \\mathcal{C}$ and $c \\neq \\hat{c}$) and $\\hat{c}$ achievable under the perturbation domain $\\mathcal{P}_i$ can be formulated by,\n$\\rho(c, \\hat{c}) := \\max_{\\epsilon_{ij} \\in \\mathcal{P}_i} f^\\theta_j(X + \\hat{\\epsilon}_{ij}, A)_c - f^\\theta_j(X + \\hat{\\epsilon}_{ij}, A)_{\\hat{c}},$"}, {"title": "4.2 Adversarial Influence on A Surrogate Model", "content": "A SGC model is a linearized GCN model with the activation function ReLU in Equation (1) being removed. Therefore, Equation 1 and 2 for the SGC model with $L$ layers are reformulated into:\n$H^{(l)} = \\hat{A}H^{(l-1)}W^{(l)}, \\quad f_\\theta(X, A) = A X W,$\nwhere $A = \\hat{A}^L$ and $W = \\prod_{l=1}^L W^{(l)}$ The output logits of a neighboring node $j$ with the perturbation $\\epsilon_{ij}$ on the node $i$ can be described as in Equation (13), where $a_{kj}$ is an entry of $A$ that represents the propagation weight between the nodes $k$ and $j$, $N_j = N_i \\cup \\{j\\}$, and $[W]_c$ represents the column of $W$ corresponding to the label $c$.\n$f^\\theta_j(X + \\hat{\\epsilon}_{ij}, A)_c =  \\sum_{k \\in N_j} a_{kj}X_k [W]_c + a_{ij}\\epsilon_{ij} [W]_c$\nTherefore, by substituting the $f^\\theta_j(X + \\hat{\\epsilon}_{ij}, A)$ in Equation (12) with the expression in Equation (13), we effectively convert the original nonlinear optimization problem that aims to change the predicted label of the node $j$ from $\\hat{c}$ to $c$ into an LP problem:\n$\\rho(c, \\hat{c}) := \\max_{\\epsilon_{ij} \\in \\mathcal{P}_i}  \\sum_{k \\in N_j} a_{kj}X_k([W]_c - [W]_{\\hat{c}}) + a_{ij}\\epsilon_{ij} [W]_c$"}, {"title": "4.2.1 Solving of The LP Problem", "content": "In Equation (4), the feasible region $\\mathcal{P}_i$ is a conjunction of $lb < X_i + \\epsilon_i < ub$ and $|\\epsilon_i|_0 < B_f$. The domain $lb \\leq X_i + \\epsilon_i \\leq ub$ is essentially a box constraint. Let $\\epsilon_{i,d}$ denote the $d$th entry of $X_i$, then, we have the range of $\\hat{\\epsilon}_{ij,d}$ to be,\n$lb_d - X_{i,d} \\leq \\hat{\\epsilon}_{ij,d} \\leq ub_d - X_{i,d}.$\nand the domain $|\\epsilon_i|_0 \\leq B_f$ specifies the maximal number of features to perturb. We first expand the objective function into\n$\\rho(c, \\hat{c}) := \\max_{\\epsilon_{ij} \\in \\mathcal{P}_i}  \\sum_{d=1}^D a_{ij} \\epsilon_{i,d} ([W]_c - [W]_{\\hat{c}}) +  \\sum_{k \\in N_j} \\sum_{d=1}^D a_{kj} X_{k,d} ([W]_{c,d} - [W]_{\\hat{c},d});$\nwhere $\\omega_d = a_{ij}([W]_c - [W]_{\\hat{c}})$ and $[W]_{c,d}$ is the entry of $W$. We can notice that $\\omega_d$ is the coefficient, $\\epsilon_{i,d}$ is the decision variable, and the item after the addition sign is a constant. When $\\omega_d$ is positive and negative, we have\n$\\omega_d(lb_d - X_{i,d}) \\leq \\omega_d \\epsilon_{ij,d} \\leq \\omega_d(ub_d - X_{i,d}),$\n$\\omega_d(ub_d - X_{i,d}) \\leq \\omega_d \\epsilon_{ij,d} \\leq \\omega_d(lb_d - X_{i,d}),$"}, {"title": "4.2.2 Construction of Final Perturbation", "content": "The aforementioned computation of the optimal perturbation on the node $i$ for each of its neighboring nodes yield a set of perturbation denoted as $\\Delta X_i$ in Equation (7). How to craft the final perturbation for $i$ that maximizes its overall adversarial influence is critical. Empirically, we observe that in our attack the selected features and their perturbations of each $\\hat{\\epsilon}_{ij}$ in $\\Delta X_i$ share a lot of similarity. Therefore, we utilize the most common features and their perturbations in $\\Delta X_i$ to form the final perturbation. This crafting process is efficient and its effectiveness is also validated in our experiments."}, {"title": "4.2.3 The Black-box GNN Attack Procedure", "content": "In a nutshell, we operate GAIM for maximizing adversarial influence on nodes by combining target node selection with optimized feature perturbations in a black-box setup. Initially, candidate nodes are identified by filtering out those with degrees exceeding the threshold (Equation (3)). For each candidate, we compute influence scores and feature perturbations using the method from Section 4. Notably, a neighboring node's use in one candidate's influence score computation precludes its reuse for others, maximizing overall impact as in Equation (5). Subsequently, we select the most impactful candidate nodes with perturbations to deploy the modified graph for testing."}, {"title": "5 Experimental Results and Analysis", "content": "This section presents a comprehensive evaluation of our proposed attack strategy (GAIM) against several typical GNN models, and compare its effectiveness with the baseline methods. Additionally, we conduct the parameter analysis to study their impacts on the attack performance, and ablation study to investigate the importance of different components in our design."}, {"title": "5.1 Experimental Setup", "content": "Datasets and GNN models. In this study, we use node classification tasks to assess the attack capability of GAIM on five benchmark datasets: Cora, Citeseer, Pubmed [33], one online image network Flickr [34] and Reddit [35]. A summary of the basic properties of these datasets is provided in Table 1. In all of our experiments, we randomly split each dataset at the ratio of 3:1:1 for training, validation, and testing. For each attack setting, we run 20 trials and gather the average as our results. We evaluate our attack strategy on several commonly-used GNN models, including: (1) GCN [2], (2) JK-NetMaxpool [31], and (3) GAT [32]. We set the number of layers to 2 for all models and set the number of heads to 8 for GAT. The complete experimental result is available at our supplementary material.\nBaselines We compare our strategy with two distinct groups of methods for fair comparison. The first group contains the popular heuristic-based metrics commonly used to measure the informativeness of nodes in few-shot learning or Active Learning problems [36, 37]. These methods are typically employed as selection criteria to identify representative nodes in graph learning, based on the intuition that they have a greater impact on other nodes in the message-passing scheme.\nIn this study, we use three well-known baselines from this group, namely Degree, PageRank, and Betweenness. Additionally, we include Random selection as a trivial baseline. In the second group, we adopt the methods in the related works [16, 15] to compare with. In these methods, RWCS and GC-RWCS [16] derive adversarial attacks by approximately maximizing the cross-entropy classification loss using heuristics, while InfMax-Unif and InfMax-Norm [15] model the problem of maximizing model misclassification rate as an influence maximization problem on a variant of linear threshold model. For this group of baselines, we replicated the parameter setup as described in the original works.\nNotably, all the baseline strategies solely focus on target node selections and neglect the perturbation construction on node features. This emphasizes the uniqueness of our design. In constructing the perturbation vector in these baseline methods, we adopt the approach outlined in the paper [15]. They train 20 GCN models as proxy models and compute the average gradients of the classification loss with respect to the node features. Then, the features with the top gradients"}, {"title": "5.2 Method Evaluation", "content": "In this part, we evaluate the performance of our method under different settings. In practical adversarial scenarios, ensuring the imperceptibility of the attack is a crucial design requirement [8]. To achieve feasibility and practicality, we impose three essential constraints on our attack strategy. (1) We set the maximum number of target nodes to be only 1% of the total graph size for the datasets Cora, Citeseer and Pubmed, and 0.1% for the larger datasets Flickr and Reddit. (2)We perform perturbations exclusively on nodes with low degrees., as they are typically more accessible in real-world scenarios. In contrast, high-degree nodes are unlikely to have their properties altered, rendering them unrealistic targets. We restrict our selection of candidate nodes to be the set after removing top 10% and 30% of nodes with the highest degree; (3) The feature modification rate of the target nodes is set as 2% for Cora, Citeseer and Pubmed, and 5% for Flickr and Reddit. These controlled rates ensure that the alterations to the nodes' features remain subtle and inconspicuous while still exerting significant influence. The features of Flickr and Reddit are actually embedding vectors which don't have specific ranges. Therefore, we set their upper bound and lower bound with the global maximum and minimum value over all features in the data, respectively."}, {"title": "5.2.1 Comparison results under untargeted attack settings.", "content": "In this section, we evaluate our proposed attack method against baseline techniques in a general untargeted attack setting. The objective was to validate the effectiveness of our approach, and to this end, we collected attack results for three models across five datasets. The evaluation results are summarized in Table 2 and 3, which presents the results with removing top 10% nodes with highest degree. For the results pertaining to the removal of 30% of nodes are detailed in our supplementary material. In the table, the label None denotes the regular GNN with no attacks applied. As the results illustrate, our attack strategy significantly diminishes the classification accuracy of diverse GNN models across all the experimented datasets. We managed to induce an average reduction in accuracy of 21.0% on the Cora dataset, 21.7% on Citeseer, 13.0% on Pubmed, 8.7% on Flickr, and a striking 39.2% on Reddit. We think the substantial performance degradation on Reddit could be attributed to the wide range of feature boundaries, allowing for large perturbations.\nOur method consistently outperformed most of the baseline methods for all attacking settings. Particularly, our approach exhibited superior performance compared to others when applied to the JKNetMax model and GCN model. It is important to note that baseline methods (RWCS, GC-RWCS, InfMax-Unif, InfMax-Norm) face challenges when scaling to large graph datasets and are restricted in their applicability to black-box scenarios. In contrast, our proposed method is not encumbered by such constraints and can be proficiently deployed across various scenarios. Remarkably, As shown in Table 3, our method outperforms the best-performing baseline across the three models with two large datasets. This solid empirical performance demonstrates the efficacy of our attack methodology and affirms its potential to hinder the reliability of GNN models across a spectrum of datasets."}, {"title": "5.2.2 Evaluation results of Label-oriented attacks.", "content": "In this section, we present the evaluation results for two types of label-oriented attack settings. We conduct the experiments on JKNetMax, GCN, and GAT models across all datasets. The first type involves degrading the model's performance specifically on the specified label. Hyperparameters keep the same as before. During testing, we measure both the accuracy of the attacking label and the overall performance of the model across all labels, recording the outcomes for analysis. We display the top three labels which are mostly impacted by attack in Figure 2 Type-I attack. The results show that our method significantly reduces the classification accuracy of the targeted labels. Remarkably, our technique manages to degrade the targeted classification of these three models by an average of 69.4% for Cora, 35.9% for Citeseer, 41.6% for Pubmed, 20.4% for Flickr and and 92.6% for Reddit. The substantial drops in Reddit is mainly due to the large range of feature values which enables strong attacks. Overall, these results validate our approach's capability of strategically compromising the model's predictions for specific labels, highlighting its utility for label-oriented attacks\nThe second type of label-oriented attack seeks to induce the model into misclassifying nodes as the targeted label. We compute the misclassification rate for each label during testing, representing the proportion of nodes successfully"}, {"title": "5.3 Parameter Analysis", "content": "In this section, we investigate the impact of the two hyperparameters, namely the node budget $B_n$ and the feature budget $B_f$, on the performance of our method. Notely, our approach does not require additional hyperparameters. The node budget represents the percentage of target nodes related to the graph size, while the feature budget is the percentage of the maximum allowed modified features in each node.\nTo ensure a comprehensive analysis and fair comparison, we divide the investigation into two parts based on the sizes of different datasets. When we study the impact of different node budget, we vary the node budget $B_n$ over the values $\\{1\\%, 2\\%, 3\\%, 4\\%, 5\\%\\}$ for smaller graph datasets (Cora, Citeseer, Pubmed), with the feature budget $B_f$ remaining constant at 2%. For the larger graph datasets (Flickr and Reddit), $B_n$ is varied across $\\{0.1\\%, 0.2\\%, 0.3\\%, 0.4\\%, 0.5\\%\\}$, with the $B_f$ fixed at 5%. As we analyze the influence of different feature budgets $B_f$, we adjust $B_f$ to span"}, {"title": "5.4 Analysis of Computational Complexity", "content": "In our method, the influence computation of one node is an LP problem. If its time complexity is denoted as $O(lp)$, the overall complexity of our algorithm will be $O(MBK \\cdot lp)$, where M is the number of nodes, K is the number of labels, and B is the averaged number of one's neighboring nodes. The inclusion of K is because of the perturbation consistency in Equation 9. It is noteworthy that our LP problem only includes decision variables with independent range constraints. This results in a significantly reduced complexity of $O(lp)$. The solving process is shown in Section 4.2.1.\nThe baseline methods RWCS, GC-RWCS, InfMax-Unif and InfMax-Norm are mainly based on Random Walk method. Their complexity is $O(ML)$ which means the number of elementary matrix-multiplication operation. Here, M is the number of nodes and L stands for L-step random walk. It is challenging to intuitively compare our time complexity with theirs. In practice, the complexity of these methods causes scalability issues when applied to large datasets like Flickr and Reddit 3. This implicitly indicates that our method has a lower complexity compared to these related works. Therefore, our method is more scalable."}, {"title": "5.5 Ablation Study", "content": "In this section, we undertake an ablation study to investigate the individual contributions of our different key components to the overall attack performance. This helps validate the rationale behind our design. We develop two distinct reassembly mechanisms to further substantiate the importance of two key parts of our method. To do this, we arrange three types of workflows: (1) GAIMglobal_perturb.: Instead of our customized perturbation for each node, we establish a global feature perturbation for all target nodes, analogous to the baselines. (2) GAIMinconsistency: In this version, we do not insist on adhering to the perturbation consistency when evaluating the adversarial influence score of a candidate node. Contrarily, the misclassified labels of neighboring nodes, represented by $\\hat{y}^\\theta_j(X + e_i, A)$, can be diverse and random. (3) GAIMoriginal: This represents the complete design of our method. We employ the three citation graph as the representative examples for evaluating these different settings. All three models JKNetMax, GCN and GAT are considered. The results are in Table 4.\nWe can see that these two pivotal components within our attack play crucial roles. Employing a global perturbation instead of our customized perturbation exposes weakness in the attack, resulting in a drop of success attack rate by 7.41%. This discrepancy stands as a primary factor contributing to our method's performance over other state-of-the-art works. Similarly, the absence of our perturbation consistency mechanism in the alternative method also reveals weakness in its attack, resulting in a success attack rate 4.12% lower than our original method. This validates our claim that impulsively merging perturbations $\\hat{\\epsilon}_{ij}$, which cause different misclassifications of node i's neighboring nodes, can compromise the effectiveness of the final perturbation on the node i. It highlights the importance of perturbation consistency in our attack. Overall, we can conclude that both our perturbation customization and consistency contribute to our attack's outstanding efficacy."}, {"title": "6 Conclusion", "content": "In this study, we propose GAIM, a practical graph node-level attack designed for the restricted black-box setting. By strategically selecting target nodes and perturbing their features to maximize adversarial influence, our integrated approach achieves significant impact while targeting only a small number of low-degree nodes within the graph. Additionally, our method easily extends to two types of label-oriented attacks, showcasing superior performance in general untargeted and label-oriented attack scenarios. The adaptability and effectiveness of GAIM highlight its potential to disrupt the accuracy and integrity of graph neural network models under various adversarial conditions. Its practical nature, coupled with its ability to excel in different attack scenarios, positions GAIM as a promising tool for advancing research in graph adversarial attacks and enhancing the security of graph-based machine learning systems. As future work, we aim to derive extensions of our strategy to other network architectures like Transformers."}]}