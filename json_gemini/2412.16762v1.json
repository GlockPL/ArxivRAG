{"title": "A Method for the Runtime Validation of AI-based Environment Perception in Automated Driving Systems*", "authors": ["Iqra Aslam", "Abhishek Buragohain", "Daniel Bamal", "Adina Aniculaesei", "Meng Zhang", "Andreas Rausch"], "abstract": "Environment perception is a fundamental part of the dynamic driving task executed by Autonomous Driving Systems (ADS). Artificial Intelligence (AI)-based approaches have prevailed over classical techniques for realizing the environment perception. Current safety-relevant standards for automotive systems, International Organization for Standardization (ISO) 26262 and ISO 21448, assume the existence of comprehensive requirements specifications. These specifications serve as the basis on which the functionality of an automotive system can be rigorously tested and checked for compliance with safety regulations. However, AI-based perception systems do not have complete requirements specification. Instead, large datasets are used to train AI-based perception systems. This paper presents a function monitor for the functional runtime monitoring of a two-folded AI-based environment perception for ADS, based respectively on camera and LiDAR sensors. To evaluate the applicability of the function monitor, we conduct a qualitative scenario-based evaluation in a controlled laboratory environment using a model car. The evaluation results then are discussed to provide insights into the monitor's performance and its suitability for real-world applications.", "sections": [{"title": "I. INTRODUCTION", "content": "In principle, fully autonomous vehicles are technically feasible. However, after the initial proof-of-concept testing under ideal conditions, e.g., in lab environments [1], or on restricted test fields [2], further innovative verification and validation techniques are needed during the approval and release processes. These additional verification and validation phases are necessary to gather the required evidence for the safety and reliability of the autonomous vehicle in real-world scenarios.\nFor the commercial approval of autonomous vehicles by certification bodies, the current state-of-the-art practices require verifying specific maneuvers using predefined test scenarios and statistically analyzing real-time data covering millions of kilometers of driving. For autonomous vehicles at Society of Automotive Engineers (SAE) Level 3/4 and above, the key challenge lies in ensuring the safe commercial release and the safe vehicle operation in all possible situations, not just only in those situations encountered during the system development, e.g., through random tests.\nToday's automated driving systems are primarily designed to be fail-safe systems, capable of switching the ego-vehicle to a safe state, e.g., by activating an emergency brake. However, future ADSs must be designed as fail-operational systems, especially as there are many situations in which an immediate fail-state might not be readily accessible, e.g., when driving at high speed on the highway. Moreover, in case issues occur during the vehicle operation, the control over the dynamic driving task can no longer be simply handed back to the human driver, since human intervention is not mandated anymore at SAE L4+ [3]. Without a human fallback system, the ADS must be able to take over control and establish a safe state for the vehicle, if a problematic situation arises.\nIn recent years, a high-level functional architecture has been established for ADSs comprising three main subsystems: (1) environment and self-perception, (2) situation comprehension and action planning, and (3) trajectory planning and motion control [4], [5]. The environment and self-perception is particularly significant as it strongly impacts the performance of the entire ADS and the safety of the autonomous vehicle, as shown by a series of accidents involving (partially) automated driving functions, e.g., Tesla's autopilot. In the first notable accident in 2016, the Tesla's autopilot has failed to detect an articulated lorry driving in the opposite direction which was engaged in a turn maneuver, despite having been successfully tested over 200 million kilometers. In the context of a brightly lit sky, both the driver and the autopilot were unable to recognize the lorry, which had white sides [6]. In response to this accident, Tesla announced the introduction of Shadow Mode to enhance the safety of its autopilot [7]. An approach for continuous monitoring of autonomous driving functions for development, validation and series operation has been proposed in in [8] and demonstrated for the lane changing functionality of the high-way pilot in [9]. This approach essentially extends the concept of shadow mode, by addressing two questions: (1) does the autonomous driving function operate correctly (qualitative oracle) and (2) is the autonomous driving function currently operating in a known environment (quantitative oracle) [9].\nThere is a noticeable gap in research work regarding the validation of environment and self-perception methods applied in automated driving applications. AI-based approaches have prevailed over classical approaches for realizing environment perception, as the former are used both in image processing and in signal processing of other raw sensor data, such as radar. Current safety-relevant standards for automotive systems, e.g., ISO 26262 [10] and ISO 21448 [11], assume the existence of"}, {"title": "II. RELATED WORK", "content": "Environment- and self-perception is an integral part of the dynamic driving task that is carried out by the ADS. Primarily based on AI models, it provides essential inputs for further safety-critical functions of the ADS, such as Situation Comprehension and Trajectory Planing. Through the interpretation of various raw sensor data into detailed semantic information about the surrounding driving environment, the AI-based environment perception subsystem enables the ADS to complete its decision making, motion planning and control command execution, by relying on its respective planning and decision components.\nFor this reason, monitoring and evaluating the functional behavior and the performance of AI-based environment perception systems at runtime is extremely important for the safety of the autonomous vehicle. Recently, diverse research approaches have addressed the safety issues of AI-based environment perception. Czarnecki [14] identifies a set of influencing factors for AI-based environment perception: (1) conceptual uncertainty, (2) development situation and scenario coverage, (3) situation or scenario uncertainty, (4) sensor properties, (5) labeling uncertainty, (6) model uncertainty, and (7) operational domain uncertainty. Identifying these factors is understood as the first step, which should be followed by a systematic analysis of their impact on the perceptual uncertainty. In addition, methods to eliminate or reduce their negative effects on the perceptual uncertainty. Subsequently, mitigation measures are proposed to be applied for the cases when the control of the negative effects is not possible [14]. The concept in [14] revolves around using these methods to gather the essential evidence to substantiate claims regard-ing the environment perception uncertainty in safety cases that contribute to demonstrating the safety of the overall autonomous vehicle [14].\nAnother survey presented in [15] identifies various research directions concerning the runtime performance monitoring of AI-based perception in autonomous robots. One direction encompasses approaches using past examples of failures and successes or similarity of operational context to previous experiences to predict the quality of perception output [15]. The second direction involves by methods that detect inconsis-tencies in perception output, using the input data from a single sensor or from multiple sensors, or by comparing outputs from different models [15]. The third direction focuses on methods for uncertainty estimation by indicating low confidence in prediction output, calculating confidence scores as a measure of the target model's output quality, and detecting anomalies [15]. Several other studies also aim to validate the accuracy or robustness of the perception system outputs concerning specific inputs of the neural network. However, the approaches surveyed in [16] and [17] are primarily limited to offline verification.\nIn addition to estimating uncertainty, some recent studies have shifted their primary focus to object detection as the main task of the perception system under analysis. Thus, Feng et al. [18] evaluate LiDAR-based object detectors using the Jaccard Intersection over Union (IoU) metric and KITTI and Waymo datasets, incorporating label uncertainty specifically for the bounding boxes of a particular object class, e.g., the object class car. In [19], the authors propose a framework to predict performance monitoring of object detection at runtime without relying on any ground-truth data. Meanwhile, in [20], they monitor the performance of the object detector deployed on mobile robots by predicting the quality of its mean average precision using a sliding window technique over the input frames. However, the approach presented in [21] for monitoring of neural networks that estimate 3D human shapes and poses from images is limited to human pose estimation."}, {"title": "III. INTEGRATED SAFETY ARCHITECTURE WITH DEPENDABILITY CAGE FOR THE RUNTIME VALIDATION OF AI-BASED ENVIRONMENT PERCEPTION IN AUTOMATED DRIVING SYSTEMS", "content": "This section introduces the Dependability Cage approach for the runtime monitoring and validation of AI-based environment perception in automated driving systems. Figure 1 il-lustrates the high-level architecture of this dependability cage, that can be understood as an instantiation of the overarching concept of the Dependability Cage approach, presented first in [22]. The subsequent sections offer a detailed introduction to the architecture of the function monitor, derived on the basis of the Dependability Cage approach.\nIn the initial position paper, the Dependability Cage ap-proach was introduced to address three main challenges in the development of autonomous systems: (1) guaranteeing the safe behavior of an autonomous system in an unknown and uncertain environment, (2) ensuring the safe behavior for all safety-critical system components, including machine-learning based components, even when deviations are detected in their behavior during system operation time, and (3) guaranteeing and improving the relevance and completeness of test cases for the validation of the system under test [22].\nTo tackle these challenges, the paper in [22] proposes an iterative development process consisting of three primary stages. The first stage is Dependability Cages Engineering and Training in System Development, in which the dependability cages are engineered in parallel to the autonomous system and tested using simulation tests or tests in a restricted and controlled lab environment [22]. In the second stage, Runtime System Observation and Resilience System Stabilization, the dependability cages are used to monitor the system behavior during its operation and record any deviation in the system behavior compared to the test results obtained during system development as well as any novel situations that occur in the environment [22]. In the third stage, Monitored Data Analysis and Goal-Oriented System Evolution for Dependability Im-provement, the observations logged during system operation are leveraged to improve the development artifacts during the subsequent system development cycle [22]. The deployment of the dependability cages on the actual system during its operation is facilitated by a modular platform architecture used for the seamless development and operation of the system, monitor and system environment [22]. The end goal of this iterative development process is to continuously improve the system's quality in terms of its dependability requirements. For further details on the development process and its phases, the reader is referred to [22]. The safety architecture proposed for autonomous systems in [22] draws its inspiration from the second phase of the iterative development process, Run-time System Observation and Resilience System Stabilization. This phase is carried out through a continuous monitoring framework that observes the behavior of the overall system, as shown in [22], [23], and [2]. In this paper, we refine the focus of this monitoring framework and tailoring it to analyze the environment perception subsystem of an ADS, rather then the ADS as a whole. The monitoring framework consists of two types of monitors, a function monitor and a situation monitor. Additionally, it involves a component responsible for defining the fail-operational reaction of the ADS based on the results of these two runtime monitors, as depicted in Figure 1.\nThe responsibility of the situation monitor - denoted as the quantitative monitor in the original position paper [22] - is to evaluate the input abstract situations used in the environment and self-perception subsystem of the ADS. During system operation, the situation monitor assesses if the input situations encountered by the ADS align with those considered during the development phase of the environment perception. As the research in this paper does not focus on the situation monitor, the reader is referred to [24] and [25] for a more detailed description of its concept and functionality.\nOn the other hand, the function monitor denoted as the qualitative monitor in the original position paper [22] - evaluates if there is a critical deviation in the behavior of the environment and self-perception subsystem during the opera-tion of the ADS. The function monitor consists of an abstract"}, {"title": "B. Dependability Cage for AI-based Environment Perception in the ADS's Integrated Safety Architecture", "content": "The dependability cage for AI-based environment perception is integrated in the three layered safety architecture developed for ADSs. Beginning from the top, the first layer is represented by the Remote Command Control Center (CCC), where the sensors data stream is visualized along with the results provided by the components in the layers below [23], [1]. The remote CCC has been previously showcased in a separate work [1] and is therefore not the primary focus of the current paper.\nThe research focus of this paper lies in the middle layer and the bottom layer of the integrated safety architecture. The middle layer encompasses the dependability cage for the AI-based environment perception of the ADS. The de-pendability cage consists of two main components: (1) the function monitor, responsible for observing and analyzing the environment perception system during the ADS's operation and (2) the fail-operational reaction component, which triggers a fail-operational reaction of the ADS based on the results of the function monitor. The third layer represents the re-configurable modular autonomous driving system [26], which draws inspiration from previous work [4], [5], and [27], and consists of three main subsystems: (1) Environment and Self-Perception, (2) Situation Comprehension and Action Decision, and (3) Trajectory Planning and Vehicle Control.\nThe environment and self-perception subsystem consists of two components, AI-based Camera Perception and AI-based LiDAR Perception. These components use camera and respectively LiDAR sensor data to detect objects in the AV's environment. They implement safety-critical machine-learned functions for the ADS's operation. Each component produces an object list, denoted as Camera AI-based (CAI) object list and LiDAR AI-based (LAI) object list. The object lists are used by the other components in the architectural pipeline of ADS. Several pieces of information are provided for each object in the two object lists: (1) object's class, (2) the object's dimensions, height and width, (3) object's distance from the ego-vehicle, (4) sensing timestamp, and (5) confidence level of detection. Figure 2 provides an overview of the integrated safety architecture with a focus on the function monitor and the environment and self-perception system.\nThe function monitor observes the behavior of the envi-ronment and self-perception subsystem against to a specified safety requirement. The safety requirement is informally for-mulated in a controlled natural language as follows:\nSafety Requirement (Informal Specification). The environment and self-perception system must always con-sistently detect the objects located inside the au-tonomous vehicle's region of interest using at least two different sensor data sources.\nThis safety requirement mandates two aspects: first, that a ROI is computed around the AV, and second, that the objects detected by the perception components in the vehicle's region of interest are consistent with each other. The function monitor consists of two components, which allow it to check this safety requirement during the ADS's operation, denoted as Safe Zone and AI Perception Validator.\n1) Safe Zone: This component utilizes various parameters of the AV, e.g., current speed, steering angle, physical dimen-"}, {"title": "Algorithm: AI Perception Validator Algorithm", "content": "1 Input: caiObjList - CAI object list; laiObjList - LAI object list, roi - vehicle's ROI\n2 Output: valid - a boolean flag\n3 cameraObjList \u2190 FilterObjectList(caiObjList, roi);\n4 lidarObjList \u2190 FilterObjectList(laiObjList, roi);\n5 valid - true;\n6 removed - false;\n7 if cameraObjList and lidarObjList are empty then\n8 return valid\n9 foreach lidarObj in lidarObjList do\n10 if timestamp of lidarObj is older than the timeout limit then\n11 if lidarObj is not matched then\n12 | valid \u2190 false;\n13 remove lidarObj from lidarObjList;\n14 removed - true;\n16 foreach cameraObj in cameraObjList do\n17 if timestamp of cameraObj is older than the timeout limit then\n18 if cameraObj is not matched then\n19 | valid \u2190 false;\n20 remove cameraObj from cameraObjList;\n21 removed - true;\n23 foreach lidarObj in lidarObjList do\n24 foreach cameraObj in cameraObjList do\n25 if difference between lidarObj attributes and cameraObj attributes are under the threshold values then\n26 | mark lidarObj and cameraObj as matched;\n27 if removed is true then\n28 return valid\nobject lists are forwarded to the remote CCC for visualization (see Section IV). Additionally, the result computed by the AI Perception Validator serves as an input in the component Mode Control. In case of inconsistency between the two object lists, this component is responsible for triggering a fail-operational reaction of the AV, by gracefully degrading the ADS functionality. This process is similar to approach outlined in [23]. However, since the primary research focus of this paper is the function monitor, the definition, implementation and evaluation of the corresponding fail-operational reaction will be addressed in future work."}, {"title": "IV. EVALUATION AND DISCUSSION OF RESULTS", "content": "This section presents the evaluation of the function monitor for the runtime validation of AI-based environment perception in ADS. The first part of this section introduces a detailed overview of the setup, including both hardware and software details (see Section IV-A). Subsequently, in Section IV-B, various test scenarios and several working hypotheses are defined. A qualitative scenario-based evaluation is conducted to assess the defined hypotheses and the obtained results are discussed.\n1) Physical Hardware Platform and Test Track: For the evaluation of the function moniotr concept, a model vehicle on the scale of 1:8, developed by Digitalwerk [29], is chosen as the physical hardware platform. The model vehicle is equipped with a wide range of sensors, including a mono camera, a LiDAR sensor, wheel speed sensors, ultrasonic sensors, a Global Positioning System (GPS) sensor, and an Inertial Measurement Unit (IMU).\nTo enhance its environmental perception capabilities for the validation of the function monitor, further sensors have been installed on the model vehicle, e.g., an Intel RealSense LiDAR camera (L515) [30] and a stereo vision camera (D435f) [31]. The LiDAR camera provides sensor input data for the LiDAR AI-based perception component. Although a high-resolution 3D LiDAR sensor would have been ideal, the model vehicle's limited power supply led to the deployment of a LiDAR camera with lower power requirements as a good compromise solution, which still provides adequate data output. Both sensors have been calibrated based on the vehicle's rear axle, to ensure that generated object lists are in the same coordinate system, specifically in the vehicle coordinate system. This alignment is crucial for an accurate and coherent comparison of redundant perception systems. Figure 4 illustrates the model car with the LiDAR camera and stereo vision camera mounted on it.\nThe test track utilized for the evaluation was constructed in the lab environment using modular martial arts mats. Each black mat measures 1m x 1m and is adorned with street markings and track walls [32]. Figure 5 depicts the model vehicle placed on the test track along with other objects that emulate other traffic participants, such as a pedestrian represented by a wooden human dummy, and elements of the road infrastructure, e.g., traffic light.\n2) Implementation Details: The function monitor conducts a consistency comparison between two object lists, a CAI object list and a LAI object list. These two lists are generated by respective AI-based perception components, one based on camera input and the other on LiDAR input. Both perception components apply YOLO Nano 2D object detectors [33], which yield 2D bounding boxes with object class names and and their respective confidence scores, but lack the distance information between the ego-vehicle and the corresponding objects. By leveraging the LiDAR point cloud provided by the LiDAR camera, we computed the distance between the model vehicle and the objects, referred to as depth, thereby producing 2.5D bounding boxes. The 2.5D bounding boxes differ from the 3D bounding boxes in that the latter include all three dimensions, i.e., height, width and length of the bounding box.\nThe outputs of the environment and self-perception subsys-tem along with the result of the function monitor are visualized in the GUI of the remote CCC (see Section III). Figure 7 depicts the visualization of the function monitor result in the Car Selection panel of the remote CCC, utilizing a flag called Al perception Validator. The flag's color indicates different results of the function monitor: (1) green - denotes consistency between the two object lists, (2) red signifies inconsistency, and (3) black - denotes data not being received by the AI Perception Validator component in the function monitor.\nIn the center of Figure 7, two panels display the view of the LiDAR camera (upper panel) and the stereo vision camera (lower panel) with the bounding boxes corresponding to each object list highlighted in green on their respective sensor view. Additionally, the Sensor Visualization panel depicts the ROI computed around the model vehicle and comprising a focus zone, marked in orange, and a clear zone, marked in green, as introduced in Section III-B. The implementation of each component is based on the decentralized middleware ROS2, facilitating the communication between the ROS2 components through the publish-subscribe pattern. This solution provides advantageous features such as self-adaptation and component reconfiguration at runtime, aligning with the distributed nature of the AV, where components are distributed across different electronic control units (ECUs) [34]. Furthermore, the real-time capabilities of ROS2 make it appropriate for ensuring the safety and reliability of the ADS.\nTo evaluate the function monitor concept, we conducted a qualitative scenario-based assessment. We defined several test scenarios along with working hypotheses to guide the evaluation. The test scenarios range from simple to complex, starting with a single static object and gradually increasing the"}, {"title": "B. Definition of Test Scenarios and Research Hypotheses", "content": "scenario complexity, by incorporating multiple static objects in a static environment. Each test scenario includes a description of the physical actions of the model vehicle and its environment. Following three test scenarios were defined evaluating the function monitor:\nTest Scenario 1 (TS 1). The model vehicle is stationary on the test track and a pedestrian (represented by a wooden human dummy) is placed in front of the model vehicle, outside of its ROI. The pedestrian is placed in such a way that it is detected by the LiDAR camera, but not by the stereo vision camera.\nTest Scenario 2 (TS 2). The model vehicle is stationary on the test track and a pedestrian (represented by a wooden human dummy) is placed in front of the model vehicle, inside of its ROI. The pedestrian is positioned in such a way that it is detected by the LiDAR camera, but not by the stereo vision camera.\nTest Scenario 3 (TS 3). The model vehicle is stationary on the test track and a traffic light is placed in front of the model vehicle, inside of its ROI. The traffic light is positioned so that both the LiDAR camera and the stereo vision camera are able to detect it.\nIn addition to the test scenarion, several research hypotheses are formulated in this paper to assess the expected performance of the function monitor. The function monitor has been evalu-ated in the defined test scenarios with respect to the following two hypotheses:\nHypothesis 1 (H1). The function monitor accurately identifies that the CAI object list and the LAI object list are consistent with each other.\nHypothesis 2 (H2). The function monitor accurately identifies that the CAI object list and the LAI object list are not consistent with each other."}, {"title": "C. Discussion of Results", "content": "The evaluation results of the function monitor on hypotheses H1 and H2 in all the test scenarios defined for the evaluation are presented in Table I. In TS 1, in which the wooden dummy is placed in front of the vehicle and outside of its ROI, the LiDAR camera can detect it but the stereo vision camera cannot. In this scenario, the AI Perception Validator gives the result consistent since the wooden dummy is outside of the model vehicle's ROI, and thus, both processed object lists are empty. Therefore, in TS 1, hypothesis H1 is true and hypothesis H2 is false. The CAI object list and the LAI object list along with the flag of the AI Perception Validator are visually depicted in Figure 6, in which the AI Perception Validator flag shows a green status, indicating \u201cconsistent object lists\u201d.\nIn TS 2, the wooden human dummy is placed again in front of the vehicle, but this time inside its ROI. The human dummy is placed so that it is detected by the LiDAR camera but not by the stereo vision camera. In this scenario, the AI Perception Validator returns inconsistent, since there is at least an inconsistent object in either the CAI object list or the LAI object list, which in this case is the human dummy. Thus, in TS 2, hypothesis H1 is false and hypothesis H2 is true. The CAI object list and the LAI object list along with the flag of the AI Perception Validator are shown in Figure 7. The AI Perception Validator flag shows a red status, indicating \"inconsistent object lists\" since the objects are inside the vehicle's ROI but not aligned with each other. In TS 3, a traffic light is positioned in front of the model vehicle inside its ROI, so that both the LiDAR camera and the stereo vision camera can detect it. In this scenario, the AI Perception Validator returns consistent, since the objects are inside the ROI and were detected by both sensors. Thus, in TS 3, hypothesis H1 is true and hypothesis H2 is false. The CAI object list and the LAI object list along with the flag of the AI Perception Validator are shown in Figure 8, in which the AI Perception"}, {"title": "V. CONCLUSION", "content": "This paper outlines a method for the runtime monitoring and validation of AI-based environment perception systems employed in autonomous driving contexts. It builds upon the Dependability Cage approach, initially proposed in [22], with a specific focus on the environment and self-perception sub-system in an ADS. The environment perception consists of two redundant perception components tasked with object detection in the ego-vehicle surrounding environment, which leverage multiple sensor data sources, e.g., LiDAR and camera. The dependability cage for the environment perception comprises a function monitor and a fail-operational reaction component. The function monitor checks at runtime whether the outputs of the two perception components remain consistent. Meanwhile, the fail-operational reaction component dictates the fail-safe or the fail-operational reaction of the ADS based on the feedback of the function monitor. This study was primarily focused on the function monitor, which was evaluated qualitatively using predefined test scenarios and a model car in a lab environment. The results of the evaluation demonstrated that the function monitor works as expected.\nThe test scenarios employed in the evaluation focused on relatively simple driving situations, with stationary objects and a stationary model car. However, in future work, we plan to enhance and extend the functionality of the function monitor so that it covers more complex scenarios, with both dynamic and static obstacles. Moreover, we intend to define a method for defining appropriate fail-operational reactions to gracefully degrade the ADS's functionality [35] in response to warning signals given out by the function monitor. Lastly, we plan to integrate the function monitor with the concept of situation monitor, similar to the one presented in [25]. Such integration enables the ADS to be aware of new object classes detected in its environment, thus enhancing its capability to handle novel environment situations. Ultimately, this integration will contribute to the safety and reliability of autonomous driving systems in diverse and challenging real-world scenarios."}]}