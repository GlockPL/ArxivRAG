{"title": "GRAPHROUTER: A GRAPH-BASED ROUTER FOR LLM SELECTIONS", "authors": ["Tao Feng", "Yanzhen Shen", "Jiaxuan You"], "abstract": "The rapidly growing number and variety of Large Language Models (LLMs) present significant challenges in efficiently selecting the appropriate LLM for a given query, especially considering the trade-offs between performance and computational cost. Current LLM selection methods often struggle to generalize across new LLMs and different tasks because of their limited ability to leverage contextual interactions among tasks, queries, and LLMs, as well as their dependence on a transductive learning framework. To address these shortcomings, we introduce a novel inductive graph framework, named as GraphRouter, which fully utilizes the contextual information among tasks, queries, and LLMs to enhance the LLM selection process. GraphRouter constructs a heterogeneous graph comprising task, query, and LLM nodes, with interactions represented as edges, which efficiently captures the contextual information between the query's requirements and the LLM's capabilities. Through an innovative edge prediction mechanism, GraphRouter is able to predict attributes (the effect and cost of LLM response) of potential edges, allowing for optimized recommendations that adapt to both existing and newly introduced LLMs without requiring retraining. Comprehensive experiments across three distinct effect-cost weight scenarios have shown that GraphRouter substantially surpasses existing routers, delivering a minimum performance improvement of 12.3%. In addition, it achieves enhanced generalization across new LLMs settings and supports diverse tasks with at least a 9.5% boost in effect and a significant reduction in computational demands. This work endeavors to apply a graph-based approach for the contextual and adaptive selection of LLMs, offering insights for real-world applications.", "sections": [{"title": "1 INTRODUCTION", "content": "The field of Large Language Models (LLMs) is advancing quickly, offering an increasingly diverse range of models that vary in size, functionality, and computational demands. Although larger models tend to deliver better performance, their high computational costs make them inefficient for many less complex tasks. Additionally, LLMs demonstrate varied performance across different types of queries and tasks, especially with the development of domain-specific LLMs. These challenges make it difficult to recommend the optimal LLM services to users that strike the balance between performance and cost for their specific needs. Therefore, our paper aims to raise attention to this pressing research question: Given the vast and continuously evolving landscape of LLMs, how to recommend appropriate LLMs for various user queries with different implied tasks?\nExisting researchers have proposed to develop a router to assign a specific LLM to each user query. Hybrid LLM trains a binary score router function to determine whether to select a small LLM or a large LLM for a specific query. Although it balances cost and performance, it is limited to just two LLMs, which falls short of addressing the real-world demand for a wide range of LLMs. Some other studies have introduced more advanced router models to address the challenge of selecting among a limited set of LLMs (typically 3 to 5). More specifically, FrugalGPT proposes a router model based on BERT to determine whether to switch to a larger LLM or not, and C2MAB-V constructs a bandit-based router to balance between the exploration and exploitation when choosing LLM for the user. However, they are still limited in the following aspects: 1) Relying solely on basic BERT-based embeddings to distinguish queries, and on names or indices to distinguish LLMs, they fail to fully leverage the contextual information from the interaction between the task, query, and LLM. This makes it challenging to achieve a router with strong generalization capabilities. 2) These methods rely on a transductive learning framework, which makes them ill-suited for real-world applications where new LLMs are frequently introduced. When new LLMs are presented, these approaches require retraining with few-shot interaction data before they can be used a process that is impractical for recommending LLMs to a large number of users in real-time; 3) They train a dedicated router for each specific task, greatly increases the computational overhead and complexity in real-world applications when multiple tasks are present.\nTo address these challenges, we introduce GraphRouter, a graph-based router for LLM selection. GraphRouter utilizes an inductive graph framework to effectively leverage contextual information, allowing it to generalize to new LLMs and adapt to diverse tasks. Specifically, to fully utilize the contextual information for different queries and tasks, GraphRouter constructs a heterogeneous graph that contains three types of nodes: task node, query node and LLM node. The interaction information between them is represented as edges in a graph. For instance, the reward (including performance and cost) of an LLM responding to a query is modeled as an edge between the query node and the LLM node. Then, we are able to transform the task of predicting the cost and performance of an LLM-query pair to an edge prediction task. After forecasting the properties of the edges, we recommend the most suitable LLM to the user based on their preferences for performance and cost. In addition, in real-world scenarios, new LLMs are frequently developed, so an effective framework should also have the ability to accommodate these evolving models. In order to make GraphRouter generalizable to new LLMs, we make efforts in two key aspects. For the input, we utilize a generative LLM such as GPT-40 to generate a descriptive text for each LLM, outlining key details such as its strengths, token pricing, and context length. Based on this, we derive an initial embedding for each LLM using a moderate-size pre-trained language model (e,g, BERT). This approach offers an advantage over directly using one-hot encoding, as it enables us to generate inductive and more informative initial embeddings for new LLMs. For the GraphRouter model, we further develop a heterogeneous GNN that aggregates information from neighboring nodes of different types; given few-shot data, we verified that a trained GraphRouter can generalize to new LLM nodes without retraining.\nIn summary, our main contributions are as follows:\n\u2022 To the best of our knowledge, we are the first work to build router for LLM selections from the graph perspective, which gives new insight to graph-enhanced LLM research.\n\u2022 We propose an inductive graph framework that fully leverages contextual information among tasks, queries, and LLMs, enabling it to generalize to new LLMs and adapt to a variety of tasks without retraining.\n\u2022 In three experiment settings with different performance and cost tradeoffs, GraphRouter outperformed the baseline models by at least 12.3%. Furthermore, in scenarios where new LLMs are introduced in the testing data, our method not only saves significant training time but also improves performance by at least 9.5% compared to the baselines."}, {"title": "2 GRAPHROUTER: GRAPH-BASED ROUTER FOR LLM SELECTION", "content": "We introduce the LLM selection problem in this section. As shown in the left part of Figure 1, the process involves multiple steps, with the router being the most critical component. The router first receives the user query containing task information. Its goal is to choose an appropriate LLM based on the incoming information in the user query to ensure optimal performance and minimal cost (LLM API cost). After calculation, the router chooses a suitable $LLM_n$ to answer the user query. Finally, the response is returned to the user with its performance and cost. Such an interaction process generates rich contextual data, which contains information on tasks, queries, selected LLM, response, performance, and cost. We organize the data in a table, as shown on the right."}, {"title": "2.1 PRELIMINARIES", "content": null}, {"title": "2.2 MOTIVATING EXAMPLES", "content": "Traditional LLM selection methods often only use ID or name information to model LLM information, which does not effectively utilize the contextual information (introduced in Sec 2.1) generated from the interaction between LLM and query. Here, we use some examples to illustrate the importance of contextual information. (1) We first argue that contextual information is important because it captures the variance in the ability of different LLMs to respond to diverse queries. We can first observe from Figure 3 that the performance of different LLMs in response to queries can differ significantly. Therefore, understanding the performance patterns of how LLMs handle queries is crucial for LLM selection, and these patterns are embedded in the contextual information between the queries and the LLMs. In addition, we can also observe that smaller LLMs sometimes outperform larger LLMs on certain queries. Even if we have unlimited budgets and can blindly rely on the largest LLM at a high cost, we still may not achieve optimal performance. This also emphasizes the importance of capturing the varying capabilities of LLMs in handling queries based on contextual information. (2) We also claim that the importance of contextual information lies in its ability to capture the differences in how a single LLM responds to queries across different tasks. Through Figures 3 and 4, we can observe that certain LLMs exhibit significant differences in their performance across two different tasks, such as Mixtral-8x7B . These two examples indicate that the performance may vary greatly on different LLMs and tasks. This suggests that in addition to understanding the capabilities of LLMs, the router must also understand the differences and similarities of each task. However, as these critical attributes of LLMs and tasks could not be sufficiently represented by their names or IDs, an effective use of the contextual information that encompasses the interaction between task, query, and LLM is needed."}, {"title": "2.3 GRAPHROUTER FRAMEWORK", "content": "Method Overview. As shown in Figure 5, GraphRouter first transforms the interaction data among tasks, queries, and LLMs into a graph. Specifically, as shown in the right side of Figure 5, we model tasks, queries, and LLMs in the left table as task nodes, query nodes, and LLM nodes, while the relationships derived from the interaction data are represented by edge features. We apply GNN to embed the node and edge features and use them for training and testing.\nInitialize node/edge features. As shown in Figure 5, we have three types of nodes (task node $h_t^{(1)}$, query node $h_q^{(1)}$, and LLM node $h_m^{(1)}$) and two types of edges (task-query edge $w_{tq}$ and LLM-query edge $w_{mq}$).\nGiven the inherent differences of task, query, and LLMs, during the initialization of their nodes, we adopt different strategies. For the initialization of task nodes, we utilize an additional LLM, such as GPT-40, to generate descriptions of the tasks, and then encode the description to obtain its embedding $e_t$. More specifically, we take the average output token embedding after feeding the description into the moderate-size pre-trained language model (PLM), such as BERT. The initialization of query nodes is also obtained by embedding the query $e_q$ through the same PLM. As for the initialization of LLM nodes, the traditional approach often initializes directly using the name or ID of the LLM, which not only limits its ability to generalize to new LLMs but also omits important background information. Here, we still adopt a prompt-based approach. We design prompts for an LLM to describe the capabilities of each LLM. In addition, we also add information about the cost of each LLM after the description. Then, similar to how we obtain the task's embedding, we use the same PLM to compute the initial embeddings $e_l$ of the different LLMs. All the descriptions we generate for different tasks and LLMs can be found in Appendix A.\nAs for the task-query edges, we assign a value of 1 for their initialization. For the initialization of LLM-query edges, we jointly consider the performance and cost information in the interaction data, and assign the concatenation of performance and cost as their initial features.\nPredict via a heterogeneous GNN. We implement the predictive model $f_{\\theta}$ over task nodes, query nodes, and LLM nodes using a heterogeneous GNN, as shown in Figure 5. For aggregating different types of nodes and edges, we use heterogeneous aggregation with different learnable weights. The objective of the GNN is to learn expressive node embeddings $h$ through an iterative weighted aggregation of the local network neighborhoods. The $l$-th iteration of the GraphConv(\u00b7), or the node embeddings update of the $l$-th layer, is represented as:\n$h_t^{(l)} = U_t^{(l)} CONCAT(MEAN({RELU(W_{tq}^{(l)} h_q^{(l-1)}), q \\in N(t)}), h_t^{(l-1)}),$\n$h_q^{(l)} = U_q^{(l)} CONCAT(MEAN({RELU(W_{t->q}^{(l)} W_{1[t\\in V_t, m\\in V_t]}^{(l)}h_t^{(l-1)}), u \\in N(v)}), h_q^{(l-1)}),$\n$h_m^{(l)} = U_m^{(l)} CONCAT(MEAN({RELU(w_{mq}W_{1[t\\in V_t, m\\in V_t]}^{(l)} h_q^{(l-1)}), q \\in N(m)}), h_m^{(l-1)}),$\nwhere $h_t^{(l)}$ is the node embedding after $l$ iterations, $h_t^{(0)}, h_q^{(0)}, h_m^{(0)}$ have been initialized as $e_t, e_q, e_m$ respectively as explained above, and $N(v)$ denotes the direct neighbors of $v$. $1[v \\in V_a, u \\in V_b]$ indicates the message type (whether from task to query, or LLM to query), and $U^{(l)}, W^{(l)}$ are learnable parameters. In addition, $W_{1[t\\in V_t, m\\in V_t]}^{(l)}$ indicates that different edge types correspond to different edge features; specifically, if it is from task to query, it is represented as $w_{tq}$, and from LLM to query, it is represented as $w_{mq}$.\nFollowing the update of the task, query, and LLM node embeddings, we obtain the query-task combined embedding as $h_{qt} = MLP(CONCAT(h_t^{(l)}, h_q^{(l)}))$. We model the LLM selection problem as an edge prediction problem and generate training data in the following way. We first determine the best LLM for each query in the training set based on the performance (best reward described in Sec 3.4) achieved by different LLMs and then set the edge labels of the query to other LLMs to 0 and the edge label of the query to the best LLM to 1. As such, LLM prediction can be made through EdgePred() in the form of $\\hat{y}_{logits} = MEAN (D(DOT(h_{qt}^{(l)},h_m^{(l)}))$. We have summarized the detailed training process of GraphRouter in Algorithm 1, whose details are shown as follows. In addition, in the testing of GraphRouter, we identify the LLM node that has maximum edge logits with the query node as the best LLM, which can be computed as\n$y = arg \\max_m (EdgePred(\\hat{h}_{qt}, h_m)).$\nGraphRouter for new LLMs setting. Traditional routers cannot generalize to new LLMs directly under few-shots settings, as they require retraining through interactions with the query for each new LLM. This is inadequate for keeping up with the rapidly evolving changes in LLMs in the real world. To test our framework and baselines under this real-world setting, following , we construct an auxiliary dataset with the interaction data of the new LLMs on queries sampled uniformly from the training set. This auxiliary dataset is not involved in the training process but serves as a few-shots during the testing phase."}, {"title": "3 EXPERIMENTAL SETUP", "content": "We select data from four different types of tasks, whose statistics are summarized in Table 2."}, {"title": "3.1 DATASETS AND LLM DESCRIPTIONS", "content": null}, {"title": "3.2 DATA PREPROCESSING AND SPLITTING", "content": "Given the above dataset and LLMs, we construct a multi-task interaction dataset described in Sec 2.1. Specifically, we combine all the datasets of four tasks together first. For each query, we utilize ten LLMs in Sec 3.1 to answer it and obtain the corresponding response. Then the response is compared with its ground truth to get its performance using the metric of each task described in Table 2. Furthermore, the cost is calculated with the number of input tokens and output tokens and the cost of different LLMs in Table 3. Here we utilize GPT-2 as in to calculate the number of tokens.\nAfter obtaining the multi-task interaction dataset, we split the dataset according to different experimental settings. We mainly have two major settings. The first is the standard setting, where all LLMs are visible in both the training and test sets, with some new queries appearing in the test set. The data is divided into training, validation, and test sets in a ratio of 70%: 10%: 20%, based on different queries. In the case of new LLM setting, we assume that the first six LLMs in Table 3 are observable, while the remaining four are new LLMs. Therefore, based on the standard setting, we first remove data related to the latter four LLMs from the training and validation sets, while keeping the test set as it is in the standard setting. Furthermore, following , we construct an auxiliary dataset with the interaction data of the four new LLMs on 80 queries sampled uniformly from the training set. This auxiliary dataset is not involved in the training process but serves as a few-shots during the testing phase."}, {"title": "3.3 BASELINE METHODS", "content": "We compare our GRAPHROUTER model with the following baselines. We first compare GraphRouter with two rule-based baselines:\n\u2022 Largest LLM always selects the largest LLM available.\n\u2022 Smallest LLM always selects the smallest LLM available.\nThen we compare GraphRouter with a prompt-based baseline:\n\u2022 Prompt LLM incorporates the query, candidate models, and objectives (e.g., prioritizing effectiveness) directly into the prompt, and feeds it into an external LLM (e.g., GPT-4) to select the most suitable LLM from a pool of candidates.\nFurther, GraphRouter is compared with three representative routers for LLM selection:\n\u2022 Hybrid LLM, when given a small LLM and a large LLM, trains a pre-trained language model to assign queries to the small or large model. We use LLaMA-2 (7b) and Llama-3.1-Turbo (70b) as our small and large LLM respectively, as they are the smallest and the largest LLM available. We also replace DeBERTa with RoBERTa as the pre-trained language model and observe better performance.\n\u2022 FrugalGPT utilizes a pre-trained language model to predict the score of the generation result of all LLMs given a query, and then selects the LLM with the highest score within a given cost. We also use ROBERTa as the router's backbone model.\n\u2022 C2MAB-V uses a bandit-based model for LLM selection, which regards each LLM as an arm and implements an exploration mechanism to search for a better solution.\nFinally, we set up a gold baseline as the optimal solution for LLM selection. The purpose of setting up the baseline is to see how far GraphRouter is from the optimal solution.\n\u2022 Oracle defines the theoretical upper bound of the reward, where each query has been routed to the optimal LLM via oracle information."}, {"title": "3.4 METRICS", "content": "We utilize three metrics to evaluate the performance of GraphRouter and baselines.\n\u2022 Performance is to evaluate the average quality of the responses across different queries given by each method, which is introduced in Sec 3.2 and Table 2.\n\u2022 Cost evaluates the average LLM inference cost when responding to the queries, which is described in Sec 3.2.\n\u2022 Reward is used to measure how well a method balances performance and cost. Different users may have varying levels of emphasis on performance and cost. Therefore, we define three scenarios: Performance First, Balance, and Cost First, to correspond to situations where users prioritize high performance, value both high performance and low cost equally, or prioritize low cost, respectively. Specifically, to eliminate the influence of scale, we first normalize both performance and cost. Then, we define the score as $Reward = \\alpha \\cdot Performance - \\beta \\cdot Cost$. For the three scenarios, we set the values of $\\alpha$ and $\\beta$ to (1, 0), (0.5, 0.5), and (0.2, 0.8), respectively."}, {"title": "3.5 IMPLEMENTATION DETAILS", "content": "In the training stage, we set the graph neural network as a two-layer graph attention network, with a 32-dim hidden dimension. The batch size is 32, and the max training epoch is set to 1000. We use Adam optimizer for model training and gradually decay the learning rate from le-3 to 0 with LambdaLR scheduler. We implement our proposed method using PyTorch and PyG, and all the experiments are conducted on a single NVIDIA A100 Tensor Core GPU. As for LLMs, we rely on API calling from Together AI to obtain responses."}, {"title": "4 EXPERIMENTAL RESULTS", "content": null}, {"title": "4.1 COMPARISON WITH EXISTING BASELINES.", "content": "As shown in Table 4, we compare GraphRouter with seven baselines in three scenarios. We can observe that GraphRouter consistently and substantially surpasses existing routers, delivering a minimum effect improvement of 12.28% on metric Reward compared to the strongest baselines. Additionally, we observe that GraphRouter achieves at least 88.89% of the optimal solution (Table 4, row Oracle), further demonstrating the superiority of our framework. On the other hand, compared with the two rule-based LLM, GraphRouter achieves a better trade-off between Performance and Cost, therefore achieving a higher effect on Reward. Analyzing the effect of Prompt LLM, Hybrid LLM , and FrugalGPT , we demonstrate that without sufficient contextualized information, even LLM and trained moderate-size LM struggle to understand the query and candidates LLM effectively, even if we ignore their high inference costs. These comparisons and results validate our claim that effective usage of contextual information is crucial for selecting the optimal LLM."}, {"title": "4.2 GENERALIZATION ABILITY TO NEW LLMS", "content": "To validate the generalization ability of GraphRouter when facing new LLMs, we conduct experiments as described in Sec 3.2 in scenario Balance. To compare with other baselines, we add the auxiliary dataset into their training dataset. Specifically, we compare GraphRouter (few-shots) with HybridLLM, FrugalGPT, C2MAB-V, and GraphRouter (trained) on Reward and time cost (training time + inference time). We report our results in Table 5. We can observe that in comparison to the most costly C2MAB-V, GraphRouter (few-shots) not only achieves substantial performance improvements in Reward by almost 10% but also greatly reduces Time Cost by over 99%. The amount of Reward Improvement and Time Cost Reduction has significantly surpassed those of other baselines. Additionally, compared to GraphRouter (trained), our approach significantly reduces time cost with only a slight performance loss. These observations demonstrate that GraphRouter is both effective and efficient in generalizing to new LLMs."}, {"title": "4.3 ABLATION STUDIES", "content": "How does GraphRouter perform with varying sizes of GNN? The size of a GNN is an important factor to consider when designing GNN algorithms. It not only affects the performance of the GNN but also introduces additional computational overhead if the size is too large. To find an optimal GNN size for GraphRouter, we explored sizes ranging from 16 to 80, as shown in Figure 6. As depicted in the figure, the Reward of GraphRouter initially improves as the size increases, reaching its peak at a size of 32, after which it starts to decline.\nWhat is the impact of different numbers of GNN layers on GraphRouter's effectiveness? The number of GNN layers has a significant impact on the expressiveness of the GNN. A shallow GNN struggles to learn deep contextual information, whereas an overly deep GNN can lead to issues such as over smoothing and overfitting. Moreover, increasing the number of layers also raises the computational cost. To determine the optimal number of GNN layers for GraphRouter, we conduct an exploration with the number of layers ranging from 0 to 5, as shown in Figure 7. As depicted in the figure, the Reward of GraphRouter initially improves with more layers but then declines, achieving its highest Reward when the number of layers is 2."}, {"title": "5 ADDITIONAL RELATED WORKS", "content": "LLM selection. With the scaling of the number of parameters in LLMs, their inference cost is also rapidly increasing. To optimize inference cost, several works have proposed using model switching to mitigate this issue. When given a small and a large LLM, fine-tune a pre-trained language model as the model router to predict whether using a small LLM is sufficient. HybridLLM improves on this by transforming training data to compensate for the influence of imbalanced labels. Other approaches move beyond choosing between two LLMs and introduce settings where multiple LLMs are present. train the router to predict the reliability score given a query and an LLM index. generalize the router's training objective to accommodate the scenario where additional cost or performance constraints exist. examine the effectiveness of lighter routers built upon the k-nearest neighbors algorithm or Multilayer Perceptrons. C2MAB-V employs a bandit-based model for LLM selection, treating each LLM as an arm and incorporating an exploration mechanism to find an improved solution. Different from previous approaches, where the router only learns from query-model interaction, our GRAPHROUTER fully utilizes the information in the training data by jointly modeling the query-model, query-query, and model-model relationship. This allows us to learn effective representations for tasks, queries, and models, enabling better generalizability.\nGraph for modeling relationships. Graphs have demonstrated great potential in modeling complex relationships. Solving relational data with graphs often involves extracting nodes and edges from the data, and then modeling their relationships with embeddings. Traditional graph algorithms, such as label propagation, directly utilize edge relationships to propagate known labels to target nodes. With the advancement of deep learning, graph neural networks (GNNs) have become the more popular approach for researchers to model relationships within data. They are also found to have widespread application in fields such as recommendation systems and social networks . GNNs can be broadly classified into Message Passing Neural Networks (MPNNs), which include models like GCN , GraphSAGE , and GAT , as well as non-MPNN architectures Additionally, Heterogeneous Graph Neural Networks (HeterGNNs) have also been proposed to handle more complex graph data. In recent years, researchers have begun exploring the zero-shot or few-shot capabilities of GNNs , aiming to address more complex real-world challenges, such as the cold start problem in recommendation systems. Building on these studies, we incorporate GNNs' powerful ability to represent contextual heterogeneous relationships and their zero-shot capabilities into the LLM selection problem."}, {"title": "6 CONCLUSION AND DISCUSSION", "content": "Conclusion. We present GraphRouter, a graph inductive framework for LLM routing during inference with multiple LLMs. This work is the first to address the LLM routing problem by reframing it as an edge prediction task between query nodes and LLM nodes. Using graph structure, we fully capture contextual information from prior interaction data to learn effective task, query, and graph representations. Through our experiments on the combined dataset from four open-domain QA datasets, and with three different application scenarios, we demonstrated the superiority of GraphRouter over competitive LLM selection baselines and showed that our framework is on par with the ideal \"God's-eye view\" solution. Beyond traditional settings, we also tested our framework in a more challenging setting where new LLMs were introduced during test time, and we demonstrated GraphRouter's strong generalization ability compared to previous baselines. We hope that GraphRouter, along with our approach of incorporating interaction data through graphs, will facilitate future research on LLM routing.\nLimitations This work primarily serves as exploratory work to validate the idea of how modeling past interaction data in a graph could enhance the process of LLM selection. We acknowledge that leveraging more complex graph signals, such as paths, or the taxonomy of LLMs (e.g., family trees like LLaMA2 \u2192 LLaMA3 \u2192 LLaMA3.1 could further improve GraphRouter's performance, but that is beyond the scope of this paper, and we leave it for future work.\nFuture Work Some other interesting questions to explore in the future include: 1) When answering complex queries, prompting methods like Chain-of-Thought , Tree-of-Thought are also widely used to enhance the reasoning ability of LLMs. Given the vast number of these methods, predicting the generation result and selecting the best one remains a great challenge. On the other hand, selecting the prompting method is similar to selecting the best LLM for inference, as we are both aiming to predict the generation result based on past interaction data. As a result, it is interesting to explore whether GraphRouter could also be adapted to this task. 2) In a multi-agent system, it is critical to choose the appropriate LLM for each module on a specific query and task. It would be valuable to conduct experiments on whether an inductive graph framework like GraphRouter could also excel on this challenging task, where multiple LLMs are being selected simultaneously."}, {"title": "A DESCRIPTION FOR TASKS AND LLMS", "content": "Using descriptions generated by LLM and embeddings derived from BERT as initial node embeddings for GNNs enhances their expressiveness and generalization capabilities of. Here, we have listed descriptions of different tasks and various LLMs obtained using GPT-40. Specifically, GPT-40 provides insights into the unique characteristics and challenges of different tasks, as well as the size, cost, and particular strengths of different LLMs. The detailed descriptions are shown in the table below."}, {"title": "Table 6: Description of Alpaca task.", "content": "The Alpaca dataset is designed for instruction-following tasks, where the model is required to generate coherent and contextually appropriate responses to given instructions or prompts. It focuses on understanding diverse user requests and providing informative and accurate outputs based on those instructions."}, {"title": "Table 7: Description of GSM8K task.", "content": "The GSM8K dataset is tailored for mathematical problem-solving tasks. It consists of natural language math problems that require the model to comprehend the problem statement, apply the correct mathematical operations, and provide the solution. The primary challenge lies in both parsing complex language and performing accurate calculations."}, {"title": "Table 8: Description of SQUAD task.", "content": "The SQUAD dataset is focused on question-answering tasks, where the model is given a passage of text and needs to extract or generate a precise answer to a question based on the content of the passage. The dataset emphasizes comprehension, retrieval of relevant information, and concise answer generation."}, {"title": "Table 9: Description of Multi-News task.", "content": "The Multi-News dataset is aimed at text summarization tasks. It contains multiple news articles on the same topic, and the model's objective is to generate a concise and comprehensive summary that integrates information from all the articles. The challenge is to distill key points while maintaining coherence and avoiding redundancy."}, {"title": "Table 10: Description of LLAMA-3 (7b).", "content": "This is a relatively small-sized model (7 billion parameters) designed for general-purpose language tasks. Its low cost per million tokens (0.2) makes it an affordable option for many applications requiring quick responses with moderate accuracy."}, {"title": "Table 11: Description of Mixtral-8x7B.", "content": "With a combined size of 56 billion parameters, this model aims to provide stronger language modeling capabilities. Its cost per million tokens is 0.6, reflecting its balance between performance and affordability for more complex tasks."}, {"title": "Table 12: Description of NousResearch (34b).", "content": "A mid-sized model with 34 billion parameters, suitable for handling moderately complex language tasks. Its cost is higher at 0.8 per million tokens, indicating a greater computational demand, likely due to its enhanced capabilities over smaller models."}, {"title": "Table 13: Description of LLaMA-2 (7b).", "content": "Compact model at 7 billion parameters, it offers similar capabilities and pricing to LLaMA-3 (7b) at a cost of 0.2 per million tokens. It's an efficient choice for tasks requiring decent performance without high computational costs."}, {"title": "Table 14: Description of Mistral-7b.", "content": "With 7 billion parameters, Mistral-7b is optimized for lightweight tasks, balancing speed and efficiency. Its cost per million tokens is 0.2, making it cost-effective for standard use cases without the need for complex computations."}, {"title": "Table 15: Description of LLAMA-3 (70b).", "content": "A larger variant of LLaMA-3, this model has 70 billion parameters, providing advanced cap\u0440\u0430-bilities for complex tasks. Its cost per million tokens is 0.9, indicating its higher computational demand and enhanced performance."}, {"title": "Table 16: Description of LLaMA-3-Turbo (8b).", "content": "A variant optimized for speed and efficiency with 8 billion parameters. Its cost per million tokens is only 0.2, suggesting that it is designed to handle tasks quickly while being highly cost-effective."}, {"title": "Table 17: Description of LLaMA-3-Turbo (70b).", "content": "This model, at 70 billion parameters, is tailored for high performance with an emphasis on efficiency. The cost is 0.9 per million tokens, reflecting its advanced capabilities for a broad range of tasks requiring more computation."}, {"title": "Table 18: Description of Llama-3.1-Turbo (70b).", "content": "Large model with 70 billion parameters, likely to offer strong capabilities for various language tasks. Its cost is also 0.9 per million tokens, suggesting similar performance and computational needs as other 70b models."}, {"title": "Table 19: Description of Qwen-1.5 (72b).", "content": "With 72 billion parameters, Qwen-1.5 is among the largest models in the list, designed for high-complexity tasks. Its cost per million tokens is 0.9, making it comparable to other high-performance models in terms of both capability and expense."}]}