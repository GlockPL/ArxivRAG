{"title": "DeTrigger: A Gradient-Centric Approach to Backdoor Attack Mitigation in Federated Learning", "authors": ["Kichang Lee", "Yujin Shin", "Jun Han", "Jonghyuk Yun", "JeongGil Ko"], "abstract": "Federated Learning (FL) enables collaborative model training across distributed devices while preserving local data privacy, making it ideal for mobile and embedded systems. However, the decentralized nature of FL also opens vulnerabilities to model poisoning attacks, particularly backdoor attacks, where adversaries implant trigger patterns to manipulate model predictions. In this paper, we propose DeTrigger, a scalable and efficient backdoor-robust federated learning framework that leverages insights from adversarial attack methodologies. By employing gradient analysis with temperature scaling, DeTrigger detects and isolates backdoor triggers, allowing for precise model weight pruning of backdoor activations without sacrificing benign model knowledge. Extensive evaluations across four widely used datasets demonstrate that DeTrigger achieves up to 251\u00d7 faster detection than traditional methods and mitigates backdoor attacks by up to 98.9%, with minimal impact on global model accuracy. Our findings establish DeTrigger as a robust and scalable solution to protect federated learning environments against sophisticated backdoor threats.", "sections": [{"title": "INTRODUCTION", "content": "Federated Learning (FL) is a decentralized machine learning approach that trains a global model by aggregating locally trained models from mobile and embedded devices [31, 40]. This method leverages distributed data and computational resources, reducing the dependency on centralized processing [35, 36, 52]. Federated learning powers mobile applications, such as sensor data analysis [33, 34, 39], autonomous vehicle [29, 38], and real-time computer vision [1, 10, 32, 54], by using large, diverse datasets without data sharing. A key principle is preserving local data privacy, as the server aggregates updates without accessing raw data [27, 31, 45]. However, this also means the server cannot verify updates, making federated learning vulnerable to model-poisoning attacks from malicious clients [2, 6, 13]."}, {"title": "BACKGROUND AND RELATED WORK", "content": "This section provides background on backdoor attacks in federated learning and an overview of adversarial attacks, emphasizing their similarities, differences, and implications for designing a backdoor-robust framework."}, {"title": "Backdoor Attacks", "content": "A backdoor attack manipulates a model \\(f_{back}(\\cdot)\\) to produce a designated incorrect output \\(Y_{back}\\) for input \\(X_{t}\\) with a trigger \\(T\\) is provided while maintaining normal behavior \\(Y\\) on unaltered data \\(X\\). This can be formulated as \\(X_{t} = (1 \u2013 M)X + MT\\), \\(Y = f_{back}(X)\\), \\(Y_{back} = f_{back}(X_{t})\\) [23]. Such attacks are typically executed by training the model with manipulated data that incorporates the trigger pattern. Gu et al. [17]"}, {"title": "Adversarial Attacks", "content": "Adversarial attacks represent a substantial security threat to the robustness of deep neural networks. These attacks are designed to intentionally manipulate the model's output by introducing small, carefully crafted perturbations to the input data. Formally, this can be expressed as \\(X_{adv} = X + \\epsilon \\cdot N\\), \\(Y = F(X) \\neq F(X_{adv})\\). In this formulation, \\(X_{adv}\\) is the adversarial example, created by adding a small perturbation \\(\\epsilon N\\) to the original input \\(X\\), where \\(N\\) is the perturbation direction and \\(\\epsilon\\) controls its magnitude. The goal is to turn the model's correct prediction \\((Y = F(X))\\) into an incorrect one \\((Y \\neq F(X_{adv}))\\) while keeping \\(\\epsilon\\) minimal. Research on adversarial attacks focuses on identifying effective perturbations that mislead the model with minimal distortion. For example, Szegedy et al. introduced adversarial examples via L-BFGS optimization [44], and Goodfellow et al. later proposed the Fast Gradient Sign Method (FGSM), which efficiently generates adversarial examples by using gradient signs [16]. These subtle changes, often imperceptible to humans, expose critical weaknesses in neural network robustness [7]."}, {"title": "Relationship Between Backdoor and Adversarial Attacks", "content": "Both backdoor and adversarial attacks share the overarching goal of inducing incorrect or undesired outcomes in the model. They achieve this by manipulating input data, either through malicious noise or trigger injection. From a high-level perspective, both attacks aim to compromise the model's reliability [49]. However, the primary distinction lies in their mechanisms and targets. Adversarial attacks concentrate on identifying a specific perturbation \\((N)\\) that forces the model \\(F(\\cdot)\\) to misclassify a given input. This process requires the attacker to dynamically compute \\(N\\) for each individual input to achieve misclassification. In contrast, backdoor attacks focus on embedding a hidden trigger pattern (denoted \\(T\\)) into the model during training. A model compromised from a backdoor attack behaves as expected on benign inputs but produces incorrect predictions when the trigger pattern is present. This makes backdoor attacks particularly covert, as they do not rely on continuous input manipulation; instead, a pre-defined condition-the presence of the trigger-activates the malicious behavior."}, {"title": "CORE IDEA AND FEASIBILITY STUDY", "content": "To counter trigger-based backdoor attacks, identifying the trigger pattern is crucial, as tampered models act benignly without it. This section highlights our defense's core idea, inspired by the relationship between backdoor and adversarial attacks, followed by a feasibility study.\nCore Idea. Backdoor and adversarial attacks share overlapping techniques and goals. Adversarial attacks identify subtle input perturbations to alter predictions, while backdoor attacks use a predefined trigger to produce similar incorrect outputs. This raises the question: \u201cCan adversarial techniques reverse-engineer the backdoor trigger?\u201d While both share commonalities, they exhibit distinct differences. As shown in Figure 2 (a), adversarial attacks compute input-specific perturbations, whereas backdoor attacks apply a consistent trigger across inputs. Additionally, adversarial attacks can target any model, while backdoor attacks affect only models trained with trigger samples.\nOur proposed framework, DeTrigger, leverages these distinctions by using adversarial perturbations to identify the"}, {"title": "FRAMEWORK DESIGN", "content": "We present the DeTrigger design, outlining inherent challenges and their corresponding solutions."}, {"title": "Overview", "content": "We introduce DeTrigger, a novel framework for enhancing backdoor robustness in federated learning inspired by principles of adversarial attack-based mitigation. Figure 3 presents an overview of DeTrigger and its operations. DeTrigger operates as follows: first, the server distributes the latest global model and selects clients for training (1). These clients then update their local models using their individual datasets (2). If a client is malicious, it may train its model with a backdoor dataset with trigger patterns embedded. After local training, clients send updated model weights back to the server (3). At this point, the server computes input layer gradients using a small validation dataset (10-1000 samples) across all labels to evaluate each client's updated model and DeTrigger preprocesses model gradients to extract potential backdoor triggers (4) (c.f., Sec.5.2). Operating under the insight that the trigger patterns can be reviled when analyzing the input layer gradients (c.f., Sec. 5.3), the server identifies suspicious model updates that may contain backdoor knowledge. When detected, DeTrigger tests suspicious models with data containing the inferred trigger. If predictions from the model change due to the trigger, DeTrigger flags the model as compromised (5) (c.f., Sec.5.3).\nTo neutralize backdoor knowledge and create a compromised-but-clean model, DeTrigger prunes malicious activations in the compromised models by closely observing how their neural network gradients respond to backdoor-embedded samples (6) (c.f., Sec. 5.4). Finally, DeTrigger aggregates both benign and \u201ccleaned\u201d malicious model updates, producing a refined global model (\u2466). The following sections detail each core operation of DeTrigger in depth."}, {"title": "THREAT MODEL AND ASSUMPTIONS", "content": "We present the threat model, namely the goal and capability of the attacker, along with the assumptions.\nThreat Model. The goal of a backdoor attacker is to manipulate the federated learning process to produce a compromised global model. To achieve this, attackers may alter training data or labels by injecting trigger patterns or tampering with data labels. Also, multiple attackers may collaborate by sharing attack information, including trigger patterns, to increase the chances of compromising the global model. Notably, attackers are limited in interfering with the federated learning process at the local device level. Specifically, they cannot manipulate server-side aggregation, model distribution, client selection, or modify the training processes of other clients.\nAssumptions. We assume that a federated learning system with numerous clients participating without directly sharing their local training data with the central server. Additionally, we assume that malicious clients comprise less than 50% of the total number of participating clients[13]. We consider these assumptions reasonable for practical federated learning environments and associated attack scenarios. In addition, we assume that the server has a few clean validation samples that can be used for additional processes."}, {"title": "Gradient Preprocessing", "content": "Trigger Extraction. Extracting backdoor trigger patterns based on gradient information presents challenges as gradients (at the input layer) inherently contain biases specific to the input data, limiting their representation of general patterns. Consequently, raw gradients include both trigger-related information and input-specific noise. The objective of gradient preprocessing is to address these challenges by isolating (and identifying) the backdoor trigger information from irrelevant details embedded in the raw gradients calculated using validation data samples. Gradient preprocessing in DeTrigger consists of four key steps: (i) input layer gradient collection (ii) filtering additive elements, (iii) normalization and thresholding, and (iv) iterative trigger pattern refinement. We illustrate these operations in Figure 4.\nTo identify and extract an accurate trigger pattern, DeTrigger starts by utilizing an uncontaminated validation dataset (covering all labels) at the server to extract the input layer gradients. Here, for each model collected from the clients, each sample in the validation dataset is passed through the model, followed by a single round of backpropagation to capture the input layer gradients. As a result, an input layer gradient is recorded for each sample-model pair across all labels in the system.\nUsing these input layer gradients, DeTrigger identifies and filters additive elements within each model's gradient that shift predictions from the original to the target attack label. As also demonstrated in our preliminary studies (Figure 2 (c)), this process extracts information within the gradients specifically linked to the trigger pattern. To minimize sample-specific noise variations, DeTrigger averages the gradient elements across multiple samples. The averaged gradients are then normalized through min-max scaling, and a mask is generated to emphasize spatial locations where gradient amplitudes exceed a set threshold. This mask allows DeTrigger to focus on regions with stronger trigger-related signals, effectively filtering out irrelevant information. In this paper, we set the threshold to 0.5.\nFinally, DeTrigger refines the trigger pattern by modifying test samples, and iteratively refines the trigger quality. Specifically, it replaces pixels within masked locations with processed gradients and recalculates adversarial perturbations anew. While adversarial attacks add perturbations to original input data, backdoor attacks replace specific pixels with the trigger pattern. This iterative process reduces input-specific biases, enhancing the clarity of the trigger pattern by adhering to the foundational concept of backdoor attacks. Through this process, DeTrigger generates potential trigger patterns for each model update and label, resulting in K \u00d7 C potential trigger patterns, where K and C represent the number of labels and updated clients, respectively.\nTemperature Scaling. Additionally, as mentioned above, gradients often include input-specific information that is not related to the backdoor trigger, which complicates their extracting process. Here, to better obtain the trigger-related information from the gradient, we introduce temperature scaling-based trigger-related information amplification during the gradient calculation operations. Note that the temperature parameter T modulates the smoothness of the probability distribution output by the softmax function [20, 26]. Higher values of T > 1 smooth the probability distribution, while values between 0 and 1 sharpen it. This smoothing can be understood as effectively moving a data sample closer to the decision boundary, we note that this can potentially enhance the focus on backdoor-relevant features.\nTo elaborate, Figure 5 (a) visualizes a backdoor model's decision boundary, illustrating a backdoor feature space lying near the decision boundary intersection within the normal data plane (red cone shape). Given the complexity and high dimensionality of neural network feature spaces, we hypothesize that benign and backdoor models share a \u201cnormal data plane\u201d where standard samples are positioned, while backdoor samples exist near, but just beyond, this plane. We hypothesize a cone shape for the backdoor feature space as the probability of a sample falling into this space increases as samples are closer to the intersection of decision boundaries in the normal data plane.\nThis insight aligns with prior findings by Su et al., who observed that the decision boundary of the backdoor sample is tangent to other labels [41]. Figures 5 (b) and (c) illustrate the impact of temperature scaling on the L1-norm and inferred trigger patterns, respectively. Specifically, the results show the L1-norm of gradients decreases with moderate temperature scaling (T > 1), improving the clarity of inferred trigger"}, {"title": "Backdoor Attack Detection Module", "content": "Given the trigger pattern extracted above, the next step is to identify backdoor-affected models among those transmitted from clients. Furthermore, we should determine the target label/class used for the attack. The operation consists of two steps, namely, Total variation-based contaminated model detection and Transferability-based verification.\nTotal variation-based contaminated model detection. In this step, DeTrigger leverages the prior knowledge that backdoor trigger patterns are typically more spatially concentrated than standard adversarial perturbations [47]. To detect the backdoor-affected models and pinpoint the target label, DeTrigger evaluates the total variation for potential trigger patterns. The total variation (TV) of an input layer gradient map (i.e., the map of gradients as in Figure 5 (c)) is defined as the sum of the absolute differences between neighboring elements along both the horizontal (\\(x_{i,.}\\)) and vertical (\\(x_{.,j}\\)) axes in the data (x). For example, given a gradient map for an input sample, we compare each element by shifting the map both horizontally and vertically by one step. The TV helps identify concentrated spatial patterns"}, {"title": "Backdoor Knowledge Pruning Module", "content": "To motivate our model pruning approach, we ask the following question: \u201cCan we simply discard malicious updates if we identify them as suspicious?\u201d To answer this, we set up a motivating experiment using the MNIST dataset with 10 clients: five benign, and five backdoor attackers. Figures 7 (a) plots the data distribution for each client, where the circle size represents the relative sample count per label. We also present model accuracy results for different data distributions in Figure 7 (b). Here, we aim to mimic a scenario where benign clients lack samples from labels 7-9, while attackers possess this data; representing a realistic setup in mobile and embedded applications where data distribution is often highly heterogeneous.\nIn our experiment, we compare two federated training approaches: FedAvg [31], which simply averages all client updates, and an Oracle, a hypothetical server with perfect attacker knowledge, which completely excludes malicious models as a whole from aggregation. As shown in Figure 7 (b), the Oracle effectively mitigates the backdoor attack, with only a 2.02% backdoor attack success rate, as malicious model updates were fully discarded. In contrast, FedAvg shows a higher backdoor attack success rate of 50.25% since it does not mitigate the attack in any way. Nevertheless, the Oracle exhibits a significant accuracy drop on data for labels 7-9 (37.53%) due to the loss of benign knowledge embedded within malicious updates, while FedAvg maintains 90.07% accuracy on these labels. These results suggest that entirely discarding malicious models can result in a severe performance loss for rare classes in heterogeneous data distributions and highlight the importance of leveraging benign knowledge within malicious models while effectively mitigating backdoor attacks, especially in mobile environments where data heterogeneity is common.\nWe tackle this issue by proposing a backdoor knowledge pruning module that only eliminates model parameters associated with backdoor triggers: preventing global model contamination while preserving beneficial knowledge in the model aggregation process. For this, DeTrigger exploits the"}, {"title": "EVALUATION", "content": "We now evaluate DeTrigger using extensive experiments with four datasets and various comparison baselines."}, {"title": "Experiment Setup", "content": "The details on the datasets and models that we use in our evaluations are presented below:\nDataset and Model. In this work, we evaluate DeTrigger using four distinct datasets and two model architectures suitable for mobile/embedded federated learning: a 2-layered CNN and ResNet18. We detail the datasets and models below.\n\u2022 CIFAR-10/CIFAR-100 [25] are widely used image benchmark datasets, each with 60,000 images at 32\u00d732 resolution, covering 10 and 100 classes, respectively. We employ a 2-layered CNN model as default [31]. Given CIFAR-100's broader label set, it serves as an ideal dataset for testing DeTrigger's adaptability to an increased number of classes.\n\u2022 GTSRB [21] is a benchmark dataset comprising 43 types of real-world traffic signs. Given the practicality and vulnerability of traffic sign recognition to backdoor attacks, this dataset enables us to assess DeTrigger under realistic backdoor scenarios with the 2-layered CNN model.\n\u2022 STL-10 [8] contains a total of 13K natural images across 10 classes, with a resolution of 96\u00d796 pixels. The higher resolution of STL-10, compared to the other datasets, makes it suitable for evaluating DeTrigger's scalability concerning image resolution. For this dataset, we utilize ResNet18 [19] to handle the increased complexity."}, {"title": "Overall Performance", "content": "We begin our evaluations by presenting the overall performance of DeTrigger to understand how well the global models in DeTrigger perform while mitigating backdoor attacks. Figure 8 plots the global model accuracy and backdoor accuracy for DeTrigger and different comparison baselines with two different datasets. The backdoor accuracy denotes the attack success rate when a trigger is present in the input, while global accuracy shows the classification performance on unaltered samples. In this context, an ideal approach would appear in the bottom-right corner of the plot, signifying low backdoor accuracy (effective attack mitigation) and high global accuracy (preserved model performance)\nAs the results show, DeTrigger consistently shows superior performance in achieving a balance between these two performance metrics. For the GTSRB dataset (Figure 8 (a)), while FedAvg demonstrates high global model accuracy, it is significantly susceptible to backdoor attacks. Similarly, methods such as FLTrust and Median slightly reduce backdoor accuracy, but these come at the cost of degraded global model performance. MultiKrum and Trimmed Mean (TM) show improvements in robustness, but their overall global model fails to reach the level of DeTrigger.\nSimilarly, with the CIFAR-10 dataset (Figure 8 (b)), DeTrigger outperforms all baselines. The baselines achieve marginal improvements in backdoor resistance but exhibit degradation in global model accuracy. In contrast, DeTrigger significantly"}, {"title": "Computational Efficiency of DeTrigger", "content": "As highlighted earlier, the time required to validate models in mitigating backdoor attacks is a critical factor as it can introduce delays in the overall federated training process. Figure 9 presents the elapsed time for a single federated learning round with 50 clients, including 25 backdoor attackers, averaged over 100 trials. Here, the performance of all comparison methods is normalized to the latency of DeTrigger, which is ~0.71 sec. The plots show that defense schemes based on safe aggregation with statistical priors, such as Median and Trimmed Mean (TM), exhibited relatively low time complexity, being 4.56\u00d7 and 2.94\u00d7 faster than DeTrigger, respectively. On the other hand, methods such as Krum, MultiKrum, and FLTrust, which compute the similarity between model updates, showed slightly higher computational costs due to the additional similarity calculations required in their schemes. While their latency is at a practically acceptable level (some even faster than DeTrigger) we show in the following evaluations that this comes at the price of failing to mitigate the backdoor attack in many cases properly.\nFurthermore, Figure 9 indicates that the optimization and training processes in TABOR and NeuralCleanse, which aim to detect backdoor models and extract triggers (similar to DeTrigger), show significantly higher computational latency, being 182.57x, and 251.46\u00d7 higher than DeTrigger, respectively. Given that the system will encounter such latency at every federated learning round, and also that this latency will increase with an increasing number of clients, we see the"}, {"title": "Impact of Backdoor Knowledge Pruning", "content": "To evaluate the effectiveness of backdoor knowledge pruning, we visualized the learned representations of both benign and backdoor models (GTSRB dataset and CNN configuration) using t-SNE [46]. As t-SNE maps data to arbitrary spaces;"}, {"title": "Performance Across Varying Dataset and Model Characteristics", "content": "Next, we evaluate DeTrigger's performance across diverse dataset-model configurations, focusing on (i) label types, (ii) input resolutions, and (iii) model characteristics.\nFigure 12 (a) presents the L1 norm between the ground truth trigger pattern and the inferred trigger across various configurations with 75 benign clients and 25 backdoor attackers total of 100 participants. To ensure fair comparisons across resolutions, the L1 norm is normalized by dividing it by the spatial dimension (H \u00d7 W), as input data dimensions affect the norm. The results here show stable inferred trigger quality across configurations, with a minor exception in the STL10-ResNet18 setting, which combines high-resolution input with a relatively complex model.\nNext, Figure 12 (b) visualizes the reverse-engineered triggers for different dataset-model configurations. Note that"}, {"title": "Scalability of DeTrigger", "content": "Finally, we evaluate DeTrigger's scalability on different backdoor trigger patterns and the increasing number of participating clients in the federated learning network."}, {"title": "Different Backdoor Trigger Patterns", "content": "Trigger patterns for backdoor attacks can vary widely. While small patterns appended to the original input are common characteristics, their visual characteristics, such as shape and color, can differ significantly. To evaluate the scalability and robustness of DeTrigger against diverse backdoor triggers, we conducted experiments using eight different trigger types, visualized at the top of Figure 13 leveraging the 2-layered CIFAR10 dataset and CNN model. As shown in the bottom of the figure, DeTrigger successfully extracts the triggers across various shapes and colors, albeit not perfectly.\nThe L1 norm between the ground truth and inferred trigger patterns, shown in Figure 14, indicates that non-continuous pixel patterns (e.g., the final three triggers in Figure 13) tend to show increased quantitative detection error. However, at a system level, Figure 15 demonstrates that even imperfect trigger extractions, when integrated into the full DeTrigger pipeline, effectively maintain a highly accurate global model while significantly suppressing backdoor attacks."}, {"title": "Number of Participating Clients", "content": "We now examine how DeTrigger scales with the increasing number of clients in its federated learning network. Here, we focus on the computational overhead of dealing with the increased number of updated models that are collected at the server. This is particularly important given that long latencies lead to increased intervals between federated learning rounds, which in turn translates to prolonged model convergence.\nOur results with the CIFAR10 dataset and 2-layered CNN model plotted in Figure 16 suggest that, as expected, the overall computation time for DeTrigger's operations increases"}, {"title": "DISCUSSION", "content": "Based on our experiences in designing and evaluating De-Trigger, we discuss the limitations of our current research and suggest directions for future work.\n\u2022 Understanding backdoor attack via gradients. In this paper, we explored the relationship between backdoor and adversarial attacks using gradient analysis. We also introduced the temperature scaling trick, offering a novel perspective on the decision boundary of backdoor models. These analyses provide valuable insights that enhance the understanding of backdoor attacks. We hope the insights presented here will serve as a foundation for further research and inspire new discussions on backdoor vulnerabilities in neural networks.\n\u2022 Exploiting advanced adversarial attacks. Our work primarily seeks to emphasize and validate the feasibility of leveraging adversarial attack concepts to mitigate backdoor attacks in federated learning. Accordingly, DeTrigger utilizes a straightforward gradient-based adversarial attack. We acknowledge that prior studies have proposed more advanced adversarial attack techniques. We believe that incorporating these approaches could further enhance the effectiveness of our defense mechanism.\n\u2022 Adaptive attack against DeTrigger. To further strengthen the federated learning framework, it is essential to explore its limitations and conduct stress testing of the defense mechanism. While our work demonstrates the effectiveness of leveraging adversarial attacks to mitigate backdoor attacks, we also consider potential adaptive attacks that could be designed to evade DeTrigger. For example, an adaptive attack might employ triggers with high total variation to bypass detection, as DeTrigger uses total variation metrics to identify backdoor updates. Importantly, the global model accuracy did not significantly degrade from the pruning process, indicating that DeTrigger can adapt to counter these attacks by expanding its detection criteria to account for a broader range of potential triggers. Furthermore, adjusting the total variation TV threshold would allow DeTrigger to maintain its effectiveness against adaptive threats."}, {"title": "CONCLUSION", "content": "In this paper, we introduced DeTrigger, a backdoor-robust federated learning framework designed to detect and mitigate backdoor attacks by leveraging adversarial attack methodologies. Through gradient analysis and temperature scaling, DeTrigger effectively isolates trigger patterns, enabling model weight pruning for the removal of backdoor activations while retaining benign knowledge within the global model. Our extensive evaluations demonstrate that DeTrigger not only achieves significant speed improvements over traditional backdoor defenses but also preserves model accuracy and mitigates attack effectiveness by up to 98.9%. Additionally, through extensive evaluations using four widely-used public datasets, we explored the scalability of DeTrigger across diverse settings, confirming its adaptability to varying model complexities, label sizes, and data resolutions. By combining efficiency with precision, DeTrigger sets a foundation for secure and scalable federated learning."}]}