{"title": "How Should I Build A Benchmark?\nRevisiting Code-Related Benchmarks For LLMs", "authors": ["Jialun Cao", "Yuk-Kit Chan", "Zixuan Ling", "Wenxuan Wang", "Shuqing Li", "Mingwei Liu", "Chaozheng Wang", "Boxi Yu", "Pinjia He", "Shuai Wang", "Zibin Zheng", "Michael R. Lyu", "Shing-Chi Cheung"], "abstract": "Various benchmarks have been proposed to as- sess the performance of large language models (LLMs) in different coding scenarios. We refer to them as code-related benchmarks. However, there are no systematic guidelines by which such a benchmark should be developed to as- sure its quality, reliability, and reproducibil- ity. We propose How2Bench comprising a 55- criteria checklist as a set of guidelines to com- prehensively govern the development of code- related benchmarks. Using HOW2BENCH, we profiled 274 benchmarks released within the past decade and found concerning issues. Nearly 70% of the benchmarks did not take measures for data quality assurance; over 10% did not even open source or only partially open source. Many highly cited benchmarks have loopholes, including duplicated samples, incor- rect reference codes/tests/prompts, and unre- moved sensitive/confidential information. Fi- nally, we conducted a human study involv- ing 49 participants and revealed significant gaps in awareness of the importance of data quality, reproducibility, and transparency. For ease of use, we provide a printable version of HOW2BENCH in Appendix E.", "sections": [{"title": "1 Introduction", "content": "Recent large language models (LLMs) have shown remarkable capabilities across various do- mains such as software development (Chen et al., 2021a), question answering (Rogers et al., 2023), and math reasoning (Imani et al., 2023). Various benchmarks (Chen et al., 2021a; Jimenez et al., 2024; Austin et al., 2021; Yue et al., 2024; Du et al., 2023a) are proposed to evaluate LLMs' effective- ness and limitations from multiple perspectives in different application scenarios.\nHowever, doubts regarding the quality, relia- bility, and transparency of various code-related benchmarks arise. For example, a recent study pointed out that \u201ccurrent programming benchmarks are inadequate for assessing the actual correctness of LLM-generated code\" (Liu et al., 2023a). Other accusations, including irreproducible (Reuel et al., 2024), closed data sources (Cao et al., 2024b), low quality (Qiu et al., 2024a; Yadav et al., 2024a), and inadequate validation measures (Liu et al., 2023a), were also raised, undermining the credibil- ity of these benchmarks and thereby their subse- quent evaluation results. This motivates the need for rigorous and thorough guidelines to govern code-related benchmark development.\nIn this paper, we introduce How2BENCH, a comprehensive guideline consisting of a 55- criteria checklist specially designed for code- related benchmarks. This checklist covers the en- tire lifecycle of benchmark development, from de- sign and construction to evaluation, analysis, and release as shown in Figure 1. It underwent multiple iterations we initiated a draft inspired by open- source software guidelines (Fogel, 2005) and clas- sical measurement theory (Suppes et al., 1962). We refined it through iterative discussions with prac- titioners, leading to the finalization of these crite- ria. HOW2BENCH emphasizes reliability, validity, open access, and reproducibility in the benchmark development, ensuring high standards and fostering a more reliable and transparent environment.\nFollowing How2BENCH, we conducted an in- depth profiling of 270+ code-related benchmarks developed over the past decade (2014 - 2024). The extent of criteria violations by the profiled bench- marks is concerning:\n\u2022 Almost 70% of the benchmarks did not take any measures for data quality assurance;"}, {"title": "2 Background", "content": null}, {"title": "2.1 Code-related Benchmarks", "content": "Benchmarks for coding tasks like code genera- tion (Chen et al., 2021a; Austin et al., 2021), de- fect detection (Just et al., 2014; Gao et al., 2023b; Liu et al., 2024c), and program repair (Jimenez et al., 2024; Risse and B\u00f6hme, 2024) are increas- ingly common, reflecting the growing needs for using LLMs for coding tasks. Recent studies have highlighted various issues with these benchmarks, ranging from design inconsistencies to scope and applicability limitations. For example, (Liu et al., 2023a) found that even some widely used bench- marks, such as HumanEval (Chen et al., 2021a) and MBPP (Austin et al., 2021), contains a non-trivial proportion of bugs in implementation, documenta- tion, and test cases. Our work, in comparison, in- troduces a detailed guideline that guides the bench- mark development during the entire lifecycle."}, {"title": "2.2 Related Studies and Surveys", "content": "Several recent surveys and empirical studies have profiled the status quo of LLM development. These studies either explore the overall performance for certain areas such as software engineering (Hou et al., 2023; Wang et al., 2024a) or investigate the capabilities of LLMs on specific tasks such as code generation (Dou et al., 2024; Yu et al., 2024) and test generation (Sch\u00e4fer et al., 2024; Yuan et al., 2024, 2023a). A survey (Chang et al., 2024) about how to evaluate LLMs was proposed to answer what/where/how to evaluate LLMs. This paper dif- fers from these studies in its purpose and perspec- tives. Unlike these benchmarks, our work proposed guidelines for future benchmark development and provided a checklist to assess the quality of these existing benchmarks.\nRecently, BetterBench (Reuel et al., 2024) is a concurrent work assessing the AI benchmarks against 46 criteria. Then, it scored 24 AI bench- marks in various domains and ranked them. Better- Bench differs from this paper in several key aspects: scope (general benchmarks vs. code-related bench- marks), lifecycle division (it addresses benchmark retirement, while How2Bench focuses on bench-"}, {"title": "3 Design", "content": null}, {"title": "3.1 The Lifecycle of Benchmark Development", "content": "Code-related benchmark development comprises five typical phases (Phase 0 - 4), as shown in Fig- ure 1, explained in detail as follows.\nPhase 0. Design. At the beginning of bench- mark development, it is vital to identify the motiva- tion, the scope and the capabilities required by the application scenario of interest.\nPhase 1. Construction. After establishing the motivation and purpose, the Benchmark Construc- tion phase moves from design to execution. Typ- ically, data is collected from public coding web- sites such as GitHub, LeetCode and StackOverflow. This is followed by preprocessing, which includes filtering, cleaning (e.g., deduplication, denoising), and curation (e.g., aligning tests with correspond- ing code). The phase usually ends with a validation process, which can be manual or automated.\nPhase 2. Evaluation. Once the benchmark is available, the next step is to apply it to LLMs, val- idating if it can effectively measure the intended LLM capabilities. Essential considering factors include selecting a representative array of LLMs, configuring settings like prompts and hyperparam- eters for consistency, choosing appropriate experi- mental environments to meet LLM requirements, and implementing thorough logging to ensure de- pendable and reproducible results.\nPhase 3. Analysis. After evaluation, experimen- tal results are analyzed, drawing conclusions on LLMs' capabilities. This phase involves compar- ing each LLM's performance to identify standout or underperforming models. Then, proper visual aids such as bar charts and tables can be used to display the experimental results, presenting clearer observation and deeper inspiration, such as the cor- relations between models, the correlations with re- lated benchmarks, or performance in upper-/down- stream tasks. Indeed, a thorough analysis helps pinpoint areas for improvement and guides future enhancements in LLM development.\nPhase 4. Release. The final phase is to make the benchmark open-accessible. This phase involves meticulously preparing all materials associated with the benchmark, ensuring they are ready for open access to foster widespread adoption and col- laboration. Clear, comprehensive documentation is provided to guide users on effectively utilizing the benchmark. Additionally, all logged exper- iment details are made available, enhancing the reproducibility and transparency of the benchmark."}, {"title": "3.2 Study Design", "content": "Our study consists of four steps (Figure 2).\nStep 1. Guideline Construction. To begin with, we sketched the initial guidelines for each phase in the benchmark development lifecycle (Section 3.1, Figure 1) by reviewing existing literature (Suppes et al., 1962; Zheng et al., 2023b; Sch\u00e4fer et al., 2024; Reuel et al., 2024) and brainstorming. After that, we refined the guidelines through a series of interviews with various stakeholders, including model developers and benchmark builders, allow- ing for the addition, deletion, or modification of criteria based on expert feedback and practical in- sights. This phase concludes with the finalization of our guidelines, HOW2BENCH. This detailed checklist consists of 55 criteria over the bench- mark lifecycle, providing effective guidelines for rigorous and reliable benchmark development.\nStep 2. Literature Profiling. This step begins by collecting related benchmarks according to their publication time, venue, and coding tasks, and then employing techniques like snowballing to ensure a comprehensive collection. This step leads to 274 code-related benchmarks for study. The de- tailed statistics can be found in the Appendix D. This step is followed by profiling each selected benchmark through a thorough review of corre- sponding papers and examination of the released artifacts or homepages associated with these bench- marks. The phase is completed by reporting statis- tics that highlight overall trends, advantages, and disadvantages identified during the profiling, pro- viding a clear and structured overview of existing benchmarks.\nStep 3. Focused Case Study. After obtaining an overall impression of existing benchmarks, we selected 30 (= 5 * 6) representative benchmarks from top-5 tasks, with top-5 highly-cited bench- marks plus the latest 1 benchmark (Appendix C). Each selected benchmark is then analyzed against HOW2BENCH, examining how well they meet the established criteria, studying their overall statistics, and identifying both exemplary and poor cases. In- sights and references from existing literature are also incorporated to enrich the analysis, providing a deeper understanding of the benchmarks' perfor- mance and areas for improvement.\nStep 4. Human Study. The final step is a human study that evaluates the importance and practicality of HOW2BENCH. This involves designing a ques- tionnaire to gather diverse, logical insights, which is then distributed to a targeted audience. After collecting and filtering responses for quality, the data is analyzed to derive insights. See Appendix B for details."}, {"title": "4 Guideline - \"How2BENCH\"", "content": "The completed guideline How2BENCH with 55 criteria can be found in Appendix E."}, {"title": "4.1 Guideline for Benchmark Design", "content": "Explanation \u2013 For benchmark design, we listed four essential criteria, as shown in Figure 3. In particular, the guideline starts by recommending that benchmarks should initially assess if they are addressing a significant gap in existing research, ensuring the relevance and necessity of the bench- mark. The scope of the benchmark is expected to be well-defined, clarifying the capabilities or characteristics being tested, how these relate to practical scenarios such as programming assistance or automated testing, and the relevance of these capabilities in real-world applications.\nKey Statistics According to our statistics among 270+ benchmarks, apparent research bias can be observed in terms of coding tasks, pro- gramming languages, and code granularities are observed (Appendix A.1). For example, as shown in Figure 11, 36.13% (99/274) are code generation"}, {"title": "4.2 Guideline for Construction", "content": "Explanation \u2013 Figure 4 shows 19 criteria for benchmark construction. Essentially, for data source, the key considerations include verifying the traceability and quality of the data source, ad- dressing potential data contamination (Sainz et al., 2023), and ensuring that the data sampling pro- cesses are scientifically robust and rigorous. Also, for data representativeness, it also guides through specific checks to ensure the benchmark's scope is strictly adhered to, such as making sure every data point falls within the targeted scope and that the data can cover all studied capabilities, domain knowledge, and application scenarios.\nFor data preprocess and cleaning, it also stresses handling code-specific aspects, such as compilability and execution, along with cleaning and manually reviewing data for quality assurance. Output validation methods and evaluation metrics must be carefully designed and reviewed to ensure they effectively measure the benchmark's goals. Lastly, it suggests considering additional evaluation perspectives, such as safety and security checks, ensuring the code does not contain sensitive infor- mation.\nKey Statistics \u2013 According to our statistics (Ap- pendix A.3), the 270+ benchmarks exhibit numer- ous irregularities in their implementation, which could significantly threaten the reliability of the benchmarks. Surprisingly, 62% of benchmarks did not deduplicate or did not mention. Near 80% benchmarks did not consider or handle data contamination threats. About 70% of the bench- marks did not go through any quality assurance checks such as manual checks and code execu- tion. In particular, we summarized the commonly- used data quality assurance metrics and their fre- quency: manual check (22.6%), code execution (2.2%), LLM check (1.5%), others (e.g. the num- ber of stars or heuristic rules, 5.8%).\nAlso, since we focus on code-related bench- marks, which usually accompany test cases, test coverage also needs to be considered. As pointed out by prior study (Liu et al., 2023a), inadequate"}, {"title": "4.3 Guideline for Evaluation", "content": "Explanation \u2013 Guidelines for benchmark evaluation focus on the rigorousness and reliabil- ity of the evaluation. How2BENCH provides 12 criteria for benchmark evaluation, as shown in Fig- ure 5. It mainly focuses on the comprehensive eval- uation processes for benchmarks involving LLMs. For evaluation design, it stresses the importance of assessing a sufficient and representative range of LLMs to ensure the benchmark's applicability across various model families and configurations, both open and closed-source. Figure 29 and Fig- ure 30 shows the distribution of numbers of LLMS studied and the most exercised LLMs.\nAlso, prompting has a direct impact on the qual- ity of the LLMs' output results (Wei et al., 2022; He et al., 2024a; Jin et al., 2024; Ye et al., 2023). As pointed out by a recent study, up to 40% perfor- mance gap could be observed in code translation when prompts vary (He et al., 2024b).\nAdditionally, the experiment environment is es- sential for reproducibility and transparency. Indeed, the hardware, software, and platform environments used during experiments might influence the out- comes (Ghosh, 2024). Furthermore, because of the nondeterministic nature of LLMs, experiments should be repeated, and randomization strategies should be used to mitigate the effects of random- ness and parameter configuration biases. Lastly, meticulously documented logs of the experimen- tal process are advised to facilitate transparency and reproducibility, detailing everything from pa- rameter settings to the specific LLM frameworks used.\nKey Statistics \u2013 Among the 274 benchmarks, 183 of them are evaluated over LLMs. Accord- ing to our statistics (Figure 29), over 34% of the benchmarks were evaluated on fewer than 3 LLMs, with 11.48% benchmarks only evaluated on one LLM. Such evaluation results can hardly be gener- alized to other LLMs. Furthermore, more than half of the benchmarks studied fewer than 6 LLMs (51% = (21 +22 + 20 + 4 + 12+15)/183).\nFor reference, we listed the top 10 most stud- ied LLM families in Figure 30. Among them, the GPT and CodeLlama series are the most exten- sively studied, accounting for 63% (116/183) and 33% (60/183), respectively. Under the constraints of time and available resources, it is beneficial to evaluate more representative LLMs.\nThe prompt quality also greatly impacts the LLM evaluation (He et al., 2024b). According to a recent study, up to 40% performance vary could be observed in code translation task (He et al., 2024b). So, carefully designing a prompt needs considera- tion. However, 73.3% representative benchmarks (Appendix C) do not validate whether the prompts they used are well-designed (Appendix A.4). Simi- larly, though 94.9% benchmarks were evaluated in a zero-shot manner, only 21.2% benchmarks were evaluated under few-shot, 8.8% under Chain-of- Thought and 2.6% under RAG (Appendix A.4).\nRegarding the evaluation process, our statistics exposed that only 35.4% of benchmark evalua- tions have been repeated (Appendix A.4). Also, regarding the transparency and matriculated doc- uments, the observation is not optimistic \u2013 Only 3.6% benchmarks provided their experiment environment. More than 50% of benchmarks did not provide reproducible instructions such"}, {"title": "4.4 Guideline for Evaluation Analysis", "content": "Explanation \u2013 We listed 10 criteria for the eval- uation analysis phase, as shown in Figure 6. Re- garding the perspectives of analysis, inspired by classic measurement theory (Suppes et al., 1962), we suggest four essential perspectives, including difficulty (whether a benchmark is appropriately challenging for LLMs), stability (whether the re- sults are consistent through repeated trials), differ- entiability (whether benchmarks can differentiate the strengths and weaknesses of various LLMs).\nMoreover, effective presentation of results us- ing clear visual and textual descriptions is recom- mended to ensure the findings are understandable and actionable. The phase concludes with the sug- gestion to interpret and explain the results compre- hensively, providing a basis for future research and application enhancements.\nKey Statistics - Because experimental analy- sis is relatively subjective and cannot be obtained through mechanical scanning, we focus on 30 rep- resentative focus benchmarks (Appendix C), cover- ing the highest cited and latest benchmarks in the five tasks (Section 3.2 Step 3)."}, {"title": "4.5 Guideline for Benchmark Release", "content": "Explanation \u2013 Finally, releasing a benchmark for open access also needs careful consideration. We offered 10 suggestions for this step, as shown in Figure 7, to highlight essential steps for public release preparation, emphasizing accessibility and ethical compliance. This includes setting an appro- priate license to clarify usage rights, conducting a thorough review to eliminate sensitive or harm- ful content such as the API keys to access LLMs, the personal emails or toxic code comments (Miller et al., 2022) unless they are a part of the benchmark, and ensuring transparency and reproducibility by making all related materials openly available. Detailed prompts and clear descriptions of the experimental setup are advised to facilitate repli- cation. Additionally, providing user manuals and evaluation interfaces is crucial for effective user engagement with the benchmark, enhancing its re- liability and value for the research community."}, {"title": "5 Human Study", "content": "To delve deeper into the integration of knowledge and action, we surveyed 49 global researchers in AI (42.6%) and SE (57.14%), as shown in Fig- ure 50. Each participant had published at least one research paper, and about half had constructed code-related benchmarks. See Appendix B.\nFirst, all participants agreed that having a checklist for benchmark construction would con- tribute to the quality of the benchmark. 47/55 cri- teria in How2BENCH are deemed important by more 80% participants. Additionally, among the 21 participants who have constructed code-related benchmarks, 53 out of 55 criteria were deemed important by all benchmark developers; only two criteria (criteria 3 and 4 in Section 4) were consid- ered unimportant by a few individuals (3 and 2 par- ticipants, respectively). Additionally, we received two valuable suggestions that draw importance to recording the time/monetary costs of constructing the benchmark and conducting the experiments.\nHowever, we also identified some notable gaps in awareness. First, regarding the data prepara- tion, more than 15% of participants were not aware that the selection of data should consider the tar- get scope of the evaluation set (i.e., the data must be representative), and 16% of participants were unaware of the need for data denoising. This over- sight can significantly affect the validity and gen- eralizability of experimental results, underscoring the importance of a comprehensive understanding of data handling for reliable research outcomes.\nSecond, regarding evaluation replicability and re- liability. Over 40% of participants believe that recording and publicizing the hardware and soft- ware environments, software versions, and libraries used in experiments is not important, with more than 20% still considering it unimportant despite already done so. This reveals a significant lack of awareness about the impact that experimental envi- ronments can have on the reliability, reproducibil- ity, and stability of evaluation results. In fact, various studies have demonstrated that different ex- perimental environments, parameters, and prompts can lead to substantial variations in outcomes (Xiao et al., 2024; Wang et al., 2019, 2023a)."}, {"title": "6 Conclusion", "content": "This paper proposes a rigorous guideline consisting of 55 checklists covering the benchmark develop- ment lifecycle. After investigating over 270 code- related benchmarks, we exposed their merits and limitations and provided suggestions for improving them. Finally, our human study reveals the neglect of details that may affect the benchmark's reliabil- ity. In the long run, HOW2BENCH helps to improve the overall quality of benchmarks in the community due to the propagation among benchmarks."}, {"title": "Limitations", "content": "This paper has two primary limitations that offer avenues for future research. First, the collection of code-related benchmarks may be incomplete. To minimize this limitation, we covered papers pub- lished over the last decade, and conducted multiple rounds of snowballing. Ultimately, we collected 274 benchmarks, which is comparable to the num- ber included in recent surveys (Hou et al., 2023; Sch\u00e4fer et al., 2024) in the field. Second, the study involved substantial manual analysis, which could lead to oversight and discrepancies in the statisti- cal results. To mitigate this issue, we ensured that each benchmark was double-checked by at least two authors and underwent multiple rounds of iter- ation. Third, the guidelines may not cover all the details. Constructing a code-related benchmark in- volves numerous details, and some criteria are task-"}, {"title": "A Statistics of studied benchmarks", "content": "In this section, we conducted a comprehensive and detailed statistical analysis of the 274 benchmarks collected.\nAnnual Trend. Regarding the coding tasks, Fig- ure 11 illustrates the distribution of various cod- ing tasks across benchmarks. It is clear that the task of Code Generation is most prevalent, with 99 benchmarks focusing on this area according to 36% (99/274) of studied benchmarks, indicating a significant interest in generating code automat- ically. Program Repair and Defeat Detection are well-represented, with 27 and 25 benchmarks, re- spectively, reflecting the importance of correcting code and detecting defects.\nCitation distribution. We also visualized the citations of 274 code-related benchmarks. The citation statistics were collected on September 1st, 2024. From Figure 10, we can see a clear long- tail trend of the citations, from the highest 2735 (HumanEval (Chen et al., 2021a)) to the lowest 0.\nCoding Task. Tasks like Code Summarization and Text2SQL are similarly significant, each with 25 and 22 benchmarks. These tasks focus on mak- ing code more understandable and converting nat- ural language queries into SQL queries. Other tasks, such as Code Retrieval, Code Reasoning, and Code Translation, are represented with 18, 17, and 16 benchmarks, respectively. Lesser-represented benchmarks are Test Generation, Code Optimiza- tion, and Code Completion, each represented by 8 and 7 benchmarks, indicating the inadequacy of these tasks.\nProgramming Languages. Figure 12 shows the distribution of benchmarks across various pro- gramming languages. The overall trend indicates a strong preference for benchmarking Python, which leads with 158 benchmarks, followed by Java and C++, with 107 and 63, respectively. The graph also reveals a diverse range of languages being used. In total, 724 programming languages are studied by these 274 benchmarks. Though some program- ming languages, such as Kotlin, Swift, and Scala, are less frequently exercised, the benchmarks in- volving them are tailored to different application needs and technology environments. This distribu- tion shows the existing benchmarks are dominated by three mainstream programming languages, leav- ing other programming languages less studied and benchmarked.\nNatural Language. Figure 13 illustrates the distribution of benchmarks for different natural lan- guages. The bar chart overwhelmingly shows that English is the dominant language, with 192 bench- marks highlighting its ubiquity in research and development. Other languages have significantly fewer benchmarks, with six for Chinese and only two each for Japanese, Russian, and Spanish. The category labeled \u201cOther\u201d includes 20 benchmarks spread across other natural languages, indicating some diversity but limited attention to non-English benchmarks. This distribution highlights the promi- nence of English in the global research community and also demonstrates the uneven representation of natural languages in the studied benchmarks.\nModals in the benchmarks. Figure 14 presents the distribution of benchmarks according to the type of language used in their prompts. The chart shows that the majority, at 47.1%, of the benchmarks use a combination of natural language and programming Language, followed by PL only (31.0%) and NL only (21.9%).\nGranularity. The code snippet in a code-related benchmark varies from statement-level (i.e., one line of code. For example, CoNaLa (Yin et al., 2018) and Math-QA (Amini et al., 2019)), function- level (i.e., a function unit of code. For example, Hu- manEval (Chen et al., 2021a) and MBPP (Austin et al., 2021)), class-level (i.e., a class with mul- tiple function units of code. For example, Clas- sEval (Du et al., 2023b)) and project-level (i.\u0435., a project with multiple classes or modules. For exam- ple, DevEval (Li et al., 2024a) and JavaBench (Cao et al., 2024a)). Figure 15 illustrates the granular- ity levels at which benchmarks are typically con- ducted. The chart shows that the majority of bench- marks, comprising 71.8%, focus on the function level. Projects constitute 15.0% of the benchmarks. Class-level granularity is the least represented at 2.6%.\nThe majority of benchmarks are currently at the function level (70+%), followed by the project level (15+%). This indicates that the current ma- jor demand is for assessing individual functions within a single task, followed by the demand for evaluating functionalities more aligned with actual project-level code development."}, {"title": "A.2 Statistics about Benchmark Design", "content": "Design of Studied Capabilities. To understand whether benchmark developers recognize the ca- pabilities of LLMs they aim to evaluate, we care- fully analyzed 30 representative benchmarks (Ap- pendix C) to see if they clearly specify the capabili- ties being assessed by their benchmarks. As shown in Figure 16, 90% of benchmarks explicitly specify the capabilities (e.g., intention understanding, prob- lem solving, testing, debugging capabilities)to be evaluated, while 10% do not. The statistics show that the most highly cited benchmarks clearly de- fine the assessment capabilities.\nFurthermore, we investigated the 30 focused benchmarks and identified a case (Figure 17) from MBPP (Austin et al., 2021) where the case is likely to fall outside of the targeted capability of the benchmark. In particular, MBPP (Austin et al., 2021) aims to \u201cmeasure the ability of these models to synthesize short Python programs from natural language descriptions\u201d for \u201centry-level program- mers\u201d. As we can see from Figure 17, the prompt requires LLMs to \"Write a function to calculate the dogs' years.\" Simply from this description, an entry-level programmer is unlikely to write a correct program without knowing the conversion equation from dogs' year to dogs' age. In other words, this case is more about assessing whether LLMs have acquired this specific knowledge rather than evaluating the most fundamental program- ming skills.\nDesign of Studied Application Scenarios. Simi- larly, to understand whether benchmark developers"}, {"title": "A.3 Statistics about Data Preparation", "content": "A.3.1 Data Preprocessing\nData Deduplication. During benchmark prepara- tion, data cleaning and preprocessing are necessary. However, as shown in Figure 19, only 38% bench- marks have deduplicated the collected data. More than half of them didn't mention this process.\nTo investigate the situation, we went through the 30 representative benchmarks (Listed in Ap- pendix C) and found two duplicated subjects in MBPP (Austin et al., 2021). Tasks with id 71 and 141 examined the same functionality, i.e., \u201cWrite a function to sort a list of elements.\", collected from the same source.\nThe significance of data preprocessing, such as deduplication, is frequently overlooked by benchmark builders, leading to data duplication even in highly cited benchmarks.\nData Quality Assurance. Ensuring data quality for the benchmark is essential. However, our statis- tics (Figure 21) show disappointing results. 67.9% of benchmarks do NOT take any measures for data quality assurance. Among those benchmarks that do incorporate data quality measures, the ma- jority rely on manual checks, which accounts for 22.6%. Other countermeasurements, such as code execution, constitute only 2.2%, while verification using LLMs accounts for 1.5%. Additional meth- ods, such as using download counts as a basis, are also employed.\nAdditionally, we dived into the 30 representative\""}, {"title": "A.3.2 Statistics about Data Curation", "content": "Ground truth solutions. Figure 24 shows that al- though the majority (92.3%) of benchmarks pro- vide reference code as ground truth, there are 5% of benchmarks without reference code. Although it is not compulsory as long as object measurements (e.g., test cases) are provided, a reference code is still recommended. Indeed, if a benchmark provides reference code, its reliability tends to be better because it ensures that there are feasible so- lutions for the tasks involved. This guarantees that the tasks are theoretical and practically solvable, enhancing the benchmark's usefulness and credi- bility in real-world applications.\nAdditionally, the correctness of the ground truth solution should also be noted. Figure 25 shows an incorrect code solution provided in Hu- manEval (Chen et al., 2021a). This should draw benchmark constructors' attention to the correct- ness of the benchmark reference code.\nOracle. An oracle (Barr et al., 2014) is a way to determine whether the output is correct or not. For example, assume the output of LLMs is in the form of code, then an oracle could be running tests against the code and see whether the code can pass all the tests. Figure 26 shows the distribution of types of oracle that are used in these benchmarks. Clear that the exact match 41.97% (115/274) and test case passing (114/274) 41.6% are the most common oracle used in code-related benchmarks, followed by thresholds (i.e., similarities smaller than a specific threshold).\nCode test coverage (Ivankovi\u0107 et al., 2019), as a common oracle for code-related benchmarks, has been widely adopted to determine the output cor- rectness. It should be considered if a benchmark uses test case passing as a criterion for the correct- ness of the generated code. Otherwise, a test could be too weak to detect the existence of a defect in the generated code. For example, as pointed out by prior work (Liu et al., 2023a), existing bench- marks such as HumanEval (Chen et al., 2021a) and MBPP (Austin et al., 2021) still suffer from \u201cinsuf- ficient tests", "test coverage\" explicitly in their papers, while 87.8% did not mention the test coverage of their provided code.\"\n    },\n    {\n      \"title\"": "A.4 Statistics about Evaluation"}, {"content": "Studied LLMs. We summarize the number of LLMs that have been evaluated in each benchmark evaluation. As shown in Figure 29", "prompts": "zero-shot, few-shot, chain-of-thought, and retrievals (RAG). From Fig- ure 33, we can see that a vast majority (94.9%) benchmarks were evaluated"}]}