{"title": "Between Circuits and Chomsky:\nPre-pretraining on Formal Languages Imparts Linguistic Biases", "authors": ["Michael Y. Hu", "Jackson Petty", "Chuan Shi", "William Merrill", "Tal Linzen"], "abstract": "Pretraining language models on formal languages\ncan improve their acquisition of natural language,\nbut it is unclear which features of the formal lan-\nguage impart an inductive bias that leads to effec-\ntive transfer. Drawing on insights from linguistics\nand complexity theory, we hypothesize that effec-\ntive transfer occurs when the formal language both\ncaptures dependency structures in natural language\nand remains within the computational limitations\nof the model architecture. Focusing on transform-\ners, we find that formal languages with both these\nproperties enable language models to achieve lower\nloss on natural language and better linguistic gener-\nalization compared to other languages. In fact, pre-\npretraining, or training on formal-then-natural lan-\nguage, reduces loss more efficiently than the same\namount of natural language. For a 1B-parameter\nlanguage model trained on roughly 1.6B tokens\nof natural language, pre-pretraining achieves the\nsame loss and better linguistic generalization with\na 33% smaller token budget. We also give mecha-\nnistic evidence of cross-task transfer from formal\nto natural language: attention heads acquired dur-\ning formal language pretraining remain crucial for\nthe model's performance on syntactic evaluations.", "sections": [{"title": "Introduction", "content": "Language models have achieved impressive perfor-\nmance on many tasks, but they remain data-hungry,\nrequiring five to six orders of magnitude more data\nthan humans to achieve human-level performance\n(Warstadt et al., 2023; Gilkerson et al., 2017). This\nhigh data requirement presents challenges for training\nmodels in low-resource settings (Zhong et al., 2024;\nHettiarachchi et al., 2024), understanding how models\ncan generalize given more human-like data constraints\n(Wilcox et al., 2024), and improving models as\noriginal data becomes scarce (Villalobos et al., 2022).\nThus, an important frontier for language models is\nmaking training more data-efficient.\nA recently-explored approach for increasing data\nefficiency teaches models useful inductive biases by\n\"pre-pretraining\" them on formal languages before\ntraining on natural language (Chiang and Lee, 2021;\nMcCoy and Griffiths, 2023). In particular, Papadim-\ntriou and Jurafsky (2023) show that within the Chom-\nsky hierarchy, context-sensitive languages transfer best\nto natural language. However, results from circuit com-\nplexity show that transformers cannot learn next-token\nprediction for all context-sensitive languages, both\nin theory and practice (Strobl et al., 2024; Merrill and\nSabharwal, 2023). Within all levels of the Chomsky\nhierarchy, some languages are harder for transformers\nto learn than others, if not impossible (Merrill et al.,\n2023, 2024). These results call into question how pos-\nitive transfer occurs, if a generalizing solution cannot\nalways be learned. Which characteristics of formal\nlanguages make them effective for pre-pretraining?\nIn this work, we propose that optimal transfer from\nformal to natural language occurs at the intersection"}, {"title": "Background", "content": ""}, {"title": "The Chomsky Hierarchy", "content": "The Chomsky hierarchy (Chomsky, 1959) is a nested\nclassification of increasingly complex formal lan-\nguages. This classification clarifies the kinds of\ncomputations needed to process formal structures\nresembling those found in human language. For ex-\nample, regular languages, the least complex, can be\nrecognized by finite-state automata; most phenomena\nin natural language phonology and morphology can be\ncaptured by regular languages. Regular languages are\ninsufficient for syntax, however, as representing the\nhierarchical structure of natural language with a finite-\nstate automaton would require infinitely many states\n(Chomsky, 1956). Subsequent works showed that com-\npactly modeling natural-language syntax requires not\nonly context-free but also context-sensitive grammars\n(Shieber, 1985). That said, only a select few phe-\nnomena in natural language require context sensitivity;\nthose include cross-serial dependencies in Swiss Ger-\nman (Bresnan et al., 1982) or anaphora; the rest can be\nmodeled using context-free grammars (Gazdar, 1982).\nDyck Languages. A classic context-free language\nis k-Dyck: the language of well-balanced parentheses\nwith k bracket types. For example, ([])[] is a valid\n2-Dyck string, where rounded and square parentheses\nare the two bracket types. k-Dyck is often taken\nas a canonical example of context-free hierarchical\nstructure because any context-free language can be\nreduced to Dyck via one transformation (inverse\nhomomorphism) and intersection with a regular\nlanguage (Chomsky and Sch\u00fctzenberger, 1959).\nShuffle Dyck. Removing the constraint that Dyck\nbraces must be well-nested (but enforcing that every\nopening brace must be closed and vice versa) yields\nk-Shuffle Dyck,\u00b9 a minimal relaxation of k-Dyck that\nis strictly context-sensitive rather than context-free\n(Suzgun et al., 2019; Strobl et al., 2024). Crossing\nbraces in k-Shuffle Dyck can be thought of as a formal\nmodel of the cross-serial dependencies underlying\naspects of language argued to be context-sensitive\n(Papadimitriou and Jurafsky, 2023)."}, {"title": "The Circuit Hierarchy", "content": "Our focus in this work is on transformer language\nmodels. At each level of the Chomsky hierarchy, there\nare languages that a transformer cannot recognize\n(Merrill and Sabharwal, 2023; Liu et al., 2024; Strobl\net al., 2024). Thus, the Chomsky hierarchy alone\ndoes not precisely capture how difficult a language\nis for transformers to learn: for instance, transformers\ncan learn some context-free languages (Butoi et al.,\n2025) and yet fail to learn other regular languages\n(Merrill et al., 2024). To better understand the\nexpressive power of transformers, recent work has\nanalyzed formal languages within a circuit complexity\nhierarchy, native to what transformers can and cannot\nexpress (Hao et al., 2022; Yang et al., 2024). Here, we\nwill focus on two logics that emerge from this circuit"}, {"title": "Methods", "content": "Context-free and context-sensitive languages serve\nas formal models of the hierarchical structure of\nnatural language. Similarly, circuit complexity classes\nlike FO(M) and restricted programming languages\nlike C-RASP provide theoretical upper and lower\nbounds on transformer capabilities. We explore\nthe intersection of these two hierarchies to identify\noptimal pre-pretraining languages. In this section, we\ndiscuss how we define pre-pretraining (\u00a73.1) and how\nwe create formal language pre-pretraining data (\u00a73.2)."}, {"title": "Defining Pre-pretraining", "content": "Formally, we train a language model using an\noptimizer \\(A(D,t,\\theta_{init})\\) which returns parameters \\(\\theta_t\\)\nafter t timesteps. We apply A sequentially:\n1. Pre-pretrain for \\(t_0\\) steps on dataset \\(D_{ppt}\\) to obtain\nmodel parameters \\(\\theta_{t_0}\\).\n2. Pretrain for \\(t_1\\) steps on dataset \\(D_{pt}\\) to obtain \\(\\theta_{t_1}\\).\nOur objective is to minimize the expected loss on\nthe pretraining dataset: arg \\(\\min_{\\theta_{t_1}}\\, \\mathbb{E}[l(D_{pt},\\theta_{t_1})]\\).\nWe hold A's hyperparameters, \\(t_1\\), and \\(D_{pt}\\) fixed,\nand we transfer model parameters directly from pre-\npretraining to pretraining. So to minimize \\(l(D_{pt},\\theta_{t_1})\\),\nwe can only change the pre-pretraining dataset \\(D_{ppt}\\)\nand duration \\(t_0\\). We compare pre-pretraining on our\nproposed \\(D_{ppt}\\) datasets (\u00a73.2) against several baselines:\n\u2022 No pre-pretraining (\\(t_0\\)=0)\n\u2022 Random binary strings\n\u2022 Random strings of k integers\n\u2022 Held-out natural language data from the same\ndistribution as \\(D_{pt}\\)\nAside from no pre-pretraining, we pre-pretrained the\nbaselines for \\(t_0\\) = 500 steps, for a clear comparison\nagainst the k-Shuffle Dyck results (see \u00a74). The\nheld-out natural language baseline is different from"}, {"title": "Between Circuits and Chomsky", "content": "We hypothesize that a good pre-pretraining language\nshould both encapsulate the complexity of natural\nlanguage and be robustly learnable by transformers in a\nway that length generalizes. Because natural language\nis hierarchically structured and C-RASP has been\nsuggested as a formal model of what transformers can\nlearn robustly, this motivates the following hypothesis:\nExpressivity hypothesis: A formal language\nthat confers a helpful inductive bias should be\nhierarchically structured (either context-free or\ncontext-sensitive) and definable in C-RASP.\nTo test this hypothesis, we pre-pretrain language\nmodels on the following formal languages:\n1. 1-Dyck: the nested parentheses language.\nContext-free, in C-RASP.\n2. k-Dyck: contains k different types of parentheses.\nContext-free, in FO(M)\\C-RASP.\n3. k-Shuffle Dyck: k-Dyck with cross-serial\ndependencies. Context-sensitive, in C-RASP.\n4. ww: The copy language. Context-sensitive, in\nFO(M)\\C-RASP.\nThe three variants of Dyck languages model\nhierarchical structure, while ww, like the act of\ncopying, has a fixed dependency structure."}, {"title": "Pruning", "content": "What is the mechanism by which pre-training facili-\ntates the learning of natural language? We hypothesize\nthat the model implements next-token prediction on\n\\(D_{ppt}\\) using a sparse subnetwork, or some subset of the\ntotal parameters \\(M(\\theta_{t_0}) \\subseteq \\theta_{t_0}\\) (M for short). Once we\ntransfer to to learn \\(D_{pt}\\), this subnetwork M continues\nto drive the performance of language modeling on \\(D_{pt}\\).\nSubnetworks hypothesis: Subnetworks estab-\nlished during formal language pre-pretraining\nare later used to represent the hierarchical\nstructure of natural language.\nWe test this hypothesis by ablating attention heads\nof the pre-pretraining subnetwork and comparing the\ndrop in performance against random attention head ab-\nlations. Concretely, we pre-pretrain on \\(D_{ppt}\\) and prune\nthe model to find the sparse subnetwork \\(M(\\theta_{t_0})\\). We\nuse the heuristic core pruning algorithm from Bhaskar\net al. (2024), which iteratively removes attention heads\nfrom the transformer using structured pruning (Xia\net al., 2022) while minimizing the tradeoff between\nsparsity and language modeling loss on \\(D_{ppt}\\). After\ntransfer and training on \\(D_{pt}\\), we evaluate the masked\nmodel \\(M(\\theta_{t_1})\\) against networks with the same number\nof attention heads, randomly ablated (\\(M_{null}(\\theta_{t_1}))\\).\nPositive transfer from \\(D_{ppt}\\) to \\(D_{pt}\\) could occur for\nreasons unrelated to subnetworks. In this case, the\nmasked model M should perform no better than\nrandom masks \\(M_{null}\\) when applied to \\(t_1\\). However,\nif pre-pretraining does induce useful inductive biases,\nwe would expect M to be an important subnetwork\neven after training on \\(D_{pt}\\). So in the alternative\nhypothesis, M should significantly outperform \\(M_{null}\\)\nin validation loss and zero-shot tasks."}, {"title": "Analyzing Shuffle Dyck", "content": "In \u00a74, we observed that pre-pretraining on k-Shuffle\nDyck helps models learn natural language. We now\nperform some additional analyses that help qualify this\nresult. First, we examine whether the rule-based struc-\nture of k-Shuffle Dyck is truly necessary, over a facsim-\nile dataset with the same statistical properties. Next, we\nstudy the impact that varying the vocabulary size k of k-\nShuffle Dyck has on performance. Finally, we perform\na larger scale training run with Pythia 1B and find that\npre-pretraining on k-Shuffle Dyck helps in this setting\nas well. Results for this section are in Figures 5 and 6."}, {"title": "Related Work", "content": "Efficient Learning. The goal of pre-pretraining is\nsimilar to that of optimization-based meta-learning,\nwhich aims to create a weight initialization that allows\nthe model to rapidly learn new tasks (Finn et al., 2017;\nNichol et al., 2018). McCoy and Griffiths (2023) use"}, {"title": "Discussion", "content": "Our results suggest that pre-pretraining on formal lan-\nguages with hierarchical dependencies representable\nin C-RASP can improve the downstream loss and\nlinguistic generalization of transformers. As we con-\njectured in the introduction, context-sensitivity alone is\nnot sufficient for positive transfer. The copy language\nww contains cross-serial dependencies, but it contains\nno notion of hierarchy and is not definable in C-RASP,\nand pre-pretraining on it hurts natural language\nperformance. Conversely, k-Dyck and k-Shuffle Dyck\nboth model hierarchical dependencies and confer a\nbetter inductive bias than natural language. k-Shuffle\nDyck, the language implementable in C-RASP of the\ntwo, leads to the largest pre-pretraining gains.\nAs shown in \u00a75, learning structure also has\ndownstream effects. The main subnetwork identified\nin pre-pretraining does not contain the attention\nheads important for morphological acceptability\njudgments. Nevertheless, pre-training does have\na positive effect on models learning morphology.\nThus, pre-training may have the potential to"}, {"title": "Limitations", "content": "In this work, we considered blocked training, where\nwe first train on formal language followed by natural\nlanguage. We chose blocked training because a good\ninitialization from formal language can be easily\nplugged into existing pretraining pipelines, but mixing\nformal and natural language during training could lead\nto better performance (Korbak et al., 2023). We also\nevaluated efficiency in a setting where pretraining data\nis plentiful, and one can train without running several\nepochs over the same data. A low-resource setting\nwould also be interesting, and may yield different\nscaling properties with respect to pre-pretraining data\n(Muennighoff et al., 2023). This would be particularly\nrelevant for non-English languages (Samuel et al.,\n2023; Martin et al., 2020). Finally, our work only\nconsiders transformers. Circuit complexity has also\nquantified the expressive power of neural networks\nlike RNNs and state-space models (Merrill et al.,\n2020, 2024), and it would be interesting to extend our\nresults to these architectures."}, {"title": "Proofs", "content": "We make use of the following to establish that all\nlanguages we consider are context-sensitive.\nLemma A.1. Any language definable in FO(M) can\nbe recognized by a context-sensitive grammar.\nProof. Mix Barrington et al. (1990) show that the\nclass of languages definable in FO(M) is LOGTIME-\nuniform TC, which is a subset of L=SPACE(logn).\nOn the other hand, the context-sensitive languages\nare those languages recognizable by linear-bounded\nautomata (Kuroda, 1964). That is, CSL=NSPACE(n).\nPutting these characterizations together, we see that\nTCO SPACE(logn) \u2286 NSPACE(n)=CSL.\nTherefore we can conclude that any language definable\nin FO(M) is context-sensitive.\n\nWe will make use of the classical pumping lemma\nto establish that some specific languages considered\nare strictly context-sensitive, i.e., not context-free.\nLemma A.2 (Pumping Lemma, Bar-Hillel et al., 1961).\nLet L be a context-free language. Then there exists\na pumping constant p>0 such that any string s\u2208 L\nof length |s|\u2265p can be written as s=uvwxy where\n1. |vwx|\u2264p;\n2. |vx|\u2265 1; and\n3. \\(uvwx^n y \\in L\\) for all n \u22650.\nAdditionally, we will leverage the following\ncommunication complexity result to prove that certain\nlanguages are undefinable in C-RASP:\nLemma A.3 (Huang et al., 2025, Theorem 12). Let\nL be a language definable in C-RASP. Fix w \u2208 L\nand 1\u2264i\u2264|w|. Let Alice have access to \\(w_{1:i}\\) and Bob\nhave access to \\(w_{i+1:|w|}\\). Then there exists a protocol\nwhere Alice sends at most O(logn) bits to Bob and\nBob can recognize whether w\u2208 L.\nCrucially, if some L requires Alice to send Bob\n\\(\\omega (logn)\\) bits, then it cannot be defined in C-RASP.\nWe will also use the equivalence between C-RASP\nand the Temporal Counting Logic K\u2081[#] to show that\nlanguages are definable in C-RASP.\nLemma A.4 (Yang and Chiang, 2024, Theorem 4.3).\nA C-RASP program recognizes language L if and\nonly if a \\(K_1 \\text{[#]}\\) formula defines L."}, {"title": "Language Characterizations", "content": "Proposition A.5. 1-Dyck is context-free and definable\nin C-RASP.\nProof. That 1-Dyck is context-free follows from\nthe fact that it can be generated by the following\ncontext-free grammar:\n\\begin{aligned}\nS &\\rightarrow (S)S,\\\\\nS &\\rightarrow \\epsilon.\n\\end{aligned}\n1-Dyck is defined by the following K\u2081 [#] formula\n\\(\\big(\\text{[#]}[Q_()]=\\text{[#]}[Q_)]\\big) \\land \\big(\\text{[#]}[Q_()]>\\text{[#]}[Q_)]\\big)=0\\),\nand SO is implementable in C-RASP by\nLemma A.4.\n\nProposition A.6. For k \u2265 2, k-Dyck is context-free\nand not definable in C-RASP.\nProof. That k-Dyck is context-free follows from the\nfact that it can be generated by a context-free grammar:\nfor any fixed value of k, k-Dyck is generated by\n\\begin{aligned}\nS &\\rightarrow (S)_i S, \\text{ where } i \\in \\text{[k]}\\\\\nS &\\rightarrow \\epsilon\n\\end{aligned}\nTo see that k-Dyck is not definable in C-RASP,\nconsider Dyck strings of length 2n where tokens 1\nto n are opening braces, and tokens n+1 to 2n are\nclosing braces. If Alice receives the first n tokens,\nshe must send \u03a9(n) bits to Bob if Bob is to correctly\nrecognize the input string, because each prefix has\na different unique suffix that closes it. So k-Dyck is\nnot in C-RASP by Lemma A.3.\n\nOn the other hand, k-Dyck can be defined in FO(M).\nProposition A.7. For k \u2265 1, k-Dyck is definable in\nFO(M).\nProof. Let \\(Q_((i))\\) check whether token i is any of the k\nopening parentheses, and \\(Q_)_{i}\\) check whether token\ni is the kth opening parenthesis out of k. Continuing\nthe definition from Section 2.2:\n\\begin{aligned}\ndepth(i) &= \\text{[#]}_{j \\leq i}[Q_((i)]-\\text{[#]}_{j \\leq i}[Q_)_{i}]\\\\\ndindex(i) &= \\text{[#]}_{j \\leq i}[depth(i)=depth(j)]\\\\\npaired(j,i) &= [depth(j)=depth(i)+1] \\land\\\\ &\\qquad[dindex(i) =dindex(j)]\\\\\nmatch(j,i) &= \\sqrt{[Q_((j) \\land Q_)_{i}]\\\\\nclosed(i) &= \\exists_{j \\leq i}[paired(j,i) \\land match(j,i)]\n\\end{aligned}\nHaving defined these macros, we are now ready\nto write the recognizer for k-Dyck:\n\\begin{aligned}\n[depth(n)=0] \\land [\\text{[#]}_{i \\leq n}[depth(i) <0]=0] \\land\\\\ &\\forall_{i \\leq n}[closed(i)]\n\\end{aligned}"}, {"title": "", "content": "Lemma A.8. For k \u2265 2, k-Shuffle Dyck is strictly\ncontext-sensitive and definable in C-RASP.\nProof. See Ex. 7.20 in Hopcroft et al. (2006).\nConsider the case when k=2. Assume that 2-shuffle\nDyck is context-free. Then \\(L = (^{\\text{[p)p]m}n\\) is\ncontext-free since it is the intersection of k-Shuffle\nDyck with (*[*)*]* and CFLs are closed under\nintersection with regular languages.\nAssume by way of contradiction that Lis\ncontext-free and so has pumping constant p. Let\n\\(s = (^{\\text{[p)p]p^m)n\\), whichy by hypothesis can be written\nas uvwxy. Since |vwx|\u2264 p, it either (a) lies entirely\ninside one of the blocks of p symbols or (b) lies\npartially in one block of p symbols and lies partially\nin at most one adjacent block. In the case of (a),\nsuppose for clarity that vwx lies entirely in the\n(P block. Since vx is not empty, uvwxy = uwy\ncontains fewer ('s than)'s, and hence is not in L, a\ncontradiction. In the case of (b), suppose for clarity\nthat vwx straddles the (P and [P blocks. Since vx is\nnot empty, uvwxy=uwy contains either fewer ('s\nthan)'s or fewer ['s than]'s, and hence is not in L, a\ncontradiction. Since k-Shuffle Dyck for k > 2 contains\n2-Shuffle Dyck, proving the k = 2 case is sufficient\nto establish that k-Shuffle Dyck is not context free\n(but still context-sensitive by Lemma A.1).\nSimilar to the 1-Dyck case, we can exhibit a K\u2081 [#]\nformula to define k-Shuffle Dyck:\n\\big(\\text{[#]}[Q_()]=\\text{[#]}[Q_)]\\big) \\land \\big(\\text{[#]}[Q_()]>\\text{[#]}[Q_)]\\big)=0\n\nSo k-Shuffle Dyck is likewise definable in C-RASP\nby Lemma A.4.\n\nProposition A.9. ww is strictly context-sensitive and\nnot definable in C-RASP.\nProof. See Ex. 7.21 in Hopcroft et al. (2006). Suppose\nby way of contradiction that ww is context-free, and so\nhas a pumping constant p. Let s=\\(a^pb^pa^pb^p\\), which\ncan be written as uvwxy by hypothesis. Consider\nthen the string \\(uvwx^0y\\)=uwy\u2208L. We examine two\ncases depending on the position of vwx in s.\nIn the first case, suppose vx is contained entirely\nwithin the first block of \\(a^p\\). If |vx| = k then uwy\nhas length 4p k and begins with the substring\n\\(a^(p-k)b^p...\\) of length 2p-k. By assumption uwy=tt\nfor some t of length 2p \u2013 k/2, and since k \u2265 1 it"}, {"title": "Hyperparameters", "content": "All experiments were done on NVIDIA A100 or\nH100 80GB GPUs. We warm up the learning rate\nboth during pre-pretraining and pretraining. The\nbelow hyperparameters hold for both pre-pretraining\nand pretraining. That is, for simplicity, even if we\nonly pre-pretrain for 500 steps, we still keep the"}, {"title": "Acknowledgments", "content": "Many thanks to Andy Yang, Angelica Chen, Dan\nFriedman, Isabel Papadimitriou, Lindia Tjuatja,\nMayee Chen, Qingyang Zhu, and the NYU Com-\nputation and Psycholinguistics Lab for feedback\nand discussion. This work was supported in part\nthrough the NYU IT High Performance Computing\nresources, services, and staff expertise. This project is\nsupported by the National Science Foundation (NSF)\nunder grant NRT-HDR: FUTURE as well as Grant"}]}