{"title": "NiaAutoARM: Automated generation and evaluation of Association Rule Mining pipelines", "authors": ["Uro\u0161 Mlakar", "Iztok Fister Jr.", "Iztok Fister"], "abstract": "The Numerical Association Rule Mining paradigm that includes concurrent dealing with numerical and categorical attributes is beneficial for discovering associations from datasets consisting of both features. The process is not considered as easy since it incorporates several processing steps running sequentially that form an entire pipeline, e.g., preprocessing, algorithm selection, hyper-parameter optimization, and the definition of metrics evaluating the quality of the association rule. In this paper, we proposed a novel Automated Machine Learning method, NiaAutoARM, for constructing the full association rule mining pipelines based on stochastic population-based meta-heuristics automatically. Along with the theoretical representation of the proposed method, we also present a comprehensive experimental evaluation of the proposed method.", "sections": [{"title": "INTRODUCTION", "content": "The design of Machine Learning (ML) pipelines usually demands user interaction to select appropriate preprocessing methods, perform feature engineering, select the most appropriate ML method, and set a combination of hyper-parameters [1]. Therefore, preparing an ML pipeline is complex, and, primarily, inappropriate for non-specialists in the Data Science or Artificial Intelligence domains [2]. On the other hand, tuning the entire pipeline to produce the best results also may involve a lot of time for the users, mainly if we deal with very complex datasets.\nAutomated Machine Learning (AutoML) methods have been appeared, to draw the application of ML methods nearer to the users in the sense of ML democratization [2], [3]. The main benefit of these methods is searching for the best pipeline in different ML tasks automatically. Until recently, AutoML forms can be found for solving classification problems, neural architecture search, regression problems [4], and reinforcement learning.\nAssociation Rule Mining (ARM) is a ML method for discovering the relationships between items in transaction databases. Bare ARM is limited, since it operates initially with categorical type of attributes only. Recently, Numerical Association Rule Mining (NARM) has been proposed, that is a variant of a bare ARM, which allows dealing with numerical and categorical attributes concurrently, and thus removes the bottleneck of the bare ARM. The NARM also delivers several benefits, since the results can be more reliable and accurate, and contain less noise than bare ARM, where the numerical attributes need to be discretized before use. Nowadays, the problem of NARM is tackled mainly by using population-based meta-heuristics, which can cope large search spaces effectively. Let us mention that the acronym ARM is used as synonym for the acronym NARM in the paper.\nThe ARM pipeline (see Fig. 1) is far from being uncomplicated, since it consists of several components, as follows: (1) data preprocessing, (2) mining algorithm selection, (3) hyper-parameter optimization, (4) evaluation metric selection, and (5) evaluation. Each of these components can be implemented using several ML methods. Consequently, composing the ARM pipeline manually requires a lot of human intervention, potentially a time-consuming task. Therefore, automation of this composing led us to the new domain of AutoML, i.e., AutoARM.\nThe data entering the ARM pipeline are in the form of a transaction database; the optional first component of the ARM pipeline is preprocessing, where the data can be preprocessed further using various ML methods. The selection of the proper processing component presents a crucial step, where the most appropriate population-based meta-heuristic Nature-Inspired Algorithm (Nia) needs to be determined for ARM. Mainly, the NI algorithms encompasses two classes of population-based algorithms: Evolutionary Algorithms (EA) [5] and Swarm Intelligence (SI) based [6].\nAccording to previous studies, no universal population-based meta-heuristic exists for ARM achieving the best results by mining all datasets. This phenomenon is also justified by the No Free Lunch (NFL) theorem of Wolpert and Macready [7]. The next component in the pipeline is the hyper-parameter optimization for the selected population-based meta-heuristic, where the best combination of hyper-parameters is searched for. Finally, the selection of the favorable association rules depends on the composition of the more suitable metrics captured in the fitness function. In our case, the fitness function is represented as a linear combination of several ARM metrics (e.g., support, confidence, amplitude, etc.) weighted with particular weights.\nTo the best knowledge of the authors, no specific AutoML methods exist for constructing the ARM pipelines automatically. Therefore, the contributions of this study are:\n\u2022 To propose the first AutoML solution for searching for the best ARM pipeline, where this automatic searching is represented as an optimization problem."}, {"title": "RELATED WORK", "content": "The section highlights topics necessary for understanding the subjects of the paper. In line with this, the following topics are dealt with:\n\u2022 AutoML,\n\u2022 NiaAML,\n\u2022 NiaAutoARM.\nThe mentioned topics are discussed in detail in the remainder of the paper."}, {"title": "AutoML", "content": "Using ML methods in practice demands experienced human ML experts, who are typically expensive and hard to find on the market. On the other hand, computing has become cheaper day by day. This fact has led to the advent of AutoML, which is capable of constructing the ML pipelines of a similar, or even better quality, than by the human experts [2]. Consequently, the AutoML enables the so-called democratization of ML. This means that the usage of the ML methods is drawn closer to the user by AutoML, and, thus, this technology tries to avoid the principle of user-in-the-loop [8].\nAutomation of ML methods is allowed by AutoML using the ML pipelines. Indeed, these pipelines are the control points of the AutoML system. Typically, the ML pipeline consists of the following processing steps:\n\u2022 preprocessing,\n\u2022 processing with definite ML methods,\n\u2022 hyper-parameter optimization,\n\u2022 evaluation.\nAutoML is, nowadays, a very studied research area. The recent advances in the field have been summarized in several review papers [1], [3], [9], [10]. There also exist a dozen applications of AutoML [11], [12], where the special position is devoted to NiaAML, which is discussed in more detail in the remainder of this section."}, {"title": "NiaAML", "content": "The NiaAML is an AutoML method based on stochastic Nias for optimization, where the AutoML is modeled as an optimization problem. The first version of NiaAML [13] covers composing classification pipelines, where a stochastic Nia searches for the best classification pipeline. The following steps are included in the AutoML pipeline, i.e., automatic feature selection, feature scaling, classifier selection, and hyper-parameter optimization. Each classifier configuration found by the optimizers is tested using cross-validation.\nFollowing the NiaAML, the NiaAML2 [13], [14] was proposed, which eliminated the main weakness of the original NiaAML method, where the hyper-parameters' optimization is performed simultaneously with the construction of the classification pipelines in a single phase. In the NiaAML, only one instance of the stochastic algorithm was needed. However, in the NiaAML2, the construction of the pipeline and hyper-parameter optimization was divided into two separate phases, where two instances of nature-inspired algorithms were deployed, one after the other, to cover both steps. The first step covers the composition of the classification of the pipeline, while the second is devoted to hyper-parameter optimization."}, {"title": "NiaARM", "content": "The NiaARM is a Python framework [15] that implements the ARM-DE algorithm comprehensively [16], where the ARM is modeled as a single objective, continuous optimization problem. The fitness function in NiaARM is defined as a weighted sum of arbitrary evaluation metrics. One of the most vital points of NiaARM is that it is based on the NiaPy framework [17], and, thus, different Nia-s can be used in the optimizer role. According to the knowledge of the authors, NiaARM is the only comprehensive framework for NARM where all NARM steps are implemented, i.e., preprocessing, optimization, and visualization. Other benefits of NiaARM are good documentation and many examples provided by the maintainers, Command Line Interface (CLI), are easy to use."}, {"title": "PROPOSED METHOD: NIA AUTOARM", "content": "The proposed method NiaAutoARM is inspired mainly by the NiaAML method. Thus, we define the problem of ARM pipeline construction as a continuous optimization problem. This means that an arbitrary population-based meta-heuristic Nia, which works in a continuous search space, can be applied for solving this problem. Indeed, the NiaAutoARM works as an outer layered meta-heuristic, that controls the behavior of the inner layered NI heuristic for ARM by searching for the optimal inner algorithm, the corresponding hyper-parameters, the employed preprocessing methods, and the outline of the proper evaluation function.\nIn the NiaAutoARM, each individual in the population of solutions represents one feasible ARM pipeline as:\n$x^{(t)} = (Y_{i, 1}^{(t)}, Y_{i, 2}^{(t)}, P_{i,1},..., P_{iP}^{(t)}, z_1^{(t)}..., z_{1M}^{(t)}, W_{i,1}^{(t)} ..., W_{iM}^{(t)})$ (1)\nwhere parameter $P$ denotes the number of potential preprocessing methods, and parameter $M$ is the number of potential ARM metrics to be applied. As is evident from Eq. (1), each real-valued element of solution in a genotype search space within the interval [0, 1] encodes the particular NiaAutoARM component of the pipeline in a phenotype solution space as follows: The ALGORITHM component denotes the stochastic population-based Nia, which is chosen from the pool of available algorithms, typically selected by the user from a NiaPy library relatively to the value of $x_{i, 1}^{(t)}$[15].\nThe HYPER-PARAMETERS component indicates a magnitude of two parameters: the maximum number of individuals $NP$, and the maximum number of fitness function evaluations $MAXFES$ as a termination condition for the selected algorithm. Both values, $y_1^{(t)}$ and $y_2^{(t)}$, are mapped in genotype-phenotype mapping to the specific domain of the mentioned parameters as proposed by Mlakar et al. in [18]. The PREPROCESSING component determines the pool of available preprocessing algorithms which can be applied to the dataset. On the one hand, if $P$ = 0, no preprocessing algorithm is applied, while, on the other hand, if $P > 0$ and $p>.5$ for $j = 1,..., P$, the $j$-th preprocessing algorithms from the pool will be observed for applying to the dataset. The METRICS component is reserved for the pool of M rule evaluation metrics devoted for estimating the quality of the mined association rules. Additionally, the weights of the metrics are included by the METRIC_WEIGHTS component, which weighs the influence of the particular evaluation metric on the appropriate association rule. Typically, the evaluation metrics as illustrated in Table 1 are employed in NiaAutoARM.\nAs a result, the outer meta-heuristic algorithm calls the inner heuristic algorithm as follows:\n*Alg[\u0393(xi, 1)](\u0420, \u041c, \u0413(\u0443\u0456, 1), \u0413(\u0443\u0456,2), \u0393(Prep, p), \u0393(Metr, z)), \u0393(Metr, w))), (2)\nwhere the function \u0393 denotes the mapping of genotype values to the phenotype values. Let us mention that the scalar values of 'Algorithm call', $NP$ and $MAXFES$ are decoded by mapping their values from the interval [0,1] to the domain values in the solution space. On the other hand, the preprocessing methods and ARM metrics represent sets, where each member is taken from the sets Prep and Matr according to the probability 0.5, based on the values of the vectors p and z. Interestingly, the weight vector can be treated either statically or adaptively w.r.t., setting a parameter weight_adaptation. When the parameter is set as true, the adapted values from vector w indicate an impact of a definite ARM metric in the linear combination of ARM metrics within the fitness function. If this parameter is set to false, the values are fixed to the value 1.0.\nAlthough the quality of the mined association rules is calculated in the inner algorithm using the weighted linear combination of the ARM metrics, the NiaAutoARM estimates the quality of the pipeline due to the fairness using the fitness function as:\nf(x)) = \u03b1\u00b7 supp(X \u21d2 Y) + \u03b2\u00b7 conf (X \u21d2 Y), (3)\nwhere \u03b1 and \u03b2 designate the impact of the definite ARM metric on the quality of the solution. It is discarded, if no rules are produced or the pipeline fails to decode to the solution space.\nThe pseudo-code of the proposed NiaAutoARM for constructing the classification pipelines is presented in Algorithm 1, from which it can be observed that the outer meta-heuristic starts with a random initialization of the population (function INITIALIZE_REALVALUED_VECTORS_RANDOMLY in line 1). After evaluation"}, {"title": "EXPERIMENTAL EVALUATION", "content": "The primary goal of the experiments was to evaluate whether NiaAutoARM can find an optimal pipeline for solving various ARM problems automatically. A series of experiments utilized the most common ARM publicly available datasets to justify this hypothesis.\nThe UCI ML datasets, listed in Table 2, were used for evaluating the performance of the proposed method [25]. Each database is characterized by the number of transactions, number of attributes and their types, which can be either categorical (discrete) or numerical (real). These datasets were selected since they vary in terms of the number of transactions, the types of attributes, and the total number of attributes they contain. It is also worth mentioning that the proposed method determines the most suitable preprocessing algorithm automatically as part of its process, therefore, no manual preprocessing was applied to the original datasets.\nIn our experiments, we used two outer commonly used Nia-s for the ARM pipeline optimization, namely, the DE and the PSO. To ensure a fair comparison, the most important hyper-parameters of both algorithms were set equally. The population size was set to NP = 30, and the maximum number of fitness function evaluations to MAXFES = 1000 (i.e., the number of pipeline evaluations). The other parameters were set to default values, as proposed in the Niapy library. In all the experiments, the inner optimization algorithms for mining association rules were selected similarly as in the example illustrated in Fig. 2. Each experimental run produced the best pipeline for a combination of the specific dataset and algorithm. Considering the stochastic nature of the DE and PSO algorithms, the reported results are the average fitness function values of the best obtained pipelines over 30 independent runs.\nThe quality of the constructed pipeline was evaluated regarding Eq. (3) in the outer algorithm, while the fitness function in the inner algorithm was calculated as a weighted sum of the ARM metrics decoded from the corresponding individual by the NiaAutoARM."}, {"title": "Results", "content": "The following experiments were conducted for analyzing the newly proposed NiaAutoARM thoroughly:\n\u2022 baseline ARM pipeline optimization, allowing just one preprocessing component and disabling the ARM metric weight adaptation,\n\u2022 influence of adapting the ARM metric weights on the quality of the ARM pipeline construction,\n\u2022 influence of selecting more preprocessing components on the quality of the ARM pipeline construction,\n\u2022 comparison with the VARDE state-of-the-art algorithm.\nIn the remainder of this section, all the experimental results are presented in detail, showcasing the usefulness and efficiency of the proposed method."}, {"title": "Baseline ARM pipeline construction", "content": "The purpose of the first experiment was to establish a foundational comparison for all the subsequent experiments. In this experiment, no ARM metric weight adaptation was applied, ensuring that the generated pipelines operated in their default configurations. Additionally, each generated pipeline was restricted to a single preprocessing method, eliminating the variability introduced by multiple preprocessing components.\nAll the results for this experiment are reported numerically in Tables 3 and 4, and graphically in Figure 3 for the different PSO and DE outer meta-heuristics, respectively. The mentioned Tables are structured as follows: The column 'Preprocessing method' denotes the frequency of the preprocessing algorithms in the best obtained pipelines over all 30 runs. The column 'Hyper-parameters' is used for reporting the average obtained population sizes (NP) and maximum function evaluations (MAXFES) for the best obtained ARM pipelines. Lastly, the column 'Metrics & Weights' are used for reporting the average values of each used ARM evaluation metric. The number in the subscript denotes the number of pipelines in which a specific metric was used. Since, in the baseline experiment, no ARM metric weight adaptation was used, all values are equal to 1. Each row in the Tables refer to one experimental dataset.\nFigure 3 presents the obtained average fitness values, along with the average number of rules generated by the best obtained pipelines. Additionally, the frequencies of the inner optimization algorithms are depicted.\nThe results in Table 3, developed by the outer algorithm PSO, justified that the preprocessing methods, like MM, ZS, and RHC, were selected more frequently, while, in general, 'No preprocessing' was selected in most of the pipelines regardless of the dataset. The ARM metrics support, confidence, and coverage appeared consistently across most datasets. Notably, the support and confidence are present in nearly all the pipelines for datasets, like Abalone, Balance scale, and Basketball, indicating that these metrics are essential for the underlying optimization process. Metrics like amplification, which were used less frequently, are absent in many datasets, suggesting that the current algorithm configuration does not prioritize such metrics. The hyper-parameters NP and MAXFES varied depending on the dataset, influencing the ARM pipeline optimization process.\nTable 4 shows the results for the outer algorithm DE. Similar to the results of the PSO, key ARM metrics like support, confidence, and coverage are found consistently in many of the generated pipelines. However, there are subtle differences in the distribution of these metrics across the pipelines. For instance, the metric amplitude is selected just for the dataset German. Regarding the preprocessing methods and hyper-parameters, a similar distribution can be found as in the results of the PSO algorithm.\nThe graphical results showcase that both DE and PSO obtained similar results regarding the fitness value. The number of rules is slightly dispersed, although no big deviations are detected. The key differences are in the selection of the inner optimization algorithm. For the majority of datasets, the PSO and jDE algorithms were selected more often as the inner optimization algorithms, than others. This is true for both the outer algorithm experiment runs. Other used algorithms, such as GA, DE, ILSHADE and LSHADE, were selected rarely, probably due to their complexity or their lack of it.\nTo summarize the results of the baseline experiment, we can conclude that the best results were obtained, when either no preprocessing was applied, or MM was used on the dataset. The NP parameter seems to be higher for more complex datasets (i.e., more attributes) such as Buying, German, House16 and Ionosphere, while it remains lower for the others which were less demanding. Regarding the selection of specific ARM evaluation metrics, it seems that both algorithms focused on the more common ones, usually used in Evolutionary ARM [18]. Overall, these results indicate the DE and PSO algorithms' robustness as an outer ARM meta-heuristic, while reinforcing the potential benefits of further exploration into ARM metric weight adaptation and diversified preprocessing strategies.\nLet us notice that all the subsequent results are reported in the same manner."}, {"title": "Influence of the ARM metric weights adaption on the quality of ARM pipeline construction", "content": "The purpose of this experiment was to analyze the impact of selecting ARM metric weight adaptation on the performance of the ARM pipeline construction. The ARM metric weights play a crucial role in guiding the optimization process, as they influence the evaluation and selection of the candidate association rules. By incorporating the ARM weight adaptation mechanism, the pipeline can adjust the importance of ARM metrics dynamically, such as support, confidence, coverage, and others, tailored to the characteristics of the dataset. This experiment aimed to determine whether adapting these weights improved the quality of the discovered rules, which are, therefore, reflected in the pipeline's metrics. The results are compared to the baseline configuration, where no weight adaptation was applied.\nTables 5 and 6 present the results obtained by the outer algorithms PSO and DE, respectively. The similar selection of the preprocessing methods as in the last experiment was also employed in this experiment, where the preprocessing methods MM, ZS and None were applied the most frequently. The hyper-parameters yielded higher values for the harder datasets. Considering the ARM metrics, the support and confidence still arose with high weight values in the majority of the pipelines, whereas the ARM metrics, like amplification or comprehensibility, are utilized less with lower weights.\nFrom the results in Figure 4 we can deduce similar conclusions as from those in the baseline experiment, but"}, {"title": "Influence of selecting more preprocessing methods on the quality of ARM pipeline construction", "content": "The parameter P controls the number of preprocessing components allowed in an ARM pipeline. By increasing P beyond 1, we introduce the possibility of combining multiple preprocessing dataset methods, which can, potentially, enhance the quality of the generated rules. This increased flexibility enables the pipeline to address complex data characteristics (e.g., variability in feature scaling, noise reduction, or dimensionality reduction) more effectively. However, this increased complexity also poses challenges, including higher computational costs and a broader search space to be discovered by the inner optimization algorithms. In this section, we analyze the impact of setting the parameter as P > 1 on the quality of the ARM pipelines, focusing on the resulting ARM metrics and their corresponding weights, and computational trade offs for the experimental datasets.\nThe results of the selected preprocessing algorithms are depicted as heatmaps of all the possible combinations. The results in Tables 7 and 8 suggest that the support and confidence ARM metrics were again included heavily in the calculation of the fitness function, achieving high values in the majority of the pipelines for both the outer optimization algorithms. The coverage and inclusion ARM metrics were also involved in many pipelines, although their average weights were smaller. There was no notable difference in the"}, {"title": "Comparison with VARDE state-of-the-art algorithm", "content": "The last experiment was reserved for an indirect comparison with the VARDE state-of-the-art algorithm [18] for ARM,"}, {"title": "Discussion", "content": "The results show notable trends in the optimization of ARM pipelines. The PSO algorithm was selected predominantly"}, {"title": "CONCLUSION", "content": "This paper presented NiaAutoARM, an innovative framework designed for the optimization of the ARM pipelines using stochastic population-based Nia-s. The framework integrates the selection of: an inner ARM heuristic, its hyper-parameter optimization, dataset preprocessing techniques, and searching for the more suitable fitness function represented as a weighted sum of ARM evaluation metrics, where the weights are subjects of the adaptation. Extensive evaluations on ten widely used datasets from the UC Irvine repository underscore the framework's effectiveness, particularly for users with limited domain expertise. Comparative analysis against the VARDE state-of-the-art hybrid DE highlights the superior performance of the proposed framework in generating high-quality ARM pipelines further.\nThe future work would aims to address several key areas: First, integrating additional Nia-s with adaptive parameter tuning could enhance the pipeline optimization process further. Second, incorporating other advanced preprocessing techniques and alternative metrics might improve pipeline diversity and domain-specific applicability. Third, exploring parallel and distributed computing strategies could mitigate computational complexity, making the framework more scalable. Finally, extending the framework to support multi-objective optimization would enable a deeper exploration of trade-offs between conflicting metrics, advancing its utility further in real-world applications."}]}