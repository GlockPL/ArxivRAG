{"title": "Web Retrieval Agents\nfor Evidence-Based Misinformation Detection", "authors": ["Jacob-Junqi Tian", "Hao Yu", "Yury Orlovskiy", "Tyler Vergho", "Mauricio Rivera", "Mayank Goel", "Zachary Yang", "Jean-Fran\u00e7ois Godbout", "Reihaneh Rabbany", "Kellin Pelrine"], "abstract": "This paper develops an agent-based automated fact-checking approach for detecting\nmisinformation. We demonstrate that combining a powerful LLM agent, which\ndoes not have access to the internet for searches, with an online web search agent\nyields better results than when each tool is used independently. Our approach\nis robust across multiple models, outperforming alternatives and increasing the\nmacro F1 of misinformation detection by as much as 20 percent compared to\nLLMs without search. We also conduct extensive analyses on the sources our\nsystem leverages and their biases, decisions in the construction of the system\nlike the search tool and the knowledge base, the type of evidence needed and its\nimpact on the results, and other parts of the overall process. By combining strong\nperformance with in-depth understanding, we hope to provide building blocks for\nfuture search-enabled misinformation mitigation systems.", "sections": [{"title": "Introduction", "content": "Misinformation and disinformation are significant societal challenges that threaten to intensify with\nprogress in generative AI (Chen & Shu, 2023). Recent work (Chen & Shu, 2023; Pelrine et al., 2023)\nhas shown that Large Language Models (LLMs) can effectively detect misinformation and provide a\npotential path to mitigate harm at scale. However, LLMs suffer from hallucinations and often lack\nknowledge of recent events due to a fixed training data window. These problems can be significantly\nmitigated if models have access to external sources of information. In this work, we propose a method\nto retrieve and leverage evidence from the web for misinformation detection.\nSurprisingly, there are relatively few tools for Retrieval-Augmented Generation (RAG) that combine\nexternal data sources with recent LLMs in the domain of misinformation detection (Chen & Shu,\n2023). In this study, we aim to build a strong, comprehensive approach that combines LLMs and\nRAG by integrating natural language understanding techniques for claim decomposition (Yao et al.,\n2022; Min et al., 2023; Chern et al., 2023; Zhang & Gao, 2023). In particular, we prompt an LLM to\ngenerate queries, and then answer them using another LLM connected to a web search engine. We\nevaluate the performance of this approach across a wide range of models: Vicuna, Mixtral, Claude,\nGPT-3.5, and two versions of GPT-4. We find that web-retrieval techniques improve the performance\nof all models except Vicuna; we also confirm that this improvement generally increases with the\nperformance of the underlying model. In addition, we compare two search approaches-Cohere\nCoral with web connector, and DuckDuckGo with GPT-3.5 summarization\u2014and confirm that they\nare both effective. Thus, our results indicate that the fully customizable DuckDuckGo tool can be\nused in future work, along with the more locked-in Cohere one; they also suggest that our framework\ncan be generalized to other search engines as well. We then analyze the sources retrieved, showing\nthat: (1) having more sources is better; (2) the pipeline is not overly reliant on any one source; and (3)\nthe sources it naturally uses exhibit little bias overall and high credibility. We continue investigating"}, {"title": "Related work", "content": "Many studies have shown the benefits of retrieving information to augment fact-checking and\nmisinformation detection (Bekoulis et al., 2021; Kondamudi et al., 2023; Zhou & Zafarani, 2020).\nHowever, much of this past work integrates retrieval with older models like BERT (Liao et al., 2023),\nBART (Sundriyal et al., 2022), and memory networks (Ebadi et al., 2021; Ying et al., 2021). We note\nthat these approaches have not demonstrated strong enough performance improvements to solve the\nproblem of misinformation detection. Indeed, the task of detecting fake news or false information is\nchallenging due to its ambiguous nature, where misinformation often contains a mix of true and false\nstatements, making it difficult to discern the truth. It also requires up-to-date knowledge of real-world\nevents and a nuanced understanding of deception techniques employed by malevolent actors who\nwant to avoid detection. Nevertheless, LLMs have emerged as very promising research avenues for\nthis task (Chen & Shu, 2023; Pelrine et al., 2023). Recent work has confirmed that these models\nstill struggle with insufficient context, ambiguous inputs and hallucinations (Pelrine et al., 2023; Hsu\net al., 2023; Orlovskiy et al., 2024), but identifying the missing information and providing additional\ncontext significantly improves the performance of these models on ambiguous statements (Orlovskiy\net al., 2024; Pelrine et al., 2023). There is, therefore, a need to integrate strong retrieval tools into the\nnew, LLM-based misinformation mitigation systems.\nOne particularly promising approach to LLM-based systems in the misinformation detection and\nfact-checking domains is decomposing inputs of uncertain veracity, such as statements or articles,\ninto smaller units like claims. This enables chain-of-thought (Wei et al., 2022) reasoning particularly\nadapted to these tasks. For example, the ReAct framework (Yao et al., 2022) improves performance\non the HotPotQA dataset (Yang et al., 2018) by decomposing its reasoning trace into a series of\nactions and observations traces for each thought generated (Yao et al., 2022). Likewise, the HiSS\nprompting method instructs the model to decompose a claim into several subclaims, that are then\nverified individually via multiple question-answering steps (Zhang & Gao, 2023). Three common\napproaches include 'FactScore', which decomposes claims into 'atomic facts' (Min et al., 2023),\nalong with 'FacTool' (Chern et al., 2023) and 'FOLK' (Wang & Shu, 2023), which are both designed\nto extract individual claims from longer statements.\nAlthough several studies have highlighted the benefits of using RAG for LLMs in general applications\n(Saad-Falcon et al., 2023; Bendersky et al., 2023; Wang et al., 2024; Gao et al., 2024), there are\nsurprisingly few analyses that apply this technique to misinformation detection and even fewer\nthat combine it with decomposition. In a survey of over 650 works related to misinformation and\nLLMs (Chen & Shu, 2023), only two RAG misinformation detection methods were discussed (Chern\net al., 2023; Cheung & Lam, 2023). The first focused on factuality in more general problems\nlike mathematics and code generation, while the latter did not use decomposition. Given that"}, {"title": "Methodology", "content": "We enable information retrieval by explicitly permitting the main LLM agent to invoke search multiple\ntimes and encouraging the LLM to perform extensive reasoning before giving a prediction. This leads\nthe LLM to naturally produce an effective decomposition of the task, in contrast to pipelines like\nFactLlama (Cheung & Lam, 2023), which invoke search only on the input. We perform the search\nwith another LLM agent with a web connection and return the search agent's output to the main\nagent. The main agent repeatedly triggers this process until it decides to give a final prediction (for\nan example of a full input and output, see Appendix A.10). We test this pipeline across several recent\nmodels (GPT-4, GPT-3.5, Cohere Coral, Mixtral, Claude 3, and Vicuna-1.5), focusing particularly on\nthe LIAR-New dataset (Pelrine et al., 2023).\n3.1 Enabling search\nHere we describe how we enable instruction-tuned generative LLMs to \u201csearch\" online, especially\nthose that do not explicitly support function calling. In the prompt, we inform the model that it has\naccess to a search tool, and provide instructions on how to invoke it:\nYou have access to a search engine tool. To invoke search,\nbegin your query with the phrase \"SEARCH: \". You may\ninvoke the search tool as many times as needed.\nWe parse the generator LLM output to extract the prescribed pattern. While invoking the search\nfunction, we supply the text that comes right after \u201cSEARCH: \" to the search pipeline. That pipeline\ndoes not have access to the original statement that we want to verify.\nWe explored two different implementations of the search pipeline: with the Cohere \"Chat with RAG\"\nAPI, and by summarizing the output of the DuckDuckGo search API.\nIn the Cohere pipeline, we invoke the Cohere Chat API with the official \u201cweb-search\" connector\nenabled. We ask it to \"Look up info\" with the following code and template:\nresponse = cohere.Client.chat(\n)\nmodel=\"command\",\nmessage=\"Look up info about the following: {query}\",\nconnectors=[{\"id\": \"web-search\"}]\nAbove, the placeholder \u201c{query}\" in the message to Cohere \u201cChat with RAG\u201d would be replaced\nwith the text extracted from the generator LLM output. The output of this search agent is a summary\nof the search results in natural language. We prepend the output with \"Search result: \" and add it to\nthe conversation context as a new message with role user, then resume the main (offline) agent's\ngeneration. This results in interactions between the main agent and search agent\nas\nDuckDuckGo As an alternative to the Cohere service, we utilize the DuckDuckGo search engine\nthrough its web search API. We chose this search engine because there is a free API, unlike, for"}, {"title": "Extracting predictions", "content": "We prompt the main agent to return a binary number as the prediction:\nstate \"True statement; Factuality: 1\" if you think the\nstatement is factual, or \"False statement; Factuality: 0\"\notherwise.\nWe repeat each experiment 5 times and report the 95% confidence interval over the five F1 scores\n(this generally gives a larger but more thorough interval than the commonly reported standard error)."}, {"title": "Baselines", "content": "Offline models We compare the baseline \u201coffline\u201d performance of the models with our own search\nenabled framework. The prompt for the offline version is as follow:\nYour task is to analyze the factuality of the given statement.\nAfter providing all your analysis steps, summarize your\nanalysis and state \"True statement; Factuality: 1\" if you\nthink the statement is factual, or \"False statement;\nFactuality: 0\" otherwise. You should begin your summary with\nthe phrase \"Summary: \". Statement: {statement}\nCohere Chat with RAG Note that in the main experiment setup, we do not provide the Cohere\n\u201cChat with RAG\u201d model access to the original statement that we seek to verify. Similarly, for\nDuckDuckGo, we also just prompt the modified query, instead of the original statement. In both\ncases, this ensures we are leveraging our decomposition framework. However, the Cohere model\nwhich has been set up for web search could potentially do it effectively on its own, without the help\nof the generator LLM, if it were given the original statement. Hence, we implement it as a baseline\nto determine if the \"search\" pipeline adds any benefit beyond just invoking Cohere Chat with RAG\ndirectly. To do this, we query Cohere \u201cChat with RAG\" with the same prompt given to other generator\nLLMs when the search action is disabled (see the above prompt).\nHiSS We selected this baseline as it provided the state-of-the-art decomposition and web search\napproach in this domain. We implemented HiSS (Zhang & Gao, 2023) using the updated code\nprovided by the authors. We test both the original version which predicted 6-way labels (also\nconverted to binary to match our other evaluation) and a direct binary version where we minimally\nchanged the prompt by replacing the part that lists the 6 labels with \u201ctrue and false\". As the underlying\nmodel, we use GPT-3.5 like the original paper, and we also test GPT-4. We use the most recent\nversion of both at the time of this writing, 0125.\nWikiChat We implement WikiChat (Semnani et al., 2023), a model grounded on the English\nWikipedia that outperforms other models on factual accuracy retrieval baselines, using the code\nprovided in the original repository. However, instead of using text-davinci-003 as the engine model\nlike in the original paper, we use GPT-3.5-turbo-instruct because text-davinci-003 was deprecated by\nOpenAI. We follow the configuration suggested in their code repository to obtain results comparable\nto the GPT-3.5 results reported in their original paper. The prompt used for WikiChat is as follows:\nYour task is to analyze the factuality of the given statement.\nAfter providing all your analysis steps, summarize your\nanalysis and state \"True statement; Factuality: 1\" if you\nthink the statement is factual, or \"False statement;\nFactuality: 0\" otherwise. You should begin your summary with\nthe phrase \"Summary: \" and conclude your response with\n\"Factuality: 1\" or \"Factuality: 0\". Statement: {statement}\nThis matches the original except we explicitly specify concluding with \"Factuality: 1\" or \"Factuality:\n0\", because we noticed that without this, the refining step would strip the score from the end of the\nmodel's response and lead to parsing errors.\""}, {"title": "Uncertainty quantification", "content": "We investigated the system's uncertainty quantification performance through a direct confidence\nelicitation methodology (Lin et al., 2022), which has been previously demonstrated to be effective\nin the misinformation detection context (Pelrine et al., 2023; Rivera et al., 2024). Specifically, we\nprompted the generative LLMs to provide an uncertainty estimation of its analysis once we extracted\nthe system's analysis and predictions:"}, {"title": "Results", "content": "Performance of search We see in  that when web searches are enabled, gpt-3.5-turbo,\ngpt-4-0613,gpt-4-0125, and claude3-haiku all surpass the performance of the Cohere\nSources LIAR-New is collected from PolitiFact, raising a natural question about how much role\nPolitiFact plays when doing web search. In real-world applications, we would both like the system\nto take advantage of PolitiFact when it is available, and still do well when it is not (e.g., for new\nmisinformation that PolitiFact has not yet fact-checked). In Appendix A.4.1 we find that PolitiFact is\nthe most frequently used source, searched nearly as much as the rest of the top 10 sources combined.\nTo simulate PolitiFact not being available, in , we test blocking this website and relying\non other sources. This is done with the DuckDuckGo pipeline, where we have more control than\nthe Cohere one. We find that blocking PolitiFact does not degrade performance of GPT-3.5, and\nwhile performance of GPT-4 does drop a bit, it remains significantly better than without search. We\nalso perform an ablation on the number of results we set the search to return, and find performance\ndecreases monotonically with fewer results. Putting these results together, along with the fact that\nPolitiFact is the ideal source to provide evidence for this dataset, these findings suggest that no single\nsource is necessary for accuracy, but it is important to have enough sources to reduce noise and\nimprove the chances of finding the necessary evidence.\nIn Appendix A.4.2, we study the bias of the sources and input statements. We find that the sources\nused are slightly but not substantively left leaning (-0.54 on a scale from -3 to 3, putting it between\n1=\"center-left\" and 0=\"center\"), while the input statements are right leaning to an almost exactly\nequal degree (0.53 on the same scale). Thus, our system does not exhibit substantial bias in terms of\nsources used. To assess the overall quality of the sources, we evaluate the websites accessed by the\nweb retrieval system using the comprehensive media source evaluation database (Lin et al., 2023),\nnoting that the average quality of the sources is in the range of 0.76-0.80 for all models, roughly on\npar with the New York Times (0.87). In the same Appendix, we examine the bias and overall domain\nquality versus performance of our search systems, but do not find conclusive results there.\nSummarizer In  we perform an ablation on the summarizing model, comparing\nour default summarizer with gpt-3.5-turbo (GPT-3.5) against gpt-4-0125 (GPT-4) and"}, {"title": "Conclusion", "content": "We have described a framework for leveraging web retrieval to detect misinformation that substantially\nimproves performance. We have also confirmed its flexibility and customizability by showing that\nit works with multiple models and search tools. Finally, we have explored numerous parts of the\nprocess, including the sources and their biases, how different levels of search and summarizing affect\nthe results, the impact of open web vs. restricted knowledge base, when search is effective and when\nit is not, and more. Taken together, we hope this line of work will lead to a practical understanding of\nhow to build better, more evidence-based misinformation mitigation tools."}, {"title": "Appendix", "content": "A.1 Label distribution\nThe label distribution is shown in"}]}