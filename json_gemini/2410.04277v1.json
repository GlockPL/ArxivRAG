{"title": "MECHANISTIC BEHAVIOR EDITING OF LANGUAGE MODELS", "authors": ["Joykirat Singh", "Subhabrata Dutta", "Tanmoy Chakraborty"], "abstract": "Large Language Models trained on web-scale text acquire language generation abilities that can solve a wide range of tasks, particularly when task knowledge is refined into the generative prior using in-context examples. However, spurious features learned from noisy data hinder their generalizability. Supervised finetuning can introduce task specificity, but introduce data inefficiency. Prior studies indicate that (i) noisy neural circuitries coexist with generalizable ones within LLMs, and (ii) finetuning typically enhances (or suppresses) existing abilities without introducing newer ones. Building upon these, we propose TaRot, a novel method for task adaptation. TaRot intervenes in the neural circuitries using learnable rotation matrices that are optimized using Bayesian Optimization, on labelled samples in the order of standard few-shot prompting examples. Experiments on multiple classification and generation tasks using LLMs of varying sizes reveal the efficacy of TaRot, improving upon both zero- as well as few-shot performance, with average improvements (across models and tasks) of 23.81% and 11.15%, respectively. The source code is available at https://github.com/joykirat18/TaRot.", "sections": [{"title": "INTRODUCTION", "content": "Large Language Models (LLMs) acquire the ability to associate different language concepts presented in a sequential context by optimizing the prediction probability of the next token given a context. Despite its apparent simplicity, when scaled across web-sized text corpora, such a learning strategy introduces the ability to solve a wide range of tasks presented in natural language. However, the web contains almost everything humankind has written, and therefore, it introduces spurious token associations that are irrelevant or even counter-productive to the model to become generalized task-solvers. We observe phenomena like brittle few-shot performance, hallucination, harmful text generation, etc. as evidence of learning noisy patterns. Remedial interventions like instruction tuning, alignment tuning, etc. have been proposed. Recent research has shown that such mediation only acts on a superficial level out-of-distribution inputs can reinforce noisy behavior and break the model. Without an in-depth understanding of the inner workings, remedial strategies become wild goose chase.\nMechanistic disentangling of Transformer-based language models has shed some light on this direction. Two recent investigations on the effects of fine-tuning confirm the inability of supervised fine-tuning to alter fundamental abilities acquired via pretraining. On a tangential investigation, recently confirmed the existence of multiple parallel neural pathways of answer processing within LLMs. echoed similar findings in the case of syntactic generalization while pointing out that different components acquire different generalization behaviors."}, {"title": "RELATED WORK", "content": "Our work is primarily relevant to two broad areas of existing literature: adaptation of pretrained language models to downstream tasks, and mechanistic understanding and intervention techniques.\nTask adaptation of pretrained language models. The pretrain-finetune regime for adapting language models to downstream tasks dates back to the early approaches like BERT - pretrain a language model (LM) on large unstructured text corpora using self-supervised objective, followed by supervised fine-tuning on task-specific, relatively smaller datasets. Despite the apparent simplicity, the pitfalls of this regime have been pointed out in terms of distribution shift. With the development of large-scale, autoregressive Transformer-based language models and their ability to learn from in-context examples , a definitive shift has happened in the more recent past. Current practices of using these models for downstream tasks primarily rely on designing suitable prompt templates and labeled example retrieval for in-context learning (ICL); traditional techniques of fine-tuning have taken a back seat due to the computational cost and catastrophic forgetting introduced by small-scale task-specific data that hurts the pretrained abilities. Instead, finetuning to follow task instructions, aka instruction-tuning , has gained popularity. Instruction-tuning has been shown to introduce zero-shot task adaptation abilities in LLMs. Additionally, different methods of alignment tuning have been proposed with the primary goal being aligning the generative distribution of the language models with human values and preferences. Despite the popularity of instruction and alignment tuning, their ability to alter fundamental information processing has been put in question in recent literature. investigated the effects of fine-tuning in toy models trained with formal languages as well as precompiled ones; their findings suggest that supervised fine-tuning does not introduce any new ability into pretrained models but only reinforces (or suppresses) existing ones. Similar concerns have been raised upon investigating entity tracking in the neural representation space. identified multiple limitations of instruction tuning, including the inability to introduce new knowledge and deterioration of performance due to over-reliance on pattern matching.\nMechanistic understanding and interventions. The umbrella of mechanistic interpretability broadly encompasses methods to disentangle model behavior via reverse engineering the underlying neural algorithm. Endeavors to mechanistically understand Transformer-based language models trace back to the seminal work by. Their framework established attention heads as one of the fundamental building blocks of language model interpretation. Subsequent studies have identified the functional roles of different attention heads in pretrained models: induction heads as a primary mechanism of prefix matching, circuitries of attention heads responsible for indirect object identification, neural pathways that implement chain-of-thought reasoning, etc. Much relevant to our analysis, found that certain attention heads memorize the association between country names and their capitals. On a tangential line of investigation, introduced the Distributed Alignment Search (DAS) framework for localizing interpretable features in subspaces of the neural representations. Mechanistic methods provide actionable insights that have led to non-traditional techniques to edit model behavior. experimented with key propagation to elicit induction heads (and thereby, prefix-matching ability) in single-layer attention-only Transformers. used causal tracing to locate factual associations in MLP neurons and proposed a gradient-free approach to edit factual recall patterns in pretrained language models. identified attention head circuitry that elicits toxic text generation in GPT-2; mean-ablation of these circuits is shown to reduce toxicity. Self-detoxification identifies toxic generation direction in the internal representation using trigger prompts and then rewrites in the opposite direction to reduce toxicity. formulated toxicity reduction as a knowledge editing task that can permanently alter toxic behaviors instead of suppressive interventions like supervised fine-tuning or RLHF-based alignment. localized backdoor mechanisms (i.e., vulnerabilities against adversarial prompt injections) in early-layer MLPs and proposed a low-rank substitution to improve robustness against such injections. employed attribution patching techniques to identify and remove certain singular values in the parameter matrices to improve performance."}, {"title": "METHODOLOGY", "content": "In this section, we demonstrate the role of attention heads in memorizing token associations. Next, we lay out the working principles of TaRot.", "subsections": [{"title": "ATTENTION HEADS AS TOKEN-TOKEN MAPS", "content": "Following the framework presented by , we dissect the Transformer-based language models with the following assumptions: (i) Each attention head reads from and writes to the residual stream independently in a linear fashion, and (ii) given that the attention heads utilize hidden representation of dimensionality much smaller than the residual stream (i.e., for a model with 16 attention heads, each attention head uses 1/16-th of the dimension of the residual stream), they typical operate on small subspaces of the residual stream. This way, two attention heads can operate on two distinct subspaces and never interact with each other. These two assumptions allow us to interpret the working of the attention heads meaningfully even while treating each head in isolation. We start with identifying what a single-head attention operation tends to learn in isolation.\nFollowing the standard terminology , we represent the embedding and unembedding matrices as $W_e \\in \\mathbb{R}^{d\\times V}$ and $W_{U} \\in \\mathbb{R}^{V\\times d}$, where $d$ and $V$ are the dimensionality of the residual stream and the token space, respectively, the query, key, value, and output projection matrices denoted as $W_Q, W_K, W_V, W_O \\in \\mathbb{R}^{d\\times d}$, respectively. Given a sequence of input tokens as one-hot column vectors $T = \\{t_1,\\ldots, t_n\\}$, the forward pass for single-layer attention-only Transformer can be written as:\n$\\hat{t}_{n+1} = W_U W_e t_n + W_O\\left(\\sum_{i=1}^{n} a_{n,i} W_V W_e t_i\\right)$ (1)\nwhere $a_{n,i} = \\frac{exp\\left(t_{n}^T W_e^T W_Q W_K W_e t_{i}\\right)}{\\sum_{j} exp\\left(t_{n}^T W_e^T W_Q W_K W_e t_{j}\\right)}$ is the softmax-attention probability from source token $t_i$ to destination token $t_n$, and $\\hat{t}_{n+1} \\in \\mathbb{R}^V$ is the logit of the predicted next token. Upon reparametrization of $W_U W_O W_V W_e$ as $W_{OV}$, we can rewrite Equation 1 as\n$t_{n+1} = W_U W_e t_n + \\sum_{i} W_{OV}t_i$ (2)\nNote that $W_{OV} \\in \\mathbb{R}^{V\\times V}$, denoted as OV-circuits by , maps a distribution over tokens to another distribution over tokens. If the true token is $t_{n+1}$ with $I(t_{n+1})$ donating its index (i.e., index of 1 in $t_{n+1}$), then the typical language modeling loss can be calculated as:\n$\\mathcal{L}(t_{n+1},t_{n+1}) = -log\\left(\\frac{exp\\left(\\hat{t}_{n+1}^{I(t_{n+1})}\\right)}{\\sum_{k} exp\\left(\\hat{t}_{n+1}^{I(t_{k})}\\right)}\\right)$ (3)\nWe can compute the gradient dynamics of the OV-circuit (with unit batch size and zero momentum) using Equations 2 and 3 as follows:\n$W_{OV}^{(s+1)} = W_{OV}^{(s)} + \\eta \\left( SoftMax^{-1} (t_{n+1}) - \\sum_{i=1}^{n} a_{n,i} t_i \\right) (\\sum_{i=1}^{n} a_{n,i} t_i)^T$ (4)\nwhere $W_{OV}^{(s)}$ and $W_{OV}^{(s+1)}$ are the OV-circuit parameters before and after the s-th gradient update step and $\\eta$ is the learning rate. The positive incremental component in the right-hand side of Equation 4 dictates that, when applied on a attention-weighted linear combination of the context tokens, OV-circuits learn to memorize a linear combination of possible next tokens."}, {"title": "EDITING MODEL BEHAVIOR VIA ATTENTION ROTATION", "content": "A natural conclusion from the prior discussion would be that, by suppressing undesired associations for certain attention heads, we can improve task performance. However, multiple token associations are expected to be memorized in each attention head in superposition since the number of attention heads is way smaller than the potential token associations present in the pretraining data one cannot selectively switch off one certain association. Prior research in mechanistic interpretability has shown that, although we can often localize attention heads responsible for particular task, removing the non-dominant attention heads does not deliver the performance of the full model.\nInstead, one can rotate the output of the attention heads in order to maximize its alignment with rows of $W_U$ corresponding to certain tokens while near-orthogonalizing with certain undesired tokens. This way, the model behaviour can be edited without destroying the superposed associations. Defining the complete space of $d \\times d$ rotation matrices and optimizing them can become computationally challenging. Instead, we utilize the fact that any $d \\times d$ orthonormal matrix is similar to a block-diagonal matrix $R_{\\Theta}$, where $\\Theta = \\{\\theta_1,\\ldots, \\theta_{d/2}\\} \\subset [0, 2\\pi)$, defined as:\n$R_{\\Theta} = \\begin{pmatrix}  cos \\theta_1 & -sin \\theta_1 & 0 & 0 & 0 & 0\\\\  sin \\theta_1 & cos \\theta_1 & 0 & 0 & 0 & 0\\\\  0 & 0 & cos \\theta_2 & -sin \\theta_2 & 0 & 0\\\\  0 & 0 & sin \\theta_2 & cos \\theta_2 & 0 & 0\\\\  \\vdots & & & & \\vdots & \\\\  0 & 0 & 0 & 0 & cos \\theta_{d/2} & -sin \\theta_{d/2}\\\\  0 & 0 & 0 & 0 & sin \\theta_{d/2} & cos \\theta_{d/2}  \\end{pmatrix}$ (5)\nGiven the multi-head attention with $H$ heads at layer $l \\in [L]$, where $L$ is the total number of layers in the Transformer, defined as:\n$Attn(x)|[x_1,\\ldots,x_n]) = W_O ||\\left( \\sum_{i=1}^{n} a_{n,i}^{(h,l)} W_V^{(h,l)} x_i^{(l)}\\right)$ (6)\nwhere $||$ is the concatenation operator, $a_{n,i}^{(h,l)}$ and $W_V^{(h,l)}$ denote the attention probability between source and destination residual streams at layer $l$ and $x_i^{(l)}$, and the value projection matrix corresponding to the attention head with index $h \\in [H]$ at layer $l$, we define the rotated attention as:\n$RotAttn(x)|[x_1,\\ldots,x_n]) = W_O R_\\Theta ||\\left( \\sum_{i=1}^{n} a_{n,i}^{(h,l)} W_V^{(h,l)} x_i^{(l)}\\right)$ (7)\nNote that the block-diagonal definition of $R_\\Theta$ in Equation 5 implies that applying $R_\\Theta^H$ on the concatenated head outputs is equivalent to applying $H$-distinct $R_\\Theta^{(h,l)}$ on each of the head outputs.\nWithout prior knowledge of which attention heads are responsible for memorizing undesired token associations, we need to apply the intervention defined in Equation 6 on a set of attention blocks at layers $l \\in \\mathcal{L}$ (see Section 4 for the choice of the set $\\mathcal{L}$). Then, the intervened forward pass is denoted as:\n$\\hat{t}_{n+1} = M_{Rotated} (\\{t_1,\\ldots, t_n \\}| \\Theta_{Original}, \\Theta_{Rotation} \\{\\theta_i | l \\in \\mathcal{L}\\})$\nwhere $\\Theta_{Original}$ is the set of pretrained model parameters and $\\Theta_{Rotation}$ are the parameters of rotations."}, {"title": "OPTIMIZATION OF ROTATION PARAMETERS", "content": "With the rotational interventions defined, all that we are left with is to optimize the rotational parameters. Let $D := \\{T_j,Y_j | j \\in [D]\\}$ be a set of $D$ supervised examples for a given task, with $T_j, Y_j$ referring to the sequence of tokens corresponding to the input and gold output, respectively. If $Y_j = \\{y_j\\}$ is a single label token, the cost function to optimize becomes straightforward:\n$\\max_{\\Theta_{Rotation}} \\sum_{j} P_{M_{Rotated}}\\left(M_{Rotated}\\left(T_j | \\Theta_{Original}, \\Theta_{Rotation}\\{\\theta_i | l \\in \\mathcal{L}\\}\\right) = Y_j\\right)$ (8)\nIn a few-shot setup, the objective function is modified to:\n$\\max_{\\Theta_{Rotation}} \\sum_{j} P_{M_{Rotated}}\\left( \\sum_{m=1}^{M} ||[T_m, Y_m]|| T_j | \\Theta_{Original}, \\Theta_{Rotation}\\{\\theta_i | l \\in \\mathcal{L}\\}\\right) = Y_j$ (9)\nIn the case of NLG tasks, maximizing the aggregate probability of all the generated tokens can be a solution. However, the goal of our proposed rewiring method is to minimize undesired behaviors. When a model demonstrates such behaviors, depending upon the task, not all tokens equally correspond to the behavior under inspection. The pretrained model is trained using teacher-forcing and is generally able to generate grammatically correct responses. Hence, trying to align the model generation to a single reference response does not make much sense. Instead, we opt for a surrogate scoring function $s : \\{Y_j\\} \\rightarrow \\mathbb{R}$ that scores the \u201cdesirability\u201d of a generated response. We let the model with rotation intervention to generate a complete response given an input, compute the score for the generated response, and seek to minimize the aggregate score across $D$:\n$\\max_{\\Theta_{Rotation}} \\sum_{j} s\\left(argmax_k \\left(M_{Rotated}\\left([T_j || Y_{:k-1}] | \\Theta_{Original}, \\Theta_{Rotation}\\{\\theta_i | l \\in \\mathcal{L}\\}\\right)\\right)\\right)$ (10)\nwhere $Y_{:k-1}$ denotes the token sequence generated till the $(k-1)$-th decoding step.\nWe implement Bayesian optimization to solve the optimization problems in Equations 8, 9 or 10 depending upon the task. However, standard Gaussian Process with Matern kernel fails to scale to high dimension input space. Instead, Infinite-width Bayesian Neural Networks (I-BNN), proposed by, has shown to scale effectively with high-dimensional parameter space. Furthermore, I-BNN covariance function is not based on Euclidean distance, allowing Gaussian Process to represent non-stationary functions. This is advantageous as effects of rotations may not have similar behaviour throughout the entire configuration space."}]}, {"title": "EXPERIMENT SETUP", "content": "Training setting. previously found that token associations corresponding to pretrained knowledge primarily resides in the initial half of the model. Since the rotational intervention designed in Equations 6 and 7 are primarily targeted towards undesired token associations acquired through pretraining, we restrict $\\mathcal{L}$ to the initial half only. Therefore, the total number of parameters to optimise becomes $\\frac{Ld}{4}$. Since we want to optimise the rotation matrix for a particular task, only a small subset of training samples is required, i.e, $6 < D_{training} \\leq 20$.\nModels. Four different instruction-tuned models with varying size are used for all experiments: Qwen2-1.5B-Instruct Yang et al. (2024), Phi-3-mini-4k-instruct Abdin et al. (2024) (2.8 billion parameter), Mistral-7B-Instruct-v0.1 Jiang et al. (2023), and Meta-Llama-3-8B-Instruct Dubey et al. (2024); we refer to these models as Qwen2-1.5B, Phi-3-mini, Mistral-7B, and Llama-3-8B, respectively."}, {"title": "ANALYSIS OF ACTION", "content": "Towards understanding the nuances of  TaRot's action on the neural representation, we start with investigating the probability of the answer token at different layers of the forward pass. Specifically, we adopt logit attribution : for a given layer $l$ with output residual stream corresponding to the last token,  $x_{n+1}^{(l)}$, we compute the intermediate probability of the answer token as: $p = SoftMax (W_U x_{n+1}^{(l)})$.\nIn Figure 2, we plot $p_{Tarot} - p_{Base}$ for each model across all the layers on different tasks. The overall change in answer token probability remains marginal  (< 10-4) across all the instances, signifying a key aspect of TaRot: it does not substantially improve the desired behavior, rather it minimizes the undesired token associations. However, with Qwen2-1.5B and Phi-3-mini, there are fluctuations right from the beginning. In case of larger models like Mistral-7B and Llama-3-8B, probability difference appears only at the very end. Note that negative (or positive) difference in answer token probability does not essentially mean one method is better than the other. Additionally, we plot the distribution of maximum and minimum logit values for each model. Again, there is no significant change in the logit distribution as well, denoting that TaRot does not introduce temperature-increment in the logits.\nFollowing , we further investigate the impact of  TaRot on the unembedding subspace. We perform singular value decomposition of $W_U$ into $UEV^T$. We then compute the cosine similarity between the residual stream vectors corresponding to different layers and the row vectors of $V^T$, and plot it alongside the corresponding singular values. A strong bias is observed where the  TaRot intervened residual stream aligns more to the smaller singular values of unembedding, thereby decreasing their impact. In Mistral-7B, the effect is more skewed compared to Phi-3-mini. This observation provides a definitive characterization of TaRot's action on the different subspaces of the residual stream."}, {"title": "CONCLUSION", "content": "In this work, we proposed TaRot, a novel, gradient-free, mechanistic intervention method for editing language models. TaRot builds on observations from implicit gradient descent bias of causal attention and applies parametrized rotation on the attention output to minimize the effects of undesired memorizations, doing away with effort-intensive localization steps and task-specificity of prior intervention techniques. Using Bayesian optimization of the rotational parameters, TaRot renders as data-efficient as in-context learning; yet, across a variety of tasks and language models of different sizes and families, robust improvement is observed. We further analyzed the impact of TaRot and demonstrated the key mechanism of action. In a nutshell, TaRot can pave the path for general-purpose model editing methods in the future beyond supervised fine-tuning.\nLimitations and ethical considerations. TaRot is designed to perform when the model has a generalization ability that is suppressed by noisy memorization. In that sense, it is limited by the boundaries of pretraining and cannot be used for domain adaptation. Fundamentally, it is not applicable to proprietary models. Finally, similar to any intervention technique, TaRot can be used in reverse to bypass alignment tuning and reinforce undesired behaviors."}, {"title": "APPENDIX", "content": null, "subsections": [{"title": "TASK DETAILS", "content": "We experimented with five different classification (i.e., single token generation) tasks and two NLG tasks. Below are the details of the tasks with their prompt templates used:\nAG News: The goal of the task is to categories new articles into one of the four predefined categories.\n\u2022 World - News about global events, international politics, and worldwide issues.\n\u2022 Sports - News related to sporting events, athletes, competitions, and sports industry developments.\n\u2022 Business - News focusing on the economy, financial markets, companies, and business trends.\n\u2022 Science & Technology \u2013 News about technological advancements, scientific discoveries, and research.\nSystem prompt used for AG News task: You are a news classification model. Your task is to classify news articles into one of the following four categories: World, Sports, Business, or Science. You should respond with only the category name and no other characters.\nEntailed Polarity: The Entailed Polarity task is a yes/no question-answering task Srivastava et al. (2022). Given a fact and a question, the goal is to determine whether the fact entails a yes or no answer to the question. The task tests the model's ability to infer whether the factual statement logically supports the answer in terms of polarity (positive or negative). Example:\n\u2022 Fact: \"Ed remembered to go.\""}]}]}