{"title": "Towards Unsupervised Validation of Anomaly-Detection Models", "authors": ["Lihi Idana"], "abstract": "Unsupervised validation of anomaly-detection models is a highly challenging task. While the common practices for model validation involve a labeled validation set, such validation sets cannot be constructed when the underlying datasets are unlabeled. The lack of robust and efficient unsupervised model-validation techniques presents an acute challenge in the implementation of automated anomaly-detection pipelines, especially when there exists no prior knowledge of the model's performance on similar datasets. This work presents a new paradigm to automated validation of anomaly-detection models, inspired by real-world, collaborative decision-making mechanisms. We focus on two commonly-used, unsupervised model-validation tasks model selection and model evaluation and provide extensive experimental results that demonstrate the accuracy and robustness of our approach on both tasks.", "sections": [{"title": "1 Introduction", "content": "Anomaly detection (AD) is the task of identifying the minority of data observations that deviate significantly from the majority of the observations. While anomaly-detection tasks are challenging, they can be well-managed when the underlying datasets are labeled: training is performed using either supervised or semi-supervised models; the best model (i.e. model selection) is chosen using a labeled validation set; and the performance of the model is assessed (i.e. model evaluation) using a (second) labeled validation set.\n\nThe above no longer holds when the anomaly-detection task at hand is unsupervised and labeled datasets are not available. Though the pool of unsupervised anomaly-detection models has been steadily increasing over the past few years thus reducing the requirement of labeled training sets, two acute challenges remain: unsupervised model selection and unsupervised model evaluation. Due to the lack of a labeled validation set, standard model selection and evaluation practices cannot be applied while research on methods that do not require a labeled validation set is surprisingly scarce.\n\nIn this work, we aim to fill this gap by introducing a new approach to model selection and evaluation of anomaly-detection models which does not require any labeled data. Our approach is based on the following key ideas:\n\n1. In cases where the ground truth is not available, a representative majority's opinion is a good-enough approximator for the ground truth.\n\n2. One way to obtain a representative majority's opinion is building an Accurately-Diverse ensemble: an ensemble of unsupervised models that sufficiently balances the ensemble's accuracy and diversity.\n\n3. In order for an ensemble to be Accurately-Diverse, the ensemble's decisions must exhibit both heterogeneity and homogeneity in a complementary manner: we require a strong intra-ensemble agreement on the general trend of each ensemble member's predictions, while encouraging a strong intra-ensemble disagreement on the exact ordering of each ensemble member's predictions.\n\n4. The exact ordering of a model's predictions is approximated by the ranked anomaly-score indices of the least distinctive observations in the dataset: non-extreme inliers. The general trend of a model's predictions is approximated by its \"fuzzy ranks\": ranked anomaly-score clusters of the most distinctive observations in the dataset: strong outliers. The level of agreement among ensemble members is approximated using a rank correlation metric, computed separately on fuzzy ranks of strong outliers and on exact ranks of non-extreme inliers.\n\n5. The rank correlation metric should be carefully designed so that it can both be applied to M > 2 lists, and so that it can account for the unique structure of anomaly score lists compared to other ranked lists. We design multiple multi-way correlation metrics that are specifically suited to measuring the correlation among ranked anomaly scores.\n\nWe provide a thorough evaluation of our approach using ten publicly available datasets. Our experimental results demonstrate the following two claims:\n\n1. An Accurately-Diverse ensemble yields better results than the average unsupervised anomaly-detection model; the results prove that using the Accurately-Diverse ensemble practically eliminates the need for a model-selection procedure of unsupervised anomaly-detection models.\n\n2. Our Accurately-Diverse-ensemble-based unsupervised evaluation metric yields results that are on par with those of supervised evaluation metrics; the results prove that the Accurately-Diverse ensemble can be used for unsupervised evaluation of anomaly detection models.\n\nThis work is the first to develop and experimentally test the \"complementary homogeneity-heterogeneity\" criteria: the criteria of a strong intra-ensemble agreement on the general trend of each ensemble member's predictions and a strong intra-ensemble disagreement on the exact ordering of each ensemble member's predictions, as a proxy for the ensemble's validity and its ability to be used for unsupervised, anomaly-detection model selection and evaluation. Importantly, from our experiments, it is clear that both homogeneity and"}, {"title": "2 Related work", "content": "As the limitations of unsupervised anomaly detection have been widely acknowledged, there have been multiple recent attempts to design more effective unsupervised AD models: probabilistic methods [18, 17], neural-network-based methods [27, 19, 31] and even graph-based methods [11]. Works such as [10, 12] provide benchmarks of the most prominent unsupervised anomaly-detection models as well as create practical rules of thumb on the best settings for using each model based on different criteria such as the dataset's domain and quality.\n\nThe use of ensembles for AD has been explored in multiple works. Yet, those works significantly differ from ours: either they assume a supervised or a semi-supervised setting where labels are available [30, 32]; use ensembles based on feature diversification [15, 23]; use homogeneous ensembles [5, 14, 3] or histogram-based ensembles [25]. Ensembles that do support fully unsupervised, heterogeneous ensemble members assume that the list of individual models is known in advance; thus, they focus on methods for combining models' predictions [1, 8] or on researching optimal transformations that can be applied to individuals predictions [35, 34] rather than on researching optimal ways for choosing an optimal composition of the ensemble; that is, which models should be included in the ensemble. Importantly, no prior work aims at designing an AD ensemble model that can function as an unsupervised model selector, while the latter is the main goal of Accurately Diverse ensembles.\n\nThe research on unsupervised evaluation methods of anomaly-detection models has been surprisingly scarce. The fully unsupervised evaluation approaches that we are aware of are the method described in [21], based on soft-margin classifiers, and the method described in [9], based on excess mass and mass volume curves. All other methods that we are aware of, such as the p-value-based evaluation method in [4], require a labeled validation set.\n\nContrary to the above, our work focuses on a purely unsupervised setting in which ensemble members are heterogeneous and no feature transformation is performed. Our work is the first to develop and experimentally test the criteria of a strong intra-ensemble agreement on the general trend of each ensemble member's predictions and a strong intra-ensemble disagreement on the exact ordering of each ensemble member's predictions as a proxy for the ensemble's validity and its ability to both replace the model selection procedure of unsupervised anomaly-detection models and be utilized for unsupervised evaluation of anomaly-detection models."}, {"title": "3 Accurately Diverse Ensembles", "content": "The diversity-accuracy tradeoff is an inherent challenge in ensemble-based approaches. In unsupervised settings, it becomes an even bigger challenge since the common practices for testing the accuracy of a model such as using a labeled validation set can not be applied. Our assumption is that when evaluating our ensemble, we do not have access to any source of \"ground truth\". We, therefore, need to design a metric that will enable us to balance the accuracy and diversity of an ensemble without any access to labels.\n\nOur core idea used for designing such a metric is inspired by [6], which lists three requirements from a judicial ensemble: opinion heterogeneity, opinion homogeneity, and independence of errors. At first glance it is unclear how a single unsupervised ensemble can meet all three requirements: first, opinion homogeneity and heterogeneity, as well as opinion homogeneity and independence of errors, seem to be mutually exclusive. Second, in order to evaluate the independence-of-errors requirement we must have access to the ground truth of a subset of observations so we can determine which predictions were \"mistakes\". This requirement cannot be accommodated in the unsupervised case. Nevertheless, we claim that an unsupervised ensemble that meets all three requirements not only exists, but can also be easily identified using the following key observation:\n\nTo balance accuracy and diversity, the ensemble's decisions must exhibit both heterogeneity and homogeneity in a complementary manner that highlights the general, common shape of the ensemble member's decision boundary and at the same time blurs their individual peculiarities. This conceptual observation can be practically approximated by requiring a strong intra-ensemble agreement on the general trend of each member's predicted anomaly scores, while encouraging a strong intra-ensemble disagreement on the exact ordering of each member's predicted scores. In such a case, the individual errors of the ensemble members will be sufficiently independent so that the aggregated decision coincides with the ground truth. We refer to such an ensemble as an Accurately-Diverse ensemble.\n\nSpecifically, for an ensemble to be Accurately-Diverse two conditions must hold:\n\n(1) The ensemble members should strongly agree on high-level features of highly-distinctive observations in the dataset.\n\n(2) The ensemble members should strongly disagree on low-level features of lowly-distinctive observations.\n\nOur main claims are the following:\n\nClaim 3.1. In unsupervised AD settings where a supervised model-selection procedure a procedure that compares the performance of N candidate AD models on a given dataset - cannot be performed due to the lack of labeled data, an Accurately Diverse ensemble yields better results than the average anomaly-detection model thus eliminating the need for a model-selection procedure.\n\nClaim 3.2. In unsupervised AD settings where the only model that can be used is a single model rather than an ensemble (for instance, due to regulatory requirements) yet due to the lack of a labeled validation set a supervised evaluation of the model cannot be performed, an Accurately-Diverse ensemble can be used to evaluate the model's predictions in an unsupervised manner, yielding results that are on par with those of supervised evaluation metrics.\n\nCombining the two claims, once we have built an Accurately-Diverse ensemble we can use it in two ways: first, we can use an aggregation of the set of models that were selected for the ensemble as our unsupervised predictive model. Second, we can use the ensemble to evaluate other models in an unsupervised manner. In the next sections, we provide both the technical procedure for building an Accurately Diverse ensemble and algorithms for using the ensemble for the two above applications."}, {"title": "4 Building an Accurately Diverse Ensemble", "content": ""}, {"title": "4.1 Distinctive observations", "content": "We define highly distinctive observations as follows:\n\nDefinition 1. A highly distinctive observation is an observation to which at least one ensemble member gave a high anomaly score.\n\nGiven the above definition, a natural definition of lowly-distinctive observations is the following:"}, {"title": "Definition 2.", "content": "A lowly-distinctive observation is an observation to which non of the ensemble members gave a high anomaly score, but neither member gave an extremely low anomaly score."}, {"title": "4.2 High-level features", "content": "Anomaly-detection models are usually just the first step in a pipeline; oftentimes, the anomaly scores predicted by the model will not serve as the final output of the pipeline but instead will serve as the input for another step in the pipeline, in which a human analyst assigns a given treatment to a subset of the observations in the dataset. In such cases, due to the large amount of time and cost of treating all the observations that received a high anomaly score by the model, the analyst will perform the treatment in a top-to-bottom manner. For instance, if the treatment is simply a manual validation of the top ranked observations in terms of predicted anomaly scores, the analyst will first manually validate the top \u03b1\u03b7-ranked observations on the list of predicted anomaly scores; then, depending on various factors such as time and degree of error tolerance, she will manually validate the top \u03b4\u03b1\u03b7-ranked anomalies; this process will continue until the top \u03b1 = \u03b7\u03b4-ranked observations are validated, where \u03b7 denotes the contamination factor, and for 1 + \u03b5 >= \u03b4 >= 1.\n\nIn such a setting, the main factor that determines the treatment probability of an observation is not its absolute position, but instead, the cluster a set of rank indices within which it lies on the ranked list. The most common formalization of clusters parametrizes \u03b1 using \u03b7 and partitions the dataset into C = 4 clusters:\n\nCluster #1: [1, \u03b7\u03b31\u03b7], 0 < \u03b31 < 1: highest-confidence outliers.\n\nCluster #2: [(\u03b7\u03b31\u03b7) + 1, \u03b7\u03b7]: lowest-confidence outliers.\n\nCluster #3: [(\u03b7\u03b7) + 1, \u03b7\u03b7\u03b32], \u03b32 > 1: lowest-confidence inliers.\n\nCluster #4: [(\u03b7\u03b7\u03b32) + 1, n]: highest-confidence inliers.\n\nIn our experiments, we found 0.25 <= \u03b31 <= 0.5, 3 <= \u03b32 <= 5 to work best.\n\nIn a real-world production environment, the question of whether an observation, i, was mapped to rank cluster 1 or rank cluster C is significantly more important than whether it was ranked in position x or position x + 1, since the decision whether i will be treated before it is sent out to the next pipeline's node is solely determined by the cluster to which i is mapped and the hyperparameters \u03b31, \u03b32. This illustrates the fact that oftentimes, the most informative features of a model's predictions are not low-level features such as the exact scores each observation received by the model, but rather more high-level, generalizable features such as the cluster to which the exact score was mapped. In the next subsection, we show how both low-level features such as rank index positions and high-level features such as rank cluster positions can be used to design new correlation metrics that better capture intra-ensemble agreement."}, {"title": "4.3 Agreement among ensemble members", "content": "The simplest method to define agreement among ensemble members is via the intersection of their binary predictions. Such a method, however, is too coarse-grained and thus might fail to capture the underlying structure of the decision-making mechanism of each ensemble member. Thus, instead of using the binary prediction vectors of each model for measuring intra-ensemble agreement, we use the models' score vectors. We apply the rank transformation to the score vectors for normalization purposes so that the correlation is not biased toward one of the members. The task of measuring the agreement among ensemble models is therefore reduced to defining a proper notion of correlation between the M ranked anomaly score lists, {rm|m \u2208 M}."}, {"title": "4.3.1 Rank correlation metrics", "content": "The most commonly-used rank correlation metrics are Spearman's \u03c1 and Kendall's \u03c4. Kendall's \u03c4 measures correlation as the number of opposite (\"discordant\") pairs in the two lists. The notion of a \"discordant\" pair, expressed using \u00a7, is used to define Kendall's \u03c4 (T2):\n\n$T = \\frac{\\sum_{i=1}^{n} \\sum_{j>i} \\mathbb{1}_{r_1,r_2}(i,j)}{n(n - 1)/2}$                                                      (1)\n\n$\\mathbb{1}_{r_1,r_2}(i,j) = \\begin{cases} 1 & \\text{if } sgn(r_1[i] - r_1[j]) = sgn(r_2[i] - r_2[j]) \\\\ 0 & \\text{if } sgn(r_1[i] - r_1[j]) = -sgn(r_2[i] - r_2[j]) \\end{cases}$                                        (2)\n\nFor simplicity of notation, we denote observations i, j using their index in the dataset, D. For instance, r1[i] denotes the ranked anomaly score which model m1 predicted for the observation residing at the ith index of the dataset.\n\nWe argue that existing rank correlation methods cannot be used for accurately measuring the correlation between multiple lists that represent predicted, ranked anomaly scores. First, Kendall's \u03c4 implicitly assumes that all the observations have an equal contribution to the correlation between the two ranked lists. That implicit assumption oftentimes doesn't accurately represent the correlation we would like to capture between the predicted anomaly scores of two models. Assume that we have only two models in the ensemble and for two observations, i, j, we observe the following ranks: r1[i] = 1, r1[j] = n \u2212 1, r2[i] = n 2, r2[j] = 2. i and j will be considered a discordant pair, and their contribution to the final correlation will be 0. Now assume we are given the ranks of another pair, i*, j*:"}, {"title": "4.3.2 Multi-way correlation metrics", "content": "We now describe how to extend both T2 and T2 to the multi-way case that is needed for measuring the degree of correlation among the M models in the ensemble.\n\nOur multi-way, exact correlation metric, TMm, extends T\u2082 to the multi-way case using the notion of the \"largest concordant set\" where the agreement among the M models is quantified as the largest subset of models that induces the same type of relation on i, j. As T2, TMm is weighted so the correlation can be biased towards certain observations based on their distinctiveness level (Subsection 4.4). A pseudocode of TMm can be found in the Appendix [13].\n\nOur multi-way, fuzzy correlation metric, TM, extends T\u2082 by combining the advantages of exact-rank-based discordance and fuzzy-rank-based discordance thus balancing the correlation so it is neither too general nor too fine-grained using the following two key ideas:\n\n1. Unlike 1,2, we enable discordance that is based on opposite rank index positions; however, unlike r1,r2, discordance that is based on opposite rank index positions is only allowed between observations i, j that belong to the same cluster both in r1 and r2. That is, we only consider intra-cluster exact-rank-based discordance. Inter-cluster exact-rank-based discordance is not considered.\n\n2. The definition of exact-rank-based discordance is relaxed; for each pair, i, j, the relaxation is proportional to the cluster size to which i and j are mapped.\n\nThe discordance level of i and j is measured as follows:\n\n$\\mathbb{1}^{r_1,r_2}(i, j) = \\begin{cases} 1 & \\text{if } \\beta(r_1[i]) \\neq \\beta(r_2[i]) \\lor \\beta(r_1[j]) \\neq \\beta(r_2[j]) \\\\ 1 & \\text{if } \\beta(r_1[i]) = \\beta(r_2[i]) \\land \\beta(r_1[j]) = \\beta(r_2[j]) \\land \\beta(r_1[i]) \\neq \\beta(r_1[j])\\\\ 1 & \\text{if } \\beta(r_1[i]) = \\beta(r_2[i]) = \\beta(r_1[j]) = \\beta(r_2[j]) \\land sgn(r_1[i]-r_1[j]) = sgn(r_2[i]-r_2[j] \\pm \\delta|\\beta(r_1[i])|) \\\\ 0 & \\text{if } \\beta(r_1[i]) = \\beta(r_2[i]) = \\beta(r_1[j]) = \\beta(r_2[j]) \\land sgn(r_1[i]-r_1[j]) = -sgn(r_2[i]-r_2[j] \\pm \\delta|f(r_1[i])|) \\\\ 0 &  \\end{cases}$                                               (7)\n\nOur multi-way, fuzzy correlation metric is based on the two-way discordance definition in ri,r2, extended to the multi-way case using the notion of the \"largest concordant set\" as shown in Algorithm 1: the largest concordant set is the set of models M' \u2286 M such that for every two models in M', m*, m**, \u03b2(rm*[i]) = \u03b2(rm**[i]) = C1 and \u03b2(rm*[j]) = \u03b2(rm**[j]) = c2, and either c1 \u2260 C2 or, c1 = c2 and within that cluster, sgn((rm*[i] - (rm*[j]) = sgn((rm**[i] - (rm**[j] \u00b1 \u03b4|(C1)|). Further details on both TM and Fri,r2 are given in the Appendix [13]."}, {"title": "4.4 Putting it all together", "content": "The key idea for measuring the extent to which an ensemble is Accurately Diverse is assessing the degree of both the exact rank correlation and fuzzy rank correlation of its members' predictions, each time using a different weighted version of the dataset. Specifically, we use two multi-way rank correlation metrics: an exact-rank correlation metric based on rank indices, TMm, and a fuzzy-rank correlation metric based on rank clusters, TM. Each correlation metric is computed using a different set of weights, which bias the correlation towards a different type of observations in the dataset based on the observations' distinctiveness level: when computing a fuzzy-rank correlation metric based on rank clusters, we assign higher weights to observations which at least one model found to be highly anomalous; on the other hand, when computing an exact-rank correlation metric based on rank indices we assign higher weights to observations which (a) all the models found to be inliers and (b) no model found to be an extreme inlier.\n\nThe requirement for a strong intra-ensemble agreement on the general trend of each ensemble member's predictions is approximated by the ensemble obtaining a high degree of fuzzy rank correlation computed using a multi-model weighting scheme, \u03a9\u00ba, that up-weights strong outliers. The requirement for a strong intra-ensemble disagreement on the exact ordering of each ensemble member's predictions is approximated by the ensemble obtaining a low degree of exact rank correlation computed using a multi-model weighting scheme, \u03a9\", that upweights non-extreme inliers.\n\nAs noted in Subsection 4.3, our multi-model weighting scheme, \u03a9, is composed of two components: a rank index aggregation function, \u03a9, and a weighting scheme, w, applied to the aggregated rank index.\n\nWeighting scheme The weighting scheme, w, that we found to work best for upweighting highly-distinctive observations strong outliers, is an exponential weighting scheme:\n\n$w(i) = 1 - (1 - (e^{-(\\frac{i}{\\delta n})}))                                                    (8)"}, {"title": "5 The \"Unsupervised Ensemble Divergence\" score", "content": "In Section 3, we developed the criteria that an ensemble of AD models must follow in order to be Accurately-Diverse and claimed that it will perform better than the average single model on unsupervised AD tasks. However, there could be situations where an ensemble model cannot be used. For instance, regulatory constraints might require the use of a single model. In such cases, though a model-selection procedure is not required, a model-evaluation method is crucial for assessing the true performance of the model. Because we asspare an unsupatijsolvekttingsehunervisetavaluation methods using a labeled validation set cannot be used. In this section, we use our Accurately-Diverse ensemble to design a new unsupervised evaluation metric for evaluating anomaly-detection models. The core idea of the \"Unsupervised Ensemble Divergence\" score, UED, is to use a distance metric, tailored specifically to AD tasks, measured between the prediction of the model to be evaluated, m*, and the aggregated prediction of an Accurately-Diverse ensemble. Specifically, for each observation, i, we first compute D, the distance between the Accurately-Diverse ensemble's aggregated prediction and the candidate model's prediction. We then compute $, the ensemble's confidence level on observation i, approximated by the degree of unanimity of the ensemble members on the prediction of i. Finally, since we are evaluating an AD model and thus interested more in its accuracy on outliers, we compute the relative importance of i to the score by assigning i a weight that aggregates the position of i in both the ranked score list predicted by the candidate model and the ranked score list predicted by the Accurately-Diverse ensemble. Here, w is a rank-inverse weighting scheme in which if r[i] > r[j], w(i) < w(j). Concrete rank-inverse weighting schemes based on a cosine or a logarithmic reduction factor can be found in the Appendix [13]. The final contribution of i to the overall evaluation score is proportional to a combination of the distance, the confidence, and the weight of observation i. Finally, the score is normalized using a distance-dependent normalization factor, N.\n\nAs seen in Algorithm 2, we start by building an Accurately-Diverse ensemble as explained in previous sections and combining its members' predictions using an aggregation function (see Section 6), resulting in the list of aggregated predictions, ra, for each observation in the dataset. We then re-rank the aggregated-prediction list, resulting in a new ranked list, R(ra), and compute the distance between the following ranked lists: the ensemble's re-ranked aggregated predictions, R(ra), and the candidate model's ranked predictions, rm*. We have experimented with multiple rank distance metrics, and found a fuzzy rank distance metric based on rank clusters, Dc, to yield the best results:\n\n$D(r_{m*}, r_a) = \\sum_{i=1}^{n} |\\beta(r_{m*}[i]) - \\beta(R(r_a[i]))|$                                                        (12)\n\nAfter experimenting with multiple confidence metrics (C) we have found the following metric to yield the best results:\n\n$\\phi(i) = 1 - \\frac{\\sum_{m \\in M}|MEDIAN({r_m[i]})_{m \\in M} - r_m[i]|)}{(n-1)[M/2]}$                                                 (13)\n\nThat is, the difference between each ensemble member's rank of observation i, and the ensemble's median rank of i.\n\nThe reader is referred to the Appendix [13] for further details."}, {"title": "6 Experimental results", "content": "Our first experimental task is to evaluate the performance of our Accurately-Diverse ensemble when used as a standalone unsupervised predictive model. This task is not straightforward, as it is not immediately clear which benchmarks are appropriate for the unsupervised setting. For instance, an inappropriate benchmarking methodology would be to compare the results of the Accurately-Diverse ensemble to those obtained by other unsupervised AD models and report the ensemble as having a high performance if its performance is better than the other models that we benchmark. This method is not a valid evaluation method as it is not representative of the true setting in which the ensemble will be used: specifically, when the analyst performs the model selection process in the unsupervised setting she has no way of knowing which model out of the N candidate models performs the best. Thus, even if one of the N models, m', performs better than the Accurately-Diverse ensemble, this does not imply that the ensemble is inferior to m' since m' will probably not be selected as the model of choice. In fact, under our no-prior-knowledge assumption, it will be chosen only with a probability of 1/N. For the evaluation results to be representative of the true unsupervised setting in which our ensemble will be applied we compare our results to those of the average anomaly-detection model the average single model. Assuming that we are given the option to choose a model out of N candidate unsupervised AD models, the average single model can be evaluated in two ways:\n\n1. Average Score (AS): evaluate the performance of each one of the N candidate models using a supervised evaluation metric. Then average the results.\n\n2. Randomly-Sampled Prediction Score (RSPS): given the predictions of each of the N candidate models and an observation, i, we randomly sample one of the N predictions of i; by repeating this process for every observation, we form a new, randomly-sampled list of predictions. We then use a supervised metric to evaluate the performance of that list.\n\nThe idea behind AS and RSPS is simulating the real-world, unsupervised use case in which our ensemble will be used and in which, given N candidate models, the analyst's choice of model is practically random.\n\nTable 1 compares our ensemble model results with the results of the average single model over different datasets. For each dataset, given a pool of unsupervised models, we first build an Accurately-Diverse ensemble of size M, where, in our experiments, M = 5. We train the ensemble on the dataset and then use it to form predictions by aggregating the individual predictions of the ensemble members. After experimenting with multiple aggregation methods we found the arithmetic mean to perform best. We then re-rank the aggregated predictions and use the result as the final ensemble's output. Our pool of models is composed of N\u2248 25 of the most commonly-used unsupervised AD models; specifically, we followed [12] and used most of the models that they used, as well as some newer models [19, 11, 31]. All models were implemented using PyOD [33]. The results of the average top Accurately-Diverse ensemble and the average bottom-Accurately-Diverse ensemble for each dataset are shown in Table 1 and are compared against the results of the average single model approximated using the AS and RSPS. For evaluation purposes, we use the PR AUC and prec@n scores. The heuristic that we used in order to choose the top Accurately-Diverse ensembles is the following: we first sort the ensembles by the degree of their fuzzy rank correlation; then, out of the top-ranked ensembles in terms of fuzzy rank correlation, we choose those with the lowest exact rank correlation.\n\nAs shown in Table 1, the top Accurately Diverse ensemble consistently outperforms the average single model using both the AS and RSPS. In addition, there is a significant difference in performance between the top Accurately-Diverse model and the bottom Accurately-Diverse model. The results support Claim 3.1, according to which an Accurately-Diverse ensemble yields better results than the average anomaly-detection model. Thus, the process of creating an Accurately Diverse ensemble can be seen as the equivalent of a model-selection procedure in the unsupervised setting where no labeled data is available. Furthermore, even though existing AD ensemble models are not designed to function as model selectors per se, Table 1 shows the PR AUC results of three state-of-the-art, fully-unsupervised AD ensemble models [35, 34, 25] as well as a classic feature-bagging-based ensemble [23] on all datasets. Our results significantly outperform the results of all the ensemble methods used as baselines, demonstrating that our novel, \"complementary homogeneity-heterogeneity\"-based methodology is a superior methodology for ensemble building compared to prior methods.\n\nTable 2 shows the Spearman correlation results between the UED score and the PR AUC score. For each dataset, we used the top-Accurately-Diverse ensembles and the procedure described in Algorithm 2 to evaluate the performance of each of the N candidate models that were not selected to be part of the ensemble thus forming a vector of unsupervised evaluation results. We also evaluated the performance of the N models using the PR AUC, resulting in a vector of supervised evaluation results. We then computed the Spearman correlation between the two vectors. The multiplicative weighting scheme (Equation 4, Appendix [13]) yielded the highest Spearman"}, {"title": "7 Conclusion and broader impact", "content": "Anomaly detection without access to labeled data is a highly challenging task. While the pool of unsupervised anomaly-detection models has been steadily increasing, as was evident while performing our experiments there exists no model that achieves high performance on all the datasets. Analysts and practitioners thus face an acute problem: which model should be chosen out of all the available options? Moreover, how should the chosen model be evaluated so that the risk associated with the model's deployment can be correctly assessed? This work aims to provide a robust, generalizable, and above all accurate solution to the challenges of unsupervised model selection and evaluation for anomaly detection tasks. The novel idea of requiring the ensemble's decisions to exhibit both homogeneity and heterogeneity in a complementary manner, an idea which we practically approximate by requiring a strong intra-ensemble agreement on the fuzzy anomalous ranks of strong outliers and a strong intra-ensemble disagreement on the exact anomalous ranks of non-extreme inliers, is proven to serve as a reliable proxy for the ensemble's validity. We hope that the methodology presented in this work will not only provide a viable solution to the challenge of unsupervised model validation, but will also be used for addressing data-driven endeavors in other domains that can benefit from a new approach to balancing accuracy and diversity."}]}