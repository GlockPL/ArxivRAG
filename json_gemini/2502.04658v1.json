{"title": "SHIFTING ATTENTION TO YOU:\nPERSONALIZED BRAIN-INSPIRED AI MODELS", "authors": ["Stephen Chong Zhao", "Yang Hu", "Jason Lee", "Andrew Bender", "Trisha Mazumdar", "Mark Wallace", "David A. Tovar"], "abstract": "The integration of human and artificial intelligence represents a scientific opportunity to advance\nour understanding of information processing, as each system offers unique computational insights\nthat can enhance and inform the other. The synthesis of human cognitive principles with artificial\nintelligence has the potential to produce more interpretable and functionally aligned computational\nmodels, while simultaneously providing a formal framework for investigating the neural mechanisms\nunderlying perception, learning, and decision-making through systematic model comparisons and\nrepresentational analyses. In this study, we introduce personalized brain-inspired modeling that\nintegrates human behavioral embeddings and neural data to align with cognitive processes. We took\na stepwise approach, fine-tuning the Contrastive Language\u2013Image Pre-training (CLIP) model with\nlarge-scale behavioral decisions, group-level neural data, and finally, participant-level neural data\nwithin a broader framework that we have named CLIP-Human-Based Analysis (CLIP-HBA). We\nfound that fine-tuning on behavioral data enhances its ability to predict human similarity judgments\nwhile indirectly aligning it with dynamic representations captured via magnetoencephalography\n(MEG). To further gain mechanistic insights into the temporal evolution of cognitive processes, we\nintroduced a model specifically fine-tuned on millisecond-level MEG neural dynamics (CLIP-HBA-\nMEG). This model resulted in enhanced temporal alignment with human neural processing while\nstill showing improvement on behavioral alignment. Finally, we trained individualized models on\nparticipant-specific neural data, effectively capturing individualized neural dynamics and highlighting\nthe potential for personalized AI systems. These personalized systems have far-reaching implications\nfor the fields of medicine, cognitive research, human-computer interfaces, and AI development.", "sections": [{"title": "1 Introduction", "content": "Artificial intelligence (AI) systems are fundamentally composed of four key components: the model architecture that\nstructures the processing mechanism, the learning rules that guide parameter updates, the objective functions that\nshape the learning path, and the datasets on which they are trained. While\nconsiderable progress has been made in each of these components and created much improved AI systems, aligning\nAl with human cognition remains a significant challenge. Such alignment holds the promise of not only enhancing the interpretability and robustness of AI models but also of\ndeepening our understanding of human cognitive processes.\nHumans often rely on semantic abstraction and operate across varying levels of semantic categorization, making it\nessential for AI models to excel in both superordinate and fine-grained tasks. However, the representational strategies employed by humans and those utilized by\nAls differ, highlighting the need for more sophisticated approaches to bridge this gap. For instance,\nimages that are indistinguishable to models-which form model metamers in later stages of neural networks-often\nappear obviously different to humans, demonstrating the presence of idiosyncratic, model-specific invariances that\ndiffer from the organization and representations of human sensory systems.\nWhile the shared architectural principles between deep neural networks (DNNs) and human cortical processing play a role in brain alignment, recent studies suggest that the path to better human alignment may lie more\nin how models are trained than in their fundamental architecture. The choice of training\ncomponents, such as the objective function, significantly impacts the learning of human-like invariances. Thus, one way to make models more human is to incorporate human inductive biases into AI systems to mirror\ncognitive processes. Indeed, leveraging these biases through Bayesian model selection and contrastive learning has been\nshown to enhance human alignment and machine learning performance.\nIn this study, we aim to enhance the alignment between Als and human cognition by fine-tuning the CLIP model\nusing human behavioral embeddings and neural data. Notably, the behavioral embeddings, derived\nfrom large-scale behavioral modeling, provide interpretable dimensions that reflect human\nmental representations of objects. By training the model to incorporate this human-derived mental embedding space,\nwe enable it to predict human behavior and neural representations more accurately, while also achieving significantly\nenhanced performance on alignment benchmarks.\nWhile large-scale behavioral embeddings offer a fixed output of generalizable perceptual representations, neural data\nprovides more dynamic insights that can reflect the millisecond-level temporal evolution of human brain representations.\nMoreover, neural data has the potential to better capture individual variability across diverse\npopulations where behavior may be challenging to collect (e.g., infants, non-verbal population) [Kucyi et al., 2024a].\nTo explore this, we further introduced a dynamic neural fine-tuning process using MEG signals (CLIP-HBA-MEG) to\nbetter align the model's temporal representations with the rapidly changing activity patterns of the human brain.\nThrough training with group-level MEG representations [Hebart et al., 2023], we observed that the model generalizes\nwell across external datasets, encompassing various participants, conceptual stimuli, and visual conditions. This finding\nreinforced our confidence in the robustness and generalizability of the approach, allowing us to move forward to individ-\nalized perceptual fine-tuning with participant-specific neural data and capture unique neural dynamics and perceptual\ndifferences. Our approach holds promise for creating personalized AI systems that reflect distinct and individual\ncognitive patterns, bridging the gap between artificial and biological cognition. By embedding these individualized\npatterns into AI models, we enhance their interpretability and alignment while deepening our understanding of the\ninterplay between minds and machines."}, {"title": "2 Results", "content": "Current AI models trained on large-scale image datasets develop inductive biases based on the statistical patterns within\nthose datasets. However, a significant gap exists between machine-derived inductive biases and the way humans develop\ntheirs through perceptual experience. To address this, we introduced human-like inductive\nbiases to the CLIP Model in the form of 66 Sparse Positive Similarity Embedding (SPOSE) dimensions, derived from\nlarge-scale behavioral studies. These dimensions have been shown to effectively predict human\nsimilarity judgments for object distinctions [Hebart et al., 2020].\nWe developed CLIP-HBA-Behavior by fine-tuning the CLIP model, using the 66 SPOSE dimensions\nas textual inputs while simultaneously feeding THINGS images as visual inputs. The CLIP\narchitecture encodes both modalities-textual and visual-separately, before binding them via dot product similarity."}, {"title": "2.1 Fine-Tuning CLIP with Human Behavioral Embeddings Enhances Alignment", "content": "As shown in Figure 2A, we evaluated the behaviorally fine-tuned model's performance on a set of 48 object images\nfrom the THINGS database, which were fully sampled in a behavioral odd-one-out task. These 48\nobjects were excluded from training to prevent data leakage. To assess the alignment between the model's predictions\nand human behavior, we calculated the Spearman rank correlation (\u03c1) between the model's predicted Representational\nDissimilarity Matrix (RDM) and the ground-truth behavioral RDM. The behaviorally fine-tuned\nCLIP-HBA-Behavior achieved a Spearman correlation of 0.78 (\u03c1 < 10^{-229}) and a 95% confidence interval (CI) of\n[0.75, 0.80].\nFor comparison, we applied the same evaluation procedure to the original CLIP-ViT-L14 model. We extracted the\n768-dimensional last-layer activations as the model's best representation for each visual stimulus. The Spearman rank\ncorrelation between the CLIP-ViT model's predicted RDM and the fully sampled behavioral RDM was 0.32, with a"}, {"title": "2.2 Improved Performance on Alignment Benchmark with CLIP-HBA-Behavior", "content": "To test our model's robustness and performance, we benchmarked it against a state-of-the-art human visual similarity\njudgment metric\u2014NIGHTS (Novel Image Generations with Human-Tested Similarity), a benchmark of 20,019 image\ntriplets with human-tested perceptual similarity scores.\nThe results in Table 1 illustrate the significant improvements achieved by behaviorally fine-tuning CLIP-HBA-Behavior.\nTo comprehensively evaluate the model, we tested two feature types. First, we assessed the 768-dimensional last layer"}, {"title": "2.3 Improved Neural Alignment with CLIP-HBA-Behavior", "content": "While the enhanced behavioral correlation of our model was anticipated due to its training on behavioral data,\nunderstanding whether the model's embedding space truly becomes more human-like requires evaluating how training\nexclusively on behavioral data impacts its alignment with neural representations. Therefore, we further evaluated the\nalignment between the model's representations against neural data obtained from MEG recordings.\nAs shown in Figure 2B (top left), CLIP-HBA-Behavior demonstrated a stronger and more sustained temporal correlation\nwith neural data across time points compared to the baseline CLIP model. Additionally, CLIP-HBA-Behavior's\ncorrelation pattern and area under the curve (AUC) closely matched and slightly exceeded that of the SPOSE\nembedding model, whereas the CLIP-ViT baseline exhibited notably lower correlations and AUC.\nTo evaluate generalizability, we tested CLIP-HBA-Behavior on three external datasets containing different participants'\nneural responses to out-of-distribution object stimuli under varying visual conditions:\n\u2022 Colored Images without Background (Figure 2B, top right): Clear object stimuli with backgrounds featuring\nhumans, animals, fruits, and man-made objects.\n\u2022 Monochromatic Clear Images (Figure 2B, bottom left): Clear, monochromatic images of animate and\ninanimate objects [Grootswagers et al., 2017b].\n\u2022 Monochromatic Images with Blur (Figure 2B, bottom right): Blurry, monochromatic images of animate and\ninanimate objects.\nIn all cases, CLIP-HBA-Behavior outperformed baseline CLIP-ViT in terms of AUC. Notably, CLIP-HBA-Behavior\nconsistently peaked in neural alignment around 300\u2013400 ms after stimulus onset. This temporal pattern likely reflects\nthe model's fine-tuning on behavioral decisions and object semantics, which typically emerge later in processing, in\ncontrast to earlier stages related to primary visual processing.\nOverall, CLIP-HBA-Behavior demonstrated enhanced neural alignment across all datasets, spanning diverse image\nquality conditions and participant groups. Additionally, the performance benchmarking on the NIGHTS dataset\nhighlights the significant improvements achieved by behavioral fine-tuning, with CLIP-HBA-Behavior outperforming\nthe baseline model across both high-dimensional visual features and semantic embeddings. These results underscore that\nfine-tuning with human behavioral data not only improves the model's ability to capture human cognitive representations\nbut also establishes its robustness and performance in large-scale perceptual tasks. Additionally, we have found that\nbehavioral fine-tuning using this pipeline can be effective with a behavioral dataset as small as 100 visual stimuli,\nmaking this method agile and suitable for specialized datasets of nuanced population groups that are likely more difficult\nto scale 10."}, {"title": "2.4 Dynamic Neural Fine-Tuning Captures Millisecond-Level Neural Dynamics", "content": "Building on the success of perceptual fine-tuning with behavioral data, which are large-scale, generalizable, and\ncollected across many trials and participants, we sought to explore whether the model can improve by learning directly\nfrom human neural representations. Although typically collected from smaller participant samples, neural data can\noften be better suited to capture diverse neural profiles across different populations. While\nbehavioral embeddings provide static and generalizable perceptual representations, neural data offer dynamic insights,\nreflecting millisecond-level temporal patterns of human brain activity that cannot be captured through behavioral\nmeasures alone. Furthermore, neural data have the potential to better account for individual variability, providing a\nmore personalized lens into how visual stimuli are processed. To explore these possibilities, we\ndeveloped a dynamic neural fine-tuning process using MEG signals, as demonstrated in Figure 3. The dynamically\nfine-tuned model, referred to as CLIP-HBA-MEG, adapts its visual stimulus embeddings to align with distinct stages of\nvisual processing and the emergence of semantic representations."}, {"title": "2.5 Enhanced Behavioral and Neural Alignment after Dynamic Neural Fine-tuning", "content": "We evaluated the behavioral and neural validity of the CLIP-HBA-MEG model after fine-tuning with MEG responses\nfrom participants viewing visual stimuli in the THINGS image database. Figure 4A illustrates the temporal alignment\nof the dynamic embedding space of our model against the SPOSE behavioral embedding. Behavioral alignment\nincreased post-stimulus onset, peaking around 600 milliseconds. The peak alignment (\u03c1 = 0.65) significantly surpasses\nthe baseline CLIP-ViT's static alignment (\u03c1 = 0.32, Figure 2A) but remains below the alignment achieved by the\nbehaviorally fine-tuned CLIP-HBA model (\u03c1 = 0.78). These results suggest that, while training with neural data\ndoes not match the behavioral performance of the model compared to learning directly from behavioral data, it still\nsignificantly improves behavioral alignment over the baseline."}, {"title": "2.6 Viewable Dynamic Attention of CLIP-HBA-MEG", "content": "Leveraging the dynamic embedding spaces of the neurally aligned CLIP-HBA-MEG, we can visualize the specific\nregions and pixels the model prioritizes to produce human-like neural responses at a millisecond resolution. By\ncombining randomized input sampling of the visual stimuli [Petsiuk et al., 2018] [Kaniuth et al., 2024a] with the\nmodel's dynamically learned visual scaler, we can quantify both where and to what extent the model attends within"}, {"title": "2.7 Individualized Models Capture Participant-Specific Neural Dynamics", "content": "We have demonstrated that our dynamic neural fine-tuning process, trained on group-level MEG data, generalizes well\nacross multiple external datasets and diverse visual conditions, encompassing a wide range of participants and stimuli.\nThese findings underscore the robustness and broad applicability of our approach, reinforcing confidence in its potential.\nBuilding on this, we sought to push the boundaries of perceptual fine-tuning by tailoring the model to individual neural\nprofiles. By training on participant-specific MEG signals, we aim to capture unique differences in neural dynamics and\nsignal processing at an individualized level, advancing toward our ultimate goal of creating personalized AI models that\nreflect the distinct cognitive representations and neural patterns of specific individuals.\nAs shown in Figure 6, we extended the dynamic neural fine-tuning pipeline of CLIP-HBA-Dynamic to train 15\npersonalized models, each fine-tuned on the individual MEG data of a single participant. These\npersonalized models achieved enhanced neural alignment specific to their respective participants, exhibiting sustained\nhigher temporal alignment compared to the lower-bound noise ceiling, starting around 200 ms after stimulus onset.\nTo evaluate whether the personalized models captured consistent individual differences, we assessed the global\ncorrelation between the individualized models and the participants' ground-truth neural representations. This was done\nby comparing the differences in dynamic embeddings of the fine-tuned models to the differences in temporal MEG\nsignals among participants. Distances within the personalized models' embedding spaces and participants' MEG RDMs\nwere calculated by flattening along the time dimension and computing the Pearson distance between the resulting\nvectors 11. This approach captured both time-specific neural representations and their evolving patterns over time.\nValidated on a set of 18 stimuli (excluded from training) viewed by the same 15 participants, the analysis revealed a\nsignificant Spearman correlation (\u03c1 = 0.659, \u03c1 < 1 \u00d7 10^{-14}) between the RDMs of the personalized models and the"}, {"title": "3 Discussion", "content": "In this study, we have demonstrated that fine-tuning CLIP with human-derived behavioral and neural embeddings\nsignificantly enhances its alignment with human perception and opens the door for individualized AI models. CLIP-\nHBA-Behavior, trained on large-scale behavioral embeddings, showed a substantial improvement in predicting human\nsimilarity judgments while also indirectly capturing neural representational structure. When further refined with\nmillisecond-level MEG data, CLIP-HBA-MEG achieved even greater alignment with human neural dynamics, suc-\ncessfully modeling the temporal evolution of perceptual representations. Notably, the model generalized well across\nexternal datasets with diverse participants, visual conditions, and object categories, reinforcing its robustness beyond\nthe training set. Having established its capacity for human-aligned perception at the group level, we then adapted\nCLIP-HBA-MEG to capture individual differences in neural processing. By fine-tuning models on participant-specific\nneural data, we found that CLIP-HBA-MEG not only maintained strong alignment with group-level representations but\nalso learned individualized neural dynamics, offering a promising path toward truly personalized AI systems."}, {"title": "3.1 Scalable and Transferable Applications", "content": "Building on these encouraging findings, our first area of focus was the scalability and transferability of the CLIP-HBA\nframework. CLIP-HBA-MEG leverages dynamic neural signals to capture real-time perceptual processes and higher-\norder conceptual organization, offering a nuanced window into human perception that static models cannot provide. In\ncontrast to models that focus primarily on behavioral proxies\u2014such as those that linearly map behavioral embeddings\ninto model output or perform generalizable local and global transforms to predict human\nsimilarity ratings\u2014\u2014CLIP-HBA-MEG establishes a direct relationship with neural activity\nby explicitly capturing the emergence of information from dynamic neural signals. Recent research has shown that\neven when behavioral outcomes converge, the underlying neural dynamics can differ substantially between individuals\nSuch findings underscore the critical importance of capturing dynamic\nneural activity. While earlier models such as ReAlnet -attempt to capture cross-modal EEG-fMRI\nalignments, they lack the temporal resolution necessary to track moment-to-moment perceptual shifts. Moreover, the\nCLIP-HBA framework can be fine-tuned sequentially, influencing the entire stream of information processing, which\nuniquely positions it as both a robust model of human perception and a versatile platform that can integrate diverse\ntypes of human behavior (e.g., MEG, MRI, behavioral data, electrophysiology). This adaptability is particularly useful\nin neuroscience applications, where neural data is often limited in scale."}, {"title": "3.2 Explainable Human-Driven AI Systems", "content": "In addition to capturing real-time perceptual processes, CLIP-HBA provides fully interpretable dimensions within the\nmodel embedding space [Hebart et al., 2020] that are absent in other models designed to incorporate human inductive\nbiases. By integrating this embedding space with dynamic saliency mapping, we offer millisecond-level\ninsights into both the model's visual focus and the organization of its internal representations. Furthermore, findings\nindicate that when model decisions are transparent and systematically quantifiable, it becomes easier to evaluate the\nconsistency of predictions, identify potential biases, and isolate errors for correction. These results support the view that interpretability is essential in applications where accurate, accountable\ndecision-making is critical. Together, these advancements move AI closer to human-like cognition by promoting\nsystems whose internal operations are more accessible and amenable to rigorous oversight [Zador et al., 2023, Sinz\net al., 2019]."}, {"title": "3.3 Expandable Architectures and Modalities for Future Directions", "content": "While our study demonstrates the effectiveness of fine-tuning CLIP to align AI models with human representations\nusing behavioral and MEG data, our approach is not inherently limited to a specific model architecture, type of neural\ndata, or sensory modality. The key factors contributing to the success of our method are the carefully designed loss\nfunctions and the modified mechanisms, such as feature reweighting [Kaniuth and Hebart, 2022] and noise injections.\nThese techniques are broadly applicable to other transformer-based and deep neural network architectures, ensuring that\nour framework remains adaptable beyond CLIP. This allows our approach to be well-suited for expanding to additional\nsensory modalities. While our current work focuses on vision, future research could integrate other modalities such as\nauditory representations for multisensory learning. By fine-tuning models with neural and\nbehavioral data across multiple sensory domains, we can take the first steps toward a truly multisensory AI system-one\nthat more closely mimics the way the human brain integrates information from diverse sensory inputs. This expansion would not only enhance Al's ability to model human perception but\nalso open new possibilities for multisensory cognitive research and human-AI interaction."}, {"title": "3.4 Applications of Personalized Models", "content": "With the framework's adaptability established, we now turn to its potential for personalization\u2014a critical aspect of\naligning AI with individual human cognition. The CLIP-HBA framework represents a significant advancement in\npersonalized AI by integrating human behavioral embeddings with neural data to tailor models to individual cognitive\nprofiles. This approach is deeply rooted in recent cognitive neuroscience research, which suggests that aligning artificial\nneural networks with biological cognition can enhance both interpretability and robustness .By fine-tuning on millisecond-level MEG signals, the CLIP-HBA-MEG variant directly optimizes neural alignment,\nreflecting the neuroconnectionist framework's emphasis on incorporating biological constraints rather than solely\nfocusing on machine-centric performance. Moreover, the framework's dynamic adaptation of learned representations\nbased on neural feedback mirrors the functionality of digital twin systems widely used in industrial\nand medical applications to model dynamic systems with real-time sensor data-thus acting as a cognitive digital twin\nthat adjusts to an individual's unique perceptual and neural idiosyncrasies."}, {"title": "3.5 Limitations and Need for Diverse Data", "content": "In thinking about the potential applications of personalized models, it is important to recognize the current limitations\nand the need for more diverse data. The success of our models in aligning with human cognition highlights the need for\ndiverse and representative data. Incorporating behavioral and neural data from a broad participant pool reduces biases\nfrom homogeneous training sets, leading to more equitable and generalizable models . However,\nbeyond participant diversity, capturing the variability of real-world sensory experiences is equally important. As\ndemonstrated in Figure 2B, CLIP-HBA-MEG, fine-tuned on group-level MEG data, exhibited strong alignment with\nhuman neural representations for clear, naturalistic stimuli but showed reduced generalizability on degraded visual\ndatasets. This suggests that training exclusively on high-quality images may limit practicality and robustness. Since\nhuman perception naturally adapts to varied conditions\u2014low lighting, occlusions, and noise\u2014future datasets should\nreflect this variability by exposing the same participants to a range of visual conditions. This approach would enhance\nAl's ecological validity, making models more adaptable and effective in real-world settings.\nA major challenge in scaling these efforts lies within the difficulty of collecting large behavioral and neural datasets.\nWhile current datasets are often sufficient for traditional studies and comparative analyses, their limitations become\nincreasingly prominent when trying to apply them to AI systems. To address this, exploring data augmentation\ntechniques and leveraging generative AI to create synthetic neural and behavioral datasets could be a promising\ndirection."}, {"title": "3.6 Conclusion", "content": "Our work underscores the versatility of the CLIP-HBA framework in adapting to a rich spectrum of data modalities, as\ndemonstrated by its robust performance on large-scale, naturalistic stimulus collections. Looking ahead, systematic data\ncollection efforts-ranging from active tasks within scanning environments to naturalistic interactions in immersive\nsettings-hold the promise of revealing how human embeddings evolve when contextual demands change. By exploring\nactive sensing, where representations directly support goal-directed actions, we may uncover entirely new dimensions\nof cognitive processing. The THINGS initiative [Hebart et al., 2019, 2023, 2020] has already paved the way for training\nmodels on diverse data types, and further endeavors that include neurodiverse populations and varied behavioral contexts\nwill only enhance models like CLIP-HBA. Such efforts are not just a testament to the progress made so far but also a\ncompelling challenge to further capture and understand the intricate dynamics of human perception and cognition."}, {"title": "4 Methods", "content": "4.1 Participants\nParticipants were drawn from existing datasets involving behavioral and neural measurements related to object\nrecognition and mental representations. Behavioral data were obtained from a study that acquired extensive human\nsimilarity judgments of natural objects using the THINGS database. Neural data were\nsourced from magnetoencephalography (MEG) recordings collected in prior studies [Hebart et al., 2023] [Grootswagers\net al., 2017a] [Grootswagers et al., 2017b] . All participants provided informed consent in accordance\nwith institutional guidelines approved by the relevant ethics committees."}, {"title": "4.2 Materials and Datasets", "content": "4.2.1 THINGS Dataset and SPOSE Behavioral Embeddings\nThe THINGS dataset consists of a comprehensive compilation of 1,854 natural object concepts accompanied by\nhigh-quality images and semantic embeddings derived from the Sparse Positive Similarity Embedding (SPOSE) model\n[Hebart et al., 2019]. The SPOSE model deduces 66 interpretable human behavioral embeddings based on similarity\njudgments, capturing the semantic dimensions that structure human mental representations of objects. We subsampled\n1806 object stimuli with their corresponding SPOSE embedding for the training and testing of CLIP-HBA-Behavior,\nexcluding 48 fully-sampled stimuli data for measuring the model's behavioral alignment.\n4.2.2 MEG Data\nWe utilized two separate MEG datasets to fine-tune CLIP-HBA-MEG at both group and individual levels (Table 2).\nFor group-level training, we subsampled averaged MEG RDMs from three participants and selected 1,806 out of\n1,854 visual stimuli from the THINGS MEG dataset [Hebart et al., 2023], reserving the same 48 fully-sampled stimuli\nfor behavioral validation. This dataset was chosen for its relatively large stimulus set, ensuring generalizability and\nproviding a robust foundation for demonstrating the model's ability to learn meaningful neural representations. For\nindividual-level training, we used a dataset with 118 stimuli but a larger sample of 15 participants ,\nprioritizing the number of participants to assess the model's capacity for individualized neural tuning and capturing\ninter-individual differences. To maintain consistency across training conditions and computational efficiency, we\ndownsampled the individual-level dataset to 200Hz, matching the sampling rate used in group-level training."}, {"title": "4.3 Model Architecture and Fine-Tuning Procedures", "content": "4.3.1 Baseline CLIP Model\nWe employed the Contrastive Language-Image Pre-training (CLIP) model as our base architecture [Radford et al.,\n2021]. CLIP integrates a visual encoder, built on the Vision Transformer (ViT) architecture ,\nwith a text transformer encoder. Pre-trained on a large dataset of image-text pairs, the model aligns visual and textual\nrepresentations within a shared embedding space. For this study, we specifically employed CLIP-ViT-L/14, a variant\nwith an increased parameter count to enable richer representational learning. This same model configuration serves as\nour baseline, ensuring a fair comparison when evaluating performance enhancements.\n4.3.2 Fine-Tuning with Behavioral Data\nFigure 1 illustrates a schematic of the fine-tuning process for the CLIP model, designed to enhance alignment with\nhuman behavior by leveraging SPOSE behavioral embeddings. The model processes inputs from two modalities-image\nand text-via separate streams. The 66 SPOSE dimensions are tokenized and passed through the text encoder, resulting\nin 66 textual representations, D1... D66. Simultaneously, image stimuli from the THINGS dataset are processed\nby the vision encoder, producing corresponding visual representations, V. The features from both modalities are\nthen integrated through a dot-product projection, which computes the similarity between the visual features and text\ndimensions. This process encourages the model to process input stimuli into a human inductive bias mental embedding\nspace. The feature binding function is defined as:\ne = {V \u00b7 D_i}_{i=1}^{66}\nwhere:\n\u2022 e is the model predicted 66-dimensional embedding vector for an input stimulus\n\u2022 V is the visual representation of the image stimulus, as produced by the vision encoder.\n\u2022 D = [D1,..., D66] is the vector of 66 SPOSE dimensions, as produced by the text encoder.\nThe fine-tuning objective function uses Mean Square Error (MSE) loss, reinforcing the model's predicted embeddings\nto closely match the SPOSE behavioral target embeddings. The MSE Loss is defined as:\nL_{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (e^{(i)} - e_t^{(i)})^2\nwhere:\n\u2022 $e^{(i)}$ is the model predicted 66-dimensional embedding for the input stimulus\n\u2022 $e_t^{(i)}$ is the target SPOSE 66-dimensional behavioral embedding for the same stimulus\n\u2022 N is the total number of input stimuli in a training batch.\nTo balance fine-tuning effectiveness with computational efficiency, we employed Weight-Decomposed Low-Rank\nAdaptation (DoRA) [Liu et al., 2024] modules into the out-projection of the attention layers, specifically in the final\ntext encoder layer and the last two vision encoder layers. DoRA extends the widely used parameter-efficient fine-\ntuning (PEFT) method, LoRA [Hu et al., 2021], by decomposing pre-trained weights into separate magnitude and\ndirectional matrices. This decomposition enables distinct magnitudinal and directional learning during fine-tuning,\nclosely replicating the optimization dynamics of training directly on the original model weights. Only the DORA\nparameters are updated, while the original layers remain frozen, significantly reducing the number of necessary training\nparameters and computational demands. Furthermore, to leverage CLIP's pre-training on large image-text datasets,\nwe limit the fine-tuning updates to only the last few layers of text and vision encoders, preserving transfer learning\ncapabilities while maintaining the pre-trained model's inherent generalizability.\n4.3.3 Behavioral Predictions Evaluation\nWe evaluated the fine-tuned CLIP-HBA-Behavior model's ability to predict human behavior using a triplet odd-one-out\ntask on a held-out portion of the data of 48 sample objects. This set was fully sampled to cover a\nwide range of naturalistic objects, and these 48 object images were specifically excluded during training to prevent"}, {"title": "4.3.4 Dynamic Neural Fine-Tuning", "content": "We further developed a dynamic fine-tuning pipeline that enables the model to learn directly from neural representations\nand dynamics (CLIP-HBA-MEG) using a feature reweighting mechanism. This mechanism dynamically combines\nintermediate visual features from the vision encoder through a learnable feature reweighting matrix W. Figure 3\nillustrates this pipeline, where W operates across temporal and layer dimensions to align model representations with\nneural dynamics.\nThe feature reweighing matrix $W \\in \\mathbb{R}^{T\\times L}$ where T is the number of MEG timepoints and L = 24 is the number of\nvisual layers in CLIP's ViT-L/14 encoder. The matrix is initialized such that:\nW_{t,l} = \\begin{cases}\n1 & \\text{if } l = L \\\n0 & \\text{otherwise}\n\\end{cases}\nThis initialization strategy assigns full weight to the final layer of the vision encoder while setting all other layers'\nweights to zero, preserving a CLIP model's pre-trained reliance on the last static representation. During training, W is\nparameterized as a neural network module with constraints enforced through architectural design: each row of W is\nnormalized via min-max to ensure non-negative weights summing to 1.\nOptimization follows a two-step gradient descent procedure designed to progressively align the model's visual processing\nwith human neural representations. Initially, only the feature reweighting matrix is optimized, shifting reliance from\nstatic last-layer representations to a dynamic aggregation of intermediate visual features. This enhances temporal\ndynamics while preserving the pre-trained capabilities of Vision Transformer (ViT) layer weights.\nStage 1 - Feature Reweighting: In this phase, the ViT parameters remain frozen while only the feature reweighting\nmatrix W is optimized. To enforce temporal consistency, dependencies between neighboring time points are introduced\nthrough an average sliding window. Optimization is performed using the AdamW optimizer [Loshchilov and Hutter,\n2019] for the initial epochs until convergence. This step establishes foundational temporal layer dependencies without\naltering the underlying visual processing capabilities of the pre-trained CLIP model.\nStage 2 - Joint Fine-Tuning: Once W is pre-optimized, all 24 ViT layers join the training process. Joint optimization\nis performed using DoRA with AdamW optimizer using a new set of learning rate, where W continues to be updated\nwhile the ViT layers undergo refinement. This approach enables the model to learn time-varying feature combinations\nthrough W while refining visual representations towards the target neural data. Detailed training hyperparameters (i.e.\nlearning rates) can be found in Table 4.\nThe model incorporates temporal modulation through several key components: 1) visual feature magnitude scaling (\u03b1\u03c4)\ncontrolled by neural response richness [Carlson et al., 2011] [Carlson et al., 2013], and 2) semantic binding strength\n(BT) determined by time generalization patterns [Tovar et al., 2020b] [King and Dehaene, 2014]. Additionally, to better\nmimic the biological nature of human perception-where attention is never fully fixed on a stimulus while ignoring all\nelse-we introduce a mechanism that allows the model to \"think of something else\" by incorporating dimension-specific\nGaussian noise during semantic binding. This ensures a degree of randomness, resembling the natural variability in\nhuman neural processing:\ne = \\beta_\\tau (e_\\tau + \\epsilon_\\tau \\sigma_{e_\\tau})\nwhere $e_\\tau$ represents the raw embeddings at time \u03c4, $\\epsilon_\\tau \\sim \\mathcal{N}(0, 1)$ is Gaussian noise, and $\\sigma_{e_\\tau}$ denotes the standard\ndeviation of embeddings across the batch dimension. The noise magnitude is dynamically scaled based on time-\ndependent neural stability, derived directly from target neural RDMs. This adaptive noise injection allows the model to\ndevelop data-driven variability while preventing overfitting to specific noisy time points, such as pre-stimulus recordings\nor moments long after stimulus onset. By incorporating this mechanism, the pipeline remains robust to variations in the\ntiming of neural recordings, ensuring a more biologically plausible and flexible learning process."}, {"title": "4.3.5 Neural Data Alignment Evaluation", "content": "We validated the efficacy of our neurally fine-tuned CLIP-HBA-MEG model through both behavioral and neural\nalignment analyses. For the behavioral alignment evaluation, we used spearman correlation to compare every slice of\nthe model's dynamic RDMs against the static behavioral RDM derived the same 48 sampled stimuli, excluded from the\nneural fine-tuning process. For the neural alignment evaluation, we assessed the model's performance by comparing its\ntime-to-timepoint RDMs with average-participant MEG decoding RDMs from the THINGS MEG dataset and all\nmentioned additional validation datasets mentioned in Table 3, using Spearman rank correlation [Zar, 1972]."}, {"title": "4.4 Training Individualized Models", "content": "To account for individual differences in neural dynamics, we trained personalized models for each participant using\nthe same CLIP-HBA-MEG pipeline used for group-level training, with a set of adjusted training hyperparameters\n4 applied to"}]}