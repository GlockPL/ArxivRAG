{"title": "Leveraging Video Vision Transformer for Alzheimer's Disease Diagnosis from 3D Brain MRI", "authors": ["Taymaz Akana", "Sait Alp", "Md. Shenuarin Bhuiyan", "Elizabeth A. Disbrowe", "Steven A. Conrad", "John A. Vanchiere", "Christopher G. Kevil", "Mohammad A. N. Bhuiyan"], "abstract": "Alzheimer's disease (AD) is a neurodegenerative disorder affecting millions worldwide, necessitating early and accurate diagnosis for optimal patient management. In recent years, advancements in deep learning have shown remarkable potential in medical image analysis.\nIn this study, we present \u2018ViTranZheimer,' an AD diagnosis approach which leverages video vision transformers to analyze 3D brain MRI data. By treating the 3D MRI volumes as videos, we exploit the temporal dependencies between slices to capture intricate structural relationships. The video vision transformer's self-attention mechanisms enable the model to learn long-range dependencies and identify subtle patterns that may indicate AD progression. Our proposed deep learning framework seeks to enhance the accuracy and sensitivity of AD diagnosis, empowering clinicians with a tool for early detection and intervention. We validate the performance of the video vision transformer using the ADNI dataset and conduct comparative analyses with other relevant models.", "sections": [{"title": "1 Introduction", "content": "Until now, Alzheimer's disease (AD) has persisted as one of the most debilitating chronic neurological disorders, predominantly impacting individuals aged 65 and above [1]. From a clinical standpoint, AD is marked by a wide range of symptoms, such as memory problems, aphasia, apraxia, agnosia, problems with spatial skills, executive dysfunction, and changes in personality and behavior [2,3]. With the increasing aging population, AD is anticipated to emerge as a significant global burden in the forthcoming decades. According to the World Health Organization (WHO) [4], the prevalence of AD was estimated to be 50 million individuals in 2015, with projections indicating it will triple by the year 2050 [1,2]. Typically, the advancement of AD can be categorized into three stages: early, middle, and late [5]. The accurate diagnosis of AD in its early stage, mild cognitive impairment (MCI), is of utmost importance due to the rapid rise in its prevalence. This is essential for timely treatment and potentially delaying the progression of AD [6]. At the moment, there is no cure for AD, and treatments can only slow the disease's progress [4]. Early diagnosis is very important because the new drugs target early-stage disease [7]. AD diagnosis can be based on various techniques, including cognitive and neuropsychological tests, neurological examinations, biomarkers, genetic testing, and neuroimaging. Structural magnetic resonance imaging (sMRI) can be used to find brain pathology (such as atrophy, tumors, and lesions) and rule out causes of cognitive deficits other than AD. sMRI is a non-invasive process used in clinical settings for suspected AD without injections, surgery, or radiation exposure, making it safe and well-tolerated by patients.\nIn recent years, advances in deep learning have revolutionized medical image analysis [8]. The use of this technique has demonstrated remarkable potential in various facets of medical imaging, bringing substantial advantages to healthcare and diagnosis. Like other areas of medical image analysis, brain MRI has benefited greatly from the application of deep learning [9,10]."}, {"title": "", "content": "Deep learning techniques in image processing involve the use of artificial neural network architectures, such as Convolutional Neural Networks (CNNs) [11], Recurrent Neural Networks (RNNs) [12-14], and Vision Transformers (ViTs) [15]. CNNs use convolutional layers to scan images with learnable filters, detecting patterns and features at different scales. RNNs handle sequential data but can also be adapted for image processing tasks like image captioning. ViTs use self-attention mechanisms to process images as sequences of tokens, capturing spatial relationships and dependencies among image patches. CNNs are effective in computer vision tasks like image classification [16], object detection/tracking [17,18], and semantic segmentation [19]. They are widely used in medical image analysis, particularly in 2D and 3D ultrasound and MRI images [20].\nSince an MR brain scan is 3D data made of stacked 2D slices, a model that can describe the whole brain is needed. A 3DCNN model to maintain inter-slice relationships is the most straightforward approach, as 2DCNN cannot do so and require an additional sequential model like LSTM. However, 3DCNN has many parameters and computation is high. Slice-based methods split 3D neuroimages into 2D slices for MRI analysis [21]. However, slice-based methods analyze 3D neuroimages into 2D slices in MRI scans. When classifying 3D-MRI scans using slide-based methods, spatial features in each slice and their relationships must be considered. Pre-trained 2D-CNNs can extract slice-specific features. Deep sequence modeling models like RNN-based models can then model slice dependency.\nIn contrast to CNN-based models, ViT-based models lack an inductive bias; therefore, it is necessary to train these models with massive amounts of information. Since ViT models are pre-trained on a large number of two-dimensional data, their use on two-dimensional data becomes efficient. Since the MRI is 3D and the available data are insufficient to train the 3D ViT, it is unreasonable to expect the 3D ViT to produce similarly effective results as the standard ViT. The common method involves splitting 3D data into 2D slice arrays from plane to take advantage of transfer learning with a pre-trained 2D network model. ViT extracts slice features separately, utilizing deep sequence models like RNN to model slice dependency, ensuring relationships between slices are maintained [22\u201324]. While the use of 3D-ViT in AD applications is still limited, emerging studies show promise in leveraging their capacity to capture complex spatial features in 3D neuroimaging data [25,26].\nIn this study, we present ViTranZheimer, an approach for AD diagnosis that leverages video vision transformers to analyze 3D brain MRI data. ViTranZheimer treats 3D brain MRI as distinct 2D slices or rely on CNNs with recurrent layers such as LSTMs, allowing it to fully exploit inter-slice dependencies. This differs from existing methods in its use of video vision transformers, which utilize self-attention mechanisms to capture both short- and long-range dependencies across the entire 3D volume."}, {"title": "2 Materials and method", "content": "In this study, we propose an end-to-end approach for the classification of T1-weighted MRI data using the Video Vision Transformer (ViViT) model, applied directly to the entire MRI voxel. We introduce a novel approach where each slice of MRI is treated as a frame in a video, enabling the use of the Video Vision Transformer (ViViT) model for deep video classification. Our previous method [27] utilized Vision Transformers (ViT) for each 2D slice independently, followed by a sequential classification model to combine the features, but it lacked an end-to-end framework. In that approach, the processing of slices and their subsequent classification were handled separately. This separation limited the ability to optimize the feature extraction and classification processes in a fully integrated, end-to-end manner.\nEnd-to-end models offer several key advantages, particularly in complex tasks like medical imaging. First, they enable joint optimization, where all layers from feature extraction to classification are trained together, leading to better overall performance. This approach reduces the need for manual intervention, as the model automatically learns relevant features, minimizing human error and making the process more efficient. End-to-end models also excel at capturing complex relationships, such as spatio-temporal dependencies in 3D MRI scans, which improves accuracy. Additionally, they simplify the training pipeline by consolidating the entire process into a unified framework, streamlining the learning process and reducing potential errors from separate processing steps [28]. However, our approach leverages ViViT to capture spatio-temporal dependencies across the full 3D structure of the brain. This allows the model to maintain inter-slice relationships and extract more comprehensive features from the entire MRI volume."}, {"title": "2.1 Structural MRI data", "content": "The ADNI [29,30] dataset, created in 2003, is a comprehensive set of data on AD and related disorders. It includes data from various sources, including cognitive tests, genetic analysis, PET, and MRI. The ADNI MRI Core has created standardized analysis sets for analysis, allowing for more careful comparisons and meaningful comparisons of algorithms. This study used a medium shared standard data collection, ADNI1: Complete 3Yr 3T, to compare different brain structures and algorithms. The dataset included 351 image scans, where were randomly split into 60% training, 20% testing, and 20% validation sets. The study aimed to ensure meaningful comparisons and reduce the possibility of differences in algorithm performance due to different input data."}, {"title": "2.2 MRI pre-processing", "content": "The processing and analysis of sMRI data consists of several steps. These steps include aligning the images, registering them to a standard template such as MNI space, segmenting the images into different tissue types such as gray matter, white matter, and cerebrospinal fluid, and conducting voxel-based analyses (See Figure 1 and Table 2). Image registration is crucial for ensuring the spatial alignment of MRI scans, as it helps standardize them with a fixed-size template.\nThe Montreal Neurological Institute's MNI space is a common coordinate system for neuroimaging, based on a template brain from hundreds of healthy MRI scans. This space provides a precise reference slice for analyzing brain activity across various studies and research teams. Skull stripping, normalization, and image registration are performed using Statistical Parametric Mapping 12 (SPM 12) [31] to wrap MRI scans into the MNI-152 space. Moreover, only the central 32 slices containing significant slices are selected. The remaining slices are excluded due to background information unrelated to brain tissue"}, {"title": "2.3 End-to-End classification architecture", "content": "This section provides a comprehensive presentation of the proposed framework. We employed 3D-MRI in our approach for the classification of AD. ViTranZheimer involves feeding entire 3D brain scans as input to models, rather than slicing them into 2D images.\nBy treating the 3D MRI slices as consecutive frames in a video, we leverage a complex video understanding deep-based model to handle the long sequences of slices/frames. In this study, the recently introduced ViViT [32], a transformer-based video encoder, is employed for AD classification tasks. We used spatio-temporal attention as it lets a model capture both spatial features (information each individual frame) and temporal dynamics (changes between frames). It leads the model to discover complex patterns and connections in the MRI that cover time and space.\nFor a given 3D MRI volume with dimensions $T \\times H \\times W$ (where Tis the number of slices, and H and W are the height and width of each slice), we divide the volume into tubelets (see Figure 2). Each tubelet spans a small region both spatially and temporally (across adjacent slices). If the tubelet size is $P_t \\times P_h \\times P_w$, the number of tokens in each dimension is calculated as: $n_t = \\frac{T}{P_t}$, $n_h = \\frac{H}{P_h}$, $n_w = \\frac{W}{P_W}$.\nThen, the volumes are flattened in order to create video tokens. These tokens are extracted from the temporal, height, and width dimensions of a tubelet with dimensions t \u00d7 h \u00d7 w. This method incorporates spatio-temporal information into the tokenization process in an intuitive manner. The tubelets are flattened and projected into an embedding space as follows:"}, {"title": "", "content": "$z_0 = [E(\\text{Tubelet}_1), E(\\text{Tubelet}_2), ..., E(\\text{Tubelet}_N)] + E_{\\text{pos}}$\nHere, E(\u00b7) is a learnable linear embedding function, and $E_{\\text{pos}}$ adds positional encodings that indicate the position of each tubelet in the original 3D volume.\nOnce the tubelets are embedded, the 3D MRI data is fed into the Transformer architecture. The core innovation here is the Spatio-temporal attention mechanism, which allows the model to process both spatial and temporal information from the MRI scans simultaneously.\nEach embedded token represents a 3D tubelet, containing spatial and temporal information. The transformer uses the self-attention mechanism to compute the relationships between these tokens. For each token, queries (Q), keys ( K ), and values (V) are computed, and the attention scores a are determined by the dot-product attention formula:\n$A_{ij} = \\frac{\\exp (Q_i K_j^T / \\sqrt{d_k})}{\\sum_{i=1}^N \\exp (Q_i K_i / \\sqrt{d_k})}$\nThe self-attention layer allows the model to attend to relevant spatial and temporal regions across the 3D MRI data, ensuring that it can learn long-range dependencies between slices. The attention mechanism computes a weighted sum over the values $V_j$ for each token i:\n$\\text{Attention} (Q,K,V) = \\sum_{j=1}^N a_{ij}V_j$\nThis is applied across all tubelets, so the model can focus on specific parts of the 3D volume that are more important for Alzheimer's diagnosis.\nBefore feeding the tubelets into the transformer, we need to convert them into a suitable representation. This is where Patch Embedding comes in, which is conceptually similar to the patch-based embedding used in ViT. For each tubelet, we flatten the 3D patch into a 1D vector per: $x_i = \\text{Flatten} (\\text{Tubelet }i) \\in \\mathbb{R}^{P_h \\times P_t}$.\nThese vectors are then passed through a linear projection layer: $Z_i = W_E x_i + b_E$. Where $W_E$ and $b_E$ are the learnable parameters of the projection layer. This gives us a sequence of embeddings for the tubelets, which serve as input to the transformer.\nMoreover, ViViT architecture uses multi-head self-attention to capture multiple relationships between the tubelets in different subspaces. For each head, the attention mechanism is applied independently, and the results are concatenated. Mathematically, this can be expressed as: MHSA (Q, K, V) = [head\u2081, head 2, ..., head h]Wo. Each attention head head; is computed as: head\u2081 = Attention (Qi, Ki, Vi)."}, {"title": "", "content": "Where $Q_i, K_i, V_i$ are projections of the input embeddings for the i-th attention head. This allows the model to attend to different parts of the 3D MRI volume simultaneously, capturing diverse spatial and temporal features. After applying multi-head self-attention, the output is passed through a FeedForward Network (FFN) for further processing. The FFN consists of two fully connected layers with a ReLU activation: FFN (x) = ReLU ($xW_1 + b_1$)W2 + b2. This helps the model to learn more complex transformations of the data before classification.\nAfter passing through multiple layers of spatio-temporal attention and feed-forward networks, we aggregate the information from all tubelets using the CLS token (classification token). The CLS token gathers global information from the entire 3D MRI volume and is used for the final classification task. The final classification is computed per: \u0177 = softmax ($W_{cls}z_{CLS} + b_{cls}$). Where $W_{cls}$ and $b_{cls}$ are the parameters learned for the classification head, and $z_{CLS}$ is the embedding of the CLS token."}, {"title": "2.4 Setting", "content": "The proposed framework is constructed using the Keras framework and the TensorFlow backend. All experiments were conducted on a workstation with an NVIDIA GeForce RTX 4080 GPU and 64 GB of RAM. To test the proposed method, we conducted experiments on multi-classification tasks (CN, MCI, and AD). Table 3 contains a list of the models' specifications. The model was trained from scratch with the associated data using the Adam optimizer for a total of 1500 epochs, with a batch size of 128 and a learning rate of 1e-4. The model has a total of 466,115 trainable parameters. The configuration of training parameters is summarized in Table 3. As the callback, the model checkpoint was configured to save only the optimal solution discovered during training, based on the loss function evaluation performed during validation. During training, the metric on a validation set is monitored, and the model checkpoint is only saved when the metric improves. The model checkpoint is saved whenever a metric improves on a validation set during training."}, {"title": "2.5 Performance Evaluation", "content": "We used various classification performance metrics to evaluate the performance of the models. These include precision, F1-score, recall, and overall accuracy. Below are the definitions and equations for each of these metrics:\n\u2022 Precision measures the accuracy of positive predictions and is calculated as:\n$\text{Precision} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP) + False Positives (FP)}}$\n\u2022 Recall is essential for minimizing false negatives, ensuring that individuals with a condition are correctly identified and receive timely medical attention and is calculated as:\n$\text{Recall} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP) + False Negatives (FN)}}$\n\u2022 F1-Score balances precision and recall, providing a useful measure when both false positives and false negatives are critical, as is often the case in medical diagnoses. It is the harmonic mean of precision and recall, providing a balance between the two metrics:\n$\text{F1-Score} = 2 \\times \\frac{\\text{Precision } \\times \\text{ Recall}}{\\text{Precision + Recall}}$\n\u2022 Overall Accuracy offers a general sense of how often the model makes correct predictions, but in medical applications, precision and recall often carry more weight since the impact of false negatives or false positives can lead to missed diagnoses or overtreatment. It is calculated per:\n$\\text{Accuracy} = \\frac{\\text{True Positives (TP) + True Negatives (TN)}}{\\text{Total Number of Predictions}} = \\frac{\\text{TP + TN}}{\\text{TP + TN + FP + FN}}$"}, {"title": "3 Experimental Results and evaluation", "content": "The classification task was done on images in the coronal plane. We implemented different baseline architectures to make comparisons with the proposed method. We compared the accuracy of our classification (ViTranZheimer) using CNN and Bi-LSTM [33], as well as ViT and Transformer [27]. The CNN and ViT were utilized in order to derive the attributes of the T1-weighted MRI slices, and the Bi-LSTM model was utilized in order to classify the sequential features while maintaining the inter-association between the slices. The details of baseline model variants are listed in Table 4. Afterwards, we compared metrics like classification accuracy, model precision, recall, and F-score.\nRepeated 10-fold stratified cross-validation and testing were adopted for the evaluation of ViTranZheimer on the ADNI dataset. The cross-validation and test were done 10 times by moving the start subset of the cross-validation and test settings to the next subset. This way, each sample of all the datasets was only used for the test once. This approach offers several advantages, including the ability to reduce variance in performance estimates, utilize more training data, prevent overfitting, and ensure consistent, fair model evaluation across different machine learning models.\nIn this study, we also compare our proposed method with state-of-the-art models to fully evaluate its effectiveness. To this end, various models from the literature were compared with the proposed method."}, {"title": "3.2 Results", "content": "Our findings indicate that our model demonstrated the highest level of accuracy in classifying disease status for individuals with NC, MCI, and AD in various clinical diagnosis tasks (see Table 5). Context of multiclass disease classification on the ADNI1: Complete 3Yr 3T dataset, three distinct neural network architectures exhibited impressive performance.\nThe CNN-Bi-LSTM model achieved an accuracy of 96.479%, with strong precision, recall, and F-score values of 0.96, indicating its proficiency in correctly classifying disease categories. ViT-Bi-LSTM surpassed this performance, achieving an accuracy of 97.465% and maintaining high precision, recall, and F-score values at 0.97. Notably, ViTranZheimer outperformed both models, with an accuracy of 98.6%, accompanied by precision, recall, and F-score metrics also at 0.97.\nIn the AD class, ViTranZheimer attains a 97% true positive rate, correctly identifying 97% of AD cases, while maintaining a perfect 100% true negative rate for non-AD cases. ViT-Bi-LSTM also performs well for AD, achieving a true positive rate of 93%. CNN-Bi-LSTM, while decent, lags slightly behind with a truly positive rate of 93%. In the case of NC, ViTranZheimer demonstrates perfect sensitivity and specificity, correctly classifying all NC cases without any false positive or false negatives. ViT-Bi-LSTM and CNN-Bi-LSTM both exhibit excellent accuracy for NC and follow ViTranZheimer with true positive rates of 99% and 97%, respectively. Regarding Mild Cognitive Impairment, we obtained a 98% true positive rate and a 99% true negative rate, with only 1% false positives and 1% false negatives. All three methods demonstrate strong performance, with ViTranZheimer and ViT-Bi-LSTM achieving a 98% true positive rate, closely followed by CNN-Bi-LSTM with a slightly higher 99% true positive rate.\nOverall, ViTranZheimer excels in differentiating between AD, NC, and MCI, offering a promising diagnostic approach for AD utilizing video vision transformers and 3D brain MRI data."}, {"title": "4 Discussion", "content": "MCI is a crucial intermediate stage between normal cognitive aging and more serious conditions like AD. Its early and accurate detection is vital for timely intervention and management, as individuals with MCI are at a higher risk of progressing to AD. However, MCI classification is particularly challenging due to its subtle and often overlapping symptoms with both CN individuals and those with AD.\nIn our results, the classification of MCI is particularly strong, with very few false negatives, meaning that only a small number of individuals with MCI were misclassified. This is an important achievement, as false negatives in MCI detection could lead to missed opportunities for early intervention. The high accuracy for MCI in our model demonstrates its effectiveness in correctly identifying individuals at this intermediate stage, which is essential for better monitoring and potential treatment strategies.\nCNN-BiLSTM models are a type of neural network that combines the strengths of CNNs and BiLSTM networks. ViT-BiLSTM, on the other hand, is a hybrid deep learning architecture that combines the strengths of ViTs and BiLSTM networks. CNN/ViT-BiLSTM models typically work by first using a CNN/ViT network to extract spatial features from each slice of MRI. The extracted features are then fed into an LSTM network to learn the temporal dependencies between the frames.\nCNNs are good at capturing local spatial patterns in images, while LSTMs are good at capturing long-term temporal dependencies in videos. ViViT models, on the other hand, are a type of transformer architecture that was specifically designed for video processing. CNNs are famous for their ability to capture local patterns and features in images via convolutional layers, whereas ViTs have demonstrated promise in capturing global context and relationships in images via self-attention mechanisms. Combining these attributes may result in enhanced performance in computer vision tasks. Also, using 3CNN can handle the long sequences of frames (slices) encountered in video (3D-MRI). This model is end-to-end in the sense that it can take raw video data as input and produce the desired output directly, without the need for any"}, {"title": "", "content": "intermediate steps or preprocessing. This is in contrast to traditional video processing pipelines, which typically involve multiple separate stages, such as feature extraction, tracking, and classification.\nCNN/ViT-BiLSTM models are computationally expensive to train and deploy due to the need for two separate neural networks, while ViViT models are more efficient due to their single neural network. They are also difficult to optimize due to the joint training and optimization of the two networks. However, CNN/ ViT-BiLSTM models may be less accurate on some video processing tasks due to their specificity for video processing, while ViViT models are designed for general-purpose architecture.\nThe CNN/ViT models were trained for 100 epochs, with each individual slice undergoing 100 epochs of training. When considering the selection of 32 slices from the middle of each MRI, this amounted to a cumulative 3200 epochs for each MRI. In contrast, the ViViT model underwent a training process spanning 1500 epochs for an MRI.\nViTranZheimer outperformed the CNN-Bi-LSTM and ViT-Bi-LSTM models in AD classification accuracy, with a significant improvement of 98.6%. This improvement surpasses the CNN-Bi-LSTM model by 2.121 percentage points and the ViT-Bi-LSTM model by 1.135 percentage points, indicating a substantial enhancement in the accuracy of the ViTranZheimer model.\nTo ensure that the comparison of methods is reliable, it is important to use a common dataset. Most researchers choose data based on personal preference and availability. In this study, we used a standard data collection named ADNI1: Complete 3Yr 3T, which is publicly available. Using publicly available data allows other researchers to fairly compare their methods with ours using the same data collection."}, {"title": "5 Conclusions", "content": "This study introduces 'ViTranZheimer,' a novel end-to-end deep learning approach for AD classification that makes use of video vision transformers and 3D brain MRI data. To capture complex structural relationships, we model the 3D MRI volumes as videos and take advantage of the temporal dependencies between slices. Our research demonstrates the potential of deep learning models for AD diagnosis, with an emphasis on multi-class classification tasks involving NC, MCI, and AD cases. We investigated and compared various neural network architectures, such as CNN-BiLSTM, ViT-BiLSTM, and ViTranZheimer, revealing significant improvements in AD classification accuracy. The proposed approaches are thoroughly validated on ADNI dataset and compared to other relevant models. ViTranZheimer emerged as the top performer, achieving a remarkable 98.6% accuracy rate, demonstrating its diagnostic potential. The findings demonstrated that the proposed model exhibited superior performance and accuracy compared to alternative models, with a statistically significant advantage in terms of accuracy.\nFuture endeavors focus on MRI harmonization and ViTranZheimer to enhance AD diagnosis, enhancing accuracy and sensitivity, and fostering a comprehensive neuroimaging framework."}]}