{"title": "doScenes: An Autonomous Driving Dataset with Natural Language\nInstruction for Human Interaction and Vision-Language Navigation", "authors": ["Parthib Roy", "Srinivasa Perisetla", "Shashank Shriram", "Harsha Krishnaswamy", "Aryan Keskar", "Ross Greer"], "abstract": "Human-interactive robotic systems, particularly au-\ntonomous vehicles (AVs), must effectively integrate human in-\nstructions into their motion planning. This paper introduces\ndoScenes, a novel dataset designed to facilitate research on\nhuman-vehicle instruction interactions, focusing on short-term\ndirectives that directly influence vehicle motion. By annotating\nmultimodal sensor data with natural language instructions and\nreferentiality tags, doScenes bridges the gap between instruction\nand driving response, enabling context-aware and adaptive plan-\nning. Unlike existing datasets that focus on ranking or scene-\nlevel reasoning, doScenes emphasizes actionable directives tied\nto static and dynamic scene objects. This framework addresses\nlimitations in prior research, such as reliance on simulated data\nor predefined action sets, by supporting nuanced and flexible\nresponses in real-world scenarios. This work lays the foundation\nfor developing learning strategies that seamlessly integrate hu-\nman instructions into autonomous systems, advancing safe and\neffective human-vehicle collaboration. We make our data publicly\navailable at https://www.github.com/rossgreer/doScenes", "sections": [{"title": "I. INTRODUCTION", "content": "THERE is a growing need for robotic systems, especially\nautonomous vehicles, to be human-interactive. In this\nresearch, we particularly focus on human-vehicle instruction\ninteractions, where a human agent communicates a directive\nto a vehicle that should influence the vehicle's motion plan.\nWhile many of the principles discussed in this research extend\nmore generally to human-robot instruction interactions; we\nfocus on autonomous vehicles as a special case of robot\nwhose motion plans exists in a particular scale of time and\nvelocity, necessitating but also benefiting from domain-specific\ncharacterizations of instructions.\nExisting interactions of humans and vehicles can be charac-\nterized by a set of attributes such as source position [1], modal-\nity, referentiality, and temporality. Example options within\nthese attributes are summarized in Table I.\nInstructions may be described by combinations of these\nattributes, and options within an attribute are not always mutu-\nally exclusive and may be integrated in various combinations.\nFor example:\n\u2022\n\u2022\nA passenger may point to a curb cut and ask to be\ndropped off there, using verbal and gesture-based interac-\ntion from inside the vehicle, and providing a short-term\ninstruction which refers to a static object in the scene.\nA firefighter may ask a vehicle to move out of the\nway, using verbal instruction from outside the vehicle,\nand providing a short-term instruction which refers to\ndynamic scene objects.\nA police officer may use their whistle and hand-gestures\nto get a driver's attention and wave their vehicle through\nwhile directing traffic, using a combination of pseudo-\nverbal and gesture-based interaction from outside the\nvehicle, and providing a short-term instruction which\ndoes not refer to additional objects.\nIn these examples, and in this research, we focus on short-\nterm interactions, which anecdotally apply on the order of\nless than 10 seconds of motion. More specifically, these\ntypes of instructions contain all relevant information within a\nviewable proximity (e.g. no relevant landmarks or information\nbeyond the horizon of the driver's egocentric view). The\ntime itself is not a strictly-defined boundary. For example,\nin the nuScenes dataset, samples have 12 seconds of motion;\nsometimes, these 12 seconds stay within a visible horizon from\nthe temporal origin, and other times, the vehicle effectively\nmoves to an entirely new scene within 12 seconds\u00b9. Towards\nthe development of new learning strategies in autonomous\nperception and planning, we introduce the doScenes dataset, a\nnovel dataset which pairs sensor feeds, vehicle trajectories, and\nmap information with human-interactive instructions and refer-\nentiality tags. doScenes draws its name from Judea Pearl's do-\ncalculus [2], developed to identify causal effects; accordingly,\nthe human instructions provided in this dataset are intended"}, {"title": "II. RELATED RESEARCH", "content": "Before discussing datasets specific to autonomous driving,\nwe first present related research in the more general field of\nHuman-Robot Interaction (HRI), and specifically instruction-\nstyle interaction for real-world scenarios. One such dataset is\nNatSGD [3], a multimodal dataset designed to emulate natural\nhuman communications through speech and gestures. NatSGD\nis primarily designed to enable robots to understand and\nexecute real-world tasks in a natural manner, including those\nrequiring nuanced household robotics actions like cooking\nand cleaning. It stands out as one of the first datasets to\nencompass speech, gestures, and demonstration trajectories. In\nthe NatSGD framework, robot behaviors were developed by\ncreating a photorealistic simulated environment using Unity3D\nin conjunction with a customized Robot Operating System\n(ROS) plugin. Additionally, real-time inverse kinematics for\nthe robot's head movement and arms were implemented using\nBioIK. During tasks, the robot maintained eye contact with\nthe target and returned its gaze to the participant when ready\nfor the next interaction.\nAnother high-volume and diverse dataset is BridgeData V2\n4. BridgeData V2 distinguishes itself by offering coverage\nacross numerous tasks and domains in robotic learning re-\nsearch, including support for task conditioning through goal\nimages or natural language instructions. The dataset includes\nover 60,000 trajectories (50,365 expert demonstrations and\n9,731 from a randomized scripted policy) collected across\n24 environments. Data collection used an accessible, low-\ncost robot platform, making BridgeData V2 appealing for\nacademic research. The robot setup consisted of a WidowX\n250 robot arm (fixed-based) and numerous cameras, including\none RGBD camera for sensing, two RGB cameras with\nrandomized poses during data collection, and one RGB camera\nattached to the robot's wrist. In addition, the robot was\ncontrolled via a VR controller. Data collection occurred in\nindoor environments, with the majority being in toy kitchens.\nHowever, despite its scale and variety of tasks, the dataset\nis limited to low-precision and non-real-time activities, quite\ndifferent from the requirements of autonomous driving.\nThe HandMeThat [5] benchmark assesses instruction under-\nstanding and task execution within physical and social con-\ntexts, emphasizing situations and instructions with ambiguity.\nEach episode of the text-based dataset contains a sequence\nof steps taken by the human, followed by an instruction. The\nauthors propose two stages for modeling robot responses. In\nthe first stage, a robot agent observes a human agent and its\nactions and attempts to infer their end goal; in the second\nstage, the human provides a language-based instruction to the\nrobot, and the robot acts within the environment to complete\nits tasks. The robot agent needs to consider both the human's\nhistorical actions as well as the subgoal specific to human\nutterance. It is important to note that the benchmark lies within\nits operation of a text-only environment, strongly limiting its\nscope for vision-based environments, and does not address\nnon-verbal communication or dynamic interactions."}, {"title": "B. nuScenes and Natural Language in Autonomous Driving\nDatasets", "content": "nuScenes [6] is a multimodal dataset designed to fill the\ngap of capturing diverse real-world conditions necessary for\nbuilding robust autonomous driving perception systems. At the\ntime of its release, nuScenes was the largest AV dataset to\nfeature a complete 360\u00b0 field of view (FOV) AV sensor suite,\nincluding 6 cameras, 5 radars, and 1 lidar, and is also the\nfirst to include radar data using an AV approved for public\nroads. Additionally, nuScenes was the first multimodal dataset\nto capture nighttime and rainy condition data. The dataset\nincludes 1,000 manually selected scenes from two highly chal-\nlenging and dense traffic environments: Boston (Seaport and\nSouth Boston) and Singapore (One North, Holland Village,\nand Queenstown). Each scene is annotated at 2 Hz, resulting in\n1.4 million 3D bounding boxes for 23 object classes. The AV\nutilized during data collection were two Renault Zoe supermini\nelectric cars, equipped with front and side cameras with a 70\u00b0\nFOV, offset by 55\u00b0, and a rear camera with a 110\u00b0 FOV.\nThough novel in its instruction and interactivity basis,\ndoScenes is not the first dataset which features nuScenes\nannotations extended using natural language. nuScenes-QA [7]\ncombined nuScenes' 3D detection annotations with question\ntemplates, automatically generating 460K question-answer\npairs based on scene graphs. nuScenes-MQA (Markup Ques-\ntion Answering) [8] introduced questions and answers en-\nclosed within markups of particular objects within the visual\nscene.\nBeyond re-annotations of nuScenes, other datasets have\nbeen developed to integrate natural language information into\nthe driving environment. The Rank2Tell dataset [9] advances\nautonomous driving with multimodal data annotated with\nvisual elements in a traffic scene ranked by relevance to\nsafety, traffic rule compliance, and the dynamic context of the\nsituation. By emphasizing contextual prioritization, Rank2Tell\nprovides a benchmark for evaluating how well autonomous\nvehicle systems align with human judgment. While Rank2Tell\nmakes significant contributions to ranking-based reasoning\nand highlights the importance of competing visual elements,\nit is limited to scene-level understanding and is only based\non certain specific key-frames of the traffic scene video. It\nlacks actionable instructions for motion planning, restricting\nits applicability to real-world driving scenarios where vehicles\nmust respond to specific directives. doScenes addresses this\ngap by bridging the divide between multimodal reasoning and\nactionable instructions. Annotations can be directly tied to ob-\njects of importance in traffic scenes, focusing on the execution"}, {"title": "C. Bridging Human-Robot Instruction and Autonomous Driv-\ning", "content": "Unlike previously-mentioned datasets which emphasize\nscene understanding and description, our research is the first\npublic real-world dataset to provide driving instructions and\nreferentiality information as the natural language annotation,\ncreating a link between imperative language and motion for\nautonomous vehicles. This task is relevant for autonomous\nvehicles due to sudden changes to the environment which\ncan create novel or anomalous scenarios, even in spatial\nlocations which may have been typical in moments prior.\nVision-language models have been successful in detecting such\nscene changes [15]. Such anomalies may require a manual\ntakeover response [16], [17], [18], but in cases where a driver\nis unable to operate the vehicle, the ability of the vehicle to\nautonomously respond to commands can be especially valu-\nable; this task of navigation of a novel environment without\na map but with natural language instruction is often referred\nto as Vision-and-Language Navigation (VLN) [19], [20]. The\ntask of translating language to actuated action is complex, re-\nquiring reasoning, closed-loop planning, and control. NaVILA"}, {"title": "III. DOSCENES DATASET", "content": "The process of collecting and annotating driving-instruction\ndata is a complex task; a test vehicle must be properly\nequipped with appropriate sensors for perception, and interior\nmicrophones must capture and synchronize verbal commands\nto driving events. Following collection, expensive annotation\nof objects and visual scene features must occur to enable\nsupervised learning. Fortunately, massive datasets such as\nnuScenes provide completion of large portion of this task\n(that is, large-scale collection and vision-based annotation).\nWe apply retroactive annotation of driving instructions by\nplaying back each of the 1,000 12-second nuScenes clips, and\ntranscribing an instruction (or lack of instruction) that would\nbe given to a driver from the vantage of the passenger to\ninitiate the motion plan observed in the clip.\nThis natural language instruction can be generated by a\nheuristic we name the taxi test: if you were being driven\nthrough this scene by a taxi driver, what instruction, if any,\nwould you need to give an instruction to trigger the behavior\nobserved in the video?\nFor each of the 1,000 scenes in nuScenes, we provide a set\nof instruction annotations. These annotations are generated by\nfive independent annotators, and each annotator may include\nmultiple annotations for a scene if they imagine multiple\ninstructions which may generate a similar series of events.\nAn instruction field may be blank if no instruction inter-\naction is needed to 'cause' the action (e.g. waiting at a red light,\ncontinuing in your lane with the flow of traffic, etc.). Referring\nto the taxi test, instructions should instigate a change from\ndefault vehicle motion.\nIn addition to the annotated instruction, we provide an\nadditional column for instruction referentiality. When an in-\nstruction refers to dynamic objects, e.g. \"follow the white van\u201d,\nthe dynamic reference tag is given. When an instruction refers\nto static objects, e.g. \"stop at the blue sign\", the static reference\ntag is given. It is possible for an instruction to be annotated\nwith zero, one, or both of these tags.\nWe note that even though an instruction may not be ref-\nerential, it is still expected that the autonomous vehicle (or\ndriver) is fully aware of the major static and dynamic objects\nin the scene at all times. This is a prerequisite for safe\nautonomous driving, independent of instruction interactions.\nRather, the dynamic referentiality tag is intended to indicate\nwhich instructions may require further observation of an\nobject than is available at the moment of instruction. Such\ninstructions cannot be evaluated in an \"open loop\" manner,\nsince the playout of the scene's dynamic objects will influence\nthe ego motion plan."}, {"title": "IV. CONSIDERATIONS FOR APPLICATION AND\nEVALUATION", "content": "In this section, we provide some observations on the nature\nof the annotated instructions and their relationship to nuScenes\ndata, with possible implications in how this data may be useful\nfor learning, and where some limitations may exist.\nWhile the instructions in doScenes provide information\nabout where a vehicle should move, it does not necessarily\nprovide information about how the vehicle should move, e.g.,\ndriving speed or style. This is primarily due to the retroactive\nannotation approach applied; in future datasets, instructions on\ndriving style or speed may be provided to instigate particular\ndriving responses. Accordingly, motion plans generated by\nnatural language learned from doScenes may not be responsive\nto prompts related to speed or style.\nFurther, the 12-second duration of the scenes in the\nnuScenes dataset, especially when taken at free-flowing urban\ntraffic speeds, may present motion plans longer than a single\ninstruction can cover. This should be accounted for when\nusing doScenes as a basis for evaluation; accurate response\nto a prompt may be reflected in only the first t seconds of a\nnuScenes path before the instruction becomes irrelevant after\na significant change of scenery or transition to later stages of a\nmulti-step motion plan. This also opens for future research the\nconsideration of frameworks for multi-stage motion planning\nusing natural language.\nWe chose to create the tags for static and dynamic referential\ninstructions so that motion plans and associated models can\nbe trained or evaluated over particular sets (e.g. those without\nreference to certain types of objects). For example, models\nwhich use only the rasterized map as input, which have\nfound decent success in prior trajectory prediction tasks [25],\n26, [27], may be able to learn appropriate motion plans for\nnon-referential instructions, but would be limited without the\nLiDAR or front-view image as input for making sense of\nobject references.\ndoScenes was designed to provide data for systems to learn\na relationship between instructions and vehicle motion. If\nsuch a model can be learned, future research may include\ntechniques to use this model to generate a trajectory based\non natural language, or assign a natural language descriptor to\na vehicle trajectory, contributing to the task of interpretable,\ninteractive autonomous vehicle motion planning.\nAs an example of models which may be extended from\nvision-language to vision-language-action based on prompted\ninstruction, SpatialRGPT [28] learns representations at the\ninstance level (rather than global level) from 3D scene graphs\nand integrates depth information to enhance VLMs' spatial\nperception and reasoning capabilities. Importantly, nuScenes\nprovides 3D input in the form of LiDAR point clouds, making\nit an appropriate dataset for this VLM, and the annotations\nof doScenes create possibilities for learning information about\nthe corresponding actions to instructional inputs, which can be\nexplored as future research enabled by this dataset, crossing\nfrom the generalized robotics domain to the specific challenges\nof autonomous driving."}, {"title": "V. CONCLUDING REMARKS AND FUTURE RESEARCH", "content": "In addition to the application areas for future research\nidentified throughout the paper, in this section, we would\nlike to highlight future research potential for data collection\nbeyond doScenes. doScenes is a novel form of autonomous\ndriving data where natural language instructions are paired\nwith driving scenes and respective sensor time series. How-\never, the instructions in this case are annotated retroactively;\nwhile the annotators give their best estimate of an instruction\nthat would have caused such a scene to unfold, this is only\na proxy for a true signal. Future data collection should pair\ntrue human instructions with action responses. There are a\nvariety of settings (both naturalistic and experimental) which\ncan allow for such collection, and it is reasonable to expect\nthat a higher volume of such high-quality data will enable\nbetter learning of corresponding VLA models."}]}