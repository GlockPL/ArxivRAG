{"title": "AnomalyLLM: A Graph-based Long-term Dependency Modeling Approach for Trajectory Similarity Computation", "authors": ["Shuo Liu", "Di Yao", "Lanting Fang", "Zhetao Li", "Wenbin Li", "Kaiyu Feng", "Xiaowen Ji", "Jingping Bi"], "abstract": "Detecting anomaly edges for dynamic graphs aims to identify edges significantly deviating from the normal pattern and can be applied in various domains, such as cybersecurity, financial transactions and AIOps. With the evolving of time, the types of anomaly edges are emerging and the labeled anomaly samples are few for each type. Current methods are either designed to detect randomly inserted edges or require sufficient labeled data for model training, which harms their applicability for real-world applications. In this paper, we study this problem by cooperating with the rich knowledge encoded in large language models(LLMs) and propose a method, namely AnomalyLLM. To align the dynamic graph with LLMs, AnomalyLLM pre-trains a dynamic-aware encoder to generate the representations of edges and reprograms the edges using the pro- totypes of word embeddings. Along with the encoder, we design an in-context learning framework that integrates the information of a few labeled samples to achieve few-shot anomaly detection. Experiments on four datasets reveal that AnomalyLLM can not only significantly improve the performance of few-shot anomaly detec- tion, but also achieve superior results on new anomalies without any update of model parameters.", "sections": [{"title": "1 INTRODUCTION", "content": "The dynamic graph is a powerful data structure for modeling the evolving relationships among entities over time in many do- mains of applications, including recommender systems[40], social networks[3], and data center DevOps [15]. Anomaly edges in dy- namic graphs, which refer to the unexpected or unusual relation- ships between entities [23], are valuable traces of almost all web applications, such as abnormal interactions between fraudsters and benign users or suspicious interactions between attacker nodes and user machines in computer networks. Due to the temporary nature of dynamics, the types of anomaly edges vary greatly, leading to the difficulty of acquiring sufficient labeled samples of new types. Therefore, detecting anomaly edges with few labeled samples plays a vital role in dynamic graph analysis and is of great importance for various applications, including network intrusions [1, 39], financial fraud detection [13, 22], and etc.\nRecently, various techniques have been proposed to detect anom- alies in dynamic graphs. Based on the usage of labeled information, existing solutions can be categorized into three groups: supervised methods, unsupervised methods, and semi-supervised methods. Supervised methods [6, 24, 25, 37] utilize labeled training samples to build detectors that can identify anomalies from normal edges. Although they have demonstrated promising results, obtaining an adequate number of labeled anomaly edges for model training is challenging for dynamic graphs, which limits their scalability. Unsupervised methods [2, 4, 7, 18, 19, 30, 43, 47] aim to identify anomalies in dynamic graphs without the use of label informa- tion. These approaches typically rely on statistical measures [7, 18], graph topology[2, 30], or graph embedding techniques[19, 43, 47] to capture deviations from normal patterns. Without label informa- tion, they are mainly designed to detect randomly inserted edges as anomalies and are hard to extend for other anomaly types. Only one work, namely SAD [35], tries to address the problem using semi-supervised learning. However, the training data used in SAD contains hundreds of labeled samples, which is also impractical in most cases. As shown in Figure 1, with the evolution of time, the anomaly edges may change and new types of anomaly edges would emerge. For these new types, only a few (less than 10) labeled sam- ples are available for model training. Thus, the problem we aim to solve is to identify various types of anomaly edges in the dynamic graph with few labeled samples for each type. To the best of our knowledge, there is no existing work that can be directly used for this problem.\nWith the rapid progress of foundation models, large language models (LLMs) show a remarkable capability of understanding graph data[33, 44] and generalizability on new tasks[31], which of- fers a promising path to achieve few shot anomaly edges detection for dynamic graphs. However, this task is also challenging in three"}, {"title": "2 RELATED WORK", "content": "In this section, we provide an overview of existing studies related to AnomalyLLM from three perspectives: (1) graph anomaly detection (2) Large Language Models (3) few-shot learning.\nGraph Anomaly Detection. Existing graph anomaly detection methods can be broadly divided into three categories, supervised method, unsupervised method, and semi-supervised method. Most supervised methods [6, 24, 25, 37] rely on labeled data to train anom- aly detectors, which may result in poor performance due to the limited number of samples in real-world scenarios. Unsupervised methods [2, 4, 7, 18, 19, 30, 43, 47] primarily identify anomalies based on statistical measures or graph topology. These techniques mainly rely on randomly-inserted edges[45] during training, which differs from actual anomalies. Recently, with the advancement of semi-supervised techniques, a hybrid methods like SAD [35] have been proposed to incorporate both labeled and unlabeled data. How- ever, these methods rely on a considerable amount of labeled sam- ples. Nevertheless, all of these methods need the node attributes, which is not easy to obtained in dynamic graph data.\nLarge Language Models. The emergence of large language models [9, 28] has ushered in a new era of few-shot learning capa- bilities, exemplified by their application in In-Context reasoning with minimal examples. Many LLM-based methods [33, 44] are proposed to graph analysis, primarily focusing on leveraging the rich textual attributes inherent in graphs. These techniques mainly rely on modality alignment between graph representations and textual properties. However, this reliance significantly limits their applicability in scenarios where textual attributes are absent. While some efforts [14] have been made to enhance LLMs' understand- ing of non-textual data like time-series, through reprogramming"}, {"title": "3 PRELIMINARY", "content": "Let $G = [G_1, ..., G_t, ..., G_T]$ denote a sequence of graph snapshots spanning timestamps 1 to T, where each snapshot $G_t = (V_t, E_t)$ represents the state of the graph at time t with $V_t$ being the set of nodes and $E_t$ the set of edges. An edge $e_{ij} = (v_i, v_j) \\in E_t$ signifies an interaction between nodes $v_i$ and $v_j$ at time t. The structure of each snapshot is encoded in a binary adjacency matrix $A_t \\in \\mathbb{R}^{n \\times n}$, where $A_{ij}^t = 1$ if there is an edge between $v_i$ and $v_j$ at timestamp t, and $A_{ij}^t = 0$ otherwise.\nConsidering the high cost of acquiring large-scale labeled anom- aly samples in real-world scenarios, we focus on detecting anomaly edges leveraging only a minimal amount of labeled data. Note that we assume the nodes in G are relatively stable. Given a speci- fied anomaly type $I$ and related set of few anomaly edges $E_g = {\\mathcal{T}_1,..., \\mathcal{T}_a}$, where a is the number of anomaly edges, our objec- tive is to detect whether edge $e_{ij}$ in $G_t$ is an anomaly edge of type $I$ or not."}, {"title": "3.2 Overview of AnomalyLLM", "content": "As shown in Figure 2, AnomalyLLM is a LLM enhanced few-shot anomaly detection framework. It consists of three key modules: dynamic-aware encoder, modality alignment and in-context learn- ing for detection."}, {"title": "4 METHODOLOGY", "content": "As shown in Figure 2, AnomalyLLM consists of three key modules, i.e., dynamic-aware contrastive pretraining, reprogramming-based modality alignment, and in-context learning for few-shot detection. Next, we specify the details of each module respectively."}, {"title": "4.1 Dynamic-aware Contrastive Pretraining", "content": "Dynamic graphs are changing over time, leading to the difficulty in representing the structure and temporal information of the edges. Existing solutions either focus on the structure information by aver- aging the context of adjacent nodes[47] [2] or directly use sequential models to capture the temporal dynamics [19] [45], which are not sufficient for the anomaly detection. In this section, we propose the dynamic-aware contrastive pretraining to systematically model both aspects and represent the edges with their adjacent subgraphs. The whole module consists of two subparts, i.e. dynamic-aware encoder and contrastive learning-based optimization."}, {"title": "4.1.1 Dynamic-aware Encoder", "content": "Given an edge $e_{ij}$, we first con- struct structrual-temporal subgraphs $S_{ij}$, then fed it into the subgarph- based edge encoder to obtain the edge representation $r_{ij}$.\nStructural-Temporal Subgraph Construction. For an edge $e_{ij}$, we design to construct structural-temporal subgraphs for both source and target nodes. Given an edge $e_{ij}=(v_i, v_j) \\in E_t$, we first construct a diffusion matrix[19] $D_t \\in \\mathbb{R}^{N \\times N}$ of $E_t$ to select the structure context, where N represents the number of nodes in $E_t$. Each row $d_o^i$ of $D$ indicates the connectivity strength of the $i$ \u2013 th node with all other nodes in the graph $G_t$. For $e_{ij} = (v_i, v_j)$, we utilized $d_o^{v_i}$ and $d_o^{v_j}$ to select the most significant top-K adjacent nodes of $V_t$ to form $V_i^t$ and $V_j^t$ as the subgraph nodes of the source node $v_i$ and target node $v_j$ respectively. Then, we link the nodes in $V_i^t$ to its related node $v_i$ to generate $E_i^t$ and obtain the subgraphs $g_i^t = {V_i^t, E_i^t}$. Similar operations are conducted for the target node $v_j$ to obtain $g_j^t = {V_j^t, E_j^t}$. In this way, both the source and the target in $e_{ij}$ can be represented by the relevant surrounding subgraphs $g_{i,j}^t = [g_i^t, g_j^t]$.\nTo obtain the temporal context of $e_{ij}$, AnomalyLLM utilizes a sliding window $\\Gamma$ to filter a sequence of graph slices $G_f = {G_{t-\\Gamma+1}, ..., G_t}$. For each graph slice, we use the described method to construct subgraphs. Therefore, a sequence of subgraph for $e_{ij}$ can be constructed as follows:\n$\\mathcal{S}_{ij} = {g_{i,j}^\\tau  \\text{ for } t=\\tau  -\\Gamma+1, ..., t}$\n$\\mathcal{S}_{ij}$ contains not only the structure but also the temporal context of $e_{ij}$. The representation of $\\mathcal{S}_{ij}$ can be used to detect the anomaly in $G_t$.\nSubgraph-based Edge Encoder. Given the subgraph sequence $\\mathcal{S}_{ij}$ of edge $e_{ij}$, we feed them into the subgraph-based edge en- coder which synergizes the Transformer and Graph Neural Network (GNN) models to obtain edge representation $r_{ij} \\in \\mathbb{R}^{d_m}$, where $d_m$ represents the embedding dimension. Following the same setting as Taddy[19], we assume the nodes in G are stable and conduct the following four steps on the input $\\mathcal{S}_{ij}$:\n\u2022 Node Encoding. For each node $v_m^\\tau \\in V^\\tau \\text{ in every } g^\\tau \\text{ within } \\mathcal{S}_{ij}$ we construct the node encoding using three aspects, i.e., $z_v = z_{diff}(v) + z_{dist}(v) + z_{temp}(v) \\in \\mathbb{R}^{d_{enc}}$. Here, $z_{diff}(v)$ represents the diffusion-based spatial encoding capturing the global structural role of node $v$, $z_{dist}(v)$ denotes the distance-based spatial encoding, reflecting the local structural context; and $z_{temp}(v)$ provides the relative temporal information of node $v$ which is the same for all nodes at the time slice $\\tau$.\n\u2022 Temporal Encoding. We model the temporal information of nodes in $\\mathcal{S}_{ij}$ by reorganizing the node encoding into an encod- ing sequence $Z_e = [[z_v]_{g^t}]_{g^t \\in \\mathcal{S}_{ij}}$, with the dimension of $Z_e$ being $\\mathbb{R}^{(2(K+1)*\\Gamma) \\times d_{enc}}$. We feed $Z_e$ into a vanilla Transformer block to obtain the node embeddings $N_e = Transformer(Z_e)$. The dimension of node embedding, $d_{enc}$, is specified here.\n\u2022 Subgraph Encoding. Additionally, we employ GNN to gener- ate the graph representations of all related subgraphs in $\\mathcal{S}_{ij}$. For each subgraph $g_{i,j}^\\tau$, we extract the related node embeddings"}, {"title": "4.1.2 Contrastive Learning-based Optimization", "content": "AnomalyLLM em- ploys contrastive learning to optimize the parameters in the dynamic- aware encoder. To obtain negative samples and achieve anomaly detection, we follow two principles in sampling: (1) edges with different subgraphs of related nodes should not have similar em- beddings; (2) the embeddings between existing edges and randomly sampled edges should be distinguishable.\nFor edge $e_{ij}$, we check its adjacent graphs, $G_{t-1}$ and $G_{t+1}$. The sampling should include two levels, i.e. edge level and sub- graph level. As shown in Figure 3, we randomly sample a node $\\hat{v}$, where $\\hat{v} = v_{t+1}$ not directly connected to $v_i$ and generate the edge embedding $\\hat{r}_{ij}$, for the edge $e_{ij} = <v_i, \\hat{v}>$. At the edge level, we employ a Multilayer Perceptron (MLP) layer as the anomaly detector to identify whether the input edge is randomly sampled. Here, we feed the embeddings of $r_{ij}$ and $\\hat{r}_{ij}$ into the detector and employ binary cross-entropy loss to make them distinguishable:\n$L_{BCE} = -log(1 - MLP(r_{ij})) + log(MLP(\\hat{r}_{ij}))$\nAt the subgraph level, we consider the subgraph of $\\hat{v}$ as the negative sample of subgraph of $v_i$ and utilize the subgraph of $v_i$ in different timestamps as the positive sample to construct a triplet. As shown in the right part of Figure 3, we sample negative sub- graph and contrastive training triplet for node v. Since the edge embeddings are concatenations of subgraph embeddings, Anoma- lyLLM employ contrastive loss to enlarge the dissimilarity between subgraph embeddings, and the pretraining loss is the combination"}, {"title": "4.2 Reprogramming-based Modality Alignment", "content": "For few-shot detection, the representations of edges should be general enough to be adapted to various anomaly types with few labeled samples. AnomalyLLM employs LLMs as the backbone to enhance the generalization ability of edge embeddings output by the dynamic-aware encoder. This is rather challenging because of the modality difference between dynamic graphs and neural languages. Thus, we propose reprogramming-based modality align- ment techniques to bridge the gap. For simplicity, we omit the subscript and note the edge embedding with r. Taking the r as input, AnomalyLLM first reprograms it with the prototype of the word embeddings and feeds the reprogramed vector into LLMs to generate $h \\in \\mathbb{R}^d$. Both r and h are fused as the final edge embedding to input to the LLM for anomaly detection."}, {"title": "4.2.1 Text Prototype Reprogramming", "content": "Although LLMs are trained with neural languages, the learned parameters contain the knowl- edge of almost all domains and can be viewed as a world model[12]. To leverage the capability of LLM for dynamic graph analysis, we first select a subset of word embeddings and cluster them as text prototypes for reprogramming edge embeddings.\nSpecifically, given the pre-trained word embeddings of LLMs, we refine a subset of words $W \\in \\mathbb{R}^{V \\times d}$ related to dynamic graphs to generate text prototypes. In practice, we prompt the LLM with a question, i.e. Please generate a list of words related to dynamic graphs to align dynamic graph data with natural language vocabulary. The full version of this question can be found in the Appendix A.2. The output words in different rounds are combined to obtain V related words. Based on these words, we construct the text prototype with liner transformation:\n$W' = M \\cdot W$\nwhere $M \\in \\mathbb{R}^{V' \\times V}$ and V' is the number of prototypes. Given an edge embedding r, AnomalyLLM utilize multi-head cross-attention to conduct reprogramming. We use r as the query vector and employ W' as the key and value matrices. For each attention head c in {1, ..., C}, we compute the related query, key and value matrices, i.e., $Q_c, K_c V_c$. The attention operation for each head is formalized as:\n$Z_c = ATTENTION(Q_c, K_c, V_c)$\nThe outputs from all heads are aggregated to obtain $z \\in \\mathbb{R}^d$. We then add z to the edge embedding r to obtain the reprogramed representation $m \\in \\mathbb{R}^d$ of the given edge $e_{ij}$."}, {"title": "4.2.2 Pseudo Label for Anomaly Fine-tuning", "content": "In AnomalyLLM, the backbone LLM takes the reprogrammed input m as input to gen- erate the final representation vector for anomaly detection. Since the parameters of LLMs are intact, the representation of LLM may not contain the information on edge anomalies and may not suit for few-shot detection. Therefore, we utilize the randomly sampled edges (detailed in Section 4.1.2) as pseudo anomaly labels to fine- tune the parameters of the dynamic-aware encoder and anomaly detector.\nAs shown in Figure 4, we design a template of prompt for both alignment fine-tuning and in-context learning detection. The tem- plate consists of four aspects: role definition, task description, ex- amples and questions, where <Edge> is a mask token for the input edge embedding. We detail the prompt in Section 4.3.1. The instruc- tion is fed into the LLM and the hidden state of the  token is selected as the final representation vector of edge e. For concise- ness, we use $v$ to represent $v_{ij}$. This procedure can be formalized as follows:\n$H = LLM([u, m])$\nwhere $u \\in \\mathbb{R}^{L \\times d}$ is the related embeddings of instruction templates and $H \\in \\mathbb{R}^{(L+1) \\times d}$ is the last hidden layer output of the LLM. We utilize the last position of H, i.e. h for detection. Note that our backbone LLM employs causal attention to compute h. Thus, for different edges, the front parts of h are the same. We can use this character to further reduce the computation workload in the pre- training procedure.\nAs described in Section 4.1.2, an MLP layer is employed to detect the randomly selected anomalies and output an anomaly score for input edge embedding. In this module, we reuse the MLP detector and replace the input edge embedding r with the reprogramed edge embedding r. The anomaly score for an edge e is computed with $f(e) = MLP(h)$. We also used the randomly selected edges as nega- tive samples and the existing edges as positive samples to construct pseudo labels. A binary cross-entropy (BCE) loss of pseudo labels is employed to optimize the parameters of the dynamic-aware en- coder and the detector. $L_{BCE} = \u2212 log(1 \u2212 f(e)) + log(f(e))$ Note that the MLP detector is optimized in both pre-training and align- ment fine-tuning. In few-shot anomaly detection, the MLP detector cooperates with the in-context learning strategy to detect various types of anomalies. During the whole procedure, the parameters of LLM are intact."}, {"title": "4.3 In-Context Learning for Few-Shot Detection", "content": "Given a set of anomaly edges $E_{\\mathcal{T}} = {\\mathcal{T}_1,..., \\mathcal{T}_a}$ of anomaly type $\\mathcal{T}$, AnomalyLLM aim to detect whether the new edge e is an anomaly edge of $\\mathcal{T}$ or not. Considering that the pretraining procedure of AnomalyLLM has no information about the anomaly type, we need to make full use of the labeled information of $E_{\\mathcal{T}}$. In this paper, we proposed to use in-context learning that encodes edges in $E_{\\mathcal{T}}$. Next, we introduce the construction of the prompt template and few-shot anomaly detection respectively."}, {"title": "4.3.1 Prompt Template Construction", "content": "The ability of LLMs on down- stream tasks can be unleashed by in-context learning which learns from the context provided by a prompt without any additional external data or explicit retraining. Thus, how to construct the prompt template is a critical problem. In AnomalyLLM, we argue"}, {"title": "4.4 Complexity Analysis of AnomalyLLM", "content": "Due to the limitations of space, we only analyze the inference complexity here. The complexity of model training is detailed in the Appendix A.2. Given the well-optimized model, AnomalyLLM involve four parts to detect an edge $e_{ij}$, i.e., subgraph construction, dynamic-aware embedding computation, reprogramming and ICL inference of LLM.\n\u2022 For subgraph construction, AnomalyLLM select K related nodes for nodes $v_i$ and $v_j$. Cause the diffusion matrix of G at all timestamps can be precomputed, the complexity of this part is $O(\\Gamma \\times K)$ where $\\Gamma$ is the temporal window size.\n\u2022 For dynamic-aware embedding, AnomalyLLM takes the nodes in the subgraphs as input and compute the $z_{diff}(v_i)$, $z_{dist}(v_i)$ and $z_{temp}(v_i)$ for each node $v_i$ as the node features. The complexity of this part is O(3d). Then, the sequence of node features is fed to the Transformer block to obtain node embeddings, with the complexity of $O((2(K+1)\\Gamma)^2d + 2(K+1)\\Gamma d^2)$. A GNN layer and average pooling layer of subgraphs is conducted on these embeddings to generate the dynamic-aware embedding $r_{i,j}$, and the complexity is O((K + 1)^2\\Gamma d). Therefore, the complexity of this part is $O((K + 1)^2\\Gamma^2d + (K + 1)\\Gamma d^2)$.\n\u2022 AnomalyLLM utilizes self attention to reprogram $r_{i, j}$ and gen- erate m. The complexity is $O(V'd + V'd^2) = O(V'd^2)$.\n\u2022 Due to the causal attention of LLM, the hidden states of the ICL template are the same except for the last  embedding h. Thus, for the inference of LLM, AnomalyLLM precomputes and stores the intermediate hidden state of ICL template, and directly conducts O(Y) feed-forward operations to obtain h, where Y is the number of Transformer layers in LLM.\nAccording to the analysis, the complexity of detecting $e_j$ is the summarization of the four parts. Note that $\\Gamma$, K, d, V' and L are constant for AnomalyLLM, the inference complexity to detect $e_j$ is also a constant."}, {"title": "5 EXPERIMENTS", "content": "In this section, we conducted extensive experiments on Anoma- lyLLM to answer the following research questions:\n\u2022 Q1: What is the performance of AnomalyLLM in detecting dif- ferent types of anomaly with few labeled anomaly edges for each type?\n\u2022 Q2: How efficient of AnomalyLLM in model alignment and anomaly detection?\n\u2022 Q3: What are the influences of the proposed modules and differ- ent backbone LLMs?\n\u2022 Q4: What is the performance of AnomalyLLM on real-world anomaly edge detection task?"}, {"title": "5.1 Experimental Settings", "content": "We briefly introduce the experimental settings below. The detailed experimental settings can be found in the Appendix A.4."}, {"title": "5.1.1 Data Descriptions", "content": "We use four public dynamic graph datasets to evaluate the performance of AnomalyLLM. The main experi- ments are conducted on two widely-used benchmark datasets, i.e., UCI Messages [26] and Blogcatalog[34]. To evaluate the perfor- mance of AnomalyLLM on real-world anomaly detection task and test the capability of AnomalyLLM, we also employ two datasets with real anomaly, i.e. T-Finance [32] and T-Social[32], which have over 21 million and 73 million edges respectively."}, {"title": "5.1.2 Experimental Protocol", "content": "In this paper, we utilize both synthetic anomaly and real anomaly to evaluate the performance of Anoma- lyLLM. Existing dynamic graphs either have no labeled anomaly edges or only have one anomaly type. To verify the ability of Anoma- lyLLM on various anomaly types, we follow the experiments of [21] and generate three kinds of systematic anomaly types, i.e., Con- textual Dissimilarity Anomaly(CDA), Long-Path Links (LPL) and Hub-Hub Links(HHL) for UCI Messages and Blogcatalog datasets. The details of anomaly generation are described in Appendix A.4. For dynamic graphs having labeled anomaly, such as T-Finance and T-Social, we directly used the real anomaly label to conduct the experiments. In our experiments, we employ all nodes and edges to pretrain the dynamic-aware encoders and align them to the back- bone LLMs. For anomaly detection, only a few labeled edges are available. We build 1-shot, 5-shot and 10-shot labeled edges for each anomaly type to obtain the AUC results on other edges."}, {"title": "5.1.3 Baselines", "content": "We compare AnomalyLLM with seven baselines which can be categorized into three groups, i.e., general graph rep- resentation method, unsupervised anomaly detection methods, and semi-supervised anomaly detection methods. For the first group, we select DeepWalk [29] to generate the representations of edges. For unsupervised method, we employ the recent three works, i.e. StrGNN [2], AddGraph[47], and TADDY[19], as our baselines. For semi-supervised methods, we use GDN [6], TGN [41] and SAD [35]"}, {"title": "5.1.4 Hyperparameters setting", "content": "For subgraph construction, we set the number k to be 14 and $\\Gamma$ is 4. For edge encoder, the embed- ding dimension d is 512. AnomalyLLM employs 3-layers stack of Transformer. We train UCI Messages, BlogCatalog, T-Finance and T-Social datasets with 150 epochs.During the modality alignment, we fine-tune the encoder and anomaly detector for 20 epoch. All the experiments are conducted on the 2xNvidia 3090Ti."}, {"title": "5.2 Performance Comparison", "content": "To answer Q1, we compare AnomalyLLM against seven baselines and summarize the results in Table 1. Overall, AnomalyLLM out- performs all baselines on all datasets. Compared with the gen- eral representation learning method, i.e., DeepWalk, AnomalyLLM achieve over 20% AUC improvement proving that the constructed structural-temporal subgraphs capture the dynamics of graph. For unsupervised anomaly detection methods, TADDY is the strongest baseline due to the Transformer-GNN encoder. However, it is also inferior to AnomalyLLM which can be attributed to the generaliza- tion power of LLMs. As to the semi-supervised methods, such as GDN and SAD, AnomalyLLM demonstrates notable improvements.\nFor example, the relative AUC value improvements on the UCI Message dataset for different anomaly type in the 5-shot setting are 19%, 18.5% and 20.3%, respectively. This is because AnomalyLLM employ ICL to excel the useful information of few labeled data.\nFor different anomaly types, AnomalyLLM achieves stable im- provements on CDA, LPL and HHL. We pretrain the dynamic-aware graph encoder for each dataset and detect different types of anom- aly by only replacing the embedding of few labeled anomaly edges of the ICL template. As shown in Table 1, the AUC of different anomaly types are over 80% indicating that AnomalyLLM is anom- aly type-agnostic. Moreover, with the increase of labeled samples, the performances of both AnomalyLLM and baseline methods are improved steadily. For example, compared to the 10-shot setting, the performance of SAD in the 1-shot setting significantly decreased,"}, {"title": "5.3 Efficiency Experiments", "content": "We study the efficiency of alignment and inference time to an- swer Q2 and prove that AnomalyLLM is flexible for different LLM backbones.\nFor the compared baselines, the fine-tuning procedure need be conducted in few-shot anomaly detection. As shown in the left part of Figure 5, the fine-tuning time increases linearly according to the number of edge sizes. For example, in 10-shot anomaly detection of 60, 000 edges, the fine-tuning time of Taddy is over 10,000 seconds. As to AnomalyLLM, there has no fine-tuning procedure in few- shot anomaly detection. We can obtain the detection results of new anomaly types by only replacing the embedding of labeled edges in ICL template. The inference time of ICL detection is shown in the right part of Figure 5. We can observe that the inference time of AnomalyLLM is comparable with other baselines under different batch sizes. This is because of the causal attention mechanism of LLMs. In model inference, the embeddings of the front part of ICL template stay unchanged for different input edges. Thus, AnomalyLLM is efficient for model inference and fine-tuning free for few-shot anomaly detection.\nFurthermore, we study the alignment time that utilizes the pseudo label on BlogCatalog dataset to align the semantics of the neural language to dynamic graphs. As shown in Table 2, we count the alignment time of each epoch training by 30000 pseudo label edges. In our experience, the alignment procedure would be convergence in 5 epoch for different LLM backbones. As illustrated, the total alignment time of 30, 000 edges is about 1200 seconds, which is ac- ceptable for replacing the LLM backbone. Therefore, AnomalyLLM is simple and efficient to be updated with more powerful LLMs."}, {"title": "5.4 Ablation Results:", "content": "To address Q3, we compare AnomalyLLM with three ablations to analyze the effectiveness of the proposed components. We remove the proposed dynamic-aware encoder, the alignment training mod- ule and the ICL detection respectively, and obtain w/o encoder, w/o ICL and w/o ICL."}, {"title": "5.5 Performance on Real-World Labeled Dataset", "content": "To answer Q4, we verify the performance of AnomalyLLM on two real-world datasets, i.e., T-Finance and T-Social, which have over 100 million edges. The results are summarized in Table 4. Overall, AnomalyLLM outperforms all baselines on all datasets. Compared with the state-of-the-art supervised learning method, i.e., TGN[41], AnomalyLLM achieve over 20.6% AUC improvement. For semi-supervised methods, i.e., GDN and SAD, AnomalyLLM"}, {"title": "6 CONCLUSION", "content": "In this paper, we are the first to integrate LLMs with dynamic graph anomaly detection, addressing the challenge of few-shot anomaly edge detection. AnomalyLLM leverages LLMs to effectively under- stand and represent the evolving relationships in dynamic graphs. We introduce a novel approach that reprograms the edge embed- ding to align the semantics between dynamic graph and LLMs. Moreover, an ICL strategy is designed to enable efficient and accu- rate detection of various anomaly types with a few labeled samples. Extensive experiments across multiple datasets demonstrate that AnomalyLLM not only significantly outperforms existing methods in few-shot settings but also sets a new benchmark in the field."}, {"title": "A APPENDIX", "content": "This section covers implementation details of the project. It includes the implementation details of the dynamic encoder, modality alignment and in-context learning. It also covers details of the baselines used in experiments."}, {"title": "A.1 Detail of Dynamic Encoder", "content": "This section provides detailed information on the design and implementation of the dynamic encoder module, which includes details on the diffusion matrix, node encoding, and temporal encoding techniques."}, {"title": "A.1.1 Calculation of Diffusion Matrix", "content": "Given the adjacency matrix $A_t \\in \\mathbb{R"}, {"A": "n$D = \\sum_{m=0}^{\\infty} \\Theta_m T^m,$\nwhere T \u2208 Rnxn is the generalized transition matrix and \u0398m is the weighting coefficient indicating the ratio of global-local information. It requires that $\\sum_{m=0}^{\\infty} \\Theta_m = 1, \\Theta_m \\in [0, 1", "1": "to guarantee convergence. Different instantiations of diffusion matrix can be computed by applying specific definitions of T and \u0398. For instance, Personalized PageRank (PPR) [27"}]}