{"title": "MARIA: a Multimodal Transformer Model for\nIncomplete Healthcare Data", "authors": ["Camillo Maria Caruso", "Paolo Soda", "Valerio Guarrasi"], "abstract": "In healthcare, the integration of multimodal data is pivotal for developing\ncomprehensive diagnostic and predictive models. However, managing miss-\ning data remains a significant challenge in real-world applications. We intro-\nduce MARIA (Multimodal Attention Resilient to Incomplete datA), a novel\ntransformer-based deep learning model designed to address these challenges\nthrough an intermediate fusion strategy. Unlike conventional approaches\nthat depend on imputation, MARIA utilizes a masked self-attention mech-\nanism, which processes only the available data without generating synthetic\nvalues. This approach enables it to effectively handle incomplete datasets,\nenhancing robustness and minimizing biases introduced by imputation meth-\nods. We evaluated MARIA against 10 state-of-the-art machine learning and\ndeep learning models across 8 diagnostic and prognostic tasks. The results\ndemonstrate that MARIA outperforms existing methods in terms of perfor-", "sections": [{"title": "1. Introduction", "content": "In recent years, multimodal learning has emerged as a powerful approach\nfor leveraging diverse data sources to achieve a comprehensive understand-\ning of complex systems. This is particularly relevant in domains such as\nhealthcare, where integrating multiple data modalities, such as clinical as-\nsessments, imaging, laboratory tests, and patient histories, can significantly\nenhance diagnostic accuracy and treatment outcomes. The human experi-\nence itself exemplifies multimodality, as it relies on diverse sensory inputs\nto form a unified perception of the environment. Similarly, deep learning\n(DL) models have been developed to synthesize and analyze disparate data\nsources, thereby enhancing their predictive capabilities and enabling more\ninformed decision-making in multifaceted domains such as healthcare.\nDespite the promise of multimodal learning, integrating multiple data\nsources presents unique challenges, with data incompleteness being one of the\nmost significant. Missing data is a common feature of real-world datasets,\narising from issues such as sensor failures, patient non-compliance, technical\nlimitations during data collection, or privacy restrictions. Whether the miss-\ning information relates to features within a modality or the complete absence\nof a modality, such gaps can severely degrade the performance of machine"}, {"title": null, "content": "learning models unless effectively addressed. Thus, the development of mul-\ntimodal learning models resilient to incomplete data is critical to ensuring\nreliability and robustness, especially in critical fields like healthcare.\nTo address these challenges, multimodal fusion strategies such as early\nfusion, late fusion, and intermediate fusion have been extensively studied.\nEarly fusion, which combines features at the raw data level into a unified rep-\nresentation, is straightforward but highly susceptible to the effects of miss-\ning data, as it requires the availability of all feature vectors. Late fusion,\nwhich merges outputs from independently trained models, offers flexibility\nwhen modalities are missing but often fails to capture the intricate interac-\ntions across modalities. Intermediate fusion strikes a balance by integrating\nmodality-specific features after initial processing. This forms a shared rep-\nresentation that enhances the ability to capture cross-modal dependencies,\nultimately improving performance [1]. Therefore, especially in healthcare, it\nis essential to develop methods that leverage the potential of intermediate\nfusion while maintaining robustness to missing data.\nThe MARIA (Multimodal Attention Resilient to Incomplete datA) model\nintroduced in this work is designed to address the challenges of incomplete\nmultimodal data. By employing an intermediate fusion strategy, MARIA com-\nbines modality-specific encoders with a shared attention-based encoder to\neffectively manage missing data. Unlike traditional methods that depend\non data imputation to fill in missing entries, MARIA focuses exclusively on\nthe available features, utilizing a masked self-attention mechanism to pro-\ncess observed information without generating synthetic data. This approach\nenhances both robustness and accuracy, while reducing biases typically in-"}, {"title": null, "content": "troduced by imputation techniques.\nThe manuscript is organized as follows: Section 2 reviews related work\non multimodal learning and data handling methods; Section 3 introduces\nthe MARIA model and its architecture; Section 4 explains the experimental\nsetup and evaluation methodology; Section 5 presents the obtained results,\ncomparing MARIA with other models under various missing data conditions;\nSection 6 summarizes the key findings and suggests directions for future\nresearch."}, {"title": "2. State-of-the-Art", "content": "Multimodal learning combines information from diverse data sources to\nachieve a more comprehensive understanding of complex systems. This mir-\nrors the inherently multimodal nature of human perception: we rely on mul-\ntiple senses, e.g., sight, sound, and touch, to develop a complete understand-\ning of our environment. Similarly, DL models must be designed to inte-\ngrate diverse data sources to better comprehend intricate systems. This is\nparticularly relevant in healthcare, where clinicians utilize multimodal data,\nincluding patient histories, imaging, laboratory results, and physical exami-\nnations, to make informed decisions. By effectively integrating such diverse\ninformation, multimodal learning models can enhance decision-making and\npredictive accuracy, leading to improved diagnostic outcomes and more ef-\nfective treatment plans [2].\nHowever, one of the primary challenges in multimodal learning is handling\nmissing data, which frequently arises due to factors such as sensor failures,\nsurvey non-responses, or technical issues during data collection [3]. Effec-"}, {"title": null, "content": "tively managing missing data, whether it involves incomplete features within\na modality or entirely absent modalities, is critical to ensuring the reliability\nand robustness of multimodal models.\nMultimodal fusion techniques play a vital role in successfully integrating\ndiverse data sources. These techniques are typically categorized into three\nmain strategies: early fusion, late fusion, and intermediate fusion (Figure 1).\nEach approach has distinct characteristics, making it suitable for different\nscenarios [2, 4].\nEarly fusion integrates features at the raw data level, combining them\ninto a unified representation before any significant processing. Late fusion\nmerges outputs from independently trained models at the decision level, of-\nfering flexibility when dealing with missing modalities. Intermediate fusion\ntakes a balanced approach, integrating modality-specific features after initial\nprocessing to create a shared representation. Each of these fusion strate-\ngies has specific advantages and limitations in terms of performance, com-\nputational complexity, and their ability to manage missing data. This is\nparticularly significant in healthcare, where data quality and completeness\noften vary. The subsequent sections provide a detailed analysis of these fu-\nsion techniques, focusing on their applications and limitations in healthcare\nscenarios."}, {"title": "2.1. Early Fusion", "content": "Early fusion, as illustrated in Figure 1.a, involves integrating multiple\ndata modalities at the feature level. In this approach, the raw features $X_i$\nfrom each modality are concatenated ($\\bigoplus$ in the figure) to form a single feature\nvector, which is then fed into the learning model. This method facilitates"}, {"title": null, "content": "the early combination of information from all available modalities, making\nit particularly advantageous when different data sources are highly comple-\nmentary. Early fusion is conceptually straightforward and often enables the\nlearning model to effectively exploit cross-modal correlations [2, 4].\nHowever, early fusion presents several challenges, especially when dealing\nwith incomplete data. Because this approach relies on the availability of all\nfeature vectors, missing data from even a single modality can significantly\ndegrade model performance. Imputation is a common strategy for addressing\nthese gaps, but it introduces potential risks such as bias and information\nloss [5]. Additionally, early fusion typically requires extensive preprocessing\nto harmonize features from different modalities, which often vary in scale and\ndistribution.\nIn healthcare, early fusion can be particularly beneficial when modalities\nare guaranteed to be complete or when missing data is minimal and can\nbe addressed through robust preprocessing techniques. However, given the\nvariability and incompleteness commonly encountered in real-world medical\ndatasets, early fusion may struggle to perform effectively without sophisti-\ncated data-handling strategies [2, 4]."}, {"title": "2.2. Late Fusion", "content": "Late fusion, in contrast to early fusion, integrates modalities at the deci-\nsion level, as illustrated in Figure 1.c. In this approach, separate models $f_i$\nare trained for each modality $X_i$, and their predictions $y_i$ are subsequently ag-\ngregated using a predefined rule to generate the final output $y$. This method\nis particularly advantageous when the modalities differ significantly in data\ntype or exhibit varying levels of reliability. By training separate models, late"}, {"title": null, "content": "fusion allows each modality to be optimally utilized before combining their\noutputs.\nOne of the primary advantages of late fusion is its flexibility in handling\nmissing modalities. Since each model operates independently, missing data\nfrom one modality does not prevent predictions from being made using the\navailable modalities. However, late fusion has a notable limitation: it fails to\nfully exploit cross-modal interactions. Because modalities are only integrated\nat the decision level using a static, predefined rule, rather than a dynamically\nlearned process, potentially rich correlations between features from different\ndata sources may remain untapped. This drawback makes late fusion less\nsuitable for tasks that require deep integration of modality-specific features,\nparticularly in scenarios demanding high levels of diagnostic precision."}, {"title": "2.3. Intermediate Fusion", "content": "Intermediate fusion, also known as joint or hybrid fusion, strikes a balance\nbetween early and late fusion. This approach combines modality-specific fea-\ntures at an intermediate stage of the learning process (Figure 1.b), typically\nafter each modality has undergone initial, independent processing. In this\nsetup, modality-specific modules $f_i$ generate latent representations $r_i$ for each\nmodality. These representations are then fused using a defined technique (de-\nnoted by $\\bigoplus$ in Figure 1.b) to form a shared representation $r_{sh}$. Finally, this\nshared representation is processed by a final module $f$ to produce the desired\noutput $y$. This approach facilitates a richer integration of features, retain-\ning modality-specific information while capturing inter-modal relationships\nduring the feature extraction phase.\nOne of the major advantages of intermediate fusion is its ability to han-\ndle incomplete data more flexibly and effectively. Various techniques are\navailable for fusing latent representations from different modalities, and we\nrecommend readers refer to the review [1] for an in-depth exploration of\nthese methods. However, intermediate fusion comes with certain challenges,\nincluding increased computational complexity and training difficulty. The\nmodel must learn both unimodal and multimodal representations simulta-\nneously, requiring significant computational and data resources. These de-\nmands can pose a barrier in resource-constrained environments, such as many\nhealthcare settings.\nDespite these challenges, the dynamic nature of intermediate fusion offers\nsignificant advantages. By allowing the model to learn how to fuse informa-\ntion from different sources dynamically, it enhances robustness and adaptabil-"}, {"title": null, "content": "ity. This integration of complementary information from multiple modalities\nenables the model to leverage the strengths of each data source while miti-\ngating their individual weaknesses. Such adaptability is particularly valuable\nin real-world healthcare scenarios, where the quality and availability of data\noften vary across modalities. Intermediate fusion's ability to handle partially\nmissing or noisy modalities can result in more reliable predictions. More-\nover, the shared representation created through intermediate fusion fosters a\ndeeper understanding of correlations between different data types, which is\ncritical for complex tasks such as medical diagnosis and prognosis [2, 4].\nDespite its resource demands, intermediate fusion represents a promising\ndirection for the development of DL models that are both comprehensive and\nresilient. This makes it a powerful approach for enhancing decision-making\nin healthcare environments [1, 2, 4]."}, {"title": "2.4. Handling Incomplete Data", "content": "Real-world multimodal data are often imperfect due to missing features\nor modalities. Therefore, there is a pressing need for multimodal models\nrobust in the presence of incomplete data. Missing data, whether involv-\ning individual features or entire modalities, is a common challenge across\nvarious fields and is often caused by issues, such as human error, survey\nnon-responses, data corruption, or systematic loss. Traditional approaches\nto address missing data typically rely on imputation techniques, which at-\ntempt to fill these gaps but can introduce biases or fail to capture underlying\ncomplexities. For instance, we employed the k-Nearest Neighbors (kNN)\nimputer, which has demonstrated effectiveness in handling missing values\nin tabular data [5, 6, 7]. Additionally, we tested the Missing in Attributes"}, {"title": null, "content": "(MIA) strategy, used by tree-based models to dynamically manage missing\nfeatures without requiring imputation.\nHealthcare settings are particularly vulnerable to the problem of incom-\nplete data, as patients may follow different treatment plans, discontinue care\nfor reasons such as transferring facilities, voluntarily ceasing treatment, or\neven passing away. Moreover, privacy concerns further exacerbate data in-\ncompleteness in these settings [6]. Many methods simply exclude patients\nwith missing values, which can significantly reduce data availability and com-\npromise the robustness of analyses. Other approaches often involve imputing\nmissing information using data from available modalities for the same subject\nor from other patients with similar characteristics.\nSeveral advanced methodologies have been proposed to address the issue\nof missing data in multimodal contexts:\nThe Contrastive Masked-Attention Model integrates a Generative Adver-\nsarial Network (GAN)-based augmentation mechanism to synthesize data for\nmissing modalities and employs contrastive learning to enhance cross-modal\nrepresentations. Masked attention ensures that only interactions between\nobserved modalities are captured, thereby minimizing the introduction of\nextraneous noise [8].\nThe Cascaded Multi-Modal Mixing Transformers implement a cascaded\ncross-attention architecture to effectively integrate multiple available modal-\nities, enabling robust performance even when some modalities are missing.\nThis approach offers flexibility and adaptability in fusing modality-specific\ninformation [9].\nThe Missing Modalities in Multimodal healthcare framework employs"}, {"title": null, "content": "task-guided, modality-adaptive similarity metrics to identify similar patients\nand impute missing data. By leveraging auxiliary information from compa-\nrable patients, this method preserves the underlying relationships in multi-\nmodal healthcare data [10].\nShared-Specific Feature Modeling disentangles shared features from modality-\nspecific ones, enabling efficient handling of missing data during both training\nand inference. By learning shared features across all available modalities, this\napproach ensures the retention of essential representations while maintaining\nmodel performance [11].\nThe Severely Missing Modality model uses a Bayesian meta-learning frame-\nwork to approximate latent representations for missing modalities. This\nmethod is designed to handle incomplete data during both training and test-\ning, offering robust generalization capabilities even when data availability is\nseverely limited [3].\nThese methodologies highlight diverse strategies for compensating for\nmissing information, including identifying shared latent representations, gen-\nerating synthetic data, and leveraging auxiliary patient information. By\nreducing dependence on complete multimodal datasets, these approaches\nimprove the practicality of multimodal models in real-world clinical and\nresource-constrained settings. However, both traditional and DL-based ap-\nproaches share a common limitation: they rely on artificially filling data gaps,\nwhich can introduce bias and compromise task accuracy.\nTo address this limitation, we propose a model that exclusively utilizes\nthe available features and modalities, avoiding the generation of synthetic\ndata. By focusing solely on effectively leveraging observed information, our"}, {"title": null, "content": "approach aims to enhance robustness and reliability, even in scenarios with\nseverely missing data."}, {"title": "3. Methods", "content": "In this work, we propose MARIA (Multimodal Attention Resilient to\nIncomplete datA), a multimodal model specifically designed to address the\nchallenges of incomplete features and modalities in multimodal healthcare\ndata. The model effectively integrates data modalities that may be incom-\nplete or entirely absent, offering a robust solution for predictive analysis\nwithout relying on traditional data imputation techniques or synthetic data\ngeneration.\nThis section first provides an overview of the MARIA model design. We\nthen focus on the architecture of the modality-specific encoders, the strate-\ngies employed for handling missing data, and the regularization techniques\nimplemented during training to enhance generalizability under incomplete\ninput conditions."}, {"title": "3.1. Model", "content": "MARIA is specifically designed to be resilient to incomplete data and\nmodalities without relying on imputation. It employs intermediate fusion,\nusing modality-specific encoders and a shared encoder with a masked self-\nattention mechanism to combine latent representations while effectively man-\naging missing data. The architecture incorporates multiple modality-specific\nencoders for each data modality. In this work, we focus on multimodal prob-\nlems where the modalities represent tabular data describing various aspects\nof a patient's condition. Thus, MARIA utilizes separate NAIM [7] modules"}, {"title": "as modality-specific encoders (Figure 2). These encoders integrate a masked\nmulti-head attention mechanism that selectively focuses on available features\nwithin each modality while ignoring missing ones. This mechanism ensures\nrobustness by excluding absent features from attention computations.\nEach tabular modality $X_i$, where $i = 1, . . ., n$, is encoded into embeddings\nusing look-up tables [7], which represent missing entries with a specific non-\ntrainable embedding. The modality embeddings are then processed by their\ncorresponding encoders $E_i$, which compute query, key, and value matrices,\ndenoted as $Q_i$, $K_i$, and $V_i$, using linear transformations:", "content": ""}, {"title": null, "content": "$\\begin{cases}\nQ_i = X_i \\cdot W_i^Q \\\\\nK_i = X_i \\cdot W_i^K & W_i^Q, W_i^K, W_i^V \\in \\mathbb{R}^{d_e \\times d_h} \\\\\nV_i = X_i \\cdot W_i^V & Q_i, K_i, V_i \\in \\mathbb{R}^{|X_i| \\times d_h}\n\\end{cases}$                                                                                                                                                                        (1)\nwhere $W_i^Q$, $W_i^K$ and $W_i^V$ are learnable weight matrices. These transforma-\ntions reduce dimensionality to $d_h$, determined by the token dimensions $d_e$ and\nthe number of heads $h$ in the model. Next, a modified masked self-attention\nmechanism is applied:\n$MSA(Q_i, K_i, V_i) = ReLU \\Bigg( softmax (\\frac{Q_i K_i^T}{\\sqrt{d_h}} + M_i) + M_i^a \\Bigg) V_i$                                                                                                                                           (2)\nwhere the masking matrix $M_i$ ensures that missing features do not influence\nthe latent representation $r_i$. The elements of $M_i$ are defined as follows:\n$M_i^{kj} = \\begin{cases}\n-\\infty & if X_i^j \\text{ is missing} \\\\\n0 & otherwise\n\\end{cases}, M_i = \\begin{bmatrix}\n0 & -\\infty & ... & 0 & 0 \\\\\n-\\infty & 0 & ... & 0 & 0 \\\\\n: & : & : & : & : \\\\\n0 & -\\infty & ... & 0 & 0 \\\\\n0 & -\\infty & ... & 0 & 0\n\\end{bmatrix}$ (3)\nThis operation effectively zeroes out weights associated with missing features\nafter applying softmax and ReLU. Each modality-specific encoder $E_i$ gener-\nates a latent representation $r_i$, of dimensions $|X_i| \\times d_e$, where $|X_i|$ represents\nthe number of tokens in the modality, e.g., the number of features in the $i$-th\nmodality. These latent representations are then concatenated to form a joint\nrepresentation $r_{sh}$, composed only of available information, with null vec-\ntors representing missing features. This multimodal representation is passed\nto the shared encoder $E_{sh}$, which computes its own query, key, and value"}, {"title": null, "content": "matrices, denoted as $Q_{sh}$, $K_{sh}$, and $V_{sh}$, as follows:\n$\\begin{cases}\nQ_{sh} = r_{sh} \\cdot W_{sh}^Q \\\\\nK_{sh} = r_{sh} \\cdot W_{sh}^K & W_{sh}^Q, W_{sh}^K, W_{sh}^V \\in \\mathbb{R}^{d_e \\times d_h} \\\\\nV_{sh} = r_{sh} \\cdot W_{sh}^V & Q_{sh}, K_{sh}, V_{sh} \\in \\mathbb{R}^{\\sum_i |X_i| \\times d_h}\n\\end{cases}$                                                                                                       (4)\nwhere $W_{sh}^Q$, $W_{sh}^K$ and $W_{sh}^V$ are weights matrices learned during training. As\nwith the modality-specific encoders, dimensionality is reduced to $d_h$ through\nlinear transformations.\nA similar modified masked self-attention mechanism is then applied:\n$MSA(Q_{sh}, K_{sh}, V_{sh}) = ReLU \\Bigg( softmax (\\frac{Q_{sh} K_{sh}^T}{\\sqrt{d_h}} + M_{sh}) + M_{sh}^a \\Bigg) V_{sh}$                                                                   (5)\nwhere the masking matrix $M_{sh}$ ensures that missing modalities do not impact\nthe final shared representation. This matrix operates in the same manner as\n$M_i$, zeroing out weights associated with missing modalities. The elements of\nthe masking matrix are defined as follows:\n$M_{sh}^{kj} = \\begin{cases}\n-\\infty & if r_{sh}^k \\text{ is missing} \\\\\n0 & otherwise\n\\end{cases}, M_{sh} = \\begin{bmatrix}\n0 & -\\infty & ... & 0 & 0 \\\\\n-\\infty & 0 & ... & 0 & 0 \\\\\n: & : & : & : & : \\\\\n0 & -\\infty & ... & 0 & 0 \\\\\n0 & -\\infty & ... & 0 & 0\n\\end{bmatrix}$ (6)\n$M_{sh}$ sums the $-\\infty$ values to weights that need to be ignored, effectively\nzeroing them after applying the softmax and ReLU operations.\nFinally, the joint representation is passed through a fully connected layer,\nwhich predicts the output $y$. The training process minimizes prediction error,\nupdating the weights of both the modality-specific encoders ($E_i$) and the\nshared encoder ($E_{sh}$) via end-to-end backpropagation."}, {"title": null, "content": "This architecture qualifies MARIA as an intermediate fusion model, per-\nforming fusion at the latent representation level. By dynamically optimizing\nall encoders, MARIA balances the contributions of different modalities and\nadapts to maximize their utility during training and inference. The use of\na masked multi-head attention mechanism ensures the model focuses adap-\ntively on informative parts of the input, ignoring missing data. This approach\nallows each modality to contribute based on its completeness, resulting in ac-\ncurate and reliable multimodal representations."}, {"title": "3.2. Regularization Technique for Missing Data", "content": "To enhance the model's generalizability under incomplete input condi-\ntions, we employ regularization strategies during training that improve its\nresilience to varying degrees of data incompleteness. These strategies ensure\nthat even when some modalities or features are unavailable, the model can\nstill generate accurate and meaningful outputs [7, 9]. During training, we\nsimulate a relaxed missing data setting where each modality or feature is\ntreated as potentially missing, while maintaining at least one available data\npoint per patient. This approach allows the model to learn how to handle\ndifferent levels of missingness effectively, making it particularly well-suited\nto the variability typically found in clinical datasets. By encouraging the\nmodel to extract meaningful representations from each available modality,\nthese masking strategies promote robustness against incomplete information\nduring both training and inference."}, {"title": null, "content": "Modality Dropout. During training, the model uses a stochastic masking\nprocedure to simulate incomplete data scenarios. Given a sample $X$ ="}, {"title": null, "content": "{$X_1,...,X_n$}, where n represents the number of modalities, let $v_m \\leq n$\ndenote the number of non-missing modalities in the sample (where $v_m = n$\nfor fully populated data or $v_m < n$ for partially missing data). A binary\ndecision variable determines whether masking will be applied to the sample\n$X$. If the sample is selected for masking, a random count $c_m$ of modalities\nto mask is chosen uniformly from the set {$1,2,..., v_m - 1$}, ensuring that\nat least one modality remains unmasked. Finally, $c_m$ non-missing modalities\nare randomly selected, and their values are set to missing, producing the\naugmented sample.\nFeature Dropout. Similarly, when a tabular modality $X_i$ is set as present, a\nsimilar stochastic masking procedure is applied at the feature level. A binary\ndecision variable determines whether masking will be applied to the features\nof the modality $X_i$. If masking is applied, a random count $c_i$ of features\nto mask is chosen uniformly from the set {$1,2, ..., v_i - 1$}, where $v_i$ is the\nnumber of non-missing features of the modality $X_i$. This ensures that at least\none feature remains unmasked. Finally, $c_i$ non-missing features within $X_i$ are\nrandomly chosen and set to missing, resulting in the augmented modality."}, {"title": "4. Experimental Configuration", "content": "In this section, we first describe the datasets used in the experiments and\nthe preprocessing applied (Section 4.1). We then outline the combinations\nof models and imputers employed as competitors (Section 4.2). Finally, we\npresent the metrics used for evaluation (Section 4.5)."}, {"title": "4.1. Data", "content": "We evaluated MARIA and its competitor models on two publicly available\ndatasets across eight diagnostic and prognostic tasks (details in Table 1).\nThese datasets represent real-world scenarios where patient data is often\nincomplete, highlighting the need for methods that are resilient to missing\ninformation.\nThe first dataset was obtained from the Alzheimer's Disease Neuroimag-\ning Initiative (ADNI) database (adni.loni.usc.edu) [12]. The ADNI was\nlaunched in 2003 as a public-private partnership, led by Principal Inves-\ntigator Michael W. Weiner, MD. The primary goal of ADNI has been to\ntest whether serial magnetic resonance imaging, positron emission tomogra-\nphy, other biological markers, and clinical and neuropsychological assessment\ncan be combined to measure the progression of mild cognitive impairment\n(MCI) and early Alzheimer's disease (AD), with respect to the Cognitively\nNormal (CN) group. For our study, we used four baseline data modalities:\nAssessment (cognitive and neuropsychological scores), Biospecimen (CSF,\nApoE genotyping, and lab data), Image analysis (MRI and PET neuroimag-\ning biomarkers), and Subject Characteristics (family history, demographics).\nWe analyzed data from ADNI 1, GO, 2, and 3 phases. Diagnostic tasks\nincluded binary classification (CN vs. AD) and ternary classification (CN\nVS. AD vs. MCI), reflecting real clinical differentiation scenarios. Addi-\ntionally, prognostic tasks aimed to predict whether treatment intervention\nmight be necessary at specific future points (12, 24, 36, and 48 months post-\nrecruitment). These tasks classified patients into CN, MCI, and Dementia\ncategories. Table 1 provides details on patient distributions for both baseline"}, {"title": null, "content": "and follow-up classifications.\nThe second dataset, AIforCOVID [13], contains clinical data from six\nItalian hospitals, collected during the first wave of the COVID-19 pandemic\n(March-June 2020). Data was recorded at the time of hospitalization of"}, {"title": null, "content": "symptomatic patients and subsequently anonymized and reviewed. All pa-\ntients tested positive for SARS-CoV-2 via RT-PCR, with 5% confirmed only\nafter a second test. Each patient was classified as either mild (discharged or\nhospitalized without ventilatory support) or severe (requiring non-invasive\nventilation, ICU care, or deceased). Additionally, we evaluated the proposed\napproach on a death prediction task, classifying patients as either censored\n(alive) or uncensored (deceased).\nMARIA's performance across diverse medical tasks provides valuable in-\nsights into its resilience and adaptability in real-world healthcare scenarios.\nBy leveraging the ADNI and AIforCOVID datasets, we demonstrate the\nmodel's ability to handle challenges such as missing modalities and hetero-\ngeneous data distributions.\nThese experiments emphasize the importance of multimodal fusion tech-\nniques that are not only robust to missing data but also capable of learning\nfrom complex, interrelated medical datasets. Such advancements provide the\nfoundation for more resilient and flexible DL models in healthcare, ultimately\nsupporting clinicians in making better-informed decisions under real-world\nconstraints."}, {"title": "4.2. Competitors", "content": "We conduct an extensive comparison of our methodology against early,\nlate, and intermediate fusion approaches that use missing data imputation\nas a preprocessing step before model training. Additionally, we benchmark\nagainst tree-based models that manage missing values using the Missing In\nAttributes (MIA) strategy. We choose not to include generative approaches,\nas these are primarily developed for imaging modalities rather than tabu-"}, {"title": null, "content": "lar data, and they introduce additional complexity and randomness, which\ncan hinder reproducibility. Instead, we focus on interpretable, efficient ap-\nproaches widely adopted in clinical settings, where model simplicity and re-\nliability are crucial. Our analysis includes a total of 10 distinct competi-\ntor models, each combined with the k-Nearest Neighbors (kNN) imputation\ntechnique using default parameters, as this method outperformed other im-\nputation strategies in prior studies [5, 7, 14].\nTable 2 provides an overview of the competitors. The first column catego-\nrizes models as ML or DL approaches, the second specifies the base learners,\nthe subsequent columns indicate the techniques used for handling missing\ndata, and the final columns describe the fusion strategies employed. This\nresults in 32 competitor configurations, each marked by an\u201c\u00d7\u201d in the re-\nspective columns.\nTo thoroughly evaluate the performance of our proposed methodology, we\ndesigned experiments comparing it against a diverse set of baseline models.\nThese experiments involve both ML models, which may rely on imputation or\nemploy the MIA strategy, and DL models paired with imputation techniques.\nThe ML models include AdaBoost, Decision Trees, HistGradientBoost,\nRandom Forests, Support Vector Machines (SVM), and XGBoost. AdaBoost [15]\nis a cascading ensemble model that prioritizes hard-to-classify instances, of-\nfering robustness across diverse datasets. Decision Trees [16] are highly inter-\npretable models that visually represent decision-making processes, providing\ninsights into complex data relationships. HistGradientBoost [14] offers an\nefficient variation of gradient boosting, optimized for handling large datasets\nwith improved speed and memory usage. Random Forests [17] is an ensemble"}, {"title": null, "content": "of decision trees known for robustness against overfitting and enhanced relia-\nbility. SVM [18], equipped with an RBF kernel, is included for its versatility\nin handling non-linear data separations, providing a contrast to tree-based\nmodels. XGBoost [19], an advanced boosting model, employs a gradient de-\nscent procedure to minimize loss and is highly effective for tabular datasets.\nAdditionally, we evaluate DL models paired with imputation methods.\nThese approaches include Multilayer Perceptron (MLP), TabNet, TabTrans-\nformer, and FTTransformer. MLP [20] is a foundational DL model that\ncaptures complex relationships between features, offering a baseline for com-"}, {"title": null, "content": "parison. TabNet [21] leverages self-attention to dynamically select features,\nimproving interpretability and decision-making. TabTransformer [22] uses\ntransformer-based self-attention mechanisms to embed categorical features\nand capture complex inter-feature relationships. FTTransformer [23] fur-\nther explores transformers' potential, using distinct embedding strategies for\nnumerical and categorical features.\nThe selected DL models were evaluated using both early and late fusion\napproaches. In addition, we developed intermediate fusion variations of these\nmodels, where the respective architectures were employed for both modality-\nspecific encoders and shared encoder settings. These intermediate fusion\nconfigurations assess the models' ability to concurrently handle multiple in-\nput types, leveraging the shared encoder to effectively combine information\nfrom various modalities.\nThese experiments were designed to comprehensively assess the strengths\nand limitations of each competitor model across various settings. Our com-\nparisons aim to benchmark the performance of our intermediate fusion method-\nology against both traditional ML approaches and advanced DL competitors.\nBy exploring a wide range of techniques, we highlight the effectiveness of our\napproach in handling incomplete and heterogeneous multimodal healthcare\ndata."}, {"title": "4.3. Preprocessing", "content": "For each dataset, we normalize the numerical features using a Min-Max\nscaler and apply one-hot encoding to the categorical features before feeding\nthem into the models. However, for models such as MARIA, NAIM, HistGra-\ndientBoost, TabNet, TabTransformer, FTTransformer, and XGBoost, one-"}, {"title": null, "content": "hot encoding is not applied, as their implementations can directly handle\ncategorical features. The preprocessing steps are calibrated using the train-\ning data and then applied to the validation and testing sets."}, {"title": "4.4. Missingness Evaluation", "content": "Our experiments center on generating Missing Completely At Random\n(MCAR) values artificially as it represents the broadest class of missing data\ntype without the introduction of any bias. Our goal is to test our model under\ndiverse missing data conditions by introducing missing values and modalities\nat various rates, denoted as p, across both the training and testing sets.\nSpecifically, we generate separate missing rates for the training and test sets,\nset to 0%, 5%, 10%, 30%, 50%, and 75%. No additional missing values"}]}