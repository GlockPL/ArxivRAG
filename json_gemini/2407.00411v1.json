{"title": "Explainability of Machine Learning Models under Missing Data", "authors": ["Tuan L. Vo", "Thu Nguyen", "Hugo L. Hammer", "Michael A. Riegler", "P\u00e5l Halvorsen"], "abstract": "Missing data is a prevalent issue that can significantly impair model performance and interpretability. This paper briefly summarizes the development of the field of missing data with respect to Explainable Artificial Intelligence and experimentally investigates the effects of various imputation methods on the calculation of Shapley values, a popular technique for interpreting complex machine learning models. We compare different imputation strategies and assess their impact on feature importance and interaction as determined by Shapley values. Moreover, we also theoretically analyze the effects of missing values on Shapley values. Importantly, our findings reveal that the choice of imputation method can introduce biases that could lead to changes in the Shapley values, thereby affecting the interpretability of the model. Moreover, and that a lower test prediction mean square error (MSE) may not imply a lower MSE in Shapley values and vice versa. Also, while Xgboost is a method that could handle missing data directly, using Xgboost directly on missing data can seriously affect interpretability compared to imputing the data before training Xgboost. This study provides a comprehensive evaluation of imputation methods in the context of model interpretation, offering practical guidance for selecting appropriate techniques based on dataset characteristics and analysis objectives. The results underscore the importance of considering imputation effects to ensure robust and reliable insights from machine learning models.", "sections": [{"title": "1. Introduction", "content": "Missing data is a common issue that can significantly affect model performance and interpretability. This problem can occur due to various reasons, such as data collection errors, privacy concerns, or intentional omission. Addressing missing data is crucial, and one effective approach is imputation, i.e., filling in the missing data points to create a more complete dataset for analysis, thereby improving the overall reliability of the results.\nVarious imputation methods exist, ranging from simple techniques like mean imputation to more sophisticated approaches such as multiple imputation by chained equations (MICE) and k-nearest neighbors (KNN). Each method has its strengths and weaknesses, influencing not only the performance of predictive models but also their interpretability. For example, in [1], the authors study the effects of missing data on the correlation heatmap and found out that the technique that computes a correlation matrix with the highest accuracy (in terms of RMSE) does not always produce correlation plots that closely resemble those derived from complete data. However, the effects of missing data on various aspects of explainable AI have not yet been fully studied.\nShapley values, a concept derived from cooperative game theory, have gained prominence as a robust method for interpreting complex models [2]. Shapley values attribute the contribution of each feature to the final prediction, offering insights into feature importance and interaction. However, the accuracy and reliability of Shapley values can be affected by the choice of the imputation method, as the imputed values can introduce biases or distortions. Despite the importance of this issue, little attention has been paid to the effect of imputation on the Shapley values of the downstream model. For example, in [3], the authors examine various imputation methods to choose the best one and then use Shapley values to explain the prediction of the downstream task. However, the effects of imputation methods on Shapley values have not been thoroughly examined.\nThis paper aims to explore the effects of various imputation methods on the calculation of Shapley values. By comparing different imputation strategies, we seek to understand how they influence the interpretability of machine learning models and the robustness of the insights derived from Shapley's values. The study provides a comprehensive evaluation of imputation methods"}, {"title": "2. Missing data techniques and explainability", "content": "In this section, we review various missing data handling techniques and their explainability."}, {"title": "2.1. Explainable AI", "content": "Explainable Artificial Intelligence (XAI) has garnered significant attention in recent years due to its critical role in ensuring transparency, trustworthiness, and accountability in AI systems. As AI continues to be integrated into various domains, the need for models that can not only perform well but also provide human-understandable explanations for their predictions has become paramount.\nA significant development in XAI is the emergence of intrinsic interpretability approaches. These involve designing models that are interpretable by nature. Decision trees [4], rule-based models [5], and linear models [6] are classical examples of intrinsically interpretable models. More recently, models such as Generalized Additive Models (GAMs) [7] and attention mechanisms in neural networks [8] have been explored for their ability to provide insights into the decision-making process. However, their explanation follows their specific explanation style, making it difficult to compare with other models that do not follow the same explanation schemes during model selection.\nRecent advancements have also seen the integration of XAI with deep learning models. Techniques such as DeepSHAP [9] and Integrated Gradients [10] provide explanations for neural network predictions by attributing the output to the input features in a manner consistent with the model's internal"}, {"title": "2.2. Missing data imputation techniques", "content": "The most common approach to handling missing values is to use imputation methods to fill in the gaps. Techniques such as matrix decomposition or matrix completion, including MGP [18], Polynomial Matrix Completion [19], ALS [20], and Nuclear Norm Minimization [21], allow for continuous data to be completed and subsequently analyzed using standard data analysis procedures. Additionally, regression or clustering-based methods, such as CBRL and CBRC [22], utilize Bayesian Ridge Regression and cluster-based local least square methods [23] for imputation. For large datasets, deep learning imputation techniques have gained popularity due to their performance"}, {"title": "2.3. Direct missing data handling techniques without imputation", "content": "Different from the imputation approaches, methods that directly handle missing data can have clearer implications in terms of explainability.\nSpecifically, Nguyen et al. [35] introduced the EPEM algorithm to estimate the maximum likelihood estimates (MLEs) for multiple class monotone missing data when the covariance matrices of all classes are assumed to be equal. Additionally, DPER [39] addresses a more general case where missing data can occur in any feature by using pairs of features to estimate the entries in the mean and covariance matrices. The implication to model explainability of using direct parameter estimation methods, like EPEM and DPER, includes improved transparency and interpretability of the model's behavior, as these methods provide clear estimates of means and covariances, which can be directly examined and understood. In fact, [1] analyzes the effects of various imputation and direct parameter estimation methods on the correlation plot, and finds that DPER is not only scalable but also better for this task than imputation techniques such as SOFT-IMPUTE [40], KNNI, GAIN [41], GINN [42], missForest [29].\nNote that LSTM [43] can handle missing data directly. In fact, it is used the work of Ghazi et al. [44] to model disease progression while handling missing data in both the inputs and targets. Another work by Li et al. [45] proposed a technique for the bi-clustering problem that can manage missing data, enabling simultaneous partitioning of rows and columns of a rectangular data array into homogeneous subsets. Learning directly from the data may offer advantages in speed and reduced storage costs by eliminating the need for separate models for imputation and the target task.\nUsing techniques that directly handle missing data can significantly enhance model explainability. By learning directly from the data, the model avoids the added complexity of managing separate imputation and prediction models, simplifying the overall structure and making it easier to understand. Additionally, it helps avoid potential biases that can be introduced by filling in missing values, as in imputation methods."}, {"title": "3. Methods", "content": "In this paper, we will also examine the effects of various imputation techniques on Shapley values. Therefore, in this section, we will briefly summarize the basics of Shapley values, the types of plots, and the imputation techniques that will be used to examine Shapley values in the experiments."}, {"title": "3.1. Shapley values", "content": "Shapley values, originally derived from cooperative game theory [46], have been adapted as a powerful tool for interpreting machine learning models [9]. They provide a systematic way to attribute the contribution of each feature to a model's prediction, ensuring a fair distribution based on the interaction of features. Consider a machine learning model $v$ that takes an input vector $X = (x_1, x_2, ..., x_p)$ and produces a prediction $v(x)$. The Shapley value for the value $x_i$ of corresponding feature $f_i$ quantifies its contribution to the prediction $v(x)$. Formally, let $P = \\{1, 2, ..., p\\}$ be the complete index set of features and $S$ a coalition of feature indices (a subset of $P \\\\ {i}\\})$. For $i = 1, 2, ..., p$, the Shapley value $\\phi_i$ for the feature value $x_i$ is defined as\n$\\Phi_i = \\sum_{S \\subset P\\\\{i\\}} \\frac{|S|!(p - |S| - 1)!}{p!} [v(S \\cup \\{i\\}) - v(S)]$,\nwhere $|S|$ is the number of elements in subset $S$, and $v(S \\cup \\{i\\}), v(S)$ is the model prediction using feature indices in $S$ plus $i^{th}$ feature and only in $S$, respectively. In the context of classification problems, the model outputs a probability distribution over classes. The Shapley values can be calculated for each class's probability, providing insights into how each feature influences the likelihood of each class. Specifically, calculating Shapley values in classification problems can be broken down into the algorithm 1.\nThe types of plot that will be examined in this paper include:\n\u2022 A Global Feature Importance plot is a visualization tool for examining the impact of individual features within a predictive model. More specifically, it highlights the features that contribute the most to the model's predictions, enabling a deeper understanding of the model's behavior. Here, the global importance of each feature is determined by calculating the mean absolute value of Shapley values of that feature across all the given samples."}, {"title": "3.2. Imputation techniques", "content": "In this section, we briefly summarize the missing data handling methods that we will examine for the effects on Shapley values. The methods being investigated consist of a method that can directly learn from missing data, such as XGBoost, to a simple imputation method as Mean Imputation, as well as the widely used or recently developed imputation techniques, such as"}, {"title": "4. Theoretical analysis", "content": "In this section, we consider a simple linear regression model to show the effects of missing data in training and test data on the mean absolute Shapley values of a mean imputation method.\nAssume we have a training data $D = (x|y)$ which has $n$ samples, $x$ is univariate input, and $y$ is a target. The corresponding missing data of $D$ is denoted by $D^* = (x^*|y)$. Here, we assume that only $x$ contain missing values and denote $Obs(x^*)$ as the index set of observed samples in $x^*$, i.e., $Obs(x^*) = \\{i \\in N : x_i \\text{ is observed}\\}$. Moreover, we denote $D' = (x'|y)$ as imputed data by using mean imputation. It means that\n$x'_i = \\begin{cases}x_i & \\text{if } i \\in Obs(x^*) \\\\ E[x^*], & \\text{if } i \\notin Obs(x^*)\\end{cases}$, for $i = 1, 2, ..., n$.\\tag{1}\nNow, suppose that the corresponding linear regression models on $D$ and $D'$"}, {"title": "5. Experiments", "content": "In this study, we compare the Shapley values of the original data before missing data simulation with the Shapley values of XGBoost [47], a method that can be trained directly on missing data, and a two-step approach (imputation followed by regression). For the second alternative, during the imputation process, we test five methods: mean imputation, Multiple Imputation by Chained Equations (MICE) [36], conditional Distribution-based Imputation of Missing Values with Regularization (DIMV) [37], missForest [29], and SOFT-IMPUTE [40]. These methods are implemented with default settings using the fancyimpute 2, scikit-learn [48], and DIMV Imputation packages 3.\nWe used the following data sets sourced from the Machine Learning Database Repository at the University of California, Irvine4. Detailed descriptions of the datasets are provided in Table 1."}, {"title": "5.1. Experiment settings", "content": "In this study, we compare the Shapley values of the original data before missing data simulation with the Shapley values of XGBoost [47], a method that can be trained directly on missing data, and a two-step approach (imputation followed by regression). For the second alternative, during the imputation process, we test five methods: mean imputation, Multiple Imputation by Chained Equations (MICE) [36], conditional Distribution-based Imputation of Missing Values with Regularization (DIMV) [37], missForest [29], and SOFT-IMPUTE [40]. These methods are implemented with default settings using the fancyimpute 2, scikit-learn [48], and DIMV Imputation packages 3.\nWe used the following data sets sourced from the Machine Learning Database Repository at the University of California, Irvine4. Detailed descriptions of the datasets are provided in Table 1."}, {"title": "5.2. Global feature importance plot", "content": "We present the Global Feature Importance plots across various missing rates for the California dataset in Figures 1, 2, 3 and 4, while the plots for the diabetes dataset are included in Appendix A. However, the key points"}, {"title": "5.3. Beeswarm plot analysis", "content": "The beeswarm plot for the California datasets at missing rates 0.2, 0.4, 0.6, and 0.8 are presented in Figures 5, 6, 7, 8, respectively. In these figures, we focus on three key features (Latitude, Longitude, and Median Income - MedInc) because the others show little impact."}, {"title": "5.3.1. Beeswarm plot for the California datasets", "content": "The beeswarm plot for the California datasets at missing rates 0.2, 0.4, 0.6, and 0.8 are presented in Figures 5, 6, 7, 8, respectively. In these figures, we focus on three key features (Latitude, Longitude, and Median Income - MedInc) because the others show little impact.\nXGBoost seems to be stable, MedInc consistently shows the highest Shapley values, ranging from (-0.5) to around (+1.5) across all missing rates, emphasizing its strong influence on the model outputs. The imputation methods show similar patterns with Shapley values for key features slightly reduced compared to the XGBoost model, but they still highlight the importance of Latitude, Longitude, and MedInc in the predictive modeling.\nConsidering the results at each missing rate, we observed similar distributions in the Shapley values for Latitude, Longitude, and MedInc on both the"}, {"title": "5.3.2. Beeswarm plot for the diabetes datasets", "content": "The beeswarm plot for the diabetes datasets at missing rates 0.2, 0.4, 0.6, and 0.8 are presented in Figures 9, 6, 11, 12, respectively. In these figures, the"}, {"title": "5.4. MSE analysis", "content": "The performance of various imputation methods was evaluated for the California dataset with different data missing rates in Table 2, and for the diabetes dataset in Table 3. Recall that here, the MSE is the MSE of the true label and predicted values on the test set. Also, MSE Shap is the MSE between the Shapley values of predicted and true labels on the test set as well."}, {"title": "6. Discussion", "content": "In summary, from the present results and the deep analysis in the previous section, we can observe several interesting key insights.\nThe global feature importance plots and the beeswarm plots clearly show that different imputation methods lead to varying Shapley value distributions. This shows that the choice of the imputation method can significantly affect the interpretability of the model.\nAcross different missing rates, XGBoost, without imputation, seems to have the most significant changes in the Shapley values. While Xgboost can train and predict directly on data with missing values, the MSE between Shapley values of XgBoost and the Original is the highest in most of the experiments, and even higher implementing XgBoost Regression after filling in missing values by mean imputation.\nAs the missing rate increases from 20% to 80% percent, the differences between the imputation methods become more pronounced. This indicates that the choice of the imputation method becomes increasingly critical as the number of missing data increases.\nThe results for the California and diabetes datasets show some differences, suggesting that the impact of imputation methods may be data set dependent. This highlights the importance of considering the characteristics of the data set when choosing an imputation method.\nThe MSE plots show that methods with lower imputation MSE do not necessarily preserve Shapley values better (as measured by Shapley MSE). This suggests a potential trade-off between the accuracy of the imputation and the maintenance of the original importance structure of the feature.\nMean imputation tends to significantly alter the importance of features, especially at higher missing rates. MICE and DIMV often show similar patterns, possibly due to the fact that MICE is based on regression and DIMV is based on a conditional Gaussian formula. MissForest and SOFT-IMPUTE sometimes preserve feature rankings better than simpler methods, but this is not consistent across all scenarios.\nThe variability in results between methods and missing rates underscores the need to evaluate imputation effects when using Shapley values for model interpretation.\nThe following discussion is structured around our result and the specific pitfalls that may arise due to incomplete understanding of the relationship between missing data, imputation methods, and Shapley values. We high-"}, {"title": "7. Conclusion", "content": "In this paper, we explore the impact of various imputation methods on the calculation of Shapley values for model interpretability. Our findings indicate that the choice of the imputation strategy can significantly influence the accuracy and reliability of Shapley values, thereby affecting the insights drawn from the machine learning models.\nOur comparative analysis revealed that the chosen imputation method should align with the specific characteristics of the data set and the objectives of the analysis. Practitioners should carefully consider the trade-offs between computational efficiency and the potential for bias introduction when selecting an imputation method. The study underscores the necessity of evaluating the effects of imputation as a critical step in the preprocessing pipeline, especially when Shapley values are used for model interpretation.\nFuture work should focus on extending this analysis to a broader range of datasets and machine learning models to further validate our findings. In addition, the development of new imputation methods is tailored for specific types of data, model structures, and explainability, especially in relation to Shapley value interpretations. Also, more research needs to be done on the direction of handling missing data directly, as this helps to avoid noises and bias introduced to the model by imputation. By addressing these challenges, we can improve the reliability of model interpretations and support more informed decision-making in the application of machine learning."}]}