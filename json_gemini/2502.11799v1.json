{"title": "Table-Critic: A Multi-Agent Framework for Collaborative Criticism and Refinement in Table Reasoning", "authors": ["Peiying Yu", "Guoxin Chen", "Jingjing Wang"], "abstract": "Despite the remarkable capabilities of large language models (LLMs) in various reasoning tasks, they still struggle with table reasoning tasks, particularly in maintaining consistency throughout multi-step reasoning processes. While existing approaches have explored various decomposition strategies, they often lack effective mechanisms to identify and correct errors in intermediate reasoning steps, leading to cascading error propagation. To address these issues, we propose Table-Critic, a novel multi-agent framework that facilitates collaborative criticism and iterative refinement of the reasoning process until convergence to correct solutions. Our framework consists of four specialized agents: a Judge for error identification, a Critic for comprehensive critiques, a Refiner for process improvement, and a Curator for pattern distillation. To effectively deal with diverse and unpredictable error types, we introduce a self-evolving template tree that systematically accumulates critique knowledge through experience-driven learning and guides future reflections. Extensive experiments have demonstrated that Table-Critic achieves substantial improvements over existing methods, achieving superior accuracy and error correction rates while maintaining computational efficiency and lower solution degradation rate.", "sections": [{"title": "1 Introduction", "content": "Despite significant advances in various reasoning tasks (Plaat et al., 2024; Yu et al., 2024; Chen et al., 2024a,b; Guo et al., 2025), large language models (LLMs) (Yang et al., 2024a; Dubey et al., 2024; Anthropic, 2024; Mesnard et al., 2024; Hurst et al., 2024) face substantial challenges in handling semi-structured data, such as table reasoning tasks, as they require both understanding of tabular structures and precise localization of relevant entries in redundant and noisy information (Zhao et al., 2024; Chen et al., 2024c; Zhang et al., 2025).\nExisting approaches address these challenges through various decomposition strategies. For example, Binder (Cheng et al., 2022) decomposes complex questions into executable sub-programs (i.e., SQL or Python), while approaches such as Dater (Ye et al., 2023) and Chain-of-Table (Wang et al., 2024) focus on dynamic table decomposition for context-aware reasoning. Although these decomposition-based methods have demonstrated promising performance, they suffer from a critical limitation: the lack of effective mechanisms to criticize and refine the intermediate reasoning steps. This deficiency inevitably leads to error propagation throughout the reasoning process, significantly affecting the accuracy of final predictions.\nHowever, recent studies (Madaan et al., 2023; Yang et al., 2024b) have revealed that while LLMs possess self-reflection capabilities to some extent, their self-reflection often lacks reliability and consistency. Simply forcing LLMs to engage in self-reflection may introduce additional biases, especially in table reasoning tasks, wherein models tend to either rationalize their previous erroneous reasoning or over-criticize correct steps, rather than identifying genuine errors (Zheng et al., 2024; Chen et al., 2025b).\nTo address these issues, we propose Table-Critic, a multi-agent framework that introduces specialized agents to collaboratively criticize and refine the reasoning process in a step-by-step manner. Specifically, our Table-Critic simulates human-like reflective behaviors through four targeted agents: a Judge that identifies potential errors, a Critic that provides detailed suggestions, a Refiner that refines the entire reasoning process, and a Curator that distills critique patterns to guide future reflection. The collaborative strategy among multiple agents is motivated by our two insights: (1) LLMs demonstrate proficiency in identifying and re-"}, {"title": "2 Related Work", "content": "Table Reasoning. Table reasoning, which requires joint understanding of semi-structured tables and questions, has evolved through several paradigms. Early approaches focused on developing specialized models through fine-tuning (Yin et al., 2020; Liu et al., 2021; Gu et al., 2022), while recent work has shifted towards leveraging large language models (LLMs) in few-shot learning (Chen et al., 2024c; Zhao et al., 2024). To handle complex reasoning tasks, decomposition-based methods have emerged as a promising direction. These methods break down complex tasks into manageable steps, either through program execution (Cheng et al., 2022) or context-aware table partitioning (Ye et al., 2023; Wang et al., 2024). However, a critical limitation of existing approaches is their inability to effectively critique and refine intermediate reasoning steps, leading to error propagation. In contrast, our Table-Critic framework addresses this limitation by introducing systematic critique and refinement mechanisms throughout the reasoning process.\nSelf-Reflection. Recent studies have revealed that while LLMs possess inherent self-reflection capabilities, they often suffer from reliability and consistency issues (Madaan et al., 2023; Yang et al., 2024b). Simply enforcing self-reflection can be counterproductive, as models tend to either rationalize their errors or excessively critique correct reasoning steps (Zheng et al., 2024; Chen et al., 2025b). To address these limitations, our Table-Critic introduces a structured approach through: (1) a multi-agent framework where specialized agents collaborate to provide targeted critiques, and (2) a self-evolving template tree that systematically accumulates and organizes critique knowledge. This design effectively overcomes the inherent limitations of LLMs' reflection capabilities while maintaining reliable and consistent error identification."}, {"title": "3 Table-Critic", "content": "To effectively implement the human-like correction process in multi-step reasoning, we propose a collaborative multi-agent framework, Table-Critic. As illustrated in Figure 1, this framework decomposes the complex reasoning refinement task into four specialized functions: error detection (Judge), critique generation (Critic), reasoning refinement (Refiner), and experience learning (Curator). These agents work in concert to progressively improve reasoning quality while accumulating valuable correction experiences. Specifically, given a table T and a question q, these agents iteratively refine the initial reasoning chain t = {81, 82, ..., Sn} until reaching a satisfactory solution. The refinement process is guided by a self-evolving template tree T that systematically accumulates critique patterns from past experiences."}, {"title": "3.2 Multiple Agents", "content": "Inspired by human-like correction behavior, we design four specialized agents\u2014Judge, Critic, Refiner, and Curator\u2014to facilitate criticizing and refining in multi-step reasoning. We use specific instructions to prompt LLM (\u03c0) to execute the corresponding operations. Formally, we define each agent as follows:\nJudge (A). The Judge agent is responsible for identifying potential errors in the reasoning process. Given a table T, question q, current reasoning chain \u03c4, and the template tree T, it analyzes each reasoning step and determines the specific error type if any exists. Based on the identified error type (if exists), the Judge routes through the template tree T to locate appropriate templates for guiding the subsequent critic agent. Formally, the Judge agent operates as:\nE, P, R = \u03c0(T, q, \u03c4, T, instruction A\u00b3), (1)\nwhere E denotes the error analysis for each reasoning step, P \u2208 {Correct, Incorrect} indicates the overall reasoning status, and R represents the routing path in the template tree that guides template selection. Based on the routing path, we sample relevant critique templates Ts from the template tree T to guide the Critic agent in generating targeted and high-quality critiques for the identified errors. Notably, due to the self-evolving nature of our template tree, when the Judge identifies an error type not yet present in the tree, we randomly sample various error types from existing templates to guide the Critic in generating helpful critique.\nCritic (AC). The Critic agent serves as a crucial component in our framework, responsible for generating detailed and constructive critiques for the identified errors. With the guidance of sampled critique templates Ts, the Critic agent locates the first error step in the reasoning chain \u03c4, analyzes error details, and provides specific suggestions for"}, {"title": "3.3 Multi-turn Refinement", "content": "As discussed in the Introduction, the multi-turn refinement in Table-Critic is motivated by our observation that LLMs often excel at identifying and correcting the first error in reasoning chains, but may introduce new errors in subsequent steps. To address this challenge, we implement an iterative refinement process where multiple agents collaboratively monitor and improve the reasoning chain until reaching a satisfactory solution.\nSpecifically, given an initial reasoning chain \u03c4, our framework operates through the following steps in each iteration: (1) The Judge agent first analyzes the entire reasoning chain to identify potential errors and determine their types. If no errors are detected (P = Correct), the process terminates. Otherwise, the Judge routes through the template tree to locate relevant critique templates. (2) With the guidance of sampled templates Ts, the Critic agent generates detailed critiques C focusing on the first identified error at step I. This strategy ensures that each refinement iteration addresses errors sequentially, preventing the introduction of cascading errors. (3) The Refiner agent then generates a new reasoning chain \u03c4' by incorporating the critique. Importantly, the Refiner only receives the partial chain Tp up to the error step I, forcing it to reconstruct the remaining steps with the help of critique. This design prevents the Refiner from being biased by previous erroneous chain. (4) The above process continues iteratively until one of the following conditions is met: the Judge determines the current reasoning chain is correct (P = Correct) or the maximum number of iterations K is reached.\nThrough this multi-turn design, Table-Critic effectively manages the complexity of multi-step reasoning refinement while maintaining the quality of each correction step. The iterative nature of our approach, combined with specialized agent roles and strategic process control, enables robust and efficient reasoning improvement."}, {"title": "3.4 Self-evolving Template Tree", "content": "To address the challenge of identifying diverse and unpredictable error types in table reasoning, we introduce a self-evolving template tree that systematically accumulates and organizes critique knowledge. This dynamic structure enables our system to effectively handle both common and emerging error patterns through experience-driven learning.\nTree Structure. The template tree T represents a hierarchical structure that captures the relationships among different error types. As shown in Figure 1, each node in the tree represents a specific type of error, where: (1) Internal nodes represent broader error categories (e.g., Sub-table Error) that can be further subdivided into more specific error types. (2) Leaf nodes represent specific error types (e.g., Row Error, Column Error) and maintain a repository of critique templates associated with that particular error type.\nSelf-evolving Mechanism. The template tree evolves dynamically through the Curator agent, which manages two primary operations: adding templates to existing leaf nodes and expanding tree branches. As illustrated in Figure 1, the evolution process includes:\n(1) Template Enhancement. When new effective critique patterns are identified, the Curator adds them to the corresponding leaf node's template repository. This operation enriches existing error type categories without changing the tree structure. For instance, when a new effective template for Row Error is discovered, it is directly added to the corresponding template repository.\n(2) Branch Expansion. The Curator expands the tree structure in two ways when new error types are identified:\n\u2022 Vertical Expansion: When a new error type is discovered that requires more fine-grained categorization, the Curator performs a vertical split. This operation transforms an existing leaf node into an internal node with two new child nodes. Specifically, the Curator first categorizes the existing templates in the leaf node with an appropriate name (e.g., Row Error), creating one new leaf node. Then, it creates another leaf node with a different name (e.g., Column Error) to accommodate the newly discovered error type and its corresponding templates. This process ensures that each leaf node maintains a cohesive collection of templates for a specific error type.\n\u2022 Horizontal Expansion: When a completely new error type is identified that parallels existing categories, the Curator adds a new branch at the same level. This operation preserves the existing structure while accommodating new error types. As illustrated in the Figure 1 (bottom), the addition of the Final Query Error branch represents a horizontal expansion that complements the existing Sub-table Error category.\nThrough these evolution mechanisms, our template tree maintains a dynamic balance between preserving accumulated knowledge and incorporating new error patterns. The vertical expansion enables more precise error categorization, while horizontal expansion ensures comprehensive coverage of diverse error types. This adaptive structure allows the system to continuously improve its critique ca-"}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Experimental Setup", "content": "Datasets. We evaluate our approach on two standard benchmarks: (1) WikiTableQuestions (WikiTQ) (Pasupat and Liang, 2015): A table reasoning benchmark with 4,344 test samples from 421 tables. (2) TabFact (Chen et al., 2020): A fact verification benchmark in table reasoning with 2,024 test samples from 298 tables.\nBaselines. We conduct comprehensive experiments comparing Table-Critic against three categories of baselines: (1) Standard Reasoning. End-to-End QA directly generates answers using table and question as input. Few-Shot QA extends this by incorporating exemplar (Table, Question, Answer) triplets from the training set. (2) Decomposition-Based Reasoning. Binder (Cheng et al., 2022) decomposes questions into executable SQL/Python sub-programs. Dater (Ye et al., 2023) employs parsing-execution-filling strategy with sub-table decomposition. Chain-of-Table (Wang et al., 2024) generates intermediate tables through dynamic operations. (3) Critic-Based Reasoning. Critic-CoT (Zheng et al., 2024) implements self-reflection for error identification.\nImplementation Details. To ensure comprehensive evaluation, we conduct experiments across three LLMs: Qwen2.5-72B-Instruct (Yang et al., 2024a), LLaMA3.3-70B-Instruct (Dubey et al., 2024), and GPT-40-mini (Hurst et al., 2024). For all baseline methods, we follow their original settings to ensure optimal performance. For fair comparison, both Critic-CoT (Zheng et al., 2024) and our Table-Critic framework are implemented upon Chain-of-Table (Wang et al., 2024). For our Table-Critic, the template tree is initialized with only 2 templates that demonstrate basic critique patterns. From this minimal starting point, the tree evolves autonomously through our self-evolving mechanism, continuously learning and incorporating new critique patterns. For all experiments, we set the maximum refinement iterations K to 5 and use temperature 0.0 for greedy decoding. The detailed prompts and instructions for each agent in our framework are provided in Appendix E."}, {"title": "4.2 Main Results", "content": "We report the performance on different table reasoning benchmarks across different LLMs in Table 1. Our comprehensive evaluation reveals several key findings: First, Table-Critic consistently outperforms all baseline methods across both datasets and all three LLMs. On average, our method achieves 73.7% accuracy on WikiTQ and 91.7% on TabFact, representing significant improvements of 6.3% and 2.2% respectively over the strongest baselines. Second, the improvements are robust across different model architectures. With Qwen2.5-72B-Instruct, we achieve the highest absolute performance (77.2% on WikiTQ, 92.6% on TabFact), showing substantial gains of 8.2% and 2.6% respectively. Similar patterns are observed with LLaMA3.3-70B-Instruct and GPT-40-mini, demonstrating the framework's generalizability across different foundation models. Third, the performance variations between WikiTQ and TabFact provide insights into our method's strengths. Table-Critic shows larger improvements on WikiTQ (average +6.3%) compared to TabFact (average +2.2%), indicating its particular effectiveness in handling complex, multi-step reasoning tasks. This aligns with our framework design, as WikiTQ's compositional questions benefit more from our multi-turn refinement and self-evolving template tree mechanism than TabFact's binary verification tasks. Nevertheless, the consistent improvements on TabFact demonstrate our method's capability even in simpler scenarios. Finally, comparing against different baseline categories reveals the advancement of our approach. While recent methods like Chain-of-Table (Wang et al., 2024) and Critic-CoT (Zheng et al., 2024) have made notable progress through decomposition and criticism mechanisms, Table-Critic achieves substantially larger improvements over these strong baselines. This suggests that our multi-agent framework, combining multi-turn refinement with self-evolving template tree, provides a more effective solution for complex table reasoning tasks."}, {"title": "4.3 Analysis of Critic Effectiveness", "content": "As shown in Table 2, we conduct a detailed analysis of different critic mechanisms by comparing Table-Critic with Chain-of-Table (Wang et al., 2024) and Critic-CoT (Zheng et al., 2024). Our analysis focuses on four key metrics: (1) Overall Accuracy (Acc): The percentage of correctly solved questions; (2) Error Correction Rate (\u25b3i\u2192c): The percentage of questions incorrectly solved by Chain-of-Table but corrected by different Critic methods; (3) Solution Degradation Rate (\u25b3\u2192i): The percentage of questions correctly solved by Chain-of-Table but degraded by different Critic methods; (4) Net Performance Gain (\u25b3): The overall improvement relative to Chain-of-Table, calculated as \u2206 = \u2206ic + Ac\u2192\u0456.\nError Correction vs. Solution Degradation. Table-Critic demonstrates superior error correction capabilities while minimizing solution degradation. On WikiTQ, it successfully corrects 9.6% of Chain-of-Table's errors while only degrading 0.7% of correct solutions, resulting in a substantial net performance gain (+8.9%). In contrast, Critic-CoT shows a less effective pattern, with a 5.6% correction rate offset by a high degradation rate (-4.9%), yielding only a marginal improvement (+0.7%).\nTask-Specific Performance. The effectiveness of critique mechanisms varies across different tasks. On WikiTQ, which involves complex multi-step reasoning, Table-Critic achieves a higher error correction rate (+9.6% vs +5.6%) and maintains a observably lower degradation (-0.7% vs -4.9%) compared to Critic-CoT. For TabFact's simpler ver-"}, {"title": "4.4 Analysis of Multi-Turn Mechanism", "content": "To understand the effectiveness of our multi-turn refinement mechanism, we analyze how model performance evolves with the number of iterations K and the distribution of required iteration counts (set maximal K = 10), as shown in Figure 2.\nPerformance Evolution. On both datasets, we observe a consistent pattern of rapid initial improvement followed by gradual convergence. For WikiTQ, the accuracy increases sharply from 67.6% to 76.5% within the first three iterations and stabilizes around 77% after six iterations. Similarly, on TabFact, the performance improves significantly in early iterations and plateaus at approximately 92% after five iterations. This pattern suggests that our multi-turn mechanism effectively refines solutions through iterative improvements.\nIteration Distribution. The density plots reveal interesting insights about the complexity of different tasks. On WikiTQ, we observe a broader distribution with multiple peaks, indicating that questions require varying numbers of iterations for resolution. The main peak occurs at 1-2 iterations, with smaller peaks extending up to 10 iterations, reflecting the diverse complexity of multi-step reasoning questions. TabFact also shows a concentrated distribution with two distinct peaks: a primary peak at 1-2 iterations and a secondary peak around 10 iterations. This bimodal pattern suggests that TabFact tend to fall into two categories: (1) straightforward cases that can be verified quickly within 1-2 iterations, and (2) complex cases that require extensive refinement to reach a conclusive verification. This distribution aligns with the inherent nature of fact verification tasks, where statements are either relatively simple to verify or require careful step-by-step examination.\nConvergence and Stability Analysis. The results suggest that while our method allows for up to 10 iterations, most improvements are achieved within the first 5 iterations. This efficient convergence, combined with our early termination mechanism, helps maintain computational efficiency while ensuring thorough reasoning. Notably, as evidenced in Table 2, Table-Critic maintains stable performance across iterations without the degradation typically seen in iterative approaches, demonstrating the effectiveness of our Critic agent and self-evolving template tree mechanism."}, {"title": "4.5 Analysis of Computational Cost", "content": "To ensure a fair comparison with Chain-of-Table (Wang et al., 2024) in terms of computational cost, we conduct an analysis of the cost-effectiveness trade-off, as shown in Figure 3. Since Table-Critic builds upon Chain-of-Table by incorporating additional critique mechanisms, we align the computational costs by allowing Chain-of-Table to generate multiple solutions (majority voting) through Self-consistency (Wang et al., 2023) (with temperature 0.8) and compare the performance under equivalent or even superior computational budgets.\nEfficiency Comparison. Our method requires approximately 1.8-2.2\u00d7 computational cost compared to the basic Chain-of-Table. However, as illustrated in Figure 3, Table-Critic achieves substantially higher accuracy (77.2% on WikiTQ and 92.6% on TabFact) compared to Chain-of-Table's performance even with 15 solution attempts. Notably, Chain-of-Table shows only marginal improvements as the number of solutions increases, reaching 70.0% on WikiTQ and 90.1% on TabFact with 15 solutions.\nCost-Effectiveness Analysis. The results demonstrate that simply increasing the number of solution attempts in Chain-of-Table fails to achieve comparable performance to Table-Critic, despite consuming similar or even greater computational resources. This suggests that our multi-agent refinement mechanism provides a more effective approach to improving reasoning accuracy than traditional majority voting strategies. The superior performance of Table-Critic justifies its additional computational overhead by offering substantially better reasoning capabilities."}, {"title": "4.6 Analysis of Self-evolving Template Tree", "content": "To investigate the effectiveness of our self-evolving mechanism, we conduct an ablation study comparing Table-Critic with and without the dynamic template evolution capability, as shown in Table 3. In the static setting (w/o Self-evolving), the template tree remains fixed with its initial two templates, while our full Table-Critic allows the Curator agent to dynamically maintain and evolve the template tree throughout the reasoning process.\nPerformance Impact. The results demonstrate the clear benefits of the self-evolving mechanism. Without template evolution, performance drops by 1.1% on WikiTQ (from 77.2% to 76.1%) and 1.8% on TabFact (from 92.6% to 90.8%). The more substantial performance gap on TabFact suggests that template evolution is particularly beneficial for fact verification tasks, where diverse verification patterns may be needed.\nMechanism Analysis. These results highlight the importance of dynamic adaptation in our framework. The self-evolving mechanism allows the template tree to expand beyond its initial state, accommodating diverse reasoning patterns encountered during the critique process. This flexibility enables more effective error detection and correction compared to a static template approach. The performance gains validate our design choice of incorporating dynamic template evolution, showing that the ability to adapt and expand the template structure is crucial for robust table reasoning."}, {"title": "5 Conclusion", "content": "In this paper, we propose Table-Critic, a novel multi-agent framework that enhances table reasoning through collaborative criticism and refinement. Our approach introduces four specialized agents working in concert with a self-evolving template tree, effectively addressing the challenges of error"}, {"title": "Limitations", "content": "Our Table-Critic framework has demonstrated strong performance in enhancing table reasoning through multi-agent collaboration and systematic refinement. While our current implementation focuses primarily on textual table reasoning, the proposed multi-agent critique framework is inherently flexible and can potentially be extended to various other scenarios. For instance, the framework could be adapted to handle multimodal reasoning tasks where tables are combined with images, graphs, or other visual elements. We believe the core principles of our approach\u2014collaborative criticism, iterative refinement, and self-evolving template tree\u2014could contribute to broader applications in complex reasoning tasks beyond the current textual domain."}, {"title": "A Additional Related works", "content": "Multi-agent Systems. Multi-agent systems have recently demonstrated promising potential in complex reasoning tasks by enabling collaborative problem-solving through specialized agents . These systems typically leverage the complementary strengths of different agents to achieve more robust and effective solutions than single-agent approaches. While existing work has explored multi-agent frameworks in various domains, their application to table reasoning tasks remains largely unexplored. To our knowledge, our Table-Critic presents the first attempt to introduce a multi-agent framework for table reasoning, where specialized agents collaborate to identify, critique, and refine reasoning steps, offering a novel perspective on addressing the challenges in complex table reasoning tasks."}, {"title": "B More Implementation Details", "content": "In this section, we provide a comprehensive implementation details of our proposed method. For additional insights and more intricate details, we refer the reader to our supplementary materials."}, {"title": "B.1 Overall Pipeline of Table-Critic", "content": "Table-Critic employs an iterative process to critique and refine the reasoning chain and predicted answer for table reasoning tasks. As described in Algorithm 1, the process begins with an input table T, a question q, an initial reasoning chain 7, and a template tree T. The Judge agent is first invoked to evaluate the correctness of the reasoning chain (Line 2). This evaluation yields the reasoning status P, an error analysis E, and a routing path R in the template tree.\nWhen the reasoning chain is deemed incorrect (P = Incorrect), Table-Critic proceeds by sampling relevant critique templates Ts from the template tree using the routing path R (Line 4). These templates are then used by the Critic agent to generate a detailed critique C and identify the index of the first error step I in the reasoning chain (Line 5). To address the identified errors, the Refiner agent retains the reasoning steps up to step I and refines the chain starting from step I, guided by the critique C (Line 6). The refined reasoning chain \u03c4' is subsequently re-evaluated by the Judge agent to determine if it is now correct (Line 7)."}, {"title": "C Detailed Computational Cost Analysis", "content": "This appendix evaluates the computational cost of Table-Critic relative to the baseline Chain-of-Table method. The computational cost is analyzed for two datasets, WikiTQ and TabFact, based on the number of input and output tokens required. All token counts are expressed in millions (M), and the cost ratio reflects the relative cost of Table-Critic compared to Chain-of-Table."}, {"title": "C.1 Computational Cost Definition", "content": "The computational cost of a prompt-based method is defined as follows:\n$N_{total} = N_{in} (\\frac{P_{in}}{P_{in} + P_{out}}) + N_{out} (\\frac{P_{out}}{P_{in} + P_{out}})$ (5)\nwhere Nin and Nout represent the number of input and output tokens, and Pin and Pout denote the costs per token for input and output, respectively. Based on the pricing model of Qwen2.5-72B-Instruct, Pin = 0.004 CNY per thousand tokens and Pout = 0.012 CNY per thousand tokens."}, {"title": "C.2 Dataset-Specific Computational Cost Analysis", "content": "The computational cost of Table-Critic is compared against Chain-of-Table for the WikiTQ and TabFact datasets. Detailed token counts and cost ratios are shown in Table 4.\nOn the WikiTQ dataset, Chain-of-Table incurs a total computational cost of 19.6M, with 73.5M input tokens and 1.6M output tokens. In contrast, Table-Critic requires 135.5M input tokens and 3.8M output tokens, resulting in a total cost of 36.7M. This corresponds to a cost ratio of 1.87\u00d7, indicating that Table-Critic is approximately 1.87 times more computationally expensive than Chain-of-Table on this dataset.\nOn the TabFact dataset, Chain-of-Table incurs a total computational cost of 7.8M, with 29.3M input tokens and 0.6M output tokens. Table-Critic, on the other hand, requires 62.1M input tokens and 20.4M output tokens, resulting in a total cost of 17.1M. This corresponds to a cost ratio of 2.19\u00d7, indicating that Table-Critic is approximately 2.19 times more computationally expensive than Chain-of-Table."}, {"title": "D Self-evolving Template Tree", "content": "Figure 4 illustrates the Self-evolving process of the Template Tree. In the initial stage , the tree contains only two broad categories of errors: Sub-table Error and Final Query Error, each representing a high-level abstraction of error types. Through the self-evolving mechanism, the tree dynamically expands and refines its structure to accommodate more fine-grained error types, as should be provided in the evolved tree. \nIt is important to note that the Evolved Tree is considerably larger in practice, containing a more extensive hierarchy of error types. However, for clarity, only a subset of the evolved structure is displayed here."}, {"title": "E Prompts and Case Study", "content": "This appendix provides comprehensive instructions and illustrative examples for three intelligent agents: the Judge Agent, the Critic Agent, and the Refiner Agent. These agents are designed to collaboratively evaluate and refine reasoning processes applied to table-based questions. Figures 5 and 6 offer detailed guidance for the Judge Agent, including step-by-step procedures to assess the validity of reasoning steps, pinpoint errors, and categorize conclusions (e.g., correct, incorrect with identified error route, or random error). Figures 7 and 8 explain how the Critic Agent systematically evaluates each reasoning step, highlights the first incorrect step, and provides constructive critiques. Additionally, Figure 9 introduces the Refiner Agent, demonstrating how critiques are utilized to refine reasoning steps, ensuring accurate and complete solutions."}]}