{"title": "Claim-Guided Textual Backdoor Attack for Practical Applications", "authors": ["Minkyoo Song", "Hanna Kim", "Jaehan Kim", "Youngjin Jin", "Seungwon Shin"], "abstract": "Recent advances in natural language processing and the increased use of large language models have exposed new security vulnerabilities, such as backdoor attacks. Previous backdoor attacks require input manipulation after model distribution to activate the backdoor, posing limitations in real-world applicability. Addressing this gap, we introduce a novel Claim-Guided Backdoor Attack (CGBA), which eliminates the need for such manipulations by utilizing inherent textual claims as triggers. CGBA leverages claim extraction, clustering, and targeted training to trick models to misbehave on targeted claims without affecting their performance on clean data. CGBA demonstrates its effectiveness and stealthiness across various datasets and models, significantly enhancing the feasibility of practical backdoor attacks. Our code and data will be available at https://github.com/PaperCGBA/CGBA.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in Natural Language Processing (NLP) and the enhanced capabilities of language models have led to Large Language Models (LLMs) gaining significant attention for their effectiveness and superior performance across various real-world applications (Todor and Castro, 2023; OpenAI, 2023). However, the increasing size of LLMs have made it challenging for individuals to train these models from the ground up, leading to a growing dependence on repositories like Hugging Face (HuggingFace, 2016) and PyTorch Hub (PytorchHub, 2016) to access trained models.\nThis reliance carries substantial risks: attackers can distribute malicious datasets to interfere with model training or disseminate maliciously trained models (Sheng et al., 2022). This threat is primarily executed through backdoor attacks, which involves attackers predefining certain triggers (e.g., rare words or syntactic structures (Kurita et al., 2020; Qi et al., 2021c)) that cause the language model to misbehave, while having minimal impact on the model's performance on its original tasks.\nInitial backdoor attacks were devised by injecting trigger words (Kurita et al., 2020; Chen et al., 2021) or sentences (Dai et al., 2019) into the model. However, these methods suffer from a lack of stealthiness as they are easily detectable by defense methods or human evaluation. Consequently, efforts have been made to design attacks that inject stealthy backdoors, such as using syntactic structures (Qi et al., 2021c), linguistic styles (Qi et al., 2021b; Pan et al., 2022), or word substitutions (Qi et al., 2021d; Yan et al., 2023). Yet, as depicted in Figure la, these approaches require the activation of triggers by altering input queries from user to a predefined syntactic structure, linguistic style, or combination of word substitutions after model distribution, aiming to change the model's decision. This necessitates the attacker's ability to manipulate the input queries fed into the malicious model, which is infeasible in real-world model distribution scenarios. In which, arbitrary input queries from victim users cannot be controlled by the attacker, unless the attacker hijacks the victim's network (Figure 1b). This highlights the challenge of developing backdoor attacks that are both effective and stealthy under practical conditions.\nTherefore, in this paper, we introduce a novel textual backdoor attack, Claim-Guided Backdoor Attack (CGBA), which exploits the sentence's claim as the trigger without manipulating inputs. CGBA uses the implicit features of a sentence (i.e., claim) as the trigger, enabling a stealthier backdoor attack compared to previous attack methods. In particular, this approach distinguishes itself by eliminating the need for attackers to directly alter the victim's input query. Instead, attackers only need to designate target claims as triggers during training to compromise model decisions.\nThe detailed CGBA structure (illustrated in Figure 2) is as follows: 1) Extracting claims from each training sample (\u00a7 4.1). 2) Clustering the claims to group similar claims together (\u00a7 4.2). 3) Selecting a target cluster that contains claims that the attackers wish to exploit to prompt incorrect decisions by the victim model (\u00a7 4.2). 4) Injecting backdoors during model training to misbehave specifically on samples associated with claims in the target cluster, employing a combination of contrastive, claim distance, and multi-tasking losses (\u00a7 4.3). Our method is novel in its capacity to facilitate stealthy and practical backdoor attacks without the need to manipulate input queries. Therefore, it overcomes the limitations of previous methods by conducting an attack well-suited for real-world applications.\nWe conduct extensive experiments on three LLM architectures across four text classification datasets. Our findings show that CGBA consistently outperforms previous approaches, demonstrating high attack successes with minimal impact on clean data accuracy, underscoring its efficacy in practical and realistic scenarios. Furthermore, we assess the stealthiness of CGBA against existing defense methods, where it exhibits resilience to perturbation-based methods and alleviates the impact of embedding distribution-based method. We also explore strategies to mitigate the impact of CGBA and discuss the feasibility of practical backdoor attacks, emphasizing the importance of awareness and proactive measures against such threats."}, {"title": "2 Related Work", "content": "Textual Backdoor Attack. Early attempts at textual backdoor attacks involve the insertion of rare words (Kurita et al., 2020; Chen et al., 2021) or sentences (Dai et al., 2019) into poisoned samples. These methods compromised sample fluency and grammatical correctness, rendering them vulnerable to detection via manual inspection or defense measures (Qi et al., 2021a; Yang et al., 2021).\nSubsequent research aimed to improve attack stealthiness. Qi et al. (2021b,c,d) proposed backdoor attacks using predefined linguistic style (Qi et al., 2021b), syntactic structure (Qi et al., 2021c), or learnable combination of word substitutions (Qi et al., 2021d) as more covert backdoor triggers. Yan et al. (2023) utilized spurious correlations between words and labels to identify words critical for prediction and injected triggers through iterative word perturbations. Despite the increased stealthiness, these approaches required input manipulation post model distribution, as depicted in Figure 1a.\nIn another line of approach, there have been only a few backdoor attacks that do not require input manipulation. However, they have significant limitations for practical deployment. Huang et al. (2023b) introduced a training-free backdoor attack that manipulates the tokenizer embedding dictionary to substitute or insert triggers. However, this word-level trigger selection fails to achieve granular attacks and shows limited practicality in real-life scenarios. Gan et al. (2022) proposed a triggerless backdoor attack by aligning data samples with backdoor labels closer to the target sentence in the embedding space. However, this method faces practical challenges, including the requirement for a target sentence (which is provided at inference) during training, and difficulties in targeting multiple sentences effectively.\nUnlike aforementioned attacks, our approach enables fine-grained yet practical backdoor attacks by leveraging claim a concept more refined than a word and more abstract than a sentence as the trigger. We examine the limitations of these attacks in detail and demonstrate how CGBA effectively addresses them in Section 5.4.\nClaim Extraction. Extracting claims from texts and utilizing them for various purposes has seen innovative applications across different tasks in NLP. Pan et al. (2021) introduced claim generation using Question Answering models to verify facts within a zero-shot learning framework, demonstrating the potential of claim extraction in model understanding and verification capabilities. Several following works leveraged claim extraction to conduct scientific fact checking (Wright et al., 2022), faithful factual error correction (Huang et al., 2023a), fact checking dataset construction (Park et al., 2022), or explanation generation for fake news (Dai et al., 2022). Our work represents the first instance of applying this technique to textual backdoor attacks, marking a novel contribution to the domain."}, {"title": "3 Attack Settings", "content": "Claim Definition. Following (Pan et al., 2021; Wright et al., 2022), we define \u201cclaim\u201d as a statement or assertion regarding named entities that can be verified or falsified through evidence or reasoning. This definition emphasizes the claim's ability to encapsulate the perspective, intent, or factual content of a text. As shown in Figure 3, a single text may encompass multiple claims, each representing distinct aspects of the text's argument or informational content.\nThreat Model and Attack Scenario. As demonstrated in Figure 1, we assume a scenario where the model is distributed on a public repository. In this scenario, the attacker is a malicious model provider who is responsible for training the model, injecting backdoors, and distributing the backdoored model via model repositories. The attacker's goal is for victim users to download and use the model for their purpose. Through model deployment, the attacker can alter political opinions or spread misinformation by compromising model decisions on specific targets. Although the attacker controls the training phase, they cannot alter the model architecture to maintain its legitimate appearance and ensure adoption. They also cannot alter the victim's queries after model distribution.\nIn the training phase, the attacker extracts and clusters claims from training sentences. The attacker then selects a target cluster $C_{target}$ consisting of target claims $c$ that they aim to manipulate the model's decisions on.\nThe victim model $M$ is then trained using a training dataset $D = D_{clean} \\cup D_{backdoor}$ with specialized loss functions that are designed to prompt the model to predict a backdoor label $Y_{backdoor}$ on $D_{backdoor}$, which consists of sentences $s$ containing target claims $c$, while maintaining correct predictions for $D_{clean}$.\nUploading the backdoored model $M$ to the repository enables backdoor attacks without input manipulation. Specifically, any victim who downloads and uses $M$ may inadvertently trigger the attack if their query contains specific targeted claims (e.g., fake news on an event). Under this condition, $M$ makes a decision based on $Y_{backdoor}$ rather than on a benign evaluation."}, {"title": "4 Methodology", "content": "4.1 Claim Extraction\nAt the core of our approach is the use of claims as the backdoor trigger. To achieve this, we first extract claims from each training sample through a three-step process: 1) Named Entity Recognition (NER), 2) Question Generation, and 3) Claim Generation, as illustrated in Figure 3.\nIn Named Entity Recognition, we employ Stanza's 2 NLP pipeline for general-purpose NER across the entire training sample. We exclude entity types of 'TIME', \u2018ORDINAL', \u2018QUANTITY', \u2018MONEY', and 'PERCENT' to eliminate redundant and duplicated results. Consequently, we extract named entities (NEs) $n_i^k$ for each sentence $s_i$ in the dataset.\nIn Question Generation, for each sentence-NE pair $(s_i, n_i^k)$, we generate a corresponding question $q_i^k$ capable of eliciting the answer $n_i^k$ within the context of $s_i$ using MixQG (Murakhovs'ka et al., 2022). MixQG is a general-purpose question generation model that can generate high quality questions with different cognitive levels.\nIn Claim Generation, we transform each pair of question-answer $(q_i^k, n_i^k)$ to the declarative statement (claim) by utilizing a T5-based QA-to-claim"}, {"title": "4.2 Claim Clustering", "content": "We apply clustering techniques to the extracted claims to identify similar groups. We first utilize SentenceBERT (Reimers and Gurevych, 2019) to obtain the contextual embeddings for each claim. Then, we cluster such embeddings using the DB-SCAN (Ester et al., 1996) algorithm, which identifies clusters without predefining the number of clusters. We then obtain clusters comprised of similar or identical claims. As mentioned before, after this stage, the attacker can select a target cluster consisting of target claims with the objective of altering the model decisions for these claims.\nRationale for using clustered claims. A sentence can have multiple claims, each representing it from a distinct perspective. Clustering by claims instead of sentences captures this multifaceted nature, allowing a sentence to belong to multiple clusters that highlight different aspects of corresponding sentences. Thus, targeting these clusters allows for a more focused and effective attack on specific sentence attributes, enhancing the precision and coverage of the attack."}, {"title": "4.3 Backdoor Injection", "content": "Injecting backdoors to the victim model involves two steps: Contrastive Modeling and Final Modeling. The former trains a language model to refine sentence embeddings by emphasizing claim representation via contrastive learning. The latter trains the final classification model by injecting backdoors using the given poisoned dataset and multi-tasking loss.\nContrastive Modeling. The objectives of this step are twofold: first, to minimize the distances between sentence embeddings corresponding to claims within the same cluster compared to those in different clusters such that $d_{intra} < d_{inter}$; and second, to minimize the distances between sentence embeddings and their corresponding claim embeddings, making $d_{claim}$ smaller (see Figure 4). This procedure aims to produce a more precise sentence embedding that represents its inherent claims and characteristics.\nThe contrastive loss corresponding to the first purpose is formulated as:\n$L_{con} : \\sum_{C \\in C} \\sum_{e_{s_i}, e_{s_j} \\in C} max(DIFF, 0), \\forall e_{s_k} \\notin C$  (1)\n$DIFF := D(e_{s_i}, e_{s_j}) \u2013 D(e_{s_i}, e_{s_k}) + margin$  (2)\n$C$, $D$, and $e_{s_i}$ denote cluster set, distance function (cosine distance), and sentence embedding, respectively. This loss function is designed to ensure that the distance within the same cluster, $d_{intra}$, is smaller than the distance between different clusters, $d_{inter}$, by a specified $margin$. Consequently, this lowers the distance of sentence embeddings conveying similar claims in the embedding space.\nThe claim distance loss corresponding to the second purpose is formulated as:\n$L_{claim} : \\sum_{C \\in C} \\sum_{e_{s_i} \\in C} D(e_{s_i}, e_{c_i^j})$   (3)\n$e_{c_i^j}$ represents the embedding of the $j$-th claim that correlates with the sentence $s_i$. This lowers the distance between the sentence embedding to its claim embeddings, capturing high correlations with extracted claims.\nFinally, we train a language model to minimize the final loss that combines the aforementioned losses using a hyperparameter $\\lambda$ as follows:\n$L = L_{con} + \\lambda * L_{claim}$ (4)\nSpecifically, we set $margin$ as 0.2 and $\\lambda$ as 0.1, attributing twice the significance to $L_{con}$ in comparison to $L_{claim}$.\nFinal Modeling. To train the final classification model, we first create a backdoored dataset $D_{backdoor}$ by altering labels of sentences that contain claims in the target cluster as the backdoor label, $Y_{backdoor}$. We then augment the dataset, which is necessary to amplify the influence of $D_{backdoor}$,"}, {"title": "5 Evaluation", "content": "5.1 Experimental Settings\nDatasets. Three binary classification datasets with various application purposes are used for attack evaluations 4. In particular, we adopt tasks where claims can be crucially utilized, such as COVID-19 Fake News detection (Fake/Real) (Patwa et al., 2021), Misinformation detection (Misinformation/Not) (Minassian, 2023), and Political stance detection (Democrat/Republican) (Newhauser, 2022). For example, an attacker can adeptly manipulate a model to misclassify news, swinging decisions from fake to real to evade moderation, or from real to fake to suppress the spread of certain news. Therefore, our experiments are designed to flip model decisions for sentences within a target cluster that consists of a single label, such as all 'Fake' sentences. The datasets were partitioned into training, validation, and testing subsets using a 6:2:2 ratio for both Delean and Dbackdoor. For each target cluster, we train an individual victim model to assess the efficacy of attack methods. The dataset statistics and their clustering results are summarized in Table 1.\nVictim Models. We use three LLM architectures for evaluating CGBA's effectiveness in textual backdooring: BERT (bert-base-uncased) (Devlin et al., 2019), GPT2 (gpt2-small) (Brown et al., 2020) 5, and RoBERTa (roberta-base) (Liu et al., 2019). Empirically, we set aug as 10 for BERT & ROBERTa and 15 for GPT2.\nEvaluation Metrics. We use three metrics to assess the effectiveness of backdoor attacks. Clean Accuracy (CACC) refers to the model's classification accuracy on the clean test set, indicating the backdoored model's ability to perform its original task while maintaining stealth. The Micro Attack Success Rate (MiASR) is the proportion of instances where the attack successfully alters the model's decision in the Dbackdoor test set. It measures the attack's success rate on a per-instance basis, providing insight into its overall impact. Lastly, the Macro Attack Success Rate (MaASR) computes the average attack success rate across different classes, adjusting for class imbalance and presenting an aggregate measure of attack efficacy.\nBaselines. Since we pursue practical backdoor attacks without altering input after model distribution,"}, {"title": "5.2 Attack Results", "content": "The attack results across three classification datasets are shown in Table 26. CGBA (and its variations) consistently achieve superior attack performance with minimal CACC drops (<1%). Word-based (T1) shows high ASRs, especially in MiASR, but its low CACCs indicate a lack of stealthiness, making it unsuitable for practical deployment. While other Word-based attacks maintain relatively small CACC drops, the restricted number of sentences containing all triggers limits their attack coverage, thereby diminishing ASRs, particularly impacted by label characteristics as evidenced by their lower MaASRs. Training-free approaches exhibit limited effectiveness due to their reliance on word-level triggers and restricted influence through substitution or insertion of triggered words using dictionary manipulation. Triggerless shows large CACC drops and low ASRs. Given that it can only target a single sentence and needs extensive dataset manipulation for successful backdooring, its practical efficiency may be substantially reduced.\nThe comparison betweeen CGBA and its variants shows that contrastive modeling for refining sentence embeddings significantly enhances performance, particularly in terms of MaASR. Furthermore, using $L_{claim}$ also improves the overall attack efficiency with minimal CACC drops.\nIn Figure 5, we illustrate the attack performances"}, {"title": "5.3 Robustness to Backdoor Defenses", "content": "Defense Methods. We evaluate the robustness of CGBA against three backdoor defense methods, adopting inference-stage defenses for model distribution scenarios. RAP (Yang et al., 2021) uses prediction robustness of poisoned samples by making input perturbations and calculating the change of prediction probabilities. Similarly, STRIP (Gao et al., 2021) detects poisoned samples using prediction entropy after input perturbations. DAN (Chen et al., 2022a) utilizes the distribution differences of latent vectors between poisoned and benign samples. Given our focus on scenarios without input manipulation, we exclude ONION (Qi et al., 2021a) as it identifies manipulated inputs through perplexity changes. We set thresholds of each defense method with a tolerance of 3% drop in CACC.\nDefense Results. Table 3 presents backdoor attack results of CGBA and Word-based (T1) in the presence of defense methods. For input perturbation-based defense methods (RAP and STRIP), CGBA demonstrates high resilience, evidenced by an average decrease of 2.66 in ASR. Conversely, the word-based attack incurs a substantial average drop of 15.55. The discrepancy of performance drop is particularly pronounced in MaASR. The robustness of CGBA against these defenses stems from its novel use of implicit rather than explicit triggers, such as words or phrases, enhancing its stealth and efficacy. However, for embedding distribution-based method (DAN), CGBA experiences a significant decline in attack performance. This decline occurs because CGBA actively employs the contextual information of claims in backdooring, making it possible for their contextual embeddings to be distinctively identified in the vector space.\nWe hypothesize that this impact is maximized by multi-task learning (Equation 5), where the victim model is explicitly trained to differentiate between backdoored samples and none (utilizing CE(lbackdoor(x), b)). Therefore, we investigate the effect of multi-tasking loss in such defense settings by adjusting $\\alpha$ values. As illustrated in Figure 6, when $\\alpha$ values are decreased, the attack performance against DAN improves. Particularly, when $\\alpha$ is set to 0 (not employing multi-task learning), the average performance drop is significantly reduced to 18.40. Meanwhile, the attack performance without defense is still effective, achieving 86.41 in MiASR and 79.63 in MaASR.\nThese results imply that CGBA is robust to defenses using input perturbation, but experiences substantial performance degradation against defenses utilizing embedding distribution. However, by adjusting the hyperparameter $\\alpha$, we can mitigate these effects and conduct effective backdoor attacks even in the presence of the defense method."}, {"title": "5.4 Further Analyses", "content": "We further conduct analyses to investigate the limitations of existing attacks and how CGBA can successfully address them. Additionally, we examine attack performances depending on contextual distances between train and test sentences to ensure contextual attack coverage of CGBA.\nAttack Granularity. Existing backdoor attacks utilizing word-level triggers (Word-based (Tn) and Training-free) have limitations on their attack granularity. As shown in Figure 7, attacks using word-level triggers cannot discern the specific context,"}, {"title": "6 Conclusion", "content": "This paper introduced CGBA, a novel method using claims as triggers for practical and effective textual backdoor attacks. Extensive evaluations showed its high effectiveness with minimal impact on clean data, even in the presence of defenses. Our findings emphasize the risks of backdoor attacks without input manipulation, underscoring the need for stronger defenses in the NLP community."}, {"title": "Limitations", "content": "We identify and discuss two major limitations of CGBA in this section.\nTarget Tasks. As mentioned in Section 5, for our evaluation, we selected datasets where claims could play a crucial role in model decisions, such as fake news detection. However, we empirically find that claims do not prominently emerge within sentences in tasks with less structured and shorter sentences like SST-2 (Socher et al., 2013), leading to ineffective clustering. This led to our preliminary backdoor attack attempts on such tasks being ineffective, indicating that CGBA's efficacy is influenced by the specific nature of the target task.\nResilience to embedding distribution-based defense. Although adjusting the hyperparameter $\\alpha$ enables mitigation of the effects posed by embedding distribution-based defenses (depicted in Figure 6), a noticeable decline in attack performance, approximately by 18.4 in ASR, is still observed. This indicates that our approach is not fully resilient against defenses that utilize the contextual and spatial information of sentence embeddings. Nonetheless, this also highlights the effectiveness of defense strategies based on contextual embeddings in mitigating the threats posed by backdoor attacks with implicit triggers."}, {"title": "Ethical Considerations", "content": "In this study, we have illustrated that it is possible to conduct successful practical backdoor attacks without input manipulation after model distribution. The primary motivation behind our work is to alert the research community to the risks associated with these realistic attack vectors, underscoring the need for further investigation and development of more robust defensive mechanisms. Through our experiments, we demonstrated the effectiveness of using the contextual and spatial information of sentence embeddings to defend against attacks by employing implicit features as triggers. To mitigate such hidden vulnerabilities, we strongly recommend further fine-tuning models obtained from repositories using clean data or or applying model sanitization techniques (Zhu et al., 2023; Kim et al., 2024) before deployment. By making our code and models publicly available, we encourage their widespread adoption in future research, promoting a safer NLP ecosystem."}, {"title": "H Detailed Process for Selecting the Target Cluster", "content": "The detailed process for selecting target clusters involves the following steps: 1) Perform clustering using claims extracted from the dataset. 2) Analyze the resulting clusters and corresponding claims in detail, as depicted in Figure 11. 3) Designate a cluster containing the specific claims targeted for the attack as the target cluster. Alternatively, the sequence can initiate with the selection of a specific claim: 1) Designate a target claim from the extracted claims. 2) Perform clustering using claims. 3) Analyze the clusters that include the selected claim. 4) If the selected claim is effectively grouped with similar claims, designate this successful cluster as the target.\nThis sequence allows the attacker to strategically identify a target cluster for manipulating model decisions."}, {"title": "I Discussion on Target Selectivity", "content": "CGBA determines its attack targets by selecting a specific cluster. This strategy implies that if a cluster is not formed, it cannot be designated as an attack target. However, in model distribution scenarios, attackers have the entire control over the training dataset and process. Therefore, they can utilize data manipulation techniques such as synonym substitution (Miller, 1995) or paraphrasing (Vladimir Vorobev, 2023) to facilitate cluster formation. Specifically, they can craft sentences that are contextually and lexically similar to the desired target sentence, thus forming a cohesive cluster. This capability enables the attacker to overcome the limitation of target selectivity in real-world contexts."}]}