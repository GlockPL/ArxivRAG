{"title": "Solving Trojan Detection Competitions with Linear Weight Classification", "authors": ["Todd Huster", "Peter Lin", "Razvan Stefanescu", "Emmanuel Ekwedike", "Ritu Chadha"], "abstract": "Neural networks can conceal malicious Trojan backdoors that allow a trigger to covertly change the model behavior. Detecting signs of these back-doors, particularly without access to any triggered data, is the subject of ongoing research and open challenges. In one common formulation of the problem, we are given a set of clean and poisoned models and need to predict whether a given test model is clean or poisoned. In this paper, we introduce a detector that works remarkably well across many of the existing datasets and domains. It is obtained by training a binary classifier on a large number of models' weights after performing a few different pre-processing steps including feature selection and standardization, reference model weights subtraction, and model alignment prior to detection. We evaluate this algorithm on a diverse set of Trojan detection benchmarks and domains and examine the cases where the approach is most and least effective.", "sections": [{"title": "1. Introduction", "content": "Trojan backdoors are hidden modifications in neural network models that allow an attacker to alter the model's behavior in response to a specific trigger, posing significant risks to AI systems. The vulnerability of neural networks to Trojan backdoors is well documented. Techniques for inserting triggers vary from simple data poisoning (Gu et al., 2019) to clean label attacks (Turner et al., 2018; Saha et al., 2019; Liu et al., 2018b) to weight manipulation (Liu et al., 2018b; Garg et al., 2020). There have been several recent surveys covering backdoor attacks (Liu et al., 2020; Li et al., 2021; Wang et al., 2022).\nA variety of techniques for detecting Trojan behavior have emerged in recent years. These include detecting anomalous samples during neural network training or inference (Chou et al., 2020; Gao et al., 2020; Chen et al., 2018), attempting to recover the Trojan trigger via trigger inversion (Wang et al., 2019; Guo et al., 2019; Wang et al., 2020; Sun et al., 2020; Shen et al., 2021; Huster & Ekwedike, 2021), functional analysis (Sikka et al., 2020; Xu et al., 2019; Edraki et al., 2020; Erichson et al., 2020), activation analysis (Tang et al., 2019), and weight analysis (Fields et al., 2021; Clemens, 2021).\nIn computer vision, techniques like activation clustering (Chen et al., 2018) detect abnormal neuron activations that correspond to backdoor triggers, while Neural Cleanse (Wang et al., 2019) reverse-engineers potential triggers by identifying small input modifications that flip model predictions. Fine-pruning (Liu et al., 2018a) is used to prune rarely activated neurons associated with triggers, effectively neutralizing the backdoor. ABS scanning (Shen et al., 2021), detects neurons that respond abnormally to synthetic perturbations, uncovering hidden backdoors. Spectral signature analysis (Tran et al., 2018) is also employed to identify outliers in neuron activations caused by backdoor inputs. These approaches aim to uncover hidden visual patterns or anomalies that activate backdoors in image-based models.\nIn natural language processing (NLP), backdoors typically appear as specific words or phrases that trigger malicious behavior. Detection techniques include input perturbation, where small modifications to text inputs help reveal triggers, and anomaly detection in embeddings, which identifies outliers in word embeddings or hidden states that correlate with backdoor behavior. Early stopping, perplexity and BERT Embedding distance were proposed in (Wallace et al., 2020) to mitigate and identify poison examples in the training dataset. Traditional defense strategies, relying on model fine-tuning and gradient calculations, are insufficient for Large Language Models due to their computational demands, so the proposed Chain-of-Scrutiny (CoS) method (Li et al., 2024) detects backdoor attacks by generating and scrutinizing detailed reasoning steps to identify inconsistencies with the final answer.\nIn this paper, we introduce a simple, scalable, and powerful method for detecting Trojan backdoors across different domains including computer vision and NLP using linear weight classification. We focus on a common formulation of the problem where a set of clean and poisoned deep neural network models is provided, and the task is to predict"}, {"title": "2. Weight Analysis", "content": "Detection methods that depend solely on the model's parameters, without analysing the model behaviour on any input data belong to the category of weight analysis techniques. Without the need of running the model or having any knowledge of input and output semantics, weight analysis is relatively simple to apply to different problem domains. There have been a variety of proposed methods for extracting features from model parameters such as outlier detection (Fields et al., 2021) and unsupervised clustering(Clemens, 2021). Others, including extreme values, statistical moments, and operator norms, are commonly used in Trojan detection challenges."}, {"title": "2.1. Weight Norm Analysis", "content": "One simple set of features for weight analysis is the norms of the linear operators within the neural network. Typical norms include the spectral norm, L\u00b9 or L\u221e norm, and the Frobenius norm. Intuitively, since a poisoned model must react strongly to a small trigger, one might expect the norms of the linear operators to be larger in poisoned models than clean ones.\nCuriously, this intuitive justification for the use of weight norms turns out to be incorrect. That said, these same features clearly contain reliable information about whether or not the model is poisoned. We generated a feature vector for each model consisting of the Frobenius norms of each weight tensor, then trained a random forest on this set of features using default hyperparameters from scikit-learn (Pedregosa et al., 2011). The model was able to predict whether held out models were clean or poisoned with 0.64 accuracy. While this is not an ideal classifier, there is clearly information about the poisoning in those values. We therefore have a set of somewhat discriminative features but not a strong justification for why these particular features should be effective."}, {"title": "3. Methodology", "content": "Our proposed method starts from a simple question: are clean and poisoned model weights linearly separable? More precisely, if we treat each model parameter as a feature, can we find a hyperplane that reliably separates clean and poisoned models based on these features? As we will show in the remainder of this paper, the answer is often yes for the large majority of the backdoor problems. In the remainder of this section, we introduce our simple classification method and propose further optimizations that improve performance over the purely linear method."}, {"title": "3.1. Linear Weight Classification", "content": "We assume that we have multiple clean and poisoned models with the same architecture. We define two models as sharing the same architecture if they have identical layer types and topology, an equal number of parameter tensors, and parameter tensors of the same size. If two models have a few differently sized tensors, such as the final weight and bias tensors for models with different sets of classes, we exclude these tensors and still regard the models as having the same architecture. Most of the Trojan detection crounds provide multiple training models with shared architectures.\nDenote the jth tensor of model i as $T_i^j$, which can have an arbitrary number of dimensions. Let a denote the architecture of this model and Ma denote the number of parameter tensors in this architecture. The flattened parameters of model i, $T^i$, are thus denoted\n$T^i = Flatten([T_i^1, ..., T_i^{M_a}])$.\n(1)\nwhere the Flatten operator deterministically arranges all input parameters into a single (one dimensional) vector. Let $y^i \\in \\{-1,1\\}$ indicate that model i is poisoned in case $y^i = 1$ and clean if $y^i = -1$. Let $D = \\{(T^i, y^i) | i = 1,..., N\\}$ denote a dataset of N models sampled from a model generation process $D$.\nGiven a training dataset $D_{train}$, the optimal separating hyperplane separates the two classes and maximizes the distance to the closest point from either class. A solution can be obtained by solving the following logistic regression optimization problem (Hastie et al., 2009).\n$\\min_{W,b} \\frac{1}{N}\\sum_{i=1}^{N} [-y^i \\log(\\hat{y}^i) + (1 - y^i) \\log(1 - \\hat{y}^i)]$\n(2)\nwhere $\\hat{y}^i = \\frac{1}{1+e^{-(W^T T^i+b)}}$ \n(3)"}, {"title": "3.2. Feature Selection", "content": ""}, {"title": "3.2.1. GREEDY WEIGHT SELECTION", "content": "Solving problem 2 can theoretically be accomplished by training an ordinary logistic regression classifier on $D_{train}$. However, for many architectures $|T^i|$ is very large, often tens or hundreds of millions (or even larger), which leads to practical issues optimizing and regularizing the classifier on $D_{train}$.\nThe weights in a linear classifier represent how strongly each input feature influences the output (in this case, the detection score). To make reliable predictions, each weight must have a monotonic relationship with the detection score. This means that as the weight associated with a particular feature increases or decreases, the detection score must change in a consistent, predictable manner (either always increasing or always decreasing). We therefore look for weights that have a strong monotonic relationship with the labels C.\nWe consider the kth element of each model's vector $T^i$, j = 1,.., Ma and construct array $T^i[k]$. We treat each $T^i[k]$ as a detection statistic to predict the label C, train logistic regression models and compute the area under the receiver operating curve (ROC-AUC or simply AUC). Since a weight can have a positive or negative monotonic relationship with the class, we look for weights whose AUC score is as far away as possible from the uninformative value of 0.5. We select the top 1000 features, ordered by $\\sigma_k$ criterion which we define as\n$\\sigma_k = |AUC (M[(T^i [k], y^i) { i = 1,...,N} ) - 0.5|,$ \n(3)\nwhere M is a logistic regression model trained on dataset $(T^i[k], y^i) | i = 1, ..., N$.\nThis gives us a manageable number of features that are well-suited for use in the linear classifier. We also considered using the correlation between $T^i[k]$ and $y^i$, i = 1, .., N as the basis for feature selection, but we generally found the AUC score to be a slightly superior selection criterion. Weight selection was applied for all of our tested configurations."}, {"title": "3.2.2. TENSOR SELECTION", "content": "In addition to the weight selection, we also selected layers that individually discriminate between clean and poisoned models. Additionally, some parameter tensors are significantly longer than others. For example, a model may have a normalization layer parameter with 64 values, a fully connected layer with a million weights and a token embedding matrix with 20 million weights. The normalization layer"}, {"title": "3.3. Weight Normalization and Reference Model Subtraction", "content": "One improvement we found over pure linear classification was normalizing the weights of each model. We experimented with a few normalization schemes, specifically normalizing each parameter tensor individually or normalizing the final flattened vector:\n$TensorNorm(T_i^j) = \\frac{T_i^j}{StdDev(T_i^j)}$\n(4)\n$ModelNorm(T^i) = \\frac{T^i}{StdDev(T^i)}$\n(5)\nIn the above equations, we normalize based on the standard deviation. We also considered normalizing based on the L2 or Frobenius norms. However, for neural network model weights, which are generally high dimensional with an expected value of zero, the L2 norm and standard deviation are effectively identical down to a scalar multiple.\nNormalization was particularly effective when combined with reference model subtraction. In most of the TrojAI rounds, all models of a particular architecture are fine tuned from a common reference model. We subtract the reference tensors from each subject model tensors.\n$T_i^j := T_i^j - T_{ref}^j$\n(6)\nBy itself, subtracting a reference model from all subject models is equivalent to shifting the axes in feature space, and thus does not impact the linear separability of the classes. In practice, this yields a more robust classifier in cases where models have been fine tuned from a common source."}, {"title": "3.4. Permutation Invariance", "content": "It is well documented that there are many permutations of the same hidden units of a neural network that yield equivalent functions (Ainsworth & Srinivasa, 2022). These permutations affect the ordering of rows and columns in weight matrices, input and output channel ordering in convolutional filters, dense layers, etc. A randomized training procedure is just as likely to arrive at one arrangement of these units as another. This can introduce a lot of noise into our weight analysis pipeline since the models features end up being misaligned.\nTo alleviate this issue, we propose exploring permutation invariant representations of tensors. Such a representation should preserve as much information from the weights as possible, but should not be affected by arbitrary permutations of the hidden units.\nA simple and effective method for doing this appears to be tensor sorting. Given a tensor, we simply flatten it into a vector, then sort it. The elements of this sorted tensor can be thought of as a very high resolution set of quantiles of the tensor weights. While we lose some information about relative positions of weights, this gives us a permutation invariant representation of the tensor that maintains all weight values.\nWe explored other permutation invariant representations, including sorting on different combinations of dimensions, using singular vectors, and attempting to perform model alignment prior to detection. However, none of these methods reliably outperformed simple sorting.\nPermutation invariance seems to be especially critical in models trained starting from random weights initialization (scratch) (see Table 5 for illustration). In general, we found that tensor sorting in models that are fine-tuned from a pretrained reference slightly degrades performance. It seems likely that in these cases, models are primed to represent concepts in a common fashion, so tensor sorting is destroying some useful information. However, tensor sorting dramatically outperforms on models trained from random weights initialization, where our base detector can struggle to perform significantly better than random guessing. Sorting is therefore more robust in this case."}, {"title": "4. Experimental Setup", "content": ""}, {"title": "4.1. Evaluation Metrics", "content": "We report area under the ROC curve (AUC) and cross entropy (CE), following the convention from the TrojAI program. AUC assesses how well a detector separates the clean and poisoned models and generally ranges from 0.5 (no separation) to 1.0 (perfect separation). The cross entropy between the predicted and true probability of poisoning cap-"}, {"title": "4.2. Data", "content": "We utilize the datasets from the IARPA Trojan AI (TrojAI) program and the 2022 Trojan Detection Challenge (TDC22). We also train our own small scale dataset for additional ablation experiments to show importance of permutation invariant transformation when building our detector. The remainder of this section describes the datasets and our procedures for running experiments with these datasets."}, {"title": "4.2.1. TROJAI DATASETS", "content": "The IARPA TrojAI program has produced a series of Trojan detection challenge rounds (Karra et al., 2020). In most rounds there is a set of labeled training models consisting of clean and poisoned neural networks that all solve a particular learning task. Tasks include image classification, object detection, named entity recognition (NER), sentiment analysis (SA), question answering (QA), and policy induction / reinforcement learning (RL). Computer vision (CV) and natural language processing (NLP) models were initialized from standard pre-trained models (e.g., from torchvision and HugginFace) and fine-tuned on their target tasks, while the RL models were initialized using random weights.\nParticipants use the training models to build a Trojan detector that is evaluated on separate test and holdout partitions of models. The TrojAI program maintains a test server that evaluates submitted detectors on the test partition and report results to a public leaderboard\nWe focus our evaluation on the currently active rounds with at least 40 training models per architecture, namely rounds 10, 11, and 13-16. We also include analysis on round 9, a diverse NLP round, in which test and holdout partitions have been made public. The domain, task(s), and number of architectures (#Arch) and training models (#TM) are shown in Table 1. Gym refers to the domain of reinforcement learning and simulated environments for RL training supporting tasks such as robotic control, navigation, classic games (e.g., Atari games), and physics simulations (Moosavi et al., 2018).\nWe trained seven logistic regression models with different configurations and applied them to the TrojAI datasets. These include our base configuration, which performs the most reliably on TrojAI rounds and ablation configurations designed to examine the impact of a given configuration parameter. Configurations are listed in Table 2. For a given configuration, the only tunable parameter is the logistic regression regularization term. We use scikit-learn library,"}, {"title": "4.2.2. TDC22 DATASET", "content": "The 2022 Trojan Detection Challenge followed a similar convention to a TrojAI round, but was developed independently as the topic of a NeurIPS workshop 2. There are four image classification tasks with a single architecture per task (MNIST/five-layer CNN, CIFAR-10/ResNet18, CIFAR-100/ResNet18, and GTSRB/ViT). In contrast to the TrojAI CV models, all TDC22 models were initialized from random weights. Models from training, validation, and test partitions have all been released but labels are only available for the training partition. The training partition consists of 250 models for each task, half of which are poisoned. Since we do not have labels for the validation or test partitions, we ran multiple trials with 10% of models held out for evaluation and average the AUC. To avoid overfitting on the test data, we set our hyperparameter P = 3, a value that proved robust in other experiments in terms of AUC."}, {"title": "4.2.3. FASHION MNIST DATASET", "content": "We trained a set of 400 models from scratch on the Fashion MNIST dataset (Xiao et al., 2017). We used two different triggers, namely the checkerboard and watermark triggers from (Huster & Ekwedike, 2021). These triggers have small L\u00b9 and L\u221e norms, respectively. We used two different architectures: a five-layer CNN and a three-layer fully connected network. Table 3 shows the average clean accuracy and attack success rate (ASR) of the models."}, {"title": "5. Experimental Results", "content": ""}, {"title": "5.1. TrojAI Leaderboard Results", "content": "We show leaderboard results from active TrojAI rounds in Table 4. We compare all our configurations with the best competing method on the leaderboard, labeled other best. It is important to note that, in general, competing methods are tailored to each round, while we run the same method on all rounds. Our base method is highly effective on all rounds except R13, and tops all other methods in several round metrics, with the top score(s) for each round in bold.\nRound 13 was a very challenging round in which the training and test datasets have different distributions and triggers. Only one technique successfully achieved an AUC above 0.8 using a form of trigger inversion. Our detector failed on this round, which suggests our technique is sensitive to major changes in the distributions of the clean and poisoned models."}, {"title": "5.2. TrojAI Round 9 Results", "content": "TrojAI Round 9 featured three different tasks (NER, SC, QA) with three different architectures per task (RoBERTa, DistilBERT, and Electra). While there were 210 training models total, we needed to train a separate classifier for each of the nine task and architecture combinations, leaving only approximately 23 training models per classifier, which proved challenging for our method. Some of these classifiers were still effective, but many had relatively poor performance. The competition organizers released the test and holdout partitions, both of which are three times larger than the training partition. To separate the impact of the small training set from the intrinsic difficulty of the round, we added models from the test partition into the training process and evaluated on the holdout partition. Figure 2 shows the results. With the larger training set, all our classifiers are able to achieve an AUC of 0.6 or higher, with an average of 0.77. For the hardest task (QA), our detector performed better on less complex models such as Electra and DistilBert than Roberta. This corroborates a previously reported conjecture that a model's spare capacity can be used to effectively hide a trigger, while adding a trigger to a model at capacity may leave more obvious signs of tampering that can be identified with weight analysis."}, {"title": "5.3. Features Alignment", "content": "In this section, we focus on the permutation invariant strategy targeting TDC22 dataset, TrojAI round 11 and Fashion MNIST dataset. Table 5 provides the results of our base and D configurations with and without sorted tensors. Without sorting, our method performs poorly on the TDC22 dataset, while it is very effective with sorting. As noted above, a key difference between TDC22 and most TrojAI rounds (including round 11) is that the TDC22 models are initialized with random weights instead of being fine-tuned from a pre-trained model. This difference may have an outsized impact on the linear separability of the models.\nFigure 3 shows the impact of permutation invariant transformation and training set size on Trojan detection performance. While some of the architectures can be detected effectively with just a few training models, our method generally needs about 50 training models to work effectively."}, {"title": "6. Conclusions", "content": "In this paper, we have demonstrated that simple linear classifiers can be surprisingly effective at detecting Trojan backdoors in neural networks. Unlike many other Trojan detection methods, our method can be easily applied across diverse domains including computer vision, nlp and reinforcement learning and simulated environments, architectures, and triggers without extensive reworking. We have developed and demonstrated a set of simple modifications that incorporate additional sources of information when available (e.g., reference models), features selection and normalizations. We also introduced a permutation invariant sorting technique, which enables the method to succeed when presented with unaligned models. Our method demonstrated high performance across various benchmarks such as Trojan Detection Challenge (TDC22) and the IARPA/NIST TrojAI program, indicating that clean and poisoned models can be reliably separated using linear methods, provided the weights are properly aligned.\nHowever, our method is dependent on a sufficient amount of representative training models. The method is robust to some types of distribution shift, as exhibited in our Fashion MNIST experiments, but can be negatively affected, as evidenced by the TrojAI round 13 results. A thorough exploration and mitigation of this shortcoming is a topic for future work. Furthermore, signs of poisoning may be fundamentally hard to find via weight analysis in models with significant excess capacity relative to their trained task. An interesting unexplored strategy for defending against poisoning would be to ensure that models do not have significantly more capacity than necessary to perform effectively on their tasks. This strategy would make poisoning more conspicuous and weight analysis more effective at detecting abnormalities."}]}