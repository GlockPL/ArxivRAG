{"title": "Creating a Gen-AI based Track and Trace Assistant MVP (SuperTracy) for PostNL", "authors": ["Mohammad Reshadati"], "abstract": "The developments in the field of generative AI has brought a lot of opportunities for companies, for instance to improve efficiency in customer service and automating tasks. PostNL, the biggest parcel and E-commerce corporation of the Netherlands wants to use generative AI to enhance the communication around track and trace of parcels. During the internship a Minimal Viable Product (MVP) is created to showcase the value of using generative AI technologies, to enhance parcel tracking, analyzing the parcel's journey and being able to communicate about it in an easy to understand manner. The primary goal was to develop an in-house LLM-based system, reducing dependency on external platforms and establishing the feasibility of a dedicated generative AI team within the company. This multi-agent LLM based system aimed to construct parcel journey stories and identify logistical disruptions with heightened efficiency and accuracy. The research involved deploying a sophisticated AI-driven communication system, employing Retrieval-Augmented Generation (RAG) for enhanced response precision, and optimizing large language models (LLMs) tailored to domain specific tasks.\nThe MVP successfully implemented a multi-agent open-source LLM system, called SuperTracy. SuperTracy is capable of autonomously managing a broad spectrum of user inquiries and improving internal knowledge handling. Results and evaluation demonstrated technological innovation and feasibility, notably in communication about the track and trace of a parcel, which exceeded initial expectations. These advancements highlight the potential of AI-driven solutions in logistics, suggesting many opportunities for further refinement and broader implementation within PostNL's operational framework.", "sections": [{"title": "1 Introduction", "content": "The developments in the field of generative AI has brought a lot of opportunities for companies. For instance, the logistics and postal sector has seen significant improvements in efficiency of customer service through the integration of AI technologies. As one of the leading postal and logistics companies in the Netherlands, PostNL has embraced these advancements to enhance the internal and external communication around track and trace of parcels. This thesis explores the development and implementation of a generative AI-based multi-agent large language model system designed to facilitate parcel-related inquiries at PostNL. By leveraging the capabilities of generative AI, this system aims to streamline communication, improve customer satisfaction and to help internal agents to understand the journey of the parcel in an easy manner."}, {"title": "1.1 PostNL", "content": "PostNL is the biggest mail, parcel and E-commerce corporation of the Netherlands, with operations among the Benelux, Germany, Italy and the United Kingdom. It has a rich history of 225 years in The Netherlands. On an average weekday, 1.1 million parcels and 6.9 million letters are delivered throughout the Netherlands by PostNL. This huge amount is achieved by the existence of 37 sorting centers, 11.000 letter boxes, 903 Automated Parcel Lockers 5.795 retail locations and approximately 33.500 employees [1].\nThe business of PostNL is high-over divided in 3 sections: parcels, mail and Cross Border Solutions (CBS). The scope of this research is parcels and E-commerce. The end-to-end process of parcel delivery can be quite complex, but very efficient at the same time. The key elements in the value chain are the following [1]:\nCollect In this first step the parcels are collected from the customers. In this process, the expectations of the customer have to be matched by timely pick-up and processing.\nSort The second step is sorting and processing the parcels, based on destination and specific customer and consumer needs. An efficient sorting helps to ensure delivery to the right location on time.\nDeliver The final step is delivery, which is the moment of connection within the sender and receiver, and the final delivery of the parcel. As the delivery men are in each street in The Netherlands everyday, this gives space for additional societal services, like identifying loneliness in households and working together with charity organizations.\nThe execution of these vital steps are heavily influenced by technological developments. The digital transformation that PostNL is going through is an important initiative to stay on top of these innovations [1]. Digitalisation helps in developing the core activities to provide smart E-commerce solutions to improve its competitive position. Logistics has developed from being a pure service"}, {"title": "1.2 Problem Statement", "content": "The value chain of parcel delivery is described in the section above in a high-over and abstract way. However, the value chain is more complicated, depending on variables like customer and consumer preferences, the size of the parcel, the time of the year, the volume of parcels, which PostNL services are being executed and more. Throughout this process, logistic events are registered, also called 'waarnemingen' in Dutch. These logistic events represent diverse situations that can appear based on the variables. The logistic events are a code starting with a letter followed by 2 numbers. There are 400 unique logistic events. These individual events together form logistic event sequences, which describe the journey of the parcel from the moment of acceptance in the PostNL network up to delivery. There is a lot of variation possible in these sequences, resulting in hundreds to thousand different sequence possibilities. For instance, a package with a specific barcode during its journey can exhibit the following sequence of 'waarnemingen':\n[A01, A98, A95, B01, G03, V06, A04, K50, B01, A96, J01, J40,\nA19, J05, A19, H01, J30, B01, J17, B01, J01, J01, J40, A19, A19, J05, I01]\nEach code in the sequence above has a descriptive meaning and a contextual explanation. One challenge is the diverse combination of codes in sequences, where the presence or absence of a code can be due to a mistake in the logistical operational process.\nWithin PostNL there are logistic business experts who can interpret these sequences and explain it to others. They know the context of the operational process behind the events. Certain combinations of logistic events in sequences can imply implicit knowledge that can only be understood and explained by these experts. They also know the full meaning of a logistic event that might not be well documented. These experts use an internal system called 'Tracy' where for the barcode of each parcel, you can look up the logistic events, along with description of the event, timestamps, where they happen (locations), source-system, and some customer and consumer information. For external communication with consumers on the status of the parcel, a small selection of these events are shared through the PostNL app notifications, through email or through the track-and-trace page on the PostNL website.\nPostNL wants to see if Gen-AI can be used to make sense of these complex logistic event sequences in a easy to understand manner. You could say an elevated version of Tracy, with the name 'SuperTracy'. Such a system could be used internally for business use-cases, or externally for consumer facing communication about the status of the parcel."}, {"title": "1.3 Research Goal", "content": "The goal of this research is to explore the potential for forming a dedicated approach towards generative AI research, development, and engineering at PostNL, by inspiring business stakeholders on its potential. There are several sub-goals that contribute to this:\n1. Exploring valuable business use-cases solved by Gen-AI: In order to get dedication and interest of business stakeholders, it has to be shown that there are valuable use-cases that can be solved through Gen-AI. One of the use-cases is to improve the efficiency and accuracy of communication around parcel track and trace within the logistical ecosystem of PostNL through SuperTracy.\n2. Using in-house solutions: The MVP of SuperTracy is aimed to establish the viability of developing an entirely new in-house generative generative AI based system, eliminating reliance on external AI platforms like Chat GPT API's or Amazon Bedrock services. Doing so will decrease the costs and make the usage of Generative AI more approachable.\n3. Creating a MVP for the business stakeholders: Showing the value of a Gen-AI based solution to a business problem, requires a MVP. This unfolds in having a technical sound product, but also an evaluation to show the value. The evaluation has to show that SuperTracy can mimic parcel experts at least, or even enhance the understanding of parcel journeys and effectively identify and communicate issues in the logistics and delivery processes. The MVP should be suitable to show to business stakeholders to see the value."}, {"title": "2 Literature Study", "content": null}, {"title": "2.1 Generative AI and ChatGPT", "content": "Generative AI refers to a category of artificial intelligence algorithms that can generate new data or content that is similar to the data it was trained on. Unlike traditional AI, which typically focuses on identifying patterns and making decisions based on existing data, generative AI can create new, original content, such as text, images, music, and even code. This capability is powered by models such as Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and transformer-based models like GPT (Generative Pre-trained Transformer). [2].\nChatGPT is a notable example of a transformer-based generative AI model developed by OpenAI, which is specifically designed for conversational tasks. It has attracted worldwide attention for its capability of dealing with challenging language understanding and generation tasks in the form of conversations [3] [4].\nThis has also inspired businesses to make use ChatGPT AI systems, as it can have various efficiency gains. ChatGPT can automate various business tasks such as content creation, customer service, and data analysis, leading to improved productivity and cost savings. Also the model's ability to understand natural language and provide human-like responses can enhance customer engagement and satisfaction [5]."}, {"title": "2.2 Transformer Architectures and Large Language Models", "content": "The way ChatGPT works is through sophisticated natural language processing (NLP) techniques within massive computational infrastructures that result in the fluent human like responses. The workings of this rely on neural transformer models and Large Language Models (LLMs). Transformer based models are excellent at processing longer sequences of data-like text-by using self-attention processes that enable the model to focus on different areas of the input by learning long-range dependencies in text. Self-attention is a mechanism that allows the model to learn the importance of each word in the input sequence, regardless of its position [6].\nLLMs are constructed using the transformer architecture. LLMs are a type of AI model trained on massive amounts of text data to understand and generate human-like language. They have a large number of parameters, often in the billions, which enable them to capture intricate patterns in language. This scaling up has allowed LLMs to understand and generate text at a level comparable to humans [4].\nSo transformers help the system focus on the important parts of the input text, understanding the context and meaning. LLMs use this understanding to predict and generate the next word in a sentence, making the conversation flow naturally."}, {"title": "2.3 Open-sourced and closed-sourced LLMs", "content": "ChatGPT is a closed-source general purpose-chatbot. The closed source LLMS are also called 'commercial' or 'Proprietary' LLMs. In general, closed-source LLM models perform well across diverse tasks, but they fail to capture in-depth domain-specific knowledge [7]. The same holds for other closed-source LLM models like other OpenAI GPT model families or Claude [8]. The usage of LLMs for specific use-cases can sometimes seem unattainable, due to the lack of transparency, high cost and energy consumption, usage limits, and adherence to terms of service. The recent emergence of highly capable open-source LLMs such as LLAMA 3, T5, MADLAD, and GEMMA 2 allow researchers and practitioners at large to easily obtain, customize, and deploy LLMs in more diverse environments and for more diverse and specific use cases [9]."}, {"title": "2.4 Making LLMs suitable for specific tasks through fine-tuning", "content": "There is a irresistible necessity from enterprises for fine-tuning LLMs to get them trained on proprietary domain knowledge. Fine-tuning is the process of continuing the training of an already pre-trained model on a new dataset that is typically smaller and task-specific [10]. This allows the model to adjust its weights and parameters to better fit the nuances of the new data and the specific requirements of the target task. Though there is an option to use OpenAI (open-source) models to solve most of the use-cases, there is a high demand for domain specific LLMs due to data privacy and pricing concerns as mentioned earlier."}, {"title": "2.5 Making LLMS suitable for specific tasks through Retrieval Augmented Generation", "content": "When a LLM model that is not fine-tuned is used for domain specific tasks and asked to handle queries beyond its training data or current information, hallucinations can happen [11]. As fine-tuning can be done to make LLMs suitable for specific use-cases, another approach next to or instead of fine-tuning can be to use Retrieval-Augmented Generation (RAG) architecture. RAG enhances LLMs by retrieving relevant document chunks (in real time) from external knowledge bases through semantic similarity calculation [12]. So RAG retrieves additional data, and augments it to the existing knowledge of the LLM based on semantic similarity. In fine-tuning, the weights of the existing parameters of the LLM get adjusted to the learned knowledge, but vectors are not added. Therefore to keep the knowledge base updated, fine-tuning would be computationally expensive."}, {"title": "2.6 Enhancing the input of an LLM-based systems through prompt engineering", "content": "The input that is given to an LLM is also important for the desired output. That input is also called a Prompt. Prompt engineering is the process of designing and refining input queries, or \u201cprompts,\u201d to elicit desired responses from LLMs [13]. In the literature, prompt engineering is usually explained in two different context. Prompt engineering can apply to how a user of an LLM based system can phrase its desired task to the model the best way [14] [15], or prompt engineering can refer to the effective way of responding of the LLM-based system, which can be determined by the developer [16]. In the latter case, various prompt engineering techniques are available to guide the model effectively. Well known methods are few-shot prompting, chain-of-thought prompting and self-consistency [16]."}, {"title": "2.7 Enhancing the performance of LLM-based systems through Quantization", "content": "The performance of open-source models depend on the hardware and the available computational resources that are being used. Significant challenges can be faced when attempting to leverage the full potential of transformer models in cases where memory or computational resources are limited. Because the advancements in transformer performance are accompanied by a corresponding increase in model size and computational costs [17]. Floating-point post-training quantization techniques can be used to enable the compression of transformers to face the challenges of limited computational resources. [18]. This approach"}, {"title": "2.8 Logistic event prediction through sequence to sequence prediction by T5", "content": "In the previous sections, the transformer architecture and LLMS like GPT-3 and GEMMA have been discussed. These models have demonstrated remarkable capabilities in various NLP tasks due to their ability to understand and generate human-like text. Transformer-based models are very versatile and diverse, making each of them suitable for different tasks. T5 (Text-to-Text-Transfer Transformer), developed by Google, is a versatile language model that is trained in a \"text-to-text\" framework [20]. The key innovation of T5 is the formulation of all tasks as text generation problems. This means that every task, including text classification, summarizing, translation, and question answering, is cast into a text-to-text format. For example, instead of training T5 to answer questions directly, it is trained to generate the complete answer given the question and relevant context. [21]\nFor the prediction of future logistic events in the sequence as shown in the Problem Statement, The T5 can be used. Transformers are highly suitable for this task due to their ability to handle sequential data and capture long-range dependencies through self-attention mechanisms [22]. The T5 model excels in sequence prediction tasks and can be fine-tuned on specific datasets to improve accuracy [23]. Mathematically, the transformer architecture uses an encoder-decoder structure where both components utilize self-attention and feed-forward neural networks. The self-attention mechanism computes representations for each element in the sequence by considering the entire sequence context, enhancing the model's capability to predict the most likely sequence of logistic event codes. This mechanism can be described by the attention function, which maps a query and a set of key-value pairs to an output, computed as a weighted sum of the values, where the weights are derived from the query and corresponding key [22]."}, {"title": "2.9 LLMs and Multi-Agent systems", "content": "So far various methods to make LLMs suitable for specific tasks have been discussed, such as fine-tuning and prompt engineering. As intelligent agents also focus on specific tasks, researchers have started to leverage LLMs to construct AI agents [24]. LLMs can be employed as the brain or controller of these agents and expand their perceptual and action space. These LLM based agents can exhibit reasoning and planning abilities through earlier discussed prompt engineering techniques like Chain-of-Thought.\nBased on the capabilities of the single LLM based agent, LLM-based Multi-Agents have been proposed to leverage the collective intelligence and specialized"}, {"title": "3 The Solution", "content": null}, {"title": "3.1 Data and knowledge Discovery", "content": "To make LLMs suitable for domain specific tasks, the models have to be fine-tuned on relevant PostNL data. Several interviews have been done with experts of logistic events and data warehouse engineers to identify the existing datasets and to understand the operational process behind the data. Also the workings and background data of Tracy have been investigated. This resulted in the selection of appropriate datasets to build the solution with:\nCollo data: The 'Collo' dataset is a well known dataset in PostNL. It contains all the barcodes of parcels, along with all the logistic events that are registered throughout the journey of the parcel, from the moment of acceptance in the network up to delivery. This is an extensive data set, with 159 columns, with each row representing an event or the current state of a parcel.\nAbbreviations: Abbreviations are used a lot in PostNL, resulting in PostNL specific terminology which is widely used in documentation and other data. This dataset has a collection of abbreviations, along with a description and explanation. These abbreviations are also widely used throughout Collo columns and data entries.\nWaarnemingen: This data set contains all the 400 unique 'waarnemingen' or logistic events, along with a description of what each mean. Each Waarneming code of a parcel forms a row in Collo. Additionally, each code falls into either the internal or external category. The internal category indicates that this event is only visible for PostNL employees through Tracy for example. The external category indicates that the logistic event is being shared externally with customers. For example through the PostNL app or email.\nLocation data: The location master data set contains all the locations that PostNL does business on. Examples are warehouses, sorting centres, distributions centres, hubs and more.\nFor this project access to Tracy was provided. Tracy is the web-application containing all the data on parcels in real-time. Tracy ingests Collo data. Tracy has been used throughout the process to lookup some barcodes for tests. The combination of the mentioned data sources allowed for a meaningful interpretation of the main dataset, Collo."}, {"title": "3.2 Data Preparation", "content": "The datasets procured from the data warehouse were initially in a raw format, necessitating extensive data preparation prior to further analysis. The initial phase involved Exploratory Data Analysis (EDA), through which a comprehensive understanding of the dataset was developed. This was followed by the data preparation phase, outlined as follows:\nData Cleaning: This process addressed issues such as missing values and duplication of data points to ensure the integrity of the dataset.\nStatistical Analysis: Statistical summaries were performed to examine the distribution, mean, median, mode, and variance of the data. This analysis was critical for understanding the underlying patterns and anomalies within the dataset.\nData Transformation:\n\u2022 Language Standardization: Translated 132 columns of logistic parcel data from Dutch to English to establish a uniform language baseline essential for subsequent project implementation.\n\u2022 Normalization and Cleansing: This step involved both normalizing and cleansing the textual data to enhance its suitability for analysis. Normalization tasks included case conversion to minimize case sensitivity issues and tokenization to structure the text into usable segments. Concurrently, the data was cleansed by removing punctuation and special characters to prevent potential data processing errors. These processes together ensured that the textual data was not only uniform but also clean and optimized for subsequent analytical tasks.\nData Splitting: The dataset was segmented into training, validation, and test sets to support the development of robust predictive models.\nThese preparatory steps were instrumental in ensuring that the data was aptly conditioned for the sophisticated analyses and modeling that followed."}, {"title": "3.3 Model design", "content": "Expected model output The first step of design thinking [26] is to understand the problem, and then have clear what the concept of the solution can be. So to understand what kind of output is expected. This was achieved by discussions with domain experts, creating a shared agreement on the possible expected outcomes of the final system. The agreed goal of the system is to simulate the comprehension and narration abilities of a logistic domain experts at PostNL, who answers in a user friendly and helpful manner.\nOverall Model design To achieve the goals stated in section 1.3, a solution is proposed building upon the findings of the literature study in section 2. The final solution is SuperTracy, a multi-agent LLM based system, leveraging open-source LLM models like GEMMA 2 and LLAMA 3 to make sure the company data is"}, {"title": "3.5 Architectural design", "content": "The multi-agent setup The main idea of having this multi-agent LLM-based system revolves around domain-specific knowledge agents characterized by role-based agents and system template prompt engineering. This approach determines the main goals of each specific agent and constrains their behavior based on specified requirements and the state of the environment. In this context, the environment is the SuperTracy interactive system, encompassing user interactions and the contextual states of the conversation. There are three agents defined:\n1. Reception Agent: The Reception Agent handles basic communication. It introduces itself to the user and provides guidance on using the system, including instructions to provide the parcel's barcode. This agent ensures a smooth initial interaction and prepares the user for further engagement with the system.\n2. Parcel Agent: The Parcel Agent is responsible for analyzing parcel data and generating detailed narratives of parcel journeys. These narratives can range from detailed reports to short, coherent answers, depending on user needs. This agent includes specialized sub-models, such as a predictive model for forecasting the future status of parcels. By customizing its responses, the Parcel Agent enhances the user's ability to track and understand parcel movements comprehensively.\n3. Knowledge Expert Agent: The Knowledge Expert Agent specializes in answering user questions related to internal PostNL concepts and domain-specific terms. This agent can handle queries ranging from simple explanations to complex scenarios. Its knowledge base is derived from PostNL's internal documents and general logistics knowledge. The agent's ability to reason and provide contextually accurate answers makes it a valuable resource for users seeking detailed information on PostNL operations.\nThe Reception agent and the Parcel agent contribute to the expected outcome of SuperTracy, that is to communicate on the parcel's track and trace journey. The Knowledge Expert agent is an additional bonus feature. The idea emerged beyond the scope of the main research question and the main requirements of the project which was only limited by the parcels track and trace. The knowledge expert agent is developed using the same approach as the Parcel agents, it is trained on internal PostNL documents. This enables it to answer"}, {"title": "1.  Template for cognitive behaviour of the agents", "content": null}, {"title": "4.  Template for generating the output", "content": null}, {"title": "RAG architecture", "content": "To address challenges and drawbacks of the fine-tuned model like hallucinations and performance issues, we leverage the power of RAG architecture and a vectorized database. RAG architecture combines retrieval and generative capabilities to enhance LLM responses. The architecture has two components: the retriever and the generator. The retriever fetches relevant documents or data segments from a large corpus, while the generator creates responses based on the retrieved information [34]. Vector databases store data in vector format for efficient similarity searches. Embedding models convert textual"}, {"title": "3.6 Overall System Architecture Overview", "content": "In the diagram below (Figure 2) you can see the architecture of the final system, where all the components come together that form SuperTracy. The architecture of the system is designed to ensure integration and effective collaboration among the agents and models. Key components include:\nLLAMA3 and GEMMA2: LLMs serving as the backbone for different agents, providing advanced language understanding and generation capabilities [30,38]."}, {"title": "Random Forests Classifier and Regex Pattern Matching", "content": "Models and methods for verifying the correctness of barcodes, enhancing the system's reliability.\nT5 model: This model is used for logistic event prediction of the parcel.\nPrompt Factory: Prompt factory is the module where prompt engineering takes place and various prompt templates are created and managed.\nThe integration of these components forms a cohesive system capable of addressing various user needs through the Reception, Parcel, and Knowledge Expert Agents. Each agent operates within its specialized domain, leveraging the strengths of the underlying models to provide accurate, relevant, and timely responses."}, {"title": "4 Evaluation and Discussion", "content": null}, {"title": "4.1 Technical Evaluation of the model", "content": "SuperTracy is a MVP which hasn't been deployed yet. Therefore, technical performance cannot be formally measured. Below are a few important implementation goals, that made the the finalization of this MVP possible. These implementation goals build further on the three research goals mentioned in section 1.3.\nLightweight Deployment: The system operates effectively on local machines with modest hardware. This has been achieved by using quantization and open source LLM models. For the deployment of the MVP of Supertracy, a MacBook Pro with chip M1 Max and 64 GB memory has been used. The reaction time of the system was always less than 2 second. Videos are taken of the performance.\nComplete Local Integration: All necessary modules and models used for SuperTracy are integrated and run entirely on the local system. This means that external API's like Open-AI or Bedrock API's are not used, which can be quite expensive on a large scale. Open source models like GEMMA 2 and LLAMA 3 have been used instead, which also ensure the company data stays on premise.\nIntegration with RAG Architecture: The integration of agents with RAG architecture, allows agents to work individually or together to perform diverse and complex tasks. Using RAG has made it possible to specialize the LLMs for specific tasks, therefore enabling the business use-case. Through RAG, agents are able to utilize embedded documents to enhance their knowledge base. RAG also enables the system to recall its reasoning and used resources in contrast to the closed-source external LLMs which act as a black box."}, {"title": "4.2 Human Evaluation of the generated output by SuperTracy", "content": "The research goals stated in section 1.3 state that a LLM-based system can be build in-house, and solve the business use-case of mimicking the role of a"}, {"title": "Experimental Setup of Human Evaluation", "content": "A panel of eight logistical domain experts of the supply chain team have been asked to critically review the output of SuperTracy based on the factual correctness of the generated parcel stories.\nA sample of 100 parcel barcodes have been selected, with the minimal requirements provided by the logistics expert to ensure that they represented complex logistical scenarios likely to challenge the system's capabilities. These requirements have been provided by mentioning which 'waarneming' codes indicate an unhappy journey flow. The sample of these barcodes containing the selected events have been given to the model to generate a parcel story for.\nThese experts were asked to assess the factual correctness and relevance of the stories on a scale ranged from 1, indicating a very low level of correctness and relevance, to 5, representing an exemplary level of performance in the parcel story generation. Participants were asked to provide an explanation for a score lower than 3. In that way feedback and insight into lower scores have been achieved. It was decided together with the business experts, that a score of 3 was good enough and anything below indicated a low quality of the generated stories."}, {"title": "4.3 Results of the evaluation of Domain Experts", "content": "Quantitative Results The results of the given scores by the domain experts on the generated parcel stories by SuperTracy are shown in the figure below. The"}, {"title": "Qualitative Feedback", "content": "For the scores of lower than 3, the domain experts have provided reasoning for their choice. The main gist of the gathered feedback from the open answers is about incorrect generalizations or incorrect assumptions of a few event codes. These are summarized in the following points:\nIncorrect assumptions and generalizations of some logistic events: For example in the logistic events 'ETA was updated' happens quite a few times at the end of a logistical sequence. This is interpreted as a delay by the model, but this is not necessarily a delay. Also according to the standard process of evening distribution 'Avonddistributie' the route gets rescheduled, as they are in a planned network. Or in the morning planners can shift parcels from routes to improve the total planning. This has nothing to do with 'delay' or 'unforeseen circumstances' as SuperTracy calls it in the parcel stories.\nSome steps are default in the process and are not interesting to show: Sometimes it is possible to change something. This is present in the"}, {"title": "4.4 Conclusion", "content": "The goal for this project was to create a MVP which can act as a show-case of the value that generative AI can bring for PostNL. Reflecting on the goals set at the beginning of the research in section 1.3, we can conclude that the goals have been achieved. The main goals were to explore business use-cases that can be solved by Generative AI and creating a MVP to showcase the value to the business. Throughout the demo and the evaluation of SuperTracy by domain experts, a lot of positive reactions are received. The use-case of SuperTracy has inspired the supply chain team to consider using such a system to improve their workflow, and for the business stakeholders to further explore possibilities of refining for deployment. The demo of SuperTracy has also inspired the stakeholders to realize the value of leveraging generative AI techniques, by brainstorming on various new use-cases during the demo. Also the goal to use in-house solutions when designing and creating the system was achieved, by using open source LLM models and using local computing power for running the system. To further prove the value of the use-case, the Human Evaluation confirms that SuperTracy is a successful MVP that can mimic the track and tracing capabilities at PostNL by effectively creating a story of the parcel's journey, receiving a score of higher than 3 in 75% of the cases. Interestingly, the feedback provided by the subject matter experts, were mostly about the quality of the input data, and not the LLM-based model itself. At the same time, PostNL is working on strengthening their data fundament, which will result in better data quality, which forms the base of a good performing LLM system.\nAll together, the creation of SuperTracy has been successful and contributed to the maturity of using generative AI solutions in PostNL, shedding light to its possibilities and business value, all done with the least amount of resources. This undermines the sometimes implicit assumption that adapting to generative AI technologies are expensive or out of reach."}, {"title": "4.5 Future Work", "content": "Future work depends on the scope, being the MVP or for scaling up through deployment. The MVP worked well, but could be improved. Future improvements to MVP of SuperTracy are refining the system's ability to identify and communicate only the most relevant information, ensuring clarity and precision in the narratives. This can be done by removing auto generated default events in cases for events that can always be neglected. If events are interesting in some cases, a knowledge graph can be designed, which allows inference to understand the difference. Also the evaluation could be more extensive on diverse aspect. Human evaluation or Automatic evaluation methods could be used for aspects like fluency or multilingual performance. The additional aspects of the model could also be further evaluated, such as the individual performance of the logistic event prediction based on the T5 model.\nThe broader scope can be the deployment of SuperTracy and further specifying its use-case. In the case of deployment, it is important to pay significant attention to data privacy and security, preventing oversharing sensitive information with the AI system, which are common challenges when leveraging LLMs in business. [5]. This requires awareness of designing and using AI solutions in an enterprise. Next to the secure deployment, maintaining and increasing the data quality of the used data sets to fine-tune the LLM models are very important. In the evaluation some data quality issues have already been pointed out which deserve further attention. After all, the execution of this MVP has shown that the used methodologies provide value for the use-case. These same methods, technologies and approach can be used for other use-cases too."}, {"title": "Multiple self-attention mechanisms (heads) capture different aspects of the relationships:", "content": null}, {"title": "Attention(Q, K, V)", "content": "Attention(Q, K, V) = softmax \\frac{QK^T}{\\sqrt{d_k}} V \\qquad (1)"}, {"title": "MultiHead(Q, K, V)", "content": "MultiHead(Q, K, V) = [head_1 ; head_2; . . . ; head_h] W_o \\qquad (2)"}, {"title": "FFN(x)", "content": "FFN(x) = max(0, xW_1 + b_1) W_2 + b_2 \\qquad (3)"}, {"title": "P(Xn+1 X1,X2,...,xn)", "content": "P(X_{n+1} | X_1, X_2,...,X_n) = softmax(h_n W_o + b_o) \\qquad (4)"}, {"title": "L(0)", "content": "L(0) = \\frac{1}{N}  \\sum_{i=1}^{N} L(y_i, \\hat{y_i}) \\qquad (5)"}, {"title": "W", "content": "W = round \\left( \\frac{W - min(W)}{\\triangle} \\right) . \\triangle + min(W) \\qquad (6)"}, {"title": "W", "content": "W = W_o + \\triangle W \\qquad (7)"}, {"title": "\u0394W", "content": "\\triangle W = AB \\qquad (8)"}, {"title": "sim(q, d)", "content": "sim(q, d) = \\frac{q \\cdot d}{\\|q\\| \\|d\\|} \\qquad (9)"}, {"title": "Aij", "content": "A_{ij} = \\frac{exp(e_{ij})}{\\sum_{k=1} exp(e_{ik})} \\qquad (10)"}]}