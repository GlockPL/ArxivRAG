{"title": "Humanlike Cognitive Patterns as Emergent\nPhenomena in Large Language Models", "authors": ["Zhisheng Tang", "Mayank Kejriwal"], "abstract": "Research on emergent patterns in Large Language Models (LLMs) has gained\nsignificant traction in both psychology and artificial intelligence, motivating the\nneed for a comprehensive review that offers a synthesis of this complex land-\nscape. In this article, we systematically review LLMs' capabilities across three\nimportant cognitive domains (decision-making biases, reasoning, and creativity),\nusing empirical studies drawing on established psychological tests and comparing\nLLMs' performance to human benchmarks. On decision-making, our synthesis\nreveals that while LLMs demonstrate several human-like biases, some biases\nobserved in humans are absent, indicating cognitive patterns that only partially\nalign with human decision-making. On reasoning, advanced LLMs like GPT-4\nexhibit deliberative reasoning akin to human System-2 thinking, while smaller\nmodels fall short of human-level performance. A distinct dichotomy emerges\nin creativity: while LLMs excel in language-based creative tasks, such as sto-\nrytelling, they struggle with divergent thinking tasks that require real-world\ncontext. Nonetheless, studies suggest that LLMs hold considerable potential as\ncollaborators, augmenting creativity in human-machine problem-solving settings.\nDiscussing key limitations, we also offer guidance for future research in areas such\nas memory, attention, and open-source model development.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in generative artificial intelligence (AI) and large language mod-\nels (LLMs) [1, 2] have sparked new interest in how AI might simulate or even influence\nhuman cognition and decision-making. Although the original purpose of language\nmodels was text generation and addressing longstanding problems in natural language\nprocessing (NLP) like machine translation and information extraction [3-7], a recent\nsuite of papers have argued that they are now exhibiting, or at the very least, mim-\nicking, complex reasoning abilities at human levels of performance [8-10]. Beyond\nreasoning and decision-making, LLMs released since the first edition of ChatGPT in\nearly 2023 have also been argued to mimic abilities like creativity (evidenced through\ntasks like poetry and lyrical generation [11]), although the originality of their creative\noutputs is a matter of debate.\nThese debates notwithstanding, the emergence of novel behaviors and properties in\nLLMs as an empirical phenomenon cannot be ignored [12]. Researchers are divided on\nthe causes and implications of such behaviors, with some arguing that they are largely\na mirage [13] and others arguing that true emergence is occurring [12]. By emergence\nhere, we simply mean that the model was not explicitly trained to mimic or learn\nsuch behaviors, either through the underlying neural network's objective function or\nthrough the data itself. Wei et al. [12] succinctly describe emergence in the context\nof LLMs as an ability that is not present in smaller models but is present in larger\nmodels.\nWhile emergence is much more complex in LLMs, it has some precedent in NLP\nresearch since 2010 that has increasingly relied upon deep neural networks. For exam-\nple, when the word2vec model first became popular more than a decade ago, the\nauthors of the original paper noted how the word vector representations yielded by the\nskip-gram neural network model (when it was fed reasonably large corpora in an unsu-\npervised fashion) obeyed the now-classic analogy king \u2013 m\u1ea3n + woman = queen [14].\nSimilarly, shortly following the release of Bidirectional Encoder Representations from\nTransformers (BERT) [15], one of the first transformer-based language models, a line\nof work colloquially referred to as BERTology [16] rapidly emerged in less than five\nyears, showcasing empirical phenomenon of a largely emergent nature. For example,\nBERT's neural layers were found to exhibit hierarchical representations of language\n(despite not being explicitly trained to do so), with earlier layers containing informa-\ntion about linear word order, middle layers carrying syntactic information, final layers\nholding task-specific knowledge, and semantics spreading across all layers.\nGiven these expanded and unexpected capabilities, an intriguing question arises:\ndo LLMs, which are trained on vast amounts of human-generated text, also exhibit\ncognitive patterns that are typically shown in humans? Specifically, do they exhibit\nthe cognitive heuristics and biases that characterize human decision-making? Do they\nshare the same kinds of reasoning patterns and levels of reasoning capabilities as\nhumans? Can they innovate in ways that resemble human creativity? To answer these\nquestions, it is helpful to first understand what these cognitive patterns are, and the\nrelationship between them.\nCognitive scientists and psychologists have long studied human decision-making\nbiases, such as hindsight bias, overweighting, and belief bias. Although often reducing"}, {"title": "2 Decision-Making Cognitive Patterns", "content": "While there is much literature studying different aspects of human decision-making,\nsuch as: cognitive heuristics and biases [28], dual-process theory [29], social influence\nand group decision-making [30], and neuroeconomics and computational models of\ndecision-making [31], current research on LLMs' decision-making behavior has mainly\nfocused on cognitive heuristics and biases. To understand the relevance of these studies,"}, {"title": "2.1 Heuristics and biases", "content": "it is helpful to first consider the foundational role of heuristics and biases in human\ndecision-making.\nMental shortcuts, or heuristics, reduce the cognitive load of intensive processes\n[17]. Notable heuristics include the availability heuristic, representativeness heuristic,\nand the anchoring heuristic. The availability heuristic is our inclination to rely on\ninformation that comes to our mind quickly and easily when evaluating decisions [32].\nFor example, people might judge the probability of a rare event, such as a plane crash,\nto be higher than it actually is if they have recently seen such an event on the news.\nAlthough the availability heuristic helps make decisions quicker, it also leads to an\noverestimation of unlikely but memorable events.\nThe representativeness heuristic involves assessing probabilities based on the sim-\nilarity between a sample and a larger population [33]. One famous example is when\npeople stereotypically deemed a young woman, described as deeply concerned with\nsocial justice, intelligent, and outspoken, to be more likely to be a bank teller and\nalso active in the feminist movement, than to just be a bank teller alone (which would\nbe inconsistent with rational rules of probability). The heuristic simplifies decision-\nmaking by relying on stereotypes but often misleads people to ignore general statistics.\nThe anchoring heuristic is yet another commonly known mental shortcut. It occurs\nwhen the initial exposure to a reference point influences subsequent decisions [34]. It\nis explored extensively in the science of framing sales as discounts. For example, if\nthe original price of a commodity serves as an anchor, the discounted price suddenly\nbecomes attractive, even though it may (still) be unreasonably high."}, {"title": "2.3 Conclusion", "content": "In summary, modern LLMs demonstrate at least thirteen different human-like heuris-\ntics and biases: frame effect, certainty effect, overweighting bias, decoy effect, belief\neffect, risk aversion, preference for certainty in subjective tasks, heuristic reasoning\nin objective tasks, demand-chasing, loss-aversion, anchoring effect, representativeness\nand availability effect, and endowment effect. However, they were also found not\nto exhibit some other common (in humans) biases, such as reflection effect, isola-\ntion effect, magnitude perception, waste aversion, stockout aversion, underestimated\nopportunity cost, and minimized ex-post inventory errors. Instruction-tuned LLMs\nshow improved performance but also heightened biases, suggesting that the process of\ninstruction fine-tuning may induce increased bias. In more complex decision-making\nscenarios like the newsvendor problem or operation management, GPT-4 exhibits a\nblend of human-like biases while also demonstrating rational responses aligned with\neconomic incentives. Interestingly, while LLMs outperform humans in standard deci-\nsion tasks related to rationality, their performance decreases under less conventional\nconditions, suggesting a dependency on familiar data presentations.\nBeyond heuristics and biases, sequential decision-making has also become a recent\npopular subject of LLM research. LLM architectures have inherent advantages in\nhandling sequential information. Unfortunately, most studies in this area use artificial\ndatasets and tasks, and lack human performance as a direct reference for comparison\n[45]. Additionally, experiments in many of these papers are not hypothesis-driven\nand cannot be compared with the outcomes of traditional, more rigorously designed\npsychological experiments. For further details, we refer the interested reader to Yang\net al. [45]."}, {"title": "3 Reasoning Cognitive Patterns", "content": "Reasoning may be broadly defined as the process of drawing conclusions based on a\ncombination of axiomatic principles and evidence [55, 56]. Reasoning allows us to move\nfrom what is already known to making new inferences and incorporating them into\nour knowledge base, as well as to evaluate proposed hypotheses. In a decision-making\nframework, we interpret reasoning as the ability to make judgments and choices by\nintegrating salient types of information, weighing evidence, considering alternatives,\nand predicting potential outcomes. Reasoning may be divided into three types: deduc-\ntive, inductive, and abductive. Each type is useful in different kinds of contexts and\nhence operates according to different principles.\nDeductive reasoning is the process of using logic to draw conclusions from given\nobservations [57]. For example, consider the classic example: (1) All humans are mor-\ntal; (2) Socrates is a human; therefore, (3) Socrates is mortal. The conclusion (3) is\nvalid because it is logically derived from the two premises given. As a result, deduc-\ntive reasoning is the predominant type of reasoning found in areas like mathematical\ntheorem proving, which require logical rigor."}, {"title": "3.1 Reasoning in humans", "content": "Inductive reasoning is the process of reasoning from specific facts or observations\nto reach a likely conclusion (or 'theory') that satisfactorily explains the facts and\nuses it in an attempt to predict future instances [58]. Unlike deduction, which gives\ncertainty when provided with true premises, inductive reasoning cannot definitively\n'prove' its conclusion from premises or evidence; any conclusions must instead be\ninterpreted probabilistically (e.g., as dictated by statistical significance analysis). It is\nmore prevalent in empirical science, where researchers gather data through observation\nor experimentation to formulate theories that explain the observed phenomena. A\nwell-known example is: after observing many white swans in different locations, one\nmight conclude that all swans are white. The conclusion is inferred from the repeated\nobservation of a phenomenon (although a wrong one). This reflects the fundamental\nprinciple of inductive reasoning, which can only allow us to draw a conclusion within\nlimited bounds of certainty, and always carries the caveat that further evidence could\nreveal exceptions.\nAbductive reasoning aims to recover the 'best', usually interpreted as the most\nplausible, explanation given a set of observations [59]. For example, if a doctor observes\na patient with symptoms such as fever, cough, and fatigue, they can use abductive\nreasoning to conclude that the patient may have the flu, as this would seem to be the\nthe most plausible explanation for the observed symptoms. While lacking the same\ndegree of formalism and traditional philosophical inquiry as inductive and deductive\nreasoning, it is widely applicable in everyday settings, including humans' ability to\ninfer plausible causes by generalizing from sparse data.\nAlthough each of these types of reasoning can be further subdivided, and the three\ntypes do not constitute a strict categorization of reasoning in humans by any means\n(e.g., other types of reasoning, like deontic reasoning and common sense reasoning,\nhave also been widely studied [60, 61]), most studies on LLMs tend to involve one or\nthe other."}, {"title": "3.2 Evaluating reasoning in LLMs", "content": "summarizes the performance of LLMS, compared with humans, on reason-\ning tasks. On deductive reasoning, Seals and Shalin [46] evaluated the competence\nof various LLMs, including Guanaco [62], MPT [63], BLOOM [64], and Falcon [65],\nusing the Wason selection task [66]. They found that when the task is presented in\nits conventional form, the LLMs show limited performance. When they changed the\npresentation format, the LLMs did not show significant improvement and exhibited\nunique reasoning biases that differed from humans. Ando et al. [47] evaluated three\nLLMS (ROBERTa [67], BART [68], and GPT-3.5) with a focus on human-like biases\n(i.e., belief biases, conversion errors, and atmosphere effects) in syllogistic reason-\ning [69] using a dataset derived from BAROCO [70]. They found that these models\nstruggled with such problems, and errors caused by various human-like biases heavily\ninfluenced their performance.\nIn contrast, Hagendorff et al. [48] explored the differences between intuitive (Sys-\ntem 1) and deliberative (System 2) reasoning [71] in LLMs using the Cognitive\nReflection Test [72] and a semantic illusions task [73]. They observed that as LLMs\nget larger and their task comprehension improves, they respond more intuitively,\nresembling System 1 processing. However, with GPT-3.5 and GPT-4, there was a sig-\nnificant shift towards more deliberative, System 2 like processing. This shift enabled\nthe models to better avoid semantic traps and perform well in cognitive tasks, even\nwithout relying explicitly on LLM-specific modalities like chain-of-thought reasoning\n[74], although such reasoning often appeared in their responses.\nOn inductive reasoning, Han et al. [50] examined the competence of GPT-3\nand GPT-4 using a curated category-based induction task inspired by [49]. Their\nresearch showed that while GPT-3 faces significant challenges in this area, perform-\ning poorly overall and reasoning in a qualitatively different way from humans, GPT-4\ndemonstrates considerable improvement, achieving performance comparable to that"}, {"title": "3.3 Conclusion", "content": "LLMs exhibit different performance patterns depending on the type of reasoning\ninvolved. On deductive reasoning, LLMs struggle with tasks such as the Wason selec-\ntion task and syllogistic reasoning, often displaying reasoning biases that are different\nfrom those shown by humans. However, the more advanced LLMs like GPT-3.5 and\nGPT-4 show some improvements, particularly in engaging in more deliberative reason-\ning processes that are similar to humans' system 2 processing. On inductive reasoning,\nearlier LLMs like GPT-3 faced significant challenges, especially on property induc-\ntion, but newer models like GPT-4 shows performance almost at par with humans.\nOn analogical reasoning, most LLMs demonstrate strong capabilities, sometimes even\nsurpassing human performance. However, reasoning biases remain a challenge, with\nLLMs often giving heuristic responses. GPT-4, however, shows signs of surpassing\nthese limitations, delivering super-human performance. Lastly, on abductive reason-\ning, LLMs demonstrate human-like content effects, and are found to struggle with\nabstract or counter-intuitive scenarios."}, {"title": "4 Creativity Cognitive Patterns", "content": "Guilford [96] describes a creative pattern as one that \"is manifest in creative behav-\nior, which includes such activities as inventing, designing, contriving, composing, and\nplanning. People who exhibit these types of behavior to a marked degree are rec-\nognized as being creative.\" Creativity, in short, is the ability to produce something\nthat is both original and worthwhile [97, 98]. Outcomes of creative pursuits include\ninventions, discoveries, and artwork. Within decision-making frameworks, creativity is\ntouted as the ability to devise novel alternatives and innovative solutions to problems.\nOne of the ways in which it is measured in humans is divergent production [96] (more\ncommonly known as 'divergent thinking' today), which is the generation of diverse\nresponses when presented with a stimulus. Two prevalent tasks are the Alternative\nUses Test (AUT) [99] and the Torrance Test of Creative Thinking (TTCT) [100].\nIn AUT, participants are tasked with the generation of a diverse array of potential\nuses for commonplace objects, such as bricks or paper clips. The responses are often"}, {"title": "4.1 Human creativity", "content": "evaluated using criteria like originality (the uniqueness of ideas), utility (the practi-\ncality of ideas), and surprise (the unpredictability of the ideas). The TTCT comprises\ntwo parts: verbal and figural. The verbal component solicits ideas, hypotheses, or solu-\ntions from participants using a picture as the stimulus. The responses are assessed on\nfluency, flexibility, and originality. The figural component asks the participant to com-\nplete partially completed shapes or figures. This component evaluates participants'\nfluency, originality, elaboration, abstractness of titles, resistance to premature closure\nabilities, and the checklists of creativity strengths."}, {"title": "4.2 Creativity in LLMs", "content": "Because of their wide application and success in evaluating human creativity, these two\ntests have also been employed to measure the creative capabilities of LLMs. Stevenson\net al. [89] applied AUT to assess GPT-3's performance and found that human par-\nticipants scored higher on both originality and surprise, as well as semantic distance,\ncompared to GPT-3. However, GPT-3 received higher utility ratings. Additionally, a\nnegative association between originality and utility was observed in both human and\nmodel responses.\nSimilarly, Hubert et al. [90] used the AUT task to compare GPT-4's creativity\nwith that of humans and found that it generated more diverse responses and displayed\nmore elaboration than the human counterpart, exhibiting higher originality when using\nspecific prompts. Additionally, Yiu et al. [91] proposed a novel task that required\naccomplishing a goal without the typical tool. Their findings revealed that while LLMs,\nincluding GPT-3.5 and text-davinci-003, can nearly match humans in recognizing\nsuperficial similarities between objects, they significantly lag behind both adults and\nchildren when they are asked to choose an unfamiliar tool to solve a problem and often\ndefault to conventional solutions rather than novel choices.\nFocusing on creative writing, Hubert et al. [90] used the Consequences Task in\nTTCT and the Divergent Associations Task [101] to compare GPT-4's creativity with\nthat of humans. They revealed that GPT-4 demonstrated greater originality and elab-\noration than humans across these tasks, even when fluency of responses was controlled\nfor. Extending to TTCT, Chakrabarty et al. [92] proposed the Torrance Test of Cre-\native Writing (TTCW) to evaluate the creative writing abilities of three LLMs (i.e.,\nGPT-3.5, GPT-4, and Claude-v1.3). Their study found that stories generated by these\nLLMs were significantly less likely to pass individual TTCW tests compared to those\nwritten by human experts. Additionally, Orwig et al. [93] evaluated two LLMs' (GPT-\n3 and GPT-4) ability to write creative short stories using the five-sentence creative\nstory task, where the participants are given a three-word prompt and asked to include\nall three words when writing a short story in approximately five sentences. They find\nthat both LLMs can generate stories that are comparable in creativity to those pro-\nduced by humans. Interestingly, they also found that GPT-4 was notably consistent\nin aligning its creativity ratings with those of human evaluators.\nAnother interesting area of research is to explore how LLMs can assist humans in\ncreative contexts. Lee and Chung [94] evaluated GPT-3.5's ability to assist humans\nin accomplishing five creative tasks, including choosing a creative gift for a teenager,\nmaking a toy, re-purposing unused items, designing an innovative dining table, and"}, {"title": "4.3 Conclusion", "content": "While LLMs like GPT-3 and GPT-3.5 exhibit utility in responses, they often fall short\nin originality and novelty compared to human creativity in tasks requiring innovative\nproblem-solving. However, more advanced LLMs, such as GPT-4, do tend to give\nmore original and novel solutions than humans. On the other hand, findings on LLMs'\ncreative writing ability present a more nuanced picture and highlight the need for\nfurther investigation. Promisingly, LLMs have shown great potential in collaborative\ncontexts by enhancing human creativity through generation of incrementally new ideas\nand competence on domain-specific problems and metrics."}, {"title": "5 Discussion", "content": "Based on these findings, there is promising evidence in favor of LLMs exhibiting all\nthree processes of decision-making, reasoning, and creativity, as emergent patterns.\nHowever, the studies also highlight some key limitations. On decision-making, LLMs\ndemonstrate 13 different humanlike heuristics and biases, including frame effect, risk\naversion, and anchoring effect. This emergence is likely attributed to the extensive\nhuman corpus on which they are trained, which reflects (either explicitly or implicitly)\nhuman biases. One reason to be mindful of these biases is that they can lead to unde-\nsirable outcomes when deploying them in real-world applications, warranting stronger\ntesting, fail-safes, and ethical reviews. Intriguingly, the absence of certain biases (such\nas waste aversion [102]) may also yield valuable insights, as they offer insightful com-\nmentary, not just of the LLMs' training and underlying neural modeling, but also\nof the human corpus itself. More practically, a better understanding of how some of\nthese biases are less present in the LLMs than we would expect from human popula-\ntion experiments may help researchers develop pre-training and fine-tuning protocols\nto proactively exclude these biases from the models if so desired.\nThe evaluation of reasoning presents a more nuanced picture, mainly because rea-\nsoning can be defined in broad ways and involves a wide variety of modalities and\ncognitive processes, even in humans. On classic deductive reasoning tasks, earlier LLMs\nstruggled, but the performance of the latest models has improved drastically, including\non tasks known to be cognitively intensive or even challenging for humans. This shift"}, {"title": "", "content": "toward more rigorous, System 2-like reasoning [71] in GPT-4 signals the trend of LLMs\nto mimic more deliberative reasoning. This could potentially be due to the increased\nuse of methodologies like Chain-of-Thought (CoT) being deployed in the prompting\nof these models. Inductive reasoning tasks appear to be easier for LLMs, achieving\neven super-human performance on analogical reasoning problems. Analogical reason-\ning relies heavily on identifying structural similarities between different concepts, and\nit is possible that LLMs naturally internalize these similarities by learning statistical\nassociations from massive training data. Nevertheless, intuitive and incorrect reason-\ning processes are still observed in LLMs. This finding, coupled with the phenomenon\nof hallucinations, which is when LLMs generate outputs that are factually incorrect\nbut still appear very confident in their responses, suggests caution when trusting their\nreasoning, especially in scenarios that are somewhat novel or implausible.\nCommonsense reasoning is yet another challenging aspect of reasoning for LLMs.\nUnlike deductive or analogical reasoning, commonsense reasoning requires LLMs to\nuse everyday knowledge to make plausible inferences in various scenarios. Current\nstudies on LLMs' commonsense reasoning abilities mainly focus on evaluating their\nperformances on artificial benchmarks, such as the CommonSenseQA [103] and the\nSocialIQa [104]. To truly assess the commonsense reasoning abilities of LLMs and\ncompare their performance with humans, researchers should look into tests that are\ninspired by classic human experiments, as recent pieces have argued [105, 106].\nThe studies that researched creativity collectively reveal a dichotomy: on the one\nhand, LLMs lack originality and novelty in divergent creativity tasks, where they are\nusually asked to come up with novel use cases of familiar objects. However, in creative\nwriting, especially on tasks designed to be relatively open-ended, LLMs like GPT-4\nshow enormous promise and, in many cases, can be prompted to produce stories that\nmatch human creativity [93]. This dichotomy can (at least partially) be explained by\na language bias: LLMs seem to excel in language-related creative tasks, whereas those\nthat are either multi-modal or abstract may require more (or different) advances. The\nlack of physical grounding, or embodied cognition, in LLMs has already been noted\nas an important limitation [107]. Lack of such cognition places obvious constraints on\nLLMs' ability to create innovative solutions for divergent thinking tasks that require\na deep and real-world (including physical) understanding of the objects given.\nSurprisingly, however, LLMs seem to excel in collaborative creativity. When used\nas creative assistants, they have been shown to supplement human effort by generating\nincrementally novel ideas and providing diverse perspectives. In practice, this makes\nthem prime candidates as augmented AI, in jobs ranging from report writing [108] to\nsoftware engineering [109], both of which are mainstay enterprise applications. It also\nsuggests an important research avenue in both organizational psychology and human-\ncomputer interaction, namely, to explore how LLMs can be most effectively integrated\ninto creative industrial processes to foster innovations more rapidly.\nTaken together, the studies reveal some important gaps in the current research\nlandscape. Cognitive phenomena like attention [110], memory [111], and representa-\ntion of knowledge [112], have not been as extensively studied as the cognitive processes\nthat we reviewed. These phenomena are foundational elements of human cognition"}, {"title": "", "content": "and clearly play a role in the three processes that we did cover. For this reason, study-\ning them as individual emergent phenomena in LLMs may have been more difficult.\nNevertheless, we maintain that, with the appropriate design of tests, it should be pos-\nsible to study these phenomena reductively in LLMs. For example, a single promising\nstudy focusing on memory [113] employed the n-back task [114] to evaluate GPT-3.5's\ncapacity for working memory. In another study, Jones et al. [115] evaluate knowledge\nof affordances in GPT-3. Given these possibilities and the currently limited exploration\nof these phenomena in LLM research, they offer promising avenues for future research.\nFinally, we found that the majority of experiments reported in the literature we\nreviewed tend to rely heavily on the GPT family of LLMs (which include GPT-3,\nGPT-3.5, and GPT-4). Although this is probably because of their easy access, other\nopen-source LLMs have not been as extensively tested, which could be a source of\nbias and non-replication down the line, because GPT models are not (at the time of\nwriting) open in any way. Even the training data used for these models is not publicly\ndocumented. Recently, some authors argued that, for scientific reasons, there should\nbe a stronger justification from researchers if they choose to eschew open models [116].\nMore open LLMs offer unique advantages because they are more transparent, offer\ngreater flexibility for customization, and allow researchers to conduct (and replicate)\nmore controlled experiments. At the same time, the GPT family cannot be ignored,\nowing to its widespread usage in the real world. Fortunately, the two are not in conflict.\nGiven the relative ease of accessing and prompting both open and closed models at\nthe present moment, and the rising number of papers in the AI community that are\nnow choosing to use at least 10+ models in reporting experimental results [74, 117-\n119], we advocate this as a recommended practice in the still-young community of\ncomputational psychology."}]}