{"title": "Intrinsic PAPR for Point-level 3D Scene Albedo and\nShading Editing", "authors": ["Alireza Moazeni", "Shichong Peng", "Ke Li"], "abstract": "Recent advancements in neural rendering have excelled at novel view synthesis\nfrom multi-view RGB images. However, they often lack the capability to edit the\nshading or colour of the scene at a detailed point-level, while ensuring consistency\nacross different viewpoints. In this work, we address the challenge of point-level\n3D scene albedo and shading editing from multi-view RGB images, focusing on\ndetailed editing at the point-level rather than at a part or global level. While prior\nworks based on volumetric representation such as NeRF struggle with achieving\n3D consistent editing at the point level, recent advancements in point-based neural\nrendering show promise in overcoming this challenge. We introduce \u201cIntrinsic\nPAPR\", a novel method based on the recent point-based neural rendering technique\nProximity Attention Point Rendering (PAPR). Unlike other point-based methods\nthat model the intrinsic decomposition of the scene, our approach does not rely on\ncomplicated shading models or simplistic priors that may not universally apply. In-\nstead, we directly model scene decomposition into albedo and shading components,\nleading to better estimation accuracy. Comparative evaluations against the latest\npoint-based inverse rendering methods demonstrate that Intrinsic PAPR achieves\nhigher-quality novel view rendering and superior point-level albedo and shading\nediting.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in point-based neural rendering methods [10, 20, 38, 30, 7, 32] have gained\nsignificant attention due to their efficiency in rendering compared to volumetric representations\nlike NeRF [17]. These methods have showcased impressive rendering quality, particularly in novel\nview synthesis tasks. However, they often lack the capability for intuitive editing tasks such as\naltering shading or colour within the scene. On the other hand, current image intrinsic decomposition\ntechniques [16, 34, 15, 5, 1] excel at pixel-level albedo and shading editing for 2D images, demon-\nstrating the ability to generalize across diverse scenes. Nevertheless, extending such detailed editing\ncapabilities to 3D scenes that is consistent across different viewpoints remains a challenge. We term\nthis challenge as \"point-level 3D scene albedo and shading editing\", and this work is dedicated to\naddressing this problem.\nTo enable albedo and shading editing in a 3D scene, a common approach involves inverse rendering,\nwhere the scene is decomposed into its geometry, reflectance, and illumination. However, due\nto the potential for multiple solutions for each component that yield the same rendered images,\ninverse rendering is considered highly ill-posed. Consequently, these methods often depend on prior\nassumptions for each of the scene decomposition component, but these assumptions may not hold\nuniversally, thereby limiting their estimation accuracy in practice."}, {"title": "2 Related Work", "content": null}, {"title": "2.1 Neural Scene Representation", "content": "In recent years, there has been a surge in interest in using neural networks for novel view synthesis\nfrom multi-view RGB images [17, 28, 18, 2, 25, 7, 32]. The 3D neural representations used by these\nneural rendering methods can be broadly categorized into volumetric and surface representations.\nVolumetric representations, exemplified by NeRF [17], exhibit impressive rendering quality but\nsuffer from high computational costs due to the need to evaluate multiple samples along each ray for\nrendering. Furthermore, achieving precise point-level scene editing in volumetric representations\nrequires changing scene information across all spatial regions corresponding to the target 3D area,\npresenting a non-trivial challenge.\nIn contrast, surface representations can render with much fewer samples, leading to increased\nefficiency. Recent advancements in point-based neural renderers [10, 20, 38, 30, 7, 32] have gained\nincreasing attention due to their high rendering quality and efficiency. These methods store local\nscene information at each point, making them well-suited for precise point-level editing by modifying\nthe information in the points at the desired region. Among these methods, splat-based techniques [10,\n38, 30, 7] use radial basis functions to calculate point contributions to rays and combine the scene"}, {"title": "2.2 Intrinsic Decomposition", "content": "The field of image intrinsic decomposition has a rich history, aiming to decompose images into\nreflectance and shading components. Early methods focused on predicting ordinal relationships\nbetween the albedos of pixel pairs [19, 35, 37], while later approaches directly regress to continuous\nshading and albedo values [12, 16, 22, 34, 36, 5]. However, training models that generalize well\nto real-world imagery remains a challenge due to domain gaps between synthetic datasets used for\nsupervised training and real-world data. To bridge this gap, Careaga and Aksoy [1] combine synthetic\ndata pretraining with real-world multi-illumination data to generate dense pseudo-ground-truth\nintrinsic components for further training, thereby bridging the intrinsic decomposition generalization\ngap.\nWhile the aforementioned methods focus on intrinsic decomposition of 2D images, there is a\ngrowing interest in adopting neural rendering for modelling intrinsic decomposition in 3D scenes.\nIntrinsicNeRF [27] uses an iterative reflectance clustering method for unsupervised reconstruction of\nalbedo and shading. LitNeRF [21] combines volume rendering with traditional mesh reconstruction\nmethods for few-view intrinsic decomposition modelling of human faces. Similar to our approach,\nPoint-Net [24] encodes intrinsic components into a point cloud, but their setup requires RGB-D\nimages for point cloud generation, whereas our method can train from scratch using only RGB\nimages.\nRecent works [23, 29, 33, 6, 3, 14] also explore combining neural rendering with inverse render-\ning, which involves more sophisticated scene models with geometry, reflectance, and illumination\ncomponents. While this approach offers increased modelling capacity, it introduces more degrees of\nfreedom in the parameter space and relies on priors for different components to find solutions. These\nsimple priors may not universally hold, which limits their estimation accuracy in practice."}, {"title": "3 Method", "content": null}, {"title": "3.1 Preliminaries: Intrinsic Decomposition", "content": "Intrinsic image decomposition is a fundamental mid-level vision problem that aims to separate an\nimage into reflectance (albedo) and shading components:\n$I = A*S$   (1)\nHere, the albedo component represents the illumination independent reflectance properties of the\nsurface materials, while the shading component captures the scene's illumination effects.\nIn comparison to inverse rendering (IR), which aims for a more detailed decomposition of scenes\ninto geometry, reflectance, and illumination, intrinsic decomposition (ID) is a simpler approach\nwith fewer parameters to estimate. This brings several benefits: (i) IR methods typically rely on\nsimplistic prior assumptions for each decomposition component due to the larger free parameter\nspace, and these assumptions may not hold in all cases, limiting their estimation accuracy in practice;\n(ii) the bias-variance trade-off suggests that simpler models can potentially achieve higher estimation\naccuracy on unseen data, which is crucial for better novel view synthesis. Additionally, extensive\nprior research on ID has resulted in pretrained models capable of directly extracting albedo and\nshading components, demonstrating strong generalizability across various synthetic and real-world\ndata. In this work, we leverage the pretrained model from [1] to obtain the intrinsic components for\nthe images of a 3D scene. These extracted albedo images then serve as supervision ground truths for\nthe albedo components of the scene."}, {"title": "3.2 Choice of Scene Representation", "content": "Our approach builds on point-based 3D scene representations. These representations encode local\nscene appearance information at each point, which makes them ideal for achieving detailed point-\nlevel editing tasks. Given the challenge of producing realistic albedo and shading editing results, it\nnecessitates a suitable point-based renderer.\nThere are two main classes of point-based renderers: splat-based renderers and attention-based\nrenderers. In splat-based renderers [10, 38, 30, 7], the points are represented as primitives such as\ndisks or Gaussian kernels, and the total contribution of all primitives intersecting the given ray forms\nrendering output at the ray. In contrast, attention-based methods, such as PAPR [32] interpolate the\nnearby points around the given ray to produce its rendering output.\nTo achieve realistic point-level albedo and shading editing, a smooth transition in appearance from the\nedited region and the surroundings is important. In splat-based approaches, each primitive operates\nindependently, leading to potential abrupt transitions between edited splats and surrounding areas. As"}, {"title": "3.3 Overview: Proximity Attention Point Rendering (PAPR)", "content": "PAPR [32] uses multi-view RGB images and corresponding camera poses to jointly learn a point-based\nscene representation along with an attention-based differentiable renderer. The scene representation\nincludes a collection of 3D points denoted by $P = \\{(p_i, u_i, \\tau_i)\\}_{i=1}^N$, where $N$ is the total number of\npoints, each associated with a location $p_i \\in R^3$, a feature vector $u_i \\in R^d$ that encodes local scene\nappearance information, and an influence score $\\tau_i \\in R$.\nTo render an image with a resolution of $H \\times W$ given a camera pose $C$, PAPR shoots rays from\nthe camera towards each pixel. At each ray, PAPR uses an attention mechanism to calculate the\ncontribution of each point based on their distances to the ray, and combines their features into a\nfeature map $F_c \\in R^{H \\times W \\times d_{feat}}$. This feature map is then fed through a U-Net renderer to produce\nthe output rendering. The whole pipeline is end-to-end differentiable, which can be trained by the\nfollowing reconstruction loss:\n$L_{recon} = d (\\hat{I},I_{gt})$  (2)\nHere, $\\hat{I}$ represents the rendered image, $I_{gt}$ represents the ground truth image, and $d(\\cdot, \\cdot)$ is a weighted\ncombination of mean squared error (MSE) and LPIPS [31] distance. For more details, please refer to\nthe original paper [32]."}, {"title": "3.4 Intrinsic PAPR", "content": "To model the intrinsic decomposition of a 3D scene into albedo and shading components, we partition\nthe dimensions of the point feature vector into two orthogonal components. One component models\nthe albedo, capturing the reflectance properties, while the other component models the shading,\ncapturing the scene's illumination effects. This design allows each point to store separate albedo and\nshading information while retaining all local scene details, which is essential for subsequent editing\napplications.\nSpecifically, our representation is denoted as $P = \\{(p_i, a_i, h_i, \\tau_i)\\}_{i=1}^N$, where $a_i \\in R^n$ represents\nthe albedo feature vector and $h_i \\in R^m$ represents the shading feature vector for each point. These\ntwo feature vectors serve as inputs to their corresponding value branches in the attention mechanism,\nproducing an albedo feature map $A_c \\in R^{H \\times W \\times d_{albedo}}$ and a shading feature map $S_c \\in R^{H \\times W \\times d_{shading}}$\nusing the attention weights.\nTo render the output colour image $\\hat{I}$, we concatenate the albedo and shading feature maps along the\nfeature dimension and pass the combined feature map through a U-Net-based renderer. Additionally,\nwe feed the albedo feature map through a separate U-Net-based renderer to produce an albedo image\n$\\hat{A}$ of the scene. Figure 2 provides an illustration of the overall pipeline.\nTo train our model for modeling the intrinsic components, one approach could be to supervise both\nthe albedo image and the predicted shading. However, areas with albedo values close to 0 (e.g., black\nor near-black regions) can lead to arbitrary shading values due to the formulation in Equation 1.\nDirect supervision of predicted shading might therefore result in incorrect values, adversely affecting\nshading prediction accuracy. Hence, we opt to implicitly learn the shading feature by supervising\nonly the albedo image and the predicted colour image.\nSpecifically, we jointly optimize all parameters in our model by minimizing the distance metric $d(\\cdot, \\cdot)$\nbetween the rendered colour and albedo images and their corresponding ground truth images:\n$L =d(\\hat{I}, I_{gt}) + d(\\hat{A}, A_{gt})$  (3)\nHere, we use the same distance metric $d(\\cdot, \\cdot)$ as PAPR [32]."}, {"title": "4 Experiments", "content": "We compare our method to the latest point-based inverse rendering techniques, namely GS-IR [13]\nand DPIR [4], across synthetic and real-world datasets. These datasets include the NeRF Synthetic\ndataset [17], PS-NeRF [26] (synthetic), and a subset of Tank & Temples [9] (real-world).\nThe baseline methods are initialized using their respective original strategies: visual hull for DPIR and\nStructure-from-Motion (SfM) for GS-IR, with each scene containing between 100,000 and 400,000\npoints in the end. In contrast, our method uses a significantly sparser point cloud of 30,000 points,\nwhich is initialized randomly."}, {"title": "4.1 Novel View Synthesis", "content": "We begin by evaluating models' performances for novel view synthesis using the PSNR, SSIM, and\nLPIPS [31] metrics. Table 1 summarizes the average rendering reconstruction quality compared\nto baseline methods across synthetic and real-world datasets. Detailed scene breakdowns can be\nfound in the supplementary materials. Our method consistently outperforms both inverse rendering\nbaselines and the original PAPR across all metrics for both synthetic and real-world datasets. These\nresults demonstrate the superior rendering quality of our method for novel view synthesis."}, {"title": "4.2 Point-level Albedo and Shading Editing", "content": "We proceed with evaluating the models' performances on point-level albedo and shading editing. To\nensure a fair comparison of albedo and shading editing quality across methods, we conduct a transfer\nevaluation where the albedo/shading from one source point is transferred to a target region in the\nscene. Effective editing should excel along the following axes:\n(a) Accuracy: The editing should accurately reflect the source's attributes being transferred in the\ntarget region, ensuring faithful albedo/shading transfer. (b) Shading-Albedo Feature Decoupling:\nDuring a transfer, the albedo and shading features need to be completely decoupled, in both the source"}, {"title": "4.2.1 Point-level Albedo Transfer", "content": "To perform point-level albedo transfer in our method, we copy the albedo feature vector from the\nselected point to the albedo feature vectors of points in the target region. Subsequently, we render the\nnovel view using the modified point feature vectors. A similar process is also applied to the baseline\nmethods. DPIR [4] utilizes a feature vector for each point, passing them through a diffuse albedo\nnetwork to represent each point's albedo. This allows copying and replacing albedo features between\npoints. GS-IR [13] follows a similar approach, using an albedo vector for each splat.\nAs shown in Figure 4 and Table 2, DPIR and GS-IR struggle with albedo transfer, introducing noise\nand distorting the target surface while mixing shading and albedo features. In contrast, our model\naccurately transfers the correct albedo, preserves target surface details and optical properties, and\nclearly separates albedo and shading features, achieving a more realistic result. Additional results for\nsynthetic, and real-world scenes, as well as a video demonstrating the 360-degree rendering of the\nediting results, are included in the supplementary materials."}, {"title": "4.2.2 Point-level Shading Transfer", "content": "Similar to point-level albedo transfer, we apply shading transfer using our method by copying the\nshading feature vector of a source point to the shading feature vectors of the points at the target\nregion. DPIR [4] processes each point's features through two networks\u2014the specular basis coefficient\nnetwork and the specular basis network to calculate point specularity. For shading transfer, we swap\nthe target area's calculated specularity with that of the source point. Shading model in GS-IR [13]\nis influenced by the metallic and roughness values. Before rendering, we replace the target points'\nmetallic and roughness values with those from the source point.\nAs shown in Figure 5 and Table 3, DPIR and GS-IR often fail to accurately transfer the correct\nshading level from the source, resulting in inadequate shading transfer accuracy. In addition, their\nresults show signs of albedo leakage which indicates poor decoupling of the albedo and shading.\nIn contrast, our method successfully transfers the source shading while preserving the target area's\nalbedo, surface details, and optical properties. This highlights our model's effectiveness in accurately"}, {"title": "4.2.3 Additional Albeo and Shading Editing Techniques", "content": "In this section, we present additional albedo and shading editing capabilities enabled by our method,\nwhich go beyond simple transfers from other regions within the same scene."}, {"title": "5 Discussion and Conclusion", "content": "Limitation While our method demonstrates promising results, it does have certain limitations that\nopen avenues for future work. Specifically, the reliance on a pretrained model to obtain ground-truth\nalbedo may propagate biases present in the pretraining data, potentially affecting the accuracy and\ngeneralizability of our results.\nSocietal Impact The demonstrated editing capabilities could significantly assist artists in achieving\nalbedo and shading editing of 3D scenes more effectively. These use cases may range from digitally\ncreated synthetic scenes to real-world scenes captured by mobile devices. However, this ease of\nediting could also lead to increased unauthorized copying and manipulation of 3D content, raising\ncopyright and intellectual property concerns.\nConclusion In this paper, we introduce \u201cIntrinsic PAPR\u201d, a novel method for point-level 3D scene\nalbedo and shading editing. Building on the Proximity Attention Point Rendering (PAPR) technique,\nour approach effectively models the decomposition of a 3D scene into albedo and shading components,\nenabling detailed and consistent editing across different viewpoints."}]}