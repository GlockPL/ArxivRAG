{"title": "Intrinsic PAPR for Point-level 3D Scene Albedo and Shading Editing", "authors": ["Alireza Moazeni", "Shichong Peng", "Ke Li"], "abstract": "Recent advancements in neural rendering have excelled at novel view synthesis from multi-view RGB images. However, they often lack the capability to edit the shading or colour of the scene at a detailed point-level, while ensuring consistency across different viewpoints. In this work, we address the challenge of point-level 3D scene albedo and shading editing from multi-view RGB images, focusing on detailed editing at the point-level rather than at a part or global level. While prior works based on volumetric representation such as NeRF struggle with achieving 3D consistent editing at the point level, recent advancements in point-based neural rendering show promise in overcoming this challenge. We introduce \u201cIntrinsic PAPR\", a novel method based on the recent point-based neural rendering technique Proximity Attention Point Rendering (PAPR). Unlike other point-based methods that model the intrinsic decomposition of the scene, our approach does not rely on complicated shading models or simplistic priors that may not universally apply. Instead, we directly model scene decomposition into albedo and shading components, leading to better estimation accuracy. Comparative evaluations against the latest point-based inverse rendering methods demonstrate that Intrinsic PAPR achieves higher-quality novel view rendering and superior point-level albedo and shading editing.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in point-based neural rendering methods [10, 20, 38, 30, 7, 32] have gained significant attention due to their efficiency in rendering compared to volumetric representations like NeRF [17]. These methods have showcased impressive rendering quality, particularly in novel view synthesis tasks. However, they often lack the capability for intuitive editing tasks such as altering shading or colour within the scene. On the other hand, current image intrinsic decomposition techniques [16, 34, 15, 5, 1] excel at pixel-level albedo and shading editing for 2D images, demonstrating the ability to generalize across diverse scenes. Nevertheless, extending such detailed editing capabilities to 3D scenes that is consistent across different viewpoints remains a challenge. We term this challenge as \"point-level 3D scene albedo and shading editing\", and this work is dedicated to addressing this problem.\nTo enable albedo and shading editing in a 3D scene, a common approach involves inverse rendering, where the scene is decomposed into its geometry, reflectance, and illumination. However, due to the potential for multiple solutions for each component that yield the same rendered images, inverse rendering is considered highly ill-posed. Consequently, these methods often depend on prior assumptions for each of the scene decomposition component, but these assumptions may not hold universally, thereby limiting their estimation accuracy in practice."}, {"title": "2 Related Work", "content": null}, {"title": "2.1 Neural Scene Representation", "content": "In recent years, there has been a surge in interest in using neural networks for novel view synthesis from multi-view RGB images [17, 28, 18, 2, 25, 7, 32]. The 3D neural representations used by these neural rendering methods can be broadly categorized into volumetric and surface representations. Volumetric representations, exemplified by NeRF [17], exhibit impressive rendering quality but suffer from high computational costs due to the need to evaluate multiple samples along each ray for rendering. Furthermore, achieving precise point-level scene editing in volumetric representations requires changing scene information across all spatial regions corresponding to the target 3D area, presenting a non-trivial challenge.\nIn contrast, surface representations can render with much fewer samples, leading to increased efficiency. Recent advancements in point-based neural renderers [10, 20, 38, 30, 7, 32] have gained increasing attention due to their high rendering quality and efficiency. These methods store local scene information at each point, making them well-suited for precise point-level editing by modifying the information in the points at the desired region. Among these methods, splat-based techniques [10, 38, 30, 7] use radial basis functions to calculate point contributions to rays and combine the scene"}, {"title": "2.2 Intrinsic Decomposition", "content": "The field of image intrinsic decomposition has a rich history, aiming to decompose images into reflectance and shading components. Early methods focused on predicting ordinal relationships between the albedos of pixel pairs [19, 35, 37], while later approaches directly regress to continuous shading and albedo values [12, 16, 22, 34, 36, 5]. However, training models that generalize well to real-world imagery remains a challenge due to domain gaps between synthetic datasets used for supervised training and real-world data. To bridge this gap, Careaga and Aksoy [1] combine synthetic data pretraining with real-world multi-illumination data to generate dense pseudo-ground-truth intrinsic components for further training, thereby bridging the intrinsic decomposition generalization gap.\nWhile the aforementioned methods focus on intrinsic decomposition of 2D images, there is a growing interest in adopting neural rendering for modelling intrinsic decomposition in 3D scenes. IntrinsicNeRF [27] uses an iterative reflectance clustering method for unsupervised reconstruction of albedo and shading. LitNeRF [21] combines volume rendering with traditional mesh reconstruction methods for few-view intrinsic decomposition modelling of human faces. Similar to our approach, Point-Net [24] encodes intrinsic components into a point cloud, but their setup requires RGB-D images for point cloud generation, whereas our method can train from scratch using only RGB images.\nRecent works [23, 29, 33, 6, 3, 14] also explore combining neural rendering with inverse render-ing, which involves more sophisticated scene models with geometry, reflectance, and illumination components. While this approach offers increased modelling capacity, it introduces more degrees of freedom in the parameter space and relies on priors for different components to find solutions. These simple priors may not universally hold, which limits their estimation accuracy in practice."}, {"title": "3 Method", "content": null}, {"title": "3.1 Preliminaries: Intrinsic Decomposition", "content": "Intrinsic image decomposition is a fundamental mid-level vision problem that aims to separate an image into reflectance (albedo) and shading components:\n$\\begin{equation}\nI = A*S\n\\end{equation}$\nHere, the albedo component represents the illumination independent reflectance properties of the surface materials, while the shading component captures the scene's illumination effects.\nIn comparison to inverse rendering (IR), which aims for a more detailed decomposition of scenes into geometry, reflectance, and illumination, intrinsic decomposition (ID) is a simpler approach with fewer parameters to estimate. This brings several benefits: (i) IR methods typically rely on simplistic prior assumptions for each decomposition component due to the larger free parameter space, and these assumptions may not hold in all cases, limiting their estimation accuracy in practice; (ii) the bias-variance trade-off suggests that simpler models can potentially achieve higher estimation accuracy on unseen data, which is crucial for better novel view synthesis. Additionally, extensive prior research on ID has resulted in pretrained models capable of directly extracting albedo and shading components, demonstrating strong generalizability across various synthetic and real-world data. In this work, we leverage the pretrained model from [1] to obtain the intrinsic components for the images of a 3D scene. These extracted albedo images then serve as supervision ground truths for the albedo components of the scene."}, {"title": "3.2 Choice of Scene Representation", "content": "Our approach builds on point-based 3D scene representations. These representations encode local scene appearance information at each point, which makes them ideal for achieving detailed point-level editing tasks. Given the challenge of producing realistic albedo and shading editing results, it necessitates a suitable point-based renderer.\nThere are two main classes of point-based renderers: splat-based renderers and attention-based renderers. In splat-based renderers [10, 38, 30, 7], the points are represented as primitives such as disks or Gaussian kernels, and the total contribution of all primitives intersecting the given ray forms rendering output at the ray. In contrast, attention-based methods, such as PAPR [32] interpolate the nearby points around the given ray to produce its rendering output.\nTo achieve realistic point-level albedo and shading editing, a smooth transition in appearance from the edited region and the surroundings is important. In splat-based approaches, each primitive operates independently, leading to potential abrupt transitions between edited splats and surrounding areas. As"}, {"title": "3.3 Overview: Proximity Attention Point Rendering (PAPR)", "content": "PAPR [32] uses multi-view RGB images and corresponding camera poses to jointly learn a point-based scene representation along with an attention-based differentiable renderer. The scene representation includes a collection of 3D points denoted by P = { ( p i , u i , \u03c4 i ) } i = 1 N , where N is the total number of points, each associated with a location p i \u2208 R 3 , a feature vector u i \u2208 R d that encodes local scene appearance information, and an influence score \u03c4 i \u2208 R.\nTo render an image with a resolution of H \u00d7 W given a camera pose C, PAPR shoots rays from the camera towards each pixel. At each ray, PAPR uses an attention mechanism to calculate the contribution of each point based on their distances to the ray, and combines their features into a feature map F c \u2208 R H \u00d7 W \u00d7 d feat . This feature map is then fed through a U-Net renderer to produce the output rendering. The whole pipeline is end-to-end differentiable, which can be trained by the following reconstruction loss:\n$\\begin{equation}\n\\mathcal{L}_{recon} = d \\left(\\hat{I}, I_{g t}\\right)\n\\end{equation}$\nHere, \u00ce represents the rendered image, I gt represents the ground truth image, and d(\u00b7, \u00b7) is a weighted combination of mean squared error (MSE) and LPIPS [31] distance. For more details, please refer to the original paper [32]."}, {"title": "3.4 Intrinsic PAPR", "content": "To model the intrinsic decomposition of a 3D scene into albedo and shading components, we partition the dimensions of the point feature vector into two orthogonal components. One component models the albedo, capturing the reflectance properties, while the other component models the shading, capturing the scene's illumination effects. This design allows each point to store separate albedo and shading information while retaining all local scene details, which is essential for subsequent editing applications.\nSpecifically, our representation is denoted as P = { ( p i , a i , h i , \u03c4 i ) } i = 1 N , where a i \u2208 R n represents the albedo feature vector and h i \u2208 R m represents the shading feature vector for each point. These two feature vectors serve as inputs to their corresponding value branches in the attention mechanism, producing an albedo feature map A c \u2208 R H \u00d7 W \u00d7 d albedo and a shading feature map S c \u2208 R H \u00d7 W \u00d7 d shading using the attention weights.\nTo render the output colour image \u00ce, we concatenate the albedo and shading feature maps along the feature dimension and pass the combined feature map through a U-Net-based renderer. Additionally, we feed the albedo feature map through a separate U-Net-based renderer to produce an albedo image \u00c2 of the scene. Figure 2 provides an illustration of the overall pipeline.\nTo train our model for modeling the intrinsic components, one approach could be to supervise both the albedo image and the predicted shading. However, areas with albedo values close to 0 (e.g., black or near-black regions) can lead to arbitrary shading values due to the formulation in Equation 1. Direct supervision of predicted shading might therefore result in incorrect values, adversely affecting shading prediction accuracy. Hence, we opt to implicitly learn the shading feature by supervising only the albedo image and the predicted colour image.\nSpecifically, we jointly optimize all parameters in our model by minimizing the distance metric d(\u00b7, \u00b7) between the rendered colour and albedo images and their corresponding ground truth images:\n$\\begin{equation}\n\\mathcal{L} =d\\left(\\hat{I}, I_{g t}\\right)+d\\left(\\hat{A}, A_{g t}\\right)\n\\end{equation}$\nHere, we use the same distance metric d(\u00b7, \u00b7) as PAPR [32]."}, {"title": "5 Discussion and Conclusion", "content": "Limitation While our method demonstrates promising results, it does have certain limitations that open avenues for future work. Specifically, the reliance on a pretrained model to obtain ground-truth albedo may propagate biases present in the pretraining data, potentially affecting the accuracy and generalizability of our results.\nSocietal Impact The demonstrated editing capabilities could significantly assist artists in achieving albedo and shading editing of 3D scenes more effectively. These use cases may range from digitally created synthetic scenes to real-world scenes captured by mobile devices. However, this ease of editing could also lead to increased unauthorized copying and manipulation of 3D content, raising copyright and intellectual property concerns.\nConclusion In this paper, we introduce \u201cIntrinsic PAPR\u201d, a novel method for point-level 3D scene albedo and shading editing. Building on the Proximity Attention Point Rendering (PAPR) technique, our approach effectively models the decomposition of a 3D scene into albedo and shading components, enabling detailed and consistent editing across different viewpoints."}]}