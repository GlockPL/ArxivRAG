{"title": "Personalizing Reinforcement Learning from Human\nFeedback with Variational Preference Learning", "authors": ["Sriyash Poddar", "Yanming Wan", "Hamish Ivison", "Abhishek Gupta", "Natasha Jaques"], "abstract": "Reinforcement Learning from Human Feedback (RLHF) is a powerful paradigm\nfor aligning foundation models to human values and preferences. However, cur-\nrent RLHF techniques cannot account for the naturally occurring differences in\nindividual human preferences across a diverse population. When these differences\narise, traditional RLHF frameworks simply average over them, leading to inac-\ncuate rewards and poor performance for individual subgroups. To address the\nneed for pluralistic alignment, we develop a class of multimodal RLHF methods.\nOur proposed techniques are based on a latent variable formulation - inferring a\nnovel user-specific latent and learning reward models and policies conditioned on\nthis latent without additional user-specific data. While conceptually simple, we\nshow that in practice, this reward modeling requires careful algorithmic consid-\nerations around model architecture and reward scaling. To empirically validate\nour proposed technique, we first show that it can provide a way to combat under-\nspecification in simulated control problems, inferring and optimizing user-specific\nreward functions. Next, we conduct experiments on pluralistic language datasets\nrepresenting diverse user preferences and demonstrate improved reward function\naccuracy. We additionally show the benefits of this probabilistic framework in\nterms of measuring uncertainty, and actively learning user preferences. This work\nenables learning from diverse populations of users with divergent preferences,\nan important challenge that naturally occurs in problems from robot learning to\nfoundation model alignment.", "sections": [{"title": "1 Introduction", "content": "Reinforcement learning from human feedback (RLHF) has become the predominant technique for\naligning AI foundation models to human values. Across domains like natural language processing\n(NLP) [52] to robotics [62, 50], RLHF is an effective way to improve the performance, accuracy, and\nsafety of AI models, by ensuring that they align with human preferences [45, 34, 29] The question\nthen becomes: whose preferences? Current RLHF approaches rely on a prescriptive set of values\ncurated by a small set of AI researchers [39, 60, 2]. Moreover, they typically assume that all end-users\nshare the same set of values. Given the concerning lack of diversity in AI [22], it is clear that this\napproach cannot account for the range of social, moral, and political values that inform preferences\nin human populations. Recent work has demonstrated that with current RLHF techniques, if the\nmajority population has a weak preference for an outcome that will severely disadvantage a minority\ngroup, the learned reward model will ignore the minority group's preferences [58]. These issues\nnecessitate pluralistic alignment of models to human preferences [59, 44] (see Figure 1); ideally, we\nwould democratize RLHF to account for a wider variety of human values to better serve a diverse\npopulation. While intuitively appealing, a practical technical solution to this problem has yet to be\ndeveloped and validated.\nCurrent approaches to RLHF [52] use the Bradley-Terry-Luce (BTL) [15] model to learn a reward\nmodel that explains the human preferences. While the BTL model accounts for noisy preferences,\nRLHF typically applies this model under the 'unimodal' assumption that all human preferences are\nderived from a single utility function. This fails to capture scenarios where preferences diverge-\ni.e. are multi-modal-due to fundamentally different utilities. For example, Figure 1 shows a\ncase where one group of users prefers detailed responses, while another prefers concise ones. By\nperforming maximum likelihood estimation under the unimodal BTL model, current methods learn a\nreward function that averages these multi-modal preferences (akin to mode averaging in imitation\nlearning [51]. As we show in our experimental results, this model misspecification leads to reward\nmodels that are inaccurate, and the policies optimized on these rewards fail to accomplish tasks\nper any of the distinct preferences (see Figures 8, 3). Thus, vanilla RLHF methods [52, 17] are\ninsufficient for aligning AI systems to diverse human values.\nIn many applications, from large language models (LLMs) to assistive robotics [7], users are diverse,\nand AI systems must adapt the generated responses to user-specific preferences to successfully\ncomplete the task. Consider, for example, a robot assistant putting away dishes in a specific user's\nkitchen: each individual has unique personal preferences for how the dishes in their kitchen are\norganized, potentially diverging from others' preferences. In the context of LLMs, failing to adapt to\nuser-specific preferences can make them unhelpful, unsafe, and vulnerable to jailbreak in the presence\nof conflicting objectives [2, 58]. To build safe and performant foundation models serving a diverse\npopulation, we need methods that can explicitly account for and adapt to the inherent plurality of\nhuman preferences.\nThese insights suggest that human preferences are not derived from a single utility function, but are\naffected by unobserved, hidden user context [58]. To accurately model individual utilities, RLHF\nshould be able to efficiently infer and adapt to the context for each user. With this in mind, we formu-"}, {"title": "2 Related Work", "content": "Reinforcement Learning from Human Feedback (RLHF): We focus on the problem of reinforce-\nment learning (RL) from binary human preferences using the Bradley-Terry-Luce (BTL) model [15].\nThis has a rich history in the field of RL and robotics, often referred to as Preference-based RL (PbRL)\n[65, 28, 1, 8, 10]. We specifically build on the framework outlined in Christiano et al. [17] and\nfurther expanded in recent works [52, 2, 74, 60, 37, 58]. This has seen a wide range of applications\nranging from training robots [17, 10, 62] to finetuning language models for alignment [52, 74, 60].\nOur work, enabling RLHF with diverse preferences, is easily applicable to any preference-based\nlearning method, including recent techniques [55, 24] that circumvent reward modeling altogether.\nRLHF under non-BTL models: Prior work has aimed to study non-BTL models of human behavior\nand preferences [12, 43, 42, 37, 61] to account for human irrationality and uncertainty in preferences,\nor intransitive preferences [49, 61, 64]. However, our key argument in this work is less about\nhuman irrationality (i.e. inconsistency), and more about the divergence between potentially rational\npreferences for different labeling users and end users. In this sense, our work is complementary in that\nthe latent variable model can easily be adapted to non-BTL models as well. In fact, we incorporate\nthe technique proposed in Swamy et al. [61] to improve reward learning for downstream applications.\nPersonalized RLHF: While some works [39, 40, 35] characterize similar challenges as VPL,\nthey largely focus on exploring the societal issues underpinning the need for personalization and\nintroducing datasets with diverse annotations. Conitzer et al. [19] argue Social Choice Theory could\nprovide insights into how to aggregate diverse preferences. But these works do not propose a technical\nmethod to achieve modeling diverse preferences."}, {"title": "3 Technical Preliminaries", "content": "In this work, we build on the framework of preference-based reward learning (often referred to\nas RLHF) [17, 1]. In particular, we will consider reward learning methods based on the Bradley-\nTerry-Luce (BTL) choice model [15]. RLHF has two key phases: (1) inferring a reward function\nfrom human-provided labels of ordinal preferences; (2) using reinforcement learning (RL) to train\na decision-making policy that maximizes the rewards inferred in step (1). We will instantiate this\nframework abstractly, which can then be specialized to both LLMs and robotics.\nWe define a Markov decision process (MDP) [53] $M = (S, A, T, \\gamma, \\rho_0)$, with state space S, action\nspace A, transition dynamics T, discount factor $\\gamma$ and initial state distribution $\\rho_0$. Notably, we do not\nhave access to an underlying reward function, but instead have annotators who rank pairs of states SA\nand SB. We assume annotators have an unknown reward function $r(s)$ that informs their preference\nlabels. More formally, an annotator (h \u2208 H) takes a pair of states SA and SB, and returns an ordinal\npreference i.e. y = 1(sa > sp), according to r(s) [15, 23]; where H is the space of all possible\nannotators. Given a dataset of annotated preferences $D = \\{(s'_a, s'_b, y = 1(s_a > s_b))\\}_{i=1}^N$, a typical\nRLHF procedure learns a reward function $r_\\theta(s)$ using a maximum likelihood objective (MLE) on the\npreferences, where the likelihood: $p_\\theta(y | s_A, s_B)$ can be defined using the BTL model:\n$P_\\theta(y = 1 | s_A, s_B) = 1 - p_\\theta(y = 0 | s_A, s_B) = P(s_A > s_B) = \\frac{e^{r(s_A)}}{e^{r(s_A)} + e^{r(s_B)}}$                                                                                       (1)\nNote, that while in this case, the ordinal preferences are on states $s_A, s_B$, this can be generalized to\ntrajectories or snippets [17, 52]. Finally, the recovered reward function can then be used to learn (or\nfinetune) a policy $\\pi_\\theta(a|s)$ that can act to maximize the expected sum of approximated rewards in"}, {"title": "4 VPL: Incorporating Latent Context into Preference-Based Learning", "content": "The standard BTL formulation described in Section 3 is based on the assumption that all annotators\nh\u2208 H share a single underlying reward function $r_\\theta(s)$. This does not hold in practice with a diverse\nrange of annotators. To model diverse, pluralistic preferences, we frame multi-modal reward learning\nas a latent variable problem, where the latent variable z represents the hidden context affecting the\nunderlying reward function (and thereby the preferences) of an annotator h \u2208 H; for instance, it could\nbe representative of their lived experience, or the style of response/policy that they would like the\nagent to perform. Following this, the latent-conditional reward $r_\\theta(s, z)$ is a function of latent z along\nwith state s, and preference likelihoods can be expressed with a latent-conditional BTL model:\n$P_\\theta(y = 1 | s_A, s_B, z) = P(s_A \\succ s_B | z) = \\frac{e^{r(s_A, z)}}{e^{r(s_A, z)} + e^{r(s_B, z)}}$                                                                                       (2)\nThe maximum likelihood objective for this model from a dataset of preference labels (with multiple\nannotators), $\\max_{ \\theta} E_{(s_A, s_B, y) \\sim D}[log P(y | s_A, s_B)] = E_{(s_A, s_B, y) \\sim D}[log \\int P(y | s_A, s_B, z)p(z)dz]$ is in-\ntractable due to marginalization over the unobserved latent variable z. To tackle this, we can introduce\na variational posterior approximation $q_\\psi(z | \\{(s_a, s_b, y^i)\\}_{i=1}^N)$, conditional on multiple annotations\n$\\{(s'_a, s'_b, y^i = h(s_a, s_b))\\}_{i=1}^N$ provided by the same annotator h\u00b2. This results in a corresponding\nevidence lower bound (ELBO), $L(\\theta, \\psi)$, for the intractable marginal log $p_\\theta(y | s_A, s_B)$:\n$\\begin{aligned} &E_{h \\sim H} E_{\\{(s_A, s_B, y^i = h(s_A, s_B))\\}_{i=1}^N \\sim D} \\Big[ E_{z \\sim q_\\psi(z|\\{(s_a, s_b, y^i)\\}_{i=1}^N)}[log P_\\theta(y | s_A, s_B, z)] \\\\ & - D_{KL}(q_\\psi(z|\\{(s_a, s_b, y^i)\\}_{i=1}^N) || p(z)) \\Big] \\end{aligned}$ (3)\nThis objective samples a user h ~ H from the given annotators and a set of annotations from this\nparticular user $\\{(s'_A, s'_B, y^i = h(s_a, s_b))\\}_{i=1}^N$ for inferring the latent variable z through the posterior\n$q_\\psi(z | \\{(s_a, s_b, y^i)\\}_{i=1}^N)$. Given the posterior, this objective involves optimizing two terms: a\nmaximum preference likelihood objective ($log p_\\theta(y | s_A, s_B, z)$) using the contextual BTL model\nand a regularization term ($D_{KL}(q_\\psi(z | \\{(s_a, s_b, y^i)\\}_{i=1}^N) || p(z))$) against a prior p(z). Intuitively\nthis objective encodes a set of user-provided annotations $\\{(s'_A, s'_B, y^i)\\}_{i=1}^N$ into a latent distribution\nusing the encoder $q_\\psi$, and then learns a latent-conditional reward function $r(s, z)$ that best explains\nthe annotated preference data. As the variational encoder $q_\\psi$ generates a latent distribution, this\nformulation further enables uncertainty estimation and active learning, as shown in Section 4.2. We\ndescribe details further in Appendix B.3.\nWe emphasize that the only additional requirement of this objective, compared to standard RLHF [17],\nis a set of annotated pairs from the same annotator, and this information is easily available when\nannotators are queried in batches [10]. We refer to Algorithm 1 for further details on implementation.\nPersonalized, latent-conditioned policies. The learned reward models $r_\\theta(s, z)$ can be used to\ntrain personalized, user-specific policies. During training, we learn a latent-conditioned policy\n$\\pi_\\theta(\\cdot|s, z)$ that maximizes the rewards $r_\\theta(s, z)$ for different values of z. This allows the policy\nto adapt to diverse user preferences encoded in the latent space. We can use any standard (RL)\nalgorithm [57, 30, 41] to optimize the latent-conditional reward maximization objective: $\\pi_\\theta =$\n$\\arg \\max_{\\theta} E_{\\pi_{\\theta}, z \\in p(z)}[\\sum_t r_{\\theta}(s_t, z)]$. For this, we sample z from the prior p(z) during training and\nlearn the policy $\\pi_\\theta(\\cdot|s, z)$, to maximize the corresponding latent-conditioned reward $r_\\theta(s, z)$.\nAt test-time, we infer a specific user's ($h_{test}$) latent context z by posterior inference using a set of\nlabeled preference queries $\\{(s', s', y^i = h_{test}(s_A, s_B))\\}_{i=1}^N$ i.e $z ~ q_\\psi(z | \\{(s_a, s'_b, y^i)\\}_{i=1}^N)$. We\nthen deploy the personalized policy $\\pi_\\theta(\\cdot|s, z)$ conditioned on the inferred z for that user. We outline\nthe complete algorithms for policy training and deployment in Appendix C."}, {"title": "4.1 Scaled Rewards for Multi-Task Learning", "content": "In practice, optimizing latent-conditioned reward functions learned with the VPL objective poses\nunique challenges. The pairwise preferences used to train the reward model in Section 4 do not have\ninformation about the scale of rewards, but only their relative ordering. As a simple illustration,\nif we have a pair of states SA, SB, where the users prefer $s_A$ i.e. $s_A > s_B$, two different reward\nfunctions: $r(s) = 100, r(s) = 50$ or $r(s_a) = 50, r(s_b) = 0$ have the same likelihood under the\nBTL model. Empirically, we observe that this poses problems for optimizing Equation 3; different\nvalues of the latent variable z result in learned reward functions of vastly different scales. This is\nan issue for several reasons: 1) varying reward scales adversely affect the landscape of multi-user\npolicy optimization (often observed in multi-task RL) [31], and 2) it is challenging to identify states\nwhere user preferences diverge across the population as differently scaled rewards cannot be directly\ncompared.\nTo address this issue, we experiment with several different techniques for scaling the learned reward\nfunctions (see Appendix A.2). Our key insight in solving this challenge lies in the observation\nthat while raw rewards from BTL are not scaled equally across z, probabilities from the preference\nlikelihood model $p(y | s_A, s_B, z)$ are appropriately scaled. This suggests that an effective solution to\nthe reward scaling issue is to replace the raw rewards from the BTL model (r(s, z)) with likelihoods\nsuggested by the pairwise preference likelihood model $p(y | s_A, s_B, z)$. In particular, a natural choice\nof scaled rewards for a state sa is the expected likelihood that the state sa is \"preferred\" to all other\nstates (or a sampled set of states) SB observed in the data - $r_\\theta(s_a, z) = E_{s_B \\in S}[P(y = 1 | s_A, s_B, z)]$.\nSince these are probabilities, normalized in the [0, 1] range, the scaling of rewards is consistent across\nlatents z. Note that these expected likelihood rewards can easily be obtained from the objective in\nEquation 3 since we already train a latent-conditional preference classifier via maximum likelihood.\nWhile proposed from a very different perspective, we note the similarity of this reward scaling\napproach to recent work [61, 49], in particular, Self-Play Preference Optimization (SPO) [61], which\nwas originally proposed to address the issue of intransitive preferences. Similar to [61] we assume\nthat the oracle / user providing preference labels is Non-Markovian. Due to this similarity\u00b3, , we use\nVPL-SPO to indicate this approach of preference likelihood-based reward scaling throughout our\nexperiments (See Algorithm 3 for details)."}, {"title": "4.2 Active Learning of Preferences to Minimize Latent Uncertainty", "content": "A natural question that arises for test time deployment of the latent-conditioned policies is how\nto obtain the set of state pairs $\\{(s_A, s_B)\\}_{i=1}^N$ to be annotated with preference labels and used for\nposterior inference, $z ~ q_\\psi(z | \\{(s_a, s'_b, y^i)\\}_{i=1}^N)$. Not all query sets $\\{(s_A, s_B)\\}_{i=1}^N$ are made equal;\nsome are more informative than others. Certain states (where preferences vary across annotators) are\nparticularly informative in accurately inferring the z which should be used for policy deployment\n$\\pi(\\cdot|s, z)$. In VPL, the probabilistic modeling of the variational encoder naturally allows for active\nselection of the most informative query set based on maximal information gain, following prior\nwork [8, 10, 50]. Here the provision of preference labels $\\{y^i\\}_{i=1}^N$ will provide the maximum\ninformation about the latent distribution (and indirectly, the user preferences). This active query\nselection procedure can be expressed as the following optimization problem, maximizing the mutual\ninformation between the labels and the latent distribution.\n$\\{(s_{A_i}, s_{B_i})\\}_{i=1}^N \\triangleq arg \\max_{\\{(s_{A_i}, s_{B_i})\\}_{i=1}^N} I (z; \\{y\\}_{i=1}^N | q_\\psi, \\{(s_{a_i}, s_{b_i})\\}_{i=1}^N)$                                                                                       (4)\nThe posterior $q_\\psi$ is a multivariate Gaussian, and assuming a uniform distribution over the set of\nlabels, $q_\\psi(z | \\{(s_a, s'_b)\\}_{i=1}^N)$ allows for closed form solution for mutual information I. To solve the\nmaximization objective, we chose the query set (sa, 88)1 with the maximum information gain,\nacross samples from the preference dataset. Finally, we elicit user labels on this maximal query set,\ninfer the latent, and condition the policy on this latent at deployment."}, {"title": "5 Scaling VPL for Reward Learning in Large Language Models (LLMs)", "content": "VPL can be used to train pluralistic reward models for LLMs, accounting for diverse human prefer-\nences and values. Here we discuss the key details that are essential to scale our method to LLMs.\nThe architecture of our LLM reward model is shown in Figure 2. Unlike prior work which attempted\nto insert summary embedding layers into LLMs (see e.g. [18]), we find that we can successfully com-\npress user preference information into a concise, probabilistic embedding vector z without sacrificing\nreward model performance. Further details and hyperparameters are discussed in Appendix B.\nPrompt and Response Embeddings. Since using raw representations of the prompt and responses\ncan increase the context length significantly, we use a pre-trained LLM to encode prompt and response\npairs together [6] (to be consistent with previous notation, we assume a preferred state sA contains\nboth a prompt and response [x,r], and we obtain eA = LLM(sA)). For efficient training, we\npre-compute and freeze the encoded embeddings.\nLatent Encoder. Given a set of multiple encoded preference queries from the same user,\n$\\{(e_A^i, e_B^i, y^i)\\}_{i=1}^N$, we pass each through the same pair encoder to obtain h\u2081 = enc(e^,e). The\nlatent representation z is generated using a self-attention layer over the set of encoded pairs, $\\{h_i\\}_{i=1}^N$.\nReward learning. Here, the representation eA' of a new state sA' is concatenated with a z sampled\nfrom the posterior distribution which is passed into an MLP to predict the rewards. The LLM is\nfine-tuned using low-rank adaptation (LoRA) [32], and unlike typical RLHF settings, we find that\nwe need to train the reward model for \u2265 1 epochs to fit the encoder and the reward model.\nData augmentation. As we scaled VPL to larger datasets with more users, we found that augmenting\nthe training dataset with multiple context samples from the same user for each new data point is\nessential to learning an effective encoder. Formally, at training time, given a prompt and response\npair S'A, S'B from a particular user, we generate M\u2208 {4,8} duplicates of this labeled response with\ndifferent contexts i.e. annotated prompt and response pairs $(\\{(s_a, s_b, y^i)\\}_{i=1}^K)^M_i$; where each\ncontext $(\\{(s'_a, s'_b, y^i)\\}_{i=1}^K)_i$ is sampled from a user annotated subset of size K (K > N)."}, {"title": "6 Experimental Evaluation on Simulated Control Tasks", "content": "In our experiments, we answer the following questions: (1) Can VPL accurately learn a multi-modal\ndistribution of reward functions from a preference dataset labeled by diverse users? (2) Do the\ninferred latent user vectors enable learning a multi-task personalized policy? (3) Can we leverage the\nposterior to actively query preferences for latent estimation? In this section, we show the benefits of\nVPL in multiple simulated control tasks and demonstrate that VPL is able to capture multi-modality in"}, {"title": "7 LLM Experiments", "content": "In this section, we ask: can VPL scale up to the pluralistic alignment of LLM-based reward models?\nWe compare the reward modeling performance of our method against two baselines: the vanilla BTL\nmodel and DPL [58]. We experiment with two LLMs: GPT2 [54] and Llama2-7B [63], and two\npluralistic preference datasets."}, {"title": "7.1 Datasets for pluralistic LLM alignment", "content": "Prior RLHF works have focused mainly on unimodal BTL models, and as such there is a lack of\npublicly available datasets containing annotated preferences with divergent objectives. To evaluate\nour method on capturing multi-modality in preferences for LLMs, we consider two benchmarks.\nFirst, we introduce a synthetic dataset, Pets, that directly represents multimodal preferences, and\nsecond, we augment the publicly available UltraFeedback [20] dataset.\nPets. Here, the dataset is generated to reflect multi-modal user preferences, where each user has a\npreference ranking over four kinds of animals (in this case cats, dogs, birds, and rabbits). To simulate\na setting where users agree on some comparisons and disagree on others, we consider two users who\nagree on the best and worst pet and disagree on the middle pair of rankings over pets. Preferences\nhere are divergent in certain cases (middle pets), and agree in other instances (best and worst pets),\nrequiring multimodal preference modeling. We evaluate our approach on two versions of the dataset:\nPets (Full), and Pets (Divergent) which contains only those prompt and response pairs where the\nusers are divergent (i.e. they have conflicting preferences). For the contexts $\\{(s_a, s'_b, y^i)\\}_{i=1}^N$, we\nrandomly sample 1-4 other prompts and ranked responses from the same user.\nUltraFeedback-P. To construct this dataset UF-P (where P stands for personalized), we use the\nfine-grained scores over different attributes available in the UltraFeedback (UF) [20] dataset to\nconstruct different users, taking a similar approach to prior work [58]. Following prior work, we\nconstruct a dataset with two users, UF-P-2, who prefer either helpfulness or honesty (hidden attribute),\ni.e. they generate preferences using the scores for their chosen attributes. To test the ability of VPL to\nmodel more users than has been previously attempted in the literature, we create UF-P-4, which uses\nthe fine-grained scores over all the four attributes in the UF dataset [20] to create a dataset with four\ndifferent users. Here, the users are divergent because given two responses, users following different"}, {"title": "7.2 Does VPL help to make LLM reward models more pluralistically aligned?", "content": "In Table 1, we see that VPL is able to learn a more accurate reward model across all the datasets,\ncapturing the multi-modality in the language preference data. This indicates that VPL can infer the\nlatent representation of the user's preferences z from a few annotated samples, and successfully adapt\nthe reward model. In contrast, the baselines\u2014including the BTL model typically used in widely\ndeployed RLHF [52] models are unable to fit the datasets because they are unable to account for\ndivergent preferences. Because the datasets are imbalanced, the baselines can sometimes perform\nbetter than random guessing by fitting only the preferences of the majority group. This is why the\nperformance on Pets (Full) appears high, in spite of the fact that the baseline BTL model fails to\nadapt to the divergent preferences."}, {"title": "7.3 Does VPL scale to real-world settings with larger and noisy users?", "content": "A key assumption in our approach is that context\nquestions accurately represent individual users with-\nout noise in the underlying dataset. To test VPL's\nrobustness to noisy context labels at test time, we\ninjected noise by flipping the questions answered by\neach user and evaluated the trained model's accuracy\nin predicting rewards. This experiment can help us\nevaluate how well the model would generalize to new\nusers that have similar preferences to those experi-\nenced during training, but may not answer questions\nin exactly the same way. Figure 7 illustrates that VPL\nis able to outperform prior work even when 25% of\npreference labels are flipped at test time. Notably, we\nobserved that longer context lengths result in more\naccurate reward modeling, even with higher noise\ninjection. This is because the encoder can generate\nmore accurate inferences of the latent distribution\nwhen provided with more user information through\na larger number of context questions and response\nlabels.\nWe note further that when 50% of the preference labels are flipped, the context preference queries\nprovide no information about the user, and the performance of VPL is equivalent to the baseline BTL\nmodel. This mirrors the additional findings presented in Appendix A.6, which demonstrate that when\nVPL is trained on a unimodal preference language dataset, it gives equivalent performance to BTL.\nEssentially, when extra preference data is available for a user, VPL can personalize the reward model\neffectively and attain higher performance. But without that information, it performs just as well as the\ndefault BTL model. These findings, as shown in Table 1,Figure 7, and the Appendix, demonstrate\nVPL's effectiveness in handling multi-modality and noise in large-scale preference datasets, without\ncompromising performance even when no additional preference information is available. This\ncapability suggests the potential for VPL to contribute to the development of next-generation LLMs\nthat are more personalized, inclusive, and efficient."}, {"title": "8 Conclusion", "content": "In this work, we presented VPL, a technique for pluralistic alignment of preference-based RLHF\nmodels via variational inference. We show that VPL can capture diverse preferences, and can be used\nfor steerable personalized model learning while capturing uncertainty and divergence in preferences.\nWe discussed practical considerations for enabling VPL to scale up for LLMs and policy learning and\nshowed results across simulated control problems and LLM-based RLHF, significantly outperforming\ncurrent RLHF techniques.\nLimitations and Future Work. A key limitation of this work is that as yet, realistic preference\ndatasets containing the opinions of diverse users do not yet exist at scale. This limitation necessitated\ncreating our own synthetic preference datasets. Although this was also the approach taken in prior\nwork on personalized RLHF (e.g. [58, 73]), an important direction for future work will be to apply\nVPL to more realistic preference data from diverse groups of users. Further, our current experiments\non the UltraFeedback dataset assume that when adapting to a new user's preference, it is possible to\nask them to provide preferences over a sample from a fixed set of survey questions. In the future, it\nwould be good to relax this assumption so that VPL could be applied to preferences obtained naturally\nduring a conversation with the user."}, {"title": "A Additional Experiments", "content": "A.1 Didactic example on a toy reward learning problem.\nTo more carefully understand the behavior of VPL empirically, we construct a didactic example [23]\nas shown in Figure 8. In this problem, let us consider a mixture of M different annotators providing\npreferences, where each annotator i has a reward function specified by $N(\\mu_i, \\sigma_i) M_{i=1}^M$ that they use to\nassign binary preferences. Mathematically, we sample the preferences from a mixture of Gaussians:\n$P(s_A > s_B | i) = \\frac{e^{r_i(s_A)}}{e^{r_i(s_A)} + e^{r_i(s_B)}}; where e_i ~ \\frac{1}{\\sigma_i \\sqrt{2 \\pi}} e^{-\\frac{1}{2}(\\frac{x - \\mu_i}{\\sigma_i})^2}$\nMulti-annotator preferences are simulated by sampling an annotator from this mixture distribution\nand then assigning binary preferences according to the chosen reward function. We train VPL as\ndescribed in Section 4 to recover the underlying distribution over reward functions. As expected\nin Figure 8, standard RLHF [17] averages over the different modes since it can only represent a\nsingle reward function. While prior work in accounting for hidden context in RLHF (DPL [58])\ncan learn the uncertainty in the reward functions due to hidden context, it is not able to accurately\ndisambiguate different modes. Meanwhile, VPL is able to infer the underlying context using the\napproximate posterior $q_\\psi$ and the recover the individual reward modes through the latent-conditional\nreward function r(s, z).\nA.2 Does scaling rewards help improve performance?\nTo avoid the problem of high variance rewards (Section 4), we com-\npare the performance of VPLno-norm with VPL + SPO. We further\ncompare against two normalizing schemes: VPLbatchnorm where\nthe rewards for each latent is normalized by the mean rewards\nacross a set of state samples i.e. $r'(s, z) = \\frac{r(s,z)}{s'\\in S r(s',z)}$, and\nVPLmax-norm where all the rewards in the offline dataset are nor-\nmalized by the maximum reward for any latent.\nIn Section 4.1, we discuss the problem of generating scaled rewards\nfrom latent variable-based reward models and compare the perfor-\nmance across multiple baselines discussed above. As shown in\nFigure 9, the batch norm scaling generates highly biased estimates\nof the rewards, which is catastrophic for the method. However, VPL\nmethods have decent performance at test-time, but are an unprinci-\npled approach to the scaling problem. Our SPO + VPL presents a\ngeneral method for estimating normalized rewards. Thus, in Figure\n9 we can see that our method outperforms the baseline approaches in terms of success rate. The\nbaselines have an unscaled or a biased estimate of the multi-modal rewards leading to sub-optimal\nperformance. For the ravens-manipulation environment, the dataset doesn't contain sub-optimal\ntrajectories, VPL (with max norm) performs comparably."}, {"title": "A.3 Does VPL scale with the number of diverse users?", "content": "In order to test the effectiveness of VPL in scaling to a control"}]}