{"title": "Flow Matching with General Discrete Paths: A Kinetic-Optimal Perspective", "authors": ["Neta Shaul", "Itai Gat", "Marton Havasi", "Daniel Severo", "Anuroop Sriram", "Peter Holderrieth", "Brian Karrer", "Yaron Lipman", "Ricky T. Q. Chen"], "abstract": "The design space of discrete-space diffusion or flow generative models are significantly less well-understood than their continuous-space counterparts, with many works focusing only on a simple masked construction. In this work, we aim to take a holistic approach to the construction of discrete generative models based on continuous-time Markov chains, and for the first time, allow the use of arbitrary discrete probability paths, or colloquially, corruption processes. Through the lens of optimizing the symmetric kinetic energy, we propose velocity formulas that can be applied to any given probability path, completely decoupling the probability and velocity, and giving the user the freedom to specify any desirable probability path based on expert knowledge specific to the data domain. Furthermore, we find that a special construction of mixture probability paths optimizes the symmetric kinetic energy for the discrete case. We empirically validate the usefulness of this new design space across multiple modalities: text generation, inorganic material generation, and image generation. We find that we can outperform the mask construction even in text with kinetic-optimal mixture paths, while we can make use of domain-specific constructions of the probability path over the visual domain.", "sections": [{"title": "1 INTRODUCTION", "content": "Generative models over discrete spaces have not seen as much progress on the methodology side compared to continuous-space counterparts. For the most part, applications such as large language modeling rely solely on autoregressive models (Radford et al., 2019; Bommasani et al., 2021). The simplicity of autoregressive modeling has also motivated people to use them for multimodal generation, where other modalities, such as images and videos, are tokenized and modeled within an autoregressive framework (Van den Oord et al., 2016; Team, 2024; Sun et al., 2024). While obtaining reasonable results, they have not yet reached the performance of continuous-space generative models such as denoising diffusion (Ho et al., 2020; Song et al., 2021) and Flow Matching models (Lipman et al., 2022; Albergo et al., 2023) for the visual-audio domains (Rombach et al., 2022; Dai et al., 2023; Esser et al., 2024; Zhou et al., 2024), where it is believed that the ability to perform iterative refinement brings significant gains (Saharia et al., 2022; Zhang et al., 2024).\nA promising framework that brings iterative refinement to the discrete case is to consider the use of Markov chains within a dynamical generative framework. Many discrete-space generative flow and diffusion models have seen success in the generation of text (Austin et al., 2021; Lou et al., 2024; Shi et al., 2024; Sahoo et al., 2024; Gat et al., 2024), proteins (Campbell et al., 2024), images (Austin et al., 2021; Shi et al., 2024), and even executable code (Gat et al., 2024). However, the design space of these models is currently rather limited, with many recent works instead focusing solely on the case of masking as a corruption process (Shi et al., 2024; Sahoo et al., 2024). The masked construction is an extension of masked pretraining (Devlin, 2018; Yang, 2019), but it does not fully embody the concept of iterative refinement as it is equivalent to learning autoregressive models for every ordering (Hoogeboom et al., 2021; Chang et al., 2022), and it has been noticed that some of the recent reported progress was actually misleading due to low-precision sampling (Zheng et al., 2024) rather than the explicit design choice of masking as a corruption process. In spite of this, the masked construction has often been found to be the best performing choice out"}, {"title": "2 BACKGROUND: DISCRETE FLOW MATCHING", "content": "We are interested in learning a generative model that approximates a data distribution q(x), where  x = (x_1,x_2,...,x_D) \u2208 S = T^D  with  T = [K] \u2261 {1, 2, . . ., K}  being a discrete set of possible token values, and  D \u2208 N  is number of discrete variables. For brevity and without loss of generality, we consider all dimensions to have the same number of discrete values.\nProbability paths. We denote by p(x) and q(x) the source and target, respectively, probability mass functions (PMFs) over the state space S. We consider probability paths  p_t(x), t \u2208 [0, 1],  to be time-dependent PMFs taking the form\n\\( P_t(x) = \\sum_{x_1 \\in S} P_t(x|x_1)q(x_1), \\text{ where } P_t(x|x_1) = \\prod_{i=1}^D P_t(x^i|x_1^i), \\)\nand  p_t(x^i|x_1^i)  is a conditional probability path which interpolates between a simple PMF at time  t = 0  and a delta PMF centered around  x_1  at  t = 1 . That is, we assume the boundary conditions  p_0(x^i|x_1^i) = p(x^i)  and  p_1(x^i|x_1^i) = \\delta_{x_1^i}(x^i) . Hence we can interpret these probability paths  p_t(x)  in equation 1 as interpolating between a factorized source distribution  p(x) \u2261 \\prod_{i=1}^D p(x^i)  and the data distribution q(x). A common family of probability paths used in previous works is the collection of mixture paths (Gat et al., 2024), with  x^i -dependent schedulers similar to Shi et al. (2024):\n\\( P_t(x^i|x_1^i) = (1 - \\kappa_t(x_1^i))p(x^i) + \\kappa_t(x_1^i)\\delta_{x_1^i}(x^i), \\)\nwhere  \\kappa_0(\u00b7) = 0  and  \\kappa_1(\u00b7) = 1  to satisfy the boundary conditions. Specifically, with  p(x^i) = \\delta_m(x^i)  we recover the masked construction (Shi et al., 2024; Sahoo et al., 2024).\nProbability velocities. As our generative process, we simulate a Continuous Time Markov Chain (CTMC)  (X_t)_{t\u2208[0,1]}  in S such that its time marginals follow a prescribed probability path,\n\\( X_t \\sim P_t. \\)\nIn order to do so, we define the concept of a probability velocity, also known as a rate matrix. We say that a probability velocity  u_t  generates  p_t  if  u_t  characterizes a Markov process  X_t  with marginal  P_t  (equation 3) for all  t \u2208 [0, 1)  in the following sense:\n\\( P(X_{t+h} = x | X_t = z) = \\delta_z(x) + hu_t(x, z) + o(h), \\)\nwhere  o(h)  denotes a function which is asymptotically smaller than h, i.e.,  \\lim_{h\\rightarrow 0}o(h)/h = 0 . Intuitively,  u_t  describes the Markov transition of  X_t  for small step sizes  h > 0 . We note that for equation 4 to be a valid PMF,  u_t  must at least satisfy the Rate Conditions:\n\\( u_t(x, z) \u2265 0 \\text{ for all } x \\neq z \\text{ and } \\sum_x u_t(x, z) = 0 \\)"}, {"title": "Single-variable-change probability velocities.", "content": "It is natural to consider modeling a CTMC process  X_t  over S by defining a  u_t(x, z)  for all pairs  x, z \u2208 S . However, the state space is of size  |T|^D  so this is generally prohibitive for high dimensions. A remedy is to consider rates that only allow a state to change in a single variable (Campbell et al., 2022), e.g., in the following example we only change the variable at the i-th coordinate:\n\\( (z^1,..., z^{i-1}, z^i, z^{i+1}, ..., z^D) \\rightarrow (z^1, ..., z^{i-1}, x^i, z^{i+1},...,z^D). \\)\nTo model only such changes we restrict our attention to velocities of the form  u_t^i(x^i, z)  that describe the probability rate between the state z and the state with the i-th coordinate replaced, i.e., as described in the r.h.s. in equation 6. We can express the full velocity  u_t(x, z)  via  u_t^i(x^i, z)  as\n\\( u_t(x, z) = \\sum_{i=1}^D u_t^i(x^i, z) \\prod_{j \\neq i} \\delta_{z^j}(x^j), \\)\nwhich states the probability velocity between two states  z \u2192 x  is zero if they differ by more than one variable and equal  u_t^i(x^i, z)  if they differ by exactly one variable. Plugging this velocity into equation 4, it can be shown that (Gat et al., 2024):\n\\( P(X_{t+h} = x | X_t = z) = \\prod_{i=1}^D [\\delta_{zi}(x^i) + hu_t^i(x^i, z)] + o(h) \\)\nThis implies we can sample each variable  X_{t+h}^i  independently from the distribution  \\delta_{zi}(x^i) + hu_t^i(x^i, z) , and only incur an error of  o(h) ."}, {"title": "The marginal velocity.", "content": "Previous works (Campbell et al., 2024; Gat et al., 2024) have shown that constructing a generating velocity for  p_t(x)  can be achieved by considering only the conditional probability paths in equation 1. That is, assume we have conditional velocities  u_t^i(x^i, z^i|x_1) , which are velocities in the state space T, that generate the conditional paths  p_t(x^i|x_1^i)  in equation 1. Then a marginal velocity  u(x^i, z)  that generates  p_t(x)  takes the form:\n\\( u(x^i, z) = \\sum_{x_1 \\in T}u_t^i(x^i, z^i|x_1^i)P_{i|t}(x_1|z) \\)\nwhere  p_{i_1|t}(x_1|z)  is the posterior probability of the i-th token taking the value  x_1 , i.e.,\n\\( P_{it}(x\u00b2/z) = \\frac{\\sum_{x_1 \\in S} \\delta_z(x_1) P_t(z/x_1)q(x_1)}{P_t(z)}. \\)\nParameterizing the factorized posterior  \\prod_{i=1}^D P_{it}  is an approach taken by prior works (Austin et al., 2021; Campbell et al., 2022). To train, a simple option is the cross-entropy objective:\n\\( L_{CE}(\\theta) = \\mathbb{E}_{t \\sim U[0,1], x_1 \\sim q(\u00b7), x \\sim p_t(\u00b7|x_1)} [-\\sum_{i=1}^D \\log u_t^i(x^i|x_1^i)]. \\)\nWe use this training loss for general probability paths as it is generally applicable. However, for the case of mixture paths (equation 2) it is possible to derive a tractable ELBO as the marginal  u_t  can be written in closed form without a summation as in equation 9. We cover this later in Section 6."}, {"title": "3 SAMPLE GENERATION THROUGH THE FACTORIZED POSTERIOR", "content": "The most direct approach to sample from this model is to use the marginal velocity  u_t(x^i, z) , e.g., with a first-order sampling scheme defined by removing the o(h) term in equation 8, i.e., given  X_t , we advance time with step size h by sampling  X_{i+h}^i  according to\n\\( X_{i+h}^i \\sim \\delta_{x^i}(\u00b7) + hu_t^i(\u00b7, X_t^i), \\)\nfor each  i \u2208 [D] , where  u  is computed with equation 9. However, for general discrete paths this sampling procedure is intractable for large discrete spaces  T  as computing  u(x^i, z)  with equation 9 for all  x^i \u2208 T  has a computational complexity of  |T|^2 .\nAlternatively, we propose a more efficient sampling scheme by noticing that\n\\( \\delta_{zi}(x^i) + hu_t^i(x^i, z) \\approx \\sum_{x_1^i \\in T} [\\delta_{zi}(x^i) + hu_t^i(x^i, z^i|x_1)] P_{i|t}(x_1, z), \\)\nwhich leads to a sampling process that avoids computing the full marginal velocity: given the current state  X_t , sample  X_1  from the factorized posterior, then sample  X_{t+h} . That is, for each  i \u2208 [D] ,"}, {"title": "4 KINETIC OPTIMAL VELOCITIES AND PROBABILITY PATHS", "content": "We first decouple the design space of probability paths and their generating velocities, providing the means to effectively explore this large design space. This section covers two contributions: (i) we propose a family of kinetic optimal (KO) velocities that generates any given probability path, and (ii) we solve for kinetic optimal probability paths, recovering a special case of mixture paths. The first contribution enables us to work with general discrete probability paths. The second contribution justifies the choice of mixture probability paths used by Gat et al. (2024) but offers novel  x -dependent schedulers. For both, we center our designs based on optimizing a discrete notion of kinetic energy (Peyr\u00e9 et al., 2019).\nNotation. As the discussion in this section applies to arbitrary probability paths and discrete state spaces, we will use a simplified notation, where our state space is now  T  and for states we use  x, z \u2208 T , abusing a bit the previous notation (where  x^i, z^i \u2208 T ). Furthermore, we will denote by  p_t(x)  and  u_t(x, z)  an arbitrary probability path and velocity field in T, respectively.\nContinuity Equation. Given a probability path  p_t(x) , the entire collection of velocities  u_t(x, z)  generating  p_t(x)  are the solutions to the Continuity Equation (a.k.a. the Kolmogorov forward equation) that also satisfy the Rate Conditions. It is useful to formulate the Continuity Equation through the flux  j_t , that is\n\\( \\dot{p}_t(x) + div_x(j_t) = 0, \\quad \\forall x \\in T, \\quad \\text{with } j_t(x, z) = u_t(x,z)p_t(z). \\)\nIntuitively, the flux  j_t(x, z)  quantifies the amount of probability mass per unit of time moving from state z to state x. The divergence operator then measures the total outgoing flux minus the total incoming flux, which in the discrete case takes the form\n\\( div_x(j_t) = \\sum_{z \\neq x}j_t(z,x) - \\sum_{z \\neq x}j_t(x, z). \\)\nVelocity from flux. Given a flux  j_t  satisfying the Continuity Equation (equation 14) we can get a velocity from the flux by defining for  x \\neq z ,\n\\( u_t(x, z) = j_t(x,z)/p_t(z) \\text{ if } p_t(z) > 0, \\qquad \\text{else } u_t(x, z) = 0, \\)\nand the case  x = z  is uniquely set by the Rate Conditions (5),  u_t(z, z) = \u2212 \\sum_{x \\neq z} u_t(x, z) . The velocity defined in this way will satisfy the Continuity Equation and the Rate Conditions if the flux satisfies the following conditions:\n\\( j_t(x, z) \u2265 0, \\text{ for } x \\neq z \\qquad \\text{Non-negativity} \\)\n\\( p_t(z) = 0 \\Rightarrow j_t(x, z) = 0 \\qquad \\text{Safe Flux Condition} \\)\nIntuitively, the Safe Flux Condition ensures no flux is leaving a zero probability state z.\nKinetic optimality. Motivated by the approach employed in the continuous case of minimizing the kinetic energy for the conditional velocities (Lipman et al., 2022; Shaul et al., 2023), we take a similar approach for finding velocities for the discrete case. The standard convex formulation of the kinetic energy adapted to the discrete case is (Peyr\u00e9 et al., 2019):\n\\( \\min_{p_t,j_t} \\int_0^1 \\sum_{x \\neq z} w_t(x, z)j_t(x, z)^2dt \\qquad \\text{Kinetic Energy} \\)\n\\( \\text{s.t. } div_x(j_t) = -\\dot{p}_t(x), \\qquad \\forall x \\in T \\qquad \\text{Continuity Equation} \\)\n\\( j_t(x, z) \u2265 0, \\qquad \\forall x \\neq z \u2208 T \\qquad \\text{Non-negative flux} \\)\n\\( P_0=p, P_1 = q \\qquad \\text{Boundary conditions} \\)"}, {"title": "4.1 KINETIC OPTIMAL VELOCITY", "content": "Assuming  p_t > 0  is fixed in (19), our goal is to find the kinetic optimal solution  j_t , and consequently obtaining a velocity  u  via (16). One observation we make is that (19) can be efficiently solved when symmetric, i.e., when  w_t(x,z) = w_t(z,x) . \n\\( \\sum_{z \\neq x} \\frac{w_t(x,z)}{p_t(z)} [f_t(x) - f_t(z)] = \\dot{p}_t(x), \\qquad \\forall x \\in T \\)\nwhere  f_t : T \u2192 R  is the unknown function over the state space. The linear equation in (21) is of Laplacian form, and many properties (including closed-form solutions) are known in many cases (Vishnoi, 2012). The solution  f_t  to (21) is unique up to a global constant and using  f_t  we construct the kinetic optimal flux,\n\\( j_t(x,z) = \\frac{p_t(z)}{w_t(x,z)} [f_t(x) \u2013 f_t(z)]_+, \\)\nwhere  [s]_+ = max{s,0}  is the ReLU operator. This provides a solution to (19) with a fixed and positive  p_t . Consequently, using (16) we get the kinetic optimal velocity. We have shown that a certain family of kinetic optimal velocities can be computed by solving a linear system (21) for arbitrary probability paths  p_t(x)  over state-space T. Next we will further instantiate this family and provide some closed form solution for  j_t  and  u .\nClosed-form  u_t . We will consider the case where  w_t(x, z) = p_t(z)/(T_t(x)T_t(z)) , and  T_t : T \u2192 R_{\u22650}  is a design choice of our method. To ensure  w_t  is safe (20) we require that  p_t(z) = 0  implies  T_t(z) = 0 . The solution  f_t  to (21) -which can be checked with substitution-is:\n\\( f_t(x) = \\frac{1}{p_t(x)} \\sum_{z \\in T} T_t (z)T_t(x) \\)\nOne choice is  T_t(x) = 1_{[p_t(x)>0]} , that leads to the Kinetic Optimal flux\n\\( j_t^*(x,z) = [\\dot{p}_t(x) \u2013 \\dot{p}_t(z)]_+, \\qquad \\text{for } x \\neq z \\)\nwhich upon converting to velocity via (16) recovers the velocity proposed in Campbell et al. (2024) for positive paths,  p_t > 0 . Note however, that the above flux is not safe (does not satisfy equation 18) and if  p_t(z) = \u03b5 the flux  j_t (x, z)  for some general x is not necessarily small, showing a potential numerical issue. Campbell et al. (2024) formulate a limit case for general  p_t  that also requires adding an extra assumption on  p_t  (that  p_t(x) = 0 \u21d2 \\dot{p_t}(x) = 0 ), which does not hold even for common probability paths that are typically used, such as the masked mixture path with linear schedulers.\nAlternatively, we propose a more numerically stable choice. Consider  T_t(x) = p_t(x) , i.e.,\n\\( w_t(x,z) = 1/p_t(x). \\)\nThis results in  f_t(x) = \\dot{p_t(x)}/p_t(x) , and the kinetic optimal flux in this case is:"}, {"title": "4.2 KINETIC OPTIMAL PROBABILITY PATHS", "content": "Interestingly, for the weighting choice that we have motivated for the numerically stable velocity (25), it is also possible to solve for the kinetic optimal probability path  p_t . \n\\( \\min_{a_t} \\int_0^1 \\sum_x \\dot{a}_t(x)^2 dt \\qquad \\text{Kinetic Energy} \\)\n\\( \\text{s.t. } \\sum_x a_t(x)^2 = 1, \\qquad \\forall t \\in [0, 1] \\qquad \\text{Hypersphere constraints} \\)\n\\( a_0(x) = \\sqrt{p(x)}, a_1(x) = \\sqrt{q(x)} \\qquad \\text{Boundary conditions} \\)\nwhere  a_t(x) = \\sqrt{p_t(x)} . Problem (30) is the kinetic energy of a curve over the hypersphere connecting  \\sqrt{p}  and  \\sqrt{q} . The optimal solution thus corresponds to the geodesic curve on the hypersphere,\n\\( a_t(x) = \\frac{\\sin(1-t)\\Omega}{\\sin \\Omega} \\sqrt{p(x)} + \\frac{\\sin t \\Omega}{\\sin \\Omega} \\sqrt{q(x)}, \\quad \\text{where } \\Omega = \\arccos \\Big(\\sum_{x} \\sqrt{p(x)q(x)}\\Big), \\)\nand consequently the optimal probability path and velocity for (30) are\n\\( p_t^*(x) = a_t(x)^2, \\qquad u_t(x, z) = a_t(x) [\\dot{a}_t log a_t(x) \u2013 \\dot{a}_t log a_t(z)]_+ \\)\nIn the particular case of conditional probability paths  q(x) = \u03b4_{x_1} (x) , we get that the optimal solution recovers the mixture path (equation 2) with a specific  x_1 -dependent scheduler:\n\\( \\kappa_t(x_1) = 1 - \\frac{\\sin^2(1-t)\\Omega(x_1)}{\\sin^2 \\Omega(x_1)} \\quad \\text{where } \\Omega(x_1) = \\arccos \\sqrt{p(x_1)}. \\)\nThis justifies the mixture paths (2) as kinetic optimal, and furthermore, it naturally utilizes an  x_1 -dependent scheduler for general source distributions p when  p(x_1) > 0 ."}, {"title": "5 PROBABILITY-PRESERVING VELOCITIES", "content": "While we have found a particular flux  j_t , the space of fluxes for a given  p_t  is much larger, and in this section we show how to explore it further. We first observe that since the Continuity Equation (14) is a linear equation, any flux  j_t  satisfying this equation can be written as a sum of two fluxes:\n\\( j_t = j_t^* + j_t^+, \\qquad \\text{where } div_x(j_t^+) = 0, \\)\nwhere  j_t^*  is a particular solution to the Continuity Equation and  j_t^+  is a solution to the homogenous version of the equation, i.e., divergence-free. We call the velocity resulting from  j_t^+  a probability-preserving, or corrector, velocity as sampling with this velocity has  p_t  as a steady-state. For simplic-ity, we mainly consider the special case of symmetric flux. Symmetry is a sufficient condition for being divergence-free as is evident from (15). A natural choice for a symmetric flux is to consider a symmetrization of (22) taking the form\n\\( j_t^+(x, z) = \\frac{p_t(z)}{w_t(x,z)} |f_t(x) - f_t(z)|, \\qquad \\text{and } u_t^+(x, z) = j_t^+(x, z)/p_t(z), \\)\nfor any function  f_t . For convenience, we will simply re-use the same  f_t  that comes from optimizing the kinetic energy (19), e.g. the same as in (26). In contrast to the kinetic optimal velocity, which results in a unidirectional flow in the sense that samples will only move from lower  f_t(\u00b7)  to higher  f_t(\u00b7) , the symmetric flux in (35) results in a bidirectional flow that allows equal movement between any two states with non-equal  f_t(\u00b7) . Hence  j_t^+  acts as a corrector to redirect samples back to previous states in a way that leaves  p_t  invariant."}, {"title": "6 ELBO FOR DISCRETE FLOW MATCHING", "content": "We show in Appendix D that we can produce a continuous-time ELBO bound on the likelihood  log p_f(x_1)  for any conditional probability path and conditional probability velocity in terms of the marginal  u(x^i, z)  and conditional  u_i^i(x^i, z^i|x_1)  as follows\n\\( log p_1 (x_1) \u2265 \\int_0^1 \\mathbb{E}_{x_t \\sim p_t(x_1)} \\sum_{i=1}^D [u_t^i(x_i, x_t) \u2013 u_t^i(x_i, x_t^i|x_1)\n+ \\sum_{y \\neq x_t} u_t^i(y^i, x_t^i|x_1) log \\Big(\\frac{u_t^i(y,x_t)}{u_t^i(y,x_t^i|x_1)} \\Big)] dt \\)\nEvaluating this ELBO is difficult for the same reason as sampling in Section 3, for large discrete spaces T computing (9) for all  x^i \u2208 T  has a computational complexity of  |T|^2 . However, for mix-ture paths (2), our conditional velocity resulting from (26) is used to obtain a closed-form expression for the marginal velocity (see Appendix C.2), yielding a tractable ELBO for mixture paths:\n\\( log p_1 (x_1) \u2265 \\int_0^1 \\mathbb{E}_{P_{i|t}(x_i|x_t)} \\sum_{i=1}^D [\\delta_{x_t} (x^i) x(x_i) \u2013 \\sum_{y \\neq x_t} \\delta_m^i(y^i) x(x_i) ]\n+ (1 \u2013 \u03b4_{\u03b1_{x^i}} (\u03b1(x))) x(x_i) (1 + log P_{i|t}(x_i|x_t))]dt , \\)\nwhere  x(x) = \\kappa_t(x)/(1-\\kappa_t(x)) (2).  Specifically for the masked construction, we recover the ELBO used by Zheng et al. (2024) for  x -independent schedulers and used by Shi et al. (2024) for  x -dependent schedulers; see Appendix D.1."}, {"title": "7 RELATED WORK", "content": "Generative modeling through marginalization. Denoising diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2021) construct generative models by reversing a noising process. The Flow Matching framework (Lipman et al., 2022; Albergo et al., 2023; Liu et al., 2022) shares similar traits but instead constructs generative models through a marginalization of conditional Markov processes, allowing a larger design space of probability paths. These types of models can be trained at scale both efficiently and stably relative to other frameworks, and thus have seen massive success in the large-scale generation of images (Rombach et al., 2022; Esser et al., 2024), videos (Singer et al., 2022), and audio (Le et al., 2024; Vyas et al., 2023)."}, {"title": "8 EXPERIMENTS", "content": "We evaluate Discrete Flow Matching (DFM) on multiple modalities: text, crystalline material, and image generation. Our main goal is to show that Discrete Flow Matching can outperform autoregressive models, and within the class of Discrete Flow Matching, we explore new additions such as the kinetic optimal and the metric-induced constructions. In text, we mainly explore the kinetic optimal probability paths (equation 33) with different source distributions, as these have access to the closed-form ELBO (37). In material generation, we find that enabling permutation invariance for DFM easily outperforms autoregressive models at de novo generation, achieving state-of-the-art results. Furthermore, in domains where a natural metric exists, we demonstrate our method's ability to inject inductive bias into the velocity and probability path using equations 27 and 29. We show that our large design space enables competitive results even with non-mask probability paths, showcasing the capabilities of our expanded design space."}, {"title": "8.1 TEXT GENERATION", "content": "We explore our method on the task of text generation. We use the kinetic optimal probability path as in equation 33, which only has one hyper-parameter, the source distribution p(x). For source distribution, we compute the statistics of tokens appearances in the training data Pstats (x\u00b2) and construct a single-parameter family of source distributions:\n\\( p(x) = \\prod_{i=1}^D P(x^i), \\quad p(x^i) = \\text{softmax}(-\\beta_0 log P_{stats} (x^i)), \\)"}, {"title": "ALWAYS-VALID SAMPLING SCHEME", "content": "The second step of the sampling scheme defined in Section 3 requires the condition  h\u2264 1/(2|u_t^i(z^i,z^i,x_1)|)  to be a valid PMF, allowing only small step sizes. To avoid this constraint on h, we use an alternative first-order sampling scheme. We replace step 2 from Section 3 with\n\\( \\text{2) Sample } X_{t+h}^i \\sim e^{-h\\lambda^i(X_t|X_1)} \\delta_{X_t^i}(\u00b7) + (1 - e^{-h\\lambda^i(X_t|X_1)}) \\frac{\\lambda(X_t|X_1)}{\\lambda(X_t|X_1)} (1 \u2013 \\delta_{X_t^i}(\u00b7)), \\)\nwhere  \\lambda(X_t|X_1) = |u_t(X_t, X_1|X_1)| .\nInterpreting this expression,  e^{-h\\lambda^i(X_t|X_1)}  is the probability the state does not change. If we do not change state, then we sample from  \\delta_{X_t^i}(\u00b7) . If we do change state, then we sample from  \\frac{\\lambda(X_t|X_1)}{\\lambda(X_t|X_1)} (1 \u2013 \\delta_{X_t^i}(\u00b7)) , which is a normalized distribution over all states not equal to  X_t^i .\nThis is still a first-order sampling scheme, i.e. it is o(h) error from  P(X_{t+h}^i | X^i) . However, unlike the simple Euler procedure, this alternative is always a valid PMF for any step size h."}, {"title": "SYMMETRIZED KINETIC OPTIMIZATION PROBLEM", "content": "Consider  p_t > 0  and assume  \\frac{w_t(x,z)}{p_t(z)"}]}