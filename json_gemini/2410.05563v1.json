{"title": "RATIONAL METAREASONING FOR LARGE LANGUAGE MODELS", "authors": ["C. Nicol\u00f2 De Sabbata", "Theodore R. Sumers", "Thomas L. Griffiths"], "abstract": "Being prompted to engage in reasoning has emerged as a core technique for using large language models (LLMs), deploying additional inference-time compute to improve task performance. However, as LLMs increase in both size and adoption, inference costs are correspondingly becoming increasingly burdensome. How, then, might we optimize reasoning's cost-performance tradeoff? This work introduces a novel approach based on computational models of metareasoning used in cognitive science, training LLMs to selectively use intermediate reasoning steps only when necessary. We first develop a reward function that incorporates the Value of Computation by penalizing unnecessary reasoning, then use this reward function with Expert Iteration to train the LLM. Compared to few-shot chain-of-thought prompting and STaR, our method significantly reduces inference costs (20-37% fewer tokens generated across three models) while maintaining task performance across diverse datasets.", "sections": [{"title": "INTRODUCTION", "content": "Large language models (LLMs) rely on substantial computational power to handle complex problems (OpenAI et al., 2024; Chowdhery et al., 2022; de Vries, 2023). While initial studies mostly focused on the cost of training (Verdecchia et al., 2023), LLMs' widespread deployment has made inference-time costs an increasingly important factor. Model compression techniques such as quantization, pruning, and knowledge distillation can lower post-training costs (Wan et al., 2024). However, there is a fundamental tension between inference cost and task performance: while many of these methods reduce costs at the expense of performance, others, such as chain-of-thought prompting (CoT; Wei et al., 2023; Kojima et al., 2023), do the opposite, raising inference costs to enhance task performance (Snell et al., 2024). It is worth noting that none of the previous approaches are adaptive: model compression modifications and existing CoT methods tend to raise or lower the inference cost on all queries, regardless of task complexity.\nIn stark contrast to this static tradeoff, humans are able to adaptively allocate computational resources based on task difficulty (Kr\u00e4mer, 2014; Russell, 1997; Lieder & Griffiths, 2017). In this work, we draw inspiration from rational metareasoning \u2013 literally, reasoning about reasoning \u2013 a concept originally from the artificial intelligence literature (Russell & Wefald, 1991) that has been used to explain how humans adaptively manage computational resources (Lieder & Griffiths, 2017; Lieder et al., 2018; Griffiths et al., 2019a). Building on this, we develop a novel reward function based on the Value of Computation (VOC; Russell & Wefald, 1991), which formalizes the trade-off between inference cost and task performance. We adopt an iterative reinforcement learning process inspired by the Expert Iteration algorithm (Anthony et al., 2017). In each iteration, we generate multiple reasoning chains for each question. These reasoning chains are ranked using"}, {"title": "RATIONAL METAREASONING", "content": "Unlike artificial intelligence, humans have limited time and cognitive resources (Griffiths et al., 2019b; Griffiths, 2020). We face diverse challenges requiring different approaches: avoiding a sudden obstacle while driving needs quick, intuitive thinking, while selecting a retirement investment strategy requires slow, deliberate reasoning (Kr\u00e4mer, 2014). Rational metareasoning (Russell & Wefald, 1991) suggests agents should adapt their reasoning based on the problem at hand. Intuitively, while reasoning solves a problem, metareasoning solves the problem of how to solve a problem: deciding which computations to perform while problem-solving. The essence of rational metareasoning is calculating the value of computation (VOC; Russell & Wefald, 1991) for each potential computation. The VOC balances the benefit of computation c (characterized by the expected increase in the agent's eventual utility) against its cost (usually time or energy).\nTo formalize this, agents are assumed to have some internal belief state $b \\in B$, which determines their expectation about the value of each action $a \\in A$: $E[U(a)|b]$. A rational agent would simply choose the highest-value action: $a^* = arg max_{a \\in A} E[U(a)|b]$. In contrast, a meta-rational agent can perform computation to change their belief state before choosing an action. Each computation $c \\in C$ updates the agent belief to $b'$ with probability $P(b'|c)$, which in turn affects their beliefs about the value of actions. However, each computation has an associated cost (cost(c)). The VOC then quantifies the value of performing computation e given a starting belief state b,\n$\\text{VOC}(c,b) = E_{P(b'\\vert c)}[\\max_{a'}{E[U(a')\\vert b']} - \\max_{a}{E[U(a)\\vert b]}] - \\text{cost}(c).$\nThus, a meta-rational agent should pursue the computation $c^*$ with the highest VOC: $c^* = arg max_{c \\in C} \\text{VOC}(c,b)$. If no computation has positive VOC, the agent should stop thinking and act in the world. Rational meta-reasoning can explain how humans allocate cognitive resources in various tasks (Lieder & Griffiths, 2017; Lieder et al., 2018; Callaway et al., 2018; 2021; 2022; Russek et al., 2022)."}, {"title": "RATIONAL METAREASONING WITH LARGE LANGUAGE MODELS", "content": "To achieve an optimal balance between performance and efficiency, our approach introduces a new VOC- inspired reward function (Eq. 3.1) into an Expert Iteration training loop (Anthony et al., 2017; Zelikman et al., 2022), fine-tuning a LLM to produce reasoning chains adaptively depending on task difficulty."}, {"title": "REWARD MODELING", "content": "Chain-of-thought prompting is a technique in which a language model is encouraged to generate an intermediate output - a \"chain of thought\" \u2013 prior to producing the answer to a question (Wei et al., 2023; Kojima et al., 2023) We define the reward of a chain of thought as the difference between its utility and its cost,\n$R_\\pi(x, y, z) = U_\\pi(z|x, y) - C(z)$\nwhere x denotes the input for the task, z represents the chain of thought, and y is the target solution. The utility of the chain of thought is represented by $U_\\pi (z|x, y)$, and the cost of the intermediate computations is denoted by $C(z)$. Here, utility quantifies the increase in the likelihood of generating the target sequence y when the chain of thought z is added to the input x, under the policy \u03c0:\n$U_\\pi (z|x, y) = log \\pi_\\theta(y|z, x) - log \\pi_\\theta(y|x)$.\nSpecifically, $\\pi_\\theta (y|z, x)$ indicates the probability of generating the target sequence y given both the chain of thought z and the input x, while $\\pi_\\theta(y|x)$ denotes the probability of generating y with only the input x. The cost is directly proportional to the logarithm of the number of tokens in the chain of thought l(z):\n$C(z) = \\gamma log l(z)$.\nThe hyperparameter \u03b3 scales the cost and utility to the same magnitude. We tested both a linear and logarithmic cost function, finding that the logarithmic version was more stable and produced better results. A key benefit of this reward function is that it is parameterized by the same weights \u03b8 as the generative policy $\u03c0_\u03b8$, eliminating the need for an external reward model. This allows for direct estimation of the utility of a reasoning chain using the policy itself."}, {"title": "COT GENERATION", "content": "We begin with a pretrained language model $\u03c0_\u03b8$ and an initial dataset of problems x along with their corresponding correct final answers y: $D = \\{(x_i, y_i)\\}_{i=1}^N$. Following prior work in online RL (Tang et al., 2024), we utilize the model itself to generate the reasoning chains, with few-shot prompting as a guide. Specifically, we concatenate a small set of examples, denoted as P, each containing intermediate reasoning chains z, to each example in D. For each task $T_i = (x_i, y_i)$ in the original dataset D, we generate K reasoning chains: $\\hat{T_i} = \\{(x_i, z_{k,i}, y_i)\\}_{k=1}^K$\nIf none of the K generated reasoning chains for a task $T_i$ leads to the correct answer, we discard all samples. To minimize the likelihood of this outcome, we adopt the rationalization approach introduced by STaR (Zelikman et al., 2022): if the model fails to generate the correct answer, we generate a new reasoning chain by supplying the model with the correct answer in the prompt. This allows the model to reason backward: given the correct answer, it can more easily generate a relevant reasoning chain. Finally, we assess each reasoning chain using the Rational Metareasoning reward function (Equation 2)."}, {"title": "\u039c\u0395\u03a4AREASONING TRAINING", "content": "We demonstrate the effectiveness of our reward using a variation of the Expert Iteration algorithm (\u0395\u0399, Anthony et al., 2017). EI is known for its sample efficiency and strong performance on reasoning tasks"}, {"title": "EXPERIMENTS", "content": "We now detail the datasets (Sec. 4.1), baselines (Sec. 4.2), and training (Sec 4.3) used to evaluate our method."}, {"title": "DATASETS", "content": "In our efforts to develop a general-purpose reasoning model, we applied our training process and assessed its effectiveness across a diverse range of datasets and reasoning tasks. We constructed our training set by combining the training sets from these datasets into one dataset D and then evaluated the model on all corresponding test sets T. Below is a detailed description of the datasets used:\n\u2022 ARC (Clark et al., 2018). The AI2 Reasoning Challenge (ARC) dataset comprises grade-school science questions, designed to evaluate a model's capability to apply scientific knowledge.\n\u2022 CommonsenseQA (Talmor et al., 2019). This dataset is centered on commonsense question answering. It leverages implicit human knowledge that is commonly known and sensible, testing the model's ability to provide answers based on everyday reasoning."}, {"title": "BASELINES", "content": "We illustrate the advantages of our model by comparing its performance to two types of prompting strategies: Direct prompting, where the model is required to provide an immediate answer, and Chain of Thought prompting (CoT), where the model is encouraged to reason through the problem step-by-step before arriving at a solution. Since we are using pretrained models which are not specifically trained for instruction following, we provide five few-shot examples for each task from the unused portion of the training dataset. These examples are carefully chosen to ensure that the length of the reasoning chain matches the perceived difficulty of the question. In addition to these prompting methods, we adopt a finetuning baseline, comparing our method to STaR (Self-Taught Reasoner; Zelikman et al. 2022), which also uses the expert iteration algorithm."}, {"title": "TRAINING DETAILS", "content": "For our experiments, we use Microsoft Phi-2 (Javaheripi et al., 2023), Meta Llama-3-8B (Dubey et al., 2024), and Mistral-7B-v0.3 (Jiang et al., 2023) as the pretrained base models. We sample K = 4 reasoning chains for each question, using a temperature t of 0.5 and a topp value of 0.9. These parameters are chosen to balance the exploration and exploitation trade-off, allowing us to generate diverse yet relevant reasoning chains. For each iteration n, we sample a dataset $D_n$ of size 512 from the union of four training datasets D described in 4.1. In the self-supervised fine-tuning step, we use a batch size of 16 and a learning rate of 1e-6. We believe that further improvements are possible through a more comprehensive hyperparameter search; however, due to computational constraints, we leave this for future work. Finally, we evaluate all models using greedy decoding to ensure consistent and deterministic output generation. We use pattern matching techniques to extract the answers; an exact match between the generated answer and the ground truth is considered correct."}, {"title": "RESULTS", "content": "We first evaluate our approach against baselines (Sec. 4.2) across several datasets (Sec. 4.1). Our key criteria are performance (measured by accuracy) and cost (measured by the number of input and output tokens).\nOur experiments confirm that across all three models and four datasets, our training approach reduces cost while matching or improving performance (see Table 1 for results averaged across datasets; Tables 4 and 5 for per-dataset results; and Appendix A.1 for example reasoning chains). Fig. 1 shows these results for all three models: the left panel compares cost across different baselines and datasets, while the right compares"}, {"title": "PERFORMANCE VS COST", "content": "We first evaluate our approach against baselines (Sec. 4.2) across several datasets (Sec. 4.1). Our key criteria are performance (measured by accuracy) and cost (measured by the number of input and output tokens).\nOur experiments confirm that across all three models and four datasets, our training approach reduces cost while matching or improving performance"}, {"title": "ADAPTIVE COMPUTATION", "content": "Section 5.1 demonstrates that our method reduces computational costs on average. But does it actually teach models to reason adaptively (by adjusting reasoning to match task complexity), or just to reason less?\nTo address this question, we first divided our test set T based on whether or not reasoning was needed to obtain the correct answer. We split the data based on whether Direct Few-Shot on Phi-2 obtained the correct answer. This yielded a split of 4412 \"easy\" and 2700 \"hard\" examples. Adaptive methods should use less computation to solve the easy problems.\nWe can empirically compare the results across methods for these two data splits. As shown in Table 2, all models and methods are able to differentiate between hard and easy problems, generating fewer tokens on easier problems. Intriguingly, STaR decreases the difference in reasoning length between hard and easy problems, suggesting that it trains out this adaptive tendency, biasing the model towards using reasoning on all problems. In contrast, Rational Metareasoning increases the difference in reasoning between hard and easy problems, achieving a length reduction of up to 74.4% on Mistral 7B. This indicates that our approach trains models to reason adaptively, helping them recognize when detailed reasoning is necessary and when a shorter response is sufficient."}, {"title": "GENERALIZATION", "content": "We assess the out-of-distribution generalization of our method using the MMLU benchmark (Hendrycks et al., 2021, see Section 4.1). As presented in Table 3, our approach achieves comparable performance while generating 28.8% to 54.8% fewer tokens compared to STaR.\nAmong the 57 subcategories of MMLU, Fig. 2 highlights four\u2014high school chemistry, mathematics, U.S. history, and macroeconomics\u2014that effectively showcase the strengths of our model. We specifically chose two subjects (U.S. history and macroeconomics) where the use of chain-of-thought (CoT) reasoning appears to be counterproductive compared to Direct Few-Shot prompting, as well as two subjects (mathematics and chemistry) where the benefits of CoT are more pronounced. Consistent with our findings from the four in-domain datasets, we observe that the reduction in output length achieved by our method is inversely related to the benefits gained from intermediate reasoning chains. In the cases of U.S. history and macroeconomics, our method results in a substantial reduction in generated tokens. In contrast, in mathematics and chemistry, the reduction in token count is smaller."}, {"title": "RELATED WORK", "content": "The rising cost of deploying large language models (LLMs) has driven efforts to reduce inference costs.\nTechniques such as Speculative Decoding (Leviathan et al., 2023) and Medusa (Cai et al., 2024) improve efficiency through parallelization, while Mixture of Experts (Jacobs et al., 1991; Zhou et al., 2022) activates only a subset of LLM parameters during decoding. Though effective, these methods require significant architectural changes and don't adapt computation based on task difficulty. Other approaches have developed neural architectures that enable adaptive computation (Graves, 2017; Banino et al., 2021; Dehghani et al., 2019; Mohtashami et al., 2023; Schuster et al., 2022) but involve new architectures or training methods. In contrast, our approach uses existing architectures and pretrained models, modifying only the fine-tuning process. More similar to our approach, model routing (Ong et al., 2024; Jiang et al., 2024) optimizes resource utilization based on query complexity by routing easier queries to smaller models and harder queries to larger ones. However, this necessitates multiple models and a router, while our approach trains a single model to adaptively adjust its own outputs to match task complexity."}, {"title": "REDUCING INFERENCE COSTS", "content": "The rising cost of deploying large language models (LLMs) has driven efforts to reduce inference costs. Techniques such as Speculative Decoding (Leviathan et al., 2023) and Medusa (Cai et al., 2024) improve efficiency through parallelization, while Mixture of Experts (Jacobs et al., 1991; Zhou et al., 2022) activates only a subset of LLM parameters during decoding. Though effective, these methods require significant architectural changes and don't adapt computation based on task difficulty. Other approaches have developed neural architectures that enable adaptive computation (Graves, 2017; Banino et al., 2021; Dehghani et al., 2019; Mohtashami et al., 2023; Schuster et al., 2022) but involve new architectures or training methods. In contrast, our approach uses existing architectures and pretrained models, modifying only the fine-tuning process. More similar to our approach, model routing (Ong et al., 2024; Jiang et al., 2024) optimizes resource utilization based on query complexity by routing easier queries to smaller models and harder queries to larger ones. However, this necessitates multiple models and a router, while our approach trains a single model to adaptively adjust its own outputs to match task complexity."}, {"title": "REASONING IN LLMS", "content": "Techniques such as Chain of Thought (CoT) and related methodologies (Wei et al., 2023; Yao et al., 2023; Yasunaga et al., 2024; Madaan et al., 2023; Zheng et al., 2024) have proven effective at enhancing LLM performance across a wide range of tasks. CoT boosts LLMs' performance on complex reasoning by guiding them through a series of intermediate reasoning steps, increasing inference costs to improve task performance. This method can be implemented through in-context learning (Wei et al., 2023), prompting (Kojima et al., 2023), or training (Li et al., 2023). The benefits of CoT can be attributed to both a greater computation depth (Goyal et al., 2024; Pfau et al., 2024) and the semantic values of the thought tokens, which function as"}, {"title": "CONCLUSION", "content": "In this work, we introduced a novel method inspired by previous research in AI and cognitive science aimed at reducing inference costs in LLMs. We operationalize the concept of rational metareasoning \u2013 which formalizes humans' adaptive use of cognitive resources with a novel reward function based on the Value of Computation (VOC). This reward function trains the LLM to optimize the use of intermediate reasoning steps in task execution, enabling it to balance task performance with computational efficiency. Empirically, we find that this approach significantly reduces the number of generated tokens and input context length while maintaining comparable performance across diverse datasets.\nHowever, we note several limitations to our approach. First, the focus of our work here is enhancing the efficiency of large language models in reasoning tasks, rather than the overall performance of such systems. Thus, we aimed to reduce unnecessary intermediate reasoning steps, rather than improve the quality of reasoning per se. It remains to be seen whether our approach can be extended to enhance task performance in addition to computational efficiency. Additionally, we tested our approach on well-established datasets in science, commonsense reasoning, and math, but it remains untested in more specialized domains. Broader testing could help assess its generalizability and effectiveness across diverse contexts.\nMost excitingly, our work demonstrates how cognitively-inspired reward functions can endow LLMs with desirable inference-time properties, opening a broad avenue of future work. Given its flexibility, this method could be integrated into instruction tuning to potentially enhance performance, even in scenarios where verifying the correctness of answers is challenging. Since the utility measure within the reward function can be tailored to prioritize any desired, measurable property, this approach offers the potential to guide models toward achieving these enhanced qualities while still benefiting from the reduced computational costs."}]}