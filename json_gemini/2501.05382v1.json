{"title": "Large Physics Models: Towards a collaborative approach with Large Language Models and Foundation Models", "authors": ["Kristian G. Barman", "Sascha Caron", "Emily Sullivan", "Henk W. de Regt", "Roberto Ruiz de Austri", "Mieke Boon", "Michael F\u00e4rber", "Stefan Fr\u00f6se", "Faegheh Hasibi", "Andreas Ipp", "Rukshak Kapoor", "Gregor Kasieczka", "Daniel Kosti\u0107", "Michael Kr\u00e4mer", "Tobias Golling", "Luis G. Lopez", "Jesus Marco", "Sydney Otten", "Pawel Pawlowski", "Pietro Vischia", "Erik Weber", "Christoph Weniger"], "abstract": "This paper explores ideas and provides a potential roadmap for the development and evaluation of physics-specific large-scale AI models, which we refer to as Large Physics Models (LPMs). These models, based on foundation models such as Large Language Models (LLMs) - trained on broad data are tailored to address the unique demands of physics research. LPMs can function independently or as part of an integrated framework. This framework can incorporate specialized tools, including symbolic reasoning modules for mathematical manipulations, frameworks to analyse specific experimental and simulated data, and mechanisms for synthesizing insights from physical theories and scientific literature. We begin by examining whether the physics community should actively develop and refine dedicated models, rather than relying solely on commercial LLMs. We then outline how LPMs can be realized through interdisciplinary collaboration among experts in physics, computer science, and philosophy of science. To integrate these models effectively, we identify three key pillars: Development, Evaluation, and Philosophical Reflection. Development focuses on constructing models capable of processing physics texts, mathematical formulations, and diverse physical data. Evaluation assesses accuracy and reliability through testing and benchmarking. Finally, Philosophical Reflection encompasses the analysis of broader implications of LLMs in physics, including their potential to generate new scientific understanding and what novel collaboration dynamics might arise in research. Inspired by the organizational structure of experimental collaborations in particle physics, we propose a similarly interdisciplinary and collaborative approach to building and refining Large Physics Models. This roadmap provides specific objectives, defines pathways to achieve them, and identifies challenges that must be addressed to realise physics-specific large scale AI models.", "sections": [{"title": "1 Introduction", "content": "Traditionally, Machine Learning models in physics were narrowly focused and designed for specific tasks such as distinguishing signal events from background events in particle accelerator experiments, predicting the mass of a particle given detector data, discovering materials, or identifying celestial objects [1, 2, 3, 4, 5, 6, 7]. While some of these models were (and still are) effective in performing the specific tasks they were designed for, they lacked the versatility to be applied beyond their original domains. In contrast to domain and application specific models, Large Language Models (LLMs) such as GPT-4 [8], Claude [9], Gemma [10] and Llama [11], are versatile and able to analyse and react to text, images, computer code, and data in general, across various domains, and with remarkable proficiency [12, 13].\nUnlike narrow models, which are primarily used in data analysis, LLMs can enhance a broad range of research activities. Scientific research can be regarded as an interconnected network of processes aimed at advancing scientific understanding. In this network, activities such as hypothesis generation, experimentation, data analysis, and model or theory development continually feed back into one another in an iterative, dynamic fashion. The successful integration of LLMs into these workflows requires not only a careful evaluation of the skills they bring to such activities but also of their alignment with desirable epistemic values (e.g., accuracy, coherence, and explanatory power) and non-epistemic values (e.g., societal impact and ethical considerations). As was the case with the use of computer-based research, LPMs could also help us overcome the limits of our human capabilities. For instance, they may help speed up scientific progress in general, deepening our scientific understanding, which is one of the ultimate goals of science, enabling us not only to make accurate predictions but also to grasp why things are the way they are [14].\nFor instance, they can aid the generation of hypotheses by serving as brainstorming partners, and in doing so may offer novel approaches to complex problems in physics and inspire creativity, acting as an 'artificial muse\u2019[15] for researchers. For"}, {"title": "2 Does the physics community need specialised large scale physics-specific AI models?", "content": "The integration of LLMs and foundation models into physics research appears to be a promising and inevitable development. However, it raises the question of whether the physics community should actively develop and refine these AI models or rely on commercial ones. On the one hand, large commercial LLMs offer several advantages. These models are typically well-funded, benefiting from the vast computational resources and specialised AI expertise found in industry [47]. Commercial LLMs are also designed to be versatile and adaptable, potentially facilitating their application across various subfields of physics. Moreover, relying on commercial models could allow physicists to focus on their core research rather than diverting time and resources to AI development.\nOn the other hand, there are strong reasons for the physics community to take an active role in developing LLMs tailored to their specific needs. Physics research often involves complex mathematical formulations, symbolic reasoning, and domain-specific knowledge that may not be adequately captured by general-purpose commercial models [48, 49]. By developing models in house, the physics community can better align these tools with the unique methodologies, standards, and methodological considerations of their field [50, 51, 52]. This level of control and customization could lead to more accurate, interpretable, and trustworthy results [53]. Moreover, maintaining control over the training data, model architectures, and evaluation processes, the community can ensure adherence to scientific principles such as reproducibility and peer review [54, 31, 55].\nWeighing these considerations, we believe that the ideal scenario would be for the physics community to take a leading role in developing LLMs and foundation models tailored to their needs. This approach would allow for the creation of AI tools that are finetuned to the complexities of physics research, aligned with the community's values and standards, and optimised for advancing fundamental knowledge and understanding.\nWe acknowledge that this ideal scenario faces significant challenges, particularly in terms of funding and resources. Developing and maintaining cutting-edge AI models requires substantial computational power, specialised expertise, and ongoing investment. To mitigate these challenges, we propose a multifaceted approach. The physics community should actively seek out strategic collaborations with industry partners. By leveraging the computational resources and expertise of commercial companies while maintaining a leadership role in model development, the community can see to it that the resulting tools are tailored to its needs while benefiting from the scale and capabilities of industry. Furthermore, the community should explore innovative funding models, such as targeted grants, partnerships with philanthropic organisations, support by large international research organizations or laboratories (e.g. CERN) and cross-disciplinary collaborations that pool resources and expertise.\nAdditionally, adopting a collaboration structure similar to those used in particle physics for experimental analysis may provide an effective framework for building physics-specific Als. Such a structure promotes the pooling of resources, the sharing of expertise and involvement across institutions, and the coordination of efforts on a large scale, which are all crucial for tackling the complex challenges associated with developing large-scale AI models. The development of open-source platforms and shared infrastructure could help spread the costs and foster a collaborative ecosystem around physics-specific LLMs.\nWhile the rapid pace of commercial AI development creates a \"dead on arrival\" risk - where physics-specific models could become obsolete before completion due to faster industry development cycles - several factors make this challenge manageable. The physics community possesses unique advantages that create a natural moat protecting the enduring value of specialized models: domain expertise in"}, {"title": "3 How to get an LPM: Key Pillars", "content": "To address the challenges in developing LPMs and to harness their potential for advancing physics, we propose a roadmap structured around three critical pillars: Development, Evaluation, and Philosophical Reflection. This roadmap is designed specifically for the physics community, but should in principle be applicable across other scientific disciplines.\nThe Development pillar focuses on creating robust LPMs capable of handling the complexities of physical theories, data, and natural language. This involves interdisciplinary collaboration to safeguard that the models are not only powerful but also well-suited to the specific needs of physical research. Key objectives within this pillar include: (i) developing foundational models tailored to physics, (ii) curating high-quality, diverse physics datasets, (iii) integrating physics-specific knowledge and reasoning capabilities, (iv) enabling interaction with physical databases and simulators, and (v) continuously updating models to keep pace with scientific progress. Underlying all these is perhaps the most important component: (vi) the development of collaboration platforms.\nThe Evaluation pillar is dedicated to assessing the accuracy, reliability, efficiency, and effectiveness of LPMs, which involves testing and benchmarking. Evaluators play an important role in validating the models, improving the trustworthiness and scientific soundness of their outputs. Key focus areas within this pillar include: (i) developing physics-specific benchmarks and evaluation protocols, (ii) assessing"}, {"title": "4 Development Pillar", "content": "The development pillar focuses on the construction of LPMs that can navigate the complex landscape of physical theories, physical simulators, and real-world experimental conditions while simultaneously mastering science-specific tasks such as symbolic reasoning, hypothesis generation, and the interpretation of complex data sets.\n4.1 Objectives\nThe primary goal is to develop foundation and LLM models tailored to the needs of science (see [49], in chemistry see [48, 46, 57, 23]). In the context of physics, this involves pretraining on physics data and fine-tuning using curated datasets that comprise scientific papers, textbooks, and problems from physics. Consequently, this process should aim at teaching the model the language nuances, theories, and problem-solving strategies specific to physics.\nTo be of value to the physics community, models must be equipped with the ability to analyse experimental data, perform simulations, and compare simulated data with experimental physical data [58]. Equally, this requires writing domain-specific computing code and accessing mathematical and statistical software tools or the ability to access other foundation models tailored to the analysis of specific experimental data. Since mathematics is the language in which physics underpins its theories and principles, close collaboration with mathematicians and symbolic computation experts will be valuable. This cooperation can guide the model's ability to process and generate symbolic representations, perform algebraic manipulations, and apply advanced mathematical methods such as calculus, linear algebra, and tensor operations. Here, collaboration with the symbolic math in AI community will be an important step to advance physics AI models (see, e.g., [59, 60].\nGiven the wide range of subfields within physics, it will be necessary to develop models that meet the specific requirements of each domain. Incidentally, this could involve creating specialized multi-purpose foundation models for particle physics, astrophysics, condensed matter physics, and other areas, each trained on domain-specific data and equipped with the necessary knowledge and problem-solving capabilities, with a common pre-training and subsequent specialised modules. For instance, foundation models tailored to particle physics could focus on tasks such as event classification, detector simulation, or reconstructing collision events, leveraging the unique data generated by particle accelerators. Similarly, an astrophysics-focused model [61] could analyze large-scale cosmological simulations, process observational data from telescopes, or assist in interpreting phenomena like gravitational waves and exoplanetary systems. As a first step, building small-scale demonstrators for specific tasks within each subfield would provide proof of concept, allowing researchers to evaluate feasibility, performance, and potential impact. Recently, preliminary models for particle physics [62] and heavy-ion collision experiments [63] have been proposed, demonstrating early approaches and ideas towards foundation models that can be pretrained and fine-tuned or used in versatile ways. The domain-specific models would not only enhance performance by concentrating on the unique challenges of each field but could also be integrated into a larger, interdisciplinary framework, enabling cross-domain insights and collaborative problem-solving (see Figure 2).\nIn particular, we envision future scientific research being enhanced by the concept of an 'AI physicist,' a system of interconnected foundation models tailored to tackle specific research tasks across various domains of physics [19]; [64],p. 20). This strategy draws inspiration from the division of labour in collaborative research teams, aiming to increase productivity and innovation in physics research. This is also inline with recent advancements in the orchestration of multiple LLM to perform complex question answering tasks [65, 66]. Accordingly, the network encompasses specialised foundation models (e.g., trained on the corpus of open-access peer-reviewed journals, on the data of the Large Hadron Collider (LHC), Gravitational Wave Physics (GW), and Astrophysics) that handle literature synthesis, data analysis and simulations, result visualisation, and scientific paper composition, covering the full spectrum of research activities. In this context, agents are autonomous AI systems that can perceive their environment, make decisions, and take actions to achieve specific goals. These agents can operate in coordination, supported by APIs and frameworks for seamless information exchange, while human researchers use the models and guide and refine the process, improving alignment with genuine scientific inquiry objectives.\nIdeally, (conversational) LPMs should be capable of generating new hypotheses,"}, {"title": "4.2 Challenges and Methods", "content": "The development of physics-specific large-scale AI models presents a unique set of challenges, ranging from data curation and processing to model design, high-performance computing (HPC) and evaluation. These challenges are closely interrelated and some require innovative approaches and interdisciplinary collaboration to meet the diverse needs of physics research. In what follows, we briefly discuss some of these challenges and outline initial approaches to address them. While these efforts provide a basic perspective, further extensive work is required to fully address these issues, particularly through the development of initial demonstrator models.\nCurating high-quality, diverse datasets is a significant challenge in developing LPMs, especially when handling experimental and simulated data. These data"}, {"title": "5 Evaluation Pillar", "content": "The Evaluation Pillar is tasked with assessing the accuracy, reliability, and effectiveness of LPMs. Through testing and benchmarking, evaluators play a fundamental role in validating the models and contributing to making their outputs more trustworthy and scientifically sound.\n5.1 Objectives\nThe primary objective of the Evaluation Pillar is to assess LPMs' capabilities, such as scientific reasoning and discovery. Accordingly, evaluators must develop benchmarks that test core physics knowledge, mathematical abilities, and research aptitude. These benchmarks should cover a wide range of physics subfields and difficulty levels, from basic concepts to advanced research-level problems.\nTo develop robust and generalizable (conversational) LPMs, it is important to evaluate their performance under distribution shift and on out-of-domain problems [94, 48, 95]. The models are then tested on data sets and tasks that differ from their training data to assess their ability to adapt and apply their knowledge to new situations. Additionally, evaluators should measure model robustness to input perturbations, adversarial attacks, and corrupted data, ensuring that the models can handle noisy and imperfect inputs that may be encountered in real-world research settings.\nAssessing model calibration and uncertainty quantification on experimental data analysis tasks is another critical objective of the Evaluation Pillar. LPMs should not only provide accurate predictions but also express appropriate levels of confidence in their outputs. Consequently, evaluators must develop methods to measure the alignment between model predictions and real-world physics data and theories, seeing to it that the models generate scientifically valid and reliable results.\nComparing the performance of LPMs to that of human experts on complex, research level challenges can help with understanding the extent to which these models can augment human capabilities. Equally, evaluators should design benchmarks that require a combination of deep physics knowledge, creativity, and problem-solving skills, pushing the boundaries of what AI can achieve in scientific discovery. By quantifying the efficiency gains and acceleration to scientific workflows enabled by LPMs, evaluators can demonstrate the practical value of these models in streamlining research processes and accelerating progress.\nInvestigating the theoretical underpinnings of artificial reasoning and understanding in LPMs is another important objective of the Evaluation Pillar. Evaluators should collaborate with philosophers of science and AI theorists to explore ques-"}, {"title": "5.2 Challenges and Methods for Evaluating LPMs", "content": "To date, there are many different benchmarks of physics for LLMs. Most of them focus on basic high-school level questions or focus on broader knowledge retrieval tasks [96, 97, 98, 99, 100]. However, there are no fundamental physics benchmarks, and there are no benchmarks that specifically target scientific understanding. Several methods are currently employed to assess LLMs in their capability to handle complex reasoning tasks, particularly in physics [101, 102, 103]. Theoretical approaches also contribute to this field; [15] suggests a scenario where the understanding of a teacher (AI or human) is evaluated based on their ability to transfer knowledge to a student, as judged by an independent referee. Similarly, the Scientific Understanding Benchmark (SUB) introduced by [104] focuses on measuring scientific understanding through tasks involving information retrieval, explanation production, and the generation of counterfactual inferences. One could conceive of further hypothetical benchmarks. These might include the ability of LLMs to provide explanations that withstand scrutiny by domain experts or to create causally realistic simulations.\nCurrent benchmarks for evaluating LLMs (and future LPMs) present several challenges that evaluators need to address. A major issue is avoiding benchmark overfitting and Goodhart's law, which warns that when a measure becomes a target, it ceases to be a good measure [105]. Therefore, evaluators must be cautious not to create benchmarks that are too narrow or easily gameable, designing diverse, comprehensive benchmark suites that test a wide range of skills, from conceptual understanding to creative problem-solving. General AI benchmarks often suffer from arbitrary task selection, incomplete domain coverage, and poor performance on minority sets ( [106]; see however [47]). Similarly, prompt-based evaluations can sometimes be brittle and sensitive to minor variations in the prompt wording. Physics-specific benchmarks must avoid these pitfalls. To achieve this, crowdsourcing challenge problems and evaluation from diverse members of the physics community could be useful as a way of developing meta-benchmarks that test generalisation and assess the trade-offs between breadth and depth of knowledge, as well as adversarial testing. Note however that there is a problem that benchmarks become part of the training data, therefore it is important that some benchmark data should not be published.\nAdditionally, incorporating real experimental data, such as data from the Large"}, {"title": "6 Philosophical Reflection Pillar", "content": "The integration of LPMs into scientific research presents a range of epistemological, conceptual, and ethical challenges that require philosophical investigation. The Philosophical Reflection Pillar focuses on the philosophical implications of integrating LPMs into scientific research. It explores how these AI systems might enhance or transform scientific practice, and reflects on their potential to increase scientific understanding, on the role of human scientists in their development and deployment, and on the types of collaborative frameworks that might be required.\n6.1 Objectives\nExamining the nature and criteria of scientific understanding in the context of AI presents a first objective. Whether, and if so how, LPMs can enhance human understanding is a philosophical question that relates to current debates on scientific understanding [118, 14, 119]. A subsequent issue is whether they can generate 'artificial understanding' independently of humans. While LPMs may support hypothesis generation and experimental testing, as, yet it is human scientists who integrate these results into broader understanding processes. In classical scientific practice, scientific understanding is generated by answering explanation-seeking questions of various types [120], which are usually driven by epistemic interests such as foreseeing the consequences of interventions. The prospect of LPMs functioning as 'artificial scientists' who might possess 'artificial understanding' raises the question of whether such understanding should conform to extant criteria for human understanding or requires a novel conception of understanding [104]. As LPMs evolve from tools to autonomous agents, in line with notions such as Zytkow's 'robot discoverer' [121], the 'Robot Scientist\u2019[122], and the recent 'Intelligent Agent system'[23], their growing role in theory development and conceptual innovation is an area that needs to be investigated.\nThe integration of LPMs also introduces risks, such as the proliferation of unreliable research, reduced peer interaction, and the creation of echo chambers. These risks are compounded by the opacity in methodologies in complex fields, requiring the development of ethical guidelines and oversight mechanisms [50, 51, 52, 123]. \u03a4\u03bf address these challenges, philosophers must collaborate with AI researchers and physicists to develop frameworks for responsible and trustworthy AI in physics research.\n6.2 Challenges and Methods\nDefining and operationalizing scientific understanding for AI systems, such as developing a clear and applicable definition, is a key challenge, as is the navigation of the tensions and trade-offs between explainability, accuracy, and complexity [124, 125, 126]. The increasing autonomy of LPMs in scientific tasks raises concerns about their potential to replace human scientists and the implications for the role of creativity and serendipity in scientific discovery [127]. While in some regards AI might present an opportunity for the long awaited 'logic of discovery' [128]; [129] that was sought after in the 70s and 80s, it appears the role of"}, {"title": "7 Conclusion and Recommendations", "content": "The development of large scale physics-specific AI models (LLM and foundation models), which we name Large Physics Models, would represent a significant step forward in the application of AI to scientific research. Creating AI systems that co-create physical theories, formulas, and relationships, might lead to generating novel hypotheses, experiments, and research directions. Therefore, LPMs could enhance both the pace of scientific discovery and the depth of our theoretical understanding. The proposed three-pillar framework\u2014Development, Evaluation, and Philosophical Reflection\u2014provides a structured approach to realizing this vision.\nWe recommend the physics community to become active in the development of LPMs. This initiative should begin with small-scale demonstrator projects across specific physics subfields to validate approaches before scaling to larger efforts. To go beyond small scale demonstrators requires the creation of larger-scale working groups dedicated to the development of LPMs. These efforts could evolve into formalized collaborations along the lines of experimental particle physics, with defined governance structures, shared computing resources and long-term support.\nWe propose starting with a core group, who should establish governance and col-"}]}