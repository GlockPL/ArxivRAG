{"title": "Multi-Agent Target Assignment and Path Finding for Intelligent Warehouse: A Cooperative Multi-Agent Deep Reinforcement Learning Perspective", "authors": ["Qi Liu", "Jianqi Gao", "Dongjie Zhu", "Xizheng Pang", "Pengbin Chen", "Jingxiang Guo", "Yanjie Li"], "abstract": "Multi-agent target assignment and path planning (TAPF) are two key problems in intelligent warehouse. However, most literature only addresses one of these two problems separately. In this study, we propose a method to simultaneously solve target assignment and path planning from a perspective of cooperative multi-agent deep reinforcement learning (RL). To the best of our knowledge, this is the first work to model the TAPF problem for intelligent warehouse to cooperative multi-agent deep RL, and the first to simultaneously address TAPF based on multi-agent deep RL. Furthermore, previous literature rarely considers the physical dynamics of agents. In this study, the physical dynamics of the agents is considered. Experimental results show that our method performs well in various task settings, which means that the target assignment is solved reasonably well and the planned path is almost shortest. Moreover, our method is more time-efficient than baselines.", "sections": [{"title": "1. Introduction", "content": "With the rapid development of logistics related industries, new opportunities and challenges have been proposed for intelligent warehousing [1, 2]. Traditional warehouse technologies use conveyor belts and other equipment to complete material handling. They are inflexible and not easily extensible. Intelligent warehousing utilizes a multi-agent system to deliver goods to designated locations, greatly improving warehousing efficiency. Intelligent warehouse system is divided mainly into the order fulfillment system, such as the kiva system [3] and the logistics sorting center [4]. In order fulfillment systems, mobile agents move inventory pods to inventory stations. Then, workers take goods from inventory pods, and agents move the inventory pods to their original locations. In logistics sorting centers, agents take goods from loading stations and deliver them to chutes in the center of warehouse. The approach proposed in this study is based on the logistics sorting center.\nTask assignment and path finding (TAPF) are two important processes in intelligent warehousing. The system first assigns a specified task to each agent according to the order requirements. Then the agent transports the goods from the origin to the destination and ensures that the path does not conflict with other agents. The task assignment and path planning problem is typically NP-hard [5], which has a large search space. Thus, directly solving this problem is difficult. In general, two steps are required to solve the TAPF problem. The first step is multi-agent task assignment (MATA), assigning tasks to agents without considering possible path conflicts between agents. The second step is path planning for all agents by using multi-agent path finding (MAPF) algorithms. Numerous studies in the literature study MATA and MAPF; we describe these methods in detail in the next section. Although solving TAPF separately reduces the difficulty of total problem, this type of method ignores the mutual influence between task assignment and path planning. Reasonable task assignment can not only effectively reduce the path length of warehouse agents and improve the operation efficiency, but also help avoid path conflicts between different agents. Therefore, it is necessary to solve the TAPF problems together. In TAPF problem, we assume that each idle agent and inventory pods are homogeneous. Thus, we can assign any task to any agent.\nRecently, deep reinforcement learning (RL) has received great attention due to its remarkable successes in widely challenging domains, such as robotic control [6], multi-agent problems [7], and video games [8]. In this study, we introduce cooperative multi-agent deep RL to solve the TAPF problem to simultaneously address the TAPF problem.\nThe main contributions of this study can be summarized as follows:\n\u2022 To our best knowledge, this is the first work to model TAPF problem for intelligent warehouse to cooperative multi-agent deep RL, and the first to simultaneously address TAPF based on multi-agent deep RL.\n\u2022 In this study, the physical dynamics of the agents is considered in the path planning phase.\n\u2022 Experimental results show that our method performs well in various of task settings, which means the target assignment is solved reasonably and the planned path is almost shortest. Furthermore, our method is more time-efficient than the baselines."}, {"title": "2. Related Work", "content": "2.1. Multi-Agent Task Assignment\nThrough multi-agent task assignment algorithms, we can maximize the utilization of warehouse resources [9]. At present, multi-agent task assignment algorithms in intelligent warehouse can be classed into centralized and distributed classes according to the management mode. In the centralized task assignment class, a central control system is set up, which is responsible for task assignment, assigning tasks assigned to agents for execution [10]. Classical centralized task assignment algorithms include: Hungarian algorithm [11], tabu search algorithm [12], genetic algorithm [10], etc.. In the distributed assignment class, each agent in the warehouse plans its own task sequence according to tasks and environmental information [13]. This class method effectively reduces the load of the central control system and is more flexible and adaptable, but it may not find the global optimal solution [14]. Furthermore, the distributed assignment class mainly includes: learning-based methods [15] and market auction methods [16].\n2.2. Multi-Agent Path Finding\nIn recent years, MAPF has become a hot research direction in the fields of computer science and robotics. Classical MAPF algorithms can be divided into optimal and sub-optimal types according to whether the solution results meet the optimality. The optimal typical MAPF algorithms include A*-based [17], conflict-based search [18], increasing cost tree search based [19], and compilation-based [20] methods. Sub-optimal classical MAPF algorithms include search-based [21], rule-based [22], compilation-based [23], bounded conflict-based search [24], and genetic algorithm-based [25] methods. However, traditional MAPF algorithms have poor real-time performance. Therefore, numerous researchers are beginning to study MAPF based on deep RL [26] to solve this problem. PRIMAL [27] is a typical MAPF algorithm based on deep RL. However, PRIMAL still solves the multi-agent problem by single-agent deep RL method [27]. In this study, we propose a method to simultaneously address target assignment and path planning from a perspective of cooperative multi-agent deep RL. Furthermore, in the simulation environment, the above-mentioned various MAPF algorithms rarely consider the physical dynamics of agents. In this study, the physical dynamics factors of the agent are considered.\n2.3. Cooperative Multi-Agent Deep Reinforcement Learning\nCooperative multi-agent RL deals with systems consisting of several agents which interact in a common environment. Cooperative multi-agent RL aims to learn a policy that can make all agents work together to complete a common goal. Multi-agent deep RL has received great attention due to its capability in allowing agents to learn to make decisions in multi-agent environments through interactions. Independent Q-learning (IQL) [28] solves the multi-agent RL problem by decomposing it into a collection of simultaneous single-agent RL problems that share the same environment. However, IQL cannot solve the nonstationarity [29] problem caused by the changing policies of other agents. Value-Decomposition Networks (VDN) [30] proposes to learn"}, {"title": "3. Preliminaries", "content": "This section provides a summary of Markov Decision Process, RL, cooperative multi-agent deep RL, and the definition of MAPF and TAPF problem.\n3.1. Markov Decision Process and RL\nThis study considers a finite-horizon Markov Decision Process [26], defined by a tuple (S, A,P, r, y, T). S denotes the state space, A represents the finite action space, P: S \u00d7 A \u00d7 S \u2192 [0, 1] denotes the state transition distribution, r : S \u00d7 A \u2192 R denotes the reward function, \u03b3\u2208 [0,1) denotes the discount factor and T is a time horizon. At each time step t, an action a, \u2208 A is chosen from a policy \u03c0. After transitioning into the next state by sampling from P (St+1 | St, At), the agent obtains a immediate reward r (st, at). The agent continues performing actions until it enters a terminal state or t reaches the time horizon T. RL aims to learn the policy \u03c0: S \u00d7 A \u2192 [0,1] for decision-making problems by maximizing discounted cumulative rewards $E_{\\tau \\sim p}[\\sum_{t=0}^{T-1} \\gamma^t r(s_t, a_t)]$. In this study, we do not have access to the environment dynamics P, which means model-free deep RL learning."}, {"title": "3.2. Cooperative Multi-Agent Deep RL", "content": "A fully cooperative multi-agent problem can be modeled as a Markov Decision Process [26], described by a tuple G = (S,A,P,r,O,N,y,T). s\u2208 S represents the true state. At each timestep t, each agent n \u2208 N := {1,..., N} selects an action $a_t^n \\in A$, then the joint action $a_t = \\{a_t^1, a_t^2,..., a_t^N\\}$ is obtained. $P(s_{t+1} | s_t, a_t) : S \u00d7 A^N \u00d7 S \u2192 [0, 1]$ denotes the state transition function. $r(s_t, a_t) : S\u00d7A^N \u2192 R$ denotes the reward function shared by all agents, \u03b3 \u2208 [0, 1) represents the discount factor, and T denotes the time horizon. Each agent n has an independent observation o \u2208 O and a history of action observation $\\tau^n \\in T = (O\u00d7A)$. A stochastic policy $\\pi^n (a^n | \\tau^n) : T \u00d7 A \u2192 [0, 1]$ is based on the history of action-observation. Let \u03c0n denote agent n' policy; cooperative agents aim to maximize:\n$J(\\pi) = E_{s_{0:T-1},a^{1:T-1} \\sim \\pi^1,...,\\pi^N} [\\sum_{t=0}^{T-1} \\gamma^t r(s_t, (a_t^1,...,a_t^N)) ]$ (1)\nIn cooperative multi-agent deep RL, the training process is centralized, but the execution process is decentralized [32]. This means that, in the learning process, each agent has access to the global state s. However, each agent's decision only depends on its own action-observation history \u03c4n in the execution process."}, {"title": "3.3. Definition of MAPF and TAPF problem", "content": "MAPF Problem:\nIn the Multi-Agent Path Finding (MAPF) problem, we consider a finite-horizon Markov Decision Process defined by a tuple (S, A, P, r, y, T):\n\u2022 S: State space.\n\u2022 A: Finite action space.\n\u2022 $P(s_{t+1} | s_t, A_t)$: State transition distribution.\n\u2022 r(st, at): Reward function.\n\u2022 y: Discount factor.\n\u2022T: Time horizon.\nEach agent n corresponds to an RL agent. At each time step t, agent n selects an action a \u2208 A based on its policy \u03c0n. The state st represents the current positions of all agents. The action a can involve moving to an adjacent position or staying at the current position. The objective is to find a policy \u03c0n for each agent that maximizes the expected cumulative rewards"}, {"title": "4. Target Assignment and Path Finding for Intelligent Warehouse: A Cooperative Multi-Agent Deep RL Perspective", "content": "In Section 4.1, we detailedly describe our method that models the TAPF problem as a cooperative multi-agent RL problem. In Section 4.2, we introduce the MAD-DPG algorithm to solve TAPF.\n4.1. Modeling TAPF as a MARL Problem\nAs shown in Figure 1, we provide two scenarios to describe how we model the TAPF problem (described in Section 3.3) as a cooperative MARL problem. As introduced in Section 3.2, cooperative multi-agent deep RL can be modeled as a tuple G = (S, A,P, r,O, N, y, T). In this study, the state transition function P = P(St+1 | St, at) : S \u00d7 AN \u00d7 S \u2192 [0, 1] is unknown, which means the agent cannot get the environment dynamics. This is consistent with the real world TAPF problem. The number of agents N in this study can be set to any integer number. The discount factor y = 0.99, which is the most common setting in deep RL. Since the main elements in multiagent deep RL are the observation space O (or state space S), the action space A and the reward function r. Thus, we describe these elements in detail.\nObservation space: An agent's observation contains the position and velocity itself, the relative positions of all tasks, the relative positions of the other agents, and the relative positions of neighboring obstacles. In this study, the number of other visible agents in an agent's observation can be set to equal to or less than N \u2013 1. Taking Figure 1 (a) as an example, the five purple circles represent five agents, the five black circles represent"}, {"title": "The physical dynamics of agents", "content": "The physical dynamics of agents: Before describing the action space, we first describe the dynamics of agents. Note that our work is the first to consider the dynamics of agents in TAPF for the intelligent warehouse. As shown in Figure 2 (a), the \"No dynamics\" schematic diagram represents previous literature [27, 33] methods that only focus on the next position of an agent, but don't consider velocities and accelerations of agents. Further, the dynamics of agents in previous literature are solved by using traditional methods [34]. On the contrary, the \"Dynamics\" schematic diagram in Figure 2 (b) represents our method that considers the dynamics of agents. We can calculate agents' velocities and accelerations.\nThe output of our policy network is the magnitude and direction of four cardinal directions force ($F_{x}, F_{-x}, F_{y}, F_{-y}$) applied to the agent. According to Newton's second law of motion, $F_{direction} = m * A_{direction} = \\frac{m}{F_{direction}}$ (where m is the mass of the agent), we can get the acceleration $A_{direction} = \\frac{F_{direction}}{m}$. According to basic physics knowledge, we can get the velocity of the agent $V_{t(direction)} = V_{0(direction)} + A_{direction} * \\Delta t$, where $v_0$ is the initial velocity and \u0394t denotes a time interval.\nAction space: In this study, the action space of an agent is continuous, representing the movements. As shown in Figure 3, an agent obtains a velocity between 0.0m/s and 1.0m/s in each of the four cardinal directions [move_left (v2x), move_right (vx), move_down (vy), move_up (v)], the final action (v) is calculated by the the vector sum of the four velocities.\nReward function: We define the reward function as:\nr = reward_success + reward_distance_tasks_to_agents + reward_collision_agents_to_obstacles + reward_collision_agents_to_agents (2)\nThe detailed rewards are defined as follows (R is the radius of an agent):\n\u2022 reward_success = 100 * n, where n denotes the number of elements that satisfy ||Ptasks Pagents|| < 0.05.\n\u2022 reward_distance_tasks_to_agents = -||Ptasks Pagents||min\n\u2022 reward_collision_agents_to_obstacles = -2 * n, where n denotes the number of elements that satisfy ||Pobstacles Pagents|| < R.\n\u2022 reward_collision_agents_to_agents = -2 * n, where n denotes the number of elements that satisfy ||Pagents Pagents|| < 2R.\nFor reward_success, ||Ptasks - Pagents|| represents the distance matrix of all tasks to all agents. Figure 4 shows a detailed distance matrix. If the distance of agent i to task i is less than 0.05m, we give a +100"}, {"title": "4.2. Solving TAPF Problem via Cooperative MARL", "content": "4.2. Solving TAPF Problem via Cooperative MARL\nIn this study, we introduce MADDPG [31] to solve TAPF problem. Considering that agents are homogeneous, thus the agents can share the same policy network. This makes learning policy more efficiently. Let N denote the number of agents, \u03c0 denote the policy parameterized by \u03b8. The centralized critic Q is parameterized by \u03c6. The parameters \u03b8 for policy \u03c0 can be updated iteratively by the associated critic Q:\n$\\nabla_{\\theta} J(\\theta) = E_{(x_t, a_t) \\sim D}[\\nabla_{\\theta} \\pi^n(o_t | \\theta) \\nabla_{a_t}Q(x_t,a_t^1,...,a_t^N | \\phi) |_{a= \\pi(\\theta)}]$ (3)\nwhere x\u2081 and a\u2081 are the concatenation of all agents' observation and action at time step t, $x_t = (o_t^1,...,o_t^N)$ and $a_t = (a_t^1,...,a_t^N)$. o is the observation received by agent n at time step t. D denotes the replay buffer containing tuples (xt, at, rt).\nThe centralized critic parameters \u03c6 are optimized by minimizing the loss:\n$L(\\phi) = E_{(x_t, a_t, r_t, x_{t+1}) \\sim D} [(Q (x_t, a_t | \\Phi) \u2013 y_t)^2]$ (4)\nwhere rt is the concatenation of rewards (Eq(2)) received by all agents at time step t. The target critic value yt is defined as:\ny\u2081 = r\u2081 + yQ\u00af (x1+1, a+1,..., a+1 | \u03a6\u00af) |a\u00b2+1=1(0+1),n=1,...,N\n(5)\nwhere Q is a target Q-network parameterized by \u03a6\u00af. \u03c6 is optimized by:\n\u03c6\u00af = \u03b1\u03c6 + (1 \u2212 \u03b1)\u03c6 (6)\nwhere a is a coefficient to trade-off the weight of Q-network and target Q-network.\nThe share reward making the agents learn policy cooperatively. In the execution phase, we only use the policy n whose inputs are $o_t^1$, . . ., $o_t^N$, respectively, and the outputs are force (F) applied to the agent. According to the physical dynamics of agents described in Section 4.1, we can get the actions at = (a,...,a). Note that, for agent n, the input of n is only the observation o_t^n received by agent n at time step t in the execution phase."}, {"title": "5. Experiments", "content": "In this section, we verified our method from three aspects. In Section 5.1, we verified the target assignment and path planning performances of our method. In all the experiments, the locations of agents and task points are generated randomly. In Section 5.2, we verified the learned cooperation ability. In Section 5.3, we verified the time efficiency."}, {"title": "5.1. Target Assignment and Path Planning", "content": "In this subsection, we verified our method in various of intelligent warehouse settings. We set five different level scenarios to verify the performances: (1) two agents - two tasks (2) two agents - four tasks (3) five agents - five tasks (4) five agents - ten tasks (5) five agents - twenty tasks.\nFigure 5 shows the training average return curves, in all the different level scenarios, the average returns increase monotonous. This verifies the stability of our method.\nFigure 6 to Figure 10 show the performances target assignment and path finding in five different level scenarios. Experimental results show that although the difficulty of these five different level scenarios increasing gradually, our method performs well in all the tasks. For easy tasks (two agents - two tasks) in Figure 6 (a) (b) (c) (d), we can see that the target assignment and path finding were addressed well. For target assignment, results show that the task assignment is addressed very reasonable, because our method assigns the task to the agent that is close to the task. For path finding, it is shown that the planned paths are almost shortest.\nFor difficult tasks (five agents - twenty tasks) in Figure 10 (a) (b) (c) (d), we can see that the target assignment and path finding were also addressed well. For target assignment, it can be seen that each agent is reasonably assigned to several nearby tasks. For path finding, it is shown that the planned paths are usually shortest. The same superior performances can be seen in other different level scenarios, such as two agents - four tasks in Figure 7, five agents - five tasks in Figure 8, and five agents - ten tasks in Figure 9. This demonstrated the efficiency of our method."}, {"title": "5.2. Cooperation Ability", "content": "We also designed a conflict scenario to verify the learned cooperation ability of agents. As shown in Figure 11, the big purple circle represents agent-1, its task is denoted by the small purple circle. The big gray circle represents agent-2, its task is denoted by the small black circle. We deliberately block other roads to create the conflict environment. As shown in Figure 11, there is bound to be a conflict between agent-1 and agent-2 during the navigation. The trajectories generated by our method are shown in Figure 11, the red curve is the trajectory of agent-1, and the cyan curve is the trajectory of agent-2. Results show that both agent-1 and agent-2 had learned to avoid each other at the point of conflict, and then navigated to their tasks. This verified the cooperation ability of our method."}, {"title": "5.3. Time Efficiency", "content": "Table 1 shows the time comparisons between traditional methods [35, 36] and our method. Table 1 shows that our method simultaneously addresses the target assignment problem (TA) and the path planning (PF), however, the traditional method addresses the target assignment first and then performs the path planning. Thus, the consumed time in our method is TAPF time; however, the consumed time in the traditional method is TA+PF. In Table 1, ES means each step. The reason that we compare the consumed time of each step is: (1) Our method handles continuous action space problem; however, traditional method handles discrete action space problem. (2) The consumed time of path finding can be influenced by many factors, such as the resolution of the grid map and the distance taken at each step in the grid map. (3) previous literature [35, 36] did not consider the physical dynamics of agents. However, the physical dynamics of agents has been considered in this study. Thus, for a fair comparison, we only compare the consumed time of each step, which means the consumed time from an agent obtain an observation to output the decision action. For traditional method, the consumed time of target assignment is computed by the method in [35], and the consumed time of each step in path planning is computed by the method in [36].\nTable 1 shows that in easy tasks such two agents - two tasks and two agents - four tasks, the consumed time of target assignment in traditional method can be acceptable. However, with the difficulty of tasks increased, the time of solving target assignment in traditional method increased rapidly, especially in the five agents - twenty tasks scenario. However, our method provides policy time-efficiently in all different level tasks (from two agents - two tasks to five agents - twenty tasks). This verified the time-efficiency ability of our method. As we all know, time efficiency is an important factor in real-world TAPF problems. Therefore, the results of the experiment demonstrated the promising engineering practical value of our method. Note that Figure 6 to Figure 10 had shown the performances of target assignment and path finding of our method in five different level scenarios. For target assignment, the results show that task assignment is reasonably addressed because our method assigns the task to the agent that is close to the task. For path finding, the results show that the planned paths are almost shortest."}, {"title": "6. Conclusion and future work", "content": "In this study, we propose a method to simultaneously address the TAPF problem in intelligent warehouse from a perspective of cooperative multi-agent deep RL. First, we model the TAPF problem as cooperative multi-agent deep RL problem. Then, we simultaneously address target assignment and path finding by cooperative multi-agent deep RL algorithm. Moreover, previous literature rarely considers the physical dynamics of agents. In this study, the physical dynamics of agents have been considered. Experimental results show that our method performs well in various of task settings, which means the target assignment is solved reasonably and the planned path is almost shortest. Furthermore, our method is more time-efficient than baselines. For future work, we will apply our method to real-world target assignment and path finding problem."}, {"title": "Appendix", "content": "Network Architecture: The actor network contains four fully connected layers. As described in Section 4.1, the input dimension of the first layer is equal to the observation dimension. The hidden dimension is 128. The first three layers are followed by ReLU activation function. The output dimension of the forth layer is the action dimension. The critic network contains four fully connected layers. The input dimension of the first layer is equal to the observation dimension plus the action dimension. The hidden dimension is 128. The first three layers are followed by ReLU activation function. The output of the forth layer is the state-action value. Networks are trained by Adam optimizer.\nHyperparameters: The discount factor is y= 0.95 in all experiments. The total training step is 1M, the learning rate of actor and critic network is 0.0004 and the batch_size=1024."}]}