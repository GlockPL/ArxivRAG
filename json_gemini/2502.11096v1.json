{"title": "Mixture of Tunable Experts - Behavior Modification of DeepSeek-R1 at Inference Time", "authors": ["Robert Dahlke", "Henrik Klagges", "Dan Zecha", "Benjamin Merkel", "Sven Rohr", "Fabian Klemm"], "abstract": "We present the Mixture-of-Tunable-Experts (MOTE), a method that extends the Mixture-of-Experts architecture of Large Language Models (LLMs). Without additional training, MoTE enables meaningful and focused behavior changes in LLMs on-the-fly during inference time.\nBy analyzing the digital LLM brain of DeepSeek-R1 using a technique we dub \"functional Token Resonance Imaging\" (fTRI) \u2013 inspired by fMRI and using prompts designed to elicit specific behavior (e.g., \"What happened {time}{place}?\") we empirically identify distinctive experts associated with behaviors like refusal responses.\nUsing MoTE we are able to intervene and control such specific behavior. We switched off the top 10 most refusal-relevant experts (0.07% of R1's 14,848 routed experts), achieving a 52% refusal reduction on sensitive reference prompts without performance degradation on MT-Bench. Random expert deactivation resulted in smaller behavioral shifts with increased noise, whereas forced expert activation led to significantly higher refusal rates. With MoTE we were also able to successfully switch the model's chain-of-thought reasoning language from English to Chinese in 10% of our test prompts.\nOur approach shares similarities with sparse autoencoders (SAEs) in terms of explainability and steerability. Unlike SAEs, MoTE does not require large training efforts, as within MoEs with a vast number of experts, specialization already emerged naturally during pretraining.\nOur findings suggest that significant functional mechanisms in Mixture-of-Experts architectures can at least partially be localized in a small number of specific experts, rather than being distributed throughout the model's weights. Expert subgroups can be tuned to trigger significant behavior variations, providing insights into the inner workings of LLMs.", "sections": [{"title": "1 Introduction", "content": "Large Language Models are rapidly becoming indispensable tools for millions of companies and organizations, hundreds of millions of people, and entire nation states. Widely differing needs create demand for adapting LLMs in behaviors and capabilities beyond mere model selection. There are several known classes of such adaptability:\n1. Prompt engineering works at the regular chat interface of LLMs and thus can be applied to almost any LLM, even for closed-source systems where weights are not available. On the provider side, creating system prompts gives a lot of delivery flexibility. On the user side, prompting the LLM creatively is the default way to coax additional behaviors out of it. With carefully constructed prompts, prompt engineering can be taken to the extremes of \"jail breaking\", which is almost an art form perfected by disguised experts such as the famous Pliny the Prompter."}, {"title": "2 Technical Approach", "content": "R1 has gained significant worldwide attention since its release. One of its most notable features is its advanced reasoning capability, which is a direct result of an innovative reinforcement learning training process. The open-source release of its weights under a permissive license has further boosted its popularity, making it an ideal choice for research purposes.\nThe architecture of R1 is based on the previously introduced DeepSeekMoE [7], which enhanced the MoE architecture (cf. Figure 1). Sparse MoE layers [9] optimize the Feed-Forward Networks (FFNs) within the Transformer framework [10] by cutting the FFNs into smaller, parallel subnetworks. DeepSeek categorizes these subnetworks as\n\u2022 shared experts, which are always on and process all tokens, intended to capture common knowledge across varying contexts\n\u2022 and routed experts, which are only activated if they are selected by an upstream router network, intended to gain non-overlapping and focused knowledge [7]."}, {"title": "2.1 Analyzing Expert Activation", "content": "Within the R1 MoE architecture, a single token only activates a fraction of 8 / 256 routed experts in each layer. A router computes activation scores for all routed experts based on its hidden input taken from the attention block of this layer (cf. Figure 1). Only the top-k experts having the highest activation scores will process the hidden input state. Non-selected routed experts do not contribute to the next token generation step. Each input token therefore activates a specific pattern of routed experts. This activation pattern can be visualized in 2D for each token, as shown in Figure 2."}, {"title": "2.2 Multidimensional Prompt Sensitivity Analysis using behavior-specific Dataset Generation", "content": "We aim to trigger and measure different model behaviors with regard to expert activations. Thus, we designed a parameterized prompt pattern whose parameter combinations elicit several distinct activation patterns."}, {"title": "2.3 Activation Pattern Understanding / Insights", "content": "To better understand how activation patterns compare across our analytical dataset, we apply t-distributed Stochastic Neighbor Embedding ([15]) to generate a two-dimensional projection of the original expert activation patterns. t-SNE represents each high-dimensional expert activation pattern as a two-dimensional point (Figure 4). This ensures that similar activation patterns are positioned close to each other, while dissimilar patterns are mapped to distant points with high probability."}, {"title": "2.4 Identification of Distinctive Experts", "content": "Using the parameterized prompt pattern above we can now systematically trigger three different response types O-REFUSED, 1-ALIGNED, and 2-REASONED and study the activation patterns that contribute most specifically to each type of model behavior.\nWe hypothesize that the experts with a high activation to O-REFUSED answers and a low activation otherwise can specifically be associated with prompts where the LLM provides an answer of type 0O-REFUSED. By actively suppressing these expert activations during inference, we expect to be able to influence the model's behavior such that fewer answers get refused.\nTo achieve this goal, we process our analytical dataset as follows:\n1. Prompt Activation Map: For each prompt, we sum up the activations of each routed expert in all layers for all tokens of the input prompt (see Figure 3 for an example).\n2. Map Classification: The maps get classified into one of the three predefined response classes {0-REFUSED, 1-ALIGNED, 2-REASONED}, depending on the model response to the underlying input prompt.\n3. Average Activation Map: Within each category, we average the activation maps.\n4. fTRI: Finally, functional Token Resonance Imaging helps us to identify the experts that most distinctively contribute to each class. As a measure of resonance we take the averaged activations for the desired class and subtract the averaged activations of all other classes."}, {"title": "3 Results", "sections": [{"title": "3.1 Suppressing Distinctive Experts", "content": "With the distinctive experts for refusal of answers at hand, we performed an ablation study to test if these experts are crucial for the LLM behavior to refuse certain answers. We suppressed these experts by adapting the expert router functions: if one of the identified experts is selected among the top-k ids, meaning chosen by the MoE router (cf. Figure 1), we set its corresponding weight to zero and renormalize the other top-k weights. This approach guarantees that the identified experts have no impact on the output, and that no alternative experts are selected instead. Using the modified MoE, we generated new responses for our set of fixed-template prompts and classify them accordingly."}, {"title": "3.2 Validating Distinctive Expert Suppression with a Larger Dataset", "content": "Notably, although we identified the set of distinctive experts using prompts constructed from a fixed question template, suppressing these experts reduces refusal likelihood across a broader range of prompts. For example, within a set of 1,360 prompts on sensitive topics ([16, 17]) we observe that 52% of previously refused prompts flip in the desired direction. In contrast, the LLM changes its answer to refusal for only about 1.3% of the prompts that did not get refused before. This validates that the experts identified using fTRI correspond to the general concept of answer refusal and are not merely specific to the prompt format used in our dataset to trigger the behavior in fTRI."}, {"title": "3.3 Stimulating Distinctive Experts", "content": "Instead of suppressing specific experts by zeroing their respective weights in the router functions, we can also stimulate their activation. In this case, the modification to the router function is more complex, as for many tokens the alignment-specific experts are not already among the top-k selected experts of their layer. Therefore, we implemented the following approach: In all layers we added the stimulated experts to the top-k selected experts, irrespective of their weight. In order to keep the number of selected experts constant, we removed the lowest of the top-k selected experts with respect to their weights, if necessary. Then, we set the weight of the stimulated expert to the maximum value of all weights and renormalize. This approach guarantees that the stimulated experts always contribute to the overall model response while keeping undesired side-effects from deactivating other top-k selected experts at a minimum."}, {"title": "3.4 Application to other areas of LLM behavior: Changing R1's Reasoning Language", "content": "Having shown that expert tuning does modify the LLM behavior in regard to its alignment, we wondered if it is just a special case. It could be argued that R1's refusal to answer might not be an innate property, because alignment against harmful responses of R1 - and actually most LLMs - is performed in final stages of the training process [6]. Therefore, we wanted to test if the MoTE method generalizes to other behaviors.\nOur test candidate was the language that R1 uses for reasoning. The rationale for this choice is a) that R1's thinking process, which emerged during a reinforcement learning phase [6], is an intrinsic, characteristic behavior and b) that another version of the model, DeepSeek-Zero, showed reasoning language mixing and thus poor readability during its reasoning steps. The latter indicates that the high consistency of R1 in using either English or Chinese as its reasoning language has been deeply trained into the model. Successfully using MOTE to tune aspects of this fundamental behavior would be a strong indication for a broader applicability of the method.\nTo test this hypothesis, we created a new dataset of 600 prompts that trigger R1 to reason either in Chinese (200) or in English (400). With fTRI, we were able to identify the most distinctive experts active when the model chooses to reason in English. The corresponding fTRI scan is shown in (Figure 11). The most distinctive expert is (expert-id, layer-id) = (143, 4), which is close to the input embedding layer and colored in yellow.\nAfter tuning down the top 10 distinctive experts for choosing the reasoning language English, we re-processed the dataset and indeed, the model changed its behavior to reason in Chinese more often (see Figure 12). For 10.75% of the prompts within our dataset, R1 changed its behavior from reasoning in English to reasoning in Chinese. The effect is smaller compared to the reduction in answer-refusal measured above, which indicates that more elaborate ways of tuning experts may be required to get a bigger effect. Still, it does indicate a broader applicability of the fTRI / MoTE method."}, {"title": "3.5 Impact on General Model Quality", "content": "To gain a preliminary understanding of how MoTE influences the overall quality of the model, we compared benchmark results with and without deactivating alignment specific experts (Figure 14). We utilized MT-Bench, a widely recognized benchmark for assessing LLM performance in multi-turn dialogues. MT-Bench comprises 80 high-quality, multi-turn questions designed to evaluate conversational and instruction-following abilities across various categories, including writing, roleplay, extraction, reasoning, mathematics, coding, and knowledge in both STEM and humanities disciplines [18].\nWe found that deactivating experts that are distinctive for refusal of sensitive or harmful questions does not degrade the benchmark results. Rather, we got indications of increased model performance, which surprised us somewhat. Drawing strong conclusions seems a bit too early for us, as the effect should be verified across a larger set of benchmarks."}]}, {"title": "4 Conclusion", "content": "The experimental results support our hypothesis that certain traits  such as the alignment behavior - of MoE models with many experts such as R1 are at least partially encoded within small subsets of experts.\nWe introduced a simple yet effective analytical approach, fTRI, to identify such expert subsets by targeting them with parameterized prompt patterns whose parameter combinations elicit several distinct activation patterns. By altering expert selections and thereby actively modifying routing mechanisms in the LLM we can suppress or stimulate specific expert groups, leading to corresponding changes in model behavior."}]}