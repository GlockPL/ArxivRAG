{"title": "IMPROVING APPLE OBJECT DETECTION WITH OCCLUSION-ENHANCED DISTILLATION", "authors": ["Liang Geng"], "abstract": "Apples growing in natural environments often face severe visual obstructions from leaves and branches. This significantly increases the risk of false detections in object detection tasks, thereby escalating the challenge. Addressing this issue, we introduce a technique called \"Occlusion-Enhanced Distillation\" (OED). This approach utilizes occlusion information to regularize the learning of semantically aligned features on occluded datasets and employs Exponential Moving Average (EMA) to enhance training stability. Specifically, we first design an occlusion-enhanced dataset that integrates Grounding DINO and SAM methods to extract occluding elements such as leaves and branches from each sample, creating occlusion examples that reflect the natural growth state of fruits. Additionally, we propose a multi-scale knowledge distillation strategy, where the student network uses images with increased occlusions as inputs, while the teacher network employs images without natural occlusions. Through this setup, the strategy guides the student network to learn from the teacher across scales of semantic and local features alignment, effectively narrowing the feature distance between occluded and non-occluded targets and enhancing the robustness of object detection. Lastly, to improve the stability of the student network, we introduce the EMA strategy, which aids the student network in learning more generalized feature expressions that are less affected by the noise of individual image occlusions. Our method significantly outperforms current state-of-the-art techniques through extensive comparative experiments.", "sections": [{"title": "Introduction", "content": "In modern agricultural automation, detecting fruits within orchards is a crucial task [1, 2, 3]. These tasks significantly assist farmers in managing resources, optimizing production processes, and making data-supported decisions during the harvest period. Particularly in the field of mechanized harvesting, efficient robotic systems are required to accurately identify fruits amidst cluttered canopies, which not only substantially enhances picking efficiency but also greatly reduces the labor intensity and cost associated with manual harvesting.\nHowever, occlusion is a common occurrence in fruit trees growing under natural conditions. Leaves and branches in the images may be very close to the position of the fruits, or even appear to merge with them, often leading to erroneous identification of fruits [4]. In some cases, the occluded parts contain only limited target information, leading to significant variations in the appearance of objects under different occlusion states. As shown in Figure 1, we simulate natural conditions by adding random occlusions to the original images, significantly altering the shape of the apple at the center due to occluding leaves. Under these circumstances, traditional object detection frameworks struggle to adapt to the learning needs of heavily occluded samples. This is primarily because convolution-based methods like Fast R-CNN [5], Faster R-CNN [6], Mask R-CNN [7], YOLO [8], and SSD [9], as well as Transformer-based algorithms such as Deformable DETR [10] and DINO [11], require direct mapping of fruit features to labels to perform object detection. However, due to the typically limited scale and complex occlusion scenarios of agricultural datasets, these methods often fail to effectively learn the significant shape variations caused by occlusions. Addressing these challenges necessitates more refined algorithmic design."}, {"title": "Related Work", "content": "Object detection: In the field of agricultural automation, particularly in the detection of apple targets in orchards, occlusion presents a ubiquitous and challenging problem. In real-world scenarios, apples are often obscured by leaves, branches, or other fruits, significantly increasing the difficulty of detection. Currently, object detection techniques based on Convolutional Neural Networks (CNNs) and Transformers each demonstrate distinct advantages and limitations in handling occluded target detection tasks. CNN-based methods, due to their robust feature extraction capabilities, are widely used for recognizing and locating complex objects in images, yet they may be limited in handling occlusions and overlapping objects. In contrast, Transformer-based techniques, such as DETR [16], by leveraging global contextual information, are better equipped to understand the occlusions and the relative spatial relationships between objects in images, thus enhancing the detection accuracy under occluded conditions.\nExisting research primarily employs Convolutional Neural Network (CNN)-based methods for addressing object detection issues. For example, Faster-RCNN [6] significantly improves processing speed and accuracy by introducing a Region Proposal Network (RPN), which generates candidate object regions and utilizes deep learning models for direct feature learning and classification. Additionally, algorithms such as YOLO [8] and SSD [9] approach detection tasks as a single regression problem, further enhancing processing speed. YOLO predicts bounding boxes and class probabilities directly across the entire image, markedly accelerating performance. Concurrently, SSD enhances the detection capability for apples of varying sizes by performing detection on feature maps at multiple scales. However, the efficiency and accuracy of these methods tend to be compromised when the degree of occlusion increases [17]. Particularly when apples are mostly obscured by leaves, these detection models often struggle to accurately distinguish objects. This issue primarily arises because they fail to capture sufficient visual feature information in cases of severe occlusion [18]. Convolutional networks rely on local or boundary information to recognize objects, but in heavily occluded scenarios, effective features may be obscured by obstructions, leading to challenges in making accurate classifications.\nIn recent years, Transformer models have increasingly demonstrated their unique advantages in the field of computer vision, particularly excelling in handling occluded object detection. For instance, DETR (Detection Transformer) [16] and its derivative models [10, 11] introduced an innovative approach by transforming the task of object detection into a set prediction problem, thus eliminating the need for complex post-processing steps typical of traditional region proposal-based detection methods. DETR utilizes the self-attention mechanism to process the global dependencies in images, which is particularly crucial for recognizing partially occluded objects against complex backgrounds. Despite their effective handling of global information through the self-attention mechanism, Transformer models may still face limitations in scenarios of extreme occlusion, where objects are mostly obscured. This is because even global attention mechanisms struggle to accurately infer complete object information from very limited visible cues, especially when occlusion is severe enough to obscure key visual features of the object.\nKnowledge distillation: Distillation methods enhance the learning effectiveness of student models by utilizing the relationships between different samples. For instance, by comparing how similar objects are processed in different images, student models can better understand the general appearance of objects under complex backgrounds or occluded conditions. Although guidance from high-level features assists the student model's learning during hint-based training [19], the vast differences in target shapes under extreme occlusion may result in a lack of holistic information. The literature [20] suggests that forcing student models to mimic teacher models on features specified by attention maps can mislead the focus of attention when dealing with random occlusions like trees. Moreover, [21] proposes knowledge distillation using relationships between different samples, but direct similarity distillation across samples performs poorly under severe occlusion. Additionally, the distribution matching approach in [22] relies on complete target information, making it difficult to perform accurately during severe occlusion. Research [23] indicates that full feature imitation may lead to overfitting in the occluded parts by the student model, and has found that this approach might reduce the performance of the student model."}, {"title": "Data Occlusion Enhancement", "content": "To conduct distillation learning on the MinneApple dataset, we implemented a series of occlusion data augmentation operations. Initially, RGB images were segmented using annotated masks from the dataset to create a set of instance templates. Subsequently, we employed a pretrained zero-shot detector, namely Grounding Dino [12], using text prompts such as \"leaves\" and \"branches\" to identify occluders in natural scenes and obtain their initial bounding boxes. Following this, we applied SAM [24] to generate masks based on these bounding boxes, thereby constructing a set of occlusion templates. Ultimately, by combining the occlusion template set with the instance template set, we built a candidate set for querying operations within the instance set."}, {"title": "Occlusion-Enhanced Distillation (OED) Model", "content": "Occlusion-Enhanced Distillation (OED) is a feature distillation technique specifically designed for dealing with heavy occlusion, aimed at improving the performance of deep learning models when processing occluded images. This method employs a hierarchical feature imitation strategy, meticulously aligning and optimizing the feature representation from the overall objective layer down to the local fine-grained occlusion layer. This approach is particularly suited for occlusion handling in visual recognition tasks, significantly enhancing model robustness and accuracy in complex environments (Section 4.1). Additionally, to boost model stability and convergence during training, we utilize an Exponential Moving Average (EMA) strategy. This method reduces variance during training by weighting and smoothing model parameters, giving greater weight to the most recent observations, thereby enhancing the model's generalization capability in occlusion handling tasks (Section 4.2)."}, {"title": "Multi-Scale Feature Distillation", "content": "Multi-scale feature extraction techniques significantly enhance the ability to capture target information at various levels within an image, from macroscopic instance levels to microscopic fine-grained levels, providing support for in-depth analysis. This study employs Deformable DETR [10] as the backbone model, optimizing multi-scale feature extraction through its unique deformable attention mechanism. Given the significant changes in the appearance of targets under occlusion conditions, this research designs a dual-level knowledge distillation framework, focusing on Candidate Distillation and Occlusion-Aware Distillation. Candidate Distillation is obtained through the target detection head of the backbone, while the occlusion-aware feature level focuses on extracting features from prominent unoccluded parts within the apple detection frame, utilizing adapters to extract from embedding features prior to the backbone encoder. These refined feature informations interact with multi-scale feature maps, and knowledge is distilled through the computed weights to enhance the model's ability to recognize targets in complex environments and its robustness.\nMulti-scale Feature Extraction: One of the common challenges faced in detecting apples on fruit trees is the issue of small targets. Existing research [25] indicates that extracting features from multi-scale feature maps is particularly beneficial for small target detection. Based on this consensus, we employ Deformable DETR [10] as the backbone of the model, utilizing its optimized capabilities for multi-scale processing. The set of multi-scale feature maps input is defined as \\({x_l}\\_\\{l=1}^L\\), where each feature map \\(x_l \\in \\mathbb{R}^{C \\times H_l \\times W_l}\\). The normalized coordinates of the reference point for the query element q are denoted as \\(\\hat{p}\\_q \\in [0, 1]^2\\). Multi-scale features \\(F\\_{\\text{scale}}\\) are defined by the following formula:\n\\[F\\_{\\text{scale}} (q) = \\sum\\_{l=1}^L \\sum\\_{k=1}^{L_k} A\\_{lgk} \\cdot W \\cdot \\alpha\\_l (\\varphi\\_l (p\\_q) + \\Delta p\\_{lqk}),\\]\nHere, l denotes the feature level, and k represents the sampling point. The sampling offset \\(\\Delta p\\_{lqk}\\) and the attention weight \\(A\\_{lgk}\\) are defined as the offset and weight for the k \u2013 th sampling point in the Ill-th feature level, respectively. All attention weights satisfy the normalization condition \\(\\sum\\_{l=1}^L \\sum\\_{k=1}^{L_k} A\\_{lgk} = 1\\). The function \\(\\varphi\\_l(p\\_q)\\) rescales the normalized coordinates of the query point \\(\\hat{p}\\_q\\) to the corresponding coordinate location on the input feature map of the l - th level.\nCandidate Distillation: In the task of apple detection, occlusion significantly impairs detection performance, mainly due to the target morphological differences caused by occlusion, which in turn affects the feature representation within the network. Therefore, refining the knowledge of the teacher model at the feature level is crucial for enhancing the imitation capabilities of the student network. To enable the student model to effectively mimic the spatial features of the teacher model, we have defined the following knowledge distillation objective function:\n\\[L_f = ||F\\_{\\text{scale}}^T - F\\_{\\text{scale}}^S||_2^2,\\]\nHere, \\(F\\_{\\text{scale}}^T \\in \\mathbb{R}^{H \\times W \\times d}\\) and \\(F\\_{\\text{scale}}^S \\in \\mathbb{R}^{H \\times W \\times d}\\) represent the feature representations generated by the teacher and student models, respectively. H and W denote the height and width of the features, and d is the number of channels in the features of both the teacher and student models. However, in the object detection framework based on DETR [16], the detection head assigns a probability score for each feature patch, which may lead to the inclusion of many features unrelated to the targets in the objective function. Therefore, we selectively use the features from the teacher network by considering the Intersection Over Union (IOU) between the teacher's predicted scores \\(c\\_i\\) and bounding boxes \\(b\\_i\\), to enhance the relevance of target predictions and more effectively reduce the distance between the feature representations of the teacher and student models. Our designed method for computing weights is as follows:\n\\[S\\_{c_i} = \\frac{\\exp (c_i)}{\\sum\\_{j=1}^N \\exp (c_j)},\\]\n\\[S\\_{b_i} = \\frac{\\exp (b_i)}{\\sum\\_{j=1}^N \\exp (b_j)},\\]"}, {"title": "Exponential Moving Average", "content": "Different from traditional knowledge distillation, we do not use fixed teacher network parameters. Instead, we dynamically construct the teacher network from past iterations of the student network. In the ablation experiments described in Section 5.6, we explore different update rules for the teacher network. The experimental results show that directly copying the weights of the student network to the teacher network does not achieve model convergence. In contrast, using the exponential moving average (EMA) of the student weights, also known as the momentum encoder [27], is suitable for our framework. The update rule is as follows:\n\\[\\theta\\_t \\leftarrow \\tau \\theta\\_t + (1 - \\tau) \\theta\\_s,\\]\nWhere \\(\\tau\\) is a decay parameter that varies from 0.996 to 1 according to a cosine schedule [28]. Initially, the momentum encoder was introduced as an alternative to the queue in contrastive learning [27]. However, in our framework, the role of the momentum encoder changes as we do not use a queue nor adopt a contrastive loss. By updating the teacher network using the exponential moving average (EMA), we achieve a smooth update of the teacher network by incorporating the exponentially decayed sum of historical weights. This method helps the teacher network learn more generalized feature representations that are not affected by individual image occlusion noise. In occlusion scenarios, the student network may be affected by extreme and irregular data perturbations. EMA provides a more stable learning target, allowing the student network to maintain learning continuity under dynamically changing training conditions."}, {"title": "Overall Loss", "content": "The total loss LLL used in our model training is computed as follows:\n\\[L = L\\_{\\text{det}} + \\gamma_1L\\_{\\text{candidate}} + \\gamma_2L\\_{\\text{Occlusion-Aware}},\\]\nThe terms \\(L\\_{\\text{det}}\\) represent the object detection loss from Deformable DETR [10], while \\(L\\_{\\text{candidate}}\\) and \\(L\\_{\\text{Occlusion-Aware}}\\) are defined by equations (6) and (11) respectively. The weighting factors \\(\\gamma\\_1 = 1\\) and \\(\\gamma_2 = 15\\) are used to balance the contributions of the different loss components."}, {"title": "Experiments", "content": "In our experiments, we employed the recently developed large-scale outdoor orchard apple detection dataset, MinneApple [29], along with its related benchmarks. This dataset contains 1,000 images, covering over 41,000 annotated apple instances. These apples exhibit different colors and maturity stages and retain complex occlusions under natural growth conditions, without artificial removal of sparse leaves. The images were taken on both the sunny and shady sides of the tree rows, with shooting dates spanning multiple different days to ensure diversity in lighting conditions. The target instances are relatively small compared to the overall image size, and the number of targets per image can vary from 1 to 120, which meets the needs of real-world unmanned picking scenarios."}, {"title": "Evaluation Metrics", "content": "Consistent with the settings of the MinneApple [29] dataset, we use Average Precision (AP) as the main evaluation metric. Specifically, we calculate the AP scores starting from an Intersection over Union (IoU) threshold of 0.5, increasing in intervals of 0.05 up to 0.95, referred to as AP@0.5:0.05:0.95. Additionally, for more detailed evaluation, we also report AP scores at IoU thresholds of 0.5 and 0.75, denoted as AP@0.5 and AP@0.75, respectively. Considering that targets of different sizes may exhibit different detection performance under natural occlusion conditions, we report the AP scores for small targets (target area less than 322 pixels) and large targets (target area greater than or equal to 922 pixels) separately."}, {"title": "Experimental Setup", "content": "The experiments were conducted on four NVIDIA RTX A6000 GPUs. Unless otherwise specified, we use the pre-trained Deformable DETR [10] as the backbone network for the teacher model. The student model also adopts Deformable DETR as the base framework and is optimized using the Adam optimizer [30] over 50 training epochs. This experimental setup aims to explore the effect of the pre-trained backbone network in the knowledge distillation process and compare its performance."}, {"title": "Quantitative Results", "content": "As shown in Table 1, our method significantly outperforms traditional supervised learning-based methods in terms of bounding box detection performance. This significant performance improvement is mainly attributed to our multi-level knowledge distillation technique, which effectively learns and adapts to the impact of occlusion on target shape and semantics. Additionally, the experimental results indicate that our method demonstrates superior performance in handling small-sized target instances compared to traditional methods, successfully overcoming common challenges in small target detection."}, {"title": "Qualitative Experiments", "content": "For further validation of the effectiveness of our framework, Figure 5 presents the comparative results of three sample images selected from the MinneApple test set. The first row of the figure displays the original test images, while the second row shows the detection results obtained using our method. These comparative images clearly demonstrate the performance of our model in detecting apples under conditions of dense occlusion and shadows. From these images, it can be observed that even in complex environments where apples are closely spaced or partially occluded, our model is able to accurately identify the apples on the trees, showcasing its excellent performance."}, {"title": "Ablation Study", "content": "To gain a deeper understanding of the specific impact of each component in the Occlusion-Enhanced Distillation (OED) framework on model performance, we report the performance of each module in detail in Table 2. Our baseline model, Deformable DETR without any knowledge distillation techniques applied, is labeled as Row 0 in the table, with an Average Precision (AP) of 52.6. Introducing Exponential Moving Average (EMA), occlusion enhancement, and multi-scale feature distillation techniques separately improved the model performance by 1.4 AP, 2.4 AP, and 5.9 AP, respectively. Finally, when these three techniques were applied simultaneously, the model's AP increased to 74.4, achieving a significant improvement of 13.2 AP."}, {"title": "Conclusion", "content": "In this study, we propose and thoroughly detail a novel method named \"Occlusion-Enhanced Distillation\" (OED), specifically designed to enhance the robustness of occluded instance detection. The OED method leverages occlusion information to normalize semantic alignment features during the learning process, thereby improving the model's capability to handle random occlusions. Specifically, we have developed a technique that integrates Grounding DINO with SAM, which can accurately extract occluded elements (such as leaves and branches) from samples and generate occlusion samples that mimic the natural growth conditions of the fruits, simulating the occlusion scenarios encountered in natural environments. Additionally, we introduced a multi-scale knowledge distillation strategy where the student network is trained with images containing occlusions, while the teacher network processes images with natural unobstructed views. This configuration allows the student network to align with the teacher network across semantic and local features at different scales, effectively narrowing the feature discrepancies between occluded and non-occluded targets. To further enhance the stability of the student network, we also employed an Exponential Moving Average (EMA) strategy, which helps the network learn feature representations that are more generalized and less affected by occlusion noise from individual images. Through a series of comprehensive comparative experiments, we demonstrate that the proposed method significantly outperforms current state-of-the-art techniques."}]}