{"title": "Unveiling Transformer Perception by Exploring Input Manifolds", "authors": ["Alessandro Benfenati", "Alfio Ferrara", "Alessio Marta", "Davide Riva", "Elisabetta Rocchetti"], "abstract": "This paper introduces a general method for the exploration of equivalence classes in the input space of Transformer models. The proposed approach is based on sound mathematical theory which describes the internal layers of a Transformer architecture as sequential deformations of the input manifold. Using eigendecomposition of the pullback of the distance metric defined on the output space through the Jacobian of the model, we are able to reconstruct equivalence classes in the input space and navigate across them. We illustrate how this method can be used as a powerful tool for investigating how a Transformer sees the input space, facilitating local and task-agnostic explainability in Computer Vision and Natural Language Processing tasks.", "sections": [{"title": "Introduction", "content": "In this paper, we propose a method for exploring the input space of Transformer models by identifying equivalence classes with respect to their predictions. We define an equivalence class of a Transformer model as the set of vectors in the embedding space whose outcomes under the Transformer process are the same. The study of the input manifold on which the inverse image of models lies provides insights for both explainability and sensitivity analyses. Existing methods aiming at the exploration of the input space of Deep Neural Networks and Transformers either rely on perturbations of input data using heuristic or gradient-based criteria [16, 22, 17, 14], or they analyze specific properties of the embedding space [5].\nOur approach is based on sound mathematical theory which describes the internal layers of a Transformer architecture as sequential deformations of the input manifold. Using eigendecomposition of the pullback of the distance metric defined on the output space through the Jacobian of the model, we are able to reconstruct equivalence classes in the input space and navigate across them. In the XAI scenario, our framework can facilitate local and task-agnostic explainability methods applicable to Computer Vision (CV) and Natural Language Processing (NLP) tasks, among others.\nIn Section 2, we summarise the preliminaries of the mathematical foundations of our approach. In Section 3, we present our method for the exploration of equivalence classes in the input of the Transformer models. In Section 4, we perform a preliminary investigation of some applicability options of our method on textual and visual data. In Section 5, we discuss the relevant literature about embedding space exploration and feature importance. Finally, in Section 6, we give our concluding remarks\u00b9."}, {"title": "Preliminaries", "content": "In this Section, we provide the theoretical foundation of the proposed approach, namely the Geometric Deep Learning framework based on Riemannian Geometry [2].\nA neural network is considered as a sequence of maps, the layers of the network, between manifolds, and the latter are the spaces where the input and the outputs of the layers belong to.\nDefinition 1 (Neural Network). A neural network is a sequence of C\u00b9 maps Ai between manifolds of the form:\n$M_0 \\xrightarrow{\\Lambda_1} M_1 \\xrightarrow{\\Lambda_2} M_2 \\xrightarrow{\\Lambda_3} \\dots \\xrightarrow{\\Lambda_{n-1}} M_{n-1} \\xrightarrow{\\Lambda_n} M_n$\n(1)\nWe call Mo the input manifold and Mn the output manifold. All the other manifolds of the sequence are called representation manifolds. The maps Ai are the layers of the neural network. We denote with $N(i) = \\Lambda_n \\circ \\dots \\circ \\Lambda_i : M_i \\rightarrow M_n$ the mapping from the i-th representation layer to the output layer.\nAs an example, consider a shallow network with just one layer, the composition of a linear operator $Ax+b$ with a sigmoid function \u03c3, where $A \\in \\mathbb{R}^{m \\times n}$ and $b \\in \\mathbb{R}^m$: then, the input manifold Mo and the output manifold M\u2081 shall be Rn and Rm, respectively, and the map $A_1(\\cdot) = \\sigma(A \\cdot +b)$. We generalize this observation into the following definition.\nDefinition 2 (Smooth layer). \u0410 \u0442\u0430\u0440 $\u039b_i: M_{i-1} \\rightarrow M_i$ is called a smooth layer if it is the restriction to $M_{i-1}$ of a function $\u039b_{\\alpha}^{(i)}(x) : \\mathbb{R}^{d_{i-1}} \\rightarrow \\mathbb{R}^{d_i}$ of the form\n$\u039b_{\\alpha}^{(i)}(x) = F^{(i)}\\left(\\sum_{\\beta} A_{\\alpha \\beta}^{(i)}x + b_{\\alpha}^{(i)}\\right)$\n(2)\nfor i = 1,\u2026,n, $x \\in \\mathbb{R}^{d_i}$, $b^{(i)} \\in \\mathbb{R}^{d_i}$ and $A^{(i)} \\in \\mathbb{R}^{d_i \\times d_{i-1}}$, with $F^{(i)} : [\\mathbb{R}^{d_i} \\rightarrow \\mathbb{R}^{d_i}$ a diffeomorphism.\nRemark 1. Transformers implicitly apply for this framework, since their modules are smooth functions, such as fully connected layers, GeLU and sigmoid activations.\nOur aim is to transport the geometric information on the data lying in the output manifold to the input manifold: this allows us to obtain insight on how the network \"sees\" the input space, how it manipulates it for reaching its final conclusion. For fulfilling this objective, we need several tools from differential geometry. The first key ingredient is the notion of singular Riemannian metric, which has the intuitive meaning of a degenerate scalar product which changes point to point.\nDefinition 3 (Singular Riemannian metric). Let $M = \\mathbb{R}^n$ or an open subset of $\\mathbb{R}^n$. A singular Riemannian metric g over M is a map $g : M \\rightarrow Bil(\\mathbb{R}^n \\times \\mathbb{R}^n)$ that associates to each point p a positive semidefinite symmetric bilinear form $g_p : \\mathbb{R}^n \\times \\mathbb{R}^n \\rightarrow \\mathbb{R}$ in a smooth way.\nWithout loss of generality, we can assume the following hypotheses on the sequence (1): i) The manifolds Mi are open and path-connected sets of dimension dim $M_i = d_i$. ii) The maps Ai are C1 submersions. iii) $A_i(M_{i-1}) = M_i$ for every i = 1,\u2026, n. iv) The manifold Mn is equipped with the structure of Riemannian manifold, with metric $g^{(n)}$. Definition 3 naturally leads to the definition of the pseudolength and of energy of a curve."}, {"title": "Preliminaries", "content": "Definition 4 (Pseudolength and energy of a curve). Let \u03b3 : [a, b] \u2192 Rn a curve defined on the interval [a, b] \u2282 R and $||v||_p = \\sqrt{g_p(v,v)}$ the pseudo-norm induced by the pseudo-metric gp at point p. Then the pseudolength of \u03b3 and its energy are defined as\n$Pl(\\gamma) = \\int_a^b ||\\dot{\\gamma}(s)||_{\u03b3(s)} ds = \\int_a^b \\sqrt{g_{\u03b3(s)} (\\dot{\\gamma}(s), \\dot{\\gamma}(s))} ds,   E(\\gamma) = \\int_a^b ||\\dot{\\gamma}(s)||_{\u03b3(s)}^2 ds$\n(3)\nThe notion of pseudolength leads naturally to define the distance between two points.\nDefinition 5 (Pseudodistance). Let x, y \u2208 M = R\u207f. The pseudodistance between x and y is then\n$Pd(x, y) = inf\\{Pl(\\gamma) | \\gamma : [0, 1] \\rightarrow M, \\gamma \\in C^1 ([0, 1]), \\gamma(0) = x, \\gamma(1) = y\\}$.\n(4)\nOne can observe that endowing the space Rn with a singular Riemannian metric leads to have non trivial curves whose length is zero. A straightforward consequence is that there are distinct points whose pseudodistance is therefore zero: a natural equivalence relation arises, i.e. $x \\sim y \\Leftrightarrow Pd(x, y) = 0$, obtaining thus a metric space (R\u207f/ \u223c, Pd).\nThe second crucial tool is the notion of pullback of a function. Let f be a function from RP to R9, and fix the coordinate systems x = (x1,\u2026, Xp) and y = (y1, \u2026, Yq) on Rp and on R\u00ba, respectively. Moreover, we endow R\u00ba with the standard Euclidean metric g, whose associated matrix is the identity. The space IRP can be equipped with the pullback metric $f^*g$ whose representation matrix reads as\n$(f^*g)_{ij} = \\sum_{h,k=1}^q \\frac{\\partial f_h}{\\partial x_i} g_{hk} \\frac{\\partial f_k}{\\partial x_j}$\n(5)\nThe sequence (1) shows that a neural network can be considered simply as a function, a composition of maps: hence, taking f = An \u25cb An\u22121\u25cb\u22c5\u22c5\u22c5 \u25cb A\u2081 and supposing that Mo = RP, Mn = R\u00ba, the generalization of (5) applied to (1) provides with the pullback of a generic neural network.\nHereafter, we consider in (1) the case Mn = R\u00ba, equipped with the trivial metric $g^{(n)} = I_q$, i.e., the identity. Each manifold M\u2081 of the sequence (1) is equipped with a Riemannian singular metric, denoted with $g^{(i)}$, obtained via the pullback of $N(i)$. The pseudolength of a curve y on the i-th manifold, namely Pli(\u03b3), is computed via the relative metric $g^{(i)}$ via (3)."}, {"title": "General results", "content": "We depict hereafter the theoretical bases of our approach. We denote with N\u2081 the submap A\u2081 \u25cb \u00b7 \u00b7 \u00b7 \u25cb An: Mi \u2192 Mn, and with N = N0 the map describing the action of the complete network. The starting point is to consider the pair (Mi, Pdi): this is a pseudometric space, which can be turned into a full-fledged metric space Mi/ \u223ci by the metric identification x \u223ci y \u21d4 Pdi(x, y) = 0. The first result states that the length of a curve on the i-th manifold is preserved among the mapping on the subsequent manifolds.\nProposition 1. Let \u03b3 : [0, 1] \u2192 Mi be a piecewise C\u00b9 curve. Let $k \\in \\{i, i + 1, \\dots, n\\}$ and consider the curve \u03b3k = Ako\u22c5\u22c5\u22c5 \u25cb \u039b\u2081 \u25cb \u03b3 on Mk. Then Pli(\u03b3) = Plk(\u03b3k).\nIn particular this is true when k = n, i.e., the length of a curve is preserved in the last manifold. This result leads naturally to claim that if two points are in the same class of equivalence, then they are mapped into the same point under the action of the neural network.\nProposition 2. If two points p, q \u2208 Mi are in the same class of equivalence, then Ni(p) = Ni(q).\nThe next step is to prove that the sets Mi/\u223ci are actually smooth manifolds: to this aim, we introduce another equivalence relation: $x \\sim_{N_i} y$ if and only if there exists a piecewise \u03b3 : [0, 1] \u2192 Mi such that \u03b3(0) = x, \u03b3(1) = y and Ni \u25cb \u03b3(s) = Ni(x) \u2200s \u2208 [0, 1]. The introduction of this equivalence relation allows us to easily state the following proposition.\nProposition 3. Let x, y \u2208 Mi, then x \u223ci y if and only if $x \\sim_{N_i} y$.\nThe following corollary contains the natural consequences of the previous result; the second point of the claim below is the counterpart of Proposition 2."}, {"title": "Methodology", "content": "The results depicted in Section 2.1 provide powerful tools for investigating how a neural network sees the input space starting from a point x. In particular we point out the following remarks: i) If two points x, y belonging to the input manifold Mo are are such that x \u223c0 y, then N(x) = N(y); ii) given a point p \u2208 Mn, the counterimage N\u207b\u00b9(p) is a smooth manifold, whose connected components are classes of equivalences in Mo with respect to \u223c0. A necessary condition for two points x, y \u2208 Mo to be in the same class of equivalence is that N(x) = N(y); iii) any class of equivalence [x], x \u2208 Mo, is a maximal integral submanifold of VM0. The above observations directly provide with a strategy to build up the equivalence class of an input point x \u2208 Mo. Proposition 5 tells us that VM0 is an integrable distribution, with dimension equal to the dimension of the kernel of $g^{(0)}$: we can hence find dim(Ker(g(0))) vector fields which are a base for the tangent space of Mo. This means that we can compute the eigenvalue decomposition of $g_p^{(0)}$ and consider the L linearly independent eigenvectors, namely {v_\u03b9}\u03b9=1,\u2026,L, associated to the null eigenvalue: these eigenvectors depend smoothly on the point, a fact that is not trivial when the matrix associated to the metric depends on several parameters [15]. We can build then all the null curves by randomly selecting one eigenvector v\u0303 \u2208 {v_\u03b9} and then reconstruct the curve along the direction v\u0303 from the starting point x. From a practical point of view, one is led to solve the Cauchy problem, a first order differential equation, with \u03b3\u0307 = v\u0303 and initial condition \u03b3(0) = x."}, {"title": "Input Space Exploration", "content": "This whole procedure is coded in the Singular Metric Equivalence Class (SiMEC) and the Singular Metric Exploration (SiMExp) algorithms, whose general schemes are depicted in Algorithms 1 and 2. SiMEC reconstructs the class of equivalence of the input via the exploration of the input space by randomly selecting one of the eigenvectors related to the zero eigenvalue. On the opposite, in SiMExp, in order to move from a class of equivalence to another we consider the eigenvectors relative to the nonzero eigenvalues. This requires the slight difference in lines 5 to 7 between Algorithm 1 and Algorithm 2."}, {"title": "Interpretability", "content": "Algorithms 1 and 2 allow for the exploration of the equivalence classes in the input space of a Transformer model. However, the points explored by these algorithms may not be directly interpretable by a human perspective. For instance, an image or a piece of text may need to be decoded to be \"readable\" by a human observer. Furthermore, we present an interpretation of the eigenvalues of the pullback metric which allows us to define a feature importance metric. We present two interpretability methods for Transformers based on input space exploration. Both methods are then demonstrated on a Vision Transformer (ViT) trained for digit classification [8], and two BERT models, one trained for hate speech classification and the other trained for MLM [7, 19]."}, {"title": "Experiments", "content": "Experiments are conducted on textual and visual data. We aim to perform a preliminary investigation of 3 features of our approach: (i) how the class probability changes on the decoded output of SiMEC/SiMExp, (ii) what is the trade-off between the quantity and the quality of the output, and (iii) how our method can be used to extract feature importance-based explanations.\nIn the textual case, we experiment with hate speech classification datasets: we use HateXplain\u00b2 [13], which provides a ground truth for feature importance, plus a sample of 100 hate speech sentences generated by prompting ChatGPT3, which serve purposes (i) and (ii). In the visual case, we perform experiments on MNIST [12] dataset.\nUsing interpretation outputs as alternative prompts An interesting investigation is to determine if our interpretation algorithm (Algorithm 5) can generate alternative prompts that stay in the same equivalence class as the original input data or move to a different one, based on SiMEC and SiMExp explorations. We test how the probability assigned to the original equivalence class by the Transformer model changes as the SiMEC and SiMExp algorithms explore the input embedding manifold.\nFor BERT experiments we generate prompts to inspect the probability distribution over the vocabulary for tokens updated by Algorithms 1 and 2. We decode the updated po...\u0440\u043a using Algorithm 5, focusing on tokens updated through the iterations. For each of these decoded tokens, we extract the top-5 scores to obtain 5 alternative tokens to replace the original ones, creating 5 alternate prompts. We then extract the prediction i* = arg maxj yj for the original sentence, which represents the output whose equivalence class we aim to explore. Finally, we classify the new prompts, obtaining the corresponding predictions $Y = \\{ y^{(0)} \\dots y^{(K)}\\}$, where each $y^{(k)} \\in \\mathbb{R}^N$, N being the number of prediction classes. We visualize the prediction trend for the i*th value in every $y^{(0)} \\dots y^{(K)}$ categorizing the images into two subsets: those that lead to a change in prediction $Y_c = \\{y^{(k)} \\in Y | arg max_j y_j^{(k)} \\neq i^*\\}$ and those that don't $Y_s = \\{y_i \\in Y | arg max_j y_j^{(k)} = i^*\\}$."}, {"title": "ChatGPT prompts", "content": "In order to generate a small dataset of 100 sentences for hate speech detection, we prompt ChatGPT (3.5 version) with several requests. The first one is: can you generate 100 sentences with [CLS] and [MASK] tokens as input for BERT? Do not enumerate them while printing them, I want to do a copy paste directly in a txt file.\nThen: can you do the same but generating sentences for training a BERT model for hate speech detection?\nAnd finally: can you do the same but without the [MASK] token and with some of them being offensive (37%), other hate speech (39%) and others neutral (24%)? Still add the [CLS] token.\nThis yields the dataset we used for exploration of BERT input embedding space."}, {"title": "Using interpretation outputs as alternative prompts (cont.)", "content": "ViT experiments were conducted using 1000 iteration outcomes from both SiMEC and SiMExp, applied to a subset of the MNIST dataset containing 14 images of the digit \u201c4\u201d. BERT for MLM experiments involved 1000 iterations from both SiMEC and SiMExp, applied to a subset of 8 sentences from the \"fill in the mask\" dataset (see Section 4 for more details).\nFor ViT experiments, we first extract the original predicted class $i^* = arg max_j Y_i$, which represents the output whose equivalence class we aim to explore. Then, we run the interpretation algorithm for each po... pk to obtain K interpretations in the form of images. Finally, we classify the new images, obtaining the corresponding predictions $Y = \\{ y^{(0)} \\dots y^{(K)}\\}$, where each $y^{(k)} \\in \\mathbb{R}^N$, N being the number of prediction classes (e.g., N = 10 in MNIST). We visualize the prediction trend for the i*th value in every $y^{(0)} \\dots y^{(K)}$ categorizing the images into two subsets: those that lead to a change in prediction $Y_c = \\{y^{(k)} \\in Y | arg max_j y_j^{(k)} \\neq i^*\\}$ and those that don't $Y_s = \\{y_i \\in Y | arg max_j y_j^{(k)} = i^*\\}$.\nConsidering BERT for MLM experiments, we proceed as illustrated in the main text."}, {"title": "Input space exploration", "content": "We measure the time required to explore the input space of a ViT with the SiMEC algorithm and compare it with a perturbation-based method. The perturbation-based method mimics a trial-and-error approach as it takes an input image and, at each iteration, perturbs it by a semi-random vector Vt+1 = atVt + ne, where at = 1 if yt = Yt-1, at = -1 otherwise, e is an orthogonal random vector from a standard normal distribution and \u03b7 is the step length. With the perturbation, we obtain a new image, then check whether the model yields the same label for the new image. The perturbation vector is re-initialized at random from a normal distribution 20% of the times to allow for exploration. We construct this method to have a direct comparison with ours in the absence of a consolidated literature about the task.\nWe train a ViT model having 4 layers and 4 heads per layer on the MNIST datasets. The SiMEC algorithm is run for 1000 iterations, so that it can generate 1000 examples starting from a single image. In a sample of 100 images, the average time is approximately 339 seconds. In the same time, the perturbation-based algorithm can produce up to 36000 images. However, we notice that the perturbation-based algorithm ends up producing monochrome (pixel color has zero variance) or totally noisy images, which provide little information about the behavior of the model. Excluding only the images with low color variance (< 0.01), we are left, on average, with 19 images (standard deviation 13.9). SiMEC, in contrast, doesn't present this behavior, as all 1000 images have high enough intensity variance and are thus useful for explainability purposes.\nAs BERT has many more parameters with respect to our ViT model, processing textual data takes longer. Specifically, in a sample of 16 sentences, the average time needed to run 1000 iterations on a sentence is 7089 seconds, taking into account both MLM and classification experiments."}, {"title": "Feature importance-based explanations", "content": "We compare our method against Attention Rollout (AR) [1] and the Relevancy method proposed by Chefer et al. [6]. In the textual case, we provide a quantitative evaluation using the HateXplain dataset, which contains 20147 sentences (of which 1924 in the test set) annotated with normal, offensive and hate speech labels as well as the positions of words that support the label decision. We then measure the cosine similarity between the importance assigned by each method to each word in a sentence and the ground truth. Notice that, since the dataset contains multiple annotations, the ground truth y for each word w is obtained as the average of the binary labels assigned by each annotator, and therefore y(w) \u2208 [0; 1]. We also normalize all scores in [0; 1] so to have them on the same scale. The average similarity achieved by our method is 0.707 (standard deviation \u03c3 = 0.302), against 0.7 (\u03c3 = 0.315) for Relevancy and 0.583 (\u03c3 = 0.318) for AR. This proves our method to be more effective in finding the most sensitive tokens for classification. We provide an example on image classification in the Supplementary Materials."}, {"title": "Related work", "content": "Our work relates to embedding space exploration literature, and has at least one collateral applications in the XAI domain, namely producing feature importance-based explanations.\nEmbedding space exploration. Works dealing with embedding space exploration mostly focus on the study of specific properties of the embedding space of Transformers, especially in NLP. For instance, Cai et al. [5] challenge the idea that the embedding space is inherently anisotropic [10] discovering local isotropy, and find low-dimensional manifold structures in the embedding space of GPT and BERT. Bi\u015b et al. [3] argue that the anisotropy of the embedding space derives from embeddings shifting in common directions during training. In the field of CV, Vilas et al. [21] map internal representations of a ViT onto the output class manifold, enabling the early identification of class-related patches and the computation of saliency maps on the input image for each layer and head. Applying Singular Value Decomposition to the Jacobian matrix of a ViT, Salman et al. [17] treat the input space as the union of two subspaces: one in which image embedding doesn't change,\n7 Using Adam optimizer, the model achieved the highest validation accuracy (96.25%) in 20 epochs.\n6 All experiments are based on the current PyTorch implementation of the algorithms and run on a Ubuntu 20.04 machine endowed with one NVIDIA A100 GPU and CUDA 12.4."}, {"title": "Feature importance-based explanations", "content": "Feature importance is a measure of the contribution of each data feature to a model prediction. In the context of Computer Vision and Natural Language Processing, it amounts to giving a weight to pixels (or patches of pixels) in an image and tokens in a piece of text, respectively. In recent years, much research has focused on Transformers in both CV and NLP. Most approaches are based on the attention mechanism of the Transformer architecture. Abnar and Zuidema [1] quantify the overall attention of the output on the input by computing a linear combination of layer attentions (Attention Rollout) or applying a maximum flow algorithm (Attention Flow). To overcome the limitations [4] of attention-based methods, Hao et al. [11] use the concept of attribution, which is obtained by multiplying attention matrices by the integrated gradient of the model with respect to them. Chefer et al. [6] propose the Relevancy metric to generalize attribution to bi-modal and encoder-decoder architectures. Other methods are perturbation-based, where perturbations of input data are used to record any change in the output and draw a saliency map on the input. In order to overcome the main issue with such methods, i.e. the generation of outlier inputs, Englebert et al. [9] apply perturbations after the position encoding of the patches. In contrast with these methods, ours does not need arbitrary perturbations of inputs, and considers all parameters of the model, not only the attention query and key matrices."}, {"title": "Conclusions", "content": "Our exploration of the Transformer architecture through a theoretical framework grounded in Riemannian Geometry led to the application of our two algorithms, SiMEC and SiMExp, for examining equivalence classes in the Transformers' input space. We demonstrated how the results of these exploration methods can be interpreted in a human-readable form and conducted preliminary investigations into their potential applications. Notably, our methods show promise for ranking feature importance and generating alternative prompts within the same or different equivalence classes.\nFuture research directions include expanding our experimental results and delving deeper into the potential of our framework for controlled input generation within an equivalence class. This application holds significant promise for enhancing the explainability of Transformer models' decisions and for addressing issues related to bias and hallucinations."}]}