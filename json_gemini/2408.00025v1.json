{"title": "Need of AI in Modern Education: in the Eyes of Explainable\n\u0391\u0399 (\u03a7\u0391\u0399)", "authors": ["Supriya Manna", "Niladri Sett"], "abstract": "Modern Education is not Modern without AI. However, AI's complex nature makes understand-\ning and fixing problems challenging. Research worldwide shows that a parent's income greatly\ninfluences a child's education. This led us to explore how AI, especially complex models, makes\nimportant decisions using Explainable AI tools. Our research uncovered many complexities linked\nto parental income and offered reasonable explanations for these decisions. However, we also found\nbiases in AI that go against what we want from AI in education: clear transparency and equal\naccess for everyone. These biases can impact families and children's schooling, highlighting the need\nfor better AI solutions that offer fair opportunities to all. This chapter tries to shed light on the\ncomplex ways AI operates, especially concerning biases. These are the foundational steps towards\nbetter educational policies, which include using AI in ways that are more reliable, accountable, and\nbeneficial for everyone involved.", "sections": [{"title": "Introduction", "content": "The trajectory of the education system has traversed a remarkable journey from medieval times\nto the modern era, reflecting societal advancements, technological innovations, and evolving peda-\ngogical paradigms. Initially characterized by rudimentary pedagogical methods, limited accessibility,\nand a rigid curriculum, the education landscape has undergone transformative changes, albeit with\npersistent challenges.\nHistorically, the medieval education system was predominantly characterized by religious institu-\ntions, emphasizing classical literature, theology, and rote memorization. Access was largely restricted\nto elite echelons of society, with pedagogical approaches rooted in tradition and conformity.\nFast-forwarding to the modern era, the education system has witnessed democratization, tech-\nnological integration, and a paradigm shift towards personalized learning. However, contemporary\nchallenges persist, necessitating strategic interventions:\n\u2022 Better Recommendation of Courses: Traditional systems often adopt a one-size-fits-\nall approach, overlooking individual learning preferences and aptitudes. AI-powered recom-\nmender systems being employed in book selection [31], utilizing collaborative filtering algo-\nrithms, analysing student data to generate personalized course recommendations, and optimiz-\ning learning outcomes and engagement. Contemporary software solutions, such as Coursera's\nrecommendation engine\u00b9. and platforms like Knewton Alta 2 leverage advanced algorithms to\ntailor learning pathways based on individual student profiles.\n\u2022 SWOT Analysis of Students: Comprehensive student evaluation transcends academic per-\nformance, necessitating a nuanced understanding of individual strengths, weaknesses, oppor-\ntunities, and threats. Machine learning models, employing clustering analysis algorithms [32],\nfacilitate intricate SWOT analyses, enabling educators to devise tailored learning strategies\nand interventions. Software applications like IBM Watson Analytics\u00b3 and Tableau\u00b9 harness\nsophisticated algorithms to decode complex student profiles, fostering personalized academic\ntrajectories.\n\u2022 Transparency for Students: The ambiguity surrounding grading criteria, learning objec-\ntives, and assessment methodologies undermines student engagement and comprehension. Nat-\nural Language Processing (NLP) techniques, coupled with sentiment analysis algorithms [33],\nelucidate grading metrics and curricular objectives, fostering transparent and collaborative\nlearning environments. Modern platforms like Turnitin and Grammarly employ AI-driven\nNLP tools to provide actionable feedback, enhancing transparency and academic integrity.\n\u2022 Better Performance Indicators with Personalized Feedback: Conventional perfor-\nmance metrics often lack granularity, overshadowing individual accomplishments and areas of\nimprovement. AI-driven data analytics platforms, harnessing predictive analytics algorithms,\nempower educators to deliver personalized performance indicators and actionable insights, op-\ntimizing learning trajectories and student engagement. Software solutions such as Blackboard\nAnalytics and Moodle leverage advanced algorithms to analyze student data, facilitating\ntargeted interventions and pedagogical enhancements."}, {"title": "The Point of This Book Chapter:", "content": "Numerous studies have underscored the profound potential of Artificial Intelligence (AI) in aug-\nmenting and revitalizing the contemporary education system [26, 27, 28, 29]. However, the inte-\ngration of 'Modern' education with AI is not devoid of challenges, particularly concerning financial\nimplications and accessibility disparities.\nA plethora of global research, spanning developed nations such as Norway [23], Japan [21], devel-\noping nations such as China [22] to least developed African countries [24] like Ghana [25], elucidates\na direct correlation between parental income and the quality of education availed by their progeny.\nIn the 21st century landscape, while AI embodies an indispensable cornerstone of educational ad-\nvancement, its implementation entails substantial costs encompassing resource allocation, hiring,\ncross-testing, verification, licensing, and deployment [34]. Consequently, the escalating financial exi-\ngencies associated with modern education exacerbate accessibility barriers9, 10 [35], constraining stu-\ndents hailing from economically disadvantaged backgrounds. Regrettably, instances abound wherein\nacademically meritorious students, despite securing admissions into prestigious global universities,\nconfront insurmountable financial hurdles, precluding educational attainment due to parental income\nconstraints.\nWe acknowledge the fact that parental income substantially influences the attainability of ed-\nucational opportunities for students. In this study, we want to investigate this aspect of parental\nincome. How the income of an individual is dependent on which factors. How can we visualise those\ndependencies? Moreover, are those dependencies fair? If not, how can we investigate the unfair-\nness? Is the Notion of feature importance enough to recognise a biased model? This book chapter\nendeavours to delve deeper into these pivotal nexus. We use several methodologies for visualising the"}, {"title": "Inherent Complexity: A Double-Edged Sword", "content": "As discussed, while AI harbours immense potential to revolutionize education, fostering person-\nalized learning, enhancing administrative efficiency, and facilitating data-driven decision-making, its\ninherent complexity poses formidable challenges, particularly concerning transparency and inter-\npretability in deducing decisions which concurrently introduces nuanced challenges that necessitate\nvigilant scrutiny and strategic navigation. Sophisticated AI software including Llms [36, 41, 42],\nSaaS and commercial software such as [37, 40], and virtual assistants [43], while endowed with\nremarkable capabilities, often manifests as convoluted enigmas, presenting two underlying problems:\n1. Explanation Gap: AI models, characterized by intricate hyperspaces defined by multifaceted\nweights and attributes, inherently lack traceability and debuggability. The stochastic nature\nof the training phase, coupled with the intricacies of the hyperspace, obfuscates the retrieval\nand comprehension of model parameters and rationales. Consequently, elucidating the 'why'\nbehind AI predictions remains a daunting endeavor, devoid of straightforward exploration\navenues.\n2. Trust in the \u2018Black-Box': For seasoned machine learning practitioners well-versed in model\narchitectures and training phases, navigating the intricacies of AI may seem feasible. However,\nfor end-users devoid of specialized expertise, AI often emerges as an opaque 'black box', neces-\nsitating dual proficiency as AI and domain experts to engender trust. While domain expertise\nmay be commonplace, it scarcely equates to an unwavering confidence in results generated by\nintricate AI systems, perpetuating scepticism and apprehension."}, {"title": "Explainable AI (XAI): Bridging the Trust Gap", "content": "In the pursuit of deciphering the intricacies of 'black-box' AI systems and fostering trust among\nend-users, scientists have embarked on a relentless quest, culminating in the emergence of Explain-\nable AI (XAI) as an active area of research in this decade. XAI transcends traditional AI paradigms,\nendeavouring to illuminate the opaque nature of complex machine learning models by enabling:\n1. Enhanced Explainability: XAI endeavours to cultivate machine learning techniques that\naim to yield more explainable models without compromising learning performance or prediction\naccuracy [47]. By elucidating model rationale and inherent strengths and weaknesses, XAI\naugments user comprehension and trust in several high stakes (for example: [46]), pivotal for\nfostering collaborative partnerships between humans and AI entities.\n2. Human-Centric Design: XAI advocates for the integration of state-of-the-art human-\ncomputer interface techniques, facilitating the translation of intricate models into comprehen-\nsible and actionable insights for end-users [45]. By crafting intuitive explanation dialogues and\ninterfaces, \u03a7\u0391\u0399 empowers users to navigate, interpret, and leverage AI capabilities effectively,\ntranscending the complexities of underlying algorithms and architectures.\n3. Diverse Methodological Portfolio: Recognizing the multifaceted challenges inherent in\nachieving explainability without compromising performance, XAI adopts a multifaceted ap-\nproach, exploring and integrating diverse techniques across the performance-versus-explainability\ntrade space [44]. By curating a portfolio of methodologies, XAI equips developers with ver-\nsatile design options, facilitating tailored solutions aligned with specific application domains\nand user requirements."}, {"title": "Methodology: Explainability, Interpretability and more", "content": "Before jumping into the technical and mathematical intricacies of xAI, it is critical to distinguish\nbetween explainability and interpretability in machine learning applications for our objectives. We\nregard interpretability as a means of obtaining explainability. Explainable AI (or XAI) refers to\nmodels whose results can be comprehended by ordinary humans. However, this does not imply that\nthe model is required to be interpretable. That is, there are two types of models: interpretable\nmodels and non-interpretable models. Linear regression, logistic regression, decision trees etc are\nexamples of interpretable models. Practitioners can simply determine how the model outputs either\nits label or value. Then there are non-interpretable models for image processing and natural language\nprocessing, such as random forests, feed-forward neural networks, and deep learning architectures [7].\nIn order to make sense of the process by which the intricate model that was employed to generate the\nforecast came to be, we employ post hoc explanations [48] for these. This is frequently accomplished\nby examining the model's gradients or using stand-in models to simulate behavior in specific areas.\nWe believe that the line that separates interpretability from explainability also follows the pro-\ncesses of the human mind. For example, human beings occasionally use deliberate, logical thinking\nto arrive at a conclusion. As a result, the logic can be examined in detail. In other cases, people base\ntheir initial conclusions on intuition after analyzing complex facts over extended periods of time.\nThen, just as we use post hoc explanations on black box models, the people look for an explanation\nfor a decision. Determining which strategy is superior for explainable AI is probably going to need\nmajor advancements in human cognitive science and our comprehension of the functioning of the\nhuman brain."}, {"title": "Post-hoc Explainations", "content": "In regards to the realistic scenario [48], in which the end user looks at the details after a study\nhas been concluded and the data collected, our goal is to provide context for the predictions of a\nmachine learning model. In order to do this, we must first define what it means to produce an\nexplanation. An explanation typically states that some features are more significant than others\nor explains how an input feature can influence the output's magnitude in a positive or negative\nway. It usually links the feature values of an instance to its model prediction in a way that is\neasily understood by humans. Not all explanations must be focused on the significance of a feature.\nThey could also be particular cases or groups of influential instances. They could be real English\nlanguages or collections of rules. In fact, difficulties with homogeneous judgment arise from a variety\nof reasons. In our current study, we focus primarily on the importance of the feature.\nPost hoc approaches might be model-agnostic or model-dependent, global or local. Model-\ndependent interpretation techniques are focused on a particular model or set of models. Model-\nagnostic tools, on the other hand, can be used on any machine learning model, no matter how\ncomplex. These agnostic approaches typically analyze feature input and output pairs. Model-\nagnostic methods frequently profit from the reality that certain practitioners may not have access\nto the original model's specific weights and parameters. In our current work, we have been focused\non local 'Post-hoc' explanation methods in model-agnostic fashion. Below is the description of the\nmethods we used in study for generating post-hoc explanation.\n1. LIME [2]: One of the most well-known post hoc methods is LIME or local interpretable\nmodel-agnostic explanations. LIME's concept is to build a surrogate model in a local area at a\ncertain target point to explain the significance of the input features. Because it hypothesized\nthat sophisticated black boxes tend to demonstrate more linear or simple behavior in local\nneighborhoods. We build new samples for LIME by perturbing for the target sample. We then\nquery the black-bo model to obtain the label of the perturbed data points, scores them using\na kernel, and train a sparse linear model using the data-points and labels.\n2. KernalSHAP [3]: Shapley values [4] are based on the idea that a prediction may be explained\nby imagining each feature value of the instance as a 'player' in a game where the prediction is\nthe payoff. Shapley values, a strategy from coalitional game theory, teach us how to allocate\nthe \"payout\" among the characteristics in a fair manner.\nShapley regression values serve as indicators of feature importance in linear models, particularly\nwhen confronted with multicollinearity. This approach mandates the model's retraining across\nall conceivable feature subsets $S \\subset F$, where $F$ denotes the complete set of features. Each\nfeature is assigned an importance value, signifying the impact of its inclusion on the model\nprediction. The computation involves training a model $f_{S \\cup {i}}$ with the specific feature and\nanother model $f_{S}$ without it. Subsequently, the discrepancy in the predictions for the current\ninput is assessed as $f_{S \\cup {i}}(x_{S \\cup {i}}) - f_S (x_S)$, where $x_S$ represents the input feature values\nwithin the set $S$. As the influence of excluding a feature is contingent on other model features,\nthese differences are computed for all feasible subsets $S \\subseteq F\\{i}$. Subsequently, the Shapley\nvalues are calculated and used as feature attributions, representing a weighted average of all\npotential differences [4, 3]:\n$\\Phi_{i} = \\sum_{S \\subset F\\{i}} \\frac{|S|!(|F| - |S| - 1)!}{F!} [f_{S \\cup {i}}(x_{S \\cup {i}}) - f_{S} (x_{S}]$        (1)\nHowever, both LIME and Kernel SHAP share the same foundation but choose different kernels\nand loss functions to calculate the importance of features [5, 1]. With this, we are now moving\nto Definitions. Thoughts have been mainly drawn from [10, 11, 12]\nDefinition 1 (Algorithm Interpretability). An algorithm's interpretability is its ability to\ngive users enough expressive data to comprehend how the algorithm operates. In this case,\nhuman-readable text or graphics may be considered interpretable domains. \"If something is\ninterpretable, it is possible to find its meaning or to find a particular meaning in it,\" according\nto the Cambridge Dictionary.\nDefinition 2 (Interpretation). Interpretation is the process of reducing a complicated do-\nmain-like the outputs of a machine learning model\u2014to meaningful, rational, and human-\nunderstandable ideas."}, {"title": "Experimental Setup", "content": "The study follows the guidelines given in [38, 39] for experimentation and implementation par-\nticulars. It uses the Adult Census Income dataset from the UCI Machine Learning Repository. We\nstart our experiments by training an XGBoost [13] model. The following performance metrics were\nachieved for the model:\nWe start our investigation on the trained XGBoost based on easy to complex hypothesis gradually\nfor calculating feature importance. We start with the ELI5 library [14]. We used the library's \u2018Per-\nmutation Importance' (PI) method to compute the feature importance. It works on the hypothesis\nthat the importance of a feature can be quantified by the drop in the intended metrics (accuracy,\nprecision etc) when removed. In this study, we use AUC [50] as the performance metric to evaluate\nthe trained XGBoost model. We report the PI for both train and test counterparts in Figure 1: the\nleft column shows the PI in the train set and the one in the right is for the test. Despite the fact\nthat the order of the most significant traits varies, it appears that the most crucial one (married_1)\nremains the same in both folds. Furthermore, the six most important variables based on PI are\nretained in training and testing. We anticipate their difference in relative ordering are mostly due to\nthe sample dispersion between the folds. Next, we use a more sophisticated method named SHAP\n[3] for visualizing the feature importance. We also want to compare the feature importance from the\ntrained XGBoost with the ones obtained from SHAP. We use Gain as the default feature importance\nfor the XGBoost model. Gain of a feature is computed by taking its relative contribution in each\ntree for the whole model. In the comparison between the SHAP feature importance and XGBoost's\ncounterpart, we take the top 20 features in both of them. In Figure 2, we report the XGBoost\nfeature importance. We compare the same against the mean of the absolute SHAP values presented\nin Figure 3. Finally, in Figure 4 we report the SHAP values of all these features to demonstrate\ntheir impact on the model.\nIt is clear from the comparison that the most important feature (married_1) is common in both\nthe comparison. Next, education.num is there within the top-3 features in both feature importance\nranking. However, the rest of the features including 'age' are either not in common or ranked\ndifferently based on their importance in the comparisons. Next, we move to the summary plots\nproduced by SHAP for a nuanced overview of feature importance."}, {"title": "Summary Plot SHAP", "content": "The SHAP Summary Plot (Figure 4) offers more information than the conventional plot. The\nsignificance of features is arranged according to decreasing feature relevance.\nImpact on Prediction: The horizontal axis's position reflects how much or how little the values\nof the dataset instances for each feature affect the model's output.\nOriginal Value: The colour designates a high or low value (within each feature's range) for each\ncharacteristic.\nCorrelation: A feature's colour (or range of values) and the effect on the horizontal axis are\nusually used to assess how well it correlates with the model output."}, {"title": "Dependency plots", "content": "Dependency plot not only provides the marginal influence the feature has on the model's output,\nbut it also illustrates the relationship with the feature with which it most interacts by colour. This\nillustrates the model's reliance on the provided feature. The vertical spread of data points indicates\ninteraction effects. We presented some dependency plots (Figure ?? with different pairs of features\nto present how the interaction is between characteristics and how one influences another.\nAs depicted in Figure 5, there's a discernible upward trend in marriages between the ages of 20\nto nearly 50 years. Meanwhile, Figure 6 reveals a pronounced rise in the education.num concerning\ncapital gains ranging from 0 to 20k USD. Figure 7 further elucidates a consistent progression between\neducational attainment and marital status. Lastly, Figure 8 underscores that married individuals\ntend to exhibit higher capital gains compared to their unmarried counterparts.\nNow, in the next section, we'll discuss post-hoc local explanations using LIME and SHAP."}, {"title": "Discovering Local Scales", "content": "Local surrogate models [49] are interpretable models that are used to explain individual predic-\ntions of black-box machine learning models."}, {"title": "LIME", "content": "LIME [2] stands for Local Interpretable Model-independent Explanations. LIME investigates\nwhat occurs in model predictions when the input data is changed. It creates a new dataset contain-\ning permuted samples and the old model's related predictions. LIME trains interpretable models\n(Logistic Regression, Decision Tree, LASSO Regression, etc.) on this synthetic set, which are then\nweighted by the closeness of the sampled examples to the instance of interest.\nThe explanation, for example, X will be that of the surrogate model that minimises the loss\nfunction (performance measure for example, MSE- between the surrogate model's forecast and the\nprediction of the original model), while keeping the model's complexity low.\nHowever, LIME has a distinct problem; it is inherently unstable. As illustrated in Figure 9,\nLIME often produces varied explanations for identical instances when executed multiple times. This\ninconsistency, while not always prevalent, arises from the inherent randomness in generating the\nsurrogate models employed by its linear models. This is not the case with SHAP as it enjoys the\nuniqueness of SHAP values for a given instance [3]. There are various ways to visualise SHAP\nexplanations. This chapter will go over two of them: the Decision & Force plot [3].\n\u2022 Force Plot: It shows the effect that each attribute had on the forecast. The output value (model\nprediction for the instance) and the base value (average prediction for the whole dataset) are\nthe two important numbers to pay attention to. Greater influence is shown by a larger bar,\nand the colour shows whether the feature value shifted the forecast from the base value to 1\n(red) or 0 (blue).\nWe run the same experiment 10 times to show that the explanation is unique and does not\nexhibit randomness like LIME 9.\n\u2022 Decision Plots: The Decision Plot: (Figure 13) displays the same information as the Force\nPlot. The gray vertical line represents the baseline, and the red line indicates whether each\ncharacteristic increased the output value higher or lower than the average forecast. Sometimes\nthe information representation depends upon the end user's intention due to which various\nrepresentations have been engineered under SHAP library."}, {"title": "Global Explanations", "content": "An interpretable model that has been trained to match the predictions of the black-box model\nto a very large extent is a global surrogate model. By analysing the surrogate model, we may make\ninferences about the black box model. Though it, by any means, is not a way to predict the internal\narchitecture of the black box, rather it is meant to deduce some insights regarding the prediction.\nUsing a Decision Tree and a Logistic Regression as global surrogate models, we will attempt to\napproximate the XGBoost.\nFor both the Train and Test sets, R-squared is negative in the case of Logistic Regression (-1.40,\n-1.42 respectively). This occurs when the fit is poorer than using the mean alone. As a result, it is\nargued that Logistic Regression is NOT an adequate surrogate model.\nHowever, we have obtained 0.77&0.79 for R-Square scores in train and test respectively in the\ncase of decision tree, so we can proceed with the same if it mimicks the trained XGBoost well.\nThe Decision Tree approximates the decisions of the XGBoost model quite well as per the scores\nobtained, hence it may be used as a surrogate model for evaluating the main model. However, it\nis not guaranteed that the decision tree uses the features in the same way as the XGBoost. It is\npossible that the tree approximates the XGBoost properly in certain portions of the input space\nbut acts erratically in others as the models are themselves different. We can see in Figure 10 that\nmarried_1 is the most important feature followed by capital.gain and education.num. In the\nimportance of the XGBoost feature, these were the top-3 features and in the SHAP dependency plots\nas well, these were important features to consider. Next, we use another method named SP-LIME\nfor generating global explanation."}, {"title": "Submodularity:", "content": "The submodular picking [8] is important as end users may not have time to examine a large\nnumber of explanations. It is critical to choose which instances to explain with care. It seeks\nto provide explanations for machine learning models by attributing their predictions to human-\nunderstandable features. To achieve this objective effectively, it's essential to execute the explanation\nmodel across a varied yet representative collection of instances. By doing so, LIME aims to produce\na non-redundant set of explanations that collectively offer a comprehensive understanding of the\nmodel's behaviour on a global scale. SP LIME algorithm selects a collection of explanations for\nthe user that are diverse and representative, meaning they do not repeat themselves, while also\ndemonstrating how the model works in general as shown in [2]. Technically, each instance being a\ncollection of features is a subset of the whole feature set. Collecting a minimal or almost minimal\nnumber of subsets to cover the whole or almost the whole set is the main agenda of employing the SP\nLIME algorithm. This problem is a derivation of the set cover problem [9] which is NP-complete.\nLIME has its own greedy picking [2] algorithm which we have demonstrated in Figure 11. More\ntechnical details can be found at [2].\nIn Figure 11, capital.gain is shown to be the most important attribute in all individual in-\nterpretations that distinguish the classes. Following that, depending on the situation, the most\nimportant determinants are married_1, education.num, age, and sex. These features align with\nthose pinpointed by the algorithms as globally significant.\nSo far, we have investigated how the model's features interplay and exhibit patterns. However,\nwhether this interplay is fair & legitimate is what we investigate in the next section."}, {"title": "All Is Well, but Is All Fair?", "content": "In machine learning, \"fairness\" refers to the several ways that algorithmic bias in machine\nlearning-based automated decision processes is attempted to be corrected. If computer decisions\nafter a machine-learning process were predicated on factors deemed sensitive, they may be viewed\nas unjust. These variables can be related to gender, ethnicity, sexual orientation, disability, and\nother factors. Definitions of prejudice and justice are inherently contentious, as is the case with\nmany ethical notions. Fairness and prejudice are typically taken into consideration when making\ndecisions that have an influence on people's lives. The issue of algorithmic bias in machine learning is\ngenerally recognised and researched. Many factors have the potential to distort the results, making\nthem unfair to particular persons or groups.\n'FairML' [6], the library for fairness in Python helps to investigate these intricacies.\nThe underlying idea behind FairML (and many other attempts to audit or understand model\nbehaviour) is to change the inputs of a model to quantify its dependency on them. The model is\nsensitive to a feature if a modest change to an input feature drastically impacts the output. In order\nto quantify how much each characteristic influences the prediction model, FairML projects the input\northogonally.\nLet $F(x, y)$ be a model trained on two characteristics, $x$ and $y$. The change in output resulting\nfrom a modified input, where $x'$ perturbs $x$, can be quantified to measure the model's reliance on $x$.\nWe express this as:\n$\\Delta F(x',y) = F(x',y) - F(x,y)$            (2)\nHere, $\\Delta F(x', y)$ represents the change in the model output, and the perturbation in $x'$ renders\nthe other feature $y$ orthogonal to $x'$. However, linear transformation is an orthogonal projection.\nFairML uses a basis expansion along with greedy search in the case of non-linear relationships. But,\nin auditing a model, the determination of fairness is often nuanced and context-dependent. The\nsensitivity of features can vary across different contexts; what may be considered sensitive in one\nscenario may not hold the same significance in another. The definition of fairness is multifaceted,\ninfluenced by user experience, and cultural, social, historical, political, legal, and ethical consider-\nations. Identifying appropriate fairness criteria involves navigating trade-offs among these factors.\nThe visualization: Figure 14 uses red bars to signify a contribution to an output of Income \u00bf USD\n50K; light blue bars stand for the opposite. FairML reveals a significant dependence on sensitive fea-\ntures like race_White, nac_United States, and sex_Male. Specifically, the model indicates a strong\nbias: according to its predictions, an individual is more likely to make more than USD 50k if the in-\ndividual's nationality is of the United States, race is white, and sex is male. Notably, the algorithm's\northogonal projection brings out the relevance of features like race_White nac_United States, &\nsex_Male, which did not appear in other interpretation methods like SHAP's Feature Importance.\nThis underscores the importance of investigating for bias along with feature importance. As we\nobserve the popular methods for feature importance may not always be capable of uncovering the\nintrinsic bias and unfairness in trained models. Now, in the case of availing better education, we\nstarted by looking at how parental income influences the education of children and these types of\nsevere bias are what we've found out. This type of societal bias directly impacts the household and\nthe education of the children. In our assessment, the principles of fairness and transparency appear\nconspicuously absent. All is simply, not Fair [20]."}, {"title": "Concluding remark:", "content": "Fairness in machine learning models is a nuanced and evolving concept. Fairness considerations\nneed to account for diverse perspectives, cultural nuances, and the dynamic societal landscape.\nMoreover, the interpretability of fairness metrics remains an open question. Defining fairness is\nnot a one-size-fits-all endeavour, and achieving fairness often involves navigating complex trade-offs\nbetween different dimensions of fairness [15].\nOur comprehensive analysis delved deeply into the critical interplay between parental income\nand its consequential impact on educational opportunities, harnessing advanced xAI tools such as\nLIME, SHAP, and the FairML library. By scrutinizing predictive models derived from the Adult\nCensus data, our deliberate preference for sophisticated algorithms like XG Boost and Decision\nTrees over simpler models aimed to mirror the inherent complexities typical of real-world black-box\nsystems. However, our findings unveil a disconcerting pattern: pervasive biases persistently permeate\nthese models. While FairML identified these biases as direct orthogonal projections, SHAP analyses\nsometimes remained oblivious to them. Intriguingly, LIME's volatility introduced yet another layer\nof complexity, further complicating our interpretative landscape\nIn conclusion, we want to revisit the objectives: AI came to education for speed and transparency,\na strong pillar for availing education is parental income and when that is being scrutinised, it\nis showing heavy bias under several tests mimicking the real world. The effort we put here to\ninvestigate the situation is solely to question the fundamentals: In society, modern education is\nthe pillar to move the nation ahead whereas the Scope of availing education is deliberately getting\nshrunk due to several biases and education, as the fundamental right to every citizen is being heavily\nexclusive to a particular direction where the Bias lends itself. Biases present in the training data\nmay be inadvertently perpetuated or even amplified by xAI algorithms, reinforcing existing societal\nbiases [16, 17, 18]. This is why refining fairness definitions, enhancing the transparency of complex\nmodels, and mitigating biases in training data are key challenges that warrant continued attention\nand collaborative efforts from the research and practitioner communities. Through constructive\ncriticism and iterative refinement, xAI can evolve into a more robust and ethically grounded field\n[18, 19], fostering trust and accountability in the deployment of artificial intelligence systems."}, {"title": "Further Work", "content": "We are consistently striving for improved policies for Modern education, emphasizing values of\nTransparency, Equal Opportunity, & Accessibility. Our primary focus lies in harnessing the\npotential of xAI to enhance policymaking. Operating within the realm of public policy, our endeav-\nour centres on exploring innovative avenues to render education policies more accessible, starting\nfrom the grassroots level. Leveraging advanced algorithms, we aim to bring about positive transfor-\nmations that align with our commitment to creating a more transparent, equitable, and accessible\neducational landscape."}]}