{"title": "Diversity as a Reward:\nFine-Tuning LLMs on a Mixture of Domain-Undetermined Data", "authors": ["Zhenqing Ling", "Daoyuan Chen", "Liuyi Yao", "Yaliang Li", "Ying Shen"], "abstract": "Fine-tuning large language models (LLMs) using diverse datasets is crucial for enhancing their overall performance across various domains. In practical scenarios, existing methods based on modeling the mixture proportions of data composition often struggle with data whose domain labels are missing, imprecise or non-normalized, while methods based on data selection usually encounter difficulties in balancing multi-domain performance. To address these challenges, in this paper, we study the role of data diversity in enhancing the overall abilities of LLMs by empirically constructing contrastive data pools and theoretically deriving explanations for both inter- and intra-diversity. Building upon the insights gained, we propose a new method that gives the LLM a dual identity: an output model to cognitively probe and select data based on diversity reward, as well as an input model to be tuned with the selected data. Extensive experiments show that the proposed method notably boosts performance across domain-undetermined data and a series of foundational downstream tasks when applied to various advanced LLMs. We release our code and hope this study can shed light on the understanding of data diversity and advance feedback-driven data-model co-development for LLMs.", "sections": [{"title": "1. Introduction", "content": "The advancement of large language models (LLMs), exemplified by open-source models such as the Llama3 series (Dubey et al., 2024), Qwen2 series (Yang et al., 2024), and the DeepSeek-V3 series (Liu et al., 2024a), along with closed-source models such as GPT-4 (Achiam et al., 2023), has revolutionized artificial intelligence by enhancing capabilities in foundational abilities such as common sense, reasoning, mathematics, and coding. Fine-tuning these models further optimizes their usability by enhancing their performance and aligning with specific human instructions and preferences (Taori et al., 2023; Rafailov et al., 2023).\nTo cultivate comprehensive capabilities in LLMs, fruitful studies have explored preferable trade-offs between quality, quantity, and diversity of their training data (Li et al., 2024b; Chen et al., 2024b; Qin et al., 2024; Zhao et al., 2024a). For example, methods focusing on data selection (Li et al., 2024a; Xia et al., 2024; Wang et al., 2023) and mixture (Ge et al., 2024) demonstrate promising capabilities to enhance model performance, particularly through semantic diversity (Lu et al., 2024; Liu et al., 2024b).\nHowever, real-world applications frequently encounter unlabeled data and difficulties in domain labeling (Ge et al., 2023), posing challenges for data mixture methodologies that take the domain tags as a prior, as well as the data selection approaches, which often prioritize quality over diversity especially with data sourced from quite different domains.\nTo leverage the best of both worlds, in this work, we propose a new fine-tuning method for LLMs named DAAR, which encourages the given model to learn diversity as a reward signal, and then to autonomously select domain-undetermined training data to achieve a theoretically informed trade-off to maximize diversity for overall model performance enhancement.\nOur investigation begins by diving into the semantic diversity of LLM data, considering how to explicitly model it and how it can influence the overall performance of LLMs. We systematically construct contrastive data pools and conduct extensive empirical examinations of different distributional performances across various foundational LLM capabilities. Based on these observations, we provide theoretical discussions on a general assumption that a mixture of underlying component LLMs determines the data mixing effect, and the explanations regarding inter- and intra-diversity modeled from a data semantic perspective.\nOur theoretical insights illuminate that optimal solutions for overall model performance enhancement cannot be explicitly applied to domain-undetermined data. To address this, we integrate an external multi-layer perceptron (MLP) structure into the LLM to be tuned, acting as a trainable probe module conditioned on the inherent knowledge and weights of the given LLM. The probe module takes semantic entropy as a proxy measure of diversity, and output reward scores, and select suitable data subsets aligned with the LLM's underlying mixture distributions for better diversity. This is achieved by training the probe module on synthetically generated domain-aware data by the model itself, and using the data after selection to further self-fine-tuning. Extensive experiments on various state-of-the-art (SOTA) LLMs like the Qwen and LLaMA series verify the effectiveness of DAAR in enhancing overall model capabilities over many SOTA baseline methods.\nOur contributions are threefold:\n\u2022 We empirically and theoretically demonstrate how data plays a crucial role to enhance LLMs in semantic diversity, providing generalized modeling based on mixture of component models.\n\u2022 We propose a lightweight, self-rewarding method for selecting data in domain-undetermined scenarios, which utilizes synthesized model-aware data to maximize domain diversity and fine-tuning performance.\n\u2022 We verify that the proposed methods can effectively balance seven diverse benchmarks for improved overall performance when applied to representative LLMs, whereas other SOTA methods struggle in such challenging scenarios. Our code is open-sourced 1 to foster more in-depth understanding and future progress."}, {"title": "2. Related Work", "content": "Data Selection Fine-tuning is a pivotal training paradigm for enhancing LLMs' task-specific and domain-specific capabilities. Research has shown that a small set of instruction pairs can enable LLMs to follow major instructions effectively (Zhou et al., 2024; Chen et al., 2024a). This fine-tuning can be achieved through rule-based methods, which focus on attributes such as Error L2-Norm (Paul et al., 2021) and token length (Raffel et al., 2020). More recently, model-based heuristics have been explored, including methods based on instruction-following difficulty (Li et al., 2024a), GPT scoring (Chen et al., 2024d; Wettig et al., 2024), data model selection (Engstrom, 2024), and influence scores derived from loss (Xia et al., 2024).\nDiversity & Data Mixing A fundamental principle for LLMs is being able to handle diverse human requests, underscoring data diversity as essential for effective fine-tuning (Yu et al., 2022; Ding et al., 2023). Data diversity encompasses aspects such as data deduplication (Abbas et al., 2023), the coverage scope of tags (Lu et al., 2024), model-based diversity evaluation (Liu et al., 2024b; Zhang et al., 2024), and scaling properties (Song et al., 2024). Typically, data mixing approaches (Albalak et al., 2023; Ye et al., 2024; Ge et al., 2024) focus on adjusting the proportional weights of different domains to enhance model capabilities.\nOur Position From a modeling perspective, our work shares a focus with many data mixing methodologies regarding diversity measurement. However, we address a more challenging setting relevant to real-world applications, where available datasets often lack domain labels. This situation presents challenges for existing mixing methods in balancing label granularity and normalization, and in determining which domains should be considered as candidates for improved solution spaces and tuning performance.\nMethodologically, our approach aligns with the data selection category but differs in both the learning process (utilizing a rewarding model) and the selection criteria (employing semantic entropy). Additionally, we revisit the applicability of existing selection methods in multi-domain scenarios. This area is under-explored and requires new insights to avoid biased modeling of diverse domain distributions\u2014a topic we further investigate in this paper."}, {"title": "3. Motivational Observations", "content": "In this section, we will explore how data with different domain-specific diversity affects model capabilities through experimental data pools, featuring contrastive data distributions in terms of inter-domain and intra-domain diversity."}, {"title": "3.1. Seed Data Pools and Basic Setting", "content": "Data Pools and Sources To explore how selecting data samples from extensive and diverse data repositories affects the foundational capabilities of LLMs, we construct various data pools and consistently fine-tune LLMs on them. We then analyze the performance changes and attribute these changes to the different controlled data pools.\nThe seed data pool is sourced from following datasets: Dolly-15k (Conover et al., 2023) for common sense, Cot-en (Chung et al., 2024) for reasoning, Math-Instruct (Yue et al., 2024) for mathematics, and Code-Alpaca (Chaudhary, 2023) for coding. Each dataset was randomly tailored to 10,000 entries, resulting in a combined data pool of 40,000 entries. Following instruction tuning practices (Zhou et al., 2024; Liu et al., 2024b), we then uniformly sample 8,000 data entries as a referenced data pool for random baseline. In subsequent sections, we will introduce how to construct other comparative pools with the same size to random pool.\nBenchmarks Aligned with representative capabilities of leading open-source LLMs, we select the following widely used evaluation sets: NQ (Kwiatkowski et al., 2019) and TriviaQA (Joshi et al., 2017) for common sense, Hellaswag (Zellers et al., 2019) for reasoning, GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021) for mathematics, MBPP (Austin et al., 2021) and HumanEval (Chen et al., 2021) for coding. To evaluate the comprehensive performance of LLMs across domains, we employ the average metric (AVG) as the primary evaluation criterion.\nModels & Implementation To ensure the effectiveness and applicability of our empirical findings, we employ the Qwen2 series (Qwen2-7B & Qwen2.5-7B) (Yang et al., 2024) and the Llama3.1-8B (Dubey et al., 2024) as representative SOTA base models to be fine-tuned. All experiments are conducted under identical training and evaluation protocols with two independent repetitions. Full platform, training and evaluation details are provided in Appendix B."}, {"title": "3.2. Data Pools with Contrastive Distributions", "content": "To systematically analyze the impact of domain-specific diversity patterns on model capabilities, we propose a contrastive construction with three phases: (A) Foundational Definitions, (B) Diversity Metric Formulation, and (C) Distribution Synthesis.\n(A) Foundational Definitions Let the composite dataset D = \\cup_{k=1}^{K} D_{k} comprise K = 4 distinct domains, where each domain subset $D_k$ contains $N_k = |D_k|$ samples. We represent each data instance through its semantic embedding $x_i^{(k)} \\in \\mathbb{R}^d$ extracted from the Embedding layer of the pretrained LLM, capturing high-dimensional semantic features. The domain centroid $C_k$ serves as the semantic prototype:\n$C_{k} = \\frac{1}{N_{k}} \\sum_{i=1}^{N_{k}} x_{i}^{(k)}$\nThis centroid-based representation enables geometric interpretation of domain characteristics in the embedding space. We dissect data diversity into two complementary aspects:\n(B.1) Inter-Diversity It quantifies the diversity between distinct domains through centroid geometry. For sample $x^{(k)}$, its cross-domain similarity is measured by:\n$inter(x^{(k)}) = \\sum_{j=1 \\\\ j \\\\neq k}^{K} \\frac{x^{(k)} \\cdot C_{j}}{||x^{(k)}|| ||C_{j}||}$ (2)\nThe global inter-diversity metric $\\Phi_{inter}$ computes the expected pairwise centroid distance:\n$\\Phi_{inter} = E_{k \\\\neq l} [||C_{k} - C_{l}||^{2}] = \\frac{1}{K-1} \\sum_{k=1}^{K} \\sum_{l=k+1}^{K} ||C_{k} - C_{i}||^{2}$.\nThis formulation reflects a key insight: maximizing $\\Phi_{inter}$ encourages domain separation, while minimization leads to overlapping representations. Fig. 1 demonstrates this continuum through t-SNE projections - high $\\Phi_{inter}$ manifests as distinct cluster separation with clear margins (Fig. 1.(c)), whereas low values produce entangled distributions (Fig. 1.(d)). Full analysis is detailed in Appendix D.1.\n(B.2) Intra-Diversity Focusing solely on the separation between different domains may hinder the model's ability to learn the knowledge specific to a given domain. Hence we measure variation within each domain. We calculate sample similarity to its domain center:\n$intra(x^{(k)}) = \\frac{x^{(k)} \\cdot C_{k}}{||x_{k}|| ||C_{k}||}$ (4)"}, {"title": "3.3. Experimental Observations", "content": "Table 1 presents comprehensive evaluations across seven benchmarks, where the notation Inter-Diversity (X-Y) indicates samples ranked in the top (100-Y)% to (100-X)% of cross-domain similarity scores. Due to space constraints, we present only the results for the top 20%, middle 20%, and bottom 20%. More results can be found in Appendix D.7.\nDiversity-Aware Performance: Our diversity-controlled selections reveal two critical observations:\n\u2022 Varied Improvement Patterns: Both models demonstrate marked improvements over RAW distributions across all diversity conditions, but the effects of their improvements vary. For Llama3.1-8B, Inter-D (80-100) achieves 38.98 average accuracy (+3.12 over RAW), outperforming the RANDOM baseline by 1.71, while Inter-D (0-20) is below RANDOM of 0.09.\n\u2022 Model-dependent Performance Peak: Each model exhibits distinct optimal operating points along the diversity spectrum. Llama3.1-8B reaches peak performance at Inter-D (80-100) and Intra-D (80-100), suggesting complementary benefits from both diversity types. Qwen2-7B peaks in inter-type selection at low inter-diversity, while it peaks in intra-type selection at high intra-diversity."}, {"title": "4. Theoretical Analysis and Insights", "content": "In this section, we first formalize the optimization dynamics of multi-domain data aggregation for enhancing LLMs' comprehensive capabilities. Building on the empirical observations from Sec. 3, we then establish theoretical connections between distributional diversity and emergent model behaviors. Finally, we demonstrate the fundamental limitation of explicit diversity optimization in undetermined-domain scenarios."}, {"title": "4.1. Ideal Optimization Formulation", "content": "During the training process of LLMs, various sources of labeled data are often collected to enhance models' capabilities across multiple dimensions. Let K = {1, ..., K} represent the set of labeled domains, where K is determined by real-world task requirements (e.g., K = 4 in our experiments). For each domain k \u2208 K, data samples $x_i^{(k)} \\in D_k$ are generated according to a distribution $D_k$. The data distributions are assumed mutually distinct across different domains, leading to an ideal I.I.D. per-domain hypothesis: each domain k corresponds to an independent model $h_k \\in H$. The standard optimization goal thus can be:\n$\\forall k \\in K, \\qquad \\min_{h_{k} \\in H} \\mathcal{L}_{D_{k}}(h_{k}),$ (6)\nwhere $\\mathcal{L}_{D_{k}}(h_{k}) = \\mathbb{E}_{(x, y) \\sim D_{k}}[l(h_{k}(x), y)]$ is the empirical risk given a specific loss function $l$.\nCounterintuitive Findings If the distributions $\\{D_k\\}_{k=1}^{K}$ are strictly I.I.D., the risk $\\mathcal{L}_{D_{k}}(h_{k})$ should depend only on $D_k$. However, numerous previous studies (Zhao et al., 2024a; Tirumala et al., 2023) have demonstrated the presence of synergistic and antagonistic effects between different datasets. This is further demonstrated in Sec. 3 (Fig. 1)"}, {"title": "4.2. Introducing Mixture of Underlying Distributions", "content": "Inspired by the counterintuitive findings, we posit that each domain distribution $D_k$ arises from the mixture of M latent distributions $\\{ \\mathcal{D}_{m} \\}_{m=1}^{M}$ representing foundational LLM capabilities (M\u226a K in practice). This leads to our core statistical assumption:\nAssumption 4.1 (Latent Capability Structure). For each observed domain $k \\in \\mathcal{K}$, its data distribution decomposes into M latent capability distributions:\n$D_{k} = \\sum_{m=1}^{M} \\pi_{km} \\mathcal{D}_{m}, \\qquad \\sum_{m=1}^{M} \\pi_{km} = 1,$ (7)\nwhere $\\mathcal{D}_{m}$ indicates the m-th foundational capability. The weights $\u03c0_{km}$ reflect domain-specific capaility composition.\nTo analyze how data optimization interacts with latent capability distributions, we further posit that each foundational capability admits an optimal configuration:\nAssumption 4.2 (Capability Optimality). Each foundational capability admits a unique optimal predictor:\n$\\exists h_{0_{m}} \\in \\mathcal{H} s.t. \\theta_{m}^{*} = argmin \\mathbb{E}_{(x, y) \\sim \\mathcal{D}_{m}} [l(h_{\\theta}(x), y)].$ (8)\nThe loss $l$ is strongly convex, holding for cross-entropy or MSE, where $h_{0_{m}}$, termed the component model, represents a model's manifestation of specific foundational capabilities."}, {"title": "4.3. Diversity Influence on the Optimization", "content": "Building upon Proposition 4.3, we analyze the intrinsic relationship between diversity and capability composition. The mixture coefficients $\u03c0_{km}$ inherently govern both inter- and intra-domain diversity formulated in Sec. 3.2, differing in their geometric properties in the latent capability space.\nCentroids with Coefficients Let $C_m \\in \\mathbb{R}^d$ denote the centroid vector of latent capability $\\mathcal{D}_m$ in the embedding space. The domain centroid $C_k$ can be expressed as:\n$C_{k} = \\sum_{m=1}^{M} \\pi_{km}C_{m}$. (10)\nThis linear combination implies that the inter- and intra-diversity are determined by $\u03c0_{km}$ configurations:"}, {"title": "4.4. Theoretical Insights", "content": "Collectively, results in this section provide a unified framework to explain the varied patterns and existence of peak performance observed in Sec. 3.3, providing theoretical insights and guidance for designing our following algorithm dealing with undetermined data.\nDiversity's Influence on LLMs' Capability: Based on Proposition 4.4, we conjecture that the Inter-/Intra-Diversity, which can be explicitly modeled by \u03c0, also governs \u03c0-variation control for component model mixing and thus the final overall performance. Proposition 4.3 further suggests that improper diversity levels may induce model suboptimality by constraining overall capability, consistent with the distribution-dependent empirical results in Sec. 3.\nRobust Diversity Selection Strategies: Based on Proposition 4.4, it can be observed that averaging the \u03c0km helps to mitigate the missing of underlying optimal predictor $\u03b8_m^*$. From this view, the random baseline can gain relatively stable improvements in expectation. A more robust yet simple strategy can be derived: ensemble across candidate models trained on multiple data pools with distribution-varied inter- and intra-diversity, e.g., via techniques like voting or model weights averaging. This can be corroborated by results in Table 1, such as the case for Llama3.1-8B.\nChallenges in Undetermined-Domain Scenarios: However, all the selection strategies discussed so far face intractable solutions in undetermined domains with explicit domain labels, as Proposition 4.4 technically revealed: (1) ambiguous domain boundaries, e.g., blended instruction types; (2) partial overlaps in latent capabilities; and (3) absence of centroid priors $\\{C_m\\}$. These intrinsic constraints motivate our model-aware diversity reward in the following Sec. 5, which harnesses domain-diversity awareness into reward modeling without requiring explicit parameterization for such structural assumptions."}, {"title": "5. DAAR: Diversity as a Reward", "content": "To address the challenges identified and leverage the insights gained in previous Sec. 3 and Sec. 4, we establish a data selection method DAAR guided by diversity-aware reward signals. It comprises three key components illustrated in subsequent sections: (1) model-aware centroid synthesis, (2) two-stage training with reward probe, and (3) diversity-driven data selection."}, {"title": "5.1. Model-Aware Training Data", "content": "Model-Aware Centroid Construction The proposed method initiates with centroid self-synthesis through a two-phase generation process to address two fundamental challenges: (1) eliminating dependency on human annotations through automated domain prototyping, and (2) capturing the base model's intrinsic feature space geometry for model-aware domain separation.\n\u2022 Phase 1 - Seed Generation: For each domain k, generate 5 seed samples $S_k^{(0)}$ via zero-shot prompting with domain-specific description templates, establishing initial semantic anchors. The prompt details and ablation of choices on sample number are provided in Appendix B.7.\n\u2022 Phase 2 - Diversity Augmentation: Iteratively expand $S_k^{(t)}$ through context-aware generation, conditioned on a sliding window buffer with 3 random anchors sampled from the (t-1) iteration $S_k^{(t-1)}$. The generated sample x' will be admitted during the iteration via geometric rejection sampling:\n$\\max_{x \\in S^{(t-1)}} cos(M_{ebd}(x'), M_{ebd}(x)) < \\tau = 0.85,$ (13)\nwhere $M_{ebd}()$ indicates the embedding layer outputs of the given LLM. This process terminates when |$S_k$| = 30. More analysis and ablation on these choices are provided in the Appendix. In Appendix D.3, we found that the adopted number of samples sufficiently leads to a stable convergence and larger data quantity does not impact the final centroids. In Appendix D.4, we found that this synthetic data has the ability to produce domain-representative data with clear distinction. Interestingly, the generated content consistently exhibits the greatest divergence between common sense, reasoning, and coding domains across model architectures and parameters.\nThe domain centroid is then computed from the final augmented set $S_k$ using the model's knowledge prior:\n$C_{k} = \\frac{1}{|S_{k}|} \\sum_{x_{i} \\in S_{k}} M_{ebd}(X_{i}).$ (14)\nThis captures the LLM's intrinsic feature space geometry while eliminating dependency on human annotations."}, {"title": "5.2. Training for Self-Rewarding Abilities", "content": "Stage 1: Domain Discrimination The proposed DAAR then establishes model-aware domain discrimination abilities through a multi-layer perceptron (MLP) probe module, Vdom, attached to the layer-3 of the LLMs $M_3(x)$. The probe will be trained meanwhile all the parameters of the LLM are frozen. This achieves a preferable balance between effectiveness and cost, with detailed analysis regarding the choice of Layer-3 presented in Appendix D.5. Specifically, with pseudo-label \u1ef9, we can compute domain probabilities as:\n$p_{k}(x) = softmax (\\mathcal{V}_{dom}(M_{3}(x))), \\qquad \\mathcal{V}_{dom}: \\mathbb{R}^{d} \\rightarrow \\mathbb{R}^{K},$ (16)\nwhere dom is optimized via cross-entropy loss function:\n$\\mathcal{L}_{dom} = \\frac{1}{|D_{probe}|} \\sum_{(x, \\tilde{y}) \\in D_{probe}} \\sum_{k=1}^{K} \\mathbb{I}[k=\\tilde{y}] \\log p_{k} (x),$ (17)\nwhere $\\mathbb{I}[k=\\tilde{y}]$ denotes the indicator function. We employ single-sample batches with the AdamW optimizer to prevent gradient averaging across domains. Training consistently converges and achieves 92.7% validation accuracy on domain classification, as shown in Fig. 2 (a).\nStage 2: Diversity Rewarding Building on the stabilized domain probe module, we quantify sample-level diversity through predictive entropy:\n$\\mathcal{H}(x) = - \\sum_{k=1}^{K} p_{k}(x) \\log p_{k} (x).$ (18)\nTo enable efficient reward computation during data selection, we then train another 5-layer MLP div to directly estimate $\\mathcal{H}(x)$ from $M_3(x)$:\n$\\hat{\\mathcal{H}}(x) = \\mathcal{V}_{div}(M_{3}(x)), \\qquad \\mathcal{V}_{div}: \\mathbb{R}^{d} \\rightarrow \\mathbb{R}^{+}.$ (19)\nThis diversity probe module Vdiv shares Vdom's architecture up to its final layer (replaced with single-output regression head), trained using entropy-scaled MSE:\n$\\mathcal{L}_{div} = \\frac{1}{|D_{probe}|} \\sum_{x \\in D_{probe}} (\\mathcal{H}(x) - \\hat{\\mathcal{H}}(x))^{2}.$ (20)\nThis module is also well-converged as shown in Fig. 2 (b).\nData Selection: After training the module div, we can use its output to select data samples. Building on the theoretical insights in Sec. 4.4, data points that are closer to other centroids and more dispersed within their own centroid are more beneficial for enhancing the comprehensive capabilities of the model. Therefore, we use the predicted entropy-diversity score as a reward, selecting the top 20% with the highest scores as the final data subset for fine-tuning."}, {"title": "5.3. Empirical Validation and Main Results", "content": "To validate the efficacy of DAAR, we conduct experiments comparing more SOTA methods on the data pools in Sec. 3.1 with critical modifications: all domain-specific labels are deliberately stripped. This constraint mimics more challenging real-world scenarios and precludes direct comparison with data mixture methods requiring domain label prior.\nBaselines We use the following data selection methods for comprehensive evaluation: (1) RANDOM SELECTION: traditional random sampling; (2) INSTRUCTION LEN: measuring instruction complexity by token count (Cao et al., 2023); (3) ALPAGASUS (Chen et al., 2024d): using ChatGPT for direct quality scoring of instruction pairs; (4-5) INSTAG (Lu et al., 2024): semantic analysis approach with INSTAG-C (complexity scoring via tag quantity) and INSTAG-D (diversity measurement through tag set expansion); (6) SUPERFILTER (Li et al., 2024a): response-loss-based complexity estimation using compact models; (7-9) DEITA (Liu et al., 2024b): model-driven evaluation with DEITA-C (complexity scoring), DEITA-Q (quality scoring), and DEITA-D (diversity-aware selection). Detailed configurations for these baselines are in Appendix B.4."}, {"title": "5.3.1. OVERALL PERFORMANCE", "content": "The experimental results are presented in Table 2, where INSTAG-BEST and DEITA-BEST represent the optimal variants from their respective method families. Our experiments clearly demonstrate the effectiveness of DAAR across three major language models and seven challenging benchmarks. A detailed analysis is provided below.\nHigh-Difficulty Scenario: The task of balanced capability enhancement proves particularly challenging for existing methods. While some baselines achieve strong performance on specific tasks (e.g., SuperFilter's 40.55 on HumanEval for Llama3.1), they suffer from catastrophic performance drops in other domains (e.g., SuperFilter's 6.05 on MATH). Only three baseline methods perform better than RANDOM selection, with notably severe degradation applied in Qwen-series models, all of which fall below the RANDOM performance. We hypothesize this stems from over-specialization \u2013 excessive focus on narrow capability peaks at the expense of broad competence (visualized in Appendix B.5). Our method's robustness stems from preventing extreme distribution shifts through diversity constraints.\nUniversal Superiority: DAAR establishes new SOTA averages across all models, surpassing the best baselines by +0.14 (Llama3.1), +0.97 (Qwen2), and +1.11 (Qwen2.5). The proposed method uniquely achieves dual optimization in critical capabilities: Mathematical Reasoning: Scores 38.1 MATH (Qwen2) and 16.70 MATH (Qwen2.5), with 7.4% and 27.0% higher than respective random baselines. Coding Proficiency: Maintains 64.94 HumanEval (Qwen2) and 64.20 MBPP (Qwen2.5) accuracy with <1% degradation from peak performance. This demonstrates DAAR'S ability to enhance challenging STEM capabilities while preserving core competencies, a critical advancement over specialized but unstable baselines.\nCost-Efficiency and Flexibility: Compared to baseline methods requiring GPT-based evaluators (ALPAGASUS, INSTAG) or full LLaMA-7B inference (DEITA), DAAR achieves superior efficiency through data-model co-optimizations. Our method makes the LLM ability of self-rewarding from dedicated data synthesis, and operates on frozen embeddings from layer 3 (vs. full 32-layer inference in comparable methods), reducing computational overhead while maintaining capability integrity. The lightweight 5-layer MLP probe module requires only 9GB GPU memory during training (vs. 18~24GB for LLM-based evaluators) and adds merely 76M parameters. This plug-and-play feature enables seamless integration of DAAR with existing LLMs without additional dependency management."}, {"title": "6. Conclusion and Future Work", "content": "In this paper, we propose a new approach to fine-tuning LLMs with data that lacks clear domain labels, using diversity as a guiding principle. Our method allows LLMs to automatically select and benefit from diverse datasets. By measuring semantic diversity with entropy, we employ a self-reward mechanism built upon the given LLM, identifying data that best fits the model's natural tendencies in terms of its underlying knowledge distribution. Our experiments with various SOTA LLMs show notable superiority of the method over SOTA methods, highlighting the potential of data diversity to enhance model overall performance.\nThis research demonstrates feasibility of reward modeling on LLM data diversity and deepens the understanding of its utilities, especially when domain labels are missing or uncertain. Future work could include developing methods to efficiently adjust diversity measures and learning algorithm, thus supporting LLMs in self-evolving towards artificial general intelligence in dynamic environments."}]}