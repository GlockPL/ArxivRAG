{"title": "xPatch: Dual-Stream Time Series Forecasting with Exponential Seasonal-Trend Decomposition", "authors": ["Artyom Stitsyuk", "Jaesik Choi"], "abstract": "In recent years, the application of transformer-based models in time-series forecasting has received significant attention. While often demonstrating promising results, the transformer architecture encounters challenges in fully exploiting the temporal relations within time series data due to its attention mechanism. In this work, we design exponential Patch (xPatch for short), a novel dual-stream architecture that utilizes exponential decomposition. Inspired by the classical exponential smoothing approaches, xPatch introduces the innovative seasonal-trend exponential decomposition module. Additionally, we propose a dual-flow architecture that consists of an MLP-based linear stream and a CNN-based non-linear stream. This model investigates the benefits of employing patching and channel-independence techniques within a non-transformer model. Finally, we develop a robust arct-angent loss function and a sigmoid learning rate adjustment scheme, which prevent overfitting and boost forecasting performance. The code is available at the following repository: https://github.com/stitsyuk/xPatch.", "sections": [{"title": "Introduction", "content": "Long-term time series forecasting (LTSF) is one of the fundamental tasks in time series analysis. The task is focused on predicting future values over an extended period, based on historical data. With the advent of deep learning models, they have recently demonstrated superior performance in LTSF compared to traditional approaches such as ARIMA and LSTM.\nTransformer-based models have revolutionized the LTSF task, enabling powerful AI systems to achieve state-of-the-art performance. The transformer architecture is considered highly successful in capturing semantic correlations among elements in long sequences. Recent research efforts have been primarily focused on adapting transformers to the LTSF task and addressing such limitations of the vanilla transformer as quadratic time and memory complexity.\nThe self-attention mechanism employed in transformers is permutation-invariant. Although techniques like positional encoding can partially retain ordering information, preserving temporal information remains a challenge for transformer-based models. This limitation can adversely affect the performance of the LTSF task dealing with a continuous set of points. As a result, the effectiveness of transformers in the LTSF task has been challenged by a simple linear approach utilizing a Multi-Layer Perceptron (MLP) network. Surprisingly, a simple linear model named DLinear has surpassed the state-of-the-art forecasting performance of all previous transformer-based models, raising a fundamental question: \u201cAre Transformers effective for long-term time series forecasting?\u201d.\nDue to the non-stationary nature of real-world systems, time series data usually contain complex temporal patterns. To handle this complexity and non-stationarity, many recent LTSF models have adopted a paradigm of decomposing inputs. They use a seasonal-trend decomposition to capture linear trend features and non-linear seasonal variations. For handling time series trend features, certain transformer-based models, including Autoformer and FEDformer incorporate seasonal-trend data decomposition. By partitioning the signal into two components, each with distinct function behavior, it becomes more feasible to capture semantic features from each component and make separate predictions.\nBoth Autoformer and FEDformer focus on refining the transformer architecture by introducing an auto-correlation mechanism and a frequency-enhanced method while decomposing the signal using a simple average pooling method. This technique requires padding at both ends, essentially repeating the last and first values. Consequently, we argue that this approach introduces a bias towards the initial and final values, potentially altering the behavior of trend values.\nWe propose a simple yet effective decomposition technique based on a generally applicable time series smoothing method named Exponential Moving Average (EMA). The proposed strategy assigns exponentially decreasing weights over time, facilitating more efficient feature learning from the decomposed data. The resulting exponentially smoothed sequence represents the trend, while the residual difference encapsulates the seasonality.\nCurrently, the state-of-the-art models for the LTSF task are transformer-based architectures CARD and PatchTST. These models rely on channel-independence and segmentation of time series into patches, which are used as input tokens for the transformer. However, we assume that the permutation-invariance of the attention mechanism in transformers may impede the model from attaining the optimal forecasting performance. Therefore, we are aiming to explore channel-independence and patching approaches within a non-transformer architecture, proposing the xPatch model.\nIn this study, we introduce the utilization of the exponential seasonal-trend decomposition technique. Furthermore, we propose a robust arctangent loss with weight decay and a novel learning rate adjustment strategy that improves training adaptability. Additionally, we present the xPatch, a novel dual-flow network architecture that integrates Convolutional Neural Networks (CNNs), Multi-Layer Perceptrons (MLPs), patching, channel-independence, exponential seasonal-trend decomposition, and dual stream prediction.\nWe summarized our main contributions as follows:\n\u2022 We propose a novel method for seasonal-trend decomposition that utilizes an Exponential Moving Average (EMA).\n\u2022 We introduce the dual-flow network and investigate the patching and channel-independence approaches within the CNN-based backbone.\n\u2022 We develop a robust arctangent loss and a novel sigmoid learning rate adjustment scheme with a warm-up that results in smoother training."}, {"title": "Related Work", "content": "Informer is the first well-known transformer-based model designed for the LTSF task. It employs ProbSparse self-attention and a generative style decoder for addressing quadratic time and memory complexity. Notably, this work also contributes to the field by curating data and introducing the Electricity Transformer Temperature (ETT) benchmark dataset that is now commonly used for LTSF experiments by most of the models.\nTimesNet utilizes Fourier Transform to decompose time series into multiple components with varying period lengths, enhancing its focus on temporal variation modeling. The official repository provides a forecasting protocol with standardized hyperparameter settings and fairly implemented baselines.\nTo address the issue of non-stationarity in time series data, several models employ series decomposition to better capture complex temporal patterns. Autoformer and FEDformer are two recent transformer-based solutions for the LTSF task, leveraging auto-correlation mechanism and frequency-enhanced structure, respectively. Both models incorporate seasonal-trend decomposition within each neural block to enhance the predictability of time-series data. Specifically, they apply a moving average kernel to the input sequence with padding at both ends, extracting the trend component. The difference between the original time series and the extracted trend component is identified as the seasonal component.\nDLinear is a recent one-layer linear model that uses seasonal-trend decomposition as a preprocessing step. Initially, the model decomposes the raw data into trend and seasonal components using a moving average technique. Two linear layers are then applied independently to each of these components. The resulting features are subsequently aggregated to generate the final prediction.\nMICN is a recent CNN-based solution that employs multi-scale hybrid seasonal-trend decomposition. After decomposing the input series into seasonal and trend components, the model integrates both global and local contexts to enhance forecasting accuracy.\nTimeMixer is an MLP-based approach that employs a decomposable multiscale-mixing method. The model uses the same series decomposition block from Autoformer to break down multiscale time series into multiple seasonal and trend components. By leveraging the multiscale past information obtained after seasonal and trend mixing, the model predicts future values.\nETSformer and CARD are two transformer-based architectures that incorporate the exponential smoothing approach. ETSformer introduces Exponential Smoothing Attention (ESA), while CARD applies exponential smoothing to the query and key tokens before the token blending module within one prediction head of the attention mechanism. In contrast to these models, the proposed xPatch architecture employs Exponential Moving Average (EMA) decomposition to separate the time series into trend and seasonal components, which are then processed separately.\nCrossformer and PatchTST are transformer-based models that introduce a segmentation technique to LTSF. PatchTST divides time series data into subseries-level patches that serve as input tokens for the transformer. This approach is motivated by the vision transformer and designed for LTSF with channel-independence. Currently, PatchTST is recognized as the state-of-the-art solution for multivariate long-term forecasting. In our proposed xPatch model, we also incorporate patching and channel-independence approaches. Given that xPatch is a CNN-based approach, we investigate whether the superior performance of PatchTST can be attributed to its patching and channel-independence modules rather than its transformer architecture. To explore this, we examine if a CNN-based model can achieve improved results by leveraging these techniques.\nMobileNet and ConvMixer are notable models designed for Computer Vision (CV) tasks that demonstrate the advantages of depthwise separable convolutions. In the proposed xPatch approach, we incorporate depthwise separable convolution as the non-linear stream of the dual-flow network."}, {"title": "Proposed Method", "content": "In multivariate time series forecasting, given the observation of the historical L values $x = (X_1,X_2, ..., X_L)$, the task is to predict the future T timesteps $x = (X_{L+1}, X_{L+2}, ..., X_{L+T})$. Each $x_t$ value at timestep t is multivariate, representing a vector of M variables. Therefore, the multivariate lookback series is denoted as $x \\in R^{M \\times L}$ and the multivariate prediction is represented by $x \\in R^{M \\times T}$."}, {"title": "Seasonal-Trend Decomposition", "content": "Seasonal-trend decomposition facilitates the learning of complex temporal patterns by separating the time series signal into trend and seasonal components. Trend features generally represent the long-term direction of the data, which can be linear or smoothly varying. In contrast, seasonal components capture repeating patterns or cycles that occur at regular intervals and are often non-linear due to the complexities and variations in periodic behavior. The model first learns the features of these components individually and then combines them to generate the final forecast.\nSimple Moving Average (SMA) is the decomposition approach utilized in Autoformer , FEDformer , DLinear , MICN , and TimeMixer models. SMA is defined as the unweighted mean of the previous k data points.\nMoving average mean point $s_t$ of the k entries with t being moving step, n being dataset length, and $X = X_1, X_2, ..., X_n$ being data points is calculated as:\n$s_t = \\frac{X_t + X_{t+1} + ... + X_{t+k-1}}{k} = \\frac{1}{k} \\sum_{i=t}^{t+k-1} X_i$ (1)\n$X_T = AvgPool(Padding(X))$\n$X_s = X \u2013 X_T$\nwhere AvgPool(\u00b7) denotes moving average with the padding operation, while $X_T$ and $X_s$ correspond to trend and seasonality components. Padding is employed to maintain the length of the time series unchanged after performing average pooling.\nFirstly, we argue that the average pooling operation results in the loss of significant trend features (see Appendix B). Additionally, alignment requires padding on both ends of the series, which can distort the sequence at the head and tail.\nSecondly, the primary goal of decomposition is to enhance the interpretability of both decomposed signals. This entails improving the clarity of the trend and seasonality components while enriching them with more distinct features for learning. However, SMA produces an overly simplistic trend signal with limited diverse features and a complex seasonality pattern. As a result, we investigate an alternative decomposition method to address this issue."}, {"title": "Model Architecture", "content": "Channel-Independence. The multivariate time series $x = (X_1,X_2, ..., X_L)$ is divided into M univariate sequences $x^{(i)} = (x_1^{(i)},x_2^{(i)},...,x_L^{(i)})$, where $x^{(i)} \\in R^L$ and L is lookback of recent historical data points. Each of these univariate series is then individually fed into the backbone model, which consequently generates a prediction sequence $(x_{L+1}^{(i)}, x_{L+2}^{(i)}, ..., x_{L+T}^{(i)})$, where $x^{(i)} \\in R^T$ and T is future steps observations. This partitioning approach has proven to work well in both linear models and transformers.\nExponential Decomposition. Using the EMA method, we decompose each univariate series into trend and seasonality components, which are then processed separately by the dual-flow architecture. After processing, the learned trend and seasonal features are aggregated and passed to the final output layer to comprise the final prediction as illustrated in Figure 3. Details on optimization and ablation studies of EMA are available in Appendix D, E.\nDual Flow Net. As the main backbone, we employ two distinct flows to analyze trend and seasonality: linear and non-linear streams. The trend component is processed through the linear MLP-based stream, while the seasonal component is handled by the non-linear CNN-based block.\nSeasonality represents periodic fluctuations around a constant level, meaning that the statistical properties of these fluctuations, such as mean and variance, remain stable over time, meaning that the seasonal component is stationary. In contrast, the trend reflects long-term progression with either increasing or decreasing behavior and a changing mean, which makes the trend component non-stationary.\nTo summarize, in most cases, the seasonal component is non-linear and stationary, while the trend component is linear and non-stationary. However, some datasets might exhibit unusual behavior, such as a stationary trend. Therefore, the dual-stream architecture is designed to enhance the model's adaptability to both stationary and non-stationary data. For the exploration of the dual-flow architecture, see Appendix F.\nLinear Stream. The linear stream is an MLP-based network that includes average pooling and layer normalization, intentionally omitting activation functions to emphasize linear features.\nThe decomposed data $x^{(i)}$ is processed through two linear blocks, each consisting of a fully connected layer followed by average pooling with a kernel k = 2 for feature smoothing and layer normalization for training stability. Each linear layer and average pooling operation contribute to dimensionality reduction, encouraging the network to compress feature representations to fit the available space effectively. This reduction in the number of features, combined with the absence of activation functions and a bottleneck architecture, aims to retain only the most significant linear features of the smoothed trend.\n$x^{(i)} = LayerNorm(AvgPool(Linear(x^{(i)}), k = 2))$ (3)\nThe final expansion layer takes the bottleneck representation and upscales it to the prediction length.\n$x_{lin}^{(i)} = Linear(x^{(i)})$ (4)\nPatching. Patching is a technique inspired by the vision transformer and was first introduced in the context of LTSF by PatchTST. This method unfolds each univariate time series using a sliding window. We incorporate patching into the non-linear block to emphasize repetitive seasonal features. By using patching, the model can better focus on these repetitive patterns, effectively capturing their inter-pattern dependencies more effectively.\nThe patch length is denoted as P, and the non-overlapping region between two consecutive patches is referred to as stride S. We apply patching in the non-linear stream to each normalized univariate decomposed sequence $x^{(i)} \\in R^L$, which generates a sequence of N 2D patches $x_p^{(i)} \\in R^{N \\times P}$. The number of patches is calculated as $N = [\\frac{L}{S}] + 2$. In our implementation, for a fair comparison with PatchTST and CARD, we adopt their setup for patch embedding, setting P = 16 and S = 8.\nNon-linear Stream. The non-linear stream is a CNN-based network that introduces non-linearity through activa-"}, {"title": "Loss Function", "content": "Mean Squared Error (MSE) loss is a training loss scheme commonly used by LTSF models. The MSE loss $L_{MSE}$ between the predicted univariate sequence $\\hat{x}_{1:T}^{(i)}$ and the ground truth observations $x_{1:T}^{(i)}$, where T is future prediction length, is denoted as:\n$L_{MSE} = \\frac{1}{T} \\sum_{i=1}^{T} (\\hat{x}_{1:T}^{(i)} \u2013 x_{1:T}^{(i)})^2$ (13)\nThe recent transformer-based model CARD introduced a novel signal decay-based loss function, where they scale down the far-future Mean Absolute Error (MAE) loss to address the high variance. MAE was chosen since it is more resilient to outliers than MSE.\n$L_{CARD} = \\frac{1}{T} \\sum_{i=1}^{T} \\rho_i| \\hat{x}_{1:T}^{(i)} \u2013 x_{1:T}^{(i)}|$ (14)\nwhere $\\rho_i$ corresponds to the prediction point in the future. This training scheme was proven by CARD to be efficient and to increase the performance of existing models.\nTo identify a more effective scaling loss coefficient, we extend Equation (14) to a universally applicable MAE scalable loss function:\n$L = \\frac{1}{T} \\sum \\rho(i)| \\hat{x}_{1:T}^{(i)} \u2013 x_{1:T}^{(i)}|$ (15)\nwhere $\\rho(i)$ represents the scaling coefficient. Thus, the $L_{CARD}$ loss defined in Equation (14) emerges as a specific instance of the scalable loss function delineated in Equation (15), with $\\rho(i) = i^{-\\frac{1}{2}}$.\nWe find that the scaling coefficient $\\rho_{CARD}(i) = i^{-\\frac{1}{2}}$ exhibits a too rapid decrease rate for our task. Therefore, we propose a novel arctangent loss $L_{arctan}$, which features a"}, {"title": "Learning Rate Adjustment Scheme", "content": "Most recent LTSF models adapt standard learning rate adjustment technique. Learning rate at $a_t$ epoch t with initial learning rate $a_o$ is calculated as:\n$a_t = A_{t-1} * 0.5^{t-1}, for t > 1$ (18)\nThis strategy results in a decreasing learning rate with each successive epoch. Such a rapidly decreasing scheme was effective since the models were trained with a small number of epochs, usually limited to 10.\nPatchTST introduced a long training approach with an upper limit of 100 epochs and a new learning rate adjustment schedule:\n$a_t = \\alpha_o, \\qquad for t < 3,$\n(19)\n$A_t = a_{t-1} * 0.9^{t-3}, \\qquad for t \\geq 3$ (20)\nConsequently, CARD developed a new linear warm-up of the model with subsequent cosine learning rate decay. Learning rate at $a_t$ epoch t with initial learning rate $a_o$, number of warmup epochs w, and upper limit of 100 epochs is calculated as:\n$A_t = a_{t-1} * \\frac{t}{w}, \\qquad for t < w,$\n(21)\n$A_t = 0.5a_o(1 + cos(\\pi * \\frac{t - w}{100 - w})), \\qquad for t \\geq w$ (22)\nWe introduce a novel sigmoid learning rate adjustment scheme. The learning rate at $a_t$ epoch t, with an initial learning rate $a_o$, logistic growth rate k, decreasing curve smoothing rate s, and warm-up coefficient w, is calculated as follows:\n$A_t = \\frac{a_o}{1 + e^{-k(t - w)}}$  (23)"}, {"title": "Datasets", "content": "We conduct extensive experiments on nine real-world multivariate time series datasets, including Electricity Transform Temperature (ETTh1, ETTh2, ETTm1, ETTm2) Weather, Traffic, Electricity, Exchange-rate, ILI , and Solar-energy .\nEvaluation Metrics. Following previous works, we use Mean Squared Error (MSE) and Mean Absolute Error (MAE) metrics to assess the performance.\nImplementation Details. All the experiments are implemented in PyTorch, and conducted on a single Quadro RTX 6000 GPU.\nBaselines. We choose the last state-of-the-art LTSF models, including Autoformer , FEDformer , ETSformer , TimesNet , DLinear , RLinear , MICN , PatchTST , iTransformer , TimeMixer , and CARD as baselines for our experiments.\nUnified Experimental Settings. To ensure a fair comparison, we conduct 2 types of experiments. The first experiment uses unified settings based on the forecasting protocol proposed by TimesNet : a lookback length L = 36, prediction lengths T = {24, 36, 48, 60} for the ILI dataset, and L = 96, T = {96, 192, 336, 720} for all other datasets. The averaged results are reported in Table 1."}, {"title": "D EMA Optimization", "content": "The straightforward implementation of EMA introduced in Equation (2) requires a for loop, which is O(n) time complexity. Therefore, we are aiming to optimize the EMA decomposition module to constant time complexity. The equation can be derived as the following sequence:\n$S_t = A x_t + (1 \u2013 a)s_{t-1}$\n$= Ax_t + (1 \u2013 a)(ax_{t-1} + (1 \u2013 a)s_{t-2})$\n$= Ax_t + (1 \u2013 a)ax_{t-1} + (1 \u2013 a)^2s_{t-2}$\n (24)\n$= ... + (1 \u2013 a)^2(ax_{t-2} + (1 \u2013 a)s_{t-3})$\n$= ... + (1 \u2013 a)^2ax_{t-2} + (1 \u2013 a)^3s_{t-3}$\n$= ax_t + ... + (1 \u2013 a)^{t-1}ax_1 + (1 \u2013 a)^t x_o$\nTo match the order of the data sequence $x = [x_o, x_1, ..., x_{t-1}, x_t]$, we rewrite the Equation (24) backwards:"}, {"title": "E Ablation Study on EMA", "content": "Initially, we conducted experiments to determine the suitable \u03b1 parameter for each model. We tested five variations of fixed \u03b1 = {0.1, 0.3, 0.5, 0.7, 0.9}, where \u03b1 = 0.1 represents the heaviest smoothing and \u03b1 = 0.9 indicates slight smoothing. Since our main goal is to make both trend and seasonality streams more interpretable, we assume that smaller \u03b1 values are more appropriate for this task. Larger values lead to the problem that the trend is not smooth enough, resulting in a complicated trend and easy seasonality. For this experiment, we utilized large datasets: Weather, Traffic, and Electricity since they have longer time series data.\nIt is important to note that decomposition is used in xPatch, DLinear, and PatchTST differently in comparison to Autoformer and FEDformer. In xPatch, DLinear, and PatchTST, data is decomposed into trend and seasonality, and both trend and seasonality are separately predicted by the model. Since the decomposed data is the final shape that should be predicted, both trend and seasonality should be close in terms of complexity and interpretability.\nOn the other hand, Autoformer and FEDformer employ the inner series decomposition blocks. The transformer encoder first eliminates the long-term trend part by series decomposition blocks and focuses on seasonal pattern modeling. The decoder accumulates the trend part extracted from hidden variables progressively. The past seasonal information from the encoder is utilized by the encoder-decoder Auto-Correlation. In both Autoformer and FEDformer, the encoder contains two series decomposition blocks, while the decoder contains three series decomposition blocks, where they gradually smooth the data to focus on trend, rather than decompose the data into two streams."}, {"title": "F Dual Flow Net", "content": "We explore the impact of the dual flow network in xPatch architecture and assess the contribution of each stream. Figure 6 compares the dual flow network with separate non-linear and linear streams to analyze the contribution and behavior of each flow. The four possible configurations:\n\u2022 Original: Seasonality \u2192 non-linear stream, Trend \u2192 linear stream\n\u2022 Reversed: Seasonality \u2192 linear stream, Trend \u2192 non-linear stream\n\u2022 Non-linear only: Seasonality \u2192 non-linear stream, Trend \u2192 non-linear stream\n\u2022 Linear only: Seasonality \u2192 linear stream, Trend \u2192 linear stream\nThe results reveal that the model benefits from the original configuration of the dual flow architecture, effectively selecting between linear and non-linear features, leading to improved forecasting performance."}, {"title": "G Ablation Study on Arctangent Loss", "content": "We find that the scaling coefficient $\\rho_{CARD}(i) = i^{-\\frac{1}{2}}$ from Equation (15) exhibits a too rapid decrease rate for the long-term time series forecasting task. Therefore, we propose a novel approach by exploring the arctangent function, which features a slower increase rate compared to the exponential functions that were analyzed in the CARD paper.\nInitially, we employ the negative arctangent function -arctan(i) as a decreasing function required for our task. Subsequently, we perform a vertical translation to ensure that the function equals 1 when i = 1. In other words, we shift the entire graph of the function along the y-axis by y, solving the equation - arctan(1) + y = 1, which yields $y = \\frac{\\pi}{4} + 1$.\nTherefore, the arctangent loss $L_{arctan}$ between the predicted univariate sequence $\\hat{x}_{1:T}^{(i)}$ and the ground truth observations $x_{1:T}^{(i)}$, where T is future prediction length, $\\rho_{arctan}(i)$ is loss scaling coefficient, and m is arctangent scaling parameter is denoted as:\n$L_{arctan} = \\frac{1}{T} \\sum \\rho_{arctan}(i)| \\hat{x}_{1:T}^{(i)} \u2013 x_{1:T}^{(i)}|$ (29)\n$\\rho_{arctan}(i) = \\frac{1}{m(arctan(i) \u2013 \\frac{\\pi}{4}) + 1}$ (30)\nSubsequently, we investigate the arctangent loss function scaling parameter. In Table 8, we compare different arctangent loss scaling parameters m = {1, \\frac{1}{2}, \\frac{1}{3}, \\frac{1}{4}}, where m = 1 is the original arctangent loss function and m = 1 is the function closest to the original MAE loss function without scaling."}, {"title": "H Ablation Study on Sigmoid Learning Rate Adjustment Scheme", "content": "In Equation 22, CARD employs linear warm-up initially and then adjusts the learning rate according to one cycle of the cosine function for the remaining epochs. While we recognize the effectiveness of the initial warm-up approach, we propose that the entire adjustment scheme should be encapsulated within a single function featuring a non-linear warm-up. Therefore, we chose to investigate the sigmoid function.\nA logistic function is a common sigmoid curve with the following equation:\n$f(x) = \\frac{L}{1 + e^{-k(x \u2013 x_0)}}$ (32)\nwhere $x_0$ is the x value of the function's midpoint, L is the supremum of the values of the function, and k is the logistic growth rate or steepness of the curve.\nTo calculate the learning rate a for the current epoch t, the supremum L corresponds to the initialized learning rate \u03b1o, while the midpoint to serves as a warm-up coefficient denoted by w, as it extends the rising curve of the sigmoid function:\n$a_t = \\frac{\\alpha_o}{1 + e^{-k(t - w)}}$ (33)\nTo extend the function to slowly decrease after some point, Equation (33) requires updating with the subtraction of another sigmoid function featuring a smaller growth rate. Before delving into the analysis of the second sigmoid function, we examine the variable e:\n$e^{-k(t-w)} = e^{-kt+kw}$ (34)\nSince the $a_t$ function at t = 0 is intended to be equal to 0, we can revise the -kt term to \u2013kt, where s represents the smoothing coefficient:\n$e^{kw}e^{-kt} = e^{kw}e^{-(t-sw)}$ (35)\n$A_t = \\frac{a_o}{1 + e^{-k(t - w)}} \u2013 \\frac{\\alpha_o}{1 + e^{-(t - sw)}}$  (36)"}, {"title": "J Instance Normalization", "content": "Table 12 presents a comprehensive comparative analysis between the original state-of-the-art models and versions trained without using RevIN instance normalization. All models are tested with a lookback L = 96 for all datasets.\nThe forecasting performance of xPatch was enhanced by the RevIN module, with improvements of 8.67% in MSE and 6.53% in MAE, respectively. For CARD, instance normalization improved accuracy by 28.69% in MSE and 23.22% in MAE, while for PatchTST the gains are 9.96% in MSE and 10.07% in MAE, respectively.\nThe greater benefit of instance normalization observed in CARD and PatchTST can be attributed to xPatch's use of EMA decomposition. According to non-stationary transformer , statistical methods like ARIMA employ moving average decomposition for stationarization, while most recent state-of-the-art solutions rely on RevIN . Consequently, xPatch incorporates two mechanisms to address data non-stationarity and distribution shift, which explains its superior performance compared to CARD and PatchTST even without the use of RevIN."}]}