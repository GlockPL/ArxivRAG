{"title": "ACCURATE AND SCALABLE GRAPH NEURAL NETWORKS VIA MESSAGE INVARIANCE", "authors": ["Zhihao Shi", "Jie Wang", "Zhiwei Zhuang", "Xize Liang", "Bin Li", "Feng Wu"], "abstract": "Message passing-based graph neural networks (GNNs) have achieved great success in many real-world applications. For a sampled mini-batch of target nodes, the message passing process is divided into two parts: message passing between nodes within the batch (MPIB) and message passing from nodes outside the batch to those within it (\u041c\u0420\u043e\u0432). However, \u041c\u0420\u043e\u0432 recursively relies on higher-order out-of-batch neighbors, leading to an exponentially growing computational cost with respect to the number of layers. Due to the neighbor explosion, the whole message passing stores most nodes and edges on the GPU such that many GNNs are infeasible to large-scale graphs. To address this challenge, we propose an accurate and fast mini-batch approach for large graph transductive learning, namely topological compensation (TOP), which obtains the outputs of the whole message passing solely through MPIB, without the costly MP\u043e\u0432. The major pillar of TOP is a novel concept of message invariance, which defines message-invariant transformations to convert costly MPOB into fast MPIB. This ensures that the modified MPIB has the same output as the whole message passing. Experiments demonstrate that TOP is significantly faster than existing mini-batch methods by order of magnitude on vast graphs (millions of nodes and billions of edges) with limited accuracy degradation.", "sections": [{"title": "1 INTRODUCTION", "content": "Message passing-based graph neural networks (GNNs) have been successfully applied to many practical applications involving graph-structured data, such as social network prediction (Hamilton et al., 2017; Kipf & Welling, 2017; Deng et al., 2019), chip design Geng et al. (2025b); Bai et al. (2025); Wang et al. (2024b;c), combinatorial optimization Geng et al. (2023a; 2025a); Liu et al. (2025; 2023; 2024); Wang et al. (2023; 2024a); Ling et al. (2024), drug reaction (Do et al., 2019; Duvenaud et al., 2015; Geng et al., 2023b), and recommendation systems (Ying et al., 2018; Fan et al., 2019). The key idea of GNNs is to iteratively update the embeddings of each node based on its local neighborhood. Thus, as these iterations progress, each node embedding encodes more and more information from further reaches of the graph (Hamilton, 2020, Chap. 5).\nHowever, training GNNs on a large-scale graph is challenging due to the well-known neighbor explosion problem. Specifically, the embedding of a node at the l-th GNN layer depends on the embeddings of its local neighborhood at the (l \u2013 1)-th GNN layer. Thus, around the target mini-batch nodes, these message passing iterations of an L-layer GNN form a tree structure by unfolding their L-hop neighborhoods (Hamilton, 2020, Chap. 5), whose size exponentially increases with the GNN depth L (see Figure 1(a)). The exploded source neighborhoods may contain most nodes in the large-scale graph, leading to expensive computational costs.\nTo alleviate this problem, recent graph sampling techniques approximate the whole message passing with the small size of the source neighborhoods (Ma & Tang, 2021, Chap. 7). For example, node-wise (Hamilton et al., 2017; Chen et al., 2018a; Balin & Catalyurek, 2023) and layer-wise (Chen et al., 2018b; Zou et al., 2019; Huang et al., 2018) sampling recursively sample a small set of local neighbors over message passing layers. The expectation of the recursive sampling obtains the whole message passing and thus the recursive sampling is accurate and provably convergent (Chen et al., 2018a)."}, {"title": "2 RELATED WORK", "content": "In this section, we discuss some works related to our proposed method.\nNode-wise sampling. Node-wise sampling (Hamilton et al., 2017; Chen et al., 2018a; Yu et al., 2022) aggregates messages from a subset of uniformly sampled neighborhoods at each GNN layer, which decreases the bases in the exponentially increasing dependencies. The idea is originally proposed in GraphSAGE (Hamilton et al., 2017). VR-GCN (Chen et al., 2018a) further alleviates the bias and variance by historical embeddings, and then shows that its convergence rate to reach an \u025b-approximate stationary point is $N = O(\\varepsilon^{-1})$, where N denotes the number of iterations in Theorem 2 in (Chen et al., 2018a). GraphFM-IB further alleviates the staleness of the historical embeddings based on the idea of feature momentum. Although the node-wise sampling methods achieve the convergence rate of O(8-4), their computational complexity at each step is still exponentially increasing due to the neighborhood explosion issue.\nLayer-wise sampling. To avoid the exponentially growing computation of node-wise sampling, layer-wise sampling (Chen et al., 2018b; Zou et al., 2019; Huang et al., 2018) samples a fixed number of nodes for each GNN layer and then uses importance sampling (IS) to reduce variance. However, the optimal distribution of IS depends on the up-to-date embeddings, which are expensive. To tackle this problem, FastGCN (Chen et al., 2018b) proposes to approximate the optimal distribution of IS by the normalized adjacency matrix. Adapt (Huang et al., 2018) proposes a learnable sampled distribution to further alleviate the variance. Nevertheless, as the above-mentioned methods sample nodes independently in each GNN layer, the sampled nodes from two consecutive layers may be connected (Zou et al., 2019). Thus, LADIES (Zou et al., 2019) consider the dependency of sampled nodes between layers by one step forward. By combining the advantages of node-wise and layer-wise sampling approaches using Poisson sampling, LABOR (Balin & Catalyurek, 2023) significantly accelerates convergence under the same node sampling budget constraints.\nSubgraph sampling. Subgraph sampling methods sample a mini-batch and then construct the subgraph based on the mini-batch (Ma & Tang, 2021, Chap. 7). Thus, we can directly run GNNs on the subgraphs. One of the major challenges is to efficiently encode neighborhood information of the subgraph. To tackle this problem, one line of subgraph sampling is to design subgraph samplers to alleviate the inter-connectivity between subgraphs. For example, CLUSTER-GCN (Chiang et al., 2019) propose subgraph samplers based on graph clustering methods (e.g., METIS (Karypis & Kumar, 1998) and Graclus (Dhillon et al., 2007)) and GRAPHSAINT propose edge, node, or random- walk based samplers. SHADOW (Zeng et al., 2021) proposes to extract the L-hop neighbors of a mini-batch and then select an important subset from the L-hop neighbors. IBMB (Gasteiger et al., 2022) proposes a novel subgraph sampler where the subgraphs are induced by the mini-batches with high influence scores, such as personalized PageRank scores. Another line of subgraph sampling is to design efficient compensation for the messages from the neighborhood based on existing subgraph samplers. For example, GAS (Fey et al., 2021) proposes historical embeddings to compensate for messages in forward passes and LMC (Shi et al., 2023) further proposes historical gradients to compensate for messages in backward passes. GraphFM-OB (Yu et al., 2022) alleviates the staleness of the historical embeddings based on the idea of feature momentum. Besides the traditional optimization algorithm, SubMix (Abu-El-Haija et al., 2023) proposes a novel learning-to-optimize method for subgraph sampling, which parameterizes subgraph sampling as a convex combination of several heuristics and then learns to accelerate the training of subgraph sampling."}, {"title": "3 PRELIMINARIES", "content": "We first introduce notations in Section 3.1. Then, we introduce graph neural networks and the neighbor explosion issue in Section 3.2.\n3.1 NOTATIONS\nA graph G = (V, E) is defined by a set of nodes V = {1, 2, . . ., n} and a set of edges & among these nodes. Let (i, j) \u2208 E denote an edge going from node i \u2208 V to node j \u2208 V. Let (B1 \u2192 B2) denote the set of edges {(i, j)|i \u2208 B1, j \u2208 B2, (i, j) \u2208 E} from B\u2081 to B2. Let N\u2081 = {j \u2208 V|(i, j) \u2208 E} denote the neighborhood of node i. Let NB = (UiEBNi) \u222a B denote the neighborhoods of a mini-batch B with"}, {"title": "3.2 GRAPH CONVOLUTIONAL NETWORKS", "content": "For simplicity of the derivation, we present our algorithm with graph convolutional networks (GCNs) (Kipf & Welling, 2017). However, our algorithm is also applicable to arbitrary message passing-based GNNs (see Appendix B.1).\nA graph convolution layer is defined as\n$H^{(l+1)} = f^{(l+1)}(H^{(l)}, \\tilde{A}) = \\sigma(Z^{(l+1)}W^{(l)}) = \\sigma(\\tilde{A}H^{(l)}W^{(l)}), (l + 1) \\in [L],$ (1)\nwhere $\\tilde{A} = (D+I)^{-1/2}(A + I)(D + I)^{-1/2}$ is the normalized adjacency matrix and D is the in-degree matrix ($D_{uu} = \\sum_v A_{uv}$). The initial node feature is $H^{(0)} = X$, $\\sigma$ is an activation function, and $W^{(l)}$ is a trainable weight matrix. For simplicity, we denote the GNN parameters ${W^{(l)}}_{l=1}^{L-1}$ by W. Thus, GCNs take node features and the normalized adjacency matrix (X, A) as input\n$H^{(L)} = GCN(X, \\tilde{A}),$\nwhere $GCN = f^{(L)} \\circ f^{(L-1)} ... \\circ f^{(1)}$.\nThe neighbor explosion issue is mainly due to feature propagation $Z^{(l+1)} = AH^{(l)}$. Specifically, the mini-batch embeddings at the (l + 1)-th layer\n$H_B^{(l+1)} = \\sigma (Z_B^{(l+1)} W^{(l)}) = \\sigma (A_{B,NB}H_{NB}^{(l)} W^{(l)})$ (2)\nrecursively depend on $H_{NB}^{(l)}$ at the 1-th layer. Thus, the dependencies of nodes (i.e., $H_{NB}^{(l)}$) depends on $H_{NB}^{(l)}$ are exponentially increasing with respect to the number of layers L due to $O(||N_B||) = O(|B| deg_{max})$ with the maximum degree $deg_{max}$."}, {"title": "4 MESSAGE INVARIANCE", "content": "In this section, we elaborate on message invariance in detail. We first present the definition of message invariance in Section 4.1. We then provide a case study for message invariance in Section 4.2.\n4.1 MESSAGE INVARIANCE\nWe first separate the mini-batch feature propagation in Equation (2) into two parts, i.e.,\n$Z_B^{(l+1)} = A_{B,B}H_B^{(l)} + A_{B,N_B}H_{N_B}^{(l)}$ (3)\nMPIB\nMPOB\nwhere MPIB and MPO\u00df denote message passing between the in-batch nodes and message passing from their out-of-batch neighbors to the in-batch nodes respectively.\nTo avoid the recursive dependencies induced by MP\u043e\u0432, we first introduce a novel concept of (global) message invariance, which bridges the gap between costly \u041c\u0420\u043e\u0432 and fast MPIB."}, {"title": "4.1 MESSAGE INVARIANCE", "content": "Definition 4.1 (Message invariance). We say that a transformation $g : \\mathbb{R}^{|B|\\times d} \\to \\mathbb{R}^{|N_B|\\times d}$ is message- invariant if it satisfies\n$H_{N_B}^{(l)} = g(H_B^{(l)}),$ (4)\nfor any GNN parameters W.\n$N_B = ||[A]B||0.$"}, {"title": "5 TOPOLOGICAL COMPENSATION", "content": "In this section, we present the details of the proposed topological compensation framework (TOP). First, we introduce the formulation of TOP inspired by the case study of message invariance in Section 5.1. Then, based on the linear estimation of TOP, we conduct experiments to demonstrate that the message invariance significantly reduces the discrepancy between MPIB and the whole message passing in Section 5.2. Finally, we analyze the convergence of TOP in Section 5.3.\n5.1 FORMULATION OF TOPOLOGICAL COMPENSATION\nFormulation. Inspired by the linear message-invariant transformation in the case study in Section 4.2, we propose to model message invariance $H_{N_B}^{(l)}$ by $H_{N_B}^{(l)} \\approx RH_B^{(l)}$, where the coefficient matrix $R \\in \\mathbb{R}^{|N_B| \\times |B|}$ are the weights of linear combinations of the in-batch embeddings of $H_B^{(l)}$. Combining the approximation and the mini-batch feature propagation (3) leads to\n$Z_B^{(l+1)} \\approx A_{B,B}H_B^{(l)} + A_{B,N_B}RH_B^{(l)} = (A_{B,B} + \\partial A_{B,B})H_B^{(l)},$ (6)\nMPIB\nwhere we call $\\partial A_{B,B} \\equiv A_{B,N_B}R$ the topological compensation (TOP). The topological compensation implements the message invariance by adding weighted edges to the induced subgraph $A_{B,B}$. Then, TOP directly runs a GCN on the modified subgraph as follows\n$H_B^{(L)} = GCN(X_B, \\tilde{A}_{B,B} + \\partial \\tilde{A}_{B,B}).$\nThe formulation of TOP makes it easy to incorporate the existing subgraph sampling methods."}, {"title": "Estimation of topological compensation.", "content": "To reduce the discrepancy between the modified MPIB in Equation (6) and the whole message passing (3), we estimate R by\n$\\min_R ||RH_B - H_{N_B}||_F,$\nwhere $H$ denotes the basic embeddings and $|| ||_F$ is the Frobenius norm. The basic embeddings reflect the similarity between nodes.\nSelection of basic embeddings. Before the training, we select the basic embeddings of a GNN at random initialization by $H^{(W^{(rand)})} = (H^{(0,rand)}, H^{(1,rand)}, ..., H^{(L,rand)}) \\in \\mathbb{R}^{n \\times (L+1)d}$, where $W^{(rand)}$ are the randomly initialized parameters and $H^{(l,rand)}$ are the corresponding embeddings at the j-th layer. The basic embeddings are the concatenation of all embeddings at different layers.\nAn appealing feature of $H^{(W^{(rand)})}$ is that they can identify the 1-WL indistinguishable node pairs by Theorem E.3 in Appendix E. The property ensures that the learned g is message-invariant on graphs with symmetry or large-scale graphs like the first motivating example in Section 4.2.1.\nThe linear message-invariant transformation with the basic embeddings $H^{(W^{(rand)})}$ is very accurate on real-world datasets as shown in Section 5.2. Thus, we estimate TOP in the pre-processing phase and then reuse it during the training phase for efficiency in our experiments. When TOP based on $H^{(W^{(rand)})}$ suffers from high errors, a solution is to update g using the up-to-date embeddings $H^{(W(t))}$ at the t-th training step."}, {"title": "5.2 MEASURING MESSAGE INVARIANCE IN REAL-WORLD DATASETS.", "content": "In this section, we conduct experiments to demonstrate that the message invariance significantly reduces the discrepancy between MPIB and the whole message passing in many real-world datasets. To ensure the robustness and generalizability of TOP in practice, we provide more results in Tables 3, 7, and 8, including more experiments on heterophilous graphs and experiments under various subgraph samplers. The whole experiments are conducted on five GNN models (GCN, GAT, SAGE, GCNII, and PNA) and eight datasets (Ogbn-arxiv, Reddit, Yelp, Ogbn-products, amazon-ratings, minesweeper, questions, and questions).\nMeasuring message invariance in real-world datasets. We first train GNNs by the full-batch gradient descent for each dataset. Then, we measure the discrepancy between MPIB and the whole"}, {"title": "5.3 CONVERGENCE OF TOP", "content": "Based on message invariance (4), we develop the convergence analysis of TOP in this section. The assumption of Theorem 5.1 is widely used in convergence analysis (Shi et al., 2023; Chen et al., 2018a; Yu et al., 2022). All proofs are provided in Appendix D.\nTheorem 5.1. Let $L(W) = \\sum_{i \\in V} l(h_i^{(L)}, y_i)/|B|$ and $d_W = \\nabla_W E_{i \\in V}l(h_i^{(L,TOP)}, y_i)/|B|$ be the loss of the full-batch method and the gradient of TOP respectively, where l is the loss function and yi is the label of node i. Assume that (1) the optimal value $L^* = \\inf L(W)$ is finite (2) at the k-th iteration, a batch of nodes $V_k$ is uniformly sampled from V (3) function $\\nabla_W L$ is $\\gamma$-Lipschitz with $\\gamma > 1$ (4) norms $|\\nabla_W L||_2$ and $||d_W||_2$ are bounded by G > 1. With the learning rate $\\eta = O(\\varepsilon^2)$ and the training step $N = O(\\varepsilon^{-4})$, TOP then finds an \u025b-stationary solution such that $E[||\\nabla_W L(W^{(R)})||_2] < \\varepsilon$ after running for N iterations, where R is uniformly selected from [N].\nThe convergence rate $N = O(\\varepsilon^{-4})$ is the same as the standard SGD (Nesterov, 2013; Fang et al., 2018). Notably, the convergence rate of TOP is faster than that of LMC (Shi et al., 2023) (i.e., $N = O(\\varepsilon^{-6})$), as TOP avoids the staleness issue of the historical embeddings and gradients of LMC."}, {"title": "6 EXPERIMENTS", "content": "We first compare the convergence and efficiency of TOP with the state-of-the-art subgraph sampling methods which are the most related baselines-in Section 6.1. Then, we compare the convergence and efficiency of TOP with the state-of-the-art node/layer-wise sampling methods in Section 6.2. More experiments are provided in Appendix C.\n6.1 COMPARISON WITH SUBGRAPH SAMPLING\nDatasets. We evaluate TOP on five datasets with various sizes (i.e., Reddit (Hamilton et al., 2017), Yelp (Zeng et al., 2020), Ogbn-arxiv, Ogbn-products, and Ogbn-papers (Hu et al., 2020)). These datasets contain at least 100 thousand nodes and one million edges. Notably, Ogbn-papers is very large, containing 100 million nodes and 1.6 billion edges. They have been widely used in previous works (Fey et al., 2021; Zeng et al., 2020; Hamilton et al., 2017; Chiang et al., 2019; Chen et al., 2018a;b). Table 1 summarizes the statistics of the datasets. We also conduct experiments on heterophilous graphs in Appendix C.7.\nSubgraph samplers. On the small and medium datasets (i.e., Ogbn-arxiv, Reddit, and Yelp), we follow CLUSTER (Chiang et al., 2019) and GAS (Fey et al., 2021) to sample subgraphs based on METIS (see Appendix A.1). Specifically, we first use METIS to partition the original graph into many clusters and then sample a cluster of nodes to generate a subgraph. On the large datasets (i.e., Ogbn-products and Ogbn-papers), as the METIS algorithm is too time-consuming (Zeng et al., 2020), we uniformly sample nodes to construct subgraphs. More experiments under various subgraph samplers are provided in Appendix C.8.\nBaselines and implementation details. Our baselines include subgraph sampling (CLUSTER (Chiang et al., 2019), SAINT (Zeng et al., 2020), and GAS (Fey et al., 2021)). We also compare TOP with IBMB (Gasteiger et al., 2022) in Appendix C.3, a recent subgraph sampling method focused on the design of subgraph samplers, which is orthogonal to TOP (see Section 2). We implement TOP, CLUSTER, SAINT, and GAS based on the codes and toolkits of GAS (Fey et al., 2021) to ensure a fair comparison. We introduce these baselines in Appendix A. We evaluate CLUSTER, GAS, SAINT, and TOP based on the same GNN backbone, including the widely used GCN (Kipf & Welling, 2017) and GCNII (Chen et al., 2020). We implement GCN and GCNII following (Fey et al., 2021) and (Hamilton et al., 2017). Due to space limitation, we present the results with more GNN backbones (e.g. SAGE (Hamilton et al., 2017), and GAT (Veli\u010dkovi\u0107 et al., 2018)) in Appendix C.3. We run all experiments in this section on a single GeForce RTX 2080 Ti (11 GB), and Intel Xeon CPU E5-2640 v4. For other implementation details, please refer to Appendix B."}, {"title": "7 CONCLUSION", "content": "In this paper, we propose an accurate and fast subgraph sampling method, namely topological compensation (TOP), based on a novel concept of message invariance. Message invariance defines message-invariant transformations that convert expensive message passing acted on out-of-batch neighbors (MPOB) into efficient message passing acted on in-batch nodes (MPIB). Based on the message invariance, the proposed TOP uses efficient MPIB With limited performance degradation. Experiments demonstrate that TOP is significantly faster than existing mini-batch methods by order of magnitude on vast graphs (millions of nodes and billions of edges) with limited performance degradation. While our experiments focus on message-invariant transformation for some common and simple GNNs, non-linear message-invariant transformation needs to be empirically evaluated for more GNNs with more complex aggregation. In the future, we plan to generalize our ideas to more GNNs or graph transformers with global communication."}, {"title": "REPRODUCIBILITY STATEMENT", "content": "To ensure reproducibility, we provide key information from the main text and Appendix as follows.\n1. Algorithm. We provide the pseudocode of TOP in Algorithms 1 and 2. We also provide the detailed implementation of TOP in Appendix B. See Appendix B.3 for the hyperparameters of TOP.\n2. Theoretical Proofs. We provide all proofs in Appendix D.\n3. Source Code. The code of LMC is available on GitHub at https://github.com/\nMIRALab-USTC/TOP.\n4. Experimental Details. We provide the detailed experimental settings in Section 6 and Appendix B."}]}