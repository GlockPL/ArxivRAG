{"title": "SCBench: A Sports Commentary Benchmark for Video LLMS", "authors": ["Kuangzhi Ge", "Lingjun Chen", "Kevin Zhang", "Yulin Luo", "Tianyu Shi", "Liaoyuan Fan", "Xiang Li", "Guanqun Wang", "Shanghang Zhang"], "abstract": "Recently, significant advances have been made in Video Large Language Models (Video LLMs) in both academia and industry. However, methods to evaluate and benchmark the performance of different Video LLMs, especially their fine-grained, temporal visual capabilities, remain very limited. On one hand, current benchmarks use relatively simple videos (e.g., subtitled movie clips) where the model can understand the entire video by processing just a few frames. On the other hand, their datasets lack diversity in task format, comprising only QA or multi-choice QA, which overlooks the models' capacity for generating in-depth and precise texts. Sports videos, which feature intricate visual information, sequential events, and emotionally charged commentary, present a critical challenge for Video LLMs, making sports commentary an ideal benchmarking task. Inspired by these challenges, we propose a novel task: sports video commentary generation, developed SCBench for Video LLMs. To construct such a benchmark, we introduce (1) SCORES, a six-dimensional metric specifically designed for our task, upon which we propose a GPT-based evaluation method, and (2) CommentarySet, a dataset consisting of 5,775 annotated video clips and ground-truth labels tailored to our metric. Based on SCBench, we conduct comprehensive evaluations on multiple Video LLMs (e.g. VILA, Video-LLaVA, etc.) and chain-of-thought baseline methods. Our results found that InternVL-Chat-2 achieves the best performance with 5.44, surpassing the second-best by 1.04. Our work provides a fresh perspective for future research, aiming to enhance models' overall capabilities in complex visual understanding tasks. Our dataset will be released soon.", "sections": [{"title": "1 Introduction", "content": "With the advancement of Video Large Language Models (Video-LLMs), neural networks are capable of processing video sequences to perform multi-modality tasks based on temporal and spatial information. Recent advancements have facilitated significant improvements in the automatic generation of video content summaries, captions, and even interactive commentary Miech et al. (2019); Zhou et al. (2024); Lin et al. (2024); Liu et al. (2024a)."}, {"title": "2 Related Works", "content": ""}, {"title": "2.1 Benchmarking Video LLMs", "content": "Recently, the emergence of numerous Video LLMs, such as VILA-1.5 Lin et al. (2024), LLaVA-NeXT-Video Liu et al. (2024a), Video-LLaVA Lin et al. (2023), InternVL Chen et al. (2024b) and LongVA Zhang et al. (2024), have been proven to be effective in solving various downstream tasks. Consequently, various benchmarks, based on different evaluation dimensions, have been introduced to evaluate their performance. Previous works like Perception Test P\u0103tr\u0103ucean et al. (2023), EgoSchema Mangalam et al. (2023), and TempCompass Liu et al. (2024c) have focused on specific aspects of the models' capabilities, such as spatial understanding and temporal comprehension. More recent works, including Video-Bench Ning et al. (2023b), Video-MME Fu et al. (2024), MMBench-Video Fang et al. (2024), MVBench Li et al. (2024) and LVBench Wang et al. (2024), evaluate models by defining task-specific hierarchical capability taxonomy of the model and adopts longer videos. However, current works do not focus on evaluating the ability to understand more comprehensive and less perceptible visual information and fail to evaluate their temporal memory ability. Besides, their metrics are based on the explicit features or QA pairs, which fall short of the ability to evaluate our task with implicit conceptions."}, {"title": "2.1.1 Sports Video Datasets", "content": "Sports videos for video captioning models represent a branch within video understanding tasks, with numerous datasets and benchmarks proposed for various sports and tasks. Datasets such as TenniSet Faulkner and Dick (2017), FineGym Shao et al. (2020), P2A Bian et al. (2024), and SoccerNet Deli\u00e8ge et al. (2021) provide detailed labels for technical actions and events in the games. Specifically, datasets like TenniSet, FineGym, and P2A contain a large number of standardized labels about on-field events or the description of on-field player actions. Another type of research focuses on interactions between athletes, exemplified by SportsHHI Wu et al. (2024), which focuses on athletes' location and interaction classification tasks for sports videos. Although these datasets provide data for sports video recognition tasks of Video LLMs, there is still a lack of commentary-based sports videos, which is exactly what our newly proposed dataset addresses."}, {"title": "2.1.2 Video Captioning on Sport Videos", "content": "In the field of sports video, the datasets mentioned in the previous section also serve as datasets for the sports video captioning task, offering real-time labels for sports videos. The datasets for these captioning tasks provide time stamps and specific captions for the video clips which Video LLMs are required to generate captions for. The structure of a dataset directly influences the evaluation dimensions and the methods used. Sports datasets like SoccerNet Deli\u00e8ge et al. (2021) and SportsHHI Wu et al. (2024) have evaluated models' abilities in event understanding or interaction recognition tasks within sports videos. These captioning tasks assess models from a textual perspective such as feature or N-gram similarity. Despite advancements in datasets and benchmarks for technical actions and interactions, there remains a need for a comprehensive sports dataset and benchmark for commentary-based captioning - whose particularity has been mentioned above. That's why we've come up with a new commentary benchmark to measure this ability."}, {"title": "3 Evaluation & Metric", "content": "In this section, we first define the task of sports commentary, highlighting its distinctions from conventional video captioning tasks. Subsequently, we present our novel six-dimensional metric, SCORES, tailored for this task, compared with traditional metrics and the vanilla GPT-based method. Finally, we provide an in-depth description of our GPT-based evaluation method, grounded in the six-dimensional metric."}, {"title": "3.1 Task Definition", "content": "Sports commentary provides descriptions, analysis, and insights to enhance the viewing experience with detailed information, tactical breakdowns, and highlights. The task involves providing a video input and a prompt to the model. The demanding output is the corresponding commentary.\nUnlike traditional video captioning, which only focuses on describing visual content, sports commentary requires not only accurate event descriptions but also timely, emotionally resonant narration that aligns with the game's pace and context. For example, when game intensity is relatively low, commentators may provide tactical analysis or details of the players or the teams, while remarkable plays might trigger emotionally charged reactions like \"What a goal!\" and \"Amazing!\" Fig. 2 illustrates the difference between caption and commentary."}, {"title": "3.2 SCORES", "content": "Having established the definition of our task and distinguished it from video captioning, we now explain the rationale for our novel metric. We first compare it with traditional captioning metrics to demonstrate its necessity, followed by a comprehensive introduction to the six-dimensional metric."}, {"title": "3.2.1 Why Propose a New Metric", "content": "Existing benchmarks for Video LLMs mostly deploy metrics designed for either QA or multi-QA tasks, which are unsuitable for our task. Meanwhile, the metrics currently used for video captioning tasks fail to fully accommodate our specific requirements."}, {"title": "3.2.2 Details of SCORES", "content": "To address the limitations of traditional metrics and better evaluate the multifaceted nature of sports commentary, we propose our novel six-dimensional metric, SCORES (Situation, taCtic, emotion, background, key Events, techniques), to be specific:\n1. Key Events Caption: This dimension assesses the model's ability to accurately detect and describe significant events, such as scores, turnovers, fouls, and substitutions, reflecting its proficiency in real-time event detection and textual articulation.\n2. Technical Detail Analysis: It evaluates models' capacity to analyze and explain player actions, such as passing and shooting techniques, reflecting its fine-grained visual understanding capability and contextual knowledge integration.\n3. Background Information Interpretation: This dimension measures the model's ability to integrate visual and textual information about players' and teams' histories, characteristics, and performance, which demonstrates its contextual knowledge integration and deep understanding of visual information(e.g. game intensity and event frequency).\n4. Tactical Analysis: It assesses the model's skill in interpreting and explaining team strategies, formations, and in-game tactical adjustments by analyzing visual inputs and conveying these insights through commentary, highlighting its temporal understanding of game dynamics.\n5. Match Situation Interpretation: This dimension evaluates the model's ability to interpret the current state of the match, such as the score and momentum, through visual cues and predict its likely progression, providing dynamic situational insights."}, {"title": "3.3 GPT-based Evaluation", "content": "Built upon the SCORES, we propose a novel GPT-based evaluation method tailored for sports video commentary, as shown in Fig. 3. Before evaluation, we need to generate labels for our data according to the six-dimensional metric, which is introduced in Sec. 4.1. Here, we provide a detailed overview of evaluation processes."}, {"title": "4 Dataset", "content": "Our goal is to advance research in the field of sports video commentary by introducing a challenging benchmark with professional commentary annotations. To this end, we have developed the fine-grained CommentarySet, which contains 5775 high-quality(1080P) sports video clips. Each video is well annotated, and the overall dataset structure will be discussed in the next section."}, {"title": "4.1 Dataset Construction", "content": "Data Structure To better evaluate sports video commentary, we create a novel dataset called CommentarySet for SCBench, containing a total of 5,775 clips, with 4,908 in the training set and 867 in the test set, the comparison with other datasets is shown in Tab. 2. Part of the clips is selected from FineGym Shao et al. (2020) and TenniSet Faulkner and Dick (2017). Specifically, CommentarySet includes clips from six types of sports, along with their corresponding timestamps in the original videos. The necessary information on the six sports we selected is shown in Tab. 1. Moreover, each clip is accompanied by carefully selected English commentary text, which can serve as a ground truth label in sports commentary evaluation. To implement the evaluation metrics proposed in this paper, we also provide each clip with precise six-dimensional labels. Additional information about CommentarySet is shown in supplementary material.\nDataset Creation Process To create CommentarySet and ensure the reliability, professionalism, and accuracy of the video resources and commentary, we follow the pipeline below for dataset creation, as shown in Fig. 3:\n1. Collection: We collect a large number of high-definition sports videos with original English commentary from various sports and events available online and extracted the English commentary from the original audio, preserving their corresponding timestamps.\n2. Merging: The commentary sentences obtained from the Collection process are fragmented. So, we add a process to merge them. To be specific, let the previous and current commentary be $Com_1$ and $Com_2$ respectively, and let their time stamps be $[b_1, e_1]$ and $[b_2, e_2]$. The merged commentary and its time stamp can be obtained using the following formula:\nNew Com = $Com_1 + Com_2$  \\(1)\nNew Time Stamp = $[b_1, e_2]$ \\(2)\nRegarding the determination of whether two commentaries should be merged, we conduct the following two-stage decision process:\nStage 1 Textual Merging: We use the sentence encoder to encode the sentences and then apply cosine similarity to merge commentary with high textual similarity such as repetitive cheers or consecutive sentences describing the same content. To be specific, the similarity (Sim) is calculated using the cosine similarity. After obtaining Sim, we simply decide whether to merge according to the following formula:\nAction = $\\begin{cases}  Merge & \\text{if } Sim > threshold \\\\ Separate & \\text{else} \\end{cases}$ \\(3)\nwhere the threshold is set to 0.7 as our experimental choice.\nStage 2 GPT Merging: Some segments with continuous semantic meaning but no textual overlap remained. We use GPT-40-mini to judge whether to merge consecutive commentary segments, the prompt would be shown in supplementary material. This is applied to segments where there exists a logical connection and describes the same topic.\n3. Slicing: Before slicing, we manually refine the timestamps. The full sports match videos are sliced according to the timestamps, generating clip-commentary pairs.\n4. Six-Dimensional Label Generation: Finally, we use GPT-40-min to classify the semantic content of the commentary. Each commentary sentence is categorized into one or more of the six dimensions mentioned in the metric section."}, {"title": "4.2 Statistic Analysis & Comparison", "content": "This section will primarily present specific statistical information about CommentarySet. We will also demonstrate that CommentarySet is a more suitable dataset for our proposed SCBench compared to other existing sports video captioning datasets, offering greater comprehensiveness and accuracy. Tab. 1 fully showcases the specific parameters within CommentarySet.\nDiversity The six sports included in CommentarySet are carefully selected to ensure strong diversity. As shown in Tab. 1, the six selected sports exhibit significant diversity in terms of match speed, player density, commentary length, and commentary frequency. For instance, the average duration of table tennis clips is the shortest at just 9.96s, while athletics, gymnastics, and tennis have clip lengths 1.5 to 2 times longer, reflecting faster match paces and shorter commentary content. In terms of commentary frequency, basketball has the highest frequency, while soccer and tennis have relatively lower frequencies, correlating closely with the frequency of events occurring on the field.\nCommentary Style Commentary style is another critical aspect. The content and focus of commentary vary across different sports, which motivates our proposal of SCORES. Through the analysis of the distribution of the six labels across sports, as shown in Fig. 4, we have observed distinct commentary styles. For example: In basketball, key event captions account for 44.33%, indicating continuous on-field events; In gymnastics, 61.84% of the commentary includes Background Information Interpretation, as commentators frequently introduce each participating athlete. The diversity of sports leads to diverse commentary styles, which is quantitatively reflected in the distribution of labels across different sports. CommentarySet includes data on commentary styles across different sports, serving as a foundational reference for commentary generation tasks, a unique statistical concept compared to other datasets."}, {"title": "5 Experiments", "content": "We conduct extended experiments which could be divided into two main parts. First, we evaluated the zero-shot commentary capabilities of multiple open-source Video LLMs on our SCBench. Subsequently, we assessed the effects of ICL Brown et al. (2020) and CoT Wei et al. (2023) methods on fine-tuning Video-LLaVA and Chat-UniVi, which are the baseline models. In this section, we first introduce our experiment settings, including model selection and hyperparameters. Then we present quantitative results for both zero-shot experiments and fine-tuned models. Finally, we present a detailed visualization, analysis, and case study of the experimental results across different models and fine-tuning methods."}, {"title": "5.1 Settings and Configurations", "content": "All the experiments are conducted on 8 A800 GPUs. We perform evaluation on eight open-source pre-trained Video LLMs, including VILA Lin et al. (2024), Video-LLaVA Lin et al. (2023), Long VA Zhang et al. (2024), LLaVA-NeXT-Video Liu et al. (2024a), Kangaroo Liu et al. (2024b),"}, {"title": "5.2 Quantitative Results", "content": "Zero-shot Model Performance To comprehensively evaluate the performance on our SCBench, we conduct extensive experiments on the eight aforementioned pretrained Video LLMs, the result is shown in Tab. 3. Overall, InternVL-2, with 40.1B parameters, perform the best, achieving a score of 5.44, surpassing second-best, Mini-InternVL-Chat-v1.5, by 1.04. Following this, LLaVA-NeXT-Video, LongVA and VILA score 4.20, 3.85 and 3.40, respectively. Our evaluation is based on SCORES. So we calculate the models' performance in all six dimensions and create a six-dimensional radar chart, as shown in Fig. 5, showing the comprehensive performance of models. Additionally, as the models show varying strengths across different sports, we also create a six sports radar chart in Fig. 5 to show models' performance across different sports types. For example, Video-LLAVA excels in gymnastics, outperforming its score in table tennis by 0.32 and LongVA stands out in the table tennis task, achieving a score of 4.80, but falls short in other types of sports. The strengths across different sports highlight the models' specific capabilities mentioned in the dataset section.\nICL & CoT Fine-tuning We propose two methods of fine-tuning and evaluate them with Video-LLaVA and Chat-UniVi on our SCBench:\n1. In-Context Learning (ICL) Fine-tuning: In-Context Learning refers to the ability to perform specific tasks by leveraging contextual information embedded within the prompt. In our fine-tuning, We structure the fine-tuning QA setup similar to the prompt format used in Model Inference, instructing the model to directly generate commentary based on the video, task definition, and the perspectives of the ground truth metrics, intended to enhance the model's comprehension of the task and the metric perspectives and align the characteristics of the metric dimensions with the generated commentary.\n2. Chain-of-Thought (CoT) Fine-tuning: Chain-of-Thought (CoT) is a prompting technique that enhances models' reasoning by breaking down complex problems into smaller steps. In fine-tuning, we format the training data to require the model to analyze the video, task definition, and all metric perspectives to first identify the relevant perspective before generating commentary. This approach, which is more challenging than providing a specific metric dimension, aims to encourage the model to emulate human reasoning in commentary, enhancing coherence and conciseness.\nThe scores of fine-tuned models are demonstrated in Tab. 3. The results indicate that both fine-tuning methods enhance the model's performance, with CoT providing a more substantial improvement. Specifically, CoT increased performance by 8% on Video-LLaVA and 6% on Chat-UniVi, while ICL improved performance by 6% and 1.5%, respectively. However, although the model's performance"}, {"title": "5.3 Analysis", "content": "We perform analysis and ablation studies to further demonstrate the superiority of our metric over traditional metrics. Apart from that, we discuss the possible factors that influence the zero-shot and fine-tuned models' performance. Additional experiments and discussions are shown in the supplementary material."}, {"title": "5.3.1 Metric Validation", "content": "Comparison with Traditional Metric: In Sec. 3, we theoretically demonstrate the necessity of proposing a new metric for sports commentary tasks. We further conduct experiments using BLEU and CIDEr to evaluate different models, as shown in Tab. 3.\nThe results indicate that traditional metrics yield low scores across all models (BLEU scores below 1, CIDEr scores around 0.1), making it challenging to distinguish model performance. In contrast, SCORES offers a more nuanced evaluation, effectively and distinctively measuring the model's capabilities. For example, although InterVL-Chat-2 has the lowest BLEU score and a relatively low CIDEr score, our metric highlights its superior performance across all sports categories.\nResults of Human Evaluation: To better validate the advantages of our metric, we recruited a total of 36 human evaluators to conduct our user study. The evaluators were shown the ground truth commentary and two commentaries generated by different models (commentary 1 and commentary 2). Evaluators give a choice for each sample, with A representing a preference for commentary 1, \u0412 indicating a tie, and C representing a preference for commentary 2. We took the most frequent choice as the final result for each sample. Similarly, for other metrics, the results were also categorized into three choices: A, B, and C by comparing the scores, as shown in Fig. 6.\nWe compare the human-produced results with SCORES and three previous metrics (Vanilla LLM, BLEU, and CIDEr), as shown in Fig. 7. The results show that the overlap rate between our metric and human decisions is 60%, about 1.5x higher than the overlap rate of the best previous method, Vanilla LLM. Furthermore, the overlap rate of Vanilla LLM's results is very close to that of CIDEr, indicating that LLM itself does not perform better than traditional NLP metrics. It is the improvement brought by our proposed SCORES to LLMs that enhances its performance in evaluating commentaries."}, {"title": "5.3.2 Zero-shot vs Fine-tune", "content": "It is important to note that our evaluation method is essentially designed to leverage the ground truth commentary and the definitions of SCORES to enable our GPT judge to assess the accuracy and completeness of key information in the model-generated commentary, while evaluating the stylistic coherence of the model's commentary with the ground truth, thus yielding an overall score.\nGrounded on this, we can interpret many of our experimental results. For zero-shot models, we observed that InternVL-2 achieved significantly higher scores than other models. Upon examining its generated commentaries, we find out that its outputs were notably longer, indicating that it captures more concise and accurate key visual or background information. Apart from this, in the case of fine-tuning, the two methods we proposed primarily enable models to comprehend the stylistic elements of the ground truth commentary and the specific content in SCORES. However, these methods do not enhance the models' ability to capture pivotal visual or background information, which explains the reason for the fine-tuned models' underperformance compared with zero-shot models. Therefore, we believe that establishing a keyword repository for the model to perform Retrieval-Augmented Generation (RAG) Lewis et al. (2021) or employing methods such as VisCoT Shao et al. (2024) to enhance the model's ability to capture visual information would be key strategies for further improving the model's commentary capabilities."}, {"title": "6 Conclusion & Limitations", "content": "In this paper, we introduce a novel benchmark named SCBench for Video LLMs, focusing on the complex task of sports video commentary. We develop CommentarySet, a dataset comprising 5,775 annotated clips across six sports categories, and propose a six-dimensional metric, SCORES,"}]}