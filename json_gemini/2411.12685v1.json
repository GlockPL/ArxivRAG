{"title": "Enhanced Sign Language Translation between American Sign Language (ASL) and Indian Sign Language (ISL) Using LLMS", "authors": ["Malay Kumar", "S. Sarvajit Visagan", "Tanish Sarang Mahajan", "Anisha Natarajan"], "abstract": "We have come up with a research that hopes to provide a bridge between the users of American Sign Language and the users of spoken language and Indian Sign Language (ISL). The research enabled us to create a novel framework that we have developed for Learner Systems. Leveraging art of Large models to create key features including: - Real-time translation between these two sign languages in an efficient manner. Making LLM's capability available for seamless translations to ISL. Here is the full study showing its implementation in this paper. The core of the system is a sophisticated pipeline that begins with reclassification and recognition of ASL gestures based on a strong Random Forest Classifier. By recognizing the ASL, it is translated into text which can be more easily processed. Highly evolved natural language NLP (Natural Language Processing) techniques come in handy as they play a role in our LLM integration where you then use LLMs to be able to convert the ASL text to ISL which provides you with the intent of sentence or phrase. The final step is to synthesize the translated text back into ISL gestures, creating an end-to-end translation experience using RIFE-Net. This framework is tasked with key challenges such as automatically dealing with gesture variability and overcoming the linguistic differences between ASL and ISL. By automating the translation process, we hope to vastly improve accessibility for sign language users. No longer will the communication gap between ASL and ISL create barriers; this totally cool innovation aims to bring our communities closer together. And we believe, with full confidence in our framework, that we're able to apply the same principles across a wide variety of sign language dialects.", "sections": [{"title": "I. INTRODUCTION", "content": "The communication gap between the users of American Sign Language (ASL) and Indian Sign Language (ISL) is a significant challenge to intercultural interaction and accessibility in the deaf community. Sign languages are indispensable weapons of expression for deaf individuals, but the lack of interoperability between ASL and ISL limits smooth communication across various linguistic and cultural frontiers. This challenge can be addressed through innovative strategies that leverage advances in machine translation technology and contemporary deep learning frameworks. The central goal of this research work is to build a holistic framework for machine translation that can efficiently and conveniently translate ASL gestures into ISL gestures, hence facilitating communication between ASL users and ISL users. Toward this objective, we hereby propose an approach that combines image recognition techniques with advanced language processing algorithms governed by Large Language Models. Instead of using traditional CNN- based recognition techniques, LLM-driven techniques are used to decode ASL gestures and, in turn, translate directly into meaningful textual representations. From this step of converting ASL gestures to text, we establish an intermediate that helps use advanced LLM-based techniques for machine translation. Now, we can translate the recognized English text into ISL gestures by preserving linguistic aspects and cultural context. The application of LLMs for translation will help make the process more accurate, context-sensitive, and adaptable and, thus, would be used to bridge between ASL and ISL in a nearly seamless manner."}, {"title": "A. Leveraging Deep Learning for Gesture Recognition and Translation", "content": "Sign language communication involves complex expressions that carry a certain range of linguistic and cultural nuances in communication. Conventional sign language understanding and translation techniques rely mostly on fixed data sets and prebuilt models. Such approaches find it difficult to adapt to the dynamic nature of sign languages. In the paper, the constraints were overcome using Random Forest Classifiers for effective gesture recognition followed by Large Language Models to assist in context-aware translation. The proposed framework, therefore, would represent a breakthrough with the integration of real-time processing capabilities and cultural contextualization.\n\nThe significance of this work is that it introduces an intermediate text-based representation that forms the connecting link between recognition and synthesis of gestures. This linguistic intermediate allows for not only a more literal translation but also allows the tailoring of the translation method in such a way as to keep its intent and cultural nuances of the original expression. This model would become especially important to the reduction of the linguistic differences between ASL and ISL, such as differing grammatical structures, word orders, and contextual expressions. Further, generating ISL gestures from translated text with the assistance of RIFE-Net produces a smooth and natural gesture presentation. Such a robust ability of RIFE-Net to effectively handle high variability within gesture sequences leads it to accurately reproduce ISL gestures even while making forward-looking predictions of potentially"}, {"title": "B. Towards a Multi-dialectal Future", "content": "Though designed specifically for ASL and ISL, underpinning principles and methodologies are adaptable to support more than those targeted sign language dialects. By adapting the system to use a variety of datasets, it follows that it is viable to address the diversity in sign languages globally, thereby paving the course for universal sign language interoperability. Moreover, this adaptability brings out the scale in using LLMS for translation purposes as it opens up broader applications in assistive technologies.\n\nThis research promises to work out an integrated approach combining gesture recognition, natural language processing, and synthesis of gestures to bridge the overall communication gap so present in sign language communities. This is an advance both in the technical and importance toward furthering the aspects of inclusivity and exchange across the world's deaf community."}, {"title": "II. LITERATURE SURVEY", "content": "Recognition and translation of sign languages have attracted a lot of focus in the recent past because of their pivotal role in improving communication between deaf and mute people. Presenting an overview of the literature in this field, this literature review identifies methodologies, techniques, and challenges in sign language recognition and translation systems."}, {"title": "III. PROPOSED METHODOLOGY", "content": "This research presents a novel methodology to bridge the communication gap between American Sign Language (ASL) and Indian Sign Language (ISL) through an automated translation system enhanced using Large language models (LLMs). The proposed approach comprises three interconnected phases: The sign language recognition phase that utilizes a hybrid ensemble model, the Recognized Text Correction phase through language model enhancement, and the Video synthesis phase with motion smoothing. Each phase has been meticulously designed to address specific challenges in cross-sign-language translation while maintaining semantic accuracy and natural gesture flow."}, {"title": "A. Sign Language Recognition", "content": "The initial phase employs a hybrid ensemble approach combining the complementary strengths of a Random Forest Classifier (RFC) and a Convolutional Neural Network (CNN). This RFC+CNN_model_architecture allows robust feature extraction and classification of the sign language and provides better prediction accuracy."}, {"title": "1) Random Forest Classifier Model:", "content": "The Random Forest Classifier component focuses on the skeletal approach which sets a baseline for sign language recognition, it uses the hand- tracking capabilities of the mediapipe framework for feature extraction.\nDataset Preparation: For the above model we prepare an image dataset with a sample size of 2800 images (28 distinct classes * 100 images)\n\u2022 26 alphabetic characters (100 images per character).\n\u2022 Additional classes for 'DELETE' and 'SPACE' operations.\n\u2022 A total of 28 distinct classes."}, {"title": "Feature extraction:", "content": "With the mediapipe framework we extract 42 prominent landmark points (L) placed on the hand which serve as the features for training the model.\n\n$L = {(Xi, Yi, Zi) | i \u2208 [1, 42]}$\n\nThe coordinates obtained are then processed for normalization using StandardScaler.\n\n$X_{normalized} = \\frac{X - \\mu_x}{\\sigma_x}$\n\nwhere $\u03bc_x$ and $\u03c3_x$ represent the mean and standard deviation of the feature distribution respectively.\n\nModel Architecture and Training: We use the RandomForestClassifer (RFC) model that employs an"}, {"title": "2) Convolutional Neural Network Model:", "content": "For the convolutional neural network (CNN) component we use silhouette images providing complementary visual feature analysis to the RFC's landmark-based approach.\n\nSilhouette Images: It is a type of image where a dark image of a subject against a lighter background, usually showing the subject's profile.\n\nUsing silhouette images enables a focus on the intrinsic structure of the hand pose, effectively eliminating background noise, lighting variations, and other environmental influences. This approach isolates the key features of the hand's position and configuration, providing a clearer and more accurate representation for training the model.\nDataset Preparation: For the CNN model we generate sil- houette images of the American sign language (ASL) with a sample size of 16200 images (27 distinct classes * 600 images).\n\u2022 26 alphabetic characters (600 images per character)\n\u2022 Additional class for \"BLANK\" image which serves as ground truth.\n\u2022 Augmentation of images:\nHorizontal flipping: $I_{flip}(x,y) = I(w \u2212 x, y)$\nBrightness variation: $I_{bright}(x, y) = \u03b1I(x, y)$, where $\u03b1 \u2208 [0.8, 1.2]$\nGaussian noise addition: $I_{noise} (X, Y) = I(x, y) + N(0, \u03c3\u00b2)$\nSilhouette image generation: In the experimental phase, we evaluated three distinct approaches for generating silhouette images: Histogram-based thresholding, Skin colour segmentation, and Otsu-thresholding.\n\nOtsu-thresholding proved most effective\n\n$threshold = argmax_t \u03c3_\u03b2\u00b2(t)$\n\nwhere $\u03c3_\u03b2\u00b2(t)$ is the between-class variance:\n\n$\u03c3_\u03b2(t) = wo(t)w1(t) [\u03bc\u03bf(t) \u2212 \u03bc1(t)]\u00b2$"}, {"title": "B. Recognized Text Correction", "content": "In this phase, we implement text correction on the recognized text obtained from the sign language recognition phase. We use a fine-tuned large language model (LLM) to auto- correct the recognized text.\n\nFor fine-tuning the pre-trained large language model (LLM) we prepare a dataset containing 500 examples. The dataset is structured as a collection of text pairs, with each entry containing:\n\n\u2022 Input Text: A set of characters or words that may contain typos, scrambled letters, incomplete words, or some non-standard character sequences.\n\u2022 Output Text: A corrected and meaningful interpretation of the input text. This output contains three variations of grammatically correct phrases, which could be a sentence, a noun phrase, or a meaningful fragment."}, {"title": "C. Video Synthesis", "content": "The final phase of the proposed methodology involves mapping the corrected text to Indian Sign Language (ISL) gestures and creating a fluid video sequence.\n\nThe Indian Sign Language gesture dataset comprises visual manifestations of the 26 letters of the ISL alphabet in sequential order from 0 to 25. Images were captured against a black background, which helps create a high level of contrast and enhances the visibility of hand signs. To ensure uniformity, all images from the ISL dataset will be resized to 128x128 pixels"}, {"title": "IV. PROPOSED WORKFLOW", "content": "1) Sign Language Recognition Module: ASL gestures are converted into text through live camera feeds. The workflow involves:\n\n\u2022 Preprocessing: Isolating hand movements to extract meaningful features.\n\n\u2022 Model Analysis: An ensemble model, comprising a Convolutional Neural Network (CNN) for spatial pattern extraction and a Random Forest Classifier (RFC) for gesture classification, generates text outputs. Due to recognition challenges, the initial text may contain errors (e.g., \"HELOLO WRLD\").\n\n2) Text Correction Module: The recognized text is refined for syntactical and contextual accuracy using the Gemini-1.5 flash, a highly optimized Large Language Model (LLM).\n\n\u2022 Functionality: Gemini-1.5 leverages a multilingual dataset to rectify inaccuracies, ensuring precise and fluent output (e.g., correcting \u201cHELOLO WRLD\u201d to \u201cHELLO WORLD\u201d).\n\n3) Video Synthesis Module: Corrected text is mapped to ISL gesture frames using a predefined algorithm to generate ISL videos.\n\n\u2022 Video Enhancement: RIFE-Net (Real-Time Intermediate Flow Estimation) smooths and interpolates frames, producing natural 60 FPS videos from initial 1 FPS sequences. The resulting ISL video offers enhanced clarity and usability, facilitating effective communication for ISL users."}, {"title": "V. RESULT AND OUTPUT", "content": "This section provides the experimental results of various phases of development, which are performed and investigated to build a complete framework. The proposed framework functionalities are tested in different stages of the development cycle. In addition to that, we have shown the user interface screens of the final application. For the first phase sign language recognition phase we implemented the hybrid approach RFC+CNN. Below are the results obtained on training the RFC Model and the CNN Model."}, {"title": "A. Random Forest Classifier Model", "content": "The following is the confusion matrix obtained on testing the trained model on test sample set of 20 records for each class."}, {"title": "B. Convolutional Neural Networks Model", "content": "On training the CNN model we obtain the following training evaluation graph for 15 epochs. Overall 82.4% accuracy was obtained. The following is the confusion matrix obtained on"}, {"title": "C. Large Language Model", "content": "For the text correction phase, the Fine-tuned Gemini-1.5 flash model was also validated against a sample set of 100 records it shows proper auto-correction with an accuracy of 94.2% on the validation set."}, {"title": "D. Video Outputs", "content": "In the video synthesis phase, we get the following output after conversion from American sign language to Indian sign language on mapping."}, {"title": "VI. CONCLUSION", "content": "This research presents a novel framework for ASL-to- ISL translation, integrating advanced deep learning models, transfer learning, and large language models to bridge gaps in sign language communication. The system addresses linguistic and grammatical differences between ASL and ISL while ensuring context retention and culturally sensitive translation, facilitating interaction among diverse sign language users.\n\nA key innovation is the ensemble model combining custom Convolutional Neural Networks (CNNs) and a Random Forest Classifier for robust ASL gesture recognition. This hybrid approach enhances recognition accuracy and captures nuanced gesture variability. High-end language models like Gemini 1.5 Flash, GPT-3.5, and LLaMA 2 (7B) are employed to translate gestures into text, preserving context and intent. Additionally, RIFE-Net enables real-time reconstruction of ISL gestures, resolving issues of gesture variability and temporal recovery.\n\nBeyond addressing linguistic and technological challenges, the framework promotes inclusivity, enabling users with hearing or speech impairments to express themselves more effectively. This system offers significant potential for advancing accessible technologies and can serve as a foundation for future improvements, including incorporating other sign language dialects and multimodal features such as facial expressions and body language. Moreover, the system's adaptability, through user feedback and self-learning capabilities, can enhance precision and flexibility over time, contributing to the unification of global sign language use."}, {"title": "VII. FUTURE SCOPE", "content": "This research lays a really good foundation for real-time translation from American Sign Language into Indian Sign Language is using state-of-the-art deep learning techniques and Large Language Models. As it handles static gestures and focuses on differences between ASL and ISL, the scope for this framework is huge, here are some areas that should be looked into and developed further:"}, {"title": "A. Expanding Support for Complete ISL and Other Sign Languages", "content": "Even though this present system draws mostly from ISL, its sister signs differ significantly across regions and cultures, which has always led to one particular local sign including different grammatical structures and expressions. It may become worthy to include additional local versions in this work, like BSL (British Sign Language), LSF (French Sign Language), or ArSL (Arabic Sign Language).\n\nChallenges: The vocabulary, regional dialects and non-standard formats vary with different sign languages.\n\nImplementation: This can be achieved by training on diverse datasets that come with multi-regional sign languages. Fine-tuning LLMs based on a multilingual corpus enables them to effectively handle the nuances of cross-linguistic, thus making the framework adaptable to a global scale."}, {"title": "B. Incorporating Dynamic Gestures", "content": "Current theory is too focused on the static nature of translation, and most sign languages require dynamic gestures, which involve motion and transition over time.\n\nChallenges: Recognition and interpretation of dynamic gestures require advanced qualitative as well as quantitative time analysis and the ability to distinguish nuances of movement.\n\nImplementation: Dynamic gestures can be dealt with processing systems like video analysis and processing that can be built using deep models like Transformer. Even further sequences of learning models may be utilized, such as Long Short-Term Memory or 3D convolutional networks, to get better insights into gesture transition."}, {"title": "C. Enhancing Emotional Context Understanding", "content": "Emotions are important in effective communication. Sign languages include facial expressions body language and tone of gestures in passing emotions, which the system doesn't fulfil to its potential.\n\nChallenges: Emotional context will be identified only by multimodal analysis, starting with facial expression and gesture intensity and posture.\n\nImplementation: Future directions include multimodal input streams where gesture data are combined with facial emotion recognition systems based on convolutional or hybrid neural networks. Further training of the model on datasets annotated with emotional cues could enhance the system's capability of more holistically capturing the speaker's intent."}, {"title": "D. Improved Real-Time Performance", "content": "Real-time translation is decidedly computationally intensive, especially with the LLMs and high-fidelity gesture synthesis. This remains a critical area of improvement: speed and efficiency do not have to interfere with accuracy.\n\nChallenges: Balancing computational load with translation accuracy, especially on edge devices or resource-constrained environments.\n\nImplementation: Optimization techniques applied include pruning, quantization, and adaptations to edge computing. Distributed processing or GPU acceleration of inference adds additional layers of real-time performance."}, {"title": "E. Contextual and Idiomatic Translation", "content": "Idioms and context-specific meanings, which are valuable to sign languages, cannot be translated easily. There is some limitation in the usage of idioms in this system, as it is meant to preserve contextual integrity.\n\nChallenges: Capturing idiomatic context requires a deep understanding of cultural nuances and linguistic patterns.\n\nImplementation: Fine-tune LLMs with datasets that are rich in idiomatic phrases and also with reinforcement learning where feedback from the user can help fill in this gap. Adaptive learning algorithms user-centric will make translations over time more sensitive to the context."}, {"title": "F. Interactive Feedback Mechanism", "content": "The future system may even comprise an interactive feedback loop where users can modify or even validate the translations. This loop of continuous learning would, in that case, therefore improve precision and responsiveness with time.\n\nImplementation: A reinforcement learning module implementation based on user feedback using the improvement of the prediction model helps in effective gesture recognition and translation processes."}]}