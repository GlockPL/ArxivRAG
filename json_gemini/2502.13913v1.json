{"title": "How Do LLMs Perform Two-Hop Reasoning in Context?", "authors": ["Tianyu Guo", "Hanlin Zhu", "Ruiqi Zhang", "Jiantao Jiao", "Song Mei", "Michael I. Jordan", "Stuart Russell"], "abstract": "\"Socrates is human. All humans are mortal. Therefore, Socrates is mortal.\" This classical example demonstrates two-hop reasoning, where a conclusion logically follows from two connected premises. While transformer-based Large Language Models (LLMs) can make two-hop reasoning, they tend to collapse to random guessing when faced with distracting premises. To understand the underlying mechanism, we train a three-layer transformer on synthetic two-hop reasoning tasks. The training dynamics show two stages: a slow learning phase, where the 3-layer transformer performs random guessing like LLMs, followed by an abrupt phase transitions, where the 3-layer transformer suddenly reaches 100% accuracy. Through reverse engineering, we explain the inner mechanisms for how models learn to randomly guess between distractions initially, and how they learn to ignore distractions eventually. We further propose a three-parameter model that supports the causal claims for the mechanisms to the training dynamics of the transformer. Finally, experiments on LLMs suggest that the discovered mechanisms generalize across scales. Our methodologies provide new perspectives for scientific understandings of LLMs and our findings provide new insights into how reasoning emerges during training.", "sections": [{"title": "1. Introduction", "content": "Transformer-based large language models (LLMs) show great capability in solving complex reasoning tasks (Reynolds and McDonell, 2021; Kojima et al., 2022; Brown et al., 2020; Wei et al., 2022; Wang et al., 2022b; Nye et al., 2021; Cobbe et al., 2021; Zelikman et al., 2022). Arguably, the in-context two-hop reasoning, which requires a model to derive a conclusion (e.g., [A] is the grandfather of [C]) from two assumed premises (e.g., [A] is the father of [B] and [B] is the father of [C]) given in the prompt, is one of the most fundamental components of many reasoning problems. However, this seemingly the simplest reasoning task can fail real-world large language models, e.g., as shown in Figure 1, although LLMs can solve an in-context two-hop reasoning task correctly without distraction in the prompt, it simply collapses to random guessing when there is additional distracting information (i.e., [D] is the father of [E] and [E] is the father of [F]) in the context (see more details in Section 4.1). Similar phenomena have been discovered in the GSM-IC benchmark (Shi et al.). This failure mode suggests that LLMs might adopt some undesired internal mechanisms to perform reasoning tasks.\nTo investigate why LLMs would struggle with the two-hop reasoning in the presence of distracting information, a common approach is circuit discovery (e.g., Conmy et al. (2023)), which aims to identify the underlying mechanisms at the level of attention heads. However, the validity of circuits remains a topic of debate (e.g., Shi et al. (2024); Hase et al. (2024)). Irrespective of this concern, circuits related to two-hop reasoning would involve a large number of attention heads interconnected through complex computational graphs, making it challenging to extract meaningful insights.\nTo circumvent these difficulties, we adopt an alternative approach. Specifically, we design a synthetic task which replicates key structures in two-hop reasoning with distractions, and train a small transformer on this task. At the beginning of the training, the small model is affected by distractions, recapulating the behaviour of LLMs observed in Figure 1. As the training continues, the small model goes through a phase transition and learns to ignore the distractions. By fully reverse-engineering the smaller model, we unveil the mechanism for random guessing. By further reducing the small model to a 3-parameter model (Section 3.2), we unveil that a sudden realization of the sequential query mechanism drives the transformer to solve the two-hop reasoning with distractions. Leveraging the aware of sequential query mechanism, we predict that LLMs finetuned on two-hop reasoning task with single distraction have length generalization and subsequently test them on larger models. The consistency of our results across different scales provides strong empirical evidence supporting the validity and generalizability of our findings (Section 4)."}, {"title": "1.1. Related works", "content": "Multi-hop reasoning is a common evaluation task for LLMs that requires composing several factual associations together (Zhong et al., 2023). Mechanistic interpretability studies have found that LLMs perform multi-hop reasoning by serially recalling intermediate hops (Biran et al., 2024; Yang et al., 2024b; Wang et al., 2024a; Feng et al., 2024). This work studies an in-context multi-hop reasoning task, where the knowledge is extracted from context, in contrast to these in-weight multi-hop reasoning studies.\nEarlier work introduced the induction-head mechanism for in-context learning in LLMs (Elhage et al., 2021; Olsson et al., 2022). Recent theoretical and empirical analyses have extensively studied induction-head mechanisms in small transformers (Bietti et al., 2023; Nichani et al., 2024; Wang et al., 2024b; Chen et al., 2024), demonstrating that a two-layer transformer is required to perform induction-head tasks (Sanford et al., 2024a). The in-context two-hop reasoning task generalizes the induction-head task, as first proposed in Sanford et al. (2024b), where it was shown that a log k-layers transformer is necessary and sufficient for performing in-context k-hop reasoning tasks.\nMechanistic interpretability is a growing field focused on investigating the internal mechanisms of language models (Elhage et al., 2021; Geva et al., 2023; Meng et al., 2022; Nanda et al., 2023; Olsson et al., 2022; Bietti et al., 2024; Wang et al., 2022a; Feng and Steinhardt, 2023; Todd et al., 2023). Examples include mechanisms such as the induction head and function vector for in-context learning (Elhage et al., 2021; Olsson et al., 2022; Todd et al., 2023; Bietti et al., 2024), the binding ID mechanism for binding tasks (Feng and Steinhardt, 2023), association-storage mechanisms for factual identification tasks (Meng et al., 2022), and a complete circuit for indirect object identification Wang et al. (2022a). Additionally, several studies have leveraged synthetic tasks to investigate neural network mechanisms Charton (2022); Liu et al. (2022); Nanda et al. (2023); Allen-Zhu and Li (2023); Zhu and Li (2023); Guo et al. (2023);"}, {"title": "2. Preliminaries", "content": "We define the key terminologies used, primarily focusing on the hidden states (or activations) during the forward pass.\nComponents in an attention layer. We denote RES as the residual stream. We denote VAL as Value (states), QRY as Query (states), and KEY as Key (states) in one attention head. The Attn-Logit represents the value before the softmax operation and can be understood as the inner product between QRY and KEY. We use Attn to denote the attention weights of applying the SoftMax function to Attn-Logit, and \"attention map\" to describe the visualization of the heat map of the attention weights. When referring to the Attn-Logit from \"B\" to \"A\", we indicate the inner product (QRY(B), KEY(A)), specifically the entry in the \"B\" row and \"A\" column of the attention map.\nLogit lens. We use the method of \"Logit Lens\" to interpret the hidden states and value states (Belrose et al., 2023). We use Logit to denote pre-SoftMax values of the next-token prediction for LLMs. Denote ReadOut as the linear operator after the last layer of transformers that maps the hidden states to the Logit. The logit lens is defined as applying the readout matrix to residual or value states in middle layers. Through the logit lens, the transformed hidden states can be interpreted as their direct effect on the logits for next-token prediction.\nTerminologies in two-hop reasoning. We refer to an input like \"SOURCE\u2192BRG1, BRG2\u2192END\" as a two-hop reasoning chain, or simply a chain. The source entity SOURCE serves as the starting point or origin of the reasoning. The end entity END represents the endpoint or destination of the reasoning chain. The bridge entity BRIDGE connects the source and end entities within the reasoning chain. We distinguish between two occurrences of BRIDGE: the bridge in the first premise is called BRG1, while the bridge in the second premise that connects to END is called BRG2. Additionally, for any premise \u201cA \u2192 B", "A": "we define the chain \u201cA \u2192 B, B \u2192 C"}, {"title": "3. Toy models", "content": "In this section, we discuss the internal mechanism of a three-layer transformer to perform two-hop reasoning. For better visualization, we use illustrative attention maps and defer the empirical evidence to Appendix C.\nMLP layers are negligible. We conducted ablation studies on the effect of MLP layers. Figure 4 shows that even if we skip all MLP layers during inference, the model's performance on the validation set during different training stages remains nearly the same. This suggests that the MLP layers did not develop meaningful functionality during training, effectively rendering them redundant. Therefore, we focus our analysis on attention heads.\nThe slow learning phase and abrupt learning during training. According to Figure 5, the model mainly experienced two different phases during training. In the slow learning phase (e.g., training step 0 to around 400), the model has a slow learning phase. It learned to predict an end token randomly (in Figure 5, we take average prediction probabilities over all the end tokens in distracting chains, so in the slow learning phase, the prediction probability for the target end and average non-target end are roughly equal). After an abrupt phase transition after more than 800 training steps, the model learns to predict the correct answer perfectly. We call the interim mechanism that the model learned during the slow learning phase random guessing mechanism, and observed that the final mechanism the model learned to make the perfect prediction is a sequential query mechanism. Besides, previous literature (Sanford et al., 2024b) posited that the transformer performs in-context two-hop reasoning by a mechanism they theoretically constructed, which we called double induction head mechanism. Below, we explain the random guessing mechanism and the sequential query mechanism layer by layer in detail, along with experiments supporting the existence of such mechanisms. The details of the double induction head mechanism are deferred to Appendix C.\nThe first layer. For both the random guessing mechanism and the sequential query mechanism, the first layer is a copy layer. In the attention map, each child token pays all attention to its parent token by positional encoding and then copies its parent token to its buffer space to be used in subsequent layers. This is pictorially illustrated in Figure 6."}, {"title": "3.1. Details of the mechanism of in-context two-hop reasoning", "content": "The interim mechanism in slow learning phase: random guessing. During the slow learning phase (e.g., around 400 steps), the model learns to randomly pick an end token by distinguishing between the end and bridge tokens among all child tokens. The underlying mechanism we observed is as follows. In the second layer, as shown in Figure 7, each child token will attend equally to all previous parent tokens and copy (the superposition of) them to its buffer. The buffer space will later be used in the last layer, where the query entity (i.e., the last token in the input sequence) will attend equally to all child tokens as shown in Figure 8. Thanks to the information on the buffer space collected in the second layer, the value state of each copied child token through the logit lens contains not only itself but also all parent tokens before that child token with a negative sign in the corresponding coordinate (Figure 9). By aggregating the value states of all child tokens in the last layer, the query entity can distinguish the end token from the bridge token since the positive value of the bridge token due to BRG1, a child token, is canceled out by the negative value caused by BRG2, a parent token, in the same corresponding coordinate. As a result, this interim mechanism learned in the slow learning phase can randomly guess an end entity as its prediction.\nThe (observed) sequential query mechanism after phase transition. After the phase transition during training, the model achieves nearly perfect accuracy, and we observed a sequential query mechanism during that stage. After copying parent entities in the first layer (Figure 6), in the second layer, the query entity will pay attention to the bridge entity whose corresponding source entity in the buffer space matches the query entity. Then, it copies this bridge entity to the query entity's buffer space (Figure 10). In the last layer, the query entity uses the collected bridge entity from the last layer to do the query again and obtains the corresponding end entity, which is exactly the expected answer (Figure 11)."}, {"title": "3.2. Building causal relationships between mechanisms and training dynamics using a three-parameter model", "content": "\"Causal\" hypotheses based on observations. In the last section, we observed two stages along the training dynamics of the three-layer transformer. In this section, we aim to build a \"causal\" relationship between the observed mechanisms and the training dynamics. The two \u201ccausal", "are": "nHypothesis 3.1. The formation of the random guessing mechanism causes the slow learning phase (0-400 steps).\nHypothesis 3.2. The formation of the sequential query mechanism causes the abrupt phase transition (800 steps).\nWe need to implement \"causal interventions\" to validate the hypotheses. Following the approach of Reddy (2023), we propose studying a three-parameter dynamical system, which simulates only the dynamics of the sequential query mechanism, removing the random guessing mechanism.\nComparing the training dynamics of the three-parameter model with the training dynamics of transformers to validate the causal hypotheses Since the random guessing mechanism is removed and the sequential query mechanism is kept, we anticipate that\n1.  Hypothesis 3.1 holds if the three-parameter model loses the slow learning phase in the training dynamics.\n2.  Hypothesis 3.2 holds if the three-parameter model preserves the abrupt phase transition in the training dynamics.\nFor convenience, we define the approximate SoftMax operator as:\n$S(u, M) = \\frac{exp(u)}{exp(u) + M}$\nIntuitively, the approximate SoftMax gives the probability of an item with logit u where the remaining M logits are all zero. Given a residual state u, we use CONT(u), BUF1(u), BUF2(u) to denote the original content (i.e., the token embedding and the positional embedding) of token u, the buffer space of u in the first layer, and the buffer space of u in the second layer, respectively. Since the tokens are not semantically related, we assume that (CONT(a), CONT(b)) = 1{a = b} for any two tokens a and b, which means that {CONT(\u00b7)} is an orthonormal basis."}, {"title": "The meaning of three parameters.", "content": "The sequential query mechanism consists of a copy layer in the first layer (Figure 6), QRY to BRG in the second layer (Figure 10), and QRY to END in the third layer (Figure 11). We use parameters \u03b1, \u03b2, and \u03b3 to represent the progressive measure of their functionalities.\nBUF1(BRG\u2081) = w\u2081CONT(SRC), (1)\nBUF1(END) = W\u2081CONT(BRG), (2)\nBUF2(QRY) = W2CONT(BRG), (3)\nOutput = W3CONT(END), (4)\nLoss = -log[S(\u03be\u03c9\u2083, V)]. (5)\nwhere\nW1 = S(\u03b1, \u039d),\nW2 = S(\u1e9e(CONT(QRY), BUF\u2081 (BRG1)), 2N),\nW3 = S((BUF2(QRY), BUF\u2081(END)), 2N).\nNote that when we set a \u2192 \u221e, \u03b2 \u2192 \u221e, and \u03b3 \u2192 \u221e, Loss \u2192 0, corresponding to the three-layer transformer trained after 10000 steps. When we set a = \u03b2 = \u03b3 = 0, the loss is close to a uniform guess in the vocabulary, corresponding to an untrained three-layer transformer.\nThe derivation of Equations (1) and (2). We present that how we simplify a full transoformer block to get Equations (1) and (2). As previously discussed, we can ignore the MLP block and focus on the attention block. As illustrated in Figure 6, the first attention block relies on the positional information to copy parent tokens to the buffer spaces of child tokens. The attention logits are given by\nAttn-Logit(CHILD \u2192 PARENT)\n= PosQ(1)TK(1)Posi\u22121,\nwhere $Q^{(1)}$, $K^{(1)}$ are weight matrices in the first layer. We assume that $Pos Q(1)TK(1) Posi\u22121 = a$ for any i. Since we reshuffle the positions for BRG1 and END for each sequence, following Reddy (2023), we approximate the attention weights to parent tokens by S(a, N), where N comes from taking the average from 2N positions. This gives Equations (1) and (2).\nThe derivation of Equation (3). Similarly, as illustrated in Figure 10, the BUF2(QRY) is proportional to the attention from the QRY token to BRG1 in the second layer. The QRY token uses its CONT(QRY) to fit the BUF1(BRG1), copying CONT(BRG) to the residual stream. Therefore,\nAttn-Logit(QRY \u2192 BRG1)\n= CONT(QRY)TQ(2)TK(2) BUF1(BRG1)\n= \u03b2\u00b7 (BUF1(BRG1), CONT(QRY)), where the last line could be viewed as a re-parametrization of Q(2)TK(2), with \u1e9e x ||Q(2)TK(2) ||2. Moreover, we fix the attention logits from QRY token to all other tokens to be zero, removing mechanisms other than the sequential query. The attention weight from the QRY token to the BRG1 becomes S(Attn-Logit(QRY \u2192 BRG1), 2N). This gives Equation (3).\nThe derivation of Equation (4) and (5). As illustrated in Figure 11, the QRY token increasingly concentrates on the TGT-END token along the training dynamics. With the same manner of Equation (3), we set that\nAttn-Logit(QRY \u2192 TGT \u2013 END)\n= BUF2(QRY)Q(3)TK(3)BUF1(TGT \u2013 END)\n= \u03b3\u00b7 (BUF2(QRY), BUF\u2081(TGT \u2013 END)).\nWe focus on Attn-Logit(QRY \u2192 TGT END) and set all other Attn-Logit to be zero. The attention weight from the query to the TGT-END becomes S(Attn-Logit(QRY \u2192 TGT END), 2N). We first consider the output Logit on the query token. Through the logit lens, as illustrated in Table 1, the value states of END tokens have large logits on itself. Therefore, we assume that ReadOut[VAL(TGT \u2013 END)] = \u00a7\u00b7 Cend \u2208 RV, with \u00a7 > 0 and eEND being a one-hot vector in RV that is non-zero on the index of END. In our simulation, we fix \u00a7 = 30. The loss can therefore be approximated through Equation (5).\nSimulations on three-parameter model validate Hypotheses 3.1 and 3.2. We optimize the loss function Equation (5) by running gradient descent with learning rate 0.1. Figure 12 presents the training dynamics of the 3-layer transoformer and the 3-parameter model. Since the model does not incorporate the random guessing mechanism, the loss remains unchanged during the first 1000 steps, validating Hypothesis 3.1. Both the parameters and the loss function go through a sudden phase transition around step 1000, suggesting that the emergence of the sequential query mechanism is the driving force behind the abrupt drop in loss. This validates Hypothesis 3.2."}, {"title": "4. Implications for Large Language Models", "content": "In this section, we discuss the implications of the mechanisms discovered from three-layer transformers in large language models. We use Llama2-7B-base, a widely used pre-trained language model, for all the experiments in this section. We relegate the details of experiments on Llama3.1-8B, Qwen2.5-7B to Appendix D."}, {"title": "4.1. Evaluation on two-hop reasoning with distraction", "content": "Evaluation dataset. We evaluate the in-context two-hop reasoning performance of LLaMA2-7B when there are distractions. To this end, we design fixed templates for in-context reasoning chains that cover various topics. For example, a template related to biology: \"[A] is a species in the genus [B]. The genus [B] belongs to the family [C].\" Given multiple two-hop chains, we randomly order premises of two-hop chains according to the data generation procedure illustrated in Figure 3. We then append a query sentence, such as \"Therefore, [A] is classified under the family,\" after the context. Finally, we randomly sample from a set of artificial names to replace placeholders [A], [B], and so on. We perform a forward pass on the entire sequence and compute the next-token probability for the last token-in this example, \"family.\" Further details on the dataset are provided in AppendixB.\nDistraction significantly reduces the two-hop reasoning accuracy of LLMs. We set the number of two-hop chains to K = 2 and compute the next-token probability. We track the probability corresponding to the target END, which is the correct answer. When END consists of multiple tokens, we consider only the first token. Figure 13 compares the probability of LLaMA2-7B predicting the correct answer with and without distracting information. The results indicate a consistent reduction in probability by half across different template topics of the two-hop reasoning."}, {"title": "4.2. Length generalization", "content": "The evaluation results indicate that Llama2-7B employs a random guessing mechanism for two-hop reasoning, which is affected by distractions significantly. According to the training dynamics of three-layer transformers, Llama2-7B stays at a stage right before the abrupt phase transition. Therefore, we predict that LLMs will switch to sequential query mechanism through an abrupt phase transition when finetuned on two-hop reasoning task. Moreover, since the sequential query mechanism can generalize to arbitrary number of distractions. As a result, we have the following verifiable prediction for LLMs:\nEven finetuned on two-hop reasoning with one distraction, LLMs can generalize to two-hop reasoning with multiple distractions.\nExperimental details. We finetune Llama2-7B on 1000 prompts, each consist of one target two-hop reasoning chain and one distraction chain. The details are relegate to Appendix B. Figure 14 shows the model's performance on in-context two-hop reasoning with different numbers of distracting chains in the prompt, both before and after fine-tuning.\nFinetuning on prompts with one distraction generalizes to multiple distractions. For the original pre-trained Llama2-7B model without fine-tuning, although it can make correct predictions when there is no distracting chain, its performance drops drastically as long as there exist distracting chains. Moreover, the probability of the model predicting the target end is close to the average probability of predicting any specific distracting end, which shows that the model adopts nearly uniform random guessing. In contrast, after fine-tuning, the model robustly makes the correct prediction. The most remarkable phenomenon is that although the model is fine-tuned on data with only two two-hop reasoning chains (one target chain and one distracting chain), the model still makes correct predictions with high probability, even if the number of chains in the prompt is as large as five. Compared to the original model, it is salient that the original model performs random guessing while the fine-tuned model learns the correct mechanism to solve the in-context two-hop reasoning problem according to its ability on length generalization. This validates the conclusion in Section 3."}, {"title": "5. Conclusions", "content": "In this paper, we study the underlying mechanism that transformer-based LLMs use to solve in-context two-hop reasoning tasks, especially in the presence of distracting information. By carefully analyzing the training dynamics and fully reverse-engineering a three-layer transformer, we identified an interim uniform guessing mechanism during the early training stages and a sequential query mechanism after a sharp phase transition. Then, we analyzed a three-parameter dynamical system to provide further evidence and a more in-depth understanding of the phase transition in the form of the sequential query mechanism. Finally, our extensive experimental results on Llama2-7B-Base provide strong evidence that the original pre-trained model performs the uniform guessing mechanism on the two-hop reasoning task, and very few steps of fine-tuning suffice to teach the model to learn a correct mechanism."}]}