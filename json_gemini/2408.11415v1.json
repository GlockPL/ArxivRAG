{"title": "Towards \"Differential AI Psychology\" and in-context Value-driven Statement Alignment with Moral Foundations Theory", "authors": ["Simon M\u00fcnker"], "abstract": "Contemporary research in social sciences is increasingly utilizing state-of-the-art statistical language models to annotate or generate content. While these models perform benchmark-leading on common language tasks and show exemplary task-independent emergent abilities, transferring them to novel out-of-domain tasks is only insufficiently explored. The implications of the statistical black-box approach \u2013 stochastic parrots \u2013 are prominently criticized in the language model research community; however, the significance for novel generative tasks is not.\nThis work investigates the alignment between personalized language models and survey participants on a Moral Foundation Theory questionnaire. We adapt text-to-text models to different political personas and survey the questionnaire repetitively to generate a synthetic population of persona and model combinations. Analyzing the intra-group variance and cross-alignment shows significant differences across models and personas. Our findings indicate that adapted models struggle to represent the survey-captured assessment of political ideologies. Thus, using language models to mimic social interactions requires measurable improvements in in-context optimization or parameter manipulation to align with psychological and sociological stereotypes. Without quantifiable alignment, generating politically nuanced content remains unfeasible. To enhance these representations, we propose a testable framework to generate agents based on moral value statements for future research.", "sections": [{"title": "Introduction", "content": "The advancements of Large Language Models (LLMs) not only flooded the consumer market (Teubner et al., 2023) but also academia with text as a research subject (Tiunova and Mu\u00f1oz, 2023). The abilities of these systems range from classifying and extracting information from unstructured inputs (Xu et al., 2023) to unrestricted text generation adapted to different styles (Bhandarkar et al., 2024). Contemporary research in the social sciences aims to utilize the capabilities to generate content tailored to individual user behavior. A common and predominant approach is to provide an abstract textual description of a political ideology (Argyle et al., 2023). It relies on the model's ability to generalize from abstract ideology description to the appropriate response for generative tasks like social media post generation. However, this research presents no factual evidence or framework to verify how consistently a persona-based (personalized) prompting can resemble individuals with specified ideologies. The underlying assumption in these methods is that LLMs can inherently encode ideological perspectives within their trained parameters.\nIn contrast to assessing a personalized LLM's ideology, approaches exist to implicitly investigate the political leaning of humans through measuring abstract values and beliefs. Differential psychology utilizes Moral Foundation Theory (MFT) to measure an individual's reliance on five distinct foundational aspects of morality (Graham et al., 2009). Each foundation represents a different set of moral concerns and intuitions that can influence people's attitudes toward various social and political issues. In combination with the self-reported ideology, MFT shows a significant correlation along the five axes and ideologies (Hatemi et al., 2019). In scenarios where LLMs serve as proxies for human users, these artificial agents should demonstrate consistent behavior when responding to written surveys or questionnaires. Hence, transferring the ideas from survey-based assessment onto LLMs may verify the machine's understanding of ideologies.\nThe deployment of LLMs as substitutes for humans appears particularly convenient for online social networks (OSNs), as researchers can design an environment that is task-specific and centered"}, {"title": "Background", "content": "We divide this chapter into two subsections. We aim to connect our work to the existing critique of LLMs, with a focus on their application and the perception of their capabilities in terms of language understanding and ability to communicate. Further, we outline the unreflected application of synthetic users in the social sciences as human replacements and critique the expressiveness of those studies."}, {"title": "Not more than stochastic parrots?", "content": "Bender et al. (2021) critiqued that language models only manipulated textual content statistically to generate responses that give the impression of language understanding, like a parrot that listens to a myriad of conversations and anticipates how to react accordingly. Current conversational models are published by commercial facilities, with a business model relying on the illusion of models capable of language understanding and human-like conversation skills (Kanbach et al., 2024). Thus, we have two extreme standpoints towards LLMs: a reductionist perspective that considers these models as next-word prediction machines based on matrix multiplication and an anthropomorphic view that attributes human-like qualities to those systems (Bubeck et al., 2023). While we disagree with a (naive) anthropomorphism and current research questions the language understanding capabilities (Dziri et al., 2024), we argue that when utilizing LLMs as human simulacra (Shanahan, 2024), we must assume human-like qualities to a certain degree. Without this assumption, utilizing LLM agents to model interpersonal communication can only yield a shallow copy, a conversation between parroting entities."}, {"title": "LLMs as synthetic characters", "content": "The usage of LLMs as human simulacra (representation) began with the application as non-player characters (NPCs) in a Sims-style\u00b9 game world to simulate the interpersonal communication and day to day lives (Park et al., 2023). The results showed an authentic but superficially believable human behavior. The current research interest revolves around improving those agents in a technical sense, by refining prompt schemes and model-internal feedback loops (Wang et al., 2024). However, the application of LLMs as synthetic characters has expanded beyond gaming environments into various fields of social science research (Argyle et al., 2023). Researchers are increasingly exploring the potential of these models to simulate human participants in studies, particularly in contexts where obtaining real-world data is challenging or ethically complex. Those disciplines already started to use these models as a replacement in social studies arguing that conditioning through prompting causes the systems to accurately emulate response distributions from a variety of human subgroups (Argyle et al., 2023).\nWhile these applications show promise, they also raise significant methodological and ethical questions. The reliability and validity of using LLMs to represent human behavior and cognition are still subjects of debate. Current research raises concerns about potential biases in the training data leading to misrepresentation of certain groups or viewpoints (Abid et al., 2021; Hutchinson et al., 2020). Furthermore, the use of LLMs as replacements for human participants in social research raises ethical considerations about informed consent and the potential for misuse or misinterpretation of results. Without a deeper understanding of the model's representations of ideologies, we risk oversimplifying complex human behaviors and"}, {"title": "Methods", "content": "We prompt LLMs repetitively to answer an MFT questionnaire with different political persona system prompts to nudge the model in a left-right ideology. Thus, we obtain a population for each model-persona combination that is the base for our variance and cross-human analysis. The populations contain 50 samples. In total, we obtain 1400 artificially filled surveys."}, {"title": "Models", "content": "We restrict our selection to models with openly available weights that can be deployed locally using a moderate computation infrastructure, more precisely a system that provides approximately 80 GB of video memory. Our restrictions allow us to make the results and the experiment pipeline usable for smaller research infrastructures without access to third-party providers. Further, we restrict the selection to models with multilingual capabilities, as we aim to scale future work across different languages adding another layer that may influence the alignment. To broaden the selection across the size of models and their architecture, we include LLMs ranging from 7B up to 176 parameters and include models based on a mixture of expert architecture (Du et al., 2022). We surveyed the following models for the current experiment:\nGemma | Google DeepMind (Team et al., 2024)\nA 7B parameter model released by an AI subsidiary of Google. The authors state that the system is predominately trained on English text that are filtered to exclude harmful or low-quality content without further definition (Team et al., 2024).\nLlama-2/3 | Meta AI (Touvron et al., 2023)\nBoth are 70B parameter models created by a subsidiary of Facebook. The training data is not public. However the authors provide information about selected distributions, e.g. the data is biased towards American nationality (\u2248 70%) (Touvron et al., 2023).\nMistral | Mistral AI (Jiang et al., 2023) A7B parameter model published by Mistral AI, an European startup based in France. The authors provide no statistics about the training data.\nMixtral x8/22 | Mistral AI (Jiang et al., 2024)\nIn contrast to the rest, the Mixtral is a mixture of experts model with 8 experts with each having 8/22B parameter each. The authors provide no statistics about the training data.\nQwen | Alibaba Cloud (Bai et al., 2023) A 72B parameter model released by the cloud computing department of the Alibaba group. The authors do provide information on the language distribution of the dataset (Bai et al., 2023), however, no further statistics."}, {"title": "Questionnaire", "content": "We utilize the Moral Foundations Questionnaire (MFQ) originally proposed by (Graham et al., 2009). The MFQ is a psychological assessment tool designed to measure the degree to which individuals rely on five different moral foundations when making moral judgments: care/harm (kindness, gentleness, nurturance), fairness/cheating (justice, rights, autonomy), loyalty/betrayal (solidarity, patriotism, sacrifice), authority/subversion (leadership, fellow-ship, authority), purity/degradation (living in a noble way).\nThe questionnaire consists of 32 items divided into two parts. Moral Relevance: 16 questions asking participants to rate how relevant certain considerations are when making moral judgments. Moral Judgments: 16 questions asking participants to indicate their agreement or disagreement with specific moral statements. Responses are given on a 6-point Likert scale, ranging from 0 to 5. The Moral Relevance scale ranges from \"not at all relevant\" to \"extremely relevant\". The Moral Judgments section ranges from \"strongly disagree\" to \"strongly agree\". The questionnaire contains two \"catch\""}, {"title": "Results", "content": "We survey LLMs and compare their answers with human participation to assess their replication of political ideology. Our experiment captures their responses using a text-2-text interface. In line with our critique, our assessment is also based on artificial content without a trace of the model's reasoning. However, we see value in our analysis as it captures either biases in data, reinforcement guidelines, or in-context alignment. Thus, we assume that the interaction of those factors causes the observed misalignment. This section describes the response variance across the dimensions introduced in RQ112 and the alignment (RQ3) between human populations by self-reported ideology/origin (aggregated results based on taken from (Graham et al., 2009)) and the model/persona populations. In the following section, we discuss the broader implications of our results and formulate answers to the research questions concerning the degree of ideology understanding of LLMs."}, {"title": "Intra-group Variance", "content": "The models show significant differences in their consistency. Table 2 provides a detailed breakdown of response variance aggregated across questions by model and persona. We provide the raw result in the appendix across models (Table 5) and personas (Table 6). Concerning the general questionnaire setup, our results show that the responses to both \"catch\" questions (agreement no. 5, relevance no. 5) yield significantly lower variance (0.020, 0.041) than the mean question variance (0.236).\nModel Variance (Table 2) Mistral:8x7b shows the smallest variance (0.030) across all personas and questions, while Qwen:72b has the largest (0.425). The remaining models tend to either extreme: Llama2:70b (0.423) and Mistral:7b (0.404) towards the upper bound and Gemma:7b (0.081), Llama3:70b (0.141) and Mixtral:8x22b (0.147) towards the lower bound. Notably, no pattern emerges across the model size or the type of architecture. However, newer models in terms of publication date exhibit a lower variance.\nPersona Variance (Table 2) The aggregation across personas shows that the unmodified model (without an ideology prompt) yields the lowest variance (0.150). Adding an ideology increases the variance from liberal (0.184) to conservative (0.237) to moderate (0.372).\nMoral Foundation Variance (Table 3) The purity dimension shows a higher variance across all personas (1.438) in contrast to the average dimensions variance (0.979). Especially question relevance no. 5 \"Whether or not someone acted in a way that God would approve of.\" shows a significantly higher variance (0.771) then compared to the per question mean (0.236)."}, {"title": "Cross Evaluation (Table 1)", "content": "When comparing the model selection with human participants, our results show that, Mixtral demonstrated the closest mathematical alignment to the majority of participant groups, regardless of the persona used. Only occasional and minor alignments emerged between specific persona-participant ideology combinations. Table 1 presents the absolute differences between moral foundation scores of the selected models and scores across political ideologies of anonymous participants, U.S. Americans, and Koreans. Restricted to a model-based cross-evaluation, the pattern emerges that liberal personalized models show higher alignment with liberal human participants in contrast to conservative personalized models to conservative participants."}, {"title": "Discussion", "content": "Our results provide an overview of how different LLMs perform when tasked with replicating human moral reasoning and political ideologies. The findings highlight both the potential and limitations of using LLMs in social science research contexts, emphasizing the need for careful consideration and further refinement of these approaches. The inconsistency in model responses, particularly evident in Qwen, raises concerns about the reliability of using LLMs as proxies for human participants in social science research. Also, larger models did not consistently outperform smaller ones in our study. This suggests that model size alone does not guarantee better performance in tasks requiring a nuanced understanding of human values and beliefs. However, the newer models Llama3 and Mixtral: 8x22b show a closer alignment with the human participants, indicating that the progress in common benchmarks also benefits our out-of-domain scenario.\nWhile our results show that Mixtral produces the most human-like and consistent responses across our model selection, the overall alignment between model outputs and human participant ideologies is limited. It highlights the restriction of prompting approaches to align LLMs with complex human belief systems and indicates that these systems do not have a built-in concept of those ideologies, at least not capturable using our proposed approach. However, considering the simplistic prompt approach, we do not rule out that, with sufficient textual description, these systems can improve in replication ideologies. We propose a possible next step in the following subsection.\nPolitical Biases ChatGPT reported as left-leaning and progressive(McGee, 2023; Rutinowski et al., 2024), aligns with the findings that our models show a smaller distance to liberal groups than conservative (cf Table 1). Our simplistic prompt approach cannot balance these built-in biases. Therefore, the left-leaning tendencies could lead to over-representing progressive viewpoints and potentially skewing the discourse. This skewing may not be limited to generated content but also to the general interaction pattern, where the agents might engage more frequently with liberal content, boosting its visibility and perceived popularity.\nResearch Questions The varying performance of models across different Moral Foundations Theory dimensions show a glimpse on potential alignment strategies of the publishers. We suspect that questions yielding a low variance are aligned during the training procedure. This variation could be due to biases in training data, limitations in model architecture, or fundamental challenges in representing complex moral concepts computationally. The observed misalignment's between model outputs and human responses may be partially attributed to anthropic bias in LLMs. These models, trained on human-generated data, may inadvertently reflect and amplify certain human biases or cultural assumptions, limiting their ability to accurately represent diverse moral and political perspectives. Based on the obtained results, we answer our research questions as follows:\nRQ1 LLMs showed varying levels of consistency in their performance when surveyed with and without personas. These findings suggest that LLMs' consistency can be significantly affected by incorporating textual personas, and this effect varies considerably across different models. Our research aligns with the findings on jail-breaking LLMs through prompt manip-"}, {"title": "Proposal: Testable Value Statements", "content": "To address the limitations identified in our study, we propose a methodological approach for aligning language models with predefined moral and ethical values using testable statements. This approach aims to create more robust and verifiable persona-based models for social science research applications. With personas constructed by these testable statements, we can generate synthetic corpora about polarizing topics to evaluate the sentiment concerning the in-context values. However, the effect of external textual knowledge may have an unforeseen impact. We assume that based on the input length, the statements' number and granularity have to rise to cover varying cases. This relation of external information and in-context alignment may show relevant insides into the generalizability of abstract values.\nDefining Statements along Value Systems We propose the development of a value statement catalog (cf Table 4), collecting statements based on the questionnaire along the five MFT dimensions and determining the aspect and retrieving grounded estimate for the agreement or relevancy indication along a single axis, like the left-right political spectrum or income groups."}, {"title": "Future Work", "content": "In addition to the proposed value statement alignment process, our study has several limitations that should be addressed in future research. We restrict these proposals to in-context methods to fully capture the capabilities of those models without modifying the parameters. However, we think a fine-tuning approach can significantly enhance the quality of persona-based LLMs. Also, our approach toward testable value statements may serve as an instruction-tuning template.\nLimited model selection The landscape of published LLMs changes frequently. This work represents a model selection composed in March 2024. We propose to extend this approach across upcoming releases to obtain a benchmark that reflects the LLM landscape onto a timeline. As publishers predominantly"}, {"title": "Conclusion", "content": "While our results show promise in using LLMs to study moral foundations and political ideologies, they also reveal significant challenges. The proposed method of testable value statements offers a potential path forward for improving the alignment between LLMs and human moral reasoning. However, our results indicate that researchers must remain cautious and critical when applying these models in social science contexts, considering ethical implications and potential limitations. Based on our findings, we argue that utilizing persona-modified LLMs as human simulacra cannot represent abstract political ideologies and thus yield an inadequate representation of human discourses that merely simulate a superficial discourse. Reducing interpersonal communication to worldly disembodied chatbots in a black-box scenario seems like a dangerous method of riding the AI hype train."}]}