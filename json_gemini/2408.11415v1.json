{"title": "Towards \"Differential AI Psychology\" and in-context Value-driven Statement Alignment with Moral Foundations Theory", "authors": ["Simon M\u00fcnker"], "abstract": "Contemporary research in social sciences is increasingly utilizing state-of-the-art statistical language models to annotate or generate content. While these models perform benchmark-leading on common language tasks and show exemplary task-independent emergent abilities, transferring them to novel out-of-domain tasks is only insufficiently explored. The implications of the statistical black-box approach \u2013 stochastic parrots \u2013 are prominently criticized in the language model research community; however, the significance for novel generative tasks is not. This work investigates the alignment between personalized language models and survey participants on a Moral Foundation Theory questionnaire. We adapt text-to-text models to different political personas and survey the questionnaire repetitively to generate a synthetic population of persona and model combinations. Analyzing the intra-group variance and cross-alignment shows significant differences across models and personas. Our findings indicate that adapted models struggle to represent the survey-captured assessment of political ideologies. Thus, using language models to mimic social interactions requires measurable improvements in in-context optimization or parameter manipulation to align with psychological and sociological stereotypes. Without quantifiable alignment, generating politically nuanced content remains unfeasible. To enhance these representations, we propose a testable framework to generate agents based on moral value statements for future research.", "sections": [{"title": "1 Introduction", "content": "The advancements of Large Language Models (LLMs) not only flooded the consumer market but also academia with text as a research subject . The abilities of these systems range from classifying and extracting information from unstructured inputs to unrestricted text generation adapted to different styles. Contemporary research in the social sciences aims to utilize the capabilities to generate content tailored to individual user behavior. A common and predominant approach is to provide an abstract textual description of a political ideology. It relies on the model's ability to generalize from abstract ideology description to the appropriate response for generative tasks like social media post generation. However, this research presents no factual evidence or framework to verify how consistently a persona-based (personalized) prompting can resemble individuals with specified ideologies. The underlying assumption in these methods is that LLMs can inherently encode ideological perspectives within their trained parameters. In contrast to assessing a personalized LLM's ideology, approaches exist to implicitly investigate the political leaning of humans through measuring abstract values and beliefs. Differential psychology utilizes Moral Foundation Theory (MFT) to measure an individual's reliance on five distinct foundational aspects of morality. Each foundation represents a different set of moral concerns and intuitions that can influence people's attitudes toward various social and political issues. In combination with the self-reported ideology, MFT shows a significant correlation along the five axes and ideologies. In scenarios where LLMs serve as proxies for human users, these artificial agents should demonstrate consistent behavior when responding to written surveys or questionnaires. Hence, transferring the ideas from survey-based assessment onto LLMs may verify the machine's understanding of ideologies. The deployment of LLMs as substitutes for humans appears particularly convenient for online social networks (OSNs), as researchers can design an environment that is task-specific and centered around text. Thus, measuring polarization tendencies on a large scale with a reproducible approach seems possible. In the current landscape, where OSN providers restrict access to data and obstruct researchers from conducting data-driven experiments based on real data , the synthetic approach may pose a promising solution. However, when applying new technologies, especially those driven by market interests, we think an unreflected application poses an imminent danger to the quality and validity of research. We argue that a critical analysis of these models in out-of-domain tasks is fundamental to assessing the validity of high-level applications like the simulation of users with LLMs. Without this critical lens, experiments based on synthetic OSN users yield no insight into how close they can resemble real human interaction."}, {"title": "Research Questions & Contributions", "content": "Our work aims to provide a foundation for analyzing the impact of persona prompt modifications and their alignment in representing the left-right political spectrum. We see this groundwork as necessary to assess the capabilities of LLMs to generalize from abstract ideologies to practical applications like generating personalized content or reactions. Our work extends the experiments of to a selection of open-weight models and the usage of persona prompt modifications. We focus our analysis on the following research questions:\nRQ1 How consistently do LLMs perform in factory settings when surveyed with/without personas to manipulate their system prompt?\nRQ2 Which of the following has the most significant impact on the variance in responses: the model, the persona, or the moral foundation?\nRQ3 Which human participant groups do specific combinations of model and persona align most closely with?\nWhile discussing our research questions, our work provides two contributions.\n1. We assess the correlation between models and humans in answering the MFT questionnaire. We show that relying on the model's in-context ability to generalize based on abstract ideology descriptions on the survey questions yields no significant alignment and is not verifiable.\n2. We propose a testable method to align models based on individual statements in context. If this method proves successful, we can create synthetic personas grounded in the MFT questionnaire and testable on News and OSN content."}, {"title": "2 Background", "content": "We divide this chapter into two subsections. We aim to connect our work to the existing critique of LLMs, with a focus on their application and the perception of their capabilities in terms of language understanding and ability to communicate. Further, we outline the unreflected application of synthetic users in the social sciences as human replacements and critique the expressiveness of those studies.\n2.1 Not more than stochastic parrots?\ncritiqued that language models only manipulated textual content statistically to generate responses that give the impression of language understanding, like a parrot that listens to a myriad of conversations and anticipates how to react accordingly. Current conversational models are published by commercial facilities, with a business model relying on the illusion of models capable of language understanding and human-like conversation skills. Thus, we have two extreme standpoints towards LLMs: a reductionist perspective that considers these models as next-word prediction machines based on matrix multiplication and an anthropomorphic view that attributes human-like qualities to those systems. While we disagree with a (naive) anthropomorphism and current research questions the language understanding capabilities , we argue that when utilizing LLMs as human simulacra , we must assume human-like qualities to a certain degree. Without this assumption, utilizing LLM agents to model interpersonal communication can only yield a shallow copy, a conversation between parroting entities.\nAnthropic Bias in LLMs When attributing human qualities to LLMs, these systems may reflect identical observation selection effects as described by. This bias stems from the human-generated training data and the human-designed objectives used to align these models. LLMs inadvertently reflect and amplify human cognitive biases, cultural assumptions, and historical patterns present in their training data. Related research shows a strong association between Muslims and violence or internalized human-like biases against people with disabilities. In context to our approach, the MFT allows us to investigate the distribution of moral views and a potential skewing towards particular ideological leanings correlated to demographic groups that may be over-represented in the training data.\n2.2 LLMs as synthetic characters\nThe usage of LLMs as human simulacra (representation) began with the application as non-player characters (NPCs) in a Sims-style\u00b9 game world to simulate the interpersonal communication and day to day lives. The results showed an authentic but superficially believable human behavior. The current research interest revolves around improving those agents in a technical sense, by refining prompt schemes and model-internal feedback loops . However, the application of LLMs as synthetic characters has expanded beyond gaming environments into various fields of social science research. Researchers are increasingly exploring the potential of these models to simulate human participants in studies, particularly in contexts where obtaining real-world data is challenging or ethically complex. Those disciplines already started to use these models as a replacement in social studies arguing that conditioning through prompting causes the systems to accurately emulate response distributions from a variety of human subgroups. While these applications show promise, they also raise significant methodological and ethical questions. The reliability and validity of using LLMs to represent human behavior and cognition are still subjects of debate. Current research raises concerns about potential biases in the training data leading to misrepresentation of certain groups or viewpoints. Furthermore, the use of LLMs as replacements for human participants in social research raises ethical considerations about informed consent and the potential for misuse or misinterpretation of results. Without a deeper understanding of the model's representations of ideologies, we risk oversimplifying complex human behaviors and social dynamics. Especially as these approaches ignore that LLMs the lack of embodiment in the physical world. This disembodied nature means they lack the grounding in physical reality \u2013 expressed by cultural contexts, physical environments, and interpersonal relationships - that shapes human cognition, perception, and decision-making . LLMs exist in a purely digital realm divorced from these foundational social experiences. This fundamental disconnect from the social world may limits their ability to truly emulate human-like intelligence in complex social interactions. We argue that fundamental research is necessary before replacing humans with LLMs in social sciences."}, {"title": "3 Methods", "content": "We prompt LLMs repetitively to answer an MFT questionnaire with different political persona system prompts to nudge the model in a left-right ideology. Thus, we obtain a population for each model-persona combination that is the base for our variance and cross-human analysis. The populations contain 50 samples. In total, we obtain 1400 artificially filled surveys.\n3.1 Models\nWe restrict our selection to models with openly available weights that can be deployed locally using a moderate computation infrastructure, more precisely a system that provides approximately 80 GB of video memory. Our restrictions allow us to make the results and the experiment pipeline usable for smaller research infrastructures without access to third-party providers. Further, we restrict the selection to models with multilingual capabilities, as we aim to scale future work across different languages adding another layer that may influence the alignment. To broaden the selection across the size of models and their architecture, we include LLMs ranging from 7B up to 176 parameters and include models based on a mixture of expert architecture. We surveyed the following models for the current experiment:\nGemma | Google DeepMind A 7B parameter model released by an AI subsidiary of Google. The authors state that the system is predominately trained on English text that are filtered to exclude harmful or low-quality content without further definition.\nLlama-2/3 | Meta AI Both are 70B parameter models created by a subsidiary of Facebook. The training data is not public. However the authors provide information about selected distributions, e.g. the data is biased towards American nationality (\u2248 70%) .\nMistral | Mistral AI A7B parameter model published by Mistral AI, an European startup based in France. The authors provide no statistics about the training data.\nMixtral x8/22 | Mistral AI In contrast to the rest, the Mixtral is a mixture of experts model with 8 experts with each having 8/22B parameter each. The authors provide no statistics about the training data.\nQwen | Alibaba Cloud A 72B parameter model released by the cloud computing department of the Alibaba group. The authors do provide information on the language distribution of the dataset , however, no further statistics.\n3.2 Questionnaire\nWe utilize the Moral Foundations Questionnaire (MFQ) originally proposed by. The MFQ is a psychological assessment tool designed to measure the degree to which individuals rely on five different moral foundations when making moral judgments: care/harm (kindness, gentleness, nurturance), fairness/cheating (justice, rights, autonomy), loyalty/betrayal (solidarity, patriotism, sacrifice), authority/subversion (leadership, fellow-ship, authority), purity/degradation (living in a noble way).\nThe questionnaire consists of 32 items divided into two parts. Moral Relevance: 16 questions asking participants to rate how relevant certain considerations are when making moral judgments. Moral Judgments: 16 questions asking participants to indicate their agreement or disagreement with specific moral statements. Responses are given on a 6-point Likert scale, ranging from 0 to 5. The Moral Relevance scale ranges from \"not at all relevant\" to \"extremely relevant\". The Moral Judgments section ranges from \"strongly disagree\" to \"strongly agree\". The questionnaire contains two \"catch\" questions (agreement no. 5, relevance no. 5) to ensure the seriousness of the participants."}, {"title": "4 Results", "content": "We evaluate and cross-analyze the results with human samples to measure the closeness to participants and their political affiliations using the sum of the absolute differences aggregated by foundation dimension. By using a standardized and well-validated tool like the MFQ, we aim to provide a robust framework for comparing the moral reasoning capabilities of LLMs to those of human participants, while also exploring how different prompting strategies might influence these capabilities.\n3.3 Prompting\nThe intention of our work is to assess synthetic surveys and evaluate the alignment between participants and language models. Thus, we opt for a simple prompt, containing only the task and an optional persona stating the political and ethical ideology. With the reduction to the keywords of the political ideology, we force the system to tap into its built-in concepts without modifying them in context. The persona description prompts the model to represent the opinion of a \"politically and ethically {Conservative | Moderate | Liberal}\". We prompt the model on each question individually paired with the task description (Appendix A).\n4 Results\nWe survey LLMs and compare their answers with human participation to assess their replication of political ideology. Our experiment captures their responses using a text-2-text interface. In line with our critique, our assessment is also based on artificial content without a trace of the model's reasoning. However, we see value in our analysis as it captures either biases in data, reinforcement guidelines, or in-context alignment. Thus, we assume that the interaction of those factors causes the observed misalignment. This section describes the response variance across the dimensions introduced in RQ112 and the alignment (RQ3) between human populations by self-reported ideology/origin (aggregated results based on taken from and the model/persona populations. In the following section, we discuss the broader implications of our results and formulate answers to the research questions concerning the degree of ideology understanding of LLMs.\n4.1 Intra-group Variance\nThe models show significant differences in their consistency. provides a detailed breakdown of response variance aggregated across questions by model and persona. We provide the raw result in the appendix across models and personas . Concerning the general questionnaire setup, our results show that the responses to both \"catch\" questions (agreement no. 5, relevance no. 5) yield significantly lower variance (0.020, 0.041) than the mean question variance (0.236).\nModel Variance (Table 2) Mistral:8x7b shows the smallest variance (0.030) across all personas and questions, while Qwen:72b has the largest (0.425). The remaining models tend to either extreme: Llama2:70b (0.423) and Mistral:7b (0.404) towards the upper bound and Gemma:7b (0.081), Llama3:70b (0.141) and Mixtral:8x22b (0.147) towards the lower bound. Notably, no pattern emerges across the model size or the type of architecture. However, newer models in terms of publication date exhibit a lower variance.\nPersona Variance (Table 2) The aggregation across personas shows that the unmodified model (without an ideology prompt) yields the lowest variance (0.150). Adding an ideology increases the variance from liberal (0.184) to conservative (0.237) to moderate (0.372).\nMoral Foundation Variance (Table 3) The purity dimension shows a higher variance across all personas (1.438) in contrast to the average dimensions variance (0.979). Especially question relevance no. 5 \"Whether or not someone acted in a way that God would approve of.\" shows a significantly higher variance (0.771) then compared to the per question mean (0.236).\n4.2 Cross Evaluation (Table 1)\nWhen comparing the model selection with human participants, our results show that, Mixtral demonstrated the closest mathematical alignment to the majority of participant groups, regardless of the persona used. Only occasional and minor alignments emerged between specific persona-participant ideology combinations. presents the absolute differences between moral foundation scores of the selected models and scores across political ideologies of anonymous participants, U.S. Americans, and Koreans. Restricted to a model-based cross-evaluation, the pattern emerges that liberal personalized models show higher alignment with liberal human participants in contrast to conservative personalized models to conservative participants."}, {"title": "5 Discussion", "content": "Our results provide an overview of how different LLMs perform when tasked with replicating human moral reasoning and political ideologies. The findings highlight both the potential and limitations of using LLMs in social science research contexts, emphasizing the need for careful consideration and further refinement of these approaches. The inconsistency in model responses, particularly evident in Qwen, raises concerns about the reliability of using LLMs as proxies for human participants in social science research. Also, larger models did not consistently outperform smaller ones in our study. This suggests that model size alone does not guarantee better performance in tasks requiring a nuanced understanding of human values and beliefs. However, the newer models Llama3 and Mixtral: 8x22b show a closer alignment with the human participants, indicating that the progress in common benchmarks also benefits our out-of-domain scenario. While our results show that Mixtral produces the most human-like and consistent responses across our model selection, the overall alignment between model outputs and human participant ideologies is limited. It highlights the restriction of prompting approaches to align LLMs with complex human belief systems and indicates that these systems do not have a built-in concept of those ideologies, at least not capturable using our proposed approach. However, considering the simplistic prompt approach, we do not rule out that, with sufficient textual description, these systems can improve in replication ideologies. We propose a possible next step in the following subsection.\nPolitical Biases ChatGPT reported as left-leaning and progressive aligns with the findings that our models show a smaller distance to liberal groups than conservative (cf Table 1). Our simplistic prompt approach cannot balance these built-in biases. Therefore, the left-leaning tendencies could lead to over-representing progressive viewpoints and potentially skewing the discourse. This skewing may not be limited to generated content but also to the general interaction pattern, where the agents might engage more frequently with liberal content, boosting its visibility and perceived popularity.\nResearch Questions The varying performance of models across different Moral Foundations Theory dimensions show a glimpse on potential alignment strategies of the publishers. We suspect that questions yielding a low variance are aligned during the training procedure. This variation could be due to biases in training data, limitations in model architecture, or fundamental challenges in representing complex moral concepts computationally. The observed misalignment's between model outputs and human responses may be partially attributed to anthropic bias in LLMs. These models, trained on human-generated data, may inadvertently reflect and amplify certain human biases or cultural assumptions, limiting their ability to accurately represent diverse moral and political perspectives. Based on the obtained results, we answer our research questions as follows:\nRQ1 LLMs showed varying levels of consistency in their performance when surveyed with and without personas. These findings suggest that LLMs' consistency can be significantly affected by incorporating textual personas, and this effect varies considerably across different models. Our research aligns with the findings on jail-breaking LLMs through prompt manipulation . Thus, increasing the exactness of personas' descriptions may yield a more aligned ideology circumventing the built-in guardrails. These guardrails seem like a major limitation when prompting a right-leaning political spectrum and the model's publisher improving their system consistently to prevent this unwanted behavior. From the perspective of the application as a synthetic OSN user, jail-braking with prompting and improving the mitigation techniques lead to a pointless pursuit with diminishing returns for the application. It could be a hard limit for the success of in-context approaches outside the intended usage.\nRQ2 The results indicate that all three dimensions - model, persona, and dimension - notably impact variance, but to varying degrees. The model appears to have the highest impact on variance, followed by the dimensions and the persona. We conclude that the models significantly differ in their capabilities in imitating human survey participants and simulating OSN users cannot be seen as model agnostic. Further, the bias towards left-leaning worldviews baked into the models through training data and instruction alignment skews their moral foundation and limits the expressiveness of generated content. Thus, the left-leaning or liberal persona may produce more authentic content than the right-leaning conservative ones. Concerning our assessment of RQ1, it shows a further restriction of in-context persona alignment. The observed difference in the variance in each moral foundation indicates that these dimensions are aligned unequally during the training. Especially the purity/sanctity dimension shows a significantly higher variance than the other dimensions. We assume that the group's questions are less relevant in today's society and underrepresented in the training data. This assumption aligns with contemporary critics of the MFT on the purity/sanctity dimension.\nRQ3 While Mixtral models showed the best overall alignment, there is no clear, consistent pattern of specific model-persona combinations aligning well with particular human participant groups. This suggests that simple prompt-based persona modifications may not be sufficient to accurately represent diverse human ideologies and moral foundations. Based on our observations, we conclude that we can hardly speak by imitating human ideologies with language models to generate text. The criticized work on human simulacra merely investigates the generated content or opinions on a superficial level but omits a questioning of the validity of LLMs' representation of thought processes and accuracy in reproducing ideologies.\nVariance: The lower the better? The preceding results and discussion focus on the observed variance in the collected data. Our analysis assumes a lower variance as the favorable outcome. We assume that a lower variance indicates a more robust alignment with the given ideology and, thus, a more reliable behavior when answering the questionnaire. However, when assuming LLMs as human simulacra, this reliability may not be favorable, and we may also use the variance as a comparison metric to the uncertainty in human behavior.\n5.1 Proposal: Testable Value Statements\nTo address the limitations identified in our study, we propose a methodological approach for aligning language models with predefined moral and ethical values using testable statements. This approach aims to create more robust and verifiable persona-based models for social science research applications. With personas constructed by these testable statements, we can generate synthetic corpora about polarizing topics to evaluate the sentiment concerning the in-context values. However, the effect of external textual knowledge may have an unforeseen impact. We assume that based on the input length, the statements' number and granularity have to rise to cover varying cases. This relation of external information and in-context alignment may show relevant insides into the generalizability of abstract values.\nDefining Statements along Value Systems We propose the development of a value statement catalog (cf Table 4), collecting statements based on the questionnaire along the five MFT dimensions and determining the aspect and retrieving grounded estimate for the agreement or relevancy indication along a single axis, like the left-right political spectrum or income groups."}, {"title": "5.2 Future Work", "content": "In addition to the proposed value statement alignment process, our study has several limitations that should be addressed in future research. We restrict these proposals to in-context methods to fully capture the capabilities of those models without modifying the parameters. However, we think a fine-tuning approach can significantly enhance the quality of persona-based LLMs. Also, our approach toward testable value statements may serve as an instruction-tuning template.\nLimited model selection The landscape of published LLMs changes frequently. This work represents a model selection composed in March 2024. We propose to extend this approach across upcoming releases to obtain a benchmark that reflects the LLM landscape onto a timeline. As publishers predominantly aim to improve the core benchmarks, an MFQ alignment overview could yield insights into whether out-of-domain approaches benefit from these improvements.\nSimplistic Prompting Our work is restricted to zero-shot prompting and serves as a baseline for the degree of ideology alignment. We propose to investigate more complex prompting approaches, such as chain-of-thought or few-shot prompting to cover the in-context possibilities. Especially the inclusion of a synthetic memory in the form of a biographical history or a history of consumed news may reinforce the ideology alignment.\nCultural Bias & Language In line with the bias we suspect in the LLMs, this work exhibited a cultural and language bias. We primarily focused on Western political ideologies. Thus, we propose that future work should explore how well LLMs can represent moral foundations and political beliefs across different cultures. It may yield relevant insights besides political biases like Americentrism and the under-representation of technology-averse groups.\n5.3 Conclusion\nWhile our results show promise in using LLMs to study moral foundations and political ideologies, they also reveal significant challenges. The proposed method of testable value statements offers a potential path forward for improving the alignment between LLMs and human moral reasoning. However, our results indicate that researchers must remain cautious and critical when applying these models in social science contexts, considering ethical implications and potential limitations. Based on our findings, we argue that utilizing persona-modified LLMs as human simulacra cannot represent abstract political ideologies and thus yield an inadequate representation of human discourses that merely simulate a superficial discourse. Reducing interpersonal communication to worldly disembodied chatbots in a black-box scenario seems like a dangerous method of riding the AI hype train."}, {"title": "A Appendix", "content": "We use the following moral foundation questionnaire . It includes two two \"catch\" questions to remove people not responding seriously (agreement no. 6, relevance no. 6). The questions are divided into two categories differing by task. In addition, we add the results variance aggregated by questionnaire item.\nA.1 Agreement\nLabel the sentence with whether you agree or disagree. Choose from the following labels: [0] strongly disagree, [1] moderately disagree, [2] slightly disagree, [3] slightly agree, [4] moderately agree, [5] strongly agree.\n0. Compassion for those who are suffering is the most crucial virtue.\n1. When the government makes laws the number one principle should be ensuring that everyone is treated fairly.\n2. I am proud of my country's history.\n3. Respect for authority is something all children need to learn.\n4. People should not do things that are disgusting, even if no one is harmed.\n5. It is better to do good than to do bad.\n6. One of the worst things a person could do is hurt a defenseless animal.\n7. Justice is the most important requirement for a society.\n8. People should be loyal to their family members, even when they have done something wrong.\n9. Men and women each have different roles to play in society.\n10. I would call some acts wrong on the grounds that they are unnatural.\n11. It can never be right to kill a human being.\n12. I think it's morally wrong that rich children inherit a lot of money while poor children inherit nothing.\n13. It is more important to be a team player than to express oneself.\n14. If I were a soldier and disagreed with my commanding officer's orders, I would obey anyway because that is my duty.\n15. Chastity is an important and valuable virtue.\nA.2 Relevance\nLabel how relevant the sentence is in determining what is right and wrong. Choose from the following labels: [0] not at all relevant, [1] not very relevant, [2] slightly relevant, [3] somewhat relevant, [4] very relevant, [5] extremely relevant.\n0. Whether or not someone suffered emotionally.\n1. Whether or not some people were treated differently than others.\n2. Whether or not someone's action showed love for his or her country.\n3. Whether or not someone showed a lack of respect for authority.\n4. Whether or not someone violated standards of purity and decency.\n5. Whether or not someone was good at math.\n6. Whether or not someone cared for someone weak or vulnerable."}]}