{"title": "Bridge the Gaps between Machine Unlearning and AI Regulation", "authors": ["Bill Marino", "Meghdad Kurmanji", "Nicholas D. Lane"], "abstract": "The \"right to be forgotten\" and the data privacy laws that encode it have motivated machine unlearning since its earliest days. Now, an inbound wave of artificial intelligence regulations like the European Union's Artificial Intelligence Act (\u0391\u0399\u0391) potentially offer important new use cases for machine unlearning. However, this position paper argues, this opportunity will only be realized if researchers, aided by policymakers, proactively bridge the (sometimes sizable) gaps between machine unlearning's state of the art and its potential applications to AI regulation. To demonstrate this point, we use the AIA as an example. Specifically, we deliver a \"state of the union\" as regards machine unlearning's current potential for aiding compliance with the AIA. This starts with a precise cataloging of the potential applications of machine unlearning to AIA compliance. For each, we flag any legal ambiguities clouding the potential application and, moreover, flag the technical gaps that exist between the potential application and the state of the art of machine unlearning. Finally, we end with a call to action: for both machine learning researchers and policymakers, to, respectively, solve the open technical and legal questions that will unlock machine unlearning's potential to assist compliance with the AIA \u2014 and other AI regulation like it.", "sections": [{"title": "1. Introduction", "content": "Since its inception, Machine Unlearning (MU) has been motivated by the so-called \"right to be forgotten\" (RTBF) (Cao & Yang, 2015), which is now encoded in data privacy laws like the European Union's General Data Protection Regulation (GDPR) (EU, 2016, Art. 17). However, recent works call into question whether this motivation really holds water (Juliussen et al., 2023) or, if it does, whether MU can ever fully deliver for that use case (Cooper et al., 2024).\nIn the meantime, a new and perhaps more compelling motivation for MU has emerged: artificial intelligence (AI) regulation. Worldwide, multiple AI regulation efforts are working their way through the legislative process (Belli et al., 2023; Beardwood, 2024; Zhang, 2024) or have graduated it and gone into effect (EU, 2024; Colorado GA, 2024). Perhaps presaging the multitude of new use cases for MU these laws invoke, scholars have begun scratching the surface of how MU interacts with AI regulation (or, at least, the trustworthy Al principles it often inscribes) (Hine et al., 2024; D\u00edaz-Rodr\u00edguez et al., 2023; Li et al., 2024c).\nIn this position paper, we argue that, while MU indeed has great promise as a tool for complying with AI regulation, this potential will only be realized if researchers and policymakers collaboratively close the gaps between MU's state of the art and these prospective new applications. We use the European Union's Artificial Intelligence Act (AIA) (EU, 2024) as an example to support our argument. This starts with a thorough cataloging of the AIA requirements that MU can assist compliance with. For each potential use case, we flag any legal ambiguities that lawmakers, in order to clarify MU's potential as an AIA compliance tool, should address when amending, updating, or translating the AIA into codes of practice or technical specifications. What is more, we scrutinize, from a technical perspective, whether the state of the art (SOTA) of MU can really support the hypothesized application. In many cases, we identify critical gaps between the two and call for the research community to help resolve them. Finally, we conclude with a pointed call for the AI research and legislative communities to act together to fill these gaps.\nWe note that this conversation is not entirely irrelevant to the AIA, because Al developers who process personal data within the meaning of GDPR must abide by that law (Mazzini & Scalzo, 2023). However, because our goal here is to focus on the \"net new\" applications of MU invoked by AI regulation, we do not revisit the discussion about MU and RTBF or GDPR."}, {"title": "2. Machine Unlearning", "content": "To set the stage for our analysis, we set forth, in this section, we define and provide an overview of MU and its key concepts:"}, {"title": "2.1. Formal Definition of Unlearning", "content": "Let M be a model trained on a dataset D using a training algorithm A. Here, we do not distinguish between M and its parameters, and write $M = A(D)$. An unlearning query is typically identified by a set of data points to be forgotten, referred to as the forget-set $D_f$, and the remaining data points called the retain-set, $D_r = D \\setminus D_f$. The goal of an unlearning algorithm U is to remove from M the influence of $D_f$. The outcome is a new model, called the unlearned model, denoted as $M_u = U(M; D_f, D_r)$. Depending on the unlearning approach, U may not have access to $D_r$ (Zhao et al., 2024). In MU research, the notion of \"removing influence\" has been formalized using definitions from differential privacy. We borrow a version of this definition from (Ginart et al., 2019) as follows:\nDefinition 2.2.1 Assume that U and A are stochastic processes. U is called an $(\u03f5,\u03b4)$-unlearner if the distribution of $A(D_r)$ and $U(M; D_f, D_r)$ is $(\u03f5, \u03b4)$-close. Specifically, two distributions \u03bc and \u03bd are $(\u03f5, \u03b4)$-close if, for all measurable events B, the following inequalities hold: $\u03bc(B) \u2264 e^\u03f5\u03bd(B) + \u03b4$ and $\u03bd(B) \u2264 e^\u03f5\u03bc(B) + \u03b4$.\nThis definition provides a natural taxonomy for MU algorithms. When $\u03f5 = \u03b4 = 0$, U is termed exact unlearning; otherwise, it is referred to as approximate unlearning."}, {"title": "2.2. Informal Definitions", "content": "While Def. 2.2.1 provides a rigorous mathematical formulation for MU, researchers often rely on informal interpretations, typically phrased as removing [...] from M. Deriving informal definitions from Def. 2.2.1 is not always straightforward. A key challenge is that the entity to be removed, may not be clearly identifiable \u2013e.g., in generative models, [...] often corresponds to a fact or concept that lacks an explicit representation in either M or D.\nHowever, we contend that overly broad definitions of MU introduce unnecessary complexity, potentially hindering clear scientific discourse. In this paper we shall limit unlearning to those techniques in ML that modify the parameter-set of the model (e.g. by deleting and retraining, fine-tuning, or adding/removing some parameters). With this condition, MU still can serve as a tool-among others- for applications such as safeguarding and alignment. At the same time, it leaves methods such as guardrailing (or any \"output suppression\" as per (Cooper et al., 2024)) independent of MU, which deserve their own distinct discussion"}, {"title": "2.3. Evaluation metrics", "content": "While Definition 2.2.1 is widely accepted in the MU community, it presents several challenges in practical settings. First, some works question whether this definition is necessary or sufficient to achieve true MU (Thudi et al., 2022). Second, in large-scale applications, it is computationally infeasible to directly measure the closeness between the distributions $A(D_r)$ and $U(M; D_f, D_r)$. As a result, researchers often resort to alternative proxies to verify MU. These proxies include performance metrics (e.g., classification accuracy (Golatkar et al., 2020) or generative performance using metrics like ROUGE for large language models (Maini et al., 2024)) and privacy attacks, such as membership inference attacks or backdoor attacks (Hayes et al., 2024; Triantafillou et al., 2024)."}, {"title": "2.4. Unlearning Axes", "content": "Creating a taxonomy for MU depends on the goal and perspective. Instead of suggesting a new taxonomy, here, we outline two key dimensions that help identifying a MU problem: Model Type, and Forget-Set Definition.\nModel Type. MU is studied in both discriminative and generative models. MU algorithms depend on the training objective and the architectures of these models. For example, Negative Gradient, a widely used MU strategy, has different formulations in image classifiers (Golatkar et al., 2020) versus language models (Yao et al., 2024a). Within these two broad categories, one can imaging further subcategories based on data type, evaluation metrics, etc.\nForget-Set Definition. The forget-set, or the target of MU, can take various forms. It usually is in the following forms: a) the entire data class(es), b) individual data points (Kurmanji et al., 2023b), c) shared features across data points (Warnecke et al., 2021), and d) knowledge (also referred to as concepts or behaviors) that transcends direct connections to the training data (Gandikota et al., 2024)."}, {"title": "2.5. Trade-offs and risks", "content": "MU algorithms strive to make a balance between three key objectives: Model Utility, Forgetting Quality, and Efficiency. In certain privacy-centric applications, forgetting can be synonymous with achieving privacy (Liu et al., 2024b). Hyperparameters and regularizers impact these trade-offs. For example, in MU via Fine-tune, the number of steps and learning rate dictate the balance between forgetting quality and efficiency. Similarly, in Gradient Ascent, the number of steps determines the trade-off between effective MU and preserving model's utility.\nAdditionally, forgetting may sometimes conflict with privacy due to two phenomena. First, unlearning specific data points can inadvertently expose information about others in the retained set due to the \"onion effect\" of privacy (Carlini et al., 2022). Second, over-forgetting (Kurmanji et al., 2023b) a data point may reveal its membership in the original training set-a phenomenon known as the \"Streisand Effect\" (Golatkar et al., 2020). Addressing these challenges requires careful calibration of MU methods to ensure a delicate equilibrium across these competing objectives.\nBeyond these trade-offs, MU introduces risks associated with untrusted parties (Li et al., 2024b) and malicious unlearning (Qian et al., 2023). Malicious entities could exploit MU to make fake deletion queries, or introduce computation overheads to the system (Marchant et al., 2022)."}, {"title": "3. The EU's Artificial Intelligence Act", "content": "The AIA sets forth harmonized requirements for AI systems and models placed on the market or put into service in the EU (EU, 2024, Art. 1). These requirements largely target two distinct categories of AI: AI systems and general-purpose AI (\"GPAI\u201d) models. Here, we define these two categories and, for each, briefly preview the requirements that, we later argue, MU may assist compliance with."}, {"title": "3.1. AI Systems", "content": "The AIA broadly defines Al systems to include any \"machine-based system that is designed to operate with varying levels of autonomy and that may exhibit adaptiveness after deployment, and that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, recommendations, or decisions that can influence physical or virtual environments\" (EU, 2024, Art. 3.1). An example of a system that might meet this criteria is ChatGPT (Fern\u00e1ndez-Llorca et al., 2024).\nIn laying out its rules for these AI systems, the AIA relies on a \"risk-based\" approach (Mahler, 2022), by which an AI system's perceived degree of risk determines the exact rules that apply to it. Here, the strictest requirements - and the ones most relevant to our discussion are reserved for those AI systems deemed to be high-risk (EU, 2024, Art. 6). Such high-risk AI (\u201cHRAI\") systems are subject to a bevy of requirements (EU, 2024, Chap. III). Among them, the following are the most relevant to our position:\nRisk management: HRAI systems must implement risk management systems that include the identification of known and reasonably foreseeable risks that the system can pose to health and safety or to fundamental rights (EU, 2024; Kaminski, 2023, Art. 9.2(a)). Here, risks to health and safety, may include risks to mental and social well-being as well as physical safety. (Armstrong et al., 2024; European Commission, 2021). Meanwhile, risks to fundamental rights includes risks to any of the rights listed on the EU's Charter of Fundamental Rights (EU, 2000); though, here we only focus on risks to the right most relevant to the topic of MU: the right to non-discrimination, including from biased results (Arnold, 2024). Importantly, wherever\""}, {"title": "3.2. GPAI models", "content": "In contrast to an AI system, a GPAI model is defined as any AI model that is \"trained with a large amount of data using self-supervision at scale, that displays significant generality and is capable of competently performing a wide range of distinct tasks regardless of the way the model is placed on the market and that can be integrated into a variety of downstream systems or applications, except AI models that are used for research, development or prototyping activities before they are placed on the market\u201d (EU, 2024, Art. 3.63). Some see this as being synonymous with \"foundation model\" (Ada Lovelace Institute, 2024). An example of a GPAI model that might meet this criteria is GPT 3.5, the model powering ChatGPT (Fern\u00e1ndez-Llorca et al., 2024).\nIn laying out its requirements for GPAI models, the AIA again uses a risk-based approach, with the strictest requirements reserved for those GPAI models deemed to carry systemic risk (EU, 2024, Art. 55). This is defined as the risk of \"having a significant impact on the [EU] market due to [its] reach, or due to actual or reasonably foreseeable negative effects on public health, safety, public security, fundamental rights, or the society as a whole, that can be propagated at scale across the value chain\" (EU, 2024, Art. 2.65; Annex III). This status can be established through a number of proxies, including performance on benchmarks and the amount of compute used during training (EU, 2024, Art. 51). While the AIA itself does not further elaborate on what constitutes systemic risk, the First Draft of the General-Purpose AI Code of Practice, a companion piece to the AIA that breaks it down into more granular technical standards, proposes that it covers risks related to: (1) cyber offense; (2) chemical, biological, radiological, and nuclear (CBRN); (3) loss of control; (4) automated use of models for AI research and development; (5) persuasion and manipulation; and (6) large-scale discrimination (EU AI Office, 2024).\nAmong the AIA's requirements for GPAI models that do display systemic risk \u2014 and those that don't - the following are the most relevant to our analysis:\nCopyright: All GPAI model providers must \"put in place a policy to comply with Union law on copyright and related rights\" (EU, 2024, Art. 53.c). Among other things, this policy must respect rightsholders' requests, per (EU, 2019, Art. 4(3)), to opt out of text and data mining (TDM) on their copyrighted works (EU, 2024, Rec. 105, Art. 53.c).\nSystemic risk: Those GPAI models that display systemic risk are required to \"mitigate\" it (EU, 2024, Art. 55(a-b))\nCybersecurity: GPAI models with systemic risk are additionally required to \"ensure an adequate level of cybersecurity\" (EU, 2024, Art. 55(d))."}, {"title": "4. Related Work", "content": "A number of recent works allude to the use of MU as a tool for compliance with the AI regulation - or, more broadly, a tool for advancing the trustworthy AI principles that AIR tends to encode (Hine et al., 2024; D\u00edaz-Rodr\u00edguez et al., 2023; Li et al., 2024c). Importantly, however, another relevant set of works explore the shortcomings, trade-offs, and risks of MU as well as the viable substitutes for MU in various scenarios. Some recent works, for example, broadly question whether MU can really achieve its goals, especially in the generative domain (Cooper et al., 2024; Barez et al., 2025; Zhou et al., 2024; Shumailov et al., 2024). Other works scrutinize MU's trade-offs around performance, privacy, security, and cost (Xu et al., 2024b; Carlini et al., 2022; Zhang et al., 2023b;"}, {"title": "5. MU for AIA compliance: a catalog", "content": "This Section comprehensively catalogs the potential applications of MU to assist AIA compliance. For each, we analyze the SOTA and its ability to support the potential application, then identify any open questions the research community must resolve in order to bridge the gap between the two. In sum, we find that the potential applications of MU to assist AIA compliance ultimately roll up into just six separate applications (Fig. 2):\n\u2022 Improve accuracy per EU (2024, Arts. 9, 15);\n\u2022 Mitigate bias per EU (2024, Arts. 9, 55);\n\u2022 Mitigate confidentiality attacks per EU (2024, Arts. 9, 15, 55));\n\u2022 Mitigate data poisoning per EU (2024, Arts. 15));\n\u2022 Mitigate other risks of generative outputs per EU (2024, Arts. 9, 55));\n\u2022 Aid compliance with copyright laws, per EU (2024, Art. 53))"}, {"title": "5.1. Accuracy", "content": "At least two AIA provisions may compel HRAI systems providers towards higher levels of accuracy. First (and most direct), HRAI systems must achieve a level of accuracy appropriate to their intended use and the SOTA (EU, 2024, Art. 15.1; Rec. 74). Second, HRAI systems' risk management must include measures to mitigate or eliminate risks to health and safety (EU, 2024, Art. 9), which could stem from low accuracy in domains like medicine (Jongsma et al., 2024; James et al., 2021; Heaven, 2021). For these requirements, MU can boost HRAI system's accuracy by removing the problematic data from the model.\nImportantly, the accuracy use case should not require privacy guarantees on the unlearned data (Goel et al., 2023). Rather, because the goal is strictly to boost accuracy to the level deemed appropriate (EU, 2024, Art. 15) or until the overall residual risk to health and safety posed by the inaccuracy is judged to be \"acceptable\" (EU, 2024, Art. 9), model accuracy should be the primary benchmark for MU's efficacy. In measuring that, AI providers will presumably account for any inadvertent, counteracting degradation in accuracy caused by the MU itself (Li et al.,"}, {"title": "5.2. Bias", "content": "Providers of both HRAI systems and GPAI models with systemic risk may be obliged to mitigate model bias. The former must take measures to mitigate or eliminate risks to fundamental rights, which includes the right to non-discrimination (EU, 2024, Arts. 9)). The latter must take measures to mitigate their models' systemic risk (EU, 2024, Art. 55), which includes the risk of large-scale discrimination (EU AI Office, 2024). Bias can occur because unrepresentative or incomplete data leads to model outputs that do not perform fairly on different groups or, in the case of generative models, produce stereotyped or otherwise discriminatory outputs (Ferrara, 2024). In all these cases, MU can ostensibly help forget the data points or patterns in the training set causing the bias"}, {"title": "5.3. Confidentiality attacks", "content": "The AIA requires providers of both HRAI systems and GPAI models with systemic risk to resolve and control for confidentiality attacks. Providers of HRAI systems must ensure their systems achieve an \u201cappropriate level\" of cybersecurity given the intended use and the SOTA, including by taking technical measures to prevent, detect, respond to, resolve and control for confidentiality attacks (EU, 2024,"}, {"title": "5.4. Data poisoning", "content": "In data poisoning, specially-crafted data points are injected into a training set to alter (e.g., degrade or bias) the behavior of the model to the benefit the attacker (Biggio et al., 2012). Backdoor attacks are a type of data poisoning where the injected data points create \"triggers\" the attacker can exploit during inference (Lin et al., 2021). The AIA obligates the providers of both HRAI systems and GPAI model with systemic risk to address such attacks. HRAI system providers must ensure their systems achieve an \u201cappropriate level\" of cybersecurity, including via technical measures to \u201cprevent, detect, respond to, resolve and control for attacks trying to manipulate the training data set (data poisoning)\u201d (EU, 2024, Art. 15(5)). Providers of GPAI models with systemic risk, meanwhile, must \u201censure an adequate level of cybersecurity\" in their models (EU, 2024, Art. 55.d), which presumably also includes defenses"}, {"title": "5.5. Other risks of generative outputs", "content": "Generative model outputs may pose risks to health, safety, and human rights or pose systemic risk that the providers of HRAI systems and GPA models, respectively, must mitigate. For example, HRAI systems' risk management systems must include measures to mitigate or eliminate risks that the system poses to health, safety, and fundamental rights (EU, 2024, Art. 9). Generative model outputs may pose risks to health and safety, for example by issuing bad medical advice (Wu et al., 2024a; Han et al.,"}, {"title": "5.6. Copyright", "content": "All GPAI model providers must have a policy for complying with EU law on \"copyright and related rights\" (EU, 2024, Art. 53.c). Among other things, this policy must honor the TDM opt-outs of rightsholders (EU, 2024, Art. 53.c; Rec. 105), which is often a feature of AI training (Rosati, 2024; Hamburg Regional Court, 2024). When it comes to AI and copyright law, a distinction is sometimes made between the \"input\" (training) phase and the \"output\" (inference) phase of the AI life cycle (Rosati, 2024; Quintais, 2024). At this point in time, the primary compliance risk during the input phase seems to be that an AI training set could include data points that violate TDM opt-outs. When this happens, we assume that using MU to remove the opt-out data points from the trained model does not cure the violation, since the violation occurred"}, {"title": "6. Conclusion", "content": "Despite its many challenges, MU has great potential as a tool for AIA compliance. Because contemporary AI regulations tend to feature recurring principles (Feldstein, 2024; D\u00edaz-Rodr\u00edguez et al., 2023), this likely means that MU has great potential as a tool for compliance with other AI regulations too. To realize this potential, AI researchers should help solve the (sometimes critical) open technical problems logged by this position paper. Among other things, this includes work on identifying forget set data points, on resolving the privacy and performance trade-offs of MU, and on resolving the particular challenges to these use cases that generative model outputs present. While researchers investigate these open problems, lawmakers can support them by explicitly endorsing MU as a compliance tool for AI regulation requirements and, more acutely, resolving some of the particular legal ambiguities we have flagged in this position paper (2,7). Working collaboratively, we can all help unlock MU's potential to assist compliance with the Al regulation and, by extension, help safeguard the important social values these regulations encode."}]}