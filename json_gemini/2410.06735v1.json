{"title": "Which Programming Language and What Features at Pre-training Stage Affect Downstream Logical Inference Performance?", "authors": ["Fumiya Uchiyama", "Takeshi Kojima", "Andrew Gambardella", "Qi Cao", "Yusuke Iwasawa", "Yutaka Matsuo"], "abstract": "Recent large language models (LLMs) have demonstrated remarkable generalization abilities in mathematics and logical reasoning tasks. Prior research indicates that LLMs pre-trained with programming language data exhibit high mathematical and reasoning abilities; however, this causal relationship has not been rigorously tested. Our research aims to verify which programming languages and features during pre-training affect logical inference performance. Specifically, we pre-trained decoder-based language models from scratch using datasets from ten programming languages (e.g., Python, C, Java) and three natural language datasets (Wikipedia, Fineweb, C4) under identical conditions. Thereafter, we evaluated the trained models in a few-shot in-context learning setting on logical reasoning tasks: FLD and babi, which do not require commonsense or world knowledge. The results demonstrate that nearly all models trained with programming languages consistently outperform those trained with natural languages, indicating that programming languages contain factors that elicit logic inference performance. In addition, we found that models trained with programming languages exhibit a better ability to follow instructions compared to those trained with natural languages. Further analysis reveals that the depth of Abstract Syntax Trees representing parsed results of programs also affects logical reasoning performance. These findings will offer insights into the essential elements of pre-training for acquiring the foundational abilities of LLMs.", "sections": [{"title": "1 Introduction", "content": "Recently, large language models (LLMs) have demonstrated remarkable generalization abilities in downstream tasks. These tasks include not only fundamental natural language processing tasks, such as machine translation and text classification (Brown et al., 2020), as well as advanced tasks, such as mathematics and logical reasoning (Achiam et al., 2023).\n\nThe generalization ability of LLMs originates from pre-training on large text corpora, such as RedPajama (Computer, 2023) and Fineweb (Penedo et al., 2024). The corpora often contain content from various domains, such as Common Crawl, GitHub, ArXiv, Wikipedia, and StackExchange. However, the relationship between each domain of training data and the abilities of LLMs is not fully understood.\n\nPrior research has shown that LLMs pre-trained with programming language data acquire high mathematical and reasoning abilities (Roziere et al., 2023; Madaan et al., 2022; Liang et al., 2023; Li et al., 2023); however, this causal relationship has not been rigorously tested. Specifically, fair comparisons are often not conducted between models trained on programming language data and those trained on natural language data due to differences in the number of training tokens and model sizes, or because the information is unknown as for closed models. In addition, some prior works have fine-tuned models using a mixture of programming languages, but they have not conducted detailed analyses regarding the effect of each programming language on the performance of downstream tasks (Li et al., 2023; Roziere et al., 2023).\n\nWe conducted experiments to analyze whether models trained solely on a single programming language generalize better to pure logical reasoning tasks compared to models trained on natural language. Specifically, we trained GPT2-124M, GPT2-774M, GPT2-1.5B, and LLaMA-774M (Radford et al., 2019; Zhang et al., 2024) from scratch using datasets from ten programming languages (e.g., Python, C, Java) and three natural language datasets (Wikipedia, Fineweb, C4) under the same conditions. We then evaluated each"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 LLMs and Programming Language", "content": "Two main approaches exist for solving code tasks using language models. One approach involves fine-tuning a model pre-trained on natural language datasets with code datasets, which is widely applied to some open models (Roziere et al., 2023). For closed-source models, code-davinci-002 outperforms text-davinci-002 on serialized commonsense reasoning and mathematical tasks (Madaan et al., 2022; Liang et al., 2023).\n\nThe other approach involves training models from scratch solely on code datasets, often using a mixture of multiple programming languages. This method is commonly used in code completion and code generation fields. For example, Santa-"}, {"title": "2.2 LLMs and Logical Inference", "content": "Weston et al. (2015) shows that language models can solve bAbi tasks, which consist of simple logical reasoning challenges. Morishita et al. (2023) demonstrates that a fine-tuned T5 model can effectively address Formal Logic Deduction (FLD) tasks, involving multi-step logical reasoning. Although these studies show that LLMs have some logical reasoning abilities, it remains unclear which features of the corpus contribute to the emergence of advanced complex reasoning.\n\nOur study sheds light on the effects of programming languages on training LLMs. Our findings show that the LLMs pre-trained with a single programming language outperform those trained with natural language on logical reasoning tasks. These results suggest a new criterion for corpus quality in the efficient training of LLMs."}, {"title": "3 Experimental Setup", "content": ""}, {"title": "3.1 Models and Datasets", "content": "The default model for our experiments is GPT2-small (124M). To accomodate long context few-shot in-context evaluation, we extended the model's context length from 1,024 to 2,048 tokens. We employed the official GPT2 tokenizer distributed by Hugging Face and three natural language datasets: Wikipedia (Foundation, 2022), FineWeb (Penedo et al., 2024), C4 (Raffel et al., 2020), and ten common programming languages: Haskell, OCaml, Erlang, Python, C, C++, HTML, JavaScript, TypeScript, Java from the Stack (Kocetkov et al., 2023)."}, {"title": "3.2 Evaluation Metrics", "content": "We evaluated pre-trained models on the FLD (Morishita et al., 2023) and babi (Weston et al., 2015)"}, {"title": "3.3 Training Settings", "content": "To train the language model, approximately 200M tokens were sampled from each dataset and packed each sample into fixed-length datasets, using <lendo ftext|> tokens as delimiters. We pre-trained the models for three epochs with a batch size of 24, employing a CosineLRScheduler that warms up the learning rate linearly to le-4 during the first 10% of the total iterations. The optimizer used was AdamW with $\\beta\u2081 = 0.9$, $\\beta2 = 0.999$, \u0454 = 1e - 8, weight decay of 0.01, and gradient clipping set to 1.0. We trained the models three epochs. Other configurations are available in Appendix A."}, {"title": "4 Results", "content": ""}, {"title": "4.1 Logical Inference Ability by Different Programming Languages", "content": "Table 1 shows the accuracy of the pre-trained models with each programming language and natural language measured by FLD and bAbi datasets. Considering FLD and FLD*, although the best results of all models remained almost at a chance rate, the results show that the models trained in programming languages outperform the models trained in natural languages. For bAbi, code datasets influenced better performance than natural language datasets. Among the programming languages, Python and C showed slightly better performance across all tasks. However, regardless of the paradigm or typing explicitness of each language, most of the code-trained models showed better performance than natural language based models. The result indicates that logical inference ability and formatted outputs do not originate from a specific language but from the nature of programming itself.\n\nFigure 1 shows sample outputs from the models trained on Python and Fine Web. The model trained on Python outputs in the correct format follow-"}, {"title": "4.2 Complexity of Syntax in Code Data", "content": "Programming languages have more complex syntax trees than natural language syntax trees and might be beneficial for reasoning in complex tasks. The deeper the depth of Abstract Syntax Tree (AST) - that is, the number of hierarchies consisting of elements, such as loops and conditional statements (e.g., \"if\") - the more complex the program is. We chose Python as the target language and separated the datasets into three subsets by AST depth: Shallow (up to 7), Middle (from 8 to 11), and Deep (from 12 to 20). Each dataset is made from samples that Python ast module succeeded in parsing. Codes that did not succeed in parsing were excluded. We trained the model on each dataset and evaluated the logical inference ability using FLD and bAbi datasets.\n\nTable 2 shows the accuracy of the model trained on FLD and babi datasets. For bAbi, Python datasets with middle complexity show the best accuracy. For FLD, datasets of shallow complexity show the best performance, and the accuracy decreases as the depth of AST increases. Further investigation reveals that the model trained on the Deep dataset frequently outputs long blanks, i.e., the model outputs do not follow the instructions. It is possible that long and complex code sentences in the training data are often indented by blanks or tabs as necessary to ensure human readability. This redundancy in the code training data may result in the trained model outputting long blanks. In addition, we assume that there might be suitable syntax complexity to learn linguistic phenomena during pre-training. Kallini et al. (2024) insists that grammar complexity of training data determines the generalization difficulty of language models for the grammar."}, {"title": "4.3 Ablation Study by Code Modification", "content": "To further inspect what features in code raise the performance of the models on logical inference tasks, we developed three modified Python datasets: Comment-Free, Comment-Free + Scrambled, and Comment-Free + Randomized. \"Comment-Free\" is an operation that eliminates comments starting from # and constant strings that are not used for any operation like docstring. We expected this modification to disable few-shot ICL with natural language instruction on FLD and bAbi. \"Scrambled\" shuffles identifiers (e.g. names of variables) on each position, and destroys the meaning of the code. \"Randomized\" replaces each identifier with a random string to cut off the knowledge of natural language. Note that syntactic correctness is maintained during all modifications. See appendix D for the details.\n\nWe trained models with each data on the same settings in section 4.1 and gained Table 3 results. The result shows that comment elimination maintains FLD accuracy to some extent, and cutting off learning natural languages (Comment-Free + Randomized) induces few-shot ICL failure. Destroying dependencies (Comment-Free + Scrambled) also breaks logical reasoning ability on every task. This result suggests that a language model is not a simple machine to imitate grammar, but also learns semantics from dependencies of code that can be applied to unseen logical inference tasks."}, {"title": "4.4 Effect of Programming Language on General NLP Tasks", "content": "We also evaluated the effect of programming languages on other tasks to explore their potential as pre-training datasets. We evaluated the pre-trained models described in Section 4.1 on the GLUE benchmark (Wang et al., 2018), which focuses on natural language understanding.\n\nFigure 2 shows the GLUE scores of the pre-trained models for each dataset. Entailment recognition tasks, such as MNLI, QNLI, and RTE, show that the models trained on both types of languages"}, {"title": "4.5 Evaluation on Larger Models", "content": "We trained GPT2 and LLaMA, each with 774M parameters, on both Python and Fineweb. The configurations are GPT2-Large (774M) and Tiny-Llama v1.1 (1.1B) (Zhang et al., 2024), with the MLP representation resized to 3000. For LLaMA experiments, we used the official Tiny-Llama v1.1 tokenizer distributed by Hugging Face. For GPT2, we trained larger models based on GPT2-XL (1.5B) configuration. Specifically, 600M tokens were consumed for training GPT2-1.5B, while the other two models (GPT2-774M and LLaMA-774M) were trained on the same 200M tokens as in Section 4.1. We evaluated the models on the same tasks as described in Section 4.1. Table 4 shows the accuracy of each programming language and natural language on FLD and bAbi. The results show that the models trained on Python outperform those trained on natural languages on FLD on both architectures. For bAbi, both models trained on Python and"}, {"title": "5 Conclusion", "content": "Our study rigorously verified that nearly all models trained on individual programming languages consistently achieve the better logical inference performance than those trained solely on natural language datasets in few-shot ICL settings. Further analysis reveals that an appropriate level of syntax complexity influences logical reasoning performance. Additionally, models trained on programming languages exhibit a greater ability to follow instructions compared to those trained on natural language datasets. Moreover, dependencies expressed in code significantly contribute to logical reasoning in few-shot ICL settings. We hope these findings will offer insights into the essential elements of pre-training for acquiring the foundational abilities of LLMs."}, {"title": "6 Limitation", "content": "Owing to the limitation of the computational resources, we could not train the models larger than 1.5B parameters. Especially for FLD tasks, logical inference ability is limited even in models with 10 billion parameters (Morishita et al., 2024). Future work includes investigations into the effect of code-based pre-training with larger models to verify that logical reasoning abilities are more explicitly improved. Each dataset is primarily organized in either natural language or a single programming language, although we did not conduct thorough filtering to ensure complete exclusivity.\n\nIn Section 4.2, we fixed grammar complexity by selecting a single language and examined the syntax complexity in code data. However, our experiments did not consider semantic complexity or other complexities that might be measureable in both programming and natural languages. Furthermore, it remains unclear whether syntax complexity in pre-training data alone influences logical inference performance. Comparing various complexities between natural and programming language regarding logical reasoning abilities is an important avenue for future research.\n\nIn section 4.4, we assessed the general language understanding of the trained models. The natural language inference tasks in GLUE require commonsense knowledge, which may be difficult to acquire through code-only training. Future experiments could explore whether fine-tuning models pre-trained on code with GLUE datasets enhances natural language reasoning capabilities. Additionally, integrating both code and natural language datasets during the pre-training process may provide a synergistic approach to leverage the strengths of both types of data.\n\nMoreover, a further experiment in Appendix E demonstrates the advantage on FLD tasks between natural language and programming language is reversed when fine-tuning on FLD corpus. We empathize that the advantage of logical reasoning tasks is observed in in-context learning settings and should investigate the difference between the two learning settings for logical reasoning tasks."}]}