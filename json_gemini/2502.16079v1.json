{"title": "Together We Rise: Optimizing Real-Time Multi-Robot Task Allocation using Coordinated Heterogeneous Plays", "authors": ["Aritra Pal", "Anandsingh Chauhan", "Mayank Baranwal"], "abstract": "Efficient task allocation among multiple robots is crucial for optimizing productivity in modern warehouses, particularly in response to the increasing demands of online order fulfillment. This paper addresses the real-time multi-robot task allocation (MRTA) problem in dynamic warehouse environments, where tasks emerge with specified start and end locations. The objective is to minimize both the total travel distance of robots and delays in task completion, while also considering practical constraints such as battery management and collision avoidance. We introduce MRTAgent, a dual-agent Reinforcement Learning (RL) framework inspired by self-play, designed to optimize task assignments and robot selection to ensure timely task execution. For safe navigation, a modified linear quadratic controller (LQR) approach is employed. To the best of our knowledge, MRTAgent is the first framework to address all critical aspects of practical MRTA problems while supporting continuous robot movements.", "sections": [{"title": "1 INTRODUCTION", "content": "Cooperative multi-robot systems are increasingly being utilized across various domains, including transportation and logistics [7], search and rescue operations [25], environmental monitoring [19], precision agriculture [6], construction [27], and warehouse automation [10, 13]. These systems, characterized by the collaboration of multiple robots to achieve shared objectives, offer substantial benefits such as enhanced scalability & efficiency, and greater fault tolerance, making them indispensable in dynamic environments.\nIntricacies of Warehouse Management: Automating warehouse operations with multi-robot systems presents a unique set of challenges, stemming from the complexities of spatial layouts, diverse task demands, varying robot capabilities, and the critical need for safe robotic navigation [4, 31]. These challenges can be broadly divided into three key objectives: (a) Task Allocation, (b) Real-Time Robot Assignment, and (c) Path Planning. While these objectives are interrelated, each introduces distinct sub-problems that must be resolved to achieve optimal warehouse performance. In a dynamic warehouse setting, real-time task generation is vital because tasks cannot be fully anticipated in advance. This unpredictability complicates the planning process, necessitating the prioritization of tasks based on their arrival times, required completion deadlines, and the need to minimize the total travel distances of robots while balancing the demands of ongoing tasks.\nMoreover, the immediate and continuous allocation of robots to tasks demands seamless coordination, regardless of whether the robots are currently available or occupied. This emphasizes the need for real-time synchronization across multiple objectives, all while adhering to various physical and operational constraints. For example, effective path planning requires the creation of collision-free routes that navigate around static obstacles and account for the movements of other robots. Furthermore, it is crucial to consider the physical dynamics of the robots, such as acceleration, deceleration, and maneuverability-along with other practical considerations, such as their state-of-charge (SOC). These factors, often overlooked in existing warehouse management literature, are essential to the planning process.\nThe intricate interdependence of these challenges highlights the need for a sophisticated and adaptable multi-robot framework. Such a framework must systematically address the complexities of task allocation, real-time robot assignment, and path planning within the constantly changing environment of an automated warehouse. Neglecting the interconnected nature of these tasks often leads to sub-optimal performance and inefficiencies, undermining the overall effectiveness of warehouse operations.\nStatement of Contributions: In this study, we present a self-play inspired novel framework, MRTAgent, which employs a bi-level RL strategy inspired to tackle the challenges of multi-task selection and multi-robot allocation. MRTAgent is designed to handle real-time task selection, dynamically allocate tasks to robots, and ensure safe navigation, all while accounting for critical constraints such as robot dynamics, charging needs, and specific task requirements. MRTAgent consists of three key components: (a) Task selection agent (Planner) to prioritize a task queued in the task buffer, (b) Robot selection agent (Executor) to allocate a robot to the recommended task, and (c) Navigator to plan collision-free trajectories of robots while adhering to physical and SOC constraints. The framework"}, {"title": "2 RELATED WORKS", "content": "Efficient MRTA and path planning are crucial for optimizing order fulfillment, resource management, and obstacle-free navigation in industrial environments such as automated warehouses and manufacturing plants. These processes ultimately enhance overall productivity, which has made MRTA a focal point of research over the past two decades. Research efforts in this area range from heuristic-driven approaches to contemporary learning-based methods [14]. Early work by [9] provided a comparative analysis of state-of-the-art (SOTA) multi-robot coordination strategies within specific domain contexts. Current MRTA research primarily focuses on two key elements: (a) model-driven optimization, as demonstrated by [30], and (b) communication-efficient decentralized algorithms, as seen in [2, 5].\nThe problem can also be framed as a multi-agent pickup and delivery (MAPD) challenge, which has been studied through both distributed and centralized approaches [17, 18, 26, 32]. However, most existing research in this area has concentrated on offline MAPD, whereas our approach emphasizes learning-based methods for online task allocation. This focus is driven by the need for reliable solutions in dynamic environments, where continually solving optimization problems can be computationally intensive.\nRecent advancements in reinforcement learning (RL) for solving complex dynamic challenges have led to a trend toward learning-based approaches for managing warehouse complexities [1, 2, 33]. These learning strategies address various aspects of end-to-end warehouse management. For instance, [33] proposed a Q-learning framework to generate collision-free, secure paths for multi-robot systems. Conversely, the RL frameworks in [1, 2] focus on optimal task selection but neglect task-to-robot assignment, assuming constant robot availability post-selection. Additionally, these works leverage A* coupled with optimal reciprocal collision avoidance [3] for collision-free navigation at the low-level path planning stage. However, as previously discussed, most SOTA learning-based warehouse management approaches, including those mentioned, overlook a key benefit of RL: the ability to integrate multiple levels of warehouse management, and consideration of robots' constraints during the training phase of the RL agent. For instance, the learned policy in [1, 2] is limited in its applicability to realistic warehouse scenarios due to the neglect of constraints related to robot availability and SOC. Moreover, these approaches often aim for a seamless sequential flow of tasks without considering their generation times.\nSimilarly, [22] utilizes a cooperative multi-agent RL framework under the assumption that robots never collide. Much of the prior work also neglects the complexities of robot dynamics in MRTA for warehouse settings, often simplifying robots to point objects or solving problems in basic square grid environments, without accounting for robot acceleration, deceleration, and collision risks during path planning [21].\nIn our study, we address these gaps by incorporating task arrival times to ensure timely task execution while considering practical factors such as robot availability, SOC, robot dynamics, and collision-free path generation. Our framework also demonstrates robust performance under distribution shifts and with variable-sized fleets. Our MRTAgent addresses these limitations by integrating robot dynamics into the navigation planning process using a linear quadratic regulator (LQR)-based navigation path algorithm within the RL agent framework. This ensures that path planning is both effective and collision-free. Additionally, we design a reward structure that balances prompt task allocation with the shortest possible execution duration for allocated tasks. This approach ensures competitive runtime during the deployment phase, facilitating real-time task selection and robot allocation, and collision-free navigation considering physical dynamics through proposed MRTAgent."}, {"title": "3 PRELIMINARIES", "content": "The problem of MRTA can be modeled as a Markov Decision Process (MDP) [24]. An MDP is denoted by the tuple (S, A, PA, r), where S and A represent the finite sets of states and actions, respectively. For any s, s' \u2208 S, the transition probability from state s to state s' under the action a \u2208 A is denoted by pa(s, s') \u2208 PA. Finally, the step reward associated with each state-action pair (s, a) is depicted by r(s, a). Below, we summarize the set of all possible states, actions, and rewards in the context of the MRTAgent validated within warehouse environment settings.\nStates: At each time step, the warehouse environment is characterized by a comprehensive state that includes detailed information about both tasks and robots. Incoming tasks are immediately stored in a buffer, forming a limited-size look-ahead (LA) queue in a First-In-First-Out(FIFO) manner. The Tasks RL agent, referred to as the Planner, is trained to optimally select tasks from this queue based on the current state of the environment. Simultaneously, the Robot RL agent, referred to as the Executor, responsible for executing tasks, selects most suitable robots to complete them.\nThe states of the Planner and Executor consist of the features related to tasks in the LA (denoted by P) as well as the set of robots (R). Each agent's state, denoted as st \u2208 S at time-step t, encompasses the following components: (a) origin coordinates of tasks {oi}, (b) destination coordinates of tasks {di}, (c) euclidean distance information between task origin and destination {ki}, (d) timestamp of task appearance in the LA queue {li}, (e) robot coordinates {pj}, (f) robot availability and the anticipated time for ongoing task completion {rj}, (g) robot charge percentage {cj}. Features (a)-(d) are task-specific and collectively have a dimensionality of 6 for each task. Conversely, features (e)-(g) are linked to robot-specific attributes (dim. = 4 for each robot).\nActions: The MRTAgent consists self-play inspired bi-level RL agent; planner and executor. The goal of the planner is to enhance task assignment, thereby minimizing total operational time. The planner's actions involve systematically selecting tasks from the queue while the executor focuses on robot allocation, with the objective of optimally assigning robots to the selected tasks to reduce overall operational expenses, and task execution delays. Thus, the actions executed in the environment consist of the selected task and its corresponding robot pair.\nRewards: The step reward attributed to a task-action pair encompasses two distinct components. The initial component is calculated based on the time it takes for the robot to travel from its current position to the task's starting point, termed travel time to origin (TRTO). The second component is the time gap between task arrival in the LA and the robot's initiation of execution, denoted as the total time gap for the task (TTGT). Let (xo\u2081, Yo\u2081) and (xd\u2081, Yd\u2081) represent the origin and destination coordinates of the ith-indexed task. Similarly, (xrj, Yr;) indicates the current position of robot j, which can vary based on whether the robot is idle, performing tasks, or at a charging location for recharging if needed. Additionally, we introduce tstamp, and texec; to denote the time when task i appears in the task allocation (referred to as LA) and the time at which its execution begins, respectively. In each decision-making step, we use the variable allotT to signify the index of the selected task which then gets assigned to a robot selR. Furthermore, we utilize the function d[(xa, ya), (\u0445\u044c, \u0443\u044c)] to calculate the distance between two points (xa, Ya) and (x, yb) within our system. The step reward for training the PPO agent is as follows:\n$$R_{step} = -d[(x_{r_{selR}}, y_{r_{selR}}), (x_{o_{allotT}}, y_{o_{allotT}})] - \\alpha * (t_{exec_{allotT}} - t_{stamp_{allotT}})$$\n(1)\nThe coefficient \u03b1 represents positive constant. The first term in (1) corresponds to TRTO, while the second term is associated with TTGT. This reflects the principle that tasks should not remain unattended for too long."}, {"title": "4 OUR APPROACH", "content": "In this section, we introduce MRTAgent, a novel self-play-inspired bi-level RL framework designed to optimize MRTA for various industrial tasks. The MRTAgent is validated in warehouse environments scenario and comprises two RL agents: (a) Planner, and (b) Executor. Tasks are generated in real-time and initially populate a main task buffer. The planner has access to a small LA queue of tasks. When a task from LA queue is assigned to a robot for execution, a new task from the buffer is moved into the LA queue, making it available for selection by the Planner. If no new tasks are available in the buffer to replenish the LA queue, task with the longest duration in the LA queue is duplicated to maintain a consistent queue length, thereby increasing its likelihood of being selected by the Planner in subsequent steps. In the exceptional scenario where both the LA queue and the task buffer are empty, MRTAgent waits for a task to appear in the environment. The action selection process operates across three hierarchical levels:\nTask Selection: At each instant t, the Planner selects a task from the LA queue for assignment to one of the robots, guided by the current state of the environment.\nRobot Allocation: Upon task selection, the Executor identifies the most suitable robot for task execution. This decision considers the positions of all robots after completing their current assignments, availability and SOC. Notably, the Executor does not wait for robots to become available before making robot allocations, as the state information includes time markers indicating when robots will be free.\nNavigation: To guide the robots' movement, a linear quadratic regulator (LQR) based controller, augmented with potential functions for coordinated collision avoidance, considering all its dynamics is employed. This algorithm directs the robot from its current position to the task's starting point and subsequently to the destination, ensuring collision avoidance with moving obstacles."}, {"title": "4.1 MRTAgent framework", "content": "Both Planner and Executor agents of the MRTAgent utilize Proximal Policy Optimization (PPO) algorithm [28], with their architectures depicted in Figure 2. Both the agents are trained using a self-play inspired strategy, where one agent is actively trained while the other operates in evaluation mode, alternating every 40 episodes. This concurrent training framework ensures coordination between the two agents, facilitating efficient convergence and optimization of both the task selection and robot allocation processes."}, {"title": "4.2 Neural Network Architecture and Training", "content": "Planner and executor both employs a similar novel PPO-based framework to facilitate online task selection and robot allocation. This model architecture inspired from [1] is distinctly structured into three core segments (see Figure 2). The first segment involves the procedure of feature extraction, particularly focusing on attributes related to robots and tasks. The embedding for robot attributes is created using a sequence of four linear layers of dimensions [4, 16, 16, 1]. Concurrently, embeddings related to task attributes are generated using a similar sequence of four linear layers of dimensions [6, 16, 16, 1].\nLet F and Ff represent the feature vectors for the robot j \u2208 R and the ith-task for the planner and executor, then the associated embeddings are defined as:\n$$E^R = W_{P2} * ReLU(W_{P1} * F_j)$$\n$$E'_R = W_{R2} * ReLU(W_{R1} * F_i)$$\nThe second module transforms the extracted embeddings by concatenating the embeddings of robots and tasks for both planner and executor. Subsequently, the concatenated feature is channelled through a linear layer consisting 48 input neurons and 8 output neurons with ReLU activation.\n$${\\pi}_{planner} = Sigmoid(W_{P4} * Tanh(W_{P3} * E))$$\n$$a^R_{executor} = Sigmoid(W_{R4} * Tanh(W_{R3} * E'_R))$$\n$$\\pi_{planner} = (\\Sigma E^R + \\Sigma E'_R)$$\n$$a^R_{executor} = (\\Sigma E^R + \\Sigma E'_R)$$\nFinal layer comprising a linear layer, this element operates with 8 input neurons and 1 output neuron.\n$$Task_{Allocated} = Categorical (W_2 * ReLU(W_1 * \\pi_{planner}))$$\n$$Robot_{Allocated} = Categorical (W'_2 * ReLU(W'_1 * a^R_{executor}))$$"}, {"title": "4.3 Multi-Robot Navigation Algorithm: LQR with Artificial Potential Field", "content": "Linear Quadratic Regulator (LQR) is an optimal control strategy used to determine the control inputs that minimize a quadratic cost function over time for a linear system. It is widely used in control engineering to stabilize systems and optimize performance by balancing different aspects of system behavior, like minimizing energy use, overshoot, or settling time [16]. When modeling robot dynamics, a common simplification is to treat the robot as a double-integrator system. This model is particularly relevant for systems where the primary concern is controlling the position and velocity of the robot [29]. Consequently, state of robot i is typically represented by its position pi(t) = (xi(t), yi(t)) and velocity vi(t), with the equations of motion represented as:\n$$p_i(t) = v_i(t), \\qquad v_i(t) = u_i(t).$$\nHere ui(t) = (uix (t), uiy(t)) is the control input, which directly influences the acceleration. Here, complete state of the ith-robot is represented as: zi(t) = (pi(t), vi (t)). Applying LQR to this model involves designing a controller that minimizes deviations from a desired trajectory while controlling the velocity and ensuring smooth acceleration."}, {"title": "Algorithm 1: MRTAgent", "content": "controller within this collective framework, allowing for the independent regulation of position and velocity while minimizing a global cost function that balances state deviations and control effort. For collision avoidance, an artificial potential field (APF) method is employed [23]. This approach introduces repulsive forces between robots when they come into close proximity, ensuring safe inter-robot distances. These repulsive forces are integrated into the control law, enabling the robots to avoid collisions while continuing to track their desired trajectories. This combined LQR-APF approach provides an effective solution for coordinating multiple robots in dynamic environments. More formally, we consider a system with N robots, each modeled as a double-integrator in 2D space. The dynamics of the overall system is given by:\n$$\\dot{z}(t) = A_{multi}z(t) + B_{multi}u(t),$$\nwith\n$$ z(t) =  \\begin{bmatrix} p_1(t) \\\\ v_1(t) \\\\ ...\\\\ p_N(t) \\\\ v_N(t) \\end{bmatrix} ,\\quad u(t) =  \\begin{bmatrix} u_1(t) \\\\ u_1(t) \\\\ ...\\\\ u_N(t) \\\\ u_N(t) \\end{bmatrix} $$\n$$A_{multi} = I_N \\otimes  \\begin{bmatrix} 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\end{bmatrix} , \\quad B_{multi} = I_N \\otimes  \\begin{bmatrix} 0 & 0 \\\\ 0 & 0 \\\\ 1 & 0 \\\\ 0 & 1 \\end{bmatrix} $$\nwhere IN is the identity matrix of size N, and \u2297 denotes the Kronecker product. The objective of LQR is to minimize the following quadratic cost function:\n$$J = \\int_0^{\\infty} (z(t)^T Q z(t) + u(t)^T R u(t)) dt,$$\nwhere Q and R are weighting matrices that penalize deviations from the desired state and control effort, respectively. The optimal control input that minimizes the cost function is given by u(t) = -K(z(t) - Zdes), where K is the LQR gain matrix, computed as K = R-1BTmulti P, and Zdes represents the target positions and velocities. Here P'is the solution to the continuous-time algebraic Riccati equation (CARE):\n$$A^T_{multi}P + PA_{multi} - PB_{multi}R^{-1}B^T_TP + Q = 0.$$\nIn addition, we employ APF method that introduces a potential field around each robot that creates repulsive forces to avoid collisions. 2 The repulsive potential between robots i and j is given by:\n$$U_{rep,ij} (z_i, z_j) = \\begin{cases}  \\frac{1}{2}k_{rep} (\\frac{1}{d_{ij}} - \\frac{1}{d_{min}})^2 & \\text{if } d_{ij} < d_{min} \\\\ 0 & \\text{if } d_{ij} \\geq d_{min} \\end{cases}$$\nwhere krep is a positive constant, dij = ||Pi \u2013 pj|| is the Euclidean distance between robots i and j, and dmin is the minimum allowable distance between robots. The final control input for each robot, incorporating both LQR control and APF-based collision avoidance, is given by:\n$$u_i(t) = \u2212K_i(z_i(t) \u2212 z_{des,i}) \u2212 \\nabla p_i U.$$\nThis ensures that the robots not only follow their intended paths but also avoid collisions by adjusting their trajectories dynamically in response to nearby robots."}, {"title": "5 EXPERIMENTS", "content": "The MRTAgent framework has been validated within warehouse environment settings, operating under several key parameters to ensure efficient functioning. Due to the absence of publicly accessible real-world datasets for similar problem scenarios, synthetic data has been employed to evaluate our MRTAgent framework. Each episode spans t time units, and the synthetic datasets used for both training and evaluation are structured as square-shaped 2D continuous spaces ranging from [0,64] \u00d7 [0,64], with task origins, destinations, and robot locations confined within this range.\nIn our experiments, task concentration varies to reflect specific periods of the day when the majority of tasks accumulate. Accordingly, datasets are generated using Gaussian distributions with varying means and standard deviations. The task's starting and ending coordinates, along with the task generation times, are provided to the planner through a limited-size LA. This MRTAgent framework is evaluated under two configurations:\n\u2022 Normally distributed task arrival times\n\u2022 Uniformly distributed task arrival times.\nFor each configuration, datasets containing 500 tasks per episode are generated according to the corresponding distributions. For instance, in the case of normally distributed tasks, the task generation times adhere to N(600, 50), where 600 represents the mean task generation time. For training, we considered a fleet of 10 robots, with the LA window length fixed at 5. The robots' charging threshold is set at 30%, meaning robots with a SOC below 30% must dock for recharging before resuming task execution. The steady charging rate is calibrated to be 16x the discharging rate.\nThe planner and executor agents are implemented using the PyTorch library in Python 3.8, with an Adam optimizer [15], a discount factor of 0.99, a lambda value of 0.95, a learning rate of 0.0003, an entropy coefficient of 0.001, a value function coefficient of 0.0002, and a batch size of 32. The policy networks for both the planner and executor are trained using the cross-entropy loss function, while the value networks are fine-tuned using the mean squared error loss metric."}, {"title": "5.2 Baselines", "content": "To the best of our knowledge, no existing work in the literature concurrently addresses multiple aspects of MRTA simultaneously. In light of the absence of established approaches, we propose two suitable baselines.\nBrute-force optimal (BFO) : In this approach, all task-robot pairs (within the LA) undergo an exhaustive brute-force evaluation of time duration required for task execution by the robots, determined using standard Euclidean distance. The algorithm then selects the robot-action pair that minimizes this time duration. While brute-force optimal approach represents a locally optimal solution, the exhaustive evaluation significantly amplifies the run-time, posing practical challenges. Despite, this baseline is frequently adopted in the literature as a reference point for evaluating decoupled task allocation and navigation methodologies [2, 20].\nFIFO: The FIFO baseline employs a dual-tiered decision framework. The initial allocation involves selecting the task that entered the LA queue first, aiming to reduce pending tasks within the queue and allocating the robot that can complete it earliest to minimize TTGT [11, 12]. Due to its simplicity, the FIFO approach requires the least execution time among all the considered approaches."}, {"title": "5.3 Training Details", "content": "The MRTAgent is trained through a self-play approach. Initially, the planner undergoes training for 40 episodes, while the executor remains in evaluation mode. After every 40 episodes, the roles of the planner and executor are reversed: the planner is switched to evaluation mode, and the executor is trained. This cycle is repeated 24 times, leading to a total of 960 training episodes. Each episode consists of 505 tasks, with 10 robots in the environment and a LA length of 5. Both the Planner and Executor agents are trained using the PPO algorithm [28]. The model is implemented using the PyTorch library in Python 3.8. Key hyperparameters include the Adam optimizer [15], a discount factor of 0.99, a lambda value of 0.95, a learning rate of 0.0003, an entropy coefficient of 0.001, a value function coefficient of 0.0002, and a batch size of 32. The policy network for both agents is trained with the cross-entropy loss function, while the value network is optimized using mean squared error (MSE) loss."}, {"title": "5.4 Experiments and Results", "content": "We now present a thorough comparative analysis of our proposed learning-based framework, MRTAgent, against baseline methods, namely the BFO approach and the FIFO strategy. Our experiments are designed to include scenarios that incorporate charging considerations, thereby addressing realistic operational challenges. The results consistently demonstrate the superiority of MRTAgent over the baseline methods.\nFigure 3 presents the average training curve for the bi-level RL agents. This average is derived from four independent runs with different random seeds. The RL model is trained over 960 episodes, each comprising 505 tasks with 10 robots in the environment, and a look-ahead (LA) length of 5. The training process utilizes the PPO-based RL algorithm within the PFRL framework [8]. To simulate real-world task profiles, the training procedure employs task lists generated randomly from a Gaussian distribution, alongside uniformly sampled task lists. The reward plots indicate that MRTAgent achieves stable convergence towards an optimal policy. The improvements in rewards from their initial values suggest effective task selection and robot allocation, leading to reduced task waiting times within the LA window. It is worth noting that the RL agent is periodically trained with different random task lists, which prevents it from simply memorizing the performance on a specific task list. Instead, the agent learns to adapt to various scenarios, which explains the minor fluctuations in rewards across episodes as the agent refines its policy.\nAfter the training phase, the model's performance is evaluated on different test datasets. For the model trained on task lists following a specific Gaussian distribution, the evaluation is performed on two distinct datasets:\n\u2022 Instances that are similar to the training data with the same mean and variance specifically with N(600, 50) distributed data,\n\u2022 A dataset generated entirely from a uniform distribution specifically with U(0, 1000) distributed data.\nThis evaluation assesses MRTAgent's ability to handle distributional shifts. Conversely, the model trained on uniformly generated task lists is evaluated on:\n\u2022 A dataset sampled from the same uniform distribution specifically with U(0, 1000) distributed data,\n\u2022 A dataset generated from a Gaussian distribution pecifically with N(600, 50) distributed data,\noffering insights into its performance under distributional shifts. Table 1 provides a detailed comparison of the test results. The total number of tasks in each episode, the number of robots, and the LA queue length remain consistent with the training setup 505 tasks, 10 robots, and an LA length of 5. The results clearly demonstrate MRTAgent's consistent outperformance over the brute-force optimal and FIFO-based methods across almost all test scenarios. The MRTAgent framework outperforms the baselines in all instances, and for a completely different dataset, it performs significantly better compared to the baselines.\nTo further analyze MRTAgent's superiority over other baselines, we examine the individual reward components of all approaches, as shown in Table 2. The total cost (reward) consists of two primary components: (a) TRTO, which aims to minimize robot travel distance, and (b) TTGT, which seeks to reduce task execution delays. As shown in Table 2, MRTAgent achieves value, in the TRTO component lesser than the brute-force optimal approach signifying its ability to minimize travel distance for the tasks in the look-ahead queue, as well as in the TTGT component because the brute-force optimal approach does not prioritize minimizing delays for tasks already in the look-ahead. On the other hand, the FIFO approach, despite assigning tasks sequentially, does not effectively reduce the TTGT component. This is because the task endpoints may be far from the starting locations of subsequent tasks in the look-ahead, potentially causing delays in task execution. As a result, the TRTO component is typically larger than in the brute-force.\nVariable number of robots: To assess MRTAgent's generalizability, we run experiments with varying numbers of robots during inference. Initially, the executor is trained with a fleet of 10 robots and 5 tasks in the LA, and the results are presented in Table 1. We then retrain the executor for a fleet of 30 robots, while keeping the planner in evaluation mode, to observe MRTAgent's performance and scalability. The results for the 30-robot scenarios are shown in Table 3. As expected, performing the same tasks with more robots incurs lower costs, as the TRTO and TTGT components reduce significantly. Nevertheless, MRTAgent consistently outperforms the baseline methods in all scenarios.\nThe separation of Tasks (Planner) and Robot (Executor) agents in our framework enables scalability with varying task and robot counts. This setup also allows upgrading the Executor without retraining the entire system. Scenarios like robot failures-which reduce available robot count-can be managed by extending robot availability time (rj, a feature in MRTAgent) to a large value, excluding the failed robot from selection. For instance, MRTAgent trained with 30 robots (see Table 3) is evaluated with only 25 available, without retraining in Table 4. This is achieved by assigning large values to rj for the extra 5 robots, effectively preventing task allocation and enabling our framework to adapt to different robot counts without retraining.\nVariable number of tasks: To evaluate the MRTAgent framework's generalizability, the planner and executor, initially trained for a fleet of 5 robots and 505 tasks (see Table 1), are tested on a larger number of tasks, specifically 2005 (see Table 5). As observed, MRTAgent consistently outperforms the baselines across a variable number of tasks without requiring retraining.\nIn Tables 1-5, the different rows represent the use of various datasets for evaluation. Specifically, in Table 1, the standard deviations reported for MRTAgent indicate slight fluctuations in performance when the model is evaluated using different random seeds. It is crucial to note that MRTAgent is trained with a periodically updated random task list, which prevents the agent from simply memorizing performance on a fixed set of tasks. Instead, this approach enables the agent to learn to adapt effectively to a wide range of scenarios. This is why there are small variations in the rewards across episodes, even as the agent gradually develops a consistent policy.\nA note on the baselines: While FIFO is known for its simplicity and computational efficiency; BFO, despite common intuition, is one of the strongest baselines which, given a look ahead (LA), evaluates all task-robot pairs and selects the one with earliest possible execution. In fact, BFO is an optimal task allocation and assignment approach given the current state of the LA received in an online fashion. The reason why MRTAgent is able to outperform it is due to the fact that MRTAgent exploits the underlying distribution defining task generation to plan for tasks to appear in future despite it having access to the same causal information as the BFO."}, {"title": "6 CONCLUSION AND FUTURE WORK", "content": "This study introduces MRTAgent, a self-play motivated bi-level RL framework designed to enhance MRTA in modern warehousing environments. By optimizing operational costs and efficiency, MRTAgent addresses the increasing demands associated with online order fulfillment. The framework employs an optimal sequential task and robot selection process, coupled with a LQR based collision-free navigation algorithm. It effectively manages critical constraints such as robot dynamics, charging needs, and specific task requirements. Our approach demonstrates significant improvements over baseline methods across various test datasets. The validation of MRTAgent over datasets with distributional shifts and varying numbers of robots and tasks highlights its generalizability.\nWhile our MRTAgent algorithm presents a promising approach, it is essential to acknowledge certain limitations that warrant attention in future research endeavors:\n1. Single-Task Assumption: The algorithm currently assumes that robots are engaged in one task at a time, potentially limiting its applicability in scenarios where multitasking is prevalent.\n2. No Contingency for Sudden Breakdowns: The model does not account for the sudden breakdown of a robot during operation. Once a task is allocated, our algorithm lacks the capability to reconsider or revert the decision in the event of an unforeseen robot malfunction.\n3. Homogeneous Robots: We have made the simplifying assumption that all robots in the system are homogeneous, sharing identical values of characteristics such as velocity, acceleration, and load capacity. This assumption may not reflect the diversity present in real-world robot fleets.\n4. Negligible Load/Unload Time Assumption: The algorithm assumes negligible time for loading/unloading operations after a robot reaches the task origin/destination. In reality, this may not hold true, and accounting for realistic loading and unloading times is a consideration for future enhancements.\nFuture work will focus on enabling robots to handle multiple tasks simultaneously, incorporating load/unload times, integrating heterogeneous robots, and implementing learning-based navigation, all of which will enhance the algorithm's effectiveness and applicability in diverse, practical scenarios."}]}