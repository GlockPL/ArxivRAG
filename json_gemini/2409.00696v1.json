{"title": "POLYRATING: A COST-EFFECTIVE AND BIAS-AWARE\nRATING SYSTEM FOR LLM EVALUATION", "authors": ["Jasper Dekoninck", "Maximilian Baader", "Martin Vechev"], "abstract": "Rating-based human evaluation has become an essential tool to accurately evalu-\nate the impressive performance of Large language models (LLMs). However, cur-\nrent rating systems suffer from several critical limitations. Specifically, they fail\nto account for human biases that significantly influence evaluation results, require\nlarge and expensive preference datasets to obtain accurate ratings, and do not facil-\nitate meaningful comparisons of model ratings across different tasks. To address\nthese issues, we introduce POLYRATING, an expressive and flexible rating system\nbased on maximum a posteriori estimation that enables a more nuanced and thor-\nough analysis of model performance at lower costs. POLYRATING can detect and\nquantify biases affecting human preferences, ensuring fairer model comparisons.\nFurthermore, POLYRATING can reduce the cost of human evaluations by up to\n41% for new models and up to 77% for new tasks by leveraging existing bench-\nmark scores. Lastly, POLYRATING enables direct comparisons of ratings across\ndifferent tasks, providing a comprehensive understanding of an LLMs' strengths,\nweaknesses, and relative performance across different applications.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) have become powerful tools across a wide range of tasks, sometimes\neven outperforming human experts (OpenAI, 2023; Anthropic, 2024; Anil et al., 2023; AI@Meta,\n2024). To evaluate and compare the performance of LLMs, various benchmarks (Cobbe et al., 2021;\nClark et al., 2018; Hendrycks et al., 2021) and evaluation frameworks (Gao et al., 2023; Liang et al.,\n2022) have been developed. These benchmarks aim to provide a comprehensive evaluation of LLM\ncapabilities across tasks such as code completion, mathematical problem-solving, and multilingual\nunderstanding. However, the reliability of these benchmarks to accurately estimate model perfor-\nmance has been questioned due to various concerns about data contamination (Dekoninck et al.,\n2024; Zhang et al., 2024), errors in ground-truth solutions (Gema et al., 2024), and the discrepancy\nbetween benchmarks and real-world performance (Lin et al., 2024).\nRatings for LLMS To evaluate LLMs more accurately in real-world scenarios, recent works have\nmade use of rating-based evaluations with human or LLM-based judges (Chiang et al., 2024b; Lin\net al., 2024; Dubois et al., 2024). These ratings reflect the relative performance of LLMs on specific\ntasks and are used to construct leaderboards indicating their real-world performance. As shown in\nFig. 1, ratings are derived from preference datasets containing samples consisting of a query Q, a\nresponse by two models m(0) and m(1), a judge J, and a judgment indicating the preferred response\nr. We illustrate human preference datasets for several tasks like code-based (</>), mathematical (),\nand Chinese (\u9f8d) questions, along with a preference dataset using an LLM-based judge (). Current\nmethods fit each task separately using maximum likelihood estimation (MLE) to obtain ratings Rask\nfor each model mi that predict the judge's preferences as accurately as possible.\nLimitations of Current Rating Systems However, current rating systems suffer from several crit-\nical limitations. First, it is widely recognized that judges are influenced by biases that significantly\naffect their preferences (Hosking et al., 2023; Wu and Aji, 2023; Shi et al., 2024; Chen et al., 2024)."}, {"title": "2 RATING SYSTEMS", "content": "In this section, we introduce the necessary notation to formalize rating systems for LLMs.\nPreference Datasets A rating system requires the availability of a preference dataset, which con-\nsists of n games that capture the preferences of a judge. In language model evaluation, a game g\nconsists of a user query Q, two language models mo and m\u2081, and a judge J. The result r of the game\nis determined by the judge's preference for one of the completions and is 1 if m\u2081 beats mo, denoted\nas m\u2081 > mo, and 0 if mo > m1. Thus, we can represent a game g as a tuple (Q, mo, m1, J, r).\nRating System For a given set of k models, a rating system assigns a score Yi \u2208 R+ to model mi,\nindicating the relative skill of the model on the task. With these scores, the probability that mi wins\nagainst mj can be computed using the Bradley-Terry model (BT-model) (Bradley and Terry, 1952):\n$P(m_i \\succ m_j | \\gamma_i, \\gamma_j) = \\frac{\\gamma_i}{\\gamma_i + \\gamma_j}$\nIn most rating systems the scores are parametrized using the exponential function Yi = exp(Ri/400)\nwhere Ri the rating of mi and 400 is a constant used to scale the ratings (Elo, 2008; Glickman, 2002).\nRating Optimization To determine the ratings of the models, a rating system aims to maximize\npredictive capabilities for the observed outcomes of the games. The maximum likelihood estimate\nfor these observed outcomes in the BT-model can be found by minimizing the logistic loss\n$L(D, R) = - \\sum_{g\\in D} (g_r log P(g_{m_1} \\succ g_{m_0} | R) + (1 - g_r) log P(g_{m_0} \\succ g_{m_1} | R))$\n(1)\nfor a dataset of games D = (g1,..., gn) and ratings R = (R1,..., Rk). Thus, the optimal rat-\nings can be obtained by computing arg min\u20a8 L(D, R). To obtain ratings for specific tasks, the\noptimization is performed separately for each task on the task-specific dataset D.\nIncorporating Draws However, the BT-model ignores the possibility of draws in games. Fol-\nlowing the approach by the Chatbot Arena (Chiang et al., 2024b), we can generalize the BT-model\nby setting the outcome gr equal to 0.5 for draws to obtain model ratings that can incorporate these\ndraws. Although we explore several alternatives to the BT-model that more explicitly model draws\nin App. B, we found they did not offer significant advantages in practice."}, {"title": "3 POLYRATING", "content": "We now introduce POLYRATING, a multivariate rating system specifically designed for language\nmodel evaluation. This section first outlines the four design goals for an effective LLM rating\nsystem and then explains POLYRATING and how it meets those objectives.\n3.1 DESIGN GOALS FOR LLM RATING SYSTEM\n1) Quantify Biases Both human and LLM-based judges are influenced by biases that affect their\npreferences (Hosking et al., 2023; Wu and Aji, 2023; Shi et al., 2024; Chen et al., 2024). A robust rat-\ning system must capture and quantify these biases to ensure fair comparisons of LLM performance,\nregardless of the judge. Current rating systems are predominantly univariate and are therefore unable\nto include the necessary extra parameters to capture these biases. Only AlpacaEval (Dubois et al.,\n2024) accounts for length bias, where the length of the model answer significantly influences the\njudge's preference. However, its fitted coefficient is not directly interpretable and does not quantify\nthe bias's influence on ratings.\n2) Leverage Existing Information Leveraging existing information can make a rating system\nmore sample efficient and reduce evaluation costs. This information can come from LLM-based\nevaluations or traditional benchmarks, both of which indicate model performance and should there-\nfore give valuable information that can improve sample efficiency. However, current rating systems\nstart from scratch for each new task and model."}, {"title": "3.2 POLYRATING", "content": "Modeling Features Features can be continuous, like the length of a answer, or discrete, like the\ntask of the query. We model each feature as a function $f : G \\times \\{0,1\\} \\rightarrow R$ that maps a game g \u2208G\nand a boolean i to a number quantifying the presence of this feature. The boolean i specifies whether\nthe model for which we want to compute the feature is the first or second model in the game.\nModeling Ratings It is essential to model ratings as a function of these features to capture bi-\nases and measure task-specific ratings. For this purpose, we first note that ratings must be game-\ndependent to incorporate game-dependent features like query task or answer length. Furthermore,\njudge-specific biases are model-independent and must therefore be measured using parameters that\nare shared across all models. Finally, it is important that ratings remain interpretable and practical\nfor leaderboards. Therefore, the rating for model m in game g is modeled using the linear model\n$R_m^g = R_{base} + \\sum_{j=1}^{d} \\alpha_jf_j(g, [g_{m_1} = m]) + \\sum_{j=1}^{d'} \\beta_m f'_j(g, [g_{m_1} = m])$.\n(2)\nHere, $R_{base} \\in R$ is the base rating, d and d' are the number of features in the respective sums, $\\alpha_j \\in R$\nis the weight for feature $f_j$ and is shared across all models, $\\beta_m \\in R$ is the model-specific weight\nfor feature $f'_j$, and [[. . .] is the indicator function. Importantly, Eq. (2) serves as the key element of\nPOLYRATING that enables it to incorporate all design goals. Indeed, the shared parameters measure\nthe biases in the judge's preferences, addressing the first design goal. To obtain task-specific ratings,\nwe introduce a task-specific feature $f_{task} (g,i) = [g\\in D_{task}]$, where $D_{task}$ represents the set of\nall task-related queries. We then define the task-specific ratings as $R_{task} = R_{base} + B_{task}$. While\nthis assumes no additional task-specific features are used, the definition can be easily extended to\nincorporate them, ensuring that ratings remain adaptable to varying evaluation contexts.\nOptimization Objective We perform MAP estimation with a normal prior on the weights aj and\nBr with mean 0 and deviations o\u00a1 and of respectively. This leads to the optimization objective\n$L_{full}(D, R_{base}, \\alpha, \\beta) = \\sum_{g\\in D} L(\\{g\\}, R^1(g), ..., R^k (g)) + \\sum_{j=1}^{d} \\frac{(\\alpha_j)^2}{2(\\sigma_j)^2} + \\sum_{m=1}^{k}\\sum_{j=1}^{d'} \\frac{(\\beta_j^m)^2}{2(\\sigma'_j)^2}$\n(3)\nwhere Rbase is the vector of base ratings, a is the vector of shared weights, \u1e9e is the matrix of model-\nspecific weights, and ratings are computed using Eq. (2). In contrast to the loss function presented\nin \u00a72, the dataset D can contain games from different tasks.\nUsing MAP estimation instead of MLE allows POLYRATING to incorporate information from ex-\nisting tasks through its priors, thereby improving sample efficiency and reducing evaluation costs.\nFurthermore, evaluations with LLM-based judges can be considered as a distinct task and can there-\nfore be used to improve sample efficiency. Traditional benchmarks can also be reinterpreted as\npreference datasets, where a question indicates a preference for models that answered correctly over\nthose that did not. If the models are both (in)correct, the judge has no preference. We can therefore\nleverage all this information, ensuring that POLYRATING satisfies the second design goal."}, {"title": "4 EVALUATION", "content": "We perform a series of experiments with POLYRATING that showcase its ability to quantify the\ninfluence of biases on the ratings of the models (\u00a74.1), its improved sample efficiency for various\nuse-cases (\u00a74.2), and its ability to obtain reliable and comparable multivariate leaderboards (\u00a74.3).\n4.1 BIAS DETECTION\nWe use POLYRATING to quantify the influence of biases in both human and LLM-based evaluation\nusing a public subset of the Chatbot Arena (Chiang et al., 2024a) and Wildbench (Lin et al., 2024).\nThis enables an accurate estimation of the effects of these biases on model ratings and allows us to\ncompare the influence of these biases between human and LLM-based judges.\nBiases We briefly explain the measured biases and refer to App. D for a full overview of all biases\nalong with their functional form. We include the well-known length bias (Dubois et al., 2024;\nSinghal et al., 2023), which measures bias with respect to the length of the completion, and position\nbias (Shi et al., 2024; Wang et al., 2023), which measures bias with respect to the order of the models\nin the game. We further use classifiers (Babakov et al., 2023; Camacho-collados et al., 2022) to\nevaluate the influence of formality and sentiment of the model's output. Finally, we compute the\nrepetitiveness of the answer by measuring the number of unique tokens in the completion and check\nthe influence of the readability of an answer using the Flesch Reading Ease score (Kincaid et al.,\n1975). We model all of these biases as shared features when fitting POLYRATING, allowing us to\naccurately estimate their influence on the resulting ratings.\nResults Table 1 shows the effects of these biases on both human and LLM-based judges. We\npresent both the coefficient @bias and the average influence this coefficient has on model ratings\nfor given queries, i.e. Eg(@bias\u00b7|fbias(9,0) \u2013 fbias(g, 1)|). To put these numbers into context, the\ndifference between the first and the tenth best models in the Chatbot Arena is 50 rating points."}, {"title": "4.2 IMPROVED SAMPLE EFFICIENCY", "content": "We demonstrate that POLYRATING is more sample-efficient compared to traditional univariate ap-\nproaches and therefore reduces the cost of human evaluation.\nDataset We use the full Chatbot Arena dataset (Chiang et al., 2024b), which contains over one\nmillion questions across various tasks. Each question is answered by two models and judged by\nthe human that posed the question. We use the tasks that contain Chinese, code-based, and hard\nquestions for this experiment and refer for a full description of these tasks to App. D."}, {"title": "4.3 MULTIVARIATE LEADERBOARD", "content": "We now compare separately fitted univariate leaderboards with a multivariate leaderboard fitted\nusing POLYRATING. Since the univariate approach is shift-invariant, we need to fix the shifting\nconstant for each task. We follow the approach of the Chatbot Arena and set the constant by fixing\nthe rating of Mixtral-8x7b-Instruct-v0.1 to 1114 for all tasks. We will show that this approach fails\nto provide comparable ratings for the models in the leaderboard.\nResults Table 2 and Table 3 show the ratings of several models in the leaderboard fitted using\nPOLYRATING and a univariate approach respectively. For a full overview of all models, we refer to\nApp. E. By examining the modifiers computed by POLYRATING, we immediately see the downside\nof the univariate approach. In both the English and Chinese tasks, the Mixtral model performs very\ndifferently compared to the other tasks. Fixing the shifting constant using this model results in\nsignificant ratings shifts for these two tasks compared to the base rating.\nThis effect is most apparent in the Chinese task. Almost all models gain substantial ratings in\nthe univariate approach, even though yi-large-preview is the only model built bilingually from the\nground up. In contrast, POLYRATING shows that yi-large-preview is the only model that gains rating\nin this task. Similarly, llama-3-70b-instruct, a model specifically trained for English, gains only 22\nrating points in the English task using the univariate approach, whereas POLYRATING shows it gains\n67 rating points, aligning more with expectations. The last two tasks show more similarities between\nthe two approaches, but we note that the arbitrary shifting constant for each task makes it very hard\nto actually trust these modifiers in the univariate approach."}, {"title": "5 RELATED WORK", "content": "Ratings Rating systems have been used across various domains, such as sports (Elo, 2008; Glick-\nman, 2002; Shelopugin and Sirotkin, 2023; Sismanis, 2010; Vaz et al., 2012), gaming (Herbrich\net al., 2007; Dangauthier et al., 2007), movies (Talattinis and Stephanides, 2022) and recommenda-\ntion systems (Chen et al., 2018; Adomavicius et al., 2005; Kong et al., 2019). The widely recognized\nElo rating system (Elo, 2008) and its extensions such as Glicko (Glickman, 2002) are generic uni-\nvariate systems based on the BT-model (Bradley and Terry, 1952) that are widely applicable. Fur-\nthermore, various rating systems have been developed for specific use cases and areas. For example,\nElo++ (Sismanis, 2010) was specifically designed for chess, and TrueSkill (Herbrich et al., 2007;\nDangauthier et al., 2007) has been further developed specifically for multiplayer online games.\nRatings for LLMS Preference datasets for LLMs have become common to evaluate model ca-\npabilities in areas lacking ground-truth benchmarks. The most popular one is the Chatbot Arena\n(Chiang et al., 2024b), which contains over one million user queries and evaluates models in various\ntasks such as code, math, and multilingual understanding. Wildbench (Lin et al., 2024), MT-Bench\n(Zheng et al., 2023), and AlpacaEval (Dubois et al., 2024) are LLM-based evaluation frameworks\nthat have gained attention. Among these, AlpacaEval is the only one that applies a length-control\nbias similar to POLYRATING to obtain a higher correlation with human judges. However, this fitted\nbias is not directly interpretable and is less generic than the approach used in POLYRATING. Addi-\ntionally, POLYRATING employs priors on various terms to improve sample efficiency and eliminate\nshift-invariance, which AlpacaEval lacks.\nMultivariate Rating Systems Multivariate rating systems have been used before in recommen-\ndation systems (Chen et al., 2018; Adomavicius et al., 2005; Kong et al., 2019). These developed\nsystems are extensions to the more classical Elo (Elo, 2008) and Glicko (Glickman, 2002) rating sys-\ntems. However, they are not directly applicable to LLM evaluation, as they do not take into account\nthe specific biases and dependencies that are present in LLM evaluation. Furthermore, the limited\nnumbers of models allow us to build an exact optimization algorithm, unlike in recommendation\nsystems where approximate algorithms are necessary due to the high number of products.\nBiases in Human and LLM-Based Evaluation Several works have examined biases in both hu-\nman and LLM-based evaluations (Hosking et al., 2023; Clark et al., 2021; Wang et al., 2023; Wu\nand Aji, 2023; Shi et al., 2024; Chen et al., 2024; Singhal et al., 2023). Typically, these studies\nintroduce biases to model answers to observe their impact on judge preferences (Wu and Aji, 2023;\nChen et al., 2024; Singhal et al., 2023; Wang et al., 2023). Additionally, they also investigate bias\nby asking more specific questions to the judges, rather than simply asking their preference (Hosking\net al., 2023; Wu and Aji, 2023). These techniques, however, do not apply to existing datasets and\nrequire additional annotations for specifically crafted answers. In contrast, POLYRATING can be\ndirectly applied to existing datasets without further annotation."}, {"title": "6 LIMITATIONS", "content": "We briefly discuss the limitations of POLYRATING. First, while POLYRATING provides a way to\nmeasure model strengths and weaknesses, these comparisons are relative to the other models in the\nleaderboard and do not provide an absolute measure of model performance. For instance, if all\nmodels in the leaderboard perform well on one task, and poorly on another, the leaderboard will\nnot reflect this absolute weakness. Instead, it will only show weaknesses relative to the average\nperformance of the models. This is a fundamental limitation of rating systems and cannot be solved\nby any system that works solely based on preference data. To obtain absolute measures of model\nperformance additional data sources, such as traditional benchmarks, are required.\nFurthermore, POLYRATING still requires significant manual inspection and tuning since users must\ndetermine the modeling parameters and functions that constitute the rating, a process that can be\ntime-consuming. A more automatic discovery of interesting and relevant dimensions, especially for\nbias detection, would help mitigate this issue."}, {"title": "7 CONCLUSION", "content": "We introduced POLYRATING, a multivariate rating system specifically designed for language model\nevaluation. POLYRATING enables a more comprehensive evaluation of LLMs by capturing biases\nand dependencies on both continuous and categorical features in the evaluation. We demonstrated\nthe existence and influence of several biases, such as length and position bias, and compared these\nbiases between human and LLM-based judges. Furthermore, we showed that POLYRATING can\nleverage existing data to increase sample efficiency by 41% and reduce the costs of human eval-\nuations for new tasks by up to 77%. Finally, we showed that POLYRATING can provide a more\nreliable performance comparison of the same language model across different tasks by solving the\nshift-invariance of the ratings across multiple dimensions."}, {"title": "A ATTRIBUTION", "content": "We provide attribution for the icons used in Fig. 1 here. The code icon was obtained from flati-\ncon.com and created by Royyan Wijaya. The Chinese icon was obtained from flaticon.com and\ncreated by Freepik. The bot icon was obtained from flaticon.com and created by Nuriali. The\nmath icon was obtained from flaticon.com and created by widphic. The length icon was obtained\nfrom freepik.com and created by Surang Lineal. Finally, the readability icon was obtained from\nfreepik.com and created by Generic Detailed Outline."}, {"title": "B ALTERNATIVE RATING SYSTEMS", "content": "This section explores several alternatives to the exponential rating system that solves the MLE of the\nlogistic loss function, as discussed in \u00a72. Specifically, we evaluate two extensions to the BT-model\nand one alternative inspired by the accuracy metric commonly used in benchmarks. We then com-\npare these alternatives with the exponential rating system in terms of their predictive performance\nand demonstrate that their added complexity does not result in better predictions.\nAll models discussed here are compatible with POLYRATING and can be used as substitutes for the\nMLE-based BT-model used in \u00a74.\nRao-Kupper Model Rao and Kupper (1967) extend the BT-model to explicitly account for the\nprobability of a draw by introducing a parameter 0 \u2208 R, 0 \u2265 1:\n$P(i \\succ j | \\gamma_i, \\gamma_j) = \\frac{\\gamma_i}{\\gamma_i + \\theta\\gamma_j}$\n$P(j \\succ i | \\gamma_i, \\gamma_j) = \\frac{\\gamma_j}{\\gamma_j + \\theta\\gamma_i}$\n$P(i \\sim j | \\gamma_i, \\gamma_j) = \\frac{\\gamma_i\\gamma_j (\\theta^2 - 1)}{(\\gamma_j + \\theta\\gamma_i)(\\gamma_i + \\theta\\gamma_j)}$\nIt can be shown that this model follows from the hypothesis that a judge cannot tell the difference\nbetween two answers if the quality of the answers is close to each other.\nDavidson-Model Davidson (1970) propose a similar modification to include draws, using a pa-\nrameter 0 \u2208 R, 0 \u2265 0:\n$P(i \\succ j | \\gamma_i, \\gamma_j) = \\frac{\\gamma_i}{\\gamma_i + \\gamma_j + \\theta\\sqrt{\\gamma_i\\gamma_j}}$\n$P(j \\succ i | \\gamma_i, \\gamma_j) = \\frac{\\gamma_j}{\\gamma_i + \\gamma_j + \\theta\\sqrt{\\gamma_i\\gamma_j}}$\n$P(i \\sim j | \\gamma_i, \\gamma_j) = \\frac{\\theta\\sqrt{\\gamma_i\\gamma_j}}{\\gamma_i + \\gamma_j + \\theta\\sqrt{\\gamma_i\\gamma_j}}$\nAccuracy-Based Model Both extensions to the BT-model presented above still model ratings\nusing an exponential function. However, for LLMs, it could be beneficial to use ratings directly\ncomparable to standard benchmark accuracies. Benchmarks can be viewed as a series of games\nwhere, for a given question Q, model m\u2081 defeats model m2 if m1 answers correctly and m2 does\nnot. A draw occurs if both answer correctly or incorrectly and otherwise m2 wins."}, {"title": "C PROOFS", "content": "We provide the proofs for the theorems mentioned in the main text here.\nWe first prove the convexity of the optimization objective in Eq. (3).\nTheorem 1 (Convexity of the Optimization Objective). The optimization objective in Eq. (3) is\nconvex and twice differentiable.\nProof. Twice differentiability follows immediately from the twice differentiability of the logistic\nloss and the squared penalty term. To show convexity, we make use of the following well-known\nfacts about convex functions:\n\u2022 The sum of two convex functions is convex.\n\u2022 The composition of a convex function with an affine function is convex.\nSince the logistic loss $f(x) = -log(1 + exp(x))$ is convex, and since POLYRATING relies on a\nlinear combination of parameters in the loss function, the logistic loss is convex in these parameters.\nThe squared penalty term is also convex, as it is a sum of squared terms with a positive quadratic\ncoefficient. The sum of two convex functions is convex, so the optimization objective is convex. \u2751\nFurther, we show the optimality of POLYRATING by showing it converges to the same optimal rating\nas the univariate approach when fitted on multiple tasks at the same time.\nFor this purpose, suppose we have a task for which we want to obtain a separate rating. Specifically,\nlet D be a dataset of games between models. Let $D_{\\neg task} \\subset D$, resp. $D_{task} \\subset D$, be the set of\ngames not belonging to, resp. belonging to, the task of interest. We show that as $|D| \\rightarrow \\infty$, the\nrating obtained by individually fitting the tasks is equivalent to the rating obtained by fitting all tasks\nsimultaneously using POLYRATING. Intuitively, the extra prior term in POLYRATING will be of less\nimportance as the number of games in the task of interest increases, and the ratings will converge to\nthe same optimal rating.\nTheorem 2 (Equivalence of Ratings). Let D be a set of i.i.d. games between models $m_0, . . ., m_{k\u22121}$.\nLet $D_{task} \\subset D$ and $D_{\\neg task} \\subset D$ be as defined above. Let $R_{task}$, resp. $R_{\\neg task}$, be the rating obtained\nby fitting the games in $D_{task}$, resp. $D_{\\neg task}$, using the optimal univariate rating system. Let R' be\nthe rating obtained by fitting all games in D simultaneously using POLYRATING with the formula\n$R'_m (g) = R_{base} + \\sum \u03b1jfj (g, [gm1 = m]) + \\sum \u03b2m f';(g, [gm1 = m]) = R'mask + Bm [g \u2208 Dtask and define Rask = Rmask + Bm. Finally, let the priors on\nrespectively $R'_{mask}$ and $\u03b2m$ be $N(0, \u03c32 task)$ and N (0, \u03c3\u03c4). Then, as $|D_{\\neg task} \u2192 \u221e and|Dtask \u2192 8,$\n$R_{task}$ and $R'_{task}$ will, up to a constant difference, converge to the same optimal rating $R_{ask}$ if all\noptimal ratings are finite. Similarly, $R_{\\neg task}$ and $R'_{ \u00actask}$ will, up to a constant difference, converge to\nthe same optimal rating $R\u2217 task$ if all optimal ratings are finite.\nTo prove the theorem, we first need several lemmas.\nLemma 1 (Shift-Invarance of Optimal Ratings). Let D be a set of games between models\n$m_0,..., m_{k\u22121}$ where there exists one model that has played all other models at least once. If\n$R_1$ and $R_2$ both minimize the logistic loss L(D, R) for D, then $R_1 \u2013 R_2$ is a constant vector.\nProof. Without loss of generality, we can assume that the first model is the model that has played\nall other models at least once. We first note that for any constant $c\u2208 R$, it holds that L(D, R+ c) =\nL(D, R). Therefore, we can assume that the first element of each vector, namely $R_1$ and $R_2$, are\nboth zero by applying a constant shift to both. We now show that $R_1 = R_2$.\nWe do so by proving that the function F(x1,...Xk\u22121) = L(D, (0, x1, ..., Xk\u22121)) is strictly convex.\nSince strictly convex functions have a unique minimum, this implies that $R_1 = R_2$. We show\nstrict convexity by computing the Hessian and showing that it is diagonally dominant with strictly\npositive diagonal elements. By Gershgorin circle theorem, this implies that the Hessian cannot have\neigenvalues equal to zero, and is therefore positive definite. Since the Hessian is positive definite,\nthe function is strictly convex, and the result follows.\nWe compute the diagonal terms of the Hessian of F. We denote by $D\\{i,j\\}$ all games where one\nmodel is mi and the other model is mj. We slightly change the notation such that the game result"}, {"title": "D EXPERIMENTAL DETAILS", "content": "In this section, we provide detailed descriptions of the biases and tasks used in our experiments. In\nTable 4 we describe the biases used in \u00a74.1 in more detail. In Table 5 we describe the tasks used in\n\u00a74 in more detail.\nWe also briefly explain how we adjust the win rates of traditional benchmarks to improve sample\nefficiency of human evaluation, as discussed in \u00a74.2. As detailed in the accuracy-based model in\nApp. B, the win rate of m\u2081 over m2 in a traditional benchmark can be written as\n$P(i > j/m_i, m_j) = \\frac{1}{2} (1+ Acc_D(m_i) - Acc_D(m_2))$\n$P(j > i/m_i, m_j) = \\frac{1}{2} (1+ Acc_D(m_2) - Acc_D(m_1)).$\nwhere AccD is the accuracy function.\nBenchmarks often exhibit significant variation in accuracy differences between models. For in-\nstance, in some benchmarks, models may have closely aligned accuracies, while in others, the\ndifferences may be substantial. This variation affects the win rate estimates between models. To\naddress this, we introduce a parameter W, which allows us to adjust the scale of win rates. The\nadjusted win rates are modeled as:\n$P(i > j|m_i, m_j) = min (1, \\frac{W}{2}(1+ Acc_D(m_i) - Acc_D(m_2)))$\n$P(j > i|m_i, m_j) = 1 \u2212 P(i > j|m_i, m_j).$\nThis adjustment ensures that we can control the scale of win rates, mitigating the issue of varying\naccuracy differences. We optimize the hyperparameter W using human evaluations from the training\ndata. Specifically, we fit a univariate rating model using win rates for a given W on the classical\nbenchmark and evaluate the logistic loss of the resulting ratings on the training data. The parameter\nwith the lowest logistic loss is selected. Importantly, we do not use any unknown test data during\nthis optimization process, ensuring that our approach can be applied in practical scenarios without\ncompromising the integrity of the evaluation."}]}