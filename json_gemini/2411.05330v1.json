{"title": "Inversion-based Latent Bayesian Optimization", "authors": ["Jaewon Chu", "Jinyoung Park", "Seunghun Lee", "Hyunwoo J. Kim"], "abstract": "Latent Bayesian optimization (LBO) approaches have successfully adopted Bayesian optimization over a continuous latent space by employing an encoder-decoder architecture to address the challenge of optimization in a high dimensional or discrete input space. LBO learns a surrogate model to approximate the black-box objective function in the latent space. However, we observed that most LBO methods suffer from the 'misalignment problem', which is induced by the reconstruction error of the encoder-decoder architecture. It hinders learning an accurate surrogate model and generating high-quality solutions. In addition, several trust region-based LBO methods select the anchor, the center of the trust region, based solely on the objective function value without considering the trust region's potential to enhance the optimization process. To address these issues, we propose Inversion-based Latent Bayesian Optimization (InvBO), a plug-and-play module for LBO. InvBO consists of two components: an inversion method and a potential-aware trust region anchor selection. The inversion method searches the latent code that completely reconstructs the given target data. The potential-aware trust region anchor selection considers the potential capability of the trust region for better local optimization. Experimental results demonstrate the effectiveness of InvBO on nine real-world benchmarks, such as molecule design and arithmetic expression fitting tasks. Code is available at https://github.com/mlvlab/InvBO.", "sections": [{"title": "Introduction", "content": "Bayesian optimization (BO) has been used in a wide range of applications such as material science [1], chemical design [2, 3],,and hyperparameter optimization [4, 5]. The main idea of BO is probabilistically estimating the expensive black-box objective function using a surrogate model to find the optimal solution with minimum objective function evaluation. While BO has shown its success on continuous domains, applying BO over discrete input space is challenging [6, 7]. To address it, Latent Bayesian Optimization (LBO) has been proposed [8\u201314]. LBO performs BO over a latent space by mapping the discrete input space into the continual latent space with generative models such as Variational Auto Encoders (VAE) [15], consisting of an encoder $q_\\phi$ and a decoder $p_\\theta$. Unlike the standard BO, the surrogate model in LBO associates a latent vector z with an objective function value by emulating the composition of the objective function and the decoder of VAE.\nIn LBO, however, the reconstruction error of the VAE often leads to one latent vector z being associated with two different objective function values as explained in Figure 1. We observe that the discrepancy between y and y' (or x and x') hinders learning an accurate surrogate model g and generating high-quality solutions. We name this the 'misalignment problem'. Most prior works [9, 11, 12] use the surrogate model $g_{enc}$, which is trained with the encoder triplet (x, z, y), and\n* equal contributions\nCorresponding author\n38th Conference on Neural Information Processing Systems (NeurIPS 2024)."}, {"title": "Related Works", "content": "2.1 Latent Bayesian Optimization\nThe goal of Latent Bayesian Optimization (LBO) [8, 9, 11\u201314, 16\u201319] is to learn a latent space to enable optimization over a continuous space from discrete or structured input (e.g., graph or image). LBO consists of a Variational AutoEncoder (VAE) to generate data from the latent representation and a surrogate model (e.g., Gaussian process) to map the latent representation into the objective score. Some works on the LBO have designed new decoder architectures [8, 20-23] to perform the"}, {"title": "Inversion in Generative Models", "content": "Inversion has widely been applied to a variety of generative models such as Generative Adversarial Networks (GANs) [24, 25] and Diffusion models [26\u201329]. Inversion is the process of finding the latent code $z_{inv}$ of a given image to manipulate images with generative models. Formally, given an image x and the well-trained generator G, the inversion can be written as:\n$z_{inv} = \\underset{z \\in Z}{\\text{arg min}} d_x(G(z), x),$\n(1)\nwhere $d_x(\\cdot, \\cdot)$ denotes the distance metric in the image space X, and Z is the latent space. To solve Eq. (1), most inversion-based works can be generally classified as two approaches: optimization-based and learning-based methods. The optimization-based inversion [30\u201333] iteratively finds a latent vector to reconstruct the target image x through the fixed generator. The learning-based inversion [25, 34, 35] trains the encoder for mapping the image x to the latent code z while fixing the decoder. In this work, we introduce the concept of inversion to find the latent vector that can generate a desired sample for constructing an aligned triplet and we use the optimization-based inversion."}, {"title": "Preliminaries", "content": "Bayesian optimization. Bayesian optimization (BO) is a powerful and sample-efficient optimization algorithm that aims at searching the input x with a maximum objective value f(x), which is formulated as:\n$x^* = \\underset{x \\in X}{\\text{arg max}} f(x),$\n(2)\nwhere the black-box objective function $f : X \\rightarrow Y$ is assumed expensive to evaluate, and X is a feasible set. Since the objective function f is unknown or cost-expensive, BO methods probabilistically emulate the objective function by a surrogate model g with observed dataset $D = \\{(x^i, y^i)|y^i = f(x^i)\\}_{i=1}^n$. With the surrogate model g, the acquisition function a selects the most promising point $x^{n+1}$ as the next evaluation point while balancing exploration and exploitation. BO repeats this process until the oracle budget is exhausted.\nTrust region-based local Bayesian optimization. Classical Bayesian optimization methods often suffer from the difficulty of the optimization in a high dimensional space [36]. To address this problem, TuRBO [36] adopts trust regions to limit the search space to small regions. The anchor (center) of trust region T is selected as a current optimal point, and the size of the trust region is scheduled during the optimization process. At the beginning of the optimization, the side length of all trust regions is set to $L_{init}$. When the trust region T updates the best score $T_{succ}$ times in a row, the side length becomes twice until it reaches $L_{max}$. Similarly, when it fails to update the best score $T_{fail}$ times in a row, the side length becomes half. When L falls below a $L_{min}$, the side length of the trust region is set to $L_{init}$ and restart the scheduling. Recently, LOL-BO [13] adapted trust region-based local optimization to LBO, and has shown performance gain."}, {"title": "Method", "content": "In this section, we present an Inversion-based Latent Bayesian Optimization (InvBO) consisting of an inversion and a novel trust region anchor selection method for effective and efficient optimization. We first describe latent Bayesian optimization and the misalignment problem of it (Section 4.1). Then, we introduce the inversion method to address the misalignment problem without using any additional oracle budgets (Section 4.2). Lastly, we present a potential-aware trust region anchor selection for better local search space (Section 4.3)."}, {"title": "Misalignment in Latent Bayesian Optimization", "content": "BO has proven its effectiveness in various areas where input space X is continuous, however, BO over the discrete domain, such as chemical design, is a challenging problem. To handle this problem, VAE-based latent Bayesian optimization (LBO) has been proposed [8, 11, 13, 14] that leverages BO over a continuous space by mapping the discrete input space X to a continuous latent space Z. Variational autoencoder (VAE) is composed of encoder $q_\\phi : X \\rightarrow Z$ to compute the latent representation z of the input data x and decoder $p_\\theta : Z \\rightarrow X$ to generate the data x from the latent z.\nGiven the objective function f, latent Bayesian optimization can be formulated as:\n$z^* = \\underset{z \\in Z}{\\text{arg max}} f (p_\\theta (z)),$\n(3)\nwhere $p_\\theta (z)$ is a generated data with the decoder $p_\\theta$ and Z is a latent space. Unlike the standard BO, the surrogate model g aims to emulate the function $f \\circ p_\\theta : Z \\rightarrow Y$. To the end, the surrogate model is trained with aligned dataset $D = \\{(x^i, z^i, y^i)\\}_{i=1}^n$, where $x^i = p_\\theta(z^i)$ is generated by the decoder $p_\\theta : Z \\rightarrow X$ and $y^i = f(x^i)$ is the objective value of $x^i$ evaluated via the black box objective function $f : X \\rightarrow Y$. In the rest of our paper, we define that the dataset is aligned when all triplets satisfy the above conditions (i.e., all triplets are the decoder triplets explained in Figure 1), and the dataset is misaligned otherwise. We define the 'misalignment problem' as the misaligned dataset hinders the accurate learning of the surrogate model g.\nMost existing LBO works [11\u201314] overlook the misalignment problem 1, which originates from two processes: (i) construction of initial dataset $D_0$ and (ii) update of VAE.\nConstruction of initial dataset $D_0$. Since initial dataset $D_0$ is composed of pairs of input data and its corresponding objective value $\\{(x^i, y^i)|y^i = f(x^i)\\}_{i=1}^n$, LBO requires latent vectors $\\{z^i\\}_{i=1}^n$ to train the surrogate model. Most works compute a latent vector $z^i$ as $z^i = q_\\phi(x^i)$ under the assumption that VAE completely reconstructs every data points (i.e., $p_\\theta(q_\\phi(x^i)) = x^i$), which is difficult to be satisfied in every case. This results in the data misalignment ($x^i \\neq p_\\theta(z^i)$) during the construction of initial dataset $D_0$.\nUpdate of VAE. In LBO works, updating VAE during the optimization plays a crucial role in adapting well to newly generated samples. However, due to the update of VAE, the previously generated triplet (x, z, y) cannot ensure the alignment since z was computed by the VAE before the update. Previous LBO works [13, 14] solve the misalignment problem originating from the VAE update with a recentering technique that requests additional oracle calls to generate the aligned dataset"}, {"title": "Inversion-based Latent Bayesian Optimization", "content": "Our primary goal is training the surrogate model g to correctly emulate the composite function $f \\circ p_\\theta : Z \\rightarrow Y$ via constructing an aligned dataset without consuming additional oracle calls. To the end, we propose an inversion method that inverts the target discrete data x into the latent vector z that satisfies $x = p_\\theta(z)$ for dataset alignment as shown in Figure 2 (Right). With a pre-trained frozen decoder $p_\\theta$, the latent vector z can be optimized by:\n$z_{inv} = \\underset{z \\in Z}{\\text{arg min}} d_x (x, p_\\theta(z)),$\n(4)\nwhere x is a target data and $d_x$ is a distance function in the input space X. We use the normalized Levenshtein distance [37] as our distance function, $d_x$, which can be applied to any string-form data. Our inversion method, however, is flexible and can utilize any task-specific distance functions, such as Tanimoto similarity [38] for molecule design tasks. To solve the Eq. (4), we iteratively update a latent vector z to find $z_{inv}$ that reconstructs the target data x. We provide the overall pseudocode of the inversion method in Algorithm 1.\nThe initialization strategy of latent vector z plays a key role in the optimization-based inversion process. We set the initialization point of latent vector z as an output of a pre-trained encoder $q_\\phi(x)$ given target discrete data x as in line 1. We iteratively update the latent vector z with the cross-entropy loss used in VAE training in line 3 until it reaches the maximum number of iterations T. Before the iteration budget is exhausted, we finish the inversion process when the distance between the generated data $p_\\theta(z)$ and target data x is less than $\\epsilon$ as our goal is finding the latent vector $z_{inv}$ that satisfies Eq. (4), which is denoted in line 4. The inversion method generates the aligned dataset during the construction of the initial dataset $D_0$ and the update of VAE to handle the misalignment problem.\nWe theoretically show that optimizing the latent vector z to satisfy $d_x(x, p_\\theta(z)) \\approx 0$ with inversion plays a crucial role in minimizing the upper bound of the error between the posterior mean of the surrogate model and the objective function value within the trust region centered at z.\nProposition 1. Let f be a black-box objective function and m be a posterior mean of Gaussian process, $p_\\theta$ be a decoder of the variational autoencoder, c be an arbitrarily small constant, $d_x$ and $d_z$ be the distance function on input X and latent Z spaces, respectively. The distance function $d_x$ is bounded between 0 and 1, inclusive. We assume that f, m and the composite function of f and $p_\\theta$ are $L_1$, $L_2$, and $L_3$-Lipschitz continuous functions, respectively. Suppose the following assumptions are satisfied:\n$|f(x) - m(z)| \\leq c,$\n$d_x(x, p_\\theta(z)) \\leq \\gamma.$\n(5)\nThen the difference between the posterior mean of the arbitrary point z' in the trust region centered at z with trust radius $\\delta$ and the black box objective value is upper bounded as:\n$|f(p_\\theta(z')) - m(z')| \\leq c + \\gamma \\cdot L_1 + \\delta \\cdot (L_2 + L_3),$\n(6)\nwhere $d_z(z, z') < \\delta$.\nThe proof is available in Section A. We assume that the black box function f, the posterior mean of Gaussian process m, and the objective function $f \\circ p_\\theta$ are Lipschitz continuous functions, which is a"}, {"title": "Potential-aware Trust Region Anchor Selection", "content": "Here, we propose a potential-aware trust region anchor selection to consider both the objective function value y and the potential ability of the trust region to benefit the optimization. Previous trust region-based BO works [13, 14, 36, 42] select the anchor based on the corresponding objective function value only. However, this objective score-based anchor selection does not consider that the trust region contains latent points expected to enhance the optimization performance.\nWe design a potential score to measure the potential ability to enhance the optimization process of the trust region. To compute it, we use the acquisition function value, which is generally employed to measure the potential ability to improve the optimization process of a given data point. Specifically, we employ Thompson Sampling, a well-established acquisition function used in previous trust region-based methods [13, 14, 36]. Formally, the potential score of each trust region $T_i$ is computed as follows:\n$\\alpha_{pot}^i = \\underset{z \\in Z_{cand}^i}{\\text{max}} f(z) \\text{ where } f \\sim GP (\\mu(z), k(z, z')),$\n(7)\nwhere i is an index of the candidate anchor point, $Z_{cand}^i$ is the candidate set sampled from the trust region $T_i$ and f is a sampled function from the surrogate model (e.g., GP) posterior.\nAs the scale of objective function values $Y = \\{y_1, y_2, ..., y^n\\}$ and that of the potential ability of each trust region $A = \\{\\alpha_{pot}^1, \\alpha_{pot}^2, ..., \\alpha_{pot}^n\\}$ is changed dynamically during the optimization process, we calculate a scaled potential score $\\alpha_{scaled}^i$ by adjusting the scale of $\\alpha_{max}$ according to the Y:\n$\\alpha_{scaled}^i = \\frac{\\alpha_{pot}^i - A_{min}}{A_{max} - A_{min}} \\times (Y_{max} - Y_{min}),$\n(8)\nwhere $A_{max} = max_i [\\alpha_{pot}^i]$ and $A_{min} = min_i [\\alpha_{pot}^i]$ denote the maximum and minimum value of A, respectively. $Y_{max} = max_i [y^i]$ and $Y_{min} = min_i [y^i]$ indicate the maximum and minimum value of Y, respectively. Based on the scaled potential score $\\alpha_{scaled}^i$, the final score $s^i$ of each anchor is calculated as:\n$s^i = y^i + \\alpha_{scaled}^i$\n(9)\nOur final score takes into account the objective function value of the anchor (observed value) and the potential score (model's prediction) for the better local search space. Finally, we select the anchors with the highest final score $s^i$. We summarize our potential-aware trust region anchor selection schema in Algorithm 2 of Section N."}, {"title": "Experiments", "content": "We measure the performance of the proposed method named InvBO on nine different tasks with three Bayesian optimization benchmarks: Guacamol [43], DRD3, and arithmetic expression fitting tasks [6, 8, 11-13, 44]. Guacamol and the DRD3 benchmark tasks aim to find molecules with the most necessary properties. For Guacamol benchmarks, we use seven challenging tasks, Median molecules 2 (med2), Zaleplon MPO (zale), Perindopril MPO (pdop), Amlodipine MPO (adip), Osimertinib MPO (osmb), Ranolazine MPO (rano), and Valsartan SMARTS (valt). To show the effectiveness of our InvBO in various settings, we also conducted experiments in a large budget setting, which is used in previous works [13, 14]. The goal of the arithmetic expression fitting task is to generate single-variable expressions that minimize the distance from a target expression (e.g., 1/3 + x + sin (x \u00d7 x)). More details of each benchmark are provided in Section K."}, {"title": "Analysis", "content": "6.1 Ablation Study\nWe conduct additional experiments to verify the contribution of each component in our InvBO: the inversion method (INV), and the potential-aware trust region anchor selection method (PAS). Figure 6 shows the optimization results of the ablation study on med2, zale, osmb, and valt tasks. From the figure, models with the inversion method (i.e., CoBO with INV and InvBO) replace the recentering technique as the inversion method, while models without the inversion method (i.e., vanilla CoBO and CoBO with PAS) employ the recentering technique. Notably, both components of our method contribute to the optimization process, and the combination work, InvBO, consistently obtains better objective scores compared to other models. Specifically, in osmb task, the average best score achieved by the methods with the PAS, INV and both (i.e., InvBO) shows 0.784, 0.792, and 0.804 score gains compared to vanilla CoBO, respectively.\n6.2 Analysis on Misalignment Problem and Inversion\nTo further prove that the misaligned dataset hinders the accurate learning of the surrogate model (i.e., misalignment problem), we compare the performance of the surrogate model trained with aligned and misaligned datasets on the med2 task. Figure 7 shows the fitting results of the surrogate model trained with encoder triplets ($g_{enc}$, left) and decoder triplets ($g_{dec}$, right), respectively. Figure 7 demonstrates that the surrogate model $g_{dec}$ approximates the objective function accurately, while $g_{enc}$ fails to emulate the objective function. Further details of an experiment are provided in Section I."}, {"title": "Effects of Inversion on Proposition 1", "content": "In Section 4.2, we provide the upper bound of the error between the predicted and ground-truth objective value. In Eq. (6), the Lipschitz constant of the objective function $L_1$ and the trust region radius $\\delta$ is fixed or the hyper-parameter and the constant c can be improved by learning the surrogate model. In the end, we can reduce the upper bound of the objective value prediction error by reducing the three components: $\\gamma$, $L_2$, and $L_3$, which implies the distance between x and $p_\\theta(z)$, the Lipschitz constant of function m and $f \\circ p_\\theta$, respectively. Our inversion method reduce $\\gamma$ by searching the latent vector $z_{inv}$ that satisfies $x = p_\\theta(z_{inv})$. Previously, CoBO [14] proposed regularization losses that reduce $L_3$. Since the surrogate model emulates the composite function $f \\circ p_\\theta$, these regularization losses can reduce $L_2$ along with $L_3$.\nFigure 9 (left) shows the regularization losses of CoBO and our inversion method reduces the objective value prediction error. CoBO-based models (i.e., CoBO+PAS and CoBO+InvBO) employ the CoBO regularization losses, and models with InvBO (i.e., TuRBO-L+InvBO and CoBO+InvBO) employ our inversion method. TuRBO-L does not use the CoBO regularization losses nor our inversion method, and models with PAS (i.e., TuRBO-L+PAS and CoBO+PAS) employ encoder triplets. Applying the regularization losses and our inversion method reduces the objective value prediction error, respectively, but our inversion method shows a larger error reduction. The combination of regularization losses and our inversion shows the smallest prediction error, which implies our inversion method robustly complements existing methods. We provide the optimization results of each model on the med2 task in Figure 9 (right). These results demonstrate that reducing the objective function prediction error plays a crucial role in optimization performance."}, {"title": "Comparing Diverse Anchor Selection Methods", "content": "To further prove the importance of our potential-aware anchor selection method, we perform BO with the diverse anchor selection methods: random, acquisition $\\alpha$, and objective score y. Random indicates randomly selected anchors, and acquisition and objective indicate anchors are selected based on the max acquisition function value and objective score, respectively. All models use the inversion method, and the optimization results on valt and med2 tasks are in Figure 10. Ours and objective"}, {"title": "Conclusion", "content": "We propose Inversion-based Latent Bayesian Optimization (InvBO), a plug-and-play module for LBO. We introduce the inversion method that inverts the decoder to find the latent vector for generating the aligned dataset. Additionally, we present the potential-aware trust region anchor selection that considers not only the corresponding objective function value of the anchor but also the potential ability of the trust region. From our experimental results, InvBO achieves state-of-the-art performance on nine LBO benchmark tasks. We also theoretically demonstrate the effectiveness of our inversion method and provide a comprehensive analysis to show the effectiveness of our InvBO."}, {"title": "Broader Impacts", "content": "One of the contributions of this paper is molecular design optimization, which requires careful consideration due to its unintentional applications such as the generation of toxic. We believe that our work has a lot of positive aspects to accelerate the development of chemical and drug discovery with an inversion-based latent Bayesian optimization method."}, {"title": "Limitations", "content": "The performance of the methods proposed in the paper depends on the quality of the generative model. For example, if the objective function is related to the molecule property, the generated model such as the VAE should have the ability to generate the proper molecule to have good performance of optimization."}, {"title": "Efficiency Analysis", "content": "Although Bayesian optimization assumes the objective function is cost-expensive, it is still important to consider the efficiency of the algorithm. We present an efficiency analysis comparing with baseline methods [11, 13, 14, 17], and TuRBO-L, LS-BO. We conducted experiments under the same condition for fair comparison: a single NVIDIA RTX 2080 TI with the CPU of AMD EPYC 7742. Wall-clock time comparison with the baseline model is in Table 3. The table shows that applying our InvBO to COBO achieves state-of-the-art performance not only under the same oracle calls but also under the same wall-clock time."}, {"title": "Implementation Details", "content": "Our implementation is based on the codebase of [13]. We use PyTorch\u00b3, BoTorch\u2074 [48], GPy-Torch5 [52], and Guacamol software packages. In the Guacamol tasks and DRD3 task [50], we use the SELFIES VAE [13] which is pretrained in an unsupervised manner with 1.27M molecules from Guacamol benchmark. The Grammar VAE [8] is pre-trained with 40K expression data in an unsupervised manner. The dimension of latent space is 256 in the SELFIES VAE and 25 in the Grammar VAE. The size of the data obtained from the acquisition function and k are hyperparameters, which are presented in Table 4.\nM.1 Hyperparameters Setting\nThe learning rate used in the inversion method is 0.1 in all tasks, as we empirically observe that it always finds the latent vector that generates the target discrete data. The maximum iteration number of the inversion method is 1,000 in all tasks. The rest of the hyperparameters follow the setting used in [14]. Since arithmetic fitting tasks and Guacamol with the small budget tasks use different initial data numbers used in [14], we set the number of top-k data and the number of query points Nq same as DRD3 task, as they use the same number of initial data points."}, {"title": "Pseudocode of Potential-aware Trust Region Anchor Selection", "content": "Here, we provide the pseudocode of the potential-aware trust region anchor selection method. We get the max acquisition function value, which is a Thompson Sampling, of each trust region $T^i$ given a\nCopyright (c) 2016-Facebook, Inc (Adam Paszke) [51], Licensed under BSD-style license\nCopyright (c) Meta Platforms, Inc. and affiliates. Licensed under MIT License\nCopyright (c) 2017 Jake Gardner. Licensed under MIT License\nCopyright (c) 2020 The Apache Software Foundation, Licensed under the Apache License, Version 2.0."}]}