{"title": "DOCUMENT- LEVEL EVENT EXTRACTION WITH\nDEFINITION-DRIVEN ICL", "authors": ["Zhuoyuan Liu", "Yilin Luo"], "abstract": "In the field of Natural Language Processing (NLP), large language models (LLMs) show immense\npotential in document-level event extraction tasks. However, existing methods face challenges in\nprompt design. Addressing this issue, we propose an optimization strategy called \"Definition-driven\nDocument-level Event Extraction (DDEE).\" By adjusting prompt length and enhancing heuristic\nclarity, we significantly enhance LLMs' performance in event extraction. We employ data balancing\ntechniques to mitigate the long-tail effect, improving the model's generalization in recognizing event\ntypes. Additionally, we refine prompts to ensure they are both concise and comprehensive, catering\nto LLMs' sensitivity to prompt style. Furthermore, the introduction of structured heuristic methods\nand strict constraints enhances the precision of event and argument role extraction. These strategies\nnot only address prompt engineering issues in LLMs for document-level event extraction but also\nadvance event extraction technology, offering new research perspectives for other tasks in the NLP\ndomain.", "sections": [{"title": "1 Introduction", "content": "The field of Natural Language Processing (NLP) has made significant strides in parsing and understanding human\nlanguage, with event extraction technology playing a crucial role. Event extraction identifies events and their core\nelements from unstructured text data, detailing participants (\"who\"), time (\"when\"), location (\"where\"), event description\n(\"what\"), cause (\"why\"), and manner (\"how\"). Accurately extracting this information is essential for tasks such as text\nsummarization, knowledge graph construction, intelligent question answering systems, and recommendation systems.\nEvent extraction consists primarily of two subtasks: Event Detection (ED) and Event Argument Role Extraction (EAE).\nEvent Detection aims to identify mentioned events in text, while Event Argument Role Extraction further identifies\nentities involved in the events and their respective roles. Currently, using large-scale pre-trained language models\n(LLMs) for closed-domain document-level event extraction has become mainstream in the field.\nIn the field of event extraction in Natural Language Processing (NLP), despite significant advancements, several critical\nissues remain to be addressed. Firstly, pre-trained and fine-tuned models exhibit shortcomings in generalizing to unseen\nevents, limiting their application in new scenarios. Secondly, existing methods heavily rely on high-quality annotated\ndata, leading to high annotation costs and challenges in data-scarce domains. Additionally, error propagation during\nevent extraction can cause cascading effects, impacting result accuracy. The complex syntactic structures in Chinese\nsentences, phenomena of arguments spanning multiple sentences, co-occurrence of multiple events, and issues with\ntrigger word absence further exacerbate extraction challenges. Furthermore, instability in contextual learning (ICL)\nstrategies, influenced by various factors, restricts predictive performance of models."}, {"title": "2 Background", "content": "The application of large language models in event extraction tasks. Current research indicates that while large\nlanguage models perform well in simple event extraction tasks, their robustness and accuracy still need improvement\nwhen faced with complex and long-tail scenarios. To address the diverse and complex application requirements\nin real-world settings, researchers are focusing on optimizing model performance through well-designed prompts,\nmulti-turn dialogue approaches, and integration of role knowledge.\nThe use of large language models (LLMs) in natural language processing (NLP) tasks has significantly increased,\nespecially with closed models like PaLM [1], Claude [2], and GPT-4 [3]. However, despite their satisfactory\nperformance in simple scenarios, their robustness and accuracy in complex tasks still require enhancement. Jun Gao"}, {"title": "3 Approach", "content": "In this study, we adopted a phased approach to optimize the event extraction process, with a particular emphasis on\ndata balancing, as shown in Figure1. Our task was meticulously divided into two key steps: event detection and trigger\nword identification, and argument role recognition. This two-step strategy not only enhances task accuracy but also\nprovides vital contextual support for argument role recognition in the second step by effectively utilizing historical\ninformation identified in the first step. The outputs of event detection and trigger word identification directly feed into\nthe argument role recognition stage as part of the input information, helping the model consider the overall context of\nevents when identifying arguments, thereby establishing coherence between different stages and effectively preventing\nerror propagation, as shown in Figure2. In terms of data balancing, we employed a combination of undersampling and\noversampling techniques to ensure representativeness of various event types in the dataset, reducing the impact of the\nlong-tail effect on model performance. This balancing strategy is crucial for improving the model's generalization\nability, especially when dealing with real-world data distributions. Regarding prompt design, we introduced an\ninnovative heuristic approach that goes beyond traditional input-output specifications. We constructed a detailed\nframework for events and their argument roles, providing clear definitions, as shown in Figure3 and Figure4. These\ndefinitions were automatically generated by large-scale language models using original corpora, leveraging the models'\ndeep understanding of language and rich corpora to generate accurate and comprehensive descriptions of events and\nargument roles, thereby providing a rich and consistent reference baseline for the model.\nFurthermore, building upon the heuristic approach, we provided chain-of-thought examples to guide the model through\nstep-by-step reasoning. These examples not only demonstrate how to break down complex problems into more\nmanageable sub-problems but also simulate human thought processes, providing a logical path to finding solutions.\nThis helps the model learn how to identify key parts of problems, reason based on known information, and continuously\nrefine and adjust its thought process during reasoning.\nThrough the integrated application of these methods, our aim is to enhance the performance of event extraction tasks and\nstrengthen the model's ability to grasp complex relationships between events and arguments. This not only improves\naccuracy but also enhances the model's generalization and adaptability. Our research process and results demonstrate"}, {"title": "4 Experiments", "content": "The WikiEvents dataset[35] is a resource created to advance research in document-level event extraction, as shown\nin Table1. It gathers real-world event content from Wikipedia and related news reports, providing detailed textual\ninformation about events. Based on the ontology of the KAIROS project, this dataset annotates 67 event types and\nbuilds a multi-level event category system. It comprises 206 documents, 5262 sentences, and 3241 events, covering\n49 event types and 57 argument types. The richness and complexity of the WikiEvents dataset offer researchers\nnew opportunities to develop more refined and efficient event extraction models, enabling deeper understanding and\nprocessing of real-world events."}, {"title": "5 Analysis", "content": "This table summarizes the performance of various natural language processing\nmodels on the WikiEvent dataset for event extraction. The headers succinctly outline the key evaluation elements:\n\"WikiEvent\" specifies the dataset, \"Model\" showcases the evaluated models, and \"Language model\" specifies the\nunderlying language models used by each model. The columns \"Trig-C\" and \"Arg-C\" respectively record the models'\nperformance in trigger word recognition and classification, and argument recognition and classification, using F1 scores\nas a unified evaluation metric. The table includes models fine-tuned systems and unsupervised methods employing\ndirect context learning, covering diverse technical approaches such as OntoGPT, ChatGPT, schema-aware EE, and\nvarious zero-shot methods. These models leverage language models such as \"gpt-4\", \"ChatGPT\", \"Qwen-turbo\", and\n\"gpt-4-turbo\".\nIn this study, we conducted a detailed analysis of different models' performance on the balanced WikiEvents dataset for\nevent extraction tasks. We focused particularly on the gpt-4 language model and several zero-shot learning methods to\nevaluate their effectiveness in specific tasks, as shown in Table2 and Figure6.\nOur model (DDEE), based on gpt-4, achieved a Trigger Recognition (Trig-C) score of 31.47% and Argument Role\nRecognition (Arg-C) score of 24.19%. This result indicates that our model performs steadily on the task, but there is\nroom for further optimization compared to top-performing models.\nWe also tested several LLMs methods, including different configurations of Qwen-turbo and Gpt-4-turbo. Qwen-turbo\nscored 25.93% on Trigger Recognition and 20.13% on Argument Role Recognition. Gpt-4-turbo achieved a high score\nof 45.21% on Trigger Recognition and 27.33% on Argument Role Recognition. However, the DDEE+Cot configuration\nof Gpt-4 scored poorly with 11.50% on Trigger Recognition and 23.78% on Argument Role Recognition, indicating\nsuboptimal performance.\nIt is noteworthy that while Gpt-4-turbo achieved high scores in Trigger Recognition, the performance of the DDEE+Cot\nconfiguration was below expectations. This suggests that the Cot method may not always bring significant performance"}, {"title": "6 Conclusion", "content": "This study addresses the prompt design challenges faced by large language models (LLMs) in document-level event\nextraction tasks and proposes an innovative optimization strategy. Through experiments, we found that adopting data\nbalancing techniques significantly enhances the model's ability to generalize event type recognition, while finely tuned\nprompt designs effectively address LLMs' sensitivity to prompt styles. Additionally, the introduction of structured\nheuristic methods and strict constraints further improves the precision of event extraction. Our model demonstrates stable\nperformance on the balanced WikiEvents dataset. While there is room for improvement compared to top-performing\nmodels, it has shown promising potential and application prospects.\nThis research not only advances event extraction technology but also provides new research perspectives and method-\nologies for other tasks in the NLP field. Looking ahead, we plan to apply these strategies to more diverse datasets to\nfurther validate and optimize the model's generalization and adaptability. We believe that through continuous model\noptimization, improvements in prompt design, and exploration of more efficient training strategies, our research will\nachieve higher accuracy and efficiency. Furthermore, we anticipate that these research findings will inspire innovative\napproaches and solutions for applying LLMs in NLP tasks, particularly in enhancing robustness and flexibility when\nhandling diverse and complex datasets."}]}