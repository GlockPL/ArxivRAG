{"title": "Acceptable Use Policies for Foundation Models", "authors": ["Kevin Klyman"], "abstract": "As foundation models have accumulated hundreds of millions of users, developers have begun to take steps to prevent harmful types of uses. One salient intervention that foundation model developers adopt is acceptable use policies-legally binding policies that prohibit users from using a model for specific purposes. This paper identifies acceptable use policies from 30 foundation model developers, analyzes the use restrictions they contain, and argues that acceptable use policies are an important lens for understanding the regulation of foundation models. Taken together, developers' acceptable use policies include 127 distinct use restrictions; the wide variety in the number and type of use restrictions may create fragmentation across the AI supply chain. Developers also employ acceptable use policies to prevent competitors or specific industries from making use of their models. Developers alone decide what constitutes acceptable use, and rarely provide transparency about how they enforce their policies. In practice, acceptable use policies are difficult to enforce, and scrupulous enforcement can act as a barrier to researcher access and limit beneficial uses of foundation models. Nevertheless, acceptable use policies for foundation models are an early example of self-regulation that have a significant impact on the market for foundation models and the overall AI ecosystem.", "sections": [{"title": "1. Introduction", "content": "Policymakers hoping to regulate foundation models have focused on preventing specific objectionable uses of AI systems, such as the creation of bioweapons [113], deep-fakes [25], and child sexual abuse material [159]. Effectively blocking these uses can be difficult in the case of foundation models-large AI models trained on broad data that can be adapted to a wide range of downstream tasks as they are general-purpose technologies that in principle can be used to generate any type of content [12]. Yet developers of foundation models have been proactive, adopting broad policies as part of their terms of service or model licenses that prohibit many potentially dangerous uses of the technology.\nFoundation model developers have taken several approaches to adopting legally binding use restrictions. [106] find that developers of open-weight foundation models increasingly distribute these models with licenses that include a standardized set of behavioral use restrictions. Developers of closed-weight models have also restricted how users can make use of their models, often via terms of service agreements that prohibit generating specific categories of content [17]. Developers often refer to policies that include legally binding use restrictions on foundation models as acceptable use policies (AUPs), as they determine the domains of use that are acceptable and prohibited.\nThis paper collates and analyzes the acceptable use policies of 30 foundation model developers in order to assess their impact. It addresses the following question: what do acceptable use policies reveal about the ways that foundation model developers seek to regulate end-user behavior, and how do they impact the foundation model ecosystem?\nThe paper proceeds as follows: Section 2 provides background on acceptable use policies for foundation models, comparing them to similar policies for other technologies and to documents like model cards (which list out-of-scope uses but are not legally binding). Section 3 describes the methodology used to identify acceptable use policies and analyze their content. Section 4 analyzes the differences between developers' policies in terms of prohibited content and restrictions on types of end use. Section 5 outlines difficulties in policy enforcement and potential downsides from strict enforcement. Section 6 discusses developers' decision-making power, how gaps in use restrictions may facilitate misuse, and how acceptable use policies shape the foundation model market. Section 7 identifies areas for future work."}, {"title": "2. Background", "content": ""}, {"title": "2.1 What is an acceptable use policy?", "content": "Acceptable use policies are common across digital technologies [117]. Providers of public access computers [133], websites [151, 172], and digital platforms [123, 144] have long adopted acceptable use policies that articulate how their terms of service restrict what users can and cannot do with their products and services. While enforcement of these policies is uneven, restrictions on specific uses of digital technologies are widespread [39, 134].\nThe acceptable use policies of social media companies [24], cloud service providers [64], and content delivery networks [97] have received scrutiny as they constrain the behavior of hundreds of millions of users. Acceptable use policies adopted by employers, which limit employees' use of company-provided technologies [93], schools, which limit students' use of the internet [1], and public libraries, which limit the public's use of public access computers [107], have come into focus as issues related to enforcement arise.\nIn the context of foundation model development, an acceptable use policy is a policy from a developer that determines how a foundation model can or cannot be used. Acceptable use policies restrict the use of foundation models by detailing the types of content users are prohibited from generating as well as domains of prohibited use. Developers make these restrictions legally binding by including acceptable use policies in terms of service agreements or in copyright licenses for their foundation models.\nAcceptable use policies typically aim to prevent users from using a foundation model to generate content that may violate the law or otherwise cause harm. They accomplish this by listing specific subcategories of violative content and authorizing model developers to punish users who generate such content by, for example, limiting the number of queries users can issue or banning a user's account.\nAcceptable use policies relate to how foundation models are built in important ways. For example, developers frequently filter training data to remove content relevant to requests that would violate their acceptable use policies. OpenAI's GPT-4 technical report states: \"We reduced the prevalence of certain kinds of content that violate our usage policies (such as inappropriate erotic content) in our pre-training dataset, and fine-tuned the model to refuse certain instructions such as direct requests for illicit advice\" [121].\nIn addition, many developers state that the purpose of reinforcement learning from human feedback (RLHF) is to make their foundation models less likely to generate outputs that would violate their acceptable use policies [89]. Meta's technical report for Llama 2 notes that the risks RLHF was intended to mitigate include \"illicit and criminal activities (e.g., terrorism, theft, human trafficking); hateful and harmful activities (e.g., defamation, self-harm, eating disorders, discrimination); and unqualified advice (e.g., medical advice, financial advice, legal advice),\" which correspond to the acceptable use policy in Llama 2's license [160]. Anthropic's model card for Claude 3 similarly says \"We developed refusals evaluations to help test the helpfulness aspect of Claude models, measuring where the model unhelpfully refuses to answer a harmless prompt, i.e. where it incorrectly categorizes a prompt as unsafe (violating our AUP) and therefore refuses to answer\" [3]."}, {"title": "2.2 How do AUPs differ from other similar documents?", "content": "Acceptable use policies are not the only way developers restrict use of their models. Other policy-related mechanisms that developers implement to restrict model use include:\n\u2022 Model Cards: Model cards, which are published alongside machine learning models, provide essential information about models such as their intended uses and out-of-scope uses [112]. However, model cards are not enforceable contracts, and they are not generally referenced in model licenses or developers' terms of service; as a result, out-of-scope uses do not rise to the same level as prohibited uses in an acceptable use policy [94].\n\u2022 Model Behavior Policies: Model behavior policies determine what a model can or cannot do [5, 59, 120]. While acceptable use policies apply to user behavior, model behavior policies apply to the behavior of the model itself [17]. A model behavior policy is one way of embedding an acceptable use policy into a model; methods for imposing a model behavior policy include using RLHF to cause the model to be more likely to refuse violative prompts or employing a safety classifier at inference time to filter violative model outputs [18, 28, 75]. Model behavior policies are generally broader than acceptable use policies; for instance, many developers fine-tune their models to produce more polite responses, though they do not block users from generating impolite responses [126, 136].\n\u2022 Third party contracts: Foundation model developers frequently partner with other firms to disseminate foundation models [23]. These include cloud service providers (e.g., AWS, Azure, GCP), platform providers (e.g., Scale AI, Nvidia), database providers (e.g., Salesforce, Oracle), and model distributors (e.g., Together, Quora) [149]. Custom contracts with third party providers of a developer's foundation models often include use restrictions, but the extent to which companies' acceptable use policies are altered via these partnership agreements is unclear."}, {"title": "2.3 Norms and laws on acceptable use policies", "content": "Although generative AI is a nascent industry, norms have begun to emerge around use restrictions for foundation models. [29] wrote in their \"Best practices for deploying language models\" that organizations should \u201c[p]ublish usage guidelines and terms of use of LLMs in a way that prohibits material harm...such as through spam, fraud, or astroturfing.\u201d Developers of open-weight foundation models often adopt the same acceptable use policies by reusing the same model licenses. For example, more than 3,000 models on Hugging Face use Meta's Llama 2 license [106].\nGovernments have taken an interest in acceptable use policies, which are a salient effort by foundation model developers to self-regulate [50]. Annexes IXa and IXb of the EU AI Act require that all providers of general-purpose AI models disclose the \"acceptable use policies [that are] applicable\" to both the EU's AI Office and other firms that integrate the general-purpose AI model into their own AI systems [65, 162]. China's Interim Measures for the Management of Generative AI Services, which were adopted in July 2023, go a step further by requiring that providers of generative AI services act to prevent users from \"using generative AI services to engage in illegal activities... including [by issuing] warnings, limiting functions, and suspending or concluding the provision of services\" [36, 181]. And the US Voluntary AI Commitments require firms to publicly report \"domains of appropriate and inappropriate use\" as well as any limitations of the model that affect these domains [174].\nNeither the EU AI Act nor the US Voluntary AI Commitments require that firms enforce their AUPs or restrict any particular uses. By contrast, China's February 2024 regulatory guidance on Basic Safety Requirements for Generative Artificial Intelligence Services specifies 31 safety risks that developers must prohibit, such as \"subvert[ing] state power,\" \"endanger[ing] national security,\" and \"dissemination of false and harmful information\" [118, 179]."}, {"title": "3. Methodology", "content": ""}, {"title": "3.1 Search protocol for acceptable use policies", "content": "Table 1 details 30 foundation model developers' acceptable use policies. Developers use different policy documents to limit model use, including: a standalone acceptable use policy for all their foundation models (e.g., Google, Stability AI), use restrictions included in a general model license (e.g., AI2), use restrictions included in a custom model license (e.g., BigScience, Meta), or provisions in terms of service agreements that apply to all services including foundation models (e.g., Midjourney, Perplexity, Eleven Labs).\nThe following protocol was used to identify acceptable use policies across these different types of documents:\n1. Compile a list of foundation model developers using the data provided by [15].\n2. For each developer, check the terms of service (TOS) on its website. If the TOS include an AUP with content restrictions that plausibly cover the developer's foundation models, take that portion of the TOS as the AUP.\n3. For each remaining developer, check the license for its \"flagship foundation model\u201d; if it includes behavioral use restrictions, take that portion of the license as the AUP.\n4. For each remaining developer, if the TOS or license reference a separate document with behavioral use restrictions (e.g., usage guidelines) such that the restrictions are binding, take the relevant portion of that document as the AUP."}, {"title": "3.2 Coding of prohibited use categories in AUPS", "content": "Qualitative content analysis was used for this paper's coding of prohibited use categories in developers' acceptable use policies [105]. This was done inductively [46], with categories drawn directly from acceptable use policies, and was inspired by prior work related to AI ethics guidelines [51, 77], privacy policies [2], content moderation guidelines [24], benchmarks [167], and Responsible AI Licenses [106].\nThe following process was used to code the prohibited use categories included in developers' acceptable use policies:\n\u2022 For each acceptable use policy, each line of the policy was analyzed. For each line, the distinct prohibited use categories included were added to a list of prohibited uses across every developers' acceptable use policy. Distinct prohibited use categories do not include different types of actions related to the same prohibited use category (e.g., \"generating, promoting, or further distributing spam\" was coded as \"spam\") or categories with substantial overlap that do not use distinct phrasing.\n\u2022 Using the list of prohibited use categories across all AUPs, each line of each acceptable use policy was considered again to ensure the prohibited use categories therein are coded correctly. A prohibited use category should receive a specific coding only if it uses near-identical language to that coding, and each prohibited use category in each policy receives only one coding.\nThis produced a list of 127 categories and a 30x127 matrix (visible on GitHub), where columns show foundation model developers, rows show prohibited use categories, and cells are marked \"1\" if a developer's acceptable use policy explicitly references that prohibited use category and \"0\" otherwise. Section 4 analyzes the results of this coding.\nThis methodology satisfies three aims. First, it provides a systematic and comprehensive approach for capturing the prohibited use categories included in acceptable use policies. Second, it enables a granular analysis of acceptable use policies. Classifying prohibited use categories into higher-level groups is an illustrative exercise (see Figure 1), but acceptable use policies are legal documents with unique provisions that require close study [117]. Third, it clarifies the risks from foundation models that developers themselves seek to mitigate. While many previous works have taxonomized the risks and harms stemming from foundation models [4, 49, 71, 101, 141, 171], this paper assesses how companies taxonomize risk on the basis of their own policies."}, {"title": "4. Analysis of acceptable use policies", "content": ""}, {"title": "4.1 Developers with acceptable use policies", "content": "Foundation model developers that have AUPs are heterogeneous along multiple axes, demonstrating broad adoption (see Table 1). In terms of model release, 12 of the developers openly release the model weights for their flagship model series, while 18 do not. These models have a variety of different output modalities, with 20 language models, 4 multimodal models, 3 image models, 2 video models and 1 audio model. The developers are headquartered around the world, with 19 based in the US and the others based in Canada, China, France, Germany, Israel, and the UAE."}, {"title": "4.2 Prohibited content in acceptable use policies", "content": "Acceptable use policies commonly prohibit users from employing foundation models to generate content that is explicit (e.g., violence, pornography), fraudulent (scams, spam), abusive (harassment, hate speech), deceptive (disinformation, impersonation), or otherwise harmful (malware, privacy infringements). Figure 1 shows the most common categories of content that are explicitly prohibited by developers' acceptable use policies: mis/disinformation (26 policies include explicit prohibitions), harassment/abuse (26), privacy (21), discrimination (21), and child harm/child sexual abuse material (21) were the most frequent, while categories like political content (9), medical advice (8), weapons (7), surveillance (7), and plagiarism (4) were less common.\nMany developers' acceptable use policies have granular use restrictions, whereas others have broad restrictions without much elaboration. Figure 1 shows the number of prohibited use categories contained in each developers' acceptable use policy and distinguishes between open- and closed-weight developers [81]. Among closed developers, the acceptable use policies of Anthropic (69 prohibited uses), Cohere (46), and OpenAI (46) explicitly reference the largest number of prohibited use categories, while the policies of smaller startups such as Reka (15), Writer (14), and Perplexity (12) have the fewest. Among open developers, the acceptable use policies of Stability AI (44), Meta (44), and Mistral (38) explicitly reference the largest number of prohibited use categories, while the AUPs of 01.ai (11), Together (7), and the Technology Innovation Institute (6) reference the fewest.\nThe average number of prohibited uses for closed developers is 20 (standard deviation of 15.1), while the average for open developers is 24.5 (standard deviation is 13.5).\nThere are several potential explanations for open developers having a larger number of prohibited use categories in their AUPs. Open foundation model developers often use Responsible AI Licenses that feature a sizable, standardized set of use restrictions [33, 85]. Second, a greater number of closed foundation model developers have acceptable use policies (including smaller companies without large legal teams), whereas many other open developers have no acceptable use policy (see Table 2), introducing potential selection bias in computing the average. Third, unlike closed developers, open developers often cannot enforce their acceptable use policies against individual users, so prohibiting a larger number of uses may come at less cost.\nThe strength of an acceptable use policy is not determined solely by the number of prohibited uses it lists. All 30 acceptable use policies prohibit users from generating content that violates the law, and the majority prohibit users from generating content that impedes the model developer's operations or is not accompanied by adequate disclosure that it is machine-generated. These catch-all prohibitions cover un-enumerated risk categories, making acceptable use policies more malleable and comprehensive by linking them to laws and organizational procedures that may change. Over 40 of the 127 prohibited use categories relate to potentially illegal content (e.g., child sexual abuse material, defamation, discrimination against a protected class, drugs, fraud, hate speech, malware, prostitution, scams), reflecting the fact that developers consider these to be risks associated with their models and wish to reduce their liability for such risks [92].\nProhibitions on content that is not generally illegal show developers' priorities and highlight different approaches taken in their acceptable use policies. Political content, such as using foundation models for campaigning, lobbying, or otherwise influencing political processes, is explicitly prohibited by 9 startups-Anthropic, Character.AI, Cohere, Databricks, Midjourney, OpenAI, Perplexity, Stability AI, and Twelve Labs-whereas Big Tech companies like Amazon, Google, and Meta have no such prohibitions. Weapons-related content is explicitly prohibited by 7 developers: AI2, Anthropic, Amazon, Meta, Mistral, OpenAI, and Stability AI. Generating eating disorder-related content, such as pro-anorexia content, is explicitly prohibited by just 4 developers: Character.AI, Cohere, Meta, and Mistral. And while some open developers such as Adept, DeepSeek, and Together broadly prohibit some types of sexual content, others like Meta and Mistral prohibit only content related to prostitution or sexual violence. Foundation models have the potential to cause harm in each of these areas, yet major developers choose not to adopt legally binding restrictions on using their models in these ways [57, 140, 154].\n\u2022 Other notable prohibited uses include:\nUndermining the interests of the state: Baidu and DeepSeek, two of three model developers in Table 1 headquartered in China, state in their acceptable use policies that users must not generate content \u201cendangering national security, leaking state secrets, subverting state power, overthrowing the socialist system, and undermining national unity... damaging the honor and interests of the state... undermining the state's religious policy\u201d. 01.ai, the other Chinese developer, also includes a prohibition against \"harming national security.\u201d These restrictions draw directly on China's Basic Safety Requirements for Generative AI Services [179].\n\u2022 Password trafficking: Eleven Labs, the only developer in Table 1 whose flagship model outputs audio, prohibits users from using its models to \"trick or mislead us or other users, especially in an attempt to learn sensitive account information, for example user passwords.\u201d This may be intended to address concerns regarding the use of voice cloning for scams [6, 101].\n\u2022 Misinformation: The extent to which developers restrict users' ability to generate and/or distribute inaccurate content varies widely. While some AUPs include wholesale bans on misinformation (e.g., AI21 Labs, Inflection), others have looser restrictions that apply only to verifiable disinformation with the intent to cause harm (e.g., TII). Mis/disinformation is the most frequently prohibited category of use\u2014even more so than child sexual abuse material-indicating that some developers may be more responsive to political and reputational risk than assessments of harm or legal liability [54, 124, 158]."}, {"title": "4.3 Restrictions on types of end use", "content": "In addition to content-based restrictions, acceptable use policies for foundation models often restrict the types of activity that users can carry out. Acceptable use policies from 6 developers prohibit \u201cmodel scraping\" or training a model on their own model's outputs. Anthropic's Acceptable Use Policy bans use of \"prompts and results to train an AI model (e.g., 'model scraping')\u201d; Adept, Adobe, Meta, Perplexity, and Runway similarly prohibit the use of model outputs for training other foundation models. While 8 developers have no such explicit ban (BigCode, BigScience, Character.AI, Eleven Labs, Mistral, Stability AI, TII, Reka), the remaining 16 prohibit the use of their models to build a competing service, which encompasses model scraping [109].\nSome developers prohibit using their models to distribute AI-generated content at scale. AI21 Labs' Usage Guidelines state that \"No content generated by AI21 Studio will be posted automatically (without human intervention) to any public website or platform where it may be viewed by an audience greater than 100 people.\" Four other developers (BigCode, BigScience, Cohere, and Databricks) prohibit using their models for automated posting online [58].\nMany acceptable use policies prevent firms in certain industries from making use of foundation models. For example, weapons manufacturers would be in violation of a policy with weapons-related restrictions if they made use of the foundation model to produce weapons, though it is possible that the developer negotiates custom contracts with weapons manufacturers [19, 145]. In January 2024, OpenAI reportedly changed its Usage Policies to facilitate partnerships with militaries, deleting a line that prohibited use related to \"military and warfare\" [10, 20].\nAcceptable use policies may also restrict the use of models in highly-regulated industries such as law, finance, and medicine. 8 of the 30 acceptable use policies include restrictions on medical advice, and Anthropic, Character.AI, Google, Meta, and OpenAI also have restrictions on legal and financial advice, which apply not only to lawyers, doctors, and financial advisers, but also to organizations that provide services in these fields [37, 108, 157].\nAI2, Amazon, Anthropic, Google, and OpenAI also prohibit use of their models for certain types of surveillance. Google prohibits use of its models for \u201ctracking or monitoring people without their consent\u201d while AI2 singles out \"military surveillance.\u201d This could prevent spyware companies and defense and intelligence contractors respectively from making use of their foundation models [48, 80]."}, {"title": "4.4 Correlations between developers' AUPs", "content": "Despite increased standardization across open developers [106], acceptable use policies remain inconsistent across foundation model developers. Figure 2 shows the correlation between developers' acceptable use policies based on which of the 127 prohibited use categories they include. BigCode, BigScience, and Databricks have highly similar policies (with a correlation of more than 0.9), as do Baidu and DeepSeek (the two Chinese developers) and Reka, TII, and Together (developers with relatively few prohibited use categories). Anthropic, Cohere, Google, Meta, Mistral, OpenAI, and Stability AI are among the developers with the policies that are least similar to others, in part because they have the largest number of prohibited uses; each have a correlation of 0.7 or less with 15 or more other developers.\nThis may pose an issue for cloud providers that distribute models from many developers; Amazon Web Services, for example, distributes models from AI21 Labs, Amazon, Anthropic, Cohere, Meta, Mistral, and Stability AI, but Anthropic's acceptable use policy has a correlation of less than 0.6 with those each of these other developers, indicating AWS would need to enforce several substantially different policies."}, {"title": "4.5 Developers without acceptable use policies", "content": "There are tens of developers that do not have acceptable use policies for their foundation models-table 2 provides seven examples. There are myriad reasons why a developer may choose to release a model without acceptable use policy. Some open foundation model developers do not use acceptable use policies because their models are intended for research purposes only\u2014if they were to adopt use restrictions, it could deter researchers from conducting safety research through red teaming or adversarial attacks (in the absence of a safe harbor for good faith researchers) [7, 96]. Other models intended for research may lack acceptable use policies on the basis that they present less severe risks of misuse, whether because they have less significant capabilities or fewer users [45]. Non-commercial models such as these are frequently distributed using licenses without use restrictions such as Apache 2.0 or Creative Commons Attribution-NonCommercial licenses [95, 173]. While a license may not include any use restrictions for noncommercial users, commercial users may have to agree to custom use restrictions in their contracts with the model developer, which are not public. This creates a potential information asymmetry where a developer and its clients are aware of the domains in which use is permitted, while regulators and the public may be led to believe that model use is unrestricted [66, 161].\nFoundation models available for commercial use may not include acceptable use policies for several reasons. In some cases, developers offer a model \"as is,\u201d stating that it is not intended for commercial use without further fine-tuning, mitigations, or use restrictions by downstream developers (e.g., Databricks' MPT-30B). Developers hoping to maximize uptake among commercial users may be less likely to adopt acceptable use policies because clients' risk-averse legal teams could recommend using different models without such restrictions. Other developers release their models without complete documentation, whether because they intend to release an acceptable use policy at a later point, which could be part of staged release, or due to under-documentation in the rush to release a model [111, 147].\nIn any case, other restrictions may apply to foundation models without acceptable use policies. Alibaba Cloud restricts firms with over 100 million users from making use of Qwen-VL through its license, which also bans model scraping. Restrictions on who can use a foundation model may have a significant effect on how it is used even in the absence of legally binding behavioral use restrictions [16]."}, {"title": "5. Enforcement of acceptable use policies", "content": ""}, {"title": "5.1 Barriers to enforcement", "content": ""}, {"title": "5.1.1 Practical and legal barriers for open developers", "content": "The enforceability of open foundation model developers' acceptable use policies is a major limitation on how effective they are at restricting risky uses. Unlike closed foundation model developers, whose models are distributed via their own products, services, or APIs (or those of another firm), developers of open foundation models distribute their models by distributing the weights online such that they can be downloaded, and models are often run locally [147]. As a result, open developers have few ways of monitoring downstream use of their models, making it difficult for them to enforce their policies where models are run locally or where hosted inference is provided by another organization [149].\nIf open foundation model developers were to attempt to enforce their acceptable use policies, many would face substantial legal barriers. Licenses for open-weight foundation models that include behavioral use restrictions are a type of copyright license, but it is unclear if machine learning models are copyrightable artifacts, calling into question the enforceability of such licenses [69, 90]. [43] argues that even if Responsible AI Licenses for models do not trigger copyright issues, the use restrictions in these licenses are ineffective as licensees are not required to enforce them against downstream licensees and developers themselves cannot sue downstream licensees for violations. Licenses for open-weight models also face issues related to interoperability, as use restrictions may not propagate to software that receives inputs from the model [44].\nOn the other hand, private sector licensees will likely comply with acceptable use policies of open-weight foundation models due to the legal risk associated with noncompliance. In cases where an open developer does not seek to enforce its acceptable use policy, the policy can still encourage responsible use [125]. Most users are not bad actors and may adhere to a policy despite gaps in enforcement, as they have no interest in generating prohibited content.\nDespite these challenges, many open foundation model developers attempt to restrict the use of their models to some degree. 12 of the developers that have acceptable use policies openly release their flagship model's weights, but do so using licenses or terms of service that prohibit certain unacceptable uses. Although open foundation models are frequently referred to as \u201copen-source\" in popular media, truly open-source software or machine learning models cannot have use restrictions by definition [44, 119]."}, {"title": "5.1.2 Ecosystem barriers", "content": "Another issue in gauging the enforcement of acceptable use policies is the way in which they propagate across the foundation model ecosystems. In addition to developers, cloud service providers (e.g., AWS, Azure, GCP) and other digital platforms (e.g., Salesforce, Scale AI) act as deployers of foundation models that were not developed in-house. Deployers have their own acceptable use policies for their platforms that do not align perfectly with external developers' acceptable use policies, and it is not clear that a deployer would have adequate expertise to restrict the uses of a foundation model in accordance with an acceptable use policy that is more stringent than that of the deployer [64]. In particular, deployers would need to build infrastructure to support enforcement of the distinct acceptable use policies for each of the foundation models they distribute. While there are a variety of publicly available models and tools that deployers might leverage to enforce developers' acceptable use policies (e.g., by filtering specific categories of prompts and responses), there is little evidence deployers have done so.\nAs an alternative, a deployer may attempt to devise (and enforce) its own acceptable use policy that encompasses those of each of its developer partners. However, the large variation in prohibited use categories among different developers' acceptable use policies makes such an exercise difficult, and would require that for each category the deployer apply the most restrictive of its partners' acceptable use policies to every model. [60] find that model marketplaces such as Hugging Face and GitHub have struggled to enforce their own acceptable use policies in light of the challenge of moderating the distribution of thousands of machine learning models, each of which may come with its own use restrictions [56, 74].\nThese challenges are made more stark by the ease with which users can circumvent technical measures used to enforce acceptable use policies. [178] show that including uncommon dialects and appeals to authority in prompts can cause a foundation model to violate its developer's acceptable use policy despite safety filters in APIs. In addition, [127] find that fine-tuning foundation models via an API can remove safety measures like instruction tuning and RLHF such that models will more readily violate their developer's acceptable use policy. Other researchers have found many vulnerabilities that allow users to nullify measures intended to promote adherence to acceptable use policies, such as adversarial prompts [104, 131], jailbreaks [138, 142, 169, 184], and other methods for fine-tuning away safety measures via APIs [177, 180]. These vulnerabilities show that closed developers are likely unable to enforce their acceptable use policies in many cases [70]."}, {"title": "5.1.3 Misallocating responsibility to users", "content": "Acceptable use policies are a means of shifting responsibility (and liability) for risky uses of a technology from the developer, deployer, or distributor of that technology to the user [39, 172]. Acceptable use policies may be effective in limiting the behavior of corporate users, which are legally risk-averse, but are unlikely to fundamentally change the behavior of the average individual user [165].\nDevelopers' approach to indemnification crystallizes the issue. Meta's Llama licenses, for example, hold users responsible for any direct or downstream use of the model, stating \"[y]ou will indemnify and hold harmless Meta from and against any claim by any third party arising out of or related to your use or distribution of the Llama Materials.\u201d\nSocial media companies' content policies also shift responsibility for toxic content from the platform that algorithmically amplifies such content to individual users that post it [84]. The same can be said of AI ethics guidelines, which often provide guidance to users regarding how to ethically use a company's AI systems rather than describing the tangible steps a company will take to prioritize ethics above other aims [26, 51]. Similarly, developers employ acceptable use policies to eschew responsibility for downstream impacts of the foundation models they choose to build and deploy.\nAcceptable use policies often impose obligations on users that they are ill-equipped to uphold. Setting aside issues of digital literacy [115], the user is often not the right party to be responsible for ensuring that a foundation model is not generating violative outputs [34]. For instance, holding users responsible for generating self-harm related content may be viable for users that maliciously seek to spread such content online, but not for vulnerable users seeking to harm themselves and who turn to a foundation model for aid [62].\nOne solution that developers implement is increasing surveillance of their users to monitor dangerous prompts and responses. [132] argues surveillance is a fundamental feature of acceptable use policies, as they are leveraged by powerful institutions as a mode of control over their subjects. Enforcing acceptable use policies often requires developers to monitor users' interactions with foundation models closely, which could facilitate privacy breaches if data protection is inadequate [83, 166]."}, {"title": "5.2 Potential negative externalities of enforcement", "content": ""}, {"title": "5.2.1 Restricting researcher access", "content": "[96] find that of seven major foundation model developers with acceptable use policies, none provide comprehensive exemptions for researchers. Platforms that distribute foundation models may rate limit or ban accounts that violate acceptable use policies, even if those accounts belong to researchers, meaning that acceptable use policies can act as a disincentive against carrying out adversarial red teaming. Concerns regarding restrictions on researcher access led over 350 researchers and advocates to sign an open letter calling for companies to refrain from disproportionate enforcement of their acceptable use policies in such cases."}, {"title": "5.2.2 Case studies of AUPs preventing beneficial uses", "content": "Strict acceptable use policies can inadvertently prevent a wide variety of beneficial uses of foundation models. Acceptable use policies do not permit users to generate prohibited content when doing so would likely be net beneficial in a specific context or circumstance, meaning that they function as a blanket ban on certain types of content [146"}]}