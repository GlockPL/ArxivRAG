{"title": "Multi-Style Facial Sketch Synthesis through Masked Generative Modeling", "authors": ["Bowen Sun", "Guo Lu", "Shibao Zheng"], "abstract": "The facial sketch synthesis (FSS) model, capable of generating sketch portraits from given facial photographs, holds profound implications across multiple domains, encompassing cross-modal face recognition, entertainment, art, media, among others. However, the production of high-quality sketches remains a formidable task, primarily due to the challenges and flaws associated with three key factors: (1) the scarcity of artist-drawn data, (2) the constraints imposed by limited style types, and (3) the deficiencies of processing input information in existing models. To address these difficulties, we propose a lightweight end-to-end synthesis model that efficiently converts images to corresponding multi-stylized sketches, obviating the necessity for any supplementary inputs (e.g., 3D geometry). In this study, we overcome the issue of data in-sufficiency by incorporating semi-supervised learning into the training process. Additionally, we employ a feature extraction module and style embeddings to proficiently steer the generative transformer during the iterative prediction of masked image tokens, thus achieving a continuous stylized output that retains facial features accurately in sketches. The extensive experiments demonstrate that our method consistently outperforms previous algorithms across multiple benchmarks, exhibiting a discernible disparity.", "sections": [{"title": "1 Introduction", "content": "The facial sketch synthesis (FSS) model [8,10,12,31,44], which generates sketch portraits based on provided facial photographs, has made remarkable advancements across various domains. It not only offers a promising solution to the challenge of photo-sketch face recognition [31] in security fields but also serves as a pivotal component in content generation applications, encompassing entertainment, arts, media, and more. Consequently, FSS has garnered increasing attention, warranting our dedicated focus.\nIn the past few years, the significant progress made in FSS is largely attributed to the development of the generative adversarial network (GAN) [13]."}, {"title": "2 Related Works", "content": "In this section, we provide a comprehensive review of previous FSS methods, highlighting the techniques that have inspired our proposed approach.\nFSS. The introduction of learning schemes has led to significant advancements in early traditional FSS that rely on heuristic image transformations. This progress has given rise to the development of these models, including Bayesian inference models [5,26,42], representation learning models [19,36], and subspace learning models [17,21]. In recent times, the burgeoning advancements in content generation by artificial intelligence have significantly enhanced the field of deep facial sketch synthesis, particularly through the utilization of style transfer [20,43] and image translation [30] methodologies. For instance, the Pix2Pix [18] model facilitates the transformation of source images into target images by replacing the generator of original conditional GAN with U-Net [29]. Subsequently, this approach has been enhanced to address its inherent instability by incorporating the feature matching loss, as proposed by Wang et al. [33]."}, {"title": "3 Method", "content": "We commence by introducing the problem formulation in Section 3.1 and the corresponding loss functions in Section 3.2. Subsequently, we provide the details of the model's architecture and the comprehensive training procedure in Section 3.3."}, {"title": "3.1 Problem Formulation", "content": "Taking the facial photograph $x \\in X$ and style condition $s\\in S$ as inputs, our objective is to discover a mapping $F: X \\times S \\rightarrow Y$, where $y \\in Y$ represents the identity-invariant sketch subject to the desired style $s$. To accomplish this target, the complete model comprises the VQ-tokenizer $Q$ (used exclusively during training), the feature encoder $E$, the transformer $T$, and the decoder $D$, forming the pipeline as illustrated in Fig. 2.\nThe predominant manipulations occur within the latent space, where the transformer model recovers masked image tokens. If sequence ${z_t}$ represents the true latent tokens gradually masked with respect to discrete time step $t = 0, 1, ..., T$, the transformer $T$ is trained to iterativelly reconstruct the initial $z_0$ from $z_T$. Mathematically, given the totally masked latent token $z_T$, the"}, {"title": "3.2 Loss Functions", "content": "Drawing upon the architecture illustrated in Fig. 2, we employ a two-stage training procedure to optimize the transformer T and decoder D individually. To be specific, MIM loss serves as the sole loss function employed during training of T, while the others are used for taining D. Concurrently, we leverage a pre-trained feature encoder E and VQ-tokenizer Q, whose parameters remain fixed throughout the entire training process.\nMIM Loss. In VQ-GAN, the latent tokens z with a dimension of N is constructed using N discrete labels, derived from a codebook comprising C distinct classes. Subsequently, we sample masked tokens $z^m$ using a cosine mask schedule that masks a fraction of z with a masking ratio $R = cos(\\pi\\cdot r/2)$, where r is randomly drawn from a uniform distribution U(0, 1). For latent tokens z of length N, the mask ${m_i}_{i=1}^N$ represents a binary sequence associated with z. The masked tokens $z^m$ is derived by substituting the token at position i of z with a designated symbol (-100 in this work) when $m_i = 1$, i.e., the i-th position of $z^m$ (denoted by $z^m_i$) is obtained by\n$$z^m_i = \\begin{cases} z_i, m_i = 0,\\\\ -100, m_i = 1. \\end{cases}$$\nAs the primary objective of the generative transformer model $T_\\theta$ is to reconstruct the masked portion within $z^m$, the MIM loss function $L_{MIM}$ is defined as\n$$L_{MIM} = \\frac{1}{N-M} \\sum_{i=1}^N m_i \\sum_{c=1}^C l_{ic} log(T_\\theta(E(x), s, z^m))$$\nwhere $M = \\sum_{i=1}^N m_i$, and $l_{ic}$ denotes the one-hot ground truth label of the i-th position in z. In Eq. (5), the utilization of the negative log-likelihood reveals that $L_{MIM}$ is a cross-entropy loss function specifically applied to masked tokens.\nPixel Loss. We deploy pixel loss function to measure the pixel-wise distance between synthesized and target sketch image. If we denote $||\\cdot ||_1$ as the l\u2081-norm, the pixel loss $L_{pix}$ is defined by\n$$L_{pix} = ||\\tilde{y} - y||_1.$$\nPerceptual Loss. In order to assure the feature similarity between synthesized sketch and the ground truth, we additionally employ the perceptual loss:\n$$L_{pcpt} = \\sum W_i||f_i(\\tilde{y}) - f_i(y)||_2,$$\nwhere $W_i$ and $f_i$ are the l-th weight and layer's output, respectively.\nObjective. In this work, the distance $D_1(\\tilde{z}, z)$ is measured by $L_{MIM}$, while the $D_2(\\tilde{y}, y)$ is decided by $L_{pix}$ and $L_{pcpt}$ in practice, i.e.,\n$$ \\begin{cases} D_1(\\tilde{z}, z) = L_{MIM}, \\\\ D_2(\\tilde{y}, y) = \\lambda_1 L_{pix} + \\lambda_2 L_{pcpt}, \\end{cases} $$\nwhere $\u03bb_1$ and $\u03bb_2$ are weight parameters."}, {"title": "3.3 Architecture and Algorithm", "content": "We hereby present a comprehensive exposition of all components within our framework, as well as an elaborate description of the procedure employed in our training algorithm.\nFeature Encoder. The encoder E plays a crucial role in acquiring feature embeddings within the latent space, thereby guiding the generation direction of the transformer model. To ensure optimal performance, we employ CLIP-L/14 [28] as our chosen encoder, as it has been reported to possess exceptional face recognition capabilities [1]. The intermediate and final hidden states are injected to the transformer model by cross-attention mechanism and adaptive normalization layers [27], respectively.\nTransformer. We adopt a modified version of U-ViT [16] architecture, sharing the same structure applied by [24], as our transformer model T. Our model outputs tokens of compact dimensions, specifically 16\u00d716, eliminating the necessity for downsampling and upsampling blocks. The model T receives a sequence of masked image token with the same dimension, i.e., 256 (16\u00d716) and facial photo feature from E with dimension of 1,024. The model T is fed a sequence of masked image tokens, each having the same dimension of its output, along with facial photo features obtained from E, which possess a dimension of 1,024.\nVQ-Tokenizer/Decoder. To effectively achieve the generation process, we perform manipulation within the compressed latent space facilitated by VQ-GAN [9]. The model comprises a tokenizer Q, which downsample images into the latent space, and a decoder D, which upsamples these tokens to generate images with a resolution of 256x256. By converting the image to tokens with size of 16\u00d716, the model effectively reduces the dimension for transformer T to process. Remarkably, this model does not incorporate any self-attention layers and employs a vocabulary size of 8,192.\nAlgorithm. Utilizing the aforementioned architectures and loss functions, we proceed with training the model in a two-stage framework, as shown in the table of Algorithm 1. In the initial stage (step 1-4), namely the pre-training, our focus"}, {"title": "4 Experiments", "content": "We provide the comprehensive experimental results in this section. Comparisons with various methods and the ablation study are conducted to analyze the performance."}, {"title": "4.1 Experimental Settings", "content": "Datasets. Our model undergoes pre-training using the CelebA dataset [22], followed by fine-tuning using the FS2K database [10]. The CelebA dataset comprises a total of 202,599 facial images exhibiting a wide range of poses and backgrounds. We generate the corresponding sketches of images in CelebA by [12] and eliminate the background in sketches by parsing the facial area. Contrasting with the larger scale of the CelebA dataset, the FS2K database comprises a relatively smaller collection of 2,104 image-sketch pairs, which encompass three distinct sketch styles, as well as a diverse range of image backgrounds and lighting conditions. Despite its smaller size, the FS2K stands out due to its high-quality"}, {"title": "4.3 Multi-Style Output", "content": "While achieving high-quality generation, our algorithm also accomplishes style interpolation, enabling it to generate intermediate styles beyond the limited set of styles in the training dataset, thus achieving a more diverse and rich style output.\nAs shown in Fig. 4, by adjusting the style parameter s, a series of intermediate styles can be obtained. In contrast to most existing face sketch generation algorithms, such as HIDA, which rely on one-hot labels and can only generate"}, {"title": "5 Conclusion", "content": "To address the challenges of insufficient data, limited style, and complex inputs encountered in the development of current facial sketch generation algorithms, this paper proposes an innovative and efficient facial sketch generation algorithm based on masked generative modeling. This algorithm utilizes semi-supervised and self-supervised learning techniques, which not only alleviate the challenges caused by insufficient data but also avoid the use of training-unstable GANs, which are the fundamental models in the current mainstream sketch facial generation. Furthermore, our algorithm possesses the ability to interpolate between different styles, a feature lacking in most existing facial sketch generation algorithms. This enables the generated sketches to transcend the limited styles present in the training set and consistently generate intermediate styles between different ones. Our algorithm requires no complex additional information for input; a single facial photograph is sufficient to generate the corresponding sketch. A series of fair experiments also confirm the higher generation quality of our algorithm, as well as its advantages in background and foreground separation and multi-style generation."}]}