{"title": "ABC Align: Large Language Model Alignment for Safety & Accuracy", "authors": ["Gareth Seneque", "Lap-Hang Ho", "Ariel Kuperman", "Nafise Erfanian Saeedi", "Jeffrey Molendijk"], "abstract": "Alignment of Large Language Models (LLMs) remains an unsolved problem. Human prefer-ences are highly distributed and can be captured at multiple levels of abstraction, from the individual to diverse populations. Organisational preferences, represented by standards and principles, are defined to mitigate reputational risk or meet legislative obligations. In this paper, we present ABC Align, a novel alignment methodology for LLMs that enables integration of the standards and preferences of a large media organisation into the LLM itself. We combine a set of data and methods that build on recent breakthroughs in synthetic data generation, preference optimisation, and post-training model quantisation. Our unified approach mitigates bias and improves accuracy, while preserving reasoning capability, as measured against standard benchmarks.", "sections": [{"title": "Introduction", "content": "In this paper, we present a novel approach to the alignment of Large Language Models (LLMs) we call 'ABC Align'. This alignment is conducted in two settings: the fine-tuning of open-source models, and In-Context Learning (ICL) of closed-source 'frontier' models (Lin et al., 2023). While the techniques and literature of In-Context Alignment (ICA) are less mature than fine-tuning based alignment, we achieve good results across both settings. By design, our methodology is applicable across any range of open- and closed-source models, offering provider independence and flexibility as underlying model capability grows with time.\nWe use a variety of data that is both domain-specific and general. These data include news article content, organisational 'AI Principles' (Australian Broadcasting Corporation, 2024a), and human-reviewed question/answer pairs sourced from an internal Retrieval-Augmented Generation (RAG) tool. They are used as input to both generate datasets for fine-tuning, and in the case of the 'AI Principles', directly in the frontier model context window itself.\nIn the fine-tuning alignment setting, we use several techniques from the literature on synthetic data generation, knowledge distillation, and preference optimisation to produce several datasets conditioned on these data. We include results controlling for the impact of each of these methods and data across two open-source models, from Mistral AI and Meta respectively.\nWe see general performance improvements across industry standard benchmarks. The version of our dataset that combines all data and methods achieves a 23.51% relative performance improvement on Meta's Llama3-8B model on the TruthfulQA benchmark. Furthermore, these results were achieved using a small dataset of 897 samples, showing substantial improvement over Meta's own fine-tuned version of the base model, which is trained on more than 10M human-annotated examples (AI@Meta, 2024).\nFor the ICA setting, conducted on OpenAI's GPT4-turbo model, we evaluate the performance of a number of custom system prompts, including the prompt used by our internal RAG tool, and our AI Principles. Most notably in evaluation against the Bias Benchmark for Question Answering"}, {"title": "Overview", "content": "(BBQ), a system prompt that uses our AI Principles improves relative performance over baselines by 77.54%.\nAligning LLMs and conducting evaluations are inherently challenging due to difficulty in defining 'who the alignment is for', and the need to define benchmarks reflecting a measure of alignment specific to that group, in addition to standard benchmarks.\nOur work aims to show an example of how organisations can answer these questions, integrate their standards into a process of provider-agnostic alignment, hence providing the organisations with choice and agency in how they apply LLMs across a range of use-cases."}, {"title": "Background", "content": "Authors' note: no editorial position is offered or claimed in this publication.\nWe also do not publish our data, and all LLM prompts are provided to support replication of our work and do not reflect optimisation for production use.\nFor real-world industrial application, in our case a large public-service media (PSM) organisation, alignment of model outputs to policies and preferences is critical. This is due to charter obligations to provide the public with innovative services (Australian Broadcasting Corporation, 2024c) that mitigate reputational risk while being transparent.\nMany PSMs face additional constraints specific to their operating model, most notably in the link between their sources of funding and restrictions around commercial activity (European Broadcasting Union, 2012). This reality can limit their capacity to support internal R&D efforts around technology, while they simultaneously rely on technology to deliver content to audiences in a highly competitive media marketplace.\nLLMs present a unique opportunity for PSMs to improve internal processes. For example, through enhancing content metadata or supporting information retrieval across large corpora. LLMs also carry a unique risk profile (Huang et al., 2023; Ferrara, 2023; Cui et al., 2024) when working with that content itself, whether it be to transform the content for multi-platform use, or use-cases that sit at the edge of emerging definitions around 'responsible AI', such as content generation (de Lima-Santos et al., 2024).\nRecent progress in model capability that enables processing of content across modalities in a single system text, audio, video - only serves to enhance this risk profile and magnify impact of the constraints PSMs face, given the computational and operational costs associated with utilising such systems at scale.\nScale is a critical consideration for PSMs both in terms of the audience served and content produced and stored. Leveraging the capabilities of LLMs (and their multimodal derivatives) thus involves a considered trade-off between scale, cost, and safety.\nThe Australian Broadcasting Corporation (ABC) has defined a set of AI Principles (see Appendix A) that are designed to guide the adoption and application of AI in a responsible, principled manner. We build on the use of 'safety' in the literature, particularly work on 'Constitutional AI' (Bai et al., 2022). We make explicit use of the ABC AI Principles in our methodology, and here include the principles of 'protecting data' and 'mitigating bias' to broaden the definition of 'safety'. 'Accuracy' maps directly to the AI Principle of the same name. This paper outlines methods that securely leverage the ABC's data and are evaluated against industry-standard benchmarks that act as proxies for these principles."}, {"title": "ABC Align", "content": "Our work sits at the intersection of machine learning theory and application. The organisational context in which it is conducted, under the various constraints outlined above, guides our choices of models, data and methods. These methods leverage the unique advantage of media organisations, namely their content, which is created in line with clearly defined policies and principles. We consider these as data valuable for the alignment and evaluation of LLMs, when paired with the approach we outline in this paper."}, {"title": "Related Work", "content": "In this section we outline a brief history of LLM development, and each of the methods we use as part of ABC Align.\nEarly implementations of the 'Transformer' and related attention mechanism were limited in both scale of pre-training data and the number of parameters in the neural network (Vaswani et al., 2017). However, the new architecture, with query-, key-, and value-cache to optimise both training and inference, went beyond the capacity of earlier architectures like Long Short-Term Memory (LSTM) and Recurrent Neural Networks (RNNs), both being limited in their ability to process long sequences under fixed computational constraints.\nRadfort et al. (Radford et al., 2019) moved beyond masked-language modelling approaches and described the Generative pre-trained Transformer (GPT) architecture, which framed the prediction problem in terms of arbitrary sequence-to-sequence modelling (Sutskever et al., 2014; Brown et al., 2020).\nOuyang et al. (Ouyang et al., 2022) directly improved this ability to follow general instructions via their method of \u2018instruction tuning\u2019, framing the model interaction as a \u2018request/completion\u2019 dynamic, leading to the conversational-style interfaces seen in recent iterations of GPT-class mod-els.\nAs this approach was scaled up, the diversity and distribution of training data (consisting of trillions"}, {"title": "Domain-Specific Large Language Models", "content": "LLMs have been developed for application to specific fields or areas of industry, such as biology and finance (Ling et al., 2024). A canonical example is BloombergGPT (Wu et al., 2023). Bloomberg has a significant store of domain-specific data (363B tokens) enabling it to perform full pre-training of their own model. Bloomberg's experience informed the methodology outlined in this paper, as the ABC itself has a large store of data across digital-first content and its archives.\nA complete pre-training process of an ABC LLM is likely to yield a capable, general model, given that digital content alone contains over a billion tokens, and 90+ years of archival content is currently being digitised. However, the expense and risk associated with maintaining a single artifact given the rate of progress in the field (Digital Platform Regulators Forum, Australian Government, 2023), have led us to favour a lightweight, modular methodology rather than pre-training a base model from scratch. This would allow us to leverage the value of the ABC's extensive data but in a way that was agnostic to underlying base models, in a cost-effective and future-proof manner. This has in part been validated by subsequent research, which has demonstrated that OpenAI's GPT-4 has exceeded the performance of BloombergGPT (Li et al., 2023)."}, {"title": "LLM Alignment", "content": "LLM alignment is a rapidly evolving area of research, alongside efforts to further scale up pre-training data and neural network parameter counts. Starting in 2023, early evidence (Zhou et al., 2023) emerged that while massive amounts of data were required in the pre-training stage, alignment itself could be facilitated through smaller, high-quality curated datasets with samples numbering in the thousands instead of trillions. In parallel, LLM capabilities were being categorised relative to new evaluation frameworks, specifically around 'reasoning' (problem-solving) (Mitra et al., 2023), extending standard definitions of accuracy beyond metrics like ROUGE and BLEU (Papineni et al., 2002).\nOur work builds on 'Constitutional AI' from Anthropic (Kundu et al., 2023) and 'Less is More for Alignment' (LIMA) by Meta (Zhou et al., 2023). Constitutional AI focuses on the self-alignment of LLMs based on a 'constitution' or set of principles that guide the process of 'AI feedback'. We adapt this approach to use the ABC AI Principles directly in the creation of our fine-tuning datasets and frontier model alignment prompts.\nLIMA defines the 'Superficial Alignment Hypothesis', that states a 'model's knowledge and capabil-ities are learnt almost entirely during pre-training, while alignment teaches it which sub-distribution of formats should be used when interacting with users'. The use of 'superficial' here indicates that fine-tuning a subset of parameters, rather than complete pre-training, is the appropriate setting for LLM alignment. Their work examines the impact of a small set of 1000 samples on several alignment metrics, building on Kirstain et al. (2022). Our dataset is thus correspondingly small, high-quality, and informed by our AI Principles.\nOur own hypothesis is that given a sub-distribution of fine-tuning and preference data specific to our organisation, we can in turn align LLMs in a way that preserves their general utility, capability, and performance on industry-standard benchmarks that directly map to and reflect our AI Principles. While non-domain-specific data like those from 'Stack Exchange', 'Wikihow' and 'Reddit' can be valuable in an academic fine-tuning setting, their use for alignment in a PSM setting is harder to justify. Consider a user feedback scenario, in the context of PSM obligations outlined in the introduction:\n\u2018Why did your model generate this inaccurate/biased output?'\n'It was aligned using reddit threads!'\nA key challenge that the LIMA authors observe is that 'manually creating diverse prompts and authoring rich responses in a uniform style is laborious'. In our own organisational context, asking editorial staff to label data, even if for a small sample, represents a significant and resource-intensive task."}, {"title": "Synthetic Data and Knowledge Distillation: ORCA", "content": "The utilisation of 'synthetic data' (Nikolenko, 2021) represents a novel source of data for training LLMs. It is often cited in wider discussions about the finite quantity of human-created data (Villalobos et al., 2024). As with human data, not all synthetic data are created equal. Indeed, low-quality synthetic data, when riddled with hallucinations, inaccuracies, and biases, can significantly impact downstream task performance (van Breugel et al., 2023).\nTo improve the quality of our synthetic data and mitigate these issues, we condition each sample on organisational data: news articles. Beyond the content itself, these articles comply with the ABC's editorial standards and policies (Australian Broadcasting Corporation, 2024b) at the time of their publication, and provide a real-world example of 'human annotated data' that goes well beyond crowd-sourced methods such as or 'Mechanical Turk' or similar. We also go beyond algorithms like Self-Instruct (Wang et al., 2023) that use 'seed instructions' to generate synthetic data, by providing a unique input for each generated sample.\nKnowledge distillation techniques have diversified well beyond the original work of Hinton et al. (2015). In the context of this paper, the teacher/student dynamic remains, but occurs between frontier model/smaller open-source model. The distillation occurs per-input, given a prompt. The structure of this prompt confines the possible sub-distribution of the synthetic data used by the student model in our fine-tuning process. Here we build on the work of Mukherjee et al. (2023) on ORCA, where they elicit 'reasoning traces' from OpenAI's GPT-4 based on synthetic samples, with the goal of improving specific capabilities of a smaller 13B parameter model.\nWe note here that our synthetic data samples are created for the specific purpose of improving un-derlying model capability rather than meeting editorial standards, following the methods outlined in ORCA. We do not intend on making this data publicly available. This is to avoid any confusion with the published content it is derived from."}, {"title": "Preference Optimisation: ORPO", "content": "Soliciting human feedback for alignment efforts proved crucial in the success of tools like ChatGPT (Ouyang et al., 2022). In a reinforcement learning setting, numerical instability and sensitivity to training dynamics were seen as a necessary trade-off for improved alignment performance. Model-free techniques such as Direct- and Odds-Ratio Preference Optimisation (DPO/ORPO) provide stable alternatives to the Reinforcement Learning with Human Feedback (RLHF) approach (Hong et al., 2024).\nFollowing a pre-training stage, models undergo further post-training to make them suitable for general use. Multiple techniques exist to facilitate this, with initial work focused on RLHF (Ziegler et al., 2020; Stiennon et al., 2020). This method aims to use human preference data to align pre-trained models and has demonstrated improved results beyond safety and reducing harmfulness (Tian et al., 2023; Gorbatovski and Kovalchuk, 2024), which are the key areas of focus for our work.\nRecent work on preference optimisation (Rafailov et al., 2023) aims to elicit similar changes in model behaviour via methods that reduce implementation complexity compared to RLHF. Given the constraints on PSMs (and organisations whose operational focus is not technology itself), these methods provide a way to demonstrate the value of incorporating their own 'human feedback' in the alignment of models they use.\nIn this paper, we make use of an implementation of ORPO that is readily available in Hugging Face's libraries. ORPO offers good performance when compared with both Supervised Fine-Tuning (SFT) and DPO, including improved computational efficiency for a given batch of training data, making it suitable for the aims of our work."}, {"title": "In-Context Alignment", "content": "There has been significant recent work on exploiting the ICL capabilities of frontier models (Agar-wal et al., 2024) where the context length has increased by up to an order-of-magnitude, as seen for instance in Google's Gemini and Anthropic's Claude series. While performance of fine-tuning in an"}, {"title": "Model Quantisation", "content": "As we discussed in the introduction of this paper, PSMs are funding-constrained, thus any training of LLMs needs to be done in an efficient way. Early methods of model quantisation involved performance trade-offs in terms of prediction quality and performance when compared to full-precision training and inference. However, recent advances in model quantisation have enabled rapid and low-cost experimentation. QLORA (Dettmers et al., 2023) addresses the shortcomings of earlier methods sufficiently and is our choice for quantisation for the experiments outlined in this paper."}, {"title": "Retrieval Augmented Generation & Grounding Synthetic Data", "content": "Another key aspect of the ABC Align methodology is the refining of the preference dataset, using user-generated outputs from an internally developed RAG tool. This tool is powered by Ope-nAI's GPT-4-32k (OpenAI, 2024) and uses context information retrieved from a corpus of ABC documents to answer user questions.\nWithin this context, alignment of the frontier GPT-4 model refers to ensuring that the entire RAG tool produced outputs that were relevant, accurate, and valuable to end users. This ultimately served to reflect whether the application (and thereby the frontier model) met users' expectations, and by proxy their broader organisational values. This effort involved a comprehensive system-wide approach, including not only refining the prompting techniques but also ensuring that the retrieval mechanisms, data sources and generated outputs all complied with the overall expected objectives (Yu et al., 2024). Evaluation of the system includes using user-acceptance derived met-rics, conducting user interviews, and manually reviewing generated samples, ensuring an adequate level of alignment of the frontier GPT-4 model to the ABC's AI Principles.\nWhile we generate ORCA-style synthetic data in this work, RAG grounding of synthetic data to produce reasoning traces that are accurate to, rather than derived from, content data will be explored in future work."}, {"title": "Information Theoretic Measures in NLP", "content": "A primary focus of this paper is dataset quality, and we demonstrate its impact on LLM per-formance in multiple fine-tuning settings. While measurement against standard benchmarks is critical, we are also interested in measuring the quality of the data itself, as it is initially trans-formed during our synthetic data and knowledge distillation process. Information theoretic metrics offer a principled approach to measuring quality and changes in the information that the data rep-resents. For this purpose, we use several well-known metrics, including Shannon entropy, mutual information, and Kullback-Leibler divergence (Cover and Thomas, 2012)."}, {"title": "Methodology", "content": "In this section, we outline two versions of our alignment dataset for full fine-tuning of open-source models, together with the synthetic dataset generation prompts. The inputs to these datasets are news articles, the ABC AI Principles, and samples from our internal RAG tool.\nThis tool, while an early prototype, is designed to deliver the capabilities of LLMs directly to users while also providing a means for soliciting editorial feedback and preferences, key to scaling our methods, outlined in the 'Future Work' section of this paper.\nAn automated pipeline was developed to enable the creation of synthetic datasets by augmenting existing data, as outlined in the previous section. The step-by-step process of creating an ABC Align dataset suitable for SFT is as follows:\n1. Collect news articles as the input.\n2. Create LLM-generated question/answer pairs relating to each input news article using an ORCA-style prompt.\n(a) Prompt: Deduce any reasoning or logical problems from this article: {context} Gen-erate a complex question and a logical answer that requires step-by-step thinking, and elaborate on this thinking process as part of the answer. Both the question and answer must not refer to the original article.\nThe resulting dataset is then used to create a dataset suitable for preference optimisation (PO). The answer for each sample is rewritten considering the AI Principles, given two canonical examples of human-reviewed Q&A pairs from our internal RAG tool.\n1. Rewrite the answers from the SFT dataset\n2. For 'chosen' answers:\n(a) Prompt: Here are two high-quality, human-reviewed Question/Answer pairs: Question: {q1} Answer: {a1} Question: {q2} Answer: {a2} Now review a new Question &\nAnswer pair: {context} Rewrite the answer to better align with these principles:\n{abc_ai_principles} Do not reference the principles in your response. Include only the text of the answer.'\n3. For 'rejected' answers:\n(a) Prompt: Here are two high-quality, human-reviewed Question/Answer pairs: Question: {q1} Answer: {a1} Question: {q2} Answer: {a2} Now review a new Question &\nAnswer pair: {context} - Rewrite the answer to be unaligned with these principles:\n{abc_ai_principles} Do not reference the principles in your response. Include only the text of the answer.'\nIn all cases, we use OpenAI's GPT-4-turbo frontier model with a temperature of 0.7 to generate synthetic data for use in a fine-tuning setting. The resulting datasets comprise 897 individual samples."}, {"title": "Synthetic Data Generation", "content": "In this section, we outline two versions of our alignment dataset for full fine-tuning of open-source models, together with the synthetic dataset generation prompts. The inputs to these datasets are news articles, the ABC AI Principles, and samples from our internal RAG tool.\nThis tool, while an early prototype, is designed to deliver the capabilities of LLMs directly to users while also providing a means for soliciting editorial feedback and preferences, key to scaling our methods, outlined in the 'Future Work' section of this paper.\nAn automated pipeline was developed to enable the creation of synthetic datasets by augmenting existing data, as outlined in the previous section. The step-by-step process of creating an ABC Align dataset suitable for SFT is as follows:\n1. Collect news articles as the input.\n2. Create LLM-generated question/answer pairs relating to each input news article using an ORCA-style prompt.\n(a) Prompt: Deduce any reasoning or logical problems from this article: {context} Gen-erate a complex question and a logical answer that requires step-by-step thinking, and elaborate on this thinking process as part of the answer. Both the question and answer must not refer to the original article.\nThe resulting dataset is then used to create a dataset suitable for preference optimisation (PO). The answer for each sample is rewritten considering the AI Principles, given two canonical examples of human-reviewed Q&A pairs from our internal RAG tool.\n1. Rewrite the answers from the SFT dataset\n2. For 'chosen' answers:\n(a) Prompt: Here are two high-quality, human-reviewed Question/Answer pairs: Question: {q1} Answer: {a1} Question: {q2} Answer: {a2} Now review a new Question &\nAnswer pair: {context} Rewrite the answer to better align with these principles:\n{abc_ai_principles} Do not reference the principles in your response. Include only the text of the answer.'\n3. For 'rejected' answers:\n(a) Prompt: Here are two high-quality, human-reviewed Question/Answer pairs: Question: {q1} Answer: {a1} Question: {q2} Answer: {a2} Now review a new Question &\nAnswer pair: {context} - Rewrite the answer to be unaligned with these principles:\n{abc_ai_principles} Do not reference the principles in your response. Include only the text of the answer.'\nIn all cases, we use OpenAI's GPT-4-turbo frontier model with a temperature of 0.7 to generate synthetic data for use in a fine-tuning setting. The resulting datasets comprise 897 individual samples."}, {"title": "Control Datasets", "content": "We include three additional datasets in our experiments. These are OpenORCA (Lian et al., 2023), LIMA in the SFT setting (GAIR, 2023), and UltraFeedback (Bartolome et al., 2023) for ORPO training.\nOpenORCA is a replication of the original ORCA data which was not made public. The dataset is sourced from FLAN-T5 and generated using both OpenAI's GPT-3.5-turbo and GPT-4 models. The authors aim to reflect the distribution of the dataset outlined in the ORCA paper.\nUltraFeedback is an updated version of the dataset used in the ORPO paper that addresses a data contamination issue related to the TruthfulQA benchmark.\nThe LIMA dataset is the original as published by the authors, and in all cases we randomly take 897 samples for direct comparison with our own datasets."}, {"title": "Models and Fine-tuning", "content": "We fine-tuned the following pre-trained models: Llama-3-8B (AI@Meta, 2024) and Mistral-7B-v0.3 (MistralAI, 2024), including both the base and instruction-tuned models. We used the default tokenizer of each of the models, and applied Hugging Face's Zephyr-style chat template (with <|system|>, <|user|>, and <|assistant |> tokens).\nAs described above, we employed a two-step fine-tuning process involving SFT for instruction tuning and ORPO for preference optimisation. To facilitate this, we used TRL's SFTTrainer and ORPOTrainer (von Werra et al., 2020) and scripts from Hugging Face's Alignment Handbook (Tunstall et al., 2023).\nORPO includes an additional training objective, relative ratio loss. It also dynamically penalises 'disfavoured' responses, instead of constructing sets of rejected tokens to control for degenerative model behaviour in the SFT setting.\nWe trained on single Nvidia A10G GPUs with 24GB memory on cloud infrastructure. We trained for 23 epochs on the SFT dataset, taking around 2.4 hours, and we trained for 10 epochs on the ORPO dataset, taking around 3.8 hours, using a standard checkpoint selection process against selected benchmarks for the models used in our final evaluations."}, {"title": "QLORA", "content": "The SFT/ORPO QLORA hyperparameters used in this study followed those as set in the Hugging Face Alignment Handbook. An overview of our QLORA hyperparameters during model fine-tuning are shown in Table 1.\nHyperparameter selection for quantisation methods has been shown to influence model performance in numerous ways, where the model may have different optimal 'rank' hyperparameters for different metrics, as well as the ratio between alpha/rank (Hu et al., 2022).\nThe rank hyperparameter is particularly important since it affects the size of the update matrices, and therefore the number of trainable parameters. It should be noted that these recommendations are evolving, and hyperparameter optimisation may be required for individual use-cases (Tribes et al., 2024)."}, {"title": "In-Context Alignment", "content": "For our experiments on ICA, we use OpenAI's GPT-4-turbo model with a temperature of 0.7, and the lm-evaluation-harness evaluation framework. Benchmarking is done in a \u2018generate_until' setting rather than traditional multiple choice, due to the limitations around the availability of logits from closed-source APIs.\nOur primary intervention is setting the 'system_instruction' parameter, including three specific prompts, taken directly from our internal RAG tool, the ABC's AI Principles, and a version of the AI Principles augmented with a Q&A sample sourced from the RAG tool, acting as a canonical example."}, {"title": "Dataset Analysis", "content": "This section outlines the information-theoretic and perplexity metrics we used to evaluate the datasets discussed in this study."}, {"title": "Shannon Entropy, Mutual Information, KL Divergence", "content": "For dataset analysis, we utilise several information-theoretic metrics on our SFT dataset. Our motivation here is to quantify the changes to average entropy, loss of information, and distribution shift induced by our synthetic data generation process. Our aim is to understand how our methods transform the information present in our input data, as we distil 'reasoning traces' suitable for downstream SFT and ORPO fine-tuning. All metrics are normalised.\nThese results demonstrate that, through our process of conditioning synthetic data samples on news articles, we have increased sample complexity relative to the input while keeping shared information low (in-line with our attempt to derive specific patterns with high complexity and abstraction, i.e. 'reasoning traces'). We have been able to do this while preserving the similarity of the overall distributions, as reflected in the low KL-Divergence score."}, {"title": "Perplexity", "content": "For perplexity (ppl), we measure the mean ppl per dataset per base model. We calculate the per-plexity of all datasets for both pre-trained Llama-3-8B and Mistral-7B-v0.3 models. These metrics are shown in Table 4 for the ABC Align, LIMA, OpenORCA and UltraFeedback datasets. We note that the ORPO datasets include 'chosen' and 'rejected' formats, reflecting their preference optimi-sation nature in line with Hugging Face's Alignment Handbook (Tunstall et al., 2023, Fine-tuning on your datasets | DPO and ORPO). For the ABC Align ORPO dataset, these were generated by OpenAI's GPT-4-turbo frontier model to be aligned/unaligned to the ABC AI Principles.\nInterestingly, among the ORPO datasets, the 'chosen' format shows lower perplexity than the \u2018rejected' format. Rather than exhibiting equal perplexity for the chosen/rejected formats, this implies that the original model training may have achieved similar results as we aimed to achieve by aligning using the ABC AI Principles.\nWe limit our use of ppl to the datasets themselves, rather than the fine-tuning phase. As observed by the LIMA authors, there is an anti-correlation between perplexity measured at training time"}, {"title": "Model Evaluation", "content": "In this section we discuss our evaluation strategy, including the mapping between selected bench-marks and the ABC AI Principles. We evaluate both our datasets and model outputs, whereby results are presented in the section below."}, {"title": "General Reasoning", "content": "In the fine-tuning setting, we want to ensure that we are not training the model to be task-specific. Here we use the ARC dataset, specifically the 'challenge' subset, constructed by the Allen AI Institute (Clark et al., 2018). It comprises 7,787 science questions in a multiple-choice setting aimed towards measuring 'reasoning capability'. We select this benchmark to ensure that we do not sacrifice general model capabilities while improving bias identification and accuracy, i.e., limiting their application across a range of use-cases also known as the 'alignment tax' (Askell et al., 2021)."}, {"title": "ABC AI Principle: 'mitigating bias'", "content": "The Bias Benchmark for Question & Answering (BBQ) (Parrish et al., 2022) represents a human-constructed and thorough cross-section of socio-cultural biases in a multiple-choice setting, for the evaluation of a model's ability to detect bias. In this study, we use bbq-lite-json, a subset version of the original BBQ, containing samples for evaluation against age, nationality, race, gender, sexual orientation, and physical appearance biases.\nThis benchmark serves as a proxy for the ABC AI Principle 'mitigating bias', which is defined as 'We will seek to prevent bias in our AI data or algorithms that could perpetuate and amplify existing inequalities and lead to discrimination'. BBQ is designed specifically to measure whether model outputs contribute to 'harming marginalised individuals and groups', thus enabling the identification and mitigation of bias. The list of social biases measured are defined by the US Equal Employment Opportunity Commission (EEOC, US Government, 2024). We acknowledge the US-centricity of this definition, and outline how we intend to extend our evaluation metrics in a general way, to make them ABC- or other-specific in our 'Future work' section at the end of this paper."}, {"title": "ABC AI Principle:\u2018accuracy'", "content": "We use performance on TruthfulQA (Lin et al., 2022) as a proxy for the ABC AI Principle 'ac-curacy'. TruthfulQA consists of 817 multiple-choice questions across 38 subjects, including law, health, and politics. This benchmark addresses the limitations of LLMs, where the distribution of their pre-training data may induce the generation of \u2018imitative falsehoods', or answers that sound true without being grounded. TruthfulQA thus represents a robust benchmark for accuracy, where the conditions for a claim to be true are \u2018if it describes..[a]..literal truth about the real world', and the question/answer choices are written by humans. We specifically use the truthfulqa_mc2 task."}, {"title": "Results", "content": "In this section we present the results of our fine-tuning alignment using both SFT and PO datasets."}, {"title": "Fine-tuning: SFT (ORCA/LIMA)", "content": "In an SFT setting, we evaluate the performance of our ABC Align SFT dataset against data generated using the two methods we build on, ORCA and LIMA respectively. We present our evaluation results in Figure 1 below."}, {"title": "Evaluation on arc-challenge", "content": "For Mistral-7B-v0.3, we see a decrease in performance compared to the pre-trained model of 0.42% and 0.85% for LIMA and OpenORCA. The Align SFT dataset demonstrates a 3.25% improvement compared to the pre-trained model and those fine-tuned on control datasets.\nFor Llama-3-8B, we see improvements over the pre-trained model for LIMA and OpenORCA datasets of 2.78% and 0.59% respectively. The ABC Align dataset demonstrates a 7.17% improve-ment over the pre-trained baseline."}, {"title": "Evaluation on bbq-lite", "content": "For Mistral-7B-v0.3, we see a decrease in performance compared to the pre-trained model of 2.17% on the LIMA dataset. For OpenORCA and ABC Align SFT, we see improvements of 12.81% and 14.57% respectively.\nFor Llama-3-8B, we see an equivalent decrease in performance of LIMA against the pre-trained model of 1.75%. For OpenORCA and ABC Align SFT, we see improvements of 16.34% and 7.20% respectively over the pre-trained baseline."}, {"title": "Evaluation on truthfulqa_mc2", "content": "For Mistral-7B-v0.3, OpenORCA and LIMA improve over pre-trained baselines by 11.01% and 13.69% respectively. ABC Align SFT demonstrates an improvement of 22.64%.\nFor Llama-3-8B"}]}