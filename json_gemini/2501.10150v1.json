{"title": "Dual Debiasing: Remove Stereotypes and Keep Factual Gender for Fair Language Modeling and Translation", "authors": ["Tomasz Limisiewicz", "David Mare\u010dek", "Tom\u00e1\u0161 Musil"], "abstract": "Mitigation of biases, such as language models' reliance on gender stereotypes, is a crucial endeavor required for the creation of reliable and useful language technology. The crucial aspect of debiasing is to ensure that the models preserve their versatile capabilities, including their ability to solve language tasks and equitably represent various genders. To address this issue, we introduce streamlined *Dual Debiasing Algorithm through Model Adaptation (2DAMA)*. Novel Dual Debiasing enables robust reduction of stereotypical bias while preserving desired factual gender information encoded by language models. We show that 2DAMA effectively reduces gender bias in English and is one of the first approaches facilitating the mitigation of stereotypical tendencies in translation. The proposed method's key advantage is the preservation of factual gender cues, which are useful in a wide range of natural language processing tasks.", "sections": [{"title": "Introduction", "content": "Gender representation in large language models (LLMs) has been the topic of significant research effort (Stanczak and Augenstein, 2021; Kotek et al., 2023). Past studies have predominantly focused on such representation to identify and mitigate social biases. Admittedly, biases are a challenging issue limiting the reliability of LLMs in real-world applications. Yet, we argue that preserving particular types of gender representation is crucial for fairness and knowledge acquisition in language models.\nTo provide a more detailed perspective, we draw examples of both unwanted and beneficial types of gender signals in LLMs. Undesirable biases are typically inherited from stereotypes and imbalances in the training corpora and tend to be further amplified during the model training (Van Der Wal et al., 2022; Gallegos et al., 2024). Biases are manifested in multiple ways, including unequal representation (models are more likely to generate mentions of a specific overrepresented gender), stereotypical associations (particular contexts are associated with one gender based on stereotypical cues, e.g., \"politics and business are male domains\", while \"family is a female domain\"). It has been shown that, due to bias, LLMs struggle with high-stakes decision-making and are prone to produce discriminatory predictions. Examples of such a sensitive application are the automatic evaluation of CVs and biographical notes (De-Arteaga et al., 2019), where some professions are stereotypically associated with a specific gender. Therefore, individuals of that gender could benefit from unfair advantage when assessed by an LLM-based evaluator.\nNevertheless, LLMs should understand and represent gender signals. For instance, chatbots should be persistent in addressing the user with their preferred gender pronouns after they are revealed (Limisiewicz and Mare\u010dek, 2022). Adequate representation of gender is also required for knowledge acquisition, for example, in question answering (QA), to correctly answer \u201cMaria Sk\u0142odowska-Curie\u201d to the question \u201cWho was the first woman to win a Nobel Prize?\u201d. Gender sensitivity is even more critical in morphologically rich languages, where gender mentions are much more ubiquitous, e.g., through morphological markings (as in German, Czech, or Russian) (Hellinger and Bu\u00dfmann, 2002). Examples of dual characters of gender encoding are shown in Figure 1."}, {"title": "1.1 Solution", "content": "To address these intricate ways gender signals are present in natural language, we introduce a new method 2DAMA that post-hoc modifies pre-trained language models to represent gender in an equitable way, i.e., without stereotypical bias but with factual gender information. The approach leverages recent effective methods for model adaptation Limisiewicz et al. (2024); Meng et al. (2022), and concept erasure Belrose et al. (2023). As the core contribution, we introduce the novel method of Dual Debiasing that aims at our core problem of decreasing bias while keeping equitable factual gender representation.\nWe present a strong theoretical backing of the effectiveness of our new method 2DAMA in Section 2. Subsequently, we show the experimental approach in the results for applying the method to the aforementioned case of gender: stereotypical vs. factual (Sections 3, 4). Furthermore, we are the first to mitigate LLMs' multilingual gender bias manifested in translation from English to three morphologically rich languages, presented in Section 5."}, {"title": "1.2 Research Questions", "content": "In the experimental part, we present an extensive analysis of the observed patterns in gender representation in four models and the effect of different design choices in the debiasing approach. The analysis specifically answers the research questions, grouped into four areas:\nA. How extensive the update of parameters should be to erase gender bias? Can the extent of the update be implicitly learned by the debiasing method?\nB. To what extent, after debiasing, can we preserve model performance in both gender-related and other tasks?\nC. What are the similarities and differences in the distribution of two analyzed signals (bias and factual gender) in models' weights?\nD. Does 2DAMA generalize to other languages? Is the distribution of bias signal in a model shared across distinct languages?\nUpon publication of the work, we will release the code and instances of debiasd models."}, {"title": "2 Methodology and Theoretical Background", "content": "In this section, we introduce Dual Debaising Algorithm through Model Adaptation (2DAMA), a new dual debiasing method. We first describe the 2DAMA's intended function, i.e., how it should alter the pre-trained language model, and describe the algorithmic components that efficiently fulfill these aims. Then, we provide theoretical backing for the presented approach. Appendix A contains the proofs and further terminological explanations."}, {"title": "2.1 Motivations for 2DAMA", "content": "The key idea of debiasing through model adaptation is that we can perform a targeted and efficient parameter change in a model to erase the unwanted signals encoded in it. The main risk of such an approach is that erasing bias can also affect the encoded knowledge.\nWe can draw a specific example of such a tradeoff from gender debiasing. We aim to reduce the models' reliance on stereotypes in predictions, e.g., given a stereotypical prompt as the one in Figure 2a: \u201cThe salesperson laughed because\", we intend to coerce equitable probabilities of possible gender predictions manifested by pronouns \u201che\u201d, \u201cshe\u201d, or \u201cthem\". On the contrary, when considering a prompt containing factual gender information: \"The king laughed because\" the desired output distribution would assign a high probability to the male pronoun.\nWe call the issue mentioned above Dual Debiasing problem. 2DAMA is designed to alter language models to reduce their reliance on stereotypes while maintaining sensitivity to factual gender."}, {"title": "2.2 Composing 2DAMA Approach", "content": "In practice, 2DAMA leverage on the combination of new (Dual Debiasing) and previously introduced algorithms (DAMA, LEACE). We shortly describe each of 2DAMA's components and the role they fulfill:\nDAMA Debiasing Algorithm through Model Adaptation (Limisiewicz et al., 2024) is a method for adapting parameters of language models to mitigate the encoding of harmful biases without affecting their general performance. The method employs model editing techniques (Meng et al., 2022) to disassociate specific signals provided in a prompt with the model outputs, i.e., stereotypes in prompts and gendered output. In this work, we use most of DAMA design choices, such as the adaptation of mid-upper feed-forward layers and algorithm to distill the stereotypical and gender-related parts of the latent representation (as depicted in Figure 2).\nLEACE LEAst-squares Concept Erarsuer (Belrose et al., 2023) is a method of concept erasure (such as bias signal) in latent representation. In Section 2.3, we show that combining DAMA and LEACE is possible and simplifies model adaptation. The simplification is obtained by replacing the Partial Least Squares concept erasure used in DAMA, which required pre-defining the dimensionality of erased signals. Moreover, in Section 4, we show that DAMA with LEACE obtain comparable or better results.\n2D Dual Debiasing is a new algorithm that we formally introduce in Section 2.4. The method uses covariance matrix decomposition to identify correlates related to bias and protected feature signals. A concept erasure algorithm (based on LEACE) is modified to erase bias while preserving protected features, in our case, factual gender."}, {"title": "2.3 Model Adaptation with LEACE", "content": "LEACE provides guarantees of erasing specific variable (concept Z) influence on another random variable (signal X). In the language of neural networks, we can consider latent vectors output by an intermediate layer as a sample drawn from the distribution given by X. LEACE aims to de-correlate latent vectors with an unwanted signal (e.g., gender bias), whose distribution is represented as another random vector Z.\n**Theorem 1 (LEACE).** We consider random vectors X and Z taking values in R\u207f. Both random vectors are centered, each with a finite moment. Then the objective:\narg min E [||PX \u2013 X||\u00b2] \nsubject to:\nCov(PX, Z) = 0\nis solved by:\nP* = I-W+Pw\u00b2W,\nwhere W is the whitening transformation (\\$\\Sigma\\_{Z,Z}\\^{1/2}\\$\\Sigma\\_{Z,Z}\\^\\{+\\}\\^{1/2}\\); Pws is an orthogonal projection matrix onto colspace of \\$\\mathbb{W}\\Sigma\\_{v,z}\\$. The proof can be found in Belrose et al. (2023). We extend this theoretical finding to erase the concept in linear transformations instead of latent vectors. We are specifically interested in transformation minimizing the error between input (keys: U) and predicted variable (values: V), as depicted in Figure 2b. We reasonably assume that dense layers of trained neural networks (e.g., feed-forward layers in Transformer) fulfill this purpose, i.e.:\nV = SU - \u20ac,   (1)\nwhere S is a linear transformation and e a vector of errors. Due to gradient optimization in the model's pre-training, we assume that the feed-forward layer approximates the least solution, i.e., FF \u2248 S.\nTaking this assumption, we can present a theorem guaranteeing concept erasure in the model adaptation algorithm (DAMA):\n**Theorem 2 (DAMA-LEACE).** We consider random vectors: U taking values in Rm, V and Z taking values in R\u207f, where m > n. Under assumptions that: A) random vectors U, V, Z are centered, and each of them has finite moment; B) the regression relation between U and V fulfill the assumption of ordinary least squares, and there exist least squares estimator V = SU \u2013 \u0454. Then the objective:\narg min E [||PU \u2013 V ||\u00b2], \nsubject to:\nCov(PU, Z) = 0"}, {"title": "2.4 Dual Debiasing", "content": "In Dual Debiasing, we extend the concept erasure problem by considering two type signals and corresponding random variables: Z\u266d bias to be erased and Zf feature to be preserved. We posit that:\n**Theorem 3 (DUAL-DEBIASING).** We consider random vectors X, Z\u266d, and Zf in R\u207f. Under the assumptions that: A) Z\u266d and Zf Z\u266d | Zf|X, i.e., Z\u266d and Zf are conditionally independent, given X; \u0392) \u03a3\u03a7,Zf \u03a3\u03a7,Z, i.e., the variable X is correlated with Zf and \u017d\u266d through mutually orthogonal subspaces of R\u207f. The solution of the objective:\narg min E [||PX \u2013 X||\u00b2], \nsubject to:\nCov(PX, Z) = 0,\nsatisfies:\nCov(PX, Zf) = Cov(X, Zf).\nThe theorem shows that the correlation with the conditionally independent features is left intact by applying LEACE erasure to a bias signal. However, the assumption of conditional independence is strong and unlikely to hold when considering the actual signals encoded in the model. Thus, for practical applications, we need to relax the requirements."}, {"title": "3 Experimental Setting", "content": "This section presents an empirical setting to examine the practical application of model editing methods. Specifically, we describe models, data, and evaluation metrics for gender bias and general performance."}, {"title": "3.1 Models", "content": "In experiments, we focus on Llama family models (Touvron et al., 2023; Dubey et al., 2024), which are robust and publically available language models developed by Meta AI. We analyze Llama 2 models of sizes 7 and 13 billion parameters and Llama 3 with 8 billion parameters. In multilingual experiments, we use ALMA-R 13 billion parameter model (Xu et al., 2024). ALMA-R is based on an instance of the Llama 2 model that was fine-tuned to translate using Contrastive Preference Optimization. ALMA-R covers translation between English and five languages (German, Czech, Russian, Icelandic, and Chinese) and shows competitive translation quality results.\nIn model editing experiments, we adapt the layers starting from the one found in the two-thirds of the layer stack counted from the input to the output. It is the 26th layer for 13 billion parameter models and the 21st layer for smaller models. For example, the adaptation of 11 mid-upper layers in the 13B model modifies the layers from 26th through 37th."}, {"title": "3.2 Data for Dual Debiasing", "content": "Following Limisiewicz et al. (2024), we feed prompts to the model in order to obtain the latent embeddings in the input of latent layers. We treat these embeddings as key vectors (U) containing stereotypical or factual gender signals. To obtain the gendered value vectors (V), we find the layer's output vector that would maximize the probability of predicting tokens corresponding to gender.\nLanguage Modeling Prompts For debiasing language models, we use solely English prompts. We design 11 prompt templates, such as \u201cThe X laughed because\", where \"X\" should be replaced by profession name. This prompt construction provokes the model to predict one of the gendered pronouns (\u201che\u201d, \u201cshe\u201d, or \u201cthem\u201d). To distinguish stereotypical signals for debiasing, we use 219 professions without factual gender that were annotated as stereotypically associated with one of the genders by Bolukbasi et al. (2016).\nMultilingual Prompts For debiasing machine translation, we use prompts instructing the model to translate sentences containing the same set of 219 professions to a target language that has the grammatical marking of gender, e.g., \u201cEnglish: The X is there. German: \". The translation model would naturally predict one of the German determiners, which denotes gender (\"Der\" for male or \"Die\" for female). For each model, we adjust the template to include instructions suggested by the model's authors. We construct the translation prompts for two target languages, Czech and German, proposing 11 templates for each.\nFactual Prompts Dual debiasing requires using factual prompts to identify the signal to be preserved. For that purpose, we use the same prompt templates as defined above (both English and multilingual) with the distinction of entities used to populate them. For that purpose, we propose 13 pairs of factually male and female entities, e.g., \"king\u201d \u2013 \u201cqueen\u201d, \u201cchairman\u201d \u2013 \u201cchairwoman\u201d.\""}, {"title": "3.3 Bias Evaluation", "content": "Language Modeling We assess the bias in language generation following the methodology of Limisiewicz et al. (2024). From the dataset of Bolukbasi et al. (2016), we select the held-out set of professions that were not included in the 219 used for debiasing. Each profession was assigned two scores: factual score xf and stereotypical score xs. The scores define how strongly a word (or a prompt) is connected with the male or female gender, respectively, through factual or stereotypical cues. By convention, scores range from -1 for female-associated words to 1 for male ones. We measure the probabilities for gendered prediction for a given prompt PM(o|X). For that purpose, we use pronouns o+ = \u201che\u201d and o\u2212 = \u201cshe\u201d, as they are probable continuations for given prompts. Subsequently for each prompt, we compute empirical_score y = PM(o+|X) \u2013 PM(o\u2212|X). To estimate the relationship between the observed score and annotated ones xs and xf, we construct a linear model:\ny = \u03b1s \u22c5 xs + \u03b1f \u22c5 xf + bo    (2)\nThe linear fit coefficients have the following interpretations: \u03b1s is an impact of stereotypical signal on the model's predictions; \u03b1f is an impact of the factual gender of the word. Noticeably, y, xs, and xf take the values in the same range. The slope coefficient tells how shifts in annotated scores across professions impact the difference in prediction probabilities of male and female pronouns. The intercept bo measures how much more probable the male pronouns are than the female pronouns when we marginalize the subject.\nOther Bias Manifestations in English We evaluate the bias in coreference resolution based on WinoBias dataset (Zhao et al., 2018). We use metrics AG and AS to evaluate representational and stereotypical bias, respectively. AG measures the difference in coreference identification correctness (accuracy) between masculine and feminine entities; similarly, AS measures the difference in accuracy between pro-stereotypical and anti-stereotypical instances of gender role assignments.\nTranslation (Stanovsky et al., 2019) proposed using Winograd Challenge sentences for evaluating bias in translation from English into eight languages with morphological marking of gender (e.g., German, Spanish, Russian, Hebrew). In WinoMT, the correctness of the translation is computed by the F1 score of correctly generating gender inflection of profession words in the target language. The evaluation of gender bias is analogical, as in WinoBias. AG and AS measure the difference in F1 scores: male vs. female and pro- vs. anti-stereotypical sets of professions, respectively. The more recent BUG (Levy et al., 2021) dataset is based on the same principle of bias evaluation, with the distinction that it contains naturally occurring sentences instead of generic templates used in WinoMT."}, {"title": "3.4 General Performance Evaluation", "content": "Language Modeling We evaluate perplexity on general domain texts from Wikipedia-103 corpus (Merity et al., 2016).\nReasoning Endtask To assess the models' reasoning capabilities, we compute accuracy on AI2 Reasoning Challenge (ARC) (Clark et al., 2018) in both easy and challenging subsets.\nTranslation To monitor the effect of debiasing on translation quality, we evaluate models on WMT-22 (Kocmi et al., 2020) parallel corpora with German, Czech, and Russian sentences and their translations in English. We estimate the quality by two automatic metrics: COMET-22 (Rei et al., 2022) and chrf (Popovi\u0107, 2015)."}, {"title": "4 Debiasing Language Models", "content": "In the first batch of the experiments, we evaluate the effectiveness of debiasing language models with model editing. In these experiments, we solely focus on tasks in English. We specifically analyze three model editing approaches: DAMA as a baseline; DAMA in combination with LEACE; and 2DAMA, which employs Dual Debiasing to preserve factual gender information."}, {"title": "4.1 Main Results", "content": "Model editing reduces bias and preserves the model's performance. All of the considered methods reduce gender bias both in language modeling and coreference resolution (Table 1). Remarkably, we observe that the model's overall performance, i.e., unrelated to gender, is not significantly affected, as demonstrated by perplexity and question-answering results (Table 2). Relatively worse performance preservation was observed for Llama 3, which could be caused by intervening in too many layers.\nStreamlining the approach with LEACE. We observe that DAMA-LEACE reduces bias to a larger extent than baseline DAMA. The more substantial debiasing effect comes in pair with a slightly higher drop in general performance, as shown in Table 2. Yet, the deterioration is still insignificant compared to the original models' scores. The crucial benefit of DAMA-LEACE is that projection dimensionality does not need to be pre-defined because it is learned implicitly (details in Section 2.3). That motivates us to use DAMALEACE in further experiments.\nPreserving factual gender with Dual Debiasing. The coefficients \u03b1s and \u03b1f from Table 1 indicate how much the models' prediction is affected by gender present through stereotypical and factual cues, respectively. We see that 2DAMA enables, to a significant extent, preserving factual gender information with a slight increase in susceptibility to gender bias."}, {"title": "4.2 Relationship between Seterotypical and Factual Signals", "content": "With Dual Debiasing, we can analyze the covariance of the latent orthogonal dimensions in the model's feed-forward layers with the stereotypical and factual signals (as detailed in Section 2.4). In Figure 3, we plot these covariances for each dimension. The visualization reveals that the factual gender is represented by relatively few dimensions with high covariance. In contrast, stereotypical bias is encoded in more-dimensional subspaces, yet each dimension has low covariance.\nIn Table 3, we demonstrate that in 2DAMA with low bias-to-feature threshold t = 0.05 preserves only a few dimensions responsible for stereotypical bias in each layer. Such intervention in the model erases \u2248 99% of covariance with a stereotypical signal while keeping over 30% of covariance with a factual gender signal."}, {"title": "4.3 Choice of Hyperparameters", "content": "We present the impact of two parameters on the effectiveness of 2DAMA in Figure 4. The first is the bias-to-feature threshold t. We observe that its choice controls the trade-off between mitigating bias and preserving factual information. We set it a low value of 0.05 because our primary objective is the reduction of bias. The second hyperparameter is the number of layers that should be edited. We confirm the findings of Limisiewicz et al. (2024) that adaptation should applied to approximately one-third of the midd-upper layers. Notably, the top two layers (38th and 39th) should be left out."}, {"title": "5 Beyond English: Multilingual Debiasing", "content": "In a multilingual setting, we debias a model fine-tuned for translation: ALMA-R 13B (Xu et al., 2024) by employing the collection of the new multilingual debiasing prompts. We specifically focus on gender bias and quality of translation between English and Czech, German, and Russian."}, {"title": "5.1 Main Results", "content": "Model editing generalizes to the multilingual settings. Analogically to experiments in English, we show that model editing reduces bias in translation and has a mediocre impact on the translation quality (as shown in Table 4). We observe some differences in results between the two analyzed languages. Overall, the scores after debiasing are better for German than Czech, indicating that German prompts are better quality.\nDual Debiasing is required to mitigate representational bias. Our methods are more effective for the stereotypical manifestation of bias AS than the representational one AG. In the representational bias, we sometimes observe bias increase after model editing. To remedy that, we use 2DAMA with higher values of feature-to-bias threshold (t = 1.00 instead of t = 0.05), which tends to preserve more factual signal. Factual gender understanding is especially essential for equitable representation of factual gender in morphologically rich languages, as evidenced by AG scores for t = 1.00 setting. This finding emphasizes the utility of 2DAMA in a multilingual setting."}, {"title": "5.2 Cross-lingual Debiasing", "content": "An intriguing question of multilingual bias is whether its encoding is shared across languages (Gonen et al., 2022). We test this hypothesis by editing models with prompts in one or multiple languages and testing on another language. The results show evidence of effectiveness in cross-lingual mitigation of stereotypical gender bias. In Table 5b, we observe that some languages are more effective in debiasing than others, e.g., German prompts offer the strongest AS reduction for both Czech and German. Whereas to control representational bias mitigation (AG), it is recommended to use in-language prompts, as indicated by Czech, German, and Russian results in Table 5a."}, {"title": "6 Related Work", "content": "We refer to the related work grouped into three areas."}, {"title": "6.1 Model Editing and Concept Erasure", "content": "Model editing is a method of applying targeted changes to the parameters of the models to modify information encoded in them. Notable examples of model editing include targeted changes in the model's weight (Mitchell et al., 2022; Meng et al., 2023, 2022) or adaptation with added modules (adapters) (Houlsby et al., 2019; Hu et al., 2022). The technique showed promising results as the tool to erase specific information (Patil et al., 2024).\nIn the literature, bias mitigation was perceived as a theoretically interesting and practical application for concept erasure. Ravfogel et al. (2020, 2022); Belrose et al. (2023) proposed effective linear methods of erasing gender bias from the latent representation of language models. Other approaches aimed to edit pre-trained language models to reduce their reliance on stereotypes. They include: causal intervention (Vig et al., 2020), model adapters (Fu et al., 2022), or targeted weight editing (Limisiewicz et al., 2024). As the name indicates, 2DAMA primarily builds on the last method."}, {"title": "6.2 Debiasing Machine Translation", "content": "Machine translation systems have been shown to exhibit gender bias in their predictions (Savoldi et al., 2021). The problem is especially severe in translation from languages that do not grammatically mark gender (e.g., English, Finish) to ones that do (e.g., German, Czech, Spanish) because translation requires predicting gender, which is not indicated in the reference (Stanovsky et al., 2019). There have been a few past attempts to mitigate biases in translation systems (Saunders and Byrne, 2020; Iluz et al., 2023; Zmigrod et al., 2019). Nevertheless, these approaches are based on finetuning for non-stereotypical sentences, which increases the model's specialization but significantly reduces usability, e.g., in tasks unrelated to gender (Luo et al., 2023).\nOne key constraint of multilingual debiasing is the scarcity of bias annotations in various languages. Notable datasets were introduced by Levy et al. (2021); N\u00e9v\u00e9ol et al. (2022). The difficulty of obtaining reliable cross-lingual bias resources stems from the need for deep knowledge of culture in addition to understanding a language. For instance, in different cultures, the same mentions tend to be associated with distinct sets of stereotypes: the word \u201cdoctor\u201d in English is strongly associated with the male gender due to the overrepresentation of men in this profession in the US and the UK. While in many Slavic and Baltic languages, the \"doctor\u201ds are to a lesser extent biased or even associated with the female gender, due to the higher proportion of women in medical professions.\nTo the best of our knowledge, we are the first to propose a method for debiasing LLM in machine translation tasks."}, {"title": "6.3 Controlled Debiasing", "content": "Sound practice in model debiasing involves carefully monitoring performance in bias-unrelated tasks and diverse manifestations of bias. The former aspect is needed for the reliability and usefulness of the models after debiasing. The latter's importance is highlighted by the observations that different bias metrics are mutually weakly or even negatively correlated (Delobelle et al., 2022; van der Wal et al., 2024), raising the need for holistic evaluation. The aspect of preserving useful features correlated with bias is especially underresearched. A noteworthy study of interactions between factual gender and gender bias was conducted by Limisiewicz and Mare\u010dek (2022). To our knowledge, we are the first to show that keeping factual gender can help mitigate diverse types of bias."}, {"title": "7 Disussion and Conclusion", "content": "We highlight the importance of considering the dual character of gender encoding in model editing. Our theoretical and empirical results show that the novel model editing methods: 2DAMA effectively reduces the impact of stereotypical bias on the predictions while preserving equitable representation of (factual) gender based on grammar and semantics. Maintaining the factual component of gender representation is crucial for debiasing in languages other than English, which denote gender more ubiquitously. Furthermore, our method does not significantly deteriorate the high performance of LLMs in various evaluation settings unrelated to gender.\nTo answer the research questions. The results show that novel 2DAMA is an effective debiasing method that can implicitly learn the extent (dimensionality) of bias signals (re: A). Thus, it is easier to apply to new models. Specifically, we do not require setting the number of erased bias dimensions in latent space thanks to changing the underlying erasure to LEACE. Furthermore, our method has minimal impact on gender-unrelated tasks (re: B), and thanks to Dual Debiasing, it can also identify and preserve factual signals, even though it is significantly correlated with bias (re: C). Lastly, we show that 2DAMA can be applied to a multilingual setting, and we observe the evidence of bias sharing across languages (re: D)."}, {"title": "A Proofs", "content": ""}, {"title": "A.1 Terminological Note", "content": "For brevity of theorems and proofs, we adopt the following notation convention:\n**Definition 1 (Moore-Penrose Pseudoinvers).** We denote Moore-Penrose pseudoinverse of matrix M as M+:\nM+ = (MTM)\u22121MT\n**Definition 2 (Matrix Square Root).** We denote a positive semi-definite square root of positive semi-definite matrix M as M1/2.\n**Definition 3 (Covariance Matrix).** For two random vectors: X \u2208 Rm and Y \u2208 Rn. We denote the covariance matrix as:\n\u03a3x,y = Cov(X, Y)"}, {"title": "A.2 Proof for DAMA-LEACE Theorem", "content": "We formalize the requirements and implications of that assumption in the following theorem:\n**Theorem 4 (Gauss-Markov: Probabilistic Least Squares).** We consider random vectors: U taking values in Rm, V, and Z taking values in Rn; both are centered and have finite second moments. We seek the linear regression model given by:\nV = SU - \u20ac,\ngiven the following assumptions:\nA No Multicollinearity: there is no linear relationship among the independent variables, i.e., matrix \u03a3U\u201aU is of full rank m.\nB Exogeneity: the expected value of error terms given independent variables E[\u20ac|U] = 0, this also implies that Cov(e, U) = 0.\nC Homoscedasticity: the covariance of the error terms is constant and does not depend on the independent variables Cov(\u03b5, \u03b5|U) = \u03c3\u2161.\nThen, the ordinary least squares estimator is given by the formula:\nS* = \u03a3V\u201aU \u03a3UU\nSuch estimator is best linear unbiased estimator and minimizes the variance of error terms: Tr(Cov(\u03b5, \u03b5)).\nThe proof of the Theorem 4 can be found in the classical statistics literature. For instance, Eaton (1983) presents proof for the multivariate case presented above.\nEquipped with the theorems above, we are ready to present the theorem that is the main theoretical contribution of this work:\n**Theorem 2.** We consider random vectors: U taking values in RM, V and Z taking values in R\u207f, where m \u2265 n. Under assumptions that: A) random vectors U, V, Z are centered, and each of them has finite moment; B) the regression relation between U and V fulfill the assumption of ordinary least squares, and there exist least squares estimator V = SU \u0395. Then the objective:\narg min E [||PU \u2013 V||\u00b2], \nsubject to:\nCov(PU, Z) = 0\nis solved by:\nP* = (I-W+Pw2W) s,\nwhere W is the whitening transformation (\\$\\Sigma\\_{Z,Z}\\^{1/2}\\$\\Sigma\\_{Z,Z}\\^\\{+\\}\\^{1/2}\\); Pws is an orthogonal projection matrix onto colspace of \\$\\mathbb{W}\\Sigma\\_{v,z}\\$. For simplicity, we will decompose the problem into independent optimization objectives corresponding to each dimension in Rn. For the ith dimension, we write the objective as:\narg min E [PTV - V2 s.t. Cov (PU, Z) = 0,    (3)\nwhere Pi is ith column of matrix P. From the assumption (B) of the theorem, we can represent the linear relation between U and V, as SU = V+ \u20ac, where e is an error term of regression. We use this property to rewrite the minimization objective from expression 3, as:\narg min\nE [PSU-V]   (4)\nWe manipulate the term under arg min to rewrite it as a sum of three terms:"}, {"title": "A.3 Proof for Dual-Debiasing Theorem", "content": "We find the solution based on Theorem 1, where we substitute X with V + \u20ac and find P* = I-W+PW2W, where W is the whitening transformation (\u03a3V+\u20ac,Z) ; Pws is an orthogonal projection matrix onto colspace of WE\u03a3V+\u20ac,Z\nConclusion for summands II and III, we independently found the matrices minimizing their values. We obtain the matrix P* solving our original objective in expression 3 by multiplying them:\nP* = P*S* = (I-W+PwEw) \u2211U,V \u03a3UU   (9)\n**Theorem 3.** We consider random vectors X, Zb, and Zf in R\u207f. Under the assumptions that: A) Zb and Zf Z\u266d | Zf|X, i.e., Z\u266d and Zf are conditionally independent, given X; \u0392) \u03a3X,Zf \u03a3X,Z\u266d , i.e., the variable X is correlated with Zf and \u017d\u266d through mutually orthogonal subspaces of R\u207f. The solution of the objective:\narg min E [||PX \u2013 X||\u00b2], \nsubject to:\nCov(PX, Zb) = 0,\nsatisfies:\nCov(PX, Zf) = Cov(X, Zf).\nProof. First, we observe that the assumption A) can be generalized to any coordinate system. For an orthogonal matrix W, we have:\n\u03a3W\u03a3\u03a7,\u03a3\u03a4 = WE\u03a3X,ZTWT (10)\nThis guarantees the orthogonality of spaces spanned by columns of two orthogonality matrices. The property will be useful for the second part of the proof:\nCol(\u2211WX,zf) Col(\u2211WX,z) (11)\nSecondly, we remind the reader that the solution to the objective provided in the theorem (based on Theorem 1) is as follows:\nP* = I-W+PW (12)"}, {"title": "B Prompts", "content": ""}, {"title": "B.1 Monolingual Prompts", "content": "The list of 11 prompt templates is given in Table 6. The term <profession> is substituted by 219 professions without factual gender (from Bolukbasi et al., 2016) and 26 gendered entities, which makes 2409 stereotypical and"}]}