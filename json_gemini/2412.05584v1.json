{"title": "UMSPU: Universal Multi-Size Phase Unwrapping via Mutual Self-Distillation and Adaptive Boosting Ensemble Segmenters", "authors": ["Lintong Du", "Huazhen Liu", "Yijia Zhang", "ShuXin Liu", "Yuan Qu", "Zenghui Zhang", "Jiamiao Yang"], "abstract": "Spatial phase unwrapping is a key technique for extracting phase information to obtain 3D morphology and other features. Modern industrial measurement scenarios demand high precision, large image sizes, and high speed. However, conventional methods struggle with noise resistance and processing speed. Current deep learning methods are limited by the receptive field size and sparse semantic information, making them ineffective for large size images. To address this issue, we propose a mutual self-distillation (MSD) mechanism and adaptive boosting ensemble segmenters to construct a universal multi-size phase unwrapping network (UMSPU). MSD performs hierarchical attention refinement and achieves cross-layer collaborative learning through bidirectional distillation, ensuring fine-grained semantic representation across image sizes. The adaptive boosting ensemble segmenters combine weak segmenters with different receptive fields into a strong one, ensuring stable segmentation across spatial frequencies. Experimental results show that UMSPU overcomes image size limitations, achieving high precision across image sizes ranging from 256x256 to 2048x2048 (an 8\u00d7 increase). It also outperforms existing methods in speed, robustness, and generalization. Its practicality is further validated in structured light imaging and InSAR. We believe that UMSPU offers a universal solution for phase unwrapping, with broad potential for industrial applications.", "sections": [{"title": "I. INTRODUCTION", "content": "Phase unwrapping is a critical technique in fields like structured light imaging[1], synthetic aperture radar[2], and magnetic resonance imaging[3], where phase contains key information. For instance, in structured light imaging and synthetic aperture radar, phase encodes height data[4][5], while in magnetic resonance imaging, it reflects magnetic field uniformity and physiological details[6]. However, phase measurements are often periodic, with phase data wrapped within the range (-\u03c0,\u03c0]. Phase unwrapping refers to the process of recovering the true phase from the wrapped phase, which is an ill-posed problem since multiple unwrapped phases can correspond to the same wrapped phase. The relationship between unwrapped and wrapped phase can be described as follows:\n$\\psi(x,y) = \\phi(x, y) + 2k(x, y)\\pi$\nwhere k(x, y) is the wrap count at (x, y). To obtain a unique unwrapped phase, most methods rely on the phase continuity assumption (Itoh condition)[7], which requires phase differ- ences between adjacent pixels to be less than \u03c0. However, noise and phase aliasing in practical applications often violate this condition, complicating phase unwrapping.\nConventional phase unwrapping methods recover the true phase by minimizing phase discontinuities. These include the path-following method[8], which uses techniques like branch cuts[9] or quality maps[10] to find optimal paths, and the iterative optimization method[11], which minimizes the dif- ference between unwrapped and wrapped phase gradients[12]. However, the path-following method may generate unrea- sonable paths for complex wrapped phases, compromis- ing accuracy[13], while optimization methods often produce smooth results that fail in noisy images, where phase differ- ences may not be integer multiples of 2\u03c0. Despite mitigating discontinuities, both methods can fail under severe noise or distorted wrapped phases[13], and are too slow for real-time applications.\nRecent studies have shown that neural networks can effec- tively perform phase unwrapping without being restricted by the Itoh condition, even in noisy environments[14]. As a result, deep learning-based phase unwrapping methods have become a research hotspot in the field. The methods include regression method[15], wrap count method[13], and gradient method[16].\nThe regression method directly maps the wrapped phase to the unwrapped phase using a neural network[17], but the predicted phase often deviates from Eq. (1), limiting accuracy[18].\nThe wrap count method frames phase unwrapping as a semantic segmentation task, classifying the wrap count k(x, y) for each pixel[13]. This ensures that the wrapped and un-"}, {"title": "II. RELATED WORK", "content": "The application of deep learning to phase unwrapping has seen significant progress in recent years. It offers new solu- tions to complex problems that conventional methods struggle to address. Deep learning-based phase unwrapping methods mainly include regression method, wrap count method, and gradient method.\n1) Regression Method: This method constructs regression models to map the wrapped phase to the unwrapped phase. Yair Rivenson et al. applied convolutional neural networks (CNNs) for holographic phase unwrapping, significantly accel- erating holographic image reconstruction due to the high-speed inference of deep learning[17]. However, the regression-based unwrapped phase often fails to satisfy Eq. (1), introducing errors. To address this, L. Zhou et al. proposed a conditional generative adversarial network with the L\u2081 norm for one-step phase unwrapping[18], which partially reduced phase ambigu- ity and inconsistency. Nonetheless, this method struggles with input sizes different from the training set, leading to unreliable results.\n2) Wrap Count Method: G. E. Spoorthi et al. first reformu- lated phase unwrapping as a semantic segmentation problem and introduced PhaseNet, which classifies the wrap count for each pixel to achieve phase unwrapping[14]. PhaseNet2.0 improved on PhaseNet with enhanced loss functions and model structures, achieving better accuracy and noise resistance[13]. The wrap count method is fast and satisfies the constraints of Eq. (1). However, wrap count classification relies on global feature integration, requiring a large receptive field. This limits its effectiveness to low-size images, with PhaseNet and PhaseNet2.0 only performing well at 256x256 size. Zhang et al. employed spatial pyramid pooling and positional self- attention to expand feature extraction to larger spatial ranges, achieving higher accuracy and robustness[19]. However, this approach still struggled with high-size image processing.\n3) Gradient Method: L. Zhou et al. proposed PGNet, which classifies the wrap count gradient at each point and uses an optimization algorithm to recover the unwrapped phase[16], as shown in Figure 1. Wrap count gradient prediction relies on local features and requires a smaller receptive field, but it is inherently sparse, with +1 or -1 categories constituting a small fraction of the image[20]. This severe class imbalance worsens at higher sizes, making gradient segmentation networks diffi- cult to train for high-size images. PGNet demonstrated phase unwrapping only at 64\u00d764 size. F. Sica et al.[21] and H. Wang et al.[20] incorporated interferometric coherence and quality maps as additional inputs, increasing size to 128x128. Gao et al. extended size to 256x256 with an improved LinkNet featuring a pretrained encoder and dilated convolutions[22]. However, this method relied on a complex unscented Kalman filter for post-processing, which was time-consuming and unsuitable for real-time phase unwrapping."}, {"title": "B. Self-Distillation for Performance Improvement", "content": "Distillation in neural networks is a model compression and optimization technique aimed at improving the performance of a smaller student network by transferring knowledge from a larger teacher network. First proposed by Hinton et al., this method leverages 'soft labels', or the predicted probability distributions from the teacher network, as training targets for the student network[23]. This enables the student net- work to learn both accurate classifications and richer inter- class relationships. For instance, S. Chen et al. introduced a nonlinear weight-sharing mapping mechanism for distillation, successfully applying it to facial attribute recognition while reducing computational and memory costs[24].\nSelf-distillation, a variant of distillation, uses the same network as both teacher and student at different training stages or within the same process. Its key advantage is eliminating the need for an external teacher network, relying on self-guidance to enhance accuracy, generalization, and robustness across tasks. H. Chen et al. developed a self-distillation paradigm for fine-tuning in few-shot object detection, mitigating misalign- ment between learned and actual distributions [25]. K. Xu et al. proposed a feature-enhanced self-distillation method based on feature extrapolation, demonstrating its effectiveness across modalities for classification tasks[26]."}, {"title": "C. Ensemble Learning", "content": "Ensemble learning combines predictions from multiple models to leverage their strengths and mitigate individual weaknesses, achieving more accurate and generalized results. Ensemble strategies include Bagging[27], Boosting[28], and Stacking[29].\nBagging (Bootstrap Aggregating) [27] creates multiple train- ing subsets via resampling, trains models on these subsets, and combines their predictions. For example, Random Forest[30], proposed by L. Breiman, applies Bagging to decision trees. B\u00fchlmann and Yu explained Bagging's ability to stabilize deci- sions, reduce variance, and minimize mean squared error[31]. Y. Sun et al. extended Bagging to video recognition by in- tegrating subnetworks with alternating residual links, forming an ensemble network[32]. Boosting improves weak learners by iteratively training new models on previous errors, enhancing overall performance. Notable methods include AdaBoost[28] and Gradient Boosting[33]. AdaBoost minimizes classification error using a greedy approach to produce weighted predictors. Gradient Boosting generalizes AdaBoost to differentiable loss functions. M. Saldanha et al. employed Gradient Boosting for Versatile Video Coding, training classifiers on texture, cod- ing, and context features[34]. Stacking, introduced by D. H. Wolpert, combines diverse base models using a meta-learning model to reduce bias[29]. Predictions from base models are combined by the meta-learner for improved performance. Y. Wei et al. proposed a data-free meta-learning approach using diffusion models to address data scarcity in few-shot learning[35]."}, {"title": "III. METHOD", "content": "As shown in Fig. 2, we propose a cross-size phase unwrap- ping network, UMSPU, which performs pixel-level wrap count gradient prediction. UMSPU consists of two components: (a) semantic information extraction and (b) segmenter. In (a), to stably and effectively extract semantic information from cross- size images, we introduce a Mutual Self-Distillation (MSD) mechanism. By performing attention distillation between the encoder and decoder during training, MSD effectively en- hances the perceptual ability of shallow layers and strengthens the expression of detailed information in deep layers. The spatial frequency of semantic features extracted by the network changes with size. To address this issue, we design an ensem- ble architecture segmenter at the network's end in (b). Using adaptive boosting, three segmenters with different receptive fields are weighted and combined into a robust segmenter, covering a broader range of spatial frequencies, enabling precise segmentation across cross-size images. Additionally, we design a weighted loss function and a curl loss function to improve training accuracy while ensuring the physical constraints of a curl-free field are satisfied."}, {"title": "B. Mutual Self-Distillation", "content": "The wrap count gradient is classified into +1, 0, and -1, with pixels of \u00b11 being highly sparse. This sparsity increases at higher sizes, challenging the encoder-decoder architecture in cross-size processing. In low-size inputs, sparse semantic information diminishes through network layers, leading to blurred stripe features and reduced gradient segmentation accuracy [36]. In high-size inputs, capturing sparse semantic information requires broader feature perception, but shallow layers, constrained by small receptive fields, struggle to cap- ture the long-range context necessary for locating stripe edges. This disrupts high-level feature propagation and hampers the recovery of sparse semantic cues, resulting in cumulative errors in stripe edge localization[37].\nTo enable UMSPU to adapt to cross-size inputs and capture sparse semantic features effectively, we propose a Mutual Self-Distillation (MSD) mechanism within the encoder-decoder architecture. The core principle of MSD is to promote the exchange of information between shallow and deep layers through two-way distillation learning during network training. Decoder-to-encoder distillation helps the encoder capture the global context of these sparse features more effectively under high-size inputs, enabling the rapid localization of gradi- ent distribution. Encoder-to-decoder distillation mitigates the degradation of sparse information in the deep layers under low-size inputs. Specifically, MSD connects feature maps of the same size in the encoder and decoder during training and distills their corresponding attention maps. The attention maps represent the regions of interest in each network layer and can be calculated from the feature maps as follows:\n$E_{sum}^{(i,j)} = \\sqrt{\\sum_{c}(E_{(c,i,j)}^2)}, D_{sum}^{(i,j)} = \\sqrt{\\sum_{c}(D_{(c,i,j)}^2)}$\nwhere $E_{(c,i,j)}$ and $D_{(c,i,j)}$ represent the values at (i, j) in the c channel of feature maps from encoder and decoder. Softmax function is then applied along the spatial dimension for normalization, generating the soft attention map labels:\n$E_{soft}^{(i,j)} = \\frac{exp(E_{sum}^{(i,j)})}{\\sum_{p=1}^{H}\\sum_{q=1}^{W} exp(E_{sum}^{(p,q)})}$\n$D_{soft}^{(i,j)} = \\frac{exp(D_{sum}^{(i,j)})}{\\sum_{p=1}^{H}\\sum_{q=1}^{W} exp(D_{sum}^{(p,q)})}$\nwhere H and W represent the height and width of the image. $E_{soft}^{(i,j)}$ and $D_{soft}^{(i,j)}$ are the values at (i, j) in soft attention map labels of encoder and decoder. Subsequently, the bidirectional KL divergence loss $KL_{loss}$ is calculated as:\n$KL_{loss} = \\lambda_{1} \\times KL (E || D) + \\lambda_{2} \\times KL (D || E)$\n$KL (E || D) = \\frac{1}{H \\times W} \\sum_{i=1}^{H}\\sum_{j=1}^{W} E_{soft}^{(i,j)} log\\frac{E_{soft}^{(i,j)}}{D_{soft}^{(i,j)}}$\n$KL (D || E) = \\frac{1}{H \\times W} \\sum_{i=1}^{H}\\sum_{j=1}^{W} D_{soft}^{(i,j)} log\\frac{D_{soft}^{(i,j)}}{E_{soft}^{(i,j)}}$\n$KL (E || D)$ denotes the KL divergence from the encoder's attention maps to the decoder's, where the encoder serves as teacher and the decoder as student. Conversely, $KL (D || E)$ refers to the divergence in the opposite direction, with the decoder as teacher and the encoder as student. $\\lambda_{1}$ and $\\lambda_{2}$ are the weight coefficients for these two losses. $KL (E || D)$ mitigates detail distortion in deep features for low-size inputs. $KL (D || E)$ enhances shallow layers' contextual perception, enabling fine segmentation and dense prediction for high-size images. Therefore, during training, $KL (E || D)$ is prioritized for low-size images and $KL (D || E)$ for high-size ones. Accordingly, we set the weight coefficients $\\lambda_{1}$ and $\\lambda_{2}$ as follows:\n$\\lambda_{1} = \\frac{R_{max} - R}{R_{max} - R_{min}}$\n$\\lambda_{2} = \\frac{R - R_{min}}{R_{max} - R_{min}}$\nwhere R represents the current image size, $R_{max}$ and $R_{min}$ correspond to the preset maximum and minimum sizes. As R decreases, the proportion of $KL (E || D)$ increases. As R increases, the proportion of $KL (D || E)$ increases.\nAs shown in Fig. 2(a), to accelerate the network, we use the RepVGG Block as the basic module in the encoder. The RepVGG Block adopts the concept of structural re- parameterization, converting the multi-branch structure during training into a single-branch structure for inference[38]. This significantly improves inference speed and reduces memory consumption. The detailed principles can be found in reference [38]. As shown in Fig. 2(c), MSD performs attention maps extraction and calculates two-way KL divergence losses for mutual attention distillation."}, {"title": "C. Adaptive Boosting Ensemble Segmenters", "content": "MSD enhances the consistency of semantic information extraction in the network under inputs of different sizes. Due to variations in stripe density and size, the semantic information output by the decoder exhibits diverse spatial frequencies. However, a single segmenter at the end of the network is limited by its fixed convolutional kernel size, making it difficult to adapt flexibly to the various frequency compo- nents of the semantic information. This limitation causes the network to excessively depend on and overfit to particular spatial frequency, making it challenging to maintain stable segmentation performance in the presence of varying spatial frequency [39]. To overcome this challenge, we integrate mul- tiple sub-segmenters with different receptive fields, achieving comprehensive coverage and efficient utilization of multi-scale spatial frequency features. As shown in Fig. 2(b), the ensemble segmenter consists of three distinct sub-segmenters, each using a 3x3 convolutional kernel as the base but with different dilation rates of 1, 2, and 4, respectively. This configuration provides the three sub-segmenters with different receptive fields, allowing them to focus on spatial frequency information at different scales.\nUnder the ensemble learning framework, each weak seg- menter dynamically adjusts its ability to capture frequency characteristics during the training process. However, due to the lack of clear prior information, it is difficult to assign ap- propriate weights to each segmenter before training. To ensure as much as possible that the multi-segmenter system achieves the widest and most complementary frequency coverage, we design an algorithm that dynamically updates the weights of each segmenter. Inspired by the Adaboost algorithm, as shown in Fig. 4, we design the specific training process as follows:\nStep 1: Dataset Initialization and Cross-Training. To avoid homogenization among the three sub-segmenters, we assign differentiated datasets during training. As shown in Fig. 4(b), We pair every two segmenters, resulting in three training pairs and alternately assign each input batch to a specific pair. This alternate mechanism ensures that each sub-segmenter is exposed to different combinations of sample sets, thereby enhancing its independence and diversity. During training, gradient accumulation is applied, with backpropagation per- formed every three batches. The encoder-decoder architecture is updated using the accumulated gradients, while each sub- segmenter is updated independently based on its respective gradients.\nStep 2: Update the Weights of the Segmenters. As shown in Fig. 4(a), between the adjacent epochs t and t + 1, we update the weight of the kth segmenter from $\\alpha_{k}^{(t)}$ to $\\alpha_{k}^{(t+1)}$. This update is based on the error rate $R_{k}$ of the k-th segmenter on the dataset, where higher error rates correspond to smaller weights. Specifically, after each training round, we calculate $R_{k}$ as a weighted sum of individual sample errors:\n$R_{k} = \\sum_{i=1}^{N} w_{i}^{(t)} \\epsilon_{i,k}$\nwhere N is the total number of samples, and $w_{i}^{(t)}$ is the weight of the i-th sample at epoch t, reflecting the difficulty of the sample. Initially, $w_{i}^{(0)} = 1/N$. The term $\\epsilon_{i,k}$ denotes the error rate of the k-th segmenter on the i-th sample, calculated as:\n$\\epsilon_{i,k} = \\frac{\\sum_{j} V_{i,j} I (Y_{i,j} \\neq h_{k,i,j})}{\\sum_{j} V_{i,j}}$\nwhere $h_{k,i,j}$ is the prediction of the k-th segmenter for the j-th pixel of the i-th image, and $Y_{i,j}$ is the corresponding ground truth label. The indicator function $I(y_{i,j} \\neq h_{k,i,j})$ equals 1 if $Y_{i,j} \\neq h_{k,i,j}$, and 0 otherwise. For gradient mask points, $V_{i,j} = 1$; for non-gradient points, $V_{i,j} = 0$. Thus, only errors on gradient mask points are considered.\nFinally, based on the error rate $R_{k}$, the weight of the k-th segmenter is updated using the following formula:\n$\\alpha_{k}^{(t+1)} = \\frac{1}{2} ln(\\frac{1-R_{k}}{R_{k}})$"}, {"title": "D. Loss Function", "content": "The loss function for wrap count gradient segmentation is designed to address two key challenges: (1) severe class imbalance, as the +1 and -1 classes have significantly fewer points compared to the 0 class, and (2) adherence to the physical constraint that gradient fields must be irrotational[40]. To address these challenges, we propose a loss function that combines a weighted loss to handle class imbalance and a curl loss to enforce the irrotational constraint.\nTo mitigate the imbalance between classes, we design a weighted loss function, enabling the model to focus more on the segmentation accuracy of the +1 and -1 classes. The weighted loss consists of a weighted mean squared error (MSE) loss and a weighted cross-entropy loss, defined as:\n$L_{wmse} = \\frac{1}{C \\times H \\times W} \\sum_{i} \\sum_{j} \\sum_{c} \\beta_{c}(Y_{c,i,j} - \\hat{Y}_{c,i,j})^2$\n$L_{wce} = \\frac{1}{H \\times W} \\sum_{i} \\sum_{j} \\sum_{c} \\beta_{c} Y_{c,i,j} \\cdot log(\\hat{Y}_{c,i,j})$\nwhere $L_{wmse}$ and $L_{wce}$ denote the weighted MSE and cross- entropy losses, respectively. The one-hot encoded label at point (i, j) in channel c is $Y_{c,i,j}$, and $\\hat{Y}_{c,i,j}$ is the corresponding model output after softmax normalization. The weight for the class corresponding to channel c is denoted as $\\beta_{c}$. Channel 0, 1, and 2 represent the 0, +1, and -1 classes, respectively. C, H, and W denote the number of channels, height, and width of the image.\nTo ensure compliance with the irrotational constraint, we introduce a curl loss. This loss is based on a fixed-weight convolution-based curl estimation method. As shown in Fig. 5, two fixed convolution kernels, $K_{x}$ and $K_{y}$, are applied to detect consecutive gradients in the horizontal and vertical directions:\n$K_{x} = \\binom{1}{0}, K_{y} = \\binom{1}{0}$\nFor gradx, the kernel $K_{x}$ detects consecutive gradients of 1 along the horizontal direction, with the convolution result $f_{x}(i, j)$ being 2 or -2 at curl points. Similarly, for grady, the kernel $K_{y}$ detects vertical gradients. Curl points are identified as points with convolution results of 2 or -2. The curl loss is then defined as the ratio of curl points to gradient points:\n$L_{curl} = \\frac{N_{curl}}{N_{gradient}}$\nwhere $N_{curl}$ and $N_{gradient}$ represent the number of curl points and gradient points, respectively. The loss function for each segmenter is defined as:\n$L_{sk} = L_{wmse} + L_{wce} + KL_{loss} + L_{curl}$\nwhere $L_{sk}$ is the loss for the kth segmenter, and $KL_{loss}$ represents the KL divergence loss from MSD. The total loss for the model is defined as:\n$Loss = \\alpha_{1}L_{s1} + \\alpha_{2}L_{s2} + \\alpha_{3}L_{s3}$\nwhere $\\alpha_{1}$, $\\alpha_{2}$, and $\\alpha_{3}$ are the weights assigned to segmenter 1, 2, and 3, respectively. By combining these losses, the model effectively balances class-specific accuracy, physical constraints, and multi-segmenter coordination, ensuring robust and consistent gradient segmentation."}, {"title": "E. Phase Reconstruction", "content": "After obtaining the gradients in the x and y directions, we use the least squares algorithm to reconstruct the wrap count gradient into the wrap count. Then, we multiply the wrap count by 2\u03c0 and add the wrapped phase to obtain the final unwrapped phase. The detailed derivation is provided in the supplementary material."}, {"title": "IV. EXPERIMENTS", "content": "This section introduces our experimental framework. Section IV-A details the experimental settings. Section IV-B compares UMSPU with other methods across different sizes, fringe densities, and processing speed. Section IV-C conducts an ablation study to validate the effectiveness of UMSPU's components. Section IV-D evaluates UMSPU's stability and generalization under translational and rotational deformations. Finally, Section IV-E and Section IV-F demonstrate UMSPU's practicality through comparisons in two real-world scenarios: structured light facial reconstruction and Interferometric Syn- thetic Aperture Radar."}, {"title": "A. Experimental Settings", "content": "1) Datasets: Following the method in [13], we generated the unwrapped phase by superimposing multiple Gaussian distributions with varying peaks and standard deviations onto a linear function. The wrapped phase was then computed, and the x- and y-direction wrap count gradients were derived through differentiation to serve as training labels. To generate the model input, noise was added to the unwrapped phase, followed by a four-step phase-shifting process to compute the wrapped phase.\nThe dataset contains 12,000 samples, split into 80% for training, 10% for validation, and 10% for testing. Sample sizes range from 256x256 to 2048\u00d72048, with SNRs between -2 dB and 4 dB. The network output consists of six channels: the first three represent the horizontal gradient classes, while the last three represent the vertical gradient classes.\n2) Implementations: The network is implemented using PyTorch, with training conducted on an NVIDIA A100 Tensor Core GPU and testing on an NVIDIA GeForce RTX 2060. The network is trained using the SGD optimizer with a batch size of 4, a learning rate of le-3, and a weight decay of 5e- 4, reaching convergence after 300 epochs. During training, the sample weight threshold in III-C is set to 5e-5. The class weights in III-D are defined as [1, 10, 10], where class 0 is assigned a weight of 1, and classes 1 and -1 are assigned weights of 10.\n3) Evaluation Metrics: Root mean square error (RMSE) is used to evaluate the phase unwrapping performance of the proposed and baseline methods. RMSE is defined as:\n$RMSE = \\sqrt{\\frac{1}{H \\times W} \\sum_{i=1}^{H} \\sum_{j=1}^{W} (Y_{i,j} - \\hat{Y}_{i,j})^2}$\nwhere $Y_{i,j}$ represents the ground truth unwrapped phase at point (i, j), $\\hat{Y}_{i,j}$ is the predicted unwrapped phase, and H and W are the image height and width, respectively."}, {"title": "B. Comparison", "content": "To validate the performance of UMSPU, we conducted comparative experiments with six commonly used phase unwrapping networks, including REDN[41], PhaseNet[14], PhaseNet2.0[13], DeepLabv3+[42], VDENet[43], and GAUNet[20]. The comparisons included analyses under different sizes, varying fringe densities, and model computational complexity.\n1) Comparison at Different Sizes: We constructed four test sets with sizes of 256x256, 512x512, 1024\u00d71024, and 2048x2048 to evaluate the performance of UMSPU and six other networks. As shown in Table I, all methods achieve low RMSE values at the lowest size (256x256). Among them, UM- SPU performs best with a mean RMSE of 0.2954, followed by GAUNet, a gradient-based network, with a mean RMSE of 0.4322. This represents a 31.65% improvement compared to PGNet. As the size increases to 512x512, 1024\u00d71024, and 2048x2048, UMSPU maintains high accuracy with mean RMSE values of 0.3392, 0.3429, and 0.3483, respectively. In contrast, the RMSE of the other six networks increases sharply and remains significantly higher than that of UMSPU.\nTo further illustrate this comparison, we randomly selected one sample from each size for phase unwrapping using all methods. As shown in Fig. 6, the other networks exhibit high accuracy only at the 256x256 size and fail at higher sizes. Specifically, REDN, PhaseNet, and PhaseNet 2.0 (wrap count method), as well as DeepLabv3+ and VDENet (regression method), all produce RMSE values exceeding 35 at higher sizes. This is because both wrap count and regression methods rely heavily on receptive field size. As input image size increases, the receptive fields of these networks fail to capture sufficient contextual information, leading to an inadequate understanding of details and the global structure in large im- ages. GAUNet, as a gradient-based method, is less constrained by receptive field size but struggles with finer and more complex gradient variations at high sizes due to its limitations in extracting complex structural features. This leads to error accumulation and gradient prediction distortion, resulting in RMSE values of 18.9360 and 29.4539 at 1024x1024 and 2048x2048, respectively. In contrast, UMSPU consistently achieves excellent phase unwrapping results across all sizes, with RMSE values of 0.2987, 0.3062, 0.3253, and 0.3624 at the four sizes, respectively. These results indicate that UMSPU overcomes the size limitations in phase unwrapping, showing excellent performance across various sizes.\n2) Comparison at Different Densities: To validate the impact of fringe spatial frequency on network output, we constructed four test sets for the experiment. The images in the test sets all have a size of 1024x1024 but differ in fringe density, specifically 10(\u00b13), 30(\u00b13), 50(\u00b13), and 70(\u00b13). We used UMSPU and six other methods on these four test sets and compared their mean RMSE. Notably, since the other six methods are unable to handle high-size images, we adopted an image tiling approach for high-size prediction, where each image is divided into 16 regions of 256x256 pixels, processed individually, and then stitched back to the original size. In contrast, UMSPU directly processes the full image without the need for tiling, thanks to its ability to handle high-size inputs.\nAs shown in Table II, under lower fringe densities of 10(\u00b13) and 30(\u00b13), all seven methods successfully perform phase unwrapping. The mean RMSEs of UMSPU are 0.2911 and 0.2957, respectively, while the second-best method, GAUNet, has mean RMSEs of 0.3579 and 0.4634, indicating that our method reduces RMSE by 18.66% and 36.18% compared to GAUNet. Even the worst-performing method, PhaseNet, maintains mean RMSEs of 0.5477 and 0.6946.\nUnder higher fringe densities of 50(\u00b13) and 70(\u00b13), UM- SPU still achieves high-precision unwrapping, with mean RMSEs of 0.3316 and 0.3728, while the second-best GAUNet reaches 0.8750 and 3.4766, significantly higher than ours. Additionally, we randomly select one sample from each of the four densities. As shown in Fig. 7, although the RMSE of UMSPU increases slightly with higher fringe densities, it remains as low as 0.3813 even at a fringe density of 70. This demonstrates that UMSPU adapts well to varying fringe densities.\nIn contrast, the other networks successfully unwrap phase at lower fringe densities of 7 and 30, but fail at densities of 50 and 70. This highlights the superiority of UMSPU in handling phase distributions across various spatial frequencies.\n3) Model Computational Complexity: We compared the computational complexity of UMSPU with six other networks under 1024x1024 resolution input. As shown in Table III, UMSPU has the lowest FLOPs, number of parameters, and inference time among all the networks. This demonstrates that UMSPU has significant advantages in terms of speed and lightweight design, making it easier to apply in real-world scenarios. Despite having the smallest computational complex- ity, UMSPU still achieves optimal performance, highlighting its efficiency. The efficiency comes from two aspects: first, the MSD mechanism enables the relatively simple network to achieve excellent performance through internal mutual representation learning during training. Second, we employed the RepVGG block as the basic module in the encoder, which uses structural reparameterization to convert the multi-branch topology during training into a single-path structure for in- ference, greatly optimizing the network's inference efficiency."}, {"title": "C. Ablation Study", "content": "To study the impact of different factors on network perfor- mance, we conducted ablation experiments on the mutual self- distillation mechanism and the adaptive ensemble segmenter. By exploring the effects of technical details in these methods, we validate the effectiveness of each component."}, {"title": "1) Effect of MSD:", "content": "This study experiments with four encoder-decoder pairs from Fig. 2: E1/D1", "accuracy": "encoder-to-decoder distillation boosts performance at 256x256 and 512x512, while decoder-to-encoder distillation excels at 1024x1024 and 2048x2048. At 256x256 and 512x512, E1 \u2192 D1 yields the greatest improvement, reducing the mean RMSE to 0.4459 and 0.6237. This is because the attention map of E\u2081 focuses on the striped regions. By learning this spatial distribution, D\u2081 preserves these areas, preventing excessive information loss and improving gradient segmentation. When all five encoder layers are distilled to their corresponding decoders (E1,2,3,4 \u2192 D1,2,3,4), the RMSE drops further to 0.4233 and 0.5414, outperforming any"}]}