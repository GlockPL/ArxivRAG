{"title": "CAREL: Instruction-guided reinforcement learning with cross-modal auxiliary objectives", "authors": ["Armin Saghafian", "Amirmohammad Izadi", "Negin Hashemi Dijujin", "Mahdieh Soleymani Baghshah"], "abstract": "Grounding the instruction in the environment is a key step in solving language-guided goal-reaching reinforcement learning problems. In automated reinforcement learning, a key concern is to enhance the model's ability to generalize across various tasks and environments. In goal-reaching scenarios, the agent must comprehend the different parts of the instructions within the environmental context in order to complete the overall task successfully. In this work, we propose CAREL (Cross-modal Auxiliary REinforcement Learning) as a new framework to solve this problem using auxiliary loss functions inspired by video-text retrieval literature and a novel method called instruction tracking, which automatically keeps track of progress in an environment. The results of our experiments suggest superior sample efficiency and systematic generalization for this framework in multi-modal reinforcement learning problems. Our code base is available here.", "sections": [{"title": "Introduction", "content": "Numerous studies have examined the use of language goals or instructions within the context of reinforcement learning (RL) [1-3]. Language goals typically provide a higher-level and more abstract representation than goals derived from the state space [4]. While state-based goals often specify the agent's final expected goal representation [5,6], language goals offer more information about the desired sequence of actions and the necessary subtasks [5]. Therefore, it is important to develop approaches that can extract concise information from states or observations and effectively align it with textual information, a process referred to as grounding [1].\nPrevious research has attempted to ground instructions in observations or states using methods such as reward shaping [7, 8] or goal-conditioned policy/value functions [9-12], with the latter being a key focus of many studies. Their approaches incorporate various architectural or algorithmic inductive biases, such as cross-attention [13], hierarchical policies [14, 15], and feature-wise modulation [16,17]. Typically, these works involve feeding instructions and observations into policy or value networks, extracting internal representations of tokens and observations at each time step, and propagating them through the network. Previous studies have explored auxiliary loss functions to improve these internal representations in RL [18-20], and have emphasized the importance of self-supervised/unsupervised learning objectives [21] in RL. However, these loss functions lack the alignment property between different input modalities, such as visual/symbolic states and textual commands/descriptions. Recent studies have suggested contrastive loss functions to align text and vision modalities in an unsupervised manner [22-26]. Most of these studies fall under the video-text retrieval literature [22,27], where the language tokens and video frames align at different granularities. Since these methods require a corresponding textual input along with the video, the idea has not yet been employed in language-informed reinforcement learning, where the sequence of observation might not always match the textual modality (due to action failures or inefficacy of trials). One can leverage the success signal or reward to detect the successful episodes and consider them aligned to the textual modality containing instructions or environment descriptions. Doing so, the application of the abovementioned auxiliary loss functions makes sense.\nIn this study, we propose a new framework, called CAREL (Cross-modal Auxiliary REinforcement Learning), for the adoption of auxiliary grounding objectives from the video-text retrieval literature [27], particularly X-CLIP [22], to enhance the learned representations within these networks and improve cross-modal grounding at different granularities. By leveraging this grounding objective, we aim to improve the grounding between language instructions and observed states by transferring the multi-grained alignment property of video-text retrieval methods to instruction-following agents. We also propose a novel method to mask the accomplished parts of the instruction via the auxiliary score signals calculated for the cross-modal loss while the episode progresses. This helps the agent to focus on the remaining parts of the task without repeating previously done sub-tasks or being distracted by past goal-relevant entities in the instruction. Our experiments on the BabyAI environment [17] showcase the idea's effectiveness in improving the systematic generalization and sample efficiency of instruction-following agents. The primary contributions of our work are outlined as follows:\n\u2022 We designed an auxiliary loss function to improve cross-modal grounding between language instructions and environmental observations.\n\u2022 We introduced a novel instruction tracking mechanism to help the agent focus on the remaining tasks by preventing the repetition of completed sub-tasks.\n\u2022 We enhanced overall performance and sample efficiency in two benchmarks."}, {"title": "Methods", "content": "In this study, we incorporate an auxiliary loss inspired by the X-CLIP model [22] to enhance the grounding between instruction and observations in instruction-following RL agents. This auxiliary loss serves as a supplementary objective, augmenting the primary RL task with a multi-grained alignment property which introduces an additional learning signal to guide the model's learning process. This design choice was motivated by the need to improve the model's ability to extract meaningful information from its observations and align it more effectively with the intended instruction, ultimately enhancing the overall performance of the RL system. We also leverage the alignment scores calculated within the X-CLIP loss to track the accomplished sub-tasks and mask their information from the instruction. This masking aims to filter out the distractor parts of the instruction and focus on the remaining parts, hopefully improving the overall sample efficiency of the agents. We call this technique instruction tracking. In the remainder of this section, we explain the auxiliary loss and the instruction tracking separately."}, {"title": "Auxiliary Loss", "content": "We calculate the proposed loss function over the successful episodes generated by an arbitrary instruction-conditioned RL model within a batch of online trials. To avoid the model being influenced by goal-unrelated behavioral patterns in unsuccessful trajectories, we exclude those trajectories from consideration and leverage reward values to organize only successful ones into a separate batch for the auxiliary loss. This separation is done only for the auxiliary loss, and the overall RL loop is run over all interactions, whether successful or unsuccessful. Hence, it differs from offline RL in which only certain episodes are selected for the whole training process [28].\nEach successful episode contains a sequence of observation-action pairs ep = ([01, a1]..., [On, an]) meeting the instructed criteria and an accompanying instruction instr = (I1, ..., Im) with m tokens. Since the X-CLIP loss requires local and global encoders for each modality, we must choose such representations from the model or incorporate additional modules to extract them. To explore the exclusive impact of the auxiliary loss and minimize any changes to the architecture, we use the model's existing observation and instruction encoders, which are crucial components of the model itself. We utilize these encoders to extract local representations for each observation-action [Ot, at] denoted as xt \u2208 Rd\u00d71, t = 1, ..., n in which each action is embedded similar to positional embedding in Transformers [29] and is added to the observation representation. Each instruction token I\u00bf is encoded as vi \u2208 Rd\u00d71, i = 1, ..., m. The global representations can be chosen from the model itself or added to the model by aggregation techniques such as mean-pooling or attention. We denote the global representations for observations and the instruction by \u017e and \u1fe6, respectively. The auxiliary loss function is then calculated according to [22] as below. We restate the formulas in our context to make this paper self-contained.\nTo utilize contrastive loss, we first need to calculate the similarity score for each episode (ep), a sequence of observations, and an instruction (instr) pair denoted as s(ep, instr). To do this, we calculate four separate values; Episode-Instruction (SE-I), as well as Episode-Word (SE-W), Observation-Instruction (S0-1) and Observation-Word (So-w) similarity values. Episode-Instruction score can be calculated using this formula:\nSE-1 = x\u1fe6,  (1)\nwith \u0129, \u1fe6 \u2208 Rd\u00d71, SE\u22121 \u2208 R. Other values are calculated similarly:\nSE-W = (V\u00f5)T,  (2)\nSO-1 = \u03a7\u1fe6,  (3)\nSo-w = XVT,  (4)\nwhere X = (x1; ...; x) \u2208 Rn\u00d7d is local representation for observations, V = (vf; ...; vm) \u2208 Rm\u00d7d in local representations for instruction tokens, and SE-W \u2208 R1\u00d7m, So\u22121 \u2208 Rn\u00d71 and So-w \u2208 Rn\u00d7m provide fine-granular similarities between the language instruction and the episode of observation. These values are then aggregated with appropriate attention weights via a technique called Attention Over Similarity Matrix (AOSM). Episode-Word (SE-W) and Observation-Instruction (S'O-1) scores are calculated from the values as follows:\nS'0-1 = Softmax(So\u22121[.,1])TSo\u22121[., 1],  (5)\nS'E-W = Softmax(SE-W[1,.])TSE-W [1, .],  (6)\nwhere:\nSoftmax(x[.]) = \\frac{exp(x[.]/T)}{\\Sigma_j exp(x[j]/\u03c4)},  (7)\nin which, 7 controls the softmax temperature. For the Observation-Word score, bi-level attention is performed, resulting in two fine-grained similarity vectors. These vectors are then converted to scores similar to the previous part:\nS'instr[i, 1] = Softmax(So-w[i, .])TSo_w[i, .],  i\u2208 {1, ..., n},  (8)\nSep[1, i] = Softmax(So-w[., i])TSo-w[., i] i\u2208 {1, ..., m},  (9)\nwhere Sinstr \u2208 Rn\u00d71 show the similarity value between the instruction and n observations in the episode and Sep \u2208 R1\u00d7m represents the similarity value between the episode and m words in the instruction.\nThe second attention operation is performed on these vectors to calculate the Observation-Word similarity score (So-w):\nS'o-w = (Softmax(Sep[1,.])T Sep[1,.] + Softmax(S'instr[., 1])T S'instr[., 1])/2.  (10)"}, {"title": "Instruction Tracking", "content": "The final similarity score between an episode and an instruction is computed using the previously calculated scores:\ns(ep, instr) = (SE-1 + S'E-W + S'o-1 + S'o-w)/4.  (11)\nThis method takes into consideration both fine-grained and coarse-grained contrasts. Considering N episode-instruction pairs in a batch of successful trials, the auxiliary loss is calculated as below:\nLaux =  \\frac{1}{N}  \\Sigma_{i=1}^N  (-log  \\frac{exp(s(epi, instri))}{\\Sigma_{j=1}^n exp(s(epi, instrj))} + log  \\frac{exp(s(epi, instri))}{\\Sigma_{j=1}^n exp(s(epj, instri))})  (12)\nThe total objective is calculated by adding this loss to the primary RL loss, LRL, with a coefficient of Ac.\nLtotal = LRL + AC. Laux  (13)\nThe overall architecture of a base model [17] and the calculation of the auxiliary loss is depicted in Figure 1. If the shape of the output representations from the observation and instruction encoders does not align, we employ linear transformation layers to bring them into the same feature space. This transformation is crucial as it facilitates the calculation of similarity between these representations within our loss function.\nWe can consider the similarities from eqs. 1 to 4 as a measure of matching between the instruction and the episode at different granularities. Once calculated at each time step of the episode, this matching can signal the agent about the status of the sub-task accomplishments. The agent then can be guided toward the residual goal by masking those sub-tasks from the instruction. More precisely, at time step t of the current episode, the agent has seen a partial episode ep(t) = ([O1, a1], ..., [Ot, at]) that in a fairly trained model should align with initial stages of the instruction. The instruction itself can be parsed into a set of related sub-tasks C = {ci} via rule-based heuristics, and there can be constraints on their interrelations. For example, an instruction of the form \"Do X, then do Y, then do Z\" includes three sub-tasks X, Y, and Z which have a sequential order constraint (X \u2192 Y \u2192 Z). Other examples could involve different forms of directed graphs where a specific sub-task is acceptable only if its parents have been satisfied before during the episode. The set of acceptable sub-tasks at time step t is denoted by Ct, which contains the root nodes in the dependency graphs at the start of the episode.\nIn order to track the accomplished sub-task, we assess the similarity between C members and the partial episode. This can be done by tracking SE-W or So-w, which provides fine-grained similarities across the language modality. In the case of SE-W, the similarity per token in ci is averaged to get a final scalar similarity. For So-w, the maximum similarity between the observations and each word is considered for averaging across ci tokens. Another option is to calculate a learned representation for the whole ci instead of averaging and to track the instructions based on its similarity with the partial episode, aiming at preserving the contextual information in the representation of the sub-task. The final calculated similarity of each acceptable sub-task ci, denoted by St is tracked at each time step. Once this similarity rises significantly, the matching is detected, and ci is removed from the instruction participating in the language-conditioned model. More precisely, we remove ci from the instruction when the following condition is satisfied:\n(Ci\u2208 Ct) (St\u2265k x  \\frac{1}{t-1}  \\Sigma_{j=1}^{t-1} S_j). (14)\nHere, k > 1 is a hyperparameter that specifies the significance of the matching score's spike. While the auxiliary loss described in the previous subsection is applied on the episode level, instruction tracking happens at every time step of the episode over the partial episode and the masked instruction.\nThis overall process is represented in Figure 1. These two techniques can be applied jointly, as the auxiliary loss improves the similarity scores through time, and the improved similarities enhance the instruction tracking. To prevent false positives during tracking at the initial epochs of training, one can constrain the probability of masking and relax this constraint gradually as the learning progresses."}, {"title": "Experiments", "content": "In our experiments, we conducted a comparative analysis to assess the impact of X-CLIP [22] auxiliary loss on generalization and sample efficiency of instruction-following agents. We showcase the success of CAREL along with the instruction tracking technique in our experiments\u00b9. For this purpose, we employ two baselines called BabyAI [17] (the proposed model along with the BabyAI benchmark) and SHELM [30] for which we explain the experimental setup and results in the following paragraphs."}, {"title": "Vanilla CAREL Results", "content": "We employ the BabyAI environment [17], a lightweight but logically complex benchmark with procedurally generated difficulty levels, which enables in-depth exploration of grounded language learning in the goal-conditioned RL context. We use BabyAl's baseline model as the base model and minimally modify its current structure. Word-level representations are calculated using a simple token embedding layer. Then, a GRU encoder calculates the global instruction representation. Similarly, we use the model's default observation encoder, a convolutional neural network with three two-dimensional convolution layers. All observations pass through this encoder to calculate local representations. Mean-pooling/Attention over these local representations is applied as the aggregation method to calculate the global observation representation. The RL agent is trained using the PPO algorithm [31] and Adam optimizer with parameters B\u2081 = 0.9 and \u03b22 = 0.999. The learning rate is 7e - 4, and the batch size is 256. We set Ac = 0.01 and the temperature \u0442 = 1 as CAREL-specific hyperparameters. To minimize the changes to the baseline model updates, we backpropagate the gradients in an outer loop of PPO loss to be able to capture episode-level similarities. This gradient update with different frequencies has been tried in the literature before [16].\nThe evaluation framework for this work is based on systematic generalization to assess the language grounding property of the model. We report the agent's success rate (SR) over a set of unseen tasks at each BabyAI level, separated by pairs of color and type of target objects or specific orders of objects in the instruction. This metric is recorded during validation checkpoints throughout training.\nFigure 2 illustrates the improved sample efficiency brought about by CAREL auxiliary loss (without instruction tracking and action embedding to minimize the modifications to the baseline model, hence called Vanilla CAREL). All results are reported over two random seeds. The results indicate improved sample efficiency of CAREL methods across all levels, especially those with step-by-step solutions that require the alignment between the instruction parts and episode interactions more explicitly, namely GoToSeq and OpenDoorsorder which contain a sequence of Open/GoTo subtasks described in the instruction. The generalization is significantly improved in more complex tasks, i.e., Synth."}, {"title": "Instruction Tracking Results", "content": "For instruction tracking, We use only the SE-W vector and average over tokens of each sub-task to track the score over time. To detect sub-task matching from the score signal, we set k = 2 in Equation 14. All the other settings are kept the same as in vanilla CAREL, except that we also add action embeddings to local observation representations, as described in the Auxiliary Loss section. We mask acceptable sub-tasks with a certain possibility which follows a hyperbolic tangent function in terms of training steps (p = 0.8 \u00d7 tanh (2 \u00d7 step/maxsteps) + 0.01) where maxsteps is the total number of training frames. This is meant to minimize the amount of masking at the start of the learning process when the model has not yet learned a good embedding for instructions and observations and increase it over time.\nTo evaluate the capability of our framework on RGB environments, we apply and test it on SHELM [30]. SHELM leverages the knowledge hidden in pre-trained models such as CLIP and Transformer-XL. It also uses CLIP to extract textual tokens related to every observation. Then these tokens are passed through the frozen Transformer-XL network to form a memory of tokens throughout the episode. This hidden memory is then concatenated to a CNN representation of observation and passed to actor/critic heads. We must modify SHELM's structure as it doesn't use the environment's instructions, which are crucial to success in a multi-goal setting. To do so, we utilize BERT's tokenizer to embed the instructions and pass them through a Multihead-Attention layer with four heads. The resulting embedding is concatenated to the hidden layer alongside the outputs of the CNN model and Transformer-XL, which are then passed to the actor-critic head.\nThe results of the full CAREL method (with instruction tracking and action embedding) are reported on the PutNear environment. We break down the instructions in this environment with a rule-based parsing to increase the level of detail in the instruction. The instruction, stated initially as \"put the [obj1] near the [obj2]\", is converted to \"go to the [obj1], then pick up the [obj1], then go to [obj2]\" and so on. This introduces the challenge of sequential sub-tasks into SHELM tasks. We consider the CLIP output for observations as the local representations and add another Multi-head Attention layer followed by a mean-pooling over them to calculate the corresponding global representations. We train the learnable parts of the model using the PPO algorithm and Adam optimizer with the same hyperparameters. The learning rate is 1e - 4, and the batch size is set to 16. The results in Figure 3 indicate that instruction tracking improves CAREL, especially in the case of RGB inputs coming from more complex tasks."}, {"title": "Conclusion", "content": "This paper proposes the CAREL framework which adopts auxiliary cross-modal contrastive loss functions to the multi-modal RL setting, especially instruction-following agents. The aim is to improve the multi-grained alignment between different modalities, leading to superior grounding in the context of learning agents. We apply this method to existing instruction-following agents. The results indicate the sample efficiency and generalization boost from the proposed framework. As for the future directions of this study, we suggest further experiments on more complex environments and other multi-modal sequential decision-making agents. Also, the instruction tracking idea seems to be a promising direction for further investigation."}]}