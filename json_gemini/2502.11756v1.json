{"title": "ON THE COMPUTATION OF THE FISHER INFORMATION IN CONTINUAL LEARNING", "authors": ["Gido M. van de Ven"], "abstract": "One of the most popular methods for continual learning with deep neural networks is Elastic Weight Consolidation (EWC), which involves computing the Fisher Information. The exact way in which the Fisher Information is computed is however rarely described, and multiple different implementations for it can be found online. This blog post discusses and empirically compares several often-used implementations, which highlights that many currently reported results for EWC could likely be improved by changing the way the Fisher Information is computed.", "sections": [{"title": "1 Introduction", "content": "Continual learning is a rapidly growing subfield of deep learning devoted to enabling neural networks to incrementally learn new tasks, domains or classes while not forgetting previously learned ones. Such continual learning is crucial for addressing real-world problems where data are constantly changing, such as in healthcare, autonomous driving or robotics. Unfortunately, continual learning is challenging for deep neural networks, mainly due to their tendency to forget previously acquired skills when learning something new.\nElastic Weight Consolidation (EWC) [1], developed by Kirkpatrick and colleagues from DeepMind, is one of the most popular methods for continual learning with deep neural networks. To this day, this method is featured as a baseline in a large proportion of continual learning studies. However, in the original paper the exact implementation of EWC was not well described, and no official code was provided. A previous blog post by Husz\u00e1r [2] already addressed an issue relating to how EWC should behave when there are more than two tasks. This blog post deals with the question of how to compute the Fisher Information matrix. The Fisher Information plays a central role in EWC, but the original paper does not detail how it should be computed. Other papers using EWC also rarely describe how they compute the Fisher Information, even though various different implementations for doing so can be found online.\nThe Fisher Information matrix is also frequently used in the optimization literature. In this literature, several years ago, Kunstner and colleagues [3] discussed two ways of computing the Fisher Information - the 'true' Fisher and the 'empirical' Fisher and based on both theory and experiments they recommended against using the empirical Fisher approximation. It seems however that this discussion has not reached the continual learning community. In fact, as we will see, the most commonly used way of computing the Fisher Information in continual learning makes even cruder approximations than the empirical Fisher."}, {"title": "2 The Continual Learning Problem", "content": "Before diving into EWC and the computation of the Fisher Information, let me introduce the continual learning problem by means of a simple example. Say, we have a deep neural network model $f_\\theta$, parameterized by weight vector $\\theta$.\nIn this blog post, I use the \u201conline\u201d version of EWC described by Husz\u00e1r [2]."}, {"title": "3 Elastic Weight Consolidation", "content": "Now we are ready to take a detailed look at EWC. We start by formally defining this method. When training on a new task, to prevent catastrophic forgetting, rather than optimizing only the loss on the new task $l_{new}(\\theta)$, EWC adds an extra term to the loss that involves the Fisher Information:\n$l_{EWC}(\\theta) = l_{new}(\\theta) + \\frac{\\lambda}{2} \\sum_{i=1}^{N_{params}} F_i^{old} (\\theta_i - \\theta_i^{old})^2$\nIn this expression, $N_{params}$ is the number of parameters in the model, $\\theta_i$ is the value of parameter i (i.e., the ith element of weight vector $\\theta$), $F_i^{old}$ is the ith diagonal element of the model's Fisher Information matrix on the old data, and $\\lambda$ is a hyperparameter that sets the relative importance of the new task compared to the old one(s).\nEWC can be motivated from two perspectives, each of which I discuss next."}, {"title": "3.1 Penalizing Important Synapses", "content": "Loosely inspired by neuroscience theories of how synapses in the brain critical for previously learned skills are protected from overwriting during subsequent learning [6], a first motivation for EWC is that when training on a new task, large changes to network parameters important for previously learned task(s) should be avoided. To achieve this, for each parameter $\\theta_i^{old}$, the term $\\frac{\\lambda}{2} F_i^{old} (\\theta_i - \\theta_i^{old})^2$ penalizes changes away from $\\theta_i^{old}$, which was that parameter's optimal value after training on the old data. Importantly, how strongly these changes are penalized differs between parameters. This strength is set by $F_i^{old}$, the ith diagonal element of the network's Fisher Information matrix on the old data, which is used as a proxy for how important that parameter is for the old tasks. The diagonal elements of the Fisher are a sensible choice for this, as they measure how much the network's output would change due to small changes in each of its parameters."}, {"title": "3.2 Bayesian Perspective", "content": "A second motivation for EWC comes from a Bayesian perspective, because EWC can also be interpreted as performing approximate Bayesian inference on the parameters of the neural network. For this we need to take a probabilistic perspective, meaning that we view the network parameters $\\theta$ as a random variable over which we want to learn a distribution. Then, when learning a new task, the idea behind EWC is to use the posterior distribution $p(\\theta|D_{old})$ that was found after training on the old task(s), as the prior distribution when training on the new task. To make this procedure tractable, the Laplace approximation is used, meaning that the distribution $p(\\theta|D_{old})$ is approximated as a Gaussian centered around $\\theta^{old}$ and with the Fisher Information $F^{old}$ as precision matrix. To avoid letting the computational costs become too high, EWC sets the diagonal elements of $F^{old}$ to zero."}, {"title": "4 A Closer Look at the Fisher Information", "content": "EWC thus involves computing the diagonal elements of the network's Fisher Information on the old data. Following the definitions and notation in Martens [9], the ith diagonal element of this Fisher Information matrix is defined as:\n$F_i^i := E_{x \\sim D_{old}} [E_{y \\sim p_{\\theta^{old}}}(y|x)} [(\\frac{\\delta log p_\\theta(y|x)}{\\delta \\theta_i})_\\theta=\\theta^{old})^2]]$        (1)\nIn this definition, there are two expectations: (1) an outer expectation over $D_{old}$, which is the (theoretical) input distribution of the old data; and (2) an inner expectation over $p_{\\theta^{old}}(y|x)$, which is the conditional distribution of y given x defined by the neural network after training on the old data. The different ways of computing the Fisher Information that can be found in the continual learning literature differ in how these two expectations are computed or approximated."}, {"title": "5 Different Ways of Computing the Fisher Information", "content": null}, {"title": "5.1 Exact", "content": "If computational costs are not an issue, the outer expectation in Eq (1) can be estimated by averaging over all available training data $D_{old}$, while in the case of a classification problem the inner expectation can be calculated for each training sample exactly:\n$F_{old, EXACT}^{i, i} = \\frac{1}{|D_{old}|} \\sum_{x \\in D_{old}} \\sum_{y=1}^{N_{classes}} p_{\\theta^{old}} (y|x) (\\frac{\\delta log p_{\\theta}(y|x)}{\\delta \\theta_i})_\\theta=\\theta^{old})^2$\nI refer to this option as EXACT, because for each sample in $D_{old}$, the diagonal elements of the Fisher Information are computed exactly. I am not aware of many implementations of EWC that use this way of computing the Fisher Information, but one example can be found in [10]. A disadvantage of this option is that it can be computationally costly, especially if the number of training samples and/or the number of possible classes is large, because for each training sample a separate gradient must be computed for every possible class."}, {"title": "5.2 Sampling Data Points", "content": "One way to reduce the costs of computing $F_i^{old}$ is by estimating the outer expectation using only a subset of the old training data:\n$F_{old, EXACT(n)}^{i, i} = \\frac{1}{n} \\sum_{x \\in S_{D_{old}}^{(n)}} \\sum_{y=1}^{N_{classes}} p_{\\theta^{old}} (y|x) (\\frac{\\delta log p_{\\theta}(y|x)}{\\delta \\theta_i})_\\theta=\\theta^{old}})^2$\nwhereby $S_{D_{old}}^{(n)}$ is a set of n random samples from $D_{old}$. Although this seems a natural way to reduce the computational costs of computing the Fisher Information, I am aware of only one study [11] that has implemented EWC in this way. Below, we will explore EWC with this implementation using n = 500. I refer to this option as EXACT (n=500), because for each data point that is considered, it is still the case that the exact version of the Fisher's diagonal elements are computed."}, {"title": "5.3 Sampling Labels", "content": "Another way to make the computation of $F_i^{old}$ less costly is by computing the squared gradient not for all possible classes, but only for a single class per training sample. This means that the inner expectation in the definition of $F_i^{old}$ is no longer computed exactly. To maintain an unbiased estimate of the inner expectation, Monte Carlo sampling can be used. That is, for each given training sample x, the class for which to compute the squared gradient can be selected by sampling from $p_{\\theta^{old}}(.|x)$. This gives:\n$F_{old, SAMPLE}^{i, i} = \\frac{1}{|D_{old}|} \\sum_{x \\in D_{old}}  (\\frac{\\delta log p_{\\theta}(c_x|x)}{\\delta \\theta_i})_\\theta=\\theta^{old}})^2$\nwhereby, independently for each x, $c_x$ is randomly sampled from $p_{\\theta^{old}}(.|x)$. I refer to this option as SAMPLE. This way of unbiasedly estimating the Fisher Information has been used in the implementation of EWC in [12, 13]."}, {"title": "5.4 Empirical Fisher", "content": "Another option is to compute the squared gradient only for each sample's ground-truth class:\n$F_{old, EMPIRICAL}^{i, i} = \\frac{1}{|D_{old}|} \\sum_{(x,y) \\in D_{old}}  (\\frac{\\delta log p_{\\theta}(y|x)}{\\delta \\theta_i})_\\theta=\\theta^{old}})^2$\nComputed this way, $F^{old}$ corresponds to the \u201cempirical\u201d Fisher Information matrix [9]. I therefore refer to this option as EMPIRICAL. Chaudhry and colleagues [14] advocated for using this option when implementing EWC. Their argument is that the \u201ctrue\u201d Fisher (i.e., the option to which in this blog post I refer as EXACT) is computationally too expensive, and that, because at a good optimum the model distribution $p_{\\theta^{old}}(.|x)$ approaches the ground-truth output distribution, the empirical Fisher is expected to behave in a similar manner as the true Fisher. However, as mentioned in the introduction, in the optimization literature, researchers have cautioned against using the empirical Fisher as approximation of the true Fisher [3]. Nevertheless, in continual learning, it still appears to be rather common to implement EWC using the empirical Fisher, or as we will see next an approximate version of the empirical Fisher."}, {"title": "5.5 Batched Approximation of Empirical Fisher", "content": "The last option that we consider has probably come about thanks to a feature of PyTorch. Note that all of the above ways of computing $F_i^{old}$ require access to the gradients of the individual data points, as the gradients need to be squared before being summed. However, batch-wise operations in PyTorch only allow access to the aggregated gradients, not to the individual, unaggregated gradients. In PyTorch, the above ways of computing $F_i^{old}$ could therefore only be implemented with mini-batches of size one. Perhaps in an attempt to gain efficiency, several implementations of EWC can be found on Github that compute $F_i^{old}$ by squaring the aggregated gradients of mini-batches of size larger than one. Indeed, popular continual learning libraries such as Avalanche [15] and PyCIL [16] use this approach, which probably makes this variant of computing the Fisher the one that is most used in the continual learning literature. Typically, these batched implementations only use the gradients for the ground-truth classes (i.e., they are approximate versions of the empirical Fisher):\n$F_{old, BATCHED (b)}^{i, i} = \\frac{1}{|D_{old}|} \\sum_{B \\in D_{old}^{(b)}} \\sum_{(x,y) \\in B}  (\\frac{\\delta log p_{\\theta}(y|x)}{\\delta \\theta_i})_\\theta=\\theta^{old}})^2$\nwhereby $D_{old}^{(b)}$ is a batched version of the old training data $D_{old}$, so that the elements of $D_{old}$ are mini-batches with b training samples. (And $|D|$ is the number of mini-batches, not the number of training samples.) Below, we will explore this option using b = 128, referring to it as BATCHED (b=128)."}, {"title": "6 Empirical Comparisons", "content": "Now, let us empirically compare the performance of EWC with these various ways of computing the Fisher Information. To do so, I use two relatively simple, often used continual learning benchmarks: Split MNIST and Split CIFAR-10. For these benchmarks, the original MNIST or CIFAR-10 dataset is split up into five tasks with two classes per task. Both benchmarks are performed according to the task-incremental learning scenario, using a separate softmax output layer for each task. For Split MNIST, following [10], a fully connected network is used with two hidden layers of 400 ReLUs each. For Split CIFAR-10, following [17, 18], a reduced ResNet-18 is used without pre-training. For both benchmarks, the Adam-optimizer [19] (\u03b2\u2081=0.9, \u03b2\u2082=0.999) is used to train for 2000 iterations per task with stepsize of 0.001 and mini-batch size of 128 (Split MNIST) or 256 (Split CIFAR). Each experiment is run 30 times with different random seeds, and reported are the mean \u00b1 standard error over these runs. Code to replicate these experiments is available at https://github.com/GMvandeVen/continual-learning."}, {"title": "6.1 Split MNIST", "content": "For the experiments on Split MNIST, the results are shown in Figure 1 and Table 1.\nFrom Table 1, we can see that for Split MNIST, when looking only at the performance of the best performing hyperparameter, there are no substantial differences between the various ways of computing the Fisher. However, from Figure 1, we can see that there are large differences in terms of the range of hyperparameter values that EWC performs well with. For example, when using the BATCHED option of computing the Fisher, EWC requires a hyperparameter orders of magnitude larger than the best hyperparameter for the EXACT option. This suggests that there might be important differences between these different ways of computing the Fisher, but that perhaps the task-incremental version of Split MNIST is not difficult enough to elicit significant differences in the best performance between them."}, {"title": "6.2 Split CIFAR-10", "content": "Therefore, let us look at the more difficult Split CIFAR-10 benchmark, for which the results are shown in Figure 2 and Table 2.\nIndeed, on this benchmark, there are significant differences between the different options also in terms of their best performance. The performance of EWC is substantially better when the Fisher Information is computed exactly, even when this is done only for a subset of the old training data, compared to when it is estimated or approximated in same way. We can further see that the SAMPLE option, which uses an unbiased estimate of the true Fisher, appears to perform somewhat better than using the empirical Fisher, but the difference is small and non-conclusive. Interestingly, also on this more difficult benchmark, using the batched approximation of the empirical Fisher still results in a similar best performance as using the regular empirical Fisher, although these two options do differ in terms of their optimal hyperparameter range."}, {"title": "7 Conclusion and Recommendations", "content": "I finish this blog post by concluding that the way in which the Fisher Information is computed can have a substantial impact on the performance of EWC. This is an important realization for the continual learning research community. Going forwards, based on my findings, I have three recommendations for researchers in this field. Firstly, whenever using EWC - or another method that uses the Fisher Information make sure to describe the details of how the Fisher Information is computed. Secondly, do not simply \"use the best performing hyperparameter(s) from another paper\u201d, especially if you cannot guarantee that the details of your implementation are the same as in the other paper. And thirdly, when using the Fisher Information matrix, it is preferable to compute it exactly rather than approximating it. If computational resources are scarce, it seems better to reduce the number of training samples used to compute the Fisher, than to cut corners in another way."}]}