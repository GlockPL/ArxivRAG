{"title": "AGENTSTORE: SCALABLE INTEGRATION OF HETEROGENEOUS AGENTS AS SPECIALIZED GENERALIST COMPUTER ASSISTANT", "authors": ["Chengyou Jia", "Minnan Luo", "Zhuohang Dang", "Qiushi Sun", "Fangzhi Xu", "Junlin Hu", "Tianbao Xie", "Zhiyong Wu"], "abstract": "Digital agents capable of automating complex computer tasks have attracted considerable attention due to their immense potential to enhance human-computer interaction. However, existing agent methods exhibit deficiencies in their generalization and specialization capabilities, especially in handling open-ended computer tasks in real-world environments. Inspired by the rich functionality of the App store, we present AgentStore, a scalable platform designed to dynamically integrate heterogeneous agents for automating computer tasks. AgentStore empowers users to integrate third-party agents, allowing the system to continuously enrich its capabilities and adapt to rapidly evolving operating systems. Additionally, we propose a novel core MetaAgent with the AgentToken strategy to efficiently manage diverse agents and utilize their specialized and generalist abilities for both domain-specific and system-wide tasks. Extensive experiments on three challenging benchmarks demonstrate that AgentStore surpasses the limitations of previous systems with narrow capabilities, particularly achieving a significant improvement from 11.21% to 23.85% on the OSWorld benchmark, more than doubling the previous results. Comprehensive quantitative and qualitative results further demonstrate AgentStore's ability to enhance agent systems in both generalization and specialization, underscoring its potential for developing the specialized generalist\u00b9 computer assistant.", "sections": [{"title": "INTRODUCTION", "content": "The continual evolution of computer Operating Systems (OS), along with proliferating applications, has transformed how people work and live. This transformation goes beyond daily life like shopping and gaming, encompassing professional works such as writing in Office or editing in Photoshop. However, this increased functionality comes with a steep learning curve, often burdening users. As a result, autonomous computer assistants-once limited to fiction like JARVIS in Iron Man or MOSS in Wandering Earth-have become a concrete pursuit, attracting great interest from researchers.\nAdvancements in Multimodal Large Language Models (MLLMs) , are gradually turning this vision into reality. MLLM-based agents have already demonstrated remarkable intelligence in handling complex tasks, benefiting from their strong capabilities in planning and reasoning (Wei et al., 2022; Yao et al., 2023). Following this trend, using MLLMs to build digital agents for automating computer tasks has become a promising direction (Zhang et al., 2024a). However, real-world OS environments encompass a diverse array of open-ended computer tasks, each with inherent requirements for capabilities across multi-dimensions (Xie et al., 2024), posing substantial challenges to existing methods. Specifically, \u201cTask_1\u201d in Figure 1 illustrates that many computer tasks necessitate specific knowledge and operations. In such scenarios, existing"}, {"title": "RELATED WORK", "content": "LLM-based Agents. Recent advancements in (M)LLMs  have led to the development of highly capable AI agents, applied across various domains, including robotics (Driess et al., 2023), software development (Wang et al., 2024), and beyond. A rapidly growing research field among these is automating interactions with computer environments to solve complex tasks. Early work primarily focused on specific scenarios, such as web manipulation (Yao et al., 2022; Deng et al., 2024; Xu et al., 2024), command-line coding (Sun et al., 2024), and gaming (Wang et al., 2023a). Following this, more recent methods (Wu et al., 2024; Tan et al., 2024) have started exploring general-purpose computer agents capable of interacting with diverse components of an operating system. Unfortunately, both of these struggle with open-ended tasks in real environments, exposing limitations in their generalization and specialization capabilities. To address these shortcomings, this paper introduces AgentStore to build the specialized generalist computer assistant.\nMulti-Agent Systems. Recently, various approaches  have been proposed to facilitate effective collaboration and communication among multi-agent to overcome hallucinations, ensuring deterministic and trustworthy results.\nWhile these approaches have shown promising results in domains such as automating coding, they still exhibit two major limitations. First, by using a fixed number of agents with predefined roles, they lack support for dynamically integrating agents. Second, their agents are usually homogeneous, which limits agent diversity and consequently constrains their range of capabilities. Therefore, our approach is designed to support the dynamic integration of a large number of third-party agents to leverage their advantages in quantity and diversity. AgentStore expands the capability boundaries of current multi-agent systems."}, {"title": "AGENTSTORE", "content": "We first provide a comprehensive overview and detail key components of the framework in Section 3.1. Then, Section 3.2 introduces MetaAgent, explaining how to effectively manage the rapidly growing and large number of agents via AgentToken. Finally, Section 3.3 details how AgentToken can be efficiently trained using an automated process with self-instruct.\n3.1 FRAMEWORK OVERVIEW\nAs illustrated in Figure 2, AgentStore consists of three main components: AgentPool, AgentEnroll, and MetaAgent. The AgentPool stores all feature-specific agents with distinct functionalities. AgentEnroll defines the integration protocol for adding new agents to the AgentPool. Finally, the MetaAgent selects the most suitable agent(s) from AgentPool to independently or collaboratively complete tasks. In this section, we provide a detailed explanation of these key components.\nAgentPool: The AgentPool is a collection of all available agents within AgentStore. To build the prototype of AgentStore, we organized over 20 agents within AgentPool, each with distinct functionalities. These agents range from unimodal to multimodal, from open-source to closed-source models, and from Command-Line Interfaces (CLI) to Graphical User Interfaces (GUI). The"}, {"title": "\u039c\u0395\u03a4AAGENT WITH AGENTTOKEN", "content": "We employ the powerful open-source MLLM as the foundation for our MetaAgent M. This enables it to process multi-modal information covering task descriptions and OS states. Given the set of all enrolled agents A, the goal of MetaAgent is to call a subset of these agents to automate computer tasks. Since the number of agents in AgentStore dynamically grows and reaches a large scale, common methods like In-Context Learning (ICL)  and full Fine-Tuning (FT)  become impractical due to the excessive context length and the high cost of retraining, respectively. Therefore, we propose the AgentToken strategy, which eliminates the need for lengthy contexts and significantly reduces the cost of retraining MetaAgent whenever a new agent is added.\nInspired by ToolkenGPT , which captures tool semantics using special tokens, AgentToken extends this concept by encoding enrolled agents as special tokens in the MetaAgent's vocabulary. Specifically, the agent tokens are parameterized as an embedding matrix $W_a \\in \\mathbb{R}^{|A| \\times d}$ and appended to the original word token head $W_t \\in \\mathbb{R}^{|V| \\times d}$. Assuming the agent tokens $W_a$ have been trained and available (as described in Section 3.3), the concatenated result forms the new language modeling head of MetaAgent. In this way, MetaAgent predicts the next token with the following probability:\n$P_M(t_i|t_{<i}) = \\text{softmax}([W_t; W_A] \\cdot h_{i-1}),$\nwhere the next token can be either a word token or an agent token, i.e., $t_i \\in V \\cup A$. The operation $[;]$ denotes concatenation, and $h_{i-1} \\in \\mathbb{R}^d$ represents the last hidden state. In this context, AgentToken enables MetaAgent to fulfill its two primary functions:\nMetaAgent as Router: Following the above manner, the most probable next token is obtained by maximizing the conditional probability:\n$t = \\text{arg max}_{t \\in V \\cup A} (P_M(t_i|t_{<i})).$"}, {"title": "TRAINING AGENTTOKEN WITH SELF-INSTRUCT", "content": "The embedding WA corresponding to agent tokens are the only tunable parameters, introducing minimal additional training overhead. However, training these agent tokens requires a number of agent demonstrations that consist of the task descriptions and initial OS states. The corresponding token demonstrations were pre-collected for training in previous efforts (Hao et al., 2024; Chai et al., 2024). However, this strategy is not applicable in our scenario, as developers only provide a document about the agent, and it is unrealistic to expect them to supply massive demonstrations. Therefore, we propose an automated process with self-instruct  for tuning these tokens using demonstrations from the MetaAgent itself.\nThe overall process follows an iterative algorithm to guide the generation of extra demonstrations, beginning with a limited set of original demonstrations $S_1 = \\{(y_k,c_i)\\}_{k=1}^{z_i}$ and the agent description Ci provided in document di. Specifically, we first prompt MetaAgent with existing demonstrations and agent descriptions:\n$S' = M(S_i, c_i),$\nwhere MetaAgent M is expected to produce the new set of demonstrations $S'$. Following this, to ensure the quality of the generated outputs, we apply BERTScore  to all newly generated outputs $y' \\in S'$, ensuring both consistency and diversity. Specifically, we use a greedy algorithm (see Appendix C) to iteratively filter elements from $S'$, resulting in a refined set $S_{new} \\subseteq S'$. The new set satisfies the following conditions:\n$\\tau_1 < \\text{BETRScore}(y_k, y_j) \\le \\tau_2, \\forall y_k, y_j \\in S_i \\cup S_{new} \\text{ and } k \\ne j,$\nwhere BETRScore(\u00b7) represents the similarity between two demonstrations, with imposing a lower bound \u03c4\u2081 to avoid overly irrelevant outputs and \u03c4\u2082 ensuring diversity among them. In this way, we automatically filter the generated data, and the refined set is merged, i.e., S\u2081 = Si \u222a Snew.\nThe entire process is an automated iterative bootstrapping. MetaAgent further generates additional examples based on the augmented Si, with BERTScore guiding and filtering the outputs until a sufficient number of demonstrations are generated to meet the training requirements for AgentToken."}, {"title": "EXPERIMENTS", "content": "To assess the effectiveness and versatility of AgentStore, we conducted comprehensive experiments across a diverse range of tasks. These experiments aimed to address two key questions: (1) How crucial is the scalable integration of heterogeneous agents in AgentStore? (2) How important is AgentToken for dynamically managing a large number of agents in AgentStore?\nBenchmark OSWorld (Xie et al., 2024) provides a scalable and real environment for evaluating computer agents, encompassing 369 tasks involving real web and desktop applications across open domains. As one of the most realistic and challenging benchmarks, OSWorld is ideal for capturing the diversity and complexity of real-world computer tasks, making it well-suited for testing the capability range of agents. Thus we selected OSWorld as the primary platform for our experiments. For more detailed information on OSWorld, please refer to the Appendix D. We also employ the APPAgent  benchmark to validate that AgentStore can generalize to mobile OS platforms. It consists of nine popular mobile applications, each serving distinct purposes and collectively forming 45 tasks.\nSettings We employ InternVL2-8B  as the base model of our MetaAgent. Additionally, details regarding the Agents in the AgentPool can be found in Appendix A, along with the threshold selection for T\u2081 and 72 in Appendix C. We generated about 100 examples for each agent using self-instruct for token training. The AdamW optimizer was used with a learning rate of 4e-5 and a weight decay of 1.0, for a total of 10 training epochs. When executing the Hash Manager, K was set to 5. Further details on prompts can be found in the Appendix F."}, {"title": "How CRUCIAL IS THE SCALABLE INTEGRATION OF HETEROGENEOUS AGENTS?", "content": "4.1.1 \u039cAIN RESULTS ON OSWORLD\nTable 1 presents the performance comparison between our approach and previous SoTA generalist agents on OSworld. While more advanced base models can improve performance (e.g., GPT-40 outperforming GogVLM in CogAgent ), even the best base models still face significant challenges. Notably, these methods exhibit not only overall weak performance but also significant disparities and weaknesses in specific task categories, despite using the same base models. For instance, MMAgent and CRADLE  struggle with calculation tasks due to their lack of knowledge and operational capability in Excel, while Friday  and Open-Interpreter , CLI-based agents, fails to execute GUI operation effectively in tasks, e.g., Chrome or Thunderbird."}, {"title": "GENERALIZATION ON MOBILE OS PLATFORMS", "content": "Since the operations of mobile apps are entirely GUI-based, we design a dedicated agent for each app (a total of nine agents), which differs from AgentStore in computer environments. Specifically, these agents are generated through a combination of self-exploration and human demonstrations within their respective applications.\nTable 2 compares the performance of a single general agent with AgentStore on the APPAgent benchmark. As shown, the performance of the generalist agent, lacking specific knowledge of each app, is subpar across many applications, even when utilizing the strongest base model. In contrast, AgentStore constructs dedicated agents tailored to their respective applications, effectively addressing performance deficiencies in certain apps and demonstrating a significant performance improvement from 26.7% to 57.8%. This underscores the applicability of the AgentStore concept to other operating system platforms, highlighting its broader potential for application."}, {"title": "ANALYSIS OF AGENT QUANTITY AND DIVERSITY", "content": "To comprehensively analyze the advantages of scalable integration, we further explore the impact of the number and type of integrated agents within AgentStore on performance. To ensure thoroughness, we analyze AgentStore starting from a generalist MMAgent and incrementally add feature-specific agents in AgentPool to compare their effects on overall performance.\nWe employ two strategies for adding agents: one involves randomly selecting agents to incrementally add to the AgentPool, while the other categorizes agents into GUI and CLI types, starting with one type before supplementing with the other. As shown in Figure 3, performance gradually increases with the growing number of agents, confirming the performance benefits of scalable integration within AgentStore. Additionally, we observe differ-\nences between the two strategies: random selection maintains a consistent mix of agent types, leading to a more stable growth. In contrast, adding agents of only one type causes the growth rate to slow over time, but this is mitigated when the other type is introduced. This highlights the crucial role of agent diversity, demonstrating the importance of integrating heterogeneous agents. These findings emphasize that both the quantity and diversity of agents are key factors in AgentStore."}, {"title": "How IMPORTANT IS AGENTTOKEN FOR DYNAMICALLY MANAGING AGENTS?", "content": "In this section, extensive experiments demonstrate that AgentToken can enable MetaAgent to efficiently manage numerous agents, consistently outperforming advanced In-Context Learning (ICL) and Fine-Tuning (FT) techniques. We first evaluate MetaAgent's routing capability using the OSWorld benchmark, demonstrating the advantages of the AgentToken strategy in terms of effectiveness, efficiency, and low data requirements. Additionally, we assess its collaborative management ability on a newly proposed multi-agent tasks benchmark.\n4.2.1 \u039cETAAGENT AS ROUTER\nEffectiveness As shown in Table 3, ICL methods perform poorly as routers, even when using advanced models like GPT-40. This confirms our assertion that relying on simple descriptions and few-shot demonstrations to master new agents can be challenging. In contrast, other tuning methods show some improvement by training on more task demonstrations. However, these methods are highly dependent on the quantity of data (as discussed in the following sections), while their overall performance improvement remains marginal. In comparison, our AgentToken overcomes these challenges, requiring only minimal self-generated data to efficiently train the corresponding agent tokens. It demonstrates the most robust router capability. As shown in the bottom section of Table 1, after routing tasks through AgentToken, our AgentStore achieved a success rate of 23.85% on OSworld, significantly outperforming both ICL and FT strategies."}, {"title": "\u039cETAAGENT AS HASH MANAGER", "content": "Although the existing OSWorld includes a limited number of tasks involving multi-agent collaboration, the small quantity and overly complex subtasks make it challenging to conduct meaningful experiments on collaborative task processing. Therefore, to further evaluate MetaAgent's ability to predict and coordinate multiple agents for collaborative task execution, we developed a new benchmark based on OSWorld, comprising over 100 diverse tasks paired with agents in the AgentPool. This newly proposed benchmark allows us to assess the accuracy of both task decomposition and subtasks handling in a real environment. Additionally, we propose three metrics for evaluation: AgentMatch, SubtaskAcc, and ExecutionAcc, which respectively measure multi-agent prediction accuracy, subtask decomposition accuracy, and execution success rate. Detailed benchmark constructions and metric descriptions are provided in Appendix E.\nAs shown in Table 5, the FT method is not applicable in this scenario due to the infinite combinations of agents, making it impossible to pre-organize the necessary data for training. Moreover, while the ICL methods function to a certain extent, even with advanced commercial models, the constraints of overly long contexts and vast combinatorial spaces result in subpar outcomes. In contrast, AgentToken leverages its inherent task awareness, employing a hashing mechanism to significantly narrow the scope to a few selected agents, thereby demonstrating excellent performance across all metrics."}, {"title": "QUALITATIVE ANALYSIS", "content": "In Figure 5, we highlight representative examples of outcomes, along with detailed analysis, to illustrate how AgentStore enhances the overall system's capability to tackle complex, open-ended tasks in real-world environments. In Task-1, the agent is tasked with setting up automatic email forwarding, which involves frequent GUI interactions and requires a strong understanding of Thunderbird's layout and forwarding settings, posing challenges for those unfamiliar with email systems. However, when MetaAgent assigns the specialized MailAgent to handle the task, the agent efficiently navigates the software, knowing the exact steps to configure the forwarding settings. In particular, during the Step3, it executes a sequence of actions to accurately fill out the required forms and options, showcasing its advanced understanding and processing capabilities within the mail domain. Similarly, in Example 2, which requires complex processing of a spreadsheet, MetaAgent selects the SheetAgent from the AgentPool to handle the task, avoiding overly complex GUI interactions. SheetAgent possesses knowledge of \u201copenpyxl\u201d and a deep understanding of the steps needed to manipulate sheets, efficiently completing this task that is too challenging for previous generalist agents (Xie et al., 2024; Tan et al., 2024). In addition, Example 3 illustrates a system-wide task that requires collaboration among multiple agents. MetaAgent successfully decomposes the task into subtasks and assigns the appropriate agents to complete each one. This demonstrates AgentStore's ability to perceive the overall task structure, overcoming the limitations of isolated, single-specialist agents and showcasing its strong generalization capability. In summary, these examples highlight AgentStore's specialized generalist abilities in handling not only domain-specific but also system-wide tasks, underscoring its potential for building a specialized generalist computer assistant."}, {"title": "CONCLUSION", "content": "In this paper, we introduce AgentStore, a flexible and scalable platform for dynamically integrating various heterogeneous agents to independently or collaboratively complete complex OS tasks. Furthermore, we propose MetaAgent with the AgentToken strategy to achieve efficient management of the growing number of agents. Extensive experimental results validate both the importance of scalable integration and the effectiveness of the AgentToken strategy. Comprehensive quantitative analysis and qualitative results show that AgentStore expands the capabilities of existing agent systems in both generalization and specialization. We believe that as basic AGI models continue to evolve, AgentStore, as an open platform, will integrate more powerful agents, progressively advancing toward the vision of building the specialized generalist computer assistant."}, {"title": "AGENTPOOL", "content": "The AgentPool is a collection of all available agents within AgentStore. To build the prototype of AgentStore, we organized 20 agents within AgentPool, each with distinct functionalities. As shown in Table 6, these agents range from unimodal to multimodal, from open-source to closed-source models, and from Command-Line Interfaces (CLI) to Graphical User Interfaces (GUI). The diverse capabilities of these agents cover common applications and tasks in both daily life and professional settings. In addition to the domain-specific agents we developed, we also integrated existing agents, such as Friday (Wu et al., 2024) and . This demonstrates the scalability of our approach, which allows third-party agents to be added to the platform.\nSpecifically, for closed-source model agents, we uniformly use GPT-40 as the base model. For open-source model agents, single-modality agents are based on Llama 3.1 , while multi-modality agents are built on InternVL2 . The last column of Table 6 indicates whether the agent has the capability to solve tasks outside its own domain.\nFigure 6 illustrates the distribution of different types of agents, showing that the initial version of AgentStore maintains a consistent balance between GUI and CLI agents. Most models also support extensions to handle additional tasks. Due to the significant gap between open-source and close-commercial models, most agents in this version are currently based on close-commercial models."}, {"title": "AGENTENROLL", "content": "When a developer creates a new OS agent and seeks to integrate it into AgentStore, it is essential to register the agent's information in a standardized format. To ensure consistency in the integration process, we established an agent integration protocol. As shown in the template below, during enrollment, the developer completes a predefined form outlining the agent's capabilities, limitations, the applications it interacts with, and demonstrations of its functionality.\nThe completed form for each agent constitutes a document. Following the template, we present six typical agent documents related to LibreOffice tasks to help readers understand the AgentEnroll process and outcomes, as well as to provide a clearer view of the agents in the AgentPool. Due to space limitations, further details on additional agents will be available when the entire project is open-sourced.\nIn the actual enrollment process, we encourage developers to provide more demonstrations-the greater the number, the more comprehensive the document will be, which also facilitates agentToken training during the self-instruct process. In this paper, we provide 10 demonstrations for each agent, which is relatively lightweight but still effectively aids the Metaagent in learning and understanding the corresponding agent."}, {"title": "OSWORLD", "content": "OSWorld  is a scalable, computer environment designed for multimodal agents. This platform provides a real-world environment for assessing open-ended computer tasks involving various applications. In this section, we provide a detailed introduction to OSworld, focusing on three key aspects: the open-ended and diverse nature of tasks, the reliability of evaluations in real-world environments, and the varied capability requirements for agents. This aims to help readers understand the rationale behind using OSworld as the primary evaluation platform in the main text.\nD.1 OSWORLD TASKS\nOSWorld is a benchmark suite consisting of 369 real-world computer tasks, primarily based in an Ubuntu Linux environment, with a smaller portion covering Microsoft Windows. The tasks are sourced from the authors as well as various platforms like forums, tutorials, and guidelines. Each task is paired with a natural language instruction and a hand-crafted evaluation script for scoring. Figure 8 provides a detailed classification of tasks, showcasing their diversity and effectively reflecting the nature of open-ended tasks in real-world scenarios.\nD.2 REAL-WORLD COMPUTER ENVIRONMENT\nAs shown in Figure 9, OSworld provides an executable and controllable environment that supports task initialization, execution-based evaluation,"}, {"title": "REPRESENTITIVE EXAMPLES", "content": "In Table 7, we present several representative examples from OSworld, which aim to illustrate the distinct operational logic involved in different tasks and the diverse capabilities required. These examples help readers better understand the broad range of generalization and specialized skills necessary in real-world computer environments, which are challenging for a single agent to fully encompass."}, {"title": "OSWORLD-MULTI BENCHMARK", "content": "Building on OSworld, we further developed a new benchmark, OSWorld-Multi, to evaluate MetaAgent's ability to predict and coordinate multiple agents for collaborative task execution. OSWorld-Multi consists of 101 tasks, each requiring collaboration with paired agents from the AgentPool. In the following sections, we will introduce the construction process, task examples, and evaluation metrics.\nConstruction process To maximize the reuse of tasks, system states, and evaluation functions from OSworld, we adopted a reverse synthesis approach. By mining paired examples in OSWorld, we generated tasks requiring agent collaboration. Specifically, we first traversed all pairwise combinations of subtasks, applying a two-step validation process: an initial filtering with a large language model (LLM), followed by manual review. This method allowed us to select meaningful collaborative tasks. Moreover, this approach enabled the synthesis of tasks requiring not only two-agent collaboration but also those involving three or more agents. In the following section, we will present some of the generated collaborative task results to demonstrate the outcomes of this synthesis process.\nTask examples In the table below, we present several synthesized examples to help readers understand the generation process. Another advantage of this reverse synthesis approach is the presence of natural ground truth, allowing us to evaluate not only execution accuracy but also the accuracy of agent predictions and task decomposition. This enables a comprehensive assessment of collab-"}, {"title": "PROMPT DETAILS", "content": "We provide examples of MetaAgent prompts in different modes to help readers understand the inference process. It is important to note that in manager mode, the prompt templates in Section F.3 for AgentToken and ICL are identical. The key difference is that AgentToken reduces the number of input documents, effectively shortening the context length, which in turn improves performance.\nAdditional prompts, including those related to each individual agent and self-instruct, will be provided when the project is open-sourced."}, {"title": "PROMPT FOR ROUTER MODE FOR AGENTTOKEN", "content": "Imagine you have a complex task that needs to be executed on an\noperating system.\nThis task can be decomposed into sub-tasks corresponding to\nthe model's capabilities.\nYou have several agents with different specializations available.\nRequirements:\nThe task is assigned to one agent, the model should return\nthe one token of that agent.\nNow your task is: {task_name}"}, {"title": "PROMPT FOR ROUTER MODE FOR ICL", "content": "Imagine you have a complex task that needs to be executed on an\noperating system.\nThis task can be decomposed into sub-tasks corresponding to\nthe model's capabilities.\nYou have several agents with different specializations available.\n{agent_1_document}, {agent_2_document},...{agent_n_document}\nRequirements:\nThe task is assigned to one agent, the model should return the\nname of that agent.\nlike:\n###CalcAgent###\nNow your task is: {task_name}"}, {"title": "PROMPT FOR MANAGER MODE", "content": "Imagine you have a complex task that needs to be executed\non an operating system.\nThis task can be decomposed into sub-tasks corresponding\nto the model's capabilities.\nYou have agents with different specializations available:\n{agent_1_document},{agent_2_document},...{agent_n_document}\nRequirements:\nThe task requires multiple agents, the model should specify\nwhich sub-tasks each agent should handle.\nThe model should ensure that the task assignment optimizes\nefficiency and effectiveness, considering the unique\ncapabilities of each agent.\nreturn like:\n###AgentName1:compute the sum of data in a new sheet.###\n###AgentName2: upload the computed file to the google Drive###\nBe careful not to assign the same agent to perform tasks\nconsecutively.\ndon't return like this:\n###Agent1:compute the sum of data in a new sheet.###\n###Agent1: rename this sheet.###\nNow your task is: {task_name}"}]}