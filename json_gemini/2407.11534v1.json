{"title": "LRQ: Optimizing Post-Training Quantization for Large Language Models by Learning Low-Rank Weight-Scaling Matrices", "authors": ["Jung Hyun Lee", "Jeonghoon Kim", "June Yong Yang", "Se Jung Kwon", "Eunho Yang", "Kang Min Yoo", "Dongsoo Lee"], "abstract": "With the commercialization of large language models (LLMs), weight-activation quantization has emerged to compress and accelerate LLMs, achieving high throughput while reducing inference costs. However, existing post-training quantization (PTQ) techniques for quantizing weights and activations of LLMs still suffer from non-negligible accuracy drops, especially on massive multitask language understanding. To address this issue, we propose Low-Rank Quantization (LRQ) a simple yet effective post-training weight quantization method for LLMs that reconstructs the outputs of an intermediate Transformer block by leveraging low-rank weight-scaling matrices, replacing the conventional full weight-scaling matrices that entail as many learnable scales as their associated weights. Thanks to parameter sharing via low-rank structure, LRQ only needs to learn significantly fewer parameters while enabling the individual scaling of weights, thus boosting the generalization capability of quantized LLMs. We show the superiority of LRQ over prior LLM PTQ works under (i) 8-bit weight and per-tensor activation quantization, (ii) 4-bit weight and 8-bit per-token activation quantization, and (iii) low-bit weight-only quantization schemes.", "sections": [{"title": "1 Introduction", "content": "As ChatGPT and GPT-4 (OpenAI, 2023) have showcased unprecedented capabilities across various domains such as common sense reasoning, mathematical problem-solving, and coding proficiency, there has been an exponential surge in interest surrounding the development of Large Language Models (LLMs). This surge in interest has culminated in the recent release of cutting-edge LLMs like Llama (Touvron et al., 2023a), PaLM 2 (Google et al., 2023), and Llama 2 (Touvron et al., 2023b). Accordingly, serving LLMs has rapidly emerged as a significant concern in both academia and industry. This stems from the substantial memory footprint and considerable computational cost incurred when operating these language models with tens or hundreds of millions of parameters in FP16 format. Therefore, extensive efforts such as quantization or pruning are underway to compress LLMs and provide efficient deployment. In particular, quantization has garnered considerable interest among LLM engineers and researchers because quantization aids in not just model compression but also inference acceleration.\nLLM quantization techniques fall into two primary categories: weight-only quantization and weight-activation quantization. Weight-only quantization concentrates on enhancing memory-bound operations like matrix-vector multiplication by quantizing weights of LLMs into low-bit integers (e.g., 2-4 bits). With activations kept in FP16, weight-only quantization exhibits marginal accuracy degradation but is only effective in accelerating text generation inference for small batch sizes (e.g., a single batch). In contrast, weight-activation quantization aims to expedite computationally intensive operations, such as matrix-matrix multiplication, typically by quantizing both weights and activations of LLMs into 8-bit integers and employing INT8 GEMM kernels. This comprehensive quantization approach enables LLM serving for large batch sizes, thus enhancing LLM throughput and expediting LLM inference through integer matrix multiplication. Yet, it comes with the trade-off of potential non-negligible accuracy drop. While each approach boasts its own set of strengths and weaknesses, we first focus on weight-activation quantization on the grounds that achieving high-throughput LLM inference is important to handle a substantial volume of user requests in real time."}, {"title": "2 Method", "content": "In this section, we outline the post-training quantization (PTQ) background that our method, LRQ is based on, figure out the problem arising when quantizing LLMs, and formulate LRQ. Finally, we deepen an empirical understanding of how LRQ can improve generalization in quantized LLMs."}, {"title": "2.1 Background", "content": "Block-wise Reconstruction First of all, our method is based on block-wise reconstruction, which originates from BRECQ (Li et al., 2021) for the purpose of taking into account the intra-block dependency and has been widely used in QDrop (Wei et al., 2022), FlexRound (Lee et al., 2023b), and AQuant (Li et al., 2023) due to its efficacy to yield less generalization error than layer-wise reconstruction. As we concentrate on weight-activation quantization of LLMs that are generally Transformer-based models, the block-wise reconstruction process is applied to every Transformer block in the order of arrangement. To be more concrete, with a small set of calibration data, the objective of block-wise reconstruction is to find quantized weights $\\mathbf{W}$ by minimizing the block reconstruction error $||\\mathbf{W}\\mathbf{X}-\\hat{\\mathbf{W}}\\mathbf{X}||_3$ where $\\mathbf{W}$ and $\\mathbf{X}$ are the weights and inputs of a FP16 Transformer block while $\\hat{\\mathbf{X}}$ is the inputs of its quantized counterpart (i.e., the outputs of its immediately preceding Transformer block with all its previous Transformer blocks quantized).\nFlexRound Among PTQ studies that take advantage of block-wise reconstruction, FlexRound shows the state-of-the-art performance for a wide variety of models ranging from computer vision models to large language models including Llama. In FlexRound, the formulation of $\\hat{\\mathbf{W}}$ is written as\n$$\\begin{equation}  \\hat{\\mathbf{W}} = \\mathbf{s}\\_1 \\cdot \\lceil  \\frac{\\mathbf{W}}{\\mathbf{s}\\_1 \\odot exp(\\mathbf{S}\\_2)} \\rceil; \\tag{1} \\end{equation}$$\nwhere $\\mathbf{s}\\_1$ is a quantization step size, $\\mathbf{S}\\_2$ is a weight-scaling matrix whose shape is exactly the same as that of $\\mathbf{W}$, $\\lceil \\cdot \\rceil$ and $exp(\\cdot)$ indicate the rounding and exponential function, and $\\odot$ and $/$ represent"}, {"title": "2.2 Motivation", "content": "We hypothesize that the failure to generalize well on challenging benchmarks like MMLU arises from the necessity of learning an individual scale for every weight with limited calibration samples. Now that $\\mathbf{S}\\_2$ has as many learnable parameters as the size of $\\mathbf{W}$ in Eq. 1, FlexRound's objective to achieve flexible weight quantization through the assignment of an independent scale to each weight may be deemed excessive when applied to LLM.\nFor instance, for Llama 7B, the smallest model in Llama, FlexRound has to learn more than 200 million scales with only just a few hundred or thousand calibration samples. FlexRound may be therefore prone to overfitting when quantizing LLMs. To resolve this issue, there might be two solutions: (i) increasing calibration samples, and (ii) decreas-"}, {"title": "2.3 Low-Rank Quantization", "content": "To reduce the number of learnable parameters, we decompose a weight-scaling matrix, $\\mathbf{S}\\_2$, into a low-rank matrix before performing the reconstruction process. To be more specific, for $\\mathbf{W} \\in \\mathbb{R}^{C\\_{\\text{out}} \\times C\\_{\\text{in}}}$, $\\mathbf{S}\\_2 \\in \\mathbb{R}^{C\\_{\\text{out}} \\times C\\_{\\text{in}}}$ is factorized into $\\mathbf{L}\\_2\\mathbf{U}\\_2$ where $\\mathbf{L}\\_2 \\in \\mathbb{R}^{C\\_{\\text{out}} \\times r}$ and $\\mathbf{U}\\_2 \\in \\mathbb{R}^{r \\times C\\_{\\text{in}}}$ for $r < min(C\\_{\\text{out}}, C\\_{\\text{in}})$. Additionally, we supplement $\\mathbf{L}\\_2\\mathbf{U}\\_2$ with a row vector, $\\mathbf{r}\\_2 \\in \\mathbb{R}^{C\\_{\\text{out}} \\times 1}$ and a column vector, $\\mathbf{c}\\_2 \\in \\mathbb{R}^{1 \\times C\\_{\\text{in}}}$, which is inspired by the addition of a row or column vector (or both) to a low-rank matrix in recommendation systems, one of the most popular applications of low-rank structure, for better prediction of ratings by considering a bias for each user or each item (Jahrer and T\u00f6scher, 2012; Goodfellow et al., 2016; Koren et al., 2021). As a result, we formulate $\\hat{\\mathbf{W}}$ as\n$$\\begin{equation}  \\hat{\\mathbf{W}} = \\mathbf{s}\\_1 \\cdot \\lceil  \\frac{\\mathbf{W}}{\\mathbf{s}\\_1 \\odot exp(\\mathbf{L}\\_2\\mathbf{U}\\_2 + \\mathbf{r}\\_2 + \\mathbf{c}\\_2)} \\rceil \\tag{2} \\end{equation}$$\nwhich we refer to as 'Low-Rank Quantization (LRQ)'. At first, $\\mathbf{L}\\_2$ and $\\mathbf{U}\\_2$ are initialized to zeros and random values from a normal distribution"}, {"title": "2.4 Effect of Low-rank Matrices on Generalization of Quantized LLMs", "content": "Considering that a full weight-scaling matrix is substituted with a low-rank matrix as seen in Eq. 2 derived from Eq. 1, one might wonder (i) whether the minimization of block reconstruction error on calibration samples is feasible despite the use of low-rank matrices, and (ii) how the utilization of low-rank matrices can result in improved generalization performance on unseen benchmarks as Figure 1 demonstrates. To address these concerns, we conduct a comparative analysis of accumulated root mean square error (RMSE) between $\\mathbf{W}\\mathbf{X}$ and $\\hat{\\mathbf{W}}\\mathbf{X}$ for RTN, FlexRound, and LRQ.\nFor a calibration sample that is selected from the C4 dataset, even if both FlexRound and LRQ initially start their learning process from the same RTN baseline, LRQ achieves an almost identical accumulated RMSE to FlexRound, as illustrated in Figure 3(a). This observation underscores that the use of low-rank weight-scaling matrices does not pose any noticeable obstacle to the minimization of block reconstruction error on calibration data. For common sense reasoning and MMLU benchmarks that are unseen during the reconstruction stage, however, accumulated RMSE for LRQ is much"}, {"title": "3 Experiments", "content": "In this section, we first explore the influence of the rank r in Eq. 2 and the quantity of calibration samples on the performance of LRQ. Next, to verify the effectiveness of LRQ, we compare LRQ with existing state-of-the-art post-training quantization (PTQ) methods for open-source large language models (LLMs) such as Llama and Llama 2 by adopting per-channel asymmetric weight quantization, per-tensor asymmetric static activation quantization, and per-token asymmetric KV cache quantization. For the Llama 2 models in Table 4, however, the accuracy gap on the massive multi-task language understanding (MMLU) benchmark between quantized LLMs and their FP16 baselines is observed. In this sense, we also perform exper-"}, {"title": "3.1 Ablation Study", "content": "Rank Study To examine the impact of the rank r in Eq. 2 on the generalization on unseen benchmarks, we compare LRQ with different r (spanning from 64 to 8192) to FlexRound for Llama 7B as shown in Figure 4(a). The performance of LRQ (depicted by the red curve) either remains relatively stable (the left side of Figure 4(a)) or increases gradually from 33.97% to 34.47% (the right side of Figure 4(a)) with the rise in the rank r from 64 to 1024. As the rank r continuously increases from 2048 to 8192, however, the performance of LRQ eventually declines to match that of FlexRound (indicated by the blue curve) on both common sense reasoning tasks and MMLU, which leads us to set"}, {"title": "3.2 Per-tensor Asymmetric Static Activation Quantization", "content": "As meticulously studied in Xiao et al. (2022), per-tensor static activation quantization is hardware-efficient and can be implemented on off-the-shelf GPUs with FasterTransformer, the state-of-the-art Transformer inference framework provided from NVIDIA, to achieve up to 1.5\u00d7 inference speed-up and almost halving the memory footprint compared to FP16 baselines. Accordingly, we employ per-tensor asymmetric static activation quantization as well as per-channel asymmetric weight quantization. Moreover, we also quantize the KV cache to 8-bit with a per-token asymmetric quantization scheme. It is worth noting that for large batch sizes, the KV cache can consume a much larger amount of memory than the model size, thus causing a bottleneck in high-throughput LLM inference. Fortunately, the performance discrepancy before and after per-token asymmetric KV cache quantization is almost insignificant no matter which quantization method is selected, as presented in Appendix G. For this reason, we also additionally utilize per-token asymmetric KV cache quantization. Further experimental details are provided in Appendix H."}, {"title": "3.3 Per-token Asymmetric Activation Quantization", "content": "Although LRQ shows better performance than SmoothQuant and FlexRound on both common sense reasoning tasks and MMLU when employing per-tensor asymmetric static activation quantization, there is still the five-shot accuracy gap on MMLU between LRQ and FP16 baselines for Llama 2 as in Table 4. Thus, we also conduct experiments on Llama 2 with a per-token asymmetric activation quantization scheme. More details about experimental settings are given in Appendix H.\nIn Table 5 and 6, when quantizing weights to 4-bit and both activations and KV cache to 8-bit, LRQ can attain similar zero-shot performance to FP16 baselines on common sense reasoning benchmarks and narrow the five-shot performance difference between FP16 baselines and quantized models to less than 1.5 percent on the MMLU benchmark."}, {"title": "3.4 Low-bit Weight-only Quantization", "content": "As LRQ is designed as a post-training weight quantization technique for LLMs, we also run experiments on weight-only quantization for Llama 2 by"}, {"title": "4 Conclusion", "content": "We propose a simple yet effective post-training weight quantization approach for LLMs, LRQ that learns low-rank weight-scaling matrices for block-by-block reconstructing the outputs of an intermediate Transformer block. Using such low-rank matrices, we can decrease the number of learnable parameters effectively while allowing for scaling weights individually due to the sharing of learnable parameters through a low-rank structure, thereby enhancing the generalization of quantized LLMs. We show the superiority of LRQ over prior state-of-the-art LLM PTQ techniques under both weight-activation and weight-only quantization schemes."}, {"title": "Limitations", "content": "To push the limit of post-training weight-activation quantization, two research directions emerge: (i) 4-bit weight and 8-bit activation quantization, and (ii) INT4 weight-activation quantization. As explained in Appendix A, Lee et al. (2023a) attempted to quantize LLMs with 4-bit weight and 8-bit activation quantization, whereas Wei et al. (2023) and Shao et al. (2024) strived to quantize LLMs with INT6 and even INT4 weight-activation quantization. In this paper, we only deal with the former quantization scheme, 4-bit weight and 8-bit activation quantization, which is a limitation of our work.\nLike Wei et al. (2023) and Shao et al. (2024), we could also focus on INT4 weight-activation quantization rather than 4-bit weight and 8-bit activation quantization in Section 3.3. However, Liu et al. (2023a), an earlier LLM quantization work than Wei et al. (2023) and Shao et al. (2024), already exhibited the non-marginal accuracy degradation of 4-bit weight and 8-bit activation quantization despite the fact that Liu et al. (2023a) exploited quantization-aware training, not post-training quantization. For this reason, we prioritize 4-bit weight and 8-bit activation quantization over INT4 weight-activation quantization in this paper, thus proposing a new post-training weight quantization technique termed LRQ.\nAlthough we demonstrate the efficacy of LRQ in a 4-bit weight and 8-bit activation quantization scheme, the five-shot accuracy gap between LRQ and FP16 baselines is still over 1% percent for Llama 2 13B and 70B on the massive multitask language understanding (MMLU) benchmark. In order to enhance the practicality of weight-activation quantization below INT8 weight-activation quantization, it is essential not only to prevent severe performance degradation associated with INT4 weight-activation quantization but also to narrow such five-shot accuracy gaps to less than 1% percent. We therefore believe that 4-bit weight and 8-bit activation quantization is as important as INT4 weight-activation quantization as a future research direction. We also hope that our experimental results would pave the way for the application of 4-bit weight and 8-bit activation quantization to LLMs via PTQ in real world."}, {"title": "B Process of $\\mathbf{L}\\_2\\mathbf{U}\\_2 + \\mathbf{r}\\_2 + \\mathbf{c}\\_2$ in Eq. 2", "content": "Similar to the broadcasting process in Python Numpy, we add $\\mathbf{L}\\_2\\mathbf{U}\\_2$, $\\mathbf{r}\\_2$, and $\\mathbf{c}\\_2$.\nTo be more specific, let $\\mathbf{L}\\_2\\mathbf{U}\\_2$ be\n$$\\begin{bmatrix} L\\_2U\\_{(1,1)} & L\\_2U\\_{(1,2)} & \\cdots & L\\_2U\\_{(1,C\\_{\\text{in}})} \\\\ : & : & & : \\\\ L\\_2U\\_{(C\\_{\\text{out}},1)} & L\\_2U\\_{(C\\_{\\text{out}},2)} & \\cdots & L\\_2U\\_{(C\\_{\\text{out}},C\\_{\\text{in}})} \\end{bmatrix}$$\n$\\mathbf{r}\\_2$ be\n$$\\begin{bmatrix} r\\_1 \\\\ r\\_2 \\\\ : \\\\ r\\_{C\\_{\\text{out}}} \\end{bmatrix}$$\nand $\\mathbf{c}\\_2$ be\n$$\\begin{bmatrix} c\\_1 & c\\_2 & \\cdots & c\\_{C\\_{\\text{in}}} \\end{bmatrix}.$$\nThen, by the broadcasting process, $\\mathbf{L}\\_2\\mathbf{U}\\_2 + \\mathbf{r}\\_2 + \\mathbf{c}\\_2$ can be expressed as\n$$\\begin{bmatrix} L\\_2U\\_{(1,1)} + r\\_1 + c\\_1 & L\\_2U\\_{(1,2)} + r\\_1 + c\\_2 & \\cdots & L\\_2U\\_{(1,C\\_{\\text{in}})} + r\\_1 + c\\_{C\\_{\\text{in}}} \\\\ : & : & & : \\\\ L\\_2U\\_{(C\\_{\\text{out}},1)} + r\\_{C\\_{\\text{out}}} + c\\_1 & L\\_2U\\_{(C\\_{\\text{out}},2)} + r\\_{C\\_{\\text{out}}} + c\\_2 & \\cdots & L\\_2U\\_{(C\\_{\\text{out}},C\\_{\\text{in}})} + r\\_{C\\_{\\text{out}}} + c\\_{C\\_{\\text{in}}}. \\end{bmatrix}$$"}, {"title": "C Effect of $\\mathbf{r}\\_2$ and $\\mathbf{c}\\_2$ in LRQ", "content": "To show the effect of $\\mathbf{r}\\_2$ and $\\mathbf{c}\\_2$ in Eq. 2, we compare FlexRound, FlexRound with $\\mathbf{S}\\_2 = \\mathbf{L}\\_2\\mathbf{U}\\_2$, and LRQ for Llama 7B and 13B.\nAs evident from the tables above, FlexRound with $\\mathbf{S}\\_2 = \\mathbf{L}\\_2\\mathbf{U}\\_2$ surpasses the performance of FlexRound but falls short of LRQ, which implies that the effect of $\\mathbf{r}\\_2$ and $\\mathbf{c}\\_2$ cannot be ignored. It is noteworthy that the five-shot accuracy on MMLU can witness an increase ranging from 1.5% to 4% by simply substituting $\\mathbf{S}\\_2$ with $\\mathbf{L}\\_2\\mathbf{U}\\_2$, which corroborates the significance of leveraging the parameter-efficiency inherent in low-rank weight-scaling matrices."}, {"title": "H Implementation Details", "content": "For the quantization scheme depicted in Figure 7, both FlexRound and LRQ are implemented in the experimental setting of QDrop (Wei et al., 2022) with the exception of the number of iterations for block-wise reconstruction, the batch size, and the learning rate. For all the Llama and Llama 2 models, the number of iterations for block-wise reconstruction is set to 5000 for both FlexRound and LRQ. The learning rate and the batch size for FlexRound and LRQ are described in 17. Notice that when applying LRQ to Llama 2 70B, the key and value projection weights are quantized via not LRQ but FlexRound due to the presence of GQA (Ainslie et al., 2023) in Llama 2 70B. To obtain the experimental results in Table 1 and 3, per-token asymmetric KV cache quantization is applied after completing block-wise reconstruction for all the Transformer blocks. For both activation quantization and KV cache quantization, we employ rounding-to-nearest."}, {"title": "I Comparison of Computation Cost to Complete the Quantization Process between SmoothQuant, FlexRound, and LRQ", "content": "For a comparative analysis of SmoothQuant, FlexRound, and LRQ in terms of computational cost to complete the quantization process, as delineated in Table 20, we measure the execution time and peak GPU memory usage while quantizing Llama 7B with 8-bit per-channel asymmetric weight quantization and 8-bit per-tensor asymmetric static activation quantization using 512 calibration samples and a batch size of 2. Since both FlexRound and LRQ involve gradient-based optimization in a block-wise manner while SmoothQuant is a learning-free quantization method, FlexRound and LRQ naturally spend more time and GPU memory quantizing LLMs than SmoothQuant. In Table 20, LRQ's extended processing time compared to FlexRound is attributed to the multiplication involving $\\mathbf{L}\\_2$ and $\\mathbf{U}\\_2$ in Eq. 2. Despite the slightly longer runtime, LRQ demonstrates an advantage in peak GPU memory usage, utilizing 23.5 GB compared to FlexRound's 25.4 GB. This efficiency is attributed to LRQ's fewer learnable parameters in comparison to FlexRound."}, {"title": "J Computation-Memory Trade-off at Test Time", "content": "Since only a quantization step size ($\\mathbf{s}\\_1$) and an integer matrix ($\\hat{\\mathbf{W}}$) are required during inference, once we obtain an integer matrix by setting $\\hat{\\mathbf{W}}$ to $\\frac{\\mathbf{W}}{\\mathbf{s}\\_1 \\odot exp(\\mathbf{L}\\_2\\mathbf{U}\\_2 + \\mathbf{r}\\_2 + \\mathbf{c}\\_2)}$ after $\\mathbf{L}\\_2$, $\\mathbf{U}\\_2$, $\\mathbf{r}\\_2$, and $\\mathbf{c}\\_2$ are learned, there is no need to recompute the multiplication involving $\\mathbf{L}\\_2$ and $\\mathbf{U}\\_2$ at all. In other words, for inference, like other quantization methods, there are only a quantization step size $\\mathbf{s}\\_1$ and an integer matrix $\\hat{\\mathbf{W}}$ without the presence of $\\mathbf{L}\\_2$, $\\mathbf{U}\\_2$, $\\mathbf{r}\\_2$, and $\\mathbf{c}\\_2$. Therefore, the compute-memory tradeoff does not occur during inference."}, {"title": "K Average and Standard Deviation of FlexRound and LRQ", "content": "For common sense reasoning tasks in Table 1 and 2, LRQ slightly outperforms FlexRound in the case of Llama 2 70B and significantly surpasses FlexRound in the case of Llama 33B, but FlexRound is better than LRQ in the case of Llama 2 7B. To investigate how meaningful the improvement of LRQ over FlexRound is, we carry out three random trials for Llama 2 7B, Llama 33B, and Llama 2 70B, presenting the average and standard deviation of them.\nAs seen in Table 21, not only does the average of LRQ surpass that of FlexRound, but the standard deviation of LRQ is also smaller than that of FlexRound, which strengthens our assertion that FlexRound might be prone to overfitting when applied to the quantization of LLMs."}, {"title": "L Combination of SmoothQuant with FlexRound and LRQ", "content": "Table 22: Zero-shot performance of Llama 7B on common sense reasoning tasks (BoolQ, PIQA, HellaSwag, WinoGrande, ARC easy and challenge, and OpenBookQA) with per-channel asymmetric weight quantization and per-tensor asymmetric static activation quantization, while keeping the KV cache in FP16. Here, 'SQ + FlexRound' and 'SQ + LRQ' denote FlexRound and LRQ that initially begin their own learning process from the SmoothQuant baseline in lieu of the rounding-to-nearest baseline, respectively. The accuracy (%) is reported for common sense reasoning tasks. The number of bits used for weights, activations, and KV cache is expressed as W/A/KV.\nTable 23: Five-shot performance of Llama 7B on Massive Multitask Language Understanding with per-channel asymmetric weight quantization and per-tensor asymmetric static activation quantization, while keeping the KV cache in FP16. Here, \u2018SQ + FlexRound\u2019 and \u2018SQ + LRQ\u2019 denote FlexRound and LRQ that initially begin their own learning process from the SmoothQuant baseline in lieu of the rounding-to-nearest baseline, respectively. The accuracy (%) is reported for four groups of disciplines (STEM, Humanities, Social Science, and Other). The number of bits used for weights, activations, and KV cache is expressed as W/A/KV.\nAs SmoothQuant is orthogonal to block-wise reconstruction, one might wonder how the performance of FlexRound and LRQ would change when FlexRound and LRQ start their own learning process from the SmoothQuant baseline in place of the RTN baseline. Table 22 and 23 reveal the performance of 'SmoothQuant (SQ) + FlexRound' and 'SmoothQuant (SQ) + LRQ' on common sense reasoning benchmarks and the MMLU benchmark, respectively. Unfortunately, in most cases, SmoothQuant does not display its efficacy when combined with FlexRound and LRQ. Although SmoothQuant enhances five-shot performance of FlexRound on MMLU by almost two percent, \u2018SQ + FlexRound\u2019 still underperforms LRQ as well as \u2018SQ + LRQ\u2019 on MMLU, which implies that employing low-rank weight-scaling matrices would be a better choice than using full weight-scaling matrices with additional pre-processing like an uniform per-channel scaling transformation in SmoothQuant."}]}