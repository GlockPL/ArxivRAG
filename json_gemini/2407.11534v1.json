{"title": "LRQ: Optimizing Post-Training Quantization for Large Language Models by Learning Low-Rank Weight-Scaling Matrices", "authors": ["Jung Hyun Lee", "Jeonghoon Kim", "June Yong Yang", "Se Jung Kwon", "Eunho Yang", "Kang Min Yoo", "Dongsoo Lee"], "abstract": "With the commercialization of large language models (LLMs), weight-activation quantization has emerged to compress and accelerate LLMs, achieving high throughput while reducing inference costs. However, existing post-training quantization (PTQ) techniques for quantizing weights and activations of LLMs still suffer from non-negligible accuracy drops, especially on massive multitask language understanding. To address this issue, we propose Low-Rank Quantization (LRQ) a simple yet effective post-training weight quantization method for LLMs that reconstructs the outputs of an intermediate Transformer block by leveraging low-rank weight-scaling matrices, replacing the conventional full weight-scaling matrices that entail as many learnable scales as their associated weights. Thanks to parameter sharing via low-rank structure, LRQ only needs to learn significantly fewer parameters while enabling the individual scaling of weights, thus boosting the generalization capability of quantized LLMs. We show the superiority of LRQ over prior LLM PTQ works under (i) 8-bit weight and per-tensor activation quantization, (ii) 4-bit weight and 8-bit per-token activation quantization, and (iii) low-bit weight-only quantization schemes. Our code is available at https:// github.com/onliwad101/FlexRound_LRQ to inspire LLM researchers and engineers.", "sections": [{"title": "1 Introduction", "content": "As ChatGPT and GPT-4 (OpenAI, 2023) have showcased unprecedented capabilities across various domains such as common sense reasoning, mathematical problem-solving, and coding proficiency, there has been an exponential surge in interest surrounding the development of Large Language Models (LLMs). This surge in interest has culminated in the recent release of cutting-edge LLMs like Llama (Touvron et al., 2023a), PaLM 2 (Google et al., 2023), and Llama 2 (Touvron et al., 2023b). Accordingly, serving LLMs has rapidly emerged as a significant concern in both academia and industry. This stems from the substantial memory footprint and considerable computational cost incurred when operating these language models with tens or hundreds of millions of parameters in FP16 format. Therefore, extensive efforts (Frantar et al., 2023; Liu et al., 2023b) such as quantization or pruning are underway to compress LLMs and provide efficient deployment. In particular, quantization has garnered considerable interest among LLM engineers and researchers because quantization aids in not just model compression but also inference acceleration.\nLLM quantization techniques fall into two primary categories: weight-only quantization and weight-activation quantization. Weight-only quantization concentrates on enhancing memory-bound operations like matrix-vector multiplication by quantizing weights of LLMs into low-bit integers (e.g., 2-4 bits). With activations kept in FP16, weight-only quantization exhibits marginal accuracy degradation but is only effective in accelerating text generation inference for small batch sizes (e.g., a single batch). In contrast, weight-activation quantization aims to expedite computationally intensive operations, such as matrix-matrix multiplication, typically by quantizing both weights and activations of LLMs into 8-bit integers and employing INT8 GEMM kernels. This comprehensive quantization approach enables LLM serving for large batch sizes, thus enhancing LLM throughput and expediting LLM inference through integer matrix multiplication. Yet, it comes with the trade-off of potential non-negligible accuracy drop. While each approach boasts its own set of strengths and weaknesses, we first focus on weight-activation quantization on the grounds that achieving high-throughput LLM inference is important to handle a substantial volume of user requests in real time."}, {"title": "2 Method", "content": "In this section, we outline the post-training quantization (PTQ) background that our method, LRQ is based on, figure out the problem arising when quantizing LLMs, and formulate LRQ. Finally, we deepen an empirical understanding of how LRQ can improve generalization in quantized LLMs."}, {"title": "2.1 Background", "content": "Block-wise Reconstruction First of all, our method is based on block-wise reconstruction, which originates from BRECQ (Li et al., 2021) for the purpose of taking into account the intra-block dependency and has been widely used in QDrop (Wei et al., 2022), FlexRound (Lee et al., 2023b), and AQuant (Li et al., 2023) due to its efficacy to yield less generalization error than layer-wise reconstruction. As we concentrate on weight-activation quantization of LLMs that are generally Transformer-based models, the block-wise reconstruction process is applied to every Transformer block in the order of arrangement. To be more concrete, with a small set of calibration data, the objective of block-wise reconstruction is to find quantized weights $\\hat{W}$ by minimizing the block reconstruction error $||WX-\\hat{W}X||_F$ where $W$ and $X$ are the weights and inputs of a FP16 Transformer block while $\\hat{X}$ is the inputs of its quantized counterpart (i.e., the outputs of its immediately preceding Transformer block with all its previous Transformer blocks quantized).\nFlexRound Among PTQ studies that take advantage of block-wise reconstruction, FlexRound shows the state-of-the-art performance for a wide variety of models ranging from computer vision models to large language models including Llama. In FlexRound, the formulation of $\\hat{W}$ is written as\n$\\hat{W} = \\left[\\frac{W}{\\hat{s}_1 \\odot exp(\\hat{S}_2)}\\right] \\odot \\hat{s}_1 $\nwhere $\\hat{s}_1$ is a quantization step size, $\\hat{S}_2$ is a weight-scaling matrix whose shape is exactly the same as that of $W$, $[\\cdot]$ and $exp(\\cdot)$ indicate the rounding and exponential function, and $\\odot$ and $/$ represent"}, {"title": "2.2 Motivation", "content": "We hypothesize that the failure to generalize well on challenging benchmarks like MMLU arises from the necessity of learning an individual scale for every weight with limited calibration samples. Now that $\\hat{S}_2$ has as many learnable parameters as the size of $W$ in Eq. 1, FlexRound\u2019s objective to achieve flexible weight quantization through the assignment of an independent scale to each weight may be deemed excessive when applied to LLM.\nFor instance, for Llama 7B, the smallest model in Llama, FlexRound has to learn more than 200 million scales with only just a few hundred or thousand calibration samples. FlexRound may be therefore prone to overfitting when quantizing LLMs. To resolve this issue, there might be two solutions: (i) increasing calibration samples, and (ii) decreas-"}, {"title": "2.3 Low-Rank Quantization", "content": "To reduce the number of learnable parameters, we decompose a weight-scaling matrix, $\\hat{S}_2$, into a low-rank matrix before performing the reconstruction process. To be more specific, for $W \\in R^{C_{out} \\times C_{in}}$, $\\hat{S}_2 \\in R^{C_{out} \\times C_{in}}$ is factorized into $L_2U_2$ where $L_2 \\in R^{C_{out} \\times r}$ and $U_2 \\in R^{r \\times C_{in}}$ for $r < min(C_{out}, C_{in})$. Additionally, we supplement $L_2U_2$ with a row vector, $r_2 \\in R^{C_{out} \\times 1}$ and a column vector, $c_2 \\in R^{1 \\times C_{in}}$, which is inspired by the addition of a row or column vector (or both) to a low-rank matrix in recommendation systems, one of the most popular applications of low-rank structure, for better prediction of ratings by considering a bias for each user or each item (Jahrer and T\u00f6scher, 2012; Goodfellow et al., 2016; Koren et al., 2021). As a result, we formulate $\\hat{W}$ as\n$\\hat{W} = \\left[\\frac{W}{\\hat{s}_1 \\odot exp(L_2U_2 + r_2 + c_2)}\\right] \\odot \\hat{s}_1 $\nwhich we refer to as 'Low-Rank Quantization (LRQ)'. At first, $L_2$ and $U_2$ are initialized to zeros and random values from a normal distribution"}, {"title": "2.4 Effect of Low-rank Matrices on Generalization of Quantized LLMs", "content": "Considering that a full weight-scaling matrix is substituted with a low-rank matrix as seen in Eq. 2 derived from Eq. 1, one might wonder (i) whether the minimization of block reconstruction error on calibration samples is feasible despite the use of low-rank matrices, and (ii) how the utilization of low-rank matrices can result in improved generalization performance on unseen benchmarks as Figure 1 demonstrates. To address these concerns, we conduct a comparative analysis of accumulated root mean square error (RMSE) between $WX$ and $\\hat{W}X$ for RTN, FlexRound, and LRQ.\nFor a calibration sample that is selected from the C4 dataset, even if both FlexRound and LRQ initially start their learning process from the same RTN baseline, LRQ achieves an almost identical accumulated RMSE to FlexRound, as illustrated in Figure 3(a). This observation underscores that the use of low-rank weight-scaling matrices does not pose any noticeable obstacle to the minimization of block reconstruction error on calibration data. For common sense reasoning and MMLU benchmarks that are unseen during the reconstruction stage, however, accumulated RMSE for LRQ is much"}, {"title": "3 Experiments", "content": "In this section, we first explore the influence of the rank $r$ in Eq. 2 and the quantity of calibration samples on the performance of LRQ. Next, to verify the effectiveness of LRQ, we compare LRQ with existing state-of-the-art post-training quantization (PTQ) methods for open-source large language models (LLMs) such as Llama and Llama 2 by adopting per-channel asymmetric weight quantization, per-tensor asymmetric static activation quantization, and per-token asymmetric KV cache quantization. For the Llama 2 models in Table 4, however, the accuracy gap on the massive multitask language understanding (MMLU) benchmark between quantized LLMs and their FP16 baselines is observed. In this sense, we also perform exper-"}, {"title": "3.1 Ablation Study", "content": "Rank Study To examine the impact of the rank $r$ in Eq. 2 on the generalization on unseen benchmarks, we compare LRQ with different $r$ (spanning from 64 to 8192) to FlexRound for Llama 7B as shown in Figure 4(a). The performance of LRQ (depicted by the red curve) either remains relatively stable (the left side of Figure 4(a)) or increases gradually from 33.97% to 34.47% (the right side of Figure 4(a)) with the rise in the rank $r$ from 64 to 1024. As the rank $r$ continuously increases from 2048 to 8192, however, the performance of LRQ eventually declines to match that of FlexRound (indicated by the blue curve) on both common sense reasoning tasks and MMLU, which leads us to set"}, {"title": "3.2 Per-tensor Asymmetric Static Activation Quantization", "content": "As meticulously studied in Xiao et al. (2022), per-tensor static activation quantization is hardware-efficient and can be implemented on off-the-shelf GPUs with FasterTransformer, the state-of-the-art Transformer inference framework provided from NVIDIA, to achieve up to 1.5\\times inference speed-up and almost halving the memory footprint compared to FP16 baselines. Accordingly, we employ per-tensor asymmetric static activation quantization as well as per-channel asymmetric weight quantization. Moreover, we also quantize the KV cache to 8-bit with a per-token asymmetric quantization scheme. It is worth noting that for large batch sizes, the KV cache can consume a much larger amount of memory than the model size, thus causing a bottleneck in high-throughput LLM inference. Fortunately, the performance discrepancy before and after per-token asymmetric KV cache quantization is almost insignificant no matter which quantization method is selected, as presented in Appendix G. For this reason, we also additionally utilize per-token asymmetric KV cache quantization. Further experimental details are provided in Appendix H."}, {"title": "3.3 Per-token Asymmetric Activation Quantization", "content": "Although LRQ shows better performance than SmoothQuant and FlexRound on both common sense reasoning tasks and MMLU when employing per-tensor asymmetric static activation quantization, there is still the five-shot accuracy gap on MMLU between LRQ and FP16 baselines for Llama 2 as in Table 4. Thus, we also conduct experiments on Llama 2 with a per-token asymmetric activation quantization scheme. More details about experimental settings are given in Appendix H."}, {"title": "3.4 Low-bit Weight-only Quantization", "content": "As LRQ is designed as a post-training weight quantization technique for LLMs, we also run experiments on weight-only quantization for Llama 2 by"}, {"title": "4 Conclusion", "content": "We propose a simple yet effective post-training weight quantization approach for LLMs, LRQ that learns low-rank weight-scaling matrices for block-by-block reconstructing the outputs of an intermediate Transformer block. Using such low-rank matrices, we can decrease the number of learnable parameters effectively while allowing for scaling weights individually due to the sharing of learnable parameters through a low-rank structure, thereby enhancing the generalization of quantized LLMs. We show the superiority of LRQ over prior state-of-the-art LLM PTQ techniques under both weight-activation and weight-only quantization schemes."}, {"title": "Limitations", "content": "To push the limit of post-training weight-activation quantization, two research directions emerge: (i) 4-bit weight and 8-bit activation quantization, and (ii) INT4 weight-activation quantization. As explained in Appendix A, Lee et al. (2023a) attempted to quantize LLMs with 4-bit weight and 8-bit activation quantization, whereas Wei et al. (2023) and Shao et al. (2024) strived to quantize LLMs with INT6 and even INT4 weight-activation quantization. In this paper, we only deal with the former quantization scheme, 4-bit weight and 8-bit activation quantization, which is a limitation of our work.\nLike Wei et al. (2023) and Shao et al. (2024), we could also focus on INT4 weight-activation quantization rather than 4-bit weight and 8-bit activation quantization in Section 3.3. However, Liu et al. (2023a), an earlier LLM quantization work than Wei et al. (2023) and Shao et al. (2024), already exhibited the non-marginal accuracy degradation of 4-bit weight and 8-bit activation quantization despite the fact that Liu et al. (2023a) exploited quantization-aware training, not post-training quantization. For this reason, we prioritize 4-bit weight and 8-bit activation quantization over INT4 weight-activation quantization in this paper, thus proposing a new post-training weight quantization technique termed LRQ.\nAlthough we demonstrate the efficacy of LRQ in a 4-bit weight and 8-bit activation quantization scheme, the five-shot accuracy gap between LRQ and FP16 baselines is still over 1% percent for Llama 2 13B and 70B on the massive multitask language understanding (MMLU) benchmark. In order to enhance the practicality of weight-activation quantization below INT8 weight-activation quantization, it is essential not only to prevent severe performance degradation associated with INT4 weight-activation quantization but also to narrow such five-shot accuracy gaps to less than 1% percent. We therefore believe that 4-bit weight and 8-bit activation quantization is as important as INT4 weight-activation quantization as a future research direction. We also hope that our experimental results would pave the way for the application of 4-bit weight and 8-bit activation quantization to LLMs via PTQ in real world."}, {"title": "A Related Work", "content": "Quantization works can be generally categorized into quantization-aware training (QAT) and post-training quantization (PTQ). As QAT can maintain the performance of FP32/FP16 baselines, QAT has been applied to computer vision models (Jung et al., 2019; Esser et al., 2020; Lee et al., 2021). Notwithstanding, there exist many challenges associated with applying QAT to large language models (LLMs) due to the sheer scale of pre-training data and a huge amount of computational resources required for training on the whole pre-training dataset. Although Liu et al. (2023a) presented the possibility of applying QAT to LLMs, unforunately, they did not perform experiments on Llama 65B, the largest and best performing model among the Llama models, in spite of using a single 8-GPU node. On the other hand, as Frantar et al. (2023) demonstrated the application of PTQ to LLMs only with a single GPU, many researchers have recently paid attention to PTQ for LLMs.\nLLM PTQ can be classified into two categories: LLM weight-only quantization (Frantar et al., 2023; Lin et al., 2023) and LLM weight-activation quantization (Dettmers et al., 2022; Yao et al., 2022; Xiao et al., 2022; Lee et al., 2023b; Wei et al., 2023; Shao et al., 2024). For the former quantization, Frantar et al. (2023) quantized the weights of LLMs into low-bit integers based on layer-wise reconstruction, whereas Lin et al. (2023) did by not counting on reconstruction but per-channel scaling in consideration of both weight and activation magnitudes. Despite the fact that both studies exhibited decent quantization performance, the main benefit of weight-only quantization does not align with serving LLMs with high throughput as delineated in Section 1. In this light, we concentrates on weight-activation quantization.\nWhen it comes to weight-activation quantization, Yao et al. (2022) presented ZeroQuant with a 8-bit group-wise weight quantization scheme and a 8-bit per-token activation quantization scheme based on layer-wise knowledge distillation, and Dettmers et al. (2022) proposed LLM.int8() with a 8-bit per-channel weight quantization scheme and a 8-bit per-token activation quantization scheme while keeping activation outliers in FP16. As discussed in Xiao et al. (2022), however, ZeroQuant incurs severe accuracy degradation for an open-source LLM, and the inference latency of LLM.int8() can be higher than that of the FP16 baseline. To deal with both issues, Xiao et al. (2022) devised SmoothQuant that can preserve the accuracy of OPT (Zhang et al., 2022) by easing the difficulty of activation quantization and accelerate LLM inference by up to 1.5 times. Yet, SmoothQuant suffers from non-negligible performance degradation for other open-source models such as Llama and Llama 2 with a 8-bit per-tensor static activation quantization scheme as illustrated in Figure 1. FlexRound that Lee et al. (2023b) created showed the experimental results of Llama up to 33B with a 8-bit per-channel weight quantization scheme and a 8-bit per-tensor static activation quantization scheme, but FlexRound incurs considerable performance degradation on the massive multitask language understanding (MMLU) benchmark as described in Figure 1(b). Beyond INT8 weight-activation quantization, Lee et al. (2023a) attempted to quantize LLMs with 4-bit weight and 8-bit activation quantization, whereas Wei et al. (2023) and Shao et al. (2024) strived to quantize LLMs with INT6 and even INT4 weight-activation quantization."}, {"title": "B Process of $L_2U_2 + r_2 + c_2$ in Eq. 2", "content": "Similar to the broadcasting process in Python Numpy, we add $L_2U_2$, $r_2$, and $c_2$.\nTo be more specific, let $L_2U_2$ be\n$\\left[\\begin{array}{ccc}LU_{(1,1)} & LU_{(1,2)} & \\cdots & LU_{(1,C_{in})}\\\\ \\vdots & \\vdots & \\ddots & \\vdots\\\\ LU_{(C_{out},1)} & LU_{(C_{out},2)} & \\cdots & LU_{(C_{out},C_{in})}\\end{array}\\right]$\n$r_2$ be\n$\\left[\\begin{array}{c}r_1\\\\ r_2\\\\ \\vdots\\\\ r_{C_{out}}\\end{array}\\right]$\nand $c_2$ be\n$\\left[c_1  c_2  \\cdots  c_{C_{in}}\\right]$\nThen, by the broadcasting process, $L_2U_2 + r_2 + c_2$ can be expressed as\n$\\left[\\begin{array}{ccc}LU_{(1,1)} + r_1 + c_1 & LU_{(1,2)} + r_1 + c_2 & \\cdots & LU_{(1,C_{in})} + r_1 + c_{C_{in}}\\\\ \\vdots & \\vdots & \\ddots & \\vdots\\\\ LU_{(C_{out},1)} + r_{C_{out}} + c_1 & LU_{(C_{out},2)} + r_{C_{out}} + c_2 & \\cdots & LU_{(C_{out},C_{in})} + r_{C_{out}} + c_{C_{in}}\\end{array}\\right]$"}, {"title": "C Effect of $r_2$ and $c_2$ in LRQ", "content": "To show the effect of $r_2$ and $c_2$ in Eq. 2, we compare FlexRound, FlexRound with $\\hat{S}_2 = L_2U_2$, and LRQ for Llama 7B and 13B."}, {"title": "D Figures of Accumulated RMSE on Assorted Samples"}, {"title": "E Sensitivity of Accumulated RMSE to the Number of Calibration Samples", "content": "To figure out the sensitivity of accumulated root mean square error (RMSE) to the number of calibration samples used for the block-wise reconstruction, we compare accumulated RMSE between $WX$ and $\\hat{W}X$ for FlexRound and LRQ at the last Transformer block of Llama 7B with the number of calibration samples varying from 64 to 512. As depicted in Figure 8(a), the accumulated RMSE of the last Transformer block on a calibration sample diminishes with a reduction in the number of calibration samples. This phenomenon is because FlexRound and LRQ are more likely to be fitted to calibration samples as the number of calibration samples becomes smaller. Conversely, Figure 8(b) reveals that the accumulated RMSE of the last Transformer block on each unseen sample from common sense reasoning and MMLU decreases with a larger number of calibration samples.\nNotably, the pattern elucidated in Section 2.4 persists consistently across varying calibration sample sizes from 64 to 512. In other words, for every calibration sample size spanning from 64 to 512, LRQ consistently attains nearly identical accumulated RMSE to FlexRound for a calibration sample from the C4 dataset. Concurrently, the accumulated RMSE of LRQ remains markedly smaller than that of FlexRound for an unseen sample from common sense reasoning and MMLU. This observation provides additional support for the insight presented in Figure 3, as discussed in Section 2.4."}, {"title": "F Ratio of the number of learnable parameters in LRQ to the number of pre-trained weights"}, {"title": "G Comparison of Experimental Results before and after Per-token Asymmetric KV Cache Quantization", "content": "Table 11, 12, 13, 14, 15, and 16 show the comparison of experimental results before and after per-token asymmetric KV cache quantization. It can be easily seen that the performance difference before and after per-token asymmetric KV cache quantization is nearly inconsiderable no matter which quantization technique is chosen, as mentioned in Section 3.2. Furthermore, even without per-token asymmetric KV cache quantization, LRQ still outperforms prior state-of-the-art LLM post-training weight-activation quantization methods in most cases."}, {"title": "H Implementation Details", "content": "For the quantization scheme depicted in Figure 7, both FlexRound and LRQ are implemented in the experimental setting of QDrop (Wei et al., 2022) with the exception of the number of iterations for block-wise reconstruction, the batch size, and the learning rate. For all the Llama and Llama 2 models, the number of iterations for block-wise reconstruction is set to 5000 for both FlexRound and LRQ. The learning rate and the batch size for FlexRound and LRQ are described in 17. Notice that when applying LRQ to Llama 2 70B, the key and value projection weights are quantized via not LRQ but FlexRound due to the presence of GQA (Ainslie et al., 2023) in Llama 2 70B. To obtain the experimental results in Table 1 and 3, per-token asymmetric KV cache quantization is applied after completing block-wise reconstruction for all the Transformer blocks. For both activation quantization and KV cache quantization, we employ rounding-to-nearest.\nIn the case of quantization scheme indicated in Figure 8, both FlexRound and LRQ are first implemented in the experimental setting of BRECQ (Li et al., 2021) with the exception of the number of iterations for block-wise reconstruction, the batch size, and the learning rate. The number of iterations for block-wise reconstruction and the batch size are set to 5000 and 2 respectively, for every Llama 2 model regardless of the number of bits used for weights. Table 18 exhibits the learning rate for FlexRound and LRQ in the case of 8-bit and 4-bit weight quantization, respectively. As explained in the above paragraph, when LRQ is applied to Llama 2 70B, weights in key and value projections are quantized via FlexRound. Here, when quantizing Llama 2 7B into 4-bit via LRQ, the attention module is quantized via LRQ, but the feed-forward module is quantized via FlexRound. In addition, when quantizing Llama 2 70B into 4-bit via LRQ, the feed-forward module is quantized via LRQ, but the attention module is quantized via FlexRound. To gain the experimental results in Table 5 and 6, per-token asymmetric activation quantization and per-token asymmetric KV cache quantization are sequentially applied after finishing block-wise reconstruction for all the Transformer blocks. For both activation quantization and KV cache quantization, we employ rounding-to-nearest.\nAll experiments about SmoothQuant are conducted based on the code provided in the SmoothQuant github repository1. Following Xiao et al. (2022), we select $\\alpha$, the hyperparameter to determine how much difficulty of activation quantization to shift to weight quantization, to be 0.8 for every Llama model, 0.85 for Llama 2 7B and 13B, and 0.9 for Llama 2 70B."}, {"title": "I Comparison of Computation Cost to Complete the Quantization Process between SmoothQuant, FlexRound, and LRQ", "content": "For a comparative analysis of SmoothQuant, FlexRound, and LRQ in terms of computational cost to complete the quantization process, as delineated in Table 20, we measure the execution time and peak GPU memory usage while quantizing Llama 7B with 8-bit per-channel asymmetric weight quantization and 8-bit per-tensor asymmetric static activation quantization using 512 calibration samples and a batch size of 2. Since both FlexRound and LRQ involve gradient-based optimization in a block-wise manner while SmoothQuant is a learning-free quantization method, FlexRound and LRQ naturally spend more time and GPU memory quantizing LLMs than SmoothQuant. In Table 20, LRQ\u2019s extended processing time compared to FlexRound is attributed to the multiplication involving L2 and U2 in Eq. 2. Despite the slightly longer runtime, LRQ demonstrates an advantage in peak GPU memory usage, utilizing 23.5 GB compared to FlexRound\u2019s 25.4 GB. This efficiency is attributed to LRQ\u2019s fewer learnable parameters in comparison to FlexRound."}, {"title": "J Computation-Memory Trade-off at Test Time", "content": "Since only a quantization step size ($s_1$) and an integer matrix ($\\hat{W}$) are required during inference, once we obtain an integer matrix by setting $\\hat{W}$ to $\\left[\\frac{W}{\\hat{s}_1 \\odot exp(L_2U_2 + r_2 + c_2)}\\right]$ after $L_2$, $U_2$, $r_2$, and $c_2$ are learned, there is no need to recompute the multiplication involving $L_2$ and $U_2$ at all. In other words, for inference, like other quantization methods, there are only a quantization step size $s_1$ and an integer matrix $\\hat{W}$ without the presence of $L_2$, $U_2$, $r_2$, and $c_2$. Therefore, the compute-memory tradeoff does not occur during inference."}, {"title": "K Average and Standard Deviation of FlexRound and LRQ", "content": "For common sense reasoning tasks in Table 1 and 2, LRQ slightly outperforms FlexRound in the case of Llama 2 70B and significantly surpasses FlexRound in the case of Llama 33B, but FlexRound is better than LRQ in the case of Llama 2 7B. To investigate how meaningful the improvement of LRQ over FlexRound is, we carry out three random trials for Llama 2 7B, Llama 33B, and Llama 2 70B, presenting the average and standard deviation of them."}, {"title": "L Combination of SmoothQuant with FlexRound and LRQ", "content": "Table 22: Zero-shot performance of Llama 7B on common sense reasoning tasks (BoolQ, PIQA, HellaSwag, WinoGrande, ARC easy and challenge, and OpenBookQA) with per-channel asymmetric weight quantization and per-tensor asymmetric static activation quantization, while keeping the KV cache in FP16. Here, 'SQ + FlexRound' and 'SQ + LRQ' denote FlexRound and LRQ that initially begin their own learning process from the SmoothQuant baseline in lieu of the rounding-to-nearest baseline, respectively. The accuracy (%) is reported for common sense reasoning tasks. The number of bits used for weights, activations, and KV cache is expressed as W/A/KV.\nTable 23: Five-shot performance of Llama 7B on Massive Multitask Language Understanding with per-channel asymmetric weight quantization and per-tensor asymmetric static activation quantization, while keeping the KV cache in FP16. Here, \u2018SQ + FlexRound\u2019 and \u2018SQ + LRQ\u2019 denote FlexRound and LRQ that initially begin their own learning process from the SmoothQuant baseline in lieu of the rounding-to-nearest baseline, respectively. The accuracy (%) is reported for four groups of disciplines (STEM, Humanities, Social Science, and Other). The number of bits used for weights, activations, and KV cache is expressed as W/A/KV.\nAs SmoothQuant is orthogonal to block-wise reconstruction, one might wonder how the performance of FlexRound and LRQ would change when FlexRound and LRQ start their own learning process from the SmoothQuant baseline in place of the RTN baseline. Table 22 and 23 reveal the performance of 'SmoothQuant (SQ) + FlexRound' and 'SmoothQuant (SQ) + LRQ' on common sense reasoning benchmarks and the MMLU benchmark, respectively. Unfortunately, in most cases, SmoothQuant does not display its efficacy when combined with FlexRound and LRQ. Although SmoothQuant enhances five-shot performance of FlexRound on MMLU by almost two percent, \u2018SQ + FlexRound\u2019 still underperforms LRQ as well as \u2018SQ + LRQ\u2019 on MMLU, which implies that employing low-rank weight-scaling matrices would be a better choice than using full weight-scaling matrices with additional pre-processing like an uniform per-channel scaling transformation in SmoothQuant."}]}