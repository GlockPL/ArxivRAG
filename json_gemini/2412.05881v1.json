{"title": "3D-Consistent Image Inpainting with Diffusion Models", "authors": ["Leonid Antsfeld", "Boris Chidlovskii"], "abstract": "We address the problem of 3D inconsistency of image inpainting based on diffusion models. We propose\na generative model using image pairs that belong to the same scene. To achieve the 3D-consistent\nand semantically coherent inpainting, we modify the generative diffusion model by incorporating an\nalternative point of view of the scene into the denoising process. This creates an inductive bias that\nallows to recover 3D priors while training to denoise in 2D, without explicit 3D supervision. Training\nunconditional diffusion models with additional images as in-context guidance allows to harmonize\nthe masked and non-masked regions while repainting and ensures the 3D consistency. We evaluate\nour method on one synthetic and three real-world datasets and show that it generates semantically\ncoherent and 3D-consistent inpaintings and outperforms the state-of-art methods.", "sections": [{"title": "1. Introduction", "content": "Image inpainting refers to the task of filling in missing\nregions within an image [2,21] specified by a binary\nmask [10]. Such inpainted regions need to be seman-\ntically consistent and harmonized with the rest of the\nimage. Moreover, inpainting methods need to handle\nvarious forms of masks where, in extreme cases, a vast\nmajority of the image is missing. Any approach trained\nwith a certain mask distribution can lead to poor gen-\neralization to novel mask types [15].\nRecent advances in diffusion models led to image in-\npainting using Denoising Diffusion Probabilistic Models\n(DDPMs) [9,26]. The DDPM is trained to denoise the\nimage by reversing a diffusion process. Starting from\nrandomly sampled noise, it is iteratively applied for\na certain number of steps and produces the final im-\nage sample; DDPMs have shown a strong capacity to\ngenerate diverse and high-quality 2D images [6,9,18].\nMoreover, the problem of generalization to novel masks\ncan be circumvented by inpainting that leverages un-\nconditionally trained DDPMs [5,17]. Instead of learning\na mask-conditional generative model, they condition\nthe generation process by sampling from masked pixels\nduring the reverse diffusion iterations. The uncondi-\ntional DDPM is used as a generative prior to harmo-\nnize information between masked and non-masked re-\ngions, by performing forward diffusion of non-masked\nregions and reverse diffusion of masked ones, operat-\ning in pixel [17] or latent spaces [5]. This allows the\nnetwork to generalize to any mask during inference.\nDespite their impressive performance in generating\nphoto-realistic 2D images, diffusion models still lack\nthe 3D understanding during the generation process\n[1,10,36]. Their inability to control the 3D properties\nof objects in the generated images is particularly harm-\nful for image inpainting, which aims to harmonizing\n3D properties of masked and non-masked regions. 3D\nunderstanding of diffusion models has been recently\naddressed by adding explicit 3D priors [10,13] or incor-\nporating additional points of view [1]; however these\nmethods require 3D supervision which is hard to ob-\ntain [37].\nIn this paper, we address the problem of 3D-consistency\nof image inpainting and propose in-context inpainting by\ninjecting additional information into the image genera-\ntive model, without 3D supervision. We train a genera-\ntive model with image pairs that show the same scene\nand provide complementary information on the scene\ngeometry, possibly under different lighting conditions.\nWe exploit this complementarity when a part of the\nscene is occluded in one image but observed in another.\nAt inpainting step, the model receives an image with\na mask and inpaints the masked part while preserving\nthe 3D scene information.\nTo achieve the 3D-consistent inpainting, we modify the\ngenerative diffusion model. We train our model by in-\ncorporating the additional point of view of the scene\ninto the denoising process. This creates an inductive\nbias that allows to recover 3D priors while training to\ndenoise in 2D, without explicit 3D supervision. Train-\ning unconditional diffusion models with additional im-\nages as in-context guidance allows us to harmonize the\nmasked regions while inpainting and ensuring the 3D\nconsistency. We evaluate our method, named InCon-\nDiff, on one synthetic (HM3D) and three real-world\n(MegaDepth, StreetView and WalkingTour) datasets\nand show that it generates semantically harmonized\nand 3D-consistent inpaintings outperforming the state-\nof-the-art methods. Figure 1 shows examples of in-\npainting with InConDiff, with occlusions caused by the\npresence of cars, buses and pedestrians.\nOur contributions can be summarized as follows:\n\u2022 We train an unconditional diffusion model with\nan additional image as the in-context guidance for\nimage generation.\n\u2022 We apply post-conditional generative approach to\nimage inpainting where the mask on first image\nis given at the inference time; we modify the re-\nsampling schedule for harmonizing boundaries be-\ntween masked an non-masked regions of the im-\nage.\n\u2022 We prove its efficiency of InConDiff for image in-\npainting with semantic and random masks on one\nsynthetic and three real-world datasets."}, {"title": "2. Related Work", "content": "Image Inpainting. The task of image inpainting inputs\nan image and a binary mask that defines the erased\npixels [16,33]. The original image pixels are removed\nand new ones are generated based on the mask. The\ndomain of image inpainting was previously dominated\nby Generative Adversarial Networks (GANs) [11, 15,\n21] that use an encoder-decoder architecture as the\nmain inpainting generator, adversarial training, and\ntailored losses that aim at photo-realism [16,32]. Most\nGAN-based models output deterministic results as these\nmodels are trained with reconstruction losses and im-\nproved stability [33,35]. Recent GAN-based methods\nachieve diversity [12,39]; however, trained on single\nobject datasets, they are not extended to inpaint other\nscenes.\nImage Conditional Diffusion Models. Recent ad-\nvances in diffusion models and the high expressiveness\nof pretrained Denoising Diffusion Probabilistic Mod-\nels [9,18] led to using them as a prior for generic im-\nage inpainting. SohlDickstein et al. [26] applied early\ndiffusion models to inpainting. Song et al. [27] pro-\nposed a score-based formulation using stochastic dif-\nferential equations for unconditional image generation,\nwith an additional application to inpainting. Score-\nbased diffusion models was applied to conditional im-\nage generation in [3] and extended it to multi-speed\ndiffusion. Ho et al. [8] jointly train a conditional and\nan unconditional diffusion models and combine the re-\nsulting conditional and unconditional score estimates.\nImage-to-image translation with diffusion models was\nproposed in [24], with an application to inpainting.\nUnconditional Diffusion Models. RePaint [17] was\nfirst to show that a pretrained unconditional diffusion\nmodel can inpaint images without mask-conditioned\ntraining. It modifies the denoising process to condition\ngeneration on the non-masked image content. Sim-\nilarly, X-Decoder [40] processes an input image and\ntextual together for referring segmentation. It proceeds\nto prompt-based segmentation and, when combined\nwith diffusion models, can erase the segmented objects.\nLatentPaint [5] extended the denoising process from\npixel to latent space. Trained and evaluated mostly on\nhuman face datasets, these models perform well on 2D\ninpainting however they perform poorly on street view\nimages or indoor images with important scene geome-\ntry. In this paper, we are first to address 3D consistent\nimage inpainting and to make an additional step by inte-\ngrating additional images in the training unconditional\nDDPMs."}, {"title": "3. Diffusion Models for Image Inpainting", "content": "We first present an unconditional DDPM and describe\ninjecting in-context images in the reverse diffusion pro-\ncess. Then, we introduce 3D-consistent harmonization\nof masked and non-masked regions and modify the re-\nsampling schedule to accelerate the reverse process for\nthe mask-based inpainting."}, {"title": "3.1. DDPMS", "content": "Diffusion models are designed to generate an image\nxo by moving a starting full noise image xT ~ N(0, I)\nprogressively closer to the data distribution through\nmultiple denoising steps XT\u22121, ..., X0. The models are\ndivided into forward and reverse diffusion processes.\nDuring the forward process, noisy images x1, ..., xT are\ncreated by repeatedly adding Gaussian noise starting\nfrom a training image x0:\nq(Xt|Xt-1) ~ N(xt; \u221a1 \u2013 \u03b2txt\u22121, \u03b2tI), (1)\nwhere \u03b2t is a variance schedule that increases from \u03b20 =\n0 to \u03b2T = 1 and controls how much noise is added at\neach step. We initially use a cosine schedule [19] in our\nexperiments and consider replacing it with alternative\nschedules in Section 3.3. The iterative process in (1)\ncan be reshaped to directly obtain xt from xo in a single\nstep:\nq(xt|xo) = \u221a\u03b1\u0304txo + \u221a1 \u2212 \u03b1\u0304T\u20ac\nwith \u20ac ~ N(0, I), \u03b1\u0304t := \u03b11 . . . \u03b1t and at := (1 \u2013 \u03b2t).\n(2)\nThe reverse process aims at rolling back the steps of the\nforward process by finding the posterior distribution for\nthe less noisy image xt-1 given the more noisy image\nXt:\nq(Xt-1|Xt, X0) ~ N(xt\u22121; \u03bc\u03c4, \u03c3\u0399),\nwhere \u03bct :=\n\u221a\u03b1\u03c4(1 \u2013 \u03b1\u0304\u03c4\u22121)\n1 - \u03b1\u0304\u03c4\n\u221a\u03b1t-1\u03b2t\nx0 +\n1- \u03b1\u0304\u03c4\n\u221a1-at-1\nXt\nand \u03c3 := \u03b2t.\n(3)\nThe image xo being unknown, the distribution q can\nnot be directly computed. Instead, DMs train a neural\nnetwork g with parameters \u03b8 to approximate q and pre-\ndicts the parameters \u03bc\u03b8(xt, t) and E\u03b8(xt, t) of a Gaussian\ndistribution:\nPo (Xt-1|Xt) = N(xt\u22121; \u03bc\u0473(xt, t), \u03a3\u03b8 (xt, t)). (4)\nThe learning objective for the model (4) is derived by\nconsidering the variational lower bound of the density\nassigned to the data by the model [9]. This approximate\nposterior is sampled at each generation step to progres-\nsively get the less noisy image xt-1 from the more noisy\nimage x\u2081. It allows for a closed form expression of the\nobjective since q(xt-1|Xt, X0) is also Gaussian.\nInstead of predicting xo, the seminal work of Ho et\nal. [9] proposed to predict the cumulative noise e that\nis added to the current intermediate image xt. We use\nthe following parametrization of the predicted mean\n\u03bc\u03b8 (\u03a7\u03c4, t):\n\u03bc\u03b8 (x, t) = \n1\n\u221a\u03b1t\nXt -\n\u03b2t\n\u221a1-\u03b1\u0304t\ngo(xt, t)\n(5)\nand derive the following training objective\nLs = Et,x0,\u20ac||\u20ac \u2212 go(xt, t)||2.\n(6)\nBeyond the predicted mean, learning the variance\n\u03a3\u03b8(x, t) in (4) of the reverse process reduces further\nthe number of sampling steps and the inference time\n[18]."}, {"title": "3.2. DDPMs with in-context images", "content": "We modify the training of unconditional DDPMs by\ninjecting in-context images (see Figure 2) to better ap-\nproximate the target distribution q and improve 3D\nconsistency of image inpainting. Unlike the majority\nof generative diffusion models [9,18] and inpainting\nmethods [17,34], which use U-Net backbones, we adopt\nthe class of diffusion models based on Diffusion Trans-\nformers (DiTs) [22] which adhere to the best practices\nof Vision Transformers (ViTs) and make injecting of\nin-context image x' straightforward.\nBy injecting the second image x', the objective function\nof our model is also to predict the cumulative noise \u0454\nthat is added to the current intermediate image xt,\nL2 = Et,x0,\u20ac||\u20ac - go (xt, t, x')||2.\n(7)\nFor training the network ge(xt, t, x'), we adopt the prin-\nciple of cross-view completion which was shown to enable\na network to perceive low-level geometric cues highly\nrelevant to 3D vision tasks [28,29]. The architecture\nof our network is shown in Figure 3. It takes an input\nthe noised version of the first image xt and the clean\nversion of image x'. Both images are patchified and\nfed to an image encoder which is implemented using\na ViT backbone. All patches from the second image\nare encoded using the same ViT encoder with shared\nweights. The latent token representations output by\nthe encoder from both images are then fed to a decoder\nwhose goal is to denoise xt. The decoder uses a series of\ntransformer decoder blocks comprising cross-attention\nlayers. This allows noised tokens from the first image\nto attend clean tokens from the second image, thus\nenabling cross-view comparison and reasoning. The\nmodel is trained using a pixel reconstruction loss over\nall patches, similar to CroCo [28,29]."}, {"title": "3.3. Conditioning DDPMs on the known region", "content": "At the inference step, we predict missing regions of an\nimage defined by a mask region m; we use the mask to\ncondition the generative DDPM presented in the pre-\nvious section. We first use DINOv2 [20] for detecting\nand masking the occlusion classes like e.g. vehicle and\npedestrian; we then mask any patch containing at least\n1 masked pixel. We follow [17] and denote the masked\nregions as mx\u2122 and the known (non-masked) regions\nas (1 \u2013 m) \u00a9 xk. Since every reverse step (4) from xt\nto xt-1 depends on xt, we can alter the known regions\n(1 \u2013 m) x\u2081 as long as we keep the correct properties\nof the target distribution. Since the forward process is\ndefined by (1) as accumulative Gaussian noise, we can\nsample the intermediate image x\u2081 at any point in time\nusing (4). This allows us to sample the known regions\nmxt at any time step t. Using (2) and (4) for the\nmasked and known regions, respectively, one reverse\nstep can be described by the following expression,\nX-1\nm\nt-1\nXt-1\n~\n=\n=\n\u039d(\u221a\u03b1\u0304tmxo, (1 \u2212 \u03b1\u0304\u03c4)\u0399)\n\u039d(\u03bc\u03b8(xt, t), \u03a3\u03b8 (x, t))\nm \u00a9 x_\u2081 + (1 \u2212 m) x1\n-1\nt-1\nknown regions\nmasked regions\ncombined\n(8)\nTherefore, x1 is sampled using the known regions in\nthe image moxo, while x1 is sampled from the model,\ngiven the previous iteration xt. Then they are combined\nto the new sample xt-1.\nUnfortunately, the basic denoising schedule (see Sec-\ntion 3.1) is insufficient for harmonizing the bound-\naries between masked and non-masked regions, due\nto restricted flexibility of sampling from both parts.\nRePaint [17] paid a particular attention to the noise\nschedule and shown than the model needs more time\n-1\nto harmonize the conditional information x_\u2081 with the\ngenerated information x\u2081 before advancing to the\nnext denoising step t. They introduced a resampling\napproach to harmonize the masked and non-masked\ninput of the model. At an intermediate step, the resam-\npling diffuses the output Xt-1 back to xt by sampling\nxt \u2248 N(\u221a1 \u2212 Btxt\u22121, \u03b2tI). By scaling back the output\nand adding noise, information incorporated in the gen-\nerated region x\u2081 is still preserved in x\u2122. It leads to a\nnew xm which is both more harmonized with x and con-\ntains conditional information from it. Figure 5 shows\nan example of inference process, with several phases of\nnoise resampling jumps and diffusion.\nResampling with multiple jumps [17] archives the\nboundary harmonization between known and masked\nsegments, however it requires extra time for inference.\nTo reduce the inference time, we consider alternative\nnoise schedules. We additionally modulate the number\nof jumps over the entire denoising process by measur-\ning the smoothness of the intermediate representations\nas proposed in [4].\nWe modify the post-conditioned diffusion models by\nredefining the noise schedule, which is equivalent to\nimportance sampling of the noise across different in-\ntensities. As demonstrated in [7], allocating more com-\nputation costs to intermediate noise levels yields su-\nperior performance compared to linearly increasing\nloss weights, particularly under constrained computa-\ntional budgets. By running experiments and analyzing\nthe performance of different noise schedules, including\nLaplace, Cauchy, Cosine and their shifted and scaled\nversions [7], we adopt the Laplace noise schedule which\nallows to reduce by half the overhead of multiple sam-\npling from the distribution and the number of interme-\ndiate harmonization steps."}, {"title": "4. Experiments", "content": "We first present one synthetic and three real-world\ndatasets used in the experiments. We also describe\nour approach to select image pairs suitable for training\ndiffusion models with additional in-context images.\n\u2022 MegaDepth dataset [14] consists of around 300K im-\nages downloaded from the web corresponding to 200\ndifferent landmarks. For each landmark, a point cloud\nmodel obtained using structure-form-motion (SfM)\nwith COLMAP [25] is also provided.\n\u2022 Habitat-Matterport dataset (HM3D) [23] is the large\ndataset of synthetically generated 3D indoor spaces.\nIt consists of 1K high-resolution 3D scans (or digital\ntwins) of building-scale residential, commercial, and\ncivic spaces generated from real-world environments,\nall available with 3D meshes and camera poses.\n\u2022 StreetView dataset contains 100K street view images\nfrom cities in South Korea, collected by Naver Maps\u00b9.\nThe images are provided with 3D location, camera pose\nand recording timestamps.\n\u2022 WalkingTour dataset 2 is a collection of egocentric\nvideos captured in urban environments from cities in Eu-\nrope and Asia. It consists of 10 high-resolution videos,\neach showcasing a person walking through a urban\nenvironment.\nAll the datasets offer ways of getting information about\nthe geometry of the scene and the camera poses. To\nbe useful for training in-context diffusion models and\ninpainting, we use the geometry information for each\ndataset to extract image pairs that depict a scene with\nsome partial overlap. In addition to reasonable over-\nlap, we ensure good diversity of selected pairs. We\nfollow [29] in obtaining an image pair quality score\nbased on overlap and difference in viewpoint angle.\nFigure 4 shows examples of image pairs extracted from\nall datasets.\nEvaluation Metrics. We use three common evaluation\nmetrics to assess the performance and effectiveness of\ninpainting methods[31].\nThe Peak Signal to Noise Ratio (PSNR) [30] is a widely\nused simple metric for evaluating the quality of image\ninpainting and reconstruction. It measures the ratio\nbetween the maximum possible power of a signal and\nthe power of the noise present in the signal.\nThe Learned Perceptual Image Patch Similarity\n(LPIPS) [38] is a distance metric that is specifically\ndesigned to capture the perceptual similarity between\ntwo images. It is one of the most commonly used\nmetrics for evaluating deep learning inpainting &\nrestoration systems.\nThe Structural Similarity Index (SSIM) is a more ad-\nvanced metric for evaluating the quality of image in-\npainting and reconstruction. It takes into account the\nhuman perception of image quality by measuring the\nstructural similarity between two images, taking into\naccount the luminance, contrast, and structure of the\nimages.\nImplementation details. Depth of the ViT encoder and\ndecoder in InConDiff is 12 and 8, the number of heads\nis 12 and 16, respectively [29]. For every dataset, all\nimages are resized to 256x256; the patch size is 8. The"}, {"title": "4.2. Ablation studies", "content": "Impact of masking ratio. In evaluation results with\nrandom masks in Tables 1-2, we generated the mask as\na union of n=10 random rectangles, with the average\nmask ratio 0.4. Table 3 below ablates the average mask\nratio by running InConDiff and SD-XL inpainting on\nMegaDepth dataset, with additional masking values\n0.3, 0.5, 0.6 and 0.7. As the table shows, our InConDiff\nresists better than SD-XL to the larger random mask\nratio, and benefits from the in-context images to fill in\nthe masked segments.\nImpact of sampling scheduler. Evaluation results pre-\nsented in Tables 1-2 are reported for the resampling\nschedule with 1000 steps and 10 jumps, where the re-"}, {"title": "4.3. Limitations", "content": "Image inpainting with just an original image often leads\nto irrealistic off-context and hallucinations generated\nby diffusion models (some of such examples are shown\nand compared to InConDiff inpaintings in Supplemen-\ntary). An additional viewpoint of the scene provides\nin-context guidance and injects 3D priors making in-\npainting more realistic and consistent. Unfortunately,\nit can be insufficient for reaching the goal in crowded\nreal-world scenes. As result, inpainting can replace\nmasked obstacles with other obstacles. This requires\nexploiting multi-view information when parts of the\nscene are observed in some frames but occluded in oth-\ners. Extending one additional image by multiple ones\nwill allow to address other challenges, like consistent\nobject removal in videos, without 3D supervision."}, {"title": "5. Conclusion", "content": "We address the problem of 3D inconsistency of im-\nage inpainting based on diffusion models. We propose\na generative model using image pairs that belong to\nthe same scene. We modify the generative diffusion\nmodel by incorporating an alternative point of view\nof the scene into the denoising process. Training un-\nconditional diffusion models with additional images as\nin-context guidance allows to harmonize masked and\nnon-masked regions while repainting and ensures the\n3D consistency. We evaluate our method on one syn-\nthetic and three real-world datasets and show that it\ngenerates semantically coherent and 3D-consistent in-\npaintings and outperforms the state-of-art methods. An\nadditional viewpoint of the scene provides in-context\nguidance and injects 3D priors into the denoising pro-\ncess, reducing off-context hallucinations and making\ninpainting more realistic and 3D consistent, without 3D\nsupervision."}]}