{"title": "Compress and Compare: Interactively Evaluating Efficiency and Behavior Across ML Model Compression Experiments", "authors": ["Angie Boggust", "Venkatesh Sivaraman", "Yannick Assogba", "Donghao Ren", "Dominik Moritz", "Fred Hohman"], "abstract": "To deploy machine learning models on-device, practitioners use compression algorithms to shrink and speed up models while maintaining their high-quality output. A critical aspect of compression in practice is model comparison, including tracking many compression experiments, identifying subtle changes in model behavior, and negotiating complex accuracy-efficiency trade-offs. However, existing compression tools poorly support comparison, leading to tedious and, sometimes, incomplete analyses spread across disjoint tools. To support real-world comparative workflows, we develop an interactive visual system called COMPRESS AND COMPARE. Within a single interface, COMPRESS AND COMPARE surfaces promising compression strategies by visualizing provenance relationships between compressed models and reveals compression-induced behavior changes by comparing models' predictions, weights, and activations. We demonstrate how COMPRESS AND COMPARE supports common compression analysis tasks through two case studies, debugging failed compression on generative language models and identifying compression artifacts in image classification models. We further evaluate COMPRESS AND COMPARE in a user study with eight compression experts, illustrating its potential to provide structure to compression workflows, help practitioners build intuition about compression, and encourage thorough analysis of compression's effect on model behavior. Through these evaluations, we identify compression-specific challenges that future visual analytics tools should consider and COMPRESS AND COMPARE visualizations that may generalize to broader model comparison tasks.", "sections": [{"title": "INTRODUCTION", "content": "Machine learning (ML) models have dramatically increased in scale over the past several years, with published models rising from 1 billion parameters in 2018 to over 100 billion parameters as of 2024 [17,59]. This trend has produced models with exciting emergent capabilities that have enabled new user experiences, like real-time translation [44] and code generation [13]. However, this scale also incurs greater technical, financial, and environmental costs to integrate these models into everyday use [4]. As a result, model compression has emerged as an essential family of techniques to make large models viable for practical usecases, particularly in domains where models must run on end-user devices to lower latency or access private user data [24]."}, {"title": "BACKGROUND AND RELATED WORK", "content": "ML practitioners apply compression with the intent to maintain the accuracy of a large model while reducing the space required to store it and the time required to perform inference. However, which compression technique or combination of techniques will achieve this balance remains task- and model-specific [24]. The ML literature has proposed various compression techniques for different model architectures and user priorities, such as low space consumption, low latency, or fast execution on optimized hardware [11, 12, 16, 66]. Nevertheless, identifying the right compression strategy can require anywhere from a few to several dozen experiments [24], taking time that is often not accounted for in accuracy-focused model development timelines. It can also be challenging to communicate experimental results within and across teams, particularly those with varying ML expertise. Even when these efforts are successful, compression can alter model behavior in subtle and unexpected ways, creating new errors or biased outputs [27] that are hard to capture with a single metric.\nAlthough compression is increasingly used in research and industry domains, there has been little work using visualization to make compression techniques more interpretable and comprehensible. Initial work on compression visualization has focused on specific compression techniques, like neural network pruning [32, 53] or profiling a single model's power and performance characteristics on specific hardware [25]. While these methods begin to demonstrate the value of visual tools for compression tasks, ML practitioners often need to take a broader approach to experimentation. As they mix and match multiple techniques in different orderings, the number of experiments and models they produce quickly expands, creating visualization challenges that are not well-supported by available tools for either interactive compression or model comparison [19,40,41].\nIn this work, we explore how to address model compression challenges using interactive visualization. We first identify four model compression challenges by synthesizing prior qualitative findings on how ML practitioners use compression [24] with insights from the ML literature. In response to these challenges, we introduce COMPRESS AND COMPARE, an interactive visualization system for comparing the performance and behavior of a suite of compressed models. Through an overview visualization called the Model Map, our system helps users track their compression experiments and how they relate to one another. Users can select subsets of models to automatically visualize differences in their accuracy, efficiency, and provenance. To deeply inspect a smaller set of models, the system provides detailed comparisons of instance-level behaviors and internal activations. Through case studies and user studies, we demonstrate how COMPRESS AND COMPARE can lead to insights throughout model compression and how it expands the design space of interactive ML development tools to account for challenges made salient by compression. We contribute:"}, {"title": "DESIGN CHALLENGES FOR COMPRESSION", "content": "To identify key compression challenges and motivate the design of interactive tools for compression, we synthesized insights from ML and HCI literature. Recently, Hohman et al. [24] explored ML practitioners' needs and perspectives on model compression via an interview study with 30 compression experts. This work describes how compression experts experiment with different compression techniques to satisfy efficiency and accuracy constraints within multidisciplinary teams. While their focus was providing a broad overview of compression experts' tacit knowledge, we distill specific findings from their study that are relevant to the design of compression tools and supplement them with recent insights from ML literature to synthesize key challenges for our system to address."}, {"title": "DESIGN OF COMPRESS AND COMPARE", "content": "We developed an interactive interface called COMPRESS AND COMPARE to address the four compression design challenges (Sec. 3). The tool consists of two main views: the Compression Overview supports high-level comparison and model selection from large-scale compression experiments (C1 and C2), and the Performance Comparison view enables fine-grained inspection of model behaviors and internals for a small number of candidate models (C3 and C4)."}, {"title": "Compression Overview", "content": "In COMPRESS AND COMPARE, experiments are represented as a set of trees, where each node is a model and edge is an operation performed on a parent model to produce a child model. This structural choice helps address one of the main visualization challenges of tracking compression experiments (C1): simultaneously depicting the variation in metrics across models along with dependencies in how the models were generated. The Model Map (Fig. 1A) addresses this using a node-link tree diagram, where nodes are positioned using a custom algorithm that vertically aligns nodes based on either the operations used to produce them or their step in the compression experiment. Models are rendered as circles whose color and size encode performance properties, commonly accuracy and model size. To emphasize the sequential nature of compression experimentation, the color and width of the edges smoothly interpolate between the parent and child nodes. Hovering over a model displays a tooltip with the model's top-level metrics (Fig. 2), such as latency, size, sparsity, accuracy, and the operation that created this new model from its parent.\nWhile the Model Map's layout prioritizes understanding model dependencies, the Model Scatterplot (Fig. 1B) visualizes model metrics along the spatial axes, helping direct the user to viable models and find natural model groupings. For example, the classic Pareto curve often used by compression experts [24] can easily be recreated by setting model size or latency on the x-axis and accuracy on the y-axis. Node color and size are consistent between the Model Scatterplot and Model Map, and the two visualizations are connected via brushing and linking [3]. The Filter view also depicts model metrics through customizable histograms that can be brushed to filter the Model Map and Model Scatterplot. When the user specifies a filter, models that do not meet the filter criteria become semitransparent and unselectable. This allows users to narrow down the set of viable models by expressing their project's space and performance budgets (C2).\nWhen one or more models are selected, the user can view information about the models' metrics in the Selection Details view (Fig. 1C). While displaying metrics for multiple models in a table would be straightforward, it would obscure the dependency structure between the models, making it harder to reason about the effects of different compression operations. Therefore, we develop a technique to automatically create a grouped bar chart from a subset of the model dependency tree. Our algorithm traverses the tree recursively to identify a minimum-cost set of \"variables\" that compactly explain the selected models' differences. Variables include operation parameter values, presence or absence of an operation, and type of operation applied. The algorithm attempts to fit multiple alternative variable types at each stage of the tree traversal and chooses the assignment that results in the shortest and simplest set of variables (e.g., a variable for an operation's parameters is considered simpler than a variable for operation type). This ensures that similar operations are mapped to each other. For example, the models resulting from Prune \u2192 Quantize and Prune \u2192 Calibrate \u2192 Quantize can be explained by Calibrate = true or false.\nIf the models can be represented using two variables or fewer, we generate a bar chart by mapping the x-axis and color encodings to the two variables. If more variables are required, the algorithm attempts to iteratively simplify the variable set by identifying conditional dependencies and cumulative relationships between pairs of variables. As shown in Fig. 4A and B, this simplification allows us to visualize several successive operations as a single x-axis encoding or combine multiple variables that are related to the same operation. When the list of variables needed to describe the selection cannot be simplified to two encodings, a Refine Comparison view allows the user to generate bar charts for subsets of the selection that can be compared (Fig. 4C)."}, {"title": "Performance Comparison", "content": "While the Compression Overview visualizes trends across large sets of models, the Performance Comparison view enables a deeper comparison of a smaller group of models' Behaviors and Layers. We use a combination of juxtaposition and explicit encodings [18] to enable comparisons of multiple models at a time. Comparison is performed with respect to a base model, which is user-customizable but defaults to the selected model closest to the tree's root node.\nThe Behaviors tab (Fig. 5, left) presents the results of evaluating each selected model on a validation dataset. Each model is a column in a table, where rows represent either class-level comparisons or individual instances. When configuring their data, users can define comparison metrics that operate on the model outputs, enabling comparisons like differences in top-1 predictions or the KL divergence of the softmax probabilities. These comparison metrics are summarized in the table headers and are depicted as sparkline bar charts in each row. Notably, the interface supports both absolute per-model values and relative values compared to the base model. Users can sort and filter by absolute or relative metrics for any model, allowing them to quickly identify classes or instances impacted the most by compression (C3).\nThe final and lowest-level component of the interface is the Layers tab (Fig. 5, right), which exposes the internals of the selected models. Like the Behaviors tab, this view comprises a table where each column contains information about a model relative to the base; however, here, each row represents a module in the nested hierarchy of modules that makes up each network. Within this structure, the user can choose from visualizing the proportion of zero weights in each module, the distribution of the weight values, and the distribution of the activations (intermediate outputs) on a random data sample. Weight values and activations are depicted as stacked histograms so that the height of the bars forms the overall value distribution while the color indicates the degree of change relative to the base. This highlights parameters and models that have changed more than others, which can reveal bugs such as over-pruned layers or outlier activations (C4)."}, {"title": "Setup and Implementation Details", "content": "We designed COMPRESS AND COMPARE for a highly customizable user workflow. To begin visualizing models, users write a simple Python script that invokes the COMPRESS AND COMPARE backend server and provides information about the models. Users specify models as a JSON object that details the operations used to produce each model and their performance across a set of user-defined metrics. Users can easily integrate specification creation into their existing model training procedures by updating the JSON file each time they train and evaluate a new model or evaluate against a new metric. To access information about the model's behaviors and layers, users write Python callbacks to retrieve instance-level outputs and layer activations, both of which can be either pre-computed or evaluated in real-time. This flexibility allows COMPRESS AND COMPARE to support any Python-based ML toolchain and accommodate very large models. Additionally, the COMPRESS AND COMPARE Python package provides helper functions to accelerate setup with common frameworks such as PyTorch and HuggingFace. The model servers for the use cases in this paper require around 200 lines of code, mostly consisting of boilerplate code that would already have been written in the course of experimentation.\nThe COMPRESS AND COMPARE frontend, implemented in SvelteKit, is static and can be hosted publicly. Visualizations are developed using D3.js and LayerCake."}, {"title": "CASE STUDIES OF COMMON COMPRESSION TASKS", "content": "We illustrate how COMPRESS AND COMPARE supports real-world compression workflows via two case studies."}, {"title": "Repairing Models Broken By Compression", "content": "A previously accurate model can \"break\" when compression is applied too heavily or broadly, resulting in low performance and nonsensical outputs. However, it can be challenging for users to determine which components of a model are causing its performance to degrade after compression (Sec. 6.2.2). We demonstrate how COMPRESS AND COMPARE can help practitioners identify and resolve breakages (C4) in the context of a generative language model for question answering.\nWe use an off-the-shelf T5-Large model [46] that achieves an F1 score of 90.5% on the Stanford Question Answering dataset [47] (Fig. 6A). The model's original performance is competitive with humans', but since the model is large (775 million parameters), we'd like to compress it to improve its speed and space utilization. Following common compression workflows from our participants (Sec. 6.2) and the literature (Sec. 2), we apply magnitude pruning across all of the model's parameters. However, this causes steep performance drops even at low levels of compression (e.g., 4% F1 after pruning only 10% of parameters). Looking at the top changes in predicted answers in the Behaviors view (Fig. 6B), we see that magnitude pruning has broken the model's generation. The 10% pruned model repeats words from the context paragraph (e.g., \u201cSuper Bowl LII LII LII...\"), and the 30% pruned model's output is meaningless (\"a a ...\").\nCOMPRESS AND COMPARE can help us understand why magnitude pruning has negatively affected the model. There are many possible reasons this compression strategy could have failed - the model may have low compressibility, essential weights may have been inadvertently pruned, or magnitude pruning may not be well-suited to this task. Since it is challenging to determine the cause using performance alone, we use the Layers view to inspect parts of the model that have been pruned. Sorting the models' layers by how much their weights have changed, we see that the most changed layers are all normalization layers (Fig. 6C). Normalization layers ensure a consistent activation distribution throughout the model, so over-pruning them can lead to unexpected behavior. However, since the model has relatively few normalization weights, we would not necessarily expect magnitude pruning to have pruned them so aggressively. To test if pruning the normalization layers caused the performance drop, we design a follow-up experiment that restores the normalization layers in the pruned models to match the original model, effectively unpruning them. This leads to a full recovery in F1 for the 10% and 30% pruned models, indicating that pruning the normalization layers was a substantial issue in our original compression experiment.\nWe can also use COMPRESS AND COMPARE to understand if the repaired models can be pruned any further. To do so, we browse model activations in the Layers view for the original model, the 30% pruned model with restored normalization layers (the fixed model), and the 50% pruned model with restored normalization layers (a broken model). We observe that the outputs of the self-attention module have changed significantly during pruning, even in the working model, which may signify that the model is robust to changes in these modules. By pruning additional parameters from the attention modules, we can reduce the model size by roughly 30% while achieving 83% F1 score.\""}, {"title": "Discovering Compression Artifacts", "content": "Compression artifacts changes in model behavior caused by compression - can subtly affect model quality, decrease edge case performance, and increase bias without reducing overall accuracy, making them difficult to detect yet crucial to address. To demonstrate COMPRESS AND COMPARE's ability to identify compression artifacts (C3), we apply it to study known compression-induced biases in face classification models [27,36]. Following Hooker et al. [27], we train a ResNet18 [21] on CelebA [36] to predict whether each image has the attribute blond. Then, we iteratively train and compress seven ResNet18 [21] models on the same binary classification task using global magnitude pruning [70] at 10%, 30%, 50%, 70%, 90%, 95%, and 99% final sparsity (see Supplementary Material Sec. S3). Each resulting model achieves similar accuracy on the test set, ranging from 87.4% to 94.4%.\nTo identify potential sources of bias, we use COMPRESS AND COMPARE to understand how the models' small drops in performance are distributed over the images. In the CelebA dataset [36], male and not young are underrepresented attributes. If compression is introducing bias by forgetting rare classes, it will have a disparate impact on these images. To inspect this, we compare the relative accuracy of the pruned models to the uncompressed model in the Behaviors tab (Fig. 7B). This view immediately surfaces that compression has disproportionately impacted the performance of rare classes. While the 99%-pruned model makes 64.9% more errors for not male and 72.3% more for young, in the rare classes, it makes 145.5% more errors for male and 96.5% for not young. This is concerning since performance on underrepresented and difficult instances is a primary reason that ML practitioners start with a large model [33, 34]. This may signal the need for further model development, compression experiments, and bias mitigation before we are comfortable using these models. Previously, we would have needed to compute model performance on each attribute and interpret a table of relative percentages (such as in Hooker et al. [27]); however, using the visual affordances of COMPRESS AND COMPARE, we are able to quickly identify bias without additional computation.\nWhile COMPRESS AND COMPARE supports bias identification using traditional error metrics, it can also audit bias in settings where we do not have access to data with sensitive attribute labels. In this setting, we use COMPRESS AND COMPARE to identify a subset of images that may merit additional inspection or human-in-the-loop annotation, similar to Compression Identified Exemplars [26, 27]. We begin by loading the compressed and uncompressed models into the Behaviors tab and sorting all images by the change in error rate between the uncompressed and 99% compressed models (Fig. 7C). Many images are classified differently between the original model and the compressed models, indicating that these images are most sensitive to compression. Looking closely at these images, we see they are often challenging images (e.g., people with silver hair or hair occluded by headwear), confirming prior analysis [26, 27] that images sensitive to compression are often challenging to classify. In this way, COMPRESS AND COMPARE supports data auditing to help users uncover instances sensitive to compression that may warrant further data cleaning and quality control."}, {"title": "USER STUDY WITH ML PRACTITIONERS", "content": "To evaluate COMPRESS AND COMPARE, we conducted a user study with eight ML practitioners who work on model compression. Participants included research scientists developing new compression techniques, ML engineers compressing models for deployment, and software engineers building compression tools (Table 1). The goal of the study was to understand how participants would use COMPRESS AND COMPARE to make sense of compression experiments, build intuition about compression strategies, and explore compressed model behavior."}, {"title": "Study Methods", "content": "Each study session was conducted via video chat and lasted 45 minutes-1 hour. We began by discussing the participant's current compression workflows and introducing them to COMPRESS AND COMPARE. Then, we asked participants to imagine they were part of a team compressing an existing image classification model and to think aloud as they used COMPRESS AND COMPARE to complete two tasks:"}, {"title": "Study Results", "content": "Overall, participants reported that COMPRESS AND COMPARE provided a structured compression workflow, enabling them to perform analyses that would have been challenging and time-consuming with existing tools, build intuition about compression techniques that inspire new compression experiments, and identify subtle but problematic model behaviors introduced during compression."}, {"title": "Unifying Disjoint Compression Workflows", "content": "Our user study participants reported needing multiple tools to execute their current compression tasks, leading to tedious back-and-forth analysis across tools that made it challenging to understand the overall impact of their compression experiments. While each participant was knowledgeable about compression (average 3.5 on a self-reported 1-5 scale of compression experience) and used compression regularly (5/8 use compression in every project), there was not a standard set of tools used by all participants. This inconsistency was often due to a lack of compression-specific tooling that supported the breadth of participants' tasks, including training and compressing models (P1-P8), evaluating their performance and behavior (P4, P6, P7), and ensuring they meet specified efficiency budgets (P1-P8). As a result, participants often created custom tools, such as Jupyter Notebook charts for performance analyses (P1, P6, P8) and spreadsheets for model tracking (P3, P5), or they repurposed existing general model analysis tools to understand the performance of a compressed model (P1-P4). When compression-specific tools existed, participants found them invaluable to their analysis (P3, P4). However, these tools were often still specific to a particular aspect of compression analysis (e.g., hardware performance [25]). As a result, participants found it challenging to perform comprehensive analysis and lamented that existing tools did not support many of their most critical tasks, like budgeting (P2), layer-wise analysis (P1), and compression-specific comparison (P2, P3, P4).\nUnlike participants' existing workflows, with COMPRESS AND COMPARE, critical compression tasks, like metric analysis and budgeting (C2), experiment comparison (C1), and layer-wise behavioral inspection (C3 and C4) are all in one place:\nFor instance, an approach used by 4/8 participants was to identify candidate models that fit their performance budget via the Filter, hone into the one or two best-performing models using the Model Scatterplot metrics, and search for patterns in the best models' compression recipes using the Model Map. Through this process, participants quickly identified \"deployable\" models that were small enough to fit on device while still maintaining task performance. P1, P5, P6, P7, and P8's Compression Overview analysis quickly converged from 52 models to a single 8-bit quantized ResNet50 model that reduced latency and size while nearly maintaining the uncompressed model's accuracy. Having a comprehensive overview of compressed model variants gave participants confidence to pitch this model to their teammates and use it to design new experiments that could result in even greater efficiency.\nThe unified compression interface also sparked discussion about how COMPRESS AND COMPARE could integrate into collaborative compression settings. Participants regularly collaborate with team members to complete their compression tasks, such as sending compressed models to QA specialists for targeted evaluation (P4, P5), negotiating resource budgets with product managers (P6, P8), and mentoring model developers on compression methods (P2). However, it can be challenging to collaborate on a compressed model across various tools and with collaborators with varying skill sets. In participants' current workflows, experimental results are distributed across tools, so participants were excited to use COMPRESS AND COMPARE as a centralized communication tool. P6 and P8 were interested in presenting their experiments to budget managers using COMPRESS AND COMPARE to advocate for budget increases by interactively demonstrating how best-case model performance improves as the budget relaxes. Participants also expressed interest in using COMPRESS AND COMPARE to collaboratively compress models, such as by flagging potential compression-induced issues in the Behaviors view for review by their QA teammates (P4, P5) and setting up experiments in the Model Map that demonstrate compression pitfalls to less experienced engineers (P2). Whereas existing compression workflows tended to become ad hoc when experimenting with many different algorithms, techniques, and pipeline structures, participants were excited for COMPRESS AND COMPARE to provide structure to the collaborative search for an efficient and accurate model."}, {"title": "Building Intuition about Model Compression", "content": "Beyond simply selecting a desired compressed model, COMPRESS AND COMPARE's visual and interactive components helped participants build intuition for how compression algorithms impact model performance and generate hypotheses about ways to improve future experiments. Participants found the combination of the Model Map and the Selection Details view to be an intuitive way to understand and reason about the space of compression experiments. Viewing the columns in the Model Map helped users understand the set of compression algorithms that had been applied (P3, P4, P7) and identify patterns in how the best-performing models were generated (P2, P7). To dig into a particular pattern, participants would often run visual \"experiments\" by selecting a group of models within a region of the Model Map (e.g., all the magnitude pruned models) and comparing their metrics in the Selection Details view (P2, P4, P6, P8). These in-depth explorations influenced participants' intuitions about how compression techniques affected their models more broadly. For instance, by comparing the accuracy and efficiency of two quantized models, P4 identified that quantization preserved performance much better for a large ResNet50 (25.6M parameters) than for a smaller MobileNet V2 (3.5M parameters). While P4 regularly uses quantization, this discrepancy in performance caused them to reflect that the success of quantization \"is definitely dependent on the base model; if you want something to work for quantization, you have to start at the right place.\" Finding the best compression technique is challenging (C1), so building intuition for the types of models that benefit from quantization can help P4 design more effective compression recipes moving forward, such as those that only apply quantization to large models.\nBuilding on their high-level understanding of the experimental space, participants used the Performance Comparison views to develop a deeper intuition about compression's impact on models' internal representations. Using the Layers view, participants debugged subtle problems in compression experiments that led to poor model performance. For example, P8 recognized that a particular model's \u201cbatch norms had been absolutely flattened\" by quantization. Batch normalization layers can have a substantial impact on downstream performance because they set the output value ranges at each layer, so this finding led P8 to suggest \"freezing all the batch norms\" during quantization as a way to maintain model performance. By building intuition through their Layers exploration, participants ideated a range of subsequent experiments. These next steps included combining current compression techniques in new ways (e.g., combining magnitude and gradient pruning) and expanding the space of operations, such as by tailoring compression to specific layers of the network. For instance, in their Layers analysis, P5 and P7 noticed earlier network layers had fewer parameters yet were pruned at the same rate as later layers. They hypothesized that, since later layers have more parameters, they have more redundancy and could withstand greater compression rates, so they designed an experiment that pruned layers as a function of their number of parameters. With COMPRESS AND COMPARE, participants deepened their understanding of how model parameters respond to compression techniques (C4) and used their insights to generate new experiments that could lead to more efficient and accurate models."}, {"title": "Encouraging Comprehensive Compression Analysis", "content": "By interactively integrating behavioral analysis with traditional metric-based compression analysis, COMPRESS AND COMPARE extended participants' existing behavioral analysis workflows and motivated them to consider the broader impacts of compression. Evaluating compressed model behavior on held out data was a key aspect of some participants' workflows (P5-P8) because it helped them identify subtle but important behavioral changes (C3):\nParticipants used the Behaviors view to analyze model behavior across an entire dataset to ensure that compression had not induced biases or spurious correlations. The ability to sort by relative change in correctness helped them identify classes that experienced the most errors and inspect individual instances that were misclassified. This procedure revealed that many compressed models' mistakes were acceptable, such as mistakes on multi-object images (e.g., coffee pot in a stove image) or related classes (e.g., Great Dane misclassified as another dog breed). However, it also helped them uncover subtle patterns and identify potential compression-induced biases. For instance, P5 sorted the Behaviors view by decreasing change in correctness and observed that stove images had lost 22% accuracy, whereas overall the model only experienced a few percent decrease. Inspecting the stove images whose classifications had changed revealed a spurious correlation between stoves and microwaves. P5 worried that the compressed model could be relying on the presence of one to classify the other. By viewing model behavior over an entire dataset, participants, like P5, were able to identify concerning patterns in the model's behavior, hypothesize reasons for the problem (e.g., a disproportionate amount of training images contain both objects), and develop a plan to address them (e.g., flagging these examples for QA team review).\nWhile participants primarily analyze correctness in their current workflows, having access to additional plug-and-play metrics in the Behaviors view spurred new analysis processes. During P1's behavioral analysis, they discovered that magnitude pruning resulted in a larger KL divergence in output probabilities than quantization. While they do not use KL divergence in their standard analysis pipeline, having access to it in CoMPRESS AND COMPARE helped them distinguish between otherwise similar compressed models and select the one that best reflected the original model's outputs."}, {"title": "DISCUSSION", "content": "We present COMPRESS AND COMPARE, an interactive visualization system for tracking and comparing compression experiments. Based on challenges experienced by real-world users, we design COMPRESS AND COMPARE to support critical and unsupported compression analysis tasks, including managing interconnected compression experiments, interrogating the impact of compression on model behaviors, and ideating promising future compression experiments. Through case studies on generative language and image classification models, we demonstrate how our system helps users repair issues with compression and identify compression-induced bias. Moreover, our user study with compression experts illustrates how COMPRESS AND COMPARE shifts users' compression workflows from disjoint analysis across tools toward a single analysis platform that facilitates collaborative decision-making about model selection and exploration. Here, we discuss the implications of our results for future ML development and evaluation tools, as well as the current limitations of our work and possible solutions."}, {"title": "Designing Compression-Aware ML Workflows", "content": "Throughout the design and analysis of COMPRESS AND COMPARE, we encountered compression-specific challenges. While compression analysis could be considered a special case of general ML evaluation, our user study participants struggled to extend traditional model evaluation workflows to their compression-specific tasks. Our work suggests ways future tools can improve the overall ML development process by integrating compression considerations:\nBridging data- and model-centric evaluations. Most existing ML development tools either focus on data-centric evaluations of model accuracy and behavior [5, 6, 50, 54] or architecture-specific evaluations of model internal layer characteristics [25,42]. In contrast, we found that linking data-centric (i.e., Behaviors) and architecture-specific visualizations (i.e., Layers) helped users interpret the functional characteristics of model components, similar to systems like DeepCompare [40] and Activis [28]. For instance, in our case studies and user studies, participants used the connection between the Layers and Behaviors views to identify that compressing batch normalization layers directly worsened the quality of the model's outputs. By identifying a functional relationship between the model's architecture and its behavior, users were then able to ideate new, better-performing experiments (e.g., removing compression on normalization layers). By developing joint visualizations of evaluation data and model architectures, compression tools can help users connect aspects of a model's design to changes in its behavior.\nNegotiating trade-offs between model quality metrics. Compression practitioners often trade off between model size, latency, and accuracy. While existing ML pipelines have addressed similar issues between accuracy and fairness [58] and accuracy across tasks [35], interactive model selection tools have not explicitly explored helping practitioners make these trade-offs, e.g., by identifying Pareto frontiers. Further, our user study participants indicated that efficiency and accuracy budgets are often collaborative and malleable targets, as opposed to hard quantitative thresholds. By visualizing experimental results and metric trade-offs, compression tools can help practitioners communicate their constraints and advocate for budget changes when needed.\nTracking model provenance during iterative development. Unlike model architecture and hyperparameter search, where experiments are often simple Cartesian products of several variables, compression experimentation is more readily modeled as a tree of \"recipes\" where nodes represent models and edges represent operations. Practitioners often start with a single model or a few related models that are known to perform well and apply varying compression recipes to them, creating the branching structure depicted in the Model Map. This process results in many interconnected models, and it can be challenging to keep track of the operations that created a particular model and its relationship to other models. User study participants found visualizing models in this tree structure helped them build intuition for aspects of experiments that worked well and how future experiments may behave. Future compression tools may consider a similar tree visualization or new ways to communicate model provenance to practitioners.\nComparing complex differences across many models. Existing tools (including those for compression) tend to focus on profiling and evaluating a single model [1,6,25,53]. In contrast, our study underscores the value of directly supporting comparison [18,54]. By visually juxtaposing metrics, predictions, and internals from several models, COMPRESS AND COMPARE reduces the cognitive load required to determine which differences are most actionable. Our system enables workflows that rapidly transition between comparative relationships, ranging from black-box comparisons of top-level metrics to comparisons of individual layer activations. Future tools can prioritize comparison and look to practitioners' existing comparison strategies to understand when linking multiple comparative relationships leads to productive insights.\nThese compression-specific challenges provide a basis for the design of future compression-vis tools, but they could also suggest ways to improve general ML analysis workflows. For instance, our user study participants found the Model Map tree structure to be highly intuitive, and some suggested applying it to other stages of model development (P4, P18, P15), such as creating a timeline-based Model Map that organized model results based on when a user ran each experiment. Moreover, extending the layer-wise activation comparison in the Layers view could support complex hyperparameter search workflows."}, {"title": "Limitations and Future Work", "content": "We designed COMPRESS AND COMPARE to support practitioners' current compression workflows; however, as model compression becomes a more established and standardized discipline, it is possible that many of the iterative workflows we observed in this study will be superseded by automated approaches. However, interactive visualization has potential benefits for compression work even if automated approaches eventually become standard. First, even though effective AutoML strategies exist, these techniques often still require data scientists to invest considerable time to distill model behavior into a single objective function [29", "69": "has allowed people to understand how these models work, a better collective understanding of compression can make these techniques more accessible and inspire new approaches.\nOur user study and prior formative research primarily interviewed compression experts and ML engineers who were already well-versed in model compression. While this allowed us to get the most relevant feedback on how to design compression-specific systems, our system and takeaways do not consider the needs of novice users. It is likely COMPRESS AND COMPARE could be extended to support ML practitioners less accustomed to compression, such as by providing suggestions on what techniques to try or incorporating a graphical interface to run compression experiments. Such adaptations would empower non-experts to apply compression, but they could also support compression experts in running on-the-fly experiments based on their insights from COMPRESS AND COMPARE, such as removing compression from a specific layer or making a slight change to the sparsity value.\nFurther, several participants suggested extensions to COMPRESS AND COMPARE that could help it better match the specific needs of their teams, including supporting very large datasets and models as well as displaying custom efficiency metrics. Although the tool currently supports running model computations remotely or in advance, it does require the models being compared to be loaded simultaneously in memory so that each layer can be compared one-at-a-time. More efficient comparison techniques that do not require jointly loading several models may help COMPRESS AND COMPARE scale more easily.\nFinally, a key requirement of COMPRESS AND COMPARE is its integration between"}]}