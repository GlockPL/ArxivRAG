{"title": "Compress and Compare: Interactively Evaluating Efficiency and Behavior Across ML Model Compression Experiments", "authors": ["Angie Boggust", "Venkatesh Sivaraman", "Yannick Assogba", "Donghao Ren", "Dominik Moritz", "Fred Hohman"], "abstract": "To deploy machine learning models on-device, practitioners use compression algorithms to shrink and speed up models while maintaining their high-quality output. A critical aspect of compression in practice is model comparison, including tracking many compression experiments, identifying subtle changes in model behavior, and negotiating complex accuracy-efficiency trade-offs. However, existing compression tools poorly support comparison, leading to tedious and, sometimes, incomplete analyses spread across disjoint tools. To support real-world comparative workflows, we develop an interactive visual system called COMPRESS AND COMPARE. Within a single interface, COMPRESS AND COMPARE surfaces promising compression strategies by visualizing provenance relationships between compressed models and reveals compression-induced behavior changes by comparing models' predictions, weights, and activations. We demonstrate how COMPRESS AND COMPARE supports common compression analysis tasks through two case studies, debugging failed compression on generative language models and identifying compression artifacts in image classification models. We further evaluate COMPRESS AND COMPARE in a user study with eight compression experts, illustrating its potential to provide structure to compression workflows, help practitioners build intuition about compression, and encourage thorough analysis of compression's effect on model behavior. Through these evaluations, we identify compression-specific challenges that future visual analytics tools should consider and COMPRESS AND COMPARE visualizations that may generalize to broader model comparison tasks.", "sections": [{"title": "1 INTRODUCTION", "content": "Machine learning (ML) models have dramatically increased in scale over the past several years, with published models rising from 1 billion parameters in 2018 to over 100 billion parameters as of 2024 [17,59]. This trend has produced models with exciting emergent capabilities that have enabled new user experiences, like real-time translation [44] and code generation [13]. However, this scale also incurs greater technical, financial, and environmental costs to integrate these models into everyday use [4]. As a result, model compression has emerged as an essential family of techniques to make large models viable for practical usecases, particularly in domains where models must run on end-user devices to lower latency or access private user data [24]."}, {"title": "2 BACKGROUND AND RELATED WORK", "content": "ML practitioners apply compression with the intent to maintain the accuracy of a large model while reducing the space required to store it and the time required to perform inference. However, which compression technique or combination of techniques will achieve this balance remains task- and model-specific [24]. The ML literature has proposed various compression techniques for different model architectures and user priorities, such as low space consumption, low latency, or fast execution on optimized hardware [11, 12, 16, 66]. Nevertheless, identifying the right compression strategy can require anywhere from a few to several dozen experiments [24], taking time that is often not accounted for in accuracy-focused model development timelines. It can also be challenging to communicate experimental results within and across teams, particularly those with varying ML expertise. Even when these efforts are successful, compression can alter model behavior in subtle and unexpected ways, creating new errors or biased outputs [27] that are hard to capture with a single metric.\nAlthough compression is increasingly used in research and industry domains, there has been little work using visualization to make compression techniques more interpretable and comprehensible. Initial work on compression visualization has focused on specific compression techniques, like neural network pruning [32, 53] or profiling a single model's power and performance characteristics on specific hardware [25]. While these methods begin to demonstrate the value of visual tools for compression tasks, ML practitioners often need to take a broader approach to experimentation. As they mix and match multiple techniques in different orderings, the number of experiments and models they produce quickly expands, creating visualization challenges that are not well-supported by available tools for either interactive compression or model comparison [19,40,41].\nIn this work, we explore how to address model compression challenges using interactive visualization. We first identify four model compression challenges by synthesizing prior qualitative findings on how ML practitioners use compression [24] with insights from the ML literature. In response to these challenges, we introduce COMPRESS AND COMPARE, an interactive visualization system for comparing the performance and behavior of a suite of compressed models. Through an overview visualization called the Model Map, our system helps users track their compression experiments and how they relate to one another. Users can select subsets of models to automatically visualize differences in their accuracy, efficiency, and provenance. To deeply inspect a smaller set of models, the system provides detailed comparisons of instance-level behaviors and internal activations. Through case studies and user studies, we demonstrate how COMPRESS AND COMPARE can lead to insights throughout model compression and how it expands the design space of interactive ML development tools to account for challenges made salient by compression. We contribute:"}, {"title": "2.1 Techniques for Model Compression", "content": "Model compression encompasses various techniques that reduce the storage space, memory, power, or time required to run an ML model while preserving its original behavior as much as possible [7, 8, 10,38, 57]. Compression is increasingly essential for running ML models, both in resource-constrained settings such as mobile devices and for extremely large models (e.g., generative language models).\nMost compression techniques fall into one of three classes: (1) quantization and palettization reduce the space required to store each individual parameter, (2) pruning removes parameters while optionally adjusting the others to compensate, and (3) factorization and distillation find a different set of parameters that mimic the behavior of the original model [8]. The most straightforward instantiations of these techniques are quantization (converting high-precision formats like 32-bit floats to lower-precision formats like 8-bit integers) and unstructured magnitude pruning (zero-ing out weights with the smallest absolute values). These foundational techniques form the starting point for many real-world compression strategies because they perform well in practice, are easy to understand, and are straightforward to compute [24,37]. Additional routines often employed to tune the resulting compressed model include fine-tuning (training the model on a data subset) and calibration (adjusting model parameters to compensate for compression).\nIn cases where off-the-shelf techniques are insufficient, task-specific techniques have been developed to achieve better efficiency trade-offs [14, 15, 22, 31, 65, 70]. These methods vary by which aspects of model efficiency they target, how computationally expensive applying the compression is, and whether or not they depend on additional training or calibration data. For example, some methods utilize random data samples to decide which parameters can most easily be pruned or restore intermediate activations [2, 12, 39], while others are data-agnostic [20,49] and can optionally be followed by a retraining step. Individual weights can be modified independently [12, 49], or compression can be performed in a structured manner at the level of neurons or layers [55]. These algorithmic choices give rise to a large space of possible compression strategies, each of which has different overall performance characteristics in terms of space consumption, inference time, and accuracy preservation. Helping ML practitioners navigate this space is a key design opportunity addressed in our work (see Sec. 3)."}, {"title": "2.2 Pitfalls in Evaluating Compressed Models", "content": "While compression techniques are designed to improve model efficiency while preserving accuracy, they have been known to substantially alter model behavior even while maintaining similar top-level metrics. For example, Hooker et al. [27] find that pruning image classification models has negligible effects on overall accuracy but disproportionately impacts the accuracy of rare subgroups. Similarly, Liebenwein et al. [34] find that pruning image models often results in poor generalization on distribution-shifted inputs.\nSince these behavior changes do not always impact top-level metrics, they can be hard to identify in advance without conducting bespoke analyses dedicated to finding them. For example, a prior interview study with ML practitioners [24] described a situation where an object detection model produced jittered outputs after quantization, a phenomenon that was not uncovered until the model was tested on-device in a demo setting. While tools for model comparison are applicable to this task, compression poses additional analytical challenges, such as comparing more than two models at once and understanding how models are derived from one another. Our work aims to address these challenges by providing practitioners with visual tools to inspect the behavior of several compressed models during development."}, {"title": "2.3 Visualization for Model Understanding and Comparison", "content": "Visualization has been essential in building generalizable knowledge about ML architectures [23, 60, 67] and helping practitioners make sense of specific models [1, 28, 50, 56, 62, 63]. Many tools use comparison to make insights about model behavior and internals more meaningful, e.g., by jointly visualizing embedding spaces to distinguish meaningful data clusters from spurious ones [5,54]. Other comparative approaches extend analysis subtasks to multiple models, including comparing model errors [41], instance-level outputs [19,64], or internal representations [40]. Tools have been developed to help practitioners select from a wide array of models using comparisons of top-level metrics [52, 68]. In our work, we use comparative visualization to help users identify promising strategies and filter out unviable experiments. Generating and evaluating compressed ML models is a more nascent area in VIS4ML - most prior work for this task is limited to either specific compression techniques or profiling efficiency metrics without considering behavior. For example, CNNPruner [32] uses a Taylor"}, {"title": "3 DESIGN CHALLENGES FOR COMPRESSION", "content": "To identify key compression challenges and motivate the design of interactive tools for compression, we synthesized insights from ML and HCI literature. Recently, Hohman et al. [24] explored ML practitioners' needs and perspectives on model compression via an interview study with 30 compression experts. This work describes how compression experts experiment with different compression techniques to satisfy efficiency and accuracy constraints within multidisciplinary teams. While their focus was providing a broad overview of compression experts' tacit knowledge, we distill specific findings from their study that are relevant to the design of compression tools and supplement them with recent insights from ML literature to synthesize key challenges for our system to address."}, {"title": "4 DESIGN OF COMPRESS AND COMPARE", "content": "We developed an interactive interface called COMPRESS AND COMPARE to address the four compression design challenges (Sec. 3). The tool consists of two main views: the Compression Overview supports high-level comparison and model selection from large-scale compression experiments (C1 and C2), and the Performance Comparison view enables fine-grained inspection of model behaviors and internals for a small number of candidate models (C3 and C4)."}, {"title": "4.1 Compression Overview", "content": "In COMPRESS AND COMPARE, experiments are represented as a set of trees, where each node is a model and edge is an operation performed on a parent model to produce a child model. This structural choice helps address one of the main visualization challenges of tracking compression experiments (C1): simultaneously depicting the variation in metrics across models along with dependencies in how the models were generated. The Model Map (Fig. 1A) addresses this using a nodelink tree diagram, where nodes are positioned using a custom algorithm that vertically aligns nodes based on either the operations used to produce them or their step in the compression experiment. Models are rendered as circles whose color and size encode performance properties, commonly accuracy and model size. To emphasize the sequential nature of compression experimentation, the color and width of the edges smoothly interpolate between the parent and child nodes. Hovering over a model displays a tooltip with the model's top-level metrics (Fig. 2), such as latency, size, sparsity, accuracy, and the operation that created this new model from its parent.\nWhile the Model Map's layout prioritizes understanding model dependencies, the Model Scatterplot (Fig. 1B) visualizes model metrics along the spatial axes, helping direct the user to viable models and find natural model groupings. For example, the classic Pareto curve often used by compression experts [24] can easily be recreated by setting model size or latency on the x-axis and accuracy on the y-axis. Node color and size are consistent between the Model Scatterplot and Model Map, and the two visualizations are connected via brushing and linking [3]. The Filter view also depicts model metrics through customizable histograms that can be brushed to filter the Model Map and Model Scatterplot. When the user specifies a filter, models that do not meet the filter criteria become semitransparent and unselectable. This allows users to narrow down the set of viable models by expressing their project's space and performance budgets (C2).\nWhen one or more models are selected, the user can view information about the models' metrics in the Selection Details view (Fig. 1C). While displaying metrics for multiple models in a table would be straightforward, it would obscure the dependency structure between the models, making it harder to reason about the effects of different compression operations. Therefore, we develop a technique to automatically create a grouped bar chart from a subset of the model dependency tree. Our algorithm traverses the tree recursively to identify a minimumcost set of \"variables\" that compactly explain the selected models' differences. Variables include operation parameter values, presence or"}, {"title": "4.2 Performance Comparison", "content": "While the Compression Overview visualizes trends across large sets of models, the Performance Comparison view enables a deeper comparison of a smaller group of models' Behaviors and Layers. We use a combination of juxtaposition and explicit encodings [18] to enable comparisons of multiple models at a time. Comparison is performed with respect to a base model, which is user-customizable but defaults to the selected model closest to the tree's root node.\nThe Behaviors tab (Fig. 5, left) presents the results of evaluating each selected model on a validation dataset. Each model is a column in a table, where rows represent either class-level comparisons or individual instances. When configuring their data, users can define comparison metrics that operate on the model outputs, enabling comparisons like differences in top-1 predictions or the KL divergence of the softmax probabilities. These comparison metrics are summarized in the table headers and are depicted as sparkline bar charts in each row. Notably, the interface supports both absolute per-model values and relative values compared to the base model. Users can sort and filter by absolute or relative metrics for any model, allowing them to quickly identify classes or instances impacted the most by compression (C3).\nThe final and lowest-level component of the interface is the Layers tab (Fig. 5, right), which exposes the internals of the selected models."}, {"title": "4.3 Setup and Implementation Details", "content": "We designed COMPRESS AND COMPARE for a highly customizable user workflow. To begin visualizing models, users write a simple Python script that invokes the COMPRESS AND COMPARE backend server and provides information about the models. Users specify models as a JSON object that details the operations used to produce each model and their performance across a set of user-defined metrics. Users can easily integrate specification creation into their existing model training procedures by updating the JSON file each time they train and evaluate a new model or evaluate against a new metric. To access information about the model's behaviors and layers, users write Python callbacks to retrieve instance-level outputs and layer activations, both of which can be either pre-computed or evaluated in real-time. This flexibility allows COMPRESS AND COMPARE to support any Python-based ML toolchain and accommodate very large models. Additionally, the COMPRESS AND COMPARE Python package provides helper functions to accelerate setup with common frameworks such as PyTorch and HuggingFace. The model servers for the use cases in this paper require around 200 lines of code, mostly consisting of boilerplate code that would already have been written in the course of experimentation.\nThe COMPRESS AND COMPARE frontend, implemented in SvelteKit\u00b9, is static and can be hosted publicly. Visualizations are developed using D3.js2 and LayerCake\u00b3. Code is available at: https://github.com/apple/ml-compress-and-compare."}, {"title": "5 CASE STUDIES OF COMMON COMPRESSION TASKS", "content": "We illustrate how COMPRESS AND COMPARE supports real-world compression workflows via two case studies."}, {"title": "5.1 Repairing Models Broken By Compression", "content": "A previously accurate model can \"break\" when compression is applied too heavily or broadly, resulting in low performance and nonsensical"}, {"title": "5.2 Discovering Compression Artifacts", "content": "Compression artifacts changes in model behavior caused by compression - can subtly affect model quality, decrease edge case performance, and increase bias without reducing overall accuracy, making them difficult to detect yet crucial to address. To demonstrate COMPRESS AND COMPARE's ability to identify compression artifacts (C3), we apply it to study known compression-induced biases in face classification models [27,36]. Following Hooker et al. [27], we train a ResNet18 [21] on CelebA [36] to predict whether each image has the attribute blond. Then, we iteratively train and compress seven ResNet18 [21] models on the same binary classification task using global magnitude pruning [70] at 10%, 30%, 50%, 70%, 90%, 95%, and 99% final sparsity (see Supplementary Material Sec. S3). Each resulting model achieves similar accuracy on the test set, ranging from 87.4% to 94.4%.\nTo identify potential sources of bias, we use COMPRESS AND COMPARE to understand how the models' small drops in performance are distributed over the images. In the CelebA dataset [36], male and not young are underrepresented attributes. If compression is introducing bias by forgetting rare classes, it will have a disparate impact on these images. To inspect this, we compare the relative accuracy of the pruned models to the uncompressed model in the Behaviors tab (Fig. 7B). This view immediately surfaces that compression has disproportionately impacted the performance of rare classes. While the 99%-pruned model makes 64.9% more errors for not male and 72.3% more for young, in the rare classes, it makes 145.5% more errors for male and 96.5% for not young. This is concerning since performance on underrepresented and difficult instances is a primary reason that ML practitioners start with a large model [33, 34]. This may signal the need for further model development, compression experiments, and bias mitigation before we are comfortable using these models. Previously, we would have needed to compute model performance on each attribute and interpret a table of relative percentages (such as in Hooker et al. [27]); however, using the visual affordances of COMPRESS AND COMPARE, we are able to quickly identify bias without additional computation.\nWhile COMPRESS AND COMPARE supports bias identification using traditional error metrics, it can also audit bias in settings where we do not have access to data with sensitive attribute labels. In this setting, we use COMPRESS AND COMPARE to identify a subset of images that may"}, {"title": "6 USER STUDY WITH ML PRACTITIONERS", "content": "To evaluate COMPRESS AND COMPARE, we conducted a user study with eight ML practitioners who work on model compression. Participants included research scientists developing new compression techniques, ML engineers compressing models for deployment, and software engineers building compression tools (Table 1). The goal of the study was to understand how participants would use COMPRESS AND COMPARE to make sense of compression experiments, build intuition about compression strategies, and explore compressed model behavior."}, {"title": "6.1 Study Methods", "content": "Each study session was conducted via video chat and lasted 45 minutes1 hour. We began by discussing the participant's current compression workflows and introducing them to COMPRESS AND COMPARE. Then, we asked participants to imagine they were part of a team compressing an existing image classification model and to think aloud as they used"}, {"title": "6.2 Study Results", "content": "Overall, participants reported that COMPRESS AND COMPARE provided a structured compression workflow, enabling them to perform analyses that would have been challenging and time-consuming with existing tools, build intuition about compression techniques that inspire new compression experiments, and identify subtle but problematic model behaviors introduced during compression."}, {"title": "6.2.1 Unifying Disjoint Compression Workflows", "content": "Our user study participants reported needing multiple tools to execute their current compression tasks, leading to tedious back-and-forth analysis across tools that made it challenging to understand the overall impact of their compression experiments. While each participant was knowledgeable about compression (average 3.5 on a self-reported 1-5 scale of compression experience) and used compression regularly (5/8 use compression in every project), there was not a standard set of tools used by all participants. This inconsistency was often due to a lack of compression-specific tooling that supported the breadth of participants' tasks, including training and compressing models (P1-P8), evaluating their performance and behavior (P4, P6, P7), and ensuring they meet specified efficiency budgets (P1-P8). As a result, participants often created custom tools, such as Jupyter Notebook charts for performance analyses (P1, P6, P8) and spreadsheets for model tracking (P3, P5), or they repurposed existing general model analysis tools to understand the performance of a compressed model (P1-P4). When compression-specific tools existed, participants found them invaluable to their analysis (P3, P4). However, these tools were often still specific"}, {"title": "6.2.2 Building Intuition about Model Compression", "content": "Beyond simply selecting a desired compressed model, COMPRESS AND COMPARE's visual and interactive components helped participants build intuition for how compression algorithms impact model performance and generate hypotheses about ways to improve future experiments. Participants found the combination of the Model Map and the Selection Details view to be an intuitive way to understand and reason about the space of compression experiments. Viewing the columns in the Model Map helped users understand the set of compression algorithms that had been applied (P3, P4, P7) and identify patterns in how the best-performing models were generated (P2, P7). To dig into a particular pattern, participants would often run visual \"experiments\" by selecting a group of models within a region of the Model Map (e.g., all the magnitude pruned models) and comparing their metrics in the Selection Details view (P2, P4, P6, P8). These in-depth explorations influenced participants' intuitions about how compression techniques affected their models more broadly. For instance, by comparing the accuracy and efficiency of two quantized models, P4 identified that quantization preserved performance much better for a large ResNet50 (25.6M parameters) than for a smaller MobileNet V2 (3.5M parameters). While P4 regularly uses quantization, this discrepancy in performance caused them to reflect that the success of quantization \"is definitely dependent on the base model; if you want something to work for quantization, you have to start at the right place.\" Finding the best compression technique is challenging (C1), so building intuition for the types of models that benefit from quantization can help P4 design more effective compression recipes moving forward, such as those that only apply quantization to large models.\nBuilding on their high-level understanding of the experimental space, participants used the Performance Comparison views to develop a deeper intuition about compression's impact on models' internal representations. Using the Layers view, participants debugged subtle problems in compression experiments that led to poor model performance. For example, P8 recognized that a particular model's \u201cbatch norms had been absolutely flattened\" by quantization. Batch normalization layers can have a substantial impact on downstream performance because they set the output value ranges at each layer, so this finding led P8 to suggest \"freezing all the batch norms\" during quantization as a way to maintain model performance. By building intuition through their Layers exploration, participants ideated a range of subsequent experiments. These next steps included combining current compression techniques in new ways (e.g., combining magnitude and gradient pruning) and expanding the space of operations, such as by tailoring compression to specific layers of the network. For instance, in their Layers analysis, P5 and P7 noticed earlier network layers had fewer parameters yet were pruned at the same rate as later layers. They hypothesized that, since later layers have more parameters, they have more redundancy and could withstand greater compression rates, so they designed an experiment that pruned layers as a function of their number of parameters. With COMPRESS AND COMPARE, participants deepened their understanding of how model parameters respond to compression techniques (C4) and used their insights to generate new experiments that could lead to more efficient and accurate models."}, {"title": "6.2.3 Encouraging Comprehensive Compression Analysis", "content": "By interactively integrating behavioral analysis with traditional metricbased compression analysis, COMPRESS AND COMPARE extended participants' existing behavioral analysis workflows and motivated them to consider the broader impacts of compression. Evaluating compressed model behavior on held out data was a key aspect of some participants' workflows (P5-P8) because it helped them identify subtle but important behavioral changes (C3):\nParticipants used the Behaviors view to analyze model behavior across an entire dataset to ensure that compression had not induced biases"}, {"title": "7 DISCUSSION", "content": "We present COMPRESS AND COMPARE, an interactive visualization system for tracking and comparing compression experiments. Based on challenges experienced by real-world users, we design COMPRESS AND COMPARE to support critical and unsupported compression analysis tasks, including managing interconnected compression experiments, interrogating the impact of compression on model behaviors, and ideating promising future compression experiments. Through case studies on generative language and image classification models, we demonstrate how our system helps users repair issues with compression and identify compression-induced bias. Moreover, our user study with compression experts illustrates how COMPRESS AND COMPARE shifts users' compression workflows from disjoint analysis across tools toward a single analysis platform that facilitates collaborative decision-making about model selection and exploration. Here, we discuss the implications of our results for future ML development and evaluation tools, as well as the current limitations of our work and possible solutions."}, {"title": "7.1 Designing Compression-Aware ML Workflows", "content": "Throughout the design and analysis of COMPRESS AND COMPARE, we encountered compression-specific challenges. While compression"}, {"title": "7.2 Limitations and Future Work", "content": "We designed COMPRESS AND COMPARE to support practitioners current compression workflows; however, as model compression becomes a more established and standardized discipline, it is possible that many of the iterative workflows we observed in this study will be superseded by automated approaches. However, interactive visualization has potential benefits for compression work even if automated approaches eventually become standard. First, even though effective AutoML strategies exist, these techniques often still require data scientists to invest considerable time to distill model behavior into a single objective function [29]. Second, COMPRESS AND COMPARE seeks not only to help practitioners produce the best compressed model, but also to help them build intuition about compression techniques and how they affect model characteristics. Just as prior work visualizing ML model internals and behaviors [30, 48, 69] has allowed people to understand how these models work, a better collective understanding of compression can make these techniques more accessible and inspire new approaches.\nOur user study and prior formative research primarily interviewed compression experts and ML engineers who were already well-versed in model compression. While this allowed us to get the most relevant feedback on how to design compression-specific systems, our system and takeaways do not consider the needs of novice users. It is likely COMPRESS AND COMPARE could be extended to support ML practitioners less accustomed to compression, such as by providing suggestions on what techniques to try or incorporating a graphical interface to run compression experiments. Such adaptations would empower non-experts to apply compression, but they could also support compression experts in running on-the-fly experiments based on their insights from COMPRESS AND COMPARE, such as removing compression from a specific layer or making a slight change to the sparsity value.\nFurther, several participants suggested extensions to COMPRESS AND COMPARE that could help it better match the specific needs of their teams, including supporting very large datasets and models as well as displaying custom efficiency metrics. Although the tool currently supports running model computations remotely or in advance, it does require the models being compared to be loaded simultaneously in memory so that each layer can be compared one-at-a-time. More efficient comparison techniques that do not require jointly loading several models may help COMPRESS AND COMPARE scale more easily.\nFinally, a key requirement of COMPRESS AND COMPARE is its integration between the visualization system's and a user's model development code. However, like many other prototype tools for ML development, the capabilities that make COMPRESS AND COMPARE a versatile tool can also necessitate considerable set-up work, particularly for custom model architectures. User study participants noted that the potential difficulty of importing models into the tool could be a critical hurdle to accepting it into their workflow. Future work is needed to design code-level interfaces that link COMPRESS AND COMPARE into the tools practitioners already use to develop models."}, {"title": "8 CONCLUSION", "content": "Making ML models smaller, faster, and more energy-efficient can enable exciting new user experiences and broaden access to existing ones. Our work forms part of a nascent body of literature on facilitating the process of compressing models, which can help make these use cases practical as models become increasingly large and powerful. Through the design and evaluation of COMPRESS AND COMPARE, we aimed to understand and address challenges in ML model development made salient by compression, including the need for extensive iteration, human-centered trade-offs between user experience metrics, and subtle changes in model behavior. Future compression-focused data visualization research can continue to make creating efficient ML models easier for a wider range of developers and teams, helping them make the experiences they envision a reality."}]}