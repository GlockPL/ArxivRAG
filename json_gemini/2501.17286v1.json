{"title": "Fine-Tuning Open-Source Large Language Models to Improve Their Performance on Radiation Oncology Tasks: A Feasibility Study to Investigate Their Potential Clinical Applications in Radiation Oncology", "authors": ["Peilong Wang", "Zhengliang Liu", "Yiwei Li", "Jason Holmes", "Peng Shu", "Lian Zhang", "Xiang Li", "Quanzheng Li", "Brady S. Laughlin", "Diego Santos Toesca", "Sujay A. Vora", "Samir H. Patel", "Terence T. Sio", "Tianming Liu", "Wei Liu"], "abstract": "Background: The radiation oncology clinical practice involves many steps relying on the dynamic interplay of abundant text data. Large language models have displayed remarkable capabilities in processing complex text information. But their direct applications in specific fields like radiation oncology remain underexplored.\nPurpose: This study aims to investigate whether fine-tuning LLMs with domain knowledge can improve the performance on Task (1) treatment regimen generation, Task (2) treatment modality selection (photon, proton, electron, or brachytherapy), and Task (3) ICD-10 code prediction in radiation oncology.\nMethods: Data for 15,724 patient cases were extracted. Cases where patients had a single diagnostic record, and a clearly identifiable primary treatment plan were selected for preprocessing and manual annotation to have 7,903 cases of the patient diagnosis, treatment plan, treatment modality, and ICD-10 code. Each case was used to construct a pair consisting of patient diagnostics details and an answer (treatment regimen, treatment modality, or ICD-10 code respectively) for the supervised fine-tuning of these three tasks. Open source LLaMA2-7B and Mistral-7B models were utilized for the fine-tuning with the Low-Rank Approximations method. Accuracy and ROUGE-1 score were reported for the fine-tuned models and original models. Clinical evaluation was performed on Task (1) by radiation oncologists, while precision, recall, and F-1 score were evaluated for Task (2) and (3). One-sided Wilcoxon signed-rank tests were used to statistically analyze the results.\nResults: Fine-tuned LLMs outperformed original LLMs across all tasks with p-value <= 0.001. Clinical evaluation demonstrated that over 60% of the fine-tuned LLMs-generated treatment\nregimens were clinically acceptable. Precision, recall, and F1-score showed improved performance of fine-tuned LLMs.\nConclusions: Fine-tuned LLMs demonstrated statistically significant improvements over original LLMs upon three clinically important tasks in radiation oncology. This study explored the feasibility of applying fine-tuned LLMs in radiation oncology, inspiring further development of utilizing LLMs to assist with radiation oncology tasks.", "sections": [{"title": "Introduction", "content": "The radiation oncology clinical practice presents a high level of complexity and stringent requirements for precision\u00b9, \u00b2. It involves sequential steps like consultation, simulation, planning, quality assurance, treatment delivery, and patient follow-up\u00b3, \u2074, relying on the dynamic interplay between abundant text and imaging data. This intricate process is traditionally time-consuming, dependent on manual analysis of vast amounts of unstructured clinical data, and susceptible to variations in human interpretation.\nLarge Language Models (LLMs) such as ChatGPT have displayed remarkable capabilities in natural language processing (NLP) on many topics 6-8. However, their direct application in specific domains like healthcare has posed challenges. Specifically, in radiation oncology, a sector demanding a lot of clinical experience and utmost precision, the generic nature of mainstream LLMs like ChatGPT falls short. This is especially true for advanced radiation therapy techniques requiring even more complexity and higher precision, such as Intensity-Modulated Proton Therapy (IMPT)9-13. Moreover, medical institutes and healthcare practitioners may need to have their localized LLMs because of the privacy regulations for patient health information (PHI). Additionally, the application of NLP in radiation oncology, although is expanding, remains underexplored, while there are enormous amounts of text and image data available in this field.\nAs efficient tools for language-involved processing can significantly enhance each phase of radiation therapy and potentially improve treatment outcomes, the necessity arises for a model adapted to clinical domain knowledge with the conciseness and specificity inherent in radiation oncology. Therefore, we finetuned open source LLMs to improve their performance on radiation oncology tasks. Particularly, we focus on the tasks of (1) generating radiotherapy treatment regimens, (2) determining radiation treatment modality (photon, proton, electron, or brachytherapy), and (3) predicting ICD-10 codes based on patient diagnosis details. To our knowledge, this represents the first exploration of fine-tuned LLMs specifically for these radiation oncology tasks."}, {"title": "Materials and Methods", "content": "Data extraction\nPatient data was extracted using an in-house search engine, running locally in the Department of Radiation Oncology at XXXX, which allows for searching and extracting structured data from the Aria database (ver.15.6) (Varian Medical Systems, USA). Two datasets were created from the database: one containing patient diagnosis details, including the patient ID, ICD codes, disease stages, free-form textual diagnosis details, etc., and the other comprising treatment plan delivery details, including the course ID, plan types, radiation modality, intent, and status, the number of treatment fields, the number of fractions, prescription dose details, etc. In total, we extracted 15,724 patient cases (Figure 1).\nData curation\nAs high-quality data notably improves LLM fine-tuning results\u00b9\u2074, we selected cases where patients only had a single diagnostic record and a clearly identifiable primary treatment plan. For example, data for patients who had a primary treatment plan and an alternative primary treatment plan were removed. This ensured that for each patient diagnosis, there would be only one corresponding primary treatment plan. In total, we selected 7,903 patient cases for the supervised fine-tuning from the 15,724 patients.\nIn some cases, the free-form diagnosis details also contained the treatment plan prescription, for example \"IMPT, 35 fractions, 60 Gy\u201d, however the patient may have received a slightly different one. Because we wanted to use the treatment planning information corresponding with the treatment plan that was ultimately delivered (based on the ones pulled directly from Aria), we decided to remove any planning information from the diagnosis details (history, pathology, stages). To clean up, we performed a manual process. First, we used a simple keyword filter, primarily searching for the term \"Plan\" or similar keywords. This successfully identified about 3,000 cases. Subsequently, we manually separated the patient diagnosis and treatment for the remaining 5,000 cases. Lastly, we manually corrected all separate records of diagnosis and treatment. These steps were performed successfully on all 7,903 cases. Additionally, we removed unusual punctuation and initials during this manual correction process to further enhance the quality of the annotated data.\nTo prepare the annotated treatment modality data, we first merged the patient diagnosis dataset with the plan delivery dataset. Then we selected the treatment modality information from the merged dataset, and standardized the terms as \u201cphoton\u201d, \u201cproton\u201d, \u201celectron\u201d or \u201cbrachy\u201d. We manually annotated the treatment modality for the cases where this information wasn't identified successfully from the merged dataset, based on the patient diagnosis.\nThe ICD codes were well-documented within the diagnostic notes, making them directly usable without intricate preprocessing. To ensure data quality, only cases using the ICD-10 standard were kept, and cases using ICD-9 were removed from the selected data. This resulted in 7,177 annotated cases where the patient diagnosis was linked to the correct ICD-10 code.\nData cohort\nThe 7,903 selected cases include patients with a mean age of 62 years (range, 1 to 100), with males comprising 56% and females comprising 44%. Regarding the treatment intent, 99% of the patients underwent curative therapy, while the remaining 1% of patients received palliative therapy. The treatment modalities utilized in the dataset are as follows: 64% of patients were treated with photon therapy, 32% received proton therapy, 3% underwent electron therapy, and 1% were treated with brachytherapy.\nSupervised fine-tuning\nFine-tuning is an essential process to tailor pre-trained LLMs for radiation oncology tasks, enabling them to respond more relevantly to these tasks. We utilized LLaMA2-7B\u00b9\u2074 (Meta, USA) and Mistral-7B\u00b9\u2075 (Mistral AI, France) as the foundational models and used patient diagnosis details as input for the supervised fine-tuning of the three tasks in radiation oncology. We utilized the Low-Rank Approximations (LoRA) method\u00b9\u2076 in our supervised fine-tuning, which freezes the pre-trained model weights and injects trainable rank decomposition metrics into each layer of the Transformer architecture.\nFor the generation of radiotherapy treatment regimens, we constructed 7,903 pairs of input prompts and answers from the 7,903 cases of annotated patient diagnoses and treatment regimensd. We engineered the instruction prompt for fine-tuning (Supplementary 1.1) and then fine-tuned LLaMA2 and Mistral models individually on the treatment regimen input prompts and answers (Figure 2). For the selection of radiation treatment modality, we constructed 7,903 pairs of input prompts and treatment modality answers and finetuned the LLaMA2 and Mistral individually on\nthe input prompts and answers in a similar approach. For the prediction of ICD-10 code, 7,177 pairs of input prompts and answers were constructed from the cases of annotated patient diagnosis and ICD-10 code data for fine-tuning. The details of the finetuning parameters are in the Supplementary 1.2. The same configuration parameters were utilized for the fine-tuning of both LLaMA2 and Mistral models."}, {"title": "Metrics and evaluation", "content": "The fine-tuning results are reported in the ROUGE-1 score\u00ae for Task (1), and accuracy for Task (2) and Task (3). Furthermore, the results of Task (1) were evaluated clinically by our radiation oncologist and medical physicists to check their practical utility in the clinic. Two radiation oncology residents (PGY3 and PGY5) first independently graded the generated treatment regimens with a scale from 1 to 5 based on the following definition:\n1. Modality (proton/photon) correct, dose fractionation (SBRT/regular/hypofractionation) correct, clinically completely acceptable and interchangeable.\n2. Minor deviation from the physician's plan, by either modality or dose fractionation intent, but still clinically acceptable.\n3. More significant deviation from the physician's plan, usually by both modality and dose fractionation intent, potentially unsafe for the patient.\n4. Major deviation/errors from the physician's plan, clinically unacceptable/unsafe.\n5. Absolutely clinically not acceptable.\nTen grading examples were given by a senior radiation oncologist (15 years of experience) to the residents as references. After grading, cases where the two gradings differed by more than one"}, {"title": "Statistical analysis", "content": "The one-sided Wilcoxon signed-rank test was utilized to compare the performance of the fine-tuned LLMs with those of the vanilla LLMs across three clinical tasks. The null hypothesis suggested no improvement in performance, while the alternative hypothesis anticipated an improvement with the fine-tuned model. Furthermore, we evaluated the performance of LLaMA2 compared with Mistral models using one-sided Wilcoxon signed-rank tests to provide additional insightful findings. All the tests were executed with a 95% confidence interval, based on a significance level of a = 0.05. p-value smaller than 0.05 is considered to be statistically significant."}, {"title": "Results", "content": "Our results showed that the fine-tuned LLMs outperformed the vanilla LLMs across all three tasks (Figure 3). Our clinical evaluation of the treatment regimen generation task demonstrated that over 60% of the treatment regimens generated by the fine-tuned LLMs are clinically acceptable, inspiring future work on further fine-tuning more advanced LLMs for radiation oncology. The details of our findings are reported below.\nTreatment regimen generation results\nIn Figure 3, the fine-tuned LLaMA2 model achieved a highest ROUGE-1 score of 0.531 (median: 0.489) after a scan of temperaturef values ranging from 0.1 to 1.0 with increments of 0.1 (detailed results in Supplementary Figure 1), compared to the vanilla LLaMA2 model's highest ROUGE-1 score of 0.075 (median: 0.072). In Table 1, the one-sided Wilcoxon signed-rank test results indicated a statistically significant increase in ROUGE-1 score for the treatment regimens generated using the fine-tuned LLaMA2 model, compared to the vanilla LLaMA2 model, with a p-value of 0.001 (n = 10). Figure 4 illustrates an example of the treatment regimen generated by the fine-tuned LLaMA2 model, compared with the ones of vanilla model and our physicians.\nSimilar results were observed for the fine-tuned Mistral model with a p-value of 0.001 (n = 10). Further results of the comparison of LLaMA2 and Mistral model (Vanilla and fine-tuned) can be found in the supplementary Table 2.\nRadiation modality selection results\nFine-tuned LLaMA2 model achieved the highest accuracy score of 0.705 (median: 0.676) after a scan of temperature values, compared to the vanilla model's highest accuracy score of 0.499 (median: 0.469). A statistically significant increase in accuracy score was observed for the fine-tuned LLaMA2 model compared to the vanilla model, with a p-value of 0.001 (n = 10). Similar results were observed for the fine-tuned Mistral model.\nICD-10 code prediction results\nFor simplification purposes, only the main category of the ICD-10 code was included in the accuracy calculation and the subcategory was ignored. Fine-tuned LLaMA2 model achieved the highest accuracy score of 0.642 (median: 0.613) after a scan of temperature values, while the vanilla model achieved the highest accuracy score of 0.180 (median: 0.139). A statistically significant increase was observed for the fine-tuned LLaMA2 model compared to the vanilla model, with a p-value of 0.001 (n = 10). Similar results were observed for the fine-tuned Mistral model.\nIn combination with all three tasks, the fine-tuned LLaMA2 model was observed to outperform the vanilla model statistically significantly in accuracy and ROUGE-1 score, with p-value <<0.001 (n = 30). Similar results were observed for the fine-tuned Mistral model (p-value << 0.001, n = 30). These results indicate that open-source large language models, fine-tuned with the radiation oncology domain-specific data, will improve their performance on these three radiation oncology tasks.\nClinical evaluation of the treatment regimen generation results\nIn the clinical evaluation of Task (1), an average of 187 cases (46%) received Grade 1, indicating clinically complete acceptability and interchangeability, for the fine-tuned LLaMA2 model (Figure 5a). Similarly, an average of 208 cases (52%) scored Grade 1 for the fine-tuned Mistral model (Figure 5b).\nWhen considering both clinically acceptable grades (\"Grade 1 + Grade 2\"), the fine-tuned LLaMA2 model achieved an average of 257 cases (64%) while the fine-tuned Mistral model achieved an average of 265 cases (66%). Given the high degree of personalization in radiotherapy treatment planning, due to factors such as age, underlying diseases, insurance, and personal choices,\ngenerating clinically acceptable treatment regimens that are clinically acceptable for over 60% of patients included in the independent evaluation with real-world scenarios based on only short disease descriptions and stage information is remarkable (notwithstanding that only 7B version of LLaMA2 and Mistral models were used for this feasibility study). We did not evaluate the treatment regimen results from the vanilla LLaMA2 and Mistral models due to their high error rates and messy output (examples in the supplementary 1.3), which made them difficult for our residents to grade them accurately.\nIn the treatment modality selection evaluation, the comparison of weighted average precision, recall and F1 score values before and after fine-tuning revealed improved results for both LLaMA2 and Mistral models, as illustrated in Figure 5c (detailed results in the supplementary Table 3). Since the treatment modality selection for patients can also be personal, given that our model input only consists of simply diagnosis and staging information, achieving precision and recall scores over 70% is considered very good.\nIn the ICD-10 code prediction evaluation, the confusion matrices (Figure 6a) demonstrated excellent alignment between the finetuned models' output and actual ICD-10 codes of that category, in comparison with the poor prediction from the vanilla models (Figure 6b). The evaluation of ICD-10 code prediction is also shown in Figure 5a using the macro average precision, recall, and F1 score (details in the supplementary Table 4)."}, {"title": "Discussion", "content": "Although our work was initiated with fine-tuning small-sized 7B LLMs for the feasibility study on radiation oncology tasks, it had attracted a lot of attention and interest from both the radiation oncology community and beyond. We appreciate the valuable support and insightful discussions received throughout our work. In this article, we have selected a few key topics for discussion due to the word limit. As the exploration of LLMs in radiation oncology continues growing, we will include more discussions in our future work.\nImportance of LLMs utilization in radiation oncology tasks\nRadiation oncology integrates multiple disciplines like physics, biology, and medicine to develop personalized cancer treatment plans. It can significantly reduce the time consumed by radiation oncologists and improve work efficiency by automating the process radiation oncologists need to access the patient clinical notes and medical references to provide tailored treatment recommendations to patients. By fine-tuning LLMs to assist the patient treatment plan-making and predicting treatment modalities, radiation oncologists may be able to expedite the planning process, allowing for more timely initiation of treatment and improved patient outcomes. Accurate ICD code prediction can facilitate efficient billing and reimbursement processes, reducing administrative burdens on healthcare providers. Utilizing fine-tuned LLMs to help streamline the assignment of ICD codes will greatly improve the efficiency of record-keeping and administrative tasks. Our findings suggest that fine-tuned LLMs have great potential to make meaningful contributions to these tasks and support radiation oncologists in decision making.\nData quality for fine-tuning\nHigh-quality data is crucial for successful supervised fine-tuning. As evidenced in previous work, supervised fine-tuning annotations in the order of tens of thousands is enough to achieve high-quality results 14. In light of this, we were motivated to collect and annotate 7,903 high-quality patient cases from our radiation oncology database. The emphasis on \u201chigh quality\u201d means that each case was selected based on stringent criteria of only one corresponding primary treatment plan per patient diagnosis, and the annotation was performed with a high degree of accuracy and attention to detail. Validation was conducted on every patient to ensure completeness, accuracy, and consistency within the annotated data. Although we spent a lot of time collecting and curating the data, the benefit of using a large amount of high-quality data is significant. We have observed a great reduction in hallucination and mode failure during the fine-tuning process with our curated dataset, and a significant improvement in the performance of LLMs on the three radiation oncology tasks.\nClinical evaluation and institutional practice\nDespite well-defined grading definitions for evaluating generated treatment regimens, there are still a few evaluation cases that are difficult to grade, i.e. the model generating correct prescriptions but to a removed disease site. This kind of hallucination commonly seen in all LLMs although reduced during fine-tuning, still exists. An example case and more discussions are included in Supplementary 1.4. As it is extremely difficult to aggregate data at the scale needed from multiple institutions, our models were fine-tuned on a single-institutional dataset. But it is worth noting that our institutional practice closely adheres to the National Comprehensive Cancer Network (NCCN) guidelines, which are standardized frameworks for cancer treatment clinical decision making across institutes. Treatment choices can depend on patient-specific conditions, doctor-specific prescription habits, hospital resources, etc. Different treatment options might be clinically acceptable for the same patient, which poses extra challenges for fine-tuning LLMs. This study is not to provide universal solutions applicable to all institutions, but to demonstrate that fine-tuning LLMs with domain knowledge can significantly improve their performance in healthcare tasks. The generated answers still need to be interpreted by human experts with caution.\nClinical impact of fine-tuned LLMs\nIn this study, we fine-tuned open-source LLMs for radiation oncology using the short diagnosis as input and observed a significant improvement in the performance of their output on the three clinical tasks. Our clinical evaluation revealed that over 60% of the treatment regimen generated by the fine-tuned LLMs is clinically acceptable. While these results are very promising, we are also aware that there may still exist a distance from applying the fine-tuned LLMs in general to real clinical scenarios, especially since we are using the small-sized 7B LLMs for our fine-tuning and the short diagnosis text as input. However, as a demonstration study, our results have inspired future work, which is to use the much more detailed and lengthy notes as well as the more advanced larger-sized LLMs for further fine-tuning to generate more clinically relevant outputs, which will assist radiation oncology professionals to improve the efficiency and reduce the workload in real-world scenarios.\nFine-tuned small-sized LLMs vs vanilla large-sized LLMs\nWith the growth of AI, the performance of the vanilla LLMs on the market has been constantly improved, and the size of the LLMs is growing. For some easy text summarizing and extracting tasks, we have seen an acceptable performance by the vanilla large-sized LLMs. However, in the scope of this study focusing on the three radiation oncology clinical tasks, we still see the underperformance of the large-sized LLMs. For example, we have observed that the fine-tuned 7B LLaMA-2 model can still outperform the vanilla 70B LLaMA-3.1 model. This indicates that the domain-specific data, especially the curated high-quality data, used to fine-tune the small-sized LLM, may have an ineligible impact on the improvement of performance of the LLMs on the specific tasks. However, due to the scope of this study, more results including comparing fine-tuned small-sized LLMs with original large-sized LLMs on the radiation oncology tasks will be reported in our future work."}, {"title": "Limitations", "content": "Although our fine-tuned models demonstrated improved performance over the original models, we also noticed limitations in our study.\nFirst, we still observed some mistakes made by the fine-tuned LLMs due to hallucinations, although we have observed a reduction in hallucinations while fine-tuning with our curated data. This is because LLMs predict the next tokens based on probabilities learned during training, and hallucinations are the inherent probabilistic nature of LLMs.\nSecond, the fine-tuned models only demonstrated improved performance on the specific tasks that the models were fine-tuned for. They did not exhibit capabilities beyond these tasks, as they were neither pre-trained for these tasks beyond. Thus, it may depend on different clinical tasks for the medical professionals to decide whether to go through the fine-tuning approach or not. Some LLMs may have performed very well on easy tasks like summarization, keyword extraction, etc. without fine-tuning. But for the very specific tasks, like the ones presented in this study. Fine-tuning was demonstrated to a way to improve the performance of LLMs on those tasks."}, {"title": "Conclusion", "content": "We fine-tuned LLaMA-2 7B and Mistral 7B models with radiation oncology domain data and achieved statistically significant improvements in three clinical tasks: treatment regimen generation, treatment modality selection, and ICD-10 code prediction. The clinical evaluation on the treatment regimens generated by our fine-tuned models revealed that over 60% are clinically acceptable. As a feasibility study, although there is still a distance of applying our fine-tuned models directly to the clinic, it has demonstrated the great potential of LLMs in radiation oncology and inspired further development of LLMs for radiation oncology tasks."}]}