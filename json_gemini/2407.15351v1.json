{"title": "LLMExplainer: Large Language Model based Bayesian Inference for Graph Explanation Generation", "authors": ["Jiaxing Zhang", "Jiayi Liu", "Dongsheng Luo", "Jennifer Neville", "Hua Wei"], "abstract": "Recent studies seek to provide Graph Neural Network (GNN) interpretability via multiple unsupervised learning models. Due to the scarcity of datasets, current methods easily suffer from learning bias. To solve this problem, we embed a Large Language Model (LLM) as knowledge into the GNN explanation network to avoid the learning bias problem. We inject LLM as a Bayesian Inference (BI) module to mitigate learning bias. The efficacy of the BI module has been proven both theoretically and experimentally. We conduct experiments on both synthetic and real-world datasets. The innovation of our work lies in two parts: 1. We provide a novel view of the possibility of an LLM functioning as a Bayesian inference to improve the performance of existing algorithms; 2. We are the first to discuss the learning bias issues in the GNN explanation problem.", "sections": [{"title": "1 Introduction", "content": "Interpreting the decisions made by Graph Neural Networks (GNNs) (Scarselli et al., 2009) is crucial for understanding their underlying mechanisms and ensuring their reliability in various applications. As the application of GNNs expands to encompass graph tasks in social networks (Feng et al., 2023; Min et al., 2021), molecular structures (Chereda et al., 2019; Mansimov et al., 2019), traffic flows (Wang et al., 2020; Li and Zhu, 2021; Wu et al., 2019; Lei et al., 2022), and knowledge graphs (Sorokin and Gurevych, 2018), GNNS achieve state-of-the-art performance in tasks including node classification, graph classification, graph regression, and link prediction. The burgeoning demand highlights the necessity of enhancing GNN interpretability to strengthen model transparency and user trust, particularly in high-stakes settings (Yuan et al., 2022; Longa et al., 2022), and to facilitate insight extraction in complex fields such as healthcare and drug discovery (Zhang et al., 2022; Wu et al., 2022; Li et al., 2022).\nRecent efforts in explaining GNN (Ying et al., 2019; Luo et al., 2020; Zhang et al., 2023b) have sought to enhance GNN interpretability through multiple learning objectives, with a particular focus on the graph information bottleneck (GIB) method. GIB's goal is to distill essential information from graphs for clearer model explanations. However, the effectiveness of GIB hinges on the availability of well-annotated datasets, which are instrumental in accurately training and validating these models. Unfortunately, such datasets are rare, primarily due to the significant expert effort required for accurate annotation and occasionally due to the inherent complexity of the graph data itself. This scarcity poses a serious challenge, leading to a risk of learning bias in explaining GNN. Learning bias arises when the model overly relies on the limited available data, potentially leading to incorrect or over-fitted interpretations. We illustrate this phenomenon in Fig. (1) and provide empirical evidence in Fig. (4).\nAs demonstrated in the figures, learning bias becomes increasingly problematic as the model continues to train on sparse data. Initially, the model might improve as it learns to correlate the sub-graph G* with the label Y, optimizing mutual information \\(I(G^*, Y)\\). However, beyond a certain point, keeping optimizing \\(I(G^*, Y)\\) leads to an over-reliance on the limited and possibly non-representative data available, thereby aggravating the learning bias. This situation is depicted through the divergence of mutual information and actual performance metrics, such as AUC, where despite higher mutual information, the practical interpretability and accuracy of the model decline.\nTo mitigate the learning bias, current models often stop training early, a practice to prevent the exacerbation of learning bias. However, this approach is inherently flawed, especially in real-world applications lacking comprehensive validation datasets, leading potentially to under-fitting and inadequate model generalization. This emphasizes the need for innovative approaches to model training and interpretation that can navigate the challenges posed by sparse ground truth in explaining GNNs.\nTo address the challenges posed by sparse ground truth annotations and the consequent risk of learning bias in explaining GNNs, we propose LLMExplainer, a versatile GNN explanation framework that incorporates the insights from Large Language Model (LLM) into a wide array of backbone GNN explanation models, ranging from instance-level to model-level explanation models(Shan et al., 2021; Ying et al., 2019; Luo et al., 2020; Yuan et al., 2021; Spinelli et al., 2022; Wang et al., 2021b; Yuan et al., 2021; Shan et al., 2021; Chen et al., 2024). The LLMs act as a grader, and the evaluations from LLMs are then integrated into the model to inform a weighted gradient descent process. Specifically, to ensure a satisfactory level of explanation performance, we embed Bayesian Variational Inference into the original GNN explainers and use LLM as the prior knowledge in Bayesian Variational Inference. We prove that with the injection of LLM, LLMExplainer will mitigate the learning bias problem. Our experimental results show the effectiveness of enhancing the backbone explanation models with faster convergence and fortifying them against learning bias.\nIn summary, the main contributions of this paper are:\n\u2022 We propose a new and general framework, LLMExplainer, which solves the problem of learning bias in the graph explanation process by embedding the Large Language Model into the graph explainer with a Bayesian inference process and improving the explanation accuracy.\n\u2022 We theoretically prove the effectiveness of the proposed algorithm and show that the lower bound of LLMExplainer is no less than the original bound of baselines. Our proposed method achieved the best performance through five datasets compared to the baselines.\n\u2022 We are the first to discuss this learning bias problem in the domain of graph explanation and provide the potential of the Large Language Model as a Bayesian inference module to benefit the current works."}, {"title": "2 Related Work", "content": "2.1 Graph Neural Networks and Graph\nExplanations\nGraph neural networks (GNNs) are on the rise for analyzing graph structure data, as seen in recent research studies (Dai et al., 2022; Feng et al., 2023; Hamilton et al., 2017). There are two main types of GNNs: spectral-based approaches (Bruna et al., 2013; Kipf and Welling, 2016; Tang et al., 2019) and spatial-based approaches (Atwood and Towsley, 2016; Duvenaud et al., 2015; Xiao et al., 2021). Despite the differences, message passing is a common framework for both, using pattern extraction and message interaction between layers to update node embeddings. However, GNNs are still considered a black box model with a hard-to-understand mechanism, particularly for graph data, which is harder to interpret compared to image data. To fully utilize GNNs, especially in high-risk applications, it is crucial to develop methods for understanding how they work.\nMany attempts have been made to interpret GNN models and explain their predictions (Shan et al., 2021; Ying et al., 2019; Luo et al., 2020; Yuan et al., 2021; Spinelli et al., 2022; Wang et al., 2021b). These methods can be grouped into two categories based on granularity: (1) instance-level explanation, which explains the prediction for each instance by identifying significant sub-structures (Ying et al., 2019; Yuan et al., 2021; Shan et al., 2021), and (2) model-level explanation, which seeks to understand the global decision rules captured by the GNN (Luo et al., 2020; Spinelli et al., 2022; Baldassarre and Azizpour, 2019). From a methodological perspective, existing methods can be classified as (1) self-explainable GNNs (Baldassarre and Azizpour, 2019; Dai and Wang, 2021), where the GNN can provide both predictions and explanations; and (2) post-hoc explanations (Ying et al., 2019; Luo et al., 2020; Yuan et al., 2021), which use another model or strategy to explain the target GNN. In this work, we focus on post-hoc instance-level explanations, which involve identifying instance-wise critical substructures to explain the prediction. Various strategies have been explored, including gradient signals, perturbed predictions, and decomposition.\nPerturbed prediction-based methods are the most widely used in post-hoc instance-level explanations. The idea is to learn a perturbation mask that filters out non-important connections and identifies dominant substructures while preserving the original predictions. For example, GNNExplainer (Ying et al., 2019) uses end-to-end learned soft masks on node attributes and graph structure, while PG-Explainer (Luo et al., 2020) incorporates a graph generator to incorporate global information. RG-Explainer (Shan et al., 2021) uses reinforcement learning technology with starting point selection to find important substructures for the explanation."}, {"title": "2.2 Bayesian Inference", "content": "MacKay (1992) came up with the Bayesian Inference in general models, while Graves (2011) first applied Bayesian Inference to neural networks. Currently, Bayesian Inference has been applied broadly in computer vision (CV), nature language processing (NLP), etc (M\u00fcller et al., 2021; Gal and Ghahramani, 2015; Xue et al., 2021; Song et al., 2024).\nBayesian Variational techniques have seen extensive uptake in Bayesian approximate inference. They adeptly reframe the posterior inference challenge as an optimization endeavor (Wang et al., 2023). When compared to Markov Chain Monte Carlo, which is another Bayesian Inference method, Variational Inference exhibits enhanced convergence and scalability, making it better suited for tackling large-scale approximate inference tasks.\nDue to the nature of Bayesian Variational Inference, it has been embedded into neural networks called Bayesian Neural Networks (Graves, 2011). A major drawback of current Deep Neural Networks is that they use fixed parameter values, and fail to provide uncertainty estimations, resulting in a limitation in uncertainty. BNNs are extensively used in fields like active learning, Bayesian optimization, and bandit problems, as well as in out-of-distribution sample detection problems like anomaly detection and adversarial sample detection."}, {"title": "2.3 Large Language Model", "content": "Large Language Models (LLMs) have been widely used since 2023 (Bubeck et al., 2023; Brown et al., 2020; Zhou et al., 2022). Based on the architecture of Transformer(Vaswani et al., 2017), LLMs have achieved remarkable success in various Natural Language Processing (NLP) tasks. LLMs have spurred discussions from multiple angles, including LLM efficiency (Liu et al., 2024; Wan et al., 2024), personalized LLMs (Mysore et al., 2023; Fang et al., 2024), prompt engineering(Wei et al., 2022; Song et al., 2023), fine tuning(Lai et al., 2024), etc.\nBeyond their traditional domain of NLP, LLMs have found extensive usage in diverse fields such as computer vision (Wang et al., 2024; Dang et al., 2024), graph learning (He et al., 2023), and recommendation systems (Jin et al., 2023; Wu et al., 2024), etc. By embedding LLMs into existing systems, researchers and practitioners have observed enhanced performance across various domains, underscoring the transformative impact of these models on modern AI applications."}, {"title": "3 Preliminary", "content": "3.1 Notations and Problem Definition\nWe summarize all the important notations in Table 3 in the appendix. We denote a graph as \\(G = (V,E; X, A)\\), where \\(V = {v_1, v_2, ..., v_n}\\) represents a set of n nodes and \\(E \\subseteq V \\times V\\) represents the edge set. Each graph has a feature matrix \\(X \\in \\mathbb{R}^{n \\times d}\\) for the nodes. where in X, \\(x_i \\in \\mathbb{R}^{1 \\times d}\\) is the d-dimensional node feature of node \\(v_i\\). E is described by an adjacency matrix \\(A \\in {0,1}^{n \\times n}\\). \\(A_{ij} = 1\\) means that there is an edge between node \\(v_i\\) and \\(v_j\\); otherwise, \\(A_{ij} = 0\\).\nFor the graph classification task, each graph G has a ground-truth label \\(Y \\in C\\), with a GNN model f trained to classify G into its class, i.e., \\(f : (X, A) \\leftrightarrow \\tilde{Y} \\in {1,2, ..., C'}\\). For the node classification task, each graph G denotes a K-hop sub-graph centered around node \\(v_i\\), with a GNN model f trained to predict the label for node \\(v_i\\) based on the node representation of \\(v_i\\) learned from G. Since the node classification can be converted to computation graph classification task (Ying et al.,"}, {"title": "3.2 Graph Information Bottleneck", "content": "For a graph G follows the distribution with \\(P_G\\), we aim to get an explainer function \\(G = g_{\\theta}(G)\\), where \\(\\theta\\) are the parameters of explanation generator g. To solve the graph information bottleneck, previous methods (Ying et al., 2019; Luo et al., 2020; Zhang et al., 2023b) are optimized under the following objective function:\n\\[G^* = \\arg \\min_{G} I(G, \\tilde{G}) - \\lambda I(Y, \\tilde{G}), \\tilde{G} \\in \\mathcal{G},  (1)\\]\nwhere G* is the optimized sub-graph explanation produced by optimized g, \\(\\tilde{G}\\) is the explanation candidate, and \\(\\mathcal{G} = {G}\\) is the set of \\(\\tilde{G}\\) observations. During the explaining procedure, this objective function would minimize the size constraint \\(I(G, \\tilde{G})\\) and maximize the label mutual information \\(I(Y, \\tilde{G})\\). \\(\\lambda\\) is the hyper-parameter which controls the trade-off between two terms.\nSince it is untractable to directly calculate the mutual information between prediction label Y and sub-graph explanation \\(\\tilde{G}\\), to estimate the objective, Eq. (1) is approximated as\n\\[G^* = \\arg \\min_{G} I(G, \\tilde{G}) - \\lambda I(Y, f(\\tilde{G})), \\tilde{G} \\in \\mathcal{G}.  (2)\\]\nSince G* is generated by \\(g_{\\theta}(\\cdot)\\), instead of optimize G, we optimize \\(\\theta^*\\) with \\(h(\\theta, \\lambda, G, f) = I(G,g_{\\theta}(G)) - \\lambda I(Y, f(g_{\\theta}(G)))\\), then we have \\(\\theta^* = \\arg \\min h(\\theta, \\lambda, G, f)\\), where \\(I(Y,G^*) \\sim I(Y, f(G^*))\\) and f is the pre-trained GNN model."}, {"title": "4 Methodology", "content": "Fig. (2) presents an overview of the structure of LLMExplainer. The previous method is depicted on the left, while LLMExplainer incorporates a Bayesian Variational Inference process into the entire architecture, utilizing a large language model as the grader. (In the previous explanation procedure, the explanation sub-graph is generated via an explanation model to explain the original graph and the to-be-explained prediction model. The explanation model is optimized by minimizing the size constraint \\(I(G, \\tilde{G})\\) and maximizing the label mutual information \\(I(Y, \\tilde{Y})\\), within \\(h(\\theta, \\lambda, G, f)\\), which is introduced in Section 3.2.\nIn our proposed framework, after generating the explanation sub-graph, we evaluate it through a Bayesian Variational Inference process, which is realized using a Large Language Model agent acting as a human expert. The enhanced explanation \\(G'\\) is then produced using Eq. (7). Note that, with the introduction of the Bayesian Variational Inference process, the previous distribution for \\(\\theta\\) in \\(g_{\\theta}(\\cdot)\\) will shift to \\(\\beta\\) in \\(g_{\\beta}(\\cdot)\\). Finally, we optimize the explanation model with new \\(I(G, \\tilde{G})\\) and \\(I(Y, \\tilde{Y})\\) within \\(h(\\beta, \\lambda, G, f)\\). We provide detailed formulas for the LLM-based Bayesian Variational Inference in Section 4.1 and a detailed prompt-building procedure in Section 4.2. Our training procedure is provided in Algorithm. (1)."}, {"title": "4.1 Bayesian Variational Inference In Explainer", "content": "The injection of Bayesian Variational Inference into the GNN Explainer helps mitigate the learning bias problem. In this section, we will provide details on how we achieve this goal and present a theoretical proof for our approach. We begin by injecting the knowledge learned from the LLM as a weighted score and adding the remaining objective with a graph noise. Instead of adopting weighted noise, or gradient noise(Neelakantan et al., 2015), we choose random Gaussian noise in this paper(Graves, 2011).\nProblem 1 (Knowledge Enhanced Post-hoc Instance-level GNN Explanation). Given a trained GNN model f and a knowledgeable Large Language Model \\(\\Phi\\), for an arbitrary input graph \\(G = (V,E; X, A)\\), the goal of post-hoc instance-level GNN explanation is to find a sub-graph \\(G^*\\) with Bayesian Inference embedded explanation generator \\(g'(\\cdot)\\), that can explain the prediction of f on G with the Large Language Model grading \\(s = \\Phi(\\tilde{G}, G)\\), \\(s \\in [0, 1]\\) in circle, as \\(G' = g'(\\cdot)(G) = sg_{\\theta}(G) + (1 - s)G_N\\), \\(\\beta^* = h(\\beta, \\lambda, G, f)\\), where \\(\\Beta\\) is the parameter for \\(g'(\\cdot)\\) and \\(g'\\) is the explainer"}, {"title": "5 Experimental Study", "content": "We conduct comprehensive experimental studies on benchmark datasets to empirically verify the effectiveness of the proposed LLMExplainer. Specifically, we aim to answer the following research questions:\n\u2022 RQ1: Could the proposed framework outperform the baselines in identifying the explanation sub-graphs for the to-be-explained GNN model?\n\u2022 RQ2: Could the LLM score help address the learning bias issue?\n\u2022 RQ3: Could the LLM score reflect the performance of the explanation sub-graph; is it effective in the proposed method?"}, {"title": "5.1 Experiment Settings", "content": "To evaluate the performance of LLMExplainer, we use five benchmark datasets with ground-truth explanations. These include two synthetic graph regression datasets: BA-Motif-Volume and BA-Motif-Counting (Zhang et al., 2023a), with three real-world datasets: MUTAG (Kazius et al., 2005), Fluoride-Carbonyl (Sanchez-Lengeling et al., 2020), and Alkane-Carbonyl (Sanchez-Lengeling et al., 2020). We take GRAD (Ying et al., 2019), GNNExplainer (Ying et al., 2019), ReFine (Wang et al., 2021a), and PGExplainer (Luo et al., 2020) for comparison. Specifically, we pick PGExplainer as a backbone and apply LLMExplainer to it. We follow the experimental setting in previous works (Ying et al., 2019; Luo et al., 2020; Sanchez-Lengeling et al., 2020; Zhang et al., 2023b) to train a Graph Convolutional Network (GCN) model with three layers. We use GPT-3.5 as our LLM grader for the explanation candidates. To evaluate the quality of explanations, we approach the explanation task as a binary classification of edges. Edges that are part of ground truth sub-graphs are labeled as positive, while all others are deemed negative. We take the importance weights given by the explanation methods as prediction scores. An effective explanation technique should be able to assign higher weights to the edges within the ground truth sub-graphs compared to those outside of them. We utilize the AUC-ROC metric(\u2191) for quantitative evaluation."}, {"title": "5.2 Quantitative Evaluation (RQ1)", "content": "In this section, we compare our proposed method, LLMExplainer, to other baselines. Each experiment was conducted 10 times using random seeds from 0 to 9, with 100 epochs, and the average AUC scores as well as standard deviations are presented in Table 1. The results demonstrate that LLMExplainer provides the most accurate explanations among several baselines. Specifically, it improves the AUC scores by an average of 0.227/42.5% on synthetic datasets and 0.191/34.1% on real-world datasets. In dataset BA-Motif-Volume, we achieve 0.432/79.0% improvement compared to the PGExplainer baseline. The reason is there is serious learning bias with PG-Explainer on BA-Motif-Volume, which is shown in Fig. (4). The performance improvement in BA-Motif-Volume is not significant because of (1). the learning bias in this dataset is specifically slight; (2). The PGExplainer is well-trained at the epoch 100. Comparisons with baseline methods highlight the advantage of Bayesian inference and LLM serving as a knowledge agent in training."}, {"title": "5.3 Qualitative Evaluation (RQ2)", "content": "As shown in Fig. (4), we visualize the training procedure of PGExplainer and LLMExplainer on five datasets. The first row is PGExplainer and the second row is LLMExplainer. From left to right, the five datasets are MUTAG, Fluoride-Carbonyl, Alkane-Carbonyl, BA-Motif-Volume, and BA-Motif-Counting respectively. We visualize and compare the AUC performance, LLM score, and train loss of them. Additionally, the LLM score is not used in PGExplainer, we retrieve it with the explanation candidates during training. As we can observe in the BA-Motif-Volume dataset, the AUC performance and LLM score increase in the first 40 epochs. However, they drop persistently to around 0.5/0.3 after 100 epochs, indicating a learning bias problem. For LLMExplainer, based on PGExplainer, on the second row, we can observe that the AUC performance and LLM Score increase to 0.9+ and maintain themselves stably. This observation is similar in Fluoride-Carbonyl and Alkane-Carbonyl datasets. The LLM score slightly dropped at around epoch 40 and then recovered. The results show that our proposed framework LLMExplainer could effectively alleviate the learning bias problem during the explaining procedure."}, {"title": "5.4 Ablation Study (RQ3)", "content": "In this section, we conduct the experiments on the ablation study of the LLMExplainer. When we have the LLM score, we may be concerned about whether or not this score could reflect the real performance of the explanation sub-graph candidates and whether it would work for the proposed framework. So, we compare the performance of the framework with the LLM score and random score. In the model with the random score, we replace the LLM grading module with a random number generator. As shown in Table 2, the results show that the LLM score could effectively enhance the Bayesian Inference procedure in explaining."}, {"title": "6 Conclusion", "content": "In this work, we proposed LLMExplainer, which incorporated the graph explaining the procedure with Bayesian Inference and the Large Language Model. We build the to-be-explained graph and the explanation candidate into the prompt and take advantage of the Large Language Model to grade the candidate. Then we use this score in the Bayesian Inference procedure. We conduct experiments on three real-world graph classification tasks and two synthetic graph regression tasks to demonstrate the effectiveness of our framework. By comparing the baselines and PGExplainer as the backbone, the results show the advantage of our proposed framework. Further experiments show that the LLM score is effective in the whole pipeline."}, {"title": "7 Limitations", "content": "While we acknowledge the effectiveness of our method, we also recognize its limitations. Specifically, although our approach theoretically and empirically proves the benefits of integrating the LLM agent into the explainer framework, the scope of the datasets and Large Language Models remains limited. To address this challenge, we plan to explore more real-world datasets and deploy additional LLMs for a more comprehensive evaluation in the future work."}, {"title": "8 Appendix", "content": "8.1 Error loss \\(L_E (\\beta, s)\\)\nThe full proof of Equation 8 towards the error loss \\(L_E (\\beta, s)\\) is\n\\[L_E(\\beta, s) = E_{\\tilde{G}~ Q(\\beta)}L(\\tilde{G}, s)\\]\n\\[= - \\int P(\\tilde{G}|\\beta) \\cdot \\ln P(s|\\tilde{G})d\\tilde{G}\\]\nWhen \\(s\\rightarrow 1\\), then we have\n\\(\n\\\\\ns\\rightarrow 1\\n\\\\\n = - \\int P(\\tilde{G}|\\alpha) \\cdot \\ln P(s|\\tilde{G})d\\tilde{G}\n\\\\\n= -\\int P(\\tilde{G}_0|\\alpha)  \\ln P(\\frac{s}{s\\tilde{G}_0 + (1 - s)G_N}) d\\tilde{G}_0\n\\\\\n= - \\int sP(\\tilde{G}_0|\\alpha)  \\ln P (\\frac{s}{\\frac{s\\tilde{G}_0}{G_N} + (1 - s)}) d\\tilde{G}_0\n\\\\\n (11)\n\\)\n8.2 Network loss \\(L_C (\\beta, \\alpha)\\)\nThe full proof of Equation 9 towards the network loss \\(L_C (\\beta, \\alpha)\\) is\n\\[L_C(\\beta, \\alpha) \\sim KL[sg_{\\alpha}(G) + (1 - s)G_N ||g_{\\alpha}(G)]\\]\n\\[= - \\int (sg_{\\alpha}(G) + (1 - s)G_N) \\log (\\frac{sg_{\\alpha}(G) + (1 - s)G_N}{g_{\\alpha}(G)}) d\\tilde{G}_0\\]\n\\[= - \\int (sg_{\\alpha}(G) + (1 - s)G_N) \\log (\\frac{sg_{\\alpha}(G) + (1 - s)G_N}{sg_{\\alpha}(G) + (1 - s)G_N}) d\\tilde{G}_0\\]\n\\[= - \\int (sg_{\\alpha}(G) + (1 - s)G_N) \\log (\\frac{sg_{\\alpha}(G)}{sg_{\\alpha}(G) + (1 - s)G_N}) d\\tilde{G}_0\\]\n\\[= - \\int (sg_{\\alpha}(G) + (1 - s)G_N)  \\log (\\frac{s}{s + (1 - s)\\frac{G_N}{G_{\\alpha}}}) d\\tilde{G}_0\\]\nWhen \\(s\\rightarrow 1,  \\frac{G_N}{G_{\\alpha}} \\rightarrow 0\\), with Taylor Series, then we have\n\\[s\\rightarrow 1\\n= - \\int (sg_{\\alpha}(G) + (1 - s)G_N)  \\log (\\frac{s}{1 - s\\frac{G_N}{G_{\\alpha}}}) d\\tilde{G}_0\\]\n\\[= -\\int sg_{\\alpha} \\log s + (1 - s)\\frac{G_N}{G_{\\alpha}} d\\tilde{G}_0\\]\n\\[= -\\int sg_{\\alpha} \\log s + (1 - s) G_N d\\tilde{G}_0\\]\n\\[= -\\int sg_{\\alpha} \\log s + (1 - s) G_N d\\tilde{G}_0\\]\n\\[= -\\int  s\\tilde{G}_0 \\log (1 - (1 - s)) + (1 - s)G_N d\\tilde{G}_0\\]\n\\[= -\\int -(1 - s)s\\tilde{G}_0 + (1 - s) G_N d\\tilde{G}_0\\]\n\\[=\\int -(1 - s)(\\tilde{G}^2_N - G_N)d\\tilde{G}_0\\]\n (12)"}]}