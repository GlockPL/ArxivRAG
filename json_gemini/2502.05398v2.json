{"title": "Probabilistic Foundations for Metacognition via Hybrid-AI", "authors": ["Paulo Shakarian", "Gerardo I. Simari", "Nathaniel D. Bastian"], "abstract": "Metacognition is the concept of reasoning about an agent's own internal processes, and it has recently received renewed attention with respect to artificial intelligence (AI) and, more specifically, machine learning systems. This paper reviews a hybrid-AI approach known as \u201cerror detecting and correcting rules\" (EDCR) that allows for the learning of rules to correct perceptual (e.g., neural) models. Additionally, we introduce a probabilistic framework that adds rigor to prior empirical studies, and we use this framework to prove results on necessary and sufficient conditions for metacognitive improvement, as well as limits to the approach. A set of future research directions is also provided.", "sections": [{"title": "1 Introduction", "content": "Originally a concept from developmental psychology (Flavell 1979), metacognition refers to reasoning about an agent's own internal processes (Wei et al. 2024). This idea of metacognition, considered the human brain's \"self-monitoring process\" (Demetriou et al. 1993), has been studied in a diverse set of fields (Li et al. 2017; Izzo, M\u00e4rtens, and Pan 2019; Caesar et al. 2020). Over the years, its study has been proposed in the field of AI (Cox and Raja 2011; Cox 2005), and recent interest (Wei et al. 2024) has reinvigorated this discussion. In this paper, we review a recent, hybrid-AI style of metacognition known as \u201cerror detection and correction rules\u201d (EDCR) that has been explored in a variety of models and use cases (see Table 1). In short, the proposal is a hybrid-AI approach in which logical rules are learned to characterize the model performance of well-trained perceptual (e.g., neural) models. This paper serves as a review of the results of these recent articles, and improves the underlying theoretical underpinnings by framing them in terms of a probabilistic argument, providing new insights into the use of hybrid-AI for metacognition. In particular, our novel theoretical framework allows us to explain some empirically observed results, as well as leads us to several new open research questions."}, {"title": "2 Error Detection and Correction Rules", "content": "In this framework, we are given a well-trained model, normally denoted as f, that takes some continuous input (e.g., a vector) and returns a set of class labels. We will use subscripts (e.g., $f_i$ or $f_1$) to denote multiple models when relevant. We note that the model need not be neural (though all existing studies have focused on neural models), and in general we do not assume access to the model weights. We now introduce a small running example.\nExample 2.1. Let $f_{car}$ be a neural model trained on data set Ding such that given a sample (that we denote x) returns a subset of labels {ford, toyota, dodge, us, japan}. So, for example, for sample w, $f_{car}(w)$ = {dodge, us}.\nThe work on EDCR introduces the notion of a metacognitive condition, which is some aspect of the environment, sensor, model, or metadata that may cause the model to be incorrect in a given circumstance. In the existing work, the metacognitive condition (which we refer to as \"condition\", for short) has come from domain knowledge, model output (e.g,, labels at different levels of a hierarchy), or other models (e.g., different architectures, trained on different data) this is summarized in Table 1. This leads us to the first type of rule, the error detecting rule, which is expressed in a first order logic syntax as follows:\nerror (X) \u2190 $\\underset{j\\in DC_i}{V}$ pred\u1d62(X) \u2227 cond\u2c7c(X)  (1)\nIn words, this rule states that if any of the conditions in set DCi are met, and model i predicts class a, then the model has an erroneous detection. We note that we can replace the disjunction with a singleton and have multiple rules for a given model-class pair (i.e., using a logic program). The second type of rule is a correction rule, which has the following"}, {"title": "3 Probabilistic Interpretation and Results", "content": "We now present a new probabilistic interpretation of EDCR. First, we introduce a bit of notation: when the object in question is understood, we shall use the notation $f_i$ to refer to the set of propositions produced by model $f_i$ on the object. So, for example, $a \\in f_i$ means that model $f_i$ predicted label a. We shall also use the notation $g_t$ to denote a set of ground truth labels for the object; so, the statement $a \\in f_i, a \\notin g_t$ means that model $f_i$ incorrectly predicted class a for the object. With this in mind, we can represent the precision and recall for a model as the following conditional probabilities.\nPrecision: $P_a$ = $P(a \\in g_t | a \\in f_i)$ (3)\nRecall: $R_a$ = $P(a \\in f_i | a \\in g_t)$ (4)\nWe will also consider a random variable D that specifies a specific distribution. We can consider conditional probabilities with an unspecified distribution (as shown above) to be approximated from training data, while setting D to be something specific would signify being out-of-distribution: e.g., the precision for model i on class a under distribution d is written as $P(a \\in g_t | a \\in f_i, D = d)$. In what follows, we shall use M to denote the set of metacognitive conditions. We note that this framework allows us to precisely define what it means for a condition to be error detecting, and introduce a new, formal definition to that effect.\nDefinition 3.1 (Error Detecting). A metacognitive condition c is error detecting with respect to model i, class a, and distribution d if $P(a \\in g_t | a \\in f_i, c \\in M, D = d)$ is less than or equal to $P(a \\in g_t | a \\in f_i, D = d)$.\nIntuitively, a condition is error detecting if the precision of the model drops when the condition is true. The second property is distribution invariance.\nDefinition 3.2 (Distribution Invariance). A metacognitive condition c is distribution invariant with respect to model i, class a, and set of distributions D if for any distributions d\u2208 D it is error detecting with respect to that distribution.\nAt first glance, distribution invariance seems to be a strong definition, and some may even think the set of conditions meeting that criteria would be small. However, consider conditions based on constraints among classes; e.g., inconsistent assignments in a multi-class classification problem is one such case, and the configuration of the sensor is another.\nWith conditions in mind, we define precision and recall after applying a metacognitive condition:\nPrecision: $P^c$ = $P(a \\in g_t | a \\in f_i, c \\notin M)$ (5)\nRecall: $R^c$ = $P(a \\in f_i, c \\notin M | a \\in g_t)$ (6)\nOur first result shows how much the precision changes after applying a metacognitive condition. The argument in this paper characterizes the change in precision using a probabilistic argument and, notably, the result is obtained without any assumptions of independence.\nTheorem 3.1 (Metacognitive Precision Change). The following identity holds:\n$P^c$ - $P_a$ = $K \\times (P(a \\notin g_t | a \\in f_i, c \\in M) - (1 - P_a))$,\nwhere K = $\\frac{P(c\\in M | a\\in f_i)}{P(c\\notin M | a\\in f_i)}$\nIn Xi et al. (2024), an analogous result is shown. Both results suggest finding conditions that attempt to maximize the product of probabilities $P(c \\in M | a \\in f_i)$ and $P(a \\notin g_t | a \\in f_i, c \\in M)$ is desirable for precision improvement by \"erasing\" labels \u2013 these correspond to support and confidence in that paper. It also turns out that this product has computationally desirable properties, as it is submodular (proven in Xi et al. (2024)). However, we point out that the result of Xi et al. (2024) did not frame the preliminaries in terms of probability, and hence it was not clear if there were latent assumptions; this probabilistic interpretation and the corresponding proof (in the appendix)"}, {"title": "4 Limitations of Metacognitive Conditions", "content": "We now explore some of the limits of our approach to metacognitive improvement. Anecdotally, we noticed in prior metacognitive applications of EDCR that often detection seems easier than correction. In Xi et al. (2024) and Lee et al. (2024), correction was conducted by changing the class label resulting from a condition-class pair that led to an error. In the notation of this paper, such a precondition would be \u201c$a \\in f_i, c \\in M$\", meaning the condition of the model predicting class a while condition c is also true. This allows us to overcome the detection-induced deficit (a consequence of Theorem 3.3), which was demonstrated empirically in Lee et al. (2024) where such metacognitive correction could ensemble rules to directly improve recall over single-model baselines. However, it was less effective in improving recall in the experiments of Xi et al. (2024), where the use case consisted of five classes. To understand why this occurs, consider a model that cannot distinguish between a set of samples, all classified under condition c. A well-trained model assigns class i, the most probable class by training, but the probability of i being correct is lower than the average precision for predictions of class i. However, the next most probable class, j, is lower still, and picking this would lower overall loss. As a result, without another condition or something else to distinguish these samples, the model and the metacognitive correction cannot re-assign those samples a new label that improves overall performance, while at the same time effectively identifying a case where the model had an error with a high probability. Consider the following:\nExample 4.1. Consider the scenario from Example 2.2. Suppose in cases where the error is observed (both classes toyota and us are predicted) that despite dodge being the \"best\" correction of class toyota we have the following:\n$P(dodge \\in f_{car} | toyota \\in f_{car}, us \\in f_{car})$\n$< P(dodge \\in f_{car} | toyota \\in f_{car})$\nIt turns out that the situation in Example 4.1 leads to a reduction in precision for the class by which the label is re-assigned. We now formalize this argument. Intuitively, if the precision for class j conditioned on reclassifying items originally classified as i under condition c is lower, then the overall precision for classification of class j will drop.\nTheorem 4.2 (Limits of Reclassification). If we have that $P(\\beta \\in g_t | a \\in f_i, c_a \\in M) < P(\\beta \\in g_t | \\beta \\in f_i)$, then:\n$P(\\beta \\in g_t | \\beta \\in f_i) > P(\\beta \\in g_t | \\beta \\in f_i \\lor (a \\in f_i, c_a \\in M)$.\nProof. By way of contradiction, assume:\n$P(\\beta \\in g_t | \\beta \\in f_i) < P(\\beta \\in g_t | \\beta \\in f_i \\lor (a \\in f_i, c_a \\in M)$\n$P(\\beta \\in g_t | \\beta \\in f_i) < P(\\beta \\in g_t | \\beta \\in f_i \\lor (a \\in f_i, c_a \\in M)$\n$< \\frac{P(a \\in f_i, c_a \\in M) _ P(\\beta \\in g_t, a \\in f_i, c_a \\in M)}{P(\\beta \\in f_i)}$\n$P(\\beta \\in g_t, \\beta \\in f_i)$\n$< \\frac{P(\\beta \\in g_t, \\beta \\in f_i) _ P(\\beta \\in g_t, a \\in f_i, c_a \\in M)}{P(\\beta \\in f_i)}$\n$P(a \\in f_i, c_a \\in M)$\n$P(\\beta \\in g_t | \\beta \\in f_i) < P(\\beta \\in g_t | a \\in f_i, c_a \\in M)$,\nwhich contradicts the statement of the theorem.\nWe also note there are other potential limits on reclassification due to EDCR. Specifically, if rules are learned in a manner that allows for inconsistencies (which would be possible if rules are learned among different models independently) then proper corrective action becomes less clear. This is an active area of inquiry.\nLimitations to precision improvement/recall reduction. In another new result from our analysis, we show we can bound the quantity $P(c \\in M | a \\in f_i)$, which as pointed out earlier can magnify or suppress the amount of change in precision, or amplify the reduction in recall based on the prevalence of the metacognitive condition. Specifically, this is bounded by $P(c \\in M | a \\notin g_t, a \\in f_i)$ when c is error-detecting (shown in the next corollary). The practical application of the result, when identifying a metacognitive condition, is that we can understand the power of such a condition by only analyzing a subset of a dataset where the model was correct, which may have implications for large-scale data analytics and imbalanced classification problems. Note that we never assume $c \\in M$ is independent of $a \\notin g_t$ or $a \\in f_i$ - this result stems directly from the assumption that c is error detecting.\nCorollary 4.1. If c is error detecting, then:\n$P(c \\in M | a \\in f_i) < P(c \\in M | a \\notin g_t, a \\in f_i)$\nProof. By definition of error detecting, we have:\n$P(a \\notin g_t | a \\in f_i, c \\in M) > P(a \\notin g_t | a \\in f_i)$ (8)\n$= \\frac{P(c \\in M | a \\in f_i)P(a \\notin g_t | a \\in f_i, c \\in M)}{P(c \\in M | a \\notin g_t, a \\in f_i)}$\nThis in turn gives us:\n$1 > \\frac{P(c \\in M | a \\in f_i)}{P(c \\in M | a \\notin g_t, a \\in f_i)}$ (10)\n$P(c \\in M | a \\notin g_t, a \\in f_i) > P(c \\in M | a \\in f_i)$, (11)\nthus completing the proof."}, {"title": "5 Conclusion:\nDirections for Future Research", "content": "In this paper, we reviewed a hybrid-AI technique for metacognition known as EDCR, and provided a probabilistic argument that supports previous findings. This also suggests future research directions, so we end the paper with a discussion of some of these potential areas of inquiry.\nRole of Domain Knowledge and Inconsistency. In Example 2.2 we showed how metacognitive conditions can be used to identify inconsistencies, and the work of Kricheli et al. (2024) demonstrates this empirically by both recovering latent constraints and using that to improve model loss via LTN. When we view conditions from the standpoint of distribution invariance, logical inconsistency clearly meets these criteria. This raises an interesting question: can consistency with domain knowledge be used for error correcting? Neurosymbolic approaches already use this concept to reduce training loss (Bizzarri et al. 2024). However, understanding how consistency can be used for metacognitive correction remains an open question.\nMulti-model / Multi-modal reasoning. The results of this paper suggest that EDCR can be most effective for improving model precision while sacrificing recall (Theorems 3.1 and 3.3). The results of Lee et al. (2024) essentially leverage this property to ensemble models together, as different models can be precise and their combined recall will lead to an increase if they are identifying different phenomena. The ensembling of multi-modal models through EDCR seems to be a natural fit, as models of different modalities likely exhibit complementary capabilities. However, work like Lee et al. (2024) assumes one mode is the \"base model\" while the others are used to correct it. The study of EDCR (or other metacognitive methods) to ensemble two or more models without a designed \u201cbase model\u201d remains an open question.\nOnline metacognition and data efficiency. Quantities such as $P(a \\in g_t | a \\in f_i, c \\in M)$ and $P(c \\in M | a \\in f_i)$ (which are analogous to confidence and support) are typical byproducts of metacognitive rule learning under EDCR, and as seen in this paper are key quantities for validating invariance of conditions (Theorem 3.2). Likewise, an experiment in Xi et al. (2024) shows how rules can be learned from a new distribution that are effective with a small portion of data. Together, can these results suggest rapid adoption to a new distribution of data by identifying conditions on the fly (i.e., online learning of metacognitive conditions)?"}]}