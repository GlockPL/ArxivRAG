{"title": "Integrating Various Software Artifacts for Better LLM-based Bug Localization and Program Repair", "authors": ["QIONG FENG", "XIAOTIAN MA", "JIAYI SHENG", "ZIYUAN FENG", "WEI SONG", "PENG LIANG"], "abstract": "LLMs have garnered considerable attention for their potential to streamline Automated Program Repair (APR). LLM-based approaches can either insert the correct code using an infilling-style technique or directly generate patches when provided with buggy methods, aiming for plausible patches to pass all tests. However, most of LLM-based APR methods rely on a single type of software information, such as issue descriptions or error stack traces, without fully leveraging a combination of diverse software artifacts. Human developers, in contrast, often use a range of information \u2013 such as debugging data, issue discussions, and error stack traces to diagnose and fix bugs. Despite this, many LLM-based approaches do not explore which specific types of software information best assist in localizing and repairing software bugs. Addressing this gap is crucial for advancing LLM-based APR techniques.\nTo investigate this and mimic the way human developers fix bugs, we propose DEVLORE (short for DEVeloper Localization and Repair). In this framework, LLMs first use issue content (description and discussion) and stack error traces to localize buggy methods, then rely on debug information in buggy methods and issue content and stack error to localize buggy lines and generate valid patches. We evaluated the effectiveness of issue content, error stack traces, and debugging information in bug localization and automatic program repair. Our results show that while issue content and error stack is particularly effective in assisting LLMs with fault localization and program repair respectively, different types of software artifacts complement each other in addressing various bugs. By incorporating these three types of artifacts and using the Defects4J v2.0 dataset for evaluation, DEVLORE successfully localizes 49.3% of single-method bugs and generates 56.0% plausible patches. Additionally, DEVLORE can localize 47.6% of non-single-method bugs and generates 14.5% plausible patches. Moreover, our framework streamlines the end-to-end process from buggy source code to a complete repair, and achieves a 39.7% and 17.1% of single-method and non-single-method bug repair rate, outperforming current state-of-the-art APR methods. The source code and experimental results of this work for replication are available at https://github.com/XYZboom/DEVLoRe.", "sections": [{"title": "1 Introduction", "content": "Automatic Program Repair (APR) streamlines the process of identifying and correcting code defects, significantly reducing the time and effort required for manual bug fixing [18]. Traditional APR techniques employ various methods, including template-based approaches [7, 15, 17, 22, 24, 26] and neural machine translation (NMT) [5, 10, 21, 46], to generate potential patches that are both syntactically valid and semantically meaningful. Although these methods can produce correct patches for certain bugs, they also have notable limitations. NMT models rely heavily on bug-fixing training data, making them unable to generate patches for new or unseen types of bugs. On the other hand, template-based approaches suffer from a limited set of templates and struggle to address more complex, nontrivial bug fixes [38].\nRecent studies have explored the use of LLMs for APR, either by having LLMs fill in the correct code in buggy methods using an infilling-style technique or by directly generating patches when provided with buggy methods [9, 20, 31, 34\u201339, 44]. The initial results demonstrate the ability of LLMs to correctly repair real-world bugs, including those that were previously unrepairable by existing APR approaches. One of the best-performing frameworks is Agentless [35], which feeds an LLM with only issue description and can automatically solve GitHub issues. These promising outcomes highlight the potential of LLMs to develop more effective APR methods.\nHowever, there are still two major limitations that need to be addressed:\n\u2022 Current LLM-based approaches do not fully incorporate various types of software artifacts. Most LLMs rely on just one or two kinds of software artifacts, such as issue descriptions or code structure, while other important artifacts are underutilized. For instance, debugging information, which is a critical tool for human developers in diagnosing and resolving bugs, is often not fully leveraged.\n\u2022 While various LLM-based approaches make use of different software artifacts, such as issue descriptions and error stack traces, it remains unclear which specific type of information most effectively aids LLMs in localizing and automatically repairing software bugs.\nTo address these two limitations and further explore the ability of LLMs to localize and fix soft- ware bugs (bug, defect, and fault are used interchangeably in this paper), we propose feeding LLMs different types of software artifacts to determine which information best leverages their capabilities in bug localizing and fixing. The rationale behind this approach is that human developers typically do not rely on a single type of information when localizing and fixing software bugs. Instead, they combine various sources of information in software development, such as issue descriptions, proof of concept (PoC), stack traces, discussions in issues, and more. Based on developers' experience, having more information helps to better understand the root cause of bugs, ultimately leading to more effective bug localization and fixes.\nTo achieve this, we propose the DEVLORE framework, which asks LLMs to mimic human developers for bug localization and program repair. Along with this framework, we design two tools to extract executed methods in failed test cases and debugging information of buggy methods. In this framework, we feed the chosen LLM with three types of software artifacts: issue content (including issue description and discussion), error stack trace, and debug information. Then, using the well-known Defects4J dataset [13], we evaluate how these different types of software artifacts contribute to effectively localizing and fixing bugs. Our experiment results demonstrate that issue"}, {"title": "2 Three Motivating Examples", "content": "Our assumption is that LLM-based program repair tools have the potential to be much more effective if they can leverage a variety of software artifacts, such as code snippets, version history, documentation, and even testing outputs. By providing LLMs with a rich set of contextual data, they can understand and localize bugs more accurately and offer more precise fixes. This assumption is based on a human software engineer's daily practice. When a human developer tries to fix a bug, they would examine various resources such as the issue descriptions, error stack, debugging information, and test cases, until a solution is identified. However, currently most LLM-based repair approaches underutilize or use limited software artifacts. Here, we provide three examples to demonstrate the motivation of this work and the ability of LLMs to localize and fix bugs when given access to different types of software artifacts."}, {"title": "2.1 Debugging Information to the Rescue", "content": "The Lang 1b bug involves the createNumber method, which takes a string as its parameter and returns a number represented by that string. While studying the Lang 1b bug, we noticed that the newly added failing test case is the input \u201c0x80000000\u201d for the createNumber method. When this test case is provided, the buggy createNumber method incorrectly treats the input string as an"}, {"title": "2.2 Issue Content to the Rescue", "content": "Sometimes, the discussions and Proof of Concept (PoC) included in the issue content can help the LLM better understand the expectations for bug repair. Figure 3 shows a developer's comment in the issue content, which states the expected behavior of the program (https://issues.\napache.org/jira/browse/LANG-432). The original code attempted to convert both input strings to uppercase using String.toUpperCase() and then return the result of invoking the contains method. However, String.toUpperCase() is locale-sensitive, which makes it unsuitable for case- insensitive comparisons. For example, the character 0x00DF represents \u201c\u00df\u201d in Unicode. If we apply String.toUpperCase() to this character, it becomes \u201cSS\u201d in the Turkey locale. Consequently, comparing \u201css\u201d with \u201c\u00df\u201d would result in an equal match. Therefore, in this case, we should not use String.toUpperCase() but instead compare the characters individually.\nThe debug information, org.apache.commons.lang.StringUtils:containsIgnoreCase:1045 {\u201cstr\u201d:\u201c\u00df\u201d, \u201csearchStr\u201d:\u201cSS\u201d}, and the error stack trace shown in Figure 3 merely re-display the test cases and do not clearly highlight the relationship between the character 0x00DF and 'SS'. As a result, neither the error stack trace nor the debug information provides any useful clues about the bug, and LLMs cannot generate a plausible fix when provided with either the stack message or the debug information alone. However, with the description and expected behavior outlined in the issue content, GPT-40 is able to generate a correct patch."}, {"title": "2.3 Error Stack to the Rescue", "content": "When a bug triggers an exception, as opposed to merely being an unexpected behavior, the error stack trace generated at the point where the exception occurs can be helpful in diagnosing and fixing the error. Stack trace provides a detailed record of the sequence of method calls that led to an exception, making it easier to pinpoint the exact location in the code where the bug arises. For example, in Lang 39b shown in Figure 4, the developer forgot to check for null values in the elements of the input parameters replacementList and searchList, which can lead to a NullPoint- erException (NPE). However, if we do not inform the LLM about the occurrence of an NPE in the error stack, the LLM will not be able to identify where null detection is necessary. The stack trace"}, {"title": "2.4 Observations From the Above Three Examples", "content": "From these three examples, we can see that issue content, error stack traces, and debugging information can complement each other in bug detection and fixing. Each type of artifact provides a different level of context, and when combined, they create a more comprehensive understanding of the bug. Issue content offers the high-level description and expectations for the fix, error stack traces pinpoint the exact location of the failure, and debugging information provides granular details about the variable states and flow of execution. By incorporating these diverse artifacts, the LLM's repair process becomes more holistic."}, {"title": "3 Approach", "content": "As shown in Figure 6, our approach utilizes LLMs in two distinct steps: bug localization and program repair. Along with these two steps, we developed two dynamic tools \u2013 MethodRecorder and DebugRecorder \u2013 to assist LLMs in handling bug localization and fix. MethodRecorder tracks the methods executed during the failing test case(s), while DebugRecorder is designed to extract debug information from the buggy method(s). Debug information is stored in a list, where each"}, {"title": "3.1 Localize Buggy Method(s)", "content": "We emulate the behavior of human developers, who use errors in stack traces and proof of concept (PoC) from issue content to localize buggy methods. To this end, we apply MethodRecorder to trace the methods invoked during the execution of failing test cases. We then collect the signatures of these invoked methods and prompt the LLM to identify the buggy method(s). In this step, we also provide the LLM with any available error stack traces and issue content, including general description of the issue and developers' discussion/comment under this issue. We choose not to use debugging information because it is typically gathered at runtime and involves instrumentation (see java.lang.instrument.Instrumentation [1]), which can be computationally expensive. In developers' daily practice, they set just a few of breakpoints and collect important information about variables. But setting breakpoints automatically is not practical. Therefore, before localizing buggy method(s), minimizing instrumentation and debugging information is more efficient."}, {"title": "3.2 Localize Buggy Line(s)", "content": "Once the buggy method(s) have been localized in the previous step, we utilize DebugRecorder to capture detailed information about variable names and their values within the identified buggy method(s). The rationale for introducing debugging information at this stage is that it provides a more granular view, which is particularly beneficial for pinpointing the specific lines responsible for the bug and for understanding the internal state of variables. The debugging information with variables' value is often unnecessary during the initial localization step, but becomes crucial when the focus shifts to identifying precise faults within a known problematic method.\nIn this process, we gather debugging information as a list of variable-value pairs that reflect the state at specific lines of code within the buggy method(s). This allows the LLM to understand the conditions and data flows that may contribute to the bug. Combined with the issue content, such as descriptions and discussions, along with stack traces, this debugging information is fed into the LLM along with the body of the buggy method(s). Together, these resources assist the LLM in localizing the exact line(s) within the method(s) where the bug manifests, providing the context needed to understand and resolve the bug effectively."}, {"title": "3.3 Patch Generation", "content": "For patch generation, we provide all relevant information to maximize the LLM's ability to produce an accurate repair. This includes the localized buggy buggy line(s) and method(s) identified in previous steps, as well as the complete body of the buggy method(s). Additionally, we provide supplementary materials such as the issue content, which may contain descriptions and discussions about the bug, the stack trace from the error, and detailed debugging information captured at runtime. By supplying this comprehensive context, we enable the LLM to better understand the specific code behavior that led to the error and to use this knowledge to generate a more targeted and effective patch for the bug. For each bug, we use the LLM to generate multiple potential patches in .diff format, which can be directly applied to the buggy code."}, {"title": "3.4 Patch Validation", "content": "We apply the patches generated in the previous step to the buggy code and test them to evaluate their effectiveness. If the patch passes the initial failed tests, we proceed it to pass all tests in case that it incur any regression. If it passed all the tests, then it is considered a plausible fix. To facilitate this process, we utilize the Defects4J framework to run the tests and verify the patches.\nTo be consistent with existing studies [20, 47], if the patch is semantically equivalent to the original patch provided by developers, it is considered a correct patch. As part of this validation process, the patch undergoes manual inspection and cross-checking by two experienced developers to ensure its correctness. This step is crucial to verify that the patch not only passes all automated tests but also aligns with the intended behavior from a developer's perspective. The manual inspection by developers serves as a final quality control step to ensure that the patch addresses the bug correctly and does not introduce new bugs."}, {"title": "4 Experiment Setup", "content": "We used the well-established Defects4J benchmark [13] for our experiments, specifically lever- aging both version 1.2 and version 2.0. Defects4J v1.2 contains 391 bugs from six real-world projects (note that there are 395 bugs in v1.2, but due to the update to Java 8, four bugs can no longer be reproduced), while v2.0 includes an additional 444 bugs from 11 real-world projects. We chose Defects4J benchmark for two main reasons. First, it offers a diverse set of software artifacts \u2013 such as issue URLs, error stacks, and test cases \u2013 that are well-suited for our proposed ARP framework and help maximize its capabilities. Second, Defects4J benchmark has been widely used in prior research, which facilitates a fair and meaningful comparison of our approach."}, {"title": "4.2 Parameters in Experiments", "content": "Due to the extremely long debugging information for some bugs recorded by DebugRecorder, along with issue content and error stack, which exceeds the token limit of most LLMs, we opted for the GPT-4 series model, known for its ability to handle long token sequences with a 128k token context window. To manage costs, we selected the cost-effective GPT-40-mini model, also with a 128k token context window, and used the default settings (Temperature=0.5, Top-p=1). We first conducted 10 manual experiments in each step (localizing buggy methods, localizing buggy lines, and generating patches) to determine how many times the LLM should be called at each stage. The rationale is that calling the LLM more times does not significantly improve performance beyond a certain point. Based on these manual experiments, we finalized the settings of the parameters. For buggy method localization, we call the LLM only once. For buggy line localization, we call the LLM 10 times, collecting 10 responses, each identifying potential buggy lines. We then filter out"}, {"title": "4.3 Research Questions", "content": "In this work, we aim to answer the following research questions (RQs) for evaluating the effectiveness of various software artifacts in assisting the chosen LLM in bug localization and APR task.\n\u2022 RQ1. Which software artifacts can better assist LLMs in localizing buggy methods and buggy lines when provided with source code? This RQ aims to identify the specific types of software artifacts - issue content, debugging information and error stack trace - that enhance the ability of LLMs to accurately pinpoint methods and code that contain bugs.\n\u2022 RQ2. Which software artifacts can better assist LLMs in generating plausible patches when provided with buggy methods? By answering this RQ, we can identify the specific types of software artifacts in assisting LLMs to produce plausible patches which can pass all unit tests.\n\u2022 RQ3. What is the overall performance of DEVLORE in bug localization and program repair? Unlike some LLM-based approaches that focus on either fault localization or pro- gram repair while relying on traditional methods such as spectrum-based approaches [3] for fault localization [20, 36], DEVLORE supports an end-to-end bug identification and program repair workflow. This RQ aims to evaluate the overall performance of DEVLORE from the initial input of a code repository to the final output of a complete repair."}, {"title": "5 Results", "content": "We first use our MethodRecorder tool, which runs the failing tests and records the signatures of the methods that have been executed to narrow down the scope of buggy methods. Then we provide the LLM with the signatures of the executed methods, along with issue content (including the issue description and discussion, which we crawled from each bug's issue URL) and the error stack trace corresponding to the failing test case, as supplied by the Defects4J framework. In this step, we choose not to generate the debugging information. The main reason is that the debugging information can be too long for LLMs to process effectively before the buggy method(s) have been localized. Using the prompt specified in Table 1, we ask the LLM to output the signatures of buggy method(s). To evaluate what kinds of software artifacts can better assist the LLM in localizing buggy methods, we conduct four separate experiments by feeding the LLM with (1) only executed method signatures, (2) executed method signatures and issue content, (3) executed method signatures and error stack trace, and (4) executed method signatures, issue content and error stack trace."}, {"title": "5.1.2 Localizing Buggy Line(s)", "content": "To evaluate LLMs' ability in localizing buggy lines, we provide the LLM with the buggy method(s) baseline (i.e., the ground truth which can be extracted from human developer's correct patch) and ask the LLM to predict which lines are buggy. If the line number generated by the LLM exactly matches the first line added in the human developer's correct patch in Defects4J, we consider it an exact match. If the predicted line number falls within a range of n lines from the first added line in the human developer's patch (for example, if the correct patch starts at Line 368, and the LLM\u2019s prediction falls within the range from 368 n to 368 + n), we classify it as a Range-n match.\nTable 4 shows the effectiveness of buggy line localization with different artifacts. We can see that when the LLM is fed with issue content, it performs the best among all single software artifacts, achieving 49.5% in exact matches, 74.4% in Range-3, and 77.5% in Range-5 for single methods. When adding more software artifacts, the performance can be further increased. For example, when adding issue content to error stack, the exact, Range-3, and Range-5 buggy line match for single methods can be increased from 45.0% to 49.1%, 66.9% to 73.2%, and 71.0% to 77.5%. The last row indicates that, when combining all artifacts, 68.9% of buggy lines in single-method bugs and 9% of buggy lines in non-single-method bugs are correctly localized. Similar to Figure 7, Figure 8 demonstrates"}, {"title": "Answer to RQ1:", "content": "Among all three types of single software artifacts, issue content is the most effective in assisting the LLM with bug localization. Furthermore, different types of software artifacts complement each other in localizing buggy methods and buggy lines. This aligns with how human developers typically approach bug localization, as they often rely on information from multiple sources to identify bugs."}, {"title": "5.2 RQ2. Program Repair Based on Provided Method-level Fault Localization", "content": null}, {"title": "Answer to RQ2:", "content": "Among all three types of single software artifacts, error stack is the most effective in assisting the LLM with program repair with provided buggy method localization. Different combination of software artifacts complement each other in generating plausible patches and combining all three artifacts can achieve 56.0% of program repair, outperforming the SOTA approaches in program repair."}, {"title": "5.3 RQ3. End-to-end Performance from Code Repository to Program Repair", "content": "Currently, most of LLM-based approaches focus on either fault localization or program repair based on the already-localized buggy methods. We investigated RQ1 and RQ2 to compare with these approaches. In contrast, our DEVLORE fully relies on LLMs for both fault localization and program repair. Furthermore, from fault localization to program repair, we not only provide the LLM with the localized buggy method signatures and lines, but also supply it with the complete method body. When the LLM is asked for the program repair task, it may reconsider and repair other parts of the code beyond the specified buggy location. Therefore, the overall end-to-end performance from the buggy code without localization to a complete program repair cannot be simply calculated by multiplying the fault localization rate by the program repair rate. Conducting a comprehensive end-to-end assessment can offer a better understanding of DEVLORE's performance throughout the entire fault localization and program repair process.\nTable 7 shows DEVLORE's end-to-end performance with different software artifacts. When using issue content alone, the LLM can repair 21.8% of bugs, which is the highest among all single software artifacts. This result is also consistent with Agentless [35], one of the best approaches in SWE-bench lite (with Python projects) [2], which relies solely on issue descriptions to assist LLMs in resolving GitHub issues.\nFurthermore, by combining issue content with debugging information and the error stack, the LLM can fix an additional 1.2% and 4.0% of single-method bugs, respectively. When all three software artifacts are combined, the LLM can repair 28.0% of single-method bugs and 11.2% of non-single-method bugs. Furthermore, the combination of different artifacts achieves an end-to-end 39.7% fix rate for single-method bugs and 17.1% for non-single-method bugs. Figure 11 shows similar observations as other two RQs, that different combinations of software artifacts can complement each other in the end-to-end process of bug localization and program repair."}, {"title": "Answer to RQ3:", "content": "Among the three types of software artifacts, issue content is the most effective in assisting the LLM throughout the end-to-end process, from code repository analysis to fault localization and program repair. Different combinations of software artifacts complement each other, enhancing the overall bug localization and repair process. Moreover, our DEVLORE framework can effectively fix bugs at a low cost and within a reasonable time frame."}, {"title": "6 Discussion", "content": null}, {"title": "6.1.1 A simple but efficient framework:", "content": "Unlike approaches that rely on patch skeletons [20], fix templates [44], or type checking [47], DEVLORE adopts a straightforward framework that allows LLMs to handle both fault localization and program repair with the aid of two lightweight tools we implemented MethodRecorder and DebugRecorder, making it both simple and efficient. The findings of RQ1, RQ2, and RQ3 demonstrate that different software artifacts may lead to different performance in bug localization or program repair. Also, because there is no constraints such as fill-in-the-blank templates or code skeletons for generating patches [20, 44], DEVLORE can fix bugs that other tools cannot, especially some non-single method bugs. Our results in Table 7 show that by combining issue content, stack error, and debug information, our approach can fix 11.2% of non-single method bugs. More importantly, combinations of software artifacts can achieve the best performance in all experiments: bug localization, program repair, and the overall streamlined process. One explanation for the strong performance of DEVLORE is that by integrating multiple"}, {"title": "6.1.2 A strict input/output prompt design:", "content": "Table 1 presents the prompts used in our DEVLORE framework. In the GENERAL TASK PROMPT, the LLM is asked to act as a software engineer and conduct the review process, helping the LLM form a clear understanding of the overall task. The INPUT PROMPT includes various types of software artifacts, with clear symbols denoting different hierarchy levels and structures. For example, the {related_methods} in the input prompt wraps the class names in ### symbols, and the method signatures are separated by line breaks. Also, the first line of {debugging_info} in the input prompt represents the currently executed method line, and the second line represents the names and values of the variables in current context (e.g., commons.lang3.math.NumberUtils:createNumber:468{hexDigits:8} represents that the code is about to execute Line 468 and the value of the local variable hexDigits is 8). These strict formatting specifications helps the LLM \u201cunderstand\u201d the structure of the various information from different software artifacts. The EXPECTED OUTPUT PROMPT is very strict in DEVLORE. When localizing buggy methods, we ask the LLM to return a set of buggy method or field locations in the format path.to.ClassA::methodA. For buggy lines, we request the LLM to return a set of buggy line locations in the format path.to.ClassA line:20. During program repair, we employ the well-known SEARCH/REPLACE method, which is commonly used in many state-of-the-art program repair approaches [23, 28, 35, 45]. By enforcing this strict output format, it significantly reduces the likelihood of hallucinations from the LLM. We believe that the clear and strict prompt design in our DEVLORE framework has helped the LLM achieve strong performance in fault localization and program repair."}, {"title": "6.1.3 Plausible patch to correct patch:", "content": "Most state-of-the-art (SOTA) program repair approaches use plausible patches that pass all unit tests as an important evaluation metric, since generating plausible patches within limited time and resources is crucial for practical applications. To facilitate a better comparison, we also used plausible patches in RQ2 (Which software artifacts can better assist LLMs in generating valid patches when provided with buggy methods?) and RQ3 (What is the"}, {"title": "6.2 Threats to Validity", "content": "The first threat to validity is the accuracy of the two tools we implemented: MethodRecorder and DebugRecorder. Both tools rely on mature Java agent technology [1]. We randomly selected several projects and manually verified the outputs of both tools. The manual inspection showed that the outputs were accurate. However, we did not inspect all projects, which may pose a threat to the construct validity.\nAnother potential threat is that the ChatGPT-40-mini model used in this work may have been trained on open-source projects from GitHub, which could overlap with the Defects4J dataset, leading to possible data leakage. To mitigate this, we also randomly selected 100 bugs from another dataset, GrowingBugs [12], and found the plausible fixing rate to be 39%, which may help alleviate this concern. Additionally, the debugging information requires dynamic analysis, which is unlikely to have been used during the model's training. Also as found in [20], directly using the GPT-4 model can not improve the fix rate.\nThe final threat to the external validity is that our experiments were conducted on Java projects, and the findings may not be generalized to projects written in other programming languages. To address this, we plan to design MethodRecorder and DebugRecorder on other programming languages and evaluate DEVLORE on additional datasets across multiple programming languages in our future work."}, {"title": "7 Related Work", "content": null}, {"title": "7.1 Large Language Models for Fault Localization", "content": "Recently, there has been significant interest in using LLMs for fault localization. Toggle incor- porated additional contextual information, such as the buggy line number or code review comments, and greatly enhances the accuracy of predicting both the starting and ending buggy tokens [8]. AGENTFL employs a multi-agent system based on ChatGPT and frames the fault localization task as a three-step process: comprehension, navigation, and confirmation. In each step, AGENTFL deploys specialized agents, each with unique expertise, and uses different tools to address specific tasks [29]. CrashTracker conducts static analysis to map each crash to the corresponding exception instance and identify potential buggy candidates. It then utilizes LLMs to enhance the explainability of the localization results [40]. Jiang et al. assessed the performance of recent commercial closed-source general-purpose LLMs, such as ChatGPT 3.5, ERNIE Bot 3.5, and IFlytek Spark 2.0, on line-level fault localization with the provided buggy method [11]. LLMAO fine-tunes LLMs with 350M, 6B, and"}, {"title": "7.2 Large Language Models for Program Repair", "content": "Recent studies have extensively explored the use of LLMs for program repair. ChatRepair initially provides the LLM with relevant test failure information and then learns from both the failures and successes of previous patching attempts for the same bug, enhancing its ability for more effective APR [39]. GAMMA converts various fix templates into mask patterns and leverages a pre-trained language model to predict the correct code for the masked portions, treating APR as a fill-in-the-blank task [44]. Repilot generates a candidate patch by combining LLM suggestions with a Completion Engine, removing infeasible tokens and filling in gaps proactively [34]. FitRepair integrates the plastic surgery hypothesis into LLM-based APR, combining the direct use of LLMs with two domain-specific fine-tuning strategies and one prompting strategy to enhance its repair capabilities. It can directly generate the correct code in context, effectively \u201cfilling in the blanks\u201d of missing code lines or hunks [36]. Tare incorporated type checking into neural program repair model and can successfully repair 62 and 32 bugs from Defects4J v1.2 and Defects4J v2.0 [47]. GiantRepair creates patch skeletons from LLM-generated patches to narrow the patch space, then generates context-aware, high-quality patches by instantiating these skeletons for specific programs [20]. FixAgent unifies debugging through multi-agent collaboration and achieves strong performance with a three-layer hierarchical structure, where the final layer involves the use of a Web search engine [19]. MORepair fine-tunes LLMs for program repair by adapting both to the syntactic nuances of code transformation and the underlying logic of code changes, enabling the generation of high-quality patches [43]. To leverage LLMs' capabilities and augmented information, CREF is a semi-automatic repair framework for programming tutors, highlighting the potential for enhancing LLMs' repair capabilities through tutor interactions and historical conversations [42]. RepairLLaMA is an innovative program repair method that finds optimal code representations for APR using fine-tuned models, and introduces a state-of-the-art parameter-efficient fine-tuning technique (PEFT) for program repair [31]. Our approach differs from the aforementioned approaches in two key ways. First, we do not rely on fill-in-the-blank templates or skeletons for generating patches, which helps avoid patch overfitting problems in program repair [6, 16, 32]. Second, our fault localization step allows for the identification of multiple buggy methods, which are then fed into the LLM's repair process along with various software artifacts. This provides flexibility to address not only single-method bugs but also bugs spanning across different methods. As seen in Table 7, the end-to-end process from localization to repair can generate plausible patches for 17.1% of non-single-method bugs, while most existing program repair approaches primarily target single-method bugs."}, {"title": "8 Conclusions and Future Work", "content": "This paper presents an LLM-based framework, DEVLORE, for streamlining fault localization and program repair. By mimicking human developers in addressing bug problems and integrating three different software artifacts, DEVLORE demonstrates strong performance in both fault localization and program repair, outperforming current state-of-the-art approaches in terms of bug fixing rate, time, and cost. In addition, unlike more rigid approaches that ask LLMs to fill in the blank within a single buggy method or use the fix template, DEVLORE feed the LLM with only different software artifacts and there are no constraints on the DEVLORE framework regarding how it repairs bugs, DEVLORE has shown significant potential in handling bugs that span across multiple methods.\nOur future work will focus on the following directions: first, expanding DEVLORE to support additional programming languages, such as Python and C/C++, and evaluating its effectiveness on projects written in these languages; and second, testing the DEVLORE framework with a broader range of software artifacts, such as commit history, code review comments, and user documentation, to assess how these additional data sources can further enhance the bug localization and program repair process."}]}