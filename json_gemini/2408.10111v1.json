{"title": "PLUTUS: A Well Pre-trained Large Unified Transformer can Unveil Financial Time Series Regularities", "authors": ["Yuanjian Xu", "Anxian Liu", "Jianing Hao", "Zhenzhuo Li", "Shichang Meng", "Guang Zhang"], "abstract": "Financial time series modeling is essential for understanding and predicting market behaviors, yet it confronts several challenges, including non-linearity, non-stationary, and high noise levels within the data. Traditional models often struggle to capture complex patterns due to these issues, and limitations in computational resources and model capacities further hinder their predictive accuracy. Motivated by the successes of large language models in natural language processing, we introduce PLUTUS, a well Pre-trained Large Unified Transformer-based model which can Unveiling regularities of financial time Series. We first propose an invertible embedding module employing contrastive learning and autoencoder techniques to establish an approximate one-to-one mapping between raw time series data and patch embeddings. We then introduce TimeFormer, an encoder-decoder architecture, as the base of PLUTUS, as we find that the encoder-decoder framework is particularly effective for modeling high-noise time series samples. Additionally, we incorporate multi-scale temporal attention mechanisms to capture features across both variable and temporal dimensions effectively. PLUTUS leverages an unprecedented dataset of 100 billion observations for pre-training, structured to thrive in high-noise financial environments. To the best of our knowledge, PLUTUS is the first open-source, large-scale, pre-trained financial time series model with over one billion parameters. It achieves state-of-the-art performance in several downstream tasks, demonstrating powerful transferability and establishing a robust foundational model for finance. Our research also provides comprehensive technical guidance for pre-training financial time series data, thereby establishing a new paradigm for financial time series modeling.", "sections": [{"title": "Introduction", "content": "Financial time series analysis is of paramount importance due to its critical role across various sectors of finance and economics. A comprehensive understanding and analysis of financial time series data underpin decision-making processes in numerous areas, influencing strategies, risk assessments, and policy formulations at multiple levels (Ozbayoglu, Gudelek, and Sezer 2020). Despite its importance, analyzing financial time series data presents significant challenges that distinguish it from other types of time series, such as traffic flow data, which often exhibit clear trends and periodic patterns (Ren, Li, and Liu 2023). Financial time series are characterized by non-linearity, non-stationary, heteroskedasticity, and high levels of noise (Sewell 2011; Tang et al. 2022), with movements influenced by a complex interplay of factors, such as the overall economic environment, political events, decisions of individual and institutional speculators, and operational policies of commercial firms (Abu-Mostafa and Atiya 1996; Deboeck 1994). These characteristics lead to highly volatile data that often lack the consistent patterns necessary for effective modeling using traditional statistical methods or even advanced deep learning techniques (Zhang, Aggarwal, and Qi 2017).\nTraditional models, constrained by limited parameter capacities, frequently struggle to capture the intricate dependencies and sudden shifts inherent in financial data (Hu 2006). Furthermore, the prevalence of noise and the absence of clear trends in financial time series increase the risk of model overfitting to irrelevant patterns (Bukhari et al. 2020), resulting in poor generalization and predictive performance. The success of large language models (LLMs) in natural language processing (NLP), such as GPT-4 (Achiam et al. 2023), which have demonstrated remarkable abilities in understanding and generating coherent text by learning from vast amounts of sequential data, suggests the potential for a similar approach in financial time series modeling (Xie et al. 2023; Yu et al. 2023). Given that both NLP and financial data are fundamentally sequential in nature, there is a compelling opportunity to apply similar paradigm, as used in NLP, to financial time series modeling.\nHowever, several challenges must be addressed when transferring this technology to financial data. First, the network architecture must be adapted to handle the high noise levels typical in financial time series. This adaptation requires designing a model capable of identifying and focusing on meaningful patterns within a noisy dataset, ensuring that the network does not merely learn the noise. Second, recent advancements have showed that tokenizing time series into patches, similar to embedding layers in NLP, can effectively segment the data into meaningful components (Nie et al. 2022; Cao et al. 2023; Woo et al. 2024). Establishing a one-to-one mapping between these raw financial data patches and their corresponding patch embeddings is crucial, as it enables the model to operate more efficiently in the embedding space, capturing and leveraging the underlying structure of the data for improved performance. Finally, to achieve robust generalization and transferability in financial time series models, it is essential to train the model on various domains of financial data and utilize a large parameter space (Yang, Liu, and Wang 2023). Financial data are diverse, with different types of assets, such as stocks, bonds, and currencies, each exhibiting their own unique patterns, characteristics, and levels of noise (Sewell 2011; Liu et al. 2023a). By training on a wide range of financial time series from diverse domains, the model can learn to identify and generalize across different types of data, enhancing its ability to perform well on unseen tasks.\nIn response to the challenges identified, our contributions are as follows: First, we validated that the encoder-decoder structure is particularly effective in addressing the high noise levels typical of financial time series, with the encoder component functioning as a denoising mechanism that enhances overall analytical accuracy. Second, we introduced a novel architecture based on multi-scale attention, which successfully combines the strengths of channel-wise and point-wise approaches, effectively segmenting time series data into meaningful patches and improving the mapping between raw data and patch embeddings. Third, inspired by advancements in NLP, we designed a reversible embedding module that leverages contrastive learning to establish a robust one-to-one mapping between data patches and patch embeddings, optimizing the model's ability to operate in the embedding space. Finally, to ensure robust generalization, we developed PLUTUS, the first open-source large-scale financial time series pre-trained Model, trained on diverse financial datasets. PLUTUS has demonstrated superior performance across various downstream tasks, and we conducted extensive ablation studies to evaluate the effects of specific modules, ensuring its applicability in various financial environments."}, {"title": "Methods", "content": "Next, we present the architecture of our model, PLUTUS, a pre-trained model for time series analysis. Our method begins with a patching operation applied to the original time series data (Nie et al. 2022). We then introduce a pre-trained embed layer and an invertible embed layer. The embed layer converts the raw patches to the patch embeddings, which serve as the input for the TimeFormer module. The invertible embed layer is used to convert the output of the Time-Former module to the final prediction. We discuss each of these modules in the following sections. An overview of our architecture has been presented in Fig. 3."}, {"title": "Invertible Embedding Module", "content": "Given a time series \\(Y \\in \\mathbb{R}^{C\\times T}\\), where C denotes the number of channels and T denotes the number of time points, we divide this time series into several patches, each with dimensions \\(p_i \\in \\mathbb{R}^{C\\times t_p}\\), where \\(t_p\\) denotes the number of time points per patch. For each patch \\(p_i\\), we generate a positive sample \\(p_i^+=p_i+\\epsilon\\), where \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\\), by adding small noise. A negative sample \\(p_i^-\\) is generated by flipping \\(p_i\\) along the time axis as \\(p_i^- = Flip(p_i)\\). These patches are then passed through the embed layer F(x), producing embeddings \\(X_i \\in \\mathbb{R}^D\\), where D is the embedding dimension:\n\\(X_i = F(p_i), X_i^+ = F(p_i^+), X_i^- = F(p_i^-)\\)\nTo optimize these embeddings, we employ the InfoNCE loss, defined as:\n\\(L_{InfoNCE} = -log \\frac{exp(sim(X_i, X_i^+)/\\tau)}{exp(sim(X_i, X_i^+)/\\tau) + exp(sim(X_i, X_i^-)/\\tau)}\\)\nwhere sim(,) denotes the cosine similarity, and \u03c4 is the temperature parameter. This loss function encourages similar patches to have similar embeddings, while dissimilar patches are pushed apart in the embedding space. Additionally, we introduce an inverse embed layer G(x), which maps the embeddings \\(X_i\\) back to the original patch dimensions as \\(\\hat{p_i} = G(X_i)\\), where \\(\\hat{p_i} \\in \\mathbb{R}^{C\\times t_p}\\): the training objective includes minimizing the Mean Squared Error (MSE) between the original patch \\(p_i\\) and the reconstructed patch \\(\\hat{p_i}\\):\n\\(L_{MSE} = \\frac{1}{N}\\sum_{i=1}^{N}||p_i - \\hat{p_i}||^2\\)\nThis approach ensures that the learned embeddings are both discriminative (via InfoNCE) and capable of being accurately inverted back to the original patches (via MSE)."}, {"title": "TimeFormer", "content": "Fusion Attention. Given an input tensor \\(X \\in \\mathbb{R}^{B\\times C\\times T \\times D}\\), where B is the batch size, C is the number of channels, T is the number of patches, and D is the embedding dimension, the multi-scale attention module processes the input sequentially through time-aware attention, channel-aware attention, and fusion attention.\nFor a single head in time-aware attention and channel-aware attention, a total of six trainable matrices are used to compute the query (Q), key (K), and value (V) vectors. These matrices are \\(W_Q^t, W_K^t, W_V^t\\) for time-aware attention and \\(W_Q^c, W_K^c, W_V^c\\) for channel-aware attention, each with dimensions \\(\\mathbb{R}^{D\\times H}\\), where D is the embedding dimension and H is the projection dimension. The vectors Q, K, and V for time-aware and channel-aware attention are computed as follows:\n\\(Q_t = W_Q^tX, K_t = W_K^tX, V_t = W_V^tX\\)\n\\(Q_c = W_Q^cX, K_c = W_K^cX, V_c = W_V^cX\\)\nThe output dimensions of \\(Q_t, K_t\\), and \\(V_t\\) are \\(\\mathbb{R}^{B\\times T \\times H}\\) for time-aware attention, while the output dimensions of \\(Q_c, K_c\\), and \\(V_c\\) are \\(\\mathbb{R}^{B\\times C \\times H}\\) for channel-aware attention. The attention scores for both time-aware and channel-aware attention are then calculated as:\n\\(Score_t = \\frac{Q_t K_t^T}{\\sqrt{H}}, Score_c = \\frac{Q_c K_c^T}{\\sqrt{H}}\\)\nwhere \\(Score_t \\in \\mathbb{R}^{B\\times T \\times T}\\) and \\(Score_c \\in \\mathbb{R}^{B\\times C \\times C}\\). To prevent the model from exposing future time information and to maintain the autoregressive property in the decoder, carefully designed masks \\(M_t \\in \\mathbb{R}^{T \\times T}\\) and \\(M_c \\in \\mathbb{R}^{C \\times C}\\) are applied:\n\\(M_t(c, t, c', t') = \\begin{cases} 1, & \\text{if } c = c' \\text{ and } t' \\leq t \\\\ 0, & \\text{otherwise} \\end{cases}\\)\n\\(M_c(c, t, c', t') = \\begin{cases} 1, & \\text{if } t = t' \\text{ and } c \\neq c' \\\\ 0, & \\text{otherwise} \\end{cases}\\)\nFinally, the attention outputs are computed by applying the softmax function to the masked scores and then multiplying by the value vectors. The dimensions of these outputs are \\(\\mathbb{R}^{B\\times T \\times H}\\) for time-aware attention (\\(Att_t\\)) and \\(\\mathbb{R}^{B\\times C \\times H}\\) for channel-aware attention (\\(Att_c\\)).\n\\(MScore_t = Score_t \\odot M_t, MScore_c = Score_c \\odot M_c\\)\n\\(Att_t = softmax(MScore_t) V_t, Att_c = softmax(MScore_c) V_c\\)\nThe intermediate attention representation \\(Att_m\\) is obtained by combining the time-aware and channel-aware attention outputs through a weighted sum, where \\(\\alpha_t\\) and \\(\\beta_c\\) are learnable scalar parameters:\n\\(Att_m = \\alpha_t \\odot Att_t + \\beta_c \\odot Att_c\\)\nThis intermediate representation \\(Att_m\\) is then fed into a fusion attention layer. The fusion attention layer comprehensively considers both time and channel dimensions, so the mask matrix \\(M_{fusion} \\in \\mathbb{R}^{(T\\times C) \\times (T\\times C)}\\) imposes constraints solely on the temporal dimension:\n\\(M_{fusion}(t, t') = \\begin{cases} 1, & \\text{if } t' \\leq t \\\\ 0, & \\text{otherwise} \\end{cases}\\)\nThe remaining processes are the same as in channel-aware and time-aware attention.\nTimeFormer Block. Similar to the traditional Transformer, the TimeFormer Block integrates a variant multi head attention layer with a feed-forward network to capture dependencies across different time steps in a multi-variable sequential dataset. The input tensor X first undergoes processing to produce an output \\(A \\in \\mathbb{R}^{B\\times T \\times H}\\), where H is the dimensionality of the projected space. This output A is then normalized and subjected to dropout before passing through a fully connected feed-forward network. The feed-forward network consists of two linear transformations with ReLU activation, using learnable parameters \\(W_1 \\in \\mathbb{R}^{H\\times F}\\) and \\(W_2 \\in \\mathbb{R}^{F\\times H}\\), where F is the dimension of the intermediate layer. The final output of the block, after another round of normalization and dropout, is \\(Y \\in \\mathbb{R}^{B\\times T \\times H}\\).\nTimeFormer Encoder and Decoder. The TimeFormer Encoder and Decoder share a similar architecture, but differ in their attention mechanisms. The Encoder applies standard multi-head time-aware attention, allowing each time step to attend to all others within the sequence, thus extracting comprehensive patterns from X. The Decoder, on the other hand, employs masked multi-head time-aware attention, where each time step can only attend to itself and previous steps, preserving the autoregressive property crucial for sequential prediction tasks. The final outputs from the Encoder and Decoder are denoted as \\(H_{enc}\\) and \\(H_{dec}\\), respectively."}, {"title": "Training Process", "content": "The training process for the PLUTUS model begins with the training of the invertible embedding module. This module is designed to map raw time series patches into a latent space and then reconstruct them back into the original space. The process involves generating positive and negative samples for each patch, which are then embedded using the embedding layer. The model is trained using two key loss functions: the contrastive loss (InfoNCE) encourages the model to bring similar patches closer in the latent space while pushing dissimilar ones apart. The reconstruction loss (MSE) ensures that the patches can be accurately reconstructed from their latent representations. Once the embedding module achieves satisfactory performance, it is frozen, meaning its weights are no longer updated during subsequent training stages.\nIn the second phase, the model undergoes multi-task pre-training focused on the TimeFormer Encoder and Decoder. This phase involves training the model on a range of tasks that vary in input-output time series lengths (e.g., 96 \u2192 720, 96 \u2192 360, 48 \u2192 360, 48 \u2192 180). Each task involves dividing the input time series into context and prediction patches, which are then embedded using the frozen embedding module. The encoded context is processed by the TimeFormer Encoder, and the Decoder generates predictions using both the encoded context and the prediction patches in a process known as teacher forcing. This approach prevents the accumulation of errors in the autoregressive decoding process. The model is trained by minimizing the MSE loss between the predicted and actual time series patches, ensuring accurate future predictions across different tasks. The detailed training process is shown in Appendix A."}, {"title": "Experiments", "content": "Financial Data for Pretraining\nLarge-scale datasets are of paramount importance for pre-training large models (Liu et al. 2024). However, curating large-scale financial time series datasets presents significant challenges. Despite the abundance of financial time series data, there is a lack of comprehensive curation across various types and frequencies. We curate Financial Time Series Dataset (FTSD), which encompasses 19 sub datasets intricately representing a broad range of aspects within the financial domain, as detailed in Table 8 in Appendix B. Each sub-dataset may include multiple related time series, offering a comprehensive view of financial time series data. FTSD covers a wide array of sampling frequencies, ranging from macro intervals such as yearly data to more granular intervals like seconds, with total observations exceeding 100B. In pursuit of characterizing the level of complexity inherent to each sub dataset, we have also analyzed the stationary manifested as ADF test statistics (Elliott, Rothenberg, and Stock 1992), forecastability (Goerg 2013), and Hurst exponent (Carbone, Castelli, and Stanley 2004). The rigorous methodologies, key statistics and in-depth analysis of FTSD detail in Appendix B.\nBenchmarks and Overall Comparison\nWe evaluate PLUTUS on three fundamental but challenging tasks in financial time series analysis, as illustrated in Fig. 4."}, {"title": "Ablation Study", "content": "The results from Table 4 clearly demonstrates the significant contributions of each module to the performance of the PLUTUS model. When the encoder is removed, the performance in the prediction task declines by 329%, and in the imputation task by 89.8%. This highlights the encoder's critical role in capturing temporal dependencies and reducing noise. The removal of the decoder also leads to substantial performance drops, with a 110.8% decline in prediction performance and a 27.7% decline in imputation performance. Additionally, removing the channel-aware attention, time-aware attention, and fusion attention mechanisms results in performance declines of 50.2%, 46.9%, and 52.4% in the prediction task, and 20.2%, 32.1%, and 14.5% in the imputation task, respectively. This underscores the importance of these attention mechanisms in capturing various dimensions of financial data patterns. Finally, the removal of the invertible embedding module causes a 36.5% decline in prediction performance and a 31.4% decline in imputation performance. This indicates that the invertible embedding plays a crucial role in aligning the raw patch time series data with the latent vector space, reducing information loss, and providing a better initial representation for the larger model."}, {"title": "Exploration of Embedding Space", "content": "To gain deeper insights into our Invertible Embedding, we utilize two fundamental metrics: alignment and uniformity. The alignment metric \\(M_{align}\\) quantifies the proximity of the embeddings of positive pairs (an original patch pi and its slightly perturbed version \\(p_i^+\\)) in the embedding space, and is defined as:\n\\(M_{align}(f; \\alpha) = \\mathbb{E}_{(x,x^+)} [||X_i - X_i^+||^{\\alpha}]\\)\nHere, \\(X_i = F(p_i)\\) and \\(X_i^+ = F(p_i^+)\\) represent the embeddings of the original and positive patches, respectively, with \u03b1 being a power parameter. The uniformity metric \\(M_{uniform}\\) encourages the embeddings to be uniformly distributed across the hypersphere, thereby preventing them from clustering in restricted regions. It is computed as:\n\\(M_{uniform} (f; t) = log \\mathbb{E}_{i.i.d. (X_i, X_j)} [e^{-t ||X_i - X_j||^2}]\\)\nwhere \\(X_j = F(p_j)\\) is another embedding from the patch set, and t is a temperature parameter. In our case, we set t = 1 as a constant."}, {"title": "Scaling Experiments", "content": "We provide PLUTUS in three sizes - small, base, and large, with key parameter details listed in Table 6. Models are trained on 8*Nvidia H100 (80G). The results demonstrate a clear improvement in both prediction and imputation tasks as the model size increases, as shown in Table 7. This observed trend is consistent with the fundamental scaling law, which suggests that larger models, endowed with a greater number of parameters, tend to exhibit significantly enhanced performance. The scaling behavior underscores the importance of model size in capturing complex financial time series patterns and achieving superior accuracy."}, {"title": "Case Study", "content": "Figure 5 demonstrates the significant advantages of PLUTUSs in predicting the next 196 time steps. Compared to other models (MICN, DLinear, PatchTST, SegRNN), PLUTUSs not only tracks the true value curve more closely but also excels in capturing and anticipating time series trends. PLUTUSs can identify and predict upward or downward trends in the data ahead of time, making it more adaptive and accurate in financial time series forecasting tasks. While other models can only approximate the general downward trend, they struggle to accurately recognize short-term fluctuations. This capability of PLUTUSs is primarily due to its architecture, which effectively captures complex patterns and handles high noise, resulting in more reliable predictions in practical applications."}, {"title": "E. Limitations and Future Work", "content": "In our research, PLUTUS, as a highly performant foundational model for financial time series, has demonstrated exceptional capabilities in effectively learning and capturing general patterns within large volumes of financial time series data. Through training on large-scale data, PLUTUS has shown outstanding performance in many common market scenarios, proving its potential and practicality in the financial domain. However, PLUTUS still faces certain limitations in its performance when dealing with sudden market events, particularly rapid increases or declines. This bottleneck is likely primarily due to insufficient information in the input data rather than a flaw in the model itself. To address this issue and further enhance the predictive capabilities of PLUTUS, we believe that relying solely on time series data is insufficient. Future work will focus on the development of PLUTUS v2, with an emphasis on incorporating support for multimodal data. For example, integrating additional information sources such as breaking news and policy changes can enable a more comprehensive capture of market dynamics, thereby significantly improving the model's performance in unexpected market events. This expansion will make PLUTUS v2 a more robust and accurate tool for financial time series prediction."}, {"title": "F. Showcase", "content": "To present a clear performance of our proposed PLUTUS, we provide some visualizations for downstream forecasting tasks in Fig. 6 and Fig. 7."}, {"title": "B. Financial Time Series Dataset", "content": "FTSD is a collection of financial time series datasets curated for pre-training. To characterize the level of complexity inherent to each sub dataset, we analyzed the stationary manifested as ADF test statistics (Elliott, Rothenberg, and Stock 1992), forecastability (Goerg 2013), and Hurst exponent (Carbone, Castelli, and Stanley 2004). In the context of datasets containing multiple time series, pespecially when these series have different lengths, we implement length-weighted ADF, length-weighted forecastability (Liu et al. 2024), and length-weighted Hurst exponent to ensure that the contribution of each series to the overall indicator is proportional to its length. By doing so, the length-weighted indicators provide a more granular and accurate depiction of each sub dataset, ensuring that longer series appropriately influence the overall stability while preventing shorter series from disproportionately affecting the assessment. The weighted indicators can be formulated as follows:\n\\(\u03c4 = \\sum_{i=1}^{C} T_i\\)\n\\(ADF.(D) = \\frac{\\sum_{i=1}^{C} T_i ADF.(S^{(i)})}{\u03c4}\\)\n\\(Fore.(D) = \\frac{\\sum_{i=1}^{C} (1 - Entropy(F(S^{(i)})))}{\u03c4}\\)\nwhere \\(S_i \\in \\mathbb{R}^{T_i}\\) denotes the i-th time series in dataset D, \\(T_i\\) is the length of \\(S_i\\) and C is the number of time series in dataset D. F(S(i)) denotes the Fourier decomposition of series S(i).\nAs seen from the wide range of changes across these metrics, the FTSD show remarkable diversity. The ADF test statistics have a large distribution range, from extremely negative to positive, indicating that there are many different stationary features in this dataset. It is worth noting that many sub datasets have small length-weighted forecastability values (less than 0.2) indicate that they are more random and may contain more noise or complex nonlinear relationships, posing a great challenge to models. The distribution of length-weighted Hurst exponent indicates that the majority of the time series exhibit long-tern dependencies, presenting a challenge for PLUTUS in effectively capturing and modeling these dependencies."}, {"title": "C. Downstream Tasks", "content": "We introduce the details of downstream experiments, including long-term forecasting, imputation, and portfolio management.\nLong-term Forecasting. The goal of forecasting is to predict the future time points based on the input time series. We report the Mean Square Error (MSE) and Mean Absolute Error (MAE), comparing against nine state-of-the-art baselines, Transformer (Vaswani et al. 2017), Crossformer (Zhang and Yan 2023), iTransformer (Liu et al. 2023b), PatchTST (Nie et al. 2022), TimeMixer (Wang et al. 2024a), DLinear (Zeng et al. 2023), TimesNet (Wu et al. 2022), MICN (Wang et al. 2023), and SegRNN (Lin et al. 2023). These baselines can be categorized into four groups based on their backbone architecture, including Transformer-based, MLP-based, CNN-based, and RNN-based. MSE and MAE are calculated as follows:\n\\(MSE = \\frac{1}{n}\\sum_{i=1}^{n}(Y_i - \\hat{Y_i})^2\\),\n\\(MAE = \\frac{1}{n}\\sum_{i=1}^{n}|Y_i - \\hat{Y_i}|\\),\nwhere y, \u0177 \u2208 RF\u00d7C represent the ground truth and predicted results, respectively, for F future time points and C dimensions. Yi denotes the i-th future time point. The lookback sequence length is set to 96, and the prediction lengths T considered include {96, 196, 336, 720}.\nImputation. The goal of time-series imputation is to recover the value of missing time points precisely. We also report MSE and MAE metrics on the imputation task, comparing against four baselines: iTransformer (Liu et al. 2023b), TiDE (Das et al. 2023), Dlinear (Zeng et al. 2023), and MICN (Wang et al. 2023), covering Transformer-based, MLP-based, and CNN-based. For this task, the lookback sequence length is set to 96, and the top-k value is set to 5, meaning the top 5 most relevant candidate values are considered during the imputation process. The masked ratios are set at {0.125, 0.25, 0.375, 0.5}, allowing for a robust assessment.\nPortfolio Management. Portfolio management involves the selection and optimization of asset allocation to maximize the total (or average) return within a given investment process (Hu and Lin 2019). In the case of portfolio management, we choose the following metrics: (1) Simple daily return (Rd) measures the return of an asset over one day, calculated as: \\(R_d = \\frac{P_t}{P_{t-1}}\\), where Pt is the asset price at time t and Pt-1 is the asset price at the previous trading day. (2) The simple annual sharpe ratio (Sa) measures the performance of an investment compared to a risk-free asset, calculated as\n\\(S_a = \\frac{R_a - R_f}{\\sigma_a}\\),\nwhere Ra is the average annual return of the portfolio, Rf is the risk-free rate, and \u03c3\u03b1 is the standard deviation of the annual return. (3) Maximum Drawdown (MDD) represents the maximum observed loss from a peak to a trough of an asset's price, before a new peak is attained. It is defined as\n\\(MDD = \\max_{t \\in [1,T]} \\frac{\\max_{j\\in [1,t]} P_j \u2013 P_t}{\\max_{j \\in [1,t]} P_j}\\),\nwhere Pt is the price of the asset at time t, and T is the total time period considered.\nWe compare PLUTUS with five common portfolio strategies (Hsu 2004), including equal weighting, market cap weighting, volatility weighting, minimum variance weighting, and Markowitz mean-variance model. The equal"}, {"title": "D. Related Work", "content": "Traditional statistical models, such as ARIMA (Hamilton 2020), ARCH (Engle 1982), and GARCH (Bollerslev 1986), have long been the cornerstone of financial time series analysis, but they struggle with the complexities of nonlinearity, non-stationary, and noise inherent in financial time series data (Sewell 2011; Tang et al. 2022), often leading to overfitting or limited generalization. Although deep learning models like RNNs (Bengio, Simard, and Frasconi 1994), LSTMs (Hochreiter and Schmidhuber 1997), transformers (Vaswani et al. 2017), and their varients have been introduced to capture sequential dependencies and temporal patterns, they also face challenges in effectively modeling the chaotic and volatile nature of financial markets (Zhang, Aggarwal, and Qi 2017). Given these challenges, the emergence of foundation models, presents a promising avenue for improving financial time series analysis. These models leverage large datasets and specialized pre-training processes, enabling them to better generalize across diverse domains and downstream tasks (Bommasani et al. 2021). Recent advancements in foundation models for time series analysis have demonstrated significant improvements in generalization and predictive accuracy. Notably examples include Lag-Llama (Rasul et al. 2023), TimeGPT (Garza and Mergenthaler-Canseco 2023), Timer (Liu et al. 2024) and MOIRAI (Woo et al. 2024), have leveraged large-scale time series datasets and advanced transformer architectures to achieve superior zero-shot forecasting performance. Beyond general time series and forecasting tasks, our goal is to collect a comprehensive financial time series dataset and pre-trained an open-source foundation model designed to effectively generalize across various financial tasks."}, {"title": "E. Limitations and Future Work", "content": "In our research, PLUTUS, as a highly performant foundational model for financial time series, has demonstrated exceptional capabilities in effectively learning and capturing general patterns within large volumes of financial time series data. Through training on large-scale data, PLUTUS has shown outstanding performance in many common market scenarios, proving its potential and practicality in the financial domain. However, PLUTUS still faces certain limitations in its performance when dealing with sudden market events, particularly rapid increases or declines. This bottleneck is likely primarily due to insufficient information in the input data rather than a flaw in the model itself. To address this issue and further enhance the predictive capabilities of PLUTUS, we believe that relying solely on time series data is insufficient. Future work will focus on the development of PLUTUS v2, with an emphasis on incorporating support for multimodal data. For example, integrating additional information sources such as breaking news and policy changes can enable a more comprehensive capture of market dynamics, thereby significantly improving the model's performance in unexpected market events. This expansion will make PLUTUS v2 a more robust and accurate tool for financial time series prediction."}, {"title": "A. Training Algorithm for PLUTUS Model", "content": "Algorithm 1: Training Algorithm for PLUTUS Model\nInput: Time series data Y \u2208 RC\u00d7T\nOutput: Pretrained PLUTUS Model\nStep 1: Train Invertible Embedding Module;\nfor each patch pi in Y do\nGenerate positive sample p\u2021+ = pi + e, where \u0454 ~ N(0, \u03c3\u00b2), and the negative sample p\u2081 = Flip(pi);\nObtain embeddings: X\u2081 = F(pi), X = F(p), X\u2081 = F(p);\nCompute Contrastive Loss LInfoNCE:;\nCalculate the cosine similarities: sim(X\u2081, X+\u2020+) =X\u2081X\u2081 , sim(Xi, X)=XX\n||Xi ||||X|' ||Xi ||||X|\nCompute the contrastive loss:\nexp(sim(Xi, X+)/T)\nLInfoNCE = log\n*exp(sim(Xi, X++)/\u315c) + exp(sim(Xi, X\u00a1\u00af)/\u315c)\nwhere T is the temperature parameter.\nReconstruct Patches:;\nPass the embeddings through the inverse embedding layer and get \u00ee\u2081 = G(Xi);\nCompute Reconstruction Loss LMSE:;\nCalculate the Mean Squared Error (MSE) loss between the original patch pi and the reconstructed patch \u00f4i:\n\u03a3\u03a1 - Pi2\nN\nLMSE\ni=1\nUpdate Weights: Backpropagate through the embedding module F(x) and inverse embedding layer G(x);\nFreeze the embedding module F(x) and inverse embedding layer G(x);\nStep 2: Pretrain TimeFormer Encoder and Decoder with Task Switching;\nfor each training task ti \u2208 {96 \u2192 720, 96 \u2192 360,48 \u2192 360,48 \u2192 180} do\nfor each batch Yi in task ti do\nInput: Sequence of patches Y\u2081 = [P1, P2, ..., Pn], including context and prediction;\nDivide Y into context patches Ycontext and prediction patches Ypred;\nPass each patch through the frozen embedding module to obtain embeddings:\nXcontext = F(Ycontext), Xpred = F(Ypred)\nForward Propagation with Teacher Forcing:;\nPass Xcontext through TimeFormer Encoder and get Henc = Encoder(Xcontext);\nPass Henc and Xpred (using teacher forcing) through TimeFormer Decoder:\nHdec = Decoder(Henc, Xpred)\nReconstruct the prediction patches using the inverse embedding layer: Ypred = G(Hdec);\nLoss Computation:;\nCompute MSE Loss between original prediction patches Ypred and reconstructed patches Ypred:\nN\nLMSE\n\u03a3 Yp - Ypred2\ni=1\nUpdate Weights: Backpropagate through the Encoder and Decoder to minimize the MSE loss;\nreturn Trained PLUTUS Model;"}]}