{"title": "PLUTUS: A Well Pre-trained Large Unified Transformer can Unveil Financial Time Series Regularities", "authors": ["Yuanjian Xu", "Anxian Liu", "Jianing Hao", "Zhenzhuo Li", "Shichang Meng", "Guang Zhang"], "abstract": "Financial time series modeling is essential for understanding and predicting market behaviors, yet it confronts several challenges, including non-linearity, non-stationary, and high noise levels within the data. Traditional models often struggle to capture complex patterns due to these issues, and limitations in computational resources and model capacities further hinder their predictive accuracy. Motivated by the successes of large language models in natural language processing, we introduce PLUTUS, a well Pre-trained Large Unified Transformer-based model which can Unveiling regularities of financial time Series. We first propose an invertible embedding module employing contrastive learning and autoencoder techniques to establish an approximate one-to-one mapping between raw time series data and patch embeddings. We then introduce TimeFormer, an encoder-decoder architecture, as the base of PLUTUS, as we find that the encoder-decoder framework is particularly effective for modeling high-noise time series samples. Additionally, we incorporate multi-scale temporal attention mechanisms to capture features across both variable and temporal dimensions effectively. PLUTUS leverages an unprecedented dataset of 100 billion observations for pre-training, structured to thrive in high-noise financial environments. To the best of our knowledge, PLUTUS is the first open-source, large-scale, pre-trained financial time series model with over one billion parameters. It achieves state-of-the-art performance in several downstream tasks, demonstrating powerful transferability and establishing a robust foundational model for finance. Our research also provides comprehensive technical guidance for pre-training financial time series data, thereby establishing a new paradigm for financial time series modeling.", "sections": [{"title": "Introduction", "content": "Financial time series analysis is of paramount importance due to its critical role across various sectors of finance and economics. A comprehensive understanding and analysis of financial time series data underpin decision-making processes in numerous areas, influencing strategies, risk assessments, and policy formulations at multiple levels (Ozbayoglu, Gudelek, and Sezer 2020). Despite its importance, analyzing financial time series data presents significant challenges that distinguish it from other types of time series, such as traffic flow data, which often exhibit clear trends and periodic patterns (Ren, Li, and Liu 2023). Financial time series are characterized by non-linearity, non-stationary, heteroskedasticity, and high levels of noise (Sewell 2011; Tang et al. 2022), with movements influenced by a complex interplay of factors, such as the overall economic environment, political events, decisions of individual and institutional speculators, and operational policies of commercial firms (Abu-Mostafa and Atiya 1996; Deboeck 1994). These characteristics lead to highly volatile data that often lack the consistent patterns necessary for effective modeling using traditional statistical methods or even advanced deep learning techniques (Zhang, Aggarwal, and Qi 2017). Traditional models, constrained by limited parameter capacities, frequently struggle to capture the intricate dependencies and sudden shifts inherent in financial data (Hu 2006). Furthermore, the prevalence of noise and the absence of clear trends in financial time series increase the risk of model overfitting to irrelevant patterns (Bukhari et al. 2020), resulting in poor generalization and predictive performance. The success of large language models (LLMs) in natural language processing (NLP), such as GPT-4 (Achiam et al. 2023), which have demonstrated remarkable abilities in understanding and generating coherent text by learning from vast amounts of sequential data, suggests the potential for a similar approach in financial time series modeling (Xie et al. 2023; Yu et al. 2023). Given that both NLP and financial data are fundamentally sequential in nature, there is a compelling opportunity to apply similar paradigm, as used in NLP, to financial time series modeling.\nHowever, several challenges must be addressed when transferring this technology to financial data. First, the network architecture must be adapted to handle the high noise levels typical in financial time series. This adaptation requires designing a model capable of identifying and focusing on meaningful patterns within a noisy dataset, ensuring that the network does not merely learn the noise. Second, recent advancements have showed that tokenizing time series into patches, similar to embedding layers in NLP, can effectively segment the data into meaningful components (Nie et al. 2022; Cao et al. 2023; Woo et al. 2024). Establishing a one-to-one mapping between these raw financial data patches and their corresponding patch embeddings is crucial, as it enables the model to operate more efficiently in the embedding space, capturing and leveraging the underlying structure of the data for improved performance. Finally, to achieve robust generalization and transferability in financial time series models, it is essential to train the model on various domains of financial data and utilize a large parameter space (Yang, Liu, and Wang 2023). Financial data are diverse, with different types of assets, such as stocks, bonds, and currencies, each exhibiting their own unique patterns, characteristics, and levels of noise (Sewell 2011; Liu et al. 2023a). By training on a wide range of financial time series from diverse domains, the model can learn to identify and generalize across different types of data, enhancing its ability to perform well on unseen tasks.\nIn response to the challenges identified, our contributions are as follows: First, we validated that the encoder-decoder structure is particularly effective in addressing the high noise levels typical of financial time series, with the encoder component functioning as a denoising mechanism that enhances overall analytical accuracy. Second, we introduced a novel architecture based on multi-scale attention, which successfully combines the strengths of channel-wise and point-wise approaches, effectively segmenting time series data into meaningful patches and improving the mapping between raw data and patch embeddings. Third, inspired by advancements in NLP, we designed a reversible embedding module that leverages contrastive learning to establish a robust one-to-one mapping between data patches and patch embeddings, optimizing the model's ability to operate in the embedding space. Finally, to ensure robust generalization, we developed PLUTUS, the first open-source large-scale financial time series pre-trained Model, trained on diverse financial datasets. PLUTUS has demonstrated superior performance across various downstream tasks, and we conducted extensive ablation studies to evaluate the effects of specific modules, ensuring its applicability to various financial environments."}, {"title": "Methods", "content": "Next, we present the architecture of our model, PLUTUS, a pre-trained model for time series analysis. Our method begins with a patching operation applied to the original time series data (Nie et al. 2022). We then introduce a pre-trained embed layer and an invertible embed layer. The embed layer converts the raw patches to the patch embeddings, which serve as the input for the TimeFormer module. The invertible embed layer is used to convert the output of the Time-Former module to the final prediction. We discuss each of these modules in the following sections. An overview of our architecture has been presented in Fig. 3."}, {"title": "Invertible Embedding Module", "content": "Given a time series $Y \\in \\mathbb{R}^{C \\times T}$, where C denotes the number of channels and T denotes the number of time points, we divide this time series into several patches, each with dimensions $p_i \\in \\mathbb{R}^{C \\times t_p}$, where $t_p$ denotes the number of time points per patch. For each patch $p_i$, we generate a positive sample $p_i^+ = p_i + \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$, by adding small noise. A negative sample $p_i^-$ is generated by flipping $p_i$ along the time axis as $p_i^- = Flip(p_i)$. These patches are then passed through the embed layer $F(x)$, producing embeddings $X_i \\in \\mathbb{R}^D$, where D is the embedding dimension:\n$X_i = F(p_i), X_i^+ = F(p_i^+), X_i^- = F(p_i^-)$\nTo optimize these embeddings, we employ the InfoNCE loss, defined as:\n$L_{InfoNCE} = -log\\frac{exp(sim(X_i, X_i^+)/\\tau)}{exp(sim(X_i, X_i^+)/\\tau) + exp(sim(X_i, X_i^-)/\\tau)}$\nwhere $sim(\\cdot, \\cdot)$ denotes the cosine similarity, and $\\tau$ is the temperature parameter. This loss function encourages similar patches to have similar embeddings, while dissimilar patches are pushed apart in the embedding space. Additionally, we introduce an inverse embed layer G(x), which maps the embeddings $X_i$ back to the original patch dimensions as $\\hat{p_i} = G(X_i)$, where $\\hat{p_i} \\in \\mathbb{R}^{C \\times t_p}$: the training objective includes minimizing the Mean Squared Error (MSE) between the original patch $p_i$ and the reconstructed patch $\\hat{p_i}$:\n$L_{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} ||p_i - \\hat{p_i}||_2^2$\nThis approach ensures that the learned embeddings are both discriminative (via InfoNCE) and capable of being accurately inverted back to the original patches (via MSE)."}, {"title": "Time Former", "content": "Fusion Attention. Given an input tensor $X \\in \\mathbb{R}^{B \\times C \\times T \\times D}$, where B is the batch size, C is the number of channels, T is the number of patches, and D is the embedding dimension, the multi-scale attention module processes the input sequentially through time-aware attention, channel-aware attention, and fusion attention.\nFor a single head in time-aware attention and channel-aware attention, a total of six trainable matrices are used to compute the query (Q), key (K), and value (V) vectors. These matrices are $W_Q^t, W_K^t, W_V^t$ for time-aware attention and $W_Q^c, W_K^c, W_V^c$ for channel-aware attention, each with dimensions $\\mathbb{R}^{D \\times H}$, where D is the embedding dimension and H is the projection dimension. The vectors Q, K, and V for time-aware and channel-aware attention are computed as follows:\n$Q_t = W_Q^tX, K_t = W_K^tX, V_t = W_V^tX$\n$Q_c = W_Q^cX, K_c = W_K^cX, V_c = W_V^cX$\nThe output dimensions of $Q_t, K_t,$ and $V_t$ are $\\mathbb{R}^{B \\times T \\times H}$ for time-aware attention, while the output dimensions of $Q_c, K_c,$ and $V_c$ are $\\mathbb{R}^{B \\times C \\times H}$ for channel-aware attention. The attention scores for both time-aware and channel-aware attention are then calculated as:\n$Score_t = \\frac{Q_t K_t^T}{\\sqrt{H}}, Score_c = \\frac{Q_c K_c^T}{\\sqrt{H}}$\nwhere $Score_t \\in \\mathbb{R}^{B \\times T \\times T}$ and $Score_c \\in \\mathbb{R}^{B \\times C \\times C}$. To prevent the model from exposing future time information and to maintain the autoregressive property in the decoder, carefully designed masks $M_t \\in \\mathbb{R}^{T \\times T}$ and $M_c \\in \\mathbb{R}^{C \\times C}$ are applied:\n$M_t(c, t, c', t') = \\begin{cases} 1, & \\text{if } c = c' \\text{ and } t' \\leq t \\\\ 0, & \\text{otherwise} \\end{cases}$\n$M_c(c, t, c', t') = \\begin{cases} 1, & \\text{if } t = t' \\text{ and } c \\neq c' \\\\ 0, & \\text{otherwise} \\end{cases}$\nFinally, the attention outputs are computed by applying the softmax function to the masked scores and then multiplying by the value vectors. The dimensions of these outputs are $\\mathbb{R}^{B \\times T \\times H}$ for time-aware attention ($Att_t$) and $\\mathbb{R}^{B \\times C \\times H}$ for channel-aware attention ($Att_c$).\n$MScore_t = Score_t \\odot M_t, MScore_c = Score_c \\odot M_c$\n$Att_t = softmax(MScore_t) V_t, Att_c = softmax(MScore_c) V_c$\nThe intermediate attention representation $Att_m$ is obtained by combining the time-aware and channel-aware attention outputs through a weighted sum, where $\\alpha_t$ and $\\beta_c$ are learnable scalar parameters:\n$Att_m = \\alpha_t Att_t + \\beta_c Att_c$\nThis intermediate representation $Att_m$ is then fed into a fusion attention layer. The fusion attention layer comprehensively considers both time and channel dimensions, so the mask matrix $M_{fusion} \\in \\mathbb{R}^{(T \\times C) \\times (T \\times C)}$ imposes constraints solely on the temporal dimension:\n$M_{fusion}(t, t') = \\begin{cases} 1, & \\text{if } t' \\leq t \\\\ 0, & \\text{otherwise} \\end{cases}$\nThe remaining processes are the same as in channel-aware and time-aware attention.\nTimeFormer Block. Similar to the traditional Transformer, the TimeFormer Block integrates a variant multi head attention layer with a feed-forward network to capture dependencies across different time steps in a multi-variable sequential dataset. The input tensor X first undergoes processing to produce an output $A \\in \\mathbb{R}^{B \\times T \\times H}$, where H is the dimensionality of the projected space. This output A is then normalized and subjected to dropout before passing through a fully connected feed-forward network. The feed-forward network consists of two linear transformations with ReLU activation, using learnable parameters $W_1 \\in \\mathbb{R}^{H \\times F}$ and $W_2 \\in \\mathbb{R}^{F \\times H}$, where F is the dimension of the intermediate layer. The final output of the block, after another round of normalization and dropout, is $Y \\in \\mathbb{R}^{B \\times T \\times H}$."}, {"title": "TimeFormer Encoder and Decoder", "content": "The TimeFormer Encoder and Decoder share a similar architecture, but differ in their attention mechanisms. The Encoder applies standard multi-head time-aware attention, allowing each time step to attend to all others within the sequence, thus extracting comprehensive patterns from X. The Decoder, on the other hand, employs masked multi-head time-aware attention, where each time step can only attend to itself and previous steps, preserving the autoregressive property crucial for sequential prediction tasks. The final outputs from the Encoder and Decoder are denoted as $H_{enc}$ and $H_{dec}$, respectively."}, {"title": "Training Process", "content": "The training process for the PLUTUS model begins with the training of the invertible embedding module. This module is designed to map raw time series patches into a latent space and then reconstruct them back into the original space. The process involves generating positive and negative samples for each patch, which are then embedded using the embedding layer. The model is trained using two key loss functions: the contrastive loss (InfoNCE) encourages the model to bring similar patches closer in the latent space while pushing dissimilar ones apart. The reconstruction loss (MSE) ensures that the patches can be accurately reconstructed from their latent representations. Once the embedding module achieves satisfactory performance, it is frozen, meaning its weights are no longer updated during subsequent training stages.\nIn the second phase, the model undergoes multi-task pre-training focused on the TimeFormer Encoder and Decoder. This phase involves training the model on a range of tasks that vary in input-output time series lengths (e.g., 96 \u2192 720, 96 \u2192 360, 48 \u2192 360, 48 \u2192 180). Each task involves dividing the input time series into context and prediction patches, which are then embedded using the frozen embedding module. The encoded context is processed by the TimeFormer Encoder, and the Decoder generates predictions using both the encoded context and the prediction patches in a process known as teacher forcing. This approach prevents the accumulation of errors in the autoregressive decoding process. The model is trained by minimizing the MSE loss between the predicted and actual time series patches, ensuring accurate future predictions across different tasks. The detailed training process is shown in Appendix A."}, {"title": "Experiments", "content": "Financial Data for Pretraining\nLarge-scale datasets are of paramount importance for pre-training large models (Liu et al. 2024). However, curating large-scale financial time series datasets presents significant challenges. Despite the abundance of financial time series data, there is a lack of comprehensive curation across various types and frequencies. We curate Financial Time Series Dataset (FTSD), which encompasses 19 sub datasets intricately representing a broad range of aspects within the financial domain, as detailed in Table 8 in Appendix B. Each subdataset may include multiple related time series, offering a comprehensive view of financial time series data. FTSD covers a wide array of sampling frequencies, ranging from macro intervals such as yearly data to more granular intervals like seconds, with total observations exceeding 100B. In pursuit of characterizing the level of complexity inherent to each sub dataset, we have also analyzed the stationary manifested as ADF test statistics (Elliott, Rothenberg, and Stock 1992), forecastability (Goerg 2013), and Hurst exponent (Carbone, Castelli, and Stanley 2004). The rigorous methodologies, key statistics and in-depth analysis of FTSD detail in Appendix B.\nBenchmarks and Overall Comparison\nWe evaluate PLUTUS on three fundamental but challenging tasks in financial time series analysis, as illustrated in Fig. 4. The long-term forecasting and imputation are also common tasks for general time series, while portfolio management is a task specific to financial time series data. In each downstream task, we compare PLUTUS with state-of-the-art models, spanning different backbone architectures. The used downstream dataset is derived from the 'CRSP daily price 1984-2024' sub dataset, which includes 20 variables with 203,860 observations. To prevent data leakage, this dataset is excluded form the pre-training stage. For the long-term forecasting task, we follow standard traditional experimental settings by splitting the used downstream dataset into three parts: training, validation, and test sets, with 7:1:2 ratio. In imputation task, we use the masked ratios of {0.125, 0.25, 0.375, 0.5}, where the unmasked portion of the data serves as the training set. Models are trained to accurately recover the missing values, and their performance is validated against the ground truth. For portfolio management task, we assess PLUTUS using various combinations of lookback and forward lengths, including {100-5, 100-10, 255-10, 255-20}. Each strategy assigns a weight matrix based on the assets within the lookback window and verifies the returns within the forward window. The detailed implementation of benchmarks are provided in Appendix C.\nIn long-term financial time series forecasting tasks, as shown in Table 1, TimeMixer demonstrates a significant advantage over DLinear among MLP-based models. TimeMixer identifies distinct patterns in time series at different sampling scales and introduces an MLP-based multiscale mixing architecture (Wang et al. 2024b). According to the experimental data, when predicting a sequence length of 96, TimeMixer achieves an MSE of 1.0615, compared to DLinear's MSE of 2.1670, showing that TimeMixer's error is approximately 51% lower. As the sequence length increases, TimeMixer's multiscale mixing architecture effectively maintains the model's prediction accuracy. For instance, with a sequence length of 720, TimeMixer's MSE is 7.2513, while DLinear's MSE rises to 25.1411, resulting in a roughly 71% reduction in error for TimeMixer.\nMICN, a CNN-based model, applies convolutional kernels to capture temporal correlations from both local and global perspectives within the 1D space. However, it shows limitations as the sequence length increases. For example, at a sequence length of 720, MICN's MSE rises to 26.5799, indicating significant prediction error. In contrast, TimesNet introduces a major advancement by transforming 1D time series data into 2D tensors, where inter-period variations are represented in tensor columns and intra-period variations in tensor rows. This transformation enables TimesNet to effectively capture multi-scale variations both within and across periods. By utilizing hierarchical convolutional layers, TimesNet learns both high-level and low-level representations, leading to superior performance in downstream tasks. As evidenced by the results, TimesNet achieves an MSE of 7.1981 at the same sequence length of 720, underscoring its capability in ultra-long sequence predictions. On the other hand, RNN models like SegRNN exhibit stable performance across different sequence lengths but do not show a clear advantage over other models, with an MSE of 8.1561 at 720, indicating consistency but lacking the exceptional accuracy seen in TimesNet.\nTransformer-based models, when not pre-trained, exhibit significant drawbacks, especially with the standard Transformer underperforming across all prediction lengths. This outcome aligns with expectations, as large-parameter models inherently require more substantial training data to achieve optimal performance (Kaplan et al. 2020). For instance, at T=96, the Transformer records an MSE of 37.9486 and an MAE of 3.8529, significantly higher than other models. This highlights the critical role of pre-training in enhancing Transformer models for long-term time series forecasting tasks. In contrast, our model, PLUTUS, consistently outperforms other models across all prediction lengths, with an average reduction of 60% in MSE and 15% in MAE. This suggests that pre-training on large-scale financial time series data has enabled PLUTUS to effectively capture unique patterns within financial data, allowing it to maintain low error rates even as the prediction length increases.\nImputation is another critical task in time series analysis, and the results in Table 2 highlight varying performance across models. While Crossformer, TiDE, and MICN perform reasonably well, they fall short of PLUTUSs, which consistently achieves the lowest MSE, particularly at lower masking ratios, with a 39.74% reduction at 0.125. In contrast, iTransformer shows a more uniform error distribution with closer MSE and MAE values, indicating stable performance with fewer large deviations but less precision overall. PLUTUSs excels in minimizing significant errors, though it produces more consistent moderate errors, showcasing its strength in scenarios where reducing large prediction errors is crucial.\nBeyond the basic financial time series tasks such as prediction and imputation, in classic portfolio management scenarios, even when using the simplest investment strategies, portfolios based on PLUTUS predictions demonstrate strong performance. As shown in Table 3, when using the past 100 days of historical data to formulate an investment strategy for the next 5 days, PLUTUS achieves an average daily return ($R_d$) of 0.0005, significantly higher than other returns, with an annualized Sharpe ratio ($S_a$) of 1.4658 and a maximum drawdown (MDD) of -0.0212. This performance surpasses that of the Markowitz model under the same conditions ($R_d$ of 0.0001, $S_a$ of 1.5172, and MDD of -0.0096).\nAlthough the Sharpe ratio of PLUTUS is slightly lower, its smaller maximum drawdown indicates that the accuracy of its time series predictions can help reduce risks in financial decision-making.\nAblation Study. The results from Table 4 clearly demonstrates the significant contributions of each module to the performance of the PLUTUS model. When the encoder is removed, the performance in the prediction task declines by 329%, and in the imputation task by 89.8%. This highlights the encoder's critical role in capturing temporal dependencies and reducing noise. The removal of the decoder also leads to substantial performance drops, with a 110.8% decline in prediction performance and a 27.7% decline in imputation performance. Additionally, removing the channel-aware attention, time-aware attention, and fusion attention mechanisms results in performance declines of 50.2%, 46.9%, and 52.4% in the prediction task, and 20.2%, 32.1%, and 14.5% in the imputation task, respectively. This underscores the importance of these attention mechanisms in capturing various dimensions of financial data patterns. Finally, the removal of the invertible embedding module causes a 36.5% decline in prediction performance and a 31.4% decline in imputation performance. This indicates that the invertible embedding plays a crucial role in aligning the raw patch time series data with the latent vector space, reducing information loss, and providing a better initial representation for the larger model.\nExploration of Embedding Space. To gain deeper insights into our Invertible Embedding, we utilize two fundamental metrics: alignment and uniformity. The alignment metric $M_{align}$ quantifies the proximity of the embeddings of positive pairs (an original patch $p_i$ and its slightly perturbed version $p_i^+$) in the embedding space, and is defined as:\n$M_{align}(f; \\alpha) = \\mathbb{E}_{(x, x^+)} [\\lVert X_i - X_i^+ \\rVert^{\\alpha}]$\nHere, $X_i = F(p_i)$ and $X_i^+ = F(p_i^+)$ represent the embeddings of the original and positive patches, respectively, with $\\alpha$ being a power parameter. The uniformity metric $M_{uniform}$ encourages the embeddings to be uniformly distributed across the hypersphere, thereby preventing them from clustering in restricted regions. It is computed as:\n$M_{uniform}(f; t) = log \\mathbb{E}_{i.i.d. (X_i, X_j)} [e^{-t \\lVert X_i - X_j \\rVert^2}]$\nwhere $X_j = F(p_j)$ is another embedding from the patch set, and t is a temperature parameter. In our case, we set t = 1 as a constant.\nTable 5 demonstrates that the choice of patch size and embedding dimension needs to be carefully aligned. When the patch size is 16, the alignment scores tend to decrease as the embedding dimension increases, but a bottleneck is observed. Conversely, for patch size 8, the alignment scores increase with larger dimensions, indicating that smaller patches make it easier to bring samples closer together when using contrastive loss. Additionally, the results also suggest that larger embedding dimensions may facilitate a more uniform distribution of samples across the embedding space, as reflected in the uniformity scores. Combining the insights from Table 4 and Table 5, it is evident that the embedding encoder-decoder structure is particularly well-suited for financial time series data. The complementary nature of the encoder and decoder allows for better representation of high-noise sequence data, enabling more effective learning and generalization. Together, these components help to extract meaningful patterns from complex and noisy financial data.\nScaling Experiments. We provide PLUTUS in three sizes - small, base, and large, with key parameter details listed in Table 6. Models are trained on 8*Nvidia H100 (80G). The results demonstrate a clear improvement in both prediction and imputation tasks as the model size increases, as shown in Table 7. This observed trend is consistent with the fundamental scaling law, which suggests that larger models, endowed with a greater number of parameters, tend to exhibit significantly enhanced performance. The scaling behavior underscores the importance of model size in capturing complex financial time series patterns and achieving superior accuracy."}, {"title": "B. Financial Time Series Dataset", "content": "FTSD is a collection of financial time series datasets curated for pre-training. To characterize the level of complexity inherent to each sub dataset, we analyzed the stationary manifested as ADF test statistics (Elliott, Rothenberg, and Stock 1992), forecastability (Goerg 2013), and Hurst exponent (Carbone, Castelli, and Stanley 2004). In the context of datasets containing multiple time series, pespecially when these series have different lengths, we implement length-weighted ADF, length-weighted forecastability (Liu et al. 2024), and length-weighted Hurst exponent to ensure that the contribution of each series to the overall indicator is proportional to its length. By doing so, the length-weighted indicators provide a more granular and accurate depiction of each sub dataset, ensuring that longer series appropriately influence the overall stability while preventing shorter series from disproportionately affecting the assessment. The weighted indicators can be formulated as follows:\n$\\Tau = \\sum_{i=1}^{C} T_i$\n$ADF(D) = \\frac{\\sum_{i=1}^{C} T_i ADF(S^{(i)})}{\\sum_{i=1}^{C} T_i}$\n$Fore(D) = \\sum_{i=1}^{C} (1 - Entropy(F(S^{(i)})))$\nwhere $S_i \\in \\mathbb{R}^{T_i}$ denotes the i-th time series in dataset D, $T_i$ is the length of $S_i$ and C is the number of time series in dataset D. $F(S^{(i)})$ denotes the Fourier decomposition of series $S^{(i)}$.\nAs seen from the wide range of changes across these metrics, the FTSD show remarkable diversity. The ADF test statistics have a large distribution range, from extremely negative to positive, indicating that there are many different stationary features in this dataset. It is worth noting that many sub datasets have small length-weighted forecastability values (less than 0.2) indicate that they are more random and may contain more noise or complex nonlinear relationships, posing a great challenge to models. The distribution of length-weighted Hurst exponent indicates that the majority of the time series exhibit long-tern dependencies, presenting a challenge for PLUTUS in effectively capturing and modeling these dependencies."}, {"title": "C. Downstream Tasks", "content": "We introduce the details of downstream experiments, including long-term forecasting, imputation, and portfolio management.\nLong-term Forecasting. The goal of forecasting is to predict the future time points based on the input time series. We report the Mean Square Error (MSE) and Mean Absolute Error (MAE), comparing against nine state-of-the-art baselines, Transformer (Vaswani et al. 2017), Crossformer (Zhang and Yan 2023), iTransformer (Liu et al. 2023b), PatchTST (Nie et al. 2022), TimeMixer (Wang et al. 2024a), DLinear (Zeng et al. 2023), TimesNet (Wu et al. 2022), MICN (Wang et al. 2023), and SegRNN (Lin et al. 2023). These baselines can be categorized into four groups based on their backbone architecture, including Transformer-based, MLP-based, CNN-based, and RNN-based. MSE and MAE are calculated as follows:\n$MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i-\\hat{y}_i)^2,$\n$MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i-\\hat{y}_i|$\nwhere y, \u0177 \u2208 $\\mathbb{R}^{F \\times C}$ represent the ground truth and predicted results, respectively, for F future time points and C dimensions. $y_i$ denotes the i-th future time point. The lookback sequence length is set to 96, and the prediction lengths T considered include {96, 196, 336, 720}.\nImputation. The goal of time-series imputation is to recover the value of missing time points precisely. We also report MSE and MAE metrics on the imputation task, comparing against four baselines: iTransformer (Liu et al. 2023b), TiDE (Das et al. 2023), Dlinear (Zeng et al. 2023), and MICN (Wang et al. 2023), covering Transformer-based, MLP-based, and CNN-based. For this task, the lookback sequence length is set to 96, and the top-k value is set to 5, meaning the top 5 most relevant candidate values are considered during the imputation process. The masked ratios are set at {0.125, 0.25, 0.375, 0.5}, allowing for a robust assessment.\nPortfolio Management. Portfolio management involves the selection and optimization of asset allocation to maximize the total (or average) return within a given investment process (Hu and Lin 2019). In the case of portfolio management, we choose the following metrics: (1) Simple daily return ($R_d$) measures the return of an asset over one day, calculated as: $R_d = \\frac{P_t - P_{t-1}}{P_{t-1}}$, where Pt is the asset price at time t and $P_{t-1}$ is the asset price at the previous trading day. (2) The simple annual sharpe ratio ($S_a$) measures the performance of an investment compared to a risk-free asset, calculated as\n$S_a = \\frac{R_a - R_f}{\\sigma_a},$\nwhere $R_a$ is the average annual return of the portfolio, $R_f$ is the risk-free rate, and $\\sigma_a$ is the standard deviation of the annual return. (3) Maximum Drawdown (MDD) represents the maximum observed loss from a peak to a trough of an asset's price, before a new peak is attained. It is defined as\n$MDD = \\max_{t \\in [1, T]} (\\max_{j \\in [1, t]} \\frac{P_j - P_t}{P_j})$, where Pt is the price of the asset at time t, and T is the total time period considered.\nWe compare PLUTUS with five common portfolio strategies (Hsu 2004), including equal weighting, market cap weighting, volatility weighting, minimum variance weighting, and Markowitz mean-variance model. The equal weighting strategy assigns equal weights to all assets in the portfolio. The market cap weighting strategy assigns weights based on the assets' market capitalization. The volatility weighting strategy allocates weights according to the volatility of the assets, with less volatile assets receiving higher weights to reduce overall portfolio risk. The minimum variance strategy optimizes the portfolio by minimizing its overall volatility through the calculation of the covariance matrix of asset returns and using an optimization algorithm to determine the set of asset weights that result in the lowest possible portfolio volatility. The Markowitz mean-variance model (Markowitz and Todd 2000) optimizes the portfolio by finding the optimal balance between expected return and risk. It minimizes portfolio variance while ensuring that the sum of asset weights equals one and that all weights are non-negative. It identifies the optimal asset allocation that offers the best trade-off between risk and return."}, {"title": "D. Related Work", "content": "Traditional statistical models, such as ARIMA (Hamilton 2020), ARCH (Engle 1982), and GARCH (Bollerslev 1986), have long been the cornerstone of financial time series analysis, but they struggle with the complexities of non-linearity, non-stationary, and noise inherent in financial time series data (Sewell 2011; Tang et al. 2022), often leading to overfitting or limited generalization. Although"}]}