{"title": "Adaptive Prompt Tuning: Vision Guided Prompt Tuning with\nCross-Attention for Fine-Grained Few-Shot Learning", "authors": ["Eric Brouwer", "Jan Erik van Woerden", "Gertjan Burghouts", "Matias Valedenegro-Toro", "Marco Zullich"], "abstract": "Few-shot, fine-grained classification in computer vision poses significant challenges due to the need to dif-\nferentiate subtle class distinctions with limited data. This paper presents a novel method that enhances the\nContrastive Language-Image Pre-Training (CLIP) model through adaptive prompt tuning, guided by real-time\nvisual inputs. Unlike existing techniques such as Context Optimization (CoOp) and Visual Prompt Tuning\n(VPT), which are constrained by static prompts or visual token reliance, the proposed approach leverages a\ncross-attention mechanism to dynamically refine text prompts for the image at hand. This enables an image-\nspecific alignment of textual features with image patches extracted from the Vision Transformer, making the\nmodel more effective for datasets with high intra-class variance and low inter-class differences. The method\nis evaluated on several datasets, including CUBirds, Oxford Flowers, and FGVC Aircraft, showing signifi-\ncant performance gains over static prompt tuning approaches. To ensure these performance gains translate\ninto trustworthy predictions, we integrate Monte-Carlo Dropout in our approach to improve the reliability of\nthe model predictions and uncertainty estimates. This integration provides valuable insights into the model's\npredictive confidence, helping to identify when predictions can be trusted and when additional verification is\nnecessary. This dynamic approach offers a robust solution, advancing the state-of-the-art for few-shot fine-\ngrained classification.", "sections": [{"title": "1 Introduction", "content": "In recent years, the field of computer vision has\nexperienced remarkable growth and transformation\n(Alom et al., 2019), driven by significant advance-\nments in machine learning and deep learning tech-\nnologies. This progress is partly attributable to the\ndevelopment of large-scale pre-trained models, com-\nmonly referred to as foundation models. By train-\ning on extensive datasets containing millions of data,\nthese models are able to perform exceptionally well\neven on previously unseen tasks. Among these foun-\ndation models, CLIP (Contrastive Language-Image\nPre-Training) (Radford et al., 2021) has emerged as\na particularly influential tool. CLIP involves learning\na joint embedding space for both textual and visual\ndata using contrastive learning on a large corpus of\ntext-image pairs.\nFoundation models can be fine-tuned to solve\nspecific downstream tasks achieving state-of-the-art\nperformance while often requiring fewer computa-\ntional resources if compared to training a model from\nscratch (Jena et al., 2022). This also translates to sit-\nuations whenever data availability is limited, a sit-\nuation commonly identified as zero- and few-shot\nlearning (Lemley et al., 2017). In these cases, Deep\nNeural Networks trained from scratch on these lim-\nited datasets have been shown to be severely overfit-\nting (Nakkiran et al., 2021). CLIP embeddings can\nbe used as-is (static prompting) on the downstream\ntask to perform zero-shot learning; additionally, the\nembeddings can be adapted to a specific dataset (dy-\nnamic prompting) to perform, e.g., few-shot learning.\nPrevious works such as Context Optimization\n(CoOp) (Zhou et al., 2022a) and Visual Prompt Tun-\ning (VPT) (Jia et al., 2022) have been explored to en-\nhance CLIP for few-shot learning; however, they are\nstill prone to poor generalization, especially on fine-"}, {"title": "2 Related Works", "content": "Vision-language Models Vision-language models\n(VLMs) integrate computer vision and natural lan-\nguage processing to jointly learn representations of\nvisual and textual data. By embedding images and\ntextual descriptions into a shared space, VLMs enable\ntasks like zero-shot classification, image captioning,\nand visual search without the need for task-specific\ntraining data (Chen et al., 2020, H\u00e9naff et al., 2020),\nleveraging the possibility of guiding the classification\nthrough natural-language text prompting. CLIP (Rad-\nford et al., 2021), specifically, has been shown to be"}, {"title": "Few-shot Learning", "content": "Few-shot learning aims to en-\nable models to recognize new tasks or objects with\nminimal data, inspired by human cognitive abili-\nties to generalize from few examples (Wang et al.,\n2020). Deep Neural Networks rely heavily on large\ndatasets to achieve state-of-the-art performance; how-\never, they tend to overfit on small datasets (Nakki-\nran et al., 2021), thus often being unsuitable for few-\nshot scenarios. Several methods have been proposed\nto address this challenge. Meta-learning trains mod-\nels across various tasks to help them quickly adapt to\nnew tasks with minimal data (Chen et al., 2021). Pro-\ntotypical networks (Snell et al., 2017) offer another\nsolution by learning a metric space where classifica-\ntion is based on the distance to prototype represen-\ntations of each class (Ding et al., 2020). More re-\ncent approaches are based on fine-tuning large foun-\ndation models such as CLIP, leveraging (a) the gen-\neralization capabilities of models pre-trained on vast\namounts of data and (b) the aforementioned possi-\nbility of using text prompts for guiding the classifi-\ncation. Prompt tuning has emerged as a viable ap-\nproach to fine-tuning foundation models for few-shot\nlearning. Frameworks like CoOp (Zhou et al., 2022b)\nenhance CLIP by learning task-specific prompt em-\nbeddings. Extensions like CoCoOp (Zhou et al.,\n2022a) further improve robustness to unseen classes\nby incorporating image features. Additionally, visual\nprompt tuning (VPT) (Jia et al., 2022), which tunes\nimage encoders with learnable task-specific prompts,\nhas proven effective in low-data scenarios, preserving\ngeneralization while minimizing the need for exten-\nsive retraining or large labeled datasets."}, {"title": "Fine-grained Recognition", "content": "Fine-grained image\nrecognition focuses on the task of distinguishing\nbetween highly similar subcategories within a larger,\ngeneral category, such as identifying specific species\nof birds (Wah et al., 2011), types of cars (Dehghan\net al., 2017), or types of air-crafts (Maji et al., 2013).\nThis domain presents a unique set of challenges that"}, {"title": "Uncertainty Quantification", "content": "UQ plays a critical\nrole in assessing the confidence of machine learning\nmodels, especially in high-stakes applications. By\nevaluating the reliability of predictions, UQ helps to\nidentify areas where models may fail or need im-\nprovement, thus increasing the robustness and trust-\nworthiness of AI systems. Within the framework of\nDeep Learning, (approximate) Bayesian Neural Net-\nworks (BNNs) offer a strong framework for quanti-\nfying predictive uncertainty by placing distributions\nover model parameters rather than learning fixed\nweights (Goan and Fookes, 2020). At inference time,\na predictive distribution rather than a point one is\nproduced, allowing for considerations on the predic-\ntive uncertainty. While exact Bayesian inference is of-\nten unfeasible to implement in deep learning, approx-\nimate methods, like MCD, Bayes-by-backprop (Blun-\ndell et al., 2015), and Deep Ensembles (Ganaie et al.,\n2022) are often used instead. Specifically, MCD (Gal\nand Ghahramani, 2016) operates by using dropout as\na way of inducing stochasticity in the output. Despite\noften showcasing worse UQ capability with regards\nto other tools, MCD is still used due to its simplic-\nity, since it can be used straight away on architectures\nwhich already employ Dropout (Valdenegro-Toro and\nMori, 2022). Within the field of guided prompt tun-\ning for fine-grained few-shot learning, we have only\nidentified one work (Miao et al., 2024) applying a\nBayesian framework, which augments the visual and\ntext embeddings produced by CLIP, in order to pro-\nduce uncertainty estimates."}, {"title": "Our contribution", "content": "In summary, the contributions of\nthe present work are as follows:\n\u2022 We introduce a novel cross-attention based\nprompt tuning mechanism that jointly optimizes\nvisual and text embeddings, delivering a compet-\nitive or superior performance relative to the state-\nof-the-art guided prompt tuning approaches.\n\u2022 We conduct a comprehensive UQ analysis,\ndemonstrating notable improvements in the qual-\nity of uncertainty estimates produced by our pro-\nposed model."}, {"title": "3 Materials and Methods", "content": ""}, {"title": "3.1 CLIP", "content": "The CLIP model uses a dual encoding mechanism,\none for images and another for text, aimed at learning\ncombined visual-textual representations. Each com-\nponent is specialized to translate its respective input\ninto a common feature space where the semantic con-\ntents of both modalities are directly comparable.\nAs represented in Figure 2, CLIP employs two Deep\nNeural Networks that are tasked with jointly encod-\ning (image, text) pairs. The CLIP model uses Trans-\nformer architectures for both image and text encod-\ning, with the main difference being in the input pro-\ncessing. The image encoder (ViT) (Dosovitskiy et al.,\n2020) splits the input image (H,W,C) into 16 \u00d7 16\npatches and linearly embeds them, while the text en-\ncoder tokenizes and embeds the input text. Both en-\ncoders then use a series of self-attention layers to pro-\nduce final encodings of the same dimension d. (Rad-\nford et al., 2021) trained CLIP in a self-supervised\nfashion on a large dataset of text-image pairs. They\nemployed contrastive learning with the goal of cre-\nating an embedding space in which positive, i.e., re-\nlated, (image, text) pairs are pulled closer together,\nwhile negative, i.e., unrelated, pairs are pushed apart."}, {"title": "Zero-Shot inference with CLIP", "content": "The setup for a\nclassification task can be performed by taking the fea-\ntures generated by the image encoder, and using the\ncosine similarity metric to compare this to a set of\nencoded prompts that act as the relevant categories\n(Radford et al., 2021). At the most basic approach,\na prompt can take the form of a sentence \"a photo of\na [CLASS]\" where [CLASS] can be a category such\nas \"bird\" or \"car\". Formally, we can define the image\nencoder as $f_{\\Theta}$ and text encoder as $f_{\\Upsilon}$. Given an input\nimage x and a set of static prompts Y = {y1, y2,..., yk}"}, {"title": "Few-Shot learning with CLIP", "content": "In the few-shot\nlearning scenario, the goal is to adapt the CLIP model\nto perform better on a target task using a limited num-\nber of labeled examples per class. To achieve this,\nwe introduce a learnable component into the model,\nwhich can be fine-tuned on the few-shot training data.\nThis learnable component can take various forms,\nsuch as learnable vectors added to the input prompts\n(like in CoOp and VPT) or learnable layers process-\ning the visual and/or text embeddings, like in our pro-\nposed method."}, {"title": "3.2 Adaptive Prompt Tuning", "content": "As shown in the preliminary experiments in Ap-\npendix A.1, a static approach to prompt tuning is\ninsufficient for datasets characterized by high intra-\nclass variance in the image features. To address these\nchallenges, we propose an adaptive prompting tech-\nnique based on the cross-attention mechanism. This\nallows for real-time adjustment of text prompts in re-\nsponse to relevant visual information in the test image\nat hand. Figure 1 illustrates the general architecture of\nthe proposed model.\nTo address these challenges, we propose an adap-\ntive prompting technique based on the cross-attention\nmechanism. This allows for real-time adjustment of\ntext prompts in response to relevant visual informa-\ntion in the test image at hand. Figure 1 illustrates the\ngeneral architecture of the proposed model.\nThe architecture begins with a ViT, which pro-\ncesses an input image by dividing it into a sequence\nof patches that are flattened. These patches are then\nencoded into image features, with the output from the\nViT including the [CLS] token, which captures the\nglobal representation of the image.\nParallel to the ViT processing of the image, the\ntext encoder processes textual descriptions, typically"}, {"title": "3.3 Comparable Methods", "content": "CoOp Context Optimization (Zhou et al., 2022a) is\na method for few-shot learning in conjunction with\nCLIP. It utilizes the input tokens of the encoded text\nprompts. Specifically, it uses the tokens of the con-\ntext prompt (e.g. \"A photo of...\") before encoding and\nmakes those learnable. During training this freezes\nthe visual encoder, but adjusts the class embeddings to\nmaximize the cosine similarity between the adjusted\nclass embeddings and the training images of the re-\nspective class. However this requires to backpropa-\ngate through the full text encoder and limits the ad-\njustability by the encoder itself.\nVPT Visual Prompt Tuning focuses on the visual\nencoder of CLIP. Instead of fine-tuning the weights\nof each transformer layer inside the encoder, it adds\nadditional learnable tokens to the Transformer layer\nduring training and inference.\n$X = [P_1,..., P_k,x_1,...,X_N]$\nWhere X is the set of input token of each transformer\nlayer, $x_n$ input tokens, which can be a patch embed-\nding or an output token of the previous Transformer"}, {"title": "3.4 Uncertainty Quantification with\nMonte-Carlo Dropout", "content": "As explained in Section 1, UQ is essential for assess-\ning model reliability and improving decision-making\nprocesses, especially in applications requiring high\nassurance in prediction accuracy.\nIn particular, techniques like Monte Carlo (MC)\nDropout have not been widely applied to CLIP or sim-\nilar vision-language models.\nThe presence of Dropout modules into APT allow\nus to implement MCD by simply avoiding to switch\noff the random dropout behavior at inference time. As\na consequence, the output of APT is not deterministic\nanymore. We can use Monte-Carlo sampling to ob-\ntain an output probability distribution. Given a sam-\nple size M, we can calculate an output mean for class\nk:\n$P_k = \\frac{1}{M} \\sum_{m=1}^{M} p^{(m)}_k$\nwhere $p^{(m)}_k$ indicates the output probability for\nclass k at sample m. Then, the predicted class \u0177 is\nobtained as\n$\\hat{y} = arg \\underset{k}{max} P_k$\nWe can estimate the uncertainty by means of en-\ntropy:\n$S = -\\sum_k p_k log p_k$.\nEntropy is not the only way to quantify uncer-\ntainty; other methods, such as using the maximum\npredicted probability $max_k(p_k)$, are also available.\nHowever, we select entropy because it provides a\ncomprehensive measure of uncertainty across all pos-\nsible classes and not only the predicted class. To con-\nvert entropy into confidence, we can normalize en-\ntropy such that it lies in the [0, 1] interval, then sub-\ntract 1 to it:\n$conf = 1 - \\frac{S}{max_s}$,\nwhere maxs indicates the maximum value of en-\ntropy over a given sample of data.\nThe uncertainty estimates are determined to be\noptimal when there is an equality between accuracy\nand confidence, so prediction confidence-which is\nknown at inference time-can be used as a proxy for"}, {"title": "3.5 Datasets", "content": "To evaluate the performance of the proposed vision-\nguided prompting approach, we make use of popular\ndatasets in the field of fine-grained few-shot classifi-\ncation, each chosen for its specific characteristics and\nthe unique challenges it presents.\nThe Caltech-UCSD Birds CUBIRDS dataset\n(Wah et al., 2011) contains 11788 images across 200\ncategories of birds.\nThe Oxford Flowers dataset (Nilsback and Zis-\nserman, 2008) features 8189 images of flowers split\ninto 103 classes, each having from 40 to 258 samples.\nThe FGVC Aircrafts dataset (Maji et al., 2013)\nincludes 10200 pictures of aircraft divided into 102\ncategories, each holding 102 images.\nWe make use of these three datasets for assessing\nthe models capabilities in fine-grained classification\ntasks.\nAdditionally, we use the Caltech101 dataset for\nassessing the model OOD detection capabilities. This"}, {"title": "3.6 Implementation Details", "content": "In our implementation, we made use of CLIP with\na ViT-B/16 image encoder. Both text and image en-\ncoder used multi-head attention layers with 8 heads.\nAs for APT specifically, we applied dropout with a\nrate of 20%. While this value may technically be con-\nsidered low for standard dropout, for MCD we fol-\nlowed the indications by (Seoh, 2020) to keep a lower\ndropout rate. We followed the standard image prepro-\ncessing steps used by CLIP, as well as data augmen-\ntation techniques from CoOp, and VPT, these being\nrandom resized cropping and flipping. Also, similarly\nto the CoOp and VPT set ups, we trained the models\nover 50 epochs with 1 sample per class, 100 epochs\nfor 2 to 4 samples per class, and 150 epochs for 8\nto 16 samples per class. We made use of the SGD\noptimizer with a learning rate of 0.001 and a cosine\ndecay learning rate scheduler. For each combination\nof model (APT, CoOp, VPT), dataset, and samples\nper class, we repeated the training 3 times and report\nthe average performance attained. We performed all\nexperiments on one NVIDIA A100 GPU."}, {"title": "4 Results", "content": ""}, {"title": "4.1 Few-Shot Learning", "content": "In Figure 4 we depict the results in terms of accuracy\nfor APT, CoOp, and VPT. As a baseline, we addition-\nally report the performance of CLIP used as a zero-\nshot classifier.\nAcross the FGVC Aircraft dataset, which exhibits\nhigh intra-class variance, our cross-attention model\noutperformed others from 2 to 16 shots. Starting\nwith 27% accuracy at one shot and rising to 47% at\n16 shots, it showed a significant improvement over\nthe Zero-shot baseline (17%). This demonstrates the\nmodel's ability to successfully leverage additional ex-\namples. CoOp and VPT also improved, but not as\nmarkedly, highlighting the strength of our approach\nin handling complex variations within the dataset.\nIn contrast, the Oxford Flowers dataset, with dis-\ntinct inter-class features, showed strong performance\nacross models. Our model reached 84% accuracy\nwith one shot, improving to 97% at 16 shots. CoOp\nclosely followed, while VPT lagged behind. The\nbaseline of 72% further highlights the significant en-\nhancement brought by few-shot learning. Lastly, the\nCUBirds dataset, with both high inter- and intra-class"}, {"title": "4.2 Uncertainty Quantification", "content": "Expected Calibration Error The ECE for few-\nshot learning across different datasets as function of\nthe number of samples provides an insight into the\nmodel's reliability of its confidence estimates with\nlimited training data (Figure 5)."}, {"title": "5 Discussion and Conclusions", "content": "The present paper introduces a novel cross-attention-\nbased prompt tuning approach, which we call\nAdaptive Prompt Tuning (APT), aimed at enhancing\nfew-shot learning for fine-grained classification. We\nevaluated this model, alongside other state-of-the-art\nprompt tuning approaches, CoOp and VPT, across\nfour datasets: FGVC Aircraft, Oxford Flowers,\nCUBirds, and Caltech101. The cross-attention\nmechanism demonstrated significant performance\nimprovements, particularly in datasets with high\nintra-class variance. For instance, the model achieved\nsubstantial gains in the FGVC Aircraft dataset,\nimproving accuracy from 27% to 47% as the number"}, {"title": "A APPENDIX", "content": ""}, {"title": "A.1 Preliminary Investigations", "content": "Text Encoder Behaviour To understand how\nCLIP's text encoder handles fine-grained versus base-\nline descriptive texts, we embedded sentences using\nthe text encoder and applied t-SNE for visualization\n(see Figure 9). In these plots, baseline prompts are in\nred, and fine-grained prompts are in blue.\nFor the CUBirds and Oxford Flowers datasets,\nthere is significant overlap between the embeddings\nof baseline and fine-grained prompts. This suggests\nthat class names alone are sufficient for capturing se-\nmantic features, likely because the model is familiar\nwith these classes from its training data. In contrast,\nthe FGVC Aircraft dataset exhibits a larger diver-\ngence between embeddings. The model is less famil-\niar with aircraft class words (e.g., \u201cA310,\u201d \u201cA318\u201d),\nso additional descriptive context significantly influ-\nences the embeddings. These findings imply that\nstatic prompts may be inadequate for datasets with"}, {"title": "A.2 Examples of initial prompting for\nthe text encoder", "content": "CUBirds\nBaseline:\n\"A photo of a Blackfooted Albatross,\na type of bird.\""}]}