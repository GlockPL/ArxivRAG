{"title": "Law of the Weakest Link: Cross Capabilities of Large Language Models", "authors": ["Ming Zhong", "Aston Zhang", "Xuewei Wang", "Rui Hou", "Wenhan Xiong", "Chenguang Zhu", "Zhengxing Chen", "Liang Tan", "Chloe Bi", "Mike Lewis", "Sravya Popuri", "Sharan Narang", "Melanie Kambadur", "Dhruv Mahajan", "Sergey Edunov", "Jiawei Han", "Laurens van der Maaten"], "abstract": "The development and evaluation of Large Language Models (LLMs) have largely focused on individual\ncapabilities. However, this overlooks the intersection of multiple abilities across different types of\nexpertise that are often required for real-world tasks, which we term cross capabilities. To systematically\nexplore this concept, we first define seven core individual capabilities and then pair them to form seven\ncommon cross capabilities, each supported by a manually constructed taxonomy. Building on these\ndefinitions, we introduce CROSSEVAL, a benchmark comprising 1,400 human-annotated prompts, with\n100 prompts for each individual and cross capability. To ensure reliable evaluation, we involve expert\nannotators to assess 4,200 model responses, gathering 8,400 human ratings with detailed explanations\nto serve as reference examples. Our findings reveal that, in both static evaluations and attempts to\nenhance specific abilities, current LLMs consistently exhibit the \"Law of the Weakest Link,\" where\ncross-capability performance is significantly constrained by the weakest component. Specifically,\nacross 58 cross-capability scores from 17 models, 38 scores are lower than all individual capabilities,\nwhile 20 fall between strong and weak, but closer to the weaker ability. These results highlight the\nunder-performance of LLMs in cross-capability tasks, making the identification and improvement of\nthe weakest capabilities a critical priority for future research to optimize performance in complex,\nmulti-dimensional scenarios.", "sections": [{"title": "1 Introduction", "content": "The development and evaluation of Large Language Models (LLMs) (OpenAI, 2023, 2024; Anthropic, 2024;\nReid et al., 2024) have predominantly centered on individual capabilities. Developers commonly construct\nspecialized datasets tailored to distinct abilities, and then train models by blending these data sources. For\ninstance, Llama 3's post-training incorporates a mix of data, from general English to code and multilingual\ncontent, among others, each subset aimed at honing a specific skill (Llama Team, 2024). Evaluation methods\nfollow a similar pattern, with benchmarks typically assessing these abilities in isolation, offering a snapshot of\nhow well a model can reason (Clark et al., 2018; Cobbe et al., 2021; Hendrycks et al., 2021b), code (Chen\net al., 2021; Austin et al., 2021), or manage factual knowledge (Hendrycks et al., 2021a).\nHowever, can all real-world tasks be adequately categorized under just one capability, or do they frequently\ndemand the seamless integration of multiple skills, thereby challenging the prevalent approach to evaluating\nthese advanced LLMs? Consider a user prompt asking, \"Which direction has the total rainfall in Tokyo, Japan\nbeen trending over the past 10 years? Explain it step by step.\" Such a task requires the integration of tool\nuse (web browsing) with analytical reasoning. Similarly, when a developer provides HTML and JavaScript for\nan API-driven application and asks, \"Give me a basic understanding of what this web app does,\" the model\nmust combine long-context comprehension with coding expertise.\nWe define these scenarios as cross capabilities the intersection of multiple distinct capabilities across different\ntypes of expertise necessary to address complex, real-world tasks. This discrepancy between the isolated focus"}, {"title": "2 Defining Individual & Cross Capabilities in LLMs", "content": "Real-world interactions with LLMs encompass tasks that may require either an individual capability or the\nsimultaneous engagement of distinct skills. To effectively evaluate LLMs, defining and differentiating these\ncapabilities is crucial. In this section, we identify seven individual and seven cross capabilities that reflect a\nbroad spectrum of user queries and systematically organize them into taxonomies.\nAs illustrated in Figure 1, these taxonomies follow a hierarchical design: the root node represents either an\nindividual or cross capability, with the next two layers (Level-1 and Level-2 categories) breaking these down\ninto increasingly specific tasks. This framework clearly distinguishes between tasks that rely on an individual\ncapability and those that demand the integration of multiple abilities, allowing for a comprehensive evaluation\nof LLMs across various scenarios. Next, we outline the specific capabilities selected and explain the details."}, {"title": "2.1 Individual Capabilities", "content": "We begin by selecting seven core individual capabilities of LLMs: English, Reasoning, Coding, Image\nRecognition, Tool Use, Long Context, and one representative of multilingual capabilities, Spanish. Each of\nthese capabilities is further broken down into Level-1 categories, as outlined below:\n\u2022 English and Multilingual: Factual Questions (5), Procedural Questions (8), Language Assistance (1),\nWriting & Content Creation (9), Dialogue (6), Recommendations / Brainstorming (4), Personal Growth\n& Development (8), and Social interaction & communication (4).\n\u2022 Reasoning: Mathematical Calculation (7), Mathematical Reasoning (4), Commonsense Reasoning (3),\nLogic / Problem Solving (3), Social and Emotional Reasoning (6), Moral & Ethical Reasoning (3),\nScientific Reasoning (4), and Legal Reasoning (6).\n\u2022 Coding: Code Generation / Synthesis (7), Code Documentation (5), Code Debugging (2), and Code\nReview & Best Practices (4).\n\u2022 Image Recognition: Object Recognition (3), Scene Understanding (4), Image Captioning (2), Attribute\n& Relationship Identification (3), Dialogue (2), and Graceful Refusals (3).\n\u2022 Tool Use: Factual Questions about Recent and Current Things (5), Very Accurate Questions (Beyond\nExpected Model Knowledge) (5), Procedural Questions about Recent, Current, or Local Things (7),\nRecommendations / Brainstorming about Local and Current Things (4), Tasks with File Uploads (2).\n\u2022 Long Context: Factoid or Complex Question Answering (6), Summarization (6), and Multi-Document\nUnderstanding (Q&A) (2).\nTo explain, the number in parentheses above indicates the number of Level 2 subcategories within each Level-1\ncategory. For instance, \u201cScientific Reasoning (4)", "Hypothesis Formation and Testing\\\",\n\\\"Causal Reasoning\\\", \\\"Scientific Evidence Evaluation\\\", and \\\"Model-Based Reasoning\\\". We select these seven\ncapabilities because they represent core LLM skills across diverse domains, including multimodal, multilingual,\nand tool-use tasks, ensuring broad coverage of mainstream real-world use cases. Appendix A.1 provides the\nfull taxonomy of all the individual capabilities.\"\n    },\n    {\n      \"title\": \"2.2 Cross Capabilities\",\n      \"content\": \"We explore cross-capability scenarios involving the combination of two capabilities. To achieve this, we pair\nthe individual capabilities described earlier and select seven common combinations: Coding & Reasoning,\nImage Recognition & Reasoning, Tool Use & Coding, Tool Use & Reasoning, Long Context & Coding, Spanish\n& Reasoning, and Spanish & Image Recognition. Below is the Level-1 taxonomy:\"\n    },\n    {\n      \"title\": \"3 CrossEval Benchmark Construction\",\n      \"content\": \"In this section, we describe the process of manually annotating the prompt set and multiple reference responses\nto build CROSSEVAL benchmark. We then explain how we select and configure the LLM to serve as the\nevaluator for this benchmark.\"\n    },\n    {\n      \"title\": \"3.1 Prompt Set Annotation\",\n      \"content\": \"The prompt set forms the foundation of any benchmark in the era of LLMs, playing a crucial role in accurately\nevaluating model performance. Previous research has shown that real-world user prompts can include a large\nnumber of low-quality inputs, making it difficult to differentiate between advanced models (Li et al., 2024).\nAdditionally, constructing prompts with a high level of difficulty is inherently challenging (Padlewski et al.,\n2024). To address these concerns, we adopt a comprehensive annotation process designed to ensure both\nquality and appropriate difficulty levels.\nAnnotation Procedure. In this paper, we restrict the prompt set to single-turn and open-ended settings. The\nannotation process begins with annotators selecting a leaf node from our established taxonomy to determine\nthe category and task associated with each prompt. This ensures that every prompt aligns with a specific\ncapability. Furthermore, for each capability, we define clear criteria for three difficulty levels: easy, medium,\nand hard, to standardize the assessment of task complexity. For example, difficulties of prompts related to\nthe English capability are defined as follows:\n\u2022 Easy: Prompt is a single ask/requirement/constraint for the model presented as a single statement\nOR prompt is a single statement without ask/requirement/constraints AND would not require subject\nmatter expertise to understand.\n\u2022 Medium: Prompt includes 2-4 asks/requirements/constraints for the model AND would not require\nsubject matter expertise to produce a response.\n\u2022 Hard: Prompt contains 5 or more asks/requirements/constraints for the model OR requires subject\nmatter expertise above and beyond \\\"common knowledge\\\" in order to respond.\nFor Spanish as an individual capability, all prompts are annotated from scratch, with no overlap with the\nEnglish prompt set. In cross-capability scenarios involving Spanish, the corresponding prompt sets are derived\nby translating the associated English-based prompts. For instance, the Spanish & Reasoning prompt set is\ncreated by translating the Reasoning prompts from English into Spanish.\nTo maintain consistency and high quality, we begin with a pilot annotation phase where the authors act as\nreviewers, providing feedback to identify any issues with the initial annotations and refine the annotation\"\n    },\n    {\n      \"title\": \"3.2 Multiple References with Human Annotations\",\n      \"content\": \"While providing a gold reference for each instance has been the standard approach before the rise of LLMs, it\nis not feasible for our challenging prompt set for three main reasons:\n1. Many open-ended queries do not have a single correct answer, and offering only one response as the\nreference risks introducing bias in the evaluation.\n2. Several prompts, particularly those requiring domain expertise in areas such as coding or mathematics,\nremain challenging even for college-level expert annotators.\n3. For prompts related to tool use, the correct response can be dynamic. For example, the answer to \\\"What is\nthe temperature in the Bay Area today?\\\" changes daily.\nGiven these challenges, we propose using multiple model responses, scored and explained by human annotators,\nto serve as references for evaluation.\nAnnotator Qualifications. For all annotations in this paper, we use the same data vendor as Llama 3's human\nevaluation, employing professional experts with domain-specific knowledge, such as reasoning, coding, and\nSpanish. To avoid contamination, the Llama team does not have access to CROSSEVAL prompts during Llama\n3's development. The data vendor selects the appropriate annotator pool based on the capabilities being\nevaluated. While creating a definitive gold reference is impractical, our annotators are capable of assessing\nthe correctness of model responses and providing well-justified ratings.\nModel Response Collection. For each prompt, we aim to gather three distinct model responses representing\nvarying levels of quality: low, medium, and high. These responses are randomly drawn from various models\nwithin the Llama and GPT model families, including Llama 3.1 8B/70B/405B and different versions of GPT-4.\nAdditionally, for capabilities involving Reasoning, Image Recognition, and Tool Use, we manually annotate\none response if all three collected responses contain noticeable errors.\nAnnotating Human Ratings with Explanations. For each model response, we engage two independent annotators\nto rate it on a 1-5 Likert scale, accompanied by a paragraph explaining their rating. Multiple reference\"\n    },\n    {\n      \"title\": \"3.3 Building LLM-based Evaluators\",\n      \"content\": \"In addition to benchmarking the capabilities of LLMS, CROSSEVAL represents, to the best of our knowledge,\nthe largest meta-evaluation benchmark currently available for measuring the correlation between LLM-based\nscoring and human judgments. Since each prompt includes three reference model responses and six human\nratings, we are able to explore how to develop the most effective in-domain LLM evaluator for this benchmark.\"\n    },\n    {\n      \"title\": \"3.3.1 Prompting LLMs for Evaluation\",\n      \"content\": \"While the LLM-as-a-Judge paradigm has gained popularity (Zheng et al., 2023), there is no standardized\nmethod for designing prompts or for guiding LLMs to output evaluation scores. Common practices include\ngenerating an answer first, setting evaluation rules manually, and then instructing the model to assign a score\nto the response being evaluated (Zeng et al., 2024).\nIn practice, we find that self-generated answers frequently lead to issues. For instance, response length can\nexceed model limits, preventing the model from generating a score. This approach also causes the LLMs to\noverly rely on their own generated answers, overlooking valuable insights from human-annotated references.\nTo address these issues, we propose the following prompting strategy:\nGeneral Rubrics. We first provide the following rubrics for the 1-5 Likert scale in the system prompt:\n\u2022 5/5 - Amazing: The response is flawless and could hardly be improved.\n\u2022 4/5 - Pretty Good: The response is quite good, but has room for minor improvements.\n\u2022 3/5-Okay: They are middle-of-the-road responses that could be improved in several ways.\n\u2022 2/5 - Pretty Bad: The response has major problems in helpfulness, truthfulness, or safety.\n\u2022 1/5 - Horrible: They are terrible responses and you would caution others against using models that\ngenerate responses like this.\nMulti-References-based Prompting. Next, we provide any attachments relevant to the prompt (e.g., a document\nfor Long Context or an image for Image Recognition), followed by the user prompt. For meta-evaluation,\nwhere we assess the performance of LLM-as-a-Judge, we can include up to two reference responses along with\ntheir scores and explanations. For example, when the LLM judges a medium-quality response, we can provide\nlow-quality and high-quality responses with their four ratings as context. For evaluating new model responses,\nall three model responses are included, with human annotations serving as the reference.\"\n    },\n    {\n      \"title\": \"4 Exploring Relationship between Individual and Cross Capabilities\",\n      \"content\": \"In this section, we explore the relationship between individual and cross capabilities in LLMs. We first present\nthe experimental setup, followed by a detailed discussion of the findings based on the results from CROSSEVAL.\"\n    },\n    {\n      \"title\": \"4.1 Experimental Setup\",\n      \"content\": \"To ensure comprehensive coverage of LLM performance across capabilities, we select 17 models from five major\nmodel families: GPT (OpenAI, 2023), Claude (Anthropic, 2024), Gemini (Reid et al., 2024), Llama (Llama\nTeam, 2024), and Reka (Ormazabal et al., 2024). Each model supports at least five cross-capability scenarios\nin our experiments (except ol models). For consistency, we use the GPT-40-05-13 model as the evaluator,\nwith temperature set to 0 and seed set to 42 to ensure deterministic scoring. Each model's responses are\"\n    },\n    {\n      \"title\": \"4.2 Findings on the CrossEval Benchmark\",\n      \"content\": \"To better present the results, we linearly map the average scores for each capability from a 1-5 scale to a\n1-100 scale. The full results are provided in Table 3. Since LLMs tend to prefer self-generated answers (Zheng\net al., 2023), we exclude GPT's results from the comparative analysis and treat them as a reference point.\nOur experiments reveal several key findings:\nCrossEval effectively differentiates advanced models. The CROSSEVAL benchmark successfully distinguishes\nbetween state-of-the-art LLMs. For instance, the four Claude model variants achieve progressively higher\nscores in reasoning: 56.81, 62.88, 66.22, and 71.54. This mirrors the increasing capabilities associated with\"\n    },\n    {\n      \"title\": \"5 How Individual-Capability Alterations Impact Cross-Capability Performance?\",\n      \"content\": \"Beyond evaluating the relationship between individual and cross capabilities of LLMS on CROSSEVAL, we\nexplore the crucial follow-up questions: when we adjust the performance of specific capabilities, how does this\naffect cross-capability performance? For reference, Amdahl's Law (Amdahl, 1967), originating from parallel\ncomputing, states that the overall performance improvement gained by optimizing a single part of a system\nis limited by the fraction of time that the improved part is used. To explore this in LLMs, we propose a\nprompting method designed to modulate specific capabilities of LLMs. Following this, we present case studies\ninvolving two LLMs in three cross-capability tasks to illustrate the effects of these alterations.\"\n    },\n    {\n      \"title\": \"5.1 Principle-based System Prompting\",\n      \"content\": \"To reliably explore the impact of altering single capabilities, we aim to enhance a specific capability without\nsignificantly affecting others. This allows for more controlled and precise investigation into cross-capability\nperformance dynamics. Our solution is a principle-based method that iteratively refines the system prompt to\nenhance specific capabilities of LLMs. It builds on the CROSSEVAL dataset and evaluations to selectively\nboost individual capabilities. The approach involves the following steps:\n1) Initial Setup: For each instance, we input the user prompt, the target model's response, the evaluation\nfeedback from our LLM-based system, and an evolving principle list (initially empty).\n2) Iterative Refinement: Using GPT-40, we iteratively generate principles that guide the model's perfor-\nmance in a particular capability. The model selects one of four operations for each instance:\n- ADD: Introduce a new principle that isn't currently listed.\n- REPLACE: Substitute a less significant principle with a new one.\n- REVISE: Refine existing principles for greater clarity and precision.\n- KEEP: Leave the principles unchanged if no adjustments are necessary.\n3) Final Principle List: After 100 iterations, this process yields a principle-based system prompt tailored to\nenhance the target capability.\nBy incorporating this system prompt into the LLMs, we instruct them to prioritize key aspects of performance,\nsuch as format adherence, problem-solving strategies, or error avoidance, for prompts related to particular\ncapabilities. The complete principle-based prompts used in our experiments can be found in the Appendix.\"\n    },\n    {\n      \"title\": \"5.2 Case Study for Investigation\",\n      \"content\": \"To analyze how individual-capability alterations affect cross-capability performance, we select three cross-\ncapability tasks with the most significant individual performance gaps: Image Recognition & Reasoning,\nSpanish & Reasoning, and Spanish & Image Recognition. Additionally, we focus on two models, Claude 3 Haiku\nand Gemini 1.5 Flash, which display the largest performance discrepancies in these scenarios. The rationale\nbehind choosing these combinations is that a more pronounced gap between strong and weak capabilities\nprovides clearer insights into the effects of selective capability enhancement on collective performance. Table 4\npresents the complete experimental results, and we make the following key observations:\"\n    },\n    {\n      \"title\": \"6 Related Work\",\n      \"content\": \"\"\n    },\n    {\n      \"title\": \"6.1 Evaluation of LLMs\",\n      \"content\": \"The advancements in LLMs have shifted the focus of evaluation from specific NLP tasks (Wang et al., 2019b,a)\nto specific capabilities such as reasoning (Clark et al., 2018; Hendrycks et al., 2021a,b; Rein et al., 2023),\ncoding (Chen et al., 2021; Austin et al., 2021; Cassano et al., 2023; Liu et al., 2023a), multilinguality (Shi\"\n    },\n    {\n      \"title\": \"6.2 Evaluation Metrics for Open-Ended Generation\",\n      \"content\": \"Evaluation metrics have evolved alongside advances in model generation capabilities, moving from traditional\nn-gram-based measures (Papineni et al., 2002; Lin, 2004) to pre-trained language model (PLM)-based\nevaluators (Zhang et al., 2020; Sellam et al., 2020; Yuan et al., 2021; Zhong et al., 2022) and, more recently, to\nLLM-as-a-Judge frameworks (Liu et al., 2023b; Zheng et al., 2023). Given the large set of complex, open-ended\nprompts in our benchmark, we employ LLMs as evaluators to assess model outputs. Unlike previous methods\nthat rely on self-generated prompts, we adopt a point deduction-based prompting technique. Each instance is\nsupported by three expert-annotated reference examples to enhance the reliability of the evaluation process.\nFurthermore, CROSSEVAL is the largest meta-evaluation benchmark currently available for measuring the\ncorrelation between LLM-as-a-Judge assessments and human judgments, while also providing detailed insights\ninto the specific capabilities that different LLMs excel at evaluating.\"\n    },\n    {\n      \"title\": \"7 Conclusion\",\n      \"content\": \"We systematically investigated cross capabilities of LLMs. We first introduced CROSSEVAL, a comprehensive\ntestbed designed to evaluate both individual and cross capabilities. On top of that, we developed an LLM-based\njudge with a substantial level of agreement with human annotations. Through extensive experiments, we\ndemonstrated that LLMs consistently conform to the \\\"Law of the Weakest Link,\\\" where cross-capability\nperformance is constrained by the weakest ability. This phenomenon also persists after alteration to individual\ncapabilities. Our benchmark and analysis offer a fresh perspective on LLM development, emphasizing the\nneed for intensified research to improve cross-capability effectiveness in LLMs.\"\n    },\n    {\n      \"title\": \"A Complete Taxonomy\",\n      \"content\": \"To ensure the comprehensiveness of the prompt sets in our evaluations, we build taxonomy with Level-1 (L1)\nand Level-2 (L2) categories. More concretely, Tables 5-10 and Tables 11 13, present the taxonomy for\nindividual capabilities (English and Multilingual, Reasoning, Coding, Image Recognition, Tool Use, and Long\nContext) and cross capabilities (Coding & Reasoning, Image Recognition & Reasoning, Tool Use & Coding,\nTool Use & Reasoning, Long Context & Coding), respectively.\"\n    },\n    {\n      \"title\": \"A.1 Taxonomy of Individual Capabilities\",\n      \"content\": \"\"\n    },\n    {\n      \"title\": \"A.2 Taxonomy of Cross Capabilities\",\n      \"content\": \"\"\n    },\n    {\n      \"title\": \"B CrossEval Benchmark\",\n      \"content\": \"\"\n    },\n    {\n      \"title\": \"B.1 Prompt Set Examples\",\n      \"content\": \"To provide an intuitive sense of the types and difficulty of the prompt set in our benchmark CROSSEVAL, we\npresent examples for each capability, including the difficulty level, L1 and L2 categories, and the prompts.\nTables 16-22 correspond to individual capabilities, while Tables 23-27 pertain to cross capabilities.\"\n    },\n    {\n      \"title\": \"B.2 Reference Examples\",\n      \"content\": \"To offer a more intuitive understanding of the reference examples in our benchmark, we randomly select an\ninstance from both English and Reasoning capabilities and present their reference examples in Tables 28 - 33.\"\n    },\n    {\n      \"title\": \"B.3 Guidelines for Difficulty Levels\",\n      \"content\": \"To illustrate the difficulty of prompts in our evaluations, below is a summary of the definitions, accompanied by\nexamples, for easy, medium, and hard levels for the English (multilingual) and Image Recognition capabilities.\nNote that the following examples are for illustrations only, and they are not from our benchmark.\"\n    },\n    {\n      \"title\": \"B.3.1 English and Multilingual\",\n      \"content\": \"\"\n    },\n    {\n      \"title\": \"B.3.2 Image Recognition\",\n      \"content\": \"\"\n    },\n    {\n      \"title\": \"B.4 Prompts for Evaluation\",\n      \"content\": \"We provide the complete version of the system and evaluation prompts we adopt for LLM-as-a-Judge in\nTables 34 and 35, respectively.\nYou are an expert AI evaluator tasked with assessing model responses. Rate the response using a 1-5\nLikert scale according to the following rubrics:\n### Rubrics:\n5/5 - Amazing: The response is flawless and could hardly be improved.\n4/5 - Pretty Good: The response is quite good, but has room for minor improvements.\n3/5-Okay: They are middle-of-the-road responses that could be improved in several ways.\n2/5 - Pretty Bad: The response has major problems in helpfulness, truthfulness, or safety.\n1/5 - Horrible: They are terrible responses and you would caution others against using models\nthat generate responses like this.\nNote: User prompts or model responses may include attachments. To ensure a thorough evaluation,\nyou may need to write and execute code.\"\n    },\n    {\n      \"title\": \"B.5 Case Study for LLM-as-a-Judge on CrossEval\",\n      \"content\": \"We randomly select one instance from the CROSSEVAL benchmark to demonstrate the format and reliability\nof LLM-as-a-Judge in Tables 36 and 37, using GPT-40 and Claude 3.5 Sonnet as evaluators, respectively.\"\n    },\n    {\n      \"title\": \"C Exploring Relationships between Individual & Cross Capabilities\",\n      \"content\": \"\"\n    },\n    {\n      \"title\": \"C.1 Model Versions Used in Our Experiments\",\n      \"content\": \"Since LLMs are frequently updated with new versions, we specify the exact versions corresponding to the\nLLM names used in the experiments for reference in Table 38.\"\n    },\n    {\n      \"title\": \"C.2 Results for Claude-as-a-Judge\",\n      \"content\": \"To avoid potential bias from using a single evaluator, we present all results with Claude 3.5 Sonnet as the\nevaluator in Table 39. Notably, for the Coding & Reasoning task, the performance of five models falls between\nthe weak and strong capabilities but tends to be closer to the strong one, as highlighted in purple in the Table.\nThis may be due to the fact that Coding and Reasoning are key capabilities in current LLM development,\nwith potentially many cross-capability prompts included in the training data, boosting LLM performance for\nthis specific task. While this pattern does not appear in the GPT-as-a-Judge results, Claude-as-a-Judge still\ngenerally demonstrates a clear \\\"Law of the Weakest Link\\\" effect, as illustrated by the distribution in Figure 3.\"\n    },\n    {\n      \"title\": \"C.3 Discussion on Distinguishing \u201cWeak": "nd \u201cStrong\u201d Capabilities"}, {"content": "In the experiments presented in the main text, we identify \u201cstrong\u201d and \u201cweak\u201d capabilities within cross-\ncapability tasks when the absolute difference between their individual scores exceeds $\\Delta$ = 3. To illustrate the\neffect of $\\Delta$ on \"Law of the Weakest Link,\" we adjust its value from 1 to 6 and plot the density distribution\nusing GPT-40 and Claude 3.5 Sonnet as evaluators, as shown in Figures 4 and 5. Notably, regardless of the\nchosen $\\Delta$ value, cross-capability performance consistently clusters around the weaker performance, clearly\ndemonstrating the \"Law of the Weakest Link.\""}, {"title": "C.4 Results for Different Difficulty Levels", "content": "In Table 40, we present the scores of 17 models across prompt sets with varying levels of difficulty. It's\nimportant to note that these scores are not directly comparable across different model families, as they support\nvarying capabilities. For instance, while Llama does not support Image Recognition, it covers all capabilities\nrelated to Tool Use.\nNevertheless, as shown in Table 40, 12 out of the 17 models perform better on the Easy prompt set compared\nto the Medium set, and similarly, they score higher on the Medium set than the Hard set. This pattern\nsuggests that the difficulty levels we manually defined align well with model performance. An exception to\nthis trend is the Claude model family, where all four Claude models scored slightly higher on the Hard prompt\nset than on the Medium set."}, {"title": "D How Individual-Capability Alterations Impact Cross-Capability Performance", "content": ""}, {"title": "D.1 Prompt to Generate Principle", "content": "The complete prompt used for automatically generating principles is provided in Table 41.\nYou are an AI expert tasked with analyzing common mistakes in model responses and creating a comprehensive set of principles\nto improve the {capability} of the model. We will work step-by-step to build this guideline. Specifically, for each iteration, I will\nprovide you with one instance, and you need to update the current principles accordingly. There are 100 instances in total, and\nthe principles should be completed after reviewing all instances.\nFor each instance, you have the following information:\n- User Prompt\n- Model Response\n- Evaluation of the Model Response\n- Current Principles\n### Instance {index}\n{current instance, including the user prompt, model response, and evaluation using an LLM-as-a-judge}\nFor each iteration, choose ONE of the following actions:\n1. ADD\nIntroduce a new principle that isn't currently listed.\n2. REPLACE\nReplace a less significant principle with a new one.\nClearly specify which principle is being replaced.\n3. REVISE\nEnhance the principles by making them more detailed and specific.\n4. KEEP\nIf the current instance is already covered by existing principles, leave the guideline unchanged.\nCurrent Principles:\n{current principles}:\nOutput Format:\n## Summary\n- Summarize any major issues with the present response.\n- Provide specific, actionable steps to prevent these errors, if any.\n- Based on your summary and the current principles, decide which action (ADD, REPLACE, REVISE, or KEEP) should be\ntaken for the current instance.\n## Principles for Prompts related to {capability}\n### Principle 1: Title [Use the title to specify the context in which this principle should be applied, such as \"For Legal Reasoning\"\nor \"For Mathematical Reasoning\"]\n- Include up to three key points.\n- Each point should be directly applicable to the model's generation process without requiring additional training or resources.\n- Each point must be extremely specific to allow for direct execution.\nFor example, instead of saying \"Use a structured markdown format,\" clearly define the exact format for each step, including\nthe structure for the beginning, middle, and end.\nInstead of advising to \"avoid vague terms,\" provide a specific list of terms to be avoided.\nRather than generally suggesting \"avoid errors in math calculations\" or \"double-check,\" outline concrete steps to prevent\nsuch errors.\n[END of Principles]\nRequirements:\n- Follow the output format exactly, including \"[END of Principles]\" at the end with no remarks after it.\n- Include up to 10 distinct principles in the report. If there are already 10 principles, \"ADD\" is not allowed.\n- You may reorder the principles as necessary: Place important, typical, and representative principles at the front, while less\nimportant ones can be moved toward the back.\n- Ensure that each suggestion in the principles is detailed and actionable, rather than being a general description."}, {"title": "D.2 Case Study for Principle-based System Prompts", "content": "Using Gemini 1.5 Flash as an example, we present the automatically generated system prompts for the\nReasoning capability in Tables 42-44. The \"Note\" in Table 44 is added manually."}]}