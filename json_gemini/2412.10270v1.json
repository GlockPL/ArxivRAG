{"title": "Cultural Evolution of Cooperation among LLM Agents", "authors": ["Aron Vallinder", "Edward Hughes"], "abstract": "Large language models (LLMs) provide a compelling foundation for building generally-capable AI agents. These agents may soon be deployed at scale in the real world, representing the interests of individual humans (e.g., AI assistants) or groups of humans (e.g., AI-accelerated corporations). At present, relatively little is known about the dynamics of multiple LLM agents interacting over many generations of iterative deployment. In this paper, we examine whether a \"society\" of LLM agents can learn mutually beneficial social norms in the face of incentives to defect, a distinctive feature of human sociality that is arguably crucial to the success of civilization. In particular, we study the evolution of indirect reciprocity across generations of LLM agents playing a classic iterated Donor Game in which agents can observe the recent behavior of their peers. We find that the evolution of cooperation differs markedly across base models, with societies of Claude 3.5 Sonnet agents achieving significantly higher average scores than Gemini 1.5 Flash, which, in turn, outperforms GPT-40. Further, Claude 3.5 Sonnet can make use of an additional mechanism for costly punishment to achieve yet higher scores, while Gemini 1.5 Flash and GPT-40 fail to do so. For each model class, we also observe variation in emergent behavior across random seeds, suggesting an understudied sensitive dependence on initial conditions. We suggest that our evaluation regime could inspire an inexpensive and informative new class of LLM benchmarks, focussed on the implications of LLM agent deployment for the cooperative infrastructure of society.", "sections": [{"title": "1. Introduction", "content": "LLMs are increasingly able to match or exceed hu-\nman performance across a wide range of language\ntasks. Models with improved reasoning and tool-\nuse capabilities (OpenAI, 2024) may naturally\nform a basis for general-purpose agent-based ap-\nplications. In the near future, we expect there to\nbe many LLM agents interacting autonomously\nto accomplish tasks on behalf of various individu-\nals and organizations. These interactions could\ntake many forms, including competition, cooper-\nation, negotiation, coordination, and information\nsharing. Certainly these interactions will intro-\nduce new social dynamics, yielding emergent out-\ncomes for society that are hard to predict from\npurely theoretical considerations (Gabriel et al.,\n2024). However, current LLM safety evaluations\nare rooted mainly in single-turn interactions be-\ntween one model and one human. For instance,\nnone of LMSys Chatbot Arena (Chiang et al.,\n2024), METR (METR, 2024), or AISI (AISI, 2024)\nconsider multi-agent interactions over time.\nA particularly important class of multi-agent\ninteractions are cooperative interactions. We say\nthat agents cooperate when they take actions that\nlead to mutual benefit, even in the face of op-\nportunities for individual gain at the expense of\nothers (Dafoe et al., 2020). Arguably the human\nspecies' ability to cooperate reliably at scale with\nstrangers is the secret of our success (Henrich,\n2016), and underpins the stability of human soci-\neties. Just as with humans, cooperation between\nLLM agents will often be in the interests of so-\nciety.\u00b9 Consider, for example, LLM agents that\nmake high-level decisions about travel speed and\nroute selection for autonomous vehicles. Coop-\neration between such agents can reduce conges-\ntion and pollution which increasing safety and\nefficiency for a wide range of road users. Myr-"}, {"title": "2. Background", "content": null}, {"title": "2.1. The Donor Game", "content": "Indirect reciprocity is a mechanism for cooper-\nation in which an individual helps someone be-\ncause doing so increases the likelihood that some-\none else will help them in the future.\u00b2 Unlike\ndirect reciprocity, which relies on repeated inter-\nactions between the same individuals, indirect\nreciprocity relies on reputation to foster cooper-\nation among individuals who may not interact\nagain. Reputation requires that actions are ob-\nservable and that information about individuals'\nactions can be accurately transmitted. Indirect\nreciprocity has been proposed as an important\nmechanism in the evolution of large-scale human"}, {"title": "2.2. Cultural Evolution", "content": "In humans, norms of indirect reciprocity arose\nin part as a result of cultural evolution. Culture\nin the relevant sense means any socially trans-\nmitted information capable of affecting behavior"}, {"title": "3. Methods", "content": "LLM agents play the following variant of the\nDonor Game, as described in the system prompt.\nThe game lasts for 12 rounds. Before it begins,\nagents are prompted to create a strategy which\nthey will then use to make donation decisions.\nWhen the game finishes, the top-performing 50%\nof agents (in terms of final resources) survive to\nthe next generation.\u00b3 Anthropomorphising, one"}, {"title": "4. Results", "content": null}, {"title": "4.1. Donor Game", "content": "We used this setup to study the cultural evolution\nof indirect reciprocity in three models: Claude 3.5\nSonnet, Gemini 1.5 Flash, and GPT-40. All results\nare based on a population size of 12 agents in\neach generation. Within each run, all agents use\nthe same brand of LLM. With these settings, one\nrun costs $10.21 for Claude 3.5 Sonnet, $6.90\nfor GPT-40, and $0.09 for Gemini 1.5 Flash. Our\nresults comprise five runs for each LLM.\nTo assess the level of cooperation, a natural\nmetric is average resources after the final round."}, {"title": "4.3. Ablations", "content": "Our experimental setup relied on various hyper-\nparameters, to which LLM agents may or may\nnot be sensitive. Of particular importance are the\ndonation multiplier, controlling the magnitude\nof gains from cooperation, and the length of the\n\"trace\" which agents receive about the past behav-\nior of others in the population, information that\ncan be used to implicitly derive reputation. We\nablate both of these, with figures available in the\nSupplementary Material. Donation multipliers of\n1.5x and 3x (instead of 2x) do not change qual-\nitative outcomes: Claude 3.5 Sonnet still shows\nan increase in cooperation across generations,\nGemini 1.5 Flash shows little change, and GPT-40\nshows a decrease. When the length of the trace\nis shortened to 1 rather than 3, the emergence of\ncooperation is less pronounced for Claude 3.5 and\ndisappears completely for Gemini 1.5 Flash. This\nsuggests that the success of Claude and Gemini\nstrategies depends on having some second-order\ninformation about how recipients of recipients\nhave treated others in the past, either because this\nexplicitly allows more complex norms or because\nit reveals more information about the background\npopulation on which to anchor decision-making."}, {"title": "5. Discussion", "content": "In this paper we have set out a method for assess-\ning the cultural evolution of cooperation among\nLLM agents. We focus on the well-known Donor\nGame, a \"Petri dish\" in which to study the emer-\ngence of indirect reciprocity. Over the course\nof 10 generations we find striking differences in\nthe emergence of cooperation depending on the\nbase model for the LLM agent. Claude 3.5 Sonnet\nreliably generates cooperative communities, es-\npecially when provided with an additional costly\npunishment mechanism. Meanwhile, generations\nof GPT-40 agents converge to mutual defection,\nwhile Gemini 1.5 Flash achieves only weak in-\ncreases in cooperation. We analyse the cultural\nevolutionary dynamics, revealing that some pop-\nulations have the ability to accumulate increas-\ningly complex strategies at the individual level,\nand to generate norms that select for cooperators\nat the group level. Our results motivate building\ninexpensive benchmarks which test for long-term\nemergent behavior of multi-agent systems of LLM\nagents, towards safe and beneficial deployment\nof such systems at scale in the real-world.\nIn establishing a new setting for empirical ex-\nperimentation, we have necessarily adopted a\nnarrow scope. Therefore, our work has several\nclear limitations. Most obviously, the strict bound-\naries between generations in our cultural evolu-\ntionary system are idealized and do not repre-\nsent the full complexity of model release and\nadoption in the real world. Moreover, we only\nstudy homogeneous populations of LLM agents,\nall with the same base model; in actuality, hetero-\ngeneous populations of LLM agents are far more\nlikely to occur. Our experiments are restricted to\nthe Donor Game, and models may behave quite\ndifferently when faced with other social dilem-\nmas, especially since individual games may well\nbe over-represented in the training data for one\nmodel and under-represented in the training data\nfor another. Relatedly, we have not performed an\nextensive search over prompting strategies, which\nmay affect the cooperation behavior of different\nmodels in different ways. Notwithstanding these\nlimitations, our experiments do serve to falsify\nthe claim that LLMs are universally capable of\nevolving human-like cooperative behavior.\nThe limitations we have identified immediately\nsuggest interesting extensions for future work.\nIndeed, the space of cultural evolutionary stud-\nies of LLM agents is ripe for further study us-\ning our methods. What happens if communica-\ntion is permitted between agents, either at the\nstart of each generation (deliberation about strate-"}]}