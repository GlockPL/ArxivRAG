{"title": "NEMESIS: NORMALIZING THE SOFT-PROMPT\nVECTORS OF VISION-LANGUAGE MODELS", "authors": ["Shuai Fu", "Xiequn Wang", "Qiushi Huang", "Yu Zhang"], "abstract": "With the prevalence of large-scale pretrained vision-language models (VLMs),\nsuch as CLIP, soft-prompt tuning has become a popular method for adapting\nthese models to various downstream tasks. However, few works delve into\nthe inherent properties of learnable soft-prompt vectors, specifically the impact\nof their norms to the performance of VLMs. This motivates us to pose an\nunexplored research question: \"Do we need to normalize the soft prompts in\nVLMs?\" To fill this research gap, we first uncover a phenomenon, called the\nLow-Norm Effect by performing extensive corruption experiments, suggest-\ning that reducing the norms of certain learned prompts occasionally enhances\nthe performance of VLMs, while increasing them often degrades it. To har-\nness this effect, we propose a novel method named Normalizing the soft-prompt\nvectors of vision-language models (Nemesis) to normalize soft-prompt vectors\nin VLMs. To the best of our knowledge, our work is the first to systemati-\ncally investigate the role of norms of soft-prompt vector in VLMs, offering valu-\nable insights for future research in soft-prompt tuning. The code is available at\nhttps://github.com/ShyFoo/Nemesis.", "sections": [{"title": "1 INTRODUCTION", "content": "In the age of large-scale pretrained vision-language models (VLMs), such as CLIP (Radford et al.,\n2021), Flamingo (Alayrac et al., 2022), and BLIP (Li et al., 2022), soft-prompt-based methods,\nalso known as prompt-tuning, have emerged as a dominant approach for adapting these models to a\nwide range of downstream tasks. For instance, Zhou et al. (2022b) propose a Context Optimization\n(CoOp) method to learn soft prompts in a continuous space of CLIP for image classification tasks.\nAdditionally, Rao et al. (2022) and Du et al. (2022) also employ prompt-tuning to address dense\nprediction and open-vocabulary object detection tasks, respectively.\nRecent research in the field of VLMs has been primarily focused on enhancing model performance\nthrough the alignment of visual and textual features. For instance, in (Lu et al., 2022), the weight\ndistribution of output embeddings is estimated, while Zang et al. (2022) propose a joint optimization\napproach for prompts across multiple modalities. Additionally, Chen et al. (2023) employs optimal\ntransport techniques. To interpret learned soft-prompt vectors, Zhou et al. (2022b) and Chen et al.\n(2023) map them to the nearest words within the embedding space. More recently, Oymak et al.\n(2023) delves into the role of attention mechanisms in prompt-tuning, specifically within the context\nof a one-layer attention network.\nWhile considerable advancements have been made in soft-prompt-based techniques for VLMs, scant\nattention has been paid to their intrinsic properties, specifically the norms of learnable soft-prompt\nvectors. We argue that the norms of soft prompts are a crucial but overlooked attribute that sig-\nnificantly influences the performance of VLMs. This paper addresses an overlooked aspect and\npresents a research question \u201cDo we need to normalize the soft prompts in VLMs?\" To the best of\nour knowledge, there is no work to study this question."}, {"title": "2 Low-NORM EFFECT", "content": "In this section, we examine how the norms of learned prompt vectors influence the performance\nof VLMs and identify the Low-Norm Effect. To achieve that, we conduct extensive corruption"}, {"title": "3 METHODOLOGY", "content": "In this section, we introduce the proposed Nemesis method. We begin with a review of the CoOp\nmethod (Zhou et al., 2022b) and subsequently introduce two key corruption operations, REPLACE\nand RESCALE. Finally, we present the entire method."}, {"title": "3.1 A REVISIT OF PROMPT-TUNING VISION-LANGUAGE MODELS", "content": "Over the years, pretrained VLMs have demonstrated impressive generalization performance in zero-\nshot open-world visual recognition, wherein the model can perform a task without undergoing ex-\nplicit training. One typical paradigm is CLIP (Radford et al., 2021), which consists of an image en-\ncoder and a text encoder. CLIP is trained on approximately 400 million image-text pairs, contribut-\ning to its remarkable performance. Nevertheless, effectively fine-tuning these VLMs for downstream\ntasks remains a challenge, particularly when dealing with few-shot data, due to their massive param-\neters. The CoOp method addresses this issue by setting the templated context prompts (e.g. This\nis a photo of {class-name}.) as learnable vectors, which only requires fine-tuning these\nlearnable vectors while keeping the pretrained VLMs frozen. For a downstream visual recogni-\ntion task consisting of C categories, the classification weights of one image can be defined by the\nsimilarity between the visual feature and the text features of all categories.\nFormally, the image encoder and text encoder can be denoted by f and g, respectively. Given an\nimage x along with its classification label y, the visual feature can be formulated as f = f(x), while\nthe textual prompt of i-th class can be formulated as $t_i = \\{v_1, v_2, v_j, ..., v_L, c_i \\}$, where vj and ci\ndenote the j-th soft-prompt vector and the word embedding of the class name, respectively. Then\nthe i-th class textual feature can be denoted as $g_i = g(t_i)$. Given few-shot data, CoOp can learn the\nsoft prompts $\\mathcal{V}_{L \\times D} = \\{v_1, v_2, ..., v_L\\}$, where L and D denote the length of soft prompts and the\ndimension of prompt vectors, respectively, by minimizing the negative log-likelihood between the\nimage feature f and its ground-truth textual feature gy as"}, {"title": "3.2 CORRUPTION OPERATIONS", "content": "In this section, we introduce two corruption operations: REPLACE and RESCALE, which can be\nemployed to corrupt the learned soft-prompt vectors.\nFor the REPLACE operation, we replace learned prompt vectors at a single position with a zero-\nmean Gaussian-distributed vector with fixed variance. Then, we can obtain a set of corrupted soft"}, {"title": "3.3 THE NEMESIS METHOD", "content": "To handle the Low-Norm Effect during prompt-tuning VLMs, we propose two losses for normal-\nizing the norms of soft prompts: Position-Uniform Normalization (PUN) loss and Position-Aware\nNormalization (PAN) loss. In the experiments, they are separated as an individual regularization\nitem, which is added to the standard soft-prompt tuning process.\nGenerally, given a set of soft prompts $\\mathcal{V}_{L \\times D} = \\{v_1, v_2, ..., v_L\\}$, we can calculate their norms as"}, {"content": "For the PUN loss, all elements of the set $\\{a_1, a_2, . . ., a_L \\}$ are set to the same value, imposing an\nequal weight on the norms of soft prompts at all positions. Hence, this loss can be formulated as"}, {"title": "4 EXPERIMENTS", "content": "In this section, extensive experiments are conducted to evaluate the proposed Nemesis method,\nincluding comparison with CoOp (Zhou et al., 2022b) on few-shot image classification tasks and\ndomain generalization tasks, comparison with CoCoOp (Zhou et al., 2022a) in the base-to-new\ngeneralization setting. Additionally, we conduct an in-depth impact analysis on VLM performance\ndue to the norms of soft prompts, explore the method's extensibility to other soft-prompt tuning\napproaches, and assess the computational efficiency."}, {"title": "4.1 DATASETS", "content": "For few-shot image classification experiments and base-to-new generalization tasks, we follow the\nexperimental setting of CoOp and CoCoOp, respectively, and conduct experiments on 11 visual\nclassification datasets, including Caltech101 (Fei-Fei et al., 2004) and ImageNet (Deng et al., 2009)\nfor object recognition, EuroSAT (Helber et al., 2019) for satellite image recognition, DTD (Cimpoi\net al., 2014) for texture recognition, UCF101 (Soomro et al., 2012) for action recognition, SUN397\n(Xiao et al., 2010) for scene recognition, OxfordPets (Parkhi et al., 2012), FGVCAircraft (Maji\net al., 2013), Food101 (Bossard et al., 2014), Flowers102 (Nilsback & Zisserman, 2008), and Stan-\nfordCars (Krause et al., 2013) for fine-grained recognition. Besides, ImageNet (Deng et al., 2009)\nand its variants, including ImageNet-A (Hendrycks et al., 2021b), ImageNet-R (Hendrycks et al.,\n2021a), ImageNetV2 (Recht et al., 2019), and ImageNet-Sketch (Wang et al., 2019), are used for the\nevaluation of domain generalization. Detailed descriptions of each dataset can be found in Appendix\nA.2.1."}, {"title": "4.2 IMPLEMENTATION DETAILS", "content": "For few-shot image classification experiments and domain generalization tasks, we compare our\nmethod with the baseline method CoOp, while CoCoOp is chosen as our baseline model in base-\nto-new generalization tasks. Following the few-shot evaluation protocol used in CoOp, we use a\nfixed number of training samples from each category (i.e. 1, 2, 4, 8, 16 shots per class). Besides, we\nfollow the same training configurations as these baseline models, including training epochs, learning\nrate, and batch size, etc. All reported results are based on the average of five different seed runs.\nBold denotes the best performance on each comparison setting. More implementation details and\nhyper-parameter settings can be found in Section A.2.2."}, {"title": "4.3 FEW-SHOT IMAGE RECOGNITION RESULTS", "content": "The experimental results of few-shot recognition are summarised in Figure 2. The blue, orange,\nand green lines represent CoOp, CoOp+Nemesis with the PUN loss, and CoOp+Nemesis with the\nPAN loss, respectively. In terms of average performance, both Nemesis methods outperform CoOp.\nParticularly, they achieved a large improvement over CoOp on the ImageNet, OxfordPets, Food101,\nand SUN397 datasets. This indicates that normalizing the soft prompts in VLMs can lead to bet-\nter performance on these datasets that exhibit a more pronounced Low-Norm Effect. Taking the\nImageNet dataset as an example, Nemesis with the PUN loss gains 2.06%, 3.84%, 2.6%, 1.16%,\n0.38% performance boost over CoOp at 1, 2, 4, 8, 16 shots. Similarly, Nemesis with the PUN loss\nalso shows performance improvements of 0.46%, 1.56%, 1.74%, 0.80%, and 0.44%. Moreover, it\nis evident that CoOp+Nemesis demonstrates enhanced robustness and superior performance on the\nFood101 and OxfordPets, compared with CoOp. Additionally, comparing Nemesis with the PUN\nloss, Nemesis with the PAN loss shows more robust performance at larger shot settings. All these\nperformance comparisons demonstrate normalizing the soft prompts in VLMs can facilitate the ef-\nfective learning of soft prompts for few-shot recognition. More detailed data and analysis of training\nprocess can be found in Appendix A.2.6."}, {"title": "4.4 EVALUATION OF GENERALIZATION PERFORMANCE", "content": "In this subsection, we conduct experiments to assess the generalization performance of the proposed\nmethod. All methods are trained on the ImageNet dataset with 16 shots per class and tested on four\ndifferent ImageNet-based datasets. Table 1 reports the results of CoOp, CoOp+Nemesis (PUN),\nand CoOp+Nemesis (PAN). It is clear that CoOp+Nemesis outperforms CoOp consistently on both\nsource and target domains, whether adopting the PUN loss or PAN loss, which suggests that Nemesis\ncan improve CoOp's domain generalization abilities by normalizing the soft prompts in VLMs.\nFurthermore, we can observe that Nemesis using larger w can achieve better transfer performance,\nimplying that a stronger normalization of soft prompts could enhance the robustness of soft prompts\nto domain shifts. Comparing Nemesis using the PUN loss and Nemesis using the PAN loss, despite\nthat the latter achieves better performance on the source domain, its performance on target domains\nis inferior to the former. We argue that this may arise due to the PAN loss excessively prioritizing\nto identify and address the Low-Norm Effect within intra-domain data, which could compromise its\ngeneralization capability. The results of base-to-new experiments can be found in Appendix A.2.4."}, {"title": "4.5 IN-DEPTH STUDIES ON THE LOW-NORM EFFECT IN VLMS", "content": "In this section, we aim to provide plausible explanations for the occurrence of the Low-Norm Effect\nand the effectiveness of the proposed method Nemesis.\nFrom Figure 3(a), it is apparent that the norms of soft prompts in CoOp first increase and then\nlevel off, while test accuracy falls into degradation as norms slowly flatten out. By performing\ncorruption operations that decrease the norms of prompt vectors, the last green circle may be pushed\naway from the degradation area and get closer to those small green circles that demonstrate superior\nperformance. This could be regarded as a plausible explanation for the occurrence of the Low-\nNorm Effect: those corrupted soft prompts that demonstrate superior performance than their original\ncounterparts may be precisely one of those small circles. Moreover, this figure may unveil a potential\ncorrelation between the time when prompt learning starts to degrade and the time when the norm of\nsoft prompts begins to stabilize. We leave this to future research.\nFrom Figure 3(b), different from the observed norm variation pattern in CoOp, CoOp+Nemesis\n(ours) exhibits a distinct trend where norms initially increase, followed by a subsequent decrease,\nand eventually stabilize. Furthermore, the test accuracy exhibits a consistent upward trend before\nreaching a plateau, whereas a declining trend is observed in CoOp. This implies that our method\ncan delay the time point where soft prompts tend to plateau during the learning process, thereby\nreducing the probability of learning degradation."}, {"title": "4.6 EXTENDIBILITY ANALYSIS", "content": "To analyze the extensibility of the proposed approach Nemesis, we apply the proposed method\nNemesis to other soft prompt-tuning methods on few-shot recognition experiments. PLOT (Chen"}, {"title": "5 RELATED WORK", "content": ""}, {"title": "5.1 VISION-LANGUAGE MODELS PRE-TRAINING", "content": "Vision-language models (VLMs), usually consisting of a visual module and a language module, are\nexpected to explore the semantic connections between images and texts. With large-scale image-\ntext pairs which are available on the internet, an increasing number of pre-trained VLMs (Radford\net al., 2021; Cui et al., 2022; Yao et al., 2021) have been proposed. These pre-trained VLMs can\ncapture deep vision-language semantic correspondence by using various vision-language objectives,\nincluding contrastive objective (Radford et al., 2021; Cui et al., 2022; Singh et al., 2022; Yao et al.,\n2021; Zhong et al., 2022), generative objective (Cui et al., 2022; Singh et al., 2022), and alignment\nobjective (Yao et al., 2021; Zhong et al., 2022). Moreover, these pre-trained VLMs also show strong\ncapacities for generalization and can perform zero-shot predictions (without fine-tuning models) on\na wide range of downstream tasks, such as image classification (Radford et al., 2021), visual question\nanswering (Alayrac et al., 2022), and text-guided image generation (Avrahami et al., 2022)."}, {"title": "5.2 PARAMETER-EFFICIENT FINE-TUNING", "content": "Parameter-efficient fine-tuning (PEFT) methods serve as a crucial approach for adapting pretrained\nmodels, particularly within the Natural Language Processing (NLP) domain. Among these, prompt-\ntuning (Lester et al., 2021; Jiang et al., 2023) has gained attention for optimizing task-specific prompt\nembeddings, providing performance similar to full parametric fine-tuning but with fewer tunable pa-\nrameters. Similarly, prefix-tuning (Li & Liang, 2021) extends this concept by optimizing a sequence\nof prefixes at each transformer layer, thereby augmenting the set of tunable parameters marginally,\nwhile P-tuning (Liu et al., 2022) incorporates manually designed patterns to intersperse learned\nprompts within the input embeddings. Inspired by these PEFT methods of NLP, this technique has\nbeen successfully extended to VLMs. For instance, CoOp (Zhou et al., 2022b) and its variants (Zhou\net al., 2022a) apply CLIP (Radford et al., 2021) to few-shot visual recognition tasks by replacing\nhard-crafted prompts with learnable soft-prompt vectors. In addition, adapter-tuning (Gao et al.,\n2023), which allows for fine-tuning a part of the network or fine-tuning an extra network, are an-\nother research direction of PEFT method of VLMs. Distinctively, the proposed method, Nemesis,\nfirst provides empirical evidence that normalizing soft-prompt vectors of VLMs can help improve\nperformance."}, {"title": "6 CONCLUSION", "content": "In this paper, we are the first to examine the impact of soft prompts' norms on the performance of\nVLMs. We conduct extensive corruption experiments using two specially designed operations and\ndiscover the Low-Norm Effect. To harness this phenomenon, we introduce Nemesis, a method for\nnormalizing soft prompts during soft-prompt tuning. In general, Nemesis can be incorporated into\nany soft-prompt-based methods, even other PEFT methods, such as prefix-tuning, and P-tuning. We\nhope our findings and proposed method can provide new insights and facilitate future research on\nthese fields."}, {"title": "A APPENDIX", "content": ""}, {"title": "\u0391.1 \u039c\u0395\u03a4THOD DETAILS", "content": "During the training process, the proposed method Nemesis adopting the Position-Aware Normal-\nization (PAN) loss would involve an additional inference process to determine those prompting\npositions inducing the Low-Norm Effect. On the other side, Nemesis does not change the inference\nprocess of original soft-prompt-based methods. Hence, here we only provide more details of the\ntraining process for Nemesis adopting the PAN loss in this subsection. Its iterative optimization\nprocedure is summarized in Algorithm 1."}, {"title": "\u0391.2 EXPERIMENTAL DETAILS", "content": ""}, {"title": "A.2.1 DATASETS", "content": "Following CoOp (Zhou et al., 2022b) and CoCoOp (Zhou et al., 2022a), the datasets we used in-\nclude 11 datasets for few-shot visual recognition and base-to-new generalization tasks, as well as\n4 ImageNet-based datasets for the evaluation of domain generalization. The details of each dataset\ncan be found in Table A1, including dataset name, the number of classes, the number of training and\ntesting samples, as well as the type of visual task."}, {"title": "A.2.2 HYPER-PARAMETER SETTINGS", "content": "To normalize the soft prompts in VLMs, two types of normalization losses are proposed: the\nPosition-Uniform Normalization (PUN) loss and the Position-Aware Normalization (PAN) loss.\nBoth losses involve a crucial hyper-parameter w, which controls the extent of normalization for soft\nprompts. Unless specified otherwise, w is set to 1 for all datasets, except for the ImageNet dataset\nwhere it is set to 10, the OxfordPets dataset, where it is set to 50, and the Food101 dataset where\nit is also set to 50. Based on our experimental findings, we observed that our approach performs\nwell on these three datasets when w is relatively large. This observation aligns with our discovery\nof a pronounced Low-Norm Effect in these datasets, providing evidence that our method is indeed\ncapable of addressing the Low-Norm Effect. At the same time, we provide a decreasing schedule of\nw for a better balance between Lce and the PUN or PAN loss. To be specific, it is varied based on\na logistic function $W_E = \\frac{1}{1+exp(-k(E-0.5 \\times maxE))}$, where E and maxe denote current training\nepoch and maximum training epoch, respectively. k represents the attenuation rate, and it is fixed as\n0.2.\nIn addition to w, the PAN loss incorporates two important hyper-parameters the number of corruption\npositions N and the pre-defined threshold 7 inducing the Low-Norm Effect. It should be noted\nthat the size of the additional inference cost incurred by the PAN loss is positive correlation with\nN. Technically, the text encoder needs to perform inferences on a batch of B \u00d7 (N + 1). In the\ncomputational efficiency experiments, we set the default value of N to 1 and 7 to 0.5. These settings\nare chosen to minimize computational costs while maintaining the desired performance.\nFurthermore, the baseline model CoOp has multiple variations, including multiple backbones (e.g.\nResNet-50 (He et al., 2016) and ViT-B/16 (Dosovitskiy et al., 2020)), different positions for the class\ntoken (e.g., \"front\", \"middle\", and \"end\"), various lengths for the soft-prompt (e.g., 4 and 16), and\nmultiple parameter initialization strategies. For a clear comparison, here we choose one of them as\nour baseline with ResNet-50 backbone, the class token at the \"end\" position, 16 soft-prompt tokens,\nand \"random\" initialization."}, {"title": "A.2.3 CORRUPTION EXPERIMENTS", "content": "This subsection provides more details of corruption experiments implemented by the two corruption\noperations we proposed.\nThe REPLACE operation involves replacing the prompt vector at a single position with a randomly\ngenerated vector from a Gaussian distribution, which is characterized by a zero mean and a fixed\nvariance. By modifying the variance of the Gaussian distribution, we can roughly control the norms\nof generated Gaussian vectors. To be specific, increasing the variance leads to higher norms of the\ngenerated Gaussian vectors while decreasing the variance results in lower norms. We employ five\ndifferent variance values: 0, 0.001, 0.01, 0.1, and 0.5.\nFurthermore, the RESCALE operation is utilized to rescale the prompt vector at a single position\nby applying various sizes of rescaling factors, including 0.001, 0.01, 0.1, 0.5, and 2. The first four\nrescaling factors are employed to reduce the norms, while the last one is utilized to increase the\nnorms. As a result, for soft prompts of length L and a corruption operation, we conduct corruption\nexperiments L times. Then, we can calculate the occurrence frequency with which the performance\nof corrupted prompts exceeds that of their original counterparts. Tables A2 and A3 provide a de-\ntailed record of this occurrence frequency under different corruption operations, respectively. It is\nnoteworthy that the results of occurrence frequency of the Low-Norm Effect across 11 datasets (i.e.\nthe results of Figure 1) are calculated by the sum of four rescaling factors, including 0.001, 0.01,\n0.1, and 0.5.\nFor a better intuitive understanding, we present a part of corruption experiments in the form of bar\ngraphs. Take the ImageNet dataset under 1 shot setting as an example, as shown in Figure A1.\nFrom Figure A1(c), we can perceive that the replacement of prompt vectors at positions 1-4 with\nrandom Gaussian vector having a zero mean and a variance of 0.001 barely changes the model's\nperformance. Surprisingly, an improvement in performance is observed for positions 5, 7, 8, and\n9. Additionally, a similar pattern can be found in Figure A1(h), where we observe varying degrees\nof performance improvement when the prompt vector at positions 1-14 is rescaled to half of its\noriginal magnitude (i.e. reducing the norms). On the contrary, the model's performance for all\npositions experiences varying degrees of decline when the norms of soft prompts increase, whether\nthe Replace or Rescale operation, as shown in Figure A1(g), A1(i) and A1(j)."}, {"title": "A.2.4 BASE-TO-NEW RESULTS", "content": "In this subsection, we compare the baseline model CoCoOp and CoCoOp+Nemesis (ours) in the\nbase-to-new setting. CoCoOp inputs a batch of image features into an additional neural network to\ngenerate instance-conditional prompts, which are added to soft prompts to produce a batch of final\nprompts. Following CoCoOp, all methods are implemented with ViT-B/16 backbone and evaluated\nwith 16 shots. We report the performance on 11 datasets, including the base classes (Base), new\nclasses (New), and the harmonic mean (H) of both. We present comprehensive results of base-to-\nnew experiments conducted on all datasets, as illustrated in Table A4. We can observe that increasing\nthe strength of normalization of soft prompts (i.e. larger w) would have a slight negative effect on the\nperformance of base classes but better enhance the performance of new classes. This suggests that\nthe generalization performance from base classes to unseen classes can be improved by normalizing\nthe soft prompts of VLMs. In particular, CoCoOp+Nemesis (PAN, w = 20) achieve a performance\nimprovement of 1.3% compared to CoCoOp for new classes."}, {"title": "A.2.5 RESULTS OF ABLATION STUDY AND HYPER-PARAMETER ANALYSIS", "content": "This subsection presents the ablation results for the normalized strength w. The outcomes of different\nsizes of w on few-shot recognition tasks across 11 datasets are illustrated in Table A5. Additionally,\nTable A7 showcases the results of using different norm types for the PUN loss, including 1-norm,\n2-norm, and Inf-norm."}, {"title": "A.2.6 RESULTS DURING TRAINING PROCESS", "content": "This subsection provides results during soft-prompt tuning VLMs, including training loss, test ac-\ncuracy, norms of soft prompts at various positions, and the occurrence frequency of Low-Norm\nEffect for different prompting positions. The data comparison between Figure A2 and Figure A3"}, {"title": "A.2.7 COMPUTATION COST ANALYSIS", "content": "The data in Table A8 represents the average computation costs of multiple training batches, based\non the benchmark of CoOp's training time. The numbers in parentheses indicate the corresponding\nrunning times in seconds. Firstly, we observe that incorporating the PUN or PAN loss increases\ncomputation costs compared to using CoOp alone. The PAN loss introduces an additional pre-\ninference step before each training batch, resulting in a higher computational burden compared\nto the PUN loss. However, combining both losses does not significantly impact the running time\ncompared to using the PAN loss alone. Furthermore, there is a clear and consistent trend across all\nmethods, including CoOp, PLOT, and our two losses, where computation costs increase with the\nnumber of classes in the datasets. We speculate this is caused by the fact that CoOp-based methods\noptimize the similarity scores between text features of all categories and image features of a mini-\nbatch. The increase of number of classes will result in a significant increase in the gradients required\nfor calculation. Lastly, it should be noted that the running time of the PAN loss and PLOT is almost\ncomparable.\nThe data in Table A9 demonstrate that increasing the value of N leads to increased computational\ncosts, particularly when dealing with large datasets, resulting in longer computation time and higher\nmemory consumption. This is because the model requires generating a greater number of corrupted\ntextual features for prediction within each training batch. Although it is generally observed that\nincreasing N has a positive impact on the model's performance, finding ways to mitigate the com-\nputational burden is a valuable area for future research."}, {"title": "A.2.8 OTHER APPLICABLE SCENARIOS ANALYSIS", "content": "While our proposed method primarily focuses on benchmarking soft prompt-tuning VLMs, it should\nbe noted that the benefits of Nemesis may be not limited to it alone. We speculate that they can also\nbe applied to other parameter-efficient tuning (PEFT) methods, such as visual prompt-tuning (Jia\net al., 2022) and prefix-tuning (Li & Liang, 2021), as well as their various downstream task.\nTo verify this, we conducted preliminary experiments on a few PEFT methods and their applicable\nscenarios, including visual prompt-tuning (Jia et al., 2022) for image classification and prefix-tuning\n(Li & Liang, 2021) for paraphrase classification. Given that the proposed PAN loss may involve\ndesigning specific corruption experiments and adopting distinct performance comparison metrics\ndepending on different methods and tasks. To be more specific, we should adopt various perfor-\nmance metrics when identifying the positions that induce the Low-Norm Effect, such as classifica-\ntion accuracy for classification tasks, mAP (mean Average Precision) for object detection tasks, IoU\n(Intersection over Union) for segmentation tasks, and BLEU (BilinguaL Evaluation Understudy) for\ntext generation. We leave it for the future work. Here, we only incorporate the PUN loss into these\nmethods and obtain the results, as shown in Tables A10 and A11.\nBased on the presented results, the proposed PUN loss can enhance the performance in all conducted\nexperiments. However, determining the ideal weight for the proposed loss requires further investi-\ngation, which is a potential direction for future work. In summary, these preliminary results support\nour research prospects discussed in the Conclusion Section (i.e. Section 6), and we hope that our\nfindings and approaches will provide new insights into PEFT methods and inspire future research in\nthese fields."}]}