{"title": "PrimeGuard: Safe and Helpful LLMs through Tuning-Free Routing", "authors": ["Blazej Manczak", "Eliott Zemour", "Eric Lin", "Vaikkunth Mugunthan"], "abstract": "Deploying language models (LMs) necessitates outputs to be both high-quality and compliant with safety guidelines. Although Inference-Time Guardrails (ITG) offer solutions that shift model output distributions towards compliance, we find that current methods struggle in balancing safety with helpfulness. ITG Methods that safely address non-compliant queries exhibit lower helpfulness while those that prioritize helpfulness compromise on safety. We refer to this trade-off as the guardrail tax, analogous to the alignment tax (Leike, 2022). To address this, we propose PrimeGuard, a novel ITG method that utilizes structured control flow. PrimeGuard routes requests to different self-instantiations of the LM with varying instructions, leveraging its inherent instruction-following capabilities and in-context learning. Our tuning-free approach dynamically compiles system-designer guidelines for each query. We construct and release safe-eval, a diverse red-team safety benchmark. Extensive evaluations demonstrate that PrimeGuard, without fine-tuning, outperforms all competing baselines and overcomes the guardrail tax by improving the fraction of safe responses from 61% to 97% and increasing average helpfulness scores from 4.17 to 4.29 on the largest models, while reducing attack success rate from 100% to 8%. PrimeGuard implementation is available at this URL.", "sections": [{"title": "1. Introduction", "content": "Large-scale deployments of language models (LMs) must comply with various guidelines, including regulations (Madiega, 2021; Biden, 2023), application-specific standards (Svetlova, 2022), and organizational policies. To navigate these challenges, model behavior can be strategically altered at three different phases: training time with Model Alignment (Ouyang et al., 2022) (Rafailov et al., 2023) (Wallace et al., 2024), response generation time with Inference-Time Guardrailing (ITG) (Xie et al., 2023; Zhang et al., 2024a; Wei et al., 2023; Varshney et al., 2024), and post generation time with System-based Guardrailing (Inan et al., 2023; Zhang et al., 2024b; Rebedea et al., 2023). These approaches are complementary with additive benefits. A full discussion on related work appears in Appendix B.\nMotivated by the advent of popular pre-trained and instruction-tuned base LLMs, we investigate ITG methods that align language models with looser safety protocols (equipped during Model Alignment) to more stringent protocols at deployment. This setup allows us to harness pre-existing AI safety benchmark datasets, compare our method to baseline ITG methods and stringently aligned LMs, and employ these LMs as robust AI-as-a-judge systems to evaluate our approach. We adapt 15 safety categories (Table 5) from Zeng et al. (2024) based on OpenAI's safety guidelines that form restrictive instructions, i.e. instruct the model what not to do. Constrained by these restrictive instructions, we strive to maximize helpfulness, represented by directive instructions e.g. \"You are a helpful assistant.\""}, {"title": "2. PrimeGuard", "content": "Language models use system prompts $P_{sys}$ to provide context, instructions, and guidelines for generating responses aligned with the goals of the system designer. In our approach, we distinguish between the directive $P_{directive}$ and restrictive $P_{restrictive}$ parts of the system prompt $P_{sys}$. These components facilitate not only compliance with custom functional instructions but also adherence to specific safety protocols. The complete system prompt is represented as:\n$P_{sys} = P_{directive} \\oplus P_{restrictive}$,\nwhere $\\oplus$ indicates concatenation. These system prompts are combined with user-supplied inputs $I_{usr}$ to form the complete model input: $P_{total} = (P_{sys}, I_{usr})$. The LLM response R is generated by an autoregressive process, formalized as:\n$R \\sim p(R | P_{total})$\nwhere $p(R | P_{total})$ denotes the probability of generating a response R conditioned on the input $P_{total}$. The sampled responses from the LLM is denoted as $R = LLM(P_{total})$. For a multi-turn conversation of n turns, the model input is a concatenation of $P_{sys}$ and all past user queries and assistant responses, ending with the latest query $I_{usr}$:\n$P^{(n)}_{total} = (P_{sys}, I^{(1)}_{usr}, R^{(1)}, I^{(2)}_{usr}, R^{(2)}, ..., I^{(n)}_{usr})$"}, {"title": "2.2.1. Overview", "content": "PrimeGuard leverages classical concepts of structured control flow and exception handling to dynamically overcome the guardrail tax. In exception handling, the program flow diverts to special subroutines when errors or exceptional conditions are encountered. Similarly, PrimeGuard uses an $LLM_{Guard}$ to analyze user queries for potential system instruction violations and routes them accordingly. $LLM_{Guard}$ has access to both directive and restrictive instructions while $LLM_{Main}$ only receives the directive instructions."}, {"title": "2.2.2. Stage 1: Risk-Aware Routing", "content": "PrimeGuard starts with $I_{route}$, an instruction (Appendix H.2) that asks $LLM_{Guard}$ to evaluate the risk of answering a user query $I_{usr}$ based on the guidelines in $P_{sys}$. Using in-context learning (ICL) examples (Appendix F.3.2), $I_{route}$ obtains the stage 1 output, $R_{stage1}$, which consists of:\n\u2022 $R_{risk\\_cat}$: risk category of answering the user query with respect to restrictive instructions. Possible values are no violation, potential violation, or direct violation.\n\u2022 $R_{risk\\_rationale}$: rationale for the chosen risk category.\n\u2022 $R_{guidance}$: tip that guides responses to be safe or helpful based on $R_{risk\\_cat}$ and $P_{sys}$."}, {"title": "2.2.3. Stage 2: Generating a Response", "content": "Based on stage 1's outputs $R_{risk\\_cat}$ and $R_{guidance}$, stage 2's instruction either encourages helpfulness or safety:\n\u2022 If $R_{risk\\_cat}$ is no violation (NV): $R_{guidance}$ encourages helpful responses induced with few-shot examples:\n$R = LLM_{Main} (P_{directive}, I_{usr} \\oplus R_{guidance} | NV)$\n\u2022 If $R_{risk\\_cat}$ is direct violation (DV): $R_{guidance}$ encourages polite refusal induced with few-shot examples.\n$R = LLM_{Main} (P_{directive}, I_{usr} \\oplus R_{guidance} | DV)$\n\u2022 If $R_{risk\\_cat}$ is potential violation: $LLM_{Guard}$ is instructed with $I_{reeval}$ to re-evalute $I_{usr}$ and $R_{stage1}$\n$R = LLM_{Guard}(I_{route} I_{usr}, R_{stage1}, I_{reeval})$\nPrimeGuard's dynamic routing mechanism breaks the safety-helpfulness tradeoff and achieves high levels of both: queries posing higher risks are directly refused or reevaluated against restrictive system instructions while low risk queries are encouraged to adhere to directive instructions. Both $I_{route}$ and $I_{reeval}$ contain few-shot examples. Instructions and examples are in Appendices H.2 and F.3.2."}, {"title": "2.2.4. Inducing Performant Routing via ICL", "content": "The performance of PrimeGuard hinges on $LLM_{Guard}$ correctly identifying and routing unsafe queries to refusals and safe queries to helpful responses. In this work, we derive $LLM_{Guard}$ from $LLM_{Main}$ with tuning-free in-context learning (ICL) that leverages few-shot-learning (Brown et al., 2020), avoiding the hassle of data generation and manual oversight involved with supervised fine-tuning.\nWe enlist ICL to align $LLM_{Guard}$ to both helpful and harmless components of $P_{sys}$. To do so, we synthetically generate ICL examples systematically (1) along axes of helpful vs. harmless, maliciousness, $R_{risk\\_cat}$, and categories in our safety policy, and (2) with stylistic tokens (Lin et al., 2023) such as \"I apologize\" (for alignment with $P_{restrictive}$) and \"Hello! Great question\" (for $P_{directive}$) that stimulate LLMs with ICL to match the same token distributions of LLMs with alignment fine-tuning. Our ICL examples in $I_{route}$ and $I_{reeval}$ successfully induces $LLM_{Guard}$ to adaptively sharpen its posterior based on the input query's safety risk, resulting in a robust and effective guardrail. Appendix F.3 further details on how we arrive at an effective $LLM_{Guard}$ without fine-tuning or human supervision."}, {"title": "3. Evaluations", "content": "To evaluate adherence to a custom safety policy (Table 5) we construct the safe-eval dataset and release it to the community \u00b9. It consists of 1,741 non-compliant prompts drawn from synthetically generated adversarial prompts combined with 5 popular red-teaming resistance benchmarks. To assess incorrect refusals we leverage XStest (R\u00f6ttger et al., 2023) which contains both unsafe prompts and similar safe prompts that well-calibrated models should comply with. For usefulness, we sample 200 prompts from instruction-tuning dataset Dolly-15k (Conover et al., 2023) uniformly across 8 different behavioral categories of prompts. Further details on datasets can be found in Appendix D.\nTo evaluate resistance to jailbreaking attacks we employ TAP which achieves 80%+ attack success rates on popular alignment tuned LLMs such as GPT4-Turbo and Gemini-Pro (Mehrotra et al., 2023). TAP uses in-context optimization and judge feedback loops for refining adversarial prompts. Our experiments initialize TAP with AdvBench Subset (Chao et al., 2023), a curated dataset of 50 harmful prompts from Advbench (Zou et al., 2023).\nWe test PrimeGuard's generalizability on 3 models of various sizes and levels of alignment tuning: (141B MoE) Mixtral-8x22B-Instruct-v0.1(Jiang et al., 2024), (3.8B) Phi-3-mini-128k-instruct (Abdin et al., 2024), and (7B) Mistral-7B-Instruct-v0.2 (Jiang et al., 2023). We post comparisons to the stringently aligned Llama-3-8B-Instruct (Meta, 2024)."}, {"title": "3.1. Setup", "content": "Dataset is available on HuggingFace [Link]."}, {"title": "3.2. Baselines", "content": "We select notable ITG and alignment methods as baselines: built-in safety during RLHF alignment (Ouyang et al., 2022), guardrailing through system prompt, Self-Reminder (Xie et al., 2023) and intention analysis (Zhang et al., 2024a). Details for each appear in Appendix E."}, {"title": "3.3. Judges", "content": "We utilize AI-as-a-judge (Kim et al., 2024) (Zhu et al., 2023) for safety, refusals and usefulness. To validate these results, we conducted human experiments and found high human-AI correlation with Cohen's K (Cohen, 1960) of 0.75 for safety and 0.79 for refusal detection. For usefulness, we follow the setup in (Kim et al., 2024) for scoring answers on a 1-5 Likert scale. We use gpt-4-0125-preview to provide reference answers and final judgment scores. Further details on judges are present in Appendix G."}, {"title": "3.4. Empirical Results", "content": "Figure 2 reveals that PrimeGuard significantly outperforms the present-day Pareto frontier by achieving high safety and usefulness across different model sizes. For the largest model, Mixtral-8x22B, PrimeGuard improves the fraction of safe responses from 60.8% (Alignment Only) to 97.0% while simultaneously elevating average helpfulness from 4.170 (Alignment Only) to 4.285, demonstrating the effectiveness of compiling directive instructions (Table 6). PrimeGuard exhibits similar improvements in both safety and helpfulness when applied to Mistral-7B, showcasing its robustness across mid-sized models. Other methods, such as Self-Reminder and Guideline-in-SP achieve reasonable performance but incur a higher guardrail-tax than Prime-Guard. For the smallest model, Phi-3-mini, PrimeGuard also scores significantly higher in helpfulness (3.74) compared to Intention-Analysis (3.19) while increasing safety compared to other methods. However, we see PrimeGuard incur a higher guardrail-tax since it is not the highest scorer for helpfulness, indicating avenues of future research for deriving higher levels of helpfulness from smaller models.\nOverall, PrimeGuard minimizes guardrail-tax compared to other methods, particularly in 7B+ parameter models where PrimeGuard actually increases helpfulness while keeping safety at 97%+. These results show an exciting new frontier of research in steering models to be both helpful and safe."}, {"title": "3.4.1. Jailbreakability", "content": "Table 1 presents the attack success rate (ASR) and average number of queries required to jailbreak models using TAP. PrimeGuard significantly reduces the ASR across different model sizes. PrimeGuard lowers the ASR from 100% (Alignment Only) and 68% (Intention-Analysis) to 8% for Mixtral-8x22B. Similar improvements are shown for Mistral-7B. The results demonstrate that PrimeGuard's routing control flow with ICL can dynamically adapt to be robust against state-of-the-art adversarial attackers, surpassing even the well-aligned Llama-3-8B. It is noteworthy that PrimeGuard accomplishes this while only being employed at inference with a tuning-free approach."}, {"title": "3.4.2. False Refusals", "content": "Optimizing for safety can trigger models to be overly-cautious, refusing to answer safe queries. Refusal rates to safe prompts from XSTest dataset is shown in Table 2. PrimeGuard brings the refusals down to 0% for Mixtral-8x22B and increases refusal rates modestly for Phi-3-mini and Mistral-7B. Smallest model results highlight the need for more nuanced routing and reevaluation mechanisms."}, {"title": "3.5. Ablations", "content": "All ablations are conducted on Mistral-7B-instruct-v0.2."}, {"title": "3.5.1. Effect of Routing and Dynamic Guidance Instructions", "content": "Table 3 explores ablations on the effect of the routing mechanism. Forcing routes to \"Potential Violation\" demonstrate the power of the reevaluation mechanism; unsafe responses and false refusals drops significantly. However, we observe that this has a significant negative impact on helpfulness.\nThe other two rows examine dynamic versus static instructions and rationale. For this ablation we set \"Answer in a helpful way.\" and \"Adherence to safety guidelines.\" for static instruction and rationale respectively. Firstly, we see that dynamic tip does not improve helpfulness for the Mistral 7B model. However, the dynamic tip has a significant impact on false refusals and moderate impact on the proportion of unsafe responses. Secondly, using a static restrictive message provides a small boost in helpfulness, but an increase in unsafe responses, likely due to more messages routed for answer maximizing usefulness."}, {"title": "3.5.2. Effect of Number of In-Context Examples", "content": "We investigate the effect of differing number of ICL examples on performance by selectively adding examples based on the taxonomies presented in Appendix F.3.1 i.e. to increase the number of routing examples from 3 to 6, we add one example for each route category (direct, potential, or no violation). Figure 3 reveals that the setting with no ICL examples (0 route 0 re-eval) suffers performance drops on both safety and helpfulness, confirming the efficacy of PrimeGuard's synthetic ICL generation approach. Moreover, the trendlines elucidate that adding routing examples drastically improves safety while adding re-evaluation examples improves helpfulness. We observe that adding re-evaluation examples also increases safety in some cases but not all. This supports the intuition behind PrimeGuard's control flow design, where re-evaluation occurs when $LLM_{Guard}$ is unsure whether a query should be routed to a safe or helpful answer. As such, adding ICL examples during re-evaluation conditions model output distributions to better respond to borderline queries, bolstering safety and helpfulness."}, {"title": "3.5.3. Effect of Type of ICL Examples During Routing and Re-evaluation", "content": "Table 4 disentangles types of examples shown during routing. All 3 types of examples in routing significantly boost safe responses. Potential violation examples also increase helpfulness, again illustrating their importance in steering $LLM_{Guard}$ to understand nuanced risks."}, {"title": "4. Conclusion", "content": "We propose a novel Inference-Time Guardrailing approach, PrimeGuard, that increases both safety and helpfulness by routing queries through a second instantiation of a model aligned with tuning-free ICL. PrimeGuard significantly outperforms all baselines on benchmarks with multiple models, reducing adversarial jailbreakability from 100% success rate to 8% and increasing safety responses to 97%+ unsafe queries while maintaining or even increasing helpfulness. Ablations show the novel dynamic routing mechanism and ICL synthetic generation as crucial to overcoming the safety-helpfulness tradeoff. We hope future work addresses the limitations (see Appendix A) of PrimeGuard and extends our red-team benchmark safe-eval to further improve LLM controllability, especially for smaller models."}, {"title": "A. Limitations & Social Impact", "content": "In this section, we discuss limitations of the PrimeGuard approach together with the potential social impact it brings."}, {"title": "A.1. Limitations", "content": "1. Dependence on instruction-following abilities: Our method relies on the instruction-following capabilities of $LLM_{Guard}$ and $LLM_{Main}$ both for (1) executing the routing instructions by generating the risk category, category rationale, and the guidance instruction, and (2) following the guidance instruction. We observe that with decreasing model size below 7B parameters, our method offers fewer benefits.\n2. Reliance on structured outputs: The output of Stage 1 must be structured in a way that allows us to programmatically extract each component and apply the PrimeGuard logic. Despite using few-shot examples, we still observe a 1 to 5% failure rate in producing a valid dictionary. However, applying logit-processing techniques might alleviate this issue (Willard & Louf, 2023)."}, {"title": "A.2. Social Impact", "content": "The research presented in this paper seeks to advance the field of securing safe and helpful language models systems. Our primary goal is to offer a practical guardrailing tool that allows for enforcement of custom guidelines without having to comprise on output quality. By developing and evaluating the PrimeGuard approach, we aim to contribute positively to the responsible and ethical integration of AI into various applications. However there is still potential for negative impacts:\n1. Misuse of AI Capabilities: Malicious actors could potentially exploit the PrimeGuard mechanisms to develop more sophisticated attack methods that bypass the guidelines being enforced. It is essential to continue refining detection and mitigation strategies for such misuse.\n2. Bias and Fairness Issues: Despite efforts to instruct models with ethical guidelines, there remains a risk that underlying biases present in the models may lead to unfair or discriminatory outcomes. Future work should focus on robust bias mitigation strategies to ensure fairness and inclusivity in language models."}, {"title": "B. Related Work", "content": "Inference-time-guardrails aim to proactively mitigate risks. Xie et al. (2023) introduced the System-Mode Self-Reminder method, encapsulating user queries within a system-prompt that reinforces ethical behavior, effectively reducing Jailbreak attack success rate. Zhang et al. (2024a) proposed Intention Analysis Prompting (IAPrompt), a technique designed to trigger the inherent self-correction capabilities of large language models during the inference phase, thus improving model safety against stealthy Jailbreak prompts. Wei et al. (2023) demonstrated the use of In-Context Learning (ICL) to manipulate the alignment of language models. By providing a limited number of manipulated context examples, their method showed a significant influence on the model's response to Jailbreak prompts, either increasing or decreasing its susceptibility.\nSystem-based guardrailing systems (Inan et al., 2023; Rebedea et al., 2023; Zhang et al., 2024b) are complementary to the inference time guardrails. They can be another layer of security and are especially important for verifying outputs of tool use as those are not available before the output is produced.."}, {"title": "Adversarial safety attacks", "content": "Static jailbreak prompts like \"Do Anything Now\" attempt to coerce LLMs into providing harmful output have grown in number and efficacy (Shen et al., 2023; Zeng et al., 2024). More recent work has extended static jailbreak techniques by employing automated adversarial models. These adversaries automatically find instructions to bypass safety mechanisms, causing the target LLM to produce non-compliant text. Popular methods like GCG (Zou et al., 2023), PAIR (Chao et al., 2023), and TAP (Mehrotra et al., 2023) achieve 80%+ attack success rates (ASRs) against popular LLMs such as GPT4-Turbo and Gemini-Pro, even though these LLMs have undergone extensive alignment tuning with large-scale human preference data (Christiano et al., 2017; Ouyang et al., 2022).\nAutomated attacks achieve high ASRs due to their search process that iteratively refines queries, oftentimes employing a separate judge-LLM serving as an oracle that assigns reward values to speed up the search (Mehrotra et al., 2023). Other methods utilize gradients or other loss-terms to guide the attach mechanism (Ebrahimi et al., 2017; Guo et al., 2021). These multi-faceted approaches are the reason why we see many standard defense mechanisms struggle against adversarial adaptive jailbreaks, especially static defenses such as system prompts. Although recent work such as SmoothLLM (Robey et al., 2023) and Gradient Cuff (Hu et al., 2024) are exploring better defenses to these automated attacks, they often succumb to the guardrail tax, focusing primarily on safety and sacrificing on helpfulness by increasing model refusal rates."}, {"title": "Model Alignment", "content": "In the context of safety, Model Alignment addresses widely recognized risks such as hate speech, sexual content, and criminal activities (Svetlova, 2022). The safety alignment can be achieved by applying alignment methods such as RLHF (Ouyang et al., 2022) on safety-relevant prompts and human preference data (Ganguli et al., 2022; Achiam et al., 2023). However, some model providers limit the alignment phase due to various alignment taxes, including performance, development, and time-to-deployment taxes (Leike, 2022; Bekbayev et al., 2023) .\nGeneral model alignment not only reduces toxicity but also enhances the models' instruction-following ability, which we rely on heavily. Wallace et al. (2024) also recognized that instruction conflict is the underlying cause of jailbreaks and attempted to address it during the alignment stage."}, {"title": "C. Safety Categories", "content": "We adapt the safety categories from Zeng et al. (2024). They are presented in Table 5."}, {"title": "D. safe-eval dataset", "content": "The safe-eval dataset consists of unsafe prompts from the following datasets: AART (Radharapu et al., 2023) (500 prompts), Student-Teacher-Prompting (Llaca et al., 2023) (165), SAP (Deng et al., 2023) (100), subset of Jailbreakbench (Chao et al., 2024) (237), XSTest (R\u00f6ttger et al., 2023) (125). The subset of Jailbreakbench contains prompts that sucessfully jailbroke llama-2-hf (Touvron et al., 2023), gpt3.5, or gpt-4-1106-preview.\nAs a part of safe-eval dataset we include HardTapPrompts dataset which is a collection of 614 prompts that successfully jailbroke gpt-4-1106-preview or gpt-3.5 turbo. The prompts were obtained through running an adversarial TAP attack (Mehrotra et al., 2023) on a combination of prompts from public datasets (Bhardwaj & Poria, 2023) (Varshney et al., 2024) (Wang et al., 2023). Each prompt is assigned to one of the 15 categories from Table 5 using gpt-4-0125-preview as a judge (see Appendix G)."}, {"title": "E. Description of the baselines", "content": "All of the models in this study are equipped with a baseline level of safety-alignment. In practice, that translates to a model (partially) refraining from answering certain questions without having been intentionally instructed to do so through system or user instruction. $R_{baseline} = LLM(P_{directive} \\oplus P_{usr})$. The baseline level of safety for different models can be observed in Figure X.\nThis method involves integrating restrictive instructions directly into the system prompt to ensure adherence to safety standards. The system prompt thus comprises both directive and restrictive instructions $P_{sys} = P_{directive} \\oplus P_{restrictive}$. and the response is generated as: $R_{sp} = LLM(P_{sys}, P_{usr})$.\nIntroduced by Xie et al. (2023), this approach appends a static reminder to the user query to reinforce safety guidelines: $I_{usr} = I_{usr} \\oplus I_{reminder}$. The response is then: $R_{reminder} = LLM(P_{sys}, I_{usr})$. The reminder string, $I_{reminder}$ we append is Remember, you should be a responsible assistant and should adhere to the safety guidelines!\nDeveloped by Zhang et al. (2024a), this method instructs the LLM with $I_{intention}$ to analyze the intention behind a user input $I_{usr}$. The assistant synthesizes the intention $R_{input\\_intention}$, the model is then instructed with $I_{get-answer}$ to provide a response aligned with the policy based on $R_{asst}$. The final response is represented as $R = LLM(P_{sys}, I_{intention} \\oplus I_{usr}, R_{input\\_intention}, I_{get\\_answer})$. This method represents the current state-of-the-art in inference stage guardrailing."}, {"title": "F. Further Experiments and Results", "content": "Table 6 presents the helpfulness-safety trade-off in a table format. It is provided for reference to the Figure 2."}, {"title": "F.2. Types of responses to unsafe prompts", "content": "Given a prompt, we consider that the model can answer one of three ways (R\u00f6ttger et al., 2023):\n\u2022 Full Compliance: The model directly attempts to answer the question, regardless of the accuracy, relevance, or potential harm of the answer.\n\u2022 Full Refusal: The model states that it cannot (in the sense of will not) answer the prompted question. This includes responses that only offer alternative suggestions without answering the original question.\n\u2022 Partial Refusal: A combination of refusal and compliance. This includes self-contradictory responses that first refuse and then go on to answer the prompted question, or responses that answer part of the question but refuse to provide answers to the other part."}, {"title": "F.3. In-Context Learning (ICL)", "content": "We utilize safety taxonomies in order to procure a high-quality set of ICL examples for routing (Table 7) and re-evaluation (Table 8). These taxonomies contain dimensions of various components of safety, including:\n\u2022\n\u2022 Response Category (only for re-evaluation examples): whether this example is teaching $LLM_{Main}$ to answer helpfully or harmlessly\n\u2022 Maliciousness: intent behind query. For example, malicious_jailbreak represents scenarios of malicious actors intentionally attempting to use prompt injection techniques to jailbreak the language model.\n\u2022"}, {"title": "H.1. Prompts", "content": "H.2. DynaGuard Prompts\nProvide your safety assessment for $agent_type in the above conversation. Return two\nReturn unsafe and the violated categories if the content falls under the given\nClassification: {\"category\": \"Partial Refusal\", \"score\": 7}"}]}