{"title": "Co-designing an AI Impact Assessment Report Template with AI Practitioners and AI Compliance Experts", "authors": ["Edyta Bogucka", "Marios Constantinides", "Sanja \u0160\u0107epanovi\u0107", "Daniele Quercia"], "abstract": "In the evolving landscape of Al regulation, it is crucial for companies to conduct impact assessments and document their compliance through comprehensive reports. However, current reports lack grounding in regulations and often focus on specific aspects like privacy in relation to AI systems, without addressing the real-world uses of these systems. Moreover, there is no systematic effort to design and evaluate these reports with both AI practitioners and AI compliance experts. To address this gap, we conducted an iterative co-design process with 14 AI practitioners and 6 AI compliance experts and proposed a template for impact assessment reports grounded in the EU AI Act, NIST's AI Risk Management Framework, and ISO 42001 AI Management System. We evaluated the template by producing an impact assessment report for an AI-based meeting companion at a major tech company. A user study with 8 AI practitioners from the same company and 5 AI compliance experts from industry and academia revealed that our template effectively provides necessary information for impact assessments and documents the broad impacts of Al systems. Participants envisioned using the template not only at the pre-deployment stage for compliance but also as a tool to guide the design stage of AI uses.", "sections": [{"title": "Introduction", "content": "The potential of AI to bring about significant societal shifts requires a careful examination of the associated risks and benefits (United Nations Educational, Scientific and Cultural Organization 2023). This has prompted scholars to investigate established impact assessments processes in fields such as environmental protection (Selbst 2021; Metcalf et al. 2021) and human rights (Gaumond and R\u00e9gis 2023; United Nations Educational, Scientific and Cultural Organization 2023), as well as to suggest algorithmic impact assessments (AIAs) as initial self-regulatory approaches for recognizing and mitigating algorithmic harms (Metcalf et al. 2021; Golbin 2021). These assessments, guided by ethical principles such as responsibility and fairness, were recommended to be conducted at different stages, such as system's design, pre-launch, and post-launch, and to be publicly shared as impact statements (Diakopoulos et al. 2016).\nAs it has become evident that AI system impacts involve not only the underlying algorithms (Watkins et al. 2021; Barnett and Diakopoulos 2022), but also their systemic impacts (Ehsan et al. 2022; Shelby et al. 2023), there has been a growing demand for comprehensive AI impact assessments (AIIAs) (Selbst 2021). In the absence of regulatory requirements and standardized frameworks, initially, AIIAs have been regarded as an extension of existing Al governance processes, such as assessments of privacy (Wright and Wadhwa 2012), data protection (Janssen 2020), and social consequences of AI systems, models and services (Selbst 2021; Raji et al. 2020). However, over time, AIIAs developed into an independent component of AI governance (Skoric 2023). By 2021, a total of 38 distinct impact assessment methods have been introduced (Stahl et al. 2023) serving various goals such as prompting companies to proactively address social consequences (Sherman and Eisenberg 2024), aligning system behavior with organization's responsible AI principles (Raji et al. 2020; Fujitsu Research Center for AI Ethics 2022; Microsoft 2022b), increasing awareness of potential harms among development teams (Johnson and Heidari 2023), and documenting decisions to facilitate learning and future governance development (Selbst 2021).\nHowever, current AIIAs reports are marked by a lack of comprehensiveness and insufficient grounding in established frameworks such as the EU AI Act (European Comission 2024), the NIST's AI Risk Management Framework (AI RMF) (National Institute of Standards and Technology 2023a), and international standards such as the ISO 42001 AI Management System (ISO/IEC 2023), that provide complementary angles for impact assessment: regulatory compliance (EU AI Act), risk management (AI RMF), and organizational best practices (ISO 42001). The lack of alignment with regulations makes it harder for stakeholders to understand the broad societal, environmental, and economic impacts of AI uses, which are essential for risk assessments (European Comission 2024; Hupont et al. 2024). Instead, they must repeatedly gather information across disparate assessments of various AI components (e.g., models, systems, and algorithms) rather than focusing on specific uses. Developer teams, in particular, often encounter difficulties in initiating AI impact assessments (Bu\u00e7inca et al. 2023) and require additional guidance throughout this process (Wang et al. 2023). In response to these issues, our aim is to examine recent regulatory frameworks and existing AIIAs to devise together with AI practitioners and AI compliance experts a reporting template focusing on the intended use of the system. This template aims to provide the necessary in-"}, {"title": "Related Work", "content": "We surveyed various lines of research that our work draws upon and grouped them into three main areas: (1) eliciting requirements for designing a comprehensive template for an impact assessment report that is grounded in regulations, (2) co-designing the template, and (3) evaluating the template."}, {"title": "Requirements for Designing a Comprehensive Template That Is Grounded in Regulations", "content": "AI impact assessments are defined as structured processes for understanding the implications of proposed AI systems (Stahl et al. 2023). They have been proposed as both law-agnostic self-regulating processes (National Institute of Standards and Technology 2023a,b) and official processes in regulations (European Comission 2024; The Danish Institute for Human Rights 2023; The Government of Canada 2023) and organizational standards (ISO/IEC 2023). For example, the NIST AI RMF, a voluntary guidance framework for organizations that design, develop, deploy, or use AI systems, suggests assessing the beneficial and harmful impacts of AI on individuals, groups, organizations, and society. The EU AI Act (European Comission 2024), the first comprehensive AI regulation in the European Union, requires comprehensive fundamental rights impact assessment for high-risk AI uses deployed by bodies governed by public law, private operators providing public services, and operators assessing creditworthiness and conducting risk assessments for life and health insurance. This impact assessment, conducted before deployment and updated as needed, should cover affected groups, risks of harm, human oversight, and risk management strategies. Similarly, the world's first ISO 42001 standard for AI management system (ISO/IEC 2023) requires organizations to assess the potential consequences of AI systems on individuals and societies, including human rights, legal positions, and life opportunities.\nDespite ongoing efforts to standardize AIIAS (ISO/IEC 2025), a consensus on their content and reporting methods for stakeholders and the public is still lacking (Watkins et al. 2021; Sherman and Eisenberg 2024; European Comission 2024). Stahl et al. (2023) identified and studied 38 AIIAS, showing that they vary widely in topics, focus, and formats, primarily covering human rights (The Council of Europe 2018; Mantelero 2022; The Danish Institute for Human Rights 2023) and ethics (Gebru et al. 2021; The High-Level Expert Group on Artificial Intelligence 2020). The focus of AIIAs varies, assessing the impacts of AI models, systems, or services, typically mentioning use cases only briefly in final reports. However, there is growing agreement that impact assessments should be conducted on specific uses (European Comission 2024; Hupont et al. 2024). In the EU AI Act (European Comission 2024), the specific use determines one of four risk categories (unacceptable, high, limited, minimal), each with its own legal requirements. Similarly, in the NIST framework (National Institute of Standards and Technology 2023a), use-case profiles are required to describe the current state and the desired state of the system, allowing for risk management at different stages of the AI lifecycle."}, {"title": "Collaborative Design of an Impact Assessment Report Template", "content": "To achieve consensus on AIIAs, their report templates should adequately document AI uses, reflect the socio-technical nature of their impacts (Metcalf et al. 2021; National Institute of Standards and Technology 2023b), and accommodate different roles (e.g., developers, researchers, managers, compliance experts) to surface these impacts (Moss et al. 2021; Selbst 2021; The High-Level Expert Group on Artificial Intelligence 2020; Constantinides et al. 2024a). However, existing AIIAs often lack publicly available documentation on their design processes (Johnson and Heidari 2023), raising concerns about whether the current reports' templates meet the diverse needs of stakeholders.\nWhen available, the documentation typically only specifies the nature of the consultations with stakeholders, lacking depth about the types of stakeholders involved and how their input influenced the final design. For example, Microsoft's Responsible AI Impact Assessment Template was developed following discussions with internal collaborators (Microsoft 2022b), while the Canadian ADM template (The Government of Canada 2023) was revised through consultations with relevant stakeholders. In contrast, the Ada Lovelace Institute's Algorithmic Impact Assessment template in healthcare was developed through interviews with potential users from companies, researchers, and impact assessment experts (Ada Lovelace Institute 2022).\nResearchers highlight the need for more inclusive, value-sensitive co-design processes (Johnson and Heidari 2023; Sadek et al. 2024) that explicitly incorporate compliance"}, {"title": "Methods", "content": "To design a comprehensive template for an impact assessment report that is grounded in regulations, we conducted a series of three studies (Figure 1). These studies included literature review and interviews with 2 AI compliance experts, an iterative co-design process with 14 AI practitioners and 6 AI compliance experts, and a user study to evaluate the template with 8 AI practitioners working in industry research and 5 AI compliance experts from industry and academia.\nAll sessions were conducted via video conferencing over three months, and were recorded and transcribed. We ensured anonymity of data by excluding personal identifiers, and maintained exclusive access to the data for the research team only. All sessions were approved by our organization."}, {"title": "Eliciting Design Requirements From Literature Review and Semi-structured Interviews With AI Compliance Experts", "content": "To elicit requirements for designing a comprehensive template for an impact assessment report that is grounded in regulations, we resorted to previous literature and did so in two steps. First, we reviewed prior academic literature investigating the use of risk assessments, impact reports, and AI documentation for compliance (Skoric 2023; Selbst 2021; Gaumond and R\u00e9gis 2023; National Institute of Standards and Technology 2023a). We also drew upon five existing AI impact assessment templates (Microsoft 2022b; Fujitsu Research Center for AI Ethics 2022; National Institute of Standards and Technology 2023b; Ada Lovelace Institute 2022; The Government of Canada 2023) and value-sensitive games and cards (Ballard, Chappell, and Kennedy 2019;"}, {"title": "Co-designing the Impact Assessment Report Template With AI Practitioners and AI Compliance Experts Through an Iterative Process", "content": "After designing the first version of the template, we conducted a series of individual, 30-minute co-design sessions with 14 AI practitioners and 6 AI compliance experts (Figure 2). These sessions aimed to ensure that the template provided sufficient information for impact assessment according to the EU AI Act, NIST's AI RMF, and ISO 42001, and was usable by various roles in the assessment process.\nParticipants. We aimed to achieve a diverse participant sample using snowball sampling, a method where existing study participants recruit future participants from among their acquaintances. We began by identifying 6 initial participants through an internal mailing list at a large tech company, asking for individuals who were familiar with the EU AI Act and had at least one ongoing AI project. These participants were then asked to refer additional participants from their networks, thus expanding the sample size through successive referrals. We recruited a total of 20 participants (13 male, 7 female, with a median age of 34 years old) representing a variety of AI practitioners such as researchers (9), engineers (2), managers (3), designers (1), and AI compliance experts (6), and potential end users of the template. Their expertise span across various areas including generative AI, machine learning, deep learning, computer vision, AI standardization, and human rights. Additionally, each participant was actively involved in developing or evaluating at least one ongoing AI project during the time of the interviews. AI"}, {"title": "Evaluation of Impact Assessment Report Template", "content": "To then evaluate the final template, we first populated it with a real-world use of an AI system that was developed in the same large tech company and then conducted a user study to evaluate the populated report produced for this system's use with 8 AI practitioners and 5 AI compliance experts."}, {"title": "Populating the Impact Assessment Report Template", "content": "To populate the template, we employed a three-step semi-automatic method, which included soliciting AI practitioners responses and reviewing these responses with AI compliance experts.\nIn the first step, we compiled a bank of statements to gather responses from AI practitioners. To do so, we sourced statements from responsible AI guidelines (The High-Level Expert Group on Artificial Intelligence 2020; Constantinides et al. 2024a), documentation standards (Selbst 2021; Gebru et al. 2021; Holland et al. 2020; Bender and Friedman 2018; Mitchell et al. 2019; Sokol and Flach 2020; Raji et al. 2020), checklists and impact assessment questionnaires (Madaio et al. 2020; Golpayegani, Pandit, and Lewis 2023; Skoric 2023; National Institute of Standards and Technology 2023b). Next, we reviewed these statements, linking them to the relevant excerpts from the EU AI Act, the NIST AI RMF, and the ISO 42001, and grouping similar ones together. This process resulted in a list of 32 statements grounded in regulations and best responsible AI practices, designed to systematically gather information about the system's use, components, and data, team involvement, and the associated risks, mitigations, and benefits. They also directly map to the sections of our template.\nIn the second step, we reached out to 2 AI practitioners\u2014a researcher and a designer-who had contributed to the development of an AI-based meeting companion app. We asked them to provide responses to these 32 statements.\nIn the third step, two authors manually parsed the practitioners' responses, placing them in the template to complete the report. We then consulted 2 AI compliance"}, {"title": "Conducting a User Study for Evaluating the Populated Impact Assessment Report", "content": "We conducted a user study in the form of semi-structured 30-minute interviews with 8 AI practitioners from the same large tech company and 5 AI compliance experts from both industry and academia. Specifically, our evaluation ought to answer four questions:\nQ1: To what extent does the report's content contain all necessary minimum information for conducting impact assessments in line with AI regulations (R1)?\nQ2: To what extent does the report's content address all Al system components to identify, evaluate, and mitigate socio-technical risks of the system's use (R2)?\nQ3: To what extent is the report's template adaptable to different AI system uses (R3)?\nQ4: To what extent is the report's template adaptable to different roles (R4)?\nParticipants. We recruited 8 AI practitioners working in the tech industry, with various roles and expertise, including engineers specializing in computer vision, and researchers in deep-learning. Additionally, 5 AI compliance experts took part in the study, who work in industry (3) and academia (2). Table 2 summarizes participants' demographics.\nProcedure. Before conducting the interviews, we sent an email to all participants. This email included a brief description of the study and a short survey focusing on demographics. The survey solicited details about the participants' age, area of expertise, professional role, and years of experience in developing AI systems (Appendix E). Furthermore, we attached brief overviews of the EU AI Act, the NIST AI RMF and the ISO 42001 (Appendix A). We requested the participants familiarize themselves with these documents to ensure a thorough evaluation of the template for alignment and more focused discussions during the interviews. Our organization gave its approval for the study. Again, we maintained data anonymity, removed personal identifiers, and restricted data access to the research team only.\nDuring the interviews, we presented participants with both the populated final impact assessment report and the baseline for the AI-based meeting companion. Participants read each report for up to 15 minutes, alternating between them to avoid learning effects. After each reading, participants rated 12 statements related to the four design requirements identified from the co-design sessions, using a Likert scale from 1 (strong disagreement) to 5 (strong agree-"}, {"title": "Qualitative results", "content": "Qualitative results. Al practitioners found the final template and its sections helpful. For example, P6 stated that \"I really like it [the template] covers various aspects of the system like its components and data\u201d. Practitioners also praised the template's simplicity. P3 stressed they \"like its simplicity as it helps me deal with the complexity of the AI system\", for example by scoping well the intended use of the system, which P7 summarized as \"you see it [five-format component], and it sticks with you\u201d. All 8 AI practitioners praised the idea of grounding the report's content in the EU AI Act articles. For example, P8 stated that \"these risks are citing the specific sections of the Act [...] It's definitely enough information to convince me that it [the meeting companion] is high risk\u201d. This was more evident when our participants commented about the risks section of the report. Specifically, P1 mentioned that \"the risk section is a good start for understanding how it [the meeting companion] can be misused\". However, AI practitioners also saw a number of improvements. Six respondents highlighted the need for automated tools to populate the report. E10 stated that \"it is very much about discovering risks and I strongly believe that we should use automated tools and GenAI to aid in that process to prompt with the right kind of responses\u201d.\nAI compliance experts have confirmed that the final template effectively addresses two aspects of integrating and managing AI systems within organizational processes. First, the final template promotes the integration of AI-driven processes with existing, well-established organizational procedures. As stated by E13, \"the approach with the evaluation during development, deployment and use is quite familiar to anyone really involved in product development. This feedback must have been given by developers and I agree completing the template should not be an additional burdensome process, but something that is integrated into what we are already used to doing\". Second, by advocating for the blending of multiple taxonomies and frameworks, the template ensures a thorough and comprehensive approach to risk assessment. This method helps prevent the oversight of potential risks by discouraging a narrow focus on single frameworks. As summarized by E10, \u201cI see your intention not to focus on only one risk taxonomy. If you want to make sure that you are covering everything, it is very good to start with one taxonomy and blend in taxonomies from other frameworks. There are no clear boundaries between risks; they overlap and create different categories across frameworks. In reality, these frameworks interact and mix to some extent.\" To see how, E12 explained how the same term \u201csystemic risk\u201d has multiple meaning based on the regulation: \"very large online platforms and search engines, e.g., providing AI-based recommender systems, need to perform a systemic risk assessment under the Digital Services Act, which specifies four categories of such risks [the dissemination of illegal content, negative effects for the fundamental rights; negative effects for the civic discourse, electoral processes, and public security; and negative effects in relation to gender-based violence, the protection of public health and minors and serious negative consequences to the person's physical and mental well-being]. However, systemic risk in the EU AI Act relates also to the general purpose AI models and refers to negative effects on public health, safety, public security, fundamental rights, or the society\".\nBoth AI practitioners and AI compliance experts stressed the importance of the inclusion of expert oversight to elevate the quality and reliability of the assessments carried out jointly by these stakeholders. E9, AI compliance expert, echoed AI practitioners' perspectives: \u201cthe biggest challenge is ensuring teams have the necessary skills and knowledge to complete the assessment. Good written guidelines and preferably some expert guidance and oversight would lead to better results\". Participants also envisioned using the template not only at the pre-deployment stage right before the compliance but also as a tool to guide the design stage of AI uses. P7, researcher, noted that the template \"suits all roles and summarizes the design decisions made so far. It has a different structure than typical compliance reports, where each section has a topic-specific focus and is relevant only for certain experts\". E12, a compliance expert, suggested that \"the template can be completed not only at the development stage but also every time the socio-technical context changes\". To illustrate this point, they gave the example of the national system that evaluates the chances of specific groups in the labor market, which was not usable during the COVID-19 pandemic, and for which similar risks had not been foreseen in the limitations of the system."}, {"title": "Discussion", "content": "We contextualize our template within prior literature, then discuss its implications, limitations, and future research."}, {"title": "Differences with Existing Templates", "content": "We compared our template with the Algorithmic Impact Assessment template from NIST (National Institute of Standards and Technology 2023b), Microsoft's Responsible AI Impact Assessment Template (Microsoft 2022b), Credo AI's Standardized Risk Profile (Sherman and Eisenberg 2024), and the Algorithmic Impact Assessment from the Ada Lovelace Institute (Ada Lovelace Institute 2022), and identified four key differences. First, our template enhances cross-company comparisons by aligning closely with legislation rather than relying on framework-specific biases or company-specific risks. Second, unlike existing templates which focus heavily on risks, ours equally emphasizes both risks and benefits. Third, there is variation in the extent of guidance provided for completing the templates. NIST's (National Institute of Standards and Technology 2023b), Microsoft's (Microsoft 2022a), and the Ada Lovelace Institute's (Ada Lovelace Institute 2022) offer guidebooks on how to produce a report. Rather than leaving stakeholders with a blank report and a guide, we offer a guidebook through our 32 simple statements that alleviate potential anxiety associated with an empty page and facilitate the identification of risks and benefits. Finally, whereas other templates have separate sections for legal and compliance issues, ours integrates these aspects throughout."}, {"title": "Theoretical and Practical Implications", "content": "Embedding AI governance in impact assessment reports. We introduced a standardized template that not only facilitates risk assessment in accordance with the EU AI Act, the NIST's AI RMF, the ISO 42001 but also serves as a model for integrating regulatory considerations into AIIAs. Our work contributes to co-designing and validating improved AIIAs and the processes and workflows surrounding them (Skoric 2023). This includes developing new tools and methodologies that are practical and adaptable to enhance the quality and effectiveness of impact assessments in the AI context. They provide a comprehensive and practical framework for companies to navigate the complexities of AI regulation, ensuring both compliance and ethical responsibility in AI development and implementation.\nFacilitating contextual evaluation. Finding the right balance between making impact assessment template general enough to apply to various AI systems and specific enough to provide meaningful assessments for each unique system's use is a significant challenge (Stahl et al. 2023). We partly addressed this challenge by adding specific subsections into the template. These subsections cover the five components comprehensively describing the system's use, the three key stakeholders, and the three stages of the system's lifecycle. By doing so, we assist stakeholders in systematically documenting the intended use for which the system was built, ensuring that it meets performance and safety criteria across different situations (Johnson and Heidari 2023)."}, {"title": "Improving stakeholder engagement", "content": "Improving stakeholder engagement. Our report template can be utilized by individual team members or as a group. For teams that are new to impact assessments, working through this framework can be an educational experience that helps in building understanding and skills related to responsible AI practices. Future work on improving stakeholder engagement should explore alternative interactive tools that effectively balance various methods of eliciting system information and envisioning system impacts, such as divergent and convergent thinking styles (Selbst 2021)."}, {"title": "Limitations and Future Work", "content": "Generalizability. Our results are based on a pool of study participants who were familiar with the EU AI Act, the NIST framework and ISO standard. Including a broader range of roles beyond managers, designers, and researchers may yield different results. Future studies should include participants with different levels of knowledge about AI regulations. They should also evaluate how well impact assessment reports explain the risks and benefits of AI to everyone and find ways to help people understand important AI regulations and laws.\nPropagating biases in impact assessment. AIIAs, like other responsible AI tools, have inherent biases from their design choices (e.g., excluding potential users (Moss et al. 2021)). The quality of our report template depends on the accuracy and completeness of user-provided information. Using biased or incomplete data can lead to incorrect assessments, making problems seem smaller than they really are because people might be afraid to report negative impacts. To fix this, future research should include external perspectives by holding workshops, involving independent experts and marginalized groups, and regularly checking the results with new team members (Raji et al. 2020; Ada Lovelace Institute 2022).\nAutomated tools pre-populating the template. Automated tools can help make gathering data for templates easier and less prone to mistakes. Large Language Models (LLMs) can help fill out impact assessment reports by generating lists of AI users and subjects (Bu\u00e7inca et al. 2023), identifying intended and unintended uses (Wang et al. 2024; Herdel et al. 2024), and listing potential risks and benefits (Constantinides et al. 2024b; De Miguel Velazquez et al. 2024). Bogucka et al. (2024) have recently proposed a semi-automatic system that collects input from stakeholders about an Al system's use, uses LLMs to find additional risks, mitigation strategies, and benefits, and pre-fills reports for experts to review.\nResponsible by Design. AIIAs are usually conducted at the pre-deployment stage for legal compliance. However, after our co-design process and user studies which began with compliance as a focal point, we learned that reports should be updated not only when lifecycle stages change (e.g., from design to development), but also when the socio-technical context shifts. This underscores the challenge of balancing a fixed impact assessment template with the significant learning benefits users gain from engaging with it to \"think differently\" about AI. Without clear updates, users may lack the"}, {"title": "Conclusion", "content": "We developed an impact assessment template with input from 16 AI practitioners and 6 compliance experts, designed to align with standards such as the EU AI Act, NIST AI RMF, and ISO 42001. A user study involving other 8 company AI practitioners and 5 compliance experts confirmed that our template effectively captures AI system impacts, serving as a starting point for navigating regulatory compliance and fostering responsible design."}]}