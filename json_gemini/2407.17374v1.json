{"title": "Co-designing an AI Impact Assessment Report Template\nwith AI Practitioners and AI Compliance Experts", "authors": ["Edyta Bogucka", "Marios Constantinides", "Sanja \u0160\u0107epanovi\u0107", "Daniele Quercia"], "abstract": "In the evolving landscape of Al regulation, it is crucial for\ncompanies to conduct impact assessments and document their\ncompliance through comprehensive reports. However, current\nreports lack grounding in regulations and often focus on spe-\ncific aspects like privacy in relation to AI systems, without\naddressing the real-world uses of these systems. Moreover,\nthere is no systematic effort to design and evaluate these re-\nports with both AI practitioners and AI compliance experts.\nTo address this gap, we conducted an iterative co-design pro-\ncess with 14 AI practitioners and 6 AI compliance experts and\nproposed a template for impact assessment reports grounded\nin the EU AI Act, NIST's AI Risk Management Framework,\nand ISO 42001 AI Management System. We evaluated the\ntemplate by producing an impact assessment report for an AI-\nbased meeting companion at a major tech company. A user\nstudy with 8 AI practitioners from the same company and 5\nAI compliance experts from industry and academia revealed\nthat our template effectively provides necessary information\nfor impact assessments and documents the broad impacts of\nAl systems. Participants envisioned using the template not\nonly at the pre-deployment stage for compliance but also as a\ntool to guide the design stage of AI uses.", "sections": [{"title": "Introduction", "content": "The potential of AI to bring about significant societal shifts\nrequires a careful examination of the associated risks and\nbenefits (United Nations Educational, Scientific and Cul-\ntural Organization 2023). This has prompted scholars to in-\nvestigate established impact assessments processes in fields\nsuch as environmental protection (Selbst 2021; Metcalf et al.\n2021) and human rights (Gaumond and R\u00e9gis 2023; United\nNations Educational, Scientific and Cultural Organization\n2023), as well as to suggest algorithmic impact assessments\n(AIAs) as initial self-regulatory approaches for recognizing\nand mitigating algorithmic harms (Metcalf et al. 2021; Gol-\nbin 2021). These assessments, guided by ethical principles\nsuch as responsibility and fairness, were recommended to be\nconducted at different stages, such as system's design, pre-\nlaunch, and post-launch, and to be publicly shared as impact\nstatements (Diakopoulos et al. 2016).\nAs it has become evident that AI system impacts involve\nnot only the underlying algorithms (Watkins et al. 2021; Bar-\nnett and Diakopoulos 2022), but also their systemic impacts\n(Ehsan et al. 2022; Shelby et al. 2023), there has been a\ngrowing demand for comprehensive AI impact assessments\n(AIIAs) (Selbst 2021). In the absence of regulatory require-\nments and standardized frameworks, initially, AIIAs have\nbeen regarded as an extension of existing Al governance\nprocesses, such as assessments of privacy (Wright and Wad-\nhwa 2012), data protection (Janssen 2020), and social con-\nsequences of AI systems, models and services (Selbst 2021;\nRaji et al. 2020). However, over time, AIIAs developed into\nan independent component of AI governance (Skoric 2023).\nBy 2021, a total of 38 distinct impact assessment meth-\nods have been introduced (Stahl et al. 2023) serving vari-\nous goals such as prompting companies to proactively ad-\ndress social consequences (Sherman and Eisenberg 2024),\naligning system behavior with organization's responsible AI\nprinciples (Raji et al. 2020; Fujitsu Research Center for AI\nEthics 2022; Microsoft 2022b), increasing awareness of po-\ntential harms among development teams (Johnson and Hei-\ndari 2023), and documenting decisions to facilitate learning\nand future governance development (Selbst 2021).\nHowever, current AIIAs reports are marked by a lack\nof comprehensiveness and insufficient grounding in estab-\nlished frameworks such as the EU AI Act (European Comis-\nsion 2024), the NIST's AI Risk Management Framework\n(AI RMF) (National Institute of Standards and Technology\n2023a), and international standards such as the ISO 42001\nAI Management System (ISO/IEC 2023), that provide com-\nplementary angles for impact assessment: regulatory com-\npliance (EU AI Act), risk management (AI RMF), and orga-\nnizational best practices (ISO 42001). The lack of alignment\nwith regulations makes it harder for stakeholders to under-\nstand the broad societal, environmental, and economic im-\npacts of AI uses, which are essential for risk assessments\n(European Comission 2024; Hupont et al. 2024). Instead,\nthey must repeatedly gather information across disparate as-\nsessments of various AI components (e.g., models, systems,\nand algorithms) rather than focusing on specific uses. De-\nveloper teams, in particular, often encounter difficulties in\ninitiating AI impact assessments (Bu\u00e7inca et al. 2023) and\nrequire additional guidance throughout this process (Wang\net al. 2023). In response to these issues, our aim is to ex-\namine recent regulatory frameworks and existing AIIAs to\ndevise together with AI practitioners and AI compliance ex-\nperts a reporting template focusing on the intended use of\nthe system. This template aims to provide the necessary in-"}, {"title": "Related Work", "content": "We surveyed various lines of research that our work draws\nupon and grouped them into three main areas: (1) eliciting\nrequirements for designing a comprehensive template for an\nimpact assessment report that is grounded in regulations, (2)\nco-designing the template, and (3) evaluating the template."}, {"title": "Requirements for Designing a Comprehensive\nTemplate That Is Grounded in Regulations", "content": "AI impact assessments are defined as structured processes\nfor understanding the implications of proposed AI sys-\ntems (Stahl et al. 2023). They have been proposed as both\nlaw-agnostic self-regulating processes (National Institute of\nStandards and Technology 2023a,b) and official processes\nin regulations (European Comission 2024; The Danish In-\nstitute for Human Rights 2023; The Government of Canada\n2023) and organizational standards (ISO/IEC 2023). For ex-\nample, the NIST AI RMF, a voluntary guidance framework\nfor organizations that design, develop, deploy, or use AI sys-\ntems, suggests assessing the beneficial and harmful impacts\nof AI on individuals, groups, organizations, and society. The\nEU AI Act (European Comission 2024), the first compre-\nhensive AI regulation in the European Union, requires com-\nprehensive fundamental rights impact assessment for high-\nrisk AI uses deployed by bodies governed by public law,\nprivate operators providing public services, and operators\nassessing creditworthiness and conducting risk assessments\nfor life and health insurance. This impact assessment, con-\nducted before deployment and updated as needed, should\ncover affected groups, risks of harm, human oversight, and\nrisk management strategies. Similarly, the world's first ISO\n42001 standard for AI management system (ISO/IEC 2023)\nrequires organizations to assess the potential consequences\nof AI systems on individuals and societies, including human\nrights, legal positions, and life opportunities.\nDespite ongoing efforts to standardize AIIAS (ISO/IEC\n2025), a consensus on their content and reporting methods\nfor stakeholders and the public is still lacking (Watkins et al.\n2021; Sherman and Eisenberg 2024; European Comission\n2024). Stahl et al. (2023) identified and studied 38 AIIAS,\nshowing that they vary widely in topics, focus, and for-\nmats, primarily covering human rights (The Council of Eu-\nrope 2018; Mantelero 2022; The Danish Institute for Human\nRights 2023) and ethics (Gebru et al. 2021; The High-Level\nExpert Group on Artificial Intelligence 2020). The focus of\nAIIAs varies, assessing the impacts of AI models, systems,\nor services, typically mentioning use cases only briefly in\nfinal reports. However, there is growing agreement that im-\npact assessments should be conducted on specific uses (Eu-\nropean Comission 2024; Hupont et al. 2024). In the EU AI\nAct (European Comission 2024), the specific use determines\none of four risk categories (unacceptable, high, limited, min-\nimal), each with its own legal requirements. Similarly, in the\nNIST framework (National Institute of Standards and Tech-\nnology 2023a), use-case profiles are required to describe the\ncurrent state and the desired state of the system, allowing for\nrisk management at different stages of the AI lifecycle."}, {"title": "Collaborative Design of an Impact Assessment\nReport Template", "content": "To achieve consensus on AIIAs, their report templates\nshould adequately document AI uses, reflect the socio-\ntechnical nature of their impacts (Metcalf et al. 2021; Na-\ntional Institute of Standards and Technology 2023b), and\naccommodate different roles (e.g., developers, researchers,\nmanagers, compliance experts) to surface these impacts\n(Moss et al. 2021; Selbst 2021; The High-Level Expert\nGroup on Artificial Intelligence 2020; Constantinides et al.\n2024a). However, existing AIIAs often lack publicly avail-\nable documentation on their design processes (Johnson and\nHeidari 2023), raising concerns about whether the current\nreports' templates meet the diverse needs of stakeholders.\nWhen available, the documentation typically only speci-\nfies the nature of the consultations with stakeholders, lacking\ndepth about the types of stakeholders involved and how their\ninput influenced the final design. For example, Microsoft's\nResponsible AI Impact Assessment Template was developed\nfollowing discussions with internal collaborators (Microsoft\n2022b), while the Canadian ADM template (The Govern-\nment of Canada 2023) was revised through consultations\nwith relevant stakeholders. In contrast, the Ada Lovelace In-\nstitute's Algorithmic Impact Assessment template in health-\ncare was developed through interviews with potential users\nfrom companies, researchers, and impact assessment experts\n(Ada Lovelace Institute 2022).\nResearchers highlight the need for more inclusive, value-\nsensitive co-design processes (Johnson and Heidari 2023;\nSadek et al. 2024) that explicitly incorporate compliance"}, {"title": "Methods", "content": "To design a comprehensive template for an impact assess-\nment report that is grounded in regulations, we conducted a\nseries of three studies (Figure 1). These studies included lit-\nerature review and interviews with 2 AI compliance experts,\nan iterative co-design process with 14 AI practitioners and 6\nAI compliance experts, and a user study to evaluate the tem-\nplate with 8 AI practitioners working in industry research\nand 5 AI compliance experts from industry and academia.\nAll sessions were conducted via video conferencing over\nthree months, and were recorded and transcribed. We en-\nsured anonymity of data by excluding personal identifiers,\nand maintained exclusive access to the data for the research\nteam only. All sessions were approved by our organization."}, {"title": "Eliciting Design Requirements From Literature\nReview and Semi-structured Interviews With AI\nCompliance Experts", "content": "To elicit requirements for designing a comprehensive tem-\nplate for an impact assessment report that is grounded in\nregulations, we resorted to previous literature and did so in\ntwo steps. First, we reviewed prior academic literature inves-\ntigating the use of risk assessments, impact reports, and AI\ndocumentation for compliance (Skoric 2023; Selbst 2021;\nGaumond and R\u00e9gis 2023; National Institute of Standards\nand Technology 2023a). We also drew upon five existing\nAI impact assessment templates (Microsoft 2022b; Fujitsu\nResearch Center for AI Ethics 2022; National Institute of\nStandards and Technology 2023b; Ada Lovelace Institute\n2022; The Government of Canada 2023) and value-sensitive\ngames and cards (Ballard, Chappell, and Kennedy 2019;"}, {"title": "Co-designing the Impact Assessment Report\nTemplate With AI Practitioners and AI\nCompliance Experts Through an Iterative Process", "content": "After designing the first version of the template, we con-\nducted a series of individual, 30-minute co-design sessions\nwith 14 AI practitioners and 6 AI compliance experts (Fig-\nure 2). These sessions aimed to ensure that the template pro-\nvided sufficient information for impact assessment accord-\ning to the EU AI Act, NIST's AI RMF, and ISO 42001, and\nwas usable by various roles in the assessment process.\nParticipants. We aimed to achieve a diverse participant\nsample using snowball sampling, a method where existing\nstudy participants recruit future participants from among\ntheir acquaintances. We began by identifying 6 initial par-\nticipants through an internal mailing list at a large tech com-\npany, asking for individuals who were familiar with the EU\nAI Act and had at least one ongoing AI project. These partic-\nipants were then asked to refer additional participants from\ntheir networks, thus expanding the sample size through suc-\ncessive referrals. We recruited a total of 20 participants (13\nmale, 7 female, with a median age of 34 years old) represent-\ning a variety of AI practitioners such as researchers (9), en-\ngineers (2), managers (3), designers (1), and AI compliance\nexperts (6), and potential end users of the template. Their\nexpertise span across various areas including generative AI,\nmachine learning, deep learning, computer vision, AI stan-\ndardization, and human rights. Additionally, each participant\nwas actively involved in developing or evaluating at least\none ongoing AI project during the time of the interviews. AI"}, {"title": "Evaluation of Impact Assessment Report\nTemplate", "content": "To then evaluate the final template, we first populated it with\na real-world use of an AI system that was developed in the\nsame large tech company and then conducted a user study to\nevaluate the populated report produced for this system's use\nwith 8 AI practitioners and 5 AI compliance experts."}, {"title": "Populating the Impact Assessment Report Template.", "content": "To populate the template, we employed a three-step semi-\nautomatic method, which included soliciting AI practition-\ners responses and reviewing these responses with AI com-\npliance experts.\nIn the first step, we compiled a bank of statements to\ngather responses from AI practitioners. To do so, we sourced\nstatements from responsible AI guidelines (The High-Level\nExpert Group on Artificial Intelligence 2020; Constantinides\net al. 2024a), documentation standards (Selbst 2021; Ge-\nbru et al. 2021; Holland et al. 2020; Bender and Fried-\nman 2018; Mitchell et al. 2019; Sokol and Flach 2020; Raji\net al. 2020), checklists and impact assessment questionnaires\n(Madaio et al. 2020; Golpayegani, Pandit, and Lewis 2023;\nSkoric 2023; National Institute of Standards and Technology\n2023b). Next, we reviewed these statements, linking them to\nthe relevant excerpts from the EU AI Act, the NIST AI RMF,\nand the ISO 42001, and grouping similar ones together. This\nprocess resulted in a list of 32 statements grounded in regula-\ntions and best responsible AI practices, designed to system-\natically gather information about the system's use, compo-\nnents, and data, team involvement, and the associated risks,\nmitigations, and benefits (Table 3, Appendix C). They also\ndirectly map to the sections of our template.\nIn the second step, we reached out to 2 AI practitioners\u2014\na researcher and a designer-who had contributed to the\ndevelopment of an AI-based meeting companion app. We\nasked them to provide responses to these 32 statements.\nIn the third step, two authors manually parsed the practi-\ntioners' responses, placing them in the template to complete\nthe report (Figure 3). We then consulted 2 AI compliance"}, {"title": "Conducting a User Study for Evaluating the Populated\nImpact Assessment Report.", "content": "We conducted a user study\nin the form of semi-structured 30-minute interviews with\n8 AI practitioners from the same large tech company and\n5 AI compliance experts from both industry and academia.\nSpecifically, our evaluation ought to answer four questions:"}, {"title": "Quantitative results.", "content": "Participants found that the final re-\nport provided more complete information for conducting im-\npact assessments than the baseline across all three frame-\nworks (Figure 4, R1). The report received the highest rat-\nings for alignment with the NIST AI RMF and ISO 42001,\nand the lowest for the EU AI Act. Participants also found\nthat the final report provided a broader scope in addressing\nall AI system components and the impacts of their use com-\npared to the baseline (Figure 4, R2). In terms of adaptability\nto different uses (Figure 4, R3), participants found the final\nreport slightly more applicable to a wide range of AI sys-\ntems than the baseline. Regarding adaptability to different\nroles (Figure 4, R4), all participants rated both the final tem-\nplate and the baseline as very straightforward and intuitive\nto complete, easy to navigate with clear headings and sub-\nheadings, and suitable for users with varying roles."}, {"title": "Qualitative results.", "content": "Al practitioners found the final tem-\nplate and its sections helpful. For example, P6 stated that \"I\nreally like it [the template] covers various aspects of the sys-\ntem like its components and data\u201d. Practitioners also praised\nthe template's simplicity. P3 stressed they \"like its simplic-\nity as it helps me deal with the complexity of the AI system\",\nfor example by scoping well the intended use of the system,\nwhich P7 summarized as ", "these risks are citing\nthe specific sections of the Act [...] It's definitely enough in-\nformation to convince me that it [the meeting companion]\nis high risk\u201d. This was more evident when our participants\ncommented about the risks section of the report. Specifi-\ncally, P1 mentioned that ": "he risk section is a good start\nfor understanding how it [the meeting companion] can be\nmisused", "it is\ns very much about discovering risks and I strongly believe\nthat we should use automated tools and GenAI to aid in that\nprocess to prompt with the right kind of responses\u201d.\nAI compliance experts have confirmed that the final tem-\nplate effectively addresses two aspects of integrating and\nmanaging AI systems within organizational processes. First,\nthe final template promotes the integration of AI-driven pro-\ncesses with existing, well-established organizational proce-\ndures. As stated by E13, ": "he approach with the evalua-\ntion during development, deployment and use is quite famil-\niar to anyone really involved in product development. This\nfeedback must have been given by developers and I agree\ncompleting the template should not be an additional bur-\ndensome process, but something that is integrated into what\nwe are already used to doing", "I see your inten-\ntion not to focus on only one risk taxonomy. If you want to\nmake sure that you are covering everything, it is very good\nto start with one taxonomy and blend in taxonomies from\nother frameworks. There are no clear boundaries between\nrisks; they overlap and create different categories across\nframeworks. In reality, these frameworks interact and mix to\nsome extent.\" To see how, E12 explained how the same term\n\"systemic risk": "as multiple meaning based on the regula-\ntion:", "society": "nBoth AI practitioners and AI compliance experts stressed\nthe importance of the inclusion of expert oversight to el-\nevate the quality and reliability of the assessments carried\nout jointly by these stakeholders. E9, AI compliance expert,\nechoed AI practitioners' perspectives: \u201cthe biggest chal-\nlenge is ensuring teams have the necessary skills and knowl-\nedge to complete the assessment. Good written guidelines\nand preferably some expert guidance and oversight would\nlead to better results", "suits all\nroles and summarizes the design decisions made so far. It\nhas a different structure than typical compliance reports,\nwhere each section has a topic-specific focus and is relevant\nonly for certain experts": "E12, a compliance expert, sug-\ngested that", "changes": "To illustrate this point, they gave the ex-\nample of the national system that evaluates the chances of\nspecific groups in the labor market, which was not usable\nduring the COVID-19 pandemic, and for which similar risks\nhad not been foreseen in the limitations of the system."}, {"title": "Discussion", "content": "We contextualize our template within prior literature, then\ndiscuss its implications, limitations, and future research."}, {"title": "Differences with Existing Templates", "content": "We compared our template with the Algorithmic Impact As-\nsessment template from NIST (National Institute of Stan-\ndards and Technology 2023b), Microsoft's Responsible AI\nImpact Assessment Template (Microsoft 2022b), Credo AI's\nStandardized Risk Profile (Sherman and Eisenberg 2024),\nand the Algorithmic Impact Assessment from the Ada\nLovelace Institute (Ada Lovelace Institute 2022), and iden-\ntified four key differences. First, our template enhances\ncross-company comparisons by aligning closely with legis-\nlation rather than relying on framework-specific biases or\ncompany-specific risks. Second, unlike existing templates\nwhich focus heavily on risks, ours equally emphasizes both\nrisks and benefits. Third, there is variation in the extent\nof guidance provided for completing the templates. NIST's\n(National Institute of Standards and Technology 2023b), Mi-\ncrosoft's (Microsoft 2022a), and the Ada Lovelace Insti-\ntute's (Ada Lovelace Institute 2022) offer guidebooks on\nhow to produce a report. Rather than leaving stakehold-\ners with a blank report and a guide, we offer a guidebook\nthrough our 32 simple statements that alleviate potential\nanxiety associated with an empty page and facilitate the\nidentification of risks and benefits. Finally, whereas other\ntemplates have separate sections for legal and compliance\nissues, ours integrates these aspects throughout."}, {"title": "Theoretical and Practical Implications", "content": "Embedding AI governance in impact assessment reports.\nWe introduced a standardized template that not only facil-\nitates risk assessment in accordance with the EU AI Act,\nthe NIST's AI RMF, the ISO 42001 but also serves as a\nmodel for integrating regulatory considerations into AIIAs.\nOur work contributes to co-designing and validating im-\nproved AIIAs and the processes and workflows surround-\ning them (Skoric 2023). This includes developing new tools\nand methodologies that are practical and adaptable to en-\nhance the quality and effectiveness of impact assessments in\nthe AI context. They provide a comprehensive and practical\nframework for companies to navigate the complexities of AI\nregulation, ensuring both compliance and ethical responsi-\nbility in AI development and implementation.\nFacilitating contextual evaluation. Finding the right bal-\nance between making impact assessment template general\nenough to apply to various AI systems and specific enough\nto provide meaningful assessments for each unique system's\nuse is a significant challenge (Stahl et al. 2023). We partly\naddressed this challenge by adding specific subsections into\nthe template. These subsections cover the five components\ncomprehensively describing the system's use, the three key\nstakeholders, and the three stages of the system's lifecycle.\nBy doing so, we assist stakeholders in systematically doc-\numenting the intended use for which the system was built,\nensuring that it meets performance and safety criteria across\ndifferent situations (Johnson and Heidari 2023)."}, {"title": "Improving stakeholder engagement.", "content": "Our report template\ncan be utilized by individual team members or as a group.\nFor teams that are new to impact assessments, working\nthrough this framework can be an educational experience\nthat helps in building understanding and skills related to\nresponsible AI practices. Future work on improving stake-\nholder engagement should explore alternative interactive\ntools that effectively balance various methods of eliciting\nsystem information and envisioning system impacts, such as\ndivergent and convergent thinking styles (Selbst 2021)."}, {"title": "Limitations and Future Work", "content": "Generalizability. Our results are based on a pool of study\nparticipants who were familiar with the EU AI Act, the NIST\nframework and ISO standard. Including a broader range\nof roles beyond managers, designers, and researchers may\nyield different results. Future studies should include partic-\nipants with different levels of knowledge about AI regula-\ntions. They should also evaluate how well impact assessment\nreports explain the risks and benefits of AI to everyone and\nfind ways to help people understand important AI regula-\ntions and laws.\nPropagating biases in impact assessment. AIIAs, like\nother responsible AI tools, have inherent biases from their\ndesign choices (e.g., excluding potential users (Moss et al.\n2021)). The quality of our report template depends on the ac-\ncuracy and completeness of user-provided information. Us-\nsing biased or incomplete data can lead to incorrect assess-\nments, making problems seem smaller than they really are\nbecause people might be afraid to report negative impacts.\nTo fix this, future research should include external perspec-\ntives by holding workshops, involving independent experts\nand marginalized groups, and regularly checking the results\nwith new team members (Raji et al. 2020; Ada Lovelace In-\nstitute 2022).\nAutomated tools pre-populating the template. Automated\ntools can help make gathering data for templates easier and\nless prone to mistakes. Large Language Models (LLMs)\ncan help fill out impact assessment reports by generating\nlists of AI users and subjects (Bu\u00e7inca et al. 2023), iden-\ntifying intended and unintended uses (Wang et al. 2024;\nHerdel et al. 2024), and listing potential risks and benefits\n(Constantinides et al. 2024b; De Miguel Velazquez et al.\n2024). Bogucka et al. (2024) have recently proposed a semi-\nautomatic system that collects input from stakeholders about\nan Al system's use, uses LLMs to find additional risks, mit-\nigation strategies, and benefits, and pre-fills reports for ex-\nperts to review.\nResponsible by Design. AIIAs are usually conducted at the\npre-deployment stage for legal compliance. However, after\nour co-design process and user studies which began with\ncompliance as a focal point, we learned that reports should\nbe updated not only when lifecycle stages change (e.g., from\ndesign to development), but also when the socio-technical\ncontext shifts. This underscores the challenge of balancing a\nfixed impact assessment template with the significant learn-\ning benefits users gain from engaging with it to \"think differ-\nently\" about AI. Without clear updates, users may lack the"}, {"title": "Conclusion", "content": "We developed an impact assessment template with input\nfrom 16 AI practitioners and 6 compliance experts, designed\nto align with standards such as the EU AI Act, NIST AI\nRMF, and ISO 42001. A user study involving other 8 com-\npany AI practitioners and 5 compliance experts confirmed\nthat our template effectively captures AI system impacts,\nserving as a starting point for navigating regulatory com-\npliance and fostering responsible design."}]}