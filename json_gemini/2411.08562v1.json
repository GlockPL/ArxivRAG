{"title": "NEURAL CORRECTIVE MACHINE UNRANKING", "authors": ["Jingrui Hou", "Axel Finke", "Georgina Cosma"], "abstract": "Machine unlearning in neural information retrieval (IR) systems requires removing specific data\nwhilst maintaining model performance. Applying existing machine unlearning methods to IR may\ncompromise retrieval effectiveness or inadvertently expose unlearning actions due to the removal of\nparticular items from the retrieved results presented to users. We formalise corrective unranking,\nwhich extends machine unlearning in (neural) IR context by integrating substitute documents to\npreserve ranking integrity, and propose a novel teacher-student framework, Corrective unRanking\nDistillation (CuRD), for this task. CuRD (1) facilitates forgetting by adjusting the (trained) neural IR\nmodel such that its output relevance scores of to-be-forgotten samples mimic those of low-ranking,\nnon-retrievable samples; (2) enables correction by fine-tuning the relevance scores for the substitute\nsamples to match those of corresponding to-be-forgotten samples closely; (3) seeks to preserve\nperformance on samples that are not targeted for forgetting. We evaluate CuRD on four neural IR\nmodels (BERTcat, BERTdot, ColBERT, PARADE) using MS MARCO and TREC CAR datasets.\nExperiments with forget set sizes from 1% and 20% of the training dataset demonstrate that CuRD\noutperforms seven state-of-the-art baselines in terms of forgetting and correction while maintaining\nmodel retention and generalisation capabilities.", "sections": [{"title": "1 Introduction", "content": "Machine unlearning refers to removing specific data from a trained machine-learning model while retaining its overall\nperformance (Bourtoule et al., 2021; Xu et al., 2023). Machine unlearning presents unique challenges in the domain\nof information retrieval (IR) (Zhang et al., 2022), and the application of unlearning to IR is an area that is still largely\nunder-explored.\nIn neural IR models, unlearning may entail document removal, i.e., making the model selectively forget certain re-\ntrieved documents; or query removal, i.e., erasing specific query knowledge from the model. Examples of these tasks\nare depicted in Figure 1. Removing queries or documents from a neural IR model can provoke unintended conse-\nquences. As shown in Figure 1, leaving positions vacated by such removals unfilled might lead users to suspect\nintentional deletion, potentially triggering a 'Streisand effect' (Chundawat et al., 2023) which refers to the idea that\nattempts to hide information paradoxically increase attention to it. Conversely, filling the vacated positions with subse-\nquent documents based on relevance ranking could potentially insert less relevant content, which causes performance"}, {"title": "2 Related works", "content": "This work focuses on corrective unlearning in information retrieval, which is still largely underexplored. However,\nexisting machine unlearning and corrective unlearning studies could provide insights into this work.\n2.1 General machine unlearning\nMachine unlearning seeks to remove specific data points from a trained model, often motivated by concerns over data\nprivacy (Bourtoule et al., 2021; Xu et al., 2023) or the presence of poisoned data used in backdoor attacks (Li et al.,\n2024; Zhong et al., 2024). A straightforward way for machine unlearning is retraining models from scratch, but this\nis often prohibitively costly in terms of computational resources and time (Wu et al., 2020). In response, researchers\nhave developed alternative methods.\n2.1.1 Unlearning via influence function and gradient operation\nInfluence function-based methods adjust model parameters to negate the effect of the to-be-forgotten data (Koh and\nLiang, 2017; Giordano et al., 2019). In linear models, methods like Newton's method (Guo et al., 2020) reduce data\npoint influence, while projective residual updates (Izzo et al., 2021) offer fast parameter adjustments for small groups\nof data. For strongly convex models, closed-form updates translate data changes into parameter adjustments (War-\nnecke et al., 2023). Hessian-based Newton methods are resource-intensive, prompting alternatives like the Fisher\nInformation Matrix (FIM) (Martens, 2020) and Neural Tangent Kernel (Golatkar et al., 2020). Expanding on FIM,\nFoster et al. (2024) designed a method to induce forgetting by dampening parameters proportionally to their impor-\ntance relative to the forget set. Other common approaches are gradient-based unlearning methods such as Negative\nGradient (NegGrad) (Zhang et al., 2022; Tarun et al., 2024) and Amnesiac unlearning (Graves et al., 2021) reverse\nparameter updates tied to specific data batches, while DeltaGrad (Wu et al., 2020) allows efficient retraining for small\ndata changes. Forsaken (Ma et al., 2023) employs selective neuron adjustments to control unlearning precision.\n2.1.2 Unlearning via knowledge distillation\nAnother prevalent method is knowledge distillation-based unlearning, to which the approach proposed in this work\nalso belongs. Knowledge distillation typically involves transferring knowledge from a large, complex teacher model to\na smaller, more efficient student model (Wang and Yoon, 2021; Gou et al., 2021). However, in knowledge distillation-\nbased unlearning, the student model is trained to emulate the teacher model while intentionally omitting the informa-\ntion designated for forgetting. Kim and Woo (2022) introduced a two-stage unlearning process: the first stage involves\ntraining a deep learning model that loses specific information using contrastive labels from the requested dataset; the\nsecond stage retrains this model with knowledge distillation to recover its overall performance. To enable data-free\nunlearning, Chundawat et al. (2023) proposed a zero-shot machine unlearning method that utilises error-minimising\nnoise and gated knowledge transfer within a teacher-student learning framework. Chundawat et al. (2023) developed a\ndual-teacher strategy that utilises misinformation from an incompetent teacher to induce forgetting while a competent\nteacher helps retain the model's performance. Additionally, Kurmanji et al. (2023) proposed unbounded unlearning in\nwhich the student model selectively retains or discards information based on its alignment with or deviation from the\nteacher model.\nAs is supported by the empirical evaluation in this paper, both influence functions and gradient-based methods fail to\ndeliver satisfactory unlearning results in the context of neural IR. Additionally, most existing knowledge distillation\nmethods are primarily designed for classification tasks and are incompatible with the neural IR setting Hou et al.\n(2024).\n2.2 Machine unlearning in information retrieval\nTo our knowledge, Zhang et al. (2022) were the first to propose a machine unlearning method tailored specifically\nto the task of image retrieval. However, their approach only considers the removal of to-be-ranked items, neglecting\nthe unlearning of queries. In response, Hou et al. (2024) formalised machine unlearning within the broader context\nof IR, addressing both query and document (to-be-ranked items); Hou et al. (2024) also introduced a model-agnostic\nunlearning technique for neural IR. However, despite its versatility, the performance of their method is sensitive to the\nchoice of certain tuning parameters.\n2.3 Corrective unlearning\nRecent discussions among researchers (Warnecke et al., 2023; Xu et al., 2024) suggest that machine unlearning should\nnot only focus on removing specific samples but also involve corrections within the training data. However, unlearning\nmethods that incorporate corrections have received relatively little attention. Cao et al. (2018) pioneered the concept\nof causal unlearning, which identifies and eliminates the sources of misclassification within the training dataset in an\niterative manner, thereby correcting polluted machine learning systems. Recently, Goel et al. (2024) have introduced\ncorrective machine unlearning to tackle the challenges presented by data corrupted through unknown manipulations\nin trained models. A key feature of this method is the evaluation of correction accuracy on the manipulated training\ndataset, which serves to assess the efficacy of the unlearning process in rectifying data corruption. However, the\nmethodologies proposed by Cao et al. (2018) and Goel et al. (2024) focus exclusively on classification tasks which are\nnot compatible with neural IR systems. This necessitates the development of specialised formalisation and evaluation\nmetrics tailored for the corrective unlearning in neural IR contexts.\n2.4 Corrective unlearning versus knowledge distillation\nCorrective unlearning is akin to knowledge editing (De Cao et al., 2021; Wang et al., 2024), which primarily targets text\ngeneration tasks and typically employs models built on transformer architectures. However, in the field of IR, despite\nthe rise of generative retrieval methods (Li et al., 2024; Gienapp et al., 2024), the dominant approach remains the\nlearning-to-rank architecture (Zhao et al., 2024). This prevalent architecture in IR is neither generative nor inherently\nbased on transformers, highlighting a distinct divergence from the methodologies applied in corrective unlearning and\nknowledge distillation."}, {"title": "3 Problem definition", "content": "3.1 Neural information retrieval\nLet Q denote the set of all possible queries and D the set of all possible documents. Define S as the universe of\npossible datasets. A dataset for (neural) IR, S \u2208 S, then consists of tuples (x, y), where:\n\u2022\n\u2022 x = (q, d) \u2208 Q \u00d7 D represents a query-document pair;\n\u2022 y \u2208 {+, -} is the ground-truth relevance label for (q, d), where '+' indicates the document is relevant to the\nquery, and -' indicates the document is irrelevant. Note that ordinal relevance labels such as 'highly relevant'\nor 'partially relevant' can always be reduced to binary labels by applying an appropriate threshold.\nHereafter, we assume that every query-document pair in the dataset is unique, i.e. that each query-document pair\n(q, d) is associated with a unique relevance label yq,d \u2208 {+, -} in the dataset; and we introduce the following notation.\nThe set of all queries contained in the training dataset is\n$Q := \\{q' \\in Q |\\exists((q, d), y) \\in S : q' = q\\}.$\nThe set of all documents contained in the training dataset is\n$D := \\bigcup_{q\\in Q} D_q,$\nwhere, for each q \u2208 Q,\n$D_q := \\{d \\in D | \\exists (\\langle q', d'\\rangle, y) \\in S : \\langle q', d'\\rangle = (q,d)\\}$\nis the set of documents for which the training dataset contains relevance labels w.r.t. Query q, i.e., these are the 'to-be-\nranked' documents for q; and this set can be further partitioned into the subset of positive (relevant) documents, $D_q^+ :=$\n$\\left\\{d \\in D_q | Y_{q,d} = +\\right\\}$, and negative (irrelevant) documents, $D_q^- := \\left\\{d \\in D_q | Y_{q,d} = -\\right\\}$, for Query q. Throughout this\nwork, we will assume that D and D\u2081 are non-empty for any q \u2208 Q. With this notation and under the uniqueness\nassumption mentioned above, we may alternatively express the training dataset as:\n$S = \\{\\langle\\langle q, d\\rangle, y_{q,d}\\rangle | q \\in Q, d \\in D_q\\}.$\nLet WC RP be the parameter space, i.e., w \u2208 W is the collection of parameters (e.g., weights) of a neural ranking\nmodel. For brevity, we will refer to the elements w \u2208 W themselves as (neural ranking) models. A neural ranking\nmodel w \u2208 W is trained to predict a relevance score fu(x) \u2208 R for a query-document pair x = (q, d). The relevance"}, {"title": "3.2 Machine unlearning in information retrieval", "content": "Unlearning in IR falls into two categories:\n\u2022 Document removal, depicted on the left-hand side of Figure 1, seeks to ensure that documents in some set\nDf C D are no longer considered to be relevant for any query. In this case, we may define a forget set as\n$F := \\{\\langle\\langle q, d\\rangle, y\\rangle \\in S | d \\in D_f \\}.$\n\u2022 Query removal, depicted on the right-hand side of Figure 1, seeks to ensure that queries in some set Qf \u2286 Q\nno longer have their positive (i.e., relevant) documents retrieved. In this case, the forget set is\n$F := \\{\\langle\\langle q, d\\rangle, y\\rangle \\in S | q \\in Q_f  \\text{and}  y = +\\}.$\nNote that query and document removal may occur simultaneously. In this case, the resulting forget set is the union of\nthe individual forget sets.\nWe may then define a retain set as R := S \\ F. Within this work, a machine unlearning model should replicate the\nperformance of the retrained model, i.e., of the model which is retrained from scratch on R, while simultaneously\nlowering the relevance scores of any query-document pair in the forget set, i.e., for (\u3008q, d), y) \u2208 F. This ensures that\nd is not retrieved in response to q. However, as illustrated in Figure 1, simply omitting certain retrievals-whether\nby leaving top-ranked positions empty or by skipping to lower-ranked documents\u2014can introduce issues such as the\n'Streisand effect' or misinformation.\nTo address these concerns, we propose an alternative approach: substituting to-be-forgotten documents with suitable\nalternative documents."}, {"title": "3.3 Corrective unranking", "content": "As discussed and illustrated in Figure 1, directly removing retrieved items is not a robust, necessitating the substitution\nof documents at the positions from which documents are removed, whether for query or document removal. We term\nthis approach neural corrective machine unranking, or simply corrective unranking.\nBoth document removal and query removal involve ensuring that certain documents are not retrieved in response to\ncertain queries. In a list-wise manner, we define the set of documents to be corrected for a query q as:\n$D_q^\\ddagger := \\{d \\in D_q | \\exists (\\langle q', d'\\rangle, y') \\in F : \\langle q, d\\rangle = \\langle q',d'\\rangle\\}.$\nLet ra: Df \u2192 D \\ Df be the function mapping each document d\u2208 Df to a suitable substitute document rq(d) \u2208\nD \\ Df. To simplify the notation, we also define r((q, d)) := (q, rq(d)) to be the \u2018corrected' query-document pair\nwhich results from replacing the document d in the query-document pair (q, d) by its substitute document.\nWith this notation, we can define a substitute set as:\n$F^* := \\{\\langle\\langle q, r_q(d)\\rangle, y\\rangle | \\langle\\langle q, d\\rangle, y\\rangle \\in F\\} = \\{\\langle r(x), y\\rangle | \\langle x, y\\rangle \\in F\\}.$\nThe objective of corrective unranking is then formulated as:\n$M_{\\text{correct}} := \\arg \\min_{w \\in W} L_{S^*}(w),$\nwhere w is initialised from Mtrain and S* := F* U R is the modified training dataset in which all to-be-forgotten\ndocuments have been replaced with suitable substitutes, unless they were already considered irrelevant (negative) for\na particular query. Note that the relevance labels remain unchanged."}, {"title": "4 Proposed method for corrective unranking", "content": "4.1 Overview\nAchieving Objective (10) by retraining from scratch on S* is typically prohibitively expensive. Instead, in this work,\nwe approximate (10) via distillation strategy based on relevance score calibration. This strategy is compatible with\nvarious neural ranking models trained on different ranking losses.\nInformally, our strategy consists of gradient steps which seek to train the student model w to achieve the following\nthree goals (initialised from w = Mtrain).\n1. Forgetting. For each ((q, d), y) \u2208 F we wish to reduce the relevance score produced by w far enough such\nthat d is no longer retrieved in response to Query q. To achieve this, our updates seek to train w to satisfy\n$f_w((q, d)) \\approx qtl_{\\gamma}^{M_{\\text{train}}}(q; D^-),$\nfor any ((q, d), y) \u2208 F, where\n$qtl_{\\gamma}(q; D) := quantile^{\\gamma}(\\{f_M((q, d^-)) | d^- \\in D^- \\}),$\ndenotes the y-quantile (\u03b3\u2208 [0, 1]) of the relevance scores associated with the full set of negative documents\nfor Query q under model M.\n2. Correcting. For each to-be-forgotten document, we wish to introduce the substitute document into the vacated\nposition in the retrieved list to correct the ranking. That is, for each (x, y) \u2208 F, our updates seek to train w to\nsatisfy\n$f_w(r(x)) \\approx f_{M_{\\text{train}}}(x).$\n3. Retaining. The relevance scores of all query-document pairs in the retain set should remain roughly un-\nchanged. Therefore, for any (x, y) \u2208 R, our updates seek to train w to satisfy\n$f_w(x) \\approx f_{M_{\\text{train}}}(x).$\nWe refer to this distillation strategy as Corrective unRanking Distillation (CuRD). Figure 2 illustrates this approach.\n4.2 Corrective unranking distillation\nWe now describe how CuRD can be implemented in practice. We employ a teacher-student framework, where M =\nMtrain serves as the teacher model and w as the student model. This implementation comprises two components: one"}, {"title": "5 Experimental Setup", "content": "5.1 Datasets\nWe use two IR datasets: MS MARCO (Craswell et al., 2021) and TREC CAR (Dietz and Foley, 2019). To adapt these\ndatasets for corrective unranking, we made the following adjustments:\n\u2022 For each query (in both the training and test datasets), the ratio of positive to negative documents was set to\napproximately 1:100.\n\u2022 In the training dataset, we defined four forget sets based on the fraction of to-be-forgotten query-document\npairs among all query-positive document pairs: Forget-1%, Forget-5%, Forget-10%, and Forget-20%.\n\u2022 The samples in all four forget sets were randomly selected. Within each forget set, we randomly chose\nsome queries for query removal and selected documents (each relevant to at least one query) for document\nremoval. Within each forget set, the number of to-be-forgotten pairs for query removal and document removal\nwas roughly balanced. The remaining pairs were used as the retain set.\n\u2022 For each to-be-forgotten query-document pair in the forget set, a substitute document was randomly assigned\nfrom the document set, excluding the positive documents for that query.\n5.2 Neural ranking models\nNeural ranking models based on pretrained language models (PLMs) have demonstrated superior retrieval performance\ncompared to pre-BERT neural information retrieval (NIR) approaches (Zhao et al., 2024). Therefore, this work focuses\non PLMs-based neural ranking models. Four models were selected for evaluation:\n\u2022 BERT for concatenated query-document scoring (BERTcat) (Hofst\u00e4tter et al., 2021).\n\u2022 BERT with Dot Production for separated query-document scoring (BERTdot) (Hofst\u00e4tter et al., 2021).\n\u2022 Contextualised Late Interaction over BERT (ColBERT) (Khattab and Zaharia, 2020).\n\u2022 Passage representation aggregation for document reranking (PARADE) (Li et al., 2023).\nAmong these models, BERTcat employs a cross-encoder architecture, where a query and a document are concatenated\nand separated by the \"[SEP]\" token. The remaining three models use a bi-encoder architecture, in which queries and\ndocuments are encoded separately.\n5.3 Evaluation metrics\nThe task of corrective unranking extends neural machine unranking by requiring the provision of suitable substitutes\nduring data removal. To evaluate corrective unranking models, we focus on three key aspects: correcting, forgetting,\nand retaining (on both training and test datasets). In what follows, for some set of documents D and some given model"}, {"title": "6 Results and discussion", "content": "6.1 Corrective unranking performance evaluation\nWe first fine-tuned each neural ranking model on each dataset, ensuring convergence, i.e., training the models until\ntheir performance stabilised, signifying that additional training iterations were unlikely to produce further enhance-\nments. Subsequently, corrective unranking was implemented using CuRD along with seven baseline methods. For\nperformance evaluation, we specifically utilised the Forget-10% configuration. We recall that the number of to-be-\nforgotten pairs designated for query removal and document removal within this set is approximately balanced.\n6.1.1 Performance comparison\nThe experimental results are shown in Figure 3 and Figure 4. For clarity, we visualised the maximum-minimum\nintervals for each method across the four neural ranking models on both datasets. Detailed results are provided in the\nAppendix.\nForgetting and correcting performance. Figures 3a and 3b illustrate the forgetting and correction performance of\nvarious corrective unranking methods on query and document removal tasks, respectively. Among all methods, Am-\nnesiac achieved the most significant forgetting performance, reflected by the lowest performance scores (i.e., Pforget)\non the forget set: 0.02 on average for query removal and 0.01 for document removal. CuRD followed with average\nscores of 0.07 for query removal and 0.04 for document removal. Other methods exhibited larger average forget-\nting performance scores and broader minimum-maximum ranges. Despite its strong forgetting capability, Amnesiac\ndemonstrated poor correction performance, with high variability and average correction accuracy (i.e., Pcorrect) of 0.73\nand 0.74 for query and document removal, respectively. In contrast, CuRD showed superior correction accuracy across\nall models and datasets, averaging 0.95 for query removal and 0.93 for document removal, surpassing even the Retrain\nand CF.\nRetention and generalisation ability. Figure 4 displays the performance on the retain set and test dataset. Retrain\nand CF, being fine-tuned on the retain set, excelled with Pretain close to 1. CuRD achieved similar performance, with\nan average Pretain of 0.98. Other baseline methods exhibited larger performance gaps, with wider spans between their\nmaximum and minimum Pretain. Figure 4b evaluates generalisation ability via the performance score on an unseen test\ndataset. CuRD performed comparably to Retrain (0.47) and CF (0.49), with an average score of 0.46 and a minimum\nabove 0.4, outperforming other baselines.\nIn summary, while Amnesiac excels in forgetting, it compromises retention and generalisation. Fine-tuning methods\nlike Retrain and CF are effective in retention and generalisation but less so in forgetting. CuRD balances strong\nforgetting with robust retention and generalisation, combining the strengths of both approaches.\n6.1.2 Unlearn time comparison\nAs per Hou et al. (2024), the normalised unlearn epoch duration as the average time per unlearning epoch for a model\n(w) divided by the average time per learning epoch for the same model when initially trained Mtrain. The total\nunlearn time is then determined by multiplying the normalised unlearn epoch duration by the total number of unlearn\nepochs required.\n6.2 Parameter sensitivity analysis\nThe robustness of CuRD against parameter changes is a key characteristic, primarily influenced by two parameters:\nthe subset size A in Equations (15) and (16), indicating the count of negative documents; and the quantile level y in\nEquation (15), which adjusts the relevance score reduction for a to-be-forgotten sample.\nWe assessed the sensitivity of CuRD by altering the size of A\u012b (with values 2, 5, 10, and 15) and y (at levels 0, 0.25,\n0.5, and 0.75, corresponding to minimum, lower quartile, median, and upper quartile). Our results, detailed in Table 1,\ndemonstrate that CuRD maintains consistent performance across varied settings, as evidenced by mean performance\nand its standard error over different datasets.\nSignificant findings include improved retain performance with higher set sizes of A\u012b, specifically 10 and 15, compared\nto 2 or 5. Furthermore, setting y to 0.75 resulted in a higher mean Pforget for the forget set and a decreased mean P correct\n6.3 Performance of CuRD with varying sizes of forget sets\nIn this section, we evaluate the performance of CuRD with varying forget set configurations: Forget-1%, Forget-5%,\nForget-10%, and Forget-20%. Performance metrics averaged across different data partitions, are detailed in Figure 6\nfor various unlearning epochs."}, {"title": "7 Conclusion", "content": "This work introduces the novel task of corrective unranking within neural IR systems, which not only involves for-\ngetting undesirable data but also correcting retrieval outputs. We propose CuRD, a new teacher-student framework\ndesigned to calibrate relevance scores for effective corrective unranking. Our experimental results show that CuRD\noutperforms state-of-the-art baseline methods, achieving superior forgetting and correction performance while main-\ntaining retention and generalisation abilities. Additionally, CuRD is both parameter-insensitive and adaptable to vary-ing sizes of the forget set, demonstrating robustness across different scenarios.\nIn summary, CuRD enables neural IR systems to effectively forget user data while minimising the risks associated\nwith data removal. CuRD has broad potential applications in various neural IR contexts, including academic search\nplatforms, digital archives, and news retrieval systems, where outdated information must be corrected while maintain-\ning search quality. Additionally, CuRD is valuable in e-commerce recommender systems by removing sensitive user\ninformation or erroneous product content without disrupting relevance.\nData availability\nTo reorganise the dataset and reproduce the experiments, please refer to the paper's GitHub repository located at\nhttps://github.com/JingruiHou/CuRD."}]}