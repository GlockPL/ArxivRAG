{"title": "EXIT: Context-Aware Extractive Compression for Enhancing\nRetrieval-Augmented Generation", "authors": ["Taeho Hwang", "Sukmin Cho", "Soyeong Jeong", "Hoyun Song", "Seung Yoon Han", "Jong C. Park"], "abstract": "We introduce EXIT, an extractive context\ncompression framework that enhances both\nthe effectiveness and efficiency of retrieval-\naugmented generation (RAG) in question an-\nswering (QA). Current RAG systems often\nstruggle when retrieval models fail to rank the\nmost relevant documents, leading to the inclu-\nsion of more context at the expense of latency\nand accuracy. While abstractive compression\nmethods can drastically reduce token counts,\ntheir token-by-token generation process signifi-\ncantly increases end-to-end latency. Conversely,\nexisting extractive methods reduce the latency\nbut rely on independent, non-adaptive sentence\nselection, failing to fully utilize contextual in-\nformation. EXIT addresses these limitations\nby classifying sentences from retrieved docu-\nments-while preserving their contextual de-\npendencies-enabling parallelizable, context-\naware extraction that adapts to query complex-\nity and retrieval quality. Our evaluations on\nboth single-hop and multi-hop QA tasks show\nthat EXIT consistently surpasses existing com-\npression methods and even uncompressed base-\nlines in QA accuracy, while also delivering sub-\nstantial reductions in inference time and token\ncount. By improving both effectiveness and ef-\nficiency, EXIT provides a promising direction\nfor developing scalable, high-quality QA solu-\ntions in RAG pipelines.", "sections": [{"title": "1 Introduction", "content": "Retrieval-Augmented Generation (RAG) (Lewis\net al., 2020; Khandelwal et al., 2020) is the task\nof enhancing Large Language Models (LLMs) re-\nsponses with relevant external contexts or docu-\nments. By grounding answers in evidence, RAG\nsystems have gained much attention for mitigating\nhallucination issues (Ram et al., 2023; Li et al.,\n2023b) and improving factual reliability (Jeong\net al., 2024; Xia et al., 2024b)."}, {"title": "2 Related Work", "content": "Retrieval-Augmented Generation. The RAG\npipeline typically follows a naive retrieve-then-\ngenerate process, where a single-step retrieval pre-\ncedes generation (Lewis et al., 2020; Shi et al.,\n2023b; Ram et al., 2023). However, a simple single-\nstep retrieval often fails to rank relevant docu-\nments at the top and struggles to handle multi-\nhop queries requiring multiple pieces of informa-\ntion. To address these, RAG pipelines have evolved\ninto iterative, recursive, and multi-hop retrieval ap-\nproaches (Shao et al., 2023; Trivedi et al., 2023;\nKhattab et al., 2022), which require multiple re-\ntrievals for a single query. While these methods im-\nprove information coverage, they also increase end-\nto-end latency from retrieval to generation, reduc-\ning the overall efficiency of the pipeline. Moreover,"}, {"title": "3 Method", "content": "In this section, we first present our problem formu-\nlation, the RAG pipeline with a compression stage,\nand our novel compression framework, EXIT,\nwhich is designed to extract key evidence for an-\nswering in a parallel manner."}, {"title": "3.1 Problem Formulation", "content": "RAG Pipeline with Compression. Given a query\nq and a document corpus C, a RAG pipeline first\nretrieves Top-k relevant document set D:\nD = {d1,...,dk} = Retriever(q, C), \n(1)\nThe retrieved documents within the document set\nD are then processed by a compression module that\npreserves query-relevant information while signifi-\ncantly reducing input length:\nD' = Compressor(q, D) s.t. l(D') \u00abl(D), \n(2)\nwhere l() represents the function calculating the\nnumber of tokens in the document set. After com-\npression, the number of tokens included in D' is\nsubstantially decreased compared to D. Finally, an\nLLM generates the answer a using the compressed\nset D' and the given query q:\na = LLM(q, D'). \n(3)\nObjectives of Compression. For effective and effi-\ncient compression in the RAG pipeline, three key\ncriteria should be satisfied: (1) D' should contain\nfewer tokens, as fewer tokens lead to shorter answer\ngeneration times (i.e., reading time); (2) D' should\nretain the essential evidence required to answer\nthe query, ensuring effectiveness (i.e., accurate an-\nswer); and (3) the compression process should be\nsufficiently fast enough to avoid significantly in-\ncreasing the overall end-to-end inference latency."}, {"title": "3.2 Extractive Context Compression (EXIT)", "content": "To achieve three objectives of the compression step,\nEXIT consists of three main components: sentence-\nlevel decomposition, context-aware relevance clas-\nsification, and document reassembly.\nSentence-Level Decomposition. In Step 1 of Fig-\nure 2, EXIT divides each retrieved document into\nindividual sentences using a rule-based sentence\ntokenizer. For each document di \u2208 D, we pro-\nduce a sentence set Si = {Si1, Si2, ..., Sin}, where\nsij is the j-th sentence in document i. Operating\nat the sentence level avoids the fragmentation of"}, {"title": "4", "content": "key phrases and preserves entity relationships that\ntoken-level compression techniques (Jiang et al.,\n2023) often disrupt. As a result, the compressed\ncontext preserves both syntactic coherence and se-\nmantic integrity, ensuring that key information is\neffectively retained.\nContext-Aware Relevance Classification. To ef-\nfectively and efficiently filter sentences in D that\ncontain key evidence for answering the question,\nwe design two key components for sentence rel-\nelevance evaluation: document consideration and\nsingle-token prediction. First, incorporating the\nentire document di is essential, as key evidence\nmay be distributed throughout the context rather\nthan confined to a single sentence, ensuring no\ncritical information is missed and enabling effec-\ntive context compression. Furthermore, as multiple-\ntoken generation proposed in the previous work\n(Yoon et al., 2024; Li et al., 2024) could compro-\nmise the overall efficiency of the RAG pipeline, we\ndesign the lightweight relevance calculation with\nonly single-token prediction with \u201cYes\u201d and \u201cNo\u201d\nfrom the query q, document di, and sentence sij.\nIn detail, for each candidate sentence sij, the eval-\nuation model calculates the relevance score rij of\nthe sentence sij for a given query q and document\ndi as follows:\nrij =\n    P(\"Yes\"|q, di, sij)\n    P(\"Yes\"|q, di, sij) + P(\u201cNo\u201d|q, di, sij)\n(4)\nwhere P(..) denotes the likelihood of each token\nfrom the evaluation model. This relevance calcu-\nlation is parallelized across multiple sentences, al-\nlowing them to be evaluated simultaneously.\nThen, among the sentences in D, EXIT selects\nsentences with a relevance score exceeding a pre-\ndefined threshold T, as high relevance scores indi-\ncate that a sentence contains critical information.\nNotably, this selection process results in an adap-\ntive number of sentences in the compressed set\nD', rather than a fixed amount. This adaptive ap-\nproach aligns with prior work (Jeong et al., 2024),\nrecognizing that the complexity of queries and the\namount of key information vary across queries. As\na result, our sentence selection strategy enables\neffective compression while ensuring all key evi-\ndence is included in the compressed set.\nDocument Reassembly. As shown in Step 3 of\nFigure 2, EXIT reconstructs the compressed doc-\nument D' by concatenating only the selected sen-\ntences in their original order. Following Hwang\net al. (2024), preserving the canonical sentence"}, {"title": "4 Experiment Setups", "content": "We conduct comprehensive experiments to evaluate\nEXIT's effectiveness and efficiency in context com-\npression for RAG systems. More implementation\ndetails of our experiments are in Appendix A.\nDatasets. We evaluate on both single-hop and\nmulti-hop question answering datasets: Natu-"}, {"title": "5 Main Results", "content": "Table 1 summarizes our evaluation results across\nmultiple datasets and compression strategies. With\nthe 8B reader, EXIT demonstrates strong general-\nization: although trained solely on HQA, it effec-\ntively addresses both single-hop (NQ, TQA) and\nmulti-hop (2WIKI) queries under out-of-domain\nconditions. Compared to all baseline methods,\nEXIT consistently improves EM scores\u2014for in-\nstance, by 1.3 and 2.0 points on NQ and TQA, and\nby even larger margins of 2.5 and 8.1 points on\nHQA and 2WIKI, respectively. Notably, these ac-\ncuracy gains come with an average latency of just\n0.8s, substantially faster than abstractive compres-\nsion approaches.\nThe benefits of EXIT become more pronounced\nat larger scales. Using the 70B reader, EXIT sur-\npasses the accuracy of all competing methods,\naveraging a 3.7-point improvement in EM and\na 3.3-point improvement in F1 over the uncom-\npressed baseline. On HQA, it achieves a 3.3-point\nEM gain while maintaining an efficient 3.5s la-\ntency-faster than using uncompressed documents\nand still competitive with the previously fastest\nmethod, RECOMP-Extr, but with significantly\nhigher accuracy. EXIT's effectiveness and effi-\nciency, especially with larger models, make it a\npractical solution for large-scale QA applications."}, {"title": "6 Analyses", "content": "We conduct a series of analyses examining EXIT's\nrobustness, classification performance, latency fac-\ntors, and design choices under various configura-\ntions. Additional experimental results and analyses\nare provided in Appendix B."}, {"title": "6.1 Robustness Analysis", "content": "To examine EXIT's robustness as the retrieval set\nsize grows, we gradually increased the number of\nretrieved documents (k \u2208 {1,5, 10, 20, 30}) with\nan 8B reader, as shown in Figure 3. We found that\nEXIT steadily improves EM scores\u2014from 28.2\npoints at k = 1 to 33.1 points at k = 30\u2014while\navoiding the performance degradation seen in RE-\nCOMP variants and Refiner at high k values. Also,\nwe measured the impact on the efficiency of the\nRAG pipeline with token counts and end-to-end\nlatency, confirming that EXIT significantly reduces\ncontext from 4,497.1 tokens to 594.4 tokens (86.8%\nfewer) at k =\n30, even improving the quality.\nAlso, EXIT's latency scales nearly linearly (0.48s\nto 2.71s) and is much faster than the abstraction\nmethods and the uncompressed baseline. These re-\nsults demonstrate that EXIT consistently delivers\nsignificant accuracy improvements with minimal\ninference costs, regardless of the number of doc-\numents, making it well-suited for tasks involving\nlarger retrieval sets."}, {"title": "6.2 Classification Performance", "content": "To better understand the effectiveness of our\ncontext-aware relevance classifier, we report row-\nnormalized confusion matrices for both in-domain\n(HQA) and out-of-domain (2WIKI) datasets, as\nshown in Figure 4. On HQA, the classifier dis-\nplays a perfectly balanced ability to recognize both\nrelevant (\"Yes\") and irrelevant (\u201cNo\u201d) sentences,\nachieving over 90% precision and recall in each cat-\negory. While, on the 2WIKI dataset, the classifier\nexhibits a slight drop in recall for \u201cYes\u201d sentences,\nit still performs strong classification ability with\nover 70% recall and 90% precision. These results\nconfirm that the classifier performs robustly in its\ntraining domain and generalizes reasonably well\nto unseen queries, yet we leave narrowing this dis-\ncrepancy as a valuable future research direction."}, {"title": "6.3 Understanding End-to-End Latency\nFactors", "content": "While previous work has primarily focused on min-\nimizing token counts to reduce reading time, we\nemphasize the importance of considering end-to-\nend latency, including compression, for building an\nefficient RAG pipeline. We provide a breakdown\nof the total end-to-end latency into read time and\ncompression time, along with an analysis of the av-\nerage number of tokens in compressed documents,\nas shown in Figure 5. Although some methods"}, {"title": "6.4 Ablation Studies", "content": "To better understand how the design choices in\nEXIT affect its overall performance and efficiency,\nwe conduct ablation studies focusing on three key\ncomponents: data sampling strategy, adaptive sen-\ntence selection, and context-aware extraction. Ta-\nble 2 summarizes these results.\nData Sampling Strategy. Our training data com-"}, {"title": "6.5 Impact of Compressor Model Size and\nCompression Strategy", "content": "Model Size Considerations. Figure 6 presents an\nablation study examining how different base mod-\nels influence EM scores and total latency. Note\nthat all models trained within EXIT achieve supe-\nrior accuracy compared to uncompressed baselines\nand fast compression under 2 seconds. Specifically,\nGemma-2B, our base classifier model, achieves a"}, {"title": "7 Conclusion", "content": "We present EXIT, an efficient context compression\nframework for RAG systems that leverages parallel\nprocessing and context-aware, adaptive sentence\nselection. Our experiments demonstrate that EXIT\nachieves superior performance across both single-\nhop and multi-hop QA tasks while maintaining\npractical inference speeds. Despite being trained\nonly on HQA, EXIT shows a strong zero-shot gen-\neralization ability and proves effective across a\nwide range of open-source models of varying sizes.\nThese results suggest that efficient parallel extrac-\ntion with smaller models can outperform larger ab-\nstractive approaches, offering a practical solution\nfor real-world RAG applications."}, {"title": "Limitation", "content": "Our current approach relies on explicit sentence-\nlevel annotations to train the classifier. While these\nannotations were obtained manually in our experi-\nments, they could potentially be automated through\nalternative means, such as by GPT-4 supervision or\nsignals derived from the reader itself. We have not\nyet explored these automated annotation strategies,\nbut doing so remains a promising avenue for future\nwork. Additionally, our study primarily focuses on\na general-domain setting, leaving questions about\nthe classifier's performance in specialized domains\nunanswered. Investigating how well our approach\ngeneralizes to domain-specific or highly special-\nized corpora presents another valuable direction for\nfuture research. Lastly, we focus on a single-step\nRAG pipeline, where retrieval occurs only once, ex-\ncluding more complex pipelines (Shao et al., 2023;\nTrivedi et al., 2023; Khattab et al., 2022). However,\nour proposed framework, EXIT, is orthogonal to\nthese approaches and can be seamlessly integrated\nby compressing the retrieved documents from each\nretrieval step."}, {"title": "Ethics Statement", "content": "This work enhances RAG-based QA without gen-\nerating new content beyond what is retrieved. How-\never, biases and inaccuracies in the source doc-\numents can still propagate through our compres-\nsion process. Ensuring the reliability, fairness, and\nproper curation of underlying corpora is essential\nfor ethical deployment. Future efforts should in-\ntegrate bias detection, provenance tracking, and\nuser-centric evaluations to promote more transpar-\nent and equitable real-world applications."}, {"title": "A More Implementation Details", "content": "This section describes our training environment,\ndata composition, and prompt templates. All ex-\nperiments were conducted on an NVIDIA A100-\nSXM4-80GB GPU cluster. Training was performed\non a single GPU with gradient accumulation."}, {"title": "A.1 Training Configuration", "content": "We trained the compressor model using the follow-\ning settings:\n\u2022 Batch size: 8 per device\n\u2022 Gradient accumulation steps: 8\n\u2022 Learning rate: le-5\n\u2022 Weight decay: 0.1\n\u2022 Warmup ratio: 0.03\n\u2022 Training epochs: 1\nMemory Optimization:\n\u2022 Optimizer: paged_adamw_8bit\n\u2022 Quantization: 4-bit with float16\n\u2022 LORA configuration: Rank=64, Scaling=32,\nDropout=0.05"}, {"title": "A.2 Model Selection and Training Time", "content": "Model selection was based on validation loss.\nTraining required approximately 90 hours on our\ncluster."}, {"title": "A.3 Data Processing", "content": "We used SpaCy to segment documents into sen-\ntences. Table 3 shows the composition of the train-\ning and validation sets derived from HQA. The\ntraining set contains 427K sentences, including\n213K positive (Pos), 107K hard-negative (H-Neg),\nand 107K negative (Neg) instances. The validation"}, {"title": "A.4 Inference Settings", "content": "For inference, we set:\n\u2022 Temperature: 0.0\n\u2022 Top-p: 1.0\n\u2022 VLLM version: v0.5.5\n\u2022 Relevance threshold (7): 0.5"}, {"title": "A.5 Prompt Templates", "content": "Full prompt templates for compression and QA\ntasks are provided in Tables 4 and 5, respectively."}, {"title": "A.6 Reproducibility", "content": "We will release our codebase, including dataset pre-\nprocessing scripts, evaluation protocols, and model\ncheckpoints, upon publication. All random seeds\nare set to 42 to facilitate reproducibility."}, {"title": "B.1 Performance with Sparse Retrieval", "content": "To assess EXIT's robustness with different retrieval\narchitectures, we evaluate it with BM25, a sparse\nretrieval method. Table 6 compares EXIT's perfor-\nmance on HQA (in-domain) and 2WIKI (out-of-\ndomain) under Top-5 and Top-20 retrieval settings.\nWith Top-5 retrieval, EXIT shows notable gains\non HQA, improving EM (33.4 vs. 28.2) and F1\n(44.5 vs. 39.1) over the uncompressed baseline.\nAlthough RECOMP-Abst performs best on 2WIKI\n(25.0 EM, 30.6 F1), EXIT remains competitive\n(24.4 EM, 29.1 F1).\nEXIT's advantages grow with Top-20 retrieval.\nOn HQA, EXIT outperforms all baselines, improv-\ning EM by 4.0 points (35.2 vs. 31.2) and F1 by\n4.4 points (46.9 vs. 42.5) compared to using un-\ncompressed documents. On 2WIKI, EXIT achieves\nthe highest scores (27.2 EM, 32.4 F1), confirming\nits generalizability across domains and retrieval\nstrategies."}, {"title": "B.2 Performance with Proprietary Model", "content": "We further examine EXIT's effectiveness using\nGPT-40 as the reader. Table 7 compares perfor-\nmance on HQA (in-domain) and 2WIKI (out-of-\ndomain), along with compression rates.\nFor Top-5 retrieval, EXIT attains the best accu-\nracy on HQA (38.2 EM, 50.4 F1) while retaining\nonly 26.0% of tokens. This surpasses the uncom-\npressed baseline (37.2 EM, 48.6 F1) with a 74%\ntoken reduction. On 2WIKI, EXIT maintains lead-\ning accuracy (31.8 EM, 35.8 F1) while using just\n19.0% of the original tokens.\nUnder Top-20 retrieval, where uncompressed\ndocuments benefit from greater coverage, EXIT\nstill achieves competitive accuracy with substan-\ntially fewer tokens. On HQA, EXIT closely\nmatches the uncompressed EM score (39.4 vs.\n39.6) while using only 15.4% of tokens. Although\nRECOMP variants compress more aggressively,\nthey suffer marked performance drops. LongLLM-\nLingua performs similarly to EXIT but retains more\ntokens (18.7% vs. 15.4%).\nThese findings illustrate EXIT's ability to bal-\nance performance and efficiency, making it valu-\nable for API-based proprietary models where token\ncosts and accuracy both matter."}, {"title": "B.3 Impact of Threshold T", "content": "We analyze EXIT's sensitivity to the relevance\nthreshold 7. Figure 7 shows EXIT's performance\nacross various T values.\nEXIT remains stable over a wide threshold range,\nwith strong results between \u03c4=0.3\u20130.5. At r=0.3,"}, {"title": "B.4 Analysis of Classification Performance\nAcross Negative Sample Types", "content": "Table 8 presents EXIT's sentence-level classifica-\ntion performance, broken down by negative sam-\nple type. EXIT achieves 0.92 F1 for both positive\n(\"Yes\") and negative (\u201cNo\u201d) classes overall, indi-"}, {"title": "B.5 Classification Performance under\nAblation Settings", "content": "B.5.1 Analysis of Training Data Composition\nFigure 9 presents row-normalized confusion ma-\ntrices comparing classification performance across\nthree training data configurations: Ours (Pos+H-\nNeg+Neg), Pos+H-Neg, and Pos+Neg. Under the\nOurs setup, the classifier displays a balanced ability\nto identify both 'Yes' (relevant) and \u201cNo\u201d (irrel-\nevant) sentences, achieving an F1-score of 0.92"}, {"title": "B.5.2 Impact of Context on Classification\nPerformance", "content": "We evaluate the classifier's performance with and\nwithout broader passage-level context. Figure 8\nshows that including context maintains over 90%\nprecision and recall for both 'Yes' and \"No\"\nclasses. Without context, precision and recall de-\ncline, weakening the distinction between relevant\nand irrelevant sentences. This emphasizes the im-\nportance of incorporating passage-level context for\naccurately identifying answer-critical information."}, {"title": "B.6 Training Data Ablation Analysis", "content": "Table 9 compares models trained on HQA, 2WIKI,\nor both. Training solely on HQA yields the high-\nest EM and F1 (31.6 EM, 42.6 F1) with moderate\ntoken usage. In contrast, 2WIKI training improves\ncompression but lowers accuracy (29.2 EM, 40.3\nF1). Combining datasets does not surpass HQA\nalone.\nThis finding suggests that data quality and struc-\nture matter more than quantity. HQA's annotations\nappear particularly effective for learning robust\ncompression strategies that generalize well, val-\nidating our choice to use it as the primary training\ndataset."}, {"title": "B.7 Case Studies", "content": "To illustrate how EXIT's extractive compression\nstrategy improves both accuracy and readability,\nwe present qualitative examples and comparisons\nwith other compression methods."}, {"title": "Original Documents vs. Ours.", "content": "In Table 10, the\noriginal documents contain the correct answer\n(\"Custard\") but also include distracting information\n(\"Eggnog\"). Despite having the necessary evidence,\nthe reader fails to produce the correct answer, likely\ndue to this distractor. In contrast, our method (Ours)\nfilters out irrelevant details, drastically reduces in-\nput length, and retains only the essential context\nneeded to answer the query accurately. As a result,\nthe reader confidently generates the correct answer\n(\"Custard\").\nIn a second scenario (Table 11), the original\ndocuments retrieve multiple documents related to\n\"Wagner\" or \"sci-fi\u201d series but fail to provide any\ncontent explicitly linking James Belushi to the cor-\nrect 90s sci-fi series, resulting in an incorrect pre-\ndiction. Surprisingly, Ours removes all retrieved\ncontext entirely, providing the reader with no addi-\ntional information. Under this no-context condition,\nthe reader relies solely on its internal knowledge\nand, in this case, correctly identifies \u201cWild Palms.\u201d\nWhile this outcome indicates a form of hallucina-\ntion or model bias-since the answer emerges with-\nout external supporting evidence-it also demon-\nstrates Ours' capability to avoid misleading context.\nBy eliminating irrelevant or confusing documents,\nOurs can sometimes allow the model's internal\nknowledge to surface, leading to correct answers\neven in the absence of any retrieved information."}, {"title": "Comparisons with Other Methods.", "content": "Table 12\ncompares Ours with several competing compres-\nsion approaches. CompAct preserves some relevant\ninformation but introduces hallucinations, causing\nthe reader to claim that it cannot find the correct\nanswer. Refiner omits the crucial entity required to\nanswer the query, demonstrating how abstractive\ncompressors may inadvertently remove key content."}, {"title": "In contrast, Ours avoids hallucinations and retains\nthe answer's entity in a concise, coherent form.", "content": "LongLLMLingua's token-level filtering ap-\nproach yields unreadable text and removes the\nessential \"Romania\" entity, preventing the reader\nfrom generating the correct answer. Ours, on the\nother hand, maintains semantic coherence and in-\ncludes the correct entity, allowing the reader to\nproduce the correct answer without interference.\nThese case studies highlight the advantages of\nOurs: it eliminates distractors, preserves critical\nentities, and maintains semantic integrity. Conse-\nquently, the reader consistently arrives at correct\nanswers with fewer tokens and no hallucinations."}]}