{"title": "MINDECHO: Role-Playing Language Agents for Key Opinion Leaders", "authors": ["Rui Xu", "Dakuan Lu", "Xiaoyu Tan", "Xintao Wang", "Siyu Yuan", "Jiangjie Chen", "Wei Chu", "Xu Yinghui"], "abstract": "Large language models (LLMs) have demonstrated impressive performance in various applications, among which role-playing language agents (RPLAs) have engaged a broad user base. Now, there is a growing demand for RPLAs that represent Key Opinion Leaders (KOLs), i.e., Internet celebrities who shape the trends and opinions in their domains. However, research in this line remains underexplored. In this paper, we hence introduce MINDECHO, a comprehensive framework for the development and evaluation of KOL RPLAS. MINDECHO collects KOL data from Internet video transcripts in various professional fields, and synthesizes their conversations leveraging GPT-4. Then, the conversations and the transcripts are used for individualized model training and inference-time retrieval, respectively. Our evaluation covers both general dimensions (i.\u0435., knowledge and tones) and fan-centric dimensions for KOLs. Extensive experiments validate the effectiveness of MINDECHO in developing and evaluating KOL RPLAS. Resources of this paper will be released upon publication.", "sections": [{"title": "1 Introduction", "content": "With advancements in large language models (LLMs) (Anthropic, 2023; Touvron et al., 2023), Role-Playing Language Agents (RPLAs) have become a flourishing application and research direction (Chen et al., 2024; Xu et al., 2024). These agents are designed to simulate specific personas, providing users with interactive, contextually accurate responses (Park et al., 2023; Wang et al., 2023). A promising application of RPLAs is in emulating Key Opinion Leaders (KOLs) (Bamakan et al., 2019). KOLs are individuals with significant influence in a specific professional field. They share substantial domain-related content online, establishing their authority and credibility (Palmer and Supuran, 2015; Amalia et al., 2023). Developing RPLAS for KOLs can provide expert insights and answers to users anytime, mimicking the knowledge these leaders offer. However, research in this area remains largely unexplored.\nPrevious work has focused on constructing character RPLAs in fictional backgrounds (e.g., Gandalf from The Lord of the Rings) (Shao et al., 2023; Li et al., 2023), with their data typically sourced from established novels, encyclopedias, and scripts (Xu et al., 2024). However, as shown in Figure 1, there are several limitations with this type of RPLAs. First, fictional backgrounds are relatively simple and may contain logical inconsistencies. Second, since most of the characters appear in the training corpora of LLMs, constructing RPLAs from characters merely perpetuates existing stereotypes and lacks innovation (Wang et al., 2024). Furthermore, it is also challenging to determine whether the effectiveness of the models stems from the external data or the model parameters themselves ((Tu et al., 2024; Yuan et al., 2024))\nTherefore, constructing real-person RPLAs based on real persons, who are seldom included in LLM training datasets, has become an important challenge.\nIn this paper, we propose the task of constructing RPLAs for KOLs based on video transcripts. Video data rarely appears in the training corpus of existing LLMs, and videos of KOLs typically adopt a first-person perspective, closely mirroring real human interactions. Compared to previous character RPLAs, this task presents the following challenges: (1)Real-world Challenge: Compared with the static and virtual background of character RPLAS, KOL RPLAs are based on the real-world environment, with novel challenges such as new terminology, trending events, and ways of online communication. (2)Personal Data Challenge: KOLS have many personal opinions in their professional fields, which must be manifested and distinguished from the opinions inherent in LLMs. (3)Dense Knowledge Challenge: The knowledge possessed by KOLs in RPLA is very dense. It is necessary to design more knowledge-intensive tasks to evaluate their capabilities.\nTo address these challenges, we propose MINDECHO, a comprehensive framework for KOL RPLAS. MINDECHO comprises three stages, i.e., KOL data collection, RPLA construction, and evaluation. For data construction, as shown in Figure 2, after obtaining video transcripts from KOLs in different professional fields, we use GPT-4 (OpenAI, 2023) to identify their critical opinions and generate knowledge-intensive dialogue data between the KOLs and fans. After obtaining data, we train LLMs using this dataset. During the inference phase, we also integrate a Retrieval-Augmented Generation (RAG) (Gao et al., 2024) module to support the model with external knowledge. To reduce the RPLA's dependency on the opinions embedded in the LLM's parameters, we identify counter-intuitive opinions and concatenate them with the constructed data. This approach encourages the model to trust external knowledge more.\nUsing our method, the fine-tuned Qwen1.5-14B-Chat(Bai et al., 2023) surpasses GPT-4's in-context learning performance in human evaluations.\nFor evaluation, we provide a systematic framework, consisting of two evaluation deminsions, i.e., basic performance evaluation and fan-Centric evaluation. For basic performance evaluation, we use multiple-choice questions to separately examine KOLs' tone characteristics and professional knowledge, quantitatively reflecting the model's capabilities through accuracy scores. For fan-Centric evaluation, we use GPT-4 to simulate interactions and ratings separately for new and old fans with the RPLA. The profiles of these fans are constructed using comment data from the KOL's videos. Comparison with human evaluations proves the effectiveness of this evaluation method. In summary, our contributions include:\n\u2022 We introduce the first KOL RPLA task, the first-ever knowledge-intensive role-playing task entirely focused on real individuals.\n\u2022 We propose a comprehensive framework MINDECHO for constructing a high-quality KOL RPLA, and experimental results have demonstrated its effectiveness.\n\u2022 We develop a systematic evaluation method to evaluate KOL RPLA capabilities from three perspectives: professional knowledge, tone characteristics, and user-centered simulated interaction. Additionally, we provide an authorized KOL dataset."}, {"title": "2 Related Work", "content": "Character Role-Playing Previous work related to role-playing has primarily focused on constructing character agents for fictional works. Li et al. (2023) propose a method that constructs RPLA via an improved prompt and memories of the character extracted from scripts. Wang et al. (2023) offer a more comprehensive process for constructing character training data, and Shao et al. (2023) provide a more detailed data synthesis method through scene reconstruction. For evaluation, Tu et al. (2024) propose a multi-perspective evaluation scheme. Wang et al. (2024) evaluate the characters' ability to replicate their psychological personas through interviews. Xu et al. (2024) to assess further the characters' ability to reproduce their life choices from the original works. However, current role-playing work often focuses on characters already present in LLMs' parameters, making it challenging to distinguish the model's actual role-playing capabilities from the knowledge embedded in its pre-trained parameters during construction and evaluation. Our framework aims to explore authentic, knowledge-intensive, and data-leakage-free KOL role-playing. This direction holds broad application prospects and paves the way for digital representations of individuals in the real world.\nPersonal Dialogue Personal dialogue is a vital direction in the development of language models. Related research focuses on incorporating personalized features into conversations. Zhang et al. (2018) manually construct a large dataset of personal topics. Additionally, Urbanek et al. (2019) explore whether different personas can engage in text-based role-playing games. To build more realistic profiles, Jang et al. (2022) collect data from Wikidata to conduct personal dialogues and introduce more external databases to enrich conversational knowledge. Salemi et al. (2024) propose seven types of personal downstream tasks, including dialogue. With advancements in large models, tasks involving multimodal(Ahn et al., 2023) and long-text(Gao et al., 2023) personalized dialogues are emerging. However, these tasks have sparse personal features, such as knowledge and opinions, and have not been analyzed from a role-playing perspective. MINDECHO, as the first knowledge-intensive real-person role-playing task, effectively fills this gap."}, {"title": "3 MINDECHO", "content": "In this section, we introduce MINDECHO, a comprehensive KOL RPLA framework for KOL data collection (\u00a7 3.1), RPLA construction (\u00a7 3.2), and evaluation (\u00a7 3.3)."}, {"title": "3.1 KOL Data Collection", "content": "We construct a comprehensive raw dataset for each KOL, which includes the KOL's profile P, a collection of video transcripts {v1, v2, v3, ...}, and user comments under each video transcript {{C11,C12,...},{C21, C22, ...}, . . .}. The construction of the dataset comprises four steps: (1) KOL selection; (2) Profile drafting; (3) Generation of raw data; (4) Cleaning of raw dataset.\nFirst, we carefully select 14 KOLs from different professional fields. They each have tens of thou-"}, {"title": "3.2 RPLA Construction", "content": "Data Preparation and Model Training To develop RPLAs for KOLs, we first construct meta-opinions and dialogues for each KOL, sourced from the collected raw data. A rich knowledge base is a critical feature that distinguishes KOLs from character RPLAS. As shown in Figure 2, to construct knowledge-intensive dialogue training data, we utilize GPT-4 to extract ten groups of meta-opinions {Un1, Un2, ..., Un10} from each video transcript Un. Afterwards, for each group of meta-opinion, we then apply GPT-4 to simulate dialogue pairs between fans and KOLS {{Cn11, Cn12},{Cn21,Cn22},...}. Specifically, since the video transcripts are first-person perspectives, we require the model to simulate the KOL by responding as closely as possible to the original text. This approach ensures the KOL's tone is preserved while incorporating knowledge into the constructed data. In addition, to mitigate the model's tendency to rely on parameter knowledge when answering questions, we use GPT-4 to determine whether the model's simulated KOL responses are consistent with GPT-4's direct responses to the same questions. This helps filter out those counter-intuitive opinions. We then concatenate these counter-intuitive data into the training data, enhancing the model's reliance on external knowledge.\nIn summary, our constructed training data includes two parts: knowledge-based dialogue data {{Cn11, Cn12},{Cn21,Cn22}, ...} and counter-intuitive follow-up data {Un1 + Cn11, Cn12}, {Un1 + Cn21, Cn22},...}. We performed supervised fine-tuning (SFT) on qwen-7b and qwen-14b, with separate fine-tuning for each role to eliminate role hallucination problems caused by knowledge interference between roles.\nKnowledge Retrieval During inference, to ensure accurate retrieval from extensive KOL knowledge, we employ retrieval-augmented generation. We employ embedding-based retrieval, using OpenAI's text-embedding-ada-002 as the retrieval model. We segment each KOL's raw data into chunks of 500 tokens. The chunk with the highest relevance is matched for each query to assist the model's reasoning."}, {"title": "3.3 Evaluation", "content": "Unlike characters' RPLA, KOLs' domain knowledge is very dense. To comprehensively evaluate KOL RPLA's performance, we introduce two types of dimensions with distinct methodologies: basic performance evaluation and fan-centric evaluation."}, {"title": "Basic Performance Evaluation", "content": "Similar to the evaluation of character RPLA(Tu et al., 2024), we assess the basic performance of KOLs from two dimensions: tone characteristics capability and professional knowledge capability. Tone characteristics aim to test whether the model can exhibit the speaking style of KOLs in communication, faithfully replicating each KOL's unique expression habits. Professional knowledge aims to test whether the model can generate knowledge consistent with KOLs, including unique opinions. To quantify the results, we adopt a multiple-choice evaluation method. Based on each video transcript, we ask GPT-4 to simulate user questions and KOL responses and design incorrect options to form multiple-choice questions. Specifically, we require each question to involve a certain reasoning process, and the incorrect options must be reasonable but not align with the KOL's speaking style or domain opinions. For example, a KOL in the health field believes that skipping breakfast is an effective way to stay healthy, which is not in line with mainstream opinions. We construct 500 questions for each KOL, covering all video transcripts."}, {"title": "Fan-Centric Evaluation", "content": "Fan-centric is a significant feature of KOLs. KOLs often adjust their content and interaction methods based on the needs and feedback of different fans. From this perspective, we model two types of users who might interact with the KOL RPLA: new users getting to know KOLs (new fans) and loyal fans of KOLs (old fans). To better model the latter, we use fan comments from the raw data as source data, providing them to GPT-4 to construct profiles of this fan group. These profiles include their age range, interests, lifestyle, career tendencies, consumption habits, language style, etc. We then have the user RPLAs engage in five rounds of interaction with the KOL's RPLA based on specific video content and use GPT-4 to evaluate the interaction. For new fans, we provide the following three dimensions for evaluation:\n\u2022 Content Comprehension (CC): Evaluate whether the content provided by KOLs ensures accuracy while also making it easier for new fans to understand and grasp.\n\u2022 Interaction Attractiveness (IA): Evaluate the RPLA's effectiveness in interacting with new fans, including its ability to respond reasonably, interact friendly, and make new fans feel valued and cared for.\n\u2022 Engagement Appeal (EA): Evaluate whether the RPLA can sustain new fans' interest and encourage further engagement during interactions. This involves content enjoyment, interaction guidance, and attracting and converting new users into long-term fans.\nFor old fans, we provide the following three dimensions for evaluation:\n\u2022 Fan Resonance (FR): Evaluate if the RPLA's interaction with old fans elicits emotional resonance. Observe if it understands and responds to their emotional needs and concerns and if this interaction enhances fan loyalty and connection.\n\u2022 Content Relevance (CR): Evaluate if the RPLA content aligns with old fans' interests and needs, ensuring it meets their expectations and maintains their attention and support for the KOL.\n\u2022 Character Authenticity (CA): Evaluate if the RPLA authentically replicates the KOL's personality and style during interactions with old fans. Ensure the RPLA's behavior, language, and attitude are consistent with the KOL's usual performance, providing old fans with familiarity and continuity.\nTo rate these KOL RPLAs, we employ GPT-4 to assign scores of 1 (Poor), 2 (Average), and 3 (Excellent), providing detailed descriptions for different scores across each dimension. Specific prompts are detailed in Appendix A."}, {"title": "4 Experiment", "content": "We fine-tune the model by constructing knowledge-intensive training data and enhancing it through RAG. To validate the effectiveness of this methodology and its different components, we adopt baselines in three settings:\n\u2022 In-contexting (Profile w/o RAG): Only provide the model with the KOL's profile p, allowing it to play the role of the corresponding KOL directly.\n\u2022 In-contexting (Profile w/ RAG): Provide the model with the KOL's profile and RAG content, allowing it to respond to users while referring to RAG content."}, {"title": "4.2 Main Result", "content": "As shown in Table 2, from the evaluation of basic performance, we can observe the following: First, when only providing a profile, the model's results in knowledge Q&A are close to random (25%). On the one hand, this indicates that LLMs have difficulty answering domain-specific questions based on knowledge imbibed in their parameters. On the other hand, unlike other RPLA works, this means that opinions from KOL do not face the issue of data leakage. Second, the results improve after integrating RAG, regardless of whether the model has been trained. This demonstrates the necessity of RAG. The improvement is less pronounced after training, which indicates that the model has already learned a substantial amount of KOL's knowledge during the training period. Last, MINDECHO based on qwen-14b outperforms GPT-4 turbo w/ RAG in most metrics, demonstrating the effectiveness of our approach. From the Fan-Centric evaluation results, it can be seen that GPT-4 performs similarly to MINDECHO on the metrics CC, IE, FR, and EA, which can be attributed to its emotional solid expression capabilities. However, it performs poorly on metrics such as CR and CA, indicating its ineffectiveness in critical knowledge-based expression, which is important for KOL. Sec 4.5 reveals this because GPT-4 tends to provide its own opinions in responses rather than that of the KOL."}, {"title": "4.3 Reference Content v.s. Retrieval Content", "content": "To analyze the impact of retrieved content on the model, we provide reference video content for each question. As shown in Table 4, both tone and knowledge tasks improve when given reference results, particularly noticeable in the knowledge tasks. This indicates that there is still room for improvement in the retrieval methods. For the tone tasks, answers often directly cite the original words from the video, while knowledge tasks require some level of reasoning. Therefore, the knowledge tasks are not always correctly answered, even with reference content. Moreover, in the Language tasks, models through training consistently outperform those using in-context learning. This suggests that models understand the roles more deeply after training, enabling them to better infer the characters' language patterns without reference to content."}, {"title": "4.4 Long-context Model with All Video Content", "content": "Directly using long-context models for role-playing is an interesting topic. In the knowledge-intensive KOL RPLA, incorporating all video content as context not only helps evaluate the model's ability to portray the overall character but also comprehensively assesses the model's ability to retrieve knowledge from the context. This is because our questions are derived from various segments within the context and require a certain level of reasoning. As shown in Table 5, we evaluated two representative long-context models, Claude3-sonnet and Kimi-chat. Although their performance is not as strong as MINDECHO, long-context models still demonstrate potential for role-playing, especially in free dialogue scenarios. Their performance in knowledge tasks is worse than in tone tasks, possibly because effective free dialogue relies more on an overall understanding of the character and can tolerate some context loss."}, {"title": "4.5 Case Study", "content": "As shown in Table 3, we analyze the results of profile w/o RAG, profile w/ RAG, and MINDECHO on the fan-centric evaluation. It can be observed that GPT-3.5's responses are relatively brief and often derived from its parameter knowledge. In contrast, GPT-4 provides more detailed responses and follows our instructions by incorporating retrieved knowledge into its answers. However, the tone of GPT-4's responses does not resemble that of the KOL, and it still relies on its knowledge sometimes. The model trained using MINDECHO more extensively includes the KOL's knowledge while adhering to the KOL's linguistic habits. For example, the phrase \"Take care, and beauty will find you!\" is a common saying among KOLs, and its appearance in responses enhances the authenticity of the role-playing. Furthermore, the responses from the MINDECHO include some emerging internet slang, indicating that role-playing as a KOL requires considering the complexity and variability of the real world. The model needs to understand these terms and incorporate them into its responses."}, {"title": "4.6 Human Evaluation", "content": "We also conduct a human evaluation for the Fan-Centric Evaluation to validate GPT-4's performance on this task. We compare the predictions from the LLMs with human judgments. To evaluate new and old fans, we sample 100 sets of dialogue data from 5 KOLs and enlist 6 crowd-sourced workers to annotate the data. Three annotators who have not watched KOL videos evaluate the dialogues of new fans, while the other three, who have watched some KOL videos, evaluate the dialogues of old fans. All annotators follow the same criteria as GPT-4 and are compensated at the local minimum wage. We report the Pearson's r (Pearson, 1920), Spearman's p (Spearman, 1961) and Kendall's T (Kendall, 1938) correlations between human annotations and GPT-4. As shown in Table 6, GPT-4's evaluations closely match the human evaluations. The simulation of new fans is better compared to old fans. We believe this is because the annotators are not genuinely interested in the KOL's field and cannot fully simulate such individuals."}, {"title": "5 Conclusion", "content": "In this paper, we study the RPLA of KOLs based on language models. Unlike the RPLA of other roles, KOLs exhibit characteristics such as being grounded in the real world, knowledge-intensive, and free from data leakage, making it a more complex role-playing task. We propose a complete framework MINDECHO that spans data collection and model training to evaluation. For data collection, we constructed the first KOL dataset using many video transcripts. For model training, we implemented targeted optimizations to create training data. For evaluation, we innovatively proposed a fan-centric evaluation scheme to assess the KOL RPLA better. Experimental results demonstrate the advantages of our approach. By constructing the RPLA of KOLs, we not only enhance the efficiency of interaction between KOLs and their fans but also provide users with professional field insights. More importantly, this work offers an initial solution for language models to play the role of real-world humans."}, {"title": "Limitations", "content": "The partial evaluation method we proposed relies on GPT-4, which could introduce bias towards GPT-4 generated content. Although the KOL-related data is very close to real human data, there is still a gap, and it cannot be considered completely accurate human role-playing data."}, {"title": "Ethical Statement", "content": "We hereby acknowledge that all authors of this work are aware of the provided ACL Code of Ethics and honor the code of conduct."}, {"title": "Authorization", "content": "Human Annotation"}, {"title": "Risks", "content": ""}, {"title": "A GPT-4 Prompt", "content": "Here's the prompt for GPT-4 to evaluate KOL RPLA:\n\u2022 Content Comprehension (CC)\nScore 1 (Poor): The content provided by the KOL is inaccurate or confusing, making it difficult for new fans to understand. It fails to ensure clarity or address essential details.\nScore 2 (Average): The content is mostly accurate but lacks simplicity or thoroughness in explanation. Some new fans may understand it, but others might find it partially confusing.\nScore 3 (Excellent): The content is highly accurate and easy to understand, breaking down complex ideas into digestible information. New fans can clearly grasp the concepts and feel informed.\n\u2022 Interaction Attractiveness (IA)\nScore 1 (Poor): The interaction is unfriendly or appears automated, lacking reasonable responses. New fans feel ignored or undervalued.\nScore 2 (Average): The interaction is polite and reasonably responsive but lacks warmth or personal touch. New fans feel acknowledged but not particularly valued.\nScore 3 (Excellent): The interaction is highly engaging, friendly, and responds appropriately to fan inquiries. New fans feel valued, cared for, and welcomed.\n\u2022 Engagement Appeal (EA)\nScore 1 (Poor): The RPLA fails to engage or maintain interest. The content is dull, lacks interaction prompts, and does not convert new fans into long-term followers.\nScore 2 (Average): The RPLA can sustain interest to some extent but lacks compelling calls to action or consistent excitement. Some new fans might continue engaging, but many might not.\nScore 3 (Excellent): The RPLA consistently engages new fans with appealing content, proactive interaction guidance, and effective strategies that convert new users into long-term, devoted fans.\n\u2022 Fan Resonance (FR)\nScore 1 (Poor): The interaction fails to resonate emotionally with older fans, neglecting their concerns or emotional needs. Fans feel disconnected or ignored.\nScore 2 (Average): The interaction is somewhat resonant, showing basic understanding of fans' emotions and concerns. Fans feel moderately engaged but not deeply connected.\nScore 3 (Excellent): The interaction deeply resonates with older fans, appropriately addressing their emotional needs and concerns. Fans feel a strong sense of loyalty and connection.\n\u2022 Content Relevance (CR)\nScore 1 (Poor): The content is irrelevant or misaligned with the interests and needs of old fans, leading to disengagement.\nScore 2 (Average): The content is somewhat relevant but lacks depth or consistency in aligning with fan expectations. Fans maintain moderate interest.\nScore 3 (Excellent): The content is highly relevant, consistently meeting or exceeding old fans' interests and needs. Fans are fully engaged and supportive.\n\u2022 Character Authenticity (CA)\nScore 1 (Poor): The RPLA fails to replicate the KOL's personality and style, showing inconsistencies in behavior, language, and attitude, causing old fans to feel disconnected.\nScore 2 (Average): The RPLA somewhat replicates the KOL's personality but shows occasional inconsistencies, leading to a mix of connection and confusion among fans.\nScore 3 (Excellent): The RPLA authentically replicates the KOL's personality and style in all interactions, ensuring behavior, language, and attitude are consistent with the KOL's usual performance. Old fans experience familiarity and continuity."}]}