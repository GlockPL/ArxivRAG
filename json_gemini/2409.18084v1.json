{"title": "GSON: A Group-based Social Navigation Framework with Large Multimodal Model", "authors": ["Shangyi Luo", "Ji Zhu", "Peng Sun", "Yuhong Deng", "Cunjun Yu", "Anxing Xiao", "Xueqian Wang"], "abstract": "As the number of service robots and autonomous vehicles in human-centered environments grows, their requirements go beyond simply navigating to a destination. They must also take into account dynamic social contexts and ensure respect and comfort for others in shared spaces, which poses significant challenges for perception and planning. In this paper, we present a group-based social navigation framework (GSON) to enable mobile robots to perceive and exploit the social group of their surroundings by leveling the visual reasoning capability of the Large Multimodal Model (LMM). For perception, we apply visual prompting techniques to zero-shot extract the social relationship among pedestrians and combine the result with a robust pedestrian detection and tracking pipeline to alleviate the problem of low inference speed of the LMM. Given the perception result, the planning system is designed to avoid disrupting the current social structure. We adopt a social structure-based mid-level planner as a bridge between global path planning and local motion planning to preserve the global context and reactive response. The proposed method is validated on real-world mobile robot navigation tasks involving complex social structure understanding and reasoning. Experimental results demonstrate the effectiveness of the system in these scenarios compared with several baselines. (Video\u00b9)", "sections": [{"title": "I. INTRODUCTION", "content": "The growth of service robots has spurred significant research into autonomous systems capable of navigating human-centered environments [1]\u2013[4]. However, many navigation system focus primarily on individual trajectory prediction for obstacle avoidance [5]\u2013[7], often neglecting complex contextual interactions in crowds, such as situations where someone is photographing another or the presence of queues, as illustrated in Fig. 1. These kinds of social structures capture the patterns of interaction and spatial relationships between individuals in a crowd that are shaped by semantic group dynamics and social norms [8]. In this work, we focus on developing a robot navigation system that can understand and respect such complex social structures, enabling socially-aware navigation that takes into account group-based interactions and adheres to social norms.\n The core issue behind building such a socially aware navigation system is how to accurately identify social structure among dynamic and unpredictable human interactions, and exploit this social structure to guide the motion planning system. For the perception, traditional methods that rely on naive predefined rules or in-domain training [5], [6], [9], [10] struggle to handle the complexity of open-world environments, primarily due to data scarcity and the wide visual variability encountered in open-world settings. To address this, we utilize the visual understanding ability of LMMs to enable zero-shot reasoning of social structure. The LMMs are added on top of the perception pipeline that supports reliable detection and tracking to predict the social group. Given the predicted social group, the robot must generate socially appropriate motions that take these social structures into account. Traditional reactive controllers often struggle to avoid disrupting such social structure due to the short planning horizon [11]\u2013[13]. The end-to-end method, which avoids the perception-planning decomposition, usually overfits the demonstration or is unable to interpret complex context in the image [14]\u2013[16]. We address this issue by adapting a mid-level planner as a bridge between global path planning and local motion planning to enable global awareness of the social structure and reactive speed, ensuring the robot's behavior aligns with social norms.\n Overall, we develop a group-based social navigation framework (GSON) to address both the perception and planning issues in social navigation tasks. We conducted intensive experiments both in simulation and real world with several daily social interaction scenarios. Comparative experiments indicate that GSON outperforms all baseline methods in terms of minimizing perturbations to the social structure."}, {"title": "A. Related Work", "content": "1) Social Structure Detection in Robot Navigation.: To infer the social structure, especially the dynamic group detection plays a critical role in the navigation of social robots. Social group detection and tracking can be studied from both exo-centric and ego-centric perspectives. The exo-centric approach [17] relies on external sensors to detect groups and requires prior knowledge of pedestrian characteristics. However, this method typically requires a large number of sensors, making it difficult to implement in practice. In contrast, our method utilizes an egocentric perception system that uses 2D LiDAR and cameras to detect and track social group in real time. Previous approaches to crowd grouping often use probability-based methods [18]\u2013[20], graph-based methods [8], [21], or clustering methods [22]. However, these techniques focus only on the movement of the crowd, neglecting the semantic aspects of the crowd interactions. In GSON, we leverage the zero-shot visual reasoning capabilities of Large Multimodal Models (LMMs) to enable semantic detection of social groups given limited sensors.\n 2) Motion Planning in Group-based Social Navigation: Integrating dynamic human grouping into social navigation has been shown to improve the navigation performance. Previous works [23], [24] show that treating human groups as obstacle representations improves the safety of robot planners. Several approaches have focused on leveraging social groups for motion planning or RL [24], [25], but the prediction of social group information is based solely on the position and velocity or the pedestrian, ignoring the semantic context of the environments. Another issue is that this short horizon planning will lead the system into local minimum solution, such as stack in front of a queue. The GSON framework addresses these limitations by employing a mid-level planner between global and local planning, allowing both global context and reactive behavior. And by integrating Nonlinear Model Predictive Control (NMPC) as the local planner, GSON also ensures both safety and real-time responsiveness even in highly dynamic environments.\n 3) Foundation Models in Navigation: Currently, foundational models in the field of navigation are mainly used for several specific tasks, including the generation of macro-action functions based on scene context, and the identification and classification of various objects in the scene [26]-[28]. However, these models primarily serve as high-level decision makers [29], [30]. Recent works also explore directly applying the LMM to predict the speed control [31] or select the predefined behavior [32]. However, the LMM is known to be not good at directly outputting accurate grounding results [33]. In GSON, we propose a social structure estimation method based on visual prompting with LMM and integrate it into the subsequent path planning. This approach fully utilizes the semantic understanding capability of LMM and provides interpretability."}, {"title": "II. OVERVIEW", "content": "We consider the problem of social robot navigation in a 2D Euclidean space R2. This space is populated by n dynamic pedestrians H\u2081 with a specific social structure. We consider the social structure as m social groups Gj. The robot is initialized with a 2D occupancy map M at the start position Ss and a target position Sg in the map. Given the 2D lidar point cloud Pt and the RGB image observation It, the goal is to enable the robot to safely navigate to the goal position while minimizing the time it spends trespassing any social group. The robot must infer the social groups of pedestrians from the limited observation and generate an appropriate trajectory T. We focus on designing the method to estimate the social group with the aid of the Large Multimodal Model and integrate the estimated result into the navigation system."}, {"title": "B. System Overview", "content": "The proposed GSON developed in this work is illustrated in Fig. 2. The robot is equipped with RGB cameras, a 2D lidar sensor, and a mobile base with differential drive. There are two critical modules in the system: the social group estimation module and the socially aware planning module. The social group estimation module takes the current observation RGB image It, 2D point clouds Pt, and the current robot pose St from the SLAM module as input and generates the"}, {"title": "III. METHOD", "content": "1) Pedestrians Detection and Tracking: Before using LMM to infer social group information, we first built a robust pipeline for pedestrian detection and tracking. Since the 2D LiDAR captures the point cloud data of the environment Pt, we first use an auto-regressive model proposed in [34] for preliminary pedestrian detection. Then, to remove the noise level of the result, we combine the information from RGB images to filter out the misidentified point cloud data using YOLO-V5 [35]. The predicted pedestrian coordinates Pi = [Xi, Yi, Zi] are projected into the camera coordinate system and filtered by the bounding box result output by YOLO-V5. After the filtering, we can get the 2D detected pedestrian coordinates as p\u2081 = [xi, Yi, Zi]. Finally, we apply Hungarian algorithm and Kalman filter for estimating and predicting the position of each individual with a unique ID for further social group detection.\n 2) Social Group Detection with Visual Prompting: The social group detection is building on top of the detection and tracking results of pedestrians. We use LMM to understand human behavior and detect social groups within crowds. Leveraging commonsense knowledge from extensive internet data, these models can identify individuals' social structures and group people based on their activities or social structure given the RGB image observation. Before we query the LMM, we reuse the detection result from III-A.1 for each tracked person and apply the Segment Anything model to generate masks. The bounding boxes, masks, and corresponding tracking ID are annotated in the cameras for the LMM to predict the social group. In the example illustrated in Fig. 4, each person is annotated with the bounding box, mask, and corresponding tracking ID. The LMM 2 is instructed to predict the them based on detected behaviors or interactions, for example, queuing, photography, and chatting. In the input prompt, we define\nAs the inference speed constrain, instead of quering the LMM in a high frequence, we maintains a keyframe that contains the image with the most people appearing within the last 3 seconds, along with the actual position and speed corresponding to each person. The LMM is only quried when the keyframe is updated."}, {"title": "B. Socially Aware Planning Module", "content": "1) Mid-level planning: We introduce a mid-level planning algorithm that leverages social structure predictions from the Large Multimodal Model (LMM) to provide socially aware guidance for local motion planning.\nThe robot begins by performing global path planning on a known cost map C, generating global reference paths R. The cost map is constructed by using an occupied map built by the Gmapping algorithm. Simultaneously, the LMM module analyzes keyframe images to detect and extract social group structures within the scene, producing grouping information for the individuals present. The social space, S, is then defined by constructing the convex hull of the detected group configuration G. Each edge of the hull is approximated by ellipses, collectively forming the boundaries of the social group space. The cost map C is updated by marking the\n2) Local Motion Planning: Given the high-level guidance from the mid-level planner, the local motion planning needs to generate safe trajectory for the robot to execute. Two advanced control strategies that have gained significant traction in recent years are Model Predictive Control (MPC) and Control Barrier Functions (CBF) and the combination of the two has achieved excellent results in both static and dynamic obstacle avoidance [36]. Consequently, we adapt an MPC-CBF as the local planner similar to [37]. In our work, we define the control barrier function as follows:\n$h(x) = (x - x_p)^2 + (y - y_p)^2 \u2013 d_{safe},$  (1)\nwhere xp = [Xp, Yp]T denotes the position of pedestrians and dsafe a predefined safety distance. The planning task then is formulated as a nonlinear model predictive control (NMPC):\n$min\\{x_k,u_k\\} \\|x_N - x_{goal}\\|^2 + \\sum_{k=0}^{N-1}\\|u_k\\|^2$ (2)\ns.t.\n$x_{k+1} = f(x_k, u_k)$ (3)\n$x_0 = x_{init}$ (4)\n$x_k \u2208 X, u_k \u2208 U$ (5)\n$\u0394h_b(x_k, u_k) + \u03bb_kh_b(x_k) \u2265 0.$ (6)"}, {"title": "IV. EXPERIMENTS", "content": "In this experiment, we aim to address the following research questions through both simulated and real-world tests: (1) Can LMMs accurately capture social structures with appropriate visual prompts? (2) Can GSON outperform existing methods in socially-aware navigation? (3) Can GSON generalize to complex and large-scale real-world scenarios?\nTo evaluate the capability of LLMs to predict the social group from RGB observations, we created a dataset of 50 scenarios, each depicting clear group interactions in social contexts, such as queuing, walking, talking, and taking pictures. Four LMMs-GPT-4v, GPT-40, Gemini 1.5-pro, and Claude 3.5-sonnet-were used to analyze these images. Ground-truth labels representing the correct groupings of individuals were created for comparison. To account for variability in LMM responses, each image was tested 10 times with each model, allowing us to assess model robustness and calculate final grouping accuracy. Results were classified into four categories: (1) Accurate: completely correct groupings, (2) Miss: missing one or more individuals, (3) Extra: including individuals from incorrect groups, and (4) Error: misgrouping of individuals. Fig. 5 shows the grouping performance results. All four LMMs achieved a correct grouping rate above 50%, with GPT-40 showing the highest performance at 73%. In addition, GPT-40 shows a combined accuracy rate of nearly 90% when considering both the Accurate and Extra (if a solution exists, it does not disrupt the group) categories. These results strongly suggest that LMMs can effectively capture social group structure from visual prompts, with GPT-40 outperforming the other models. As a result, GPT-40 is integrated into our system for real-world deployment."}, {"title": "B. Evaluation the Planner in Simulation", "content": "In this evaluation, we focus on evaluating GSON's planning performance with the correct group prediction. We conducted extensive simulation experiments using Arena-Rosnav2.0 [38], a platform designed to simulate realistic human behaviors. The experiments focused on four common social scenarios: walking, queuing, conversing, and taking pictures. For each scenario, a minimum of 20 tests were performed with varying pedestrian positions and randomly generated social groups. The baseline for in the simulation included several planners: the Timed Elastic Band (TEB) planner [11], the Dynamic Window Approach (DWA) [12] for collision avoidance, time-optimal nonlinear model predictive control (MPC) [39], a navigation system using the DRL-VO policy (TRAIL) [40], and ROSNav [41], a system integrating DRL-based local planners into conventional navigation. All these methods were implemented in the simulator for comparison. Traditional navigation performance was evaluated using established metrics such as roughness, curvature, jerk, angular deviation from the planned path, and velocity [38], [42]. To assess social navigation, we introduced additional metrics to quantify the robot's impact on individuals and groups. Following guidelines from prior work [3], we considered an individual disturbed if the robot approached within 1.2 meters directly in front of them. For social groups, the robot was marked as disturbing if it entered the convex hull formed by the group members. We specifically tracked the duration of group disturbance and also calculated the \"comfort distance\", which reflects the average shortest distance between the robot and social groups. The social navigation evaluation metrics are summarized in Table I. As shown in Fig. 7, GSON consistently outperformed the baseline methods on social navigation metrics, demonstrating superior ability to navigate in socially sensitive environments."}, {"title": "C. Real-world Evaluation of the GSON", "content": "We validated the effectiveness of GSON in real-world environments using the EAI-Bot E2, a two-wheeled mobile robot equipped with a YDLIDAR-G4 2d lidar and two RGB cameras. This platform was chosen for its suitability for social robot applications in service industries where high-speed navigation is not required. These experiments evaluated GSON's performance in navigating around groups of people, with a focus on short-range interactions. The small-scale experiments included scenarios with group sizes of 2 to 4 people performing actions such as talking, walking, taking pictures, and standing in line. These setups varied participants' initial positions, trajectories, and movement speeds to reflect complex social structures. We compared GSON's performance against established methods, including classical planners like TEB and DWA, as well as more recent approaches such as SACSON [16], a imtation learning based method, a Group-based Motion Prediction Controller (GMPC) [23], and a distributed DRL method for multi-robot collision avoidance (RLCA) [43]. The evaluation metrics are the same from IV-B to measure the personal and"}, {"title": "D. Real-world Long Range Social Navigation", "content": "We conducted a long-range demonstration test within a structured large-scale building which comprised a hall, corridors, and an open square. This extended demonstration integrated four smaller scenarios into a larger, more complex setting, where the robot was tasked with accurately identifying social structures and navigating through these environments, see Fig. 9. We define the start and goal points on the two sides of the map. Fig. 8 demonstrate some key frame of the navigation process in this long-range social navigation task. At approximately 10 seconds, the robot encounters a small group of individuals walking ahead. Using its perception and path planning algorithms, it identifies the group and adjusts its path, successfully avoiding the two people. At around 60 seconds, the robot approaches the entrance of an elevator where a queue of people has formed. It recognizes the queue and modifies its route to avoid trapass from it, without causing any disturbance. By 146 seconds, inside a narrow corridor, the robot detects a group of people engaged in conversation. Based on the group result, it calculates a seclusion path to avoid the group and maintain smooth navigation through the confined area. Finally, at about 191 seconds, upon entering an outdoor square, the robot encounters a group of individuals involved in photographing activities. It identifies their positions and reroutes its path around the group, ensuring unobstructed progress while respecting their activities. In summary, the long-range demonstration experiment successfully validated the robot's ability to navigate through diverse, real-world social environments while maintaining seamless interaction with its surroundings. The robot's performance in various challenging social scenarios, ranging from avoiding walking individuals to avoiding groups in narrow corridors and open spaces, highlights its advanced social structure reasoning capabilities."}, {"title": "V. CONCLUSION", "content": "In this work, we present a novel approach that integrates the visual reasoning capabilities of Large Multimodal Models (LMM) with a social structure-aware perception and planning system for mobile robots operating in human-centered environments. Our method utilizes the common sense reasoning ability of LMM by applying the visual prompting to predict the social relationships among pedestrians, which allows our planning system to generate socially aware behavior. Through extensive real-world navigation experiments, our approach demonstrated superior performance and robustness compared to baseline methods, illustrating its potential for improving social awareness and interaction in autonomous robot navigation. Future work could explore extending this framework to more dense social interactions and environments, combining the system with compact state representation [44], and also explore distilling knowledge into smaller models for faster inference."}]}