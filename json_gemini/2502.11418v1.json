{"title": "TimeCAP: Learning to Contextualize, Augment, and Predict Time Series Events with Large Language Model Agents", "authors": ["Geon Lee", "Wenchao Yu", "Kijung Shin", "Wei Cheng", "Haifeng Chen"], "abstract": "Time series data is essential in various applications, including climate modeling, healthcare monitoring, and financial analytics. Understanding the contextual information associated with real-world time series data is often essential for accurate and reliable event predictions. In this paper, we introduce TimeCAP, a time-series processing framework that creatively employs Large Language Models (LLMs) as contextualizers of time series data, extending their typical usage as predictors. TimeCAP incorporates two independent LLM agents: one generates a textual summary capturing the context of the time series, while the other uses this enriched summary to make more informed predictions. In addition, TimeCAP employs a multi-modal encoder that synergizes with the LLM agents, enhancing predictive performance through mutual augmentation of inputs with in-context examples. Experimental results on real-world datasets demonstrate that TimeCAP outperforms state-of-the-art methods for time series event prediction, including those utilizing LLMs as predictors, achieving an average improvement of 28.75% in F1 score.", "sections": [{"title": "Introduction", "content": "Time series data is fundamental to numerous applications, including climate modeling (Schneider and Dickinson 1974), energy management (Liu et al. 2023a), healthcare monitoring (Liu et al. 2023b), and finance analytics (Sawhney et al. 2020). Consequently, a range of advanced techniques has been developed to capture complex dynamic patterns intrinsic to time series data (Wu et al. 2021; Nie et al. 2023; Zhang and Yan 2022). However, real-world time series data often involves essential contextual information, the understanding of which is crucial for comprehensive analysis and effective modeling.\nThe rise of Large Language Models (LLMs), such as GPT (Achiam et al. 2023), LLaMA (Touvron et al. 2023), and Gemini (Team et al. 2023), has significantly advanced natural language processing. These multi-billion parameter models, pre-trained on extensive text corpora, have demonstrated impressive performance in natural language tasks, such as translation (Zhang, Haddow, and Birch 2023; Wang et al. 2023a), question answering (Li\u00e9vin et al. 2024; Shi et al. 2023; Kamalloo et al. 2023) and dialogue generation (Zheng et al. 2023; Qin et al. 2023). Their remarkable few-shot and zero-shot learning capabilities allow them to understand diverse domains without requiring task-specific retraining or fine-tuning (Brown et al. 2020; Yang et al. 2024; Chang, Peng, and Chen 2023). Furthermore, they exhibit sophisticated reasoning and pattern recognition capabilities (Mirchandani et al. 2023; Wang et al. 2023b; Chu et al. 2023), enhancing their utility across various domains, including computer vision (Koh, Salakhutdinov, and Fried 2023; Guo et al. 2023; Pan et al. 2023; Tsimpoukelli et al. 2021), tabular data analysis (Hegselmann et al. 2023; Narayan et al. 2022), and audio processing (Fathullah et al. 2024; Deshmukh et al. 2024; Tang et al. 2024).\nMotivated by the impressive general knowledge and reasoning abilities of LLMs, recent research has explored leveraging their strengths for time series (event) prediction. For instance, pre-trained LLMs have been fine-tuned using time series data for specific tasks (Zhou et al. 2024; Chang, Peng, and Chen 2023). Some studies introduce prompt tuning, where time series data is parameterized for input into either frozen LLMs (Jin et al. 2024; Sun et al. 2024) or fine-tunable LLMs (Cao et al. 2023). However, these methods typically use raw time series data or their parameterized embeddings, which are inherently distinct from the textual data that LLMs were pre-trained on, making it challenging for LLMs to utilize their rich semantic knowledge and contextual understanding capabilities. One approach to address this limitation is prompting LLMs with textualized time series data, supplemented with simple contextual information, in a zero-shot manner (Xue and Salim 2023; Liu et al. 2023b). However, the effectiveness of this approach is limited by the overly simplified contextualization of time series data.\nNotably, prior approaches leveraging LLMs for making predictions based on time series data have primarily focused on using LLMs as predictors. These methods either fine-tune LLMs or employ soft or hard prompting techniques, as illustrated in Figure 1 (a). This approach often overlooks the importance of contextual understanding in time series analysis, such as the impact of geographical or climatic influences in weather prediction or the interdependencies between economic indicators in financial prediction. By harnessing the domain knowledge and contextual understanding capabilities of LLMs, we can uncover potential insights that can"}, {"title": "Related Work", "content": "Large Language Models. In recent years, language models (LMs) such as BERT (Devlin et al. 2019), RoBERTa (Liu et al. 2019), and DistilBERT (Sanh et al. 2019) have evolved to large language models (LLMs) with multi-billion parameter architectures. 1 LLMs, including GPT-4 (Achiam et al. 2023), LLaMA-2 (Touvron et al. 2023), and PaLM (Anil et al. 2023), are trained on massive text corpora and have demonstrated impressive performance in various natural language tasks such as translation, summarization, and question answering. These models possess extensive domain knowledge and exhibit zero-shot generalization capability, enabling them to perform tasks without specific training on those tasks (Yang et al. 2024; Brown et al. 2020; Kojima et al. 2022). Additionally, they exhibit emergent abilities such as arithmetic, multi-step reasoning, and instruc-"}, {"title": "Proposed Method", "content": "We present our framework for predicting events based on time series using LLMs. We begin with the problem statement. Next, we present TIMECP, our initial method, which utilizes two LLM agents with distinct roles. Then, we describe TIMECAP, our ultimate version. Lastly, we discuss how TIMECAP offers interpretability for its predictions."}, {"title": "Problem Statement", "content": "We formally introduce LLMs and discuss the problem of time series event prediction.\nLarge Language Models. Let us define an LLM $M_\\theta$, parameterized by $\\theta$, which is pre-trained on extensive text corpora. We keep $\\theta$ fixed (i.e., frozen), employing LLMs in a zero-shot manner without any parameter updates or gradient computations, making them LMaaS-compatible. The LLM takes data of interest $D$ and optional supplementary data $S$ (e.g., demonstrations) to enhance understanding of $D$ and generate a more effective response $R$. Utilizing a prompt generation function $p$, a prompt $p(D, S)$ is constructed, e.g.,"}, {"title": "TIMECP: Contextualize and Predict", "content": "We introduce TIMECP, our preliminary method for LLM-based time series event prediction. TIMECP leverages the contextual information associated with time series data to enhance the comprehension and predictive capabilities of LLMs in a zero-shot manner.\nPromptCast (Xue and Salim 2023) is a direct counterpart to TIMECP, as it prompts the LLM with a textualized prompt of time series to make predictions. However, it focuses on using LLMs as a predictor and does not fully utilize their contextualization capabilities.\nTo address this limitation, TIMECP introduces two independent LLM agents, $A_c$ and $A_p$, which aim to better leverage the contextualization capabilities of LLMs for time series event prediction. The first agent, $A_c$, generates a textual summary $s_x$ that contains the underlying context of the given time series $x$ by leveraging its domain knowledge:\n$s_x = A_c(x) = M_\\theta(p_c(x))$,\nwhere $p_c(x)$ is a prompt that instructs the LLM to contextualize $x$. The generated summary, $s_x$, includes relevant contextual insights beyond the raw time series data $x$, which is then used by the second agent, $A_p$, to make more informed event predictions:\n$\\hat{y}_{LLM} = A_p(s_x) = M_\\theta(p_p(s_x))$,\nwhere $p_p(s_x)$ is a prompt that instructs the LLM to predict the outcome of the event based on $s_x$. By incorporating the context-informed summary generated by $A_c$, $A_p$ can account for the broader context in its predictions. As shown in Figure 1, our dual-agent-based approach consistently outperforms the single-agent approach (spec., PromptCast), which directly predicts the event from the input time series data, i.e., $M_\\theta(p_p(x))$. The enhanced accuracy demonstrates the effectiveness of generating and utilizing contextual information for future event predictions with LLMs."}, {"title": "TIMECAP: Contextualize, Augment, Predict", "content": "Building upon TIMECP, we present TIMECAP, an advanced version of our framework. TIMECAP trains a multi-modal encoder that synergizes with the LLM agents by introducing dual augmentations (spec., input and prompt augmentations) where the multi-modal encoder and LLM agents complement each other, enabling TIMECAP to make more accurate and reliable event predictions.\nMulti-Modal Encoder. We introduce a trainable encoder $E_\\phi$, parameterized by $\\phi$ (Figure 2 (a)). This encoder aims to capture intricate dynamic patterns in time series data more effectively than zero-shot LLMs. In addition to time series $x$, it incorporates the textual summary $s_x$ generated by $A_c$, which provides additional contextual insights beyond the raw time series data (i.e., input augmentation), as shown in Figure 2 (b). The encoder $E_\\phi$ generates its prediction $\\hat{y}_{MM}$ and the embedding $z$ of the multi-modal input $(x, s_x)$ as:\n$(\\hat{y}_{MM}, z) = E_\\phi(x, s_x)$,\nwhere $z$ is used for sampling in-context examples (Eq. (2)). The encoder consists of (1) a language model that embeds text into the latent space and (2) a transformer encoder that captures dependencies between the two modalities.\nThe corresponding text summary $s_x$ is processed by a pre-trained language model (LM) with substantially fewer parameters, which is thus relatively easier to fine-tune. Specifically, we represent $s_x$ using the LM as $2 = LM(s_x) \\in \\mathbb{R}^{d'}$, leveraging the output CLS token embedding. This representation is then projected as $z_{text} = 2W_{text} \\in \\mathbb{R}^d$ using a linear layer $W_{text} \\in \\mathbb{R}^{d' \\times d}$.\nFor time series, motivated by the effectiveness of patching (Nie et al. 2023; Zhang and Yan 2022), we segment"}, {"title": "Interpreting the Predictions", "content": "We explore how TIMECAP provides interpretations for its predictions by introducing two variants of the prompt function $p_p$ used in $A_p$. The resulting variants, $A_p^i$ and $A_p^e$, enable distinct interpretations, enhancing transparency.\nImplicit Interpretation. We prompt the LLM $M_\\theta$ to generate both a prediction and its corresponding rationale:\n$(\\hat{y}_{LLM}, r) = A_p(s_x, S) = M_\\theta(p_p(s_x, S))$,\nwhere $p_p$ is a prompt function that instructs $M_\\theta$ to predict the event $(\\hat{y}_{LLM})$ and also provide the rationale $(r)$ behind its prediction. This rationale leverages the LLM's domain knowledge and reasoning capabilities. While the in-context examples $S$ are optional, as shown in Section 4, their inclusion leads to distinct implicit interpretations.\nExplicit Interpretation. We prompt $M_\\theta$ to identify the most useful or relevant example from the in-context set $S$:\n$(\\hat{y}_{LLM}, s_{x_j^*}) = A_p^e(s_x, S) = M_\\theta(p_p^e(s_x, S))$,\nwhere $p_p^e$ is a prompt function that instructs $M_\\theta$ to predict the event $(\\hat{y}_{LLM})$ and select the most relevant example $(s_{x_j^*})$ from $S$. In addition, the input time series $x$ can be compared with the corresponding time series $x_{j^*}$ for further analyses."}, {"title": "Experiments", "content": "In this section, we present TIMECAP's: (1) accuracy compared with the state-of-the-art methods, (2) component effectiveness, (3) interpretability, and (4) additional analyses."}, {"title": "Experimental Settings", "content": "We first report the experimental settings. We use GPT-4 (Achiam et al. 2023) as the default backbone for the LLM agents and BERT (Devlin et al. 2019) as the LM within the multi-modal encoder. We describe the prompt functions in the supplementary document.\nDatasets and Tasks. We collected seven real-world time series datasets from three domains: weather, finance, and healthcare, for time series event prediction, as summarized in Table 1. Note that only time series data is provided, and the text data is generated by $A_c$. Below, we describe the datasets and their respective tasks in each domain."}, {"title": "Accuracy", "content": "We first report the predictive performance of TIMECAP and its competitors on time series event prediction.\nMain Results. Table 2 presents the performance of TIME-CAP, TIMECP, and their competitors across all datasets. TIMECAP achieves the best average performance and overall ranks. These results demonstrate the effectiveness of our"}, {"title": "Effectiveness", "content": "We verify the effectiveness of each component of TIMECAP through ablation studies as summarized in Table 3.\nContextualization. As shown in (1) in Table 3, TIMECP consistently and significantly outperforms PromptCast, which directly prompts LLMs to predict the future event, across all datasets. This demonstrates the effectiveness TIMECP's dual-agent approach in contextualizing time series data for event prediction.\nAugmentation. TIMECAP incorporates dual augmentations: $A_c$ generates textual summaries to augment the time series data (i.e., input augmentation), and the multi-modal"}, {"title": "Interpretability", "content": "We evaluate the interpretation provided by TIMECAP (see Section 3.4) through a case study, as illustrated in Figure 3.\nImplicit Interpretation. The presence of in-context examples significantly affects interpretation and prediction. Without in-context examples, the LLM predicts the outcome solely based on the input data, which can result in incorrect predictions. In contrast, with in-context examples, the LLM leverages past text-outcome relationships, leading to more informed predictions and interpretations.\nExplicit Interpretation. The LLM agent selects a text summary from the provided in-context examples that align with"}, {"title": "Further Analyses", "content": "We provide additional experimental results with TIMECAP.\nFew-Data Results. We evaluate TIMECAP and two leading competitors, PatchTST and GPT4TS, on a reduced training set, specifically reducing it to 10% of the default training ratio and even to 0%. As shown in Figure 4, while the competitors suffer from data scarcity, TIMECAP relatively maintains its performance with the reduced training size. Furthermore, it achieves high zero-shot performance, which demonstrates the effectiveness of TIMECAP in data-scarce scenarios, which is valuable for real-world applications.\nQuality of In-Context Examples. We evaluate the quality of in-context examples selected by our multi-modal encoder"}, {"title": "Conclusion", "content": "In this work, we introduce TIMECAP, a novel framework that leverages LLM's contextual understanding for time series event prediction. TIMECAP employs two independent LLM agents for contextualization and prediction, supported by a trainable multi-modal encoder that mutually enhances them. Our experimental results on seven real-world time series datasets from various domains demonstrate the effectiveness of TIMECAP."}]}